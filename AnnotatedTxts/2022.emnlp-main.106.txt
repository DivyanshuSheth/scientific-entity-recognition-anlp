  Jiao Ou , Jinchao Zhang , Yang Feng , Jie ZhouKey Laboratory of Intelligent Information Processing ,   Institute of Computing Technology , Chinese Academy of Sciences ( ICT / CAS)University of Chinese Academy of SciencesPattern Recognition Center , WeChat AI , Tencent Inc , China   { oujiao17b,fengyang}@ict.ac.cn , { dayerzhang,withtomzhou}@tencent.com   Abstract   The construction of open - domain dialogue sys-   tems requires high - quality dialogue datasets .   The dialogue data admits a wide variety of   responses for a given dialogue history , espe-   cially responses with different semantics . How-   ever , collecting high - quality such a dataset   in most scenarios is labor - intensive and time-   consuming . In this paper , we propose a data   augmentation method to automatically aug-   ment high - quality responses with different se-   mantics by counterfactual inference . Specifi-   cally , given an observed dialogue , our counter-   factual generation model first infers semanti-   cally different responses by replacing the ob-   served reply perspective with substituted ones .   Furthermore , our data selection method filters   out detrimental augmented responses . Experi-   mental results show that our data augmentation   method can augment high - quality responses   with different semantics for a given dialogue   history , and can outperform competitive base-   lines on multiple downstream tasks .   1 Introduction   Open - domain dialogue systems have attracted   much attention ( Chen et al . , 2017 ; Huang et al . ,   2020 ; Ni et al . , 2021 ; Fu et al . , 2022 ) due to their   potential applications . Generally , training open-   domain dialogue systems requires high - quality dia-   logue datasets . The dialogue data admits a wide va-   riety of responses for a given dialogue history ( Hou   et al . , 2018 ) . Specifically , a given dialogue his-   tory can exist many valid responses with different   semantics , and the response of each semantic in-   formation can also have abundant alternative ex-   pressions ( Li et al . , 2019 ) . However , manually col-   lecting high - quality such datasets is usually labor-   intensive and time - consuming in practice .   A feasible solution to address this problem is   to use data augmentation techniques . Currently , Figure 1 : An example of a counterfactual response ,   which is a semantically different response re - inferred   by changing the observed reply perspective .   some data augmentation methods have been used in   open - domain dialogues ( Sennrich et al . , 2016 ; Niu   and Bansal , 2019 ; Li et al . , 2019 ; Cai et al . , 2020 ;   Zhang et al . , 2020a ; Xie et al . , 2022 ) to augment   data . However , the augmented data have limited   semantic differences from the observed data based   on the restrained changes . These existing methods   only consider word- or sentence - level alternative   expressions of the observed data without augment-   ing more valid responses with different semantics .   In this paper , we propose to augment valid re-   sponses with different semantics for a given dia-   logue history . Imagine that when humans infer   different - semantic responses , they may naturally   ask a question : Given an observed dialogue , what   the response would happen if we change the reply   perspective , while keeping the current environment   unchanged ? Answering this question will infer a   different response , given an example shown in Fig-   ure 1 . The imagination of different responses under   the current environment is so - called counterfactual   inference ( Pearl et al . , 2000 ) , which ensures the   quality of inferred responses ( Zhu et al . , 2020 ) .   Motivated by this , we propose a Counterfactual   data Augmentation method via Perspective   Transition , CAPT for short , to generate counter-   factual responses for a given observed dialogue .   CAPT interprets a counterfactual generation model   as a structural causal model ( SCM ) , which de-1635scribes the generation process under the current   environment . The current environment is modeled   by unobserved variables in the SCM that capture all   unobserved but relevant factors that affect response   generation . Counterfactual responses are then gen-   erated by intervening in the reply perspective in the   SCM , i.e. , replacing the observed reply perspec-   tive with valid alternatives , while keeping these   unobserved variables unchanged . To achieve an   alternative , we first construct a shift graph based on   all observed dialogues , which explicitly represents   the shift associations between both focuses of atten-   tion on dialogue histories and their corresponding   responses respectively . We then randomly choose   a focus on the given dialogue history and regard   its 1 - hop neighbors in the shift graph as candidates .   A valid alternative can be predicted from these   candidates . After achieving all counterfactual aug-   mented responses , the augmented data are further   filtered using a data selection module . Finally , we   merge the observed data with this augmented data   as training data for downstream tasks .   Experiment results indicate that CAPT can aug-   ment high - quality responses with different seman-   tics , and our augmented data contributes to the   performance improvement of both retrieval - based   and generation - based open - domain dialogue mod-   els . Our contributions are summarized as follows :   ( 1 ) We propose a counterfactual data augmenta-   tion method via perspective transition to augment   responses with different semantics for a given dia-   logue history . To the best of our knowledge , this is   the first study to augment more valid responses with   different semantics in open - domain dialogues . ( 2 )   Automatic and manual evaluation show that CAPT   generates semantically different responses , which   can be further used to improve the performance   of downstream tasks . ( 3 ) Extensive experiments   show that providing more responses with different   semantics can further improve performance .   2 Background   In this section , we describe task definitions and   review the concept of the structural causal model .   Please see task definitions in Appendix A.   2.1 Structural Causal Model   Definition . A structural causal model ( SCM )   consists a set of observed variables V=   { V , . . . , V}and a set of independent unob-   served random variables U={U , . . . , U}withdistribution P(U ) , which are connected by a set of   functions F={f , . . . , f } . Specifically , ∀i , V   is caused by a set of parent variables PAandU ,   i.e. ,V = f(PA , U ) , where PA⊆V\Vin   the causal DAG ( Buesing et al . , 2019 ) .   For the counterfactual generation model , it can   be cast as an SCM with three observed vari-   ables , including dialogue history X , reply per-   spective Zandresponse Y. The counterfactual   generation SCM turns the conditional distribution   P(Y|X , Z)into a deterministic function Y=   f(X , Z , U ) , where Ucaptures all unobserved but   influential factors of the current environment , such   as speaking style . The function fis defined by the   learned counterfactual generation model . Overall ,   SCM can infer counterfactual responses given the   known function fand the posterior of the unob-   served variable P(U|X = x , Z = z , Y = y ) .   Intervention . Before observing what the ob-   served variable Vwould happen , an interven-   tion would be given on a parent variable V ,   V∈PA , where the intervention in the SCM   is an action by changing the observed value . For   the counterfactual generation SCM , the interven-   tion is to replace the observed value zof the reply   perspective Zwith a different value ˜z .   Counterfactual Inference . Given an SCM and   observed a variable V = v , counterfactual infer-   ence answers the question that what the observed   variable Vwould have changed if a parent vari-   ableVhas been intervened while keeping the   current environment unchanged . Accordingly , gen-   erating a counterfactual response involves a query   about what the response Ywould have happened if   an intervention is taken by setting Zas a different   value˜z , rather than the observed value z.   Overall , to generate counterfactual responses ,   we can follow a three - step procedure ( Pearl et al . ,   2016 ): ( 1 ) Abduction : Predict the “ current envi-   ronment of the SCM ” , i.e. , compute the posterior   P(U|X = x , Z = z , Y = y)and sample u   from it . ( 2 ) Action : Perform an intervention by re-   placing the observed value zofZwith a different   value ˜z . ( 3 ) Prediction : Reason a counterfactual   response ˜y , given the posterior sample uand the   known function f.   3 Method   In this section , our goal is to take an input dialogue   sample ( x , y)and augment high - quality responses1636   that have different semantics from y. To this end , in   Section 3.1 , we introduce a technique called Coun-   terfactual Generation via Perspective Transition   for intervening in the observed reply perspective   to augment responses under the current environ-   ment . In Section 3.2 , we describe how to train   those models involved in Section 3.1 , including   the reply perspective predictor and the counterfac-   tual generator . In Section 3.3 , we design a data   selection method , named Bi - directional Perplexity   Selection , to select high - quality augmented data .   3.1 Counterfactual Generation via   Perspective Transition   This paper mainly focuses on single - turn dialogues .   Given a post - response pair ( x , y ) , we use the SCM   to generate a counterfactual response ˜yfollowing   the three - step procedure shown in Figure 2 .   1 . Abduction . This step is to estimate the unob-   served variable given the observed sample ( x , z , y )   ( for more details about zsee the action step ) .   Specifically , when generating the t - th token of y ,   our counterfactual generator outputs a categorical   distribution P(Y|X = x , Z = z , Y = y ) ,   where yis the token sequence generated in the   previous time step . According to Oberst and Son-   tag ( 2019 ) , the impact of the unobserved random   variable Uis simulated by introducing Gumbel   random noises . Thus , we perform the Gumbel - Max   Trick ( Luce , 1959 ) for this categorical distributionas follows ,   p = P(Y = k|X = x , Z = z , Y = y ) ,   y = arg max(logp+u ) ,   ( 1 )   where u∼Gumbel ( 0,1)and|V|denotes the   vocabulary size .   Consequently , our counterfactual generation   SCM transforms into a Gumbel - Max SCM ( Oberst   and Sontag , 2019 ) . The estimation of the unob-   served variable is then to sample from the poste-   rior distribution over these Gumbel random vari-   ables . Fortunately , a straightforward way to in-   fer posterior ( Maddison et al . , 2014 ) is utilizing   the properties of the shifted Gumbel variables   g= log p+u : in the posterior , the maxi-   mum value is independent with the argmax of the   shifted Gumbel variables and is distributed as a   standard Gumbel . Thus , we first let y = k ( *   denotes the observed token ) and sample the maxi-   mum value gfrom Gumbel ( 0,1 ) . Secondly , we   sample the remaining values gfrom the shifted   Gumbel distribution Gumbel ( logp,1)truncated   atg . Then , for each index of k , a sample of u   is obtained by subtracting off the location parame-   terlogpfrom g. Finally , the resulting sample   u= [ u , . . . , u]is used to infer the counter-   factual responses .   2 . Action . This step is to replace the observed   reply perspective zwith a substituted reply per-1637spective ˜z . However , two sub - problems need to   be addressed : representing the reply perspective   andpredicting a substituted valid reply perspective .   By observing human dialogues , we find that a re-   ply perspective can be represented by a keyword ,   like “ stop smoking ” in Figure 2 . It can be achieved   based on the process that humans first naturally fo-   cus on a certain point of a given post like “ smoking ”   and then would unconsciously shifting this focus   point to another one . The focus point of the post   can be similarly represented by a keyword . We   name the focus point on the post and the shifted   one as the focus andreply perspective respectively .   When humans have different focuses ( e.g. , “ health ”   in Figure 2 ) or different shifts on the same focus ,   they will obtain substituted reply perspectives .   To achieve valid alternatives , it is critical to make   valid shifts from a focus . We build a shift graph   based on all observed samples , where head and tail   vertices are focuses and reply perspectives respec-   tively , and edges represent observed shifts between   focuses and reply perspectives . Inspired by Xu   et al . ( 2020 ) and Zou et al . ( 2021 ) , we can regard   1 - hop neighbors of a given focus as candidates and   predict a valid alternative from these candidates . It   is based on the fact that the corresponding reply   perspectives can be shared if posts containing the   same focus have similar semantics .   We build the shift graph Gwith two steps : ver-   tex construction and edge construction . For vertex   construction , we first exploit a rule - based keyword   extraction method ( Campos et al . , 2020 ) to iden-   tify salient keywords from utterances in the ob-   served dialogue dataset D. To further identify the   focuscfrom all keywords of x , we use guidance   from the future information ( i.e. , response ) to se-   lect the keyword that is semantically closest to y.   To identify the reply perspective z , we select the   keyword with the closest semantics to c. More   concretely , we use cosine similarity between their   embedding via BERT ( Devlin et al . , 2019 ) as the   measure of semantic closeness , where each em-   bedding is achieved by taking the average of the   hidden state of each token . For edge construction ,   we build an edge by connecting cwithz . In this   way , we characterize all shift associations in D.   Once the shift graph is built , we predict ˜zas   ˜z= arg maxP(Z|C=˜c , X = x , N = N(˜c ) ) ,   ( 2 )   which is given by a trained reply perspective pre-   dictor . Note that ˜ccan be any keyword in the postxandN(˜c))denotes 1 - hop neighbors of ˜c .   3 . Prediction . This step is to generate the coun-   terfactual response given the posterior sample u=   [ u , . . . , u ] . Specifically , when generating the   t - th token of the counterfactual response , our coun-   terfactual generator computes the categorical distri-   bution as follows ,   ˜p = P(Y = k|X = x , Z=˜z , Y=˜y ) ,   ˜y = arg max(log ˜p+u ) ,   ( 3 )   where ˜zis the predicted reply perspective , ˜yis   the token sequence generated in the previous step .   Overall , counterfactual generation via perspec-   tive transition can be used as an effective data aug-   mentation method for open - domain dialogues to   augment responses with wider semantic coverage .   We show this method in Algorithm 1 . The algo-   rithm takes an observed sample ( x , y)as an in-   put and loop through every keyword of xas a   different focus ˜c . For each ˜c , to sample multi-   ple corresponding reply perspectives , we equally   divide the candidate set N(˜c)intoKsub - sets   { N(˜c ) , . . . , N(˜c)}for nested loop . At each iter-   ation it predicts a different ˜zfor perspective transi-   tion to output a counterfactual sample ( x,˜y ) .   3.2 Model Training   CAPT relies on the reply perspective predictor and   the counterfactual generator , which greatly influ-   ence the quality of augmentation . Inspired by Yang   et al . ( 2020 ) ; Schick and Schütze ( 2021 ) , we choose   a pre - trained encoder - decoder model BART ( Lewis   et al . , 2020 ) as the backbone model .   Reply Perspective Predictor . We fine - tune   BART on Dto learn P(Z|C , X , N ) . In particular ,   the input is a concatenated text sequence consisting   of the post X , the focus C , and the candidates N.   The output is the predicted reply perspective Z.   We maximize the objective as follows ,   L=−/summationdisplaylogP(Z|[C , X , N],Z),(4 )   where the bracket [ · , · , · ] denotes concatenation   with the token [ SEP ] . The candidates Nare also   concatenated with commas . Zis a prefix of the   reply perspectives . |Z|denotes the length of Z.   Counterfactual Generator . We fine - tune BART   onDto learn P(Y|X , Z ) . Specifically , the gen-   erator is trained to generate the response Ywith1638Algorithm 1 : Data Augmentation   Input : ( x , y ): An observed sample   C : All keywords { ˜c , . . . , ˜c}ofx   G : The shift graph   Output : A counterfactual sample ( x,˜y)Get the observed reply perspective z;fori←1to|C|do Get 1 - hop neighbors N(˜c)fromG Remove zfromN(˜c ) Equally divide N(˜c)into   { N(˜c ) , . . . , N(˜c ) } forj←1toKdo ˜y←Trans ( x , y , z,˜c , N(˜c))Function Trans ( x , y , z,˜c , N(˜c ) ): InferufromP(U|x , y , z ) Predict ˜zfromP(Z|x,˜c , N(˜c ) ) Reason ˜yfromP(Y|x,˜z)under the   current environment u return ˜y   the input prompt consisting of the post Xand the   reply perspective Z. Similarly , we maximize the   following objective :   L=−/summationdisplaylogP(Y|[X , Z],Y),(5 )   3.3 Bi - directional Perplexity Selection   Filtering out detrimental augmented samples can   improve downstream performance ( Bras et al . ,   2020 ) . Existing methods ( Axelrod et al . , 2011 ;   Xie et al . , 2020 ; Zhang et al . , 2020a ) pick out sam-   ples that the model only trained on the observed   data is most confident about . However , these mod-   els have only seen limited samples so they may   not identify valid but unseen samples from the   counterfactual - generated data . Inspired by Lee   et al . ( 2021 ) , we leverage a large - scale dialogue pre-   trained language model DialoFlow ( Li et al . , 2021 ) ,   utilizing its powerful ability of transfer learning .   Since large - scale dialogues have been seen , it can   identify valid but unseen samples like “ an expert ”   via perplexity ( PPL ) scores . Nonetheless , the re-   sulting samples might contain samples with generic   responses . Inspired by Li et al . ( 2016 ) , we further   introduce backward PPL to rerank responses for   prioritizing those valid and interesting samples .   Specifically , we independently fine - tune Di-   aloFlow to learn P(Y|X)andP(X|Y)onD   for calculating forward andbackward PPL scores . Once we obtain the forward PPL scores for all   samples , we find the best threshold ηthat sepa-   rates valid samples from invalid samples . Inspired   by Lee et al . ( 2021 ) , we leverage the validation set   to find the optimal single threshold parameter η ,   where we regard observed samples from the val-   idation set as valid samples , and invalid samples   are constructed by replacing the responses of valid   samples with randomly - sampled responses . Fur-   thermore , we rerank the responses of each post in   the valid samples via backward PPL scores . Since   the higher the backward PPL score , the more likely   the response is dull ( Li et al . , 2016 ) , we choose   samples in order from low to high until the desired   number of augmented samples are obtained .   4 Experimental Setup   4.1 Settings   The experiments are conducted on the Chinese   Weibo corpus ( Zhang et al . , 2020a ) . Specifically ,   the dataset Dcontains training , validation , and test   sets with 300K,5 K , and 10 K post - response sam-   ples , respectively . Please see Appendix B for more   details on data and method implementations .   4.2 Baselines   We compare CAPT with a set of baselines : ( 1 )   Observed , which only uses the observed data to   fine - tune dialogue models . ( 2 ) Augmented , which   only uses our augmented data to fine - tune dialogue   models . ( 3 ) Back - Trans ( Sennrich et al . , 2016 ) ,   which back - translates responses via Google Trans-   late . ( 4 ) MLM ( Cai et al . , 2020 ) , which fine - tunes   the BERT - large model on Dto substitute some   words of responses . The substituting probability   is 0.15 . ( 5 ) DL(Zhang et al . , 2020a ) , which con-   structs post - response pairs where both post and   response are retrieved from the unpaired data . Aug-   mented dialogues are further filtered by their rank-   ing module . ( 6 ) BM25 ( Gangal et al . , 2021 ) , which   uses the BM25 algorithm to retrieve the top - k simi-   lar post to the observed post , and the corresponding   response of the retrieved post is regarded as the aug-   mented response . ( 7 ) BART ( Lewis et al . , 2020 ) ,   which fine - tunes the BART - large model that takes   the post as the input to generate responses with dif-   ferent decode strategies , including greedy search ,   sampling with temperature 0.5 , and top - k sampling   ( k=10,25 ) . They are denoted as BART - greedy ,   BART - samp , BART - k10 , and BART - k25 , respec-   tively . Augmented pairs generated by BM25 and1639   BART are filtered by our data selection method .   4.3 Evaluation Metrics   Automatic Evaluation . The following metrics   are used to automatically evaluate retrieval - based   models . ( 1 ) Mean Average Precision ( MAP ): the   average of Average Precision ( AP ) over test sam-   ples . AP is the average of precision scores at the   ranks where references are found ; ( 2 ) R@k : the   percentage of references among the top - k selected   responses ( k=1,2,5 ) when given 10 candidates in   total . The following metrics are used to evaluate   generation - based models . ( 1 ) BLEU : the overlap   of n - grams ( n<4 ) between the generated response   and the reference . ( 2 ) Dist - n : the ratio of unique   n - grams ( n=1,2 ) over all n - grams in the generated   responses , which measures the n - gram diversity .   As we sample 3 responses for each test post , eval-   uation is performed both within and among the   sampled responses . Intra - Dist calculates that ra-   tio within each sampled response , and Inter - Dist   calculates that ratio among all three responses . ( 3 )   BS : the F1 - value of BERTScore ( Zhang et al . ,   2020b ) , which measures the semantic similarity   between each 2 responses in 3 sampled responses .   Lower scores imply greater semantic diversity .   We also use Dist - n andBSto automatically   evaluate the quality of augmented data , which eval-   uates the diversity among the generated responses .   In addition , we introduce the following metrics   to evaluate the diversity with respect to the orig-   inal response . ( 1 ) Novelty - n : the ratio of new n-   grams ( n=1,2 ) in the augmented responses . Intra-   Novelty similarly calculates the ratio within each   augmented response , i.e. , n - grams that are covered   by the augmented response but not in the original   response . Inter - Novelty calculates the ratio within   the three augmented responses . ( 2 ) BS : the F1 - value of BERTScore , which measures the semantic   similarity between the augmented response and its   corresponding original response .   Manual Evaluation . The following metrics are   used to manually evaluate the quality of augmented   data and generation - based models . Three anno-   tators are employed to rate the samples . ( 1 ) Flu-   ency ( Flu . ) : is the response fluent ? ( 2 ) Coherence   ( Coh . ) : is the response serve as a valid continua-   tion of the preceding post ? ( 3 ) Interesting ( Int . ) :   is the response generic ? ( 4 ) Richness ( Rich . ) : do   the three sampled responses express different se-   mantics ? The rating scale is of 0 to 2 , in which 0   means worst and 2 best .   5 Results and Discussion   5.1 Evaluating Augmented Data   We first evaluate the quality of augmented data .   Specifically , we respectively select 900 K aug-   mented post - response pairs generated by these   methods , on which automatic evaluation is per-   formed . We further conduct manual evaluation on   600 samples , which contain 200 randomly - sampled   posts and each post has 3 corresponding responses .   The inter - annotator agreement is measured via the   Fleiss ’s kappa κ(Randolph , 2005 ) . The κvalues   forFluency , Coherence , Interesting andRichness   are 0.67 ( moderate agreement ) , 0.46 ( moderate   agreement ) , 0.64 ( moderate agreement ) and 0.69   ( moderate agreement ) , respectively .   The results are shown in Table 1 and 2 , which   indicates that our augmented data outperforms all   the baselines . We further observe that : ( 1 ) Our   augmented data achieve similar scores as the ob-   served data over all the metrics , which indicates   that our augmented data is high - quality . We present   some cases of the augmented data to show the gen-1640   eration process of different - semantic responses in   Figure 3 . ( 2 ) Our augmented data achieve better   scores of BS , BSand Richness , which indicates   that CAPT can augment more responses with differ-   ent semantics . In particular , BART - samp vs. CAPT   shows the effectiveness of intervention in the re-   ply perspective . ( 3 ) BART - k10 achieves relatively   good scores on all the metrics compared to other   baselines . This indicates that the top - k sampling   ( k=10 ) is superior to the other decoding strategies .   Thus , the top - k sampling ( k=10 ) can be used for   the following generation - based models .   5.2 Evaluating Dialogue Model   We further evaluate the benefit of our augmented   data on retrieve - based and generation - based dia-   logue models . Specifically , we follow Zhang et al .   ( 2020a ) and select 300 K augmented post - response   samples for all methods for a fair comparison . We   conduct automatic evaluation on 5 K test data and   manual evaluation on 600 samples that contain   200 randomly - sampled posts with 3 generated re-   sponses . The κvalue for Fluency , Coherence , In-   teresting andRichness are 0.67 ( moderate agree-   ment ) are 0.71 ( substantial agreement ) , 0.59 ( mod-   erate agreement ) , 0.48 ( moderate agreement ) and   0.53(moderate agreement ) , respectively .   The results on retrieve - based and generation-   based models are respectively shown in Table 3   and 4 , which indicates that CAPT outperforms all   the baselines on almost all the metrics for both   dialogue models . This confirms the effectiveness   of augmenting valid responses with different se-   mantics . We can further observe that : ( 1 ) CAPT   achieves higher scores for almost all the metrics   compared to other BART - based methods , espe-   cially BART - samp . This demonstrates that inter-   vention in the reply perspective is effective for im-   proving the performance of dialogue models . ( 2 )   CAPT achieves higher BSand Richness ratings   but a relatively lower BLEU score . We speculate   that augmenting more semantically different sam-   ples enables dialogue models to generate more re-   sponses that differ from references .   5.3 Further Discussion   Further , we also investigate the impact of the   amount of augmented responses and the effect of   each component of CAPT.1641   The Impact of Amount . We select 0x , 1x , 2x , 3x   the amount of training samples to assess the impact   of providing more responses and compare CAPT   with the baseline , i.e. , BART - samp . Note that 3x   represents that 3 * 300 K augmented post - response   samples are selected . Considering that samples se-   lected in order have different interesting degrees ,   we eliminate the impact of interesting by uniformly   selecting 900 K augmented samples and randomly   select from them . The results are shown in Figure 4 .   We can observe that : ( 1 ) The MAP score on BART-   samp reaches a peak at 2x and drops afterward , and   BSkeeps increasing from 0x to 3x augmentation .   We speculate that BART - samp only outputs alter-   native expressions with diversified words , which   have limited semantic differences . Augmentation   of similar samples at high amounts would nega-   tively affect training . ( 2 ) However , the MAP score   on CAPT keeps increasing and BSdoes not in-   crease . This indicates that CAPT can augment   responses with different semantics , and providing   more semantically different responses can further   improve the performance of downstream tasks .   Ablation Study . We perform the following abla-   tion tests to validate the effect of each component :   ( 1 ) Randomly choose a keyword from candidates   as the reply perspective without the prediction step   ( -Predictor ) ; ( 2 ) Only take the post and the focus as   the input to the predictor without 1 - hop neighbors   as candidates ( -Candidate ) ; ( 3 ) Do not filter out   the augmented data via data selection ( -Selection ) ;   ( 4 ) Leverage a general pre - trained language model   GPT2 , which does not see enough dialogue sam-   ples , to replace the dialogue pre - trained language   model DialoFlow ( -Dial PLM ) ; ( 5 ) Only use the for-   ward PPL scores to filter out invalid samples with-   out ranking via the backward PPL scores ( -Back   PPL ) . ( 6 ) Generate responses not under the cur-   rent environment , i.e , without the posterior Gum-   bel noises ( -Gumbel ) . The results are shown in   Table 5 . We observe that ablating each component   brings varying degrees of performance drop . This   demonstrates the necessity of designing all these1642components .   6 Related Work   Data Augmentation . Data augmentation has   been widely used in various NLP tasks and sur-   veyed by Shorten and Khoshgoftaar ( 2019 ) ; Wen   et al . ( 2021 ) ; Feng et al . ( 2021 ) ; Ni et al . ( 2021 ) ;   Chen et al . ( 2021 ) . Overall , data augmentation   methods either add slightly modified copies of ex-   isting data or create synthetic data . Some work   propose to use heuristic rules ( Du and Black , 2018 )   or paraphrasing - based methods ( Niu and Bansal ,   2019 ; Li et al . , 2019 ; Cai et al . , 2020 ; Zhang et al . ,   2020a ; Xie et al . , 2022 ; Cao et al . , 2022 ) . An-   other line of work ( Chang et al . , 2021 ; Yang et al . ,   2020 ; Schick and Schütze , 2021 ; Wang et al . , 2021 ;   Zheng et al . , 2022 ) is exploiting large - scale pre-   trained language models for data augmentation .   However , these existing methods do not focus on   creating semantically different responses .   Semantically Different Augmentation . Gangal   et al . ( 2021 ) utilizes knowledge sources , includ-   ing COMET ( Bosselut et al . , 2019 ) and corpus   retrieval ( Robertson et al . , 1994 ) to augment seman-   tically diverse references for dialogue evaluation .   Both methods only pre - define limited augmented   perspectives . In contrast , CAPT obtains richer re-   ply perspectives by building a shift graph .   Counterfactual Inference . Our work is based   on counterfactual inference ( Pearl et al . , 2000 ) ,   which has shown promising results in various NLP   tasks , including question answering ( Paranjape   et al . , 2022 ; Yu et al . , 2021 ) , machine transla-   tion ( Liu et al . , 2021 ) and story generation ( Qin   et al . , 2019 ; Hao et al . , 2021 ; Chen et al . ) . In par-   ticular , Zhu et al . ( 2020 ) uses counterfactual infer-   ence for response generation , which explores poten-   tial responses via counterfactual off - policy training .   However , CAPT focuses on counterfactual data   augmentation , which can be used to improve the   performance of multiple downstream tasks .   Graph Construction . Some researches ( Xu   et al . , 2020 ; Zou et al . , 2021 ) also build a graph   to manage concept shifts for response generation ,   which aims to form a more coherent and control-   lable dialogue . In contrast , CAPT builds a shift   graph to predict valid substituted reply perspec-   tives , which are used to augment responses with   different semantics . Due to the different purposesof use , our graph construction is different from   these existing works .   7 Conclusion   This paper presents a counterfactual data augmen-   tation method , CAPT , to augment more responses   with different semantics for a given dialogue his-   tory . Specifically , CAPT employs counterfactual in-   ference to generate counterfactual responses by in-   tervening in the observed reply perspective , which   replaces with different reply perspectives for gen-   erating semantically different responses . Experi-   mental results show that CAPT can augment high-   quality responses with different semantics , which   can be further used to improve the performance   of downstream tasks . In future work , we plan to   explore an appropriate training strategy for further   preventing dialogue models from being affected by   noises in our augmented data , and extend CAPT on   multi - turn dialogues . We hope that CAPT will en-   courage future research for other generation tasks .   Limitations   CAPT works well in scenarios with a certain   amount of observed data . A small amount of ob-   served data would lead to a small - scale shift graph .   Thus , it is difficult to provide enough candidates   to pick out more valid reply perspectives , and then   augment sufficient valid post - response samples . In   addition , CAPT may be more suitable for open-   domain dialogue augmentation in some languages   that require good - quality keyword extraction meth-   ods and pre - trained models for that language . e.g. ,   Chinese and English . When transferred to different   languages , e.g. , English , the modifications are re-   quired as follows : ( 1 ) use the English - version key-   word extraction method and keyword / sentence en-   coder when building the graph ; ( 2 ) use the English-   version pre - trained model as the backbone model   for the reply perspective predictor and the counter-   factual generator .   Ethics Statement   In this work , we employ three annotators to man-   ually evaluate the quality of augmented data and   generation - based dialogue models . We pay $ 0.2to   each annotator for each sample .   Acknowledgement   We sincerely thank the anonymous reviewers for   their thorough reviewing and valuable suggestions.1643References164416451646A Task Definitions   Response Selection . Given a dataset D=   { ( x , y , l ) } , the retrieval - based dialogue   model learns a matching function to correctly   identify the positive response from a set of neg-   ative responses . Specifically , the matching func-   tionP(l|x , y)predicts whether the response y   matches the dialogue history x.l∈ { 0,1}de-   notes a matching label , which indicates that yis a   proper response for xifl= 1 , otherwise l= 0 .   The model parameters θcan be learned by mini-   mizing the loss function that is formulated as   L=−/summationdisplay[llogP(l= 1|x , y )   + ( 1−l ) logP(l= 0|x , y)](6 )   Generally , the training negative responses are ran-   domly selected from the dataset D.   Response Generation . Given a dataset D=   { ( x , y ) } , the generation - based dialogue   model learns to model the distribution P(y|x )   of the response ygiven the dialogue history x.   The model parameters ϕcan be learned by mini-   mizing the following loss :   L=−/summationdisplaylogP(y|x ) ( 7 )   However , a dialogue dataset that admits multi-   ple semantically different responses for each dia-   logue history is usually expensive to collect , as it   requires annotators to write a large variety of valid   responses . Although such a dataset can be crawled   from social networks , it will contain many noisy   and meaningless responses . It is also expensive to   pick out sufficient high - quality dialogues that meet   requirements . Thus , counterfactual data augmen-   tation aims to further augment different - semantic   responses ˜yforxinDwithout manually collect-   ing new data . In the following sections , we will   omit the superscript ifor simplicity .   B Experimental Details   B.1 Data   The experiments are conducted on the Chinese   Weibo corpus ( Zhang et al . , 2020a ) . Specifically ,   the dataset Dcontains training , validation , and test   sets with 300K,5 K , and 10 K post - response sam-   ples , respectively . To build the shift graph , we ap-   ply YAKE ( Campos et al . , 2020 ) that relies on thestatistical features of the text to automatically ex-   tract the most important keywords of each utterance   in the training data . Keywords are limited to nouns ,   adjectives and verbs . The number of keyword ver-   tices and edges are 77,439and202,266respec-   tively . Furthermore , we randomly sample 200   post - response samples and employ three human   annotators to evaluate the appropriateness of both   keywords of focus and reply perspective . About   86 % keyword pairs are accepted by the annota-   tors . The average number of candidate keywords   at training and augmentation times are 102 and   124 respectively . After achieving augmented data ,   we similarly evaluate whether the responses share   similar core semantics with the given reply perspec-   tives . About 96.5%responses are accepted by the   annotators .   B.2 Implementation Details   CAPT . For graph construction , we pursue bert-   as - service ( Xiao , 2018 ) to achieve the embedding   by mapping a variable - length text sequence to a   fixed - length vector . Our predictor and generator   are independently fine - tuned on the BART - large   model ( Shao et al . , 2021 ) using the loss in Eq . 4   and 5 for ten epochs , with the batch size of 64 , the   learning rate of 1e-5 . The other hyper - parameter   setting follows that of Shao et al . ( 2021 ) . The max-   imum sequence length is set to 512 . We thus limit   the maximum candidate size of our predictor to   100 . If the candidate size is greater than 100 , we   randomly sample 100 candidates . We then filter   out those samples whose candidate size is less than   5 . For data selection , we implement the score func-   tions by fine - tuning the pre - trained DialoFlow ( Li   et al . , 2021 ) model with Dfor two epochs , with the   batch size of 64 and the learning rate of 1e-5 . The   best threshold ηis 10 .   At augmentation time , we also limit the range   of the candidate size from 5to100 . Thus , we   divide the whole candidate set into Ksub - sets   and set the candidate size of each sub - set N=   max(min(,100),5 ) , where Kis initialized   by 20 . We further update K=. The predic-   tor outputs reply perspectives with greedy search .   The generator samples counterfactual responses   from posterior Gumbel noises , the temperature is   set to 0.5 .   Retrieve - based Model . The retrieve - based   model is built by fine - tuning the pre - trained   BERT - base ( Devlin et al . , 2019 ) for two epochs,1647with the learning rate of 2e-5 , the batch size of 64 ,   and the max sequence length of 512 . we adopt the   last checkpoint for evaluation .   Generation - based Model . The generation - based   model is built by fine - tuning the pre - trained BART-   large ( Shao et al . , 2021 ) for five epochs , with the   learning rate of 1e-5 , the batch size of 64 , and the   max sequence length of 512 . At inference time ,   we use the top - k sampling ( k=10 ) , and the maxi-   mum decoded length is set to 50 . we adopt the last   checkpoint for evaluation .   Training and Evaluation . We train retrieve-   based dialogue models with 4 GPUs , generation-   based models with 8 GPUs , the reply perspective   predictor with 8 GPUs , and the counterfactual gen-   erator with 8 GPUs . We use Nvidia Tesla V100   GPUs . The training time for retrieve - based mod-   els , generation - based models , the reply perspective   predictor , and the counterfactual generator is ap-   proximately 2h , 4h , 4h and 5h , respectively . At   augmentation time , it takes 55min to predict re-   ply perspectives and 1h to generate counterfactual   responses for all augmented samples . When cal-   culating the forward and backward PPL scores , it   takes 40min respectively.1648