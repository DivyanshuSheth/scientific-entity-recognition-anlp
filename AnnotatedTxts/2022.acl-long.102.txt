  Esin DurmusFaisal LadhakTatsunori HashimotoStanford UniversityColumbia University   esindurmus@cs.stanford.edu faisal@cs.columbia.edu   thashim@stanford.edu   Abstract   Model - based , reference - free evaluation metrics   have been proposed as a fast and cost - effective   approach to evaluate Natural Language Genera-   tion ( NLG ) systems . Despite promising recent   results , we find evidence that reference - free   evaluation metrics of summarization and di-   alog generation may be relying on spurious   correlations with measures such as word over-   lap , perplexity , and length . We further observe   that for text summarization , these metrics have   high error rates when ranking current state - of-   the - art abstractive summarization systems . We   demonstrate that these errors can be mitigated   by explicitly designing evaluation metrics to   avoid spurious features in reference - free evalu-   ation .   1 Introduction   Building reliable automated evaluation metrics is   a key factor for quick development of better NLG   systems . Recent work has proposed reference - free   evaluation metrics as a way to judge the quality of   generated outputs without the need for human ref-   erences ( Celikyilmaz et al . , 2020 ) . Many of these   reference - free evaluations achieve remarkably high   correlations with human evaluations , raising hopes   that they may soon become a viable alternative   to expensive human evaluations ( Kryscinski et al . ,   2020 ; Goyal and Durrett , 2020 ; Sinha et al . , 2020 ;   Phy et al . , 2020 ; Gao et al . , 2020 ) .   However , simply looking at correlation with hu-   man scores may not be sufficient to determine the   efficacy and robustness of an evaluation metric . In   our work , we study recently proposed reference-   free evaluation metrics of text summarization and   dialog generation . We find that it is possible to   achieve similar levels of correlation with human   judgment , using simple spurious correlates such   as word overlap , length , and perplexity . Further-   more , we find that the learned metrics have a rela - tively high correlation with the spurious correlates   as compared to human scores , which suggests that   these metrics may rely heavily on spurious correla-   tions . This may be a potential explanation for the   robustness issues that are observed in recent work ,   despite the seemingly high reported correlations   with human judgements ( Gabriel et al . , 2021 ; Yeh   et al . , 2021 ) .   We further analyze reference - free faithfulness   evaluation metrics and show that the reliance on   spurious correlations leads to errors in model se-   lection and development . First , we show that word   overlap , a spurious correlate for the task , does as   well as recently proposed reference - free metrics at   system - level ranking . Then , we look at rankings   amongst systems that are relatively abstractive and   faithful , i.e. , the current state of the art , and find that   these learned metrics perform significantly worse   for these systems . This is because word - overlap is   not a good measure for ranking these systems in   terms of their faithfulness since all of these systems   have similarly low word overlap . This suggests that   we need metrics that are not overly reliant on word   overlap in their faithfulness prediction .   Finally , we explore whether a simple mitigation   strategy of adversarially training a faithfulness eval-   uation metric to avoid spurious correlates can lead   to a more robust metric . We find that our adversari-   ally trained metric performs well at overall pairwise   ranking while having a significantly lower corre-   lation with the spurious correlate of word - overlap .   Crucially , we show that our proposed metric has   improved performance in ranking between abstrac-   tive and faithful systems , which is a failure mode   for existing reference - free faithfulness evaluation   metrics .   2 Reference - free Evaluation of Text   Generation   We begin by defining the task of reference - free eval-   uation , as well as the example - level andsystems-1443level evaluation of these metrics .   We define a reference - free evaluation metric as   a function F(x , y)that can assign a quality score   to an output sequence yfor a given input sequence   x. The goal of a reference - free evaluation metric   F(x , y)is to assign high scores to desirable out-   putsyfor some attribute , such as the faithfulness   of a summary . Measuring the quality of this met-   ric is challenging , and prior work has relied upon   correlation to human judgments H(x , y ) .   Example - level evaluation : A number of exist-   ing reference free evaluations rely upon a procedure   which we call example - level human correlations   ( Fabbri et al . , 2020 ; Phy et al . , 2020 ; Sinha et al . ,   2020 ) , which measures the effectiveness of a met-   ric by computing a Pearson or Spearman correla-   tioncorr(H(x , y ) , F(x , y))over some sampled   evaluation data p(x , y ) .   System - level evaluation : An alternative ap-   proach to evaluation is systems - level rankings   ( Mathur et al . , 2020 ; Kocmi et al . , 2021 ) , which   we define as the ability to identify which model is   better amongst a set of models M.Fis evaluated   via its accuracy in matching human evaluation H   on all pairs ( m , m)∈M×Mwhere m̸=m .   The definitions of example and system level cor-   relations suggest that evaluations of these metrics   may have a strong dependence on the example and   systems distributions p(x , y)andM. As an   example , consider an evaluation for dialogue re-   sponse quality . Building a truly accurate predictor   for dialogue response quality is challenging , but if   p(x , y)consists of all either professionally writ-   ten examples or ungrammatical nonsense , a simple   grammar checker would perform exceedingly well .   This is an instance of what is called a spuri-   ous correlate . More formally , we define this as   some attribute S(x , y)which is correlated with H   inp(x , y)but is not correlated with Hfor a care-   fully constructed test distribution p(x , y ) . We   say that Fisspuriously correlated withSif :   1.Fand Hare highly correlated under   p(x , y)but not under p(x , y ) .   2.Fremains correlated with Sunder p(x , y ) .   3 Example - level Analysis of Learned   Evaluation Metrics   In this section , we look at example - level Spearman   correlations with human judgements for reference-   free evaluation metrics that have been proposed forsummarization and dialog generation . We compare   the metrics to spurious correlates such as word-   overlap , length and perplexity , in order to under-   stand whether the metrics can perform better than   these simple measures . We also measure to what ex-   tent the proposed metrics are correlated with these   spurious measures .   3.1 Faithfulness Evaluation in Text   Summarization   State - of - the - art text summarization models are ca-   pable of producing fluent summaries . However ,   they suffer from generating information that is not   consistent ( i.e. , unfaithful ) with the information in   the source article ( Cao et al . , 2018 ) . Prior work   showed that reference - based metrics are not able to   capture such consistency errors ( Falke et al . , 2019 ) .   This motivated researchers to build evaluation met-   rics to capture these faithfulness issues since col-   lecting human evaluations for faithfulness is ex-   pensive and time - consuming ( Wang et al . , 2020 ;   Durmus et al . , 2020 ; Kryscinski et al . , 2020 ; Goyal   and Durrett , 2020 ) .   In this section , we analyze recently proposed   reference - free faithfulness evaluation metrics and   compare their performance against the spurious cor-   relate of word overlap . Furthermore , we analyze   the correlation between the learned metrics and   word overlap to understand to what extent these   metrics rely on spurious correlations . We focus   on learned entailment - based faithfulness evaluation   metrics due to their high performance in identifying   faithfulness issues ( Pagnoni et al . , 2021 ) . In partic-   ular we evaluate FactCC ( Kryscinski et al . , 2020 )   and DAE ( Goyal and Durrett , 2021 ) , which have   been shown to achieve higher example - level corre-   lations with human judgements than existing faith-   fulness evaluation metrics ( Pagnoni et al . , 2021 ) .   FactCC . Kryscinski et al . ( 2020 ) proposed an   entailment - based method where they train a BERT-   based model to predict whether or not the source   article entails a summary . To train this model , they   generate synthetic training data by applying a set   of transformations to source article sentences in   order to get article , summary pairs . They evaluate   their approach on the CNN / DM dataset ( See et al . ,   2017 ) and report a high accuracy on example - level   comparisons on a human - annotated test set .   DAE . Goyal and Durrett ( 2021 ) collected human   annotations at the word - level and arc - level to study   faithfulness at a finer granularity . They also trained1444   Metric Human Density   FactCC 0.36 0.59   DAE 0.38 0.76   a dependency arc entailment model for faithfulness   detection ( Goyal and Durrett , 2020 ) . They evaluate   on the same test set as Kryscinski et al . ( 2020 ) and   report improved results over FactCC .   We look at how these learned , reference - free met-   rics compare with word overlap – a simple spurious   correlate . One simple measure of whether a gener-   ated summary is faithful is to look at its word over-   lap with the source article ; summaries with a higher   word overlap are more likely to be faithful ( Ladhak   et al . , 2021 ) . However , this measure of faithfulness   is spurious because it can not distinguish between   faithful and unfaithful summaries that have similar   word overlap . In particular , we look at two metrics   of word - overlap following Grusky et al . ( 2018 ):   coverage anddensity .Coverage measures the per-   centage of the words in the summary that are also   present in the article . Density instead looks at the   average length of the segments in the summary that   are extracted from the article .   Results . We use the large - scale faithfulness hu-   man annotations collected by Fabbri et al . ( 2020 )   for16summarization models on the CNN / DM   dataset ( See et al . , 2017 ) for our analysis . Fig-   ure 1 shows the example - level correlations with   human scores for each of the factuality metrics as   well as the spurious correlates . We note that den - sityhas a similar correlation with human scores   as DAE , and is significanltybetter than FactCC .   This result is alarming because density is a spurious   correlate , yet it can achieve similar performance as   the metrics that have been trained for faithfulness   evaluation .   Moreover , we also see that both FactCC and   DAE have a significantly higher correlation with   density than they do with human scores ( Table 1 ) .   This indicates that these metrics may rely upon   spurious correlations and are not yet capturing a   deeper understanding of faithfulness .   3.2 Learned Metrics for Dialog Generation   Dialog generation systems need to be able to gen-   erate a response given the dialog context . The   ability to automatically evaluate the quality of a   response is essential for building dialogue systems .   Liu et al . ( 2016 ) show that referenced - based eval-   uation metrics do not correlate well with human   judgments of response quality . This has led to an   increased interest in reference - free evaluation met-   rics for evaluating dialogue response quality .   Similar to our analysis in § 3.1 , we aim to look   at recently proposed metrics for reference - free eval-   uation , along with spurious correlates for dialog   response quality , and compare them against human   judgments .   DialogRPT . Gao et al . ( 2020 ) finetune GPT-2   to predict the different types of human feedback   ( replies , upvotes , etc . ) in Reddit threads and com-   bine these to form a composite score for response   quality . They evaluate their approach on the Reddit   data that they collected and show that their method   achieves higher example - level agreement with hu-   man judgments than baseline metrics .   MAUDE . Sinha et al . ( 2020 ) propose a model   that encodes each utterance in the dialog context   using a pre - trained BERT model and leverages   the temporal transitions between them to score a   response . They add noise to existing dialog re-   sponses to create negative examples and train their   system to distinguish them from valid responses   using noise contrastive estimation ( NCE ) . They   evaluate their model on the PersonaChat ( Zhang   et al . , 2018 ) dataset and report improved example-   level Spearman correlation with human judgments   compared to existing baseline metrics.1445   Human Perplexity Length PPL+Len   PersonaChatDialogRPT -0.033 -0.017 0.086 0.068   Maude 0.303 0.373 -0.089 0.137   USL - H 0.496 0.092 0.506 0.469   TopicalChatDialogRPT 0.117 -0.011 0.272 0.276   Maude 0.135 0.243 -0.191 -0.148   USL - H 0.318 0.037 0.359 0.355   DailyDialogDialogRPT 0.025 -0.182 0.359 0.270   Maude -0.074 -0.076 0.102 0.033   USL - H 0.094 0.048 -0.208 -0.236   USL - H. Phy et al . ( 2020 ) decompose response   quality into three aspects and train a model to score   a response along each of these aspects . They then   combine the scores hierarchically into one compos-   ite score for response quality . They evaluate their   metric on the DailyDialog ( Li et al . , 2017 ) dataset   and report significantly higher example - level corre-   lations than previous baseline metrics .   MNLI+Adv . Dziri et al . ( 2021 ) introduce an   entailment - based metric that evaluates the ground-   edness of a dialog response , i.e. , whether the gener-   ated response is consistent with the information in   the provided external context , such as a Wikipedia   article . They trained their metric on automatically   generated adversarial data by applying perturba-   tions to the evidence . They further collect human   annotations for the various aspects of dialog gen-   eration , such as entailment , genericness , etc . , and   show that their method is more effective in accu-   rately categorizing the generations than existingentailment models .   To assess these metrics , we look at two spurious   correlates for dialog quality – perplexity and length   of the generated output – as well as a simple com-   bination of two measures . We compute perplexity   using a pre - trained GPT-2 language model ( Rad-   ford et al . , 2019 ) . Perplexity ( PPL ) and length are   spurious correlates since they do not account for the   dialog context , and therefore it is possible to have   high - quality and low - quality responses with similar   perplexities / lengths . For groundedness evaluation ,   we look at the same word overlap measures , as we   did for summarization , i.e. , density andcoverage ,   and we measure overlap between the response and   the provided external evidence .   Results . We evaluate metricsfor response qual-   ity estimation on three popular multi - turn dialog   datasets – DailyDialog , which contains dialogs1446about everyday topics ( Li et al . , 2017 ) , TopicalChat ,   which contains dialogs conditioned on a set of 8   broad topics ( Gopalakrishnan et al . , 2019 ) , and Per-   sonaChat , which contains dialogs conditioned on   personas ( Zhang et al . , 2018 ) .   To evaluate the recently proposed metric for   response groundedness , we use human annota-   tions collected by Dziri et al . ( 2021 ) on Wizard   of Wikipedia ( Dinan et al . , 2019 ) , a dataset that   consists of dialogues conditioned on information   from Wikipedia articles . In particular , we use their   entailment annotations , where human annotators   judge whether or not the external evidence entails   a generated response .   Figure 2 shows the correlations with the human   scores and the spurious correlates for the dialog   generation evaluation metrics . In DialyDialog , we   find that perplexity achieves a similar correlation   with human judgments as USL - H. In TopicalChat ,   perplexity or length alone does not beat out any of   the learned metrics ; however , combining the two   measures achieves a significantly better correlation   with humans than learned metrics . In PersonaChat ,   USL - H achieves the highest correlation with hu-   man judgment , though the combined PPL+Len   score is close . We observe that USL - H is more   consistent than the other reference - free metrics and   achieves significantly higher correlations with hu-   man scores than MAUDE and DialogRPT for Per-   sonaChat and TopicalChat . We further find that   the reference - free metrics have a higher correlation   with the spurious correlates than the human scores   ( Table 2 ) , which again suggests that these learned   metrics may be relying upon spurious correlations . Metric Human Coverage Density   USL - H 0.298 0.467 0.515   MNLI+Adv 0.373 0.451 0.514   For groundedness evaluation , both coverage   anddensity achieve significantly higher correlation   with human scores than MNLI+Ad and USL - H.   Furthermore , MNLI+Ad and USL - H get a higher   correlation with these spurious correlates than hu-   man scores ( Figure 3 ) .   Despite relatively high correlations on their orig-   inal datasets , these metrics seem to perform sim-   ilarly to simple spurious correlations on other   datasets . In order to better understand the effec-   tiveness of these reference - free evaluation metrics ,   we suggest that future research includes compar-   isons to potential spurious correlates and that re-   search communities come up with a set of potential   standard spurious correlates .   4 Learned Metrics in System - level   Evaluation   4.1 Pairwise Ranking of Systems   Our example - level analysis demonstrates that re-   cently proposed learned evaluation metrics achieve   worse correlations with human scores than spurious   correlates for almost all the settings . Since an im-   portant goal of building these metrics is to be able   to rank arbitrary systems , we analyze whether these   concerns we observe at the example level manifest   into harms at the system level ( i.e. , ranking systems   incorrectly ) . In order to study this , we need a large   collection of human evaluation data across a wide   range of systems . Fabbri et al . ( 2020 ) have recently   released human evaluations for faithfulness across   16summarization systems on CNN / DM . Therefore ,   we focus on system - level rankings of faithfulness   for the remainder of the paper .   We first measure pairwise ranking accuracy for   all the systems shown in Figure 4.We find that   system - level rankings suffer from a similar issue as   the example level correlations : density and cover-1447   All Pairs Within AF   Coverage 56.54 26.60   Density 81.01 40.45   FactCC 78.87 38.26   DAE 80.39 37.88   age appear as spurious correlations ( Table 4 ) . From   this observation , we perform a finer - grained anal-   ysis and show that these factuality metrics fail on   the most important subset of model comparisons :   abstractive but faithful summarization system ( AF )   – where the current state - of - the - art abstractive sum-   marization systems fall .   4.2 Results   Both faithfulness metrics perform relatively well   when we look at pairwise ranking accuracy across   all pairs of models ( Table 4 ) . However , they areunable to improve over density , which achieves the   highest overall accuracy . When we look at ranking   within the abstractive faithful group , we see density   is no longer a good measure for the faithfulness of   a system since these systems are relatively close   in terms of density . Similarly , the performance of   the learned metrics drops significantly , which is an   expected result since our analysis in § 3.1 showed   that both FactCC and DAE are spuriously corre-   lated with density . We claim that our system - level   analysis is further evidence that these metrics may   be relying heavily on simple spurious measures   such as word overlap .   These results highlight the importance of per-   forming analyses across different distributions of   systems . If we were looking at just the overall rank-   ing accuracy of the metrics , we would conclude that   DAE and FactCC correctly measure faithfulness .   However , on closer examination , we see that both   metrics perform relatively poorly in ranking AF   systems , which is arguably the most crucial group   since most state - of - the - art systems operate in this   regime , and there is substantial interest in building   abstractive and faithful summarization systems.1448   All Pairs Within AF   FactCC - Electra 77.85 27.70   FactCC 78.87 38.26   DAE 80.39 37.88   Adversarial 85.27 59.20   5 Adversarial Model   In our earlier example - level analysis , we found that   learned metrics have higher correlation with spu-   rious correlates than human judgment . We further   saw in our system - level analysis that learned met-   rics for faithfulness are unable to outperform den-   sity . One natural question that follows is whether   we can build metrics that do well at the systems   level by learning representations that rely less on   spurious correlates .   In order to do this , we train an entailment based   model using the synthetically generated data from   FactCC in an adversarial setup similar to Ganin   et al . ( 2016 ) . In particular , our approach augments   the standard faithfulness predictor with a density   predictor that tries to predict the density of the sum - mary from the model ’s internal representation . We   use this density predictor as an adversary , and our   goal is to predict faithfulness while ensuring that   it is difficult to predict density using this same rep-   resentation . To achieve this , the gradients from   the density predictor are reversed , which makes it   harder to predict the density from the encoder ’s rep-   resentation , and thus makes the faithfulness predic-   tions less reliant on density . The model architecture   is shown in Figure 5 . We initialize the parameter   λto0and gradually increase it to 1 , following the   schedule detailed in Ganin et al . ( 2016 ) .   We fine - tune a pre - trained Electra model ( Clark   et al . , 2020 ) using the transformers library ( Wolf   et al . , 2020 ) for this task . We chose Electra in order   to match the model architecture in DAE . Since the   original FactCC metric was fine - tuned on BERT ,   we also fine - tune our own version of FactCC on   Electra ( FactCC - Electra ) as an ablation . Our ad-   versarially trained model is essentially the same as   FactCC - Electra , but with an additional adversarial   head for predicting density .   Results . We note that the FactCC - Electra model   performs worse than the original FactCC , which   is consistent with the findings in Goyal and Dur-   rett ( 2021 ) . Our adversarially trained metric has a   significantly lower example - level correlation with   density ( 27.71 % ) , as compared to FactCC ( 59.10%)1449and DAE ( 76.37 % ) . We find that the adversarial   modelcan achieve a significantly better perfor-   mance than existing learned evaluation metrics in   ranking systems within the abstractive faithful ( AF )   group ( Table 5 ) . This suggests that it is possible to   learn effective metrics that are not overly reliant on   spurious correlates . Furthermore , our metric is also   effective in overall pairwise ranking of the systems   achieving 85.27 % accuracy .   6 Related Work   Most existing work on assessing the evaluation   methodology of evaluation metrics has focused on   reference - based evaluation . For example , Mathur   et al . ( 2020 ) take a critical look at the use of   example - level correlations to measure reference-   based evaluation metrics in Machine Translation .   They show that evaluating these metrics using   example - level correlations can be sensitive to the   presence of outliers which can lead to false con-   clusions about a metric ’s efficacy . Furthermore ,   Kocmi et al . ( 2021 ) show that proper assessment   of evaluation metrics is crucial as uninformed use   of automated metrics such as BLEU can lead to   bad deployment decisions . Caglayan et al . ( 2020 )   has shown that automated reference - based eval-   uation metrics have robustness issues which can   cause them to score generated outputs higher than   human written outputs . Furthermore , Bhandari   et al . ( 2020 ) has studied the limitations of reference-   based evaluation metrics of text summarization ,   comparing these metrics across different datasets   and application scenarios . In contrast , our work   focuses on analyzing learned , reference - free eval-   uation metrics in summarization and dialog gener-   ation , accounting for potential spurious correlates   for these evaluation tasks .   There has been some recent work comparing   existing reference - free evaluation metrics for text   summarization and dialog generation . Pagnoni   et al . ( 2021 ) has measured the efficacy of exist-   ing reference - free faithfulness evaluation metrics   of summarization on two different summariza-   tion datasets relying on example - level correlations .   Similarly , Gehrmann et al . ( 2021 ) has evaluated   automated metrics of text summarization across a   wide range of datasets . Gabriel et al . ( 2021 ) has   proposed a meta - evaluation framework to evaluate   the evaluation metrics looking at certain aspects ofthese metrics such as robustness , sensitivity , high   correlation with human scores , etc . , and measured   existing evaluation metrics across these aspects .   Yeh et al . ( 2021 ) perform a comprehensive study   of existing dialog generation metrics across several   different datasets and find that the performance of   metrics varies widely across datasets .   Gabriel et al . ( 2021 ) and Yeh et al . ( 2021 ) are the   most related to our work since they study robust-   ness of these metrics looking at their performance   across different datasets . In our work , however , we   explicitly study spurious correlations and show that   these may potentially be contributing to the robust-   ness issues . We further present initial promising   results suggesting that controlling for these spuri-   ous correlates may result in more robust evaluation   metrics .   7 Conclusion   In conclusion , we study reference - free evaluation   metrics for summarization and dialog generation   and show that simply looking at overall example-   level correlation with human judgment paints an   incomplete picture of the effectiveness of a metric .   In particular , we show that these metrics are unable   to do better than simple spurious correlates for the   task . We see that this trend carries over in system-   level ranking for summarization systems , where   a spurious correlate for the task performs as well   as existing learned evaluation metrics . We find   that despite the relatively high overall system - level   ranking performance , the learned metrics are not   robust to distribution shifts . We show that they fail   to properly rank abstractive and ( relatively ) faithful   systems , which is where the current state of the   art operates . Finally , we train a faithfulness metric   that scores the faithfulness of a summary without   relying on the spurious overlap correlate . We show   that our metric is more robust across distribution   shifts and does better at ranking abstractive , faithful   summarization systems .   We suggest that future work in designing   reference - free evaluation metrics should be mindful   of the distribution of the evaluation data . In par-   ticular , metrics should be assessed across different   distributions of systems in order to test for robust-   ness and failure modes . Simple spurious correlates   can be used as a tool to indicate potential overes-   timates of the effectiveness of proposed metrics .   Finally , we highlight the importance of collecting   large - scale human evaluation datasets across a wide1450range of systems , similar to Fabbri et al . ( 2020 ) , to   enable more comprehensive analyses of evaluation   metrics .   8 Acknowledgements   ED is supported by SAIL Postdoc Fellowship . We   further thank the anonymous reviewers and the   Stanford NLP group for their invaluable feedback .   References145114521453A Text Summarization Models   Model Name Paper   M0 Lead-3 baseline   M1 Zhou et al . ( 2018 )   M2 Dong et al . ( 2018 )   M5 Wu and Hu ( 2018 )   M8 See et al . ( 2017 )   M9 Chen and Bansal ( 2018 )   M10 Gehrmann et al . ( 2018 )   M11 Kry ´ sci´nski et al . ( 2018 )   M12 Hsu et al . ( 2018 )   M13 Pasunuru and Bansal ( 2018 )   M14 Guo et al . ( 2018 )   M15 Jiang and Bansal ( 2018 )   M17 Raffel et al . ( 2019 )   M20 Ziegler et al . ( 2019 )   M22 Lewis et al . ( 2020 )   M23 Zhang et al . ( 2020)1454