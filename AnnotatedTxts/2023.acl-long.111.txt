  Yuxin RenZihan ZhongXingjian ShiYi ZhuChun YuanMu LiTsinghua University , Boson AI   { ryx20,zhongzh22}@mails.tsinghua.edu.cn , { xingjian,yi,mu}@boson.ai   yuanc@sz.tsinghua.edu.cn   Abstract   It has been commonly observed that a teacher   model with superior performance does not nec-   essarily result in a stronger student , highlight-   ing a discrepancy between current teacher train-   ing practices and effective knowledge trans-   fer . In order to enhance the guidance of the   teacher training process , we introduce the con-   cept of distillation influence to determine the   impact of distillation from each training sam-   ple on the student ’s generalization ability . In   this paper , we propose Learning GoodTeacher   Matters ( LGTM ) , an efficient training tech-   nique for incorporating distillation influence   into the teacher ’s learning process . By prior-   itizing samples that are likely to enhance the   student ’s generalization ability , our LGTM out-   performs 10 common knowledge distillation   baselines on 6 text classification tasks in the   GLUE benchmark .   1 Introduction   The recent success of natural language processing   ( NLP ) is driven by the adoption of large - scale pre-   trained language models ( Devlin et al . , 2019 ; Liu   et al . , 2019 ; Dai et al . , 2019 ; Yang et al . , 2019 ) . As   these models are scaling up in depth and width , they   become increasingly computational and storage in-   tensive , making deployment difficult . To address   this issue , different methods have been proposed   for crafting efficient models with minimal loss in   performance , such as weight pruning ( Fan et al . ,   2019 ; Li et al . , 2021a ) , network quantization ( Kim   et al . , 2021 ; Zhang et al . , 2020 ) , and knowledge   distillation ( KD ) ( Sun et al . , 2019 ; Tang et al . ,   2019 ; Sun et al . , 2020 ) . Among these methods ,   KD has proven to be effective in various NLP ap-   plications ( Jiao et al . , 2020 ) and is widely adopted .   The idea of KD involves asking a lightweight stu - dent model to mimic the output of a large teacher   model so as to transfer the knowledge .   Ideally , a teacher with better performance should   be able to transfer more knowledge to the stu-   dent . Therefore in most knowledge distillation   algorithms , the teacher network is trained to max-   imize its own performance . However , multiple   studies ( Wang et al . , 2022a ; Cho and Hariharan ,   2019 ) have observed that a teacher with higher   performance does not necessarily lead to a better-   performing student , and may even cause a perfor-   mance degradation . Stanton et al . ( 2021 ) has at-   tributed this inefficiency in knowledge distillation   to challenges during optimization . As the model   capacity gap between the student and the teacher   increases , the optimization process becomes more   likely to be trapped in local optima ( Cho and Hari-   haran , 2019 ; Mirzadeh et al . , 2020 ) .   One way to address the performance degradation   in KD is to update the teacher via feedback from   student ’s performance , also known as learning to   teach ( L2 T ) ( Fan et al . , 2018 ; Zhou et al . , 2022 ) .   L2 T allows the teacher model to adjust its “ teaching   agenda ” by interacting with the student . Among   the L2 T algorithms , online distillation ( Zhang et al . ,   2018 ; Zhu et al . , 2018 ; Shi et al . , 2020 ) trains the   student and teacher concurrently and enforces sim-   ilarity between their outputs on the training set .   However , online distillation focuses on transfer-   ring the knowledge of the teacher to the student   on training set without explicitly considering how   well the student will perform on validation set . On   the other hand , meta distillation ( Zhou et al . , 2022 ;   Pham et al . , 2021 ) takes the generalization ability   of student on the held - out validation set into ac-   count , and guides the teacher ’s learning process to   maximize the generalization ability . However , the   optimization objective of meta distillation may re-   sult in a degraded teacher model , as it only receives   supervision from the student model .   It is well - known that humans are more efficient1990learners when their teachers provide guidance on   the level of attention they should devote to certain   problems based on their current knowledge . Sim-   ilarly , it is possible that a student model could be   trained more effectively if it receives such guidance   from a teacher . To accomplish this goal , the teacher   should prioritize samples that are likely to enhance   the student ’s generalization ability during training ,   thus allowing the student to perform better on the   held - out validation set .   In this work , inspired by the concept of influ-   ence function ( Pruthi et al . , 2020 ; Koh and Liang ,   2017 ) , we propose distillation influence to estimate   how distilling on each training sample impacts the   student ’s performance on the validation set . In addi-   tion , we are able to interpret existing L2 T methods   from the perspective of influence function , so as   to gain a deeper understanding of their limitations .   The optimization process of existing L2 T methods   are often impacted by outliers , because they as-   sign all training samples in the mini - batch the same   weight . Hence , we propose our L2 T framework ,   Learning GoodTeacher Matters ( LGTM ) , which   assigns loss weights of the training samples based   on their distillation influence .   Extensive experiments have shown that LGTM   enables more effective knowledge transfer .   In summary , our contributions are as follows :   1.We propose distillation influence to quantify   how distilling from each training sample im-   pacts the student ’s generalization ability .   2.We introduce finite difference approximation   to efficiently incorporate distillation influence   into the teacher ’s learning process .   3.Comparing to 10 common KD baselines , our   proposed LGTM demonstrates consistently   better performance on 6 text classification   tasks in GLUE benchmark .   2 Notations   Suppose we have a teacher model denoted as   T(·;θ)and a student model denoted as S(·;θ ) .   The corresponding model parameters are θandθ .   ηandηare the learning rates adopted for model   update . We use |t|and|s|to denote the dimensions   ofθandθ , i.e. , θ∈Randθ∈R. The   time step before and after model parameter updates   are denoted as mandm+ 1 , respectively . It is   used to track the evolution of the model parameters   during the training process .   Given a labeled training dataset D , a batch   ofBtraining samples and their corresponding   labels are referred to as z= ( x , y ) , where r   indicates training . We index each sample in the   training batch zasz . Similarly for validation   dataset D , we define the batch of samples as   z= ( x , y ) , where eindicates validation .   In addition , we introduce the notation of the   Jacobian matrix in the context of working with   the chain rule and gradient . In particular , let f :   R→Rbe a differentiable function , and let v∈   Rbe a vector . We use the notation∈R   to represent the Jacobian matrix of f , which has   dimensions k×n . For simplicity , we annotate   as∇. We use Xto denote the transpose of the   matrix X.   3 Revisiting Learning to Teach   In this paper , we focus on task - specific distillation   given pre - trained language models . Under this set-   ting , the teacher model is already pre - trained in   an unsupervised manner and the student model is   either derived from part of the teacher model or   pre - trained in an unsupervised manner as well .   Vanilla distillation The typical approach to   knowledge distillation is a two - stage process . It in-   volves first fine - tuning a pre - trained teacher model1991to maximize its performance on a specific task .   Once the teacher model has converged , a student   model is trained to closely imitate the output of   the teacher model on the training data . The opti-   mization objective for the student model at each   mini - batch is :   L(θ , θ , z ) = αL(y , S(x;θ ) )   + ( 1−α)L(T(x;θ ) , S(x;θ)).(1 )   The update of the student follows :   θ = θ−η∇L(θ , θ , z).(2 )   The limitation of vanilla distillation is that it does   not allow teacher to adjust its behavior according   to student ’s feedback , as the teacher ’s parameters   are fixed during the distillation process .   Online distillation To achieve student - aware dis-   tillation , online distillation ( Zhang et al . , 2018 ; Zhu   et al . , 2018 ; Shi et al . , 2020 ) is proposed which   involves the simultaneous fine - tuning of both the   student and teacher models in one - stage .   In addition to minimizing the cross - entropy loss   with respect to the ground truth labels , the target   distribution of the teacher model is constrained to   be close to that of the student model through the   minimization of the cross - entropy loss between the   outputs of the teacher and student models :   L(θ , θ , z ) = αL(y , T(x;θ ) )   + ( 1−α)L(T(x;θ ) , S(x;θ)).(3 )   The training process involves iteratively updating   the parameters of both models :   θ = θ−η∇L(θ , θ , z )   θ = θ−η∇L(θ , θ , z).(4 )   Through iterative update , the student model is able   to learn from the learning curve of the teacher   model ( Shi et al . , 2020 ) , which improves its perfor-   mance on the given task .   However , online distillation focuses on transfer-   ring the knowledge of the teacher to the student on   training set without explicitly considering how well   the student model will perform on unseen test data .   This might lead to the student model only memo-   rizing the training examples without generalizing   well to new ones ( Zhou et al . , 2022).Meta distillation Meta distillation ( Zhou et al . ,   2022 ; Pham et al . , 2021 ) is a technique that takes   into account the feedback from the student model   and guides the optimization of the teacher model   to maximize the generalization ability of the stu-   dent . The generalization error of the student model   is measured by the cross - entropy loss computed   between the ground truth labels and the predictions   of the student model on the validation set :   L(θ , z ) = L(y , S(x , θ ) ) . ( 5 )   Meta distillation decomposes models ’ learning   process into two stages . The first stage is to fine-   tune a good teacher on task - specific data similar   to vanilla distillation , while the second stage in-   volves iterative update of the teacher and student   models . Note that compared to online distillation ,   meta distillation obtains the student feedback from   validation data , not training data .   During the second stage , the student model is   first updated through the standard distillation pro-   cess by minimizing the distillation loss in eq . ( 1 ) .   Then the teacher model is optimized to minimize   the updated student ’s loss on the held - out valida-   tion set , which ensures it is able to guide the stu-   dent towards better generalization . During this   process , the teacher is only trained for the purpose   of knowledge transfer . Formally , the student model   is updated as follows :   θ = θ−η∇L(θ , θ , z).(6 )   The teacher model is then updated as follows :   θ = θ−η∇L(θ , z),(7 )   However , the optimization objective of meta dis-   tillation can result in a degraded teacher model be-   cause it only receives supervision from the student .   This will prevent the teacher model from continu-   ing to learn and improve in the second stage , thus   impeding its ability to adapt to new data .   4 Methods   To overcome the aforementioned limitations , we   introduce our L2 T framework , Learning Good   Teacher Matters ( LGTM ) to enable more effective   knowledge distillation . We first introduce distilla-   tion influence , which estimates how much will the   student ’s performance on validation data change1992if we put one training sample in the knowledge   distillation process .   Afterwards , we introduce an efficient training   method based on finite difference approximation   for incorporating distillation influence into the   teacher ’s update . Finally , we interpret current L2 T   methods from the perspective of influence function .   Distillation influence Influence function ( Pruthi   et al . , 2020 ; Koh and Liang , 2017 ) is a way of   measuring the influence of training samples on the   model ’s predictions . It can be utilized to identify   instances that have a disproportionate effect on the   model ’s behavior , whether due to their status as   outliers or due to incorrect labeling ( Jia et al . , 2019 ;   Ghorbani and Zou , 2019 ; Hara et al . , 2019 ) . By   calculating the influence function for a particular   example , it is possible to estimate the extent to   which the model ’s prediction would be altered as a   result of operations on that sample .   In vanilla distillation , for the student model , we   derive the distillation influence of zas the gradient   similarity between the training sample zand the   validation batch z :   The detailed derivation can be found in appendix A.   The influence reflects how well the knowledge   gained from a particular sample generalizes . It   follows that the teacher should focus on teaching   the student to capture training samples that have   the highest distillation influences .   In order to incorporate the per - sample influ-   ence into knowledge distillation , we adjust the loss   weight of each sample based on its distillation in-   fluence . This allows us to determine the relative   importance of each sample , and helps to control   how much each sample contributes to the teacher ’s   learning process . Samples that are deemed to be   more beneficial for the student ’s generalization are   assigned higher weights . Then we propose training   the teacher using the following objective :   where w = I(z , z ) . By including the in-   fluence in the knowledge distillation loss function ,   we can tailor the training process to better suit the   characteristics of the target task . Algorithm 1 LGTM   Finite difference approximation For standard   neural network training , we often compute a con-   solidated gradient for a mini - batch of Btrain-   ing samples to enhance computational efficiency .   However , in the context of determining the distil-   lation influence for each sample , the computation   of per - sample gradient L(T(x;θ ) , S(x;θ ) )   will slow down the training by a factor of B.   In addition , a naive implementation is memory   intensive , because it requires to keep a copy of   ∇L(y , S(x;θ ) ) .   To address this , we propose an efficient method   for updating the teacher with the distillation influ-   ence by utilizing finite difference ( Gleich , 2005 ) ,   a technique commonly used in numerical analysis   for approximating the derivative of a function at   a given point . Similar to ( Pham et al . , 2021 ; Liu   et al . , 2018 ) , we approximate L by   where θ = θ±ϵL(y , S(x;θ ) ) andϵis   a small scalar . Our proposed method for evaluating   the finite difference is computationally efficient ,   as it only requires two forward passes for θand   one backward pass for θfor a single batch , as   opposed to a naive implementation which requires   Bforward and backward passes for θand one   backward pass for θ . We provide more details of   the derivation in appendix B.   Teacher ’s auxiliary loss Inspired by ( Pham et al . ,   2021 ) , in order to balance the trade - off between   self - evolution and transferability of the teacher1993   model , we incorporate the loss with respect to the   ground truth as Linto the final objective :   where αis the loss ratio .   Overall , our method allows the teacher to adapt   to the student ’s abilities and provide more person-   alized guidance while improving the student ’s gen-   eralization capability . We present the algorithm of   LGTM in algorithm 1 .   Relationship with other L2 T methods Here we   interpret current learning to teach methods from   the perspective of influence function .   In the case of online distillation , it is assumed   that all training samples possess an equivalent dis-   tillation influence and that the teacher model is   responsible for reducing the transfer difficulty of   all training samples .   In contrast , the key differentiating factor be-   tween meta distillation and online distillation is   the utilization of a dynamic loss weight . We in-   terpret this weight as a measure of the distillation   influence of the current training batch zon the   generalization ability of the student model . Specifi-   cally , it reflects the similarity between the gradients   of the training and validation batches , indicating   the effect of the current training batch zon the val-   idation batch z(as detailed in appendix C ) . How-   ever , it should be noted that this weight functions   primarily as an adaptive learning rate , adjusting   the gradient step proportionally to the degree of   similarity in gradients . We illustrate the general   workflow of vanilla distillation , online distillation ,   meta distillation and LGTM in fig . 1.5 Experiments   In this section , we first describe our experiment   setup including datasets and baselines in Sec . 5.1 .   Then we compare our proposed LGTM to meta dis-   tillation to gain some basic understanding of how   to incorporate the student ’s feedback in Sec . 5.2 .   To further verify the effectiveness of our method , in   Sec . 5.3 we compare to 10 widely adopted knowl-   edge distillation baselines and show consistently   better results . Then we demonstrate how distil-   lation influence works in Sec . 5.4 , followed by   ablation studies of LGTM in Sec . 5.5 .   5.1 Experimental Setup   Datasets We evaluate our proposed approach   on text classification tasks in GLUE ( Wang   et al . , 2018 ): MRPC ( Dolan and Brockett , 2005 ) ,   RTE ( Wang et al . , 2018 ) , SST-2 ( Socher et al . ,   2013 ) , MNLI ( Williams et al . , 2018 ) , QNLI ( Ra-   jpurkar et al . , 2016 ) and QQP ( Chen et al . , 2018 ) .   For MRPC and QQP , we report both F1 and accu-   racy . And for other datasets , we report accuracy .   Baselines We compare our LGTM with 10 base-   lines : 1 ) KD ( Hinton et al . , 2015 ) 2 ) PKD ( Sun   et al . , 2019 ) 3 ) SKD ( Guo et al . , 2022 ) 4 )   DIST ( Huang et al . , 2022 ) 5 ) TAKD ( Mirzadeh   et al . , 2020 ) 6 ) RCO ( Jin et al . , 2019 ) 7 )   DML ( Zhang et al . , 2018 ) 8) ProKT ( Shi et al . ,   2020 ) 9 ) PESF - KD ( Rao et al . , 2022 ) and 10 ) Meta   Distill ( Zhou et al . , 2022 ) .   Training setup Following previous works ( Sun   et al . , 2019 ; Zhou et al . , 2022 ) , we distill BERT-   Base ( Devlin et al . , 2019 ) to a 6 - layer BERT model .   For all two - stage baselines , we fine - tune the mod-   els on each task . For fair comparison , both Meta   Distill and LGTM utilize feedback from the vali-   dation set in the calculation of the distillation loss.1994   Detailed training hyperparameters can be found in   appendix D.   5.2 Comparison with Meta Distillation   Given our proposed LGTM is closely related to the   meta distillation line of work , here we first conduct   a comparison between LGTM and a specific meta   distillation method , Meta Distill ( Zhou et al . , 2022 ) ,   to demonstrate the benefit of adopting distillation   influence .   We observe that for Meta Distill ( blue curve ) in   fig . 2 ( a ) and ( b ) , the validation loss of the student   model gradually increases in later iterations while   the validation accuracy keeps improving until a sta-   ble plateau . This clearly indicates that the student   model is experiencing overfitting . One possible   explanation is that excessive emphasis is placed on   certain training samples that generate high loss ,   e.g. , hard samples or outliers . This negatively   impacts the generalization ability of the student   model , which leads to overfitting .   The key difference between Meta Distill and our   LGTM ( orange curve ) is that LGTM accounts for   the per - sample distillation influence while Meta   Distill treats all training samples in a batch equally .   This enables the filtering of samples that have a   detrimental effect on generalization performance   of the student model , leading to a steady decrease   of validation loss ( fig . 2 ( a ) ) and an improved vali-   dation accuracy ( fig . 2 ( b ) ) .   In terms of teacher model , it should not only im-   part their current knowledge to the student , but also   actively seek out new information and perspectives   to improve their own understanding . As can beseen in fig . 2 ( c ) , LGTM allows for the effective   transfer of knowledge from the teacher model by   incorporating the teacher auxiliary loss . The valida-   tion accuracy of the teacher model keeps improving   for LGTM , but drops quickly for Meta Distill .   5.3 Main Results   Here we show the results of our proposed method   on the test set of text classification tasks in GLUE   benchmark . As can be seen in table 1 , LGTM out-   performs all 10 baselines including recent strong   KD methods ( Guo et al . , 2022 ; Huang et al . , 2022 ;   Rao et al . , 2022 ; Zhou et al . , 2022 ) , which high-   lights the effectiveness of our method .   To be more specific , our proposed method   achieves state - of - the - art performance in compar-   ison to those rely on carefully designed training   pipelines or loss functions , e.g. , PKD ( Sun et al . ,   2019 ) , SKD ( Guo et al . , 2022 ) and DIST ( Huang   et al . , 2022 ) . PKD proposes two distillation   schemes , to enable the student to learn from mul-   tiple intermediate layers of the teacher model for   incremental knowledge extraction . SKD and DIST   both modify the form of KL - divergence loss to   narrow the gap between the teacher and student   models . LGTM also does not require a series of   teacher assistant models as TAKD ( Mirzadeh et al . ,   2020 ) and RCO ( Jin et al . , 2019 ) .   Compared to online distillation methods , LGTM   performs better than DML ( Zhang et al . , 2018 ) ,   ProKT ( Shi et al . , 2020 ) and PESF - KD ( Rao et al . ,   2022 ) . This highlights the importance of incorpo-   rating student ’s feedback during the training pro-   cess . An overemphasis on knowledge transfer from1995   the training set may lead to the student overfitting   the teacher ’s outputs , resulting in a reduction in its   generalization abilities .   Furthermore , unlike meta distillation methods ,   e.g. , Meta Distill ( Zhou et al . , 2022 ) , our method   allows for computing distillation influence of in-   dividual training samples , which enables filtering   out samples that may hurt student ’s generalization .   Therefore , LGTM is able to help the student to   develop general understanding of the overall task   while alleviate the overfitting issue .   5.4 Analysis of Distillation Influence   We further explore the trend of the distillation in-   fluence of samples during the real training pro-   cess . Here , we conduct experiments on the MRPC   dataset . The task is to predict whether the sen-   tences in a sentence pair are semantically equiva-   lent ( Wang et al . , 2018 ) .   First , we select two representative samples pre-   sented in fig . 3 to visualize the trend of the distil-   lation influence and its relationship between the   teacher ’s and the student ’s prediction .   On the left - side of fig . 3 , we can see that dur-   ing the initial stages of training , both the teacher   ( green ) and the student ( orange ) have made wrong   predictions . It might suggest that this sample poses   a significant challenge for both models to learn . In   this case , we do not want student model to mimic   the output from teacher models too much becauseteacher model is also wrong about this sample . Our   method is able to gradually adjust the loss weight to   negative , indicating we will filter out this mislead-   ing training sample for now to make both models   learn faster . As a result , the student model first   escapes this predicament . Then through student   feedback on the validation set , the teacher model   also learns to make the correct prediction . Finally   as training progresses , it is observed that both the   student and the teacher are able to correctly classify   this sample , resulting in the distillation influence   stabilizing at a near - zero value .   We present another example in fig . 3 right , where   both the student and the teacher are able to accu-   rately predict a given sample . It might suggest this   sample is too easy for the teacher and the student .   In this case , we want to give this sample a high   positive weight to form a student - friendly decision   boundary . This is similar to design a curriculum to   learn from easy samples to hard ones in curriculum   learning ( Soviany et al . , 2022 ) .   We also visualize an average trend of distillation   influence in fig . 4 , based on 64 samples that are   randomly chosen from MRPC . We observe that the   distillation influence is usually insignificant in the   beginning and end of the training , but fluctuates in   the middle . This is reasonable since our method is   assigning varying weights to each sample during   training , with the goal of filtering difficult samples   and focusing on samples better for generalization.1996   5.5 Ablation Study   Given limited space , we present three studies in   this section and show more ablation studies in ap-   pendix E.   Finite difference approximation Recall in sec-   tion 4 , we introduce finite difference approximation   ( FDA ) for estimating the distillation influence of   each sample . It is designed to address the slowness   of computing per - sample gradients . As shown in   table 3 , here we conduct an ablation experiment on   the MRPC dataset to evaluate its usefulness . We   show that with FDA , our method only requires 11   minutes to complete the training , while the naive   training without FDA requires 117 minutes . Such   a significant reduction in training time ( i.e. , more   than 10×speedup ) highlights the computational   efficiency of the proposed FDA technique . Further-   more , we assess the performance on the validation   set of the MRPC dataset and observe that train-   ing with FDA result in an F1 score of 90.4 , while   training without FDA resulted in a score of 90.7 .   There is only a slight drop in performance with the   approximation .   Distillation loss There are other distillation   losses in the context of knowledge distillation .   Here we want to evaluate whether LGTM can adapt   to those objectives . In particular , we consider the   modified loss used in DIST ( Huang et al . , 2022 )   and the common mean squared error ( MSE ) . As   can be seen in table 2 , our LGTM consistently   beats the original methods that utilize these distil-   lation objectives , which validates the compatibility   of LGTM to different distillation objectives .   Student model size Here we conduct experi-   ments to evaluate the performance of our proposed   method in scenarios where there is a larger capacity   difference between the teacher and student mod-   els . Specifically , we perform knowledge distillation   from a BERT - Base model ( Devlin et al . , 2019 ) to a   4 - layer BERT model . As can be seen from table 4 ,   LGTM consistently outperforms other baselines   in most of the tasks except competitive results on   SST-2 . This indicates the robustness of our method   which suggests its wide usage in various knowledge   distillation settings .   6 Related Work   The core of knowledge distillation ( Hinton et al . ,   2015 ) relies on how to formulate and transfer the   knowledge from the teacher to student . Three1997   key aspects are typically considered : the teacher   model from which knowledge is transferred ( learn-   ing target ) , the data on which the model is trained   ( learning material ) , and the objective function that   defines the learning objective . Efforts have been   made to make knowledge distillation more student-   friendly by reducing the difficulties in these as-   pects(Li et al . , 2021b ) .   On learning target , Jin et al . ( 2019 ) ; Mirzadeh   et al . ( 2020 ) introduce teacher assistant models of   intermediate timestep or training time step respec-   tively to narrow the gap between the teacher and   student models . Park et al . ( 2021 ) ; Shi et al . ( 2020 )   propose updating the teacher and student jointly to   make the teacher aware of the student ’s state . Rao   et al . ( 2022 ) trains for more timestep to smooth the   distribution of the teacher for a easier transfer .   In terms of learning material , TinyBERT ( Jiao   et al . , 2020 ) suggests augmenting the training data   to make it more diverse . Kim et al . ( 2022 ) pro-   poses training the student with samples that are   easy for the teacher but difficult for the student .   With respect to learning objective , the most com-   mon approach is to match the probabilistic predic-   tion scores of the teacher and student models using   KL - divergence . However , this can cause problems   during training , leading to poor performance . Guo   et al . ( 2022 ) ; Huang et al . ( 2022 ) propose to soft   the constraint by a more tolerated loss . Pham et al .   ( 2021 ) ; Zhou et al . ( 2022 ) propose using the stu-   dent ’s performance as the optimization objective   for the teacher model , allowing the teacher to op-   timize its knowledge transfer based on feedback   from the student . Wang et al . ( 2022b ) proposes   to select the appropriate knowledge to guide the   optimization of the student.7 Conclusion   In this paper , we first revisit several learning to   teach paradigms in knowledge distillation . Then   we propose distillation influence to determine   how distilling from each training sample impacts   the student ’s generalization ability . By visualiz-   ing how the distillation influence of each sample   changes during training , we can see that a simple   re - weighting using distillation influence is able to   help student training , e.g. , reduce overfitting . Built   on top of distillation influence , we propose our   learning to teach framework , LGTM , that consis-   tently outperforms existing knowledge distillation   methods on text classification tasks in the GLUE   benchmark .   Limitations   Although LGTM has demonstrated superior per-   formance in task - specific knowledge distillation ,   it is worth investigating the potential benefits of   combining LGTM with pre - training knowledge dis-   tillation ( Jiao et al . , 2020 ; Wang et al . , 2020 ) . Ad-   ditionally , while our experiments have been limited   to text classification tasks , which are relatively sim-   ple for current pre - trained language models , future   work should explore the application of LGTM to   more complex text generation tasks .   Ethics Statement   During the training process , the teacher and stu-   dent models are initialized from pre - trained models .   However , pre - trained language models are vulnera-   ble to potential ethical and social risk as mentioned   by Bommasani et al . ( 2021 ) and Weidinger et al .   ( 2021 ) . Therefore , the teacher and student mod-   els can be exposed to similar social risks of large   language models .   Acknowledgements   We thank Yongfei Liu and Zhengkun Zhang for   their insightful discussion and the anonymous re-   viewers for their helpful comments . This work was   supported by the National Key R&D Program of   China ( 2022YFB4701400/4701402 ) , SZSTC Grant   ( JCYJ20190809172201639 , WDZC20200820200-   655001 ) , Shenzhen Key Laboratory ( ZDSYS2021-   0623092001004 ) , and Beijing Key Lab of Net-   worked Multimedia.1998References19992000   AThe Derivation of Distillation Influence   As described by Pruthi et al . ( 2020 ) , the influence   of a training sample z= ( x , y)on a test sam-   plez= ( x , y)can be traced by examining the   change in loss of model won the test sample . The   influence function is defined as the total reduction   in loss on the test sample zinduced by the training   process whenever the training sample zis utilized :   where w = w−ηL(w , z)andηis the learn-   ing rate and the model are parameterized by wand   w.   In this context , we will focus on the influence of   the current training batch on the student model ’s   performance on the validation data . To improve   computation efficiency , a batch of samples is drawn   from the validation set to evaluate the model ’s   generalization performance . As a result , the in-   fluence on a single validation sample , as described   in eq . ( 12 ) , is extended to a batch of validation sam-   plesz . The influence of the current training batch   zon the validation batch zis defined as follows :   where θ = θ−ηL(θ , θ , z ) .   By applying the Taylor expansion , we can ap-   proximate L(θ , z)as follows :   As a result , we approximate the I(z , z)as fol-   lows :   The contribution of a single sample z= ( x , y )   in the training batch zis defined as follows : By excluding loss irrelevant to the teacher in   eq . ( 16 ) , we define the distillation influence of z   to be :   B Approximation Methods   Here , we efficiently approximate this gradient sim-   ilarity using a Taylor expansion :   where θ = θ±ϵL(y , S(x;θ ) ) andϵis   a small scalar .   C A Closer Look at Meta Distillation   In meta distillation , the loss on the validation set   with respect to the teacher can be derived as fol-   lows :   ( 19 )   where2001D Hyperparameters   Hyperparameter   α 0.6   maximum sequence length 128   distillation temperature 1   fine - tuning epochs 6   student learning rate 1e−4,3e−5,5e−5   batch size 32   For our method , online distillation and meta distil-   lation baselines , we fix the teacher learning rate at   3e−5 .   E More ablation study   E.1 Datasets for Student ’s Feedback   In our method , we utilize the feedback from the   student model on the provided validation set of   GLUE datasets directly . In this section , we investi-   gate the impact of utilizing feedback derived from   a new validation set that has been separated from   the original training set .   We random sample 5 % and 10 % samples of   the training set to generate a new validation set   respectively . Then we apply our method to the new   training set .   The data used to measure the generalization of   the student , whether it be from an existing vali-   dation set or a newly separated set , remains infor-   mative in both cases . As such , it is reasonable to   expect that the feedback provided by the student   to the teacher would not exhibit significant differ-   ences between the two sources .   Our experiments demonstrate that utilizing feed-   back from a validation set , whether pre - existing or   newly separated from the training set , does not lead   to significant variations in performance . However ,   it should be noted that the number of training sam-   ples may play a role in the results . When a subset   of the training set is selected to form a new valida-   tion set , the number of training samples is reduced .   This reduction may lead to overfitting in datasets of   small or medium size , as there is not enough data   information provided to the model . Conversely ,   in large datasets , the number of samples is suffi-   cient to encompass a substantial portion of the data   information , thus having minimal impact on the   results .   E.2 Ratio of Teacher ’s Self - evolution   A student - friendly teacher should strike a balance   between self - evolution and knowledge transfer . It   is believed that an excessive focus on self - evolution   may result in neglect of feedback provided by the   student , leading to instruction that is not centered   on the student ’s needs . Conversely , inadequate   focus on self - evolution may prevent the teacher   from improving their own abilities , resulting in   suboptimal instruction for the student . In either   scenario , the outcome is not conducive to fostering   a student - friendly environment .   Therefore , we ablate on the ratio of the teacher ’s   self - evolution to see how it contributes to the perfor-   mance of the student . αis the ratio of the teacher ’s   loss with respect to ground truth in eq . ( 11 ) . We set   it from { 1.0,0.8,0.6,0.4}.2002   In table 7 , the performance of the student ex-   hibits a unimodal distribution , which is in agree-   ment with our proposed assumption . Specifically ,   the results indicate that when the ratio of the   teacher ’s self - evolution is set at 0.6 , the perfor-   mance of the student is optimal .   F Analysis   We further discuss some design choices of cur-   rent methods , including the initialization state of   the teacher and the updating order of the teacher   and student models . Following ( Guo et al . , 2022 ) ,   we apply the entropy gap to evaluate these design   choices .   F.1 Impact of the Teacher ’s initial state   While vanilla distillation and meta distillation em-   ploy a two - stage training approach , online distilla-   tion and LGTM employ a one - stage joint training   strategy for the teacher and student models . The   key difference is whether to involve fine - tuning the   teacher network on target task . In this study , weinvestigate the impact of the teacher network ’s state   on the student network .   A teacher network initialized in the same state   as the student network can maintain the student   network ’s progress at all times , but its capabilities   may be relatively weak . In contrast , a converged   teacher network has superior performance but also   a larger gap , which can prevent the student network   from gaining knowledge effectively .   As show in fig . 5 , a lower initial confidence gap   between the teacher model and the student model   leads to more efficient knowledge transfer . When   the initial ability gap is relatively high , it takes   more iterations for the student model to catch up   to the fine - tuned teacher model . In contrast , when   the initial ability gap is lower , a teacher model   initialized at the same state as the student model   is able to transfer knowledge to the student more   quickly . Specifically , in the early stages , the teacher   model focuses more on self - evolution than knowl-   edge transfer , causing the entropy gap to increase .   Then , the teacher model shifts its focus towards   knowledge transfer , resulting in an increasing and   then decreasing trend in the entropy gap .   F.2 Prioritizing the Teacher or Student   Online distillation and meta distillation and LGTM   all use bi - level optimization . However , online dis-   tillation and LGTM updates the teacher network   followed by the student network , while meta dis-   tillation updates the student network followed by   the teacher network . In this section , we study the   optimal order for updating the teacher network and   student network in knowledge distillation .   As shown in fig . 6 , updating the teacher model   first could lead to a lower entropy gap and faster   convergence speed . We assume that the teacher   could formulate an appropriate ‘ teaching plan ’ for   the student in this updating order .   The teacher should strive to guide the student to   identify the most important samples and informa-   tion , to help the student develop a deep and general   understanding of the task . Furthermore , the teacher   should also take into consideration that some sam-   ples may be difficult for the teacher itself to classify   or understand . And for those samples , a lower crite-   rion should be set for the student , which may form   a more student - friendly decision boundary .   Therefore , the teacher ’s output serves as a dy-   namic learning target for each sample . By updating   based on the student ’s feedback in advance , the2003teacher is able to reach a state that is optimal for the   student ’s learning . In this case , the teacher could   provide an appropriate learning signal . Leveraging   this updated supervision signal , the student could   make up for the ability gap faster . For the other   two updating orders , the teacher has n’t updated   yet , lacking of making trade - offs between the sam-   ples that are more beneficial for generalization and   those that are more challenging to learn from . This   may lead to a certain degree of lag in knowledge   transfer , resulting in a larger entropy gap between   the student and the teacher.2004ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Yes . Section " Limitations "   /squareA2 . Did you discuss any potential risks of your work ?   Yes . Section " Ethics Statement "   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract + End of Section 1 : Introduction   /squareA4 . Have you used AI writing assistants when working on this paper ?   Checking the presentation style of some sentences via ChatGPT . Use prompt like " help me rephrase   XXX " . However , sometimes ChatGPT will generate very wordy sentences and we have n’t used many   recommendations .   B / squareDid you use or create scientiﬁc artifacts ?   Section 5.1 . We use the GLUE benchmark .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 5.1   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We have n’t discussed the term + license explicitly since they are in the GLUE paper and other papers   we cited .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   GLUE has widely been used by the research community . We are writing a research paper so we   have n’t used spaces to discuss the intended usage of GLUE .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . We are using the datasets from GLUE .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . We are using the datasets from GLUE .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 5.12005C / squareDid you run computational experiments ?   Section 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 5   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 5.1 and Appendix D   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   No , we reported results on the test set of the GLUE benchmark . There is limitation to the total   number of submissions each person can make for the GLUE benchmark .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 5 and Appendix D   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.2006