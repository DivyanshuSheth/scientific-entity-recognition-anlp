  Ori Ernst , Avi Caciularu , Ori Shapira , Ramakanth Pasunuru ,   Mohit Bansal , Jacob Goldberger , and Ido DaganBar - Ilan UniversityUNC Chapel Hill   { oriern , avi.c33 , obspp18}@gmail.com   { ram , mbansal}@cs.unc.edu   { jacob.goldberger@ , dagan@cs.}biu.ac.il   Abstract   Text clustering methods were traditionally in-   corporated into multi - document summariza-   tion ( MDS ) as a means for coping with con-   siderable information repetition . Particularly ,   clusters were leveraged to indicate information   saliency as well as to avoid redundancy . Such   prior methods focused on clustering sentences ,   even though closely related sentences usually   contain also non - aligned parts . In this work ,   we revisit the clustering approach , grouping   together sub - sentential propositions , aiming at   more precise information alignment . Speciﬁ-   cally , our method detects salient propositions ,   clusters them into paraphrastic clusters , and   generates a representative sentence for each   cluster via text fusion . Our summarization   method improves over the previous state - of-   the - art MDS method in the DUC 2004 and   TAC 2011 datasets , both in automatic ROUGE   scores and human preference .   1 Introduction   Common information needs are most often satis-   ﬁed by multiple texts rather than by a single one .   Accordingly , there is a rising interest in Multi-   Document Summarization ( MDS ) — generating   a summary for a set of topically - related documents .   Inherently , MDS needs to address , either explic-   itly or implicitly , several subtasks embedded in this   summarization setting . These include salience de-   tection , redundancy removal , and text generation .   While all these subtasks are embedded in Single-   Document Summarization ( SDS ) as well , the chal-   lenges are much greater in the multi - document   setting , where information is heterogeneous and   dispersed , while exhibiting substantial redundancy   across linguistically divergent utterances .   An appealing summarization approach that   copes with these challenges , and is especially rele - Figure 1 : An example of a cluster of propositions ,   shown within their source sentence context , from TAC   2011 ( topic D1103 ) . Clustering these as sentences   would yield noisy unaligned information , however   grouping together only the marked propositions keeps   information alignment clean . The ﬁrst sentence is illus-   tratively divided into propositions , where only one of   them is aligned to those in the other sentences .   vant for MDS , is clustering - based summarization .   In such an approach , the goal is to cluster redun-   dant paraphrastic pieces of information across the   texts , which roughly convey the same meaning .   Repetition of information across texts , as captured   by paraphrastic clustering , typically indicates its   importance , and can be leveraged for salience detec-   tion . Moreover , representing a paraphrastic cluster   may facilitate generating a corresponding summary   that eliminates repetitions while fusing together   complementary details within the cluster .   Traditionally , clustering - based approaches were   widely used for summarization , mostly in extrac-   tive and unsupervised settings ( Radev et al . , 2004 ;   Zhang et al . , 2015 ; Nayeem et al . , 2018 ) . Notably ,   most of these works generated sentence - based clus-   ters , which tend to be noisy since a sentence typi-   cally consists of several units of information that   only partially overlap with other cluster sentences .   As a result , such clusters often capture topically   related sentences rather than paraphrases . Fig-   ure 1 exempliﬁes such a noisy cluster , which does   contain paraphrastic propositions ( marked in blue )   within their full sentences ( marked in black ) . An-   other line of research in summarization coped with1765such noisy sentence - based setting , and looked into   the use of sub - sentential units for summarization ,   e.g. , Li et al . ( 2016 ) summarizes with Elementary   Discourse Units ( EDUs ) , while Ernst et al . ( 2021 )   endorse using Open Information Extraction ( Ope-   nIE ) -based propositions ( Stanovsky et al . , 2018 )   for summarization .   In this paper , we revisit and combine the   clustering - based approaches along with sub-   sentential setting , two research lines that were ex-   plored only individually and rather scarcely in re-   cent years . Speciﬁcally , we apply clustering - based   summarization at the more ﬁne - grained proposi-   tional level , which avoids grouping non - aligned   texts , yielding accurate paraphrastic clusters . These   clusters also provide better control over the gener-   ated summary sentences – as the generation compo-   nent is only required to fuse similar propositions .   Our model ( § 3 ) leverages gold reference sum-   maries to derive training datasets for several sum-   marization sub - tasks . First , salient document   propositions were extracted , to train a salience   model , by greedily maximizing alignment with the   reference summaries . Then , an available propo-   sition similarity model , trained from summary-   source alignments ( Ernst et al . , 2021 ) , provides the   basis for agglomerative clustering ( Ward , 1963 ) .   Finally , we created training data for a BART - based   model for sentence fusion ( Lewis et al . , 2020 )   by aligning reference summary propositions with   source proposition clusters . Similar to many other   works , we leave inter - sentence coherence and sen-   tence planning and ordering outside the scope of   the current paper . Accordingly , our process pro-   duces a bullet - style summary of individual concise   and coherent sentences .   Overall , our experiments ( § 4 ) show that this   multi - step model outperforms strong recent end - to-   end solutions , which do not include explicit model-   ing of propositions and information redundancy . To   the best of our knowledge , our approach achieves   state - of - the - art results in our setting on the DUC   2004 and TAC 2011 datasets , with an improvement   of more than 1.5 and 4 ROUGE-1 F1 points respec-   tively , over the previous best approach . Finally ,   we also suggest ( § 5 ) that clustering - based methods   provide “ explanations " , or supporting evidence , for   each generated sentence , in the form of the source   cluster propositions ( see an example in Table 1).2 Background and Related Work   Clustering - based summarization . Clustering-   based summarization approaches typically involve   salience detection while avoiding redundancy . One   such approach clustered topically - related sentences ,   after which cluster properties were leveraged for   rating sentence salience ( Radev et al . , 2004 ; Wang   et al . , 2008 ; Wan and Yang , 2008 ) . Another ap-   proach rated sentence salience and clustered sen-   tences simultaneously , iteratively improving the   two objectives ( Cai et al . , 2010 ; Wang et al . , 2011 ;   Cai and Li , 2013 ; Zhang et al . , 2015 ) . Recently ,   however , clustering methods have been gradually   marginalized out , being replaced by neural tech-   niques . More recently though , some approaches   ( Nayeem et al . , 2018 ; Fuad et al . , 2019 ) presented   abstractive clustering - based summarization , where   topically - related sentences in each cluster are fused   together to generate a summary sentence candidate .   While most of previous clustering approaches op-   erated at the noisy sentence level , in our work we   present more accurate proposition - level clustering   that eventually enhances summarization .   Sub - sentence units in summarization . While   many summarization approaches extract full doc-   ument sentences , either for extractive summariza-   tion or as an intermediate step for abstractive sum-   marization , there are methods that operated the   sub - sentential level . Li et al . ( 2016 ) ; Liu and   Chen ( 2019 ) ; and Xu et al . ( 2020 ) produced extrac-   tive summaries consisting of Elementary Discourse   Units ( EDUs ) – clauses comprising a discourse unit   according to Rhetorical Structure Theory ( RST )   ( Mann and Thompson , 1988 ) . Such extractive ap-   proaches usually focus on content selection , pos-   sibly disregarding the inferior coherence arising   from the concatenation of sub - sentence units . Ac-   cordingly , Arumae et al . ( 2019 ) established the   highlighting task , where salient sub - sentence units   are marked within their document to provide sur-   rounding context . Recently , Cho et al . ( 2020 ) pro-   posed identifying heuristically self - contained sub-   sentence units for the highlighting task .   Abstractive approaches have been extracting sub-   sentence units as a preliminary step for generation .   Such units range from words ( Lebanoff et al . , 2020 ;   Gehrmann et al . , 2018 ) , to noun or verb phrases   ( Bing et al . , 2015 ) , to OpenIE propositions ( Pa-   sunuru et al . , 2021 ) . In our work , we follow the   same extract - then - generate pipeline , using Ope-1766   nIEs ( Stanovsky et al . , 2018 ) as propositions . Since   propositions are meant to contain single standalone   facts consisting of a main predicate and its argu-   ments , they are beneﬁcial for grouping mostly over-   lapping paraphrases ( unlike sentential paraphrases ) .   In addition , propositions extracted with OpenIE   can be noncontiguous , while alternative options ,   like EDUs , are limited to contiguous sequences .   3 Method   This section ﬁrst provides an overview of our   method , followed by subsections describing its   components . We follow previous clustering - based   approaches , where text segments are ﬁrst clustered   into semantically similar groups , exploiting redun-   dancy as a salience signal . Then , each group is   fused to generate a merged sentence , while avoid-   ing redundancy . As we operate at the proposition-   level , we ﬁrst extract all propositions from the input   documents ( § 3.1 ) . Then , to facilitate the clustering   step , we ﬁlter out non - salient propositions using a   salience model ( § 3.2 ) . Next , salient propositions   are clustered based on their semantic similarity   ( § 3.3 ) . The largest clusters , whose information was   most repeated , are selected to be included in the   summary ( § 3.4 ) . Finally , each cluster is fused to   form a sentence for a bullet - style abstractive sum-   mary ( § 3.5 ) . In addition , we provide an extractive   version where a representative ( source ) propositionis selected from each cluster ( 3.6 ) . Overall , clus-   tering explicit propositions induces a multi - step   process that requires dedicated training data for   certain steps . To that end , we derive new training   datasets for the salience detection and the fusion   models from the original gold summaries . The full   pipeline is illustrated in Figure 2 , where additional   implementation details are in § B in the Appendix .   3.1 Proposition Extraction   Aiming to generate proposition - based summaries ,   we ﬁrst extract all propositions from the source doc-   uments using Open Information Extraction ( Ope-   nIE ) ( Stanovsky et al . , 2018 ) , following Ernst et al .   ( 2021 ) . To convert an OpenIE tuple containing   a predicate and its arguments into a proposition   string , we simply concatenate them by their origi-   nal order , as illustrated in Figure 3 in the Appendix .   3.2 Proposition Salience Model   To facilitate the clustering stage , we ﬁrst aim to ﬁl-   ter non - salient propositions by a supervised model .   To that end , we derive gold labels for proposition   salience from the existing reference summaries .   Speciﬁcally , we select greedily propositions that   maximize ROUGE-1 + ROUGE-2against   their reference summaries ( Nallapati et al . , 2017 ;   Liu and Lapata , 2019 ) and marked them as salient.1767   Using this derived training data , we ﬁne - tuned   the Cross - Document Language Model ( CDLM )   ( Caciularu et al . , 2021 ) as a binary classiﬁer for   predicting whether a proposition is salient or not .   Propositions with a salience score below a certain   threshold were ﬁltered out . The threshold was   optimized with the full pipeline against the ﬁnal   ROUGE score on the validation set . All proposi-   tions contained in the clusters in Table 1 are exam-   ples of predicted salient propositions . We chose   to use CDLM as it was pretrained with sets of re-   lated documents , and was hence shown to operate   well over several downstream tasks in the multi-   document setting ( e.g. , cross - document corefer-   ence resolution and multi - document classiﬁcation ) .   3.3 Clustering   Next , all salient propositions are clustered to se-   manticly similar groups . Clusters of paraphrasticpropositions are advantageous for summarization   as they can assist in avoiding redundant information   in an output summary . Furthermore , paraphrastic   clustering offers redundancy as an additional indi-   cator for saliency , while the former salience model   ( § 3.2 ) does not utilize repetitions explicitly . To   cluster propositions we utilize SuperPAL ( Ernst   et al . , 2021 ) , a binary classiﬁer that measures para-   phrastic similarity between two propositions . All   pairs of salient propositions are scored with Super-   PAL , over which standard agglomerative clustering   ( Ward , 1963 ) is applied . Examples of generated   clusters are presented in Table 1 .   3.4 Cluster Ranking   The resulting proposition clusters are next ranked   according to cluster - based properties . We exam-   ined various features , listed in Table 2 , on our vali-   dation sets . The features examined include : aver-1768age of ROUGE scores between all propositions in   a cluster ( ‘ Avg . ROUGE ’ ) , average of SuperPAL   scores between all propositions in a cluster ( ‘ Avg .   SuperPAL ’ ) , average of the salience model scores   of cluster propositions ( ‘ Avg . salience ’ ) , minimal   position ( in a document ) of cluster propositions   ( ‘ Min . position ’ ) , and cluster size ( ‘ Cluster size ’ ) .   For each feature , ( 1 ) clusters were ranked ac-   cording to the feature , ( 2 ) the proposition with the   highest salience model score ( § 3.2 ) was selected   from each cluster as a cluster representative , ( 3 )   the representatives from the highest ranked clus-   ters were concatenated to obtain a system summary .   We also measured combinations of two features   ( ‘ Cluster size + Min . position ’ for example ) , where   the ﬁrst feature is used for primary ranking , and   the second feature is used for secondary ranking in   case of a tie . In all options , if a tie is still remained ,   further ranking between clusters is resolved accord-   ing to the maximal proposition salience score of   each cluster . The resulting ROUGE scores of these   summaries on validation sets are presented in Table   2.We found that ‘ Cluster size ’ yields the best   ROUGE scores as a single feature , and ‘ Min . po-   sition ’ further improves results as a secondary tie   breaking ranking feature . Intuitively , a large cluster   represents redundancy of information across docu-   ments thus likely to indicate higher importance .   3.5 Cluster Fusion   Next , we would like to merge the paraphrastic   propositions in each cluster , while consolidating   complementary details , to generate a new coherent   summary sentence . As mentioned , this approach   helps avoiding redundancy , since redundant infor-   mation is concentrated separately in each cluster .   To train a cluster fusion model , we derived train-   ing data automatically from the reference sum-   maries , by leveraging the SuperPAL model ( Ernst   et al . , 2021 ) ( which was also employed in § 3.3 ) .   This time , the model is used for measuring the simi-   larity between each of the cluster propositions ( that   were extracted from the documents ) and each of   the propositions extracted from the reference sum-   maries . The reference summary proposition with   the highest average similarity score to all cluster   propositions was selected as the aligned summary   proposition of the cluster . This summary proposi-   tion was used as the target output for training the   generation model . Although these target OpenIE   propositions may be ungrammatical or non-ﬂuent ,   a human examination has shown that BART tends   to produce full coherent sentences ( mostly contain-   ing only a single proposition ) , even though it was   ﬁnetuned over OpenIE extractions as target . Exam-   ples of coherent generated sentences can be seen in   Table 1 .   Accordingly , we ﬁne - tuned a BART generation   model ( Lewis et al . , 2020 ) with this dedicated train-   ing data . As input , the model receives cluster propo-   sitions , ordered by their predicted salience score   ( § 3.2 ) and separated with special tokens . The ﬁ-   nal bullet - style summary is produced by appending   generated sentences from the ranked clusters until   the desired word - limit is reached .   3.6 Extractive Summarization Version   To support extractive summarization settings , for   example when hallucination is forbidden , we cre-   ated a corresponding extractive version of our   method . In this version , we extracted a represen-   tative proposition for each cluster , which was cho-   sen according to the highest word overlap with the   sentence that was fused from this cluster by our   abstractive version .   4 Evaluation   4.1 Experimental Setup   Datasets . We train and test our summarizer with   the challenging DUC and TAC MDS benchmarks.1769   Speciﬁcally , following standard convention ( Mao   et al . , 2020 ; Cho et al . , 2019 ) , we test on DUC 2004   using DUC 2003 for training , and on TAC 2011   using TAC 2008/2009/2010 for training . These   sets contain between 30 and 50 topics each . For   validation sets , we used DUC 2004 for the TAC   benchmark and TAC 2011 for the DUC benchmark .   Automatic evaluation metric . Following com-   mon practice , we evaluate and compare our sum-   marization system with ROUGE-1/2 / SU4 F1 mea-   sures ( Lin , 2004 ) . Stopwords are not removed , and   the output summary is limited to 100 words .   4.2 Automatic Evaluation   As seen in Table 3 , our abstractive model , de-   noted ProClusterfor Propositional Clustering ,   surpasses all abstractive baselines by a large mar-   gin in all measures on both TAC 2011 and DUC   2004 . Moreover , while the abstractive system   scores were typically inferior to extractive system   scores , ProClusternotably outperforms all ex-   tractive baselines in both benchmarks . Overall , our   ProClusterprovides the new abstractive MDS   state - of - the - art score in this setting . In Figure 4   we present an example of a ProClustersystem   summary along with previous abstractive and ex - tractive state - of - the - art system summaries and the   reference summary .   As said in § 3.6 , we also developed an extractive   version , denoted ProCluster . As ProCluster   selects document propositions that have the highest   overlap with ProClustersentences , ProCluster   achieves similar scores to ProCluster , yielding   the new extractive MDS state - of - the - art results .   For comparison we selected strong baselines , in-   cluding previous state - of - the - art in this setup , in   both the extractive and abstractive settings . See   in Appendix § C for more concise details over   each baseline . For reference , we also present a   proposition - based extractive upperbound for each   dataset ( Oracle ) , where document propositions   were selected greedily to maximize ROUGE-1   + ROUGE-2with respect to the reference sum-   maries .   4.3 Ablation Analysis   To better apprehend the contribution of each of the   steps in our pipeline , Table 5 presents results of the   system when applying partial pipelines .   First , Saliencegenerates summaries simply   consisting of the highest scoring document propo-   sitions , according to the CDLM - based salience   model ( § 3.2 ) . We also trained the salience model   on the sentence- rather than the proposition - level ,   and similarly generated summaries of salient sen-   tences , denoted Salience . The notable im-   provement of Salienceover Saliencein both1770   datasets reveals the advantage of working at the   proposition level for exposing salient information .   This observation is also apparent when compar-   ing the proposition - based oracle ( Oracle ) to the   sentence - based oracle method ( Oracle ) . The re-   sults indicate that proposition - based systems have a   higher ROUGE upperbound across the board , sup-   porting its merit for use in summarization .   Next , we would like to assess the contribu-   tion of the clustering step . Therefore , we applied   Saliencefollowed by clustering and ranking of   clusters ( Sections 3.2 , 3.3 and 3.4 ) , while leaving   the fusion step aside . From each cluster we then se-   lect the proposition with the highest salience score   to be in the system summary . In both datasets ,   the clustering stage provides added improvement ,   suggesting its contribution to our pipeline .   To further demonstrate the potential of our ap-   proach , we also present two additional oracle   scores for extractive upperbound analysis . First , we   examine the potential of optimally selecting clus-   ter representatives for the summary . We greedily   select a single representative per cluster following   the original cluster ranking ( § 3.4 ) that optimizes   the overall ROUGE-1 + ROUGE-2score of   all selected representatives with respect to the ref-   erence summaries ( Oracle ) . These results   express the improvement comparing to our ﬁnal   model ( ProCluster ) , that a better cluster repre - sentative choice could produce , i.e. , up to ~2 R-2   points in TAC 2011 and ~1 point in DUC 2004 .   Another aspect to examine is the potential of   enhanced cluster ranking . To that end , we ﬁrst   selected the highest salience - scoring proposition   as a representative from each cluster . Then , we   greedily selected representatives , one at a time , that   maximized the overall ROUGE-1 + ROUGE-   2against the reference summaries . Effectively ,   this points to a greedily optimized cluster choice   ( Oracle ) . The potential improvement of bet-   ter cluster ranking compared to our ﬁnal model   ( ProCluster ) is hence up to ~5 R-2 points in   TAC 2011 and ~3 points in DUC 2004 . Indeed ,   our approach leaves cluster ranking improvement   to future work .   Overall , we observe that all components of our   multi - step approach are indeed effective for MDS ,   and that there is a great potential for further im-   provements within this architecture .   4.4 Human Evaluation   We further assessed our primary system ,   ProCluster , through manual comparison   against PG - MMR and RL - MMR , which are   state - of - the - art MDS systems in the abstractive and   extractive settings ( respectively ) . Crowdworkers   on Amazon Mechanical Turkwere shown the1771   summaries for a given topic from the three systems   in arbitrary order , along with a corresponding   reference summary . They were asked to rank   the systems with respect to Content ( content   overlap with the reference ) , Readability ( the   degree to which a summary is readable and   well - understood ) , Grammaticality ( avoiding   grammar errors ) , and Non - Redundancy ( avoiding   information repetition ) . Focusing on evaluating   our system , we extract from this ranking a pairwise   comparison between our ProClusterand each   of the two baseline systems , separately . For each   topic , this procedure was repeated for each of the   four available reference summaries . Each such   evaluation instance was judged independently by   three workers , taking their majority vote for each   pairwise comparison .   Table 6 presents the results of these pairwise   comparisons , showing the percentage of cases in   which our system was preferred over each one of   the two baselines , under each of the four evaluation   criteria . As can be seen , our system was favored   in all cases , for both datasets . Furthermore , prefer-   ence is almost always by a large margin , except for   Non - Redundancy against RL - MMR , which avoids   redundancy at a similar success level . Notably ,   as our clustering - based method is focused on im-   proving content selection , the large gap in favor   of ProClusterin the content criterion supports   its advantage , consistently with our ROUGE - score   advantage in the automatic evaluation ( § 4.2 ) .   While our summaries are ( somewhat non-   conventionally ) structured as bullet - style lists of   propositions rather than a coherent paragraph , eval-   uators preferred our style of summarization in   terms of readability . Moreover , as Table 7 points   out , ProClusterappears to be more abstractive   than PG - MMR , as suggested by the reduced n-   gram and sentence overlap with source documents .   Speciﬁcally , about half of the system summary sen-   tences of PG - MMR are fully copied , compared to   about a quarter in our method . While the intensi-   ﬁed abstractiveness of our summaries could have   potentially hindered readability , our system was   nevertheless preferred along this aspect as well .   Our approach leaves fertile ground for further   improving readability by fusing several clusters   together to generate sentences containing multiple   propositions , and by developing sentence planning   and ordering models . Compatible training datasets   for these models can be derived out of the gold   reference summaries , as was done in this work for   the salience ( § 3.2 ) and fusion ( § 3.5 ) models.17725 Paraphrastic Clusters as Summary   Evidence   A unique advantage of a cluster - based summary is   that each summary sentence is linked explicitly to   a group of propositions from which the sentence   was generated , in so providing an “ explanation ” ,   or support evidence , for the output . These cluster   explanations can expand the reader ’s knowledge   and provide complementary facts from the nearby   source context regarding the information from the   generated sentence . Such a feature may be incor-   porated in interactive summarization systems , as   applied in ( Shapira et al . , 2017 ) , where a user can   choose to expand on the facts within a sentence of   the presented summary .   To assess the reliability of such feature , we   veriﬁed that clusters indeed “ explain ” their gen-   erated sentences . To that end , we conducted a   crowdsourced annotation , where a worker marked   whether a cluster proposition mentions the main   idea of its corresponding generated sentence . Each   pair was examined by three workers , with the ma-   jority vote used for the ﬁnal decision . On a random   selection of 25 % of the clusters , we found that , on   average , 89 % and 84 % of a cluster ’s propositions in   DUC 2004 and TAC 2011 support their correspond-   ing generated sentence , with an average cluster size   of 3.4 and 4.8 propositions , respectively .   Furthermore , given this strong alignment of a   cluster to its generated sentence , a cluster facilitates   effective veriﬁcation of faithfulness of its corre-   sponding generated abstractive sentence . Since the   output sentence is based solely on its cluster propo-   sitions , the sentence ’s correctness can be veriﬁed   against the “ explaining " cluster instead of against   the full document set . An example of an unfaith-   ful abstraction is marked in red in Table 1 . To   the best of our knowledge , this is the ﬁrst attempt   for efﬁcient manual assessment of faithfulness in   MDS . We conducted a respective evaluation pro-   cess , through crowdsourcing , to assess the faith-   fulness of our system summaries . A worker saw   a cluster and its generated sentence and marked   whether the sentence was faithful to its origin clus-   ter or not . Overall , this task cost a reasonable price   of 240 $ for both the DUC 2004 and TAC 2011   datasets together . Over the full test sets , the annota-   tions showed that 80 % and 90 % of the DUC 2004   and TAC 2011 summary sentences , respectively ,   were faithful to their corresponding clusters.6 Conclusion   We advocate the potential of proposition - level units   as a cleaner and more accurate unit for summariza-   tion . To that end , we present a new proposition-   level pipeline for summarization that includes an   accurate paraphrastic propositional clustering com-   ponent followed by fusion of cluster propositions ,   to generate concise and coherent summary sen-   tences . Our proposed method outperforms state - of-   the - art baselines in both automatic and human eval-   uation on the DUC and TAC MDS benchmarks . We   provide an ablation study that indicates the beneﬁt   of each of the pipeline steps , as well as the poten-   tial for future improvement . Moreover , we demon-   strate the utility of the clustering - based approach   for providing source documents explanations and   for manually validating summary faithfulness .   Acknowledgments   The work described herein was supported in part   by the PBC fellowship for outstanding PhD candi-   dates in data science , Intel Labs , the Israel Science   Foundation grant 2827/21 , and by a grant from the   Israel Ministry of Science and Technology .   Ethical Considerations   Computation . We ran on 3 GPUs for 20 min-   utes to ﬁnteune each of the salience model and the   fusion model .   The summarization model runs 10 minutes on 4   GPUs to generate a summary . Most of the time is   spent on the clustering step , in which we calculate   the SuperPAL similarity score between all salient   proposition pairs .   Dataset . The DUC 2003 and 2004 and TAC   2008 - 2011 datasets were acquired according to the   required NIST guidelines ( ) .   Crowdsourcing . All human annotations and   evaluations conducted with crowdsourcing were   compensated as a 12 $ per hour wage . We esti-   mated the task payment by completing sample as-   signments and obtaining the average assignment   time .   References177317741775   A Data Statistics   B Implementation Details   B.1 Proposition Salience Model   Datasets . For many previous summarization sys-   tems these benchmarks were insufﬁciently large   enough for training their models . Consequently ,   they pretrained on a large scale summarizationdataset , such as CNN / DailyMail ( Hermann et al . ,   2015 ) , and then ﬁnetuned on DUC / TAC datasets   ( e.g. , Lebanoff et al . , 2018 ; Mao et al . , 2020 ) . In   our case , we avoid external sources . However , as   DUC training data is much smaller than TAC ’s   ( 30 topics vs. 138 ) , and it was apparently too   small for the salience model training , we adopted   the trained salience model for TAC benchmark   ( that was trained with TAC 2008 - 2010 ) as a pre-   trained model and then ﬁnetuned it with DUC 2003 .   Accordingly , validating the TAC benchmark us-   ing DUC 2004 during the salience model training   causes data leakage since this model is later ﬁne-   tuned to test on the same DUC 2004 . To avoid that ,   during the salience model training we used part of   TAC 2010 that was omitted from training data , as a   validation set ( instead of DUC 2004 ) .   Training Parameters . We trained the model for   10 epochs with learning rate of 1e-5 and batch size   of 6 instances on 3 V100 GPUs ( meaning effective   batch size was 18 ) .   Training . The CDLM model is fed with a propo-   sition within its document and the other documents   in the set . Speciﬁcally , since CDLM ’s input size is   limited to 4,096 tokens , it is infeasible to feed the   full document set as a long sequence . Therefore ,   following Lebanoff et al . ( 2019 ) , only the ﬁrst 20   sentences of each document are considered . Ac-   cordingly , a candidate proposition is input within   its full document ( up to 20 sentences ) , while other   documents , ordered by their date , are truncated   evenly and concatenated to ﬁll the remaining space   ( 9 sentences per document on average ) .   Each instance contains a proposition marked   with start and end special tokens , within its multi-   ple document context . A discontinuous proposition   is marked with special tokens before and after each   of its parts . In addition , sentence special token sep-   arators and document special token separators are   used , as required for CDLM .   In order to reduce computation complexity ,   CDLM uses “ local attention " ( of 512 tokens ) for   all tokens , while speciﬁc tokens are attended to all   4096 tokens ( “ global attention " ) . In our case , we   assigned global attention to the CLS token and to   the candidate proposition tokens , including their   special tokens .   For classiﬁcation , we have added a binary classi-   ﬁer layer on top of our CDLM . The classiﬁcation   layer gets the CDLM ’s CLS output representation1776   concatenated to the sum of the CDLM output rep-   resentations of the candidate proposition tokens :   CLS⊙/summationdisplayT ( 1 )   where Tis the CDLM output representative of the   i - th token , and Prop contains the token indices of   the candidate proposition .   As our proposition salience training dataset con-   tains only a few positive ( i.e. , salient ) propositions   with respect to all propositions , it creates an unbal-   anced dataset that may strongly bias the model to   give a negative prediction . To cope with this , we   randomly ﬁlter out 60 % of the non - salient propo-   sitions , while over sampling salient propositions   until the dataset becomes balanced .   B.2 SuperPAL Usage   In this work we used the SuperPAL model ( Ernst   et al . , 2021 ) as the similarity metric between propo-   sitions for the clustering step ( § 3.3 ) , and to create   training data for the fusion model ( § 3.5 ) . Origi-   nally , SuperPAL was tuned with a validation set   that contains three topics from DUC 2004 ( taken   from the full validation set which also contains 7   additional topics , not from DUC 2004 ) . In our set-   ting , it may cause leakage since DUC 2004 is used   as the test data . To avoid such leakage , we tuned   SuperPAL again without using DUC 2004 topics   at all ( using the other 7 topics as a validation set ) .   B.3 Cluster Ranking   For computation time consideration , we set a max-   imum number of clusters to be selected for each   topic . Since in most topics the 100 - word limit isexceeded after 8 - 10 propositional sentences , we set   the maximum number of clusters to 10 . Accord-   ingly , the 10 ( or fewer ) highest ranked clusters are   selected for the summary of each topic .   B.4 Fusion Model   Training Parameters . We trained the model for   3 epochs with learning rate of 3e-5 and batch size   of 10 instances on 3 V100 GPUs ( meaning effective   batch size was 30 ) .   C Compared Methods   We compare our method to several strong abstrac-   tivebaselines : Opinosis ( Ganesan et al . , 2010 ) gen-   erates abstracts from salient paths in a word co-   occurrence graph ; Extract+Rewrite ( Song et al . ,   2018 ) selects sentences using LexRank and gen-   erates for each sentence a title - like summary ; PG   ( See et al . , 2017 ) runs a Pointer - Generator model   that includes a sequence - to - sequence network with   a copy - mechanism ; PG - MMR ( Lebanoff et al . ,   2018 ) selects representative sentences with MMR   ( Carbonell and Goldstein , 1998 ) and fuses them   with a PG - based model ; Hi - MAP ( Fabbri et al . ,   2019 ) is a hierarchical version of the PG model   that allows calculating sentence - level MMR scores ;   MDS - Joint - SDS ( Jin and Wan , 2020 ) is a hierar-   chical encoder - decoder architecture that is trained   with SDS and MDS datasets while preserving doc-   ument boundaries .   We additionally compare to several strong ex-   tractive baselines : SumBasic ( Vanderwende et al . ,   2007 ) extracts phrases with words that appear fre-   quently in the documents ; KLSumm ( Haghighi and   Vanderwende , 2009 ) extracts sentences that opti-1777mize KL - divergence ; LexRank ( Erkan and Radev ,   2004 ) is a graph - based approach where vertices   represent sentences , the edges stand for word over-   lap between sentences , and sentence importance   is computed by eigenvector centrality ; DPP - Caps-   Comb ( Cho et al . , 2019 ) balances between salient   sentence extraction and redundancy avoidance by   optimizing determinantal point processes ( DPP ) ;   HL - XLNetSegs andHL - TreeSegs ( Cho et al . , 2020 )   are two versions of a DPP - based span highlight-   ing approach that heuristically extracts candidate   spans by their probability to begin and end with   an EOS token ; RL - MMR ( Mao et al . , 2020 ) adapts   a neural reinforcement learning single document   summarization ( SDS ) approach ( Chen and Bansal ,   2018 ) to the multi - document setup and integrates   Maximal Margin Relevance ( MMR ) to avoid re-   dundancy .   D Annotation Guidelines   We used Amazon Mechanical Turkfor all three   crowdsource tasks with a list of 90 pre - selected   workers from English speaking countries . These   workers accomplished high quality work in other   NLP - related tasks we have conducted in the past .   The crowdsourcing instructions of the tasks men-   tioned in § 4.4 & 5 are as follows :   D.1 General Summarization System   Evaluation .   Read the following four texts ( Text A , B , C , and D )   and answer the following questions .   Text A :   < Reference summary >   Text B :   < System summary 1 >   Text C :   < System summary 2 >   Text D :   < System summary 3 >   •Which of the texts B , C , or D has the highest   content overlap with text A ?   •Which of the texts B , C , or D has   the 2nd highest content overlap with text A ?   •Which of the texts B , C , or D is the most read-   able and well - understood ?   •Which of the texts B , C , or D is the 2nd most   readable and well - understood?•Which of the texts B , C , or D avoids grammar   mistakes the best ?   •Which of the texts B , C , or D avoids grammar   mistakes the 2nd best ?   •Which of the texts B , C , or D avoids informa-   tion repetition the best ?   •Which of the texts B , C , or D avoids informa-   tion repetition the 2nd best ?   D.2 Supporting Cluster Evaluation .   Read the following two text spans , and answer the   question below .   Span Text A :   < The generated sentence >   Span Text B :   < A proposition from the cluster >   Is the main fact of Span Text A mentioned in   Span Text B ? ( ignoring additional details )   Yes / No   D.3 Faithfulness Evaluation .   Read the following group of text spans A and text   span B , and answer the questions below . You can   assume that all text spans in group A describe the   same event , and therefore can be consolidated to-   gether to imply Text Span B.   Examples :   1.Group of Text Spans A :   •They arrested John .   •John was arrested .   Text Span B :   The FBI arrested John   Is the Group of Text Spans A implies the   fact in Text Span B ?   Text Span B add a detail that is not mentioned   in A. Therefore the answer is No .   2.Group of Text Spans A :   •there were 10 - 12 girls and 15 boys in the   schoolhouse   •there were boys and girls in the school-   house   Text Span B :   there were 1012 girls and 15 boys in the   schoolhouse1778Is the Group of Text Spans A implies the   fact in Text Span B ?   Text Span B contradicts Group A ( instead of   10 - 12 girls it says 1012 girls ) . Therefore the   answer is No.1779