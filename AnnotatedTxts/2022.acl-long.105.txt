  Yun Tang , Hongyu Gong , Ning Dong , Changhan Wang ,   Wei - Ning Hsu , Jiatao Gu , Alexei Baevski , Xian Li ,   Abdelrahman Mohamed , Michael Auli , Juan Pino   Meta AI   fyuntang , hygong , dnn , changhan , wnhsu , jgu , abaevski , xianl ,   abdo , michaelauli , juancarabina g@fb.com   Abstract   We describe a method to jointly pre - train   speech and text in an encoder - decoder mod-   eling framework for speech translation and   recognition . The proposed method incor-   porates four self - supervised and supervised   subtasks for cross modality learning . A   self - supervised speech subtask leverages un-   labelled speech data , and a ( self-)supervised   text to text subtask makes use of abundant text   training data . Two auxiliary supervised speech   tasks are included to unify speech and text   modeling space . Our contribution lies in in-   tegrating linguistic information from the text   corpus into the speech pre - training . Detailed   analysis reveals learning interference among   subtasks . Two pre - training conﬁgurations   for speech translation and recognition , respec-   tively , are presented to alleviate subtask inter-   ference . Our experiments show the proposed   method can effectively fuse speech and text in-   formation into one model . It achieves between   1.7 and 2.3 BLEU improvement above the   state of the art on the MST - C speech transla-   tion dataset and comparable WERs to wav2vec   2.0 on the L speech recognition   task .   1 Introduction   Pre - training can learn universal feature represen-   tations from a large training corpus and is beneﬁ-   cial for downstream tasks with limited amounts   of training data ( Peters et al . , 2018 ; van den   Oord et al . , 2018 ; Chung et al . , 2018 ; Zoph et al . ,   2020 ) . With the advancement of computational   power and self - supervised pre - training approaches ,   large volumes of unlabeled data may now be used   in pre - training . Methods , such as BERT ( Devlin   et al . , 2019 ) , BART ( Lewis et al . , 2020b ) and   wav2vec2.0 ( Baevski et al . , 2020b ) , have emerged   as the backbone of many speech and natural lan-   guage processing tasks . The aforementioned pre - training methods focus   on learning feature representation either from text   or speech . Many speech applications combine in-   formation learnt from both speech and text corpora   to achieve state of the art results . In speech process-   ing , transcribed speech training data is generally   very scarce for many languages . It is difﬁcult to   build robust linguistic knowledge representation   solely based on labeled speech training data . Jia   et al . ( 2019 ) ; Chen et al . ( 2021 ) propose to gen-   erate synthetic data from text to augment speech   training corpus . Li et al . ( 2021 ) demonstrate that   models initialized with pre - trained wav2vec2.0 and   mBART ( Liu et al . , 2020 ) modules are competi-   tive for the multilingual speech to text translation   task . Chuang et al . ( 2020 ) propose to concatenate   the acoustic model and BERT model for speech   Q&A. Chung et al . ( 2021b ) align speech utterance   representation to the corresponding text sentence   representation , in which both representations are   generated from unsupervised pre - trained models ,   for speech understanding .   In this study , we are interested in pre - training   for speech to text tasks using the Attention based   Encoder - Decoder ( AED ) framework . In particu-   lar , we seek to answer the question whether the   integration of data from different modalities is ben-   eﬁcial for representation learning . To answer this   question , we propose Speech and Text joint Pre-   Training ( STPT ) , a multi - task learning framework   to combine different modalities , i.e. , speech and   text , in the pre - training stage . A self - supervised   speech subtask and a ( self-)supervised text to text   subtask dominate the pre - training computation to   leverage large amounts of unlabelled speech data   and abundant text training corpus . Two auxiliary   supervised speech subtasks are used to unify dif-   ferent modalities in the same modeling space . The   proposed method fuses information from the text   and speech training corpus into a single model , and   it effectively improves the performance of down-1488stream tasks , such as speech to text translation ( ST )   and automatic speech recognition ( ASR ) . Our con-   tributions are summarized as follows :   1.We propose a multi - task learning framework   to learn four speech and text subtasks in one   model and successfully integrate linguistic in-   formation from the text corpus into the speech   pre - training .   2.We conduct detailed analyses on the proposed   pre - training method , which reveal the interfer-   ence among different subtasks .   3.Two joint pre - training conﬁgurations are pro-   posed to alleviate learning interference for   ASR and ST respectively .   4.State - of - the - art results are achieved on the   downstream tasks . We obtain at least 1.7   BLEU improvement compared with the best   MST - C ST system reported and comparable   WERs as wav2vec 2.0 in the L   ASR task .   2 Related work   Pre - training : Self - supervised pre - training is usu-   ally optimized with two different criteria : con-   trastive loss ( van den Oord et al . , 2018 ; Chung and   Glass , 2020 ; Baevski et al . , 2020b ) and masked   prediction loss ( Devlin et al . , 2019 ) . Contrastive   loss focuses on distinguishing the positive samples   from the negative ones given the reference sam-   ple and it has achieved great success for speech   recognition ( Baevski et al . , 2020b ) . Masked predic-   tion loss has been ﬁrst studied for natural language   processing tasks ( Devlin et al . , 2019 ; Lewis et al . ,   2020b ) with subsequent application to speech pro-   cessing ( Baevski et al . , 2020a ; Hsu et al . , 2021 ) .   Chung et al . ( 2021a ) combine contrastive loss and   masked prediction loss , which shows good perfor-   mance for the downstream ASR task . The opti-   mization of our self - supervised speech task is more   related to the masked prediction loss . Instead of   predicting the hard discretized label for the masked   frames , which is error prone , we use KL divergence   to minimize the distribution difference between the   same feature frames with and without masking .   Please refer to subsection 3.2 for more details .   Self - training ( or iterative pseudo labelling ):   self - training is another widely used approach to   take advantage of unlabelled speech data to im-   prove the ASR performance ( Kahn et al . , 2020 ; Xuet al . , 2020 ; Pino et al . , 2020 ; Zhang et al . , 2020 ;   Wang et al . , 2021a ; Xiao et al . , 2021 ; Wang et al . ,   2021b ) . A seed model , which usually is trained   with a small amount of supervised speech train-   ing data , is employed to generate pseudo labels   for the unlabelled speech data . The speech data   with pseudo labels is augmented into the training   dataset to build another model , which is expected   to outperform the seed model due to more train-   ing data exposure . Similar to self - training , we also   use small amounts of supervised data to unify the   speech and text modeling space . However , the   self - supervised speech training in this work avoids   making hard predictions and uses KL divergence to   maximize the mutual information between masked   span and observed feature frames .   Multi - task learning : Due to data scarcity , multi-   task learning is widely adopted to leverage parallel   text training data for ST ( Weiss et al . , 2017 ; Anasta-   sopoulos and Chiang , 2018 ; Tang et al . , 2021b ; Ye   et al . , 2021 ) . Those methods primarily use super-   vised speech data sets during multi - task learning ,   whereas our method can leverage large amounts of   unlabeled speech data during the pre - training stage ,   which has the potential to improve performance   even further .   A concurrent work from Ao et al . ( 2021 ) also   proposes to jointly pre - train speech and text for   ASR and text to speech application , which is fully   unsupervised . Our method focuses on taking advan-   tage of the supervised speech data , which could be   the same data used for ﬁne - tuning , to improve the   joint speech text pre - training . Our results demon-   strate the efﬁcacy of supervised speech data in pre-   training . Another concurrent work is from Bapna   et al . ( 2021 ) , which focuses on speech encoder   pre - training using both speech and text data . Our   method emphasizes the encoder - decoder frame-   work and training both encoder and decoder in the   pre - training stage .   3 Method   ASR and ST are the two main downstream tasks for   the proposed pre - training method . Figure 1 depicts   our joint pre - training framework , which consists of   four subtasks :   1 . ( Self-)supervised Text to Text subtask ( T2 T )   2.Self - supervised Speech Learning subtask   ( SSL)1489   3.Supervised Speech to Phoneme classiﬁcation   subtask ( S2P )   4.Supervised AED based Speech to Text sub-   task , which is the same as the downstream   task , i.e. , ST or ASR ( S2 T )   The choice of the T2 T subtask depends on the   downstream task . For ASR , the T2 T subtask is a   denoising autoencoder task ( BART ) ( Lewis et al . ,   2020a ) while ST utilizes a text based neural ma-   chine translation task . The SSL subtask is a self-   supervised speech learning task to leverage large   amounts of unlabelled speech data optimized by   the masked prediction loss . The last two supervised   speech tasks ( S2P and S2 T ) unify two modalities ,   i.e. , speech and text , into one modeling space .   In this study , we ﬁnd that the subtasks for the   ASR pre - training are complementary , while sub-   task interference is observed in the ST pre - training   at some encoder layers . We propose two different   conﬁgurations : fully shared encoder ( FSE ) ( Fig-   ure 1(a ) ) for the ASR pre - training , and partially   shared encoder ( PSE ) ( Figure 1(b ) ) for the ST   pre - training . The FSE conﬁguration aims to en-   courage information sharing between different sub-   tasks while the PSE conﬁguration tries to minimize   the information sharing between encoder only sub-   tasks , i.e. , SSL and S2P , and sequence to sequence   AED tasks , i.e. , subtask T2 T and S2T. More sub-   task interference analysis is presented in subsec-   tion 5.2 . We describe the details of each subtask in   the following subsections.3.1 ( Self-)supervised text to text subtask   In the sequence to sequence ASR and ST tasks ,   the decoder is a text generator conditioned on the   encoder outputs . Large amounts of training sam-   ples are required to cover different linguistic as-   pects of the target language . Abundant text is an   ideal supplement to the limited supervised speech   data corpus . Assume the target text sequence is   Y= ( y;y;;y ) , its corresponding corrupted   version , X = N(Y ) = ( x;x;;x ) ,   can be created by masking or replacing token spans   inY(Lewis et al . , 2020a ) for the ASR pre - training .   If the downstream task is ST , Xis the correspond-   ing source token sequence . The task is optimized   by maximizing cross entropy   L= Xlogp(yjy;X ) ( 1 )   In this subtask , we also convert the input text   into the corresponding pronunciation form , i.e. ,   phoneme sequence , as it would be easier to align   the encoder outputs from speech and text ( Tang   et al . , 2021b ) . The purple and black lines in Fig-   ure 1 describe the data ﬂow in the T2 T subtask .   3.2 Self - supervised speech subtask   The SSL subtask aims to leverage vast amounts of   unlabelled speech data and learn general speech   representations . The model conﬁguration follows   wav2vec2.0 ( Baevski et al . , 2020b ) where the   speech model includes a feature extractor and a1490context encoder . The context encoder corresponds   to the speech encoder in Figure 1(b ) in the ST   pre - training . If ASR is the downstream task , the   context encoder includes one extra shared encoder   as shown in Figure 1(a ) . We use different frame-   works for the ST and ASR pre - training to reduce   interference among subtasks . The detailed subtask   interference is discussed in subsection 5.2 .   We propose a masked KL divergence loss to   optimize the SSL subtask . It consists of two-   pass computation . Given the speech input S=   ( s;s;;s ) , the feature extractor and context   encoder outputs are Z= ( z;z;;z)and   O= ( o;o;;o)respectively , where the   speech input is down - sampled by the feature ex-   tractor and T > T. In the ﬁrst pass , the out-   putOis compared with the phoneme embedding   E= ( e;e;;e ) , which is from the T2 T sub-   task described in subsection 3.1 . Iis the phoneme   vocabulary size . The predicted phoneme distribu-   tionp(oje)is deﬁned as   p(oje ) = exp(oe)Pexp(oe)(2 )   In the second pass , speech feature spans ^ZZ   are selected and corrupted as wav2vec2.0 ( Baevski   et al . , 2020b ) . ^Ois the corresponding context en-   coder output from ^Z. We train the model to infer   the corrupted p(^oje)being similar as p(oje)by   minimizing KL divergence .   L= XXp(oje ) logp(^oje )   p(oje)(3 )   Compared with the masked prediction loss , in-   stead of predicting the hard discretized label for   the masked frames , we use the soft label prediction ,   i.e. , predicted phoneme distribution from the ﬁrst   pass , to learn speech representation and avoid the   hard prediction errors .   3.3 Supervised speech to phoneme   classiﬁcation   The S2P subtask is employed to unify the self-   supervised trained speech and text models . It   shares the same model as in the SSL subtask . In   this subtask , a transcribed ASR data set is used   and the goal of this task is to predict the frame   level phoneme labels . A HMM - GMM model is   trained with the same transcribed dataset using   Kaldi ( Povey et al . , 2011 ) to generate the frame-   level labels with forced - alignment . The phoneme classiﬁcation task is optimized   with the cross entropy loss ,   L= Xlogp(oje ) ( 4 )   wherea(j)is the phoneme label associated with   the context encoder output o. The data ﬂow in   the S2P subtask is depicted with steelblue lines in   Figure 1 .   3.4 Supervised AED based speech to text   subtask   Besides the S2P subtask mentioned in the previous   subsection , we include the potential downstream   AED based task , i.e. ASR or ST , as another aux-   iliary subtask during the pre - training stage . In   many speech translation datasets , such as MuST-   C ( Gangi et al . , 2019 ) or CoV oST ( Wang et al . ,   2020 ) , we have both speech transcription and trans-   lation labels . The speech transcription is used in   the S2P subtask while the S2 T subtask can make   use of the corresponding translation labels . We   hope this auxiliary task would make the transition   from pre - training to ﬁne - tuning smooth and result   in better performance in downstream tasks . The   components involved during optimization are con-   nected with blue lines in encoder and black lines   in decoder as shown in Figure 1 . They are trained   with cross entropy criterion ,   L= Xlogp(yjy;O ) ( 5 )   whereOis the input speech and Y= ( y;;y )   is the target labels .   The overall pre - training loss is deﬁned as the   combination of four losses discussed above   L = L+  L+  L+   L ( 6 )   where  ,  and   are task weights for the SSL , S2P   and S2 T subtasks respectively .   During the pre - training , the shared encoder in-   puts come from two sources , either from speech   encoder outputs in the S2 T subtask or phoneme   embeddings in the T2 T subtask . The shared en-   coder inputs might be in different numerical scales .   In order to stabilize the multi - task training , a Lay-   erNorm ( Ba et al . , 2016 ) is applied to the shared   encoder inputs and places those inputs in the same   numerical scale as shown in Figure 1.14914 Experimental setting   In the pre - training , we ﬁrst train modules with the   T2 T subtask until they are converged . It helps to   stabilize the training and achieve a better result .   Then the entire model is jointly optimized with   all subtasks mentioned in section 3 . Finally , the   pre - trained model is ﬁne - tuned on the downstream   tasks . In the ﬁne - tuning stage , we keep optimizing   the model with the T2 T and S2 T subtasks . Two   encoder - only subtasks ( SSL and S2P ) are dropped ,   since the model has learnt good speech representa-   tion from the unlabeled speech data in pre - training .   Two downstream tasks , ASR and ST , are ex-   amined . The ASR system is evaluated on four   L ( Panayotov et al . , 2015 ) evaluation   sets : dev - clean , dev - other , test - clean and test - other .   WER is reported in the experiments . ST mod-   els are evaluated on two translation directions :   English - Spanish ( EN - ES ) and English - French ( EN-   FR ) . Case - sensitive detokenized ( Post ,   2018 ) is reported on the tst - COMMON testset from   MST - C ( Gangi et al . , 2019 ) .   For both ASR and ST pre - training , 60k hours   of unlabelled English speech data from Libri-   light ( Kahn et al . , 2020 ) is used to build the self-   supervised speech task if not speciﬁcally men-   tioned . We employ the same labelled data for the   supervised learning in pre - training and ﬁne - tuning ,   i.e. , L training data for ASR and   MST - C for ST . For ASR pre - training , the L- language model ( LM ) training dataset   is used to build the monolingual BART model . For   ST pre - training , we take the parallel training corpus   from WMT . More details about the training data   could be found in Appendix A.   4.1 Model conﬁguration   The model takes raw speech audio as input . The   feature encoder contains seven blocks and the tem-   poral convolutions in each block have 512 chan-   nels with strides ( 5,2,2,2,2,2,2 ) and kernel widths   ( 10,3,3,3,3,2,2 ) . The speech encoder , shared en-   coder and shared decoder are all with 6 transformer   layers , model dimension 768 , inner dimension   ( FFN ) 3,072 and 8 attention heads . We adopt Pre-   LN in the transformer block as Xiong et al . ( 2020 ) .   The total number of parameters is 169 million .   The task weight for each subtask is set by the   number of mini - batches used during training . In   the pre - training , the ratio of mini - batch numbers   for each subtasks are 1.0 , 7.0 , 0.5 and 0.5 for theT2 T , SSL , S2P and S2 T subtasks respectively .   We mask 30 % tokens in the T2 T BART subtask   in ASR pre - training , and no masking is applied for   the T2 T NMT subtask in the ST pre - training . 7 %   of the feature frames in the SSL subtask and 3 %   of the feature frames in the two supervised speech   subtasks are randomly selected as the mask span   starting time - step . The mask span length is 10 .   The masking percentage is selected via grid search   ( ( 20;30)for text masking , ( 6;6:5;7)and(2;3)for   speech masking ) . Additional experimental details   such as optimization hyper - parameters are included   in Appendix B.   5 Experimental results   5.1 Main results   We present the L recognition results   in Table 1 . Recognition results without / with an   decoding LM are reported . The WERs obtained   with LM are displayed within “ ( ) ” . The second   column shows the dataset used as unlabeled data   in pre - training . “ LS-960 ” stands for L   training dataset and “ LV-60k ” is the 60,000 hours   Librilight dataset . The decoding LM is built with   theL text training corpus , which is   the text corpus used by the T2 T subtask in the ASR   pre - training and ﬁne - tuning .   The ﬁrst part of the table shows results from the   wav2vec 2.0 base model , which is a CTC based   ASR system . Second part of the table presents re-   sults from two AED based ASR systems , and we   mainly compare the proposed method with those   two AED systems . LAS is a LSTM based system   trained with the L data only . Trans-   former ( Tang et al . , 2021b ) is based on multi - task   learning and jointly trained with a text task .   The results from STPT models are presented   in the third part of the table . The fourth row   shows results from a model that uses 960 hours   L training data as the unlabelled pre-   training data while the model in the ﬁfth row is pre-   trained with the 60k hours Librilight data . STPT   outperforms all previous reported AED - based sys-   tems . On average , there is a 1.2 absolute WER   reduction obtained compared to the jointly trained   transformer model ( Tang et al . , 2021b ) . STPT also   reduces 2.2 WER compared with the CTC based   wav2vec model if no external LM is applied and   achieves comparable WERs when it is decoded   with a LM . One interesting observation is the de-   coding LM is not very helpful for the STPT model,1492   that only 0.2 WER reduction is observed when a   decoding LM is applied . Other systems , on the   other hand , show a considerable WER reduction   when the LM is applied during decoding . It indi-   cates that our multi - task learning in the pre - training   and ﬁne - tuning stages can effectively fuse linguis-   tic information in the text data corpus into the ASR   model . LM might not be required if it is trained on   the same text corpus . We also report results from   model pre - trained with 60k hours Librilight data   at the ﬁfth row . Compared with the LS-960 STPT   model , Librilight data helps to reduce the WER   in two difﬁcult “ other ” datasets . In the following   experiments , we will use Librilight as unlabelled   data in pre - training .   In Table 2 , we present the speech translation re-   sults on the MuST - C datasets . Row one to four   are the latest results from literature . Row one   shows the results by training a speech to text trans-   lation task alone . Row two and three present re-   sults from two multi - task systems with speech and   text jointly trained together . Row four is the best   system reported , which is initialized with the pre-   trained wav2vec 2.0 and machine translation model ,   then ﬁne - tuned with the joint speech and text train-   ing . Our method achieves 2.3 and 1.7 more BLEU   scores for EN - ES and EN - FR translation directions   compared with the best system ( Ye et al . , 2021 ) .   5.2 Impact of model structure   Interference among subtasks may impede the   progress of multi - task learning and lead to inferior   results . In this study , we examine the task interfer - ence via comparing the gradient similarity between   pair subtasks . We choose the pre - trained models   using the FSE conﬁguration discussed in section 3   and accumulate gradients from one of four jointly   trained subtasks . We prepare 20batches of training   samples for each subtask , and retrieve the accu-   mulated gradients by sending these batches to the   models . Then we calculate the pairwise cosine sim-   ilarity between gradients from any two subtasks .   The pairwise subtask gradient similarity from   the shared encoder are presented in Figure 2 . The   Figure 2(a ) demonstrates the gradient similarity   in ASR pre - training . In most layers , the gradient   similarities are small . No serious gradient inter-   ference is observed . The Figure 2(b ) depicts the   gradient similarity from the ST pre - training . Com-   pared with the ASR pre - training , the S2 T and T2 T   subtasks are replaced by speech translation and   text based neural machine translation subtasks in   pre - training . The interference between different   subtasks is signiﬁcant as large positive and nega-   tive gradient similarities are observed in the third   and ﬁfth layers in Figure 2 .   Similarly , we compare task gradients in the   speech encoder and no obvious task interference   is observed within the speech encoder for both   ASR and ST pre - training . Detailed analysis on   the speech encoder is included in the Appendix C.   In order to alleviate the task interference , the   PSE conﬁguration is proposed for the ST pre-   training . Table 3 presents the performance com-   parison between two conﬁgurations on both ASR   and ST pre - training . On the left part of the table ,   we list the ASR results using 100 hours labelled   speech data ( train - clean-100 ) in pre - training and   ﬁne - tuning . While the right part of the table shows   the BLEU evaluated on the MST - C dataset . As   we expected , the FSE conﬁguration encourages   information sharing among tasks and it achieves   lower WER for the ASR task . It indicates subtasks   in the ASR pre - training are complementary to each   other . On the other hand , the PSE conﬁguration1493   minimizes the information sharing between AED   subtasks and encoder only subtasks , and it leads to   higher BLEU for the ST task .   5.3 Impact of training data   The supervised speech data connects the text and   speech modeling and uniﬁes the representation   from different modalities . An interesting question   we want to investigate is how much supervised   data is enough to learn a good cross modality repre-   sentation . In this experiment , we choose different   amounts of labelled data for ASR pre - training and   ﬁne - tuning , varied from 960 hours ( the full dataset ) ,   100 hours ( train - clean-100 ) and 10 hours as ( Kahn   et al . , 2020 ) , to answer this question .   In Table 4 , the ﬁrst column shows the amounts   of supervised speech data available during the pre-   training and the second column presents the amount   of labelled data used in the ﬁne - tuning stage . In   pre - training , the same supervised speech data is   used in the S2P and S2 T subtasks .   The ﬁrst observation is that more supervised   speech data in the pre - training stage is always help-   ful to get smaller WER . For example , if the models   are ﬁne - tuned with the full L train-   ing dataset , the average WER are 3.3 ( row one ) ,   3.6 ( row two ) and 4.0 ( row four ) for experiments   with 960 , 100 and 10 hours labelled data in the   pre - training stage . The second observation is that   we are still able to obtain good speech representa-   tions even with small amounts of labelled data . In   row four , the model is pre - trained with 10 hours   labelled data , then ﬁne - tuned with 960 hours su-   pervised speech data . It can achieve an average   4.0 WER , which is better than the results of the   AED systems in Table 1 . However , we also no-   tice the performance degrades quickly if only small   amounts of labelled speech data are available . The   average WER is increased to 24.6 ( row six ) when   only 10 hours of supervised speech data is em-   ployed in both pre - training and ﬁne - tuning .   Another question we are interested is the gen-   eralizability of the pre - trained model . There are   two data partitions in L : “ clean ” and   “ other ” . The “ clean ” partition is supposed to be   “ higher recording quality and with accents closer to   US English ” while the “ other ” partition is difﬁcult   speakers with high WER ( Panayotov et al . , 2015 ) .   We create four data partitions for pre - training and   ﬁne - tuning to simulate the mismatched training   conditions . “ train - clean-100 ” is used as the pre-   training “ clean ” data set ( “ PT C ” ) and the ﬁrst   30,000 utterance from “ train - clean-360 ” as the ﬁne-   tuning “ clean ” dataset ( “ FT C ” ) . The ﬁrst 30,000 ut-1494   terances and the following 30,000 utterances from   “ train - other ” are used as the pre - training ( “ PT O ” )   and ﬁne - tuning “ other ” ( “ FT O ” ) datasets . Each   dataset includes approximately 100 hours speech   data . In Table 5 , models are trained under 4 dif-   ferent combinations with different supervised pre-   training and ﬁne - tuning data sets . We report aver-   age WER on the ” dev - clean ” and “ test - clean ” test   sets as “ clean ” , and average WER on the“dev - other ”   and “ test - other ” as “ other ” to reduce the result vari-   ation . From Table 5 , we have following observa-   tions . 1 ) a model achieves the best results on the   matched condition . The model “ PT C + FT C ”   achieves the lowest WER on the “ clean ” set while   “ PT O + FT O ” achieves the best results on the   “ other ” set . 2 ) training and test on totally differ-   ent conditions could increase WER signiﬁcantly .   The model “ PT C + FT C ” increases 0.9 WER on   the “ other ” set compared with the “ PT O + FT O ”   model . 3 ) mismatched pre - training and ﬁne - tuning   might slightly increase the WER , 0.1 to 0.2 in this   experiment .   5.4 Masked KL divergence loss v.s.   contrastive loss   In the SSL subtask , we optimize the model to re-   duce the KL divergence loss between input without   masking and with masking as described in subsec-   tion 3.2 . It is a variant of the masked prediction   loss ( Baevski et al . , 2020a ) and no target labels   are required in our implementation . Contrastive   loss is another widely used method for the self-   supervised speech learning ( Baevski et al . , 2020b ) .   We compare the both criteria in Table 6 . The num-   ber of distractors in the contrastive loss is 100 as   ( Baevski et al . , 2020b ) . Both ASR and ST re-   sults are reported in Table 6 , where the masked   KL divergence loss achieves about 0:6lower WER   in the Librispeech dev sets and 0:71:4more   BLEU scores in the MuST - C tst - COMMON sets .   It demonstrates the effectiveness of the proposed   masked KL divergence loss for the SSL subtask .   5.5 Ablation study   In Table 7 , we present an ablation study by remov-   ing different steps / tasks in the pre - training stage .   In order to make the pre - training more stable ,   the model training adopts a three - stage optimiza-   tion strategy : 1 ) pre - training the T2 T subtask to   have a good initialization on the phoneme embed-   dings 2 ) joint pre - training with four subtasks to   leverage large amounts of unlabelled speech data   and abundant text data and 3 ) ﬁne - tuning the model   on the downstream task for best performance . In   the second row , we skip the T2 T pre - training step   and initialize the model randomly for the joint pre-   training . 0.5 WER increase is observed in average   on two L dev sets . It also has more im-   pact on the EN - ES translation direction where 1.2   BLEU score is lost without proper initialization .   In the third row , we present the results without   the S2 T subtask . For both ASR and ST , signiﬁ-   ca nt performance degradation is observed , with an   average 1.1 WER increase for two ASR tests and   1.8 BLEU decrease for two ST directions . We also   try removing the S2P subtask while still keeping   the S2 T subtask . The training does n’t converge .   The SSL subtask is with very small or zero cost   since all predictions collapse into one or two target   phonemes . Also , little progress has been made for   the S2 T subtask even though it is co - trained with   the SSL and T2 T subtasks .   In the last row , the model is trained without pre-   training , i.e. , only the T2 T and S2 T subtasks are   optimized . Compared with the STPT results , there   is about 1.4 WER increase for two L   test sets and 3.4 BLEU decrease for the two ST   directions on average .   6 Conclusion   In this work , we present a method to jointly pre-   train speech and text in one model for speech trans-   lation and recognition under the AED framework.1495   It includes four self - supervised and supervised sub-   tasks from two different input modalities , hence the   proposed method can leverage large amounts of un-   labelled speech data and abundant text data in the   pre - training stage . We conduct detailed analysis   on the interference among different subtasks and   propose two model conﬁgurations for the ASR and   ST pre - training respectively to alleviate the subtask   interference . Our experimental results show STPT   can effectively fuse information within text and   speech training data into one model . We achieves   between 1:7and2:3BLEU improvement over the   state of the art on the MST - C EN - FR and EN - ES   speech translation tasks , and comparable WERs as   wav2vec 2.0 in the L ASR task .   7 Acknowledgments   We want to thank the anonymous reviewers for   their insightful comments and suggestions .   8 Broader Impact   We highlight the potential that this work has pos-   itive impact in the society : augmenting speech   processing tasks with text corpus , and improving   speech related applications . At the same time , this   work may have some negative consequences if the   text data is not handled in a proper way . Before   using the text data to train a speech system , one   should evaluate fairness in the collected data , and   make sure not to train on offensive or any type of   inappropriate data . References14961497A Pre - training data setting   T2 T : For ASR pre - training , the language   model ( LM ) training datasetfor L- ( Panayotov et al . , 2015 ) is used   to build the monolingual BART model . It has   about 800 million words . For ST pre - training , we   take the parallel training corpus from WMT . We   examine our methods on two translation directions   inMST - C : English - Spanish ( EN - ES ) , which   uses WMT13 training corpus , and English - French   ( EN - FR ) , which takes the WMT14 training data .   There are 370 million and 1 billion English words   in the EN - ES and EN - FR parallel training datasets   respectively .   We use “ g2p en ” Python package ( Lee and Kim ,   2018 ) to convert the training text into the corre-   sponding phoneme representation , which is based   on the CMU English dictionary . We further ex-   tend the phoneme set by distinguishing the ﬁrst   phoneme in the word with an additional “ ” mark   appended , which is similar to the notation in the   SentencePiece process . The input phoneme vocab-   ulary size is 134 .   SSL : For both ASR and ST pre - training , 60k   hours of unlabelled English speech data from Libri-   light ( Kahn et al . , 2020 ) is used to build the self-   supervised speech task if not speciﬁcally men-   tioned . We set the maximum utterance duration   to 37.5 seconds and minimum duration to 4 sec-   onds . We randomly sample audio segments with   maximum duration if utterances are longer than the   maximum duration . No voice activity detection is   applied .   S2P : We use the transcribed L dataset   for ASR pre - training . In ST pre - training , the   MST - C training dataset is used , where the corre-   sponding English transcription is used as the train-   ing target labels after it is converted into phoneme   representation . The phoneme level segmentation is   obtained via force - alignment , which is conducted   using HMM / GMM trained from the same speech   data with the Kaldi toolkit ( Povey et al . , 2011 ) .   S2 T : We use the same labelled data in the S2P   subtask for the S2 T subtask , i.e. , L   training data for the ASR pre - training and MST-   Cdata for the ST pre - training . Instead of using   phoneme representation , the target labels are en-   coded with SentencePiece ( Kudo and Richardson ,   2018 ) . For both ASR and ST tasks , the vocabu-   lary is an Unigram model with size 10k and full1498   character coverage on the training text data .   B Optimization setting   The models are optimized with Adam ( Kingma   and Ba , 2014 ) for both pre - training and ﬁne - tuning .   The ﬁnal results are evaluated using an averaged   model from checkpoints of the last 10 epochs .   T2 T subtask pre - training The T2 T model is pre-   trained with learning rate 0.01 using Adam opti-   mization . The maximum tokens per mini - batch is   2048 with 8 V100 GPU cards . The model is up-   dated 400,000 until fully converged .   Pre - training with all subtasks The model then   keeps optimizing with all four subtasks : T2 T , SSL ,   S2P and S2 T , with learning rate 0.001 . The model   is trained using 16 A100 GPU cards with update fre-   quency 12 . The maximum token number per batch   for the T2 T subtask is 2048 while the maximum   sample number is 750,000 ( 46s ) for the speech in-   put in three speech subtasks . The maximum update   number is 800,000 and 200,000 for the ASR pre-   training and the ST pre - training respectively .   Fine - tuning The model is ﬁne - tuned on the down-   stream task with learning rate 0.0003 and 8 V100   GPU cards . The update frequency set to 3 . The   maximum update numbers are dependent on the   amounts of supervised speech data available . We   choose 100,000 for the ASR task with 960 hours   training data and 20,000 for 100 or 10 hours train-   ing data . For the ST task , the maximum update   number is set to 50,000 .   C Gradient similarity of the speech   encoder   Three subtasks : SSL , S2P , and S2 T , share the   speech encoder during the joint pre - training . Sim-   ilar pairwise gradient similarity analysis is con - ducted on these three subtasks at the speech en-   coder , as shown in Figure 3 . The gradient similarity   analysis for the ASR pre - training is presented in   the left subﬁgure while the ST - pretraining is listed   in the right . In both cases , the gradient similarities   for different subtask pairs are small , i.e. , absolute   values of the gradient similarities are all below 0.2 .   It indicates the task interference between different   subtasks are not signiﬁcant.1499