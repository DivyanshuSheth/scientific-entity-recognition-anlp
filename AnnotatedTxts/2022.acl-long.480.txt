  Javier Iranzo - Sánchez and Jorge Civera and Alfons Juan   Machine Learning and Language Processing Group   Valencian Research Institute for Artiﬁcial Intelligence   Universitat Politècnica de València   Camí de Vera s / n , 46022 València , Spain   { jairsan,jorcisai,ajuanci}@vrain.upv.es   Abstract   Simultaneous Machine Translation is the task   of incrementally translating an input sentence   before it is fully available . Currently , simul-   taneous translation is carried out by translat-   ing each sentence independently of the previ-   ously translated text . More generally , Stream-   ing MT can be understood as an extension of   Simultaneous MT to the incremental transla-   tion of a continuous input text stream . In this   work , a state - of - the - art simultaneous sentence-   level MT system is extended to the stream-   ing setup by leveraging the streaming history .   Extensive empirical results are reported on   IWSLT Translation Tasks , showing that lever-   aging the streaming history leads to signiﬁcant   quality gains . In particular , the proposed sys-   tem proves to compare favorably to the best   performing systems .   1 Introduction   Simultaneous Machine Translation ( MT ) is the task   of incrementally translating an input sentence be-   fore it is fully available . Indeed , simultaneous MT   can be naturally understood in the scenario of trans-   lating a text stream as a result of an upstream Au-   tomatic Speech Recognition ( ASR ) process . This   setup deﬁnes a simultaneous Speech Translation   ( ST ) scenario that is gaining momentum due to the   vast number of industry applications that could be   exploited based on this technology , from person - to-   person communication to subtitling of audiovisual   content , just to mention two main applications .   These real - world streaming applications moti-   vate us to move from simultaneous to streaming   MT , understanding streaming MT as the task of   simultaneously translating a potentially unbounded   and unsegmented text stream . Streaming MT poses   two main additional challenges over simultaneous   MT . First , the MT system must be able to lever-   age the streaming history beyond the sentence level   both at training and inference time . Second , thesystem must work under latency constraints over   the entire stream .   With regard to exploiting streaming history , or   more generally sentence context , it is worth men-   tioning the signiﬁcant amount of previous work   in ofﬂine MT at sentence level ( Tiedemann and   Scherrer , 2017 ; Agrawal et al . , 2018 ) , document   level ( Scherrer et al . , 2019 ; Ma et al . , 2020a ; Zheng   et al . , 2020b ; Li et al . , 2020 ; Maruf et al . , 2021 ;   Zhang et al . , 2021 ) , and in related areas such as lan-   guage modelling ( Dai et al . , 2019 ) that has proved   to lead to quality gains . Also , as reported in ( Li   et al . , 2020 ) , more robust ST systems can be trained   by taking advantage of the context across sen-   tence boundaries using a data augmentation strat-   egy similar to the preﬁx training methods proposed   in ( Niehues et al . , 2018 ; Ma et al . , 2019 ) . This   data augmentation strategy was suspected to boost   re - translation performance when compared to con-   ventional simultaneous MT systems ( Arivazhagan   et al . , 2020 ) .   Nonetheless , with the notable exception   of ( Schneider and Waibel , 2020 ) , sentences in   simultaneous MT are still translated indepen-   dently from each other ignoring the streaming   history . ( Schneider and Waibel , 2020 ) proposed   an end - to - end streaming MT model with a Trans-   former architecture based on an Adaptive Com-   putation Time method with a monotonic encoder-   decoder attention . This model successfully uses the   streaming history and a relative attention mecha-   nism inspired by Transformer - XL ( Dai et al . , 2019 ) .   Indeed , this is an MT model that sequentially trans-   lates the input stream without the need for a seg-   mentation model . However , it is hard to interpret   the latency of their streaming MT model because   the authors observe that the current sentence - level   latency measures , Average Proportion ( AP ) ( Cho   and Esipova , 2016 ) , Average Lagging ( AL ) ( Ma   et al . , 2019 ) and Differentiable Average Lagging   ( DAL ) ( Cherry and Foster , 2019 ) do not perform6972well on a streaming setup . This fact is closely   related to the second challenge mentioned above ,   which is that the system must work under latency   constraints over the entire stream . Indeed , current   sentence - level latency measures do not allow us to   appropriately gauge the latency of streaming MT   systems . To this purpose , ( Iranzo - Sánchez et al . ,   2021 ) recently proposed a stream - level adaptation   of the sentence - level latency measures based on   the conventional re - segmentation approach applied   to the ST output in order to evaluate translation   quality ( Matusov et al . , 2005 ) .   In this work , the simultaneous MT model based   on a unidirectional encoder - decoder and train-   ing along multiple wait- kpaths proposed by ( El-   bayad et al . , 2020a ) is evolved into a streaming-   ready simultaneous MT model . To achieve this ,   model training is performed following a sentence-   boundary sliding - window strategy over the paral-   lel stream that exploits the idea of preﬁx training ,   while inference is carried out in a single forward   pass on the source stream that is segmented by a   Direct Segmentation ( DS ) model ( Iranzo - Sánchez   et al . , 2020 ) . In addition , a reﬁnement of the uni-   directional encoder - decoder that takes advantage   of longer context for encoding the initial positions   of the streaming MT process is proposed . This   streaming MT system is thoroughly assessed on   IWSLT translation tasks to show how leveraging   the streaming history provides systematic and sig-   niﬁcant BLEU improvements over the baseline ,   while reported stream - adapted latency measures   are fully consistent and interpretable . Finally , our   system favourably compares in terms of transla-   tion quality and latency to the latest state - of - the - art   simultaneous MT systems ( Ansari et al . , 2020 ) .   This paper is organized as follows . Next section   provides a formal framework for streaming MT to   accommodate streaming history in simultaneous   MT . Section 3 presents the streaming experimental   setup whose results are reported and discussed in   Section 4 . Finally , conclusions and future work are   drawn in Section 5 .   2 Streaming MT   In streaming MT , the source stream Xto be trans-   lated intoYcomes as an unsegmented and un-   bounded sequence of tokens . In this setup , the   decoding process usually takes the greedy decision   of which token appears next at the i - th position ofthe translation being generated   ^Y = argmaxp   y  X;Y   ( 1 )   whereG(i)is a global delay function that tells   us the last position in the source stream that was   available when the i - th target token was output , and   Yis the target vocabulary . However , taking into   account the entire source and target streams can be   prohibitive from a computational viewpoint , so the   generation of the next token can be conditioned to   the lastH(i)tokens of the stream as   ^Y = argmaxp   y  X;Y   :( 2 )   Nevertheless , for practical purposes , the concept   of sentence segmentation is usually introduced to   explicitly indicate a monotonic alignment between   source and target sentences in streaming MT . Let   us consider for this purpose the random variables   aandbfor the source and target segmentation of   the stream , respectively . Variables aandbcan be   understood as two vectors of equal length denoting   that then - th source sentence starts at position a ,   while then - th target sentence does so at position   b.   In the next sections , we reformulate simultane-   ous MT in terms of the more general framework of   streaming MT . This reformulation allows us to con-   sider opportunities for improvement of previous   simultaneous MT models .   2.1 Simultaneous MT with streaming history   In the conventional simultaneous MT setup , the   aforementioned variables aandbare uncovered at   training and inference time , while in streaming MT   aandbare considered hidden variables at infer-   ence time that may be uncovered by a segmentation   model . In fact , in conventional simultaneous MT   the history is limited to the current sentence being   translated , while in streaming MT we could exploit   the fact that the history could potentially span over   all the previous tokens before the current sentence .   To this purpose , the global delay function G(i )   introduced above would replace the sentence - level   delay function g(i)commonly used in simultane-   ous MT . However , it should be noticed that we   could express g(i)asG(i) awithbi <   b. Delay functions are deﬁned as a result of   the policy being applied . This policy decides what   action to take at each timestep , whether to read6973a token from the input or to write a target token .   Policies can be either ﬁxed ( Ma et al . , 2019 ; Dalvi   et al . , 2018 ) depending only on the current timestep ,   or adaptive ( Arivazhagan et al . , 2019 ; Ma et al . ,   2020b ; Zheng et al . , 2020a ) being also conditioned   on the available input source words . Among those   ﬁxed policies , the sentence - level wait - k policy pro-   posed by ( Ma et al . , 2019 ) is widely used in simul-   taneous MT with the simple local delay function   g(i ) = k+i 1 : ( 3 )   This policy initially reads ksource tokens with-   out writing a target token , and then outputs a target   token every time a source token is read . This is   true in the case that the ratio between the source   and target sentence lengths is one . However , in   the general case , a catch - up factor   computed as   the inverse of the source - target length ratio deﬁnes   how many target tokens are written for every read   token , that generalises Eq . 3 as   g(i ) =    k+i 1       : ( 4 )   The wait - kpolicy can be reformulated in stream-   ing MT so that the wait- kbehaviour is carried out   for each sentence as   G(i ) =    k+i b       + a 1 ( 5 )   wherebi < b.   In streaming MT , we could take advantage of the   streaming history by learning the probability distri-   bution stated in Eq . 2 , whenever streaming samples   would be available . However , training such a model   with arbitrarily long streaming samples poses a se-   ries of challenges that need to be addressed . Firstly ,   it would be necessary to carefully deﬁne G(i)and   H(i)functions so that , at each timestep , the avail-   able source and target streams are perfectly aligned .   Given that the source - target length ratio may vary   over the stream , if one uses a wait- kpolicy with a   ﬁxed   , there is a signiﬁcant chance that source and   target are misaligned at some points over the stream .   Secondly , every target token can potentially have   a differentG(i)andH(i ) , so the encoder - decoder   representation and contribution to the loss would   need to be recomputed for each target token at a sig-   niﬁcant computational expense . Lastly , current MT   architectures and training procedures have evolved   conditioned by the availability of sentence - levelparallel corpora for training , so they need to be   adapted to learn from parallel streams .   To tackle the aforementioned challenges in   streaming MT , a compromise practical solution   is to uncover the source and target sentence seg-   mentations . At training time , parallel samples are   extracted by a sentence - boundary sliding window   spanning over several sentences of the stream that   shifts to the right one sentence at a time . In other   words , each sentence pair is concatenated with its   corresponding streaming history that includes pre-   vious sentence pairs simulating long - span preﬁx   training . Doing so , we ensure that source and tar-   get streams are properly aligned at all times , and   training can be efﬁciently carried out by consid-   ering a limited history . The inference process is   performed in a purely streaming fashion in a single   forward pass as deﬁned in Eq . 2 with H(i)being   consistently deﬁned in line with training , so that   the streaming history spans over previous sentences   already translated .   2.2 Partial Bidirectional Encoder   In simultaneous MT , the conventional Transformer-   based bidirectional encoder representation ( of the   l - th layer ) of a source token at any position jis   constrained to the current n - th sentence   e = Enc   e   ( 6 )   whereajG(i ) , while the decoder can only   attend to previous target words and the encoding   of those source words that are available at each   timestep   s = Dec   s;e   : ( 7 )   As a result , the encoder and decoder representa-   tions for positions jandi , respectively , could be   computed taking advantage of subsequent positions   to positionjup to position G(i)at inference time .   However , at training time , this means that this bidi-   rectional encoding - decoding of the source sentence   has to be computed for every timestep , taking up to   jyjtimes longer than the conventional Transformer   model .   To alleviate this problem , ( Elbayad et al . ,   2020a ) proposes a wait- ksimultaneous MT model   based on a modiﬁcation of the Transformer archi-   tecture that uses unidirectional encoders and mul-   tiple values of kat training time . In this way , the6974model is consistent with the limited - input restric-   tion of simultaneous MT at inference time . The   proposed unidirectional encoder can be stated as   e = Enc   e   ; ( 8)   that is more restrictive than that in Eq . 6 , and it   consequently conditions the decoder representation ,   sinceG(i)in Eq . 7 depends on the speciﬁc kvalue   employed at each training step .   As mentioned above , the unidirectional encoder   just requires a single forward pass of the encoder at   training time , and therefore there is no additional   computational cost compared with a conventional   Transformer . However , it does not take into ac-   count all possible input tokens for different values   ofk . Indeed , the encoding of the j - th input to-   ken will not consider those tokens beyond the j - th   position , even if including them into the encod-   ing process does not prevent us from performing a   single forward pass .   A trade - off between the unidirectional and bidi-   rectional encoders is what we have dubbed Partial   Bidirectional Encoder ( PBE ) , which modiﬁes the   unidirectional encoder to allow the ﬁrst k 1source   positions to have access to succeeding tokens ac-   cording to   e = Enc   e   : ( 9 )   PBE allows for a longer context when encoding   the initial positions and is consistent with Eq . 7 . At   training time a single forward pass of the encoder-   decoder is still possible as in the unidirectional   encoder , and therefore no additional training cost   is incurred . At inference time , we fall back to the   bidirectional encoder .   Figure 1 shows a graphical comparison of the   attention mechanism in j= 3 across the bidi-   rectional ( left ) , unidirectional ( center ) and PBE   ( right ) encoders with k= 4 for two consecutive   timestepsi= 1 withG(1 ) = 4 ( top ) andi= 2   withG(2 ) = 5 ( bottom ) . As observed , PBE can   take advantage of additional positions from j+ 1   up tokwith respect to the unidirectional encoder .   In a streaming setup , the bidirectional encoder-   decoder of Eqs . 6 and 7 are not necessarily con-   strained to the current sentence and could exploit a   streaming history of H(i)tokens   e = Enc   e   ( 10 )   s = Dec   s;e   :( 11)Likewise , the proposed PBE with streaming his-   tory states as follows   e = Enc   e   :( 12 )   3 Experimental setup   A series of comparative experiments in terms   of translation quality and latency have been car-   ried out using data from the IWSLT 2020 Eval-   uation Campaign ( Ansari et al . , 2020 ) , for both   German!English and English ! German . For the   streaming condition , our system is tuned on the   2010 dev set , and evaluated on the 2010 test set for   comparison with ( Schneider and Waibel , 2020 ) .   Under this setting , words were lowercased and   punctuation was removed in order to simulate a   basic upstream ASR system . Also , a second non-   streaming setting is used for the English ! German   direction to compare our system with top - of - the-   line sentence - based simultaneous MT systems par-   ticipating in the IWSLT 2020 Simultaneous Trans-   lation Task .   Table 1 summarizes the basic statistics of the   IWSLT corpora used for training the streaming   MT systems . Corpora for which document in-   formation is readily available are processed for   training using the sliding window technique men-   tioned in Section 2.1 . Speciﬁcally , for each   training sentence , we prepend previous sentences ,   which are added one by one until a thresh-   oldhof history tokens is reached . Sentence   boundaries are deﬁned on the presence of spe-   cial tokens ( < DOC>,<CONT>,<BRK>,<SEP > )   as in ( Junczys - Dowmunt , 2019 ) . Byte Pair Encod-   ing ( Sennrich et al . , 2016 ) with 40 K merge opera-   tions is applied to the data after preprocessing .   Our streaming MT system is evaluated in terms   of latency and translation quality with BLEU ( Pap-   ineni et al . , 2002 ) . Traditionally , latency evaluation   in simultaneous MT has been carried out using6975   AP , AL and DAL . However , these measures have   been devised for sentence - level evaluation , where   the latency of every sentence is computed indepen-   dently from each other and as mentioned before ,   they do not perform well on a streaming setup .   Thus , we revert to the stream - based adaptation of   these measures proposed in ( Iranzo - Sánchez et al . ,   2021 ) unless stated otherwise .   Latency measures for a sentence pair ( x;y)are   based on a cost function C(x;y)and a normaliza-   tion termZ(x;y )   L(x;y ) = 1   Z(x;y)XC(x;y ) ( 13 )   where   C(x;y ) = 8   > <   > : g(i ) AP   g(i) AL   g(i) DAL(14 )   and   Z(x;y ) = 8   > <   > : jxjjyj AP   argminiAL   jyj DAL(15 )   Latency measures can be computed in a stream-   ing manner by considering a global delay function   G(i ) , that is mapped into a relative delay so that it   can be compared with the sentence - level oracle de-   lay . For thei - th target position of the n - th sentence ,   the associated relative delay can be obtained from   the global delay function as g(i ) = G(i+b) a .   So , the stream - adapted cost function of the latencymeasures is deﬁned as   C(x;y ) = 8   > <   > : g(i ) AP   g(i) AL   g(i) DAL(16 )   withg(i)deﬁned as   max8   > <   > : g(i ) (   g(jxj ) + i= 1   g(i 1 ) + i>1(17 )   This deﬁnition assumes that the source and tar-   get sentence segmentation of the stream are uncov-   ered , but this is not always the case ( Schneider and   Waibel , 2020 ) or they may not match that of the   reference translations . However , sentence bound-   aries can be obtained by re - segmenting the system   hypothesis following exactly the same procedure   applied to compute translation quality in ST eval-   uation . To this purpose , we use the MWER seg-   menter ( Matusov et al . , 2005 ) to compute sentence   boundaries according to the reference translations .   Our streaming MT models have been trained   following the conventional Transformer BASE   ( German$English streaming MT ) and BIG   ( English!German simultaneous MT ) conﬁgura-   tions ( Vaswani et al . , 2017 ) . As in ( Schneider   and Waibel , 2020 ) , after training is ﬁnished , the   models are ﬁnetuned on the training set of MuST-   C ( Di Gangi et al . , 2019 ) .   The proposed model in Section 2 assumes that   at inference time the source stream has been seg-   mented into sentences . To this purpose , we opt   for the text - based DS model ( Iranzo - Sánchez et al . ,   2020 ) , a sliding - window segmenter that moves over   the source stream taking a split decision at each6976token based on a local - context window that ex-   tends to both past and future tokens . This seg-   menter is streaming - ready and obtains superior   translation quality when compared with other seg-   menters ( Stolcke , 2002 ; Cho et al . , 2017 ) . As the fu-   ture window length of the DS segmenter conditions   the latency of the streaming MT system , this length   was adjusted to ﬁnd a tradeoff between latency and   translation quality . The DS segmenter was trained   on the TED corpus ( Cettolo et al . , 2012 ) .   4 Evaluation   Figure 2 reports the evolution of BLEU scores on   the German - English IWSLT 2010 dev set as a func-   tion of thekvalue in the wait- kpolicy for a range of   streaming history lengths ( h = f0;20;40;60;80 g ) .   We show results for the 3 encoders introduced pre-   viously . History lengths were selected taking into   account that the average sentence length is 20 to-   kens . A history length of zero ( h= 0 ) refers to   the conventional sentence - level simultaneous MT   model . The BLEU scores for the ofﬂine MT sys-   tems with a bidirectional encoder are also reported   using horizontal lines , in order to serve as reference   values . We report ofﬂine results for h= 0and the   best performing history conﬁguration , h= 60 . All   systems used the reference segmentation during   decoding .   As observed , BLEU scores of the simultaneous   MT systems leveraging on the streaming history   ( h>0 ) are systematically and notably higher than   those of conventional sentence - based simultaneous   MT system ( h= 0 ) over the range of wait- kvalues .   Indeed , as the streaming history increases , BLEU   scores also do reaching what it seems the optimal   history length at h= 60 and slightly degrading at   h= 80 . As expected , when replacing the unidirec-   tional encoder by the PBE , BLEU scores improve   as the wait - kvalue increases , since PBE has ad-   ditional access to those tokens from j+ 1up to   k. For instance , for k= 32 andh= 60 , PBE is   0:7BLEU points above the unidirectional encoder .   On the other hand , it can be observed how using   an encoder which is not fully bidirectional during   training , creates a performance gap with respect   to the ofﬂine bidirectional model when carrying   out inference in an ofﬂine manner ( k32 ) . It   can be also observed how the PBE model is better   prepared for this scenario and shows a smaller gap .   It is important to keep in mind that although both   ofﬂine and PBE models behave the same way dur-   ing inference for a large enough k , during training   time the PBE model , trained using the multi- kwith   krandomly sampled for each batch , has been opti-   mized jointly for low , medium and high latencies .   In general , the bidirectional encoder shows poor   performance for simultaneous MT . This can be ex-   plained by the fact that there exists a mismatch   between the training condition ( whole source avail-   able ) and the inference condition ( only a preﬁx of   the source is available for k < 32 ) . These results   are consistent with ( Elbayad et al . , 2020a ) . Keep in   mind that this bidirectional model is different from   the ofﬂine one because it has been subject to the   constraints of Eq . 7 during training . As a result of   the BLEU scores reported in Figure 2 , the stream-   ing MT system with h= 60 and PBE was used in   the rest of the German - English experiments .   Following ( Schneider and Waibel , 2020 ) ’s setup ,   the test set is lowercased and concatenated into a   single stream . In order to measure the latency of   the pipeline deﬁned by the segmenter followed by   MT system , it is necessary to take into account not   only the latency of the MT system but also that   of the segmenter . Thankfully this is straightfor-   ward to do in our pipeline , as a segmenter with a6977   future window of length wmodiﬁes the pipeline   policy so that , at the start of the stream , wREAD   actions are carried out to ﬁll up the future win-   dow . Then , every time the MT system carries out   a READ action , it receives one token from the   segmenter . Thus , the integration of the segmenter   into the pipeline is transparent from a latency view-   point . Figure 3 shows BLEU scores versus stream-   adapted AL and DAL ( sscale = 0.85 ) ﬁgures re-   ported with segmenters of future window length   w = f0;1;2;3;4gfor a streaming evaluation on   the IWSLT 2010 test set . Points over each curve   correspond to k = f1;2;4;8;16gvalues of the   wait - kpolicy used at inference time . Results for a   w= 0oracle are also shown as an upper - bound .   As shown , stream - adapted AL and DAL ﬁgures   achieved by our streaming MT system are reason-   able , lagging 2 - 10 tokens behind the speaker for   nearly maximum BLEU scores with a best BLEU   score of 29.5 points . The same happens with AP   ﬁgures ranging from 0.6 for w= 0to 1.3 forw= 4 .   These ﬁgures highlight the advantages of tying to-   gether our translation policy with the sentence seg-   mentation provided by the DS model . Every timethe DS model emits an end - of - sentence event , the   MT model is forced to catch - up and translate the   entire input . In this way , the MT model never strays   too far from the speaker , even if the source - target   length ratio differs from the   deﬁned at inference   time . See Appendix A for streaming translation re-   sults in the reverse direction ( English ! German ) .   Next , we compare our proposed streaming MT   ( STR - MT ) model with the = 0:3ACT sys-   tem ( Schneider and Waibel , 2020 ) in terms of   BLEU score and stream - adapted latency measures   on Table 2 . Stream - level AL and DAL indicate that   the ACT models lags around 100 tokens behind   the speaker . Although both MT systems achieve   similar translation quality levels , they do so at sig-   niﬁcantly different latencies , since the ACT model   Model BLEU AP AL DAL   ACT 30.3 10.3 100.1 101.8   STR - MT 29.5 1.2 11.2 17.86978lacks a catch - up mechanism to synchronize and   keep the pace of the speaker .   The STR - MT model is now compared on the   English - German IWSLT 2020 simultaneous text-   to - text track ( Ansari et al . , 2020 ) with other par-   ticipants : RWTH ( Bahar et al . , 2020 ) , KIT ( Pham   et al . , 2020 ) and ON - TRAC ( Elbayad et al . , 2020b ) .   This comparison is carried out in order to assess   whether the proposed streaming MT system is com-   petitive with highly optimized systems for a simul-   taneous MT task . Given that the test set of this track   remains blind , we use the results reported on the   MuST - C corpus as a reference . In order to evaluate   all systems under the same conditions , the refer-   ence segmentation of the MuST - C corpus is used in-   stead of the DS model . Additionally , given that all   other participants translate each sentence indepen-   dently , the conventional sentence - level AL latency   measure is reported . Figure 4 shows the compari-   son of BLEU scores versus AL measured in terms   of detokenized tokens . As deﬁned in the IWSLT   text - to - text track , three AL regimes , low ( AL3 ) ,   medium ( 3 < AL6 ) and high ( 6 < AL15 )   were considered .   ON - TRAC and our streaming MT system exhibit   a similar progression , which is to be expected given   that they are both based on the multi- kapproach .   However , our system consistently outperforms the   ON - TRAC system by 1 - 2 BLEU . This conﬁrms   the importance of utilizing streaming history in   order to signiﬁcantly improve results , and how the   proposed PBE model can take better advantage of   the history .   RWTH and KIT systems are closer in translation   quality to our proposal than ON - TRAC , for AL be-   tween 5 and 7 . However , these systems do not show   a ﬂexible latency policy and are not comparable   to our system at other regimes . Indeed , for that to   be possible , these systems need to be re - trained , in   contrast to our system in which latency is adjusted   at inference time .   5 Conclusions   In this work , a formalization of streaming MT as a   generalization of simultaneous MT has been pro-   posed in order to deﬁne a theoretical framework in   which our two contributions have been made . On   the one hand , we successfully leverage streaming   history across sentence boundaries for a simultane-   ous MT system based on multiple wait - k paths that   allows our system to greatly improve the results of   the sentence - level baseline . On the other hand , our   PBE is able to take into account longer context in-   formation than its unidirectional counterpart , while   keeping the same training efﬁciency .   Our proposed MT system has been evaluated   under a realistic streaming setting being able to   reach similar translation quality than a state - of - the-   art segmentation - free streaming MT system at a   fraction of its latency . Additionally , our system   has been shown to be competitive when compared   with state - of - the - art simultaneous MT systems op-   timized for sentence - level translation , obtaining   excellent results using a single model across a wide   range of latency levels , thanks to its ﬂexible infer-   ence policy .   In terms of future work , additional training and   inference procedures that take advantage of the   streaming history in streaming MT are still open for   research . One important avenue of improvement   is to devise more robust training methods , so that   simultaneous models can perform as well as their   ofﬂine counterparts when carrying out inference at6979higher latencies . The segmentation model , though   proved useful in a streaming setup , adds complexity   and can greatly affect translation quality . Thus , the   development of segmentation - free streaming MT   models is another interesting research topic .   Acknowledgements   The research leading to these results has re-   ceived funding from the European Union ’s Hori-   zon 2020 research and innovation programme   under grant agreements no . 761758 ( X5Gon )   and 952215 ( TAILOR ) , and Erasmus+ Educa-   tion programme under grant agreement no . 20-   226 - 093604 - SCH ( EXPERT ) ; the Government of   Spain ’s grant RTI2018 - 094879 - B - I00 ( Multisub )   funded by MCIN / AEI/10.13039/501100011033 &   “ ERDF A way of making Europe ” , and FPU schol-   arships FPU18/04135 ; and the Generalitat Valen-   ciana ’s research project Classroom Activity Recog-   nition ( ref . PROMETEO/2019/111 ) . The authors   gratefully acknowledge the computer resources at   Artemisa , funded by the European Union ERDF   and Comunitat Valenciana as well as the technical   support provided by the Instituto de Física Corpus-   cular , IFIC ( CSIC - UV ) .   References6980   A Extended Streaming Translation   Results   Figure 5 shows a close - up of Figure 2 , which con-   tains results for the German - English IWSLT 2010   dev set . We can observe how the PBE models   obtain consistent quality improvements over their   unidirectional counterparts .   Apart from the previously reported German !   English streaming MT results , we have also con-   ducted experiments in the reverse direction , En-   glish!German . These are shown in Figure 6 .   The results show a similar trend to previous ex-   periments , with the addition of streaming history   allowing our systems to obtain signiﬁcant improve-   ments over the sentence - based baseline . Unlike the   previous case , the optimum history size in this case   ish= 40 instead ofh= 60 .   In order to enable streaming translation , the best   performing h= 40 systems has been combined   with a German DS system . Similarly to previous   experiments , we have conducted tests using dif-   ferent values of wandkin order to balance the6981   latency - quality trade - off , shown in Figure 7 . Un-   der the streaming condition , the wait- kpolicy and   DS model allow the model to follow closely the   speaker while achieving good quality , with a la-   tency that can be easily adjusted between 4 and 15   tokens depending on the requirements of the task .   There are diminishing returns when increasing the   latency above 6 - 7 tokens , as only marginal gains in   quality are obtained .   B Efﬁciency of the proposed models   During training of the unidirectional and PBE en-   coders , the constraints imposed by Eqs . 8 and 9   are efﬁciently implemented by full self - attention ,   as in the bidirectional encoder , followed by an at-   tention mask , for each token to only attend those   tokens fulﬁlling the constraints . The attention mask   sets the weights of the other tokens to  1 before   application of the self - attention softmax . This is   exactly the same mechanism used in the standard   Transformer decoder to prevent the auto - regressive   decoder from accessing future information .   This means that the three encoder types have an   identical computational behavior . We are not aware   of alternative GPU - based acceleration techniques   to speed up the training of the unidirectional en-   coder . If so , this could be also applicable to the   training of the standard Transformer decoder .   During inference time , however , the unidirec-   tional encoder has some advantages . Given that   the unidirectional encoder is incremental , mean-   ing that the encodings of old tokens do not change   when a new token becomes available , the process   can be sped up by only computing the encoding   of the newly available token . Although encoder   self - attention still needs to be computed , a single   vector is used as the query instead of the full matrix .   Table 3 shows inference statistics for the different   components of the En ! De Transformer Big with   h=60 . Two setups have been tested : CPU - only in-   ference , and GPU inference . Results were obtained   on an Intel i9 - 7920X machine with an NVIDIA   GTX 2080Ti .   The unidirectional encoder is four times faster   than the bidirectional encoder when run on a CPU .   However , both encoders perform the same when   run on a GPU . For the streaming MT scenario con-   sidered in this work , no latency reduction is gained6982   Component CPU GPU   Unidir . Encoder 0.034s 0.002s   Bidir . Encoder 0.138s 0.002s   Decoder 0.242s 0.004s   by not re - encoding previous tokens due to the GPU   paralellization capability . When run on a GPU , the   proposed model works seamlessly under real - time   constraints .   C MT System conﬁguration   The multi - ksystems have been trained with the ofﬁ-   cial implementation ( https://github.com/   elbayadm / attn2d ) . Models are trained for   0.5 M steps on a machine with 4 2080Ti GPUs .   Total training time was 40h for BASE models , and   60h for BIG models . The following command was   used to train them:6983   with   ARCH = waitk_transformer_base ;   TOK=4000   for the BASE conﬁguration , and   ARCH = waitk_transformer_big ;   TOK=2000   for the BIG one .   For ﬁnetuning , we change to the following :   --lr - scheduler fixed \   --lr 4.47169e-05 \   For the streaming translation scenario , the data is   lowercased and all punctuation signs are removed .   For the simultaneous scenario ( IWSLT 2020 simul-   taneous text- to - text ) , it is truecased and tokenized   using Moses . We apply language identiﬁcation to   the training data using langid ( Lui and Baldwin ,   2012 ) and discard those sentences that have been   tagged with the wrong language . SentencePiece   ( Kudo and Richardson , 2018 ) is used to learn the   BPE units , and we use whitespace as a sufﬁx in   order to know when an entire target word has been   written during decoding .   In order to obtain samples that can be used for   training streaming MT models , a sliding window   that moves over whole sentences is used to extract   consistent source - target samples . Figure 8 shows   an example of corpus construction using h= 5 .   The generated streaming data is upsampled to keep   a 1 - to-3 ratio with the regular sentence - level data .   D Segmenter System conﬁguration   The Direct Segmentation system has been   trained with the ofﬁcial implementation   ( https://github.com/jairsan/   Speech_Translation_Segmenter ) .   The following command was used to train the   segmenter system :   with the following conﬁgurations :   ( len=11 ; window=0 )   ( len=12 ; window=1 )   ( len=13 ; window=2 )   ( len=14 , window=3 )   ( len=15 , window=4)6984Sentece pair Source Target   1 xxyy   2 xxxyy   3 xxxyyy   4 xxyy   Sentence pair Source   1 < DOC > xx < BRK >   2 < DOC > xx < SEP > xxx < BRK >   3 < DOC > xx < SEP > xxx < SEP > xxx < BRK >   4 < CONT > xxx < SEP > xx < END >   Sentence pair Target   1 < DOC > yy < BRK >   2 < DOC > yy < SEP > yy < BRK >   3 < DOC > yy < SEP > yy < SEP > yyy < BRK >   4 < CONT > yyy < SEP > yy < END>6985