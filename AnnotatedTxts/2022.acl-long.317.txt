  Kuan - Hao HuangI - Hung HsuPremkumar Natarajan   Kai - Wei ChangNanyun PengComputer Science Department , University of California , Los AngelesInformation Science Institute , University of Southern California   { khhuang , kwchang , violetpeng}@cs.ucla.edu   { ihunghsu , pnataraj}@isi.edu   Abstract   We present a study on leveraging multilin-   gual pre - trained generative language models   for zero - shot cross - lingual event argument ex-   traction ( EAE ) . By formulating EAE as a lan-   guage generation task , our method effectively   encodes event structures and captures the de-   pendencies between arguments . We design   language - agnostic templates to represent the   event argument structures , which are compat-   ible with any language , hence facilitating the   cross - lingual transfer . Our proposed model   ﬁnetunes multilingual pre - trained generative   language models to generate sentences that ﬁll   in the language - agnostic template with argu-   ments extracted from the input passage . The   model is trained on source languages and   is then directly applied to target languages   for event argument extraction . Experiments   demonstrate that the proposed model outper-   forms the current state - of - the - art models on   zero - shot cross - lingual EAE . Comprehensive   studies and error analyses are presented to bet-   ter understand the advantages and the current   limitations of using generative language mod-   els for zero - shot cross - lingual transfer EAE .   1 Introduction   Event argument extraction ( EAE ) aims to recog-   nize the entities serving as event arguments and   identify their corresponding roles . As illustrated   by the English example in Figure 1 , given a trig-   ger word “ destroyed ” for a Conﬂict : Attack event ,   an event argument extractor is expected to iden-   tify “ commando ” , “ Iraq ” , and “ post ” as the   event arguments and predict their roles as “ At-   tacker ” , “ Place ” , and “ Target ” , respectively .   Zero - shot cross - lingual EAE has attracted con-   siderable attention since it eliminates the require-   ment of labeled data for constructing EAE models   in low - resource languages ( Subburathinam et al . ,   2019 ; Ahmad et al . , 2021 ; Nguyen and Nguyen , Figure 1 : An illustration of cross - lingual event ar-   gument extraction . Given sentences in arbitrary lan-   guages and their event triggers ( destroyed and起义 ) ,   the model needs to identify arguments ( commando ,   Iraq andpost v.s.军队 , and反对派 ) and their cor-   responding roles ( Attacker , Target , and Place ) .   2021 ) . In this setting , the model is trained on the ex-   amples in the source languages and directly tested   on the instances in the target languages .   Recently , generation - based modelshave shown   strong performances on monolingual structured pre-   diction tasks ( Yan et al . , 2021 ; Huang et al . , 2021b ;   Paolini et al . , 2021 ) , including EAE ( Li et al . , 2021 ;   Hsu et al . , 2021 ) . These works ﬁne - tune pre - trained   generative language models to generate outputs fol-   lowing designed templates such that the ﬁnal pre-   dictions can be easily decoded from the outputs .   Compared to the traditional classiﬁcation - based   models ( Wang et al . , 2019 ; Wadden et al . , 2019 ;   Lin et al . , 2020 ) , they better capture the structures   and dependencies between entities , as the templates   provide additional declarative information .   Despite the successes , the designs of templates in   prior works are language - dependent , which makes   it hard to be extended to the zero - shot cross - lingual   transfer setting ( Subburathinam et al . , 2019 ; Ah-   mad et al . , 2021 ) . Naively applying such mod-   els trained on the source languages to the tar-   get languages usually generates code - switching   outputs , yielding poor performance for zero - shot4633cross - lingual transfer , as we will empirically   show in Section 5.4 . How to design language-   agnostic generation - based models for zero - shot   cross - lingual structured prediction problems is still   an open question .   In this work , we present a study that leverage   multilingual pre - trained generative models for zero-   shot cross - lingual event argument extraction and   propose X - G ( Cross -lingual Generative Event   Argument extracto R ) . Given an input passage and   a carefully designed prompt that contains an event   trigger and the corresponding language - agnostic   template , X - G is trained to generate a sen-   tence that ﬁlls in a language - agnostic template   with arguments . X - G inherits the strength of   generation - based models that captures event struc-   tures and the dependencies between entities better   than classiﬁcation - based models . Moreover , the   pre - trained decoder inherently identiﬁes named en-   tities as candidates for event arguments and does   not need an additional named entity recognition   module . The language - agnostic templates prevents   the model from overﬁtting to the source language ’s   vocabulary and facilitates cross - lingual transfer .   We conduct experiments on two multilingual   EAE datasets : ACE-2005 ( Doddington et al . , 2004 )   and ERE ( Song et al . , 2015 ) . The results demon-   strate that X - G outperforms the state - of - the-   art zero - shot cross - lingual EAE models . We fur-   ther perform ablation studies to justify our de-   sign and present comprehensive error analyses   to understand the limitations of using multilin-   gual generation - based models for zero - shot cross-   lingual transfer . Our code is available at https :   //github.com / PlusLabNLP / X - Gear   2 Related Work   Zero - shot cross - lingual structured prediction .   Zero - shot cross - lingual learning is an emerging   research topic as it eliminates the requirement of   labeled data for training models in low - resource   languages ( Ruder et al . , 2021 ; Huang et al . , 2021a ) .   Various structured prediction tasks have been stud-   ied , including named entity recognition ( Pan et al . ,   2017 ; Huang et al . , 2019 ; Hu et al . , 2020 ) , de-   pendency parsing ( Ahmad et al . , 2019b , a ; Menget al . , 2019 ) , relation extraction ( Zou et al . , 2018 ;   Ni and Florian , 2019 ) , and event argument ex-   traction ( Subburathinam et al . , 2019 ; Nguyen and   Nguyen , 2021 ; Fincke et al . , 2021 ) . Most of them   areclassiﬁcation - based models that build classi-   ﬁers on top of a multilingual pre - trained masked   language models . To further deal with the discrep-   ancy between languages , some of them require ad-   ditional information , such as bilingual dictionaries   ( Liu et al . , 2019 ; Ni and Florian , 2019 ) , transla-   tion pairs ( Zou et al . , 2018 ) , and dependency parse   trees ( Subburathinam et al . , 2019 ; Ahmad et al . ,   2021 ; Nguyen and Nguyen , 2021 ) . However , as   pointed out by previous literature ( Li et al . , 2021 ;   Hsu et al . , 2021 ) , classiﬁcation - based models are   less powerful to model dependencies between enti-   ties compared to generation - based models .   Generation - based structured prediction . Sev-   eral works have demonstrated the great success   of generation - based models on monolingual struc-   tured prediction tasks , including named entity   recognition ( Yan et al . , 2021 ) , relation extraction   ( Huang et al . , 2021b ; Paolini et al . , 2021 ) , and   event extraction ( Du et al . , 2021 ; Li et al . , 2021 ;   Hsu et al . , 2021 ; Lu et al . , 2021 ) . Yet , as mentioned   in Section 1 , their designed generating targets are   language - dependent . Accordingly , directly apply-   ing their methods to the zero - shot cross - lingual   setting would result in less - preferred performance .   Prompting methods . There are growing inter-   ests recently to incorporate prompts on pre - trained   language models in order to guide the models ’   behavior or elicit knowledge ( Peng et al . , 2019 ;   Sheng et al . , 2020 ; Shin et al . , 2020 ; Schick and   Schütze , 2021 ; Qin and Eisner , 2021 ; Scao and   Rush , 2021 ) . Following the taxonomy in ( Liu et al . ,   2021 ) , these methods can be classiﬁed depending   on whether the language models ’ parameters are   tuned and on whether trainable prompts are intro-   duced . Our method belongs to the category that   ﬁxes the prompts and tunes the language models ’   parameters . Despite the ﬂourish of the research   in prompting methods , there is only limited atten-   tion being put on multilingual tasks ( Winata et al . ,   2021 ) .   3 Zero - Shot Cross - Lingual Event   Argument Extraction   We focus on zero - shot cross - lingual EAE . Given   an input passage and an event trigger , an EAE4634   model identiﬁes arguments and their correspond-   ing roles . More speciﬁcally , as illustrated by the   training examples in Figure 2 , given an input pas-   sage xand an event trigger t(killed ) belonging   to an event type e(Life : Die ) , an EAE model   predicts a list of arguments a= [ a;a;:::;a ]   ( coalition , civilians , woman , missile , houses )   and their corresponding roles r= [ r;r;::;r ]   ( Agent , Victim , Victim , Instrument , Place ) . In   the zero - shot cross - lingual setting , the training   setX = f(x;t;e;a;r)gbelongs to the   source languages while the testing set X=   f(x;t;e;a;r)gare in the target languages .   Similar to monolingual EAE , zero - shot cross-   lingual EAE models are expected to capture the   dependencies between arguments and make struc-   tured predictions . However , unlike monolingual   EAE , zero - shot cross - lingual EAE models need   to handle the differences ( e.g. , grammar , word or-   der ) between languages and learn to transfer the   knowledge from the source languages to the target   languages .   4 Proposed Method : X - G   We formulate zero - shot cross - lingual EAE as a   language generation task and propose X - G ,   aCross -lingual Generative Event Argument ex-   tracto Rthat is illustrated in Figure 2 . There are   two challenges raised by this formulation : ( 1 ) The   input language may vary during training and test-   ing ; ( 2 ) The generated output strings need to be   easily parsed into ﬁnal predictions . Therefore , the   output strings have to reﬂect the change of the in-   put language accordingly while remaining well - structured .   We address these challenges by designing   language - agnostic templates . Speciﬁcally , given   an input passage xand a designed prompt that   contains the given trigger t , its event type e , and   alanguage - agnostic template , X - G learns to   generate an output string that ﬁlls in the language-   agnostic template with information extracted from   input passage . The language - agnostic template is   designed in a structured way such that parsing the   ﬁnal argument predictions aand role predictions r   from the generated output is trivial . Moreover ,   since the template is language - agnostic , it facil-   itates cross - lingual transfer .   X - G ﬁne - tunes multilingual pre - trained gen-   erative models , such as mBART-50 ( Tang et al . ,   2020 ) or mT5 ( Xue et al . , 2021 ) , and augments   them with a copy mechanism to better adapt to   input language changes . We present its details as   follows , including the language - agnostic templates ,   the target output string , the input format , and the   training details .   4.1 Language - Agnostic Template   We create one language - agnostic template Tfor   each event type e , in which we list all possible as-   sociated rolesand form a unique HTML - tag - style   template for that event type e. For example , in   Figure 2 , the Life : Die event is associated with four   roles : Agent , Victim , Instrument , and Place . Thus ,   the template for Life : Die events is designed as:4635   For ease of understanding , we use English words   to present the template . However , these tokens   ( [ None ] , < Agent > , < /Agent > , < Victim > , etc . ) are   encoded as special tokensthat the pre - trained mod-   els have never seen and thus their representations   need to be learned from scratch . Since these special   tokens are not associated with any language and are   not pre - trained , they are considered as language-   agnostic .   4.2 Target Output String   X - G learns to generate target output strings   that follow the form of language - agnostic tem-   plates . To compose the target output string for   training , given an instance ( x;t;e;a;r ) , we ﬁrst   pick out the language - agnostic template Tfor   the event type eand then replace all “ [ None ] ”   inTwith the corresponding arguments in a   according to their roles r. If there are multiple   arguments for one role , we concatenate them   with a special token “ [ and ] ” . For instance , the   training example in Figure 2 has two arguments   ( civilians and woman ) for the Victim role , and   the corresponding part of the output string would be   If there are no corresponding arguments for one   role , we keep “ [ None ] ” inT. By applying this   rule , the full output string for the training example   in Figure 2 becomes   Since the output string is in the HTML - tag style ,   we can easily decode the argument and role predic-   tions from the generated output string via a simple   rule - based algorithm .   4.3 Input Format   As we mentioned previously , the key for the genera-   tive formulation for zero - shot cross - lingual EAE is   to guide the model to generate output strings in the   desired format . To facilitate this behavior , we feed   the input passage xas well as a prompt toX - G ,   as shown by Figure 2 . The prompt contains allvaluable information for the model to make predic-   tions , including a trigger tand a language - agnostic   templateT. Notice that we do not explicitly in-   clude the event type ein the prompt because the   templateTimplicitly contains this information .   In Section 6.1 , we will show the experiments on   explicitly adding event type eto the prompt and   discuss its inﬂuence on the cross - lingual transfer .   4.4 Training   To enable X - G to generate sentences in differ-   ent languages , we resort multilingual pre - trained   generative model to be our base model , which mod-   els the conditional probability of generating a new   token given the previous generated tokens and the   input context to the encoder c , i.e ,   P(xjc ) = YP(xjx;c ) ;   wherexis the output of the decoder at step i.   Copy mechanism . Although the multilingual   pre - trained generative models can generate se-   quences in many languages , solely relying on   them may result in generating hallucinating argu-   ments ( Li et al . , 2021 ) . Since most of the tokens   in the target output string appear in the input se-   quence , we augment the multilingual pre - trained   generative models with a copy mechanism to help   X - G better adapt to the cross - lingual scenario .   Speciﬁcally , we follow See et al . ( 2017 ) to decide   the conditional probability of generating a token t   as a weighted sum of the vocabulary distribution   computed by multilingual pre - trained generative   modelPand copy distribution P   wherew2[0;1]is the copy probability com-   puted by passing the decoder hidden state at time   stepito a linear layer . As for P , it refers to the   probability over input tokens weighted by the cross-   attention that the last decoder layer computed ( at   time stepi ) . Our model is then trained end - to - end   with the following loss :   L= logXP(xjx;c ):   5 Experiments   5.1 Datasets   We consider two commonly used event extraction   datasets : ACE-2005 and ERE . We consider En-4636glish , Arabic , and Chinese annotations for ACE-   2005 ( Doddington et al . , 2004 ) and follow the pre-   processing in Wadden et al . ( 2019 ) to keep 33 event   types and 22 argument roles . ERE ( Song et al . ,   2015 ) is created by the Deep Exploration and Fil-   tering of Test program . We consider its English and   Spanish annotations and follow the preprocessing   in Lin et al . ( 2020 ) to keep 38 event types and 21 ar-   gument roles . Detailed statistics and preprocessing   steps about the two datasets are in Appendix A.   Notice that prior works working on the zero - shot   cross - lingual transfer of event arguments mostly   focus on event argument role labeling ( Subburathi-   nam et al . , 2019 ; Ahmad et al . , 2021 ) , where they   assume ground truth entities are provided during   both training and testing . In their experimental data   splits , events in a sentence can be scattered in all   training , development , and test split since they treat   each event - entity pair as a different instance . In   this work , we consider event argument extraction   ( Wang et al . , 2019 ; Wadden et al . , 2019 ; Lin et al . ,   2020 ) , which is a more realistic setting .   5.2 Evaluation Metric   We follow previous work ( Lin et al . , 2020 ; Ahmad   et al . , 2021 ) and consider the argument classiﬁca-   tion F1 score to measure the performance of mod-   els . An argument - role pair is counted as correct if   both the argument offsets and the role type match   the ground truth . Given the ground truth arguments   a , ground truth roles r , predicted arguments ~a , and   predicted roles ~r , the argument classiﬁcation F1   score is deﬁned as the F1 score between the set   f(a;r)gand the setf(~a;~r)g . For every model ,   we experiment with three different random seeds   and report the average results .   5.3 Compared Models   We compare the following models and their imple-   mentation details are listed in Appendix B.   •OneIE ( Lin et al . , 2020 ) , the state - of - the - art for   monolingual event extraction , is a classiﬁcation-   based model trained with multitasking , includ-   ing entity extraction , relation extraction , event   extraction , and event argument extraction . We   simply replace its pre - trained embedding with   XLM - RoBERTa - large ( Conneau et al . , 2020 ) to   ﬁt the zero - shot cross - lingual setting . Note that   the multi - task learning makes OneIE require ad-   ditional annotations , such as named entity anno-   tations and relation annotations.•CL - GCN ( Subburathinam et al . , 2019 ) is a   classiﬁcation - based model for cross - lingual   event argument role labeling ( EARL ) . It con-   siders dependency parsing annotations to bridge   different languages and use GCN layers ( Kipf   and Welling , 2017 ) to encode the parsing infor-   mation . We follow the implementation of previ-   ous work ( Ahmad et al . , 2021 ) and add two GCN   layers on top of XLM - RoBERTa - large . Since   CL - GCN focuses on EARL tasks , which assume   the ground truth entities are available during test-   ing , we add one name entity recognition module   jointly trained with CL - GCN .   •GATE ( Ahmad et al . , 2021 ) , the state - of - the-   art model for zero - shot cross - lingual EARL , is   a classiﬁcation - based model which considers   dependency parsing annotations as well . Unlike   CL - GCN , it uses a Transformer layer ( Vaswani   et al . , 2017 ) with modiﬁed attention to encode   the parsing information . We follow the original   implementation and add two GATE layers on top   of pre - trained multilingual language models .   Similar to CL - GCN , we add one name entity   recognition module jointly trained with GATE .   •TANL ( Paolini et al . , 2021 ) is a generation-   based model for monolingual EAE . Their   predicted target is a sentence that embeds   labels into the input passage , such as [ Two   soldiers|target ] were attacked ,   which indicates that “ Two soldiers ” is a   “ target ” argument . To adapt TANL to zero - shot   cross - lingual EAE , we change its pre - trained   generative model from T5 ( Raffel et al . , 2020 )   to mT5 - base ( Xue et al . , 2021 ) .   •X - G is our proposed model . We consider   three different pre - trained generative language   models : mBART-50 - large ( Tang et al . , 2020 ) ,   mT5 - base , and mT5 - large ( Xue et al . , 2021 ) .   5.4 Results   Table 1 and Table 2 list the results on ACE-2005   and ERE , respectively , with all combinations of   source languages and target languages . Note that   all the models have similar numbers of parameters4637   except for X - G with mT5 - large .   Comparison to prior generative models . We   ﬁrst observe that TANL has poor performance   when transferring to different languages . The rea-   son is that its language - dependent template makes   TANL easily generate code - switching outputs ,   which is a case that pre - trained generative model   rarely seen , leading to poor performance . In con-   trast , X - G considers the language - agnostic   templates and achieves better performance for zero-   shot cross - lingual transfer .   Comparison to classiﬁcation models . X - G   with mT5 - base outperforms OneIE , CL - GCN , and   GATE on almost all the combinations of the source   language and the target language . This suggests   that our proposed method is indeed a promising   approach for zero - shot cross - lingual EAE .   It is worth noting that OneIE , CL - GCN , and   GATE require an additional pipeline named entity   recognition module to make predictions . Moreover ,   CL - GCN and GATE need additional dependencyparsing annotations to align the representations of   different languages . On the contrary , X - G is   able to leverage the learned knowledge from the   pre - trained generative models , and therefore no   additional modules or annotations are needed .   Comparison to different pre - trained generative   language models . Interestingly , using mT5 - base   is more effective than using mBART-50 - large for   X - G , although they have a similar amount of   parameters . We conjecture that the use of special   tokens leads to this difference . mBART-50 has   different begin - of - sequence ( BOS ) tokens for dif-   ferent languages . During generation , we have to   specify which BOS token we would like to use as   the start token . We guess that this language - speciﬁc   BOS token makes mBART-50 harder to transfer the   knowledge from the source language to the target   language . Unlike mBART-50 , mT5 does not have   such language - speciﬁc BOS tokens . During gen-   eration , mT5 uses the padding token as the start   token to generate a sequence . This design is more   general and beneﬁt zero - shot cross - lingual transfer .   Larger pre - trained models are better . Finally ,   we demonstrate that the performance of X - G   can be further boosted with a larger pre - trained   generative language model . As shown by Table 1   and Table2 , X - G with mT5 - large achieves the   best scores on most of the cases .   6 Analysis   6.1 Ablation Studies   Copy mechanism . We ﬁrst study the effect of   the copy mechanism . Table 3 lists the performance   ofX - G with and without copy mechanism . It   shows improvements in adding a copy mechanism4638   when using mT5 - large and mT - base . However , in-   terestingly , adding a copy mechanism is not ef-   fective for mBART-50 . We conjecture that this is   because the pre - trained objective of mBART-50 is   denoising autoencoding ( Liu et al . , 2020 ) , and it   has already learned to copy tokens from the input .   Therefore , adding a copy mechanism is less useful .   In contrast , the pre - trained objective of mT5 is to   only generate tokens been masked out , resulting in   lacking the ability to copy input . Thus , the copy   mechanism becomes beneﬁcial for mT5 .   Including event type in prompts . In Section 4 ,   we mentioned that the designed prompt for X-   G consists of only the input sentence and the   language - agnostic template . In this section , we   discuss whether explicitly including the event type   information in the prompt is helpful . We consider   three ways to include the event type information :   •English tokens . We put the English version   of the event type in the prompt even if we are   training or testing on non - English languages , for   example , using Attack for the event type Attack .   •Translated tokens . For each event type , we   prepare the translated version of that event type   token . For example , both Attack and攻击rep-   resents the Attack event type . During training or   testing , we decide the used token(s ) according   to the language of the input passage . Since all   the event types are written in English in ACE-   2005 and ERE , we use an off - the - self machine   translation tool to perform the translation .   •Special tokens . We create a special token for   every event type and let the model learn the rep-   resentations of the special tokens from scratch .   For instance , we use < -attack- > to represent   theAttack event type .   Table 4 shows the results . In most cases , includ-   ing event type information in the prompt decreases   the performance . One reason is that one word in   a language can be mapped to several words in an-   other language . For example , the Lifeevent type is   related to Marry , Divorce , Born , and Diefour sub-   event types . In English , we can use just one word   Lifeto cover all four sub - event types . However , In   Chinese , when talking about Marry andDivorce ,   Lifeshould be translated to “ 生活 ” ; when talking   about Born andDie , Life should be translated to   “ 生命 ” . This mismatch may cause the performance   drop when considering event types in prompts . We   leave how to efﬁciently use event type information   in the cross - lingual setting as future work .   Inﬂuence of role order in templates . The or-   der of roles in the designed language - agnostic   templates can potentially inﬂuence performance .   When designing the templates , we intentionally   make the order of roles close to the order in natural   sentences . To study the effect of different orders ,   we train X - G with templates with different   random orders and report the results in Table 5 .   X - G with random orders still achieve good   performance but slightly worse than the original or-   der . It suggests that X - G is not very sensitive   to different templates while providing appropriate   order of roles can lead to a small improvement .   Using English tokens instead of special tokens   for roles in templates . In Section 4 , we men-   tioned that we use language - agnostic templates4639   to facilitate the cross - lingual transfer . To further   validate the effectiveness of the language - agnostic   template . We conduct experiments using English   tokens as the templates . Speciﬁcally , we set format   to be the template for Life : Die events . Hence , for   non - English instances , the targeted output string   is a code - switching sequence . Table 6 lists the   results . We can observe that applying language-   agnostic templates bring X - G 2.3 F1 scores   improvements in average .   6.2 Error Analysis   We perform error analysis on X - G ( mT5 - base )   when transferring from Arabic to English and trans-   ferring from Chinese to English . For each case , we   sample 30 failed examples and present the distribu-   tion of various error types in Figure 3 .   Errors on both monolingual and cross - lingual   models . We compare the predicted results from   X - G(ar)en ) with X - G(en)en ) , or   from X - G(zh)en ) with X - G(en)en ) .   If their predictions are similar and both of themare wrong when compared to the gold output , we   classify the error into this category . To overcome   the errors in this category , the potential solution is   to improve monolingual models for EAE tasks .   Over - generating . Errors in this category happen   more often in X - G(ar)en ) . It is likely be-   cause the entities in Arabic are usually much longer   than that in English when measuring by the number   of sub - words . Based on our statistics , the average   entity span length is 2.85 for Arabic and is 2.00   for English ( length of sub - words ) . This leads to   the natural for our X - G(ar)en ) to overly   generate some tokens even though they have cap-   tured the correct concept . An example is that the   model predicts “ The EU foreign ministers ” , while   the ground truth is “ ministers ” .   Label disagreement on different language splits .   The annotations for the ACE dataset in different   language split contain some ambiguity . For exam-   ple , given sentence “ He now also advocates letting   in U.S. troops for a war against Iraq even though   it is a fellow Muslim state . ” and the queried trigger   “ war ” , the annotations in English tends to label Iraq   as the Place where the event happen , while similar   situations in other languages will mark Iraq as the   Target for the war .   Grammar difference between languages . An   example for this category is “ ... Blackstone Group   would buy Vivendi ’s theme park division , including   Universal Studios Hollywood ... ” and the queried   trigger “ buy ” . We observe that X - G(ar )   en ) predicts Videndi as the Artifact been sold and   division is the Seller , while X - G(en)en)4640can correctly understand that Videndi are the Seller   anddivision is the Artifact . We hypothesize the   reason being the differences between the gram-   mar in Arabic and English . The word order of   the sentence “ Vivendi ’s theme park division ” in   Arabic is reversed with its English counterpart ,   that is , “ theme park division ” will be written be-   fore“Vivendi ” in Arabic . Such difference leads to   errors in this category .   Generating words not appearing in the passage .   InX - G(zh)en ) , we observe several cases   that generate words not appearing in the passage .   There are two typical situations . The ﬁrst case is   thatX - G(zh)en ) mixes up singular and plu-   ral nouns . For example , the model generates “ stu-   dios ” as prediction while only “ studio ” appears in   the passage . This may be because Chinese does   not have morphological inﬂection for plural nouns .   The second case is that X - G(zh)en ) will   generate random predictions in Chinese .   Generating correct predictions but in Chinese .   This is a special case of “ Generating words not   appearing in the passage ” . In this category , we   observe that although the prediction is in Chinese   ( hence , a wrong prediction ) , it is correct if we trans-   late the prediction into English .   6.3 Constrained Decoding   Among all the errors , we highlight two speciﬁc   categories — “ Generating words not appearing in   the passage ” and“Generating correct predictions   but in Chinese ” . These errors can be resolved by   applying constrained decoding ( Cao et al . , 2021 )   to force all the generated tokens to appear input .   Table 7 presents the result of X - G with con-   strained decoding . We observe that adapting such   constraints indeed helps the cross - lingual transfer-   ability , yet it also hurts the performance in some   monolingual cases . We conduct a qualitative in-   spection of the predictions . The observation is that   constrained decoding algorithm although guaran-   tees all generated tokens appearing in the input , the   coercive method breaks the overall sequence distri-   bution that learned . Hence , in many monolingual   examples , once one of the tokens is corrected by   constrained decoding , its following generated se-   quence changes a lot , while the original predicted   sufﬁxed sequence using beam decoding are actually   correct . This leads to a performance decrease .   7 Conclusion   We present the ﬁrst generation - based models for   zero - shot cross - lingual event argument extraction .   To overcome the discrepancy between languages ,   we design language - agnostic templates and pro-   pose X - G , which well capture output depen-   dencies and can be used without additional named   entity extraction modules . Our experimental re-   sults show that X - G outperforms the current   state - of - the - art , which demonstrates the potential   of using a language generation framework to solve   zero - shot cross - lingual structured prediction tasks .   Acknowledgments   We thank anonymous reviewers for their helpful   feedback . We thank the UCLA PLUSLab and   UCLA - NLP group for the valuable discussions and   comments . We also thank Steven Fincke , Shan-   tanu Agarwal , and Elizabeth Boschee for their help   on data preparation in Arabic . This work is sup-   ported in part by the Intelligence Advanced Re-   search Projects Activity ( IARPA ) , via Contract   No . 2019 - 19051600007 , and research awards spon-   sored by CISCO and Google .   Ethics Considerations   Our proposed models are based on the multilin-   gual pre - trained language model that is trained on   a large text corpus . It is known that the pre - trained   language model could capture the bias reﬂecting   the training data . Therefore , our models can poten-   tially generate offensive or biased content learned   by the pre - trained language model . We suggest   carefully examining the potential bias before de-   ploying our model in any real - world applications.4641References464246434644A Dataset Statistics and Data   Preprocessing   Table 8 presents the detailed statistics for the ACE-   2005 dataset and ERE dataset .   For the English and Chinese splits in ACE-2005 ,   we use the setting provided by Wadden et al . ( 2019 )   and Lin et al . ( 2020 ) , respectively . As for Ara-   bic part , we adopt the setup proposed by Xu et al .   ( 2021 ) . Observing that part of the sentence breaks   made from Xu et al . ( 2021 ) being extremely long   for pretrained models to encode , we perform addi-   tional preprocessing and postprocessing procedures   for Arabic data . Speciﬁcally , we split Arabic sen-   tences into several portions that any of the portion   is shorter than 80 tokens . Then , we map the mod-   els ’ predictions of the split sentences back to the   original sentence during postprocessing .   B Implementation Details   We describe the implementation details for all the   models as follows :   •OneIE ( Lin et al . , 2020 ) . We use their provided   codeto train the model with the provided de-   fault settings . It is worth mention that for the   Arabic split in the ACE-2005 dataset , OneIE is   trained with only entity extraction , event extrac-   tion , and event argument extraction since there   is no relation labels in Xu et al . ( 2021 ) ’s prepro-   cessing script . All other parameters are set to   the default values .   •CL - GCN ( Subburathinam et al . , 2019 ) . We re-   fer the released code from Ahmad et al . ( 2021 )   to re - implement the CL - GCN method . Speciﬁ-   cally , we adapt the baseline framework that de-   scribed and implemented in OneIE ’s code ( Lin   et al . , 2020 ) , but we remove its relation extrac-   tion module and add two layers of GCN on top   of XLM - RoBERTa - large . The pos - tag and de-   pendency parsing annotations are obtained by   applying Stanza ( Qi et al . , 2020 ) . All other pa-   rameters are set to the be the same as the training   of OneIE .   •GATE ( Ahmad et al . , 2021 ) . We refer the ofﬁ-   cial released code from Ahmad et al . ( 2021 ) to   re - implement GATE . Similar to CL - GCN , we   adapt the baseline framework that described and   implemented in OneIE ’s code , but we removeits relation extraction module and add two lay-   ers of GATE on top of XLM - RoBERTa - large ,   mT5 , or mBART-50 - large . The pos - tag and de-   pendency parsing annotations are also obtained   by applying Stanza ( Qi et al . , 2020 ) . The hyper-   parameter of in GATE is set to be [ 2 , 2 , 4 , 4 ,   1,1,1,1 ] . All other parameters are set to   the be the same as the training of OneIE .   •TANL ( Paolini et al . , 2021 ) . To adapt TANL   to zero - shot cross - lingual EAE , we adapt the   public codeand replace its pre - trained based   model T5 ( Raffel et al . , 2020 ) with mT5 - base   ( Xue et al . , 2021 ) . All other parameters are set   to their default values .   •X - G is our proposed model . We consider   three different pre - trained generative language   models : mBART-50 - large ( Tang et al . , 2020 ) ,   mT5 - base , and mT5 - large ( Xue et al . , 2021 ) .   When ﬁne - tune the pre - trained models , we set   the learning rate to 10for mT5 , and 10for   mBART-50 - large . The batch size is set to 8 . The   number of training epochs is 60 .   C Constrained Decoding Detailed   Results   Table 9 shows the detailed results for X - G us-   ing constrained decoding algorithm during testing   time . We directly apply constrained decoding algo-   rithms on the trained models we have in Table 1.46454646