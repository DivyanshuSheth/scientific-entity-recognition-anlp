  Ali ModarressiHosein MohebbiMohammad Taher PilehvarIran University of Science and Technology , IranCognitive Science and AI , Tilburg University , NetherlandsTehran Institute for Advanced Studies , Khatam University , Iran   m_modarressi@comp.iust.ac.ir   h.mohebbi@uvt.nl   mp792@cam.ac.uk   Abstract   Pre - trained language models have shown stel-   lar performance in various downstream tasks .   But , this usually comes at the cost of high   latency and computation , hindering their us-   age in resource - limited settings . In this work ,   we propose a novel approach for reducing the   computational cost of BERT with minimal loss   in downstream performance . Our method dy-   namically eliminates less contributing tokens   through layers , resulting in shorter lengths and   consequently lower computational cost . To   determine the importance of each token rep-   resentation , we train a Contribution Predictor   for each layer using a gradient - based saliency   method . Our experiments on several diverse   classification tasks show speedups up to 22x   during inference time without much sacrifice   in performance . We also validate the quality   of the selected tokens in our method using hu-   man annotations in the ERASER benchmark .   In comparison to other widely used strategies   for selecting important tokens , such as saliency   andattention , our proposed method has a sig-   nificantly lower false positive rate in generat-   ing rationales . Our code is freely available   athttps://github.com/amodaresi/   AdapLeR .   1 Introduction   While large - scale pre - trained language models ex-   hibit remarkable performances on various NLP   benchmarks , their excessive computational costs   and high inference latency have limited their us-   age in resource - limited settings . In this regard ,   there have been various attempts at improving the   efficiency of BERT - based models ( Devlin et al . ,   2019 ) , including knowledge distilation ( Hinton   et al . , 2015 ; Sanh et al . , 2019 ; Sun et al . , 2019 ,   2020 ; Jiao et al . , 2020 ) , quantization ( Gong et al . ,   2014 ; Shen et al . , 2020 ; Tambe et al . , 2021 ) , weightpruning ( Han et al . , 2016 ; He et al . , 2017 ; Michel   et al . , 2019 ; Sanh et al . , 2020 ) , and progressive   module replacing ( Xu et al . , 2020 ) . Despite pro-   viding significant reduction in model size , these   techniques are generally static at inference time ,   i.e. , they dedicate the same amount of computation   to all inputs , irrespective of their difficulty .   A number of techniques have been also proposed   in order to make efficiency enhancement sensitive   to inputs . Early exit mechanism ( Schwartz et al . ,   2020b ; Liao et al . , 2021 ; Xin et al . , 2020 ; Liu et al . ,   2020 ; Xin et al . , 2021 ; Sun et al . , 2021 ; Eyza-   guirre et al . , 2021 ) is a commonly used method   in which each layer in the model is coupled with   an intermediate classifier to predict the target la-   bel . At inference , a halting condition is used to   determine whether the model allows an example   to exit without passing through all layers . Vari-   ous halting conditions have been proposed , includ-   ing Shannon ’s entropy ( Xin et al . , 2020 ; Liu et al . ,   2020 ) , softmax outputs with temperature calibra-   tion ( Schwartz et al . , 2020b ) , trained confidence   predictors ( Xin et al . , 2021 ) , or the number of agree-   ments between predictions of intermediate classi-   fiers ( Zhou et al . , 2020 ) .   Most of these input - adaptive techniques com-   press the model from the depth perspective ( i.e. ,   reducing the number of involved encoder layers ) .   However , one can view compression from the   width perspective ( Goyal et al . , 2020 ; Ye et al . ,   2021 ) , i.e. , reducing the length of hidden states .   ( Ethayarajh , 2019 ; Klafka and Ettinger , 2020 ) .   This is particularly promising as recent analytical   studies showed that there are redundant encoded   information in token representations ( Klafka and   Ettinger , 2020 ; Ethayarajh , 2019 ) . Among these   redundancies , some tokens carry more task - specific   information than others ( Mohebbi et al . , 2021 ) ,   suggesting that only these tokens could be con-   sidered through the model . Moreover , in contrast   to layer - wise pruning , token - level pruning does not1come at the cost of reducing model ’s capacity in   complex reasoning ( Sanh et al . , 2019 ; Sun et al . ,   2019 ) . PoWER - BERT ( Goyal et al . , 2020 ) is one of   the first such techniques which reduces inference   time by eliminating redundant token representa-   tions through layers based on self - attention weights .   Several studies have followed ( Kim and Cho , 2021 ;   Wang et al . , 2021 ) ; However , they usually optimize   a single token elimination configuration across the   entire dataset , resulting in a static model . In addi-   tion , their token selection strategies are based on   attention weights which can result in a suboptimal   solution ( Ye et al . , 2021 ) .   In this work , we introduce Adap tiveLength   Reduction ( AdapLeR ) . Instead of relying on at-   tention weights , our method trains a set of Contri-   bution Predictors ( CP ) to estimate tokens ’ saliency   scores at inference . We show that this choice re-   sults in more reliable scores than attention weights   in measuring tokens ’ contributions . The most re-   lated study to ours is TR - BERT ( Ye et al . , 2021 )   which leverages reinforcement learning to develop   an input - adaptive token selection policy network .   However , as pointed out by the authors , the prob-   lem has a large search space , making it difficult for   RL to solve . To mitigate this , they resorted to extra   heuristics such as imitation learning ( Hussein et al . ,   2017 ) for warming up the training of the policy net-   work , action sampling for limiting the search space ,   and knowledge distillation for transferring knowl-   edge from the intact backbone fine - tuned model .   All of these steps significantly increase the train-   ing cost . Hence , they only perform token selection   at two layers . In contrast , we propose a simple   but effective method to gradually eliminate tokens   in each layer throughout the training phase using   a soft - removal function which allows the model   to be adaptable to various inputs in a batch - wise   mode . It is also worth noting in contrast to our ap-   proach above studies are based on top - k operations   for identifying the k most important tokens during   training or inference , which can be expensive with-   out a specific hardware architecture ( Wang et al . ,   2021 ) .   In summary , our contributions are threefold :   •We couple a simple Contribution Predictor   ( CP ) with each layer of the model to estimate   tokens ’ contribution scores to eliminate redun-   dant representations .   •Instead of an instant token removal , we grad-   ually mask out less contributing token repre - sentations by employing a novel soft - removal   function .   •We also show the superiority of our token   selection strategy over the other widely used   strategies by using human rationales .   2 Background   2.1 Self - attention Weights   Self - attention is a core component of the Trans-   formers ( Vaswani et al . , 2017 ) which looks for   the relation between different positions of a sin-   gle sequence of token representations ( x , ... , x )   to build contextualized representations . To this   end , each input vector xis multiplied by the corre-   sponding trainable matrices Q , K , andVto respec-   tively produce query ( q ) , key ( k ) , and value ( v )   vectors . To construct the output representation z , a   series of weights is computed by the dot product of   qwith every kin all time steps . Before applying   a softmax function , these values are divided by a   scaling factor and then added to an attention mask   vector m , which is zero for positions we wish to   attend and −∞ ( in practice , −10000 ) for padded   tokens ( Vaswani et al . , 2017 ) . Mathematically , for   a single attention head , the weight attention from   token xto token xin the same input sequence   can be written as :   α= softmax   qk√   d+m !   ∈R ( 1 )   The time complexity for this is O(n)given the   dot product qk , where nis the input sequence   length . This impedes the usage of self - attention   based models in low - resource settings .   While self - attention is one of the most white - box   components in transformer - based models , relying   on raw attention weights as an explanation could   be misleading given that they are not necessarily re-   sponsible for determining the contribution of each   token in the final classifier ’s decision ( Jain and Wal-   lace , 2019 ; Serrano and Smith , 2019 ; Abnar and   Zuidema , 2020 ) . This is based on the fact that raw   attentions are being faithful to the local mixture of   information in each layer and are unable to obtain a   global perspective of the information flow through   the entire model ( Pascual et al . , 2021 ) .   2.2 Gradient - based Saliency Scores   Gradient - based methods provide alternatives to at-   tention weights to compute the importance of a2   specific input feature . Despite having been widely   utilized in other fields earlier ( Ancona et al . , 2018 ;   Simonyan et al . , 2013 ; Sundararajan et al . , 2017 ;   Smilkov et al . , 2017 ) , they have only recently be-   come popular in NLP studies ( Bastings and Fil-   ippova , 2020 ; Li et al . , 2016 ; Yuan et al . , 2019 ) .   These methods are based on computing the first-   order derivative of the output logit yw.r.t . the   input embedding h(initial hidden states ) , where   ccould be true class label to find the most impor-   tant input features or the predicted class to interpret   model ’s behavior . After taking the norm of output   derivatives , we get sensitivity ( Ancona et al . , 2018 ) ,   which indicates the changes in model ’s output with   respect to the changes in specific input dimensions .   Instead , by multiplying gradients with input fea-   tures , we arrive at gradient ×input ( Bastings and   Filippova , 2020 ) , also known as saliency , which   also considers the direction of input vectors to de-   termine the most important tokens . Since these   scores are computed for each dimension of embed-   ding vectors , an aggregation method such as L2   norm or mean is needed to produce one score per   input token ( Atanasova et al . , 2020a ):   S=∥∂y   ∂h⊙h∥ ( 2 )   3 Methodology   As shown in Figure 1 , our approach relies on drop-   ping low contributing tokens in each layer and   passing only the more important ones to the next .   Therefore , one important step is to measure the im-   portance of each token . To this end , we opted for   saliency scores which have been recently shownas a reliable criterion in measuring token ’s con-   tributions ( Bastings and Filippova , 2020 ; Pascual   et al . , 2021 ) . In Section 5.1 we will show results   for a series quantitative analyses that supports this   choice . In what follows , we first describe how we   estimate saliency scores at inference time using a   set of Contribution Predictors ( CPs ) and then elab-   orate on how we leverage these predictors during   inference ( Section 3.2 ) and training ( Section 3.3 ) .   3.1 Contribution Predictor   Computing gradients during inference is problem-   atic as backpropagation computation prolongs in-   ference time , which is contrary to our main goal .   To circumvent this , we simply add a CP after each   layer ℓin the model to estimate contribution score   for each token representation , i.e. , . The model   then decides on the tokens that should be passed to   the next layer based on the values of . CP com-   putesfor each token using an MLP followed   by a softmax activation function . We argue that ,   despite being limited in learning capacity , the MLP   is sufficient for estimating scores that are more gen-   eralized and relevant than vanilla saliency values .   We will present a quantitative analysis on this topic   in Section 5 .   3.2 Model Inference   Most BERT - based models consist of Lencoder   layers . The input sequence of ntokens is usually   passed through an embedding layer to build the   initial hidden states of the model h. Each encoder   layer then produces the next hidden states using the3ones from the previous layer :   h = Encoder(h ) ( 3 )   In our approach , we eliminate less contribut-   ing token representations before delivering hidden   states to the next encoder . Tokens are selected   based on the contribution scoresobtained from   the CP of the corresponding layer ℓ. As the sum   of these scores is equal to one , a uniform level   indicates that all tokens contribute equally to the   prediction and should be retained . On the other   hand , the lower - scoring tokens could be viewed as   unnecessary tokens if the contribution scores are   concentrated only on a subset of tokens . Given   that the final classification head uses the last hid-   den state of the [ CLS ] token , we preserve this   token ’s representation in all layers . Despite pre-   serving this , other tokens might be removed from   a layer when [ CLS ] has a significantly high esti-   mated contribution score than others . Based on this   intuition , we define a cutoff threshold based on the   uniform level as : δ = η·/with0 < η≤1to   distinguish important tokens . Tokens are consid-   ered important if their contribution score exceeds δ   ( which is a value equal or smaller than the uniform   score ) . Intuitively , a larger ηprovides a higher δ   cutoff level , thereby dropping a larger number of   tokens , hence , yielding more speedup . The value   ofηdetermines the extent to which we can rely   on CP ’s estimations . In case the estimations of   CP are deemed to be inaccurate , its impact can be   reduced by lowering η . We train each layer ’s η   using an auxiliary training objective , which allows   the model to adjust the cutoff value to control the   speedup - performance tradeoff . Also , since each   input instance has a different computational path   during token removal process , it is obvious that   at inference time , the batch size should be equal   to one ( single instance usage ) , similarly to other   dynamic approaches ( Zhou et al . , 2020 ; Liu et al . ,   2020 ; Ye et al . , 2021 ; Eyzaguirre et al . , 2021 ; Xin   et al . , 2020 ) .   3.3 Model Training   Training consists of three phases : initial fine-   tuning , saliency extraction , and adaptive length re-   training . In the first phase , we simply fine - tune the   backbone model ( BERT ) on a given target task . We   then extract the saliencies of three top - perfroming   checkpoints from the fine - tuning process and com-   pute the average of them to mitigate potential in-   consistencies in saliency scores ( cf . Section 2.2 ) .   The final step is to train a pre - trained model us-   ing an adaptive length reduction procedure . In   this phase , a non - linear function gradually fades   out the representations throughout the training pro-   cess . Each CP is jointly trained with the rest of   the model using the saliencies extracted in the pre-   vious phase alongside with the target task labels .   We also define a speedup tuning objective to deter-   mine the thresholds ( via tuning η ) to control the   performance - speedup trade - off . In the following ,   we elaborate on the procedure .   Soft - removal function . During training , if to-   kens are immediately dropped similarly to the in-   ference mode , the effect of dropping tokens can-   not be captured using a gradient backpropagation   procedure . Using batch - wise training in this sce-   nario will also be problematic as the structure   will vary with each example . Hence , inspired by   the padding mechanism of self - attention models   ( Vaswani et al . , 2017 ) we introduce a new proce-   dure that gradually masks out less contributing to-   ken representations . In each layer , after predicting   contribution scores , instead of instantly removing   the token representations , we accumulate a nega-   tive mask to the attention mask vector Musing a   soft - removal function :   m(˜S ) =       λ(˜S−δ)−β   λ˜S < δ   ( ˜S−1)β   ( 1−δ)λ˜S≥δ(4 )   This function consists of two main zones ( Figure   2 ) . In the first term , the less important tokens with   scores lower than the threshold ( δ ) are assigned   higher negative masking as they get more distant4from δ . The slope is determined by λ=/ ,   where λis a hyperparameter that is increased ex-   ponentially after each epoch ( e.g. , λ←10×λaf-   ter finishing each epoch ) . Increasing λmakes the   soft - removal function stronger and more decisive   in masking the representations . To avoid under-   going zero gradients during training , we define   0 < β < 0.1to construct a small negative slope   ( similar to the well known Leaky - ReLU of Maas   et al . 2013 ) for those tokens with higher contribut-   ing scores than δthreshold . Consider a scenario in   which ηsharply drops , causing most ofget over   theδthreshold . In this case , the non - zero value   in the second term of Equation 4 , which facilitates   optimizing η .   Training the Contribution Predictors . The CPs   are trained by an additional term which is based   on the KL - divergenceof each layer ’s CP output   with the extracted saliencies . The main training   objective is a minimization of the following loss :   L = L+γL ( 5 )   Where γis a hyperparameter which that specifies   the amount of emphasis on the CP training loss :   L = X(L−ℓ)D(ˆS||˜S )   = X(L−ℓ)XˆSlog(ˆS   ˜S)(6 )   Since Sis based on the input embeddings , the   [ CLS ] token usually shows a low amount of con-   tribution due to not having any contextualism in   the input . As we leverage the representation of   the[CLS ] token in the last layer for classification ,   this token acts as a pooler and gathers information   about the context of the input . In other words , the   token can potentially have more contribution as it   passes through the model . To this end , we amplify   the contribution score of [ CLS ] and renormalize   the distribution ( ˆS ) with a trainable parameter θ :   ˆS = θS1[i= 1 ] + S1[i > 1 ]   θS+PS(7 )   By this procedure , the next objective ( discussed   in the next paragraph ) will have the capability of   tuning the amount of pooling , consequently con-   trolling the amount of speedup . Larger θpush theCPs to shift the contribution towards the [ CLS ] to-   ken to gather most of the task - specific information   and avoids carrying redundant tokens through the   model .   Speedup Tuning . In the speedup tuning process ,   we combine the cross - entropy loss of the target   classification task with a length loss which is the   expected number of unmasked token representa-   tions in all layers . Considering that we have a   non - positive and continuous attention mask M ,   the length loss of a single layer would be the   summation over the exponential of the mask val-   uesexp(m)to map the masking range [ −∞,0 ]   to a[0(fully masked / removed ) , 1(fully retained ) ]   bound .   L = L+ϕL   L = XXexp(m)(8 )   Equation 8 demonstrates how the length loss is   computed inside the model and how it is added to   the main classification loss . During training , we   assign a separate optimization process which tunes   ηandθto adjust the thresholds and the amount of   [ CLS ] poolingalongside with the CP training .   The reason that this objective is treated as a sep-   arate problem instead of merging it with the pre-   vious one , is because in the latter case the CPs   could be influenced by the length loss and try to   manipulate the contribution scores for some tokens   regardless of their real influence . So in other words ,   the first objective is to solve the task and make it   explainable with the CPs , and the secondary objec-   tive builds the speedup using tuning the threshold   levels and the amount of pooling in each layer .   4 Experiments   4.1 Datasets   To verify the effectiveness of AdapLeR on infer-   ence speedup , we selected eight various text classi-   fication datasets . In order to incorporate a variety   of tasks , we utilized SST-2 ( Socher et al . , 2013 ) and   IMDB ( Maas et al . , 2011 ) for sentiment , MRPC   ( Dolan and Brockett , 2005 ) for paraphrase , AG ’s   News ( Zhang et al . , 2015 ) for topic classification ,   DBpedia ( Lehmann et al . , 2015 ) for knowledge   extraction , MNLI ( Williams et al . , 2018 ) for NLI,5   QNLI ( Rajpurkar et al . , 2016 ) for question answer-   ing , and HateXplain ( Mathew et al . , 2021 ) for hate   speech . Evaluations are based on the test split of   each dataset . For those datasets that are in the   GLUE Benchmark ( Wang et al . , 2018 ) , test results   were acquired by submitting the test predictions to   the evaluation server .   4.2 Experimental Setup   As our baseline , we report results for the pre-   trained BERT model ( base - uncased ) ( Devlin et al . ,   2019 ) which is also the backbone of AdapLeR.   We also compare against three other approaches :   DistilBERT ( uncased ) ( Sanh et al . , 2019 ) as a   static compression method , PoWER - BERT and   TR - BERT as two strong length reduction methods   ( cf . Sec . 1 ) . We used the provided implemen-   tations and suggested hyperparametersto train   these baselines . To fine - tune the backbone model ,   we used same hyperparameters over all tasks ( see   Section D for details ) . The backbone model and   our model implementation is based on the Hug-   gingFace ’s Transformers library ( Wolf et al . , 2020 ) .   Trainings and evaluations were conducted on a dual   2080Ti 11 GB GPU machine with multiple runs .   Hyperparameter Selection . Overall , we intro-   duced four hyperparameters ( γ , ϕ , λ , β ) which are   involved in the training process . Among these , ϕ   andγare the primary terms that have considerable   effects on AdapLeR ’s downstream performance   and speedup . This makes our approach comparable   to existing techniques ( Goyal et al . , 2020 ; Ye et al . ,   2021 ) which usually have two or three hyperpa-   rameters adjusted per task . We used grid search tofind the optimal values for these two terms , while   keeping the other hyperparameters constant over   all datasets . Hyperparamter selection is further   discussed in Section D.   FLOPs Computation . We followed Ye et al .   ( 2021 ) and Liu et al . ( 2020 ) and measured com-   putational complexity in terms of FLOPs , i.e. , the   number of floating - point operations ( FLOPs ) in   a single inference procedure . This allows us to   assess models ’ speedups independently of their op-   erating environment ( e.g. , CPU / GPU ) . The total   FLOPs of a given model is a summation of the   measured FLOPs over all test examples . Then , a   model ’s speedup can be defined as the total FLOPs   measured on BERT ( our baseline ) divided by the   corresponding model ’s total FLOPs . To have a fair   comparison , we also computed FLOPs for PoWER-   BERT in a single instance mode , described in Sec-   tion C.   4.3 Results   Table 1 shows performance and speedup for   AdapLeR and other comparison models across   eight different datasets . While preserving the same   level of performance , AdapLeR outperforms other   techniques in terms of speedup across all tasks   ( ranging from +0.2x to +7.4x compared to the best   model in each dataset ) .   It is noteworthy that the results also reveal some   form of dependency on the type of the tasks . Some   tasks may need less amount of contextualism dur-   ing inference and could be classified by using only   a fraction of input tokens . For instance , in AG ’s   News , the topic of a sentence might be identifiable   with a single token ( e.g. , soccer →Topic : Sports ,   see Figure 6 in the Appendix for an example ) .   PoWER - BERT adopts attention weights in its to-   ken selection which requires at least one layer of   computation to be determined , and TR - BERT ap-6   plies token elimination only in two layers to reduce   the training search space . In contrast , our proce-   dure performs token elimination for all layers of   the model , enabling a more effective removal of re-   dundant tokens . On the other hand , we observe that   TR - BERT and PoWER - BERT lack any speedup   gains for tasks such as QNLI , MNLI , and MRPC   which need a higher degree of contextualism dur-   ing inference . However , AdapLeR can offer some   speedups even for these tasks .   Speedup - Performance Tradeoff . To provide a   closer look at the efficiency of AdapLeR in com-   parison with the other state - of - the - art length re-   duction methods , we illustrate speedup - accuracy   curves in Figure 3 . We provide these curves for   two tasks in which other length reduction methods   show comparable speedups to AdapLeR. For each   curve , the points were obtained by tuning the most   influential hyperparameters of the corresponding   model . As we can see , AdapLeR significantly out-   performs the other two approaches in all two tasks .   An interesting observation here is that the curves   for TR - BERT and AdapLeR are much higher than   that of PoWER - BERT . This can be attributed to   the input - adaptive procedure employed by the for-   mer two methods for determining the number of   reduced tokens ( whereas PoWER - BERT adopts a   fixed retention configuration in token elimination ) .   5 Analysis   In this section , we first conduct an experiment to   support our choice of saliency scores as a super-   vision in measuring the importance of token rep-   resentations . Next , we evaluate the behavior of   Contribution Predictors in identifying the most im-   portant tokens in the AdapLeR.   5.1 Rationale as an Upper Bound   A natural question that arises when dealing with   token pruning is that of importance measure : what   is the most appropriate criterion for assessing the   relative importance of tokens within a sentence ?   We resort to human rationale as a reliable up-   per bound for measuring token importance . To   this end , we used the ERASER benchmark ( DeY-   oung et al . , 2020 ) , which contains multiple tasks   for which important spans of the input text have   been highlighted as supporting evidence ( aka “ ra-   tionale ” ) by human . Among the tasks in the   benchmark , we opted for two diverse classifica-   tion tasks : Movie reviews ( Zaidan and Eisner ,   2008 ) and MultiRC ( Khashabi et al . , 2018 ) . In   the former task , the model predicts the sentiment   of the passage . Whereas the latter contains a pas-   sage , a question , and multiple candidate answers ,   which is cast as a binary classification task of   passage / question / answer triplets in the ERASER   benchmark .   In order to verify the reliability of human ratio-   nales , we fine - tuned BERT based on the rationales   only , i.e. , by excluding those tokens that are not   highlighted as being important in the input . In Ta-   ble 2 , the first two rows show the performance of   BERT on the two tasks with full input and with hu-   man rationales only . We see that fine - tuning merely7   on rationales not only yields less computation cost ,   but also results in a better performance when com-   pared with the full input setting . Obviously , human   annotations are not available for a whole range of   downstream NLP tasks ; therefore , this criterion is   infeasible in practice and can only be viewed as an   upper bound for evaluating different strategies in   measuring token importance .   5.2 Saliency vs. Attention   We investigated the effectiveness of saliency and   self - attention weights as two commonly used strate-   gies for measuring the importance of tokens in   pre - trained language models . To compute these ,   we first fine - tuned BERT with all tokens in the   input for a given target task . We then obtained   saliency scores with respect to the tokens in the   input embedding layer . This brings about two ad-   vantages . Firstly , representations in the embedding   layer are non - contextualized , allowing us to mea-   sure the importance of each token independently   from the others . Secondly , the backpropagation   of gradients through layers to the beginning of the   model provides us with aggregated values for the   relative importance of each token based on the   entire model . Similarly , we aggregated the self-   attention weights across all layers of the model   using a post - processed variant of attentions called   attention rollout ( Abnar and Zuidema , 2020 ) , a pop-   ular technique in which the attention weight matrix   in each layer is multiplied with the preceding ones   to form aggregated attention values .   To assign an importance score to each token , we   examined two different interpretation of attention   weights . The first strategy is the one adopted by   PoWER - BERT ( Goyal et al . , 2020 ) in which for   each token we accumulate attention values fromother tokens . Additionally , we measured how much   the[CLS ] token attends to each token in the sen-   tence , a strategy which has been widely used in   interpretability studies around BERT ( Abnar and   Zuidema , 2020 ; Chrysostomou and Aletras , 2021 ;   Jain et al . , 2020 , inter alia ) . For a fair comparison ,   for each sentence in the test set , we selected the   top - ksalient and attended words , with kbeing the   number of words that are annotated as rationales .   Results in Table 2 show that fine - tuning on the   most salient tokens outperforms that based on the   most attended tokens . This denotes that saliency   is a better indicator for the importance of tokens .   Nonetheless , recent length reduction techniques   ( Goyal et al . , 2020 ; Kim and Cho , 2021 ; Wang   et al . , 2021 ) have mostly adopted attention weights   as their criterion for selecting important tokens .   Computing these weights is convenient as they   are already computed during the forward pass ,   whereas computing saliency requires an additional   backpropagation step . Note that in our approach ,   saliency scores are easily estimated within infer-   ence time by the pre - trained CPs .   5.3 Contribution Predictor Evaluation   In this section we validate our Contribution Predic-   tors in selecting the most contributed tokens . Fig-   ure 4 illustrates two examples from the SST-2 and   QNLI datasets in which CPs identify and gradually   drop the irrelevant tokens through layers , finally   focusing mostly on the most important token rep-   resentations ; pedestrian ( adjective ) in SST-2 and   tesla coil in the passage part of QNLI ( both of   which are highly aligned with human rationale ) .   In order to quantify the extent to which   AdapLeR ’s CPs can preserve rationales without   requiring direct human annotations in an unsuper-8   vised manner we carried out the following exper-   iment . To investigate the effectiveness of trained   CPs in predicting human rationales we computed   the output scores of CPs in AdapLeR for each to-   ken representation in each layer . We also fine - tuned   a BERT model on the Movie Review dataset and   computed layer - wise raw attention , attention roll-   out , and saliency scores for each token represen-   tation . Since human rationales are annotated at   the word level , we sum the scores across tokens   corresponding to each word to arrive at word - level   importance scores . In addition to these soft scores ,   we used the uniform - level threshold ( i.e. ,/ ) to   reach a binary score indicating tokens selected in   each layer .   As for evaluation , we used the Average Precision   ( AP ) and False Positive Rate ( FPR ) metrics by com-   paring the remaining tokens to the human rationale   annotations . The first metric measures whether the   model assigns higher continuous scores to those   tokens that are annotated by humans as rationales .   Whereas , the intuition behind the second metric   is how many irrelevant tokens are selected by the   model to be passed to subsequent layers . We used   soft scores for computing AP and binary scores for   computing FPR .   Figure 5 shows the agreement between human   rationales and the selected tokens based on the   two metrics . In comparison with the other widely   used strategies for selecting important tokens , such   as salinecy and attention , our CPs have signifi-   cantly less false positive rate in preserving ratio - nales through layers . Despite having similar FPRs   at the final layer , CP is preferable to attention in   that it can better identify rationales at the earlier   layers , allowing the model to combine the most   relevant token representations when building the   final one . This in turn results in better performance ,   as was also shown in the previous experiment in   Section 5.2 . Also , we see that the curve of mAP for   saliency is consistently higher than other strategies   in terms of alignment with human rationales which   supports our choice of saliency as a measure for   token importance .   Finally , we note that there is a line of research   that attempts at guiding models to perform human-   like reasoning by training rationale generation si-   multaneously with the target task that requires hu-   man annotation ( Atanasova et al . , 2020b ; Zhao   et al . , 2020 ; Li et al . , 2018 ) . As a by - product of the   contribution estimation process , our trained CPs   are able to generate these rationales at inference   without the need for human - generated annotations .   6 Conclusion   In this paper , we introduced AdapLeR , a novel   method that accelerates inference by dynamically   identifying and dropping less contributing token   representations through layers of BERT - based mod-   els . Specifically , AdapLeR accomplishes this by   training a set of Contribution Predictors based on   saliencies extracted from a fine - tuned model and   applying a gradual masking technique to simulate   input - adaptive token removal during training . Em-   pirical results on eight diverse text classification   tasks show considerable improvements over exist-   ing methods . Furthermore , we demonstrated that   contribution predictors generate rationales that are   highly in line with those manually specified by   humans . As future work , we aim to apply our   technique to more tasks and see whether it can be   adapted to those tasks that require all token rep-   resentations to be present in the final layer of the   model ( e.g. , question answering ) . Additionally ,   combining our width - based strategy with a depth-   based one ( e.g. , early exiting ) might potentially   yield greater efficiency , something we plan to pur-   sue as future work .   Broader Impact   Using our proposed method , pre - trained language   models can use fewer FLOPs , reducing energy use   and carbon emissions ( Schwartz et al . , 2020a).9References101112   A Inclusive KL Loss Consideration   We opted for an inclusive KL loss since CPs should   be trained to cover all tokens considered important   by saliency and not to be mode seeking ( i.e. , cover-   ing a subset of high contributing tokens considered   by the saliency scores . ) . Suppose an exclusive KL   is selected . Due to the limited learning capacity   of the CP and miscalculation possibility from the   saliency , the CP may be trained to maximize its   contribution on noninformative tokens . While in   an inclusive setting , it trains to extend its coverage   over all high - saliency tokens . Additionally , our initial research indicated that   using a symmetric loss ( e.g. Jensen - Shannon di-   vergence ) would produce similar results but with a   significantly longer convergence time .   B Optimization of θ   In Section 3.3 , we introduced θas a trainable pa-   rameter that increases the saliency score of [ CLS ] .   We can deduce from Equations 6 and 7 that this pa-   rameter does not exist in the model ’s computational   DAG and we need to compute the derivative of ˜S   w.r.t . θto train this parameter . Hence , first we   assume that ˜Sis a close estimate of ˆS(due to the   CPs ’ training objective ) . Second , using a dummy   variable θ — that is involved in the computational   graph and is always equal to 1 — we reformulate   ˜S :   ˆS≈˜S = θ˜S1[i= 1 ] + ˜S1[i > 1 ]   θ˜S+P˜S(9 )   This reformulation is valid due to θ= 1 andP˜S= 1 . Now we compute the partial deriva-   tive w.r.t . θwhich is the gradient that is computed   in the backpropagation :   ∂˜S   ∂θ=˜S(P˜S1[i= 1]−˜S1[i > 1 ] )   ( θ˜S+P˜S )   ( 10 )   By knowing that θ= 1 :   ∂˜S   ∂θ=˜S((1−˜S)1[i= 1]−˜S1[i > 1])(11 )   Now using our initial assumption ( ˆS≈˜S ) , we   can substitute ˜Swith ˆSbased on Equation 7 :   ∂˜S   ∂θ=ˆS((1−ˆS)1[i= 1]−ˆS1[i > 1 ] )   = θS(PS1[i= 1]−S1[i > 1 ] )   ( θS+PS )   ( 12 )   In addition , the gradient of ˆSw.r.t . θis as follows   ( cf . Equation 7 ):   ∂ˆS   ∂θ = S(PS1[i= 1]−S1[i > 1 ] )   ( θS+PS )   ( 13 )   By comparing Equations 12 and 13 , these deriva-   tives are related with a term of θ :   ∂ˆS   ∂θ≈∂˜S   ∂θ=1   θ∂˜S   ∂θ(14)13Therefore , during training , we can compute the   gradient w.r.t . the dummy variable θand then   divide it by θ .   C Evaluating PoWER - BERT in Single   Instance Mode   Due to the static structure of PoWER - BERT , the   speedup ratios reported in Goyal et al . ( 2020 ) are   based on wall time acceleration with batch - wise   inference procedure . This means that some inputs   might need extra padding to make all inputs with   the same token length . However , since our ap-   proach and other dynamic approaches are based   on single instance inference , in our procedure in-   puts are fed without being padded . To even out   this discrepancy , we apply a single instance flops   computation on the PoWER - BERT , which means   we compute the computational cost for all input   lengths that appear in the test dataset . Some in-   stnaces may have shorter input length than some   values in the resulting retention configuration ( num-   ber of tokens that are retained in each layer ) . To   overcome this issue , we update the retention con-   figuration by selecting the minimum between the   input length and each layers ’ number of tokens re-   tained , to build a new retention configuration for   each input length . For instance , if the retention con-   figuration trained model on a given task be ( 153 ,   125 , 111 , 105 , 85 , 80 , 72 , 48 , 35 , 27 , 22 , 5 ) , for an   input with 75 tokens length , the new configuration   which is used for speedup computation will be : ( 75 ,   75 , 75 , 75 , 75 , 75 , 72 , 48 , 35 , 27 , 22 , 5 ) .   D AdapLeR Training Hyperparameters   For the initial step of fine - tuning BERT , we used the   hyperparameters in Table 3 . For both fine - tuning   and training with length reduction , we employed an   AdamW optimizer ( Loshchilov and Hutter , 2019 )   with a weight decay rate of 0.1 , warmup proportion   6 % of total training steps and a linear learning rate   decay which reaches to zero at the end of training .   For the adaptive length reduction training step ,   we also used the same hyperparameters in Table   3 with two differences : Since MRPC and CoLA   have small training sets , to prolong the gradual soft-   removal process , we increased the training duration   to 10 epochs . Moreover , we increase the learning   rate to 3e-5 . Other hyperparameters are stated in   Table 4 . To set a trend for λ , it needs to start from   a small but effective value ( 10 < λ < 100 ) and   grow exponentially per each epoch to reach an ex-   tremely high amount at the end of the training to   mimic a hard removal function ( 1e+5 < λ ) . Hence ,   datasets with the same amount of training epochs   have similar λtrends .   E Statistics of Datasets   F Additional Qualitative Examples1415