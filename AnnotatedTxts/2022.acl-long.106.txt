  Yoshinari Fujinuma Labs   Amazon.comJordan Boyd - Graber , , , iSchool   University of MarylandKatharina Kann   Computer Science   University of Colorado Boulder   Abstract   Pretrained multilingual models enable zero-   shot learning even for unseen languages , and   that performance can be further improved via   adaptation prior to finetuning . However , it   is unclear how the number of pretraining lan-   guages influences a model ’s zero - shot learn-   ing for languages unseen during pretraining .   To fill this gap , we ask the following re-   search questions : ( 1 ) How does the number   of pretraining languages influence zero - shot   performance on unseen target languages ? ( 2 )   Does the answer to that question change with   model adaptation ? ( 3 ) Do the findings for   our first question change if the languages used   for pretraining are all related ? Our experi-   ments on pretraining with related languages   indicate that choosing a diverse set of lan-   guages is crucial . Without model adaptation ,   surprisingly , increasing the number of pre-   training languages yields better results up to   adding related languages , after which perfor-   mance plateaus . In contrast , with model adap-   tation via continued pretraining , pretraining on   a larger number of languages often gives fur-   ther improvement , suggesting that model adap-   tation is crucial to exploit additional pretrain-   ing languages .   1 Introduction   Pretrained multilingual language models ( Devlin   et al . , 2019 ; Conneau et al . , 2020 ) are now a stan-   dard approach for cross - lingual transfer in natural   language processing ( NLP ) . However , there are   multiple , potentially related issues on pretraining   multilingual models . Conneau et al . ( 2020 ) find the   “ curse of multilinguality ” : for a fixed model size ,   zero - shot performance on target languages seen   during pretraining increases with additional pre-   training languages only until a certain point , afterwhich performance decreases . Wang et al . ( 2020b )   also report “ negative interference ” , where monolin-   gual models achieve better results than multilingual   models , both on subsets of high- and low - resource   languages . However , those findings are limited to   target languages seen during pretraining .   Current multilingual models cover only a small   subset of the world ’s languages . Furthermore , due   to data sparsity , monolingual pretrained models   are not likely to obtain good results for many low-   resource languages . In those cases , multilingual   models can zero - shot learn for unseen languages   with an above - chance performance , which can be   further improved via model adaptation with target-   language text ( Wang et al . , 2020a ) , even for limited   amounts ( Ebrahimi and Kann , 2021 ) . However , it   is poorly understood how the number of pretraining   languages influences performance in those cases .   Does the “ curse of multilinguality ” or “ negative   interference ” also impact performance on unseen   target languages ? And , if we want a model to be   applicable to as many unseen languages as possible ,   how many languages should it be trained on ?   Specifically , we ask the following research ques-   tions : ( 1 ) How does pretraining on an increasing   number of languages impact zero - shot performance   on unseen target languages ? ( 2 ) Does the effect of   the number of pretraining languages change with   model adaptation to target languages ? ( 3 ) Does the   answer to the first research question change if the   pretraining languages are all related to each other ?   We pretrain a variety of monolingual and mul-   tilingual models , which we then finetune on En-   glish and apply to three zero - shot cross - lingual   downstream tasks in unseen target languages : part-   of - speech ( ) tagging , named entity recogni-   tion ( ) , and natural language inference ( ) .   Experimental results suggest that choosing a di-   verse set of pretraining languages is crucial for   effective transfer . Without model adaptation , in-   creasing the number of pretraining languages im-1500proves accuracy on unrelated unseen target lan-   guages at first and plateaus thereafter . Last , with   model adaptation , additional pretraining languages   beyond English generally help .   We are aware of the intense computational   cost of pretraining and its environmental im-   pact ( Strubell et al . , 2019 ) . Thus , our experiments   in Section 4 are on a relatively small scale with   a fixed computational budget for each model and   on relatively simple NLP tasks ( tagging , ,   and ) , but validate our most central findings   in Section 5 on large publicly available pretrained   models .   2 Cross - lingual Transfer via Pretraining   Pretrained multilingual models are a straightfor-   ward cross - lingual transfer approach : a model pre-   trained on multiple languages is then fine - tuned   on target - task data in the source language . Subse-   quently , the model is applied to target - task data in   thetarget language . Most commonly , the target   language is part of the model ’s pretraining data .   However , cross - lingual transfer is possible even if   this is not the case , though performance tends to be   lower . This paper extends prior work exploring the   cross - lingual transfer abilities of pretrained models   forseen target languages depending on the number   of pretraining languages to unseen target languages .   We now transfer via pretrained multilingual models   and introduce the models and methods vetted in   our experiments .   2.1 Background and Methods   Pretrained Language Models Contextual rep-   resentations such as ELMo ( Peters et al . , 2018 )   and ( Devlin et al . , 2019 ) are not just use-   ful for monolingual representations . Multilingual ( Devlin et al . , 2019 , ) , ( Lample   and Conneau , 2019 ) , and XLM - RoBERTa ( Con-   neau et al . , 2020 , - ) have surprisingly high   cross - lingual transfer performance compared to the   previous best practice : static cross - lingual word   embeddings ( Pires et al . , 2019 ; Wu and Dredze ,   2019 ) . Multilingual models are also practical —   why have hundreds of separate models for each   language when you could do better with just one ?   Furthermore , Wu and Dredze ( 2020 ) report that   models pretrained on 100 + languages are better   than bilingual or monolingual language models in   zero - shot cross - lingual transfer . Model Adaptation to Unseen Languages   Adapting pretrained multilingual models such as and -to unseen languages is one   way to use such models beyond the languages   covered during pretraining time . Several methods   for adapting pretrained multilingual language   models to unseen languages have been proposed ,   including continuing masked language model   ( ) training ( Chau et al . , 2020 ; Müller et al . ,   2020 ) , optionally adding Adapter modules ( Pfeiffer   et al . , 2020 ) , or extending the vocabulary of the   pretrained models ( Artetxe et al . , 2020 ; Wang   et al . , 2020a ) . However , such adaptation methods   assume the existence of sufficient monolingual   corpora in the target languages . Some spoken   languages , dialects , or extinct languages lack   monolingual corpora to conduct model adaptation ,   which motivates us to look into languages unseen   during pretraining . We leave investigation on the   effect of target language - specific processing , e.g. ,   transliteration into Latin scripts ( Muller et al . ,   2021 ) , for future work .   2.2 Research Questions   A single pretrained model that can be applied to any   language , including those unseen during pretrain-   ing , is both more efficient and more practical than   pretraining one model per language . Moreover , it   is the only practical option for unknown target lan-   guages or for languages without enough resources   for pretraining . Thus , models that can be applied or   at least easily adapted to unseen languages are an   important research focus . This work addresses the   following research questions ( ) , using English   as the source language for finetuning .   RQ1 : How does the number of pretraining lan-   guages influence zero - shot cross - lingual transfer   of simple NLP tasks on unseen target languages ?   We first explore how many languages a model   should be pretrained on if the target language is   unknown at test time or has too limited monolin-   gual resources for model adaptation . On one hand ,   we hypothesize that increasing the number of pre-   training languages will improve performance , as   the model sees a more diverse set of scripts and   linguistic phenomena . Also , the more pretraining   languages , the better chance of having a related   language to the target language . However , multi-   lingual training can cause interference : other lan-   guages could distract from English , the finetuning   source language , and thus , lower performance.1501RQ2 : How does the answer to RQ1 change with   model adaptation to the target language ?   This question is concerned with settings in which   we have enough monolingual data to adapt a pre-   trained model to the target language . Like our   hypothesis for1 , we expect that having seen   more pretraining languages should make adapta-   tion to unseen target languages easier . However ,   another possibility is that adapting the model makes   any languages other than the finetuning source lan-   guage unnecessary ; performance stays the same or   decreases when adding more pretraining languages .   RQ3 : Do the answers to RQ1 change if all pre-   training languages are related to each other ?   We use a diverse set of pretraining languages   when exploring1 , since we expect that to be   maximally beneficial . However , the results might   change depending on the exact languages . Thus ,   as a case study , we repeat all experiments using a   set of closely related languages . On the one hand ,   we hypothesize that benefits due to adding more   pretraining languages ( if any ) will be smaller with   related languages , as we reduce the diversity of   linguistic phenomena in the pretraining data . How-   ever , on the other hand , if English is all we use dur-   ing fine - tuning , performance might increase with   related languages , as this will approximate training   on more English data more closely .   3 Experimental Setup   Pretraining Corpora All our models are pre-   trained on the 2017 Wikipedia dump ( Gin-   ter et al . , 2017 ) . To use equal amounts of data   for all pretraining languages , we downsample   all Wikipedia datasets to an equal number of se-   quences . We standardize to the smallest corpus ,   Hindi . The resulting pretraining corpus size is   around 200 MB per language . We hold out 1 K   sequences with around 512 tokens per sequence   after preprocessing as a development set to track   the models ’ performance during pretraining .   Corpora for Model Adaptation For model   adaptation ( RQ2 ) , we select unseen target lan-   guages contained in both ( Conneau et al . ,   2018b ) and Universal Dependencies 2.5 ( Nivre   et al . , 2019 ): Farsi ( ) , Hebrew ( ) , French   ( ) , Vietnamese ( ) , Tamil ( ) , and Bulgar-   ian ( ) . Model adaptation is typically done for   low - resource languages not seen during pretraining   because monolingual corpora are too small ( Wang   et al . , 2020a ) . Therefore , we use the Johns Hopkins   University Bible corpus by McCarthy et al . ( 2020 )   following Ebrahimi and Kann ( 2021 ) .   Tasks We evaluate our pretrained models on the   following downstream tasks from the   dataset ( Hu et al . , 2020):tagging and . For   the former , we select 29 languages from Universal   Dependencies v2.5 ( Nivre et al . , 2019 ) . For the   latter , we use all fifteen languages in ( Con-   neau et al . , 2018b ) . We follow the default train ,   validation , and test split in .   Models and Hyperparameters Following Con-   neau et al . ( 2020 ) ’s -Base model , we train   transformers ( Vaswani et al . , 2017 ) with 12 lay-   ers , 768 units , 12 attention heads , and a maximum   of 512 tokens per sequence . To accommodate all1502   languages and facilitate comparability between all   pretraining setups , we use - ’s vocabulary and   the SentencePiece ( Kudo and Richardson , 2018 )   tokenizer by Conneau et al . ( 2020 ) .   We use masked language modeling ( ) as our   pretraining objective and , like Devlin et al . ( 2019 ) ,   mask 15 % of the tokens . We pretrain all models   for 150 K steps , using Adam W ( Loshchilov and   Hutter , 2019 ) with a learning rate of 1×10and   a batch size of two on either NVIDIA RTX2080Ti   or GTX1080Ti 12 GB , on which it approximately   took four days to train each model . When pretrain-   ing , we preprocess sentences together to generate   sequences of approximately 512 tokens . For contin-   ued pretraining , we use a learning rate of 2×10   and train for forty epochs , otherwise following the   setup for pretraining . For finetuning , we use a learn-   ing rate of 2×10and train for an additional ten   epochs for bothtagging and , and an ad-   ditional five epochs for , following Hu et al .   ( 2020 ) .   Languages Table 1 shows the languages used in   our experiments . English is part of the pretraining   data of all models . It is also the finetuning source   language for all tasks , following Hu et al . ( 2020 ) .   We use two different sets of pretraining languages :   “ Diverse ( Div ) ” and “ Related ( Rel ) ” ( Table 2 ) . We   mainly focus on pretraining on up to five languages ,   except fortagging where the trend is not clear   and we further experiment on up to ten .   Fortagging and , we regard seventeen   of the twenty - nine languages available in   asunseen , while the remaining twelve languages   are pretraining languages for at least one model .   For , six languages are seen and the rest are   unseen . The order in which we add pretraining   languages follows the size of their original   2017 Wikipedia dumps , with larger sizes being   added first .   4 Results   We now present experimental results for each RQ .   4.1 Findings for RQ1   POS Tagging Figure 1 shows the tagging   accuracy averaged over the 17 languages unseen   during pretraining . On average , models pretrained   on multiple languages have higher accuracy on   unseen languages than the model pretrained exclu-   sively on English , showing that the model benefits   from a more diverse set of pretraining data . How-   ever , the average accuracy only increases up to six   languages . This indicates that our initial hypothesis   " the more languages the better " might not be true .   Figure 2 provides a more detailed picture , show-   ing the accuracy for different numbers of pretrain-   ing languages for all seen and unseen target lan-   guages . As expected , accuracy jumps when a lan-   guage itself is added as a pretraining language . Fur-   thermore , accuracy rises if a pretraining language   from the same language family as a target language   is added : for example , the accuracy of Marathi   goes up by 9.3%after adding Hindi during pre-   training , and the accuracy of Bulgarian increases   by31.2%after adding Russian . This shows that   related languages are indeed beneficial for transfer   learning . Also , ( partially ) sharing the same script   with a pretraining language ( e.g. ,and ,   and ) helps with zero - shot cross - lingual transfer   even for languages which are not from the same1503   family . These results are consistent with the out-   come of Müller et al . ( 2020 ) and partially support   the hypothesis by Pires et al . ( 2019 ) that shared   scripts are effective on unseen languages .   But how important are the scripts compared to   other features ? To quantify the importance of it ,   we conduct a linear regression analysis on the   tagging result . Table 3 shows the linear regression   analysis results using typological features among   target and pretraining languages . For the script   and family features , we follow Xu et al . ( 2019 )   and encoded them into binary values set to one if   a language with the same script or from the same   family is included as one of the pretraining lan-   guages . For syntax and phonology features , we de-   rive those vectors from the URIEL database using   lang2vec ( Littell et al . , 2017 ) following Lauscher   et al . ( 2020 ) . We take the maximum cosine simi-   larity between the target language and any of the   pretraining languages . Table 3 further confirms   that having a pretraining language which shares   the same script contributes the most to positive   cross - lingual transfer .   We sadly can not give a definitive optimal num-   ber of pretraining languages . One consistent find-   ing is that , for the large majority of languages , us-   ing only English yields the worst results for unseen   languages . However , adding pretraining languages   does not necessarily improve accuracy ( Figure 1 ) .   This indicates that , while we want more than one   pretraining language , using a smaller number than   the 100 commonly used pretraining languages is   likely sufficient unless we expect them to be closely   related to one of the potential target languages .   NER Our results show a similar trend .   Therefore , we only report the average performance   in the main part of this paper ( Figure 3 ) , and full1504   details are available in Appendix A. For , trans-   fer to unseen languages is more limited , likely due   to the small subset of tokens which are labeled as   entities when compared totags .   NLI Ourresults in Figure 4 show a similar   trend : accuracy on unseen languages plateaus at a   relatively small number of pretraining languages .   Specifically , Div-4 has the highest accuracy for 8   target languages , while Div-5 is best only for two   target languages . Accuracy again increases with   related languages , such as an improvement of 3.7 %   accuracy for Bulgarian after adding Russian as a   pretraining language . Full results are available in   Appendix B.   4.2 Findings for RQ2   POS Tagging Figure 5a shows thetagging   results for six languages after adaptation of the   pretrained models via continued pretraining . As   expected , accuracy is overall higher than in Fig-   ure 2 . Importantly , there are accuracy gains in   Farsi when adding Turkish ( +9.8 % ) and in He-   brew when adding Greek ( +7.7 % ) , which are not   observed before adapting models . We further in-   vestigate it in Section 5 .   NER results in Figure 5b show similarities   betweentagging ( e.g. , improvement on Bulgar-   ian after adding Russian ) . However , there is limited   improvement on Farsi after adding Arabic despite   partially shared scripts between the two languages .   This indicates that the effect of adding related pre-   training languages is partially task - dependent .   NLI For , accuracy increases slightly after   adding a second pretraining language . Results for   two to five pretraining languages are similar for all   target languages and , for Greek and Turkish , still   similar to the English - only model . This indicates   that , similar to our findings fortagging , a few   pretraining languages could be sufficient for model   adaptation . Full results are available in Appendix B.   Finally , ourresults are low overall . This is   likely due to the size of the pretraining corpus being   one of the top correlated features for(Lauscher1505   et al . , 2020 ) , unlike for tagging ( Hu et al . ,   2020 ) .   4.3 Findings for RQ3   POS Tagging In contrast to1 , tagging   accuracy changes for most languages are limited   when increasing the number of pretraining lan-   guages ( Figure 6 ) . The unseen languages on which   we observe gains belong to the Germanic , Ro-   mance , and Uralic language families , which are   relatively ( as compared to the other language fami - lies ) close to English . The accuracy on languages   from other language families changes by < 10 % ,   which is smaller than the change for a diverse set   of pretraining languages . This indicates that the   models pretrained on similar languages struggle to   transfer to unrelated languages .   NER F1 scores of , Rel-2 , Rel-3 , Rel-4 , and   Rel-5 are .218 , .219 , .227 , .236 , and .237 respec-   tively . Compared to Div - X , pretraining on related   languages also improves up to adding five lan-   guages . However , these models bring a smaller   improvement , similar totagging .   NLI Figure 7 shows a similar trend for :   when adding related pretraining languages , accu-   racy on languages far from English either does not   change much or decreases . In fact , for nine out of   thirteen unseen target languages , Rel-5 is the worst .   5 More Pretraining Languages   Our main takeaways from the last section are :   ( 1 ) without model adaptation , increasing the   number of pretraining languages does not improve   accuracy on unrelated unseen target languages ;   ( 2 ) model adaptation largely helps exploiting   models pretrained on more languages ; and ( 3)1506   when using more than one pretraining language ,   diversity is important .   However , there are limitations in the experimen-   tal settings in Section 4 . We assume the follow-   ing : ( 1 ) relatively small pretraining corpora ; ( 2 )   the target languages are included when building   the model ’s vocabulary ; ( 3 ) fixed computational   resources ; and ( 4 ) only up to ten pretraining lan-   guages . We now explore if our findings for1   and2 hold without such limitations . For this , we   use two publicly available pretrained models   ( Lample and Conneau , 2019 ) , which have been pre-   trained on full size Wikipedia in 17 ( -17 ) and   100 ( -100 ) languages , and -base model   trained on a larger Common Crawl corpus ( Con-   neau et al . , 2020 ) in 100 languages . We conduct a   case study on low - resource languages unseen for   all models , including unseen vocabularies : Maltese   ( ) , Wolof ( ) , Yoruba ( ) , Erzya ( ) , and   Northern Sami ( ) . All pretraining languages   used in Div - X are included in -17 except for   Finnish , and all 17 pretraining languages for -   17 are a subset of the pretraining languages for -100 . We report the averages with standard   deviations from three random seeds .   5.1 Results   RQ1 For models without adaptation , accuracy   does not improve for increasing numbers of source   languages ( Figure 8a ) . Indeed , the accuracy on   both -17 and -100 are on par even though   the former uses 17 pretraining languages and the   latter uses 100 . One exception is Northern Sami   ( Uralic language with Latin script ) due to -   17 not seeing any Uralic languages , but -100does during pretraining . When further comparing   Div-10 and -17 , increase in accuracy by ad-   ditional pretraining languages is limited . Erzya   remains constant from five to 100 languages ( ex-   cept for - ) , even when increasing the pretrain-   ing corpus size from downsampled ( Div - X ) to full   Wikipedia ( -17 and -100 ) .   RQ2 For the models with adaptation ( Figure 8b ) ,   there is a significant gap between -17 and -   100 . This confirms our findings in the last section :   more pretraining languages is beneficial if the pre-   trained models are adapted to the target languages .   Thus , a possible explanation is that one or more   of -100 ’s pretraining languages is similar to   our target languages and such languages can only   be exploited through continued pretraining ( e.g. ,   Ukrainian included in -100 but not in Div - X ) .   Therefore , having the model see more languages   during pretraining is better when the models can   be adapted to each target language .   6 Related Work   Static Cross - lingual Word Embeddings Static   cross - lingual word embeddings ( Mikolov et al . ,   2013 ; Conneau et al . , 2018a ) embed and align   words from multiple languages for downstream   NLP tasks ( Lample et al . , 2018 ; Gu et al . , 2018 ) ,   including a massive one trained on 50 + lan-   guages ( Ammar et al . , 2016 ) . Static cross - lingual   embedding methods can be classified into two   groups : supervised and unsupervised . Supervised   methods use bilingual lexica as the cross - lingual   supervision signal . On the other hand , pretrained   multilingual language models and unsupervised1507cross - lingual embeddings are similar because they   do not use a bilingual lexicon . Lin et al . ( 2019 )   explore the selection of transfer language using   both data - independent ( e.g. , typological ) features ,   and data - dependent features ( e.g. , lexical overlap ) .   Their work is on static supervised cross - lingual   word embeddings , whereas this paper explores pre-   trained language models .   Analysis of Pretrained Multilingual Models   on Seen Languages Starting from Pires et al .   ( 2019 ) , analysis of the cross - lingual transferabil-   ity of pretrained multilingual language models has   been a topic of interest . Pires et al . ( 2019 ) hy-   pothesize that cross - lingual transfer occurs due   to shared tokens across languages , but Artetxe   et al . ( 2020 ) show that cross - lingual transfer can be   successful even among languages without shared   scripts . Other work investigates the relationship   between zero - shot cross - lingual learning and typo-   logical features ( Lauscher et al . , 2020 ) , encoding   language - specific features ( Libovický et al . , 2020 ) ,   and ’s multilinguality ( Dufter and Schütze ,   2020 ) . However , the majority of analyses have   either been limited to large public models ( e.g. , , - ) , to up to two pretraining languages   ( K et al . , 2020 ; Wu and Dredze , 2020 ) , or to target   languages seen during pretraining . One exception   is the concurrent work by de Vries et al . ( 2022 )   on analyzing the choice of language for the task-   specific training data on unseen languages . Here ,   we analyze the ability of models to benefit from an   increasing number of pretraining languages .   7 Conclusion   This paper explores the effect which pretraining   on different numbers of languages has on unseen   target languages after finetuning on English . We   find : ( 1 ) if not adapting the pretrained multilingual   language models to target languages , a set of di-   verse pretraining languages which covers the script   and family of unseen target languages ( e.g. , 17 lan-   guages used for -17 ) is likely sufficient ; and   ( 2 ) if adapting the pretrained multilingual language   model to target languages , then one should pretrain   on as many languages as possible up to at least 100 .   Future directions include analyzing the effect of   multilingual pretraining from different perspectives   such as different pretraining tasks and architectures ,   e.g. , mT5 ( Xue et al . , 2021 ) , and more complex   tasks beyond classification or sequence tagging . Acknowledgements   We sincerely thank the reviewers for their construc-   tive and detailed feedback . We also thank the mem-   bers of University of Colorado Boulder ’s NALA   group , especially Abteen Ebrahimi for providing   the code and Stéphane Aroca - Ouellette for giving   feedback on an early draft . Boyd - Graber is sup-   ported by ODNI , IARPA , via the BETTER Pro-   gram contract 2019 - 19051600005 . The views and   conclusions contained herein are those of the au-   thors and should not be interpreted as necessarily   representing the official policies , either expressed   or implied , of ODNI , IARPA , or the U.S. Gov-   ernment . The U.S. Government is authorized to   reproduce and distribute reprints for governmental   purposes notwithstanding any copyright annotation   therein .   References150815091510   A Results   We show additional experimental results on in   Figures 9 and 10 .   BResults   Tables 5 and 6 shows the results without model   adaptation , and Table 4 shows the full results with   model adaptation .   C Notes on the Experimental Setup for   Model Adaptation   Following are the additional notes on the setup of   the model adaptation :   •No vocabulary augmentation is conducted un-   like Wang et al . ( 2020a ) . We use - ’s   vocabulary throughout all experiments in this   paper .   •The Bible is used instead of Wikipedia for the   continued pretraining or model adaptation to   minimize the corpus size and contents incon-   sistency across languages.15111512