  Rohit Sridhar   School of Interactive Computing   Georgia Institute of Technology   rsridhar37@gatech.eduDiyi Yang   School of Interactive Computing   Georgia Institute of Technology   dyang888@gatech.edu   Abstract   Warning : This paper contains content that is   offensive and may be upsetting .   Biased or toxic speech can be harmful to var-   ious demographic groups . Therefore , it is   not only important for models to detect these   speech , but to also output explanations of why   a given text is toxic . Previous literature has   mostly focused on classifying and detecting   toxic speech , and existing efforts on explaining   stereotypes in toxic speech mainly use standard   text generation approaches , resulting in generic   and repetitive explanations . Building on these   prior works , we introduce a novel knowledge-   informed encoder - decoder framework to utilize   multiple knowledge sources to generate impli-   cations of biased text . Experiments show that   our knowledge informed models outperform   prior state - of - the - art models significantly , and   can generate detailed explanations of stereo-   types in toxic speech compared to baselines ,   both quantitatively and qualitatively .   1 Introduction   The toxic speech detection and classification prob-   lem has seen increasing interest in recent years .   However , it is not only important for AI agents   to recognize and classify toxic speech , but to also   explain why it is toxic . For instance , debiasing   methods that use information about toxic language   may benefit from additional information given by   detailed explanations of toxicity in text ( Ma et al . ,   2020 ) . Furthermore , detailed explanations of tox-   icity may facilitate human interaction with toxic-   ity detection systems ( Rosenfeld and Richardson ,   2019 ) . They can also help humans who work with   toxicity classifiers use more information about the   input when making decisions about toxic speech .   To elucidate , consider the following offensive joke :   “ What type of punch do you use against a kinder-   gartener ? A sandy - hook . " . While the literal text is   not toxic , the implied meaning is offensive , partic-   ularly to those affected by school shootings . An AIagent capable of generating the implied meaning   could thus provide additional information to down-   stream actors . Note that , we use the term biased   andtoxic interchangeably in this work .   Existing work largely addresses the problem of   detecting andclassifying toxic speech ( Waseem   and Hovy , 2016 ; Founta et al . , 2018 ; Davidson   et al . , 2017 ) . As mentioned earlier , explanations   of toxicity can help with downstream tasks such   as debiasing or decision making by humans , thus   there has been increasing demand for explainable   machine learning classifiers ( Ribeiro et al . , 2016 ;   Došilovi ´ c et al . , 2018 ) . Recent work around ex-   plainable toxicity classification introduced Social   Bias Frames ( Sap et al . , 2020 ) , a formal framework   which combines explanations of toxicity along with   toxicity classifications along multiple dimensions .   However the explanations generated from the cur-   rent state - of - the - art methods tend to be generic ,   without much detail . For instance , explanations   may focus on certain toxic components of the input   but ignore others , or include irrelevant stereotypes   about the minority group affected .   To fill this gap , our work proposes to leverage   different types of knowledge to provide rich context   and background for toxicity explanation . Specif-   ically , we introduce a novel framework to utilize   three distinct knowledge sources . Prior work ( Yu   et al . , 2022 ) divides knowledge broadly into inter-   nal and external knowledge , where internal knowl-   edge is knowledge embedded in the input text , and   external knowledge is derived from sources outside   the input . Building upon these , we leverage expert   knowledge that comes from high - quality expert   annotations of the input , and explicit knowledge   from knowledge graphs and bases , as such sym-   bolic knowledge can provide relevant information   to the output text ( Yu et al . , 2022 ; Mou et al . , 2016 ) .   While knowledge graphs and bases deterministi-   cally retrieve and restructure knowledge from raw   text sources , large pretrained generative models are811   found to be effective in outputting useful knowl-   edge in a probabilistic manner , complementing the   expert and explicit knowledge ( Razniewski et al . ,   2021 ) . To this end , we also include implicit knowl-   edge models which source knowledge from large   pretrained text generation models . We further build   a family of mixture models , MGEN , to synthe-   size knowledge from all three sources , as shown   in Figure 1 . To sum up , our contributions are two-   fold : ( 1 ) We leverage three different sources of   knowledge , and further combine them using simple   yet effective mixture models to explain toxic text .   ( 2 ) We show that our models outperform prior state-   of - the - art baselines and generate more detailed ex-   planations .   2 Related Work   Prior work on knowledge enhanced text generation   ( Yu et al . , 2020 ) can be viewed across two different   knowledge sources .   2.1 Internal Knowledge   Internal knowledge includes knowledge that is   available within the input . For instance , Latent   Dirichlet Allocation ( LDA ) ( Blei et al . , 2003 ) can   learn topics from inputs , which can then be incorpo-   rated into text generation models ( Cao et al . , 2015 ;   Guo et al . , 2020 ) . Keywords may also be extracted   from input text using techniques like TF - IDF , PMI   or independent classifiers . In one such work , Song   et al . ( 2019 ) extract emotion oriented keywords us-   ing an independent emotion classifier to enhance   dialogue generation . Similarly , Mou et al . ( 2016 )   use PMI to find relevant keywords for short text   conversation . Forbes et al . ( 2020 ) develop concep-   tual formalisms that rely on annotations about the   input to generate text . In this work , we denote the   use of independent annotations on input to enhancetext generation as expert knowledge , as such an-   notations often come from human experts .   2.2 External Knowledge   Knowledge graphs and bases are commonly used as   a form of external knowledge . Zhang et al . ( 2019 )   use knowledge graph embeddings to model con-   versation flow , while Guan et al . ( 2020 ) use triples   extracted from knowledge graphs to enhance story   generation . Finally , Lian et al . ( 2019 ) develop   probabilistic mechanisms to select knowledge from   knowledge bases for response generation . Knowl-   edge from conventional sources are determinsti-   cally created , in that they simply restructure raw   text and store them in a knowledge base or graph .   We refer to this type of external knowledge as ex-   plicit knowledge . On the other hand , there has   been increasing interest in the use of large pre-   trained generative models as a source of knowl-   edge . Heinzerling and Inui ( 2021 ) argue that large   pretrained models can in fact serve as knowledge   bases , while Davison et al . ( 2019 ) argue that pre-   trained models can accurately assess the validity of   knowledge mined from raw text . While pretrained   models are also trained on raw texts , similar to   knowledge bases and graphs , they draw from this   knowledge probabilistically and thus are a distinct   approach . We denote this type of external knowl-   edge as implicit knowledge .   2.3 Toxic Text Understanding   Prior work around toxicity understanding mainly   focuses on detection ( Schmidt and Wiegand , 2017 ) .   Early approaches include using n - grams ( Waseem   and Hovy , 2016 ; Sood et al . , 2012 ; Perera and Fer-   nando , 2021 ) as well as word clustering ( Xiang   et al . , 2012 ; Zhong et al . , 2016 ) . Recently , knowl-   edge enhanced approaches have also been used   for toxicity detection . For instance , Dinakar et al .   ( 2012 ) use ConceptNet to detect anti - LGBT bul-   lying . The use of meta - information , such as infor-   mation about the user ( Dadvar et al . , 2012 ) , has   proven to be useful , depending on the type of in-   formation used . Sap et al . ( 2020 ) use Social Bias   Frames to produce both toxicity classifications and   explanations of toxicity . Similarly , our approach   attempts to explain toxicity by leveraging different   sources of knowledge to provide more context and   grounding for the models to generate explanations .   Different from many prior works , we synthesize   these diverse knowledge sources in a unified frame-   work to utilize the unique contribution from each812individual knowledge source .   3 Knowledge Enhanced MG   This section presents our selected three different   types of knowledge — expert knowledge , explicit   knowledge andimplicit knowledge , and our M-   Gmodels for toxicity explanation .   3.1 Expert Knowledge   Expert knowledge is sourced from annotations of   the input . For instance , in the Social Bias Frames   dataset ( Sap et al . , 2020 ) , such expert knowledge   include human judgements towards the lewdness ,   offensiveness , intent to offend , and group targeted   categories . This type of expert knowledge provides   useful insights and heuristics for the toxicity expla-   nation task , if they are available .   We incorporate expert knowledge into the gener-   ation process using the join embedding technique   ( Pryzant et al . , 2020 ) along with toxicity classifi-   cation models . The join embedding architecture   uses attention weights from the toxicity classifiers   to inform the text generation model about parts   of the input post relevant to toxicity classification ,   thus providing a heuristic for the related toxicity   explanation task . Formal details of the architecture   can be found in Appendix A.1 .   We denote these models with the naming conven-   tion , “ E [ F ] " , where “ [ F ] "   is the categorical variable we use for the join em-   bedding . We replace “ [ F ] " with “ A "   when we train on all features .   3.2 Explicit Knowledge   Explicit knowledge is sourced from some knowl-   edge base or graph . Common sources include Con-   ceptNET , DBpedia , WikiData , etc . ( Auer et al . ,   2007 ; Vrande ˇci´c and Krötzsch , 2014 ) . We opt   to use ConceptNet ( Speer et al . , 2017 ) , since it   contains commonsense knowledge ( Liu and Singh ,   2004 ) . Commonsense knowledge incorporates ev-   eryday concepts , especially knowledge regarding   social groups and situations .   Following Chang et al . ( 2020 ) , given a BART   model and an input , we extract ranked triples re-   lated to the input post and keep the top ktriples per   post where we vary the k∈ { 3,5,10,15,20,25 } .   We experiment with both concatenation and atten-   tion based methods to incorporate the top ktriples ,   but settle on a concatenation based approach due   to its simplicity and the lack of performance gainsfrom the attention based approach . Results and   analysis for both the concatenation and attention   based approaches are provided in Appendix 12 . We   denote these models with the naming convention ,   “ E ( ) " , wheredenotes the number of   triples used .   3.3 Implicit Knowledge   Implicit knowledge is obtained from some text gen-   erator , such as a large pretrained generative model .   Prior work such as Heinzerling and Inui ( 2021 ) ,   argue that large pretrained models can in fact serve   as knowledge bases . Implicit knowledge grants   models a probabilistic view of external raw text   sources related to a given scenario or input , since   generative models tend to generate based on sta-   tistical correlations found in their training corpora   ( Razniewski et al . , 2021 ) .   To use implicit knowledge , we first train a BART   model to generate the target minority group from   the input post . Following Sheng et al . ( 2019 ) , we   use the predicted target minority corresponding   to each input post and a set of prompts to induce   biased prompt completions from GPT models . We   may use multiple prompts per input post , where   the number of prompt completions generated is   governed by a hyperparameter , k. Then we train an   independent BART model to generate these biased   prompt completions , given the origin input post .   This BART model is then retrained to produce the   implied stereotype , given the input post . Again , we   provide a formal description in Appendix A.3 .   We denote these models with “ I   [ GPT|GPT-2 ] ( ) " , where “ GPT " and “ GPT-2 "   correspond to the model used for prompt comple-   tion , whilecorresponds to the number of biased   prompt completions generated per input post .   3.4 MGEN Models   We introduce a simple and effective approach to   combine all three knowledge sources as input for   ourMGEN family of models , and generate   the final stereotype by integrating complementary   knowledge from these sources . In our design , we   take inspiration from Mixture of Experts models   ( Masoudnia and Ebrahimpour , 2012 ) , which com-   bine base expert model outputs into a final output   using a gating mechanism . Here , we rely on at-   tention mechanisms over the knowledge informed   model outputs to serve as the gating mechanism .   We build two variants . The first variant is called   MGEN C which uses concatenation to813   combine outputs from the knowledge informed   models , as shown in Figure 2 . The second vari-   ant is called MGEN M V which uses   views to perform self attention over outputs of   the knowledge informed models ( Chen and Yang ,   2020 ) . Since the BART model already uses self   attention over input tokens , we experiment with   the MultiView architecture to see whether the addi-   tional self attention mechanisms of the MultiView   model causes changes in performance .   ForMGEN C , suppose we have k   trained models , M , . . . , M , each trained to   produce the implied stereotype given the in-   put post , and each informed by one of the   aforementioned knowledge types . We con - catenate the outputs of each knowledge based   model , M , to produce a new input string .   Thus if each model Moutputs “ s " , we   get the following concatenated input string :   “ s[SEP ] s···[SEP ] s " . Now ,   letMbe a standard pretrained BART model . We   train model Mto produce the implied stereotype   using “ s[SEP ] s···[SEP ] s "   as input . Note that the knowledge based models ,   M , . . . , Mare fixed when training M. Model M   serves as the final MGEN C model .   MGEN M V uses the MultiView ar-   chitecture proposed by Chen and Yang ( 2020 ) . In   this case , the outputs of M , . . . , Mare treated   as separate views . If each model Moutputs   “ s " given the input post , then for each   model Mwe configure the corresponding view   as the string “ vs[SEP ] · · · vsv ·   [ SEP ] s " , where v , v , andvare view   tokens . Here , vis always the first token in the   view string and vandvsurround M ’s output .   We configure ksuch views ( one for each model )   and pass each into the BART MultiView model as   a set of views corresponding to the original input   post . The BART MultiView model is then trained   to produce the corresponding output stereotype .   For details on the MultiView architecture , please   see Chen and Yang ( 2020 ) .   3.5 Training   We utilize the BART encoder decoder framework   throughout ( Lewis et al . , 2020 ) . We use batch gra-   dient descent when training . For a batch Bwith   padded input sequences Xof length Nand corre-   sponding padded target sequences Yof length N ,   along with knowledge Kfrom some knowledge814   source , we minimize cross entropy loss :   L=−1   |B|N·/summationdisplay / summationdisplaylogp(Y|Y , X , K)(1 )   4 Experimental Setup   4.1 Dataset   We conduct our experiments on the SBIC dataset   ( Sap et al . , 2020 ) and the Implicit Hate dataset   ( ElSherief et al . , 2021 ) . The SBIC dataset contains   an input post , toxicity annotations and free text an-   notations of the implied stereotype . We work with   the input post and the the implied stereotype . The   Implicit Hate dataset ( ElSherief et al . , 2021 ) con-   tains free text annotations of the implied stereotype .   Dataset statistics are provided in Table 2 .   4.2 Baselines   We compare our models with BART , and state of   the art baselines from Sap et al . ( 2020 ):   •GPT : Following Sap et al . ( 2020 ) , we train   the GPT pretrained model from huggingface   to generate the toxicity classifications , the Tar-   get Minority , and the Implied Stereotype as a   string , when prompted with the input post .   •GPT-2 : We train with the same setting as the   GPT Baseline , but use the GPT-2 pretrained   model from huggingface .   •BART : We train a standard pretrained BART   model to generate the implied stereotype   when given the input post .   4.3 Evaluation Metrics   We use BLEU ( Papineni et al . , 2002 ) , ROUGE - L   ( Lin , 2004 ) and BERTScore ( Zhang * et al . , 2020 )   to evaluate our models and take the maximum score   for each hypothesis over all of the corresponding   references . We use BERTScore since it looks forsemantic similarity , unlike the other two metrics .   While we acknowledge it as a limitation , we ulti-   mately do not use human evaluation for multiple   reasons . First , the generated stereotypes are min-   imal in length compared to other text generation   tasks . Moreover , we perform manual analyses of   the results in Section 5 . Second , we are following   precedent set by prior work ( Sap et al . , 2020 ) . Fi-   nally , we want to minimize annotator exposure to   harmful content .   4.4 Results on SBIC   Our results on both dev and test are described in   Table 3 . Here we focus on dev since both sets of   results track similar trends . We observe that the   MGEN models outperform all other models . Af-   terMGEN , the model using Implicit Knowledge   sources perform best . These are followed by the   model using Explicit Knowledge , in turn followed   by model using Expert Knowledge .   Both models with explicit and with implicit   knowledge outperform expert language models .   The models using implicit knowledge tend to per-   form best overall ( BLEU : from 0.650 to0.683 ,   ROUGE - L : from 0.624 to0.659 , BERTScore :   from 0.759to0.800 ) . This is likely because im-   plicit knowledge is less structured and hence easier   to induce bias from ( Petroni et al . , 2019 ) . On the   other hand , while our source ( ConceptNET ) for   explicit knowledge may be biased ( Mehrabi et al . ,   2021 ) , retrieved stereotypes are often mixed with   general , unbiased facts .   TheMGEN models outperform every other   model . This makes intuitive sense since MGEN   synthesizes multiple types of knowledge . Unex-   pectedly , MGEN M V model does not   improve performance ( the absolute difference is   within 0.002 across all scores ) over MGEN   C . This is likely due to the fact that the   MultiView model was intended to capture meta-   sequences in text ( Chen and Yang , 2020 ) , whereas   in our setting the input is not sequential . We also   note that the MGEN models perform better than   the source models , despite their input being sourced   from the source models . Thus , despite differing   performance , models from different knowledge   sources are likely providing some distinct and com-   plementary information .   4.5 Results on Implicit Hate Speech Corpus   Results on the implicit hate corpus ( ElSherief et al . ,   2021 ) are given in Table 4 . All models ( including815   baselines ) generally perform worse than they do   on the SBIC dataset . This is likely because the im-   plicit hate corpus contains one reference per post ,   in contrast to the SBIC dataset ( see Table 2 ) . While   the Expert knowledge model performs worse than   the baseline , the other models perform slightly bet-   ter . This is likely because the Expert model relies   on toxicity classifications , which were n’t available   in the implicit hate corpus . We believe our models   can be generalized to text generation tasks on other   datasets , but they likely need multiple reference   points where the implicit hate corpus only has one .   5 Error Analysis and Ablation Studies   We perform analyses and ablation studies on model   results on the SBIC dataset . We do not perform   these on the implicit hate dataset , since we have   too few references per example . Examples of the   error and challenge types below are given in Table   5 . An additional full set of examples for each error   and challenge type is given in Appendix 13 .   5.1 Error Analysis   We categorize the types of errors made by models   on a small sample of 200examples from the dev   set and provide the distributions in Figure 3 . We   provide the error categories below .   1 . Non - Existent Stereotype : Model gener-   ates a stereotype when the reference stereo-   type is an empty string .   2 . Ignores Stereotype : Model does not gener-   ate a stereotype when the reference stereotype   is a non - empty string .   3 . Incorrect Target Minority : Model uses   the incorrect target minority .   4 . Incorrect Stereotype : Model uses the cor-   rect target minority but generates an incorrect   or overly general stereotype.816   The baseline GPT models tend to make more   errors of every type , except that the expert model   makes more errors of type 4 . The expert model   likely focuses on tokens that trigger toxicity clas-   sifications , which makes it less likely to focus on   other relevant tokens . In the second example of   Table 5 , the token “ black " may be triggering the   Expert Model , causing an error of type 4 .   The knowledge enhanced models rarely fail to   generate a stereotype ( type 2 error ) . On the other   hand , they often detect non existent stereotypes   ( type 1 error ) , but less often than the baselines . In   the third example of Table 5 , “ contraceptive " may   be incorrectly triggering the implicit and explicit   models , while the expert model is not triggered .   5.2 Challenges in Stereotype Generation   We categorize the various challenges faced by our   text generation models and provide distributions in   Figure 4 and counts in Appendix 15 . We analyze   the same sample of 200examples from Section 5.1 .   1 . Misunderstands Post : The model funda-   mentally misunderstands the post and gener-   ates an irrelevant stereotype as a result .   2 . High Sensitivity : The model is highly   sensitive to trigger words , which causes it   to detect non - existent stereotypes or generate   stereotypes based on the triggers alone .   3 . Localized Generation : The model focuses   only on parts of the input and generates stereo-   types based on those parts , rather than on the   entire input post.4 . Does not Draw Connections : The model   clearly considers the entire input post , but   does not draw connections between the dif-   ferent parts of the post .   5 . Misunderstands Sarcasm or Irony : The   model takes a more literal interpretation of   a sarcastic or ironic post , causing it to out-   put text that has the opposite meaning of the   reference stereotype .   6 . Ignores Stereotype : The model does not   generate a stereotype despite a non - empty ref-   erence stereotype .   Interestingly , the MGEN model tends to mis-   understand sarcasm and irony at a slightly higher   rate than the other knowledge model types . In the   fourth example of 5 , MGEN and the Implicit   Model take literal interpretations of the input post .   The Implicit Model type has difficulty with drawing   connections over the input ( challenge type 4 ) . An   example is given in the 6th row of Table 5 , where   the model does not draw a connection between the   target minority and the stereotypes present .   5.3 How MGEN Synthesizes Knowledge   Table 5 provides examples of MGEN synthesiz-   ing knowledge across sources . In the third example ,   MGEN produces an empty string even though   two of the more reliable sources ( explicit and im-   plicit knowledge ) produce non empty strings . In   the fifth example , MGEN clearly combines parts   of all the knowledge sources , while in the sixth ex-   ample MGEN produces a more accurate stereo-   type than any of the other models . Thus , it seems   thatMGEN does not simply attempt to copy the   correct model , but actually possess a deeper under-   standing of the knowledge types it synthesizes .   5.4 Implicit Knowledge Ablation Study   Table 6 contains results of ablations on the Implicit   Knowledge models . We vary the amount ( k ) and   source ( GPT and GPT-2 ) of implicit knowledge .   The model which sources knowledge using GPT   only outperforms the model sourcing knowledge   using GPT-2 when k= 3 . When k= 15 , the latter   model sourcing GPT-2 knowledge outperforms the   model sourcing knowledge from GPT . It is possible   that GPT-2 is less biased than GPT , thus benefiting   our model for low k , but that these benefits decrease   askincreases . With greater k , both models have a   greater chance of exhibiting bias.817   5.5 MGEN CONCAT Ablation Study   In Table 7 , we look at ablations on the number   of knowledge informed models , for the MGEN   CONCAT model . Let kbe the number of knowl-   edge informed models . The variant using k= 6   models performs best . Since the models variantswithin each knowledge type only vary by some hy-   perparameter , information gain probably saturates   as the number of models increases ; however per-   formance decreases when k= 9 . Since we tend to   include models with lower performance for larger   k , worse performing models are likely counter pro-   ductive .   6 Limitations   We discuss limitations of our study here . Per the   discussion in Subsection 4.3 , we do not perform   human evaluation of our results for multiple rea-   sons . We do believe this is a limitation , and think   future work may benefit from some form of human   evaluation , while mitigating some of the concerns   mentioned in Subsection 4.3 .   TheMGEN model requires significant com-   putational power . One needs to train models across   knowledge sources , and then train the MGEN   model itself . Future work may alleviate this bur-   den by considering end to end solutions , or more   efficient knowledge retrieval techniques .   Future work could perform an “ in the wild " anal-   ysis , produced by procuring random comments   from the internet and running our proposed models   on these comments , to determine how effective the   models might be in a real world setting . Further   ablations may also provide insight . For instance ,   with respect to our implicit knowledge models , it818may be helpful to remove BART altogether and   instead use GPT for predicting the target minority ,   pretraining on implicit knowledge and for generat-   ing the final stereotype . Finally , it may be helpful   to standardize the incorporation of knowledge into   the models , such that the models using different   knowledge types may be directly compared .   7 Conclusion   In this paper , we propose a novel framework M-   Gto generate the stereotypes present in toxic so-   cial media posts , using multiple knowledge sources .   We categorized three different sources of knowl-   edge and synthesize the sources of knowledge us-   ing the MGEN models . While the knowledge   models perform as well as baselines , models built   on different knowledge types vary in strengths and   weaknesses . For instance , the expert model suffers   from high sensitivity to trigger words , while the   implicit models may not draw connections over   complex inputs . The MGEN models takes this   into account and minimizes the number of exam-   ples on which it has errors and/or faces challenges .   We conclude that mixture and ensemble methods   as simple as concatenation can leverage the com-   plementary nature of distinct knowledge sources to   produce high quality text generations .   Ethical Considerations   Models such as the one proposed in this paper ,   which output toxicity classifications of text or   speech and reasoning behind such classifications   should be used with care . Considerations of al-   gorithmic fairness should be taken into account   ( Corbett - Davies et al . , 2017 ) , as well as cultural   differences ( Oliva et al . , 2020 ) and racial biases   ( Xia et al . , 2020 ) which can lead to misclassifica-   tions of offensiveness . Care should be taken to   avoid political bias in training datasets , when train-   ing these models for deployment purposes ( Wich   et al . , 2020 ) . Finally , concerns about censorship   should be taken seriously ( Heins , 2014 ) .   References819820   A Formal Details for Knowledge   Incorporation   A.1 Expert Knowledge   A.1.1 Incorporating Expert Knowledge using   the Join Embedding Technique   Given the BERT classifier with mattention heads ,   an input with length n , and a BART model with   hidden dimension d , we pass the input to the BERT   classifier to retrieve the attention head outputs of   the last layer , namely a , . . . , a , each in R. We821also pass the input to the BART model and retrieve   the BART encoded input , namely H∈R. Let   v , . . . , vinRbe trainable weight vectors . For   thej - th row vector , hofH , we compute an en-   riched hidden state has follows :   ( h)=h+/summationdisplayav ( 2 )   We then pass the enriched hidden state through the   BART decoder to generate the output stereotype .   To combine knowledge from multiple variables , we   sum the enriched hidden state in Equation ( 2 ) over   each variable .   A.2 Explicit Knowledge   A.2.1 Incorporating Explicit Knowledge using   Concatenation   In this section , we provide formal details on how   we incorporate explicit knowledge from Concept-   NET using concatenation . In order to retrieve the   topktriples ( varying k∈ { 3,5,10,15,20,25 } ) ,   we first extract nouns , verbs , and adjectives from   the input as our query tokens . We then query Con-   ceptNet for triples associated with the query tokens   and sort the triples by the product of the query ’s   IDF weight and the triple ’s edge weight .   We then translate the triples into English ( Robyn   Speer , 2019 ) . For example , if the entities “ car "   and “ vehicle " are connected by the edge relation   “ IsA " , the translation would be “ Car is a vehi-   cle " . We concatenate the translations to the input   post to form a new input . Formally , let the in-   put be “ s " and “ s " be the sentence derived   from the i - th triple . The modified input is then   “ s[SEP ] s [ SEP]···[SEP ] s " . We then   pass the modified to the BART model which gener-   ates the implied stereotype .   The concatenation based approach allows the   model to encode the external knowledge into its   own embedding space .   A.2.2 Incorporating Explicit Knowledge using   Attention   In this section , we provide formal details on how   we incorporate explicit knowledge from Concept-   NET using attention and the fusion layer described   in Chang et al . ( 2020 ) . This is an alternative method   we tried in addition to the method described in Sec-   tion A.2.1 .   We first accumulate the top ktriples ( varying   k∈ { 5,10,20 } ) associated with a post , using themethod described in Appendix A.2.1 . We then   concatenate the numberbatch embeddings ( each of   dimension p ) of the two entities in each of the k   triples vertically to produce a vector in R. We   then horizontally stack the concatenations to pro-   duce a matrix , H∈R. The encoded input   generated by the BART model can be represented   by the matrix H∈R , where nis the input   length and dis the hidden size of the BART model .   We compute knowledge aware attention over the   input as follows :   Q = H∗W+b ( 3 )   K = H∗W+b ( 4 )   V = H∗W+b ( 5 )   H = softmax / parenleftigQK   √   dV / parenrightig   ( 6 )   where W∈R , W , W∈Rand   b , b , b∈R. The knowledge aware matrix is   H∈R. Finally , we concatenate the origi-   nal encoded input and the knowledge aware matrix ,   Hand perform an affine transformation :   H= ( H⊕H)∗W+b ( 7 )   where ⊕denotes column - wise concatenation ,   W∈Randb∈R.H∈Ris the   new hidden state , which fuses the old hidden state   and the new knowledge aware hidden state . His   passed into the BART decoder .   A.3 Implicit Knowledge   A.3.1 Incorporating Implicit Knowledge   using GPT   In this section , we provide formal details on in-   corporating implicit knowledge using a pretrained   GPT models .   LetMbe a BART model trained to predict   the target minority of a given input post . Let the   target minority string predicted by Mbe “ s "   and let “ s " be a prompt . We provide a list   of prompts in Table 8 We then prompt the GPT   models with the following string “ Thess " .   The GPT models complete the prompt with a gen-   erated string , “ s " , so that the final string is   “ Thesss " . For GPT generations , we   choose hyper - parameters based on methods in   Patrick von Platen ( 2020 ) . For each input , we ran-   domly select various prompts to generate ksen-   tences of the form “ Thesss " , varying   k∈ { 3,15 } . Note that if there is no predicted822Prompt   were known for   were described as   were regarded as   were thought of as   were well - known for   were seen as a   have a reputation for   would frequently   worked as   have a job as   have a part - time job as   earned money by   started working as   have various hobbies such as   would regularly engage in   frequently talked about   behaved as though   liked to   target minority , sis the empty string and no   sentence is generated using GPT . In this case , the   input is paired with just the empty string .   We then pair the input with each of the kgener-   ated sentences ( or the empty string if there is no   predicted target minority ) and train a BART model   Mto predict the generated sentences . Model M   is then used as a pretrained model and is retrained   to predict the implied stereotype given the same in-   put . The retrained BART model is our final implied   stereotype generator .   B Implementation Details   B.1 Implementation Details for BART   Encoder Decoder Models   We train our BART models using a learning rate   of5e−6for3epochs with a batch size of 2or   4 , depending on the size of the input . This ex-   cludes the BERT classifier models , whose settings   are given in Appendix B. The BART models have   406 M parameters and all training is done on an   Nvidia TITAN V GPU , with 12 GB memory , a   boost clock speed of 1455 MHz , 640Tensor coresand5120 CUDA cores . Training under this regime   takes approximately 90 - 120 minutes . We remove   all URLs , “ RT " strings , and “ @ " mentions from the   input post . We train these models on just a single   seed and results are reported on just that seed , as   we had limited time to train and test our models .   The baseline GPT-2 and GPT models are trained   for5epochs , as in the original paper by Sap et al .   ( 2020 ) . Following the paper , we perform minimal   preprocessing to the input text before training and   testing and only remove all URLs . During infer-   ence , we pass batches of input from the dev and   test sets to the generate method of the huggingface   BART model class . We use beam search for gener-   ation , with a beam width of 10and a length penalty   of5.0 .   B.2 Implementation Details for BERT   Classifier Models   We train base BERT models , which have 110 M   parameters . Training takes approximately 30 - 40   minutes on the GPUs described in B. We train these   models on just a single seed , as results did not vary   much as the seed varied and we had limited time   to train and test our models .   Model LR Batch Size Epochs   Offensiveness 5e-6 32 2   Intent to Offend 5e-7 32 1   Lewdness 5e-6 32 1   Group Targeted 5e-6 32 2   BERT Classifier Model training settings are   given in Table 9 and Table 10 .   C Further Ablation Studies   In this section , we look at ablation studies con-   ducted on the Expert Knowledge models and the   Explicit Knowledge models .   C.1 Expert Knowledge Ablation Study   Recall that we use the last attention layer of a BERT   classifier with the join embedding architecture in-   troduced in Pryzant et al . ( 2020 ) to enhance the hid-   den states of the BART model performing stereo-   type generation . We perform ablation studies by   replacing the classifier over a few different anno-   tated categories , namely Offensiveness , Intent to823   Offend , Lewdness , and Group Targeted . We also   train a model that uses all of the classifiers . The   results for the ablations are in Table 11 .   It is important to note the relative performance   of the models . The Expert Knowledge model us-   ing the Group Targeted BERT classifiers performs   better than the other single classifier models , and   performs on par with the Expert Knowledge model   leveraging all of the classifiers . It ’s likely that the   tokens used by the BERT model to identify whether   a minority group is targeted aligns closely with the   portions of the encoded input used to generate the   stereotypes . This makes intuitive sense , since the   set of posts targeting some minority group likely   has some stereotype mentioned in the post and vice   versa . The same can not be said for the other cat-   egories . This intuition is further strengthened by   the fact that the Expert Knowledge model using all   the classifiers does not perform much better than   the Expert Knowledge model using just the Group   Targeted classifier . Thus the other classifiers may   be contributing little additional knowledge to the   stereotype generation task .   C.2 Explicit Knowledge Ablation Study   In this section , we specifically discuss the num-   ber of knowledge triples we use when training ex-   plicit knowledge models and note trends in the   BERTScore as kvaries . The results are in Table   12 . We discuss an additional attention based model   not mentioned in the main paper . The detailed   methodology for this model is given in Appendix   A.2.2 .   We note that when concatenated directly to the   input , performance increases as kincreases up to a   point and then starts to decrease . We believe that   this occurs because the usefulness of knowledge   initially increases then decreases as kincreases . In   particular , as kincreases , many of the latter triples   tend to be synonymous with earlier triples or un-   related to the original input . We also note that   incorporating knowledge as attention tends to pro-   duce a better BERTScore , while not performing as   well on other metrics . We suspect that this is due   to the numerbatch embeddings capturing more se-   mantic meaning than the BART embeddings . The   numberbatch embeddings draw from a variety of   sources , in addition to the graph structure itself and   they perform well on benchmarks measuring word   similarity .   D Analysis   We provide a few examples of stereotype genera-   tion , comparing and contrasting the different model   types in Table 13.824   D.1 Error Analysis   Table 14 provides the counts for the Error Analysis .   Clearly , the MGEN models minimize the total   number of errors made , although MGEN may   not have the minimal number of errors in each   category . D.2 Challenges in Stereotype Generation   Table 15 provides the counts for the Challenges   faced by each model . Clearly , the MGEN mod-   els minimize the total number of challenges face ,   although MGEN may not have faced the mini-   mal number of challenges within each category.825Error Type GPT GPT-2 Expert Explicit Implicit MGEN   1 45 46 37 44 30 24   2 8 7 1 0 3 4   3 13 11 11 10 10 8   4 28 37 41 24 27 27   5 106 99 110 122 130 137   Challenge Type GPT GPT-2 Expert Explicit Implicit MGEN   1 8 6 6 4 2 4   2 ( Non - existent Stereotypes ) 44 46 37 44 30 24   2 ( Remainder ) 15 9 33 13 18 11   3 8 19 10 7 8 8   4 7 9 2 5 6 5   5 4 5 1 5 3 7   6 8 7 1 0 3 4   7 106 99 110 122 130 137826