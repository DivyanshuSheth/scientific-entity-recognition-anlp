  Thong Nguyen , Andrew Yates , Ayah Zirikly , Bart Desmet , and Arman CohanUniversity of Amsterdam , Amsterdam , NetherlandsMax Planck Institute for Informatics , Saarbrücken , GermanyJohns Hopkins University , Baltimore , MarylandNational Institutes of Health , Bethesda , MarylandAllen Institute for AI , Seattle , WA   Abstract   Automated methods have been widely used to   identify and analyze mental health conditions   ( e.g. , depression ) from various sources of in-   formation , including social media . Yet , deploy-   ment of such models in real - world healthcare   applications faces challenges including poor   out - of - domain generalization and lack of trust   in black box models . In this work , we pro-   pose approaches for depression detection that   are constrained to different degrees by the pres-   ence of symptoms described in PHQ9 , a ques-   tionnaire used by clinicians in the depression   screening process . In dataset - transfer experi-   ments on three social media datasets , we find   that grounding the model in PHQ9 ’s symptoms   substantially improves its ability to generalize   to out - of - distribution data compared to a stan-   dard BERT - based approach . Furthermore , this   approach can still perform competitively on in-   domain data . These results and our qualitative   analyses suggest that grounding model predic-   tions in clinically - relevant symptoms can im-   prove generalizability while producing a model   that is easier to inspect .   1 Introduction   Given the significance of mental health as a pub-   lic health challenge ( Brådvik , 2018 ) , much work   has investigated approaches for detecting mental   health conditions using social media text ( Yates   et al . , 2017 ; Coppersmith et al . , 2018 ; Shing et al . ,   2020 ; Harrigian et al . , 2021 ) . Such approaches   could be used by at - risk users and their clinicians   to monitor behavioral changes ( e.g. , by monitor-   ing changes in the presence of symptoms related   to depression as treatment progresses . These ap-   proaches generally rely on datasets consisting of   users with self - reported diagnoses ( e.g. , based on   a statement like “ I was just diagnosed with de-   pression ” ) for training and evaluation ( e.g. , Yates   et al . , 2017 ; Cohan et al . , 2018 ) . Despite promis-   ing results on these tasks , related work argues thatassessing depression and suicidal behavior is dif-   ficult in practical settings and even experienced   clinicians frequently struggle to correctly interpret   signals ( Coppersmith et al . , 2018 ) . Furthermore ,   recent work has found that models trained on par-   ticular mental health datasets do not always gen-   eralize to others . Harrigian et al . ( 2020 ) ; Ernala   et al . ( 2019 ) find that systematic , spurious differ-   ences between diagnosed and control users can   prevent trained models from generalizing to even   other , similar social media data . Similarly , outside   the mental health domain , recent work reports that   neural models often struggle to generalize to data   outside their training distribution ( Geirhos et al . ,   2020 ; D’Amour et al . , 2020 ; Harrigian et al . , 2020 ) .   In this work , we explore approaches for con-   straining the behavior of depression detection meth-   ods by the presence of symptoms known to be re-   lated to depression , like mood and sleep issues .   To do so , we develop nine symptom detection   models that correspond to questions present in   PHQ9 , a screening questionnaire that has been clin-   ically validated and commonly used in practical   setting ( Kroenke et al . , 2001 ) . These questions ask   how often the patient has experienced symptoms   from nine symptom groups ( e.g. , how often have   you had “ little interest / pleasure in doing things ? ” ) .   Grounding depression detection in a trusted di-   agnostic tool produces several benefits . From the   perspective of mental health professionals , the out-   put of such model is inherently more reliable than a   black - box model , because classification decisions   are based on the presence of specific symptoms   in specific posts that can be inspected in order to   assess the quality of evidence for a diagnosis . Fur-   ther , we find this improves the model ’s ability to   generalize , which may be due to limiting its ability   to use spurious shortcuts . This strategy is com-   plementary to strategies for reducing temporal and   topical artifacts ( Harrigian et al . , 2020 ) .   Our proposed approach consists of two sim-8446ple yet effective models : a questionnaire model   that detects symptoms from PHQ9 and a depres-   sion detection model . We instantiate both with a   range of methods that are progressively less con-   strained . At one end of the spectrum , the ques-   tionnaire model uses only manually - defined pat-   terns and the depression model makes classifica-   tion decisions by counting how many times these   patterns appear in a user ’s posts . At the oppo-   site end of the spectrum , there is no explicit ques-   tionnaire model and BERT ( Devlin et al . , 2019a )   serves as an unconstrained depression detection   model . In between , we relax the questionnaire   model by training BERT - based symptom classifiers   using the manually - defined patterns , by consider-   ing symptom representations rather than counts ,   and by adding an extra trainable ‘ other ’ symptom .   We find that our constrained models perform   competitively compared to a standard uncon-   strained BERT classifier when trained and eval-   uated on the same dataset , while additionally pro-   viding a model whose behavior can be understood   in terms of relevant symptoms in specific posts .   However , dataset - transfer evaluations demonstrate   substantial degradation in BERT ’s effectiveness . In   this setting , our constrained models outperform the   unconstrained BERT and show improved general-   izability , even across similar datasets .   Our contributions are : ( 1)comprehensive pat-   tern sets for identifying the symptoms in PHQ9   and heuristics for using them to train weakly-   supervised symptom classifiers , ( 2)a range of pro-   gressively less constrained methods for performing   depression detection based on these symptoms , and   ( 3)an extensive evaluation of depression detection   methods . Our implementation is available online .   2 Related Work   Natural language processing methods have been   widely used for automatic mental health assess-   ment . To support automated analyses of men-   tal health related language , a variety of datasets   have been proposed . Coppersmith et al . ( 2014 ) fo-   cused on predicting depressed and PTSD users in   Twitter , whereas Milne et al . ( 2016 ) ; Shing et al .   ( 2018 ) and Zirikly et al . ( 2019 ) aimed to detect   high risk and suicidal users from their ReachOut   and Reddit posts , respectively . Yates et al . ( RSDD ;   2017 ) , Cohan et al . ( SMHD ; 2018 ) , and Wolohanet al . ( 2018 ) investigated identifying depression   and other mental health conditions from Reddit .   Rich bodies of work in this area focused on study-   ing language use and linguistic styles in depressed   users . LIWC ( Tausczik and Pennebaker , 2010 ) has   been one of the most popular tools to characterize   depression language ( Ramirez - Esparza et al . , 2008 ;   De Choudhury et al . , 2013 ) . Similarly to other NLP   domains , the use of contextualized embeddings   has improved the performance of classifiers ( Jiang   et al . , 2020 ; Matero et al . , 2019 ) .   Recent work shows that while such NLP models   achieve promising results , they have poor gener-   alization to new data platforms and user groups ;   For example , Harrigian et al . ( 2020 ) investigated   various factors , including sample size , class imbal-   ance , temporal misalignment ( e.g. , language dy-   namic , linguistic norms ) , deployment latency , and   self - disclosure bias that may cause performance   degradation when a model is transferred to a new   dataset or domain . The issues can occur even when   datasets appear to be similar , such as when Reddit-   based datasets employ different rules for selecting   diagnosed and control users . Another problem is   the black - box nature of model predictions which is   a major hurdle in deploying AI models in clinical   practice ( Mullenbach et al . , 2018 ) . In this work   we aim at reducing this problem by proposing to   ground depression assessment in a clinical ques-   tionnaire for measuring severity of depression .   Others have considered making predictions more   explainable in the mental health domain . Amini   and Kosseim ( 2020 ) focused on leveraging a user-   level attention mechanism for detecting signs of   anorexia in social media profiles . Our method dif-   fers from theirs in that the explanations are the re-   sults of the analysis of the attention weights , while   our approaches ground model predictions in a well-   established clinical instrument .   Outside of our work , we are aware of two   datasets that incorporate questionnaire information   such as PHQ9 for identifying depression . The most   recent eRisk shared task ( Losada et al . , 2019 ) re-   lies on the Beck Depression Inventory ( BDI ) , a   21 - item questionnaire that assesses level of depres-   sion based on the presence of feelings like sadness ,   pessimism , etc . Models are built to estimate the   user - level BDI score at given time frames . Our   approach differs in that we use PHQ9 and evalu-   ate item scores at the post level , which grounds   predictions in the presence of clinically - relevant8447   symptoms . In eRisk , a sum of BDI scores is the   modeled outcome ( corresponding to our baseline   pattern - based ( threshold ) classifier ) . We use user-   level labels for evaluating depression status and   evaluate how constraining on PHQ9 symptoms af-   fects the user - level classification performance .   Delahunty et al . ( 2019 ) used a deep neural net-   work to predict PHQ4 scores using clinical data   that contains patients ’ PHQ4 scores ( Gratch et al . ,   2014 ) . Our work does not require access to PHQ   labeled clinical data , which can be hard to obtain   at scale . Furthermore , Delahunty ’s approach gen-   eralizes poorly to social media data . Rinaldi et al .   ( 2020 ) predict depression based on screening in-   terviews that rely on PHQ9 categories . In their   setting , PHQ9 is a channel to retrieve the depressed   label , but is not used for explainability . Yadav   et al . ( 2020 ) propose a multitask learning frame-   work that uses PHQ-9 and figurative language de-   tection as auxiliary tasks . Lee et al . ( 2021 ) con-   temporaneously propose a micromodel architecture   that they apply to mental health assessment tasks .   Our work shares several similarities with this ap-   proach , which uses micromodels that are similar to   our symptom classifiers ( questionnaire models ) .   3 Methodology   3.1 Pattern - based methods   Our most straightforward and constrained methods   are pattern - based classifiers that make classifica-   tion decisions based on the presence of positive   symptom patterns . This method could be decom-   posed into two components : a questionnaire model   and a depression model .   Questionnaire model . The questionnaire model of   pattern - based methods is simply a pattern matcher   that matches each user post against symptom pat-   terns . It produces a binary pattern matching matrix   of size ( num _ post×9)whose entry at ( i , j)is1   if a match is found between the ipost and anypattern of the jsymptom ( question ) .   Depression model . We implement two variations   of the depression model whose input is the pattern   matching matrix generated by the previous ques-   tionnaire model : a count - based approach and a   CNN approach . The count - based approach simply   considers whether the number of patterns found in   the pattern matrix exceeds a threshold . The CNN   approach applies CNN kernels cascaded with a lin-   ear layer over the pattern matrix . This approach   allows consecutive posts to be weighted differently .   In pilot experiments , we also tested a variant that   closely mirrors PHQ9 by summing scores over a   two - week window ; this variant performed worse   due to the new temporal requirement that often   creates data sparsity within windows .   3.2 Classifier - constrained methods   3.2.1 PHQ9   One drawback of pattern - based classifiers is the   inflexibility of pattern matching . The classifier-   constrained methods relax the pattern - matching re-   quirement by training a questionnaire model on the   weakly - supervised data described in Section 4.2 .   This results in models that remain grounded in the   clinical questionnaire but are capable of generaliz-   ing beyond the pattern sets . The PHQ9 architecture   is also comprised of a questionnaire model and a   depression model .   Questionnaire model . The questionnaire model   receives BERT ( Devlin et al . , 2019b ) token em-   beddings of every post and is trained to predict   the answer ( positive or negative ) for each of the   questions in the PHQ9 instrument . This model con-   sists of 9symptom classifiers , anhedonia , concen-   tration , eating , fatigue , mood , psychomotor , self-   esteem , self - harm andsleep , corresponding to the   questionnaire ’s 9questions . Each symptom clas-   sifier is a CNN classifier with a linear layer on   top . As illustrated in Figure 1 , all symptom clas-8448sifiers were separately trained on weakly - labeled   data , which we describe in Section 4.2 . The ques-   tionnaire model ’s ability to generalize to unseen   patterns comes from two sources : BERT embed-   dings and weakly - labeled symptom data . First ,   BERT embeddings have been successfully used to   transfer knowledge across domains in many NLP   applications ( Rietzler et al . , 2020 ; Peng et al . , 2019 ;   Houlsby et al . ) . Second , in weakly - labeled data , the   background or contextual text around the matched   patterns could provide relevant cues , which is a   means of generalization . For example , in the text   “ now I do n’t want to do anything . I ca n’t do more   than sleep , eat , and watch tv . ” , background phrases ,   such as “ I ca n’t do more ... ” , are as useful as the   underlined pattern for identifying the symptom an-   hedonia .   Depression model . The depression model predicts   whether a user is depressed based on the question-   naire model ’s output for each post . The question-   naire model ’s output can be either the final question   scores ( i.e. , symptom scores ) or the hidden layers   ( i.e. , symptom vectors ) of the 9sub - models . The   former represents each post with a single vector of   size9 , which is compact but less informative , while   the latter is a larger matrix of size hidden _ size×9   that preserves more information . Any classifica-   tion architecture could be used for this depression   model . For simplicity , we use a linear classifier   on top of features extracted by CNN kernels of   various sizes . CNN kernels help summarize symp-   toms within a sliding window of consecutive posts   sorted by timestamp and therefore are a relaxation   of the two - week windows considered by the PHQ9   instrument . This relaxation allows more posts to be   considered by each CNN kernel , which mitigates   the data sparsity problem of the hard two - week   window approach .   This depression model is trained using user - level   depression labels , and while this model is being   trained , the encoder and questionnaire components   are frozen . The frozen weights ensure that each   questionnaire model does not drift away from its   original purpose of detecting symptoms .   3.2.2 PHQ9Plus   PHQ9Plus extends the PHQ9 method by append-   ing an additional symptom ( neuron ) to the PHQ9   symptoms that form the questionnaire model . This   neuron is connected to post embedding and pro-   duces a score for every post . Furthermore , wemake this additional neuron trainable end - to - end   to learn other signals similarly to PHQ9 symptoms .   Doing that allows PHQ9Plus to learn from train-   ing data other depressive signals in addition to the   PHQ9 symptoms . However , in return , it also risks   incorporating undesirable shortcuts that harm the   model ’s generalizability .   3.3 Unconstrained method ( BERT baseline )   In the previous classifier - constrained methods , de-   pression classifications are constrained by a ques-   tionnaire model that determines the presence of   symptoms . This is an information bottleneck in-   tended to make the model generalize better . In   order to quantify the impact of this bottleneck , we   also consider an unconstrained model that replace   the questionnaire model in the previous methods   by only a BERT encoder . This gives a loose up-   per bound on the classifier - constrained methods ’s   performance since this approach has access to the   raw BERT embeddings and thus can utilize more   signals ( even spurious ones ) than those captured by   the questionnaire model .   4 Experiments   We conduct experiments on three datasets ; all con-   sist of Reddit social media data but follow different   construction methodologies ( e.g. , identifying de-   pressed users based on a self - report statement vs.   based on starting a thread in a support subreddit ) .   In addition to evaluating methods on each dataset ,   dataset - transfer evaluation allows us to evaluate   how well methods generalize to similar datasets   with different construction methodologies .   4.1 Depression datasets   The three datasets selected for experiments are   RSDD ( Yates et al . , 2017 ) , eRisk2018 ( Losada and   Crestani , 2016 ) and TRT ( Wolohan et al . , 2018 ) .   The RSDD ( Reddit Self - reported Depression Diag-   nosis ) dataset was constructed from Reddit posts   and contains approximately 9,000self - reported di-   agnosed users and 107,000matched control users .   eRisk2018 is a smaller dataset of 214depressed   users and 1,493control users curated to evaluate   the effectiveness of early risk detection on the In-   ternet . Similar to RSDD , the depression group in   eRisk2018 was collected based on user self - reports ;   Here , posts from mental health subreddits were not   excluded like in RSDD . Due to the small size of the   original training set , which makes the deep learning8449approaches unstable , we re - partitioned this dataset   to allow more data for training .   Different from RSDD and eRisk2018 , TRT   ( Topic - Restricted - Text ) was constructed based on   community participation . Specifically , the de-   pressed users were drawn from members of the   /r / depression subreddit , and control users were sam-   pled from the /r / AskReddit subreddit . Following   the construction guideline described in ( Wolohan   et al . , 2018 ) and discussion with the authors , we   re - generated a version of TRT containing 6,805   depressed users and 57,155control users .   On all datasets , we report the F1 score of the   positive ( i.e. , diagnosed ) class , and the area under   the receiver operating characteristic curve ( AUC ) .   4.2 Questionnaire dataset   The questionnaire model is tasked with classifying   if a given post contains a PHQ9 symptom ( positive )   or not ( negative ) . Given the lack of training data for   this task , we collected regular expression patterns   and heuristics to construct weakly - supervised train-   ing data for each of the symptoms . We describe   the process succinctly here and provide additional   details in the Appendix . We note that this weakly-   supervised data is used only for training .   4.2.1 Positive class   For each question , we prepare a set of positive   symptom patterns ( e.g. , “ can’?t sleep ” ) . Each pat-   tern set is then matched against a post collection   crawled from 127 mental - health subreddits . In   addition , we also include posts from the SMHD   dataset ( Cohan et al . , 2018 ) , which excludes posts   from mental - health subreddits , to diversify the   training data . In the labeling step , we select posts   containing symptom patterns as positive examples .   While being fast and transparent , pattern match-   ing may produce many false positives ( FPs ) . We   used additional heuristics to remove instances of   the four most common types of FPs we observed :   Positive sentiment . Posts containing symptom   patterns but conveying a positive / happy sentiment .   Conditional clause . Posts describing a symptom   hypothesis rather than an experience .   Third - person pronouns . Posts discussing symp-   toms of other people ( e.g. , friends , relatives ) rather   symptoms the user is experiencing .   Negation . Posts containing symptom patterns with   negation words ( e.g. , “ not ” , “ never ” ) preceding.4.2.2 Negative class   Identifying hard negative samples is crucial for the   quality of the trained classifiers . We use five heuris-   tics to identify and synthesize negative examples   for each symptom :   Keywords . Posts that contain keywords ( e.g. ,   “ sleep “ ) related to positive patterns ( e.g. , “ ca n’t   sleep “ ) but do not match any positive pattern .   Pronouns . Posts synthesized by replacing first-   person pronouns ( e.g. , “ I “ ) from the positive exam-   ples with third - person pronouns ( e.g. , “ She “ ) .   Other symptoms . Posts sampled randomly from   positive examples of other symptoms ( without   matching a pattern for the current symptom ) .   Negation . Posts synthesized by negating symptom   patterns in positive examples using hand - crafted   mappings ( e.g. , “ tired “ to “ never tired “ ) .   Positive sentiment . Posts sampled from neutral   or positive classes in the Sentiment140 sentiment   analysis corpus ( Go et al . , 2009 ) .   4.3 Experimental setup   We designed experiments to analyze our two main   component : the questionnaire and depression mod-   els . The setup for these experiments is summarized   in Table 1 and specific hyperparameters are de-   scribed in the Appendix .   4.4 Depression detection results   The results from prior work and our methods on   RSDD , TRT , and eRisk are shown in Table 2 .   Depression detection results in dataset - transfer   evaluation are shown in non - gray blocks in Table 2 .   Unlike standard within - dataset evaluations , this sce-   nario requires methods trained on one dataset to   generalize to other ( highly similar ) datasets . While   all datasets consider the same social media plat-   form , their dataset construction methodologies dif-   fer , and thus , they are likely to contain different   dataset artifacts . Unconstraind methods have the   flexibility to learn shortcuts induced by these ar-   tifacts , which can lead to poor generalization be-   yond the training corpus . This effect is observed8450   in the results of the unconstrained model : as sum-   marized in Figure 2 , BERT is outperformed by   our PHQ9 ( scores , vectors ) or even pattern - basedmethods in many dataset - transfer settings . In terms   of F1 and AUC , our two PHQ9 variants general-   ize better than BERT with only 1“Loss ” at most8451over6dataset - transfer settings , and the number   of “ Win ” always dominates . Compared to BERT ,   our PHQ9 ( vectors ) obtains 5“Win ” , 1“Draw ” ,   and no “ Loss ” in terms of F1 . Regarding AUC , we   only observe 1 “ Loss ” replacing a “ Win ” . The   method using PHQ9 scores generalizes slightly   worse than the one using vectors but still performs   better than the unconstrained models . For exam-   ple , when trained on TRT and tested on RSDD ,   our PHQ9 ( scores ) method improves over BERT   by roughly 59 % F1 score and 18 % AUC . This be-   havior may reflect the unusual selection of control   users in TRT , where control users are sampled from   “ r / AskReddit ” . This may introduce shortcuts ( e.g. ,   specific topics or styles ) that make BERT vulner-   able to the change of testing environment . Our   classifier - constrained methods with scores and vec-   tors are designed to avoid the spurious shortcuts   present in this setting .   For similar reasons , the extra neuron gives the   PHQ9Plus model more freedom to learn shortcuts ,   leading to inferior generalization than PHQ9 ( with   both scores and vectors ) . In addition to generaliz-   ing better , the methods with symptom scores can   be used to identify evidence in the form of spe-   cific posts related to the symptoms in PHQ9 , which   makes them more trustworthy from the perspective   of mental health professionals who can examine   the posts to verify that symptoms are present .   On standard within - dataset evaluations ( in gray   cells ) , when models are trained and tested on the   same corpus , we find that F1 and AUC increase   as the models become less constrained , with the   standalone BERT model and PH9Plus performing   the best on all datasets . However , as previously   shown , this performance does not transfer to more   realistic dataset - transfer settings . The two pattern-   based methods perform worse than the best prior   method on each dataset , though they are the easi-   est to interpret due to the PHQ9 symptom scores   associated with each post .   When the patterns are used to train a PHQ9   ( scores ) model , both F1 and AUC increase sub-   stantially , with the largest improvement of 0.13 F1   and 0.12 AUC in TRT . Methods using PHQ9 ( vec-   tors ) perform slightly better than those using scores ,   but the latter is easier to interpret since each post   is associated with a symptom score . Both perform   well in comparison with the baselines despite the   fact that they are constrained by the PHQ9 symp-   toms . The add - on neuron contributes significantlyto the in - domain effectiveness of PHQ9Plus , which   even outperforms BERT in several settings .   4.5 Symptom detection results   To quantify the performance of our weakly-   supervised questionnaire ( symptom ) models , we   additionally prepared a dataset of 900samples man-   ually labeled by three annotators . The annotation   procedures are described in the Appendix B. The   results of our symptom classifiers evaluated on the   test sets are shown in Table 4 . Overall , our symp-   tom classifiers perform well despite being trained   on weak labels . The “ concentration ” , “ eating ” and   “ self - harm ” classifiers show strong performance ,   while a lower F1 is observed with the “ anhedonia ” ,   “ mood ” and “ fatigue ” classifiers . Interestingly , we   find that the F1 scores of symptom classifiers tend   to positively correlate with the annotator ’s agree-   ment ( Pearson ρ > 0.5 ) . This suggests the low F1   score in some symptom classifiers , such as “ anhe-   donia ” and“fatigue ” , might partly be due to the   ambiguity of texts . For example , it is challenging to   distinguish between an ordinary bad mood versus   a depressive mood . Additionally , in our analysis ,   we find many wrong predictions where posts use   symptom - like language in a more specific context ,   such as “ I completely lost my interest in him ” or   “ I ca n’t concentrate on that movie ” . These alone   might not indicate a symptom , but recurrence of   them might be significant .   4.6 Do symptom classifiers generalize ?   To examine whether our symptom classifiers can   generalize beyond pattern matching , we split each   pattern set into two non - overlapping groups ( g1 ,   g2 ) , which split the original dataset into two ex-   clusive subsets . Because pattern distribution are   uneven , the resulting subsets are sometimes imbal-   anced . We then evaluate our symptom classifiers   on two settings ( i.e. , train on g1 & test on g2 , and8452train on g2 & test on g1 ) . The results shown in Ta-   ble 5 show that our symptom classifiers still achieve   fairly high F1 scores in both settings . Note that ,   on some symptoms ( e.g. , concentration , fatigue ) ,   given a small coverage of patterns in g2 , our mod-   els could still achieve good performance compared   to models trained on the much larger data covered   byg1 . This suggests that the symptom classifiers   can generalize beyond the specific patterns they   were trained with .   4.7 Effect of labeling methods   In Figure 3 , we visualize the effect of various data-   construction factors on the performance of symp-   tom classifiers . Regarding the data source , data   obtained from mental health subreddits has more in-   fluence on effectiveness than the more general posts   in SMHD . Discarding data from mental health sub-   reddits leads to an average drop of nearly 0.23 F1   score in all symptoms , while the decrease after re-   moving SMHD is 0.02 . We attribute the immense   contribution of data from mental health subreddits   to the fact that mental health is the main topic of dis-   cussion in those forums ; therefore , pattern match-   ing returns fewer false - positive cases and denser   symptoms , resulting in better quality training data . We further investigate the role of each method to   remove FP matches in the positive class . For that   purpose , we put filtered - out FP examples back into   the training data and observe the variation of F1   score on the manually labeled test sets . In general ,   adding back FP examples filtered by our methods   causes a total drop of nearly 0.12 in the averaged   F1 score . Among them , instances with positive   sentiment cause the highest decrease of roughly   0.04 . Posts with third - person pronouns contribute   around 0.03 , while conditional clause and negation   contribute modestly at around 0.02 F1 . Similarly ,   we analyze the effectiveness of methods to weakly   annotate the negative class by removing each of   them from the training data and record the change   in F1 score . We find that removing three methods ,   including keywords , pronouns , and other symp-   toms , causes a similar drop of roughly 0.06 each .   Interestingly , eliminating data with positive senti-   ment from the negative class has a similar effect to   adding them to the positive class , causing a drop of   almost 0.04 F1 . The method that changes positive   examples to negative examples has the smallest   impact on the F1 score ( roughly 0.01 ) . Overall , ex-   cept for the data sources , no single labeling method   has a superior impact on the quality of symptom   classifiers than other methods .   4.8 Contribution of PHQ9 symptoms   To measure the contribution of a symptom to de-   tecting depression , we remove the corresponding   symptom from the model and observe the drop   in the F1 score . The results are reported in Ta-   ble 6 . On average , we could see that “ self - harm ” ,   “ fatigue ” , and “ anhedonia ” are the strongest indi-   cators of depression . Removing them causes a   0.13 - 0.17 drop in the F1 score . This is in line   with the prior finding that suicidal ideation or self-   harm is highly correlated with depression ( Brådvik ,   2018 ) . “ Mood ” , “ psychomotor ” , and “ self - esteem ”   contribute moderately to depression detection , with   roughly a 0.09 drop in F1 score for each . The re-   maining three symptoms , including concentration ,   eating , and sleep , play a less important role in de-   tecting depression , with each contributing around   0.05 to the F1 score .   4.9 Comparison with Few - shot Learner   Recent work has demonstrated that GPT-3 is   a strong few - shot learner ( Brown et al . , 2020 ) .   Herein , we are interested in how well our classifier-   constrained methods compare to the GPT-3 with8453   prompted examples . We prompt GPT-3 with four   examples for each ( positive , negative ) class from   one dataset ( e.g. , TRT ) and evaluate on other   datasets ( e.g. , RSDD , eRisk ) . Due to the high com-   putational cost of GPT-3 , we only evaluate on 100   positive samples and 100negative samples from   each dataset .   We can see in Table 7 that prompted GPT-   3 is consistently outperformed by our classifier-   constrained methods , and the margin is often large .   For example , among models trained on RSDD ,   the classifier - constrained model with CNN vectors   achieves the highest F1 of 0.79 and 0.64 when   tested on TRT and eRisk , respectively . GPT-3   performs worse with at least a 0.12 drop in F1 .   This result demonstrates that depression detection   is still challenging for large few - shot learners , fur-   ther highlighting our contributions of generaliz-   able methods . However , we note that this setting   has several limitations that prevent a completely   fair comparison . Our methods have access to hun-   dreds of posts , while GPT-3 has a limitation on   the prompt length . In addition , prompt examples ,   which have high influence on the GPT-3 few - shot   performance , need to be carefully selected and   tuned . It is possible that we were unable to identify   near - optimal prompts . Furthermore , it is difficult   to know which posts or users should be prompted   to GPT-3 , so we opted to select randomly . Lifting   this limitation would require a separate model to   identify which posts should be used as input .   5 Case study   In Table 3 , we demonstrate approaches trained   on TRT using text from an anonymized and para-   phrased depressed user from the eRisk2018 dataset .   We show the top two posts ranked by the drop in   depression score when excluding each post . All   models were able to produce correct labels with   very high confidence . However , there is a clear   difference in the posts that models rely primar-   ily on for prediction . The PHQ9 ( scores ) model   found highly relevant posts with convincing associ-   ated symptoms . For example , in the first post , the   PHQ9 models found 3 symptoms , including “ an-   hedonia ” , “ mood ” and “ self - esteem ” . By looking   at those posts and symptoms , mental health pro-   fessionals could quickly understand the patient ’s   circumstances and make further decisions . The   two most important posts for PHQ9Plus and BERT   are more about daily life concerns or complaints ,   which may be less useful to explain a high de-   pression score than the top posts used to explain   the PHQ9 ( scores ) model . While these posts are   relevant , they are more difficult to interpret than   posts directly mentioning symptoms that are known   to be relevant . Furthermore , in the TRT training   dataset , due to the biased selection of control users ,   those life concerns / complaints may form a shortcut   that effectively differentiates depressed users from   control users . However , in more realistic deploy-   ment scenarios ( i.e. dataset - transfer settings ) , the   fact that such shortcuts do not generalize makes   PHQ9Plus and BERT more unreliable and fragile .   6 Conclusion   In this work , we propose a spectrum of methods   for depression detection that are constrained by the   presence of PHQ9 symptoms . In our experiments   on the three datasets , we find these methods to   perform well compared to strong baselines while   generalizing better to similar datasets . This can be   viewed as a proof - of - concept demonstrating that   grounding depression predictions in PHQ9 can im-   prove the generalizability of depression detection   and the interpretability of the model . While this   research focuses only on depression detection , the   idea of constraining models to consider only rel-   evant causes may be applied to a wider range of   tasks , including detection of other mental health   conditions with diagnostic questionnaires.8454Ethics Statement   Due to the sensitivity of the mental health related   data , additional consideration needs to be taken   into account when accessing and analyzing such   data , as highlighted by Benton et al . ( 2017 ) . All   datasets used in this research were obtained accord-   ing to each dataset ’s respective data usage policy .   We did not interact with users in any way , and we   refrained from showing any direct excerpts of the   data in this manuscript to prevent risks from identi-   fying users ’ pseudonyms . ( All excerpts have been   paraphrased . ) Similarly , we made no attempt to   identify , deanonymize , or link users to other social   media accounts . These precautions ensure we do   not draw attention to specific users who may be   suffering from depression .   All models proposed in this research were   trained on social media data . Thus , they are likely   to fail on data coming from other sources ( e.g. , clin-   ical notes ) , and there are no accuracy guarantees   even within social media data . Our models are not   intended to replace clinicians . Instead , we envision   the approaches we describe being used as assistive   tools by mental health professionals .   References84558456A Details of experimental setup   We designed various experiments to validate and   analyze two main components : the questionnaire   and depression models . The hyper - parameters of   those models were set as follows :   Questionnaire model : In classifier - constrained   methods , we trained a CNN for each of the nine   symptom classifiers . We used filters of sizes   [ 2,3,4,5,6 ] , and one filter for each size . Conse-   quently , the max - pooling produces a vector of size   5 , which is then fed into the final linear layer for   prediction . We apply Lregularization specifically   to the CNN kernels . The Lweights were fine-   tuned with three options [ 0.1,0.01,0.001 ] .   Depression model : We experimented with 6   variations described in Table 1 . All of these varia-   tions , except for the first one , use a CNN classifier   on top of different inputs ranging from BERT em-   bedding to pattern scores . The CNN used here has   ( filter _ size= [ 2,3,4,5,6],num _ filter = 50 ) ,   and(k= 5 ) for k - max pooling . The threshold for   the first variation was tuned on from 1to10 .   In all experiments with pre - trained BERT , we   used the BERT - base version ( Devlin et al . , 2019a ) .   We do not fine - tune BERT ’s parameters since in a   pilot study , we found that fine - tuning BERT does   not improve the generalization . All models were   optimized using a cross - entropy loss with class   weights of 0.1and0.9for the control and depressed   classes , respectively , a 1cycle learning rate sched-   uler ( with a maximum of 0.01 ) , batches of size 64 ,   and early stopping after five epochs . When cal-   culating F1 , we set the decision threshold to 0.5 ,   because it can not be safely tuned in dataset - transfer   experiments .   BManually - labeled questionnaire dataset   To evaluate the performance of our weakly-   supervised symptom classifiers , we prepared 900   examples manually labeled by three annotators .   The labeled samples were randomly selected posts   containing carefully selected keywords ( e.g. , key-   words that are close to positive patterns - “ sleep “   in “ ca n’t sleep “ ) to avoid including too many easy   true negatives . The labeling process involved three   annotators . The first annotator labeled all 100 in-   stances , and the second annotator re - labeled 50   of them . If the agreement on twice - labeled exam-   ples was weak ( κ < 0.60 ) , the second annotator   would continue to annotate the remaining 50 ex-   amples . The third annotator adjudicated label dis-8457   agreements between the first two annotators .   C Details of questionnaire dataset   construction   This section describes the data creation process   for the questionnaire models , which consists of 9   symptom classifiers corresponding to 9 questions   in the PHQ9 instrument ( e.g. , “ trouble falling or   staying asleep ? ” ) . PHQ9 questions ask how often   the patient experienced each symptom ; we adapt   this approach to our domain by classifying whether   a given post contains a symptom . Given the lack   of training data for this task , we collected regular   expression patterns and used these patterns together   with heuristics to construct training data ( positive   class and negative class ) for each symptom .   C.1 Positive class   For each question in the PHQ9 diagonstic instru-   ment , we prepared a set of positive patterns that   each indicates the presence of a symptom described   by the question . For example , the patterns “ don’?t   feel like doing anything ” and “ can’?t fall back   to sleep ” describe the anhedonia and sleep symp-   toms , respectively . The number of patterns for   each symptom is shown in Table 8 . Each pattern   set is then matched against a collection of posts   crawled from 127 mental - health subredditsand   also against posts from the SMHD dataset ( Cohan   et al . , 2018 ) , which was constructed from Reddit   but excludes mental health subreddits . The purpose   of using these two raw datasets is to increase the   diversity and minimize the bias of the data . In the   labeling step , if a post contains a match with any   positive pattern of a question , we select that post   as a positive training sample for the corresponding   symptom question .   While pattern matching is fast and transparent , it   is inflexible and may produce many false positives   ( FP ) . Below , we introduce the four most popular FP   cases discovered in our analysis and the techniques   we employed to mitigate them when constructing   weakly - labeled training data . Positive sentiment . Some posts contain positive   patterns but do not show depressive signal ; for ex-   ample , “ Friends and I stayed up all night playing   a game ” contains the positive pattern “ stayed up   all night ” , but it shows excitement about the game   rather than a sleep issue . As a solution , we re-   moved all posts containing positive sentiment with   the help of Allen NLP ’s sentiment model ( Gardner   et al . , 2017 ) .   Conditional clause . Sometimes users hypothesize   about their health conditions , such as “ If I lost my   appetite for days at a time , that ... would n’t be sus-   tainable for me . ” . We remove these posts by using   regular expressions to identify popular conditional   clause formats .   Third - person pronouns . Users may attribute a   condition to someone else , such as in “ he is eas-   ily distracted . ” We identified posts of this kind   by checking whether the closest pronoun to the   positive pattern is third - person or first - person , and   removing posts in the former category .   Negation . Positive patterns may be negated , such   as in “ have n’t had a suicidal thought in ages . ” . To   handle this situation , we removed posts containing   positive patterns preceded by a negation word , such   as ( “ not ” , “ never ” , ‘ rarely ” , etc . ) .   C.2 Negative class   Identifying hard negative samples is crucial for the   quality of the trained classifiers . Models trained on   negative examples that are too easy might be prone   to over - fitting or perform only keyword matching .   Therefore , we propose five heuristics for identify-   ing and synthesizing negative examples .   Keywords . We collect negative posts that contain   some keywords , such as " sleep " , but do not contain   a positive pattern ( “ ca n’t sleep ” ) . This hinders   models to perform simple keyword matching .   Pronouns . We replace the first - person pronouns   appearing in posts from the positive class with third-   person pronouns or proper nouns , such as replacing   “ I ” with “ she . ”   Other symptoms . We use randomly selected posts   labeled positive for other questions ( symptoms )   as negative examples for the given question . We   ensure the selected posts do not contain positive   patterns for the given question .   Negation . For each positive pattern defined in   the previous section , we created a corresponding   negated one , such as negating “ have sleep apnea ”   to “ never have sleep apnea ” . Only matched sen-8458tences were used in this method , because using the   whole post with only some sentences being negated   could lead to contextual inconsistencies .   Positive sentiment . We use training instances la-   beled neutral or positive in a sentiment dataset   as negative examples . In particular , we used the   Sentiment140 corpus , which contains 1.6 millions   tweets ( Go et al . , 2009).8459