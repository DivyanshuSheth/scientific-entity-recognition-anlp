  Itay LevyBen BoginJonathan Berant   The Blavatnik School of Computer Science , Tel - Aviv University   { itay.levy,ben.bogin,joberant}@cs.tau.ac.il   Abstract   In - context learning has shown great success in   i.i.d semantic parsing splits , where the train-   ing and test sets are drawn from the same dis-   tribution . In this setup , models are typically   prompted with demonstrations that are similar   to the input utterance . However , in the setup   of compositional generalization , where models   are tested on outputs with structures that are   absent from the training set , selecting similar   demonstrations is insufficient , as often no ex-   ample will be similar enough to the input . In   this work , we propose a method to select di-   verse demonstrations that aims to collectively   cover all of the structures required in the output   program , in order to encourage the model to   generalize to new structures from these demon-   strations . We empirically show that combining   diverse demonstrations with in - context learn-   ing substantially improves performance across   three compositional generalization semantic   parsing datasets in the pure in - context learn-   ing setup and when combined with finetuning .   1 Introduction   Despite strong performance of pretrained language   models ( LMs ) across many tasks , they have been   shown to struggle in a compositional generalization   setting ( Lake and Baroni , 2018 ; Furrer et al . , 2020 ;   Shaw et al . , 2021 ) , when tested on their ability to   process and generate novel combinations of previ-   ously observed elements . For example , a model   might fail to interpret the request “ Book a meeting   with Jake ’s supervisor ” even when “ Book a meet-   ing with Jake ” and“Who is Jake ’s supervisor ? ”   were observed during training . In semantic parsing ,   the task of mapping natural language utterances   to formal queries , such generalization is important   ( especially in a real - world setting ) , since models   are required to interpret new combinations that areFigure 1 : Compositional generalization setup : ( a ) Se-   lecting demonstrations by considering only similarity   to the input yields repetitive demonstrations that do not   cover the structures in the target program . ( b ) However ,   choosing diverse demonstrations enables better cover-   age and leads to a correct prediction .   not covered by the annotated training data ( Herzig   and Berant , 2019 ; Yin et al . , 2021 ) .   Recently , large LMs have shown impressive per-   formance on downstream tasks by conditioning on   a text - based prompt that contains a few training ex-   amples . This type of few - shot inference is known   asin - context learning ( ICL , Brown et al . , 2020 ) . A   core component of in - context learning is the set of   examples in the prompt , often termed task demon-   strations . With the right demonstrations , ICL can   be an effective approach to improving LMs ’ compo-   sitional generalization abilities ( Qiu et al . , 2022b ) .   Selecting a relevant set of demonstrations is cru-   cial for generalization . However , most past work   only considered the relevance of each example in   isolation , ignoring the quality of the entire set of ex-   amples ( Liu et al . , 2022 ) . For instance , a retriever   can be used to select the examples most similar to   the input ( Rubin et al . , 2022 ) . A set of demonstra-1401   tions that are all highly relevant but highly similar   to one another may not be as effective as a more di-   verse set . In compositional splits , where no single   demonstration is sufficiently similar to the input ,   choosing diverse demonstrations can be especially   beneficial since it leads to better coverage of struc-   tures in the target program ( Fig . 1 ) .   In this paper , we study how to leverage ICL to   improve compositional generalization for semantic   parsing , by optimizing the entire set of demon-   strations and increasing the diversity of examples   in this set . We investigate two approaches for in-   creasing diversity : ( a ) a coverage - based approach ,   where we define a set of elements conditioned   on the input utterance , and select examples that   cover those elements ( e.g. , covering potential sub-   structures in the output program ) , and ( b ) a second   approach , where we select a subset of examples   that are most dissimilar from one another , such   that diversity is independent of the input utterance .   Empirically , we find that coverage - based diversity   results in better performance .   Our method can be used in the “ pure ” in - context   learning setup without finetuning , which leverages   the ability of large LMs , such as Codex ( Chen   et al . , 2021 ) , to generalize from the selected diverse   demonstrations . Furthermore , it can be combined   with finetuning by training a model with demon-   strations as part of the input . This can be viewed   as meta - learning , where the model learns to use   demonstrations during training and build new struc-   tures based on them during inference ( Finn et al . ,   2017 ; Lake , 2019 ; Conklin et al . , 2021 ; Min et al . ,   2022 ; Chen et al . , 2022 ) . It can , however , lead   to an over - reliance on demonstrations , especially   in compositional splits . We address this by using   “ noisy ” demonstrations during training . We empirically test our method on three com-   positional generalization semantic parsing datasets .   We show that diverse demonstrations , both with   and without finetuning , improve performance by   up to 23 absolute points ( e.g. , 50.3 →73.5   on SMCalFlow - CS ) compared to a baseline that   retrieves demonstrations according to similarity   alone , and lead to state - of - the - art results in mul-   tiple compositional setups . Finally , we show that   our method reduces the number of demonstrations   needed for generalization and improves test perfor-   mance on hard examples .   2 Diversity for Compositional   Generalization   In semantic parsing , we define compositional splits   of datasets as splits where train and test programs   do not overlap ( Finegan - Dollak et al . , 2018 ) . Re-   cent work has shown that increasing the number of   different program structures a model sees during   training improves performance on compositional   splits . This can be done by augmenting the training   set ( Qiu et al . , 2022a ) or through efficient sampling   of diverse examples ( Oren et al . , 2021 ; Bogin et al . ,   2022 ; Gupta et al . , 2022 ) . While past work focused   on increasing structure diversity in the training   set , we focus on diversity in the demonstration set   within an ICL setup .   Increasing diversity is important as we want the   demonstrations to cover all structures of the ex-   pected output program . In the few - shot setting ,   where the model is unfamiliar with the formal lan-   guage of the output programs , increasing coverage   also improves generalization simply since other-   wise the model will be unaware of the required   program symbols ( predicates and logical opera-   tors ) . However , selecting demonstrations that cover1402larger structures ( sub - trees of the program tree ) are   potentially more beneficial , for two reasons : ( 1 )   it reduces the amount of new structures that the   model needs to produce , making demonstration   fusion easier , and ( 2 ) it exposes the model to struc-   ture compositions in different contexts , providing   the model with valuable information about how   structures can be composed in the data .   3 Diverse Demonstrations Selection   Problem setup Given a training set T=   { ( x , y)}containing utterance - program pairs   and a test utterance x , our objective is to select a   subset of training examples D={(x , y)}⊂   T , where k≪n , termed demonstrations . Those   demonstrations are then formatted as a text - based   prompt P. When feeding the concatenation of   the prompt and the test utterance ( [ P;x])to the   model , the desired output is y.   Overview Fig . 2 provides an overview of our   framework for obtaining and leveraging diverse   demonstrations for better compositional generaliza-   tion . Given an input utterance , x , we propose   two approaches for selecting demonstrations . In   the first ( § 3.1 ) , we optimize coverage : we define   a set of elements that we want our demonstrations   to cover ( either structures in the program or utter-   ance words ) , and then iteratively select examples   that contain these elements . The second approach   ( § 3.2 ) increases diversity by selecting a subset of   examples with minimal similarity . Fig . 2 shows an   example of the former approach ( Cover - LS ) , where   we predict and then attempt to cover local struc-   tures ( LS ) , i.e. , sub - trees of the output program .   Local structures were shown to be key for compo-   sitional generalization in Bogin et al . ( 2022 ) .   Having selected demonstrations , we use them   to construct a prompt ( § 3.3 ) . We show that our   method can be combined with finetuning to meta-   train the model to learn in - context ( § 3.4 ) .   3.1 Coverage - based Selection   Bogin et al . ( 2022 ) have recently shown , in the   context of finetuning semantic parsers , that models   fail to generalize to programs with local structures   that were not observed at training time , where lo-   cal structures of a program are defined to be a set   of its sub - trees . Inspired by this observation , we   propose Cover - LS , an algorithm that given the test   utterance x , attempts to choose examples that   collectively cover as many local structures as pos - sible from the set Sof local structures of the   program y. Since we have no access to yat   test time , we predict what local structures are likely   using an auxiliary model , assuming that predicting   local structures is easier than predicting the entire   program . Then , we iteratively select examples that   cover the predicted local structures .   Local structures definition We follow the defini-   tion of Bogin et al . ( 2022 ) , and given a program   y , convert it to its abstract syntax tree , where each   tree node is a program symbol and parent - child   edges connect functions to their arguments . In   addition , we add “ sibling ” edges between consec-   utive arguments . The local structures , S , are   a subset of all of the connected sub - graphs in the   abstract syntax tree ( e.g. , state→next_to_2 and   most→state→loc_1 in Fig . 2 , see more exam-   ples in Tab . 8) , as defined in App . B. Unlike Bogin   et al . ( 2022 ) , we consider local structures with any   number of nodes . In addition , we anonymize pro-   grams by replacing values such as strings and num-   bers with constants ( string andnumber ) , since   such values are usually not relevant for program   coverage .   Predicting local structures As mentioned , we as-   sume predicting local structures is easier than pre-   dicting an entire program . Thus , we train an aux-   iliary model by finetuning T5 ( Raffel et al . , 2020 )   on the training set in the standard manner , training   it to output anonymized programs given input ut-   terances with no demonstrations . Then , for each   test utterance , x , we use beam search to output   Bcandidate programs { ˜y}and define the set   of local structures as S=/uniontextS.   Covering local structures Our goal is to choose   a set of demonstrations , D , that covers the local   structures in S. Choosing an example for each   local structure is infeasible due to prompt length   limitations , and thus we propose Alg . 1 , whose goal   is to choose a small set of demonstrations that are   ( a ) similar to the test utterance xand ( b ) cover   as many local structures in Sas possible .   We sort the LSs based on their size ( number of   nodes ) in descending order ( line 2 ) . By first se-   lecting training examples with programs that con-   tainlarger LSs from S , we are more likely to   include training examples similar to the test utter-   ance , which should improve few - shot performance .   Then , we iterate over all LSs , and for each local   structure sweretrieve the most similar training   example that contains s(line 6 ) , and add it to D1403Algorithm 1 : Cover - LS Algorithm   ( line 7 ) . We then update the pool of LSs such that it   will include only LSs that are not yet covered ( line   8) . To further encourage diversity , we remove from   our example pool all examples that share the same   template ( program after anonymization ) as the cho-   sen examples ( line 9 ) . We keep choosing examples   until reaching the desired amount of demonstra-   tions , which might result in choosing more than   one example for each local structure ( lines 3 - 4 ) .   We assume ( line 6 ) access to a retriever that   takes as input an utterance and returns similar train-   ing examples , from which we filter only exam-   ples that contain the desired structure . A variety   of retrievers can be used , such as BM25 ( Robert-   son and Zaragoza , 2009 ) or SBERT ( Reimers and   Gurevych , 2019 ) .   We observe that in our setup , the running time of   Cover - LS is negligible compared to the decoding   time of the LMs .   Utterance coverage We propose a simpler variant   that does not require predicting a set of local struc-   tures with an auxiliary model . This variant , termed   Cover - Utt , uses the same coverage - oriented al-   gorithm , but covers words in the input utterance ,   rather than predicted local structures . This is bene-   ficial when the quality of the auxiliary model , and   consequently predicted LSs , is low .   3.2 Diversity without Coverage   The primary challenge with coverage - based ap-   proaches is identifying the elements that need to be   covered . An alternative approach is to define diver-   sity more explicitly and select a subset of demon-   strations that are dissimilar from one another ( while   being relevant for the input utterance).A natural approach for choosing a subset of   high - quality and diverse demonstrations from the   training set is Determinantal Point Process ( DPP )   ( Kulesza and Taskar , 2012 ) , a probabilistic model   that defines a probability distribution over subsets   of items , giving high probability to subsets that   contain relevant anddiverse items . DPP requires a   relevance score for each item and a similarity score   between pairs of items . In our case , we define the   relevance of a demonstration through its retriever   score for the input test utterance . To compute the   similarity between demonstration pairs , we first   extract LSs and compute tf - idf vectors for each   demonstration . The similarity of each pair is then   the cosine similarity between their tf - idf vectors .   Full implementation details are in App . E.   3.3 Prompt Construction   We order the chosen demonstrations according to   their retriever score with respect to the input utter-   ance in ascending order , in accordance to common   practices ( Liu et al . , 2022 ) . When finetuning the   model ( § 3.4 ) , demonstrations are shuffled . Demon-   strations are formatted to a prompt according to   the format in App . D , concatenated with the test   utterance , and fed to the model .   3.4 Finetuning with Prompts   Despite the success of “ pure ” in - context learning ,   where model parameters are frozen , it has been by   and large restricted to very large LMs . Conversely ,   finetuning requires more training data , but performs   well even with smaller models . In - context learning   can be easily integrated with finetuning by training   a model with demonstrations as part of the input .   This paradigm can be considered as meta - learning ,   where the model learns how to use demonstrations   during training ( Min et al . , 2022 ) .   When meta - learning is used in the i.i.d . setup ,   where the training and test examples are drawn   from the same distribution , one can use the same   procedure to select demonstrations at both training   time and test time . However , in a compositional   generalization setup , this does not work : at train-   ing time , the model will observe demonstrations   that are similar to the target output and will learn   to heavily rely on demonstrations and copy large   chunks of them . Thus , the model will not learn   to compose demonstration parts and will struggle   with examples drawn from a different distribution .   To address this phenomenon , which we term   over - copying , past work ( Pasupat et al . , 2021;1404   Zemlyanskiy et al . , 2022 ) used sampling to add   noise to the demonstrations . Here , we also reduce   the similarity of demonstrations to the input utter-   ance , but with a simpler approach . Recall that our   Cover - LS algorithm picks similar examples by ( a )   finding demonstrations that share large LSs with   the predicted program ( lines 2 - 6 in Alg . 1 ) , and ( b )   using a retriever to find the most similar examples   among these . To address over - copying , we mod-   ify this : at training time , we only consider LSs of   size 1 , i.e. , program symbols , and for each such LS   we randomly choose an example that contains this   symbol rather than use a powerful retriever .   4 Experiments   We present our experimental setup and results   on different compositional semantic parsing tasks ,   with finetuning ( FT ) and without ( NoFT ) .   4.1 Datasets   We evaluate our methods on three datasets ( exam-   ples in Tab . 1 ) .   SMCalFlow - CS is a few - shot compositional gen-   eralization dataset proposed by Yin et al . ( 2021 )   derived from SMCalFlow ( Andreas et al . , 2020 ) .   It contains single - turn natural sentences involving   two domains ( organization structure and event cre-   ation ) , each having its own set of program symbols .   The test set of the compositional splits contains   only cross - domain examples , where both domains   appear . We show results for a few - shot setting   ( split k - C , where k∈ { 8,16,32 } ) where the train-   ing set includes only kcross - domain examples ,   and a zero - shot setting ( split 0 - C ) . We also eval - uate on an i.i.d . splitwhere the test set contains   only single - domain examples . Prior studies on the   dataset employed LISP and LISPRESS program   formats , resulting in v1 and v2 versions , respec-   tively ( see an example in Tab . 9 ) . We default to   using v1 , unless otherwise specified .   For our FT experiments , we use SMCalFlow-   CS Simple , which contains the same utterances   as SMCalFlow - CS , but with programs that use a   simplified syntax provided by Meron ( 2022 ) . We   opt for this version because programs are much   shorter , leading to a smaller memory footprint and   accelerating training and inference .   GeoQuery ( Zelle and Mooney , 1996 ; Tang and   Mooney , 2001 ) contains 880 natural language ques-   tions about US geography . We use the standard   ( i.i.d . ) and compositional splits created by Shaw   et al . ( 2021 ): ( 1 ) template split , where target pro-   grams are anonymized into templates and then   the templates are randomly split between train-   ing and test sets ( Finegan - Dollak et al . , 2018 ) ; ( 2 )   TMCD split , which makes the distributions of com-   pounds in training and test sets as divergent as   possible ( Keysers et al . , 2020 ) ; and ( 3 ) length split ,   where test sequences are longer than training ones .   Similar to prior work , we average results across   three TMCD and template splits to reduce variance   caused by the small dataset size .   COVR-10 COVR ( Bogin et al . , 2022 ) is a syn-   thetic dataset based on a variable - free functional   language . COVR-10 contains 10 compositional   grammar splits , in which each test set includes pro-   grams featuring a particular set of local structures   not observed at training time . Results are averaged1405   across the 10 splits .   4.2 Experimental setup   Models We use Codex ( code - davinci-002 ) ( Chen   et al . , 2021 ; Ouyang et al . , 2022 ) for all NoFT   experiments , and T5 - large ( Raffel et al . , 2020 ) for   FT experiments . T5 - large is used to predict LSs in   both the NoFT and FT setups .   Evaluation Like prior work , we use exact match   accuracy as the main metric for evaluation . Results   are averaged over 3 random seeds unless stated   otherwise . In the FT setup , we use the entire test set   for evaluation . In the NoFT setup , we use 100 test   examples due to rate limits of the Codex inference   API ( and another 100 development examples for   hyperparameter tuning ) .   Prompt We use a prompt size of k= 24 for NoFT   experiments and k= 3 for FT experiments , un-   less stated otherwise . A prompt is truncated when   its length exceeds the model ’s context length ( ex-   cluding the tokens reserved for generation ) . In FT   experiments , we included only the programs in our   demonstrations and discarded their utterances , due   to limitations of memory and sequence length ( pre-   liminary experiments with utterances showed this   does not affect accuracy ) .   Retrievers In NoFT setup , we use BM25 over   lower - cased utterance words . In FT setup , we use   BM25 over predicted program symbols in S   ( predicted using T5 ) . In Cover - LS experiments we   use a random retriever at training time to avoid   over - copying . We analyze other possible retriever   choices in § 4.5 .   Hyperparameter tuning and model selection We   train two types of models in this work : ( a ) models   for predicting LSs , and ( b ) models finetuned with   prompts . For both cases , we use the developmentset whenever it is available for model selection , oth-   erwise , we use the last checkpoint . Similarly , we   use the development set to tune the number of beam   candidates Bwhen predicting local structures , and   if there is no development set , we set B= 1 . We   detail finetuning hyperparameters in App . F.   Local structure size In some experiments , we limit   the maximum size of local structures ( the num-   ber of nodes they contain ) . A subscript notation   ( Cover - LSorDPP ) indicates a limit up to size d.   4.3 Baselines   Finetuning without prompts Vanilla - finetuned T5   model which is trained without demonstrations ,   similar to the one used to predict LSs ( § 3.1 ) , except   that it is trained on non - anonymized programs .   Top - K We construct the prompt with the top- k   examples that are most similar to xaccording to   the retriever score .   Random We construct a prompt by randomly sam-   pling ktraining examples without repetition .   We also conduct oracle experiments , where at   test time we have access to yboth for retrieval   and LS coverage . The retriever takes as input   the gold program and scores demonstrations us-   ing BM25 over the gold program symbols . In ora-   cle Cover - LS , we cover local structures from S   without predicting them with a model .   4.4 Main Results   NoFT We observe ( Tab . 2 ) that all methods for in-   creasing diversity ( Cover - Utt , DPP and Cover - LS )   outperform Top - K , which selects similar demon-   strations without accounting for diversity , in 7 out   of 8 compositional splits . In fact , all non - oracle   diversity methods outperform an oracle Top - K in1406   7 out of 8 compositional splits , suggesting that re-   trieval methods that only consider similarity are   sub - optimal even in an oracle setup . Similarly ,   all diversity methods improve performance com-   pared to a finetuned T5 model in all compositional   splits except GeoQuery ’s template splits . Further-   more , sampling random examples ( Random base-   line ) results in poor performance in GeoQuery   and SMCalFlow - CS , but achieves high accuracy in   COVR-10 , beating all methods except Cover - Utt .   This can be explained by the synthetic nature and   small vocabulary of COVR-10 .   Comparing diversity methods , Cover - LS and   Cover - Utt are better than DPP in 7 out of   10 splits , showing that covering the target in-   put / program goes beyond simply picking diverse   examples . Cover - Utt , which covers utterance   words , works surprisingly well considering its sim-   plicity . Coverage - based methods also outperform   Top - K in i.i.d splits . One noticeable failure of   Cover - LS is the 0 - C split , where it fails to gener-   alize , due to the poor T5 performance on this split   ( T5 baseline gets 0 accuracy ) . This emphasizes that   if one can not reasonably predict LSs , then covering   input words is a viable alternative . Lastly , oracle   methods outperform their non - oracle counterparts   in most settings , but not always . This occurs be-   cause our oracle method , which has access to the   gold program , does not guarantee the selection of   the optimal set of demonstrations , a phenomenon   also observed in Qiu et al . ( 2022b ) .   Tab . 3 shows accuracy on the entire test set   ( NoFT setup ) . Since the underlying models differ   substantially , a fair comparison to previous work is   impossible . Nevertheless , a comparison still pro-   vides a high - level overview for the state of these   tasks . Results show that using Codex with Cover-   LS outperforms a T5 finetuned with augmentation   ( Qiu et al . , 2022a ) in 4 compositional splits out of 6   ( TMCD , Length , 8 - C and 32 - C ) , and outperforms   non - finetuned PaLM 540B , where demonstrations   are selected using BM25 , in all splits .   Number of demonstrations ( NoFT ) We exam-   ine how performance is affected by the number of   demonstrations in Fig . 3 . Cover - LS outperforms   Top - K by a large margin across all prompt sizes .   Moreover , Cover - LS requires just four demonstra-   tions in order to obtain roughly the same results as   Top - K with 24 demonstrations . The gap between   Cover - LS and Cover - Utt or Cover - LSshows the   importance of covering structures rather than just   program symbols or utterance words , especially for   small demonstration sets .   FTFinetuning results are shown in Tab . 4 , where   we detail separately the method used for demon-   stration selection at both training time and test time ,   as those may diverge to avoid over - copying .   First , using random demonstrations at test time ,   without controlling for diversity or using any re-   triever , is better compared to using no demonstra-   tions at all . Our main method constructs prompts   with Cover - LS at test time , but during training ,   prompts are retrieved with Cover - LS , that only   covers program symbols , but not local structures ,   to avoid over - copying ( see § 3.4 ) . This combination1407   leads to higher performance in all compositional   splits compared to baselines that use Top - K or ran-   dom sampling . Interestingly , using Top - K at both   training time and test time yields low accuracy in   compositional splits , but high results in i.i.d . splits .   This corroborates our assumption that diversity is   needed in compositional setups . Finally , A vari-   ant of our method , where Cover - LSis used both   during training and test time , is comparable to our   main method across all splits .   We observe that limiting coverage at training   time to program symbols is crucial : accuracy drops   in all splits if we limit Cover - LS to structures up   to size 2 ( Cover - LS ) instead of 1 , or if we have   no such limitation at all . The oracle Cover - LS   outperforms all non - oracle models ( unlike in NoFT ,   where this is not always the case ) .   4.5 Analysis   Stratified analysis Our main results show that   Cover - LS outperforms Top - K in most composi-   tional splits . But what examples does it perform   better on ? We analyze properties of test example   groups , where grouping is based on NoFT predic-   tion outcome : ( 1 ) Top - K succeeds ; ( 2 ) Cover - LS   succeeds ; ( 3 ) only Cover - LS succeeds ; and ( 4)both fail . For each group we estimate difficulty   by measuring the average accuracy achieved by a   T5 model ( finetuned without prompts ) , and also   compute the percentage of examples that have an   unobserved local structure ( ULS ) with respect to   the training set . This measure is central to deter-   mining whether generalization to a test instance is   hard , as shown in Bogin et al . ( 2022 ) .   We see ( Fig . 4 ) that as the group index in-   creases , T5 accuracy decreases and ULS rate in-   creases . This finding confirms the claim in Bogin   et al . ( 2022 ) that a test instance containing an ULS   is hard . Examining groups 1 and 3 , we observe that   the group for which Cover - LS performs better than   Top - K , is also tougher for T5 and has more ULS .   Both methods fail on examples with low T5 accu-   racy and high ULS scores ( group 4 ) . This is also an   evidence that T5 and Codex agree on the difficulty   of examples , despite their different training and   inference schemes . We provide error analysis in   App . A.   Prompt metrics We analyze the characteristics of   prompts constructed with different demonstration   selection methods in Tab . 5 . Symbol Coverage   shows the average fraction of symbols in ythat   are covered by the demonstration set , and similarly   LS Coverage the fraction of covered LSs . While   symbol coverage is generally high across all meth-   ods when using 24 demonstrations , LS coverage is   significantly higher in Cover - LS , suggesting that   only covering relevant symbols in prompts is n’t as   efficient as covering LSs . Utterance Similarity mea-   sures average cosine similarity between SBERT   embeddings of the test utterance and prompt ut-   terances , which is highest for Top - K as expected.1408   To approximate diversity between demonstrations ,   we calculate the average number of unique LSs   in demonstrations , and observe it is substantially   higher in Cover - LS and DPP compared to Top - K.   This implies structural coverage and diversity are   more important than input similarity in composi-   tional splits .   Robustness to retrieval methods To assess our   method ’s robustness , we test how sensitive it is to   the chosen retriever in the NoFT setup . First , we   use our default retrievers , which are BM25 over   utterance words ( BM25 - Utterance ) , and BM25   over predicted program symbols ( BM25 - Predicted ) .   We add a random retriever that is identical to the   R baseline introduced in § 4.3 when com-   bined with Top - K. We also evaluate the SBERT   retriever ( Reimers and Gurevych , 2019 ) , which   encodes input utterances and measures the cosine   similarity between pairs of encodings . As seen in   Fig . 5 , Cover - LS outperforms Top - K in all settings   by a significant margin . Moreover , while BM25-   Utterance performs best , variance across retrievers   is low for Cover - LS , but higher for Top - K.   5 Related Work   Example selection One of the central issues in in-   context learning is the selection of examples , which   can either be based on parameter - free retrievers   ( Wang et al . , 2022 ; Zemlyanskiy et al . , 2022 ) or   neural - based retrievers ( Pasupat et al . , 2021 ; Liu   et al . , 2022 ; Rubin et al . , 2022 ) . These studies con-   sider each example separately , which often leads to   a lack of coverage and diversity .   Our approach is similar to the retrieval procedure   in Zemlyanskiy et al . ( 2022 ) , which makes a prelim-   inary prediction and retrieves demonstrations with   similar programs . However , while they use classic   tf - idf with predicted tokens , we use predicted local   structures and aim to cover them .   Some studies encourage diverse example selec-   tion regardless of prompting . To address multi-   answer retrieval , Nandigam et al . ( 2022 ) employ   DPP , and Min et al . ( 2021 ) autoregressively se-   lect instances based on previous selections . Other   works include Su et al . ( 2022 ) , which selects in-   stances with varying confidence scores for anno-   tation and ( concurrent work ) Ye et al . ( 2022 ) who   propose a MMR - based selection strategy .   In - context learning for compositional general-   ization There have been previous attempts to ad-   dress compositional generalization problems using   LLMs equipped with demonstrations . When se-   lecting demonstrations , some also consider target   coverage or structure similarity , but only in oracle   setups ( Hosseini et al . , 2022 ; Qiu et al . , 2022b ) .   Drozdov et al . ( 2022 ) try to cover the syntactic   parse tree constituents with demonstrations but rely   heavily on manually - picked examples .   6 Conclusion   In this paper , we studied how to leverage ICL to   improve compositional generalization in semantic   parsing , by increasing diversity among demonstra-   tions . We found that choosing demonstrations that   cover the structures required in the output program   substantially improves performance across three   compositional semantic parsing datasets in the pure   in - context learning setup and when combined with   finetuning . We further demonstrated that by aiming   for structural coverage , we can reduce the number   of demonstrations needed for generalization , and   improve test performance on hard examples . Our   approach can be applied to a wide range of NLP   tasks where demonstrations should cover comple-   mentary aspects of the task , and we hope it will   encourage further exploration of our method to im-   prove generalization across diverse applications.1409Limitations   Demonstration selection methods We assume that   diversity can be obtained by choosing demonstra-   tions with different program structures . This is   based on previous work that demonstrated the im-   portance of diversifying program structures in se-   mantic parsing tasks ( Oren et al . , 2021 ; Bogin et al . ,   2022 ; Gupta et al . , 2022 ) . We also try to diversify   utterance words or program symbols but do not con-   sider more complex utterance features that could be   applied to a wider range of language understating   tasks .   We also assume that recall matters more than pre-   cision when designing Cover - LS algorithm . That   means we aim to choose a set of demonstrations   that covers every predicted local structure in S ,   since it has the potential to be a correct one . We do   not predict whether a specific structure should be   covered . Furthermore , our approach for increasing   gold structure coverage by using additional beam   candidates could be improved by employing search   methods specifically targeted for diversity ( Meister   et al . , 2021 ; Narayan et al . , 2022 ) .   Retrievers We used different retrievers for NoFT   and FT setups based on the retriever that worked   best on the development set . Future research should   be conducted to understand why different retriev-   ers are preferred in different setups . A poten-   tial method could be to consider both input utter-   ances and programs for retrieval , as suggested in   Zemlyanskiy et al . ( 2022 ) .   Ethics Statement   In this work , we studied methods for choosing di-   verse demonstrations to improve in - context com-   positional generalization in semantic parsing . We   have only evaluated our methods on semantic pars-   ing datasets in English . It is our hope , however ,   that improvements in compositional generalization   will eventually allow systems to generalize better   to languages that are not well represented in small   training sets .   Acknowledgements   We thank Shivanshu Gupta and Jonathan Herzig   for their helpful comments . This research was par-   tially supported by The Yandex Initiative for Ma-   chine Learning , and the European Research Coun-   cil ( ERC ) under the European Union Horizons   2020 research and innovation programme ( grantERC DELPHI 802800 ) . This work was completed   in partial fulfillment for the Ph . D degree of Ben   Bogin .   References1410141114121413A Additional Analysis   Error analysis We analyze errors ( NoFT setup )   and show results in Tab . 6 . Inspired by the metrics   in Qiu et al . ( 2022b ) , we automatically compute   statistics for the following cases when the predic-   tion is wrong : ( 1 ) Syntax Errors , when the model   produces a program with invalid parentheses ; ( 2 )   Over - Copying , when the entire prediction has the   same anonymized form as one of the demonstra-   tions ; ( 3 ) OOV ( out - of - vocabulary ) Hallucination ,   where the anonymized predicted program contains   a symbol missing from the gold program or any   prompt demonstration ; and ( 4 ) Missing Symbol(s ) ,   where the predicted program is missing at least one   symbol .   The distribution of errors is similar across   demonstration selection methods . Syntax errors are   rare in both datasets . Many predictions are over-   copied , especially in SMCalFlow - CS , but when   diversity is increased with DPP , this number de-   creases significantly . Surprisingly , despite having   a smaller vocabulary , GeoQuery has more out - of-   vocabulary hallucinations . Almost all incorrect   predictions have a missing symbol , but Top - K pre-   dictions are especially prone to this type of error .   Change of retriever in FT setup Tab . 7 shows   results for the FT setup when using BM25 over   lower - cased utterance words as retriever , instead of   BM25 over predicted program symbols .   B Local Structures   We follow the definition of local structures from Bo-   gin et al . ( 2022 ) , which were defined for structures   of sizes 2 - 4 , and extend them to local structures of   any size . Given a program y , we parse it into a tree   T= ( V , E ) , such that each node v∈ V is labeled   by the program symbol ( function or value ) that it   represents in y(or a special symbol for the root   node ) , and the set of edges E={(p , c)}expresses   parent - child relations between the nodes .   We capture sibling relations by defining a graph   based on the tree Tthat contains an edge set Eof   sibling edges : G= ( V , E ∪ E ) . Specifically , for   each parent node p , the program yinduces an order   over the children of p:(c , ... , c ) , where Nis   the number of children . We then define E=/uniontext{c , c } , that is , all consecutive siblings   will be connected by edges .   We define a local structure of size nas the subset   Gof all connected sub - graphs of size ninG   such that for every pair ( x , y)of nodes in Git   holds that ( x , y)∈ Eiffxandyare both leaves   inG. That is , informally , the relations between   nodes in the the sub - graph include parent - child and   siblings , but not e.g. cousins or uncles . All program   symbols are local structures of size 1 . Tab . 8 shows   a partial list of local structures for a given program .   B.1 Fixes for Local Structure Extraction   We try to fix syntax errors in the predictions made   using the auxiliary model to enable parsing them   to ASTs and extraction of LSs . We add or remove   closing parentheses based on the number of missing   or redundant parentheses at the end of the program .   C Dataset Details   We provide representative examples of the datasets   used in this work in Tab . 1 and Tab . 9 . We report   dataset sizes in Tab . 10 . Due to conversion errors ,   SMCalFlow - CS Simple has fewer training exam-   ples than SMCalFlow - CS . However , those missing   examples are not cross - domain examples .   We used publicly available datasets from pre-   vious peer - reviewed studies . Those datasets do   not contain any information that uniquely identi-   fies individual people or offensive content . The   COVR-10 dataset is completely synthetic . The   GeoQuery dataset contains only basic information   about U.S. geography . SMCalflow - CS contains   crowd - sourced queries collected in a simulated en-   vironment .   D Prompt Format and Examples   We add special prefixes “ source : ” and “ target : ” for   retrieved source - target pairs and separate them with   break lines . Tab . 11 shows prompt examples for   different demonstration selection methods , where   the only prompt that contains all the required pro-   gram symbols and produces the correct prediction   is Cover - LS ’s prompt.1414   E DPP Details   DPPs are probabilistic models that are effective at   modeling a distribution on all the subsets of the   ground set Tjointly considering the quality and   diversity . A subset Dis drawn according to the   probability distribution P :   P(D ⊂ T ; L)∝det(L ) ( 1 )   Where L∈Ris a PSD matrix and Lis the   submatrix of Lindexed by items in D.Lmatrix   takes into account the quality of each training ex-   ample and its similarity to other training examples   through :   L = qϕϕq ( 2 )   withq∈Rbeing normalized retriever scores that   model the quality of each example ; and { ϕ }   denoting normalized tf - idf vectors over LSs , which   model the different aspects that are contained   within each training example . The dot product   of those vectors is used to model the similarity   between two train examples .   log det ( L)is a submodular function which   satisfies the diminishing marginal returns property .   Therefore , we can find a subset of training exam-   plesD ⊂ T , |D|=kthat maximizes it in a feasi-   ble manner using a greedy optimizer ( Kaushal et al . ,   2022 ) . Specifically , we used the Naive Greedy opti-   mizer . We used scikit - learn ( Pedregosa et al . , 2011 )   for calculating tf - idf vectors .   F Finetuning Details   We provide implementation details for finetuning   experiments ( we use the same configuration for   all FT experiments and training of the auxiliary   model ) . We finetune the T5 - large model ( 770   million parameters ) with the AdamW optimizer   ( Loshchilov and Hutter , 2019 ) and a learning rate   of1e . We use a polynomial decay learning rate   with an ending rate of 1e , and 100 warmupsteps . We train for 250/50/70 epochs and evalu-   ate on the validation set every 3/5/10 epochs for   Geo / SMCalFlow ( both versions)/COVR respec-   tively . We use batches of size 8 for all datasets ( and   gradient accumulation in case batch can not fit in   memory ) . We used a single GPU for each T5 - large   finetuning experiment : Nvidia GeForce RTX 3090   when training on GeoQuery and COVR-10 , and   A100 ( 80 GB ) for SMCalFlow - CS and SMCalFlow-   CS Simple . GeoQuery experiments with prompts   trained for an average of 2 hours , COVR for 8   hours , and SMCalFlow - CS Simple for 41 hours .   We use the AllenNLP library ( Gardner et al . ,   2018 ) for training and evaluation . We use Rank-   BM25 ( Brown , 2020 ) as a BM25 implementation .   Standard deviation We report standard devia-   tion results in the FT setup in Tab . 13 . Results are   computed across 3 random seeds .   G NoFT Details   All NoFT experiments were conducted using the   OpenAI inference API with the sampling tempera-   ture set to 0 . Our setup requires a single API call   per test instance . The total number of API calls is   estimated at 160K.   Standard deviation We report standard devia-   tion results in NoFT setup in Tab . 12 . Results are   computed using 3 random seeds for a subset of 100   test examples .   Tuning the number of beam candidates We   use the development set to tune the number of   beam candidates Bwhen predicting local struc-   tures . Tab . 14 shows the results of using different   values of Bin NoFT setup on a random subset   of 100 development examples . Prompts are con-   structed using Cover - LS with k = 8 demonstrations .   H Artifact Licensing   We include license information for all artifacts used   in this work in Tab . 15 . Our use of artifacts was1415consistent with their intended purpose when it was   specified .   I GenBench Evaluation Card   Our GenBench ( Hupkes et al . , 2022 ) evaluation   card is presented in Fig . 6.14161417141814191420ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Ethics Statement   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix H   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Appendix H   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Appendix C   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Ethics Statement , Section 4.1   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Appendix C   C / squareDid you run computational experiments ?   Section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendices F - G1421 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4 , Appendices F - G   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4 , Appendices F - G   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendices D - G   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.1422