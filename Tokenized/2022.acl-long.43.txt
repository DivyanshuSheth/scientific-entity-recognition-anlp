  Rui WangTong Yu * Handong ZhaoSungchul Kim   Subrata MitraRuiyi ZhangRicardo HenaoDuke UniversityAdobe Research   { rui.wang16,ricardo.henao}@duke.edu   { tyu,hazhao,sukim,sumitra,ruizhang}@adobe.com   Abstract   Previous work of class - incremental learning   for Named Entity Recognition ( NER ) relies   on the assumption that there exists abundance   of labeled data for the training of new classes .   In this work , we study a more challenging   but practical problem , i.e. , few - shot class-   incremental learning for NER , where an NER   model is trained with only few labeled samples   of the new classes , without forgetting knowl-   edge of the old ones . To alleviate the prob-   lem of catastrophic forgetting in few - shot class-   incremental learning , we generate synthetic   data of the old classes using the trained NER   model , augmenting the training of new classes .   We further develop a framework that distills   from the NER model from previous steps with   both synthetic data , and real data from the cur-   rent training set . Experimental results show   that our approach achieves significant improve-   ments over existing baselines .   1 Introduction   Existing models of Named Entity Recognition   ( NER ) are usually trained on a large scale dataset   with predefined entity classes , then deployed for   entity extraction on the test data without further   adaptation or refinement . In practice , data of new   entity classes that the NER model has not seen   during training arrives constantly , thus it is desir-   able that the NER model can be incrementally up-   dated over time with knowledge of data for these   new classes . In this case , one challenge is that   the training data of old entity classes may not be   available due to privacy concerns or memory lim-   itations ( Ma et al . , 2020 ) . Then , the model can   easily degrade in terms of the performance on old   classes when being fine - tuned with only annota-   tions of new entity classes , i.e. ,catastrophic for-   getting . In addressing this problem , previous work   in class - incremental learning for NER ( Monaikulet al . , 2021 ) regularizes the current model by dis-   tilling from the previous model trained on old ( ex-   isting ) classes , using text from the training dataset   of new classes . However , this requires abundance   of data in the new training dataset being used for   distillation . Such an assumption is usually unreal-   istic since the token - level annotations required by   NER training are labor - consuming and scarce , espe-   cially for the new unseen classes . In this paper , we   study a more realistic setting , i.e. , few - shot class-   incremental learning for NER , where the model   ( i ) incrementally learns on new classes with few   annotations , and ( ii ) without requiring access to   training data for old classes .   There is very limited work in few - shot class-   incremental learning for NER . Such a setting is   more challenging compared with class - incremental   learning for NER . First , the few - shot datasets in   few - shot class - incremental learning may not con-   tain enough information for the trained model to   generalize during testing . Second , it is more chal-   lenging to solve the catastrophic forgetting prob-   lem in few - shot class - incremental learning when   data for old classes is not available and new data   is scarce . In class - incremental learning for NER   ( Monaikul et al . , 2021 ) , the same training sequence   may contain entities of different classes . There-   fore , when the training dataset for new classes is   sufficiently large , its context , i.e. , words labeled   as not from entities of new classes , may also con-   tain abundant entities of the old classes . That is ,   the new training data can be regarded as an unla-   beled replay dataset of the existing entity classes .   In such case , we can simply address the problem   of catastrophic forgetting by distilling from the pre-   vious model ( trained on old classes ) to the current   one , using text from such a replay dataset ( Mon-   aikul et al . , 2021 ) . However , in few - shot class-   incremental learning , we can not expect to avoid   catastrophic forgetting by distilling with only the   few samples from the new training dataset , since571there may not exist sufficient ( if any ) entities of the   old classes .   In this paper , we propose a framework to enable   few - shot class - incremental learning for NER . As   mentioned above , since the few - shot dataset may   not contain enough entities of old classes as replay   data for distilling from the previous model , which   leads to catastrophic forgetting , we consider gen-   erating synthetic data of the old entity classes for   distillation . Such data is termed as synthetic replay .   Specifically , we generate synthetic data samples of   old classes by inverting the NER model . Given the   previous model trained on the old classes , we op-   timize the token embeddings of the synthetic data ,   so that predictions from the previous model can   contain old entity classes , given the synthetic data   as input . In this way , the synthetic data is likely to   contain entities of old classes , and distilling from   the previous model with such data will thus encour-   age knowledge preservation of old classes . Ad-   ditionally , to ensure the synthetic ( reconstructed )   data to be realistic , we propose to leverage the   readily available real text data for new classes , via   adversarially matching the hidden features of to-   kens from the synthetic data and those from the   real data . Note that the synthetic data generated   from such adversarial match with real data will   contain semantics that are close to the real text   data for new classes . Consequently , compared with   training with only the few samples of new classes ,   the synthetic data will provide more diverse con-   text that are close to the samples of the few - shot   dataset , augmenting the few - shot training for the   new classes . Further , with the generated synthetic   data , we propose a framework that trains the NER   model with annotations of the new classes , while   distilling from the previous model with both the   synthetic data and real text from the new training   data . Our contributions of this work are summa-   rized as follows :   •We present the first work of studying few - shot   incremental learning for Named Entity Recog-   nition ( NER ) , a more practical but challeng-   ing problem compared with class - incremental   learning for NER .   •We approach the problem by proposing a   framework that distills from the existing   model with both , real data of new entity   classes and synthetic data reconstructed from   the model as replay data of old entity classes .   •Experiments show that our method signifi-   cantly improves over existing baselines for   the task of few - shot class - incremental learn-   ing in NER .   2 Background   2.1 Problem Definition   Assume there is a stream of NER datasets   D , . . . , D , . . . , annotated with disjoint entity   classes , where tis the time step and D=   { ( X , Y)}contains centity classes . Here   X= [ x , · · · , x]andy= [ y , · · · , y ]   are the NER token and label sequences , respec-   tively , with length N , and|D|is the size of the   dataset . Dataset Dis the base dataset , assumed of   reasonably large size for classes of step t= 1 . The   datasets { D}are the few - shot datasets with   about Ksamples for each class . In few - shot class-   incremental learning , the NER model will be incre-   mentally trained with D , D , . . . , over time , with   data from Donly available at the ttime step .   After being trained with D , the model will be eval-   uated jointly on all entity classes encountered in   D,···,D , i.e. , we do not learn separate predic-   tion modules for each time step . Figure 1 shows an   example of annotations for different incremental   learning steps on classes of PER , LOC , and TIME .   In Figure 1 , we should note that tokens that are   labeled as Oin the current step are likely to contain   abundant entities from the previous classes . For   instance , tokens annotated as Oin step 3 include   entities of previous classes , i.e. ,PER andLOC .   Therefore , when a large amount of training data is   available for the new classes , the new dataset can   be regarded as unlabeled replay data of previous   classes . As an example , in Monaikul et al . ( 2021),572   their performance of class - incremental learning on   CoNLL2003 has been comparable or even better   than training with full annotations of all the classes   encountered , by just distilling with the training data   of the new classes . However , in few - shot class-   incremental learning , the few training samples of   the current step may not contain enough entities of   the previous classes . In Section 4 , we also discuss   the difference between few - shot class - incremental   and few - shot learning for NER .   3 Few - Shot Class - Incremental Learning   for NER   3.1 The Proposed Framework   Following Beltagy et al . ( 2019 ) ; Souza et al . ( 2019 ) ,   we use the BERT - CRF as our NER model , which   consists of a BERT(Devlin et al . , 2018 ) en-   coder with a linear projection and a conditional ran-   dom field ( CRF ) ( Lafferty et al . , 2001 ) layer for pre-   diction . We denote Mas the NER model for step t.   Mis initialized from Mto preserve knowledge   of old classes . For time step t > 1,Mis expected   to learn about the new classes from D , while   not forgetting the knowledge from { D } . As-   sume we have already obtained a synthetic dataset   D={E , Y}of previous entity classes   from{D } , where E= [ e , · · · , e]and   Y= [ y , · · · , y]are the reconstructed to-   ken embeddings and reference label sequence . Y   is a randomly sampled label sequence containing   classes from the previous steps and Eis opti-   mized so the output from MwithEmatches   Y. We will discuss the construction of the syn-   theticDin Section 3.2 . Given the current training   dataDandMthat has been trained on D ,   we propose to train Mby distilling from Mwith both the real data from Dand synthetic data   fromD. The challenge of such distillation is that   the predictions from MandMare likely to   contain different set of labels , i.e. ,Mshould also   predict with the new entity classes from D. This   is different from the standard setting of distillation ,   where the teacher and student models share the   same label space ( Hinton et al . , 2015 ) . In tackling   such a problem of label space discrepancy , we pro-   pose separate approaches of distillation for Dand   D , respectively .   3.1.1 Distilling with Real Data D   The distillation from MtoMinvolves match-   ing the output distributions between MtoM.   However , given an input sequence XfromD , the   CRF layer outputs correspond to a sequence - level   distribution P(Y|X),i.e . , probabilities for all pos-   sible label sequences of X , the cardinality of which ,   grows exponentially large with the length of X.   Therefore , it is infeasible to match with the exact   output distributions of CRF . Following the current   state - of - the - art approach of NER distillation ( Wang   et al . , 2020b ) , we approximate the sequence - level   output distribution of CRF with only its top Spre-   dictions . Specifically , for model M , we have ,   ˆP(Y|X ) = [ P(ˆY|X ) , . . . , ( 1 )   P(ˆY|X),1−XP(ˆY|X ) ] ,   where { ˆY}are the top Smost probable pre-   dictions of label sequence from M. We set   S= 10 . In this way , the output from the CRF   ofMbecomes tractable . However , Mstill   can not be trained with such an output from M.   This is because Mwas not trained with the573new classes in D. Therefore , when Xis from D ,   Mwill have wrong predictions on the tokens   labeled as being from entities of new classes . In   order to distill with M , we propose a correc-   tion for { ˆY } . Figure 2(b ) shows an example   of such a process . Specifically , on the positions of   the sequence where Dhas labeled as new classes ,   we replace the predictions in { ˆY}with the an-   notations from D. We denote the corrected set of   predictions as { ˆY } . For training of M , we   first calculate the predicted distribution of Mwith   respect to { ˆY } , as   ˆP(Y|X ) =[ P(ˆY|X ) , · · · , P(ˆY|X ) ,   1−XP(ˆY|X ) ] , ( 2 )   where we compute the predicted probabilities from   Mwith regard to { ˆY}from M. Then ,   Mcan be trained by minimizing the cross entropy   between ˆP(Y|X)andˆP(Y|X)via   L(D ) = ( 3 )   −1   |D|XCE(ˆP(Y|X),ˆP(Y|X ) ) ,   where CE(·,·)is the cross entropy function . Note   that the definition of Ois different in MandM.   Take Figure 2(b ) as an example , the prediction of   Oin step 2 corresponds to both OandTIME for   step 3 , since TIME is not in the target entity classes   of step 2 . However , from the annotation of step 3 ,   we know that tokens annotated as Oare not TIME .   Therefore , we can safely assume that the prediction   ofOin{ˆY}from Mmatches the definition   ofOinM , i.e. , the semantics of Oin{ˆY}is   the same for MandM.   3.1.2 Distilling with Synthetic Data D   Different from data in D , in which we know to-   kens annotated as Oare not from the new classes ,   data from Dis reconstructed from Mand only   contains labels for the previous classes . Any token   predicted with " O " from Mcan be potentially   labeled as Oor the new classes by M. Therefore ,   withD , it is unclear how to correct the output   of CRF from M , i.e.{ˆY } , for training of   M. By considering the above , we resort to another   approach that decomposes the output from CRF ,   i.e. , sequence level label distribution , into marginal   label prediction for each token , using the forwardand backward method in Lafferty et al . ( 2001 ) . Fig-   ure 2(a ) shows a graphic example of our distillation   lossLwithD. Specifically , let Cbe the cumu-   lative number of possible labels for any given token   in NER at step t , i.e. ,C = Pc , with cbe the   number of class in D. For each token with embed-   dinge , we define p= [ p;p;p]and   p= [ p;p]as the predicted marginal   distribution of a token from MandM , re-   spectively . p , p∈Rare the probabilities   for class O , whereas p , p∈Rare   the probabilities for entity classes encountered up   to step t−1 . Further , p∈Rare probabili-   ties for the new classes in step t. Since Ofrom   stept−1corresponds to the Oand the cnew   classes in step t , we first collapse pby comput-   ingˆp= [ sum ( p , p);p ] , where we   merge the predictions of Oandcnew classes . In   this way , ˆpwill have the same dimension as p.   LetEbe the set of embeddings for all tokens con-   tained in D. The distillation loss for Dis   L(D ) = EKL(ˆp||p ) , ( 4 )   where KL(·||·)is the KL divergence .   3.1.3 General Objective   The general objective of Mfor training at step t   is given by   L = L(D ) + αL(D ) , ( 5 )   where L(·)andL(·)corresponds to distilla-   tion with the real data in Dand synthetic data in   D , respectively , and αis a parameter balancing   between the losses for DandD. We set α= 1 in   the experiment .   3.2 Synthetic Data Reconstruction   Now we describe how to reconstruct Dfrom   M. Given a randomly sampled label sequence   Ycontaining the old entity classes from { D } ,   we seek to reconstruct the embedding sequence E   corresponding to its training data . In doing so , we   randomly initialize embeddings E , then optimize   the parameters of Ewith gradient descent so that   its output with Mmatches the expected label se-   quence Y. Formally , we optimize Eby minimizing   the training loss of the CRF as   L=−logP(Y|E ) . ( 6 )   One problem of such reconstruction is that the re-   sulting synthetic Emay not be realistic . This will574result in a domain gap of training on the synthetic   data of old entities but testing on the real data . To   alleviate this problem , we propose to encourage   synthetic data to be more realistic by leveraging   the real data from D.   Leth(E)be the hidden state from the   llayer of the BERT encoder in M , regarding   the set of synthetic token embeddings , E. Simi-   larly , let h(emb(X))be the output hidden   states from the llayer of M , regarding the   set of real tokens , X , from D. Moreover , emb ( · )   is the embedding layer . We propose to adversar-   ially match h(E)andh(emb(X ) )   so that hidden states from the real and synthetic   are not far away from each other . In this way , the   reconstructed embeddings from Dare likely to be   more realistic . Specifically , let Mbe a binary dis-   criminator module , i.e. , one layer linear projection   with sigmoid output , whose inputs are the real and   synthetic hidden states ,   M= argmin−ElogM(h )   −Elog(1−M(h ) ) ,   L = Elog(1−M(h ) ) . ( 7 )   Finally , the loss for reconstructing Dis   L = L+βXL , ( 8)   where l= 2,4,···,12,i.e . , we match every two   layers of the BERT encoder in M.βis a balanc-   ing parameter and is default to 10 in the experiment .   Since we train Mwith the reconstructed token em-   beddings from M , we freeze the BERT token   embedding layer during training , so that Mand   Mcan share the same token embeddings . This   is also reasonable for the setting of few shot learn-   ing , since tuning all the model parameters with few   samples will result in overfitting .   Another problem we should consider is that the   real data Dand synthetic data Dmay contain   different sets of entity classes , i.e. , the few - shot   dataset Dmay not contain entities of old classes   inD. In this case , for the token embeddings of   old classes in D , s.t . ,{e|y̸=O } , matching   the hidden states of these embeddings with those   from Dwill distract these embedding from being   optimized into the entities of old classes , which   we will show in the experiments . Therefore , we   overload the definition of Ein(4)by excluding   embeddings of the old entity classes in Dfrommatching , i.e. ,E={e|y = O } , while X   contains all the real tokens from D. Algorithm 1   shows the complete procedure for constructing D.   SinceDcontains entities of old classes from   previous steps , distilling with L(D)will help   preserving knowledge of old entity classes , i.e. ,   avoiding catastrophic forgetting , without access-   ing the real data from previous steps . Additionally ,   withD , Mis no longer trained with only few   samples from D , thus is less likely to overfit . This   is because Dcan construct a relative larger scale ,   e.g. , several thousand sentences , within the com-   putation limit . Additionally , the semantics of D   can be close to D , since their token embeddings   are closely matched . Thus , compared with train-   ing only with D , Dprovides more diverse text   information for Mduring training . Moreover , the   entity of old classes from Dcan be regarded as   negative samples for training of the new classes in   D , thus reducing the confusion between old and   new classes for Mduring training .   4 Related Work   Class - Incremental Learning : Different from con-   tinual learning , e.g. , ( Hu et al . , 2018 ) , which se-   quentially learn on different tasks ( usually with   different classes ) and requires task labels for pre-   diction , class - incremental learning aims at jointly   predicting with all the encountered classes without   knowing task labels . Sun et al . ( 2019 ) ; Ke et al .   ( 2021 ) have study continual learning for different   tasks of NLP . Recently , Monaikul et al . ( 2021 ) stud-   ies class - incremental learning for NER , building a   unified NER classifier for all the classes encoun-   tered over time . There are two problems regarding   this method . Firstly , Monaikul et al . ( 2021 ) only   works with a non - CRF - based model . However ,   many current state - of - the - art NER models are built   with a CRF module ( Liu et al . , 2019 ; Chen et al . ,   2020 ; Wang et al . , 2021 ) . Secondly , it assumes   that a large amount of data for the new classes is   available , which is unrealistic since annotations for   unseen classes are usually scarce . In this work ,   we assume only few - shot datasets are available for   the new classes , i.e. , few - shot class - incremental   learning , which was proposed in Tao et al . ( 2020 ) ;   Mazumder et al . ( 2021 ) , yet not studied in NER .   Also , note that class - incremental learning is dif-   ferent meta - learning with episode training ( Ding   et al . , 2021 ; Finn et al . , 2017 ) , since tasks / classes of   meta - leaning may appear multiple times in episode575training , while we assume the dataset of each class   only appear once in class - incremental learning .   Few - Shot Learning : Models of few - shot learn-   ing are generally trained with a base dataset , then   learned to predict unseen target classes with few   samples . One branch of the works is based on met-   ric learning . These generally involves predicting by   learning to compare token features with class pro-   totypes ( Hou et al . , 2020 ) or stored query samples   ( training data ) of target classes ( Yang and Kati-   yar , 2020 ) . The latter violates our setting of class-   incremental learning , for which it is prohibitive to   store the training data for e.g. , privacy issue . Al-   ternatively , Huang et al . ( 2020 ) avoids overfitting   of few - shot learning by augmenting with noisy or   unlabeled data from the web . Our approach is simi-   lar to Huang et al . ( 2020 ) , in that we also augment   few - shot training of the current step with additional   data , except we use generated synthetic instead of   real data . Recently , ( Cui et al . , 2021 ) proposes Tem-   plate NER , a few - shot friendly model for NER that   convert NER into a sequence - to - sequence problem .   Our few - shot class - incremental learning is different   from few - shot learning in that i)Few - shot learning   requires data of different classes arrives at the same   time and with complete annotations for all the tar-   get classe , while data of few - shot class - incremental   learning arrives sequentially , containing annotation   of only classes of the current step . ii)Existing   works of few - shot NER build separate prediction   modules for the target and base classes and ignore   the performance of base classes during evaluation ,   thus incompatible with class - incremental learning .   Data - Free Distillation : Data - free distillation   refers to the case in which we distill from a teacher   model to a student model with the training data of   the teacher not available . A typical solution is to   reconstruct synthetic training data from the trained   teacher model for distillation . Such a setting was   previously explored for model compression of im-   age classification ( Yin et al . , 2020 ) and text classi-   fication ( Ma et al . , 2020 ) . However , it has not been   studied for NER scenarios . We use data - free distil-   lation for transferring knowledge between models   from the current and previous steps for few - shot   class - incremental learning .   5 Experiments   5.1 Datasets and Implementation   Following the previous work of class - incremental   learning for NER ( Monaikul et al . , 2021 ) , weAlgorithm 1 Algorithm for constructing Dfrom   M.   Input : Model from the previous step , M ,   set of old classes up to t−1,V={v } .   Output : The reconstructed data D.   D=∅   forvinVdo   foriin1···Ndo   Uniformly sample n∈[1 , n ] .   Uniformly sample n∈[n , n ] .   Uniformly sample k∈[1 , n−n+ 1 ] .   Construct a target label sequence Yof   length n , with a length nentity of class   vstarting from position k.   Randomly initialize an embedding se-   quence Eof length n.   while not converge do   Update Ewith ( 8)   end while   Add{E , Y}toD.   end for   end for   experiment with two datasets : CoNLL2003 and   Ontonote 5.0 . For CoNLL2003 , our results are av-   erage over eight ordering of entity classes for each   step as in Monaikul et al . ( 2021 ) . For Ontonote   5.0 , we rank the entities in alphabetic order and   experiment with two combinations of different en-   tity classes for different steps . Table 3 and 4 in the   Appendix list the entity classes used for each step .   Since CoNLL2003 is a relative smaller dataset , we   conduct both 5 - shot and 10 - shot experiments for   CoNLL2003 and 5 - shot experiments for OntoNote   5.0 . Following Yang and Katiyar ( 2020 ) , our base   datasets , i.e. , dataset of step 1 , is the training data   of CoNLL2003 and OntoNote 5.0 , labeled with   only entity classes included in step 1 . The few - shot   datasets are sampled from the evaluation dataset   with greedy sampling ( Yang and Katiyar , 2020 ) .   The resulting NER model of each step is tested on   the entire test set . Please refer to the Appendix for   addition details .   5.2 Baselines and Ablation Study   We compare with the state - of - the art work of class-   incremental learning for NER ( CI NER ) . Addition-   ally , we implement EWC++ ( Chaudhry et al . , 2018 )   withα= 0,i.e . , using weights regularization to   avoid forgetting instead of generating synthetic   data . We also implement FSLL ( Mazumder et al . ,576   2021 ) , a state - of - the - art method of few - shot class-   incremental learning for image classification with   metric learning . As mentioned in the related work   section , our method can be considered as data - free   distillation . Therefore , we also include AS - DFD   ( Ma et al . , 2020 ) , the state - of - the - art method of   data - free distillation in text classification . Specif-   ically , we construct Dwith the adversarial regu-   larization described in AS - DFD instead of ( 8) . We   also adapt L - TAPNet+CDT ( Hou et al . , 2020 ) for   comparison . L - TAPNet+CDT is a state - of - the - art   work of few - shot learning for sequence labeling   with CRF module . Please refer to Appendix for   how we adapt it for class - incremental learning .   As an ablation study , we compare our method   with : i ) Ours ( α= 0 ) , only train with only D   withα= 0.ii ) Ours ( α= 0 , marg ) , which is   also training with α= 0 . The difference is that   instead of using the sequence - level distillation with   L(D ) , we decompose the output of CRF into   marginal predictions for each token , as described   before ( 4 ) . In this way , we can directly apply the   token - level distillation in CI NER ( Monaikul et al . ,   2021 ) for the CRF - based NER model . Compared   with Ours ( α= 0 ) , this is included to show the per - formance of directly applying token - level distilla-   tion for CRF - based model . iii)InOurs ( β= 0 ) , we   examine the usefulness of Lby setting β= 0 .   iv ) Ours ( all tokens ) , which matches all the syn-   thetic tokens in Dwith real tokens in D , instead   of matching with only those labeled as OinD , as   described after eq ( 8) .   5.3 Results of Few - Shot Class - Increnmental   Learning   Table 1 and 2 show the F1 scores from different   steps of few - shot class - Incremental learning on   CoNLL2003 . The values are averaged over eight   permutations as in ( Monaikul et al . , 2021 ) . Our   methods outperform all the considered baselines   for both 5 - shot and 10 - shot learning . Especially ,   CI NER ( Monaikul et al . , 2021 ) has the worst re-   sult among all the methods . This is because the   performance of CI NER relies on a large amount of   data from Dfor replay of previous entities . There-   fore , it does not work well in the few - shot scenario ,   where Dwith only few samples may not contain   entities of old classes for replay . Additionally , we   find that the performance of AS - DFD ( Ma et al . ,   2020 ) is slightly lower than Ours ( β= 0),i.e . ,577   distilling using data reconstructed with only L.   AS - DFD is designed for text classification , where   they use the feature of the special token [ CLS ]   from BERT for classification , while features of the   non - special tokens ( within text ) are trained with an   augmented task of language modeling . However ,   in NER , features of the non - special tokens are di-   rectly used for prediction . Thus , simultaneously   training such features with language modeling may   distract the model from learning the task specific   information needed for NER .   In the ablation study , we find that our adversarial   matching indeed improves the quality of the syn-   thetic data ( Ours vs .Ours ( β= 0 ) ) , especially   when excluding tokens of the reconstructed old en-   tities from matching ( Ours vs .Ours ( all tokens ) ) .   Further , Ours ( α= 0 , marg ) has lower perfor-   mance than Ours ( α= 0 ) , showing that it might   not be optimal to directly apply CI NER ( Monaikul   et al . , 2021 ) with CRF based models .   Figure 3 shows the results of class - incremental   with OntoNote 5.0 . Since there are more steps rel-   ative to the experiments for CoNLL2003 , follow-   ing previous works in few - shot class - incremental   learning ( Tao et al . , 2020 ; Mazumder et al . , 2021 ) ,   we plot the F1 scores as curves , to highlight the   relative difference of different methods over time .   Our method consistently outperforms the baselines .   Note that with larger number of steps of incre-   mental learning , the curves may not be necessarily   monotonically decreasing . This may indeed hap-   pen because training with some classes can benefit   the performance of other downstream classes , thus   ( locally ) increasing the overall performance . In   Appendix , we also present the ablation study with   OntoNote 5.0 .   5.4 Visualization of Token Embeddings   Figure 4 shows the t - sne plots of hidden states of   tokens from 10 - shot LOC→PER ( explained in the   caption ) . In ( a ) , we can find that there are syn-   thetic tokens that are very close to the real LOC   tokens ( green dots in the black ellipse ) . These   synthetic tokens ( within the black ellipse ) are the   reconstructed LOC . On the contrary , the synthetic   context , i.e. , the rest of the synthetic tokens outside   the ellipse , are far away from the real distribution .   This may because the context contains more di-   verse information , which makes it more difficult   to be reconstructed . Such a difference between   real and synthetic tokens may cause a domain shift   between training and testing , since we are train-   ing on synthetic token and testing on real tokens.578Note that there is no tokens from D(red dots )   in the black ellipse of LOC , indicating that there   may not be LOC entities in the few - shot dataset   D , unlike in non - few - shot learning where Dcan   contain a lot of entities of the old classes ( LOC ) .   ( b ) shows the result of matching all the synthetic   tokens from Dwith all the real ones from D. In   this way , most of the synthetic tokens are matched   with the real ones , except that only few synthetic   tokens are aligned with the real LOC tokens . This   is because the few - shot dataset Dmay not con-   tain entities from the old classes LOC . In this case ,   the adversarial matching will distract synthetic to-   kens from being reconstructed as LOC . Then , the   reconstructed embedding sequences will contain   less information from the old classes ( LOC ) . In ( c ) ,   we exclude synthetic tokens that are intended to   be reconstructed as the old class LOC , i.e. , labeled   asLOC in the target label sequence Yin Algo-   rithm 1 . As a result , the synthetic tokens contain   both LOC and context that is aligned with the real   distribution .   5.5 Results of Varying β   We investigate the relationship between model per-   formance and the value of β , i.e. , the parameter   controlling the degree of adversarial matching . Fig-   ure 5 shows the the F1 scores from different steps   on CoNLL2003 , with different values of β . We   experiment with 10 - shot and report the gain of   F1 score compared with β= 0 . We noticed that   there is positive gain of the average F1 score on   the whole experiment for a range of βvalues , i.e. ,   [ 1,16 ] . These results demonstrate that the proposed   adversarial matching between the real and synthetic   data ( DandD ) is generally beneficial and is not   sensitive to the selection of β .   6 Conclusion   We present the first work of few - shot class-   Incremental learning for NER To address the prob-   lem of catastrophic forgetting , we proposed to re-   construct synthetic training of the old entity classes   from the model trained at the previous time step .   Additionally , the synthetic data allows the model   to be trained with a more diverse context , thus less   likely to overfit to the few training samples of cur-   rent step . Experimental results showed that our   method outperforms the baselines , enabling the   NER model to incrementally learning from new   classes with few samples . Acknowledgements   This work was carried out during an internship at   Adobe Research . Further , it was supported by NIH   ( NINDS 1R61NS120246 ) , DARPA ( FA8650 - 18 - 2-   7832 - P00009 - 12 ) and ONR ( N00014 - 18 - 1 - 2871-   P00002 - 3 ) . We thank all the researchers involved   from Adobe Research and the support from Duke   University .   References579580   A Entity Classes for each step   B Algorithm for Data Reconstruction   Algorithm 1 shows the procedure of sampling syn-   thetic label sequences and reconstructing the token   embedding sequence . Specifically , we construct a   label sequence by sampling a length for the token   sequence and entity span , respectively .   C Ablation Study for OntoNote 5.0   Figure 6 shows the ablation study on OntoNote 5.0 .   α= 0corresponds to training only with eq L.β= 0 means reconstructing without adversarial   training .   D Additional Details   We set the learning rate for our NER model is 5e-5 .   The model is trained with 50 epochs of D , with   learning batch size of 1 for 5 - shot and 2 for 10 - shot   experiments . The batch size of training with Dis   5 for each reconstructed class . Following Ma et al .   ( 2020 ) , the token embeddings in Dare initialized   withN(0,0.35 ) , then optimized with a learning   rate of 1e-2 . We have n= 4andn= 30 .   Our code is modified based on Huggingfacewith   python 3.7 and pytorch 1.7.0 , run on 8 P100 GPUs ,   each with a 16 GB memory . Following the prior   work , when constructing D , we add the tokens   of[CLS ] and[SEP ] before and after the sequence   of token embeddings that are intended to be re-   constructed , so that it is consistent with the input   format of BERT . Then , we append after [ SEP ] with   [ PAD ] to construct a padded embedding sequence   of length 128 . In algorithm 1 , we freeze the embed-   ding of [ CLS ] , [ SEP ] and[PAD ] , i.e. , the gradients   from eq ( 10 ) in the main paper are not backpropa-   gated into embeddings of [ CLS ] , [ SEP ] and[PAD ] .   At each time step , we reconstructed 150 samples   for each previous class . These samples are dis-   carded after training of the current step .   E Adapting L - TapNet+CDT for   Class - Incremental Learning   There are two challenge adapting L - TapNet+CDT   for class - incremental learning . i)Unlike dew - shot581learning , where the few - shot dataset contains com-   plete annotations for all the target classes , dataset of   class - incremental leanring only contains data of the   current classes , i.e. , entities of old or future classes   are labeled as O.ii)The sematics of Ochanges with   different time step . Without a complete annotation   of both current and previous classes , it is unclear   how to collect Otokens for constructing the proto-   type of Ofor the current class , since tokens labeled   asOin the current step may be within span if old   entity classes .   To solve the above problem , for each time step ,   we samples a fake few - shot dataset labeled with   both current and old classes . We use entities of   new classes to construct corresponding prototypes .   These prototypes are saved for future steps , i.e. ,   fixed as classification weights for classification of   these classes . We use tokens labeled as Oin the   fake dataset for constructing the current Oproto-   types . Labels for the old classes in the fake dataset   are not directly used for prediction . Note that this   can be a easier problem compared with few - shot   class - incremental learning , since the fake dataset   contains labels of old classes , though we avoid   using them for prediction.582