  Thomas HartvigsenSaadia GabrielHamid PalangiMaarten Sap   Dipankar RayEce KamarMassachusetts Institute of TechnologyUniversity of WashingtonMicrosoft ResearchAllen Institute for AICarnegie Mellon UniversityMicrosoft   Abstract   Toxic language detection systems often falsely   ﬂag text that contains minority group men-   tions as toxic , as those groups are often the   targets of online hate . Such over - reliance on   spurious correlations also causes systems to   struggle with detecting implicitly toxic lan-   guage . To help mitigate these issues , we cre-   ate TG , a new large - scale and machine-   generated dataset of 274k toxic and benign   statements about 13 minority groups . We de-   velop a demonstration - based prompting frame-   work and an adversarial classiﬁer - in - the - loop   decoding method to generate subtly toxic and   benign text with a massive pretrained language   model ( Brown et al . , 2020 ) . Controlling ma-   chine generation in this way allows TG   to cover implicitly toxic text at a larger scale ,   and about more demographic groups , than pre-   vious resources of human - written text . We   conduct a human evaluation on a challeng-   ing subset of TGand ﬁnd that annota-   tors struggle to distinguish machine - generated   text from human - written language . We also   ﬁnd that 94.5 % of toxic examples are labeled   as hate speech by human annotators . Using   three publicly - available datasets , we show that   ﬁnetuning a toxicity classiﬁer on our data im-   proves its performance on human -written data   substantially . We also demonstrate that T-   Gcan be used to ﬁght machine - generated   toxicity as ﬁnetuning improves the classiﬁer   signiﬁcantly on our evaluation subset .   1 Introduction   Toxic language detectors often over - rely on minor-   ity identity mentionswhen ﬂagging a statement   as toxic , without considering the deeper seman-   tic meaning of the statement ( Dixon et al . , 2018 ;   Röttger et al . , 2021 ) . This can lead to severe under-   detection of subtle hate ( e.g. , “ They have been bredto be good at sports and entertainment , but not   much else ” ; Figure 1 ) and over - detection of benign   statements ( e.g. , “ child abuse is wrong , racism   is wrong , sexism is wrong ” ; Figure 1 ) . Impor-   tantly , such biases in toxicity detection risk further   marginalizing or censoring minority groups ( Yasin ,   2018 ; Sap et al . , 2019 ; Dias Oliva et al . , 2020 ; Are ,   2020 ; Díaz and Hecht - Felella , 2021 ) .   We introduce TG , a large - scale machine-   generated dataset of 274,186 toxic and benign state-   ments . To create this dataset , we leverage the   massive pretrained language model GPT-3 ( Brown   et al . , 2020 ) , which is known to produce close-   to - human - like text ( Clark et al . , 2021 ; Dou et al . ,   2021 ) but also easily generates socially biased and   toxic content ( Sheng et al . , 2019 ; Gehman et al . ,   2020 ) . While such human - like bias and toxicity   poses real threats , we use this undesirable behavior   in models like GPT-3 to improve existing toxic lan-   guage classiﬁers , providing a path forward for miti-   gating systemic bias . Created using demonstration-   based prompting and pretrained toxicity classiﬁers ,   TGcovers over 135k toxic and 135k benign   statements about 13 minority identity groups ( e.g. ,   African Americans , women , LGBTQ+ folks , etc . ) .   Using this machine generated approach has two   advantages over scraping posts from the web as   done by previous work ( e.g. , Davidson et al . , 2017 ;   Founta et al . , 2018 ; Zampieri et al . , 2019 ) . First ,   it allows us to limit spurious identity - toxicity cor-   relations ( Dixon et al . , 2018 ; Zhou et al . , 2021 )   by generating equal numbers of toxic / benign state-   ments for each demographic group , including those   that are often overlooked in toxic language corpora   ( e.g. , Native Americans ) . Second , machine genera-   tion and careful prompting enables us to generate   implicit toxicity ( i.e. , without swearwords or slurs ) ,   which is by deﬁnition hard to detect or ﬁnd and thus   often missing in toxic language corpora ( Wiegand   et al . , 2021 ) . Indeed , 98.2 % of TGstate-   ments are implicit , i.e. , devoid of explicit profanity,3309   slurs , or swearwords ( Table 1 ) .   To generate a challenging subset of TG ,   we introduce A , an adversarial classiﬁer - in-   the - loop decoding algorithm . We use A to   control the toxicity of output text by pitting a toxic-   ity classiﬁer against a text generator during beam   search decoding . Given a toxic prompt , we can   encourage generations to be less toxic based on   the classiﬁer scores . Similarly , we can steer a   language model with neutral prompting towards   higher toxicity generations . Our experiments with   ﬁve publicly - available toxicity classiﬁers show that   the generated sentences in both cases above fool   toxicity classiﬁers ( see Figure 1 ) .   We validate the quality of our machine - generated   dataset through a comprehensive human evaluation .   Our results show that on a sample of 792 machine-   generated sentences , 90 % could be mistaken for   human - written text . We also ﬁnd that the gener-   ated data indeed contains a wide variety of speciﬁc   references to the minority groups mentioned in the   prompts ( § 4.2 ) . This indicates that our data gen-   eration approaches ( with or without A ) suc-   cessfully control the generation towards the desired   toxicity and minority group mention .   Further experimental results demonstrate thatﬁne - tuning existing classiﬁers on TGcon-   sistently improves performance ( +7–19 % ) on 3 ex-   isting human -written implicit toxic datasets : Im-   plicitHateCorpus ( ElSherief et al . , 2021 ) , SocialBi-   asFrames ( Sap et al . , 2020 ) , and DynaHate ( Vidgen   et al . , 2021 ) . This indicates that the dataset gen-   erated in this work and the approaches for gener-   ating data provide major steps towards improving   toxicity classiﬁers , and could potentially be used   downstream to address the issues from biased ma-   chine generation ( Sheng et al . , 2019 ) or neutral   toxic degeneration ( Gehman et al . , 2020 ) .   We release our code and the TGdataset   publicly . We also include two models pretrained   on TGalong with our human evaluations .   2 Implicit Hate Against Minority Groups   Detecting implicit toxicity about minority groups   ( e.g. , stereotyping , microaggressions ) , remains an   elusive goal for NLP systems ( Han and Tsvetkov ,   2020 ; Wiegand et al . , 2021 ) . One key challenge is   that , in contrast to explicit toxicity , implicit toxicity   is not marked by the use of profanity or swear-   words , is sometimes positive in sentiment , and is   generally harder to detect or collect at scale ( MacA-   vaney et al . , 2019 ; Breitfeller et al . , 2019 ) . Nonethe-   less , implicitly toxic language about minority or   marginalized groups is often psychologically dam-   aging to members of those groups ( Sue et al . , 2007;3310   Nadal et al . , 2014 ; Kanter et al . , 2017 ; Nadal , 2018 ;   Saleem and Anderson , 2013 ) and can reinforce   stereotypical or hateful perceptions of them ( Behm-   Morawitz and Mastro , 2008 ; Soral et al . , 2018 ) .   A second challenge for detecting subtle toxicity   about minority groups is that minority mentions are   more often the targets of social biases and toxicity   ( Hudson , 2017 ) . As such , minority mentions often   co - occur with toxicity labels in datasets scraped   from online platforms ( Dixon et al . , 2018 ) . For ex-   ample , over 93 % of mentions of Jewish folk in Sap   et al . ( 2020 ) are toxic ( Wiegand et al . , 2021 ) . In   turn , models trained on such data can exploit these   spurious minority - toxicity correlations instead of   considering the deeper semantics of text ( Zhou   et al . , 2021 ) . Importantly , the spurious correla-   tions are also learned by large language models ,   which are known to produce stereotypical , biased ,   or toxic content when prompted with minority men-   tions ( Sheng et al . , 2019 ) . Given that the main mit-   igation approach to prevent Large Language Mod-   els ( LLM ) from generating toxic language is to   train new classiﬁers to detect such language , these   classiﬁers also learn the spurious correlations and   start blocking most language referencing minority   groups . This risks erasure ( Xu et al . , 2021 ) .   With TG , we aim for generating a large   scale dataset that represent implicit toxicity while   balancing between toxic and benign statements , to   address the gaps of previous work . As shown in   Table 1 , existing datasets contain large amounts   of explicit toxicity . While valuable , most previ-   ous work has relied on scraping data from online   platforms , which leads to dataset imbalances withrespect to minority - mentioning posts that are toxic   vs. benign . Examples are collected at scale us-   ing keyword - based scraping approaches ( Waseem ,   2016 ; Davidson et al . , 2017 ; Zampieri et al . , 2019 ) ,   the bootstrapped scraping approaches ( Founta et al . ,   2018 ) , and machine - vs - human adversarial data col-   lection ( Dinan et al . , 2019 ; Vidgen et al . , 2021 ) ,   among others . In contrast , using large language   models to generate our dataset allows us to control   the minority groups mentioned in our statements ,   as well as their implicitness , at larger scale .   3 Creating TG   To create TG , we use demonstration - based   prompting for LLMs , encouraging a text generator   to produce both toxic and benign sentences that   mention minority groups without using explicit lan-   guage . We introduce a classiﬁer - in - the - loop de-   coding method based on constrained beam search ,   A , which , along with samples generated with-   outA , contributes to generating a challenging   subset of TG . Using these methods , we gen-   erate a massive set of statements ( over 274,000 )   containing equal numbers of toxic and benign sen-   tences for 13 identity groups — see Table 2 .   3.1 Prompt Engineering   TGis generated by prompting a language   model to produce both benign andtoxic sentences   that ( 1 ) include mentions of minority groups by   name and ( 2 ) contain mainly implicit language ,   which does not include profanity or slurs . To   achieve this , we perform demonstration - based   prompt engineering : Acquiring example sentences,3311   passing them to a LLM , and collecting the sub-   sequent response . For example , when shown ten   sentences mentioning Muslim folks neutrally , GPT-   3 often responds with a similar neutral sentence .   3.1.1 Demonstration - based prompting   Prompts are text fragments passed into language   models that can encourage certain behaviors   ( Brown et al . , 2020 ) . However , designing prompts   is notoriously challenging ( Liu et al . , 2021c ) .   While there are several approaches for prompt-   ing pretrained LLMs ( Liu et al . , 2021b ) , a recent   and promising direction is demonstration - based   prompting ( Gao et al . , 2021 ; Mishra et al . , 2021 ) .   Here , example statements are passed to an LLMs ,   encouraging it to produce a similar , but distinct ,   statement . For example , to encourage LLMs to   produce benign statements about Asian people , we   might use the following examples : ( 1 ) many im-   pactful people are of asian descent , ( 2 ) there are   hundreds of asian cultures . To engineer prompts   that lead to high - quality , group - mentioning toxic   and benign statements at scale , we ﬁrst gather and   curate sets of examples .   3.1.2 Collecting demonstrations   To generate both benign and toxic responses from   LLMs that mention minority groups , we ﬁrst col - lect many examples . Intuitively , given many exam-   ples of benign sentences that mention one particular   group , a language model can be used to produce   more . For benign prompts , we encourage realistic   text generation and include diverse voices by col-   lecting benign sentences from blog posts and news   articles that mention a group . However , ﬁnding   large amounts of such data at scale is challenging —   this is why implicit datasets are hard to acquire .   To build a large enough set of demonstrations ,   we begin with a small number of examples from the   wild , then engage a human - in - the - loop process : col-   lect some demonstrations , pass them to our LLM ,   comb through many responses , and add the best   examples to a growing set . Ensuring that a set of ex-   amples consistently produces benign responses that   still mention the targeted minority group is chal-   lenging and so we iterate this loop many times , sam-   pling random subsets of our examples to serve as   prompts and observing the responses . This way , we   collect 20 - 50 demonstration sentences per group ,   all of which we release .   To encourage implicit toxicity from a LLM , we   ﬁnd examples of human - written sentences with im-   plicit toxicity towards each group from hate forums   ( de Gibert et al . , 2018 ) and Reddit ( Breitfeller et al . ,   2019 ) . We repeat the human - in - the - loop process to   expand our sets of examples . Overall , by repeating   this process for both toxic and benign examples for   all 13 target groups , we create 26 sets of prompts,3312   with two ( benign and toxic ) per target group .   3.2 A : Attacking Toxicity Classiﬁers   with Adversarial Decoding   Demonstration - based prompting alone consistently   produces toxic and benign statements about mi-   nority groups ( see Section 4 ) . There is no guar-   antee that these statements will be challenging to   existing toxicity detectors . Therefore , we also de-   velop A , a variant of constrained beam search   ( CBS ; Anderson et al . , 2017 ; Hokamp and Liu ,   2017 ; Holtzman et al . , 2018 ; Lu et al . , 2021 ) during   decoding that generates statements that are adver-   sarial to a given pre - trained toxicity classiﬁer .   A creates an adversarial game between a   pre - trained language model ( PLM ) and a toxicity   classiﬁer ( CLF ) during constrained beam search   decoding . In many CBS settings , constraints are   added during beam search decoding to force the   model to either include or exclude a speciﬁc wordor group of words in the output ( Anderson et al . ,   2017 ; Hokamp and Liu , 2017 ; Lu et al . , 2021 ) .   With A , we instead want to enforce softcon-   straints on the probabilities coming from a given   toxicity classiﬁer CLF during beam search :   logp(wjw)/   logp(wjw ) + logp(w )   ( 1 )   Here,anddenote hyperparameters that de-   termine the respective contribution of the language   model and classiﬁer to the decoding scoring func-   tion . By using this weighted combination , we can   steer generations towards a higher or lower prob-   ability of toxicity without sacriﬁcing coherence   enforced by the language model . To create exam-   ples that challenge existing toxicity classiﬁers , we   use two adversarial setups :   •False negatives : We use toxic prompts to en-   courage the language model to generate toxic   outputs , then maximize the classiﬁer ’s proba-   bility of the benign class during beam search .   •False positives : We use benign prompts to en-   courage the language model to generate non-   toxic outputs , then maximize the probability   of the toxic class during beam search .   In the ﬁrst approach , we are also able to detox-   ify model outputs when the classiﬁer successfully   steers the generations towards non - toxic language .   A is illustrated in Figure 2 .   3.3 Decoding Details   We generate TGdata with and without   ALICE . Without ALICE , we use top- kdecoding   ( Fan et al . , 2018 ) alone with our toxic and benign   prompts . With ALICE , we use the HateBERT ﬁne-   tuned OffensEval model from Caselli et al . ( 2021 )   as the toxicity classiﬁer ( CLF ) . This model covers   a range of direct and veiled offense types . We use   GPT-3 for the language model . For decoding , we   use== 0:5 , a maximum generation length   of 30 tokens , a beam size of 10 , and a temperature   of 0.9 . Due to limitations imposed by the OpenAI   GPT-3 API on accessing log probabilities for the   full model vocabulary , we restricted the vocabulary3313   size to the top 100 tokens , and then resample from   the “ allowed ” tokens ( tokens not appearing in the   prompt ) using top- k.   3.4 TGStatistics   Statistics of TGare presented in Table 2 .   In our ﬁnal dataset , generation length varies sig-   niﬁcantly and , as expected , almost all the state-   ments are implicit . As we show in § 4 , the A -   generated data is successful at attacking the given   toxicity classiﬁer , contributing a challenging , ad-   versarial subset of TG.In the released data ,   we split off a test set that is validated by human   annotators ( see § 4.2 ) .   4 Human Validation of TG   To ensure the quality of TG , we conduct hu-   man validation experiments and create TG-   H V , a human - validated test set . Speciﬁ-   cally , we investigate the reliability of our prompt-   based and A -based methods at generating   human - like statements and controlling statements ’   toxicity and the minority groups mentioned ( § 4.2 ) .   Additionally , we measure the effectiveness of A -- generated statements ( vs. top- k - generated ) at   fooling classiﬁers ( § 4.3 ) .   4.1 Human Validation Design   For each generated statement , we ask the annota-   tors various questions , described below , that take   into account multiple dimensions of how toxicmachine - generated language presents a potential   harm to readers . See Appendix B for an annotation   screenshot and other study details .   Perceived hatefulness with respect to human   or AI - authored text . We ﬁrst ask annotators to   guess whether the statement ’s author was a human   or an AI system ( H OAI ) . Then , we ask   whether the statement would be harmful to any-   one if an AI system wrote it ( IAI ) , as   well as if a human wrote it ( IH ) ;   we hypothesize that readers may have different   standards for machine - generated text than human-   written text . For all questions measuring harmful-   ness of text , we consider potential harm on a 1 - 5   scale with 1 being clearly benign and 5 indicating   very offensive or abusive text .   Perceived intent of the writer . We ask readers   whether statements were likely intended to be harm-   ful ( I ) , since some biased state-   ments can be positively intended ( e.g. , benevolent   sexism ; Glick and Fiske , 1996 ) . Additionally , we   ask if the statement exhibits a positive stereotype   ( PS ) , which is also harmful ( e.g. , model   minority myths ; Cheryan and Bodenhausen , 2000 ) .   Detailed harm explanations . To better under-   stand how harm may be perpetrated against the   minority group , we ask readers in - depth questions   about text ’s content , following Sap et al . ( 2020 ) and   Olteanu et al . ( 2018 ) . We ask whether or not the   statement is lewd or sexual ( L ) , whether and   how it references the targeted group or other groups   ( G , F ) , whether it   claims to be factual or opinion ( FOO ) .   4.2 Constructing TG - H V   Data and Setup . We selected 792 statements   from TGto include in our test set , such that   no training statement had cosine similarity above   0.7 with any test statement . Each test statement   was then rated by 3 annotators from a pool of 156   prequaliﬁed annotators from Amazon MTurk ( See   Appendix B for details ) .   Inter - annotator agreement . To investigate the   quality of our annotations , we compute agreement   on toxicity ratings . We ﬁnd that annotators agreed   moderately and are higher than or equal rates to   prior work on hate speech annotation ( Ross et al . ,3314   2017 ; Sap et al . , 2020 ) , with a Fleiss ’ =0.46   ( Fleiss , 1971 ) and Krippendorff ’s  = 0.64 ( Krippen-   dorff , 1980 ) . In 55.17 % of cases , all 3 annotators   agree , while a majority ( 2/3 ) agree for 93.4 % .   Human validation results . First , we ﬁnd that   our machine - generated statements are largely indis-   tinguishable from human - written statements . For   example — see Table 3 — human annotators often   predict that our text is generated by a human . In   fact , on average 90.5 % of machine - generated ex-   amples are thought to be human - written by a ma-   jority of annotators , as shown in Figure 4 . We   also note that harmful text confuses readers slightly   more than non - harmful text : 92.9 % of toxic exam-   ples are mislabeled as human - written compared to   90.2 % for non - toxic . Most toxic examples are also   hate speech ( 94.56 % ) . While opinions are com-   mon in both toxic and non - toxic examples , most   fact - claiming text is non - toxic .   Second , we ﬁnd that demonstration - based   prompting reliably generates toxic and benign   statements about minority groups ( § 4.3 ) . Further ,   for the machine - generated examples , we ﬁnd that   30.2 % are harmful ( given a score of > 3 ) , while   only 4 % are ambiguous . This indicates that these   data are sufﬁciently toxic or benign . We also ﬁnd3315that all identity groups covered by the dataset were   represented in the human study ( see Figure 3 ) , and   observe that the identity group referenced by the   prompt is generally the same as the group refer-   enced by the corresponding TGtext , though   there is some deviation . This is likely due to GPT-3   conﬂating identities or mentioning multiple groups .   Interestingly , there is no signiﬁcant difference   in toxicity when we account for whether annota-   tors perceive scores as written by humans or AI   ( Figure 5 ) . This ﬁnding indicates that our machine-   generated text is perceived as similarly harmful to   human text . We also ﬁnd that the most common   framing tactic is “ moral judgement ” , or question-   ing the morality of an identity group , which has   been linked to toxicity by prior work ( Hoover et al . ,   2019 ) .   4.3 Comparing Generation Methods   As further validation , we investigate whether A -- generated statements are more adversarial com-   pared to top- k - generated ones . For 125 randomly-   selected prompts ( 62 toxic and 63 non - toxic ) , we   generate two statements : one with A and one   without ( top- k ) . We then collect annotations for the   250 statements using the setup described in § 4.1 ,   and get toxicity scores from HateBERT .   We ﬁnd that for top- ksampled sentences , the   prompt label indeed matches the desired label   ( 95.2 % of non - toxic examples and 67.7 % of toxic   examples ) . For ALICE , 40.3 % of toxic examples   match the prompt label and 92.1 % of non - toxic ex-   amples match . We also ﬁnd that A succeeds in   fooling HateBERT ( 26.4 % of A -decoded sen-   tences fool HateBERT vs. 16.8 % of top- ksampled   sentences ) . Finally , A is effective for detox-   ifying generated text : the avg . human - annotated   toxicity score for A -decoded sentences with   a toxic prompt is 2.97 , compared to 3.75 for top-   k. This difference is statistically signiﬁcant with   p < 0:001.A therefore leads to harder , more   ambiguous examples . We greatly expand on these   ﬁndings in Appendix E with a larger scale hu-   man evaluation ( 10,000 samples ) comparing sen-   tences generated with and without A .   5 Improving Toxicity Classiﬁers   To further showcase the usefulness of TG ,   we investigate how it can enhance classiﬁers ’   abilities to detect human - written and machine-   generated implicit toxic language . We ﬁne - tune   the widely - used HateBERT ( Caselli et al . , 2021 )   and ToxDectRoBERTa ( Zhou et al . , 2021 ) mod-   els on the training portion of TG , using the   prompt labels as proxies for a true toxicity label .   Then , we compare the performance of the out - of-   the - box models to those ﬁne - tuned on TG   on three publicly available human - written datasets   ( I HC ( ElSherief et al . , 2021 ) ,   theS BF test set ( Sap et al . , 2020 ) ,   andDH(Vidgen et al . , 2021 ) ) as well as   the evaluation portion of our machine - generated   dataset ( TG - H V ) . To ablate the   contribution of each decoding method , we also split   TGinto equal numbers of ALICE - generated   and top - k - generated examples .   Our results — see Table 4 — show that ﬁne - tuning   HateBERT and ToxDectRoBERTa on TG   improves performance across all datasets . The im-   provement on human - written datasets shows that   TGcan be used to improve existing clas-   siﬁers , helping them better tackle the challeng-   ing human - generated implicit toxicity detection   task . Fine - tuned HateBERT performs strongly on   TG - H V , demonstrating that our   data can successfully help guard against machine-   generated toxicity .   6 Conclusions   In this work , we used a large language model to cre-   ate and release TG , a large - scale , balanced ,   and implicit toxic language dataset . TGis3316far larger than previous datasets , containing over   274k sentences , and is more diverse , including men-   tions of 13 minority groups at scale . The generated   samples are balanced in terms of number of benign   and toxic samples for each group . We proposed   A , an adversarial decoding scheme to evalu-   ate robustness of toxicity classiﬁers and generate   sentences to attack them , and showed the effective-   ness of A on a number of publicly - available   toxicity detection systems . In our experiments , we   showed that ﬁne - tuning pre - trained hate classiﬁers   onTGcan improve their performance on   three popular human -generated toxicity datasets .   We also conducted a human study on a subset of   TG , verifying that our generation methods   successfully create challenging statements that an-   notators struggle to distinguish from human - written   text : 90.5 % of machine - generated examples were   thought to be human - written .   7 Societal and Ethical Considerations   Risks in dataset release While the purpose of   our work is to curate diverse and effective hate   speech detection resources , our methods encour-   age a large language model to make its generation   more toxic . This poses a potential misuse case   where bad actors exploit these methods for nefar-   ious purposes like spreading machine - generated   hate speech . Still , ignoring this possibility does not   make it go away and our work introduces an op-   portunity for the community to push back against   harm towards minority groups . Our ultimate aim   is to shift power dynamics to targets of oppres-   sion . Therefore , we do not consider identity dimen-   sions that are historically the agents of oppression   ( e.g. , whiteness , heterosexuality , able - bodied - ness ) .   Please also note that there is still a lot that this   dataset is not capturing about toxic language . Our   annotations might not capture the full complexity   of these issues related to human experiences . There   is need for multi - disciplinary work to better under-   stand these aspects .   ALICE The proposed method in this work at-   tacks content ﬁlters via an adversarial game be-   tween two AI systems and thus passes the existing   content ﬁlters — as we show for 5 publicly - available   systems . It is important to leverage this and similar   approaches to improve content ﬁlters and prevent   large scale attacks against sensitive platforms . Improving Toxicity Detection Effective classi-   ﬁers for machine biases are required to combat the   scale of online harm . Without such systems , mi-   nority groups are likely to be targeted by current   ( biased ) systems . Our work is a signiﬁcant step   towards advancing this crucial classiﬁcation task .   Still , toxicity is inherently subjective ( Sap et al . ,   2021 ) . Therefore , moving beyond binary detection   tasks to a focus on more nuanced labeling systems   ( ElSherief et al . , 2021 ; Leonardelli et al . , 2021 ) will   prove crucial in developing responsible systems .   Relationship to Policy The topic of detecting   and mitigating toxicity is relevant to the ongoing   work and discussions in the space of policy and   legislation for AI technology ( Wischmeyer and   Rademacher , 2020 ; Reich et al . , 2021 ) . Carefully   crafted policy and regulation can play an important   role in providing oversight into the development   and deployment of content moderation systems and   toxicity detection algorithms in practice ( Benesch ,   2020 ; Gillespie et al . , 2020 ) . Getting this right car-   ries a crucial importance for the society as errors in   content moderation can disproportionately affect   minority groups ( Sap et al . , 2019 ) . We see a path   forward in which tools and techniques like those   presented in this work are paired with human ex-   pertise and well - informed policy & regulation in   bringing scalable and reliable solutions to practice .   We acknowledge and encourage the critical role the   NLP research community is poised to play in this   inter - disciplinary effort .   8 Acknowledgements   We thank Azure AI Platform and Misha Bilenko   for sponsoring this work and providing compute   resources , Microsoft Research for supporting our   large scale human study , and Alexandra Olteanu   for her feedback on human evaluation . We also   thank the crowdworkers for their time and effort .   References33173318331933203321   A Generation Details   To generate sentences for a given minority group ,   we sample 5 random sentences from the corre-   sponding set of examples , then join them into one   string with each example being preceded by a hy-   phen ( “ – ” ) and ending with a newline character   ( “ \n ” ) . By appending an extra hyphen to the end   of the prompt , LLMs writes a new sentence match-   ing the style of the presented examples . We stop   GPT-3 ’s generation once it produces a new newline   character , indicating the end of the sentence . For   each generated sentence , we use a new , randomly-   selected set of 5 random sentences .   A.1 Language Model Selection   While we use GPT-3 to generate statements in this   work , in principle , our methods can be used with   any models that generate realistic text , such as GPT-   Neo ( Black et al . , 2021 ) , GPT - J ( Wang and Komat-   suzaki , 2021 ) , or Turing - NLG ( Rasley et al . , 2020 )   B Human Validation Details   B.1 Selecting MTurk Workers   For human validation , we select 156 MTurk work-   ers with prior experience annotating toxic language   ( Sap et al . , 2020 ) . 51 of these workers participated   in data annotation . We collect worker demograph-   ics using an optional survey at the end of the anno-   tation task . We ﬁnd that 56.9 % identify as White ,   9.8 % as Black , 3.9 % as Hispanic , 3.9 % as Asian   and 5.9 % as Other . Also , 45.1 % of workers identify   as female , 37.3 % as male and 2 % as non - binary .   The majority of workers are between 25 and 45   ( 58.8 % ) . Politically , 25.5 % of workers identify as   left - leaning , 23.5 % as very left - leaning , 13.7 % as   moderate , 17.6 % as right - leaning and 3.9 % as very   right - leaning . Lastly , we ﬁnd that 5.9 % of work-   ers also identify as LGBTQ+ and 2 % identify as   Paciﬁc Islander .   B.2 Annotation Interface   Figure 6 shows a screenshot of the annotation inter-   face given to the Amazon Mechanical Turk work-   ers . Prior to annotation , we provide a strong warn-   ing and require signed consent before any text is   shown . C How does perplexity change across   groups ?   Our decoding approaches should ideally generate   low - perplexity sentences . We measure the per-   plexity assigned by a pre - trained language model   across different minority groups for sentences gen-   erated with and without ALICE . This will give us   an idea of how good the set of sentences are from   the perspective of the pre - trained language model   in terms of perplexity . We use GPT-2 model from   Huggingface to measure perplexity . As some sen-   tences have extremely high perplexity according   to GPT-2 , we drop sentences ( roughly 10 % of the   dataset ) with perplexity over 500 for this analy-   sis . As shown in Table 5 , the ALICE - generated   sentences have signiﬁcantly lower perplexity than   top - kacross all minority groups . We also ﬁnd that   the average perplexity can range signiﬁcantly be-   tween subgroups , though perplexity varies more   for top - k - generated text . Interestingly , text men-   tioning Black people is deemed most - likely across   the board , while the least - likely generations dif-   fer by generation method : amongst the ALICE-   generated text , sentences mentioning Latino people   is the least likely , while for top- k , text mentioning   Women is the least likely . In all cases , ALICE gen-   erates text with up to 5 times lower perplexity than   regular decoding .   D Does generated text actually mention   the targeted groups ?   In the human validation study ( § 4 ) , we ask an-   notators to determine whether or not the text ac-   tually includes references to the targeted groups;3322   each prompt was generated with one group in mind .   Here , we compare the proportion of text that men-   tions each group , split by decoding method . As   shown in Table 6 , we ﬁnd that both ALICE and   top - kgenerate text that mentions corresponding   minority group in the prompt almost equally good   ( slightly better for ALICE ) , though the exact pro-   portion changes by the group . For instance , in text   generated for Latino people , ALICE has a 100 %   hit rate , while top- khas only 72 % . However , for   text mention LGBTQ+ people , top- ktext succeeds   to mention them 97 % of the time while ALICE   has only 91 % . These values may depend on the   underlying language model : in our case , GPT-3   may have been trained on less Latino - mentioning   text and therefore beneﬁt more from controlled de-   coding .   E Analysis of Large - Scale Human   Validation   Summary Statistics . In addition to the human-   validated evaluation set described in Section 4 , we   obtain labels for 8,960 randomly sampled training   examples using the same annotation framework   and pool of MTurk workers . This sample is evenly   split between top- kand ALICE generated texts   ( 50.9 % for top- k , 49.1 % for ALICE ) . Please note   that the samples are drawn randomly from T - Gtraining data and we did not enforce hav-   ing the same prompt for top- kand ALICE . The   analysis for having the same prompt for top- kand   ALICE has already been done in § 4.3 . In Figure   7 , we show that average toxicity scores are similarfor the two decoding methods given a prompt la-   bel , though ALICE - generated texts have a slightly   higher average toxicity for generations from toxic-   labeled prompts and a lower average toxicity for   generations from non - toxic prompts than top- k.   We observe that 66.86 % of ALICE - generated texts   with a toxic prompt label are actually toxic ( com-   pared to 57.91 % of top- kexamples ) and 93.21 %   of ALICE - generated texts with a non - toxic prompt   label are actually non - toxic ( compared to 90.01 %   of top - kexamples ) . We also ﬁnd that ALICE is   more effective at generating adversarial language -   58.97 % of toxic ALICE - generated examples fool   HateBERT , compared to 26.88 % of toxic top- kgen-   erated examples . ALICE - generated non - toxic ex-   amples also fool HateBERT more often than top- k ,   though the difference is smaller ( 15.51 % of ALICE-   generated non - toxic examples vs. 11.35 % of top- k   generations ) . Lastly , ALICE - generated examples   lead to more consistent target group references . At   least one annotator identiﬁed a direct or indirect   reference to the exact target group for 70.4 % of   top - kgenerated examples compared to 78.3 % of   ALICE - generated examples . A full breakdown is   given in Figure 8 .   Subjectivity in Annotation . As we address   broadly in Section 7 , subjectivity is an area of con-   cern for annotation of toxicity . Prior work has   pointed out the role that annotators ’ belief systems   and sociodemographic backgrounds play in their   perception of toxicity ( Sap et al . , 2019 , 2021 ; Da-   vani et al . , 2022 ) . This is highlighted by such train-   ing examples as   “ I ’d like to thank you .. I do n’t think I ’d   have made it if I did n’t ﬁnd MGTOW !   You guys are great ! "   Annotators predicted this example to be non - toxic ,   likely due to not recognizing MGTOW as a misog-   ynistic group .   F Example Prompt   Prompt engineering can have signiﬁcant effects   on the quality of text generated by language mod-   els . Following the lead of other recent works , we   use demonstration - based prompting , and introduce   demonstrations to encourage language models to   generate group - mentioning text . For example , a   prompt that generates neutral mentions of Native   American people is:3323   “   - it is a common misconception that na-   tive americans are all savages but they   have a rich culture and history\n   - indigenous people are often left out of   conversations around policies that im-   pact them\n   - some native american communities   have reclaimed the term indian , despite   a complex history\n   - some native americans were granted   land after the british were defeated in   the american revolution by arrows\n   - indigenous people have lived in north   america for more than 12,000 years\n   - there was a lot of resentment in the 19th   century to native americans and some   indians were even killed off\n   -   ”   The trailing hyphen encourages the language   model to provide another sentence in the growing   list that resembles the prior examples , ending in \n .   All of our generated prompts ( 26,000 ) are released   with the dataset .   G Releasing a Pretrained Model and its   Propagated Labels   We further ﬁnetune and release a RoBERTa clas-   siﬁer on the 8,960 human - annotated sampled inTG , beginning with the weights from ( Zhou   et al . , 2021 ) . Along with our publicly - available   code , this pretrained model will serve as an entry   point for community engagement with our work .   We run this pretrained model on the full TG   dataset , collecting its predictions and release them   along with TG . These new labels may serve   to correct some mislabeling .   H Dataset Description   We release TGas a dataframe with the fol-   lowing ﬁelds : prompt contains the prompts we   use for each generation . generation is the T-   Ggenerated text . generation method denotes   whether or not A was used to generate the cor-   responding generation . If this value is A , then   A was used , if it is top- k , then A was not   used . prompt_label is the binary value indicating   whether or not the prompt is toxic ( 1 is toxic , 0   is benign ) , and therefore the generation should be   toxic as well . This label is slightly noisy , though   largely accurate — as deemed by human annotators .   group indicates for which group the prompt was   generated . Finally , roberta_prediction is the prob-   ability predicted by our corresponding RoBERTa   model for each instance . This ﬁeld can be used as   propagated labels according to this model .   I Further comparing toxicity classiﬁers   We also compare ﬁnetuning classiﬁers on subsets of   TG - Vwith and without ALICE , shown in   Table 7 . As expected , when ﬁnetuning on each sub-   set individually , performance is strong on their re-   spective evaluation sets . Further , without any ﬁne-   tuning , each model performs worse on the ALICE-   generated data , indicating ALICE successfully gen-   erates data that are more confusing to each model.332433253326