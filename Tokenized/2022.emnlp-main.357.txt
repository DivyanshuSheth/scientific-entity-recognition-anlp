  retrieval   Yifu Qiu , Hongyu Li , Yingqi Qu , Ying Chen , Qiaoqiao She ,   Jing Liu*,Hua Wu , Haifeng WangInstitute for Language , Cognition and Computation , University of Edinburgh , UKBaidu Inc. , Beijing , China   y.qiu-20@sms.ed.ac.uk   { lihongyu04 , quyingqi , chenying04 , sheqiaoqiao , liujing46 , wu_hua , wanghaifeng}@baidu.com   Abstract   In this paper , we present DuReader ,   a large - scale Chinese dataset for passage re-   trieval . DuReader contains more than   90 K queries and over 8 M unique passages   from a commercial search engine . To alle-   viate the shortcomings of other datasets and   ensure the quality of our benchmark , we ( 1 )   reduce the false negatives in development   and test sets by manually annotating results   pooled from multiple retrievers , and ( 2 ) re-   move the training queries that are semanti-   cally similar to the development and testing   queries . Additionally , we provide two out-   of - domain testing sets for cross - domain eval-   uation , as well as a set of human translated   queries for for cross - lingual retrieval eval-   uation . The experiments demonstrate that   DuReader is challenging and a num-   ber of problems remain unsolved , such as the   salient phrase mismatch and the syntactic mis-   match between queries and paragraphs . These   experiments also show that dense retrievers   do not generalize well across domains , and   cross - lingual retrieval is essentially challeng-   ing . DuReader is publicly available   athttps://github.com/baidu/DuReader/   tree / master / DuReader - Retrieval .   1 Introduction   Passage retrieval requires systems to select can-   didate passages from a large passage collection .   In recent years , pre - trained language models ( De-   vlin et al . , 2019 ; Liu et al . , 2019 ) have been ap-   plied to retrieval problems , known as dense re-   trieval ( Karpukhin et al . , 2020 ; Qu et al . , 2021 ;   Zhan et al . , 2021 ) . The success of dense retrieval   relies on the availability of high quality , large - scale ,   human - annotated corpora . A number of popular   datasets are already available for English passage   retrieval , including MS - MARCO ( Nguyen et al . ,2016 ) , TriviaQA ( Joshi et al . , 2017 ) , and Natural   Questions ( Kwiatkowski et al . , 2019 ) . In contrast ,   existing datasets for non - English retrieval ( e.g. ,   Chinese ) , are either small or machine generated .   For example , TianGong - PDR ( Wu et al . , 2019 ) has   only 70 questions and 11 K passages . Even though   the multilingual dataset mMARCO(Bonifacio et al . ,   2021 ) is large in size , it is constructed by ma-   chine translation from the English MS - MARCO   dataset . Sougou - QCL ( Zheng et al . , 2018 ) is con-   structed based on click logs of web data with-   out human annotation . In this paper , we present   DuReader , a large - scale Chinese dataset   for passage retrieval from web search engine , that   is manually annotated . The dataset contains more   than 90 K queries and over 8 M unique passages .   All queries are selected from real requests made   by users at Baidu Search , and document passages   are from the search results . Similar to ( Karpukhin   et al . , 2020 ) , we create the DuReader   from DuReader ( He et al . , 2018 ) , a Chinese ma-   chine reading comprehension dataset , and obtain   the human labels for paragraphs by distant su-   pervision ( See Section 2.2 ) . An example from   DuReader is shown in Table 1 , and a com-   parison of different datasets is shown in Table 2 .   Additionally , recent works point out two major   shortcomings of the development and testing sets   in the existing datasets :   •Arabzadeh et al . ( 2021 ) and Qu et al . ( 2021 ) ob-   serve that false negatives ( i.e. relevant passages   but labeled as negatives ) are common in the pas-   sage retrieval datasets due to their large scale but   limited human annotation . As a result , the top   passages retrieved by models may be superior to   labeled relevant positives , and this will affect the   evaluation .   •Lewis et al . ( 2021 ) find that 30 % of the test - set   queries in the common machine reading com-   prehension datasets ( Kwiatkowski et al . , 2019 ;   Joshi et al . , 2017 ) have a near - duplicate para-5326   phrase in their corresponding training sets , thus   leaking the testing information into model ’s train-   ing . The similar issue has been observed in MS-   MARCO ( Zhan et al . , 2022 ) .   In the construction of DuReader , we try to   alleviate the above issues and improve the quality   of the development and testing sets in the following   two ways ( see Section 2.3 ):   •To reduce the false negatives in the development   and testing set , we invite the internal data team   to manually check and relabel the passages in   the top retrieved results pooled from multiple   retrievers .   •To reduce the leakage of testing information into   model ’s training , we use a query matching model   from ( Zhu et al . , 2021 ) to identify and remove   the training queries that are semantically similar   to the development and testing queries .   Moreover , inspired by Thakur et al . ( 2021 ) , we pro-   vide two testing sets ( see Section 2.4 ) from the med-   ical domain ( cMedQA ( Zhang et al . , 2018)and   cCOVID - News ) as the separate testing sets for out-   of - domain evaluation . Additionally , we also pro-   vide a set that contains human translated queries for   cross - lingual retrieval evaluation ( see Section 2.5 )   ( Asai et al . , 2021a ; Sun and Duh , 2020 ) .   In this paper , we conduct extensive experiments .   In our in - domain experiments , we find that there   are many challenges to be addressed , such as salient   phrase mismatches and syntactic mismatches ( see   Section 3.5 ) . It is also difficult for dense retrieversto generalize well across different domains as we   observed in the out - of - domain experiments ( see   Section 3.6 ) . Finally , the cross - lingual experiments   indicate that cross - lingual retrieval is essentially   challenging ( see Section 3.7 ) .   We summarize the characteristics of our dataset   and our contributions as follows :   •We present a large - scale Chinese dataset for   benchmarking the passage retrieval models .   Our dataset comprises more than 90 K queries   and more than 8 M unique passages from   Baidu Search .   •We leverage two strategies to improve the   quality of our benchmark and alleviate the ex-   isting shortcomings in other existing datasets ,   including reducing the false negatives with   human annotations on pooled retrieval results ,   and remove the training queries semantically   similar to the development and testing queries .   •We introduce two extra out - of - domain test   sets to evaluate the domain generalization ca-   pability , and a cross - lingual set to assess the   cross - lingual retrievers .   •We conduct extensive experiments and the re-   sults demonstrate that the dataset is challeng-   ing and passage retrieval has plenty of room   for improvement .   2 DuReader   In this section , we introduce our DuReader   dataset ( See dataset statistics in Table 3 ) . We first   formally define the passage retrieval task in Section5327   2.1 . We then introduce how we initially construct   our dataset from DuReader by distant supervision   in Section 2.2 . Our strategies for further improv-   ing the data quality are discussed in Section 2.3 .   Finally , we introduce two out - of - domain test sets   in Section 2.4 and a cross - lingual set in in Section   2.5 .   2.1 Task Definition   DuReader is created for the task of pas-   sage retrieval , that is , retrieving a list of relevant   passages in response to a query . Formally , given a   query qand a large passage collection P , a retrieval   system Fis required to return the top- Krelevant   passages P=/braceleftig   p , p , ... , p / bracerightig   , where Kis   a manually defined number . Ideally , all the rele-   vant passages to qwithin Pshould be included and   ranked as high as possible in the retrieved results   P.   2.2 Dataset Construction   2.2.1 An Introduction of DuReader Dataset   DuReader is developed based on the   Chinese machine reading comprehension dataset   DuReader ( He et al . , 2018 ) . All queries in   DuReader are posed by the users of our chosen   commercial search engine , and document - level   contexts are gathered from search results . Each   instance in DuReader is a tuple < q , t , D , A > ,   where qis a query , tis a query type , Dis the   top-5 retrieved documents constituted by their para-   graphs returned by our chosen commercial search   engine . Ais the answers written by human annota-   tors .   2.2.2 Constructing DuReader from   DuReader   In this section , we describe that how we con-   struct DuReader from DuReader . First ,   we describe our approach to labelling the posi-   tive passages . Then , we discuss our approaches   to dealing with the two challenges in constructing   DuReader from DuReader : 1 ) the original   paragraphs are too short to provide meaningful con-   text ; and 2 ) the term overlap between the queries   and the document titles may ease the challenges for   passage retrieval .   Distant Supervision for Annotations Follow-   ing MS - MARCO Passage Ranking ( Nguyen et al . ,53282016 ) , we use the human - written answers from   DuReader ( He et al . , 2018 ) to label the positive   passages by the distant supervision . A paragraph   is considered positive if it contains any human-   written answer . Specifically , we leverage the span-   level F1 score to measure the match between each   human - written answer and the paragraphs in docu-   ments . If a span - answer pair gets a F1 - score higher   than the threshold ( 0.5 ) , we label the paragraph as   positive . We show the details of our annotation   process in Algorithm 1 .   Algorithm 1 Span - level F1 Annotation for Posi-   tives   Input : { ⟨p , a⟩},p : candidate paragraph , a : answer ,   τ : threshold for positive labelling .   Output : l∈ { 0,1 } : label , 0and1denote nega-   tive and positive p , separately .   forany span sinpdo   ifCalculate F1(s , a)≥τthen :   l←1   return   end if   end for   l←0   return   Passage Length Control Additionally , most   paragraphs in DuReader are too short to form mean-   ingful contexts . We concatenate the paragraphs of   each document in DuReader by the following rules :   1 ) In a document of less than 256 Chinese char-   acters , all paragraphs are concatenated into one   passage ; 2 ) In a document of more than 256 Chi-   nese characters , a paragraph of less than 256 is   concatenated with the next one , and the concate-   nation does not stop until the length of the new   passage exceeds 256 . The new passage is labelled   as positive if any of its components are originally   labelled positive in DuReader . After the processing ,   the median and the mean of the passage length are   304 and 272 , respectively .   Removing Document Titles We remove the ti-   tles from all documents in DuReader , since we ob-   serve that there is many term overlaps between the   queries and the titles . If we keep them , the retrieval   systems may easily match the queries with the doc-   ument titles and achieve high performance . But   we expect the retrievers to capture all contextual   information in passages to answer queries.2.3 Quality Improvement   As we discussed in the previous section , there are   shortcomings of other existing datasets . To alle-   viate such shortcomings , we further design two   strategies to ensure the quality of the development   and test sets in DuReader . Although in   this work we apply our quality improvement ap-   proaches to the Chinese passage retrieval dataset ,   the proposed method allows flexibility extended to   other languages ( e.g. , English ) , benefiting the fu-   ture evaluation and development of dense retrieval   systems .   Reducing False Negatives A common issue   in existing passage retrieval datasets ( Qu et al . ,   2021 ; Arabzadeh et al . , 2021 ) is false negatives ,   i.e. , query - relevant passages not labelled as posi-   tives , in the development and testing sets . In this   section , we discuss our strategy for reducing the   false negatives in the development and testing sets   of DuReader .   We use human annotation as a complement to   the distant supervised labeling approach discussed   in Section 2.2 . We invite the internal data team to   manually check the labels in the development and   test sets and fix them if necessary . To avoid induc-   tive bias in our annotation process , we follow the   pooling method in TREC competitions ( V oorhees   et al . , 2005 ) to select candidate passages for anno-   tation . The top - ranked passages retrieved for each   query by a set of contributing retrievers are pooled   for annotation . In particular , the annotator is pre-   sented with a query and the top-5 passages pooled   from five retrieval systems . We use BM25 and   four neural retrievers with the initialization from   ERNIE ( Sun et al . , 2019 ) , BERT ( Devlin et al . ,   2019 ; Cui et al . , 2021 ) , RoBERTa ( Liu et al . , 2019 )   and MacBERT ( Cui et al . , 2020 ) to serve as our   contributing retrievers . We combine their top-50   retrieved passages as candidates . An ensembled   re - ranker is then used ( See Appendix A.1 for im-   plementation details ) to select the top-5 passages   for human annotation . To ensure data quality , we   perform all annotations on our internal annotation   platform . Please refer to the Appendix A.3 for   annotation settings and quality control .   After adopting our strategy for reducing false   negatives , the average positive paragraph per query   has increased from 2.43to4.91.71.53 % of queries   have at least one false negative relabeled by anno-   tators , which shows there are many false negatives   in the raw corpus derived directly from DuReader.5329Removing Similar Queries Retrieval systems   should avoid merely memorizing queries and their   relevant items in the training set and directly ap-   plying such memorization during inference . Lewis   et al . ( 2021 ) find that in some popular datasets ,   including Natural Questions ( Kwiatkowski et al . ,   2019 ) , WebQuestions ( Berant et al . , 2013 ) and   TriviaQA ( Joshi et al . , 2017 ) , 30 % of the test - set   queries have a near - duplicate paraphrase in their   corresponding training sets , which leaks the testing   information into the model training . In this paper ,   we use a model - based approach to remove training   queries that are semantically similar to develop-   ment and testing queries .   We use the query matching model in ( Zhu et al . ,   2021 ) , which computes the similarity score ranging   between [ 0,1]for a query pair . We set a threshold   of 0.5 , meaning that if the similarity between a   training query and a test query is higher than 0.5 ,   we mark the query pair as semantically similar .   There are 566 training queries semantically similar   to 387 queries in the development and the test set ,   accounting for approximately 6.45 % of total de-   velopment and test queries . All these 566 training   instances are removed in DuReader .   2.4 Out - of - domain Evaluation   Recent work ( Thakur et al . , 2021 ) reveals that   the dense retrievers do not generalize well cross-   domain . To assessing the cross - domain generaliza-   tion ability of the retrievers , we carefully select two   publicly available Chinese text retrieval datasets ,   i.e. , cMedQA ( Zhang et al . , 2018 ) created from on-   line medical consultation text and cCOVID - News   from COVID-19 news articles . We randomly select   949 and 3,999 samples from cCOVID - News and   cMedQA , respectively , as out - of - domain testing   data .   2.5 Cross - lingual Evaluation   Cross - lingual passage retrieval has recently re-   ceived much attention ( Shi et al . , 2021 ; Asai et al . ,   2021b ) , which aims to retrieve the passages in the   target language ( e.g. , Chinese ) as the response to   the query in source language ( e.g. , English ) .   In DuReader , we provide a cross - lingual   retrieval set which contains the English queries   paired with Chinese positive passages . The to-   tal numbers of training / development / testing En-   glish queries are 9.5K/4K/2 K , respectively . All   English queries in our cross - lingual set are trans-   lated and the passage annotations are aligned with   DuReader . To obtain English queries , we   first translate Chinese queries to English queries   by using machine translation . Then , we ask the   internal data team to manually check the quality   of the machine - translated queries , and modify the   translated queries if necessary . The quality controls   for translated queries are the same as our previous   human annotations for the in - domain development   and testing set as in Appendix A.3 .   3 Experiments and Results   3.1 Baselines   We use the recent two - stage framework ( retrieve-   then - rerank ) ( Dang et al . , 2013 ; Qu et al . , 2021 )   for passage retrieval and evaluate two retrieval and   two reranking models on our DuReader   dataset . In particular , we utilize the dual - encoder   and cross - encoder architecture in RocketQA ( Qu   et al . , 2021 ) to develop our neural retrievers and   re - rankers . We introduce the baselines as follows .   BM25 BM25 is a sparse retrieval baseline ( Robert-   son and Zaragoza , 2009 ) .   DE w/ BM25 Neg Karpukhin et al . ( 2020 ) shows   that the hard negatives from BM25 are more effec-   tive at training the dense retrievers than in - batch   random negatives . With BM25 ’s hard negatives ,   we train a dual - encoder as our first neural retriever .   CE w/ BM25 Neg We use BM25 ’s hard neg-   atives to train a cross - encoder as our first neural   re - ranker .   CE w/ DE Neg CE w/ DE Neg is the second   enhanced re - ranker . We follow Qu et al . ( 2021 ) to   train CE w/ DE Neg . Specifically , we use CE w/   BM25 Neg to initialize the parameters , and use DE   w/ BM25 Neg to retrieve negatives from the entire   passage collection.5330   The relationships among our neural retrievers   and re - rankers are shown in Figure 1 . The training   and architectural settings for all models are detailed   in the Appendix A.2 .   3.2 Evaluation Metrics   We use the following evaluation metrics in our ex-   periments : ( 1 ) Mean Reciprocal Rank for the top   10 retrieved documents ( MRR@10 ) , ( 2 ) Recall for   the top-1 retrieved items ( Recall@1 ) and ( 3 ) Re-   call for the top-50 retrieved items ( Recall@50 ) .   Recall@50 is more suitable for evaluating the first-   stage retrievers , while MRR@10 and Recall@1   are more suitable for assessing the second - stage   re - rankers .   3.3 Baseline Performance   We report the in - domain baseline performances for   the first - stage retrievers in Table 4 . Compared with   the traditional retrieval system BM25 , it is expected   that DE w/ BM25 Neg outperforms the traditional   system among all metrics , thanks to the powerful   expressive ability of the neural encoder .   We then report the in - domain baseline perfor-   mances for the second - stage re - rankers in Table 5 .   We observe that training the re - ranker with the hard   negatives sampled from the neural retriever ’s top   predictions is shown to outperform the negatives   sampled from BM25 ’s retrieved results in terms of   MRR@10 and Recall@1 .   3.4 Effects of Quality Improvements   In this section , we examine the effects of   our strategies to improve the data quality of   DuReader as in Section 2.3 .   Reducing False Negatives We test three models ,   including BM25 , a dense retrieval model ( DE w/   BM25 Neg ) and a re - ranking model ( CE w/ BM25   Neg ) based on BM25 ’s top-50 retrieved results , to   quantify the impact of our strategy on reducing   false negatives . Specifically , we compare the per-   formance of the same model on the development   set either with or without reducing false negatives .   As shown in Figure 2 , all metrics of the three mod-   els are significantly improved after adopting our   strategy . These results suggest that there are many   false negatives in the raw retrieval dataset derived   from DuReader , and that our strategy successfully   captures and reduces false negatives in develop-   ment and testing sets .   Removing Similar Queries We conduct an ex-   periment to quantify the effects of removing the   training queries that are semantically similar to the   development and testing queries . We train our re-   ranking model ( CE w/ BM25 Neg ) by using the   training data without ( CE w/o Sim . Q ) and with   ( CE w/ sim . Q ) semantically duplicated queries ,   respectively . We then test both models on all 387   semantically duplicated queries ( Duplicated ) in   the development and testing sets , as well as the rest   of the development set ( Others ) . We use BM25 ’s   top-50 retrieved results for the re - ranking models   to re - rank . As shown in Table 6 , comparing the two   models ’ performance on Duplicated , we find model   trained with those semantically similar queries ( CE   w/ Sim . Q ) has a higher score on both MRR@10   and Recall@1 . This suggests that using seman-   tically similar queries in training may allow the   model to simply memorize the data during training   and achieve better performance during testing.5331   3.5 The Challenges and Limitations   In this section , we analyze the results of our best   baseline system ( i.e. , retrieving the top-50 passages   by DE w/ BM25 Neg , then re - ranking by CE w/ DE   Neg ) to better understand the specific challenges   and limitations of DuReader . Specifically ,   we manually analyze 500 query - passage predic-   tions of the baseline . The 500 query - passage pairs   are from 100 random - selected development queries   with the top-5 passages retrieved and re - ranked by   the baseline . To help understand the challenges   and limitations of DuReader , we ensure   that the top-5 passages of these 100 queries contain   no positive passages .   Salient Phrase Mismatch We observe that the   mismatch in salient phrases between the query and   the retrieved passages is particularly challenging   for the baseline system as found in ( Chen et al . ,   2021 ) , accounting for 53.4 % of total incorrect pre-   dictions . We further divide salient phrase into sev-   eral sub - categories , i.e. , entity , numeral , and modi-   fier . Examples and explanations are in Table 10 in   Appendix A.4 .   Syntactic Mismatch We also observe that around   1 % predictions have a syntactic mismatch between   the query and the passage . The case in Table 10   in Appendix A.4 suggests that it is difficult for   the baseline system to ensure the consistency in   syntactic relationship between the query and the   passages .   Other Challenges We also show two other typ-   ical challenges accounting for 22.6 % incorrect   predictions : 1 ) Over - sensitivity on term overlap :   whether the baseline system is over - sensitive to   retrieve the negative passages that contains a few   lexical overlap with queries . 2 ) robustness on typo :   whether the baseline system is robust against ty-   pos in queries or passages . Note that our dataset is   constructed from the real query log of a commer-   cial search engine . The noise in data ( e.g. typos )   challenges the robustness of the baseline system .   Limitations in False Negatives We notice that   there are still about 14.8 % false negatives . This   suggests that despite the success of our strategy in   Section 2.3 to reduce false negatives in develop-   ment and testing sets to some extents , the presence   of false negatives remains a challenge in building a   high - quality passage retrieval benchmark .   3.6 Out - of - Domain Evaluation   We evaluate the out - of - domain ( OOD ) generaliza-   tion ability of our dense retriever ( DE w/ BM25   Neg ) on the two OOD testing sets . We report the   results in two settings : 1 ) Zero - shot setting : we   directly evaluate DE w/ BM25 Neg without fine-   tuning . 2 ) Fine - tuning setting : we fine - tune DE w/   BM25 Neg with the data from the target domain   and evaluate it on OOD testing sets . The perfor-   mance of the fine - tuned models is the estimated   upper - bound that DE w/ BM25 Neg can achieve on5332   OOD testing sets .   In Table 7 and 8 , we summarize the results of the   out - of - domain experiments . First , we notice that   the performance of the dense retriever is largely   degraded on the two OOD testing sets . According   to the in - domain evaluation ( see Table 4 and 5 ) , the   dense retriever considerably outperforms BM25 ,   however it has no obvious advantage over BM25   in the zero - shot setting , or even worse . In addition ,   the dense retriever can be significantly improved by   fine - tuning . Its can maintain a large advantage over   BM25 after fine - tuning on the target - domain . This   results show that the dense retriever has limited   domain transfer capability as observed in ( Thakur   et al . , 2021 ) .   3.7 Cross - lingual Evaluation   In the cross - lingual evaluation , we experiment with   three dense retrieval models based on multilingual   BERT ( mBERT ) ( Devlin et al . , 2019 ) .   •Supervised Model We directly fine - tune   mBERT using the parallel data of English   queries and Chinese passages .   •Zero - shot Model We fine - tune an mBERT   retriever on the full monolingual Chi-   nese training data ( i.e. , 86 K Chinese   queries with Chinese positive paragraphs in   DuReader ) , and directly evaluate it   on the cross - lingual testing set .   •Transferred Model We further fine - tune   Zero - shot Model by using the parallel data   paired with English queries and Chinese pas-   sages , and then evaluate it on the cross - lingual   testing set .   As shown in Table 9 , we note that the perfor-   mance of Zero - shot Model on cross - lingual testing   set is less effective than Supervised Model . Fur-   thermore , Zero - shot Model performs significantlyworse on cross - lingual data than on monolingual   data . According to these findings , cross - lingual re-   trieval is more difficult than monolingual retrieval ,   since the retriever can not find relevant passages   by simply matching shared terms between queries   and passages ( Litschko et al . , 2021 ) . Instead , cross-   lingual retrievers must capture the semantic rel-   evance of the query and passages . Additionally ,   Transferred Model outperformed other baselines ,   demonstrating the validity of transferring knowl-   edge from the monolingual Chinese annotated data .   4 Related Works   Passage Retrieval Benchmarks . Passage retrieval   and open - domain question - answering are challeng-   ing tasks that attracts much attention in develop-   ing the benchmarks . MS - MARCO ( Nguyen et al . ,   2016 ) contains queries extracted from the search   log of Microsoft Bing , which poses challenges in   both the retrieval of relevant contexts and read-   ing comprehension based on the contexts . Nat-   ural Questions ( Kwiatkowski et al . , 2019 ) is an   open - domain question answering benchmark that   consist of real queries issued to the Google search   engine . These datasets are widely used for the re-   search of passage retrieval . However , Lewis et al .   ( 2021 ) find that there are 30 % of test - set queries   have semantically overlaps in the training queries   for Natural Questions . Arabzadeh et al . ( 2021 )   observe that false negatives are common in MS-   MARCO . TianGong - PDR ( Wu et al . , 2019 ) and   Sougou - QCL ( Zheng et al . , 2018 ) are two Chi-   nese retrieval datasets for the news documents and   web - pages , separately . However , these datasets are   either small or have no human annotation . Despite   the progress in developing benchmarks for English   passage retrieval , the large - scale and high - quality   benchmarks for the non - English community are   still limited .   Dense Retrieval Model . Information retrieval is   a long - standing problem . In contrast to the tradi-   tional sparse retrieval methods ( Salton and Buck-   ley , 1988 ; Robertson and Zaragoza , 2009 ) , recent   dense retrievers aim at encoding the query and   retrieved documents as contextualized representa-   tions based on the pre - training language models   ( Devlin et al . , 2019 ; Sun et al . , 2019 ) , then cal-   culate the relevance based on similarity function   ( Karpukhin et al . , 2020 ; Luan et al . , 2021 ; Qu et al . ,   2021 ) ( e.g. cosine or dot product ) . Based on dif-   ferent learning paradigms , neural retrieval systems5333can be divided into two categories : 1 ) unsuper-   vised : pre - training the retrieval without annotated   data ( Chang et al . , 2020 ; Gao and Callan , 2021 ) ;   2)supervised : training the query and document   encoders by contrasting the positives with designed   negatives ( Karpukhin et al . , 2020 ; Xiong et al . ,   2021 ; Zhan et al . , 2021 ) . In terms of system ar-   chitecture , the recent systems typically follow the   two - stage framework ( retrieval - then - re - ranking ) , in   which a retriever ( Mao et al . , 2021 ; Nogueira et al . ,   2019 ; Dai and Callan , 2019 ) first retrieve a list of   top candidates and the re - ranker ( Gao et al . , 2020 ;   Khattab and Zaharia , 2020 ) will re - rank retrieved   candidates . It has been shown that large - scale an-   notated datasets are one of the keys to successfully   train dense retrievers ( Karpukhin et al . , 2020 ) .   5 Conclusion   This paper presents a large - scale Chinese passage   retrieval dataset to benchmark the retrieval systems .   In order to ensure the quality of our dataset , we em-   ploy two strategies : 1 ) reducing the false negatives   in development and testing sets using a pooling   approach and human annotations , and 2 ) removing   the training queries that are semantically similar to   the development and testing queries . In addition ,   we provide two testing sets for out - of - domain eval-   uation , and a set for cross - lingual evaluation . We   examine several retrieval baselines , including the   traditional sparse retrieval system and the neural   retrievers , and present the challenges and the lim-   itations of our dataset . We hope this dataset can   help facilitate the research of passage retrieval .   6 Limitations   As we discussed in Section 3.5 , we still observe   that approximately 14.8 % of our best re - ranking   model ’s wrong predictions are indeed caused by   false negatives , even though we observed that our   quality improvement strategy discussed in Section   2.3 was effective . This is primarily due to the diffi-   culty of annotating the training data in a way that   captures all positives .   Secondly , two out - of - domain testing sets are re-   stricted to the medical domain . cMedQA focuses   on the medical question - answering conversations ,   and cCOVID - NEWS focuses on the medical news   domain . It may limit the ability to evaluate retrieval   systems in other domains ( e.g. , the financial or le-   gal domains).7 Ethical Consideration   Our DuReader is developed only for re-   search purpose . All data is collected from either   the open - source projects , respecting corresponding   licences ’ restrictions , or publicly available bench-   marks . We do not guarantee that we have the copy-   right of this data , any may further discard data   resources without copyright if necessary .   8 Acknowledgements   We thank the anonymous reviewers for their help-   ful comments . We also appreciate the feedback   received from other members of the Deep Ques-   tion Answering Team at Baidu NLP , specifically   Qifei WU , for his valuable discussions and com-   ments . This work is supported by the National   Key Research and Development Project of China   ( No.2018AAA0101900 ) .   References533453355336A Appendix   A.1 Details for Re - ranker Used in Reducing   False Negatives   We first use four different pre - training models , in-   cluding ERNIE ( Sun et al . , 2019 ) , BERT ( Devlin   et al . , 2019 ; Cui et al . , 2021 ) , RoBERTa ( Liu et al . ,   2019 ) and MacBERT ( Cui et al . , 2020 ) , as the ini-   tializations to train four cross - encoder re - rankers   as in ( Qu et al . , 2021 ) with negatives sampled from   the pooled passages as discussed in Section 2.3 .   We then ensemble these four re - ranking models by   averaging their prediction scores for each query-   passage pair .   A.2 Baseline Implementation Details   We conduct all experiments with the deep learning   framework PaddlePaddle ( Ma et al . , 2019 ) on up to   eight NVIDIA Tesla A100 GPUs ( with 40 G RAM ) .   We use the ERNIE 1.0 base ( Sun et al . , 2019 )   as the initializations for both our first dual - encoder   retriever ( DE w/ BM25 Neg ) and cross - encoder   re - ranker ( CE w/ BM25 Neg ) . ERNIE shares the   same architecture with BERT but is trained with   entity - level masking and phrase - level masking to   obtain better knowledge - enhanced representations .   To train our second enhanced re - ranker ( CE w/ DE   Neg ) , we use the parameters from CE w/ BM25   Neg as initialization .   For training settings , we also use the Cross - batch   negatives setting as in ( Qu et al . , 2021 ) . When sam-   pling the hard negatives from the top-50 retrieved   items , we sample 4 negatives per positive passage .   The dual - encoders are trained with the batch size of   256 . The cross - encoders are trained with the batch   size of 64 . The dual - encoders and cross - encoders   are trained with 10 and 3 epochs . We use ADAM   optimizer for all models ’ trainings and the learn-   ing rate of the dual - encoder is set to 3e-5 with the   rate of linear scheduling warm - up at 0.1 , while the   learning rate of the cross - encoder is set to 1e-5 with   no warm - up training . We set the maximal length of   questions and passages as 32 and 384 , respectively .   In inference time of our dense retrieval model   ( DE w/ BM25 Neg ) , we use FAISS ( Johnson et al . ,   2019 ) to index the dense representations of all pas-   sages .   A.3 Details for Human Annotations   We perform the annotation in our internal annota-   tion platform to ensure the data quality , where allthe annotators and reviewers are full - time employ-   ees . The pairs of all queries and their pooled top-5   paragraphs retrieved by all models are divided into   packages , with 1 K samples for each . Annotators   are asked to identify whether each query - paragraph   pair is relevant for a single package . Then at least   two reviewers check the accuracy of this package   by reviewing 100 random query - paragraph pairs   independently . If the average accuracy is less than   the threshold ( i.e. , 93 % ) , the annotators will be   asked to revise the package until the accuracy is   higher than the threshold .   A.4 Cases for Challenges in Error Analysis   We present the selected cases in Table 10 and dis-   cuss them in this section to support our error analy-   sis in Section 3.5 .   Salient Phrase Mismatch Taking the entity mis-   match as an example , we expect that the main entity   in the retrieved passage should be consistent with   the query . However , the second example in Ta-   ble 10 shows that the query asks for information   about Taobao , but the retrieved passage is related   toAlipay instead . There is a challenge for retrieval   systems to filter out passages that entail entities   inconsistent with the query .   Syntax Mismatch Given the case showed in   Table 10 as an example , the retrieval system is hard   to understand the subject and object in the example   query are Taipei andRuifang , instead , it simply   ranks the candidate passage entailing Taipei and   Ruifang to a top predictions .   Other Challenges In our analysis , it is found   that about 21 % of the errors are due to the retrieval   system simply predicting its output based on the   presence of co - occurring low - frequency terms ( e.g. ,   " wow " in the example in Table 10 ) in query and   paragraph , but their semantic meanings are not   related indeed . And about 1.6 % of the errors are   due to noise in the query or paragraph . For example ,   misspelling the " iPhone " as " ipone " .53375338