  Divyanshu Aggarwal , Vivek Gupta , Anoop KunchukuttanDelhi Technological University;University of Utah;Microsoft IDC;AI4Bharat   divyanshuggrwl@gmail.com ; vgupta@cs.utah.edu ; ankunchu@microsoft.com   Abstract   While Indic NLP has made rapid advances re-   cently in terms of the availability of corpora   and pre - trained models , benchmark datasets   on standard NLU tasks are limited . To this   end , we introduce I XNLI , an NLI dataset   for 11 Indic languages . It has been created by   high - quality machine translation of the orig-   inal English XNLI dataset and our analysis   attests to the quality of I XNLI . By fine-   tuning different pre - trained LMs on this I - XNLI , we analyze various cross - lingual   transfer techniques with respect to the im-   pact of the choice of language models , lan-   guages , multi - linguality , mix - language input ,   etc . These experiments provide us with use-   ful insights into the behaviour of pre - trained   models for a diverse set of languages .   1 Introduction   Natural Language Inference ( NLI ) is a well - studied   NLP task ( Dagan et al . , 2013 ) that assesses if   a premise entails , negates , or is neutral towards   the hypothesis statement . The task is well suited   for evaluating semantic representations of state - of-   the - art transformers ( Vaswani et al . , 2017 ) mod-   els such as BERT ( Devlin et al . , 2019 ; Radford   and Narasimhan , 2018 ) . Two large scale datasets ,   such as SNLI ( Bowman et al . , 2015 ) and MultiNLI   ( Williams et al . , 2018 ) , has recently been developed   to enhanced the relevance of the NLI task .   With the availability of multi - lingual pre - trained   language models such as mBERT ( Devlin et al . ,   2019 ) , and XLM - RoBERTa ( Conneau et al . , 2020a )   promising cross - lingual transfer and universal mod-   els , multi - lingual NLP has recently gained a lot   of attention . However , most languages have a   scarcity of datasets resources . Some multi - lingual   datasets have attempted to fill this gap , including   XNLI ( Conneau et al . , 2018 ) for NLI , XQUAD(Dumitrescu et al . , 2021 ) , MLQA ( Lewis et al . ,   2020 ) for question answering , and PAWS - X for   paraphrase identification ( Yang et al . , 2019 ) . In   many practical circumstances , training sets for non-   English languages are unavailable , hence cross-   lingual zero - shot evaluation benchmarks such as   XTREME ( Hu et al . , 2020a ) , XTREME - R ( Ruder   et al . , 2021 ) , and XGLUE ( Liang et al . , 2020 ) have   been suggested to use these datasets .   However , NLI datasets are not available for ma-   jor Indic languages . The only exceptions are the   test / validation sets in the XNLI ( hi and ur ) , Tax-   iXNLI ( hi ) ( K et al . , 2021 ) and MIDAS - NLI ( Up-   pal et al . , 2020 ) datasets . Furthermore , because   MIDAS - NLI is based on sentiment data recasting ,   hypotheses are not linguistically diverse and span   limited reasoning . In this work , we address this   gap by introducing I XNLI , an NLI dataset   forIndic languages . I XNLI consists of En-   glish XNLI data translated into eleven Indic lan-   guages . We use I XNLI to evaluate Indic -   specific models ( trained only on Indic and English   languages ) such as IndicBERT ( Kakwani et al . ,   2020 ) and MuRIL ( Khanuja et al . , 2021 ) , as well   as generic ( train on non- Indic languages ) such as   mBERT and XLM - RoBERTa . Furthermore , we   experimented with several training strategies for   each multi - lingual model . Our experimental results   answers multiple important questions regarding ef-   fective training for Indic NLI . Our contributions   are as follows :   •We introduce I XNLI , an NLI bench-   mark dataset for eleven prominent Indo - Aryan   indic languages from the Indo - European and   Dravidian language families .   •We investigate several strategies to train multi-   lingual models for NLI tasks on I XNLI .   We also explore models cross - lingual NLI   transfer ability across Indic languages and   Intra - Bilingual NLI ability of pretrained multi-   lingual language models.10994The I XNLI dataset , along with scripts ,   is available at https://indicxnli.github .   io/.   2 The I XNLI dataset   We created I XNLI , a NLI data set for In-   diclanguages . I XNLI is similar to existing   XNLI dataset in shape / form , but focusses on In-   diclanguage family . I XNLI include NLI   data for eleven major Indic languages that includes   Assamese ( ‘ as ’ ) , Gujarat ( ‘ gu ’ ) , Kannada ( ‘ kn ’ ) ,   Malayalam ( ‘ ml ’ ) , Marathi ( ‘ mr ’ ) , Odia ( ‘ or ’ ) ,   Punjabi ( ‘ pa ’ ) , Tamil ( ‘ ta ’ ) , Telugu ( ‘ te ’ ) , Hindi   ( ‘ hi ’ ) , and Bengali ( ‘ bn ’ ) . Next we describe the I - XNLI construction and its validation in details .   I XNLI Construction . To create I - XNLI , we follow the approach of the   XNLI dataset and translate the English XNLI   dataset ( premises and hypothesis ) to eleven   Indic -languages . We use the IndicTrans ( Ramesh   et al . , 2022 ) , a state - of - the - art , publicly available   translation model for Indic languages , for trans-   lating from English to Indic languages . The train   ( 392,702 ) , validation ( 2,490 ) , and evaluation sets   ( 5,010 ) of English XNLI were translated from   English into each of the eleven Indic languages .   IndicTrans is a large Transformer - based sequence   to sequence model . It is trained on Samanantar   dataset ( Ramesh et al . , 2022 ) , which is the   largest parallel multi - lingual corpus over eleven   Indic languages . IndicTrans outperforms other   open - source models based on mBART ( Liu et al . ,   2020 ) and mT5 ( Xue et al . , 2021 ) for Indic   language translations and is competitive with   paid translation models such as Google - Translate   or Microsoft - Translate on several benchmarks   ( Ramesh et al . , 2022 ) . Our choice of IndicTrans   was motivated by cost , language coverage and   speed , refer Appendix § A.   I XNLI Validation . While translation may   lose the semantic link between the sentences , re-   cent study by K et al . ( 2021 ) disproved this . K et al .   ( 2021 ) qualitative analysis illustrate that when a   high - quality machine translation system is utilized ,   classification labels and reasoning categories are   only minimally altered for translated NLI datasets .   We also demonstrate the high quality of IndicTrans   translation for I XNLI in two ways ( a. ) man-   ual human validation and , ( b. ) automatic metric   BERTScore ( Zhang * et al . , 2020 ) . Our valida-   tion approach guarantee correctness for the I - XNLI labels . Next , we ’ll discuss on how to   evaluate IndicTrans translations .   H V : We followed SemEval-   2016 Task - I ( Agirre et al . , 2016 ) guidelines . We   hired 2 annotators per languages and calculated the   pearson ( Kirch , 2008 ) and spearman ( spe , 2008 )   correlation over annotations scores of sentences .   D S : Since human validation is   time - consuming and expensive . We sampled 100   diverse sentences of the test set for validation . We   apply the Determinantal Point Process ( Kulesza ,   2012 ) ( DPP ) over sentence representations for di-   verse sampling . DPP maximizes coverage volume   using a minimal sampled set , thus guaranteeing   diversity during sampling . We first used sentence   transformers to convert data to BERT embeddings ,   and then use k - DPP ( Kulesza and Taskar , 2011 )   with k = 100 to sample 100 examples . Using DPP   for diverse sampling is a cost - effective method of   evaluating translation quality . For scoring guide-   lines refer to Appendix § B.   H E : We recruited , 2 speakers for   each of the 11 indic languages as annotators . These   professional annotators are multilingual ( English ,   Indic ) and fluent in both mother - tongue indic and   English language . The remuneration paid was 6.6   cents per sentencefor each indic language .   E : Table 1 shows the final human   evaluation scores . In general , we see that average   human scores is more than 0.85 for all languages .   The Pearson and Spearman Correlation values are   more than 0.7 and 0.8 for all languages respectively .   High human ratings and high correlation between   the annotations support high quality IndicTrans   translation , hence validating I XNLI quality .   A V : Given the absence   ofIndic language XNLI reference data , we use   BERTScore similarity between the original English   and English translated I XNLI for automatic   evaluation . Here too , we use the IndicTrans model   for translating I XNLI into English . This ap-   proach estimates the upper bound on error for the10995   English to Indic translation ( i.e. I XNLI qual-   ity ) , as it approximates the combined error of both   English to Indic translation ( I XNLI creation ) ,   andIndic to English translation ( evaluation ) ( Rapp ,   2009 ; Miyabe and Yoshino , 2015 ; Edunov et al . ,   2020 ; Behr , 2017 ) . We utilize BERTScore for as-   sessment since it correlates better with human judg-   ment at the sentence level than BLEU ( Zhang *   et al . , 2020 ; Papineni et al . , 2002 ) .   We evaluate two translation models , Google   Translate and IndicTrans on the testsets of I - XNLI dataset . We incorporate Google Trans-   late to demonstrate IndicTrans ’s competitiveness in   comparison to commercial translation approaches .   In Table 2 , we used two evaluation strategies for   our evaluation ( a. ) EngTrans : which take the I - XNLI sentence and translated it back to En-   glish using BERT model . ( b. ) Multilingual : directly   compare the English sentences with multilingual   I XNLI sentences using mBERT model .   OnIndic languages , we notice that IndicTrans   is comparable to , and sometimes outperforms ,   Google Translate . Additionally , when results are   compared in a Multilingual setting , we observe a   marginal decrement in scores . This can be because   mBERT does not produce as precise multilingual   embedding as BERT does for English . Addition-   ally , we see a similar pattern in the distribution   of scores across languages for both assessment   strategies on both models . We also computed the   BERTScore ( using mBERT ) between the Hindi test   set of XNLI andI XNLI was found to be 0.87 ,   supporting the high quality of I XNLI .   3 Experiments   E S : Our experiments   compare the performance of several multi - lingual   models , including one particularly developed for   Indic languages . We consider 2 broad categories ,   ( a)Indic Specific which includes IndicBERT and   MuRIL due to their indic specific pretraining , and   ( b)Generic which includes mBERT and XLM-   Roberta due to their pretraining in more than 100   languages . We fine - tuned pre - trained multi - lingualmodels to develop NLI classifiers . The classifiers   takes two sentence as input , i.e. the premise and the   hypothesis and predicts the inference label . See Ap-   pendix § C and § D for models and hyper - parameters   details respectively .   Training - Evaluation Strategies . To train the   NLI classifier , we investigate several strategies .   While the pre - trained multi - lingual models remain   constant , the training and evaluation datasets vary .   1.Indic Train : The models are trained and evalu-   ated on I XNLI . The training set is translated   from the XNLI English , thus a translate - train sce-   nario . 2 . English Train : The models are trained   on original English XNLI data and evaluated on   I XNLI data . This is a zero - shot evaluation   training scenario . 3 . English Eval : The model are   trained on original English XNLI data , but eval-   uated on English translation of I XNLI data .   This is the translate - test scenario . 4 . English +   Indic Train : This approach combines approaches   ( 1)and(2 ) . The model is first pre - finetuned ( Lee   et al . , 2021 ; Aghajanyan et al . , 2021 ) on English   XNLI data and then finetuned on Indic language   ofI XNLI data . 5 . Train All : This approach   begins by fine - tuning the pre - trained model on   English XNLI data , followed by training on all   eleven Indic languages ofI XNLI sequen-   tially . 6 . Cross Lingual Transfer : Additionally ,   we assess the models ’ capacity to transfer between   languages . Where the model is trained on a sin-   gle Indian language and then assessed on all other   Indian languages as well as the training language .   7 . Intra - Bilingual Inference : Lastly , we also   asses the model ’s capability to perform natural lan-   guage inference with premise in English and hy-   pothesis in Indic language .   R A .We summarizes our   findings from Table § 3 results across 4 categories :   A M : In all experiments , MuRIL   performs the best across all indic languages except   in English Eval setup . This can be attributed to   ( a. ) The large model size ( b. ) indic - specific pre-   training data , ( c. ) A Mixture of Masked Language   Modeling ( MLM ) , Translation Language Model-   ing ( TLM ) , and ( d. ) use of transliterated data in   pre - training . XLM - RoBERTa beats MuRIL in   rare scenarios , notably in which the model solely   deals with English data ( e.g. English Eval ) . XLM-   RoBERTa outperforms MuRIL in such cases be-   cause it is better at assessing English than MuRIL,10996   which is designed mostly for indic language . Ad-   ditionally , we discover that , compared to XLM-   RoBERTa , MuRIL indic - specific training further   enhances the model ’s performance . Despite indic-   specific pretraining , IndicBERT performs worse   than mBERT . This can be attributed to the smaller   size of the IndicBERT model , i.e. only 33 M com-   pared to 167 M mBERT ( c.f . Table § 5 in appendix ) .   A L : We see a strong positive   correlation between language performance with   their resource availability . Hindi and Bengali out-   perform , whereas Odia mostly underperform on   majority of benchmarks . Low - resource languages   such as Marathi , Assamese , and Kannada surpris-   ing also perform well . This can be attributed to   the similarity of Marathi with Hindi script , As-   samese with Bengali script , and Kannada with   Tamil and Telugu scripts . This is discussed in de-   tail in appendix 4 . Odia , a low resource language ,   lacks script sharing language partners and hence   performs poorly . Overall , English + Indic Train   method outperforms , with MuRIL performing best .   A S : Our experiments show   that models benefit from language - specific fine   tuning . English + Indic train and Train All have   the best results with minimal deviation across   languages for XLM - R and MuRIL . Additionally ,   Train All follows a high - to - low resource hierarchy   to mitigate the impact of catastrophic forgetting(Goodfellow et al . , 2015 ) . Due to the followed   language order English + Indic train outperform   Train All setting marginally for high resource lan-   guages . Overall , English + Indic Train strategy   performs the best and MuRIL performs the best in   that strategy . This can be attributed to the indic spe-   cific pre - training process of MuRIL which include   both translation and transliteration . Furthermore ,   MuRIL has the second largest size after XLM - R.   C -L T : Models favour   high resource languages such as Hindi andBen-   gali training for cross - lingual transfer . These   language are pre - trained on large mono - lingual   corpora which enhanced performance ( Conneau   et al . , 2020a ) . This setting can be thought equiv-   alent of Hindi and Bengali substitution for En-   glish training . Additionally , when evaluated for   allindic languages , model trains on non- Hindi   and non- Bengali perform substantially better for   Hindi andBengali . Table 3 present results sum-   mary as average evaluation score across all indic   language(rows ) when train on the several indic lan-   guages ( columns ) .   I -B I : We also eval-   uate models on mixed input inference task E-   I XNLI , which consists of English premises   paired with corresponding indic hypothesis . We10997   train model on mixed input using English + In-   dicTrain andTrain All strategies . Table 4 shows   performance of English + Indic Train andTrain   Allmodels on E - I XNLI . Compared to uni-   language inference task , mixed - language input task   perform poorly . Furthermore , contrary to earlier   observations , generic model such as XLM - R out-   performs the Indic specific models . However , In-   dicBERT and MuRIL both perform substantially   better than mBERT . Furthermore , English data aug-   mentation enhance the English + Indic Train set-   ting performance . This can be because , the model   " meta - learns " the task successfully with English   data training ( premise language ) , and further pri-   oritises the model ’s language - specific abilities with   the follow - up indic data training .   4 Error Analysis   In this section , we investigate the correlation be-   tween the language similarity and the model perfor-   mance . We see that the model performs similarly   on similar languages . We evaluate our results on   MuRIL on the English+ Indic finetuning Strategy .   In Figure 1 ( Appendix ) , we observe that the over-   all Correct and Incorrect predictions , Bengali vs   Assamese pair has the total of 81 % overlap , Tamil   vs Kannada has 83 % overlap , Hindi vs Maratha   has 82 % overlap . All the language pairs have the   largest overlap for entailment label for correct la-   bels and largest overlap in contradiction label for   incorrect overlaps . In Figure 2 ( Appendix ) , inter-   estingly Bengali vs Assamese pair and Hindi vs   Marathi has the highest percentage of overlap in   predictions where the most overlap is in entailment   and minimum overlap is in contradiction . While   for Tamil vs Kannada pair has the highest overlap   for neutral and minimum for contradiction .   We have also done error analysis of model perfor-   mance on original Hindi test data already present   in XNLI and data obtained through translations   from IndicTrans in Figure 3 ( Appendix ) . We ob-   serve a total of 82 % overlap in error consistency , and we observe that the greatest number of correct   overlaps is for the entailment label , whereas the   greatest number of incorrect predictions is for the   contradiction label . We see the maximum overlap   in neutral prediction and the least overlap in con-   tradiction prediction in terms of consistency . This   demonstrates that the model performs identically   on both the original Hindi data and the machine-   translated Hindi data , bolstering the legitimacy of   our dataset .   5 Related Work   Recently many Indic -specific resources are devel-   oped such as IndicNLPSuite ( Kakwani et al . , 2020 ) ,   which include ( a. ) word embeddings : IndicFT ,   ( b. ) transformer models : IndicBERT , ( c. ) monolin-   gual corpora : IndicCorp , ( d. ) and , evaluation bench-   mark : IndicGLUE . Furthermore , Indic -specific   pre - processing libraries such as iNLTK ( Arora ,   2020 ) and Indic - nlp - library ( Kunchukuttan , 2020 ) ,   other Indic monolingual corpora : Common Crawl   Oscar Corpus ( Wenzek et al . , 2020 ; Ortiz Suárez   et al . , 2020 ) , multilingual parallel corpora : PMIn-   dia ( Haddow and Kirefu , 2020 ) and Samantar   ( Ramesh et al . , 2022 ) , transformer model MuRIL   ( Khanuja et al . , 2021 ) and language specific Indic-   Transformers ( Jain et al . , 2020 ) exists .   6 Conclusion   With I XNLI we extend the XNLI dataset   forIndic languages family . We benchmark   I XNLI with several multi - lingual models   using various train - test strategies . We also study   the use of English XNLI as pre - finetuning dataset .   Furthermore , we also evaluate models on mixed-   language inference input and cross - lingual transfer   ability . We aim to integrate I XNLI and   benchmark models in IndicGLUE ( Kakwani et al . ,   2020 ) . We also intend to enhance I XNLI   with advanced translation techniques . Another   direction is accessing model performance on   I -I XNLI task , where both premises and   hypothesis are in two distinct Indic languages.109987 Limitations   One of our work ’s key limitations is that the dataset   IndicXNLI was created by machine translation of   the original English XNLI dataset . Although In-   dicXNLI is not human translated , it has been care-   fully evaluated for translation accuracy by a num-   ber of natural bilingual Indic speakers ( 2 for each   language ) . Furthermore , as shown in our research   ( Table 2 ) , employing automatic assessment mea-   sures such as round trip English - English evaluation   via back translation and direct Indic - English sen-   tence comparison is effective . In the past , such   a metric has been shown to be highly beneficial   for comparing without - reference machine trans-   lation ( Bapna et al . , 2022 ; Huang , 1990 ; Moon   et al . , 2020a , b ) . Furthermore , as did with the Hindi   dataset in Appendix E , we might use correlation in   the prediction score between human and machine   translated sets for evaluating translation quality .   Second , adapting an existing dataset risks trans-   ferring biases and shortcomings from the original   XNLI dataset into ours . However , it has been es-   tablished that XNLI is a typical benchmark for   evaluating multilingual and cross - lingual sentence   representation , and it has been used to evaluate sev-   eral multilingual models ( Conneau et al . , 2020b ;   Hu et al . , 2020b ) . Morphological analysis of re-   lated languages , as well as insights into their per-   formance behavior , may be useful . The authors ,   however , are not experts in that area , and such an   assessment would have been outside the scope of   the current work . This study might be expanded   to include language groups other than Indian lan-   guages such as Indo - European . Third , because   of limited resources , the current study did not in-   clude large versions of well - known models such as   XLM - RoBERTa - Large and MuRIL - Large . How-   ever , for IndicBert , mBERT , XLM - RoBERTa , and   MuRIL , we assessed model performance in relation   to model size ( # parameters ) in Table 5 .   Acknowledgement   We thank members of the Utah NLP group for   their valuable insights and suggestions at various   stages of the project ; and reviewers their helpful   comments . We would also like to thank Suhani   Aggarwal , Shibani Krishnatraya and Ayush Dhall   for participating in the dataset verification activity   and helping us find fluent speakers in many differ-   ent indic languages . Additionally , we appreciate   the inputs provided by Vivek Srikumar and EllenRiloff . Vivek Gupta acknowledges support from   Bloomberg ’s Data Science Ph.D. Fellowship .   References109991100011001   A Further Discussions   Why Indic languages ? Indic languages are spo-   ken by more than a billion people in the Indian sub-   continent . With the introduction of IndicNLPSuite   ( Kakwani et al . , 2020 ) by AI4Bharatthere has   been has an increased interest and effort towards   the research for Indic languages model . Recently ,   IndicBERT , MuRIL ( Khanuja et al . , 2021 ) based on   BERT ( Devlin et al . , 2019 ) were introduced for the   Indic languages . Furthermore , generation model   IndicTrans ( Ramesh et al . , 2022 ) and IndicBART   ( Dabre et al . , 2022 ) based on seq2seq architecture   was also published recently . These model use the   Indic enrich monolingual corpora : Common Crawl ,   Oscar and IndicCorp and parallel corpora : Saman-   tar and PMIndia ( Haddow and Kirefu , 2020 ) on   Indic languages for training . Despite significant   progress through large transformer - based Indic lan-   guage models in addition to existing multilingual   models e.g. mBERT ( Devlin et al . , 2019 ) , XLM-   RoBERTa ( Conneau et al . , 2020a ) , and mBART   ( seq2seq ) ( Liu et al . , 2020 ) there is currently a   paucity of benchmark data - sets for evaluating these   huge language models in the Indic language re-   search field . Such benchmark dataset is necessary   for studying the linguistic features of Indic lan-   guages and how well they are perceived by different   multilingual models . Recently , IndicGLUE ( Kak-   wani et al . , 2020 ) was introduced to handle this   scarcity . However , the scope of this benchmark , is   confined to only few tasks and datasets .   Why I XNLI task ? This research provides   an excellent chance to investigate the efficacy of   various Multilingual models on Indic languages   that are rarely evaluated or explored before . Some   of these Indic languages such as ‘ Assamese ’ and   ‘ Odia ’ serve as unseen ( zero - shot ) evaluation for   models such as mBERT ( Pires et al . , 2019 ) , i.e. not   pre - trained on ‘ Assamese ’ . While other models ,   such as XLM - RoBERTa , IndicBERT and MuRIL   covers all our languages but in widely varying pro-   portions in their training data . Our work investi-   gate the correlation effect of cross - lingual training11002for English on these rare Indic languages , which   are not explore by prior studies . Furthermore , we   also investigate the cross - lingual transfer effect   across Indic languages , also not explored before .   We explore the impact of Multilingual training ,   english - data augmentation , unified Indic model per-   formance , cross - lingual transfer of closely related   Indic family and English- Indic NLI through our   work . All the above mention topics are not explore   forIndic language before . We aim to integrate I - XNLI and benchmark models in IndicGLUE   ( Kakwani et al . , 2020 ) . Such a benchmark dataset   is required for investigating the linguistic proper-   ties of Indian languages and how accurately they   are interpreted by various multilingual models . An-   other direction is accessing model performance on   I -I XNLI task , where both premises and   hypothesis are in two distinct Indic languages .   Why IndicTrans for Translation ? We use the   IndicTrans as a translation model for converting   English XNLI toI XNLI because of the fol-   lowing reasons : ( a. ) Open - Source : IndicTrans is   open - source to public for non - commercial usage   without additional fees , while Google - Translate   and Microsoft - Translate require a paid subscription .   ( b.)Light Weight : IndicTrans is the fastest and   the lightest amongst mBART and mT5 on single   GPU machines . Google - Translate and Microsoft-   Translate are also relatively slower due to repeated   network - intensive API calls . ( c. ) indic Cover-   age : Seq2Seq models like mBART and mT5 are   not designed for all languages in the indic fam-   ily . mBART supports seven ( excludes kn , or , pa , as )   while mT5 supports nine languages ( excludes or , as )   out of eleven indic languages . Google - Translate   supports ten out of eleven indic languages ( ex-   cludes Assamese ) . Microsoft Translate supports all   the eleven indic languages . In future , we plan to   enhance I XNLI with better translation .   B Human Validation Scoring   We provide English and indic language I - XNLI ( IndicTrans translated ) sentence to the   recruited native speaker of that indic language for   validation . Before the annotation work , each expert   was given a full explanation of the guidelines that   needed to be followed . The validation instructions   ( mturk template and detailed examples ) are taken   from the Semeval-2016 Task - I. The native speaker   access the sentence pairs assign an integer score   between 0and5 , as follows : 0 : The two sentencesare completely dissimilar . 1 : The two sentences are   not equivalent , but are on the same topic . 2 : The   two sentences are not equivalent , but share some de-   tails . 3 : The two sentences are roughly equivalent ,   but some important information differs / missing . 4 :   The two sentences are mostly equivalent , but some   unimportant details differ . 5 : The two sentences   are exactly equivalent , as they mean the same . The   score depicts the goodness of translated sentence in   terms of semantics , i.e. same meaning as original   English sentence . Scores are then normalized to   a probability range ( between 0 and 1 ) . The final   validation score for each language is determined as   the average of all 100 instances ’ scores .   Additionally , we also computed the BERTScore   between the English and the Hindi test split of   theXNLI , using multi - lingual strategy which   came out to be 70 ( ×102 ) . We presume that the   lower score is attributable to the fact that human-   translated dataset encapsulates a large number of   linguistic nuances , resulting in a change in the   structure and tonality of the sentences , which is   frequently overlooked by machine translation sys-   tems , as highlighted by Bianchi et al . ( 2022 ) .   C Details : Multi - lingual Models   Indic Specific : These models are specially pre-   trained using Mask Language Modeling ( MLM ) or   Translation Language Model ( TLM ) ( CONNEAU   and Lample , 2019 ) on monolingual / bilingual In-   diclanguage corpora . These include models such   as MuRIL and IndicBERT trained on 17 and 11   Indic languages ( + English ) respectively . MuRIL   is pre - trained using Common - Crawl Oscar Cor-   pus ( Ortiz Su’arez et al . , 2019 ) , PMIndia ( Haddow   and Kirefu , 2020 ) on the following languages : en ,   hi , bn , gu , te , ta , or , ml , pa , kn , mni , as , ur . In-   dicBERT is pre - trained using Indic -Corp(Kakwani   et al . , 2020 ) on the following languages : en , hi , bn ,   ta , ml , te , Mr , kn , gu , pa , or , as . Moreover , MuRIL   is also pre - trained with TLM objective ( with MLM   objective ) on machine translated data and machine   transliterated data .   Generic : These are massive multi - lingual mod-   els pre - trained on large number of languages   with MLM . These include multi - lingual BERT   i.e. mBERT ( cased / uncased ) and multi - lingual   RoBERTa i.e. XLM - RoBERTa which are trained   on more than 100 languages . XLM - RoBERTa also11003includes pre - training on all eleven Indic languages .   XLM - RoBERTa is pre - trained using the common   crawl monolingual data . mBERT ( cased / uncased )   includes pre - training on nine of eleven Indic lan-   guages ( Assamese and Odia excluded ) and uses   multi - lingual Wikipedia data for pre - training .   D Details : Hyper Parameters Settings   All the models were trained on google collabora-   toryon TPU - v2 with 8 cores . The code was   built in the PyTorch - lightning framework . We used   accuracy as mentioned in the original XNLI pa-   per ( Conneau et al . , 2018 ) as our metric of choice .   The training was run with an early stopping call-   back with the patience of 3 , validation interval of   0.5 epochs and AdamW as optimizer(Loshchilov   and Hutter , 2019 ) . In Table 5 the hyperparamaters   are abbreviated as mentioned below : ( a. ) PO : Pre-   training Objective . Where MLM stands for masked   Language Modelling , TLM stands for Translation   Language Modelling and TrLM stands for Translit-   eration Language Modelling , ( b. ) CU : Corpus   Used , ( c. ) LR : Learning Rate , ( d. ) BS : Batch Size ,   ( e.)WD : Weight Decay , ( f. ) MSL : Maximum Se-   quence Length , ( g. ) MS : Model Size described as   number of parameters in millions , ( h. ) WS : Warm-   up Step .   E Indic Cross - lingual Transfer   Table 6 ( extension § 3 ) are the cross - lingual trans-   fer results of XLM - R , IndicBERT , mBERT and   MuRIL respectively . The rows of the table consist   of the languages on which the model is trained ,   while the columns represent the evaluation lan-   guages . E.g. , in table 6 the first row represents   that the model is trained on “ as " and then tested   on all the languages in the column . The values in   the row are the accuracy scores of the model when   trained on the language in its leftmost column and   tested on the language in its top - most row column .   XLM - R. the model perform best for the “ bn "   language . The model gives the best performance   average across all other languages if trained on   “ bn " . A model trained in other languages , on average ,   also performs best for “ bn " language . XLM - R   also struggles to correlate with “ kn " , “ or " , and   “ ml " , thus performs poorly on average if trained for   them . At the same time , all models have poor cross-   lingual ability transferability for the “ as " language . IndicBERT . the overall score is comparable to   XLM - R despite it ’s smaller size . On average ,   across languages , the cross - lingual transfer ability   for models trained on varying indic languages were   consistently similar ( b / w 0.5 - 0.6 ) . However , the   evaluation performance for cross - lingual models   evaluated on “ ml " were poor for all indic trained   models . For model trained on some languages ,   “ kn " , “ ml " and“pa " , the best performance was   across diagonal , i.e. indicating the model performs   best on the trained language . This trend was , how-   ever , was not shown in other indic languages , indi-   cating remarkable cross - lingual transfer ability of   the IndicBERT model .   mBERT . the model performs worse for “ or " on   average for both when evaluated and train on . How-   ever , all models performs very consistently for   other indic languages . Model trained on kn , pa ,   ta , hi , and bnperform best on average across lan-   guages . Here too , the best cross - lingual transfer   ability was shown for bnlanguage . mBERT also   have best performance across diagonal for some   languages e.g. “ as",“gu " , “ ml " , “ pa " and“te " .   MuRIL . shows the best overall cross - lingual   transfer ability amongst all the models . MuRIL   only fails to generalize well when trained for “ or "   language . However , model train on other indic   language when evaluated on “ or " performs well .   Model trained on “ ta " and“ml " performs best   across all languages . The best cross - lingual trans-   fer ability was shown for “ bn " and“hi " . Over-   all , MuRIL has better cross - lingual transfer ability   across all languages compared to other models . It   also shows less performance bias for languages   such as “ bn " and“hi " , as compared to XLM - R.   F Intra - Bilingual Inference   We observed a performance loss except for XLM-   RoBERTa when the model is evaluated on E-   I XNLI inference task . The inference models   struggle to correlate and reason together on two dif-   ferent languages ( English , Indic ) sentences . Con-   trary to earlier observation , a generic model such   as XLM - RoBERTa outperforms the Indic specific   models . However , IndicBERT and MuRIL perform   better than mBERT . Bengali perform best for both   the training strategies . We also observe the bene-   fit of English data augmentation English + Indic   Train model , rather than all language augmentation   Train All model.110041100511006