  Alexis RossyTongshuang Wu}Hao Peng}Matthew E. Peters yMatt Gardner   yAllen Institute for Artiﬁcial Intelligence , Seattle , WA , USA   } Paul G. Allen School of Computer Science and Engineering , University of Washington   Microsoft Semantic Machines , USA   { alexisr,matthewp}@allenai.org   { wtshuang,hapeng}@cs.washington.edu   mattgardner@microsoft.com   Abstract   Controlled text perturbation is useful for eval-   uating and improving model generalizability .   However , current techniques rely on training   a model for every target perturbation , which is   expensive and hard to generalize . We present   Tailor , a semantically - controlled text gener-   ation system . T ailor builds on a pretrained   seq2seq model and produces textual outputs   conditioned on control codes derived from se-   mantic representations . We craft a set of op-   erations to modify the control codes , which   in turn steer generation towards targeted at-   tributes . These operations can be further com-   posed into higher - level ones , allowing for ﬂex-   ible perturbation strategies . We demonstrate   the e  ectiveness of these perturbations in mul-   tiple applications . First , we use T ailor to   automatically create high - quality contrast sets   for four distinct natural language processing   ( NLP ) tasks . These contrast sets contain fewer   spurious artifacts and are complementary to   manually annotated ones in their lexical di-   versity . Second , we show that T ailor per-   turbations can improve model generalization   through data augmentation . Perturbing just   2 % of training data leads to a 5.8 - point gain   on an NLI challenge set measuring reliance on   syntactic heuristics .   1 Introduction   Semantic perturbation through controlled text gen-   eration modiﬁes sentences to match certain target   attributes , such as verb tense or sentiment ( e.g. , pos-   itive!negative ) . It has been widely applied to a   variety of tasks , e.g. , changing text style ( Reid and   Zhong , 2021 ) , mitigating dataset biases ( Gardner   et al . , 2021 ) , explaining model behaviors ( Ross   et al . , 2021 ) , and improving model generaliza-   tion ( Teney et al . , 2020 ; Wu et al . , 2021 ) . Existing   e  orts train task - speciﬁc generators , e.g. , trainingFigure 1 : A compositional perturbation using T ai-   lor . Given ( A ) an original sentence , we abstract each   span into a structured header that contains its seman-   tic roles and keywords . Arguments to preserve are in-   cluded in the context , along with blanks ( < i d _ * > ) de-   noting where new generated text may be inserted . We   specify desired perturbations by modifying each con-   trol code ( e.g. , changing role LOCATIVE ) TEMPORAL in   ( B ) , verb tense past ) present , and patient keyword speci-   ﬁcity complete ) partial ) . Given these perturbed control   codes in the input ( C ) , T ailor generates a new sentence   ( D ) that reﬂects the desired perturbations .   a sentiment style transferer requires instances an-   notated with positive andnegative labels ( Madaan   et al . , 2020b ) . As a result , they require costly anno-   tated data and re - training for every task of interest .   This work introduces Tailor , a system that sup-   ports application - agnostic perturbations . At its core   is acontrolled generator ( § 2 ) that ﬂexibly gener-   ates outputs from target semantic attributes , which   we represent through structured control codes in   the inputs . As shown in Figure 1 , these control   codes build on the PropBank semantic analysis   ( Palmer et al . , 2005 ) of the original sentence : For   each argument span , the semantic role andkey-   word control codes specify the desired semantic   content for the span at varying levels of granu-3194larity . To encourage control code following , we   train the Tailor generator with unlikelihood train-   ing(Welleck et al . , 2020 ) to penalize generations   that are not aligned with designated control codes .   The use of semantic role control codes allows   Tailor to perform ﬁne - grained changes to individ-   ual arguments in a sentence ( e.g. , one can change   only the PATIENT in Figure 1 ) . Instead of spec-   ifying a perturbation with a generic target prop-   erty ( e.g. , positive ) negative ) , we can specify the   linguistic transformation used to achieve the prop-   erty ( e.g. , changing sentiment through negation or   antonym replacement ) . Making such ﬁne - grained   perturbations allows for more careful evaluation   and improvement of models ’ language understand-   ing ( Kaushik et al . , 2020 ; Wu et al . , 2021 ) .   To highlight the perturbations facilitated by Tai-   lor , we craft a list of primitive perturbation opera-   tions ( § 3 ) on inputs to the generator ; these can be   easily composed to achieve more complex pertur-   bations . In Figure 1 , Tailor transforms sentence   A to D through a series of perturbations : syntac-   tic rewriting ( changing verb tense ) , then sentence   expansion ( extending “ the athlete ” ) , and ﬁnally   data recombination ( i.e. ,generating new text that   contains “ in ” but follows the TEMPORAL control ) .   Compared to existing approaches that require train-   ing a separate model for every step or annotat-   ing a dataset that represents this transformation   end - to - end , such compositions make Tailor more   cost - e  ective and generalizable . In fact , on nine   ﬁne - grained and compositional StylePTB pertur-   bations ( Lyu et al . , 2021 ) , Tailor achieves perfor-   mance compatible with task - speciﬁc baselines , and   even outperforms them on ﬁve transfers ( § F ) .   Tailor ’s ﬂexible and human - readable control   codes allow for broad , easily extendable applica-   bility . We demonstrate its utility in evaluating and   improving NLP model robustness , showing that   Tailor can help replicate existing contrast sets on   four diverse tasks . By abstracting manual perturba-   tion types in prior work into perturbation strategies   with Tailor , we can apply the changes to larger   datasets while saving manual annotation e  orts .   Our analysis suggests that these contrast sets not   only have high rates of validity , but also reduce   spurious artifacts compared to the original evalua-   tion datasets . In addition , Tailor -produced contrast   sets complement human annotated ones in terms of   lexical diversity : only 10 % of their unique tokens   overlap with manually created contrast sets . Wealso explore Tailor ’s utility in data augmentation .   We ﬁnd that augmenting training data with just   2 % of Tailor perturbations improves the robust-   ness of natural language inference ( NLI ) models to   inference heuristics , increasing performance on the   HANS evaluation set ( McCoy et al . , 2019 ) by an av-   erage of 5.81 points and outperforming a previous   syntactic augmentation method for NLI .   2 Tailor ’s Controllable Generator   Here , we provide an overview of the Tailor gener-   ator . We ﬁrst outline three types of controls ( § 2.1 )   that allow for specifying sentence meanings at vary-   ing granularity . Next , we explain how to embed   them within inputs ( § 2.2 ) to the generator . We   train the generator to follow control codes with   unlikelihood training ( § 2.3 ) .   2.1 Three Types of Controls   We use the following three types of controls to   specify the shallow semantics , actual content , and   ordering of various phrases in a sentence .   Semantic roles to denote shallow semantics .   We rely on the PropBank semantic formal-   ism ( Palmer et al . , 2005 ) , as it provides well-   established representations of meanings that are   generalizable across di  erent predicates and lan-   guages ( Haji ˇc et al . , 2009 ) . It represents sentence   meanings with predicate - argument structures . Pred-   icates ( e.g. , “ comforted ” in Figure 1 ) are usually   evoked by verbs and reﬂect events ( what happened ) .   Arguments , usually spans of tokens , realize the-   matic roles of predicates ; they include core argu-   ments such as whodid something ( e.g. ,“the doctor ”   in Figure 1 ) and to whom ( “ the athlete ” ) , as well as   adjunct arguments like where something happened   ( “ In the operation room ” ) and how .   Keywords for steering the actual generated con-   tent of predicates and arguments . The keywords   can be complete and fully specify the target text of   a given span ( e.g. , “ the doctor ” for the AGENT in   Table 1A ) , sparse and add no constraints beyond   the semantic role ( e.g. , * for LOCATIVE ) , or partial   and specify some of the target text ( e.g. , “ athlete ”   forPATIENT ) . As later shown in Table 3 , these   keyword controls are important for supporting a   variety of perturbation strategies and applications .   Span ordering for determining how the the-   matic roles should be combined . We use predicate   form to control the order of core arguments . For ex-   ample , to distinguish “ the athlete was comforted by3195   the doctor ” from the semantically equivalent “ the   doctor comforted the athlete , ” we target the former   ordering through a passive control , and the latter   through an active control . Additionally , we use   the location of blank tokens ( < i d _ * > in Figure 1   and Table 1 ) to determine the position of generated   arguments ( Wu et al . , 2021 ) — e.g. , where “ in the   operating room ” appears in the generation .   2.2 Input Format Design   We integrate the aforementioned controls into the   input format detailed in § A.1 and ﬁnetune seq2seq   models to output corresponding full sentences .   As shown in Table 1 , we start our input with   a bracketed header , which contains a series of   abstract control codes ( Table 2 ) that denote the   semantic role and keywords ( content /speciﬁcity )   to realize for each predicate and argument . For   example , in Table 1A , the control codes for the   predicate are “ VERB + active : past ” and the agent   argument are “ AGENT + complete : the doctor . ” Wemap original semantic roles in PropBank to human-   readable labels ( i.e. ,ARG0!AGENT ) in order to   leverage knowledge learned by pretrained models   about roles ’ meanings ( Paolini et al . , 2021 ) .   After the header , we append the context , which   consists of text to preserved and blanks specify-   ing where new text should be generated . Given   such inputs , we train our generator to output text   augmented with control codes and brackets , which   together specify which generated spans correspond   to which controls . For example , in Table 1B ,   “ [ LOCATIVE : In the operating room ] ” represents the   target span of control codes “ LOCATIVE + sparse :   * ” and is generated at the location of blank < id_0 >   right before the preserved context “ the doctor . ”   We make three key design choices to allow Tai-   lorto generate roles ﬂuently even when the opti-   mal ordering of roles is unknown ( e.g. , when in-   troducing a new argument ) . First , we explicitly   separate signal about role placement ( e.g. , blanks   in the context ) from the role ’s semantic controls   ( e.g. , control codes in the header ) such that we   can specify the target semantic attributes for a role   without tying them to a speciﬁc target placement .   Second , we order the control codes in the header in   an input - independent way ( see § A.1 ) to discourage   the generator from learning to rely on their relative   orders . Third , we insert extra empty blanks into the   context ( e.g. , < id_3 > in Table 1B ) such that the   Tailor generator can generate spans in the blank   locations that result in the most ﬂuent text .   With this ﬂexibility in argument ordering comes   the challenge of making strict controls on a single   argument : Even if we only want to change verb   tense , the generator may reorder other arguments.3196To enable strict control over generations , which fa-   cilitates minimal perturbations ( Ross et al . , 2021 ) ,   we further vary the number of arguments encoded   in the header . As in Table 1C , our generator can   take inputs that only mask a subset of arguments ,   such that , e.g. , any changes on the LOCATIVE argu-   ment or VERB do not a  ect the agent and patient .   2.3 Training   We ﬁnetune T5 - base ( Ra  el et al . , 2020 ) on input-   output pairs derived from gold semantic roles   in OntoNotes 5.0 train ( Table 1 ; Pradhan et al . ,   2013).To train our generator to handle the dif-   ferent input formats described in § 2.2 , for each   original input , we randomly sample the numbers of   arguments to mask , number and placement of extra   empty blanks , and keyword content /speciﬁcity for   each role . See § A.2 for details .   Standard maximum likelihood estimation ( MLE )   is insu cient for training our generator to follow   the controls , as there may exist signals beyond the   given controls for the form of a generation . Con-   sider the input : [ VERB+active + past : comfort   |AGENT + partial : athlete |PATIENT + complete :   the doctor ] In the operating room , < id_0 > , < id_1 >   < id_2 > . A generator trained with MLE may ignore   controls AGENT andPATIENT and instead output   text “ The doctor comforted the athlete ” rather than   “ The athlete comforted the doctor , ” as the training   data distribution may reﬂect that the former is more   natural given context “ in the operation room . ”   To encourage reliance on controls , we incorpo-   rateunlikelihood training ( Welleck et al . , 2020 )   to penalize generations that conﬂict with input con-   trols . That is , besides Table 1A – C which are used   for MLE , we also create “ negative ” samples by ran-   domly perturbing the control codes in our header   ( as in Table 1N , last row ) , such that most spans in   the target output are not aligned with the control   codes . We create up to three negative samples per   input by randomly perturbing 1 ) verb voice /tense   and primary controls for arguments , 2 ) keyword   contents , and 3 ) keyword speciﬁcities ( § A.1).Our   ﬁnal training data consists of 223 K positive and   541 K negative examples .   3 Creating Perturbations with T ailor   With Tailor , we can create diverse perturbations   by modifying input controls . Given an originalsentence , we transform it to an input for Tailor by   extracting its semantic parses , masking spans we   wish to modify , and providing their control codes .   Then , we modify the control codes in the input to   generate perturbed sentences with Tailor , ﬁltering   out degenerate ones .   Primitive perturbation operations . We pro-   vide an easily - extendable set of perturbation   macros , which capture three common types of per-   turbations in prior work , shown in Table 3 : First ,   syntactic rewriting primarily involves shu ing text   to create paraphrases ( Zhang et al . , 2019 ) or ad-   versarial examples ( Iyyer et al . , 2018 ) . We imple-   ment such shu ing through operations that per-   turb predicate forms , move blank tokens , and swap   keyword contents of arguments . Second , expan-   sion and abstraction add or remove text fragments   from a sentence ( Wu et al . , 2021 ) . We recreate   these through operations on keywords ( e.g. , dele-   tion ) . Finally , data recombination involves recom-   bining existing textual fragments , within or across   inputs ( Akyürek et al . , 2021 ; Andreas , 2020 ) . With   CHANGE_CONTENT , we can integrate additional con-   text ( e.g. , from corresponding paragraphs in ques-   tion answering tasks ) into generations .   While our control codes are mostly derived   from semantic roles , these primitive operations   broadly cover both syntactic and semantic changes .   They can also be used in conjunction with external   knowledge bases to achieve targeted edits . , or be   composed to achieve more complex perturbation   strategies as shown in § 5 , § 6 , and Appendix § F.   Filtering generations . We notice that the Tailor   generator produces degenerate outputs for some   inputs ; we exclude these heuristically based on   content and perplexity scores ( see § C for details ) .   4 Intrinsic Evaluation   Following previous work ( Wu et al . , 2021 ; Ross   et al . , 2021 ) , we evaluate Tailor generations on   sentence likelihood , controllability , and closeness.3197   We additionally evaluate Tailor ’s unique ability to   make ﬁne - grained and compositional perturbations .   Metrics . Likelihood measures whether the gener-   ated text is grammatically correct and semantically   meaningful . Following Ross et al . ( 2021 ) , we ask   whether perturbing a sentence with Tailor drasti-   cally changes its likelihood . Using a pretrained   GPT-2 , we compute language modeling losses for   both the original and edited texts and report the   ratio of edited /original . We desire a value of 1.0 ,   which indicates equivalent losses for the two .   Controllability measures if the generator re-   sponds to the controls given in inputs . We rely   on cycle consistency to evaluate the controls in Ta-   ble 2 : For a given generation , we check whether   the predicted semantic roles from an SRL system   match the control codes in the input ( e.g. , whether   “ in the midst of the earthquake ” in Figure 1 gets   detected with a TEMPORAL tag ) . Since SRL predic-   tions can be noisy , we manually inspect a subset   of 98 generated spans and verify that cycle consis-   tency measures positively correlate with ground-   truth controllability , with Matthews correlation co-   ecient  = 0:49 ( more details in § B ) .   Closeness captures whether the generated sen-   tence involves only necessary changes . Since our   generator takes controls at the argument level , we   measure closeness with a weighted F1 score on   the expected - to - change and actually - changed spans   in the original sentence . We identify expected - to - change spans from perturbation operations ; in Fig-   ure 1A , all spans should be changed except for   agent “ the doctor . ” Then , we deem a span actually   edited if50 % tokens within a span are changed   ( e.g. , “ operation room ” in LOCATIVE ) .We weigh   spans by their lengths to arrive at the ﬁnal F1 score .   Compositionality . We evaluate Tailor without   any ﬁnetuning on the StylePTB benchmark ( Lyu   et al . , 2021 ) , which builds on the Penn Treebank   and assesses both single , ﬁne - grained transfers ( e.g. ,   To Future Tense ) and compositional ones that con-   currently edit multiple dimensions ( e.g. , To Future   Tense + Active To Passive ) . We report mean BLEU   scores and compare to the transfer - speciﬁc base-   lines reported in the S tylePTB paper ( See § F ) .   Data . We use StylePTB ( Lyu et al . , 2021 ) to   evaluate compositionality . For other metrics , we   perturb 1,000 randomly selected sentences from the   OntoNotes 5.0 validation dataset , created the same   way as negative samples during training ( § A.1 ) ,   and evaluate on these perturbations.31984.1 Results   Tailor generates perturbations with a loss ratio of   0.982 , indicating no notable change in language   modeling loss after perturbation . As shown in Ta-   ble 4 , Tailor perturbations also tend to be close to   the original sentence ( F1 = 64:3 % ) , with reason-   ably correct predicates ( 74.3%-81.6 % of the time )   and arguments ( 70.5 % controllability on semantic   roles and 64.5 % on contents . ) Tailor also demon-   strates the ability to make compositional changes ; it   achieves results comparable to those of ﬁne - tuned   baselines on 8 /9 tested transfers , and even outper-   forms the ﬁne - tuned baseline on 5 of them ( See § F   and Table 11 for more details ) .   E  ect of Unlikelihood Training . We compare   Tailor with a baseline that is ﬁnetuned on T5with-   outunlikelihood training ( called Tailorin Ta-   ble 4 ) . Across all metrics , unlikelihood training out-   performs Tailor , with more controllable and   closer perturbations ( up to a 20 % increase ) .   Modulating likelihood and closeness . As men-   tioned in § 2.2 , our input format supports modu-   lating likelihood and closeness . We can increase   closeness by only masking the arguments we want   to perturb . To quantify this e  ect , we randomly se-   lect a single argument to perturb for 1 K sentences ,   but vary the number of masked arguments and num-   ber of inserted blanks . As desired , closeness is   maximized when we mask only the argument we   wish to perturb , as in Table 1B ( with F1=67:4 % ) ,   whereas masking two extra arguments and inserting   six extra blanks decreases closeness by 3 % and 6 % ,   respectively . On the other hand , we can prioritize   likelihood ( at the cost of closeness ) by adding more   blanks ( e.g. , insert extra roles whose optimal loca-   tions are not known in advance ) . On another 1 K   sentences , we observe that adding six extra blanks   increases the likelihood ratio from 0.93 to 0.95 .   5 Contrast Set Creation   Manually creating contrast sets is expensive , e.g. ,   Gardner et al . ( 2020 ) reported spending 10 - 15 min-   utes per perturbation for UD Parsing , whereas la-   beling existing data is more e cient ( Wu et al . ,   2021 ) . We show that Tailor can reduce human   labor by automatically generating contrast set in-   stances such that annotators only have to label them .   We create Tailor -generated contrast sets for fourtasks : boolean question answering ( BoolQ : Clark   et al . , 2019 ) , extractive QA ( SQuAD : Rajpurkar   et al . , 2016 ) , dependency tree parsing ( UD En-   glish : Nivre et al . , 2016 ) , and temporal relation   extraction ( MATRES : Ning et al . , 2018 ) .   5.1 Replicating Contrast Sets with T ailor   We take advantage of two key properties of Tai-   lor : First , Tailor can make context - dependent   changes . To recreate the BoolQ contrast set , we   replicate Entity Change in Gardner et al . ( 2020 )   by replacing content keywords in questions with   words in the paragraph that have the same seman-   tic roles . For example , the paragraph in Table 5   indicates that “ his bride ” can serve as an AGENT .   Second , Tailor allows for compositonal changes .   For example , as in Table 5 , we change preposi-   tional phrase ( PP ) attachments from noun!verb   to recreate the UD Parsing contrast set through   the following composition of perturbation opera-   tions : remove the prepositional phrase from the   patient keyword ( e.g. , “ a diverse range of food   atallprices andstyles ” ) , and introduce an adjunct   argument with the preposition as partial keyword   ( e.g. ,LOCATIVE “ at ” ) . More details are in § D.1 .   Contrast set validity . We consider our perturba-   tion strategies successful if they help reduce human   labor , i.e. ,a contrast set author can easily label or   take inspiration from Tailor ’s generations . Two   authors sampled 100 original instances per task ,   inspected the top - K Tailor perturbations , and la-   beled an instance to be valid if there is at least one   perturbation that changes the groundtruth answer   while being ﬂuent or requiring only minor ﬁxes .   Table 5 shows that these Tailor perturbation strate-   gies generate contrast sets with high validity .   5.2 Measuring Contrast Set Quality   We sanity check that Tailor -generated contrast sets   can be used to reveal model errors . For example ,   aT5 - base model ﬁnetuned on BoolQ ( with test   accuracy 83 % ) has a performance of 65 % on both   Tailor -generated contrast sets and Gardner et al .   ( 2020 ) ’s ( more in § D.2 ) . However , this metric is3199   only a proxy for the quality of evaluation data , since   it can be made intentionally low if we generate all   examples to target a known model error . Thus , we   directly analyze the quality of Tailor contrast sets   by measuring their lexical diversity and impact on   token - level dataset artifacts , both of which play   important roles in dataset debiasing .   We measure lexical diversity on UD Parsing con-   trast sets because it involves su cient generation   of new content . We compare Tailor - and human-   generated ( Gardner et al . , 2020 ) contrastive edits   for the same 100 UD instances : we randomly sam-   ple one edit for each valid instance , heuristically   extract modiﬁed PPs , and compute diversity as the   ratio of unique to total new tokens in the PPs , ﬁl-   tering stopwords . For noun!verb , the ratios are   respectively 0.78 and 0.99 for Tailor and humans ;   forverb!noun , both are 1.0 . Thus , Tailor can help   generate contrast sets without signiﬁcantly reduc-   ing lexical diversity . Furthermore , Tailor outputs   are distinguishable from humans ’ : their unique to-   kens only overlap for < 15 % inverb!noun , and   6 % for noun!verb , suggesting that Tailor can be   used as a collaborative tool to diversify generation .   We also ask whether Tailor perturbations can   reduce dataset artifacts . Gardner et al . ( 2021 )   devise a statistical test for dataset artifacts that   builds on the argument that no simple feature ( e.g. ,   single token ) should show statistically signiﬁcant   correlation with labels in a language understand-   ing problem . In Figure 2 , we display the results :   We plot the numbers of occurrences of each token   against the conditional probability of the positive   label given that token for both the BoolQ validationdata ( red dots ) and the contrast created by Tailor   ( green dots ) . All tokens above or below the blue   line show statistically signiﬁcant correlation with   positive labels and thus are considered dataset arti-   facts in Gardner et al . ( 2021 ) ’s framework . While   many tokens in the original BoolQ data exhibit sig-   niﬁcant correlations , most in the Tailor contrast set   fall within the conﬁdence region . Thus , Tailor can   help create less evaluation data with fewer artifacts .   5.3 Discussion   Across the four tasks , we are able to replicate   all perturbation strategies described by authors of   the original contrast sets . While Tailor requires   manual e  ort to implement perturbation strate-   gies , we believe the overall saved annotation ef-   fort outweighs this initial cost . First , once imple-   mented , Tailor perturbations can be applied to   large datasets without requiring additional anno-   tation e  ort . This large - scale applicability is espe-   cially useful for tasks whose single - instance annota-   tion time is signiﬁcant ( e.g. , UD Parsing ) . Second ,   given that Tailor generations are distinguishable   from human ones , they may have the potential to   compensate for human omissions and thereby in-   crease test case variety , which has been shown to   be beneﬁcial in prior work ( Ribeiro et al . , 2020 ) ;   an interesting direction for future work would be   to investigate this hypothesis in more detail . Third ,   the implementation overhead itself diminishes as   more strategies are implemented . In BoolQ , while   Gardner et al . ( 2020 ) manually created “ a diverse   set of perturbations , including adjective , entity , and   event changes ” ( see their Appendix B.9 ) , these are3200   all a type of data recombination in Table 3 , and we   can unify their implementations with Tailor into   the aforementioned keyword replacement in § 5.1 .   6 Data Augmentation   We explore whether Tailor can be combined   with noisy automated labeling for data augmen-   tation . For the Stanford Natural Language Infer-   ence ( SNLI ) task ( Bowman et al . , 2015 ) , we show   that data augmentation with Tailor perturbations   increases model robustness to inference heuristics .   Min et al . ( 2020 ) ﬁnd that augmenting SNLI   training data by swapping hypotheses ’ sub-   ject / objects ( e.g. , This collection contains 16 El   Grecos.916 El Grecos contain this collection ) im-   proves performance on HANS , a challenge set for   diagnosing fallible syntactic heuristics in NLI mod-   els ( McCoy et al . , 2019 ) . Following this , we use   Tailor to perturb hypotheses with the SWAP_CORE   operation such that original hypothesis ! premise   andperturbed hypothesis ! new hypothesis .   We ﬁnetune RoBERT a - base ( Liu et al . , 2019 ) on   di  erent training datasets : original SNLI train data   ( unaugmented baseline ) , SNLI train augmented   with Min et al . ( 2020 ) ( augmented baseline , re-   ferred to as Syntactic Perturb . in Table 6 ) , and   SNLI train augmented with Tailor perturbations .   We augment2 % of SNLI train . For each subset ,   we train 20 models with di  erent random seeds .   We evaluate each classiﬁer on the in - domain SNLI   test set and the out - of - domain HANS test set .   As shown in Table 6 , augmentation with Tailor   leads to 5.8 - point gain on HANS overall , HANS   and a 29.2 - point gain on “ non - entailment , ” com-   pared to the unaugmented baseline . The improve-   ments are signiﬁcant , with t= 6:42,p<10   using Student ’s t - test . Thus , Tailor perturbations   decrease reliance on the lexical - overlap - based in-   ference heuristic for NLI .   Furthermore , Tailor outperforms Syntactic Per-   turb . , an augmented baseline designed speciﬁcally   for NLI . We hypothesize that although they cre-   ate augmentations through similar transformations ,   Min et al . ( 2020 ) ’s approach is limited to inputs   with speciﬁc syntactic conﬁgurations , whereas Tai-   lor’sSWAP_CORE argument is applicable to any   AGENT andPATIENT arguments . Thus , Tailor is   useful for improving model robustness – more so   than template - based approaches .   7 Related Work   Controllable text generation has been widely used   to inﬂuence various properties of generated text for   text summarization ( Peng et al . , 2019 ) , data aug-   mentation ( Lee et al . , 2021 ) , style transfer ( Reid   and Zhong , 2021 ; Madaan et al . , 2020a ) , adver-   sarial example generation ( Iyyer et al . , 2018 ) , etc .   Most generators take simple controls like tense ( Hu   et al . , 2017 ) , topic ( Keskar et al . , 2019 ) , or senti-   ment polarity ( Dathathri et al . , 2020 ) , which un-   derspecify desired transformations . In contrast ,   Tailor concretizes otherwise sparse controls ( e.g. ,   we can specify making a sentence more negative   through negation . ) Recent works incorporating   syntactic structures for paraphrasing ( Iyyer et al . ,   2018 ; Chen et al . , 2019 ; Bao et al . , 2019 ; Kumar   et al . , 2020 ; Sun et al . , 2021 ; Huang and Chang ,   2021 ) or discrete semantic signatures for diverse   generation ( Weir et al . , 2020 ) are similar to Tailor   in their high - dimensional speciﬁcation .   Also closely related are methods that reconstruct   sentences from structured semantic representations .   The most similar related work is InFillmore ( Ou   et al . , 2021 ) , which uses semantic representations   derived from FrameNet with constrained decod-   ing to guide generation . While InFillmore tunes   the higher - level semantics of a sentence , Tailor ’s   semantic controls incorporate ﬁne - grained infor-   mation about the location and semantics of tex-3201tual phrases ; in addition , we demonstrate two new   applications for semantically - guided generation ,   contrast set generation and data augmentation . Ab-   stract Meaning Representation ( Banarescu et al . ,   2013 ; Mager et al . , 2020 ) is an alternative semantic   representation worth exploring for data perturba-   tion , as it may further enable controls on entity   recursions ( Damonte and Cohen , 2019 ) , though   expressing such relationships is nontrivial .   Controlled generators have also been success-   fully used to perturb text for model training ,   evaluation , and explanation . They usually rely   on application - speciﬁc labels ( Ross et al . , 2021 ;   Madaan et al . , 2020b ; Sha et al . , 2021 ; Akyürek   et al . , 2021 ) or require pairs of original and per-   turbed sentences ( Wu et al . , 2021 ) , which are ex-   pensive to generalize .   Also related are the creation of minimally edited   datasets , either through manual rewriting ( Gard-   ner et al . , 2020 ; Kaushik et al . , 2020 ) , or creating   perturbation templates ( Andreas , 2020 ; Li et al . ,   2020 ; Ribeiro et al . , 2020 ; Wu et al . , 2019 ) ; Tailor   reduces the human e  orts these studies require .   8 Discussion   We propose Tailor , a system that enables task-   agnostic , complex and context - aware perturbations .   Tailor demonstrates that it is possible to drive   ﬁne - grained perturbations with semantic features   directly derived from an instance . Crucially , it   shows that incorporating classical linguistic struc-   tures with modern large - scale neural architectures   is feasible : With the help of modern pretrained   large models , PropBank - style shallow semantic   representations can help steer generation towards   desired meanings .   Factors that a  ect T ailor ’s capability . Though   broadly applicable , Tailor ’s controllability and ef-   fectiveness vary for di  erent inputs . First , creating   automatic perturbations with Tailor requires ex-   ternal SRL predictors , which can be noisy on rare   semantic roles or low - resource languages . Em-   pirically , this did not seem to be a bottleneck , as ex-   posing biases in downstream tasks does not usually   require rarity at the semantic role level ( e.g. , test-   ing syntactic heuristics in NLI requires swapping   only agents and patients ) . However , perturbing   more challenging linguistic phenomena may re - quire careful SRL predictor augmentation or even   manual semantic role annotation .   We also notice Tailor can sometimes produce   degenerate outputs . We hypothesize that this is a   byproduct of unlikelihood training — i.e. ,the gen-   erator learns to reduce the likelihood of negative se-   quences by generating tokens that are very unlikely   to appear in natural text . Generation hyperparame-   ters ( e.g. , number of beams ) can reduce the number   of degenerate outputs . While we perform unlike-   lihood training at the sequence level , future work   can investigate the e  ect of penalizing generation   at the level of tokens or spans , which may provide   ﬁner - grained signals for which spans should be   considered unlikely , as well as more strategically   balancing positive and negative samples .   Extending T ailor .We believe the Tailor gener-   ator is well - suited for controlled generation tasks   beyond the perturbation - based tasks we explore .   Given key entities or arguments as keywords and   fully masked contexts , we envision Tailor can help   generate arguments ( Schiller et al . , 2021 ) , compo-   sitionally augment data ( Akyürek et al . , 2021 ) , or   generate captions ( Chen et al . , 2020 ) . In particular ,   as shown in § 5 , Tailor ’s human - readable controls   can support humans on data curation , which sug-   gests that designing NLP models for augmenting   human capabilities is a promising direction .   The design of controls is also worthy of in - depth   exploration . As mentioned in § 7 , AMR might be   an alternative for semantic representation , if our   primary goal is to express non - sequential relations .   On the other hand , dependency parsing labels are   useful for syntactic changes ; future work may try   to balance syntactic and semantic controls .   Having noted these opportunities , we believe   Tailor is already a powerful tool for perturbation ,   particularly for tasks where compositional changes   are required . Tailor is opensource , and available   athttps://github.com/allenai/tailor .   Acknowledgements   We thank Ana Marasovi ´ c , William Merrill , Thomas   R. McCoy , and Daniel S. Weld for their helpful   suggestions , and the anonymous reviewers for their   feedback . Hao Peng is supported by a Google   Fellowship.3202References3203320432053206   A T ailor Generator Details   A.1 Input and Output Formats   All headers in inputs to the Tailor generator be-   gin with predicate controls , followed by core   argument controls ( ﬁrst AGENT , then PATIENT ) ,   and then randomly ordered adjunct argument con-   trols ( LOCATIVE , TEMPORAL , etc . ) . Secondary con-   trols are always given in the order of control   code + voice + tense : lemma for verbs and control   code + keyword speciﬁcity : keyword content for ar-   guments . We also blank the auxiliary verbs of the   predicate in an input , using spacy to detect them .   We exclude discontinuous arguments ( e.g. , those   with raw SRL labels B - C- * ) , as well as those with   referents ( e.g. , those with raw SRL labels B - R- * ) ,   from input headers . We map ARG0!AGENT and   ARG1!PATIENT . For other numbered arguments ,   we create human - readable labels by using argument   functions included in the PropBank frame for the   given predicate ( Palmer et al . , 2005 ) .   On the output side , we ask the model to generate   the full sentence ( Table 1 ) . We add the semantic   roles for all the generated arguments , to help the   generator build explicit mappings between the in-   put control codes and the output spans – this can be   important when the input codes are ambiguous ( e.g. ,   aTEMPORAL argument and a LOCATIVE argument   that both have keywords “ in ” ) . To use generations   in downstream applications , we remove these con-   trol codes to obtain cleaned outputs using regular   expression matching .   A.2 Training details   Training inputs . During training , we randomly   select , with equal probabilities , whether to mask   all arguments or a subset . If a subset , we uniformly   select the proportion of arguments to mask . To de-   termine the number of extra blanks , we uniformly   select a value less than 10 and set the number of   blanks to be the maximum of that selected value   and the number of arguments to mask . Any extra   blanks ( i.e. ,remaining after masking arguments )   are inserted between subtrees of the predicate .   We also randomly select keyword contents and   keyword speciﬁcities . For each argument span , we   extract , using spacy , four keyword types from the   span : noun chunks , random subtrees , exact key-   words , and preﬁxes . For preﬁxes , we uniformlyselect a number of tokens to include as the key-   word ( from 1 to the entire span ) . Once we extract   all keyword candidates , we create corresponding   keyword speciﬁcities : A keyword is complete if   it contains all tokens in the original span , partial   if it contains at least all but 5 tokens , and sparse   otherwise . Then , we uniformly select a keyword   content /speciﬁcity pair for each span from the set   of keyword candidates ( including the * symbol ) .   To generate unlikelihood samples , we use three   perturbation strategies on inputs : 1 ) Change seman-   tic roles by swapping thematic role control codes   ( agent /patient ) , changing adjunct argument control   codes to a uniformly selected other adjunct control   code , and changing verb tense /voice . We swap verb   tense / voice because the control code VERB does not   have natural candidate swaps , given that predicates   are the building block for semantic parses . We   also swap the control codes in the target output . 2 )   Change keyword contents by replacing verb lem-   mas and keywords for both the predicate and all   arguments . To make content swaps , we ﬁrst gather   the most commonly occurring keyword contents   for each argument and predicate in Ontonotes 5.0   train , extracted according to the same process as   described above for creating training inputs . For   each primary control code and keyword speciﬁcity   ( e.g. ,TEMPORAL+partial ) , we store the 15most   commonly occurring keyword contents . To create   the negative inputs , for each span , we uniformly   sample from these stored keywords given the span ’s   control code and keyword speciﬁcity . This pertur-   bation is designed to discourage the generator from   ignoring the keyword content and merely generat-   ing commonly occurring text for particular seman-   tic roles . 3 ) Change keyword speciﬁcities by uni-   formly selecting a di  erent speciﬁcity . We weight   each unlikelihood sample equally , with a reward of   -1 ( vs +1 for positive samples ) .   Hyperparameters . We train the Tailor genera-   tor using Transformers ( Wolf et al . , 2020 ) for 103207epochs with early stopping . We use batch size 4   and default values for other parameters ( learning   rate of 5e-5 , Adam optimizer ) .   B Intrinsic Evaluation Details   E  ectiveness of cycle consistency . To evaluate   to what extent cycle consistency reﬂects true con-   trollability , we conducted additional manual an-   notation on role - following . We sampled 25 sen-   tences from the Ontonotes 5.0 development set ,   transformed them into inputs with varying num-   bers of masked arguments and blank tokens , and   created up to two perturbed inputs per sentence   by randomly replacing their blanked adjunct argu-   ments with other candidate semantic roles ( using   CHANGE_TAG ) . The candidate roles were extracted   from the frameset for each predicate verb . We   also changed the keyword speciﬁcity to SPARSE , to   make these role swaps more plausible .   We collected Tailor andTailorgenerations   from both the original and perturbed inputs , and   one author manually validated the generated span   for each speciﬁed argument ( 98 in total ) . Our anno-   tations were following ornot following the control   ( i.e. ,the span matches /does not match the desig-   nated semantic role ) , or the set of controls can be   impossible to follow if the human annotator could   not think of any generation that would satisfy the   control codes , due to a conﬂict between the role ,   keywords , and blank placement . We then com-   puted the Matthews correlation coe cient ( MCC )   between the controllability of the role label as mea-   sured by the SRL predictor with the gold controlla-   bility annotations for the subset of roles without an-   notation impossible . The MCCs are 0.49 and 0.51   forTailorandTailor , respectively , suggest-   ing that the cycle consistency measures positively   correlate with true controllability measures .   Additionally , we measure to what extent the con-   trollability measures from cycle consistency cor-   relate with whether a set of controls is impossible   to follow . The MCCs are -0.33 for both Tailor   andTailor ; thus , incorrect role - following as   measured by cycle consistency is positively corre-   lated with controls that are impossible to follow .   14/98 instances were manually annotated as hav-   ing impossible - to - follow controls , suggesting that   a nontrivial proportion of the generations for which   our intrinsic evaluation measures in § 4 found to be   unaligned with designated role control codes may   be explained by impossible - to - follow controls . C Degenerate Outputs   We observe that Tailor produces degenerate out-   puts for some inputs , as shown in Table 8 . We   hypothesize that this is a byproduct of unlikeli-   hood training : The generator may learn to reduce   the likelihood of negative sequences by generating   tokens that are very unlikely to appear in natural   text . Certain generation hyperparameters , such as   the number of beams , can reduce the number of   degenerate outputs . While we perform unlikeli-   hood training at the sequence level , future work   can investigate the e  ect of penalizing generation   at the level of tokens or spans , which may provide   ﬁner - grained signals for which spans should be   considered unlikely , as well as more strategically   balancing positive and negative samples .   Filtering . To exclude degenerations when using   Tailor generations in downstream applications , we   employ a combination of heuristics and perplexity-   based ﬁltering . As shown by the examples in Ta-   ble 8 , degenerate outputs are easy to detect : We   can simply search for whether the output includes   “ sanatate . ” We also use cuto  s in perplexity scores   computed with GPT-2 to ﬁlter degenerations , as   degenerations have signiﬁcantly lower perplexities   than non - degenerate outputs : For generations for   300 randomly sampled validation inputs , the Tailor   generator produced generations with a mean per-   plexity of -346.46 for degenerate outputs ( 12 /300 )   compared to -86.747 for others .   D Contrast Set Details ( § 5 )   D.1 Perturbation Strategies   In Table 7 , we illustrate our perturbation strategies   for creating contrast sets . Besides BoolQ , already   introduced in § 5 , the Matres contrast set Gardner   et al . ( 2020 ) relies on within - sentence context : As   a task that requires detecting and changing the tem-   poral order of two verbs , our perturbations heavily   rely on their syntactic relationships . For example ,   to change the appearance order of verbs in text ( as   described in ( Gardner et al . , 2020 ) ) , we would take   the parent verb as the base predicate , and MOVE the   text span containing the child verb .   ForQA implication ( Ribeiro et al . , 2019 ) , we   combine Tailor with semantic heuristics : by deﬁn-   ing mappings between WH - words and answer   types ( e.g. , “ who ” and “ the Huguenots ” ) , we can   easily create new questions about di  erent targets.32083209   ForUD English ( Nivre et al . , 2016 ) , we use   constrained decoding ( Hokamp and Liu , 2017 ) to   prevent generation of the original prepositional   phrase . Our strategy for changing prepositional   phrase ( PP ) attachments from verb!noun is sim-   ilar to that of noun!verb , introduced in § 5 . We   use the following composition of perturbation op-   erations : append the preposition to the patient key-   word ( e.g. ,“ham or sausages with ” ) , change patient   keyword speciﬁcity from complete ) partial ( to   generate a new PP attaching to the patient ) , and   delete the argument with original verb attachment   ( e.g. ,ADVERBIAL “ with your breakfast ” ) .   We note that Tailor achieves higher validity   changing attachment from noun!verb ( 82 % ) than   verb!noun ( 48 % ) . This result is expected , as all   semantic role labeling arguments attach to verb   predicates ; thus , introducing controls for an SRL   argument ( e.g. , LOCATIVE with keyword content   “ at ” ) to generate a preopositional phrase with verb   attachment ( “ at every turn ” ) reﬂects the training   objective of the generator . On the other hand ,   ourverb!noun strategy involves appending the   preposition to the keyword control for an argument ,   and none of our controls explicitly reﬂect the tar-   get attachment of a prepositional phrase within   an argument ( e.g. , keyword controls do not spec-   ify whether “ with ” should attach to “ sausages ” vs   “ ham ” ) . Furthermore , preposition keywords within   an SRL argument do not deterministically lead to   noun attachments in our training data – Sometimes   a preposition within an argument may reﬂect verb   attachment ( e.g. , in the case of “ Do [ AGENT : you ]   [ VERB : prefer ] [ PATIENT : eating with a fork or eat-   ing with a knife ] ? ” ; here , “ eating with a fork or   eating with a knife ” is the patient of “ prefer ” but   prepositional phrase “ with a fork ” attaches to verb   “ eating . ” ) Because the training objective of our   generator does not provide deterministic signal for   noun attachment outputs , we do not expect our   verb!noun strategy to always result in generations   with noun attachment . Our verb!noun strategy is   instead intended to facilitate the collection of text   with noun attachment . Future work can investigate   incorporating auxiliary signals about target conﬁg-   urations of keyword contents in outputs ( e.g. , that   a preposition should depend on a particular word   in the span ) .   D.2 Predictor Performance Evaluation   The performances of downstream predictors on   original task evaluation data and contrast sets , both   Tailor -generated and human - expert - generated , are   shown in Table 9.For SQuAD , we evaluate a   ﬁne - tuned RoBERT a , the most downloaded model   hosted on Huggingface , and use the QA impli-   cation challenge set ( Rajpurkar et al . , 2016 ) as the   human contrast set . Since we could not ﬁnd read-   ily available predictors for BoolQ and MATRES ,   we formulate these tasks as a text - to - text task and   ﬁne - tune T5 - base for 10 epochs ; we evaluate the3210   checkpoint with the lowest validation loss .   The drops in predictors ’ accuracies on the Tai-   lor - generated contrast sets ( compared to original   test accuracies ) show that they can be used to re-   veal model errors not reﬂected in original valida-   tion data . However , this result should be interpreted   with caution , as it is not directly reﬂective of dataset   quality . For instance , if the contrast data tests one   error type or is adversarially constructed to include   instances where predictors fail , then lower accuracy   does not necessarily mean exposing more model   errors . Thus , we treat these performance metrics as   secondary to other direct metrics of dataset quality ,   discussed in § 5 , and run this analysis on a small   number of contrast set instances as a sanity check .   That said , the fact that predictors perform poorly   onTailor -generated contrast sets even without in-   cluding an adversarial component in our contrast   set creation suggests that Tailor can be useful for   creating evaluation data to ﬁnd model errors .   E Data Augmentation Details ( § 6 )   Augmented data . To create our augmented data ,   we ﬁlter generations by perplexity scores from   GPT-2 such that we retain 75 % of generations . Ex-   amples of augmented inputs are shown in Table 10 .   Classiﬁers . We train all SNLI classiﬁers , which   build on RoBERT a - base ( Liu et al . , 2019 ) , using   AllenNLP ( Gardner et al . , 2018 ) . We train for 10   epochs using the Adam optimizer with a learning   rate of 2e-05 and batch size 32 ; we use early stop-   ping with a patience of 3.F Tailor ’s ﬁne - grained and compositional   perturbations on S tylePTB   Here , we show how Tailor can be applied to ﬁne-   grained style transfer . We evaluate T ailor without   any ﬁnetuningon the StylePTB benchmark ( Lyu   et al . , 2021 ) , which builds on the Penn Treebank   and assesses ﬁne - grained stylistic changes , both on   single transfers ( e.g. , To Future Tense ) and compo-   sitional ones that concurrently edit multiple stylis-   tic dimensions ( e.g. , To Future Tense + Active To   Passive ) .   Transfers Evaluated . We evaluate on the trans-   fers in StylePTB for which Lyu et al . ( 2021 ) report   results , as their baselines require training separate   models for each transfer . Within this subset of   transfers , we exclude PP Back to Front andPas-   sive to Active from evaluation , as they contain < 5   test inputs . We also exclude the transfers Substate-   ment Removal , Information Addition , Adjective Em-   phasis , and Verb /Action Emphasis , for which our   semantic - role - derived inputs are not well - suited .   For example , Substatement Removal involves re-   moving substatements that represent “ referring ”   and “ situations , ” both of which are technical philo-   sophical concepts that can not be straightforwardly   detected through semantic roles . As another ex-   ample , Information Addition requires adding un-   ordered keyword contents to a sentence ( eg the   work force provides the third arm of the alliance ;   add keywords : force black!the work force pro-3211   vides the third arm of the black alliance force .   While the Tailor generator was only trained with   ordered arguments , one could extend the keyword   contents to also include unordered target tokens .   Perturbation strategies . For transfers modify-   ing only verb tense ( e.g. , To Future Tense ) , we   mask the verb , modal arguments , and negation ar-   guments , as these are relevant to verb conjugations ,   and make relevant perturbations on the secondary   verb control specifying tense . For transfers mod-   ifying verb voice , we mask the verb , agent , and   patient . For transfers requiring removal of certain   parts of speech ( POS ) — i.e. , ADJ or ADV Removal ,   PP Removal , and all compositional Tense + PP   Removal sub - transfers — we ﬁrst use spacy to de-   tect such POS , next mask all arguments containing   them , and ﬁnally perturb the keyword contents to   remove the POS for these arguments . For PP Front   to Back , we mask the argument at the beginning of   the original text and implement the change using   CHANGE_IDX .   We use cased keywords ( A.2 ) to encourage gen-   erations with similarly ordered arguments as the   original sentence , except for the PP Front to Back   transfer , which calls for di  erently ordered argu-   ments . For transfers modifying verb form only , we   set the number of extra blanks to be 2 to allow for   generation of helper verbs ; for other transfers , weallow for 0 extra blanks to preserve the original   order of generated spans . We decode perturbed   sentences greedly using beam search ( with beam   width 10 ) and preventing repeated bigrams .   For each transfer , we create perturbations for   each predicate in the original input , and report   mean BLEU scores . Because this process results   in multiple perturbations ( one per verb ) , we choose   the one with the lowest perplexity from GPT-2 to   represent the transfer . Unsuccessful transfers , ei-   ther due to a failure of perturbation strategy ( e.g. ,   no verbs are found by our SRL predictor ) or due   to a degenerate output ( see § C ) , are given a BLEU   score of 0.0 .   Baselines . We work with baselines reported by   Lyu et al . ( 2021 ): GPT-2 andRetrieve Editare the   best - performing single - transfer models evaluated   but require separate models to be trained for each   transfer . CS - GPT * are models trained on compo-   sitional subsets of data ( e.g. , Tense + Voice , detailed   in Table 11 caption ) . CS - S ys - Genare ablations of   CS - GPT * trained only on corresponding individual   changes but evaluated on compositional transfers .   Result . On compositional transfers , we ﬁnd that   Tailor outperforms the baseline system trained3212without compositional ﬁne - tuning , CS - S ys - Gen ,   on 8/9 compositions , and even outperforms CS-   GPT * ( models with compositional ﬁnetuning ) on 5   cases . It also achieves compatible or better results   than GPT-2 andRetrieve Editon single transfers .   Low Tailor performance on some transfers ( e.g. ,   ToFuture + ActiveToPassive ) appears to be driven by   unsuccessful transfers , rather than generations that   do not follow controls , as indicated by the higher   performances on the subset where unsuccessful   transfers are removed ( Filtered Test ) . Importantly ,   Tailor achieves these gains with a single model   andwithout any transfer - speciﬁc ﬁnetuning .3213