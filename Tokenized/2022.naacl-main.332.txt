  Siddharth Verma Justin Fu Mengjiao Yang Sergey Levine   UC Berkeley   vsiddharth@berkeley.edu   { justinjfu,sherryy,svlevine}@eecs.berkeley.edu   Abstract   Conventionally , generation of natural language   for dialogue agents may be viewed as a statis-   tical learning problem : determine the patterns   in human - provided data and generate appro-   priate responses with similar statistical proper-   ties . However , dialogue can also be regarded   as a goal directed process , where speakers at-   tempt to accomplish a specific task . Reinforce-   ment learning ( RL ) algorithms are designed   specifically for solving such goal - directed prob-   lems , but the most direct way to apply RL –   through trial - and - error learning in human con-   versations , – is costly . In this paper , we study   how offline reinforcement learning can instead   be used to train dialogue agents entirely using   static datasets collected from human speakers .   Our experiments show that recently developed   offline RL methods can be combined with lan-   guage models to yield realistic dialogue agents   that better accomplish task goals .   1 Introduction   Constructing fluent and intelligent dialogue agents   could pave the way for intuitive interfaces and au-   tomation of human - interactive tasks . However , this   requires dialogue agents that both generate flu-   ent , natural responses and effectively pursue the   goals of the given dialogue task . A predominant   approach to training dialogue agents is through su-   pervised learning , where an agent is tasked with   imitating language provided by humans . While   this can provide for fluent responses , it becomes   difficult to ensure that such agents systematically   pursue the goals of the dialogue interaction . If we   instead view dialogue as a control problem , frame-   works such as reinforcement learning ( RL ) could   allow agents to automatically optimize dialogue   with respect to a task goal through a trial - and - error   process and improve over human behavior .   However , implementing an RL system in prac-   tice , where an agent learns online from interact-   ing with real humans , can be prohibitively expen - sive and time - consuming . This is in stark contrast   to supervised learning approaches , where we can   cheaply construct datasets for training imitation   agents by simply logging conversations . There-   fore , existing RL approaches for dialogue often   rely on interacting with a learned model of a hu-   man ( Li et al . , 2016b ; He et al . , 2018 ) , from which   experience can be generated inexpensively . How-   ever , naïve training in this manner can result in the   dialogue agent exploiting the model , which can de-   generate into non - intelligible language . To mitigate   this , algorithms must typically enforce strong pri-   ors to keep generated language similar to those seen   in the dataset ( Li et al . , 2016b ; Jaques et al . , 2019 ) ,   or adopt dialogue management and template - based   approaches which directly re - use language seen in   the dataset ( He et al . , 2018 ) .   Issues such as model exploitation and distribu-   tion shift when training on static datasets are a   primary concern of offline RL ( Levine et al . , 2020 ) ,   and can provide a formalized approach to tack-   ling these problems . While offline RL is moti-   vated by scaling RL to large datasets , annotated   datasets for dialogue are still small compared to   the large amount of raw text datasets available to-   day . Therefore , we propose an offline , model - free   approach to dialogue generation that leverages lan-   guage models . Because the size of unlabeled lan-   guage datasets dwarfs that of curated datasets for   dialogue , using a pre - trained language model as   a central component of our method allows it to   learn aspects of language fluency from unlabeled   datasets , while learning higher - level strategies for   goal - directed dialogue via RL on a smaller anno-   tated datasets . This combined approach enables us   to utilize the large amounts of existing language   data that standard RL methods can not .   The main contribution of this work is CHAI   ( CHatbot AI ) , an algorithm for learning task-   oriented dialogue that utilizes a language model   in conjunction with offline RL . We show that this4471leads the policy to generate goal - oriented dialogue   that is both realistic and functional , and does not re-   quire training against a simulated model of human   language . We evaluate our method on a negotiation   task , which requires the model to both reason about   strategic aspects of conversation along with gen-   erating fluent language . We show that CHAI con-   sistently bargains for better prices and with higher   rates of successful negotiation than prior RL ap-   proaches to goal - oriented dialogue .   2 Related Work   Recent developments in deep learning have led   to end - to - end approaches to dialogue using super-   vised learning , such as sequence - to - sequence mod-   els ( Dušek and Jurcicek , 2016 ; Eric and Manning ,   2017 ) , hierarchical models ( Serban et al . , 2017 ) ,   attention ( Mei et al . , 2017 ; Chen et al . , 2019 ) ,   and Transformer - based models ( Wu et al . , 2021 ;   Hosseini - Asl et al . , 2020 ; Peng et al . , 2020 ; Adiwar-   dana et al . , 2020 ) . However , supervised learning   only allows an agent to imitate behaviors , requires   optimal data , and does not allow agents to exceed   human performance . Supervised learning for dia-   logue generation also has well - known issues such   as outputting commonplace responses ( e.g. , I do   not know ) regardless of the inputs ( Li et al . , 2016a ) .   Therefore , additional training of the dialogue agent   is required for performing goal - oriented tasks .   Task - oriented dialogue has been formulated as a   sequential decision making problem in a Markov   Decision Process ( MDP ) since the 1990s ( Smith   and Hipp , 1994 ; Singh et al . , 1999 ; Williams and   Young , 2007 ; Young et al . , 2013 ; Paek , 2006 ; Hen-   derson et al . , 2008 ; Gao et al . , 2018 ; Pieraccini   et al . , 2009 ; Young et al . , 2013 ; Su et al . , 2015 ;   Chen et al . , 2020 ) . Dialogue is converted into ab-   stract states and actions from which an agent is   trained using reinforcement learning ( RL ) ( Eck-   ert et al . , 1997 ; Levin et al . , 2000 ; Chung , 2004 ;   Georgila et al . , 2006 ; Schatzmann et al . , 2007 ;   Heeman , 2009 ; Georgila and Traum , 2011 ; Su   et al . , 2016 ; Fatemi et al . , 2016 ; Asri et al . , 2016 ;   Zhao et al . , 2019 ; Zhang et al . , 2020 ; Wang et al . ,   2020 ) . These methods differ in how the abstract   states / actions are designed and whether the simu-   lated environment for training the policy is hand   created , learned as a fixed model , or is an agent   itself . For instance , Eckert et al . ( 1997 ) ; Levin   et al . ( 2000 ) learn a fixed transition model from hu-   man conversations and Georgila and Traum ( 2011)learn negotiation agents where each agent is the   user simulator for the other agent . These meth-   ods also differ in how the decision making policy   is trained , e.g. , online ( Gaši ´ c et al . , 2011 ) or off-   policy / offline ( Yu et al . , 2016 ; Pietquin et al . , 2011 )   using actor - critic ( Su et al . , 2017 ) , policy gradi-   ent ( He et al . , 2018 ) , or fitted Q - iteration ( Pietquin   et al . , 2011 ) . Regardless of the RL method used ,   since policies are trained on abstract states and ac-   tions , these methods lack the ability to generate   natural language ( i.e. , response is created via tem-   plates depending on an abstract action ) .   To overcome these limitations , recent work has   trained policies directly on text , using a recurrent   neural network to output language tokens , and us-   ing self - play for policy training while interacting   with another learned agent ( Li et al . , 2016b ; Lewis   et al . , 2017 ; Liu et al . , 2018 ) . To further improve   the generated language quality , hierarchical meth-   ods decouple the strategic high - level dialogue deci-   sions from generation ( Yarats and Lewis , 2018 ; He   et al . , 2018 ; Saleh et al . , 2020 ) . These model - based   approaches require accurate estimation of the en-   vironment / human ( e.g. , the trained self - play agent   needs to mimic complex human behavior ) , which   is beyond current capability of model - based rein-   forcement learning algorithms . Similar to our pro-   posal , Jaques et al . ( 2019 ) use offline RL based on   KL - control for text generation in open - domain dia-   logue . Our work differs in that our model is able to   utilize large amounts of unsupervised data through   the use of pre - trained language models , and that our   work focuses on task - oriented ( as opposed to open-   domain ) dialogue tasks . Goal - oriented tasks have   clearly defined objectives that can be quantified ,   allowing us to provide an objective comparison   between our method and prior approaches .   3 Preliminaries   In this section , we describe our evaluation task and   cover the necessary background and notation .   3.1 Negotiation via Task - Oriented Dialogue   We evaluate our approach on the CraigslistBargain   task ( He et al . , 2018 ) . CraigslistBargain consists of   6682 advertisements scraped from Craigslist , along   with dialogues for each advertisement collected via   Amazon Mechanical Turk where two users play   the role of buyer / seller . An example advertisement   from this dataset is shown in Fig . 1 , along with a   sample conversation between a human and CHAI.4472   During each round of interaction , the buyer and   seller can execute four possible response types . A   message allows one player to send an utterance to   the other . An offer allows one player to propose   a price at which to conduct the transaction . Once an   offer is made , the other player can either accept   orreject the offer , which ends the episode . A   reward is then computed based on the transaction   price . Our bot receives a reward equal to the nor-   malized price the item is sold for at the end of an   episode ( normalized by the list price ) , scaled by a   constant factor of 10 . Additionally , we penalize the   bot by a constant of -20 for episodes resulting in a   reject to incentivize the agent to make deals .   We selected this task because it provides a clear   objective , allowing us to illustrate our approach   with quantifiable metrics . Of course , practical ap-   plications of CHAI to goal - directed dialogue may   tackle other problems , including non - adversarial   problms such as helping a user to answer a question   or fulfill a request . However , our choice of tasks   was constrained by the limited availability of pub-   lic datasets for dialogue tasks that are goal - directed   and have objective task goals .   3.2 Reinforcement Learning Setup   We formulate the task - oriented dialogue problem   as an RL problem , where the agent serves the roleofseller in the CraigslistBargain problem , and the   environment serves the role as the buyer . We con-   sider a Markov decision process defined as a tuple   ( S , A , T , r , γ ) . The state and action spaces , Sand   A , consist of three main components : the action   typetype ( one of the four described in Sec . 3.1 ) ,   an utterance u(used only for message responses ) ,   and a normalized price price , expressed as a frac-   tion of the list price . Additionally , the context com-   ponent s contains the advertisement listing   description . The price component is used in two   ways . First , because we do not wish to represent   prices in the dialogue as discrete tokens , we instead   replace prices in the utterance with a placeholder   token understood to be substituted with the price   component . Second , the price component is used in   offer response type to communicate the desired   transaction price . In all other cases , it is ignored .   An example of the state space is shown in Fig . 2 .   We write individual states as s =   { s , s , s , s } and actions as   a={a , a , a } , where s , adenote   the utterances , or sequences of words generated   by the environment and agent . { s , a }   denote the action types , { s , a}denote   the prices , and s denotes the listing context .   The transition distribution T(s|s , a)governs the   distribution over responses generated by the buyer   agent ( environment ) , and the reward Rdefines the   task objective . The goal of RL is to find a policy   π(a|s)that maximizes the expected returns :   E / bracketleftigg / summationdisplayγr(s , a)/bracketrightigg   ,   where r : S × A ∝ ⇕ ⊣√∫⊔≀→ Ris the reward function , and   γ∈(0,1]is the discount factor . In online RL , the   agent is interacts with the environment to maxi-   mize this objective . In contrast , offline RL uses4473a dataset Dof pre - collected interaction . This is   a natural approach in many settings , such as dia-   logue , where online RL could require interacting   with real humans for impractically long periods of   time , whereas offline RL can utilize pre - recorded   dialogues between humans .   4 Offline Reinforcement Learning with   Language Models   Offline RL potentially allows reinforcement learn-   ing methods to leverage large datasets for policy   learning . However , it still requires datasets to be   annotated with rewards , and for conversations to   come from the task at hand . Because of this , anno-   tated dialogue datasets , such as the CraigslistBar-   gain dataset presented in Section 3.1 , are many or-   ders of magnitude smaller than unlabeled datasets   collected for unsupervised and language model-   ing tasks . In order to utilize these large unlabeled   datasets , we propose an algorithm that combines   offline RL with fine - tuned language models .   Our approach begins with training a language   model , such as GPT-2 ( Radford et al . , 2018 ) , and   fine - tuning it on our task - specific dialogue corpus   ( Sec . 3.1 ) . We use LM(u|s)to denote a distri-   bution over utterances uproduced by the language   model given the dialogue history , denoted s. We   then train a critic or Q - function as described in   Sec . 4.1 , which is responsible for scoring good and   bad responses and is used to select responses from   a pool of candidates generated from the language   model . Our approach can be viewed as using a   Q - function to steer a language model ( which has   no concept of a task ) towards producing language   that accomplishes some task - specific goal .   4.1 Q - Learning with Language Models   In this section we describe how to train a Q-   function that can score candidate responses based   on their potential to maximize returns . We imple-   ment and evaluate three different training proce-   dures , each utilizing different offline RL methods .   In the overall Q - learning framework , we sample a   batch of transitions ( consisting of states , actions ,   rewards , and successor states ) from our dataset and   perform updates based on minimizing a modified   Bellman loss :   J(θ ) = ( Q(s , a)−Q(s , a ) ) , ( 1)where target value Q(s , a)is typically com-   puted via the Bellman operator defined as   Q(s , a ) = r(s , a ) + γE / bracketleftig   max¯Q(s , a)/bracketrightig   .   ( 2 )   However , using this update directly can lead to   problems if we only have access to offline datasets .   A widely studied issue in offline RL is the chal-   lenge of handling out - of - distribution actions : when   the maximization over the action in the target value   is not constrained in any way , it is easy to obtain   actions for which the Q - value predictions are erro-   neously high ( Levine et al . , 2020 ) . In dialogue , this   issue is greatly exacerbated , since the Q - function is   only trained on responses in the dataset , and there-   fore is unlikely to make accurate predictions for   arbitrary strings . The following modifications to   Eqn.1 and Eqn.2 address this issue .   Proposal sampling ( CHAI - prop ) In the proposal   sampling approach , the target value Q(s , a)is   computed via a modified Bellman operator that uti-   lizes a proposal distribution based on the language   model , µ(a|s ) , to generate Nresponse propos-   als , and then uses the target Q - function , ¯Q , to score   those responses and selects the highest one :   Q(s , a )   = r(s , a ) + γEE / bracketleftbigg   max¯Q(s , a)/bracketrightbigg   .   This sampling scheme serves a dual purpose : it   both constrains the responses to be naturalistic ,   and it also prevents out - of - distribution inputs to   the Q - function in the target value calculation .   This approach resembles a number of prior of-   fline RL methods that also employ proposal dis-   tributions ( Kalashnikov et al . , 2018 ; Kumar et al . ,   2019 ; Fujimoto et al . , 2019 ; Wu et al . , 2019 ) .   Similarly to several prior works , we use sam-   ples from a proposal distribution for the target   value , without an explicit actor ( Kalashnikov et al . ,   2018 ; Ghasemipour et al . , 2020 ) . Unlike these ap-   proaches , our method leverages a pretrained and   finetuned language model LM , which additionally   makes use of extensive unsupervised prior datasets   during the pretraining stage and enables our method   to handle the complex and combinatorial action   space of dialogue generation . In addition , follow-   ing prior work , we use a separate target network ¯Q   whose weights are updated to track those of Q using   a soft update rule as done in prior methods ( Lilli-   crap et al . , 2016 ; Haarnoja et al . , 2018).4474   The proposal distribution µ(a|s)represents a   distribution over actions a={a , a , a } .   We use the language model in order to sample   utterances a∼LM(·|s ) . To make training   more computationally efficient , we pre - generate a   batch of 5 utterances per transition in the dataset   using the language model , and resample these dur-   ing training as an approximation of directly sam-   pling from the language model . For the prices   a , we uniformly sample a value between 70 %   to100 % of the previously offered price , which   roughly matches the distribution of the seller ’s of-   fers in the dataset . Finally , we infer the message   type based on the utterance sampled using a simple   heuristic , as the CraigslistBargain task requires us   to specify a type for each response . During lan-   guage model fine - tuning , we replaced each offer ,   accept , orreject action with the utterances   “ offer ” , “ accept ” , and “ reject ” , respectively . We   then check if the language model generated any of   these tokens and return the corresponding action   type , and label the action as a message otherwise .   This simplifies our method and allows us to use the   language model to generate the action types as well   as the utterances .   Conservative Q - learning ( CHAI - CQL ) Conser-   vative Q - learning ( CQL ) ( Kumar et al . , 2020 ) pro-   poses a complimentary approach to reducing the   harmful effect of out - of - distribution Q - values by   explicitly penalizing the Q - value of actions not   seen in the dataset . We adapt CQL as an additional   regularizer on the Q - value in addition to the pro-   posal sampling scheme . Specifically , we use the   CQL(H)variant , which add an additional regular-   izerfto the Q - learning objective :   J(θ ) = ( Q(s , a)−Q(s , a))+αf(θ),where the regularizer is defined as :   f(θ ) =   E / bracketleftigg   log / summationdisplayexp(Q(s , a))−E[Q(s , a)]/bracketrightigg   Our adaptation of CQL differs from Kumar et al .   ( 2020 ) in that they propose an actor - critic method   which trains an explicit actor . Rather , the remainder   of our method is identical to the proposal sampling   variant , specifically in regards to the computation   ofQ and language model sampling .   Behavior - regularized Q - learning ( CHAI - BRAC )   Behavior - regularized actor - critic ( BRAC ) ( Wu   et al . , 2019 ) proposes an alternative method   for regularizing the Q - function such that out - of-   distribution Q - values are penalized . Adapting this   method to the setting of dialogue with language   models , we use this approach to regularize the   price proposal mechanism . Rather than uniformly   sampling prices as described for proposal sam-   pling , we train an additional price proposal net-   work π(a|s)that outputs a Gaussian dis-   tribution over prices given the conversation states .   Using the notation a∼π , µto denote sampling   prices from the proposal network , utterances from   the language model , and action types uniformly ,   the proposal network is trained according to the   objective   maxE / bracketleftbig   E[Q(s , a ) ]   −D(π(·|s ) , π(·|s))/bracketrightbig   .   The prior proposal network , π , is estimated as a   univariate conditional Gaussian of the current offer   given the previous offer , where the mean and stan-   dard deviations are linear functions of the previous   offer . The target value is then computed as :   Q(s , a ) = r(s , a ) + γE / bracketleftbig¯Q(s , a)/bracketrightbig   .4475This adaption of the behavior regularized objective   controls out - of - distribution queries on the Q - value   by regularizing the price towards those seen within   the dataset . This prevents the target Q - value from   being queried in low - data regimes which can cause   inaccuracies during training .   4.2 Dialogue Generation   Once the Q - function has been trained , dialogue   generation from our model is a three - phase process .   The first phase is sampling : given a response from   the buyer , we query the language model to sam-   ple 5 candidate utterances , and sample 5 candidate   prices . The next step is scoring : we then take the   cross - product of these sets , and score each potential   action with the Q - function . Finally , in the selection   phase , several methods are considered in order to   select an action . A straightforward method is to   return the action that had the highest Q - value . How-   ever , we found that this approach resulted in behav-   ior with low diversity . Instead , we opted to follow   the approach in soft Q - learning ( Haarnoja et al . ,   2018 ) and sample actions from a softmax distri-   bution over the Q - values , p(a|s)∝exp{Q(s , a ) } ,   which increases diversity in the responses as sub-   optimal actions are occasionally sampled . This   decoding process is depicted in Fig . 3 .   4.3 Architecture Details   For our language model , we use an off - the - shelf   implementation of GPT2 - medium ( Radford et al . ,   2018 ) . This language model is finetuned on a tran-   script of each scenario in the dataset containing   the context ( title , description ) and spoken dialogue .   The prices in the dialogue are masked out with a   special price token allowing us use GPT to gener-   ate templates which we can substitute prices into .   The input to the language model is a concatenation   of the scenario context and the dialogue history .   The Q - function is parameterized as a feedfor-   ward network that maps states and actions into a   single scalar representing the Q - value . To process   the utterances into the state and action , we sep-   arately compute state and action embeddings by   taking the average of the masked GPT2 attention   embeddings of the entire dialogue history up to the   current utterance . These embeddings are then con-   catenated with the prices ( represented as a fraction   of the list price ) and message types ( represented   as a one - hot vector ) to produce a single vector   that is given to the critic as input . The critic is   parametrized using a 2 - layer feedforward networkwith hidden sizes of 256 and ReLU nonlinearities .   Additional details about our model architectures ,   language model sampling method , and how inputs   to the Q - function are structured , can be found in   Appendix A.1 .   5 Experiments   Our experimental evaluation aims to compare our   proposed goal - directed offline RL dialogue method   to both prior dialogue management approaches and   language modeling baselines . We conduct two   studies : an objective evaluation against other di-   alogue agents to measure each method ’s perfor-   mance in negotiation , and a subjective human study   to measure the overall end - to - end performance of   the system in a similar manner to prior work ( He   et al . , 2018 ; Jaques et al . , 2019 ) . Qualitative re-   sults showing actual dialogue generated from our   method can be found in Appendix A.4 . We con-   sider 4 baseline approaches . The first is the current   state - of - the - art approach for the CraigslistBargain   task proposed by He et al . ( 2018 ) ( referred to as   the retrieval - based baseline ) . This is a hierarchi-   cal approach to dialogue generation that separately   handles language generation and dialogue manage-   ment . This method parses utterances into coarse   “ dialogue acts , ” which represent high - level catego-   rizations of the utterance such as greetings , offers ,   or counter - offers . An RL agent is then trained   against a learned model of the environment to se-   lect “ dialogue acts ” , and a retrieval - based generator   is then used to convert “ dialogue acts ” back into   text . In contrast , our method directly generates   text , and does not require any manually designed   categorizations of natural dialogue into dialogue   acts . Since our method utilizes a modern language   model , we also include a pure language modeling   baseline , which consists of the same GPT-2 lan-   guage model Radford et al . ( 2018 ) finetuned on the   CraigslistBargain dataset using the same method as   done in CHAI . This baseline allows us to determine   whether any improvement from our method is due   to the language model , or to the use of offline RL .   Finally , we also evaluate the end - to - end approaches   described by Lewis et al . ( 2017 ) , which include a   dialogue agent trained via supervised learning , and   an RL agent optimized for the task objective .   5.1 Simulated Evaluation   To evaluate the effectiveness of our offline RL goal-   directed dialogue system , we first conducted a sys-4476   tematic study including the 3 variations of CHAI   outlined in Sec . 4 : the proposal sampling method   ( CHAI - prop ) , CQL method ( CHAI - CQL ) , and be-   havior regularized method ( CHAI - BRAC ) . In order   to ensure that the results are not overfit to a single   strategy , we run each method against a suite of   5 evaluation buyer agents , based on the retrieval   agents presented by He et al . ( 2018 ) . We choose   these agents because they have been evaluated by   humans as being human - like and have the strongest   performance on the CraigslistBargain benchmark   task . Specifically , we use the rule - based and RL   agents proposed by He et al . ( 2018 ) ( trained using   “ utility ” , “ fairness ” , and conversation “ length ” as   rewards ) . To introduce additional variety in negoti-   ation styles , we additionally modify the rule - based   agent to offer 25 % of the difference between offers   rather than splitting the difference between offers ,   which we refer to as the “ Stingy ” rule - based agent .   We record the percentage of negotiations that result   in an accept and the average normalized revenue   generated per negotiation , which totals the averagesale price ( rejections have zero revenue ) normal-   ized by the listing price of the advertisement . Our   results are presented in Table 1 .   Overall , we find that among the variations of   CHAI , the conservative Q - learning variant per-   forms the best by a small margin , but results are   very comparable between all 3 variations . This   suggests that the particular choice of offline regu-   larizer is far less important than the CHAI frame-   work of utilizing a pre - trained language model with   Q - function scoring . On average , CHAI - CQL per-   forms significantly higher on acceptance rate and   similarly on revenue to the next best agent , the   retrieval agent ( He et al . , 2018 ) using conversa-   tion length as reward . Computing statistical sig-   nificance between these two methods , we find that   p < 1.96∗10using a chi - squared test for ac-   ceptance rate , indicating that the difference in ac-   ceptance rates is statistically significant . We find   thatp < 0.946using a t - test for revenue , indi-   cating that the difference in revenue is not signifi-   ca nt . We also note that the performance of CHAI4477   has significantly less variation across evaluations   against different buyer agents than the retrieval-   based agents . For example , the retrieval agent with   utility reward scores near - zero on 3 evaluations but   scores near - perfectly on the other two . This sug-   gests that the CHAI framework produces dialogue   agents that are more consistent and less suscepti-   ble to exploitation . We also note that implement-   ing the retrieval method ( He et al . , 2018 ) requires   hand - designing high - level dialogue actions , topic   categories , and rules for parsing or labeling these   components . These designs are specifically tailored   to the CraigslistBargain task , whereas such hand-   engineering for CHAI does not exist outside of   the interface requirements to the task itself . Thus ,   CHAI has significantly weaker assumptions , gen-   erates dialogue end - to - end via RL , and yet is able   to narrowly outperform prior methods . Among   prior methods with similar assumptions to CHAI   ( the language modeling baseline and ( Lewis et al . ,   2017 ) ) , CHAI outperforms by a wide margin on   both acceptance rates and revenue .   We ran an additional ablation study on the choice   of reward in Appendix A.3 . We find that this has a   significant effect on performance , and we based our   reward design on balancing between maximizing   acceptance rate ( through a rejection penalty ) and   revenue ( through the utility function).5.2 Human User Study   To evaluate the effectiveness and naturalness of   our offline RL goal - directed dialogue system , we   conducted a user study with 16 individuals , who   were each asked to carry out 2 negotiations with   each of three agents . The users were then asked   to rate the conversation on fluency , coherency , on-   topicness , and human - likeness on a 5 - point Lik-   ert scale . Fluency specifically refers to the fre-   quency of grammatical and word - choice errors . Co-   herency measures whether the agent ’s responses   are coherent . On - topic measures how well the   agent was aligned with performing the task at hand .   Finally , human - likeness measures how similar the   agent ’s responses were to a human . Because of the   cost of human evaluations , we were limited in our   ability to evaluate as many baselines . Therefore ,   we chose methods that were the most directly com-   parable - the simplest variation of CHAI ( CHAI-   prop ) optimized for utility , evaluated against the   utility - optimized agent from He et al . ( 2018 ) , and a   language model baseline that shares the same fine-   tuning procedure as CHAI . Additional details of   the user study , including the questions posed to the   users and statistical significance tests , are included   in Appendix A.3.1 .   Results are shown in Table 2 . We ran a one-   way repeated measures ANOV A test , and found   that the type of agent used leads to statistically sig-4478nificant rating differences for all metrics ( with at   leastp < 0.01 ) . CHAI outperforms both baselines   on almost all metrics , except for fluency , where   both CHAI and the language model perform simi-   larly . The fact that fluency is similar between the   two models makes sense , since both methods use a   GPT-2 model to generate utterances . However , the   langauge modeling baseline lacks an understand-   ing of the task goal , and therefore makes unrea-   sonable offers or responds in illogical ways ( see   Appendix A.4 for examples ) . It therefore scores   lower on other metrics as compared to CHAI . This   result suggests that the ability of language models   to execute goal - directed dialogues is limited by a   lack of awareness of the task objectives , and that   offline RL potentially addresses this issue , produc-   ing dialogue that is perceived as more coherent ,   task - oriented , and human - like .   In Fig . 4 we show a comparative example be-   tween CHAI , the retrieval - based agent ( He et al . ,   2018 ) , and the language modeling baseline on the   same scenario with human responses . CHAI and   the language modeling baseline tend to produce   more specific responses to the prompt due to the   use of a language model , rather than generating text   via the usage of templates . For example , when pre-   sented with a questioned about utilities and viewing   time , both methods are able to answer the ques-   tion , whereas the retrieval - based agent gives a non-   sequitur answer . However , the language modeling   baseline struggles in understanding prices , and of-   fers$2200 when the buyer requested $ 3000 . CHAI   is able to demonstrate understanding both language   and the flow of the negotiation by offering reason-   able counter - offers to the user , such as responding   to a low offer with “ I ca n’t go that low ” and offer-   ing a higher counter - offer that the Buyer accepts .   Additional examples from our human evaluation   can be found in Appendix A.4 .   6 Discussion and Future Work   We presented a system for goal - directed dialogue   based on combining offline RL with finetuned lan-   guage models . CHAI learns with RL , but does   so from offline datasets of human dialogue . The   language model allows CHAI to benefit from large-   scale unsupervised pre - training , and the offline RL   component enables CHAI to select responses that   are more likely to lead to a successful task outcome .   Quantitatively , CHAI achieves higher acceptance   rate at higher revenue than prior dialogue manage - ment systems designed for this task .   Goal - oriented dialogue agents have many poten-   tially useful applications such as building personal   assistants , improving accessibility to technology   for the disabled or the elderly , and simply saving   time by automating menial tasks . Of course , as   with any natural language generation technology ,   this kind of method can be used both beneficially   and maliciously , for example by users who aim to   create intentionally deceptive and realistic agents .   While CHAI provides a proof - of - concept that   offline RL can successfully learn complex human-   interactive tasks such as dialogue , it also has lim-   itations . The goal of RL is to maximize reward ,   which can lead to unintended responses – for ex-   ample , without additional objective terms , there is   no reason for CHAI to be truthful . Similar issues   affect language models more broadly , though we   anticipate that it would be easier to address such   issues in RL by employing better reward design .   Although reward design can itself be a difficult   problem , it does provide a more direct lever for   influencing the agent ’s behavior than what is avail-   able in standard language models , which must be   directed either through the choice of training data   or other indirect mechanisms . In CHAI , we only   investigated a single task due to architectural con-   straints , and the exact same architecture presented   in this paper ( e.g. a price prediction head ) may not   be directly transferable to other domains . However ,   using a value - based selection mechanism is more   generally applicable to any goal - oriented task .   An exciting direction for future work is to ex-   tend offline RL to address a wider range of human-   interactive tasks , particularly tasks with longer-   range dependencies and delayed rewards , where   complex task goals can lead to the emergence of   dialogue that is ultimately more useful to human   users .   7 Acknowledgements   We thank Natasha Jacques , Daniel Fried , Dilek   Hakkani - tur , Yang Liu , Alexandros Papangelis , and   Wei Wei for insightful discussions . We would also   like to thank all of the anonymous participants in   the user study . This research was supported by the   Office of Naval Research.4479References448044814482A Appendix   A.1 Architecture Details   For our language model , we use an off - the - shelf implementation of GPT2 - medium ( Radford et al . , 2018 ) .   This language model is finetuned on a transcript of each scenario in the dataset containing the context   ( title , description ) and spoken dialogue . The prices in the dialogue are masked out with a special price   token allowing us use GPT to generate templates which we can substitute prices into . To produce language   samples , we concatenate the context of the scenario along with the dialogue history and feed it to the   language model . The language model then generates the next utterance . This process is repeated in order   to get multiple samples .   The Q - function is a feedforward network that maps the states and actions into a single scalar representing   the Q - value . To process the utterances into the state and action , we separately compute state and action   embeddings by taking the average of the masked GPT2 attention embeddings of the entire dialogue history   up to the current utterance . State and action embeddings are then concatenated with the prices ( represented   as a percentage of the list price ) and message types ( represented as a one - hot vector ) to produce a single   vector that is given to the critic as input . The critic is parametrized as a 2 - layer feedforward network with   hidden sizes of 256 and ReLU nonlinearities .   The equations below describe the inputs of the Q - function . “ STRCAT ” is a custom string formatting   function that concatenates the context and utterances in the dialog and prefixes each utterance with the   string “ Buyer : ” or “ Seller : ” . “ EMBED ” calculates the masked attention embeddings from GPT2 . dialog   represents the dialog history up to the current state , [ s , a , . . . , a , s ] , andcandidate denotes   a candidate utterance agenerated by the language model that is being considered for scoring .   s = EMBED(STRCAT ( s , dialog ) )   a = EMBED(STRCAT ( s , dialog + candidate )   s= [ s , s , s ]   a= [ a , a , a ]   q = Q(s , a )   A.2 Experiment Details   Hyperparameter Selection . For our Q - learning algorithm , we used default hyperparameters from an   SAC implementation and did not vary the parameters . We used a critic learning rate of 3∗10 , and a soft   target update rate of 0.05 . For the language model architecture , we finetuned 2 GPT models architectures   ( GPT2 - small , and GPT2 - medium ) . In order to select which model to use , we manually rated samples   from the language generated in their quality across both model types and checkpoints , and selected the   best performing model . We selected the GPT2 - medium architecture at training epoch 2000 .   Compute Resources . We finetuned our language models on TPUs ( TPU v3 - 8 , 16 GB memory per core   with 8 cores ) within a GCP instance . We trained our Q - learning models on an internal compute cluster   using an Nvidia 1080 GPU ( 12 GB memory ) .   A.3 Reward Ablation Study   Because we are limited in the number of evaluations possible in a human study , we use a simulated   evaluation against another chatbot to run an ablation study measuring the effect of using different reward   functions . Specifically , we instantiate a rule - based dialogue manager proposed by ( He et al . , 2018 ) as the   “ buyer ” and negotiate with it on randomly sampled scenarios from the dataset . This is done for both our   method and the baselines , and the results are tabulated in Table 3 .   We evaluated 5 variations of CHAI . CHAI(final ) is the method used in our paper , which uses 2   components to the reward : positive reward for the price the item was sold at , and a penalty for the episode   ending in a rejection . The specific reward used was 10 * the price sold ( normalized by the list price ) if the   offer was accepted , or a penalty of -20 if the offer was rejected . CHAI(penalty ) uses the same reward,4483except with an increased rejection penalty . CHAI(accept ) is given a positive reward of +20 for episodes   ending in an accept and negative reward of -20 for episodes ending in a rejection , without regard to the   price . CHAI(utility ) is rewarded solely for the price an item is sold at at 10 * the price sold . Finally ,   CHAI(fair ) is rewarded for negotiating to a midpoint price between the buyer and seller ’s target prices .   We report the acceptance rate , average revenue , and average offers made and offers accepted . We see   that CHAI(accept ) , CHAI(fair ) and CHAI(penalty ) achieve higher acceptance rates , but offer lower prices   on average . In contrast , CHAI(utility ) and CHAI(final ) offer higher prices with lower acceptance rates ,   with the pure utility optimizing agent CHAI(utility ) offering the highest prices with the lowest acceptance   rates . Thus , we can see how changing the reward function can significantly affect the behavior of the   resulting agent .   Method Accept Rate Prices Offered Prices Accepted Revenue   CHAI(accept ) 0.74 0.80±0.15 0 .74±0.13 0.55±0.35   CHAI(fair ) 0.76 0.80±0.15 0 .75±0.13 0.57±0.34   CHAI(penalty ) 0.90 0.77±0.14 0 .77±0.12 0.68±0.26   CHAI(utility ) 0.34 0.96±0.29 0 .76±0.13 0.29±0.42   CHAI(final ) 0.66 0.84±0.14 0 .87±0.07 0.51±0.38   A.3.1 User Study Parameters   Setup . We conducted our user study through a web - interface , where an advertisement from the test set of   CraigslistBargain is displayed to the user . This is shown in Fig . 5 . Users were instructed to “ type any   message to speak with the bot and negotiate ” , and a chatbot agent replied to each message . After the   user is satisfied with the negotiation , they are asked to indicate whether they want to accept or reject the   current offer on the item .   After interacting with the chatbot agent , users were given a survey and asked to rate the bot on a 5 - point   Likert scale : strongly agree ( 5 ) , agree ( 4 ) , neutral ( 3 ) , disagree ( 2 ) , strongly disagree ( 1 ) . The ratings   were:4484• The bot was fluent ( did not make grammatical or word choice errors ) .   • The flow of the conversation was coherent .   • The bot was on - topic .   • The bot demonstrated human - like behavior .   These questions correspond to the fluency , coherency , on - topicness , and human - likeness scores reported   in our paper , respectively .   Manipulated factors . We evaluate 3 chatbot agents . The first is CHAI . The second is a retrieval - based   baseline ( RL(act)as proposed in ( He et al . , 2018 ) ) . The this is a language modeling baseline based   on finetuning GPT2 - medium on the CraigslistBargain dataset .   Dependent measures . We measure the performance of each chatbot agent according to 4 subjective   metric ( fluency , coherency , on - topicness , and human - likeness ) , which correspond to the survey questions   given to participants described above . Each metric is rated on a 5 - point Likert scale . We also measure the   price that was agreed upon , and whether the negotiation resulted in an acceptance or a rejection .   Risks The risks presented to participants were minimal , and participants were never placed in the   way of physical harm . There was a small probability the chatbot could generate offensive or otherwise   inconsiderate langauge , as the language generation for CHAI and the language model baseline were   unconstrained . However , we did not observe this behavior prior to the study , and did not observe this   behavior during the study . We minimized the risk of confidentiality breaches by anonymizing all data   stored as user IDs – participant names were not stored on our servers .   Subject allocation . We recruited 10 male and 6 female participants , with an average age of 24 .   Participation was voluntary , and subjects were not compensated for their participation . Participants were   asked for consent before participating , and consent for including examples from their interactions for   this paper . All examples contained in this paper are anonymized and contain no personally identifiable   information .   Prior to participation in the study , each user was provided with instructions for how to use the interface ,   and a practice evaluation against the retrieval agent to familiarize them with the user interface and rating   system . Each user interacted with all 3 dialogue agents ( CHAI , retrieval , and language model ) twice , with   the order of interaction randomized per user . The advertisement displayed to the user is also uniformly   sampled from the test set for each trial , independently from the agent being used .   Analysis . We ran a one - way repeated measures ANOV A test for each metric reported ( fluency ,   coherency , on - topic , human - like ) to examine the effect of chatbot agent on the metric .   Results showed that the type of agent used lead to statistically significant differences in the ratings . We   found that :   • For fluency , f(2,30 ) = 5 .8333 , p < 0.0072   • For coherency , f(2,30 ) = 16 .9518 , p < 0.0001   • For on - topicness , f(2,30 ) = 10 .1840 , p < 0.0004   • For human - likeness , f(2,30 ) = 20 .1592 , p < 0.00014485A.4 Additional Qualitative Results   In this section , we include additional examples collected from human dialog for CHAI - prop , the retrieval   method of ( He et al . , 2018 ) , and the pure language modeling baseline .   A.4.1 CHAI - prop   Title : Ubiquiti Networks UAP - AC - LITE - $ 65   Description : Price is firm . Just a few month old . As New . With box and all parts . Great reliable AP . Set   up once and forget about it ; never need to reboot , restart like consumer AP ; especially when you have   high demand traffic with lots of concurrent users . Perfect for large house or business .   Buyer : I noticed that your price is firm . Would you   be flexible in any way ?   CHAI : That is good . I am asking $ 52 for this   excellent product .   Buyer : how old is your product   CHAI : It has been in use for only about 6 months .   It has never been restarted .   Buyer : why are you selling it   CHAI : I do not use this product .   Buyer : is it damaged   CHAI : It has a small scratch on the front . It is still   new . I would offer $ 52.Buyer : Could you do 40   CHAI : I would be willing to let it go for $ 40 .   Buyer : How about $ 10   CHAI : It is new , in excellent condition . I would   not take less than $ 40 .   Buyer : But you just said it was scratched   CHAI : It is not scratched .   Buyer : Sorry I ca n’t do more than 35   CHAI : Well , I can go down to $ 35 .   Buyer : ok   CHAI : Great .   Buyer : accept   Title : Nokia Batteries - One new one used - $ 10   Description : I have these two batteries left over from my old phone . Maybe you can use them ? Both are   in excellent working order and ready to use . Message me if you are interested .   Buyer : I ’ve been looking for batteries , what about   50 cents ?   CHAI : Hi . I still have them . They ’re used , but I   have used them and they work perfectly .   Buyer : How much life does the used one have ?   CHAI : $ 9   Buyer : How many times have you used the old   battery ?   CHAI : 1 to 2 times   Buyer : Did you not use your phone much ?   CHAI : Nope , I ’ve been away from it for about 6   months .   Buyer : Ah , I see that makes sense .   CHAI : I can do $ 6   Buyer : I doubt many people would want to buy a   used battery though , how many other offers do you   have?CHAI : I will let them go for $ 7 .   Buyer : But you offered 6 , 7 is higher ?   CHAI : No . But I ’m really looking for around $ 6   for them .   Buyer : I ’ll pick it up right now for 4 dollars   CHAI : No deal   Buyer : But you were not being fair earlier when   you went from six to 7 dollars   CHAI : I can go to $ 5   Buyer : I ’m holding firm on 4 dollars , especially   because the batteries are used   CHAI : I can let them go for $ 5   Buyer : No one really buys used batteries though ,   what other offers are you getting ?   CHAI : I ’m just going to let them go for $ 4   Buyer : accept4486Title : Couch and love seat - $ 80   Description : Couch and love seat from a non - smoking pets free house . Upholstery is clean and does n’t   have any stain . Some zippers attaching the cushions to the frame are broken . Solid wood frame . pics   coming   Buyer : I am interested in purchasing your couch   and love seat   CHAI : Great ! These were ordered from Crate &   Barrel and are in like new condition . I ’m willing   to negotiate a little , but I ’m asking $ 72 for them .   Buyer : How long have you owned them ?   CHAI : I ’ve had them for a few years . They are   perfect for my new apartment and they were an   absolute steal when I bought them .   Buyer : How much did you purchase them for ?   CHAI : They are brand new , I ’m asking $ 66 .   Buyer : How much did you originally buy the love   seat and couch from from Crate and Barrel?CHAI : I originally paid $ 65 for both , but I figured   I ’d give it a shot since I love the couch .   Buyer : So you are asking for more money then you   originally bought them for ?   CHAI : Yes . I was hoping to get more for the couch   and love seat . How much would you like for the   couch ?   Buyer : I do , but I can only offer $ 40   CHAI : I can only go down to $ 40 if you come pick   up and I will throw in two chairs for you to use .   Buyer : Great ! That ’s a deal   CHAI : Great !   Buyer : accept   Title : Fit Bike VH2 - $ 200   Description : 20.75 ” top tube All in perfect working order . Very good condition - no rust - always stored   indoors . Shiny . Few scratches / scrapes on pegs , etc , but very well cared for . Gyro brake . Thanks for   interest . If its still here it is still available .   Buyer : Hello , I ’m interested in the bike   CHAI : It is in great shape . It ’s a well - run bike . I   bought it new , so I ’m making it available .   Buyer : how long have you had it ?   CHAI : I bought it new last year . It ’s in excellent   condition .   Buyer : can I see pictures ?   CHAI : Of course . How much are you looking to   spend on the bike?Buyer : I ’m looking to spend under $ 150   CHAI : That ’s not a bad price . Would you be able   to pay $ 180 for it ?   Buyer : How about $ 160 ?   CHAI : $ 160 and you have a deal .   Buyer : Done   CHAI : Great !   Buyer : accept4487A.4.2 Retrieval   Title : Media Cabinet ( Moving Sale ) - $ 20   Description : Slide out tray for media storage Condition : Structurally sound Good condition Search   Steve2017 for my other listings or simply click on the " more ads by this user " link found under the map .   Please email me if you are interested . Serious inquiries only . Thank you   Buyer : Hello , how are you ?   Retrieval : i am well , how are you ?   Buyer : Good . I ’m interested in buying the media   cabinet .   Retrieval : how can i help you today ?   Buyer : Can you provide more pictures?Retrieval : yes   Buyer : How old is the item ?   Retrieval : less than 1 year olf   Buyer : I would like to buy it for $ 15 .   Retrieval : deal   Buyer : accept   Title : Adams trail - a - bike shifter 7 - $ 100   Description : This is an awesome bike trailer for your kid . It features 7 shiftable gears so that your kid   can learn to shift but more importantly shift down or up to continue to help you pedal the bike . I know   that without the shiftable gears my kid would either think the pedaling was too hard at times or going so   fast they could n’t feel their pedaling helping . It took some time to teach them but definitely the way to go .   Buyer : Hello , I ’m wondering about the availability   of this bike trailer   Retrieval : hello   Buyer : Is this bike trailer still available ?   Retrieval : i ’ve had it for about 5 years now .   Buyer : great , is it still available ?   Retrieval : yes it is   Buyer : When would I be able to pick it up ?   Retrieval : yeah , that ’s fine .   Buyer : Is there a date by which I would be able to   pick it up ?   Retrieval : sure , i could do that for you . Buyer : Okay , is there any damage to the bike trailer   Retrieval : yes what questions do you have ?   Buyer : I ’m wondering if there ’s any damage to the   bike trailer   Retrieval : great ! i ’m asking $ 100.00 for it !   Buyer : Would you be able to decrease the price to   $ 90 ?   Retrieval : sure . i ’d be willing to let it go for $ 90.00   Buyer : Okay great , I ’ll pick it up today . Does that   work for you ?   Retrieval : yes absolutely . its a great deal   Buyer : accept4488Title : File Cabinet metal - $ 70   Description : I have 4 cabinets file metal 4 - Drawer for sell . They are good condition . they are black .I sell   them for 70 $ each . Thank you for reading !   Buyer : Why are you selling   Retrieval : yes it is still for sale .   Buyer : How much is it   Retrieval : it ’s new   Buyer : Sorry , can you repeat that   Retrieval : where are you . how far from alameda (   ca ) ?   Buyer : I am close to [ removed ] . It ’s pretty far   Retrieval : how about $ 70.00 ?   Buyer : I do n’t know the condition . Could you   please provide me with some detailsRetrieval : it looks extremely heavy and in great   condition ! i could do $ 70.00 ...   Buyer : Is there any damage   Retrieval : nope . overall condition is extremely   well   Buyer : How heavy are they   Retrieval : yes , very good condition .   Buyer : How about 25   Retrieval : deal   Buyer : accept   Title : Samsung Gear S2 - $ 175   Description : Available in Black or white Flawless ! Gear , Samsung , S3 , S2 , smartwatch , smart , watch ,   fitbit , pedometer , fitband , fit , band , bit , Samsung Gear Fit 2 , classic , iwatch   Buyer : Is this an S3 or S2 ?   Retrieval : yes . are you interested ? it ’s in great   condition   Buyer : I ’m interested .   Retrieval : do you have any questions ?   Buyer : Is it a Gear S2 or Gear S3 ?   Retrieval : yes its a full tablet so it does come with   the charger   Buyer : Is htis a watch or a tablet?Retrieval : yes .   Buyer : Can I see pictures ?   Retrieval : sure thing .   Buyer : can I have it for $ 50 ?   Retrieval : i am willing to do $ 50.00 if you can   pick it up at my place of work .   Buyer : sure , where is that ?   Retrieval : yes , that is correct .   Buyer : reject4489A.4.3 Language Model   Title : Gorgeous 3 bedrooms available in August ! Perfect way to end the summer - $ 3795   Description : Call Now - Use this link for more info : http://thegreystar.com/42v07z Everyday Exceptional .   Where You Belong . This is a 3 Bedroom , 2 Bath , approximately 1422 Sq . Ft . NEW APARTMENT   HOMES IN PLEASANTON , CA . A contemporary apartment community that is as much an escape as it   is a destination . A place where beautifully appointed homes , resort - style amenities , rolling lawns and   parks , and boutique shops come together to create a distinctly new leisure lifestyle . Please click on the   link below to check out our virtual tour !   Buyer : Hello , is this 3 bedroom home still avail-   able for rental ?   LM : Hi , yes it is . The asking price is $ 2430 .   Buyer : Is that a monthly cost ?   LM : The lease is for one year , but if you have a   family member or friend who are already employed   and would be willing to sign a longer lease , we   could offer you the option of moving in right away .   Buyer : Would the price be lower if I signed a   longer lease ?   LM : The price is firm at $ 2425 , and I ’m willing to   negotiate   Buyer : Would you be able to rent for $ 2300 ?   LM : Yes   Buyer : Great , what documents would I need to   provide to apply to rent this place ?   LM : This place has been carefully designed with   you in mind .   Buyer : Does this apartment have a deposit fee ?   LM : This apartment is located in a convenient lo-   cation , so that the lowest that can be reasonable for   you is 10 % of your deposit .   Buyer : What would the deposit fee bee ?   LM : If you sign a longer lease , the deposit fee will   be $ 75 .   Buyer : Okay , great ! How long would my lease   need to be ?   LM : I ’d be willing to let it run for 5 months at no   additional cost . Buyer : So if I sign a 5 month lease the deposit fee   would be $ 75 and rent $ 2300 per month ?   LM : That ’s too low for me . I could come down to   $ 90 in monthly rent . I can also drop the rent down   to $ 100 for you .   Buyer : Oh wow ! So I could get this place for $ 90   in monthly rent ?   LM : I think that is fair . How long of a lease are   you looking for ?   Buyer : 1 year   LM : This is a very good deal . I can accept $ 80 for   a 3 year lease .   Buyer : I want a 1 year lease   LM : Ok , you have a deal !   Buyer : Okay , 1 year lease at $ 80 per month ?   LM : Yes   Buyer : Great , thank y   LM : I would like to hear your budget .   Buyer : $ 80 per month works for me   LM : No , sorry .   Buyer : I thought you had provided that as your   previous offer ?   LM : I am sorry , but I do not . So $ 80 is the price .   Buyer : Okay , confirming $ 80 per month ?   LM : How about $ 90 per month ?   Buyer : That also works .   LM : You have a deal !   Buyer : Great !   LM : offer $ 80   Buyer : accept4490Title : Parker Apartments - Now Accepting Below Market Rate Applications - $ 979   Description : * * * * Parker is that thrill you feel when inspiration strikes . Perched proudly on Shattuck   Avenue in Berkeley , this collection of 155 carefully crafted , sustainably designed apartments offer   immediate access to music venues , boutique shopping , nightlife , green space , the Gourmet Ghetto , and   removed . At home , get to know your neighbors by the crackling light of a fire pit , get focused in our   co - working space , or burn some calories at the on - site Equinox Fitness Club .   Buyer : Hi , when can I move in ?   LM : Today   Buyer : Are there grocery stores nearby ?   LM : Yes , and the inside of our apartments is com-   pletely gated . Buyer : Is there parking available ?   LM : No , but there is a free shuttle that you can use   to get there .   Buyer : Would you be able to do $ 900 ?   LM : offer $ 2800   Buyer : reject   Title : 20 " LED Emergency Strobe Light Lamp Bar - $ 30   Description : 20 " LED Emergency Vehicle Dash Warning Strobe Flashing Light . these lights are a great   addition and will defiantly get people out of your way as so you can respond on time ! 7 Different modes   of flashing / scanning mode . Please read the description to see a detail description of each mode . Includes :   1 x Traffic strobe light bar with on / off switch and mode changer button Fitment : Universal for all vehicle   with a 12V power source Very bright led that are visible even during daytime conditions . 2 colors : Red ,   Amber . Please let me know what color you are interested .   Buyer : are both colors the same price   LM : The colors are different . I can sell them both   for $ 200 each .   Buyer : I thought it was $ 30   LM : I am sorry , I have to change my mind .   Buyer : oops there is only one light   LM : The bulbs come with the two matching colors .   It would be a hassle to flash them with the other   lights in your vehicle . The best thing about the   bulbs is they will shine for you . It ’s a great deal at   only $ 20Buyer : is there free shipping   LM : I think I could negotiate a little but I am not   able to accept this offer .   Buyer : can I pay $ 7   LM : I have another offer but I ca n’t go any lower   than $ 13   Buyer : what about $ 11   LM : offer $ 7   Buyer : oh ok sure $ 7 and free shipping ?   LM : Yes , I accept that !   Buyer : accept4491