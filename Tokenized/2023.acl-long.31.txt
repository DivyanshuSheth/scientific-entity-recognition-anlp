  Ziqiao MaJiayi PanJoyce Chai   Computer Science and Engineering Division , University of Michigan   { marstin,jiayipan,chaijy}@umich.edu   Abstract   The ability to connect language units to their   referents in the physical world , referred to as   grounding , is crucial to learning and under-   standing grounded meanings of words . While   humans demonstrate fast mapping in new word   learning , it remains unclear whether modern   vision - language models can truly represent lan-   guage with their grounded meanings , and how   grounding may further bootstrap new word   learning . To this end , we introduce Grounded   Open V ocabulary Acquisition ( GOVA ) to ex-   amine grounding and bootstrapping in open-   world language learning . As an initial at-   tempt , we propose World - to - Words ( W2W ) , a   novel visually - grounded language model by   pre - training on image - text pairs highlighting   grounding as an objective . Through extensive   experiments and analysis , we demonstrate that   W2Wis a more coherent and fast grounded word   learner , and that the grounding ability acquired   during pre - training helps the model to learn   unseen words more rapidly and robustly .   1 Introduction   Language is learned through sensorimotor experi-   ence in the physical world ( Bisk et al . , 2020 ) . The   ability to connect language units to their referents   in the physical world , referred to as grounding ,   plays an important role in learning and understand-   ing grounded meanings of words ( Harnad , 1990 ) .   As shown in Figure 1 , a human reader would eas-   ily ground noun phrases to the corresponding en-   tities captured in the image . Even when the term   “ incinerator ” is new to human learners , they can   still locate the object of interest through the lan-   guage and visual context , and acquire its meaning .   In fact , this ability to bootstrap new word learn-   ing with only minimal information , known as fast   mapping , is demonstrated abundantly in cognitiveFigure 1 : Even when the term “ incinerator ” ( high-   lighted yellow ) is new to human learners , they can still   locate the most likely referent ( indicated by the yellow   bounding box ) in the perceived world by grounding .   literature on human language acquisition ( Carey   and Bartlett , 1978 ; Carey , 1978 ; Golinkoff et al . ,   2000 ; Smith and Yu , 2008 ) .   Recently , there has been a substantial effort on   pre - training vision - language models ( VLMs ) ( Du   et al . , 2022a ) . Despite the exciting performance of   these models on a variety of downstream vision and   language tasks , it remains unclear whether these   models can truly understand or produce language   with their grounded meanings in the perceived   world , and how grounding may further bootstrap   new word learning . These questions are of inter-   est from both a scientific and an engineering point   of view . From a scientific perspective , ground-   ing is crucial to language learners , as children at-   tend to intended objects in the environment when   producing ( Tanenhaus et al . , 1995 ; Meyer et al . ,   1998 ) and comprehending ( Smith et al . , 2007 ) ut-   terances . From an engineering perspective , even   with the availability of grounded vision language   datasets ( image - text pairs with fine - grained word-   object mappings ) ( Plummer et al . , 2015 ) , the costly   grounding annotation can hardly cover the whole   vocabulary space during the training time . Build-   ing upon the pre - trained models , it ’s important for   the agent to have the ability to learn grounded new   words in a few shots of raw image - text pairs with-   out word - object mappings .   To this end , we introduce Grounded Open V o-   cabulary Acquisition ( GOVA ) , a scalable formulation   to examine grounding and bootstrapping in open-   world language learning . In this formulation , lan-524guage learning is a combination of learning to pre-   dict a word in a linguistic context as well as learn-   ing to ground the word in the physical world . Un-   der this formulation , we explore the framework in   which the model first acquires the grounding ability   during pre - training , and then transfers this ability to   learn unseen words without grounding supervision .   As an initial step , we developed World - to - Words   ( W2W ) , a novel visually grounded language model   motivated by recent advances in detection trans-   formers ( DETR ) ( Carion et al . , 2020 ; Kamath et al . ,   2021 ) . Compared to many existing VLMs , W2W   performs language modeling upon explicit object   representations . The model first acquires the ability   to ground during pre - training , and then transfers   this intrinsic ability to learn unseen words when   grounded supervision is no longer available .   Our empirical results show that learning to map   words to their referents plays a significant role in   grounded word acquisition . By pre - training with   fine - grained word - object mappings , W2Wdemon-   strates stronger performance in learning grounded   meanings of words , both seen and unseen , yet with   orders of magnitude fewer data compared to other   competitive VLM baselines . The pre - trained model   can further provide a foundation for efficient learn-   ing of new grounded words with a few examples .   We further present an in - depth analysis to under-   stand potential predictors of W2Win word learning ,   which demonstrates intriguing behaviors in com-   parison to human language learning . Our findings   will provide a stepping stone for future work on   grounded language learning in an open world .   2 Grounded Open Vocabulary   Acquisition ( GOVA )   We start by introducing the settings of grounded   word acquisition and few - shot learning of new   words tasks , which are two key components of   the Grounded Open V ocabulary Acquisition ( GOVA )   task formulation . We further present a unified eval-   uation protocol and introduce the dataset we cu-   rated for this problem .   2.1 Grounded Word Acquisition   Many vision - language tasks have been developed   in the past , e.g. , visual question answering , vi-   sual commonsense reasoning , etc . However , these   tasks are mainly focused on the end task perfor-   mance without scrutinizing whether words are   grounded to their corresponding visual entities . We   consider a formulation that directly examines if   vision - language models have the ability to acquire   grounded meanings of words , specifically , through   both language modeling andobject localization .   Figure 2 shows an instance of the word acquisi-   tion task . A model is presented with an image   x∈ Iand an incomplete caption x∈ T with   one of its groundable words w(e.g . , nouns and ad-   jectives ) replaced by a MASK . The model is tasked to   predict this missing word w∈ V based on all avail-   able context and localize the corresponding objects   O={o , o , · · · , o}in the image by proposing   the bounding boxes of them . Overall , a model capa-   ble of solving the grounded word acquisition task   is a function f : I × T → V × R.   The language modeling part takes the form of a   cloze test , which predicts an open vocabulary word   and is widely adopted to evaluate pre - trained lan-   guage models ( Paperno et al . , 2016 ; Petroni et al . ,   2019 ; Jin et al . , 2020 ) . However , language model-   ing alone fails to provide a comprehensive evalu-   ation of language grounding . For example in Fig-   ure 2 , a model may correctly produce the word   “ boat , ” but mistakenly attributes the evidence to   the larger white boat in the image . To address   this limitation , we require models to localize the   corresponding object in the image . This design   is motivated by the disentanglement of object de-   tection into object localization and class recogni-   tion ( Singh et al . , 2018 ; Zareian et al . , 2021 ; Zhong   et al . , 2022 ) . It enables vision models to develop a   sense of objectness without relying on a predefined   set of object classes , thereby potentially allowing   them to generalize to unseen objects . Further com-   parison with related task setups is discussed in Sec-   tion 5 and illustrated in Figure 8 in the Appendix .   2.2 Evaluation Metric   In language model evaluation , the commonly used   measures for assessing performance are the stan-525dard hit - rate - at- k(HR@k ) measure and perplex-   ity ( Salazar et al . , 2020 ; Jin et al . , 2020 ) . In masked   language modeling , the log perplexity of a word w   is defined as the log pseudo - perplexity :   In object detection evaluation , especially for   phrase grounding where multiple referents are   possible ( Kamath et al . , 2021 ) , Any - Protocol   and All - Protocol are commonly adopted . As-   suming nground truth bounding boxes B=   { b , b , · · · , b}andmpredicted bounding boxes   /tildewideB={/tildewideb,/tildewideb,···,/tildewiderb } , the intersection - over - union   ( IoU ) in both protocols is defined as :   However , these metrics only capture unimodal   performance without concerning the correctness of   cross - modal mapping . We design two new metrics   to combine language and vision performance :   •Grounded hit - rate ( G - HR @k ) , the proportion   of tests with the masked word appearing in the   top - kcandidates and a localization IoU over 0.5 .   •Grounded perplexity ( G - PPL ) as follows :   2.3 Few - Shot Learning of New Words   Although there are grounding datasets available ,   i.e. , image - text pairs with word - object mapping an-   notation ( Plummer et al . , 2015 ) , it is impractical   to obtain such fine - grained annotation on a large   scale and to cover the whole vocabulary space V.   We therefore explore grounded new word learning   as a few - shot learning problem , especially under   the setting of incremental class learning ( Mandz-   iuk and Shastri , 1999 ; Kemker et al . , 2018 ) . An   intuitive illustration of the few - shot new word learn-   ing framework is provided in Figure 3 . Under   this framework , a computational model is devel-   oped in two stages . During the pre - training stage ,   the model receives image - caption pairs , with fine-   grained word - object annotation for a set of base   words V⊆ V. After pre - training , the model is   provided with few samples of raw text - image pairs ,   each containing a set of unseen words V⊆ V   that the model has to acquire .   Tests are performed after each training stage .   It ’s important to note that the unseen words may   not be completely new , e.g. , the models may have   encountered these words in its language encoder   initialized with pre - trained language models . We   consider them “ unseen ” because the model never   sees these words paired with their referent , i.e. , the   grounded meanings of the words are unknown .   2.4 Dataset Curation   We build our dataset based on the Flickr30 K Enti-   ties dataset ( Plummer et al . , 2015 ) , which contains   image - text pairs with dense annotations between   groundable phrases and bounding boxes of objects .   The groundable phrases and regions are defined by   the dataset , as chunks of text that refer to object   bounding boxes . To construct word grounding in-   stances , we use Stanza ( Qi et al . , 2020 ) to parse the   caption , enumerate every word in the groundable   phrase , and identify those with a POS tag of NOUN   orADJ . These groundable words are replaced by   MASK one at a time and matched to their correspond-   ing bounding boxes .   The dataset is divided into 4 splits : pre - training   set , unseen words training set , seen words test set ,   and unseen words test set . We start by selecting 31   unseen words and holding out all text - image pairs   containing these words from the training split of   Flickr30 K Entities . The hold - out text - image pairs   are further divided into the training and test sets   for unseen words . The remaining training split   of Flickr30 K Entities is used for the pre - training   set . To prevent frequent words ( e.g. , “ man ” ) from   dominating the test results of the seen words , we   choose 60 seen words and sample an equal number   of test instances for each word from the test split   of Flickr30 K Entities . More details and statistics   of the dataset are available in Appendix A.526   3 Computational Models   3.1 The World - to - Words ( W2W ) Model   Humans demonstrate fast mapping , the ability   to learn new words with only minimal infor-   mation ( Carey and Bartlett , 1978 ; Carey , 1978 ;   Golinkoff et al . , 2000 ) . Motivated by how vi-   sual grounding helps humans in bootstrapping new   words , we propose a computational framework   that first acquires the ability to ground during pre-   training , and then transfers this intrinsic ability to   learn unseen words when grounded supervision is   no longer available . We introduce World - to - Words   ( W2W ) , a novel visually - grounded language model   with an end - to - end design as illustrated in Figure 4 .   Model Architecture . Similarly to dual - stream   vision - language models , W2Wencodes the textual   input with a pre - trained language model ( Liu et al . ,   2019 ) , and encodes image input with convolutional   backbone ( He et al . , 2016 ) with 2D positional en-   coding added . The text and image representations   are linearly projected onto a joint semantic space   and concatenated . The multimodal representation   is then forwarded into a cross - encoder with self-   attention layers . The cross - encoded representations   in the final layer are sent into an object decoder ,   together with a set of learnable object queries . The   object decoder produces an object embedding for   each input object query , which can be considered   as a representation of the proposed object . The   object representations are further forwarded to the   text decoder , which allows language modeling to   explicitly attend to the perceived objects . We dis-   cuss the pre - training objectives , especially how the   model acquires grounding in the following para-   graphs . Other details are available in Appendix B.   Masked Language Modeling ( MLM ) . As an in-   trinsic task , we follow the majority of existing pre-   trained vision - language models to perform maskedlanguage modeling with a two - layer MLP . Words in   input text are randomly masked out , and the model   predicts the masked words conditioned on the cor-   rupted sentence and image . Words in groundable   phrases are masked with a probability of 0.4 and   those in non - groundable regions are masked with a   lower probability of 0.1 .   Object Localization ( OL ) . Each object represen-   tation will be decoded by a shared three - layer MLP   to produce a bounding box . We follow prior de-   tection transformers ( DETR ) ( Carion et al . , 2020 ;   Kamath et al . , 2021 ) to perform bipartite matching   between proposed boxes and ground truth boxes   with a Hungarian loss ( Kuhn , 1955 ) . The pre-   dicted boxes are optimized towards ground truth us-   ing the generalized intersection - over - union ( GIoU )   loss ( Rezatofighi et al . , 2019 ) and the L1 loss .   Grounding . The notion of Grounding is real-   ized by grounded pre - training through word - region   alignment ( WRA ) which enables fine - grained   cross - modal mapping between words and objects .   It consists of two levels of alignment : positional   alignment andsemantic alignment . In positional   alignment , the model learns to map each object rep-   resentation to words in the sentence , which could   possibly be a MASK or an additional no - object label   ∅(Yu and Siskind , 2013 ; Kamath et al . , 2021 ) . We   use a fully - connected layer to predict the distribu-   tion over token positions with cross - entropy loss .   In semantic alignment , the model learns to bring   word representations closer to the object represen-   tations that they ground to , and push the unrelated   pairs farther . We use a contrastive loss over the   final layers of the object and text decoders .   3.2 Baselines   Groundless Baseline . A baseline with no   grounding ability is developed by pre - training W2W   in the same condition but removing the grounding527   objectives in the loss function . We refer to this   groundless model as W2W. Like a typical pre-   trained VLM , e.g. , VisualBERT ( Li et al . , 2019 ) ,   W2W performs language modeling based on the   object features , without explicit cross - modal refer-   ential grounding . We apply W2W onGOVA task   by fine - tuning the model on the pre - training dataset   with grounding objective until convergence .   Pre - trained Baselines . For the majority of the   pre - trained VLMs , the unseen words are known   during pre - training . Also , the primary focus of   this work is to understand grounding and boot-   strapping in grounded word acquisition . It ’s not   our goal to scale up or re - train all variants of pre-   training frameworks . Therefore , we compare our   model to the pre - trained VLMs with equal or rea-   sonably larger scales for only reference and analy-   sis purposes . We choose representative baselines   in phrase grounding , as presented in Table 1 :   •“Detect - and - Recognize ” Baseline : Models un-   der this framework rely on a pre - trained frozen   object detector , and then learn to predict words   from proposed objects . We choose the fine - tuned   VisualBERT ( Li et al . , 2019 ) for this type .   •“Produce - and - Localize ” Baseline : Models un-   der this framework rely on a pre - trained vision-   language model to predict the missing word ,   and then perform referring expression compre-   hension and propose objects . We combine   ViLT ( Kim et al . , 2021 ) and MDETR ( Kamath   et al . , 2021 ) for their competitive performance   in vision - conditioned language modeling and   phrase grounding individually .   4 Empirical Findings   4.1 Grounded Pre - training   The results of this section are obtained from the   test immediately following pre - training .   Pre - training Results on Seen Words The main   results for the pre - training stage are summarized   in Table 1 . Our direct observation is the strong   performance of W2W in terms of both grounded   metrics , Top-1 Grounded Hit - Rate ( G - HR @1 ) and   Grounded Perplexity ( G - PPL ) . W2Wsignificantly   outperforms the groundless baseline W2W and   pre - trained baselines , even for systems pre - trained   with a significantly larger amount of data and com-   puting , as shown in Table 2 . While W2Wproduces   correct predictions of the missing words as well   as the locations of the corresponding bounding   boxes , it turns out to be challenging for baselines   to achieve them both . For “ Detect - and - Recognize ”   baseline ( VisualBERT ) , we observe a compara-   ble object localization performance empowered   by the frozen object detector . However , it suffers   from a poor language modeling ability ( as demon-   strated by HR @1and PPL , weaker than a fine-   tuned RoBERTa ) . For the “ Produce - and - Localize ”   baseline ( ViLT+MDETR ) , we observe a strong lan-   guage modeling performance due to the scale of   ViLT . Yet , correct word grounding remains difficult ,   as can be seen from the poor localization perfor-   mance . These results demonstrate that the GOVA   task is challenging , and W2Wis competitive in learn-   ing grounded word meanings during pre - training .   Bootstrapping through Grounded Objectives .   We further provide a cross - time analysis to under-528stand the role of grounded objectives in pre - training   efficiency . The results of different training steps   are provided in Table 3 . From the table , we observe   thatW2Woutperforms both of its groundless vari-   ants in language modeling , object localization , and   jointly under the grounded perplexity . What ’s even   more striking is that W2Wachieves better perfor-   mance with 10 times less training data compared to   the model trained without the grounding objective   ( i.e. , the WRA objective ) . These results confirm   the crucial role of explicit word - object alignment   in efficient grounded word learning . This can be   explained by that the grounded objectives attempt   to align the vision and language semantic spaces ,   which ideally benefit both visually conditioned lan-   guage modeling and language - conditioned object   localization . Although it is possible to build a   mapping between word and object representations   through cross - modal probing and fine - tuning af-   ter pre - training , these methods are not comparable   to systems with grounded objectives in terms of   efficiency and performance .   Pre - training Results on Unseen Words : Word-   Agnostic Grounding One important finding of   the pre - trained model is the surprising performance   in localizing the unseen words behind the MASK s.   As shown in Table 1 , W2Wachieves a high Any-   IoU of 56.3 % and Any - localization accuracy of   61.3 % for the unseen words , which are very close   to its performance on the seen set and surpass base-   lines that have seen these words . Moreover , as   anticipated , since these words are held out during   pre - training , W2Wfails to correctly unmask these   unseen words , leading to a high log perplexity of   11.01 and low HR of 4.2 , compared to that of 1.26   and 66.9 on the seen words . Figure 5 shows an   example of such word - agnostic grounding .   This performance disparity in language mod-   eling and referent localization on unseen words   suggests that W2Whas developed a certain level of   word - agnostic grounding , i.e. , to locate the most   likely referent of a word through both the linguistic   context and the visual context , even if the word   itself is never seen during pre - training . A similar   situation is faced by human language learners when   inferring the grounded meaning of a new word , as   we described earlier in Figure 1 . Our experiment   demonstrates that , through grounded pre - training ,   it is possible for a vision - language system to ac-   quire word - agnostic grounding ability , which opens   up the opportunity to enable human - like fast map-   ping when learning new words .   4.2 Few - Shot New Words Acquisition   In this section , we task W2Wto acquire unseen words   from a few samples of raw image - text pairs , with-   out any bounding boxes or word - object mappings   annotation . As we have demonstrated the model ’s   word - agnostic grounding , we seek to explore if   this ability can be transferred to facilitate learn-   ing unseen words when a large amount of data   and grounded supervision are no longer available .   Specifically , we perform few - shot learning on the   pre - trained W2Wwith only masked language model-   ing ( MLM ) as the learning objective . More hyper-   parameter details are available in Appendix B.2 .   Learning New Words through Incremental   Learning . We first explore the multi - class incre-   mental learning setting , in which the pre - trained   model is tasked to acquire the 31 unseen words   from a few - shot learning session . The experiment   is repeated with sample sizes of 8 , 16 , 24 , and 32   immediately after pre - training . As shown in Fig-   ure 6 , even with as few as 8 samples per word ,   W2Wcan significantly bring down the grounded per-   plexity of unseen words , while mostly maintaining   the grounded perplexity of the seen words without   catastrophic forgetting . Compared to W2Wwithout   the grounding objective , the full W2Wdemonstrates   better acquisition performance for unseen words.529It ’s important to note that these few shot exam-   ples are text / image pairs without explicit ground-   ing annotation . Our W2Wis able to quickly acquire   grounded meanings of the new words ( e.g. , only   with 8 examples ) with a performance close to that   of seen words .   We further perform a word - specific controlled   study with a one - class incremental learning setting .   We present results on two unseen words ( pizza   andcircular ) in Table 4 . The complete results   are available in Appendix D.   4.3 Predictors of Model Behaviors   There has been an interest to identify predictors   that can explain / anticipate the performance or be-   havior of pre - trained language models ( Chang and   Bergen , 2022 ) . This exploration not only offers   valuable insights for future model development ,   but also serves as a cognitive inquiry to evaluate   the extent to which language models align with hu-   man language acquisition patterns . In this section ,   we present the first work of this nature on vision-   language models . Specifically , we note that the   W2Wmodel relies on a RoBERTa encoder , which   might have already been equipped with prior lin-   guistic knowledge . To assess the cognitive align-   ment of vision - language models to human language   acquisition , we additionally pre - trained the W2W   andW2W models with a randomly initialized   RoBERTa encoder . To comprehensively capture various aspects of   words , we carefully select eight distinct predictors   that encompass intrinsic psycho - linguistic charac-   teristics , distribution patterns within the training   corpus , and visual representations within the train-   ing images . We select 3 psycho - linguistic predic-   tors , each collected and normalized from the MRC   Database ( Coltheart , 1981 ):   •Familiarity , the degree of familiarity or expo-   sure people have to words ;   •Concreteness , the degree to which words have   a perceptible physical referent or are associated   with tangible objects or experiences ;   •Imageability , the degree to which words elicit   people ’s mental imagery .   Another 3 linguistic predictors are considered :   •Unigram perplexity ;   •RoBERTa perplexity , where RoBERTa is fine-   tuned on the captions to serve as the upper bound   of unimodal language model performance ;   • # Co - occur phrases , the average number of   co - occurring groundable phrases in a caption .   We finally choose 2 perceptual predictors :   • # Co - occur objects , the average number of   co - occurring objects in an image ;   •Bbox size , the average proportion of an image   occupied by the bounding boxes of the referents .   To assess the statistical significance of each pre-   dictor , we performed linear regressions with like-   lihood ratio tests on different variants of models .   Similar to Chang and Bergen ( 2022 ) , we compare   the overall regression including the target predictor   to a regression that included all predictors except   the target . We additionally present the beta weights   ( with signs ) to capture the magnitude and direc-   tion of the correlation . Figure 7 displays heatmaps   indicating the statistical significance ( in terms of   negative logarithmic p - values ) of each predictor   concerning Log G - PPL , Log PPL , and Any IoU.   Insignificant tests are omitted from the figure .   Correlation with Linguistic and Perceptual Pre-   dictors . Our findings revealed a positive correla-   tion between the unigram and RoBERTa log per-   plexity and the models ’ log perplexity , both for   grounded and ungrounded scenarios . This indicates   that vision - language models still heavily rely on   distributional statistics , similar to unimodal mod-   els . While the ungrounded perplexity showed lit-   tle correlation with perceptual predictors , the Any530   IoU demonstrated a significant correlation with the   number of co - occurring objects and average sizes   of bounding boxes . This suggests concepts that are   visually salient and less perceptually ambiguous   are easier to localize and acquire , consistent with   human learners ( Smith and Yu , 2008 ) .   Correlation with Psycho - linguistic Predictors .   Counter - intuitively , there was a positive alignment   between the human perceived familiarity of words   and the machine ’s perplexities , i.e. , the more famil-   iar humans are with a word , the more perplexed   models get . This contrasts with the ideal cog-   nitive plausibility of language acquisition in hu-   mans . This discrepancy implies that current vision-   language models may not fully achieve cognitive   plausibility , which might be explained by the fact   that many concepts ( e.g. , wild animals , musical   instruments ) appear abundantly in internet images   but not in daily lives . In terms of imageability ,   it aligned well with human intuition , exhibiting   a positive correlation with Any IoU and a nega-   tive correlation with perplexities . However , the   concreteness predictor surprisingly exhibited the   opposite correlation . This discrepancy could be   attributed to the nuanced distinction between im-   ageability and concreteness . For instance , while   “ hat ” is concrete because it refers to a tangible ob-   ject , it also possesses visual diversity due to its   generality ( e.g. , many types of hats which look   very differently ) , making it challenging to acquire .   Conversely , “ blue ” is more imageable as it eas-   ily evokes a color , relatively stable , despite not   referring to a specific tangible object . To learn the   meaning of “ hat , ” a human language learner may   benefit from physically interacting with the object ,   and understand that the hat is an item to cover for   the head , regardless of its visual appearance . Toaddress this gap , a potential future direction could   involve developing language learning agents that   acquire words through physical interactions rather   than passive perception , allowing for a more com-   prehensive understanding of word meanings .   5 Related Work   Vision - Language Mapping Mapping plays a   central role in classic lexicon acquisition prob-   lem ( Gleitman and Landau , 1994 ; Clark , 1995 ) .   Primarily , researchers focused on grounding words   to their meaning symbols , building learning mecha-   nisms using specific mental biases to simulate chil-   dren ’s word acquisition , and giving computational   accounts for psycholinguistic phenomena ( Siskind ,   1996 ; Regier , 2005 ; Goodman et al . , 2007 ; Fazly   et al . , 2010 ) . Early efforts along this line incorpo-   rate visual grounding either by learning a statisti-   cal or neural mapping from object categories ( Roy   and Pentland , 2002 ; Yu , 2005 ; Xu and Tenenbaum ,   2007 ; Yu and Ballard , 2007 ; Yu and Siskind , 2013 )   and more complicated visual features ( Qu and Chai ,   2010 ; Mao et al . , 2019 , 2021 ; Pratt et al . , 2020 ) to   linguistic labels . These studies are usually in a   closed world with limited vocabulary ( Krahmer   and van Deemter , 2019 ) , and words are usually   isolated from the natural context of use . More   recently , multi - modal understanding tasks , e.g. , ob-   ject retrieval ( Guadarrama et al . , 2014 ; Hu et al . ,   2016 ) , referring expression comprehension and   grounding ( Liu et al . , 2014 ; Yu et al . , 2016 ; Mao   et al . , 2016 ; Wu et al . , 2020 ) , and phrase ground-   ing ( Plummer et al . , 2015 ) map referring expres-   sions to corresponding objects . Our setup is closely   related to this line as we position grounding as an   explicit word - referent mapping problem . The dif-   ference is that , our work goes beyond grounding531to study open - vocabulary acquisition through fast   mapping , a more complicated but realistic chal-   lenge faced by AI agents .   Vision - Language Pre - training Distributional   word representations can be acquired through lan-   guage modeling , and developing language models   from visual data has been extensively studied by   the community ( Chrupała et al . , 2015 ; Lazaridou   et al . , 2015 ; Li et al . , 2017 ; Surıs et al . , 2020 ) . Re-   cent years have seen increasing research to enrich   language representations with visually - augmented   language modeling ( Tan and Bansal , 2020 ; Lu et al . ,   2022 ; Wang et al . , 2022 ) and to learn multimodal   representations with vision - language pre - training   ( VLP ) ( Du et al . , 2022a ) . We are particularly inter-   ested in VLP models with fine - grained grounding   objectives , e.g. , Word - Region Alignment ( WRA ) .   These models either pre - train with weakly super-   vised alignment algorithms like optimal transport   that matches words with patches ( Kim et al . , 2021 )   or proposals from a frozen detector ( Chen et al . ,   2020 ; Su et al . , 2020 ) , or perform explicit word   grounding by pre - training a language - conditioned   detector ( Kamath et al . , 2021 ; Li et al . , 2022 ; Zhong   et al . , 2022 ; Dou et al . , 2022 ) . Our model falls   along this line , which jointly performs language   modeling , object localization , and grounding dur-   ing pre - training , rather than relying upon a pre-   existing object detector .   Vision - Language Tasks To evaluate vision-   language systems , many downstream tasks have   been formulated . Some related formulations are   summarized in Table 5 in Appendix . While demon-   strating some vision - language capabilities , these   down - stream tasks provide limited insights into   whether these models truly capture the grounded   meaning of words with respect to the external en-   vironment . Our task design specifically targets   the machine ’s ability to predict words and ground   words to perception . More akin to our formulation   is the vision - based language modeling task ( Jin   et al . , 2020 ) in a continual learning setting . Our   work differs mainly in two aspects . First , the task   proposed by Jin et al . ( 2020 ) only predicts masked   tokens based on the visual context , which leaves   the referential uncertainty ( i.e. , grounding ) unat-   tended ( e.g. , in Figure 2 , correct prediction of the   word “ boat ” does not guarantee correct grounding ) .   Also , this work primarily focuses on composition-   ality , while we seek to address few - shot grounded   word learning when unseen words are encounteredafter pre - training .   Open - Vocabulary Object Detection Early   works formulate fast mapping of new words as a   zero - shot object classification problem , which aims   to generalize from known object labels to unknown   ones ( Socher et al . , 2013 ; Frome et al . , 2013 ;   Elhoseiny et al . , 2013 ; Lazaridou et al . , 2014 ) . The   setting later extends to a localization task , referred   to as zero - shot object detection ( ZSD ) ( Bansal   et al . , 2018 ; Zhu et al . , 2019 , 2020 ; Rahman et al . ,   2020 ) . More recently , open - vocabulary object   detection ( OVD ) ( Zareian et al . , 2021 ; Gu et al . ,   2022 ; Du et al . , 2022b ; Minderer et al . , 2022 )   combines ZSD with weakly supervised object de-   tection ( WSD ) to address the unrealistic constrain   of traditional zero - shot settings . OVD assumes   the availability of coarse - grained image - caption   pairs , and attempts to generalize from limited   fine - grained annotation of object categories to   unseen ones . Nevertheless , this line of work   positions words as object categories and isolates   them from their linguistic context ( e.g. , sentences ) .   Our setup instead challenges models to perform   language modeling in human - generated captions .   6 Conclusion and Future Work   The connection between language and their refer-   ents captures the grounded meaning of words , and   an explicit treatment is key to empowering efficient   open - world language learning abilities in humans   and AI agents . This work introduces Grounded   Open V ocabulary Acquisition ( GOVA ) , a scalable   formulation to examine grounding and fast map-   ping in open - world grounded language learning .   We propose World - to - Words ( W2W ) , a novel visually   grounded language model to investigate a paradigm   where the model initially acquires grounding abil-   ity during pre - training and subsequently applies   this ability to quickly learn new words without   explicit grounding supervision . Our empirical find-   ings highlight the significance of visual grounding   in neural word acquisition . Especially , we find   that pre - trained W2Wcan serve as a foundation for   fast mapping of novel grounded words via few-   shot learning . We also conduct a comprehensive   analysis to explore potential predictors influenc-   ing the performance of vision - language models ,   revealing both consistent and surprising behaviors   with respect to human language learning patterns .   These insights pave the way for future research in   grounded language learning in the open world.532Limitations   In this work , we limit ourselves to object - centric   grounding , which ignored that language can ground   events , attributes , manners , mental states , etc . The   grounded meaning of some groundable words , es-   pecially ADVs , NUMs , VERB s , and PRON s , can not be   fully captured by the bounding boxes alone . Future   work should explore better task formulations to   study the acquisition of their grounded meanings .   An exciting future work along this line is to ex-   tend the setting from images to videos and physical   interactions with the environment , and to incorpo-   rate the rich temporal dynamics of the world for   language acquisition . In addition , we ignored the   social aspects of language learning , where children   infer the referents of words from their caregivers   through communication ( Carpenter et al . , 1998 ;   Bloom , 2000 ) . Future work could also investigate   grounded word acquisition from natural dialogue .   Ethics Statement   This project does not involve any research artifacts   generated through human subject studies . Despite   the considerable promise of W2W , it is crucial to   examine its ethical and societal implications . The   computational model relies on pre - trained language   models and extensive text - image datasets , which   could contain hidden biases that may result in fair-   ness problems within the algorithms . By recogniz-   ing and actively addressing these implications , we   aim to increase awareness among practitioners if   the model is deployed as a language - learning agent   in the future .   Acknowledgments   This work was supported in part by NSF IIS-   1949634 , NSF SES-2128623 , and by the Automo-   tive Research Center ( ARC ) at the University of   Michigan . The authors would like to thank the   anonymous reviewers for their valuable feedback .   References533534535536537AGOVA Dataset Details   A.1 Illustrated Comparison of Setting   We present an illustrated comparison of task formu-   lations related to language grounding and grounded   language learning in Figure 8 . Among these task   formulations , our Grounded Open V ocabulary Ac-   quisition ( GOVA ) task is the only one that chal-   lenges vision - language systems to perform visually   grounded and object - centric language modeling .   The formulation is natural and simple , with fun-   damental requirements on computational models   to perform masked language modeling and object   localization , and thus is particularly good for zero-   shot analysis .   A.2 Evaluation Protocols Explained   We present an adequate evaluation protocol for   grounded word acquisition in the main paper . This   section provides more in - depth explanation for   the metrics and implementation details for repro-   ducibility purposes .   Perplexity Metric Details We follow prior prac-   tice in cloze tests ( Salazar et al . , 2020 ; Jin et al . ,   2020 ) to evaluate the perplexity of a word w. We   use log pseudo - perplexity in masked language mod-   eling , defined as   logPPL(w ) = −logP(w|x , x )   However , the majority of the language models em-   ploy sub - word tokenization methods to segment   and encode text . In particular , one lexical word   can be segmented into several tokens , and different   tokenizers can lead to different tokens for the same   input . We thus introduce a tokenizer - dependent   measure for perplexity . For tokenizer T , we repre-   sent the Ntokens of word wasT(w)and   logPPL(w ) = −1   N / summationdisplaylogP(t|x , x )   IoU Metric Details we face the same challenge   as Kamath et al . ( 2021 ) where multiple refer-   ents are possible for a masked word . In a simi-   lar manner , we adopt the Any - Protocol and All-   Protocol to evaluate the grounded detection task .   Assuming nground truth bounding boxes B=   { b , b , · · · , b}andmpredicted bounding boxes   /tildewideB={/tildewideb,/tildewideb,···,/tildewiderb } . The intersection - over-   union ( IoU ) under Any - Protocols is defined as theaverage IoU of the best matching predicted bound-   ing box for each ground truth object :   IoU=1   n / summationdisplaymaxIoU(b,/tildewideb )   The intersection - over - union ( IoU ) under All-   Protocols is defined as the IoU between the joint   bounding box of ground truth and predicted bound-   ing boxes :   IoU = IoU(∪B,∪/tildewideB )   A.3 Word List   •60 words are in the seen - set , each with 80 test   cases : baby , ball , beach , bench , bike , black ,   blond , blue , boy , brown , building , car , child , dark ,   dog , dress , face , female , field , floor , food , girl ,   glasses , grass , gray , green , guitar , guy , hair , hand ,   hat , head , horse , jacket , jeans , lady , large , lit-   tle , long , man , orange , pants , person , player , red ,   shirt , sidewalk , sign , small , snow , street , striped ,   table , top , wall , water , white , woman , yellow ,   young .   •31 words are in the unseen - set , each with 50 test   cases : aged , bamboo , barefoot , brush , button ,   cafe , cheese , circular , classroom , crosswalk , di-   verse , doctor , donkey , elephant , fluffy , foreign ,   gym , heart , newborn , pan , pizza , product , se-   curity , sink , star , steep , stove , student , teacher ,   telephone , warm .   B Computational Model Details   B.1 Pre - training Objectives   Masked Language Modeling ( MLM ) . The   MLM head can be placed at multiple possible   places , and our design is an exploration after pre-   liminary experiments on smaller - scale training . We   strictly follow the setup of RoBERTa to implement   the MLM head with a two - layer MLP , based on   the implementation of huggingface . Words in   groundable phrases are masked with a probabil-   ity of 0.4 and those in non - groundable regions are   masked with a lower probability of 0.1 . For a token   selected to mask , we follow RoBERTa to assign a   probability of 80 % to replace with MASK , 10 % with   a random token , and 10 % to do nothing.538   Object Localization ( OL ) . We follow MDETR   to decode object embeddings with a three - layer   MLP to produce bounding boxes . Similar to most   prior work , we apply a filter over boxes with confi-   dence below 0.7 . In our framework , this means that   the object corresponds to the no - object label ∅(Fig-   ure 4 ) with a probability over 0.3 . We strictly fol-   low DETR to perform bipartite matching between   proposed boxes and ground truth boxes with a Hun-   garian loss . The predicted boxes are optimized to-   wards ground truth by the generalized intersection-   over - union ( GIoU ) loss and the L1 loss . Grounding . In positional alignment , the model   learns to map each object representation to tokens   in the sentence with a fixed length of 257 , which   could possibly be a MASK or an additional no - object   label∅(Figure 4 ) . The object and the token are   considered a match given a mapping probability   over 0.1 . We use a fully - connected layer to predict   the distribution over token positions with cross-   entropy loss . In semantic alignment , the model   learns to bring word embeddings closer to the ob-   ject embeddings that they ground to , and push the   unrelated pairs farther . We strictly follow the con-   trastive loss function defined in MDETR for every   object and groundable token for this purpose.539B.2 Few - shot Learning Details .   Since no bounding box or word - object mappings   annotation is available , we train W2W with only   masked language modeling ( MLM ) in few - sample   new word learning . We reduce the batch size to   8 considering the fewer number of samples , and   set the convergence criteria to a fixed number , i.e. ,   50 steps . All the rest of the experimental settings   remain the same as pre - training .   C Experiment Reproducibility   C.1 W2WImplementation Details   Our W2W model mainly consists of one cross-   modal transformer with inputs from uni - modal en-   coders from image and text domain . Specially ,   we select the ResNet-50 ( He et al . , 2016 ) pre-   trained on ImageNet from TIMMas the image en-   coder , and RoBERTa - base ( Liu et al . , 2019 ) from   huggingfaceas the text encoder . The cross-   modal encoder and two decoders each consists of   4 transformer blocks with 8 attention heads , an   input and output dimensionality of 512 , and an   inner - layer dimensionality of 2,048 . Besides , 50   learnable object queries are included to query the   cross - modal decoder to generate bounding box pro-   posals .   C.2 Hyper - parameter Decisions   We include the major hyper - parameter tuning deci-   sions for reproducibility purpose . For more details ,   please refer to the supplementary codes .   • Learning Rate :   – Image Encoder : frozen   – Text Encoder : 1×10   – Multi - modal Transformer : 1×10   • Batch Size : 128   • Pre - training Loss Coefficients :   – MLM Loss : 32   – Cross Entropy for Positional Alignment : 1   – Contrastive Loss for Semantic Alignment : 1   – L1 Localization Loss : 5   – GIoU Localization Loss : 2   • Few - shot Learning :   – Batch size : 8   – Other Hyper - parameters : Same as Pre - trainingC.3 Computational Resources   Our W2Wmodels is pre - trained on 8 NVidia A40   GPUs . With mixed - precision pre - training and a   batch size of 128 , W2Wwas trained for 150,000   steps where each step takes about 1.4 second .   C.4 Evaluation on GOVA   W2W For our proposed W2Wmodel , given a GOVA   test , with its corresponding image and textual cloze   pair passing into the model , the bounding box pre-   dictions are generated by keeping only the bound-   ing box proposals that are mapped to at least one   masked token within the cloze , while the masked   token prediction results are directly decoded from   its language modeling head .   VisualBERT For the “ Detect - and - Recognize ”   baseline model VisualBERT , we use phrase-   grounding fine - tuned version of VisualBERT to   perform object localization , and , as it lacks the lan-   guage modeling head , another vanilla pre - trained   VisualBERT to perform mask token prediction .   Specifically , for the bounding box localization part ,   we treat it as a standard phrase grounding task and   follow ( Li et al . , 2019 ) to select the top-1 bound-   ing box prediction in the last masked token as the   output .   ViLT+MDETR For the “ Produce - and - Localize ”   baseline model ViLT + MDETR , in stage one , we   feed the input image and text into ViLT , collect-   ing its top-1 cloze token prediction result . Then , at   stage two , the input image and ViLT - completed text   are fed into MDETR , performing phrase - grounding   to localize the object associated with the original   cloze . Finally , the cloze token prediction result   from ViLT together with the bounding box propos-   als from MDETR are used for GOVA evaluation .   D Addendum to Results   D.1 Ablation Study   We performed an ablation study on several W2W   model variants to pinpoint what makes our W2W   model effective . These included models without   language encoder initialization ( w/o Init ) , without   grounding objective ( w/o G ) , without any object-   centric representation ( w/o O ) , and a text - only   setup without any vision input ( w/o V ) . For consis-   tency , we control the number of transformer layers   and the number of parameters for each variation .   Despite tweaking various hyperparameters , no sig-   nificant improvements were observed . As a result,540we retained the same hyperparameters as in the W2W   model .   •w / o G : This refers to the model variant without   grounding loss , as has already been described in   Section 3.2 ;   •w / o O : This variant excludes all object - centric   representations , retaining only the masked lan-   guage modeling ( MLM ) objective . With this   model , the object decoder transformer is unnec-   essary , thus no grounding nor localization is per-   formed . Instead , we consolidate all 12 trans-   former blocks into the multi - modal encoder and   directly attach the MLM objective to it .   •w / o V : This text - only model operates without   any vision input or supervision , reducing it to a   unimodal language model ( RoBERTa ) with 12   additional transformer blocks .   Following the analysis of Chang and Bergen   ( 2022 ) in unimodal language models , we present   the KL - Divergence between the model predictions   and the unigram distribution in Figure 9 . An imme-   diate observation is that all variants converge to the   shallow unigram statistics at around 10steps of   pre - training . This aligns with the findings of Chang   and Bergen ( 2022 ) that unimodal language mod-   els would converge to unigram before acquiring   more complicated contextual representations . We   noticed that in both text - only and W2W cases   where MLM is the only pre - training objective , the   models tend to stay around the unigram word dis-   tribution even with 10steps of training . How-   ever , variants with an object - centric representation   quickly departed from the unigram distribution .   Comparatively , models with language model ini-   tialization moves quickly away from the unigram   distribution , and models with a grounded objective   have a marginally faster deviation . These results   confirm that vision - language models can benefit   from unimodal pre - training on a large corpus , and   that performing language modeling upon object   representations is crucial . We note that we compare   the KL - Divergence from unigram only to under-   stand the models ’ behaviors , and the metric itself   does not serve as an evaluation of a system ’s perfor-   mance in grounded open vocabulary acquisition .   D.2 Addendum to Results in Multi - Class   Incremental Learning   We present additional results in Table 6 .   D.3 Learning New Words through One - Class   Incremental Learning .   We further perform a more controlled study with   a word - specific one - class incremental learning set-   ting . The pre - trained model is tasked to acquire one   single unseen word from a few - shot learning ses-   sion with |V|= 1 . The results of this section   are obtained from the test immediately following   the new session . We present the test result in Ta-   ble 7 . Again , we observe that with as few as 8 sam-   ples , W2Wcan achieve a satisfyingly low grounded   perplexity . In the majority of the cases , W2Wdemon-   strates the better ability to acquire unseen words   over the groundless baseline.541542ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 7 , Limitations   /squareA2 . Did you discuss any potential risks of your work ?   This study does not contain any human subjects or human studies . The study proposes a problem   formulation and a computational framework , which is not deployable to any real - world applications   in the foreseeable future .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 2   /squareB1 . Did you cite the creators of artifacts you used ?   Section 2.4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Will be included along with the code release   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Will be included along with the code release   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 2 and Appendix A   C / squareDid you run computational experiments ?   Section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix C543 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix C   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   The study involves a pre - training framework which is not economically feasible for repeated runs .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix C   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.544