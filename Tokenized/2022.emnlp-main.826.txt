  Qiyu Wu , Chongyang Tao , Tao Shen , Can Xu , Xiubo Geng , Daxin JiangThe University of Tokyo , Tokyo , JapanMicrosoft Corporationqiyuw@g.ecc.u-tokyo.ac.jp{chotao,shentao,caxu,xigeng,djiang}@microsoft.com   Abstract   Learning sentence embeddings in an unsuper-   vised manner is fundamental in natural lan-   guage processing . Recent common practice   is to couple pre - trained language models with   unsupervised contrastive learning , whose suc-   cess relies on augmenting a sentence with a   semantically - close positive instance to con-   struct contrastive pairs . Nonetheless , exist-   ing approaches usually depend on a mono-   augmenting strategy , which causes learning   shortcuts towards the augmenting biases and   thus corrupts the quality of sentence embed-   dings . A straightforward solution is resort-   ing to more diverse positives from a multi-   augmenting strategy , while an open question re-   mains about how to unsupervisedly learn from   the diverse positives but with uneven augment-   ing qualities in the text ﬁeld . As one answer ,   we propose a novel Peer - Contrastive Learning   ( PCL ) with diverse augmentations . PCL con-   structs diverse contrastive positives and nega-   tives at the group level for unsupervised sen-   tence embeddings . PCL performs peer - positive   contrast as well as peer - network cooperation ,   which offers an inherent anti - bias ability and an   effective way to learn from diverse augmenta-   tions . Experiments on STS benchmarks verify   the effectiveness of PCL against its competitors   in unsupervised sentence embeddings .   1 Introduction   Sentence embedding learning , which aims at deriv-   ing semantically meaningful ﬁxed - sized vectors   for sentences , is a natural language processing   ( NLP ) technique of great signiﬁcance , especially   for time - sensitive downstream tasks ( Reimers and   Gurevych , 2019 ) . Recently , contrastive learning   ( CL ) is proven effective to learn representation ( Wu   et al . , 2018 ; Tian et al . , 2020 ; He et al . , 2020 ) and   Table 1 :   substantially improve its performance ( Yan et al . ,   2021 ; Gao et al . , 2021 ) when coupling with pre-   trained language models ( PLMs ) . The main idea   of contrastive learning for sentence embedding is   pulling semantic neighbors together and pushing   semantic non - neighbors apart ( Hadsell et al . , 2006 ) ,   which naturally requires effective contrastive pairs .   As effective contrastive pairs are usually scarce   and require much human effort to collect , how to   learn sentence embeddings in an fully unsupervised   manner has become a challenging yet attractive re-   search area ( Wang et al . , 2021 ; Giorgi et al . , 2021 ) .   The key to unsupervised contrastive learning for   sentence embedding is to augment a given anchor   sentence with an effective positive instance to con-   struct the pairs . Hence , many efforts have been   made to design augmentation methods by adding   noises or using heuristics , which mainly fall into   two categories in terms of augmentation format –   discrete andcontinuous . The former operates di-   rectly on words or n - grams in the sentence , e.g. ,   synonym substitution ( Su et al . , 2021b ) , shufﬂing   and word deletion ( Yan et al . , 2021 ) . The latter   operates on latent embeddings derived by neural   encoder(s ) , e.g. , SimCSE with twice dropouts ( Gao   et al . , 2021 ) . However , these existing approaches   usually depend on a mono - augmenting format ( i.e. ,   either discrete or continuous ) with a limited number   of augmenting strategies , which suffer from learn-   ing shortcuts ( Ilyas et al . , 2019 ; Du et al . , 2021 ) 12052   towards the augmenting biases and thus corrupt   the quality of learned embeddings . For example ,   learning shortcuts caused by discrete augmenting   biases are shown in Table 1 , and SimCSE based   solely on dropout in continuous format is biased   towards the sentence length ( Wu et al . , 2021b ) .   To prevent the learning shortcuts caused by the   potential biases from the mono - augmenting strat-   egy , one straightforward solution coming into our   mind is that we can consider more diverse aug-   mentations for a given sentence in both continuous   and discrete formats . Besides learning from di-   verse instances for inherent anti - bias ability , it can   also bring a great opportunity for more effective   learning . In particular , controlling the qualities of   the noisy and heuristic augmentations for different   sentences is almost impossible . As illustrated in   Figure 1(a ) , the resulting contrastive instances may   become ineffective and even poisonous for conven-   tional CL . Nonetheless , diverse instances from var-   ious augmentation strategies can notably improve   the possibility of at - least - one effective positives in   the contrastive instances , so how to leverage the   rich relations among the diverse augmentations for   more effective CL is worth exploiting .   To this end , we propose a brand - new Peer-   Contrastive Learning ( PCL ) with diverse augmen-   tations , and an illustration of its overall framework   is shown in Figure 1(b ) . Firstly , PCL not only per - forms the vanilla positive - negative contrasts but   also takes the opportunity to learn the rich struc-   tured relations among the diverse positives ( i.e. ,   peer - positive ) to highlight the more possibly effec-   tive ones . Then , to learn the structured relations in   a fully unsupervised manner , we propose a coop-   erative learning framework consisting of two peer   embedding networks ( i.e. , peer - network ) . The two   networks learn from each other to prevent error re-   inforcement in sole - network and achieve a common   agreement from different views ( as shown in Fig-   ure1(c ) ) . Consequently , the sentence embedding   network is equipped with ( i ) anti - bias abilities by   CL on the diverse augmentations and ( ii ) improved   effectiveness by the unsupervised PCL , leading to   a high quality of sentence embeddings .   We conduct experiments on 7 standard semantic   textual similarity ( STS ) tasks ( Agirre et al . , 2012 ,   2013 , 2014 , 2015 , 2016 ; Cer et al . , 2017 ; Marelli   et al . , 2014 ) to evaluate PCL . Results demonstrate   that PCL signiﬁcantly outperforms state of the art   on 7 STS tasks . Typically , PCL achieves a 2.85 %   improvement over the previous best approach in the   averaged Spearman ’s correlation of 7 STS tasks in   the BERTsetting . PCL also outperforms previ-   ous approaches across different PLMs initialization   and model sizes . Moreover , ablation study and anal-   ysis show that the two proposed components , i.e. ,   peer - positive contrasts and peer - network coopera-   tion , are both capable of improving unsupervised   sentence embedding learning.120532 Peer - Contrastive Learning ( PCL )   This section begins with a formal deﬁnition of un-   supervised sentence embeddings , followed by de-   tailed formulations of our PCL with diverse aug-   mentations ( § 2.1and § 2.2 ) . Lastly , we will elab-   orate on our training and inference procedure for   unsupervised sentence embeddings ( § 2.3 ) .   Unsupervised Sentence Embedding . Given a   sentence x∼X , the target of this task is to learn   a neural network f(parameterized by θ ) without   any human - labeled data . Then , the network can be   applied to xand derive a dense real - valued vector   representation , i.e. , h = f(x)∈R. Conse-   quently , hcan be used to represent the seman-   tics of xand fulﬁll downstream sentence - related   tasks , e.g. , semantic textual similarity . Thereby ,   this task depends on the designs of unsupervised   ( a.k.a self - supervised ) objectives based on Xto   learn feffectively .   2.1 Contrastive Representation Learning   The recent common practice of representation   learning in an unsupervised manner is contrastive   learning ( Zhang et al . , 2020 ; Yan et al . , 2021 ; Gao   et al . , 2021 ) , which aims to learn effective repre-   sentations to pull similar instances together and   push apart the other instances . Thereby , compared   to supervised contrastive learning that has already   offered contrastive pairs , how to augment the given   anchor ( e.g. , an image and sentence ) with effective   positive and negative instances to construct the con-   trastive pairs is critical in the unsupervised scenario .   More recently , a simple contrastive learning frame-   work ( SimCLR ) is proposed in visual representa-   tion learning ( Chen et al . , 2020 ) , which constructs   positive instances by different views ( e.g. , chop )   of an image then learn to minimize the following   InfoNCE loss .   L(X , δ;θ ) = ( 1 )   −E / bracketleftBigg   loge   /summationtexte / bracketrightBigg   ,   where δ(·)denotes using a different view of the   image xas the positive instance during visual   contrastive learning , xdenotes negative instances   against xto construct contrastive pairs with δ ( · ) ,   ands[·,·]denotes a similarity metric between two   dense vectors . And Ldenotes this loss function   is optimized w.r.t the subscript θ . It is also notewor-   thy that x∼Xis usually implemented by usingother in - batch instances during mini - batch SGD   ( a.k.a in - batch negatives ) .   However , when switching to unsupervised sen-   tence embedding , augmenting an input sentence by   a fully random chop or permutation may become   very intractable . This is because these operations   are most likely to destroy the original sentence in   both semantics and syntax and cause trivial posi-   tive augmentations . Hence , many research efforts   have been made to design δfor effective positive   augmentations in the NLP community . These ef-   forts mainly fall into two categories in terms of   augmentation format – ‘ discrete ’ and ‘ continuous ’ .   Discrete augmentation format denotes operating di-   rectly on the inputted sentence , where δis deﬁned   as word deletion , shufﬂing ( Yan et al . , 2021 ) , back   translation ( Xie et al . , 2020 ) , etc . In contrast , con-   tinuous ones operate on hidden states or network   parameters , where δis deﬁned as network twice   dropout ( Gao et al . , 2021 ) , etc .   Nonetheless , compared to visual contrastive   learning that barely introduces new data distribu-   tion for the positive instances , such heuristic aug-   mentation methods in the text ﬁeld cause shortcut   learning ( Ilyas et al . , 2019 ; Du et al . , 2021 ) – each   method exposes the learning procedure to potential   biases towards the augmented instances and thus   corrupts the quality of learned embeddings . There-   fore , existing unsupervised contrastive sentence   embedding works usually depend on the limited-   or even mono - augmenting methods for their posi-   tive instances and inevitably suffer from the biases   in the positive instances .   2.2 Contrast - Cooperation with Peers   To prevent learning shortcuts caused by the po-   tential biases from mono - augmenting strategy and   exploit rich relations among diverse augmenta-   tions for more effective positives , we propose a   brand - new contrastive learning method , called peer-   contrastive learning ( PCL ) . Besides the vanilla   contrastive objective , it contains a novel ‘ contrast-   cooperation ’ learning mechanism , which we will   detail in the following .   2.2.1 Multi - Augmenting Strategy   First , we adopt a multi - augmenting strategy for ex-   tensively diverse augmentations . Given a sentence   x∼X , it considers extensive augmentation meth-   ods from both continuous and discrete perspectives.12054This can be formally written as   ∆ = { δ|δ∈∆∪∆ } , ( 2 )   where ∆denotes a set of multiple augmentation   methods from both the continuous ∆set and   discrete ∆set , and |∆|=K. Then , we can   obtain diverse augmented positives by applying ∆   to a sentence x∼X , i.e. ,   ˆX={x = δ(x)|δ∈∆ } . ( 3 )   The contrastive sentence embedding based on   diverse positives can mitigate the biases towards   mono - augmenting strategy , but it comes with a   double - edged sword . That is , the ˆx∼ˆXvaries   a lot with many factors ( e.g. , input sentence x   and augmentation method δ ) , making it hard to   control the quality of each x. To one extreme ,   one augmentation can become ineffective and even   poisonous if its semantics is largely changed and   thus corrupt the model .   2.2.2 Contrast among Peer Positives   To effectively learn from the uneven qualities of   the augmented positives , we propose a brand - new   peer - contrastive learning framework that not only   performs the vanilla positive - negative contrast but   a positive - positive contrast . This is because our   diverse augmentations provide a great opportunity   to model rich structured relations among the posi-   tives and improve the probability of ‘ at - least - one ’   effective positive in ˆX. And the positive - positive   contrast can mimic ‘ peer - competition ’ to highlight   more likely effective positives but weaken the oth-   ers ’ effects by suppressing them .   Formally , we ﬁrst derive a group - wise probabil-   ity distribution by contrasting the anchor xwith   both diverse positives ˆXand in - batch negatives   x∼X∧j̸=i . That is ,   p(x):= P - Cf ( x,∆;θ , θ ) = ( 4 )   softmax ( { s[f(x ) , f(ˆx)/τ]}+   { s[f(x ) , f(x)/τ ] } ) ,   where ‘ + ’ here denotes a union of two sets . Identi-   cal to the vanilla contrastive sentence embedding   ( Gao et al . , 2021 ) , we also leverage a softmax   normalization to fulﬁll peer - contrast among aug-   mented positives . Please note we introduce θand   θfor clear deliveries in the remaining sections ,   and the two parameters here can be either tied ( i.e. ,θ = θ = θ ) or not . Although using the aug-   mented positives to ‘ compete ’ each peer sounds   attractive for contrastive learning , one critical ques-   tion remains about how to learn merely from effec-   tive positives and guide the positive - peer contrasts   p(x)in a fully unsupervised way .   2.2.3 Cooperation across Peer Networks   We propose a cooperative learning framework to   learn contrasts among the augmented positives . It   contains two peer embedding networks , and the   two networks learn from each other to prevent er-   ror reinforcement in sole - network and achieve a   common agreement from different views . Specif-   ically , we ﬁrst build a peer network θwhich acts   like a momentum encoder ( He et al . , 2020 ) to coop-   eratively learn with θ . Here , θandθcan be untied   or even heterogeneous . Then , we present the loss of   the momentum - like cooperative learning , which is   a combination of two Kullback – Leibler divergence   losses . That is   L(X,∆;θ , θ ) = ( 5 )   E[KL[p(x),p(x)]+   KL[p(x),p(x ) ] ] .   We call this ‘ momentum - like ’ since θis not   strictly a history of θfor more different views   but depends on the second KL term to pre-   vent signiﬁcant divergence from θ . Meanwhile ,   the ﬁrst KLterm is to reach an agreement be-   tween the main network θand its peer network   θ . This ‘ learning - from - agreement ’ paradigm , in-   cluding mutual - distillation ( Zhang et al . , 2018 )   and denosing - by - agreement ( Wei et al . , 2020 ) , is   proven effective in improving performance and   learning from label noises by prior supervised   works . In contrast , we hold a distinct motivation   that the implicit relations in a group of diverse   positives and in - batch negatives are expected to un-   supervisedly match each peer embedding network   with another view ( e.g. , structures and parameters ) .   Remark . The meaning of ‘ peer ’ has two folds :   ( i ) It denotes that we want to learn the rich struc-   tured relations among the diverse augmented posi-   tives to highlight the effective ones ; ( ii ) It involves   a cooperative learning framework based on peer   networks for modeling positive - positive contrasts   to achieve PCL with diverse augmentations.120552.3 Training and Inference   Training Objective . We write the loss as a com-   bination of ( i ) our proposed contrast - cooperation   learning for both θandθsimultaneously to high-   light more effective positives and ( ii ) vanilla con-   trastive learning that is applied to θandθsepa-   rately and based on our diverse augmentations ∆   for their strong anti - bias initializations . That is ,   L = L(X,∆;θ , θ)+ ( 6 )   β / summationdisplay / bracketleftBig   L(X , δ;θ ) + L(X , δ;θ)/bracketrightBig   ,   where βis a hyperparameter to control if the train-   ing inclines to vanilla CL for the anti - bias purpose .   Hence , βcould be annealing to provide strong unbi-   ased initializations for different views at the begin-   ning and then focus on contrast - cooperation with   peers for more effective learning .   Inference . Due to the symmetrical learning   paradigm , we empirically found θandθachieve   comparable performance in our pilot experiments .   Nonetheless , we only use the main embedding net-   work θrather than ensembles them to encode each   sentence for fair comparisons with its competitors .   3 Experiments   3.1 Unsupervised Corpus and Benchmark   We train and evaluate our model in a fully unsuper-   vised manner . Following Gao et al . ( 2021 ) , we train   our model on 10randomly sampled sentences   from Wikipedia English . We evaluate our model   on the semantic textual similarity ( STS ) tasks with-   out using any STS training data . We report re-   sults on 7 datasets , namely the STS benchmark   ( STSb ) ( Cer et al . , 2017 ) the SICK - Relatedness   ( SICK - R ) dataset ( Marelli et al . , 2014 ) and the STS   tasks 2012 - 2016 ( Agirre et al . , 2012 , 2013 , 2014 ,   2015 , 2016 ) ( STS12 - STS16 ) . These datasets pro-   vide a gold standard semantic similarity between   0 and 5 for each sentence pair , which include texts   from various domains , and we obtain them from   the SentEval toolkit ( Conneau and Kiela , 2018 ) .   3.2 Implementation of PCL   Augmentation Strategies In this paper we uti-   lize ﬁve unsupervised augmentation strategies that   are commonly adopted in previous works ( Wei   and Zou , 2019 ; Yan et al . , 2021 ; Gao et al . , 2021 ) .Augmentations from discrete perspectives ∆in-   cludes : 1 ) Shufﬂed Sentence ( SS ) shufﬂes the posi-   tion of words in the sentence ; 2 ) Inverted Sentence   ( IS ) inverts the original sentence as the augmented   sample ; 3 ) Words Repetition ( WR ) duplicates part   of words and randomly insert them into the original   sentences ; 4 ) Words Deletion ( WD ) deletes part of   words in the sentences . The augmentations from   the continuous perspective ∆include Dropout   ( DP ) . It generates augmentation instances in the   embedding level by passing the original sentence   again into the encoder with different dropout masks .   More implementation details about augmentation   are presented in § Adue to the page limit .   Network Implementation We initialize the net-   works θandθwith the PLMs checkpoint down-   loaded from Huggingface ’s Transformersof   BERT ( Devlin et al . , 2019 ) or RoBERTa ( Liu et al . ,   2019 ) . The encoder consists of 12 and 24 Trans-   former layers for the base and large model , respec-   tively . The hidden size is set to 768 and 1024 , and   the number of attention heads is set to 12 and 16   for base and large models , respectively . We choose   the representation of the [ CLS ] token as the em-   bedding of the input sentence . The hyperparameter   βis set to 1 for training simplicity without tuning ,   and the number of augmentations Kis 9 for base   models . Due to the computation resource limita-   tion , particularly for large models , we set Kto 4 ,   and the two networks θandθare tied , in which   the cooperative learning is performed between the   two passes through the network .   Training Setups . We follow common practices   and carry out preliminary grid search on the devel-   opment set of STSb to decide the hyper - parameter   conﬁguration . The learning rate is set to 3e-5 for   base models and 1e-5 for large models , respec-   tively . Except for learning rate , We use the same   training hyper - parameters for all experiments with   the batch size of 64 and the maximum length of   32 . The temperature parameter τis set to 0.05 , and   the dropout probability is set to 0.1 . We train our   model for 1 epoch and evaluate the model on the   STSb development set every 125 steps , and keep   the best checkpoint by following Gao et al . ( 2021 ) .   Evaluation Setups . We evaluate PCL on 7 STS   tasks , including STS12 - STS16 , STSb , and SICK - R   as introduced in § 3.1 . No training data of STS   tasks are used during training and evaluation . Gao12056   et al . ( 2021 ) has studied the evaluation settings   for sentence embedding . We adopt their sugges-   tions and follow the standard settings in Sentence-   BERT ( Reimers and Gurevych , 2019 ) . Speciﬁcally ,   we do not train an additional regressor for STSb   and SICK - R , use Spearman ’s correlation as the   metric , concatenate all tasks and report the overall   Spearman ’s correlation . To fairly compare with   previous approaches , we use the evaluation scripts   released by Gao et al . ( 2021 ) . Moreover , we train   our model for ﬁve times with different random   seeds and report the average of these ﬁve results .   We also evaluate PCL on 7 transfer tasks ( Con-   neau and Kiela , 2018 ) . PCL achieves competi-   tive performance and the detailed results are pre-   sented in Appendix C.2 . As mentioned in previous   works ( Reimers and Gurevych , 2019 ; Gao et al . ,   2021 ) , the main goal of sentence embeddings is to   cluster semantically similar sentences . Hence we   only take STS as the main results in this paper.3.3 Competitive Baselines   We compare our model with previous state - of - the-   art unsupervised sentence embedding approaches ,   including basic embedding approaches ( e.g. aver-   age of GloVe ( Pennington et al . , 2014 ) , BERT ( De-   vlin et al . , 2019 ) or RoBERTa ( Liu et al . , 2019 )   embeddings ) and contemporary contrastive learn-   ing approaches ( e.g. IS - BERT ( Zhang et al . , 2020 ) ,   ConSERT ( Yan et al . , 2021 ) , SG - OPT ( Kim et al . ,   2021 ) , Contrastive Tension ( Carlsson et al . , 2021 ) ,   DeCLUTR ( Giorgi et al . , 2021 ) , Mirror - BERT ( Liu   et al . , 2021b ) and SimCSE ( Gao et al . , 2021 ) ) .   SGPT ( Muennighoff , 2022 ) and Sentence - T5 ( Ni   et al . , 2021 ) are proposed with new paradigm and   far larger models , which underperform with compa-   rable model size . Trans - Encoder ( Liu et al . , 2021a )   proposes a cooperative method with in - domain   pairwise data for mutual beneﬁts of bi- and cross-   encoder , making the results incomparable . Please   refer to § Bfor more details.120573.4 Main Quantitative Results   Experimental results on STS tasks are shown in   Table 2 . We can ﬁnd that our PCL signiﬁcantly   outperforms the previous best result on all seven   tasks as well as the average STS score with a large   margin compared to the baseline methods based   on BERTor RoBERTaPLMs . Speciﬁcally ,   PCL improves the previous best result on average   STS score from 76.25to78.42for BERTand   76.57to77.91for RoBERTa , respectively . As   SimCSE ( Gao et al . , 2021 ) did not report their   performance on BERT , we compare all large   models together , and the results are shown in the   last rows of the table . We can observe that PCL   outperforms the best result on all tasks apart from   the STS16 . Despite this , our PCL still obtain an im-   provement from 78.90to80.14on the average STS   score . Our PCL achieves more signiﬁcant improve-   ment over the base models than the large models ,   And even so , PCL still outperforms SimCSE on al-   most all tasks in large models and all tasks in base   models , which shows that PCL is effective across   different model sizes and different types of PLMs .   3.5 Analysis of Diverse Augmentations   Diversity and the number of augmentations are two   crucial factors of PCL . In this section , we test the   performance of PCL with varying Kand diversity .   For PCL and all variants of PCL in this section , we   train them for 5 times with different random seeds ,   and take the average as the ﬁnal results .   The number of augmentations . To mitigate the   model bias towards the mono - augmenting strat-   egy , we propose to augment the input sentence   with a group of positive instances . The number of   augmentations Kis a crucial hyper - parameter in   this framework . To check if the performance of   PCL is sensitive to K , we conduct experiments on   PCL - BERTwith varying Kon 7 STS tasks and   report the average STS score . We keep the diver-   sity of augmentations ∆as much as possible when   K > 1 . As shown in Figure 2 , the performance   of PCL maintains an upward trend with increasing   K. This indicates that multiple augmentation strat-   egy improves unsupervised sentence embeddings   compared with learning with mono - augmenting   strategy . This supports our motivation that con-   trastive learning with mono - augmenting strategy   causes learning shortcuts . More detailed results on   all 7 STS tasks are presented in § C.3 .   The diversity of augmentations . Another crit-   ical factor is the diversity of augmentations . We   ﬁxK= 9and reduce the diversity of augmenta-   tions ∆to check if PCL is sensitive to the diver-   sity . We conduct experiments on PCL - BERT   with K= 9 but only use onetype of augmen-   tation strategy from discrete and continuous per-   spective , respectively . In other words , we keep at   least one DP augmentation for all variants . We   compare PCL with ﬁve mono - augmentation vari-   ants that are denoted as PCL , PCL , PCL ,   PCLand PCL , respectively . The details of   augmentation strategies are introduced in § A. Av-   erage Spearman ’s correlation scores of 7 STS tasks   are shown in the Figure 3 . Experimental results   show that PCL signiﬁcantly outperforms its mono-   augmenting variants , even keeping the Kconstant ,   indicating a better generalization . This supports our   motivation that PCL with diverse augmentations   can mitigate the shortcut learning biased towards   mono - augmenting strategy . Particularly , SimCSE   utilizes dual - dropout to construct the contrastive   pairs , hence the PCLvariant ( 9 positive instances   generated by the dropout ) can be regarded as Sim-   CSE w/ 9 augmented positive samples . Our pro-   posed contrasts and cooperation among peers im-   prove SimCSE from 76.25 to 77.14 , but the score   is still lower than PCL with a large margin . This is   another piece of evidence that shows the advantage   of the diversity of augmentations . The detailed   results on all 7 STS tasks are presented in § C.3 .   3.6 Ablation Study   We ﬁrst check the impact of the proposed two   components of PCL , peer - network cooperation and   peer - positive contrast , i.e. , the two terms in Equa-   tion6respectively . We designed two variants of   PCL on PCL - BERTby removing the coopera-   tion loss and contrast loss , which are denoted as   PCLand PCLrespectively . To ensure the net-   works have the essential ability to learn sentence   embeddings , we keep the contrastive loss with a12058   single DP augmentation for PCL , which is equal   to the setting in Figure 2when K= 1 . We train   each variant for ﬁve times with different random   seeds and take the average of these seeds as the   ﬁnal results . As in Table 3 , the average scores of   PCL drop by 1.02and1.08when removing the co-   operation loss and contrast loss , respectively . This   indicates that our proposed peer cooperation and   peer contrast are both beneﬁcial to unsupervised   learning of sentence embeddings . Among the two   components , the peer cooperation loss plays a more   important role as it incorporates contrasts among   peer positives and peer networks , enabling an inher-   ent anti - bias ability and an effective way to learn   from diverse augmentations .   Fixed peer encoder vs. trainable peer encoder .   Particularly , We are also curious about the impact   of ‘ learning from agreement ’ ( i.e. , the second term   in Equation 5 ) in the cooperative learning objec-   tive . Therefore , we further test additional vari-   ants of PCL with a ﬁxed peer encoder , denoted   as PCL . Speciﬁcally , we download a check-   point of SimCSE as the peer encoder but ﬁx its   parameters while training . Experimental results   show that the performance of PCL drops with   a large margin on STS12 , STS13 , STS14 , STS15 ,   STSb , and the average STS . The reason may be   that although SimCSE is by far the best practice   of sentence embeddings , it is still biased towards   a mono - augmenting strategy . Hence , cooperative   learning with a biased peer network can be harmful   to the network with diverse augmentations . This   also indicates that it is necessary to simultaneously   update the two peer networks and learn the agree-   ment between them in PCL . Furthermore , there   can be a considerable discrepancy between the em-   bedding spaces produced by two methods , which   hinders the cooperative training of two networks .   4 Related Works   Unsupervised Sentence Embedding . Common   practice of unsupervised sentence embedding is   to take the average of pre - trained word embed-   dings ( Mikolov et al . , 2013 ; Pennington et al . ,   2014 ) PLMs , like BERT ( Devlin et al . , 2019 ) or   RoBERTa ( Liu et al . , 2019 ) , Wu et al . ( 2021a ) takes   the average of word embeddings as context embed-   ding to enhance the language pre - training . Other   works also take the [ CLS ] embedding from the   last layer of PLMs with post - processing ( Li et al . ,   2020 ; Su et al . , 2021a ) . Some works ( Kiros et al . ,   2015 ; Logeswaran and Lee , 2018 ; Hill et al . , 2016 )   directly train a deep model for sentence embed-   dings using co - occurrence information . Recent ap-   proaches couple PLMs with CL ( Zhang et al . , 2020 ;   Yan et al . , 2021 ; Kim et al . , 2021 ; Carlsson et al . ,   2021 ; Giorgi et al . , 2021 ; Gao et al . , 2021 ; Xie   et al . ,2022 ) with a particular single strategy to con-   struct contrastive pairs . It is straightforward to ex-   tend mono - augmentation into multi - augmentation   to learn expressive representations . For example ,   CLEAR ( Wu et al . , 2020 ) uses various token / span   manipulations for noise - invariant representations   while Mirror - BERT ( Liu et al . , 2021b ) employs   several fast augmentation strategies . However ,   these methods usually take the augmented positives   equally , regardless of the uncontrollable qualities .12059Thereby , we take a step further to consider the   contrasts among the augmented positives to ﬁgure   out which augmentation is relatively reasonable .   This is achieved by our novel cooperative learning   method with peer networks . More related to our   work , ESimCSE ( Wu et al . , 2021b ) found learning   on dual - dropout causes sentence length bias so it   employs another augmentation strategy , i.e. , word   repetition , to prevent the length bias . However ,   word repetition introduces learning shortcut by   itself , not to mention it makes the sentence unnatu-   ral and even semantics - wrong . To circumvent this   dilemma , we propose exhaustive augmentations   to ensure “ at - least - one ” true positive and reduce   learning shortcuts by complementary augmenting   strategies . By doing so , PCL achieve a better per-   formance on STS tasks . Please refer to § D.1for   more discussion details .   Contrastive Learning . The main idea of CL is   to pull semantic close neighbors close and push   non - neighbors apart ( Hadsell et al . , 2006 ; Zbontar   et al . , 2021 ) . It is shown to be a successful way to   learn representation . Approaches in computer vi-   sion ( CV ) ( Chen et al . , 2017 ; Wu et al . , 2018 ; Tian   et al . ,2020 ; He et al . , 2020 ; Zbontar et al . , 2021 ) try   to make an image to be invariant to transformations   on itself , while remaining discriminative to other   images . More references in CV are discussed in   the recent survey ( Jaiswal et al . , 2021 ) . CL is also   coupled with PLMs to learn sentence embeddings .   But , recent works ( Xiao et al . , 2021 ) argue that   learning invariance to particular transformations   may be harmful to the robustness of the model .   This also supports our idea of leveraging diverse   augmentations to improve unsupervised sentence   embeddings from another angle .   Learning from Agreement . Another line of   work close to ours is learning from agreement , e.g. ,   Decoupling ( Malach and Shalev - Shwartz , 2017 ) ,   Co - teaching ( Han et al . , 2018 ; Yu et al . , 2019 ) ,   and mutual CL Yang et al . ( 2021 ) . This paradigm   has been proven effective in improving model per-   formance and learning with label noises by prior   fully - supervised works . As text data is discrete and   compositional , qualities of multiple augmentations   can be uneven , which may corrupt the generaliza-   tion of sentence embeddings . Besides widely used   regularization like dropout ( Srivastava et al . , 2014 )   and weight decay ( Krogh and Hertz , 1991 ) , we con-   sider learning from agreement paradigm to offer a   robust way to learn from our diverse positives.5 Conclusion   In this paper , we propose a brand - new contrastive   learning framework , dubbed as peer - contrastive   learning ( PCL ) , to capture rich relations among   diverse positive peers and highlight effective pos-   itives , which are learned by cooperative learning   by peer networks . Besides inherent anti - bias abil-   ity by diverse augmentations , it can learn from   unsupervised corpus more effectively than vanilla   contrastive in the text ﬁeld . Experiments show that   the number and diversity of augmentations are cru-   cial to PCL . Ablation study also shows that the two   components of PCL , i.e. , peer - positive contrast and   peer - network cooperation , are both beneﬁcial to   unsupervised CL for sentence embeddings .   Limitation . We also recognize that our PCL   framework has its certain limitations : ( i ) Due to   peer positives and encoders , our framework needs   higher ( ~3 × , i.e. , 2.5 GPU - hours for base mod-   els ) computation overheads compared to vanilla   CL . Nonetheless , the acceptable extra overhead   and same inference makes our framework still prac-   tical and scalable . ( ii ) As a work addressing the   general shortcut learning problem in a fundamen-   tal task , the proposed PCL is only evaluated on   resources in English . It can be further extend to   more applications such as in low - resource or other   languages . ( iii ) The performance of our framework   relies on the choice of augmentation methods , and   it is hard to strictly claim which combination of   the methods is optimal except experimental veri-   ﬁcation . Although we have analysed the effect of   varying combinations of augmentations with ex-   tensive experiments , we can only select several   widely - adopted augmentations to evaluate the gen-   eral effectiveness of our framework .   Ethics Statement   This paper investigates unsupervised contrastive   learning for sentence embedding . There will not   be any ethical problems or negative social conse-   quences from the research . The data in this paper   are all publicly available and are widely adopted   by researchers . The proposed method does not   introduce ethical bias in the data .   Acknowledgement   We thank Yoshimasa Tsuruoka , Ryokan Ri and   Jing Zhou for valuable discussions . Qiyu Wu   was supported by JST SPRING , Grant Number   JPMJSP2108.12060References120611206212063A Augmentation Strategies   We propose diverse augmentation strategies for   each sentence . In this paper we utilize ﬁve unsuper-   vised augmentation strategies that are commonly   adopted in previous works ( Wei and Zou , 2019 ;   Yan et al . , 2021 ; Gao et al . , 2021 ) . Augmentations   from discrete perspectives ∆includes : 1 ) Shuf-   ﬂed Sentence ( SS ) shufﬂes the position of words in   the sentence . SS corrupts the order of the original   sentence but preserves the semantic information of   words ; 2 ) Inverted Sentence ( IS ) inverts the origi-   nal sentence as the augmented sample . Apart from   the reading order , IS preserves all language prop-   erties even including n - gram statistics ( Dufter and   Schütze , 2020 ) ; 3 ) Words Repetition ( WR ) dupli-   cates part of words and randomly insert them into   the original sentences ; 4 ) Words Deletion ( WD )   deletes part of words in the sentences . WD and   WR change the length and words of the original   sentence but roughly preserve the reading order .   The deletion and repetition ratio are empirically set   to 0.2 . The augmentations from the continuous per-   spective ∆include Dropout ( DP ) . It generates   augmentation instances in the embedding level by   passing the original sentence again into the encoder   with different dropout masks . The above ﬁve strate-   gies can be repeatedly applied in practice . As there   is randomness in the processes of augmentation and   encoding , repeatedly generated instances with the   same strategy can be regarded as diverse positives .   But the diversity may accordingly decline . Note   that the primary goal of this paper is to verify the   effectiveness of our PCL framework , hence all of   the chosen augmentations are common and simple .   We speculate that our PCL can be further improved   with more ﬁne - tuned augmentation strategies .   B Baselines   We compare PCL with previous state - of - the - art un-   supervised sentence embedding approaches . Basic   approaches include taking the average of GloVe ,   BERT , or RoBERTa embeddings . Besides , BERT-   whitening and BERT-ﬂow post - process the em-   beddings distribution of BERT . We also compare   PCL with recent approaches using contrastive learn-   ing , including IS - BERT , ConSERT , SG - OPT , Con-   trastive Tension , DeCLUTR , and SimCSE . The   following are the details of these baselines ,   •GloVe ( Pennington et al . , 2014 ) maps words   into a meaningful space where the distance be - tween words is related to semantic similarity .   The results of the average of GloVe embed-   dings are from Reimers and Gurevych ( 2019 ) .   •Su et al . ( 2021a ) takes the average of the ﬁrst   and last layers of BERT ( Devlin et al . , 2019 )   or RoBERTa ( Liu et al . , 2019 ) embeddings .   We report the results from Gao et al . ( 2021 ) .   •BERT - whitening ( Su et al . , 2021a ) and BERT-   ﬂow ( Li et al . , 2020 ) post - process the embed-   dings distribution of BERT . We report the re-   sults from Gao et al . ( 2021 ) for a fair compar-   ison .   •IS - BERT ( Zhang et al . , 2020 ) encourages the   representation of a speciﬁc sentence to encode   all aspects of its local context information , us-   ing local contexts derived from other input   sentences as negative examples for contrastive   learning . We report the results from the origi-   nal paper .   •ConSERT ( Yan et al . , 2021 ) contrasts a pair   of sentences augmented by different augmen-   tation methods . We report the results in the   original paper .   •SG - OPT ( Kim et al . , 2021 ) is a contrastive   learning method using self - guidance . The re-   sults are from the original paper .   •Contrastive Tension ( CT ) ( Carlsson et al . ,   2021 ) propose a training objective that aligns   the embeddings of the same sentence encoded   by two different encoders . We report the re-   sults from Gao et al . ( 2021 ) .   •DeCLUTR ( Giorgi et al . , 2021 ) is a con-   trastive approach that takes different spans   from the same document as contrastive pairs .   The results are from Gao et al . ( 2021 )   •Mirror - BERT ( Liu et al . , 2021b ) employs sev-   eral fast augmentation strategies for effective   representations . The results are from the orig-   inal paper .   •SimCSE ( Gao et al . , 2021 ) contrasts a pair of   embeddings of one sentence encoded with dif-   ferent dropout masks . The results are from the   original paper . We had re - run SimCSE with   same setups and it performs worse than the   numbers reported in the original paper ( e.g. ,   75.36 averaged over 5 seeds on BERT).12064   We reported higher numbers for a fair compar-   ison .   Due to the surge of this topic , many concurrent   works emerge with two trends : new model structure   andmore in - domain data . SGPT ( Muennighoff ,   2022 ) and Sentence - T5 ( Ni et al . , 2021 ) are pro-   posed with new paradigm and far larger models ,   which underperform with comparable model size .   Trans - Encoder ( Liu et al . , 2021a ) proposes a coop-   erative method with in - domain pairwise data for   mutual beneﬁts of bi- and cross - encoder , making   the results incomparable .   C Additional Experimental results   C.1 Comparison of controlled setups   We can also ﬁnd some variants of SimCSE in our   controlled experiments . For example , PCL   in Table 3is regarded as SimCSE w/ multi-   augmentations . PCLin Figure 2can be re-   garded as SimCSE w/ peer - network cooperation ,   and PCLin Figure 3is regarded as SimCSE   w/ 9 dropout augmented positive samples . As we   discussed in the § 3.5and § 3.6 , the comparison   between the variants of SimCSE and PCL show   the advantages and importance of our proposed   peer - contrast and peer - cooperation .   C.2 Transfer tasks   We also evaluate PCL on 7 transfer tasks ( Con-   neau and Kiela , 2018 ) . As the Table 4shows ,   PCL achieves competitive performance compared   with baselines . Note that as mentioned in previousworks ( Reimers and Gurevych , 2019 ; Gao et al . ,   2021 ) , the main goal of sentence embeddings is to   cluster semantically similar sentences . Hence we   only take STS as the main results in this paper .   C.3 Detailed experimental results on analysis   of diverse augmentations   In this section , we present detailed results of exper-   iments of diverse augmentations on all 7 STS tasks .   We test the performance of PCL with varying K   and diversity . Experimental results are shown in   Table 5 . As the results and our analysis in § 3.5   show , the performance of PCL maintains an up-   ward trend with increasing K. Besides , it is also   shown that PCL signiﬁcantly outperforms its mono-   augmenting variants , even keeping the Kof them   constant , which indicates a better generalization .   As a result , PCL with more diverse augmentations   performs better . We also speculate that our PCL   can be further improved with larger Kand more   ﬁne - tuned augmentation strategies .   D Discussion   D.1 Distinction between PCL and other   contemporary methods .   Unsupervised sentence embedding w/ mul-   tiple positive augmentations . It ’s straightfor-   ward to extend mono - augmentation into multi-   augmentation to learn expressive representations .   For example , CLEAR ( Wu et al . , 2020 ) uses var-   ious token / span manipulations for noise - invariant   representations while Mirror - BERT ( Liu et al . , 12065   2021b ) employs several fast augmentation strate-   gies for effective representations . However , these   methods usually take the augmented positives   equally , regardless of the uncontrolable qualities .   For example , given “ Two men are wrestling on the   ﬂoor ” , we get “ Two men are squirming on the ﬂoor ”   and “ Two persons are wrestling on the ﬂoor ” by   word replacement , but only the 2nd is reasonable .   Thereby , we take a step further to consider the   contrasts among the augmented positives to ﬁgure   out which augmentation is relatively reasonable .   This is achieved by our novel cooperative learning   method with peer networks .   Unsupervised sentence embedding for anti - bias .   More related to our work , ESimCSE ( Wu et al . ,   2021b ) found learning on dual - dropout causes sen-   tence length bias so it employs another augmenta-   tion strategy , i.e. , word repetition , to prevent the   length bias . However , word repetition introduces   learning shortcut ( i.e. , order andBoW as in Table   1 ) by itself , not to mention it makes the sentence   unnatural and even semantics - wrong ( e.g. , repeat-   ing “ no ” ) . To circumvent this dilemma , we propose   exhaustive augmentations to ensure “ at - least - one ”   true positive and reduce learning shortcuts by com-   plementary augmenting strategies ( See Figure 1   and3 : if we employ every strategy , the shortcuts   can be blocked ) . Nonetheless , PCL still has a bet-   ter performance ( 78.42 vs. 78.27 ) compared with   ESimCSE on STS tasks .   Cooperative learning has more parameters , is it   the reason leading better performance ? One of   the advantages of our cooperative learning is to ef - fectively learn from the positives with uncontrolled   qualities , or it can be also interpreted as ‘ noisy la-   bels ’ . In the noisy circumstance , more parameters   not necessarily lead to better performance . And it   can be anticipated that it possibly leads to worse   performance because the over ﬁtting in the noisy   positives .   D.2 Efﬁciency & Impact of augmentations .   Efﬁciency Due to peer positives and encoders ,   our framework needs higher ( ~3 × , i.e. , 2.5   GPU - hours for base models ) computation over-   heads compared to vanilla CL ( Gao et al . , 2021 ) .   Nonetheless , the same inference makes our frame-   work still practical and scalable . Since PCL con-   tains more loss items , we do not see any signiﬁcant   difference in convergence time .   Impact of augmentations The performance of   our framework relies on the augmentation methods ,   and it is hard to claim which combination of the   methods is optimal except experimental veriﬁca-   tion . In this work , we only intuitively select several   methods without extensive trials . We have illus-   trated the performance of mono - augmentation in   Figure 3.12066