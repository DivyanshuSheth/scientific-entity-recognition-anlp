  Ankur SikarwarArkil PatelNavin GoyalI2R , A*STAR SingaporeMila - Quebec AI InstituteMcGill UniversityMicrosoft Research India   ankursikarwar.as@gmail.com   arkil.patel@gmail.com , navingo@microsoft.com   Abstract   Humans can reason compositionally whilst   grounding language utterances to the real world .   Recent benchmarks like ReaSCAN ( Wu et al . ,   2021 ) use navigation tasks grounded in a grid   world to assess whether neural models exhibit   similar capabilities . In this work , we present   a simple transformer - based model that outper-   forms specialized architectures on ReaSCAN   and a modified version ( Qiu et al . , 2021 ) of   gSCAN ( Ruis et al . , 2020 ) . On analyzing the   task , we find that identifying the target location   in the grid world is the main challenge for the   models . Furthermore , we show that a particular   split in ReaSCAN , which tests depth generaliza-   tion , is unfair . On an amended version of this   split , we show that transformers can generalize   to deeper input structures . Finally , we design a   simpler grounded compositional generalization   task , RefEx , to investigate how transformers   reason compositionally . We show that a single   self - attention layer with a single head general-   izes to novel combinations of object attributes .   Moreover , we derive a precise mathematical   construction of the transformer ’s computations   from the learned network . Overall , we provide   valuable insights about the grounded composi-   tional generalization task and the behaviour of   transformers on it , which would be useful for   researchers working in this area .   1 Introduction   Natural Languages are believed to be composi-   tional ( Partee et al . , 1984 ) , i.e. , the meaning of   an expression is determined by the meaning of   its constituents and how they are combined . The   field of compositional generalization seeks to un-   derstand whether neural models used for language   processing exhibit compositional behaviour . In re-   cent years , the field has received increased attention   resulting in the development of many new bench-   marks ( Lake and Baroni , 2018 ; Kim and Linzen , Figure 1 : An example from the ReaSCAN dataset .   2020 ; Keysers et al . , 2020 ) and approaches ( Li   et al . , 2019 ; Lake , 2019 ; Chen et al . , 2020 ; Liu   et al . , 2021 ) to solve them .   Natural language utterances are also grounded to   the real world . To encourage development of sys-   tems that are both compositional andgrounded ,   Ruis et al . ( 2020 ) created the gSCAN dataset .   Recently , Qiu et al . ( 2021 ) proposed the GSRR   datasetand Wu et al . ( 2021 ) proposed the ReaS-   CAN dataset to address certain limitations in   gSCAN . These tasks consist of navigation com-   mands grounded in a 2D grid world containing   an agent and multiple objects with different visual   attributes . Given a command and grid world , a   model needs to output the sequence of actions for   the agent to execute . Fig . 1 shows an example from   ReaSCAN . The difficulty of the task lies in general-   izing to out - of - distribution splits that are formed by   systematically holding out particular compositions   of object attributes and command structures from   train set . Heinze - Deml and Bouchacourt ( 2020 ) ;   Kuo et al . ( 2021 ) developed specialized architec-   tures for gSCAN that are either difficult to adapt to   other problems or require extra supervision .   Contributions . Our goal is to better understand   these grounded compositional generalization tasks   and design generic ML models to solve them . Our   contributions include:648   ( i)We propose the Grounded Compositional   Transformer ( GroCoT ) , which was created by mak-   ing simple and well - motivated modifications to a   multi - modal transformer model ( Qiu et al . , 2021 ) .   GroCoT achieves state - of - the - art performances on   both , GSRR and ReaSCAN.Our results clearly   show that simple transformer - based models gener-   alize well on these tasks .   ( ii)We design a series of experiments to under-   stand the underlying challenges in these tasks . We   show that identifying the target location , rather   than sequence generation , is the main difficulty .   We also demonstrate that the split , testing depth   generalization in ReaSCAN is unfair in that the   training data does not provide the models with   sufficient information to correctly choose among   competing hypotheses . On experimenting with a   modified training distribution , we show that simple   transformer - based models can successfully gener-   alize to commands with greater depths .   ( iii)We examine why transformers are so suc-   cessful at generalizing compositionally on these   tasks . To this end , we introduce a new task called   RefEx ( ‘ Referring Expressions ’ ) , which provides   a simpler setting isolating some of the main fea-   tures of ReaSCAN . We find that a 1 - layer , 1 - head   attention - only transformer is capable of grounding   and generalizing to novel compositions of multiple   visual attributes ; moreover , it admits a complete   interpretation of the computations . RefEx also al-   lows easier probing and leads us to identify and   solve an overfitting issue with transformers on a   particular ReaSCAN split .   2 Background   We focus on the gSCAN ( Ruis et al . , 2020 ) , GSRR   ( Qiu et al . , 2021 ) and ReaSCAN ( Wu et al . , 2021 )   datasets . A model , provided with a natural lan-   guage command , is tasked with generating a se-   quence of actions to navigate an agent in a 2D grid   world populated with objects . Below , we shall ex-   plain the setting of the ReaSCAN task in detail .   More information about other datasets is provided   in Appendix C.   Each example consists of a d×dgrid world   ( d= 6 ) , a natural language command and the cor-   responding output sequence . Each cell in the grid   world is described by a c - dimensional vector that   concatenates one - hot encodings for the three object   attributes , color C={red , green , blue , yellow } ,   shapeS={circle , square , cylinder , box } , and size   D={1,2,3,4}along with information about   agent orientation O={left , right , up , down }   and agent presence B={yes / no } . Hence ,   the entire grid world is represented as a tensor   W∈R. The natural language command   x:= ( x , x , . . . , x)is generated using a context-   free grammar ( CFG ) , which is described in Ap-   pendix C.1 . ReaSCAN has three types of in-   put commands which we illustrate in Table 1 .   The output sequence y:= ( y , y , . . . , y)is   made up of a finite set of action tokens A=   { walk , push , pull , stay , turn left , turn right } .   The main challenge of the task is generalizing   on the specially designed test splits that consist of   various types of examples systematically held - out   from the train set as shown in Table 2 ( more details   in Appendix C.1 ) .   The results of various previously proposed meth-   ods are shown in Table 3 for GSRR , Table 4   for ReaSCAN , and Table 12 for gSCAN . Qiu   et al . ( 2021 ) outperformed all previous methods   ( Gao et al . , 2020 ; Kuo et al . , 2021 ; Heinze - Deml   and Bouchacourt , 2020 ) on gSCAN and GSRR .   Hence , for ReaSCAN , we do n’t re - implement those   methods as baselines ; rather , we compare directly   against Qiu et al . ( 2021).649   3 Our Approach   We start with the multimodal transformer model   as used in Qiu et al . ( 2021 ) . This model , hereafter   called the base model , follows encoder - decoder   structure Vaswani et al . ( 2017 ) and uses cross-   modal attention in the encoder .   Encoder maps world state W∈Rto vi-   sual representation Hthrough multi - scale CNNs   followed by linear layers . The command tokens   x:= ( x , x , . . . , x)are encoded into embed-   dings H={h , h , . . . , h } . These are passed   through Ntransformer blocks , each consisting of   two parallel multi - head attention blocks ( one for   vision and one for language modality ) , with repre-   sentation of one modality passed as key and value   to the attention block of the other modality .   Decoder consists of Nstacked blocks similar to   the decoder in Vaswani et al . ( 2017 ) . Each block   contains one self - attention block and one multi-   head attention block over the contextual represen-   tation H= [ H;H]of the encoder .   Below , we describe the modifications we make   to this base architecture to create GroCoT. Imple-   mentation details are provided in Appendix B.   Improving Spatial Representation . For this   task , models need to perform spatial reasoning be-   tween objects that may possibly be very far from   each other in the grid world . The base model ( Qiu   et al . , 2021 ) employed a multi - scale CNN to en-   code the world state Wbefore feeding it to the   transformer . However , CNNs , without the pres-   ence of large filters ( i.e. , large receptive fields ) ,   are inept at understanding the spatial relationships   between parts of the image that are not in imme-   diate vicinity . To address this limitation , insteadof passing the world state tensor Wthrough a   multi - scale CNN , we propose tokenizing the grid   cells and projecting them to a higher dimension   W={w , w , . . . , w},w∈R. In   line with Lu et al . ( 2019 ) , we separately encode   the spatial information of grid cells in a 2D vector ,   where the first dimension holds the row value and   the second one holds the column value . We project   these spatial encodings to a higher dimension   S={s , s , . . . , s},s∈R and add   them to their corresponding grid cell representa-   tions to obtain the final grid cell embedding input to   the transformer H={h , h , . . . , h},h=   w+s∈R.   Interleaving Self - Attention . The base model   uses cross - modal attention in all encoder layers .   While this facilitates grounding of semantic in-   formation across both modalities , we believe this   method to be inefficient . We know that differ-   ent layers in both vision transformers and lan-   guage transformers encode different levels of se-   mantic knowledge ( Dosovitskiy et al . , 2021 ; Raghu   et al . , 2021 ; Jawahar et al . , 2019 ) . To allow effi-   cient grounding , we want both visual and language   modality streams to develop their own representa-   tions before synchronizing them with each other   via cross - modal attention . Hence , we propose inter-   leaving self - attention layers between co - attention   layers to allow intra - modal interaction within each   stream before cross - modal interaction .   Modified World State Encoding . In ReaSCAN ,   for particular examples where another object is   present in the top left corner of a box object , the   grid cell embedding corresponding to that corner   is calculated by adding up the vector encodings   corresponding to the object and the box ( Wu et al . ,650   2021 ) . However , this design is inherently flawed   because the attributes of the two objects can not   be disambiguated from the sum of their individual   encodings . This issue causes models to fail in such   examples . To handle such cases , we propose using   a higher dimensional grid cell embedding ( see Fig .   2 ) to represent color and size properties of the box   separately from other objects .   Discussion . The results are provided in Table 3   for GSRR and Table 4 for ReaSCAN . We also pro-   vide exhaustive ablations of our approach on ReaS-   CAN in Table 5 . On both datasets , GroCoT outper-   forms all specialized architectures . From the abla-   tion study on ReaSCAN , we observe that both , im-   proved spatial representation and interleaved self-   attention , lead to significant improvements . The   modified embedding structure additionally helps   when examples contain box objects ( see improve-   ment in ReaSCAN B2 split ) . Our model also sat-   urates performance on most splits in gSCAN ( see   Table 12 ) . We also evaluated the effect of using   vanilla self - attention ( as used in the original Trans-   former ( Vaswani et al . , 2017 ) ) in GroCoTon ReaS-   CAN and found that it achieves surprisingly highaccuracies ( see Table 4 ) . Our hypothesis is that   vanilla self - attention facilitates individual process-   ing of both modalities similar to our interleaving   self - attention approach , and hence it does not hurt   the model performance significantly .   Overall , our results show that a simple   transformer - based model is capable of generaliz-   ing compositionally on most of the challenges pro-   posed by gSCAN , GSRR and ReaSCAN . Instead of   presenting GroCoT as a broadly applicable method   solving compositional generalization , we only wish   to establish that such simple transformer - based   models can exhibit strong compositional general-   ization capabilities and serve as powerful baselines .   4Analyzing the Grounded Compositional   Generalization Tasks   4.1 Target Identification vs Navigation : What   is the Challenge ?   In order to solve these tasks , a model needs to per-   form two subtasks : ( 1 ) identify the target location   by composing the words and reasoning about the   relative clauses , and ( 2 ) navigate the agent in the   grid world by generating the right set of output to-   kens . To understand why models fail and how to   improve them , we need to pinpoint where the main   difficulty in the task lies . Below we describe a set   of experiments that demonstrate that target identi-   fication is the main challenge in ReaSCAN rather   than navigation or sequence generation . We show   this for the gSCAN dataset in Appendix E.1.3 .   Target Identification from Encoder Represen-   tations . We train a linear layer on top of the last   layer of learned encoder representations of the best-   performing model to perform target identification.651   We model the task as a 36 - way classification prob-   lem , where each grid location is treated as a distinct   class . We train the model over all ReaSCAN ex-   amples with the ground truth target locations . Note   that we only update the weights of the linear layer ;   the parameters of the encoder are kept frozen . We   then test this model ’s target identification abilities   over the systematic generalization splits .   The first row of Table 6 shows the performance   of the model on target identification . We see the   same trend in performance across all splits as we   saw for the full model on ReaSCAN ( see Table 4 ) .   This indicates that target identification might be the   main difficulty for the model . To illustrate more   concretely , we calculate the overlap of errors be-   tween the target identification model and the ReaS-   CAN model . As can be seen from the second row   in Table 6 , for each split , out of all the examples   where the ReaSCAN model failed , the percentage   of examples where the target identification model   also failed is extremely high .   We also provide exhaustive ablations for this ex-   periment in Table 7 . Our proposed modifications do   indeed enable better target identification . However ,   there might be other minor aspects of the problem   that are tackled by our modifications ( model with   ISR performs better overall on ReaSCAN but is   not the best on target identification ) . Also note   that these experiments are performed without the   decoder , which is essential in solving ReaSCAN.Sequence Generation from Gold Target Loca-   tions . We provide the model with gold target loca-   tions when training it on the ReaSCAN training set .   We enumerate all the 36 grid cell locations and sim-   ply append gridnum to the end of the natural lan-   guage command where gridnum ∈ { 1,2 , ... , 36 } ,   depending on the ground - truth target location for   a particular example . The results are provided in   the last row of Table 6 . Clearly , the model is able   to generalize almost perfectly when provided with   the ground - truth target locations . This shows that if   the target is identified , the model has no difficulty   in navigating the agent towards it .   In this section , with comprehensive empirical   evidence , we showed that models are highly compe-   tent at agent navigation and that the chief difficulty   lies in identifying the target location .   4.2 Issues in ReaSCAN Test Set Design   Compositional generalization setups are used to   assess specific capabilities of models . However ,   if the train - test splits within these setups are not   carefully created , then the experimental results may   lead to false conclusions ( Patel et al . , 2022 ) . In this   section , we show that the C2 test set of ReaSCAN   is unfair because of lack of necessary information   in the train set . We then propose a correction in the   train - test setup that allows us to fairly evaluate the   depth generalization capabilities of models .   C2 Test Set is Unfair . The train set of ReaS-   CAN consists of commands with different struc-   tures as shown in Table 1 . The C2 test set is652made up of commands with the other type of   2 - relative - clause structure ( e.g. , “ walk to the   red square that is in the same row as the blue cylin-   derthat is in the same column as a green circle ” ) .   This split tests whether a model is able to perform   recursion to higher depths . It is clear that the only   difference between the 2 - relative - clause com-   mands in the train set and the C2 test set is that the   ‘ and ’ connecting the last two clauses is replaced   with ‘ that is ’ . Hence it is crucial for the model to   understand the difference between these terms to   successfully generalize on the C2 test set . How-   ever , based on the train set , they both perform the   same role : they act as a connector between two   clauses in the command where the target needs   to be identified based on the attributes in the first   clause after satisfying the constraint of the follow-   ing clause . We explain this more intuitively in   Appendix E.2.1 . To illustrate this empirically , we   show that the average consistency between model   predictions before and after replacing all ‘ and ’ with   ‘ that is ’ in ReaSCAN test sets is 93.5.Hence , the   train set of ReaSCAN is insufficient for the model to   disambiguate between ‘ and ’ and ‘ that is ’ , thereby   rendering the C2 generalization task unfair .   Model Learns a Reasonable Alternate Hy-   pothesis for C2 . We hypothesize that for the   commands in the C2 split , the model treats the   second ‘ that is ’ as if it were ‘ and ’ , similar to the   2 - relative - clause commands it has seen in the   train set . We consider this model behavior to be   reasonable , since this hypothesis is consistent with   the train set . To verify this empirically , we create a   new set of examples , called C2- alt , by replacing   the second ‘ that is ’ with an ‘ and ’ in all examples   in the C2 test set . The model ’s predictions for C2   matched those for C2- alt93 % of the time , clearly   validating our hypothesis .   Transformers Generalize to Higher Depths   of Relative Clauses . Since we showed that the   C2 test set is unfair , we designed a new , fairer test   split to check generalization capability of models to   higher depths . We include commands up to depth 2   ( i.e. , the type of commands in the C2 test set ) in the   train set and test on commands of depth 3 . We call   this new test set as C2- deeper .By including com-   mands up to depth 2 , we alleviate the issue of the   model not being able to disambiguate between ‘ that   is ’ and ‘ and ’ . Our best model achieved 85.6 % accu-   racy on C2- deeper . This is very surprising ( since   transformers have been known to struggle at depth   recursion ( Kim and Linzen , 2020 ) ) and clearly   shows that the multimodal transformer model is   capable of generalizing systematically to higher   depths in this setting . This also re - affirms our claim   that the original C2 test set was unfair .   5 RefEx : Understanding How   Transformers Ground and Compose   We wish to understand how transformers succeed   on grounded compositional generalization tasks .   However , the complexity of both , the ReaSCAN   task , and the multi - modal transformer , makes it   difficult to interpret the model . Hence , we design a   new task , RefEx , based on the target identification   subtask of ReaSCAN.In the following sections ,   we show that even a one - layer , one - head attention-   only model can successfully ground and compose   multiple object attributes in RefEx . We then give   a precise construction of the model , which demon-   strates the detailed computations corresponding to   grounding and composition .   5.1 Task Setup and Test Splits   Given a command that refers to a unique target   object in the accompanying grid world , a model   needs to identify the target location . Compared to   ReaSCAN , etc . , we get rid of subtasks like path   planning and action sequence generation , and focus   only on target identification .   We design three variants of the RefEx task with   different command structures varying in difficulty :   ( i)two - attr : = $ COL $ SHP . Model needs to ground   and compose color andshape attributes .   ( ii)three - attr : = $ SIZ $ COL $ SHP . Model needs653   to additionally handle the sizeattribute , which re-   quires relative reasoning .   ( iii ) three - attr - rel : = $ OBJ $ REL $ OBJ . Model   effectively needs to perform two three - attr sub-   tasks sequentially based on the relation between   the referent and target objects .   Here , $ COL ∈ { red , green , blue},$SHP ∈   { square , circle , cylinder } , $ SIZ∈ { small , big } ,   $ OBJ : = $ SIZ ? $ COL ? $ SHP ? , and $ REL∈   { same size , same color , same shape } . Figure 3   shows examples from all three variants . Addition-   ally , we create four compositional generalization   test splits as described in Table 8 .   5.2 Model   We consider attention - only transformers ( with   residual connections ) with two layers or less . We   use natural sparse embedding matrices ( see Fig .   14 , 15 , 16 ) to represent the command and world   state . The input sequence length n= 2 + 36 for   two - attr where 2and36correspond to the num-   ber of command and grid world tokens , respec-   tively . The output representations corresponding to   the grid world tokens are mapped to logits by taking   element - wise sum followed by softmax for 36 - way   classification ( each class represents a unique grid   location ) . See Appendix F.2 for more details .   5.3 Results and Discussion   The performance of the model described above on   the RefEx task is shown in Table 9.Efficacy of Self - attention Layers . Fortwo - attr ,   we found it surprising to see that a one - layer , one-   head attention - only transformer can successfully   ground and compose the attributes for correct target   identification . Moreover , the model generalizes to   novel compositions of the attributes as can be seen   from its performance on the compositional splits .   Inthree - attr , which is more difficult than   two - attr , surprisingly , a one - layer , one - head   model can ground and compose three different at-   tributes , including size , which requires complex   relative reasoning . Finally , in three - attr - rel ,   we find that at least two layers are required to solve   the task . This makes intuitive sense : each layer   will solve one three - attr subtask to identify the   referent or target object .   From RefEx to ReaSCAN . Inspired by these re-   sults , we evaluate attention - only transformers on   ReaSCAN target identification . Examples in ReaS-   CAN can contain up to three referring expression   subtasks . Therefore , based on our intuition above ,   the model ’s performance should saturate after 3   layers . We show these results in Fig . 12 . More-   over , since the design of splits in RefEx is similar   to that of ReaSCAN , we were able to derive useful   insights from models trained on RefEx in order to   improve the performance on ReaSCAN A2 split .   See Appendix F.3 for details .   5.3.1 Interpreting how Transformers Ground   and Compose   We completely describe how an attention-   only transformer with one attention layer and   one head solves two - attr ( three - attr and   three - attr - rel are similar but more complex ) .   Let ’s first recall how the one - layer , one - head model   works . We denote the input token embeddings   byx , . . . , xand the final output embeddings by   r , . . . , r. LetW , W , Wbe the parameter   matrices for queries , keys , and values ; we can then   define the query , key and value vectors for i∈[n ]   byq = Wx , k = Wx , andv = Wx .   For each i∈[n]we compute the output of atten-   tion block as ˜r = W / summationtextαv , where the   attention scores are given by ( α , . . . , α ) =   softmax ( ⟨q , k⟩ , . . . , ⟨q , k⟩ ) . The residual   connection then gives r=˜r+x . As described   in Section 5.2 , in our model , the final grid cell con-   taining the target is chosen by applying softmax   to the logits corresponding to 36 grid world to-   kens , L , . . . , L. Let1be the all - ones vector ;   fortwo - attr , we have ⟨1,x⟩= 2 when icorre-654sponds to grid world tokens ( see embedding matrix   in Figure 14 ) . Now , we show the computation of   logitLwhere icorresponds to grid world tokens .   L=⟨1,r⟩=⟨1,x⟩+⟨1,˜r⟩   = 2 + ⟨1,W / summationdisplayαv⟩   = 2 + /summationdisplayα⟨1,Wv⟩   = 2 + /summationdisplayαs   We now qualitatively illustrate on a specific ex-   ample of two - attr and show how the learned   parameters ( M in Figure 4 ) ) lead to the   correct target prediction . Note that the matrix   M∈R here contains the dot prod-   uct of query and key vectors of all possible pairs   of tokens in the vocabulary . The rows in Mcorre-   spond to queries while the columns correspond to   keys . Let the command tokens be ⟨red⟩,⟨cylinder ⟩.   We now show that the logit for ⟨red cylinder ⟩is   the maximum among all possible values for grid   tokens . In the learned model , we observe that when   iandjcorrespond to grid world tokens either α   orsis very small :   L≈2 + /summationdisplayαs ,   where Cis the set of indices of command tokens   in our example , i.e. , C={1,2 } . When there is a   match in an attribute in the grid world token i(say   ⟨red circle ⟩ ) and a command token j(say,⟨red⟩ ) ,   then the corresponding summand in the above sum ,   i.e.αsis large , and when there is no match   ( e.g. , when the command token jis changed to   green ) then it is small . Therefore , in our example ,   the logit computation for the token ⟨red cylinder ⟩   has two large summands and corresponds to a full   match , whereas for tokens like ⟨blue cylinder ⟩and   ⟨red circle ⟩ , there is only one large summand . This   corresponds to partial match . Finally , for a token   like⟨green square ⟩ , there is no match i.e. none of   the summand , in this case , is large . While the above   argument is qualitative , by looking at the entries   ofM it can be made quantitative ; M   led us to construct M ( Fig . 4(b ) ) which   makes the computations transparent while stay-   ing faithful to the learned model . Looking along   the columns of the matrix corresponding to tokens   ⟨red⟩and⟨cylinder ⟩ , we can see that the row corre-   sponding to ⟨red cylinder ⟩grid world token has two   “ dark ” cells ( full match ) , while rows corresponding   to⟨blue cylinder ⟩,⟨red circle ⟩and⟨green square ⟩   have at most one “ dark ” cell ( i.e. partial match   orno match ) . Thus the grid cell corresponding   to⟨red cylinder ⟩will be the model ’s output . Full   match here corresponds to the idea that both visual   attributes ⟨red⟩and⟨cylinder ⟩mentioned in the   command , were successfully grounded to the grid   world token ⟨red cylinder ⟩ , and the composition of   these two successful groundings contributed to two   large summands in the final logit computation .   Finally , empirically our constructions attain per-   fect accuracies across all splits . The matrices of   our construction for three - attr are in Figure 20   and 22 .   6 Related Works   Compositional Generalization . Modern deep   learning models perform extremely well on in-   distribution test sets . However , unlike humans ,   they fail at generalizing compositionally ( Lake   and Baroni , 2018 ; Kim and Linzen , 2020 ; Keysers   et al . , 2020 ) . Recent works have investigated the   compositional generalization abilities of models in   grounded setups using datasets such as CLEVR   ( Johnson et al . , 2017 ) , CLOSURE ( de Vries et al . ,   2019 ) , and gSCAN ( Ruis et al . , 2020 ) . In this work ,   we focus on the task setup of gSCAN , and addi-   tionally work with GSRR ( Qiu et al . , 2021 ) and   ReaSCAN ( Wu et al . , 2021 ) . Both these works655propose new challenging splits for gSCAN . Prior   works have proposed many different specialized   methods for solving gSCAN ( Gao et al . , 2020 ;   Kuo et al . , 2021 ; Heinze - Deml and Bouchacourt ,   2020 ) . Similar to Csordás et al . ( 2021 ) ; Patel et al .   ( 2022 ) , in this work , we show that even simple   transformer - based models , with minor modifica-   tions to the architecture or training data distribu-   tion , perform well on the task and serve as strong   baselines for future work .   Probing and Interpreting Models . In this work ,   we used a linear probe ( Belinkov , 2022 ) to ana-   lyze the target identification abilities of models .   Similar to our work , Weiss et al . ( 2021 ) ; Elhage   et al . ( 2021 ) ; Kobayashi et al . ( 2020 ) and many   others also attempt to explain the inner workings   of transformers , possibly for non - synthetic prob-   lems . Unlike most of these works , however , our   construction essentially completely describes the   computations of the learned models . To the best of   our knowledge , we are the first to study how self-   attention facilitates compositional generalization in   grounded environments .   7 Conclusion and Future Work   Recent benchmarks like gSCAN and ReaSCAN   test grounded compositional generalization abili-   ties of ML models . In this work , we identify key   modifications in multimodal transformers that im-   prove compositional generalization on these bench-   marks . With a battery of probing experiments , we   found that identifying the target location is the   main challenge for the models . Additionally , we   showed that a particular test set in ReaSCAN is   unfair and proposed a modified train - test split in its   stead . Finally , we designed a new task , RefEx , to   study grounding and composition in attention - only   transformers . We showed the efficacy of single   self - attention layer with single head in successfully   grounding and composing multiple visual attributes   in a grid world environment . From the learned   models , we derived an explicit and interpretable   construction that captures the model ’s behavior and   completely describes the detailed computations cor-   responding to grounding and composition .   While our focus has been on tabula rasa models ,   it is also of interest to see if pretraining on large   datasets enables good performance on the consid-   ered benchmarks . Our preliminary investigations   on GPT-3 and Codex ( Appendix D ) suggest that   these text - based models have some way to go ; morethorough investigation is left for the future .   We expect future work to address the current   limitations of models on action sequence side com-   positional generalization , i.e. , generalizing to novel   combinations of action tokens . Moreover , our re-   sults indicate that designing compositional general-   ization splits can be surprisingly subtle and require   careful scrutiny . Finally , grounded compositional   generalization benchmarks should also target more   realistic setups with natural images in the future .   8 Limitations   Our proposed approach fails on some gSCAN   splits , specifically D , G , and H. These splits are   designed to test output sequence - side systematic   generalization capabilities of the model . In the fu-   ture , we intend to extend our model by making   architectural changes on the decoder side in order   to tackle these splits .   On ReaSCAN , our approach achieves 86.1 % on   the B2 split and 76.3 % on the C1 split , which is   relatively less compared to the performance on   other splits , suggesting room for further improve-   ment . We expect that the C1 split also has similar   issues like the C2 split ( see section 4.2 ) , although   we have n’t yet succeeded at concretely identifying   these issues .   In line with previous models , our model also   uses grid cell encodings which are very simple   and explicitly represent different attributes of the   world objects . However , natural images are high-   dimensional and contain entangled representations   of object attributes . Ideally , we would like to eval-   uate the compositional generalization abilities of   models in a real - world setting using natural images   since that has direct applications .   Our constructions for attention - only transform-   ers in Section 5 are given only for the RefEx task   which is a relatively simpler task than ReaSCAN .   We plan to give similar constructions for more com-   plex tasks than RefEx in the future .   Acknowledgements   We thank the anonymous reviewers for their con-   structive comments and suggestions . We would   also like to thank Satwik Bhattamishra for review-   ing an early draft of the paper and providing valu-   able feedback . We are grateful to our colleagues   at Microsoft Research India for engaging in useful   discussions and constantly supporting us through-   out this work.656References657658A Model Architecture   The model architecture of our proposed approach   GroCoT is shown in Figure 5 .   B Implementation Details   We use PyTorch ( Paszke et al . , 2019 ) for all our   implementations . All our models were trained from   scratch , and the parameters were updated using   Adam optimizer . We designed a compositional   validation set by taking few examples from each   compositional splits of the respective dataset . The   best model is selected based on the accuracy on   this compositional validation set . Hyperparameter   tuning was done using grid search . We show the   best hyperparameters for our models corresponding   to different datasets in Table 10 . Moreover , we   show the average performance of 3 different runs   with random seeds for all the models in the paper .   We used 8 NVIDIA Tesla V100 GPUs each with   32 GB memory for all our experiments .   C Details of Datasets   C.1 ReaSCAN   ReaSCAN consists of around 500 K train , 30 K vali-   dation , and 6 K test examples where each example   is a pair of command and world state . Given these   two as input , models are supposed to output the   correct sequence of action tokens . Apart from the   above splits , ReaSCAN also has 7 systematic gen-   eralization test splits in total . ReaSCAN has three   types of input commands .   •Simple : = $ VV $ ADV ? ( equivalent to gSCAN   commands )   •1 - relative - clause : = $ VV $ OBJ that is   $ REL_CLAUSE $ ADV ?   •2 - relative - clauses : = $ VV $ OBJ that is   $ REL_CLAUSE and$REL_CLAUSE $ ADV ?   See Table 11 for expansions of non - terminals in   the grammar . Below , we describe the ReaSCAN   test splits in detail :   A1Novel Color Modifier : For this split , the train   set never contains “ yellow square ” in the command .   However , commands containing expressions such   as “ yellow circle ” and “ blue square ” are present in   the training set . During test time , this split expects   model to zero - shot generalize on the phrase “ yellow   square” . A2Novel Color Attribute : Here , the examples in   which red squares are targets are held out from the   train set . Commands in the train set also never con-   tain the phrase “ red square ” . However , the train set   may contain red square objects as distractors in the   background . This particular split tests model per-   formance on novel combination of target object ’s   visual attributes .   A3Novel Size Modifier : In this split , a particular   combination of size and shape attributes is held out   from the train set . Specifically , the model never   sees phrases like “ small cylinder ” or “ small green   cylinder ” during training . While testing , the model   needs to generalize to examples where cylinders   of any color are referred using the “ small ” size   attribute . Additionally , size being a relative concept   in ReaSCAN , adds another level of complexity . For   instance , size “ small ” can refer to an object of size   2 in a particular example , and in another example   size “ small ” can instead refer to an object of size 3 ,   depending on the other objects present in the grid   world .   B1Novel Co - occurrence of Objects : In this split ,   the commands contain those objects which never   co - occur in training ( e.g. “ small red circle ” and   “ big blue square ” ) . However , models do encounter   these objects co - occurring with other objects dur-   ing training . In summary , this split tests whether   models can generalize over novel co - occurrence of   objects .   B2Novel Co - occurrence of Relations : Commands   containing both “ same size as ” and “ inside of ” re-   lations are held out from the training data . At   test time , models must generalize to the novel co-   occurrence of these two relations . Importantly ,   during training model does encounter commands   where the relation “ inside of ” co - occurs with other   relations except “ same size as ” .   C1Novel Conjunctive Clause Length : This split   contains commands with additional conjunctive   clause i.e. the commands contains 3 relative clauses   ( e.g. “ push the small red circle that is in the same   column as a big green square and inside of a big   blue box and in the same row as a blue square   hesitantly ” ) . Models trained with up to 2 - relative   clauses must generalize to longer commands which   contain 3 relative clauses .   C2Novel Relative Clauses : In ReaSCAN train   data , commands contain a maximum of 1 recursive   relative clause ( e.g. “ push the circle that is in the   same column as a yellow square and inside of a659   big box cautiously ” ) . However , this test split con-   tains commands with 2 - recursive relative clauses   i.e. there are two “ that is ” clauses in the commands   ( e.g. “ push the blue circle that is in the same col-   umn as a blue cylinder that is in the same row as a   green square hesitantly ” ) .   C.2 gSCAN   gSCAN is very similar to ReaSCAN ; both are es-   sentially grounded navigation tasks . The grid world   in gSCAN is the same as that of ReaSCAN , al-   though commands in ReaSCAN are much more   complicated than gSCAN . Commands in ReaS-   CAN contain relative clauses , whereas gSCAN   commands have no relative clauses . For examplecommand from gSCAN looks like walk to a red   big square . gSCAN contains around 350 K train-   ing and 20 K test examples for the compositional   splits . Below , we briefly describe the individual   compositional test splits in gSCAN :   ARandom : This test split contains random exam-   ples and is supposed to test in - distribution general-   ization .   BYellow squares : For this split , the train set does n’t   contain examples where yellow squares are referred   with color and shape attributes .   CRed Squares : Here , the examples where the tar-   get is red square are held out from the train set .   DNovel Direction : For this split , the examples   where the target objects are located south - west of660   the agent , are held out from training set .   ERelativity : To create this split , examples where   circles with size 2 are referred as small are held out   from the train set .   FClass Inference : In this split , all examples where   the verb is push , and target is a square of size 3   are held out from the training set . Note that the   model needs to infer that this object is of class   ‘ heavy ’ based on the size 3 and needs to push twice   to move an object by one grid cell .   GAdverb k=1 : Only one example with the adverb   cautiously is shown in the train set .   HAdverb to Verb : For this split , examples where   the commands have verb pull and adverb while   spinning are held out from the train set .   C.3 GSRR   Based on the gSCAN task setup , ( Qiu et al . , 2021 )   proposed 5 additional compositional generalization   splits that also test spatial reasoning capabilities   of models . GSRR contains around 250 K train ex-   amples , from which specific kind of examples are   held - out to create the systematic splits . Below , we   describe the compositional splits in GSRR :   IRandom : This split contains random examples   for testing in - distribution generalization .   IIVisual : For this split , the train set does n’t contain   examples where red squares are present either as   targets or referent object .   IIIRelation : Here , the examples which contain   both green squares andblue circles are held - out   from the training data .   IVReferent : For this split , the examples where   yellow squares are referred as target are held - out   from the trainset .   VRelative Position 1 : Here , all examples where   the targets are north of their referent objects are   held - out from the training set .   VIRelative Position 2 : In this split , all examples   where the target is south - west of the referent object   are not seen in training data .   D Performance of Large Language   Models   We design a simpler version of the task to evaluate   the performance of large language models ( LLMs )   such as GPT-3 ( Brown et al . , 2020 ) and Codex   ( Chen et al . , 2021 ) . Given a 3×3grid world and   a simple command stating an object ’s color and   shape , the model needs to output that object ’s loca-   tion . The world state as well as the output are in   a textual description format to make the task com-   patible with the input and output space of LLMs .   Along with each test example , the models are given661   prompts containing multiple in - context examples .   Note that the context provided to the model is en-   sured to contain all necessary information that the   model might need to answer the test example .   In the most basic version of the experiment ,   called direct - grounded , we directly refer to the   objects in the grid world with their attributes used   in the command . See Fig . 7 for an illustration . We   provide 30 in - context train examples to the model   as part of the prompt for each of the 20 test exam-   ple . In this setup , Codex achieved 95 % accuracy   while GPT-3 achieved 65 % accuracy . This experi-   ment merely serves as a sanity - checking baseline   for the other experiments we described next .   Ideally , the model should learn the mappingsof words in the commands to the tokens in the   world state . Hence , in this experiment , called   nonsense - grounded , we use non - sense words to   refer to the objects in grid world as shown in Fig .   8 . This task setting more closely resembles the   target identification task in ReaSCAN ( while be-   ing a much more simpler version of it ) . In this   setup , the models fail badly . Codex achieves only   25 % accuracy , and GPT-3 achieves only 30 % . This   clearly shows that LLMs are as yet unable to tackle   such grounded compositionality tests , even when   provided with sufficient evidence via in - context   training examples in the prompt .   Following recent work ( Wei et al . , 2022 ) , we   provide explicit chain - of - thoughts to the LLMs   to make them understand the task . While from   a purely evaluation point - of - view , this can be con-   sidered cheating , we were merely curious to check   whether the chain - of - thought idea , which has led to   so much success in reasoning tasks , would help the   models do better in this task setting . An illustration   of the chain - of - thought provided to the models is   given in Fig . 9 . Codex performs much better when   provided with such chain - of - thoughts , achieving   70 % accuracy . However , GPT-3 still struggles on   the task , achieving only 25 % accuracy .   E Additional Details and Results of   Analysis Experiments   E.1 Target Identification vs Navigation : What   is the Challenge ?   E.1.1 Comparison with Other Methods   Studies with similar objectives to ours have been   carried out by Ruis et al . ( 2020 ) and Qiu et al .   ( 2021 ) . However , the results of their experiments   are not very conclusive . Ruis et al . ( 2020 ) exam-   ined the object in the grid world that was being max-   imally attended by the agent and checked whether it   is the target . However , their results do not correlate   well with the conclusions . For instance , they find   that in the error cases on the gSCAN ‘ C ’ split , the   agent attends over the correct target object half the   time ! Qiu et al . ( 2021 ) compare the final position662   of the agent with the ground - truth target location .   However , such an analysis fails to disentangle the   subtasks of target identification and agent naviga-   tion and discards the causal relationship between   them . Also , because of the interpretation of verbs   such as push andpull , the final position of the agent   may be very different from the target location .   E.1.2 Predicting Target Location from Earlier   Encoder Representations   We experimented with predicting the target loca-   tion from earlier encoder representations , including   the embeddings and randomly initialized represen-   tations . The latter two act as baselines for our prob-   ing results described in section 4.1 . The results are   provided in Figure 10 .   E.1.3 Target Identification Results on gSCAN   We show that target identification is the main chal-   lenge for most of the gSCAN splits as well . Simi-   lar to the experiment described in section 4.1 , we   experiment with providing ground - truth target lo-   cations to the model . As seen from the results   provided in Table 13 , identifying the target loca-   tion is the main challenge for most of the splits in   gSCAN .   E.2 Issues in ReaSCAN Test Set Design   E.2.1 Intuitive Explanation of Unfairness in   C2 Split Design   In this section , we explain why the model would   not be able to disambiguate between ‘ and ’ and   ‘ that is ’ based on the train set . Since the meanings   of‘and ’ and‘that is ’ are apparent to humans , to   understand things from the model ’s perspective , let   us replace them with non - sense words ‘ axyo ’ and   ‘ tafyo ’ respectively . The model sees commands   such as “ walk to the small red square tafyo in the   same row as the big blue cylinder ” and “ walk to   the small red square tafyo in the same row as the   big blue cylinder axyo in the same column as a   green circle ” during training . There is n’t enough   information here to make the model understand   that‘tafyo ’ applies the constraint on its right to the   clause on its immediate left while ‘ axyo ’ applies   the constraint on its right to the clause that occurs   on the immediate left of the ‘ tafyo ’ before it . Look-   ing at these examples , both ‘ tafyo ’ and‘axyo ’ do   the exact same thing , i.e. , apply the constraint on   their immediate right over the first clause in the   command .   E.2.2 Experimental Details of Evaluation on   C2 - deeper   We randomly select 100,000 examples from the   train set and 6,000 examples from the C2 test set .   This forms the new train set . We generate 4500 new   examples of depth three to form the C2- deeper test   set.663F Additional Details and Results on our   RefEx Task   F.1 Differences with Other Benchmarks   RefEx dataset is different from existing similar-   looking synthetic benchmarks like SHAPES ( An-   dreas et al . , 2016 ) , CLEVR ( Johnson et al . , 2017 ) ,   and CLEVR - Ref+ ( Liu et al . , 2019 ) . RefEx aims   to test the systematic generalization capabilities of   neural models in a grounded setting while keeping   the task simple enough to allow easier interpreta-   tion of the model ’s behaviour . On the contrary , pre-   vious diagnostic benchmarks are more concerned   with testing the overall reasoning capabilities of the   model . Also , the close resemblance between RefEx   and ReaSCAN allows us to use insights gained   from the RefEx task to improve performance on   ReaSCAN .   F.2 Details about the Task and Model   F.2.1 Details About the Task   Each variant in RefEx contains 90 K training , 2.5 K   validation , and 2.5 K test examples .   A1We hold out all examples where the command   contains “ green square ” . As a result , the model   never sees a green square object as the target al-   though green squares occur in the background in   the train set . This split expects the model to zero-   shot generalize over the composition of “ green ”   and “ square ” attributes .   A2We hold out all examples where the command   contains “ red circle ” while ensuring that model   never encounters red circle object during training .   A3We hold out all examples where the command   is “ small green circle ” and the corresponding target   is of size 2 , meaning that the model has never seen a   green circle of size 2 being referred to with “ small ” .   A4We hold out all examples where the command   is “ small blue cylinder ” . At test time , the model   needs to zero - shot compose the concept of “ small ”   with “ blue cylinder ” objects .   F.2.2 Details About the Model   Below , we describe the details of our attention - only   transformer . We begin by mapping the command   and the world state to d size embeddings using   our embedding matrix . These embeddings consti-   tute the initial input X∈Rto our network ,   where nis the sequence length . Here , n= 2 + 36   fortwo - attr variant , n= 3 + 36 forthree - attr   variant , and n= 8 + 36 forthree - attr - rel vari-   ant . Here , 2,3 , and 8correspond to the tokens   in the referring expression . Note that this input   representation Xcontains information from both   modalities . This representation is fed into the first   multi - head attention block , and the subsequent out-   puts are residually added back to the initial input .   We repeat this mechanism for successive attention   blocks , and after the nattention block , the final   representations corresponding to the world state   tokens are mapped to logits by taking element - wise   sum along the d dimension . In the end , we   apply softmax operation on the logits for 36 - way   classification , where each class corresponds to a   particular grid cell . The architecture is illustrated   in Figure 11 .   The only learnable parameters for our model   come from the query , key , value , and output ma-   trices of different attention layers . For two - attr   andthree - attr variants , we do n’t require posi-   tional information for the command while we in-   corporate learned positional embeddings for the   three - attr - rel variant .   F.3 Understanding Model Performance on the   RefEx A1 Split   When we first created the Referring Expressions   train and test sets , the attention - only transformer   achieved perfect generalization on all splits except   A1 . On A1 , the model achieved average accuracy.664   This was very surprising because the model was   able to solve A2 , which seems a strictly harder   task than A1 . The only difference between the two   splits is that in A1 , the held - out target object ( green   square ) is seen as a distractor while in A2 , the   object ( red circle ) is never seen as a distractor . One   probable hypothesis we had for the model failing   on A1 was that the model was overfitting on the   fact that green squares are always distractors in the   train set , thereby preventing the model to predict it   as a target at test time . To confirm this , we decrease   the average number of green square distractors per   example by about 75 % in the train set and retrain   the model . The model performance immediately   jumps to 100 % , thereby confirming our hypothesis .   Similarly , ReaSCAN also has a test split ( A2 ) ,   which is analogous to the A1 split in Referring Ex-   pressions . We observe that the Transformer model   performs comparatively worse on A2 than other   similar splits on ReaSCAN . Considering the above-   mentioned result , we believe that the ReaSCAN A2   split is also suffers from the same issue . Hence , we   modify the training distribution to vary the number   of examples where “ red square ” object occurs as a   distractor . We show our results in Figure 13 . Note   that the size of train set in this experiment is 200 K   examples which is less than half of the full ReaS-   CAN trainset . Even with less number of training   samples , the model trained on the modified training   distribution ( No examples contain Red Squares as   distractors ) outperforms the model trained on full   ReaSCAN , on the A2 split . This validates our hy-   pothesis about the overfitting issue in transformers.665666667668669