  Bin Wang , C.-C. Jay Kuo , Haizhou LiNational University of Singapore , SingaporeUniversity of Southern California , USAThe Chinese University of Hong Kong , Shenzhen , ChinaKriston AI , China   bwang28c@gmail.com   Abstract   Word and sentence embeddings are useful fea-   ture representations in natural language pro-   cessing . However , intrinsic evaluation for em-   beddings lags far behind , and there has been   no signiﬁcant update since the past decade .   Word and sentence similarity tasks have be-   come the de facto evaluation method . It leads   models to overﬁt to such evaluations , nega-   tively impacting embedding models ’ develop-   ment . This paper ﬁrst points out the prob-   lems using semantic similarity as the gold stan-   dard for word and sentence embedding evalua-   tions . Further , we propose a new intrinsic eval-   uation method called EvalRank , which shows   a much stronger correlation with downstream   tasks . Extensive experiments are conducted   based on 60 + models and popular datasets to   certify our judgments . Finally , the practical   evaluation toolkit is released for future bench-   marking purposes .   1 Introduction   Distributed representation of words ( Bengio et al . ,   2003 ; Mikolov et al . , 2013a ; Pennington et al . ,   2014 ; Bojanowski et al . , 2017 ) and sentences   ( Kiros et al . , 2015 ; Conneau et al . , 2017 ; Reimers   and Gurevych , 2019 ; Gao et al . , 2021 ) have shown   to be extremely useful in transfer learning to many   NLP tasks . Therefore , it plays an essential role in   how we evaluate the quality of embedding models .   Among many evaluation methods , the word and   sentence similarity task gradually becomes the de   facto intrinsic evaluation method .   Figure 1 shows examples from word and sen-   tence similarity datasets . In general , the datasets   consist of pairs of words ( w;w ) ( or sentences )   and human - annotated similarity scores S. To eval-   uate an embedding model  (  ) , we ﬁrst extract em-   beddings for ( w , w ): ( e;e ) = (  ( w ) ;  ( w)).Figure 1 : Word and sentence pairs with human-   annotated similarity scores from WS-353 and STS - B   datasets ( scaled to range 0 ( lowest ) to 10 ( highest ) ) .   Then , a similarity measure is applied to compute an   predicted score S = sim(e;e ) , where cosine   similarity is adopted as sim unquestionably in the   majority of cases . Finally , the correlation between   SandSis computed , and a higher correlation   suggests good alignment with human annotations   and a better embedding model .   Many studies , especially those targeting on infor-   mation retrieval via semantic search and clustering   ( Reimers and Gurevych , 2019 ; Su et al . , 2021 ) ,   have used the similarity task as the only or main   evaluate method ( Tissier et al . , 2017 ; Mu et al . ,   2018 ; Arora et al . , 2017 ; Li et al . , 2020 ; Gao et al . ,   2021 ) . We observe a number of issues in word   or sentence similarity tasks ranging from dataset   collection to the evaluation paradigm , and consider   that focusing too much on similarity tasks would   negatively impact the development of future em-   bedding models .   The signiﬁcant concerns are summarized as fol-   lows , which generally apply to both word and   sentence similarity tasks . First , the deﬁnition of   similarity is too vague . There exist complicated   relationships between sampled data pairs , and al-   most all relations contribute to the similarity score ,   which is challenging to non - expert annotators . Sec-   ond , the similarity evaluation tasks are not directly6060relevant to the downstream tasks . We believe it   is because of the data discrepancy between them ,   and the properties evaluated by similarity tasks are   not the ones important to downstream applications .   Third , the evaluation paradigm can be tricked with   simple post - processing methods , making it unfair   to benchmark different models .   Inspired by Spreading - Activation Theory   ( Collins and Loftus , 1975 ) , we propose to evaluate   embedding models as a retrieval task , and name it   asEvalRank to address the above issues . While   similarity tasks measure the distance between   similarity pairs from all similarity levels , EvalRank   only considers highly similar pairs from a local   perspective .   Our main contributions can be summarized as   follows :   1We point out three signiﬁcant problems for   using word and sentence similarity tasks as the   de facto evaluation method through analysis or   experimental veriﬁcation . The study provides   valuable insights into embeddings evaluation   methods .   2We propose a new intrinsic evaluation method ,   EvalRank , that aligns better with the proper-   ties required by various downstream tasks .   3We conduct extensive experiments with 60 +   models and 10 downstream tasks to certify   the effectiveness of our evaluation method .   The practical evaluation toolkit is released for   future benchmarking purposes .   2 Related Work   Word embedding has been studied extensively , and   popular work ( Mikolov et al . , 2013a ; Pennington   et al . , 2014 ; Bojanowski et al . , 2017 ) are mainly   built on the distributional hypothesis ( Harris , 1954 ) ,   where words that appear in the same context tend   to share similar meanings . The early work on sen-   tence embedding are either built upon word em-   bedding ( Arora et al . , 2017 ; Rücklé et al . , 2018 ;   Almarwani et al . , 2019 ) or follow the distributional   hypothesis on a sentence level ( Kiros et al . , 2015 ;   Hill et al . , 2016 ; Logeswaran and Lee , 2018 ) . Re-   cent development of sentence embedding are in-   corporating quite different techniques including   multi - task learning ( Cer et al . , 2018 ) , supervised   inference data ( Conneau et al . , 2017 ; Reimers and   Gurevych , 2019 ) , contrastive learning ( Zhang et al . ,2020 ; Carlsson et al . , 2020 ; Yan et al . , 2021 ; Gao   et al . , 2021 ) and pre - trained language models ( Li   et al . , 2020 ; Wang and Kuo , 2020 ; Su et al . , 2021 ) .   Nonetheless , even though different methods choose   different evaluation tasks , similarity task is usually   the shared task for benchmarking purposes .   Similarity task is originally proposed to mimic   human ’s perception about the similarity level be-   tween word or sentence pairs . The ﬁrst word sim-   ilarity dataset was collected in 1965 ( Rubenstein   and Goodenough , 1965 ) , which consists of 65 word   pairs with human annotations . It has been a stan-   dard evaluation paradigm to use cosine similarity   between vectors for computing the correlation with   human judges ( Agirre et al . , 2009 ) . Many stud-   ies raise concerns about such evaluation paradigm .   Faruqui et al . ( 2016 ) and Wang et al . ( 2019b )   points out some problems with word similarity   tasks , including low correlation with downstream   tasks and lack of task - speciﬁc similarity . Reimers   et al . ( 2016 ) , Eger et al . ( 2019 ) and Zhelezniak   et al . ( 2019 ) states current evaluation paradigm for   Semantic Textual Similarity ( STS ) tasks are not   ideal . One most recent work ( Abdalla et al . , 2021 )   questions about the data collection process of STS   datasets and creates a new semantic relatedness   dataset ( STR ) by comparative annotations ( Lou-   viere and Woodworth , 1991 ) .   There are also other intrinsic evaluation meth-   ods for word and sentence embedding evaluation ,   but eventually did not gain much popularity . Word   analogy task is ﬁrst proposed in ( Mikolov et al . ,   2013a , c ) to detect linguistic relations between pairs   of word vectors . Zhu and de Melo ( 2020 ) recently   expanded the analogy concept to sentence level .   However , the analogy task is more heuristic and   fragile as an evaluation method ( Gladkova et al . ,   2016 ; Rogers et al . , 2017 ) . Recently , probing tasks   have been proposed to measure intriguing proper-   ties of sentence embedding models without worry-   ing much about practical applications ( Zhu et al . ,   2018 ; Conneau et al . , 2018 ; Barancíková and Bo-   jar , 2019 ) . Because of the lack of effective in-   trinsic evaluation methods , Reimers and Gurevych   ( 2019 ) and Wang et al . ( 2021 ) seeks to include   more domain - speciﬁc tasks for evaluation .   3 Problems with Similarity Tasks   In this work , we discuss the problems of similarity   tasks both on word and sentence levels . They are   highly similar from data collection to evaluation6061paradigm and are troubled by the same problems .   3.1 Multifaceted Relationships   First , the concept of similarity and relatedness   are not well - deﬁned . Similar pairs are related   but not vise versa . Taking synonym , hypernym ,   and antonym relations as examples , the similarity   rank should be “ synonym > hypernym > antonym ”   while the relatedness rank should be “ synonym >   hypernymantonym ” . This was not taken into   consideration when constructing datasets . Agirre   et al . ( 2009 ) intentionally split one word similar-   ity dataset into similarity and relatedness subsets .   However , we ﬁnd that obtained subsets are erro-   neous towards polysemy , and the relatedness be-   tween pair ( ‘ stock ’ , ‘ egg ’ , 1.81 ) is much lower than   pair ( ‘ stock ’ , ‘ oil ’ , 6.34 ) . It is because only the   ‘ ﬁnancial stock market ’ is compared but not the   ‘ stock of supermarkets ‘ . Furthermore , relationships   between samples are far more complicated than   currently considered , which is a challenge to all   current datasets .   Second , the annotation process is not intuitive   to humans . The initial goal of the similarity task   is to let the model mimic human perception . How-   ever , we found that the instructions on similarity   levels are not well deﬁned . For example , on STS   1316 datasets , annotators must label sentences   that ‘ share some details ’ with a score of 2 and   ‘ on the same topic ’ with a score of 1 . Accord-   ing to priming effect theory , ( Meyer and Schvan-   eveldt , 1971 ; Weingarten et al . , 2016 ) , humans are   more familiar with ranking several candidate sam-   ples based on one pivot sample ( priming stimulus ) .   Therefore , a more ideal way of annotation is to   give one pivot sample ( e.g. ‘ cup ’ ) and rank candi-   dates with different similarity levels ( e.g. ‘ trophy ’ ,   ‘ tableware ’ , ‘ food ’ , ‘ article ’ , ‘ cucumber ’ ) . In other   words , it is more intuitive for human to compare   ( a , b)>(a , c ) than ( a , b ) > ( c , d ) as far as similarity   is concerned . However , in practice , it is hard to   collect a set of candidates for each pivot sample ,   especially for sentences .   3.2 Weak Correlation with Downstream   Tasks   In previous studies , it was found that the perfor-   mance of similarity tasks shows little or negative   correlation with the performance of downstream   tasks ( Faruqui et al . , 2016 ; Wang et al . , 2019b ,   2021 ) . An illustration is shown in Table 1a . We   think there are two reasons behind 1 ) low testing   corpus overlap and 2 ) mismatch of tested proper-   ties .   First , similarity datasets have their data source   and are not necessarily close to the corpus of down-   stream tasks . For example , Baker et al . ( 2014 )   collect word pairs for verbs only while Luong et al .   ( 2013 ) intentionally test on rare words . Also , for   STS datasets , ( Agirre et al . , 2012 ) annotates on sen-   tence pairs from paraphrases , video captions , and   machine translations , which has limited overlap on   downstream tasks like sentiment classiﬁcation .   Second , the original goal for the similarity task   is to mimic human perceptions . For example , STS   datasets are originally proposed as a competition   to ﬁnd the most effective STS systems instead of   a gold standard for generic sentence embedding   evaluation . Some properties evaluated by similar-   ity tasks are trivial to downstream tasks , and it is   more important to test on mutually important ones .   As examples in Figure 1 , the similarity tasks inher-   ently require the model to predict sim ( p)>sim(p )   and sim(p)>sim(p ) , which we believe are unnec-   essary for most downstream applications . Instead ,   similar pairs are more important than less simi-   lar pairs for downstream applications ( Kekäläinen ,   2005 ; Reimers et al . , 2016 ) . Therefore , it is enough   for good embedding models to focus on gathering   similar pairs together while keeping dissimilar ones   far away to a certain threshold.6062   3.3 Overﬁtting   As similarity tasks become one de facto evalua-   tion method for embedding models , recent work   tend to overﬁt the current evaluation paradigm , in-   cluding the choice of similarity measure and the   post - processing step .   Similarity Metrics . Cosine similarity is the de-   fault choice for similarity tasks . However , simply   changing the similarity metric to other commonly   used ones can lead to contradictory results .   In Table 1b , we compare recent ﬁve BERT - based   sentence embedding models including BERT ( De-   vlin et al . , 2019 ) , BERT - whitening ( Su et al . , 2021 ) ,   BERT-ﬂow ( Li et al . , 2020 ) , SBERT ( Reimers and   Gurevych , 2019 ) and SimCSE ( Gao et al . , 2021 ) .   The results on standard STS - Benchmark testset are   reported under both cosine and lsimilarity . As   we can see , the performance rank differs under dif-   ferent similarity metrics . This is especially true   for BERT-ﬂow and BERT - whitening , which do not   even outperform their baseline models when evalu-   ating withlmetric . Therefore , we can infer that   some models overﬁt to the default cosine metric   for similarity tasks .   Whitening Tricks . A number of studies attempted   the post - processing of word embeddings ( Mu et al . ,   2018 ; Wang et al . , 2019a ; Liu et al . , 2019b ) and   sentence embeddings ( Arora et al . , 2017 ; Liu et al . ,   2019a ; Li et al . , 2020 ; Su et al . , 2021 ) . The shared   concept is to obtain a more isotropic embedding   space ( samples evenly distributed across directions )   and can be summarized as a space whitening pro-   cess . Even though the whitening tricks help a lot   with similarity tasks , we found it is usually not   applicable to downstream tasks or even hurt the   model performance . We think the whitening meth-   ods are overﬁtted to similarity tasks and would like   to ﬁnd the reasons behind .   First , we take the whole STS - Benchmark dataset   and create subsets of sentence pairs from certain   similarity levels . We test on two baseline sentence   embedding models : GloVe , BERT ; three whitening   tricks : ABTT on GloVe ( Mu et al . , 2018 ) , BERT-   whitening , BERT-ﬂow ; two strong sentence em-   bedding models that perform well on both STS   and downstream tasks : SBERT , SimCSE . Figure 2   shows the result , and we can see that the whitening-   based methods are boosting the baseline perfor-   mance mainly for less similar pairs ( e.g. , pairs   with a similarity score within [ 2,0 ] ) . In contrast ,   the models that perform well on downstream tasks   show consistent improvement on all subsets with   different similarity scores . As discussed in Section   3.2 , highly similar pairs are more critical than less   similar pairs for downstream tasks . Since the post-   processing methods mainly help with less similar   pairs , they do not help much on downstream tasks .   4 Evaluation by Ranking   4.1 Theory and Motivations   In cognitive psychology , Spreading - Activation The-   ory ( SAT ) ( Collins and Loftus , 1975 ; Anderson ,   1983 ) is to explain how concepts store and interact   within the human brain . Figure 3 shows one ex-   ample about the concept network . In the network ,   only highly related concepts are connected . To ﬁnd   the relatedness between concepts like engine and   street , the activation is spreading through mediating   concepts like carandambulance with decaying fac-   tors . Under this theory , the similarity task is mea-   suring the association between any two concepts   in the network , which requires complicated long-   distance activation propagation . Instead , to test   the soundness of the concept network , it is enough   to ensure the local connectivity between concepts .   Moreover , the long - distance relationships can be   inferred thereby with various spreading activation6063   algorithms ( Cohen and Kjeldsen , 1987 ) .   Therefore , we propose EvalRank to test only on   highly related pairs and make sure they are topo-   logically close in the embedding space . It also   alleviates the problems of similarity tasks . First ,   instead of distinguishing multifaceted relationships ,   we only focus on highly related pairs , which are   intuitive to human annotators . Second , it shows a   much stronger correlation with downstream tasks   as desired properties are measured . Third , as we   treat the embedding space from a local perspective ,   it is less affected by the whitening methods .   4.2 Methodology   We frame the evaluation of embeddings as a re-   trieval task . To this purpose , the dataset of Eval-   Rank contains two sets : 1 ) the positive pair set   P = fp;p;:::;pgand 2 ) the background sam-   ple setC = fc;c;:::;cg . Each positive pair   p= ( c;c)inPconsists of two samples in C   that are semantically similar .   For each sample ( c ) and its positive correspon-   dence ( c ) , a good embedding model should has   their embeddings ( e , e ) close in the embedding   space . Meantime , the other background samples   should locate farther away from the sample c.   Some samples in the background may also be posi-   tive samples . We assume it barely happens and is   negligible if good datasets are constructed .   Formally , given an embedding model  (  ) , the   embeddings for all samples in Care computed as   fe;e;:::;eg = f  ( c ) ;  ( c ) ; : : : ;   ( c)g . The   cossimilarity and lsimilarity between two sam-   ples ( c;c ) are deﬁned as :   S(c;c ) = ee   jjejjjjejj   S(c;c ) = 1   1 + jje ejj   Further , the similarity score is used to sort all back-   ground samples in descending order and the per-   formance at each positive pair pis measured by   the rank ofc ’s positive correspondence cw.r.t all   background samples :   rank = rank ( S(c;c);[jjS(c;c)])wherejjrefers to the concatenation operation . To   measure the overall performance of model  ( )on   all positive pairs in P , the mean reciprocal rank   ( MRR ) and Hits@k scores are reported and a higher   score indicates a better embedding model :   MRR = 1   mX1   rank   Hits @k=1   mX1[rankk ]   Note that there are two similarity metrics , and we   found thatSshows a better correlation with   downstream tasks while Sis more robust to   whitening methods . We use Sin the experiments   unless otherwise speciﬁed .   4.3 Dataset Collection   Word - Level . We collect the positive pairs from 13   word similarity datasets ( Wang et al . , 2019b ) . For   each dataset , the pairs with the highest 25 % sim-   ilarity score are gathered as positive pairs . Back-   ground word samples contain all words that appear   in the similarity datasets . Further , we augment the   background word samples using the most frequent   20,000 words from Wikipedia corpus .   Sentence - Level . Similarly , the pairs with top 25 %   similarity / relatedness score from STS - Benchmark   dataset ( Cer et al . , 2017 ) and STR dataset ( Abdalla   et al . , 2021 ) are collected as positive pairs . All   sentences that appear at least once are used as the   background sentence samples .   In both cases , if positive pair ( c;c)exists , the   reversed pair ( c;c)is also added as positive pairs .   Detailed statistics of EvalRank datasets are listed   in Table 2 .   4.4 Alignment and Uniformity   Recently , Wang and Isola ( 2020 ) identiﬁes the   alignment and uniformity properties as an expla-   nation to the success of contrastive loss . It shares   many similarities with our method and can also   shed light on why EvalRank works . First , the align-   ment property requires similar samples to have sim-   ilar features , which aligns with the objective of6064   EvalRank . Second , the uniformity property is mea-   sured by the average Gaussian distance between   any two samples . In contrast , EvalRank focuses   on the distance between points from a local per-   spective and would require the pivot sample to   have longer distances to any background samples   than its positive candidate . Measuring the distance   from a local perspective has unique advantages   because the learned embedding space will likely   form a manifold and can only approximates eu-   clidean space locally . Therefore , simple similarity   metrics like cosorlare not suitable to model   long - distance relationships .   4.5 Good Intrinsic Evaluator   A good intrinsic evaluator can test the properties   that semantically similar samples are close in vec-   tor space ( Reimers and Gurevych , 2019 ; Gao et al . ,   2021 ) and serve as prompt information to real-   world applications . As EvalRank directly test on   the ﬁrst property , we design experiments to show   the correlation with various downstream tasks as   a comparison of intrinsic evaluators . To be com-   prehensive , we ﬁrst collect as many embedding   models as possible and test them on the intrinsic   evaluator and downstream task . The Spearman ’s   rank correlation is computed between the results ,   and a higher score indicates better correlation with   downstream tasks and better intrinsic evaluator .   Meantime , we do not think similarity evaluations   should be discarded , even though it fails to corre-   late well with downstream applications . It has its   advantages as aiming to mimic human perception   about semantic - related pairs.5 Word - Level Experiments   5.1 Experimental Setup   Word Embedding Models . We collect 19 word   embedding models from GloVe ( Pennington et al . ,   2014 ) , word2vec ( Mikolov et al . , 2013b ) , fastText   ( Bojanowski et al . , 2017 ) , Dict2vec ( Tissier et al . ,   2017 ) and PSL ( Wieting et al . , 2015 ) . Meantime ,   we apply ABTT ( Mu et al . , 2018 ) post - processing   to all models to double the total number of em-   bedding models . When testing on downstream   tasks , the simplest bag - of - words feature is used   as sentence representations in order to focus on   measuring the quality of word embeddings .   Word Similarity Tasks . 9 word similarity datasets   are compared as the baseline methods including   WS-353 - All ( Finkelstein et al . , 2001 ) , WS-353-   Rel ( Agirre et al . , 2009 ) , WS-353 - Sim ( Agirre   et al . , 2009 ) , RW - STANFORD ( Luong et al . , 2013 ) ,   MEN - TR-3 K ( Bruni et al . , 2014 ) , MTURK-287   ( Radinsky et al . , 2011 ) , MTURK-771 ( Halawi   et al . , 2012 ) , SIMLEX-999 ( Hill et al . , 2015 ) ,   SIMVERB-3500 ( Gerz et al . , 2016 ) . The word   similarity datasets with less than 200 pairs are not   selected to avoid evaluation occasionality . Cosine   similarity and Spearman ’s rank correlation are de-   ployed for all similarity tasks .   Downstream Tasks . SentEval ( Conneau and   Kiela , 2018 ) is a popular toolkit in evaluating sen-   tence embeddings . We use 9 downstream tasks   from SentEval including MR ( Pang and Lee , 2005 ) ,   CR ( Hu and Liu , 2004 ) , MPQA ( Wiebe et al . ,   2005 ) , SUBJ ( Pang and Lee , 2004 ) , SST2 ( Socher   et al . , 2013 ) , SST5 ( Socher et al . , 2013 ) , TREC   ( Li and Roth , 2002 ) , MRPC ( Dolan et al . , 2004 ) ,   SICK - E ( Marelli et al . , 2014 ) . Previous work spot6065   that SentEval tasks are biased towards sentiment   analysis ( Wang et al . , 2018 ) . Therefore , we add one   extra domain - speciﬁc classiﬁcation task SCICITE   ( Cohan et al . , 2019 ) which assigns intent labels   ( background information , method , result compari-   son ) to sentences collected from scientiﬁc papers   that cite other papers . For all tasks , a logistic re-   gression classiﬁer is used with cross - validation to   predict the class labels .   5.2 Results and Analysis   Table 3 shows the word - level results . In short ,   EvalRank outperforms all word similarity datasets   with a clear margin . For evaluation metrics , we   can see that Hits@3 score shows a higher corre-   lation than MRR and Hits@1 scores . However ,   the gap between the evaluation metrics is not big ,   which makes them all good measures . Among all   10 downstream tasks , EvalRank shows a strong   correlation ( >0.6 ) with 7 tasks and a very strong   correlation ( >0.8 ) with 5 tasks . While , among   all word similarity datasets , only one dataset ( RW-   STANFORD ) shows a strong correlation with one   downstream task ( SST2 ) .   For word similarity datasets , RW - STANFORD   dataset shows the best correlation with downstream   tasks . It conﬁrms the ﬁnding in Wang et al . ( 2019b )   that this dataset contains more high - quality and   low - frequency word pairs .   Ablation Study . We experiment with several vari-   ants of our EvalRank method and the result is   shown in Table 4 . First , if we do not augment the   background word samples with the most frequent   20,000 words from the Wikipedia corpus , it leads   to certain performance downgrading . Without suf-   ﬁcient background samples , positive pairs are not   challenging enough to test each model ’s capabil-   ity . Second , we tried to add more positive samples   ( e.g. 5k samples ) using synonym relations from   WordNet ( WN ) database ( Miller , 1998 ) . However ,   no obvious improvement is witnessed because the   synonym pairs in WN contain too many noisy pairs .   Last , for similarity measures , we notice that cos   similarity is consistently better than lsimilarity   while both outperform word similarity baselines .   Benchmarking Results . In Table 5a , we com-   pared four popular word embedding models , in-   cluding GloVe , word2vec , fastText , and Dict2vec ,   where fastText achieves the best performance .   6 Sentence - Level Experiments   6.1 Experimental Setup   Sentence Embedding Models . We collect 67 em-   bedding models , where 38 of them are built upon   word embeddings with bag - of - words features and   29 of them are neural - network - based models . For   neural - network - based models , we collect variants   from InferSent ( Conneau et al . , 2017 ) , BERT ( De-   vlin et al . , 2019 ) , RoBERTa ( Liu et al . , 2020 ) ,   BERT-ﬂow ( Li et al . , 2020 ) , BERT - whitening ( Su   et al . , 2021 ) , SBERT ( Reimers and Gurevych ,   2019 ) and SimCSE ( Gao et al . , 2021 ) .   Sentence Similarity Tasks . We evaluate on 7 stan-   dard semantic textual similarity datasets includ-   ing STS1216 ( Agirre et al . , 2012 , 2013 , 2014 ,   2015 , 2016 ) , STS - Benchmark ( Cer et al . , 2017 )   and SICK - Relatedness ( Marelli et al . , 2014 ) . Re-   cently , Abdalla et al . ( 2021 ) questioned the labeling   process of STS datasets and released a new seman-   tic textual relatedness ( STR ) dataset , which is also   included in our experiments .   Downstream Tasks . We use 7 classiﬁcation tasks6066   from SentEval evaluation toolkit , including MR ,   CR , MPQA , SUBJ , SST2 , SST5 , TREC , as well   as the domain - speciﬁc classiﬁcation task SCICITE .   We exclude the MRPC and SICK - E because they   are highly similar with STS tasks ( Conneau and   Kiela , 2018 ) .   6.2 Results and Analysis   Table 6 shows the sentence - level results . Eval-   Rank outperform all sentence similarity datasets   with a clear margin . For evaluation metric , Hits@1   shows a higher correlation comparing with MRR   and Hits@3 . Among all 7 downstream tasks , Eval-   Rank shows strong correlation (  > 0:6 ) with 6   tasks .   For sentence similarity datasets , no one clearly   outperforms others . Additionally , we found that   STR dataset shows the worst correlation with down-   stream tasks . Even though STR adopts a better data   annotation schema than STS datasets , it still fol-   lows the previous standard evaluation paradigm   and is exposed to the same problems . It further ver-   iﬁes our discussion about problems with sentence   similarity evaluation .   Correlation Visualization . Figure 4 shows the   performance rank of 67 sentence embedding mod-   els on ﬁve tasks , including 2 downstream tasks   ( MR , SST2 ) and 3 intrinsic evaluations ( STS - B ,   STR , EvalRank ) . The models ’ performance rank   on the MR task is used as the pivot .   As MR and SST2 datasets are both related to sen-   timent analysis , they correlate well with each other .   Among the three intrinsic evaluation tasks , Eval-   Rank shows a higher correlation with downstream   tasks as the blue dots roughly follow the trend of   red dots . In contrast , the dots of STS - B and STR   are dispersed in different regions . This shows that   the performance of STS - B and STR is not a good   indicator of the performance on downstream tasks .   Ablation Study . In Table 7 , we show the perfor-6067mance of EvalRank with different data sources . By   combining the positive pairs collected from both   STS - B and STR datasets , EvalRank leads to the   best performance . Interestingly , according to our   results , even though STR evaluation does not corre-   late well with downstream tasks , the positive pairs   collected from STR have better quality than STS - B.   It also conﬁrms the argument that STR improves   the dataset collection process ( Abdalla et al . , 2021 ) .   Benchmarking Results . Table 5b benchmarked   seven popular sentence embedding models . As   the widely accepted SOTA model , SimCSE outper-   forms others with a clear margin .   7 Conclusion   In this work , we ﬁrst discuss the problems with cur-   rent word and sentence similarity evaluations and   proposed EvalRank , an effective intrinsic evalua-   tion method for word and sentence embedding mod-   els . It shows a higher correlation with downstream   tasks . We believe that our evaluation method can   have a broader impact in developing future embed-   ding evaluation methods , including but not limited   to its multilingual and task - speciﬁc extensions .   Acknowledgement   This research is supported by the Agency for Sci-   ence , Technology and Research ( A*STAR ) under   its AME Programmatic Funding Scheme ( Project   No . A18A2b0046 ) and Science and Engineering   Research Council , Agency of Science , Technology   and Research ( A*STAR ) , Singapore , through the   National Robotics Program under Human - Robot   Interaction Phase 1 ( Grant No . 192 25 00054 ) .   References6068606960706071   A Embedding Models   A good intrinsic evaluator should be a good indi-   cator for downstream tasks . We want to compute   the correlation between the results from intrinsic   evaluators and downstream tasks to measure the   quality of intrinsic evaluators . For this purpose , we   collect as many models as possible and ﬁnally in-   volved 38 word embedding models and 67 sentence   embedding models in our experiments . We give a   detailed introduction to the collected embedding   models in this section .   A complete set of selected word embedding mod-   els is shown in Table 8 . We collect pre - trained word   embeddings with different dimensions and training   corpus from GloVe , word2vec , fastText , Dict2vec ,   and PSL . ABTT ( Mu et al . , 2018 ) post - processing   is further applied to each model to double the total   number of word embedding models .   A complete set of selected sentence embed-   ding models is shown in Table 9 . Besides the   models obtained using bag - of - words features from   word embeddings , we also include popular neural-   network - based models including InferSent , BERT ,   RoBERTa , SBERT , BERT - whitening , BERT-ﬂow ,   and SimCSE . Different variants of these models   are considered in order to be more comprehensive .   B More Experimental Details   In Section 3 , we conduct several experiments to cer-   tify our judgments , and we would like to elaborate   on the detailed experiment settings here .   In Table 1b , the performance rank of ﬁve BERT-   based sentence embedding models are shown under   bothcosandldistance measure . Detailed model   settings are shown below:6072•SBERT : BERT - base model trained on   Natural Language Inference data with mean   token embeddings .   https://huggingface.co/   sentence - transformers/   bert - base - nli - mean - tokens   •SimCSE : Unsupervised SimCSE trained   upon BERT - based - uncased .   https://huggingface .   co / princeton - nlp/   unsup - simcse - bert - base - uncased   •BERT : BERT - based uncased model   https://huggingface.co/   bert - base - uncased   •BERT-ﬂow : We use the BERT - base - uncased   and average the word representations from the   ﬁrst and last layers as the sentence represen-   tation . The Gaussian mapping is trained on   the target corpus , which is the STS - B testset   in our case .   •BERT - whitening : Similar to BERT-ﬂow , the   averaging of word representation from ﬁrst   and last layers are used as sentence represen-   tations , and the BERT - base - uncased model is   used . The whitening objective is computed   using the target corpus .   In Table 1a , 6 models are selected , and their   performance on one similarity task : STS - B and   two downstream tasks : MR and SST2 are reported .   The setting of the models follows the experiments   in Table 1b . For GloVe and InferSent , the following   settings are used :   •GloVe : 300 - dimensional vector trained on   Common Crawl corpus ( 840B tokens ) .   •InferSent : Version 1 of InferSent is used   where the GloVe model is served as input .   Figure 2 shows a detailed analysis on different   similarity levels . For the experiment , we ﬁrst col-   lect all sentence pairs from STS - B dataset . Then ,   we split the pairs into four subsets based on their   similarity levels ( [ 5,3],[4,2],[3,1],[2,0 ] ) . Further ,   we randomly sampled 3,000 samples for each sub-   set as the ﬁnal dataset splits to keep the number of   samples even . C More Discussions   C.1 Effect of Whitening on Downstream   Tasks   A lot whitening methods been proposed targeting   on improving the quality of word embeddings ( Mu   et al . , 2018 ; Wang et al . , 2019a ; Liu et al . , 2019b )   and sentence embeddings ( Arora et al . , 2017 ; Liu   et al . , 2019a ; Li et al . , 2020 ; Su et al . , 2021 ) . How-   ever , in previous studies , the whitening methods   are only proven to be effective with similarity tasks .   The performance comparison on downstream tasks   is either missing or limited .   Therefore , we conduct extensive experiments on   two popular post - processing methods . For word   embedding , the ABTT ( Mu et al . , 2018 ) post-   processing technique is examined . For sentence   embedding , the Principal Component Removal   ( Arora et al . , 2017 ) method is applied for word-   embedding - based models and BERT - whitening ( Su   et al . , 2021 ) or BERT-ﬂow ( Li et al . , 2020 ) is ap-   plied to BERT - based models . Arora et al . ( 2017 )   propose a weighting schema and post - processing   step for sentence embeddings . Here , we solely test   the effectiveness of the post - processing step .   Table 10 shows the performance comparison be-   tween the original model and the post - processed   model . From both word - level and sentence - level   experiments , we conclude that the post - processing   methods play no obvious role or even hurt the per-   formance in downstream tasks . In contrast , the   results on similarity tasks improve a lot .   C.2 Alignment and Uniformity   Wang and Isola ( 2020 ) discussed alignment and uni-   formity property as an explanation to the success   of contrastive learning . EvalRank can be viewed   as a variant of these two measures and focus more   on the local perspective . Therefore , the success   ofEvalRank also can be explained under the same   umbrella . Meantime , measuring from a local per-   spective is more suitable for word and sentence   embedding models because they are likely to form   a manifold and can only approximate euclidean   space locally .   Alignment : In Wang and Isola ( 2020 ) , the align-   ment loss is deﬁned with the average distance be-   tween positive samples :   L(f ;  ) = E[jjf(x) f(y)jj ]   It measures the total distance between positive   pairs , and the smaller , the better . The alignment6073measure does not consider the local properties of   the embedding space . In contrast , EvalRank re-   quires the positive pairs to be close in the embed-   ding space while considering the density of the   local embedding regions . If the density of embed-   ding space around positive pairs is high , EvalRank   method requires the embeddings of positive pairs   to be more tightly closed . If the density of embed-   ding space around positive pairs is low , EvalRank   has a looser distance requirement for the positive   pairs .   Uniformity : In Wang and Isola ( 2020 ) , the uni-   formity loss is designed as the logarithm of the   average pairwise Gaussian potential :   L ( f;t ) = log E[e ]   Intuitive , the uniformity loss asks features to be far   away from each other . In contrast , EvalRank score   focus on a local perspective . It requires the negative   samples to have larger embedding distances than   positive samples concerning the pivot sample . For   the negative samples that are far away from the   pivot sample in the embedding space , they are less   likely to be confusing with positive samples and ,   therefore , not considered as important .   C.3 Correlation Results without   Post - Processing Models   In previous experiments , we select as many mod-   els as possible in order to be more comprehensive .   However , the side effect is that a reasonable por-   tion of the models is built with post - processing   techniques . It may lead to some concern that our   selected embedding models might be biased on   post - processed models . Therefore , we re - do the   experiments on sentence embedding evaluations   without considering post - processed models .   We ﬁlter out all models related to post-   processing techniques , and as a result , 34 sentence   embedding models are kept . We further conduct   correlation analysis between the performance on   intrinsic evaluation methods and downstream tasks .   The result is shown in Table 11 . As we can   see , EvalRank still outperforms sentence similarity   tasks in 7 of the tasks . And we can witness a higher   correlation between EvalRank and the downstream   tasks comparing with the results in Table 6 . Eval-   Rank shows strong correlation (  > 0:6 ) on all 8   tasks and very strong correlation ( >0:8 ) on 7 of   the tasks . The result again proves the effectiveness   ofEvalRank .6074607560766077