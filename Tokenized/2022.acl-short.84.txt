  Ling Liu and Mans Hulden   University of Colorado   first.last@colorado.edu   Abstract   Deep learning sequence models have been suc-   cessful with morphological inﬂection genera-   tion . The SIGMORPHON shared task results   in the past several years indicate that such mod-   els can perform well , but only if the training   data covers a good amount of different lem-   mata , or if the lemmata to be inﬂected at test   time have also been seen in training , as has in-   deed been largely the case in these tasks . Sur-   prisingly , we ﬁnd that standard models such as   the Transformer almost completely fail at gen-   eralizing inﬂection patterns when trained on a   limited number of lemmata and asked to inﬂect   previously unseen lemmata — i.e. under “ wug   test”-like circumstances . This is true even   though the actual number of training examples   is very large . While established data augmen-   tation techniques can be employed to allevi-   ate this shortcoming by introducing a copying   bias through hallucinating synthetic new word   forms using the alphabet in the language at   hand , our experiment results show that , to be   more effective , the hallucination process needs   to pay attention to substrings of syllable - like   length rather than individual characters .   1 Introduction   The Transformer model has delivered convincing   results in many different tasks related to word-   formation and analysis ( Vylomova et al . , 2020 ;   Moeller et al . , 2020 , 2021 ; Liu , 2021 ) . Especially   on inﬂection tasks , where an input lemma such   asdog , and input inﬂectional features such as   { N , PL } , are expected to produce an output such as   dogs , the model has shown to be particularly adept   at generalizing patterns ( Vylomova et al . , 2020 ; Liu   and Hulden , 2020a , b ; Wu et al . , 2021 ) . However ,   we have discovered that this is only true if the train-   ing data covers a diversity of lemmata or some   variant of the input lemma to be inﬂected has beenFigure 1 : Transformer performance in the common-   practice setting ( left ) , “ wug test ” -like setting ( middle ) ,   and ‘ ‘ wug test”-like setting with our best data hallu-   cination method ( right )   witnessed during training . In a “ wug test ” ( Berko ,   1958 ) setting where the witnessed lemmata are usu-   ally limited and a previously unseen lemma — like   wug — is to be inﬂected in some way , we ﬁnd that   the Transformer almost completely fails to general-   ize inﬂection patterns , despite abundant inﬂected   forms for training . It has been noted earlier that   neural sequence - to - sequence models are apt to per-   form poorly for morphological inﬂection if they   have been exposed to little training data and data   augmentation can be leveraged to alleviate the prob-   lem ( Cotterell et al . , 2017 , 2018 ; Kann and Schütze ,   2017 ; Liu and Hulden , 2021 ) . Our starting point   is our observation that the poor “ wug test ” perfor-   mance is maintained even with abundant training   inﬂected forms .   In our study , we show three main results . ( 1 )   We demonstrate that , even if trained with relatively   large amounts of inﬂected forms , a Transformer   model of the kind that has been very successful at   recent shared tasks largely fails to generalize in-   ﬂection patterns if it has not been exposed during   training to a variety of lemmata or any lemmata in739the test set . This is true even for datasets where all   words inﬂect in the same way — i.e. there are no in-   ﬂectional classes or allomorphs of morphemes , as   is found in the low - resource Niger - Congo language   datasets used in SIGMORPHON 2020 shared task   ( Vylomova et al . , 2020 ) . ( 2 ) We show that simply   exposing the model to uninﬂected lemmata in the   test set — without providing a single inﬂected form —   allows the model to dramatically improve its perfor-   mance when inﬂecting such lemmata . ( 3 ) Further ,   we investigate several strategies that avoid leverag-   ing test set lemmata . We show that when inducing   a copy bias in the model by hallucinating new lem-   mata , or by hallucinating new inﬂected forms , the   method of hallucination is much more effective if   it is sensitive to substrings of syllable - like length   rather than individual characters or stems . Our   best models achieve substantial improvement upon   earlier state - of - the - art data hallucination methods   ( Silfverberg et al . , 2017 ; Anastasopoulos and Neu-   big , 2019 ) .   2 Data   2018 - languages We use six languages from   the CoNLL - SIGMORPHON 2018 shared task 1   medium setting , where each language has 1,000 triples   for training ( Cotterell et al . , 2018 ) . The six   languages , Czech , Finnish , German , Russian ,   Spanish and Turkish , are selected to represent the   diversity of language typology and morphological   inﬂection challenges . Though there are only   1,000 training triples , they cover a fair number   of lemmata as each lemma appears only once or   twice , an amount very hard to obtain for really   low - resource languages . In the original shared   task , between 2 % and 27 % of the lemmata in the   dev and test sets are also found in the training set .   To prepare training data for the “ wug test”-like   circumstance , we select the UniMorph ( Kirov et al . ,   2018 ) paradigms for the ﬁrst 100 most frequent   lexemes found in Wikipedia text , which are not   included in the 2018 shared task 1 dev and test   sets . The shared task dev and test sets are used for   validation and evaluation without any change . The   100 full inﬂection tables give us over 1,000 ( for   Czech , German and Russian ) or over 7,000 ( for   Finnish , Spanish and Turkish ) training triples . Niger - Congo languages In addition , we use   six Niger - Congo languages from SIGMORPHON   2020 shared task 0 ( Vylomova et al . , 2020 ): Akan ,   Ga , Lingala , Nyanja , Southern Sotho and Swahili .   These languages are low - resource , but the dataset   only contains very regular inﬂections . In the orig-   inal shared task data split , The overlap between   the lemmata in the dev and test sets and those in   the training set is 100 % . The number of paradigms   which we can obtain by combining the training , dev   and test sets of this dataset is around 100 for Akan ,   Ga and Swahili , 227 for Nyanja , 57 for Lingala and   only 26 for Southern Sotho .   For the “ wug test ” , we divide the inﬂection tables   reconstructed from this dataset into a 7:1:2 train-   dev - test split , i.e. we use the same ratio as the   shared task , but the division is by inﬂection tables   rather than lemma - tag - form triples , to ensure that   the lemmata used for validation and test are disjoint   from those for training . We provide details on the   data statistics in Appendix A for reference .   3 Experiments   Inﬂection model The Transformer ( Vaswani   et al . , 2017 ) is the seq2seq architecture which pro-   duces the current state - of - the - art result on the mor-   phological inﬂection task ( Vylomova et al . , 2020 ;   Liu and Hulden , 2020a , b ; Wu et al . , 2021 ) . It takes   the lemma and target tag(s ) as input and predicts   the target form character by character . Our experi-   ments use the Transformer implemented in fairseq   ( Ott et al . , 2019 ) and adopt the same hyperparame-   ters as Liu and Hulden ( 2020a ) .   Evaluation metric The evaluation metric is ac-   curacy . For the original shared task data and exper-   iments on 2018 languages , we train ﬁve inﬂection   models each with a different random initialization   and report the average accuracy with standard de-   viation . Due to data scarcity , for Niger - Congo lan-   guages at the “ wug test”-like setting , we perform   a 5 - fold cross - validation and report the average   accuracy and the standard deviation .   Common - practice test and “ wug test ” We ﬁrst   compare the performance of the Transformer in the   common - practice setting and the “ wug test”-like   setting . The “ common practice ” is represented by740   previous years ’ shared tasks and related work ( Cot-   terell et al . , 2016 , 2017 , 2018 ; McCarthy et al . ,   2019 ; Vylomova et al . , 2020 ) ; here the training   data usually covers a fair number of lemmata and   there is overlap between lemmata in the training   and test sets . We use the shared task data to rep-   resent the common - practice setting . In the “ wug   test ” setting , we control the number of lemmata   for training but not inﬂected forms ( as explained   in Section 2 ) and the lemmata to be inﬂected are   always previously unseen . To our surprise , the per-   formance of the Transformer at the “ wug test”-like   setting is very poor despite the large amount of   training triples for 2018 - languages or the very reg-   ular and straightforward inﬂection for Niger - Congo   languages . The performance is dramatically infe-   rior to the common - practice setting , even when the   number of training triples is seven times larger for   Finnish , Spanish and Turkish ( see Figure 1 ) .   We hypothesize four reasons for the poor per-   formance of the model under the “ wug test”-like   circumstance : ( 1 ) missing copy bias regarding the   entire stem , i.e. the model ca n’t copy a stem abcde   if that exact stem has never been seen during train-   ing , ( 2 ) missing copy bias on individual letters , i.e.   the model ca n’t copy letter aif the letter is under-   represented in training , ( 3 ) missing copy bias on   subsequences of letters , i.e. the model ca n’t copy   sequence abif the sequence is underrepresented in   training , ( 4 ) some combination of all the factorsabove . To test these hypotheses , we conduct ﬁve   experiments designed to help the model learn to   copy with different biases by adding to the training   set for each language 2,000dummy data points   generated in ﬁve different ways , explained below .   + copy - dev - test - lemmas In order to test the ﬁrst   hypothesis that the model does not learn to copy   parts of a stem it has not seen at the training stage ,   we augment the training data for each language by   adding to it the lemmata in its development and   test sets with a special tag COPY . In other words ,   2,000 triples are added to   the initial “ wug test ” training set for each language .   + copy-2k - char and + copy-2k - substr Previous   work found that adding random strings can help   seq2seq models learn a copy bias and thus improve   the performance when the training data is limited   ( Kann and Schütze , 2017 ) . We adopt a similar   method to augment the training data with dummy   lemmata generated by the process shown in Figure   2 ( a ) . The + copy-2k - char method takes as input   the alphabet created by collecting characters in the   language ’s training set .   Considering that a natural linguistic sub - unit   of a word is a syllable , we propose to use sub-741   strings of syllable - like length for the + copy-2k-   substr method . The input of this method is the   set of bigrams , trigrams and four - grams from the   language ’s training data . For both methods , we   generate the dummy lemma by uniformly sampling   from the input and concatenating the sampled items   to a random length between the minimum and max-   imum word length we see in the training data . The   output of the dummy lemma generation process is   a triple of a dummy lemma , a special symbol COPY   and the dummy lemma , which is added to the initial   “ wug test ” training set for data augmentation .   + hall-2k - char and+hall-2k - substr The dummy   lemma generation methods do not leverage knowl-   edge about word structure which can be inferred   from the training data . Silfverberg et al . ( 2017 )   found that it is very effective to augment training   data in low - resource situations with a data halluci-   nation approach by replacing a hypothesized stem   of the training triples with a random string . Anas-   tasopoulos and Neubig ( 2019 ) improves this data   hallucination method by taking into discontinuous   stems into consideration as well ; this is the best   data hallucination method so far . We conduct the+hall-2k - char experiment by augmenting the initial   “ wug test ” training set with dummy data generated   with Anastasopoulos and Neubig ( 2019 ) ’s method .   The implementation from SIGMORPHON 2020   shared task 0 baseline is used .   In addition , we propose to generate the dummy   stem by uniformly sampling from substrings of   syllable - like length , i.e. the bigram , trigram and   four - gram set . This experiment is referred to as   + hall-2k - substr . Speciﬁcally , both data hallucina-   tion methods ( illustrated in Figure 2 ( b ) ) take as   input a triple from the training set , aligns the lemma   and the target form with the alignment method from   SIGMORPHON 2016 shared task baseline ( Cot-   terell et al . , 2016 ) , ﬁnds the common substrings   between the lemma and the target form as the stem ,   replaces the stem with a dummy stem , and out-   puts a dummy triple which is adopted for data   augmentation . Our proposed method is different   from Anastasopoulos and Neubig ( 2019 ) ’s method   at the dummy stem generation step in two main   aspects : ( 1 ) Instead of sampling from the alpha-   bet , we sample from the set of bigrams , trigrams   and four - grams . ( 2 ) Instead of forcing the dummy   stem to be of the same length as the stem to be742replaced , we only constrain the minimum and max-   imum length of the stem based on the training data .   In addition , for discontinuous stems , we only re-   place the ﬁrst part of the stem .   4 Results and discussion   “ Wug test ” with data augmentation Figure 3   shows results for the “ wug test”-like setting and   results after augmenting the initial training set with   different methods . Every language sees a substan-   tial improvement with data augmentation , indicat-   ing that the Transformer model in the vanilla “ wug   test ” circumstance will not learn a copy bias well .   The substring - based data hallucination we pro-   pose , + hall-2k - substr , achieves accuracies which   are substantially higher than other methods for   most languages . For Turkish and Nyanja , + hall-2k-   substr is lower than the best performance , but the   difference is not obvious . For Lingala , + hall-2k-   substr has the same best performance as + hall-2k-   char . The consistent advantage of + hall-2k - substr   implies that substrings of syllable - like length is   more helpful than individual characters for data   hallucination . It also provides support to the fourth   hypothesis we made in section 3 that the poor per-   formance of the Transformer in the vanilla “ wug   test”-like setting is due to a combination of factors   including missing copying bias for letters , subse-   quences of letters and even entire stems .   Common practice vs “ wug test ” Figure 1 plots   the Transformer accuracies with standard devia-   tions in the common - practice setting , vanilla “ wug   test”-like setting , and “ wug test”-like setting with   data augmentation by the substring - based data hal-   lucination methods ( + hall-2k - substr ) . Though   data augmentation can improve the model ’s per-   formance for a “ wug test ” , results are still infe-   rior to the common practice setting without any   data augmentation for most languages , especially   the morphologically challenging 2018 CoNLL-   SIGMORPHON languages .   5 Conclusion   In this work , we examine limiting the number of   training lemmata and keeping training lemmata   disjoint from the evaluation sets in morphologi-   cal inﬂection . By comparing the performance ofthe Transformer under the “ wug test”-like circum-   stance with the common practice , we ﬁnd that the   common - practice setting where the training data   covers a fair amount of lemmata and there is over-   lap of lemmata in training and evaluation , has ob-   scured the difﬁculty of the task . We propose to aug-   ment the training data with substring - based data   hallucination , and achieve substantial improvement   over previous data hallucination methods .   Considering the ﬁndings in this paper , we sug-   gest that future experiments include evaluations on   model performance using lemmata not found in the   training set and use unique lemma counts rather   than triple counts to document data set sizes .   References743744745A Data information   triple - counts lemma - counts lemma - overlap ( % )   Language train dev test train dev test dev - train test - train   czech 1000 1000 1000 848 848 849 24.53 20.38   ﬁnnish 1000 1000 1000 985 983 987 2.34 3.04   german 1000 1000 1000 961 945 962 9.42 9.46   russian 1000 1000 1000 973 985 977 3.65 3.79   spanish 1000 1000 1000 906 902 922 15.74 16.49   turkish 906 928 912 764 802 779 26.06 26.57   triple - counts lemma - counts lemma - overlap ( % )   Language train train dev - train test - train   czech 1582 100 0 0   ﬁnnish 7136 100 0 0   german 1290 100 0 0   russian 1311 100 0 0   spanish 7132 100 0 0   turkish 7632 100 0 0   triple - counts lemma - counts lemma - overlap ( % )   Language train dev test train dev test dev - train test - train   akan 2793 380 763 96 94 95 100.0 100.0   ga 607 79 169 95 59 80 100.0 100.0   lingala 159 23 46 57 23 34 100.0 100.0   nyanja 3031 429 853 227 199 226 100.0 100.0   southern sotho 345 50 99 26 24 25 100.0 100.0   swahili 3374 469 910 97 97 96 100.0 100.0746B Data augmentation size comparison747C Performance of the encoder - decoder with hard monotonic attention model   Considering that the encoder - decoder with hard monotonic attention model ( Aharoni et al . , 2016 ; Aharoni   and Goldberg , 2017 ; Makarov et al . , 2017 ; Makarov and Clematide , 2018c , a , b ; Wu et al . , 2018 ; Wu and   Cotterell , 2019 ) is designed for the morphological generation task and bias towards copying symbols in   the input by leveraging edit actions , we evaluate the performance of the encoder - decoder with exact hard   monotonic attention in the “ wug test”-like circumstance as well in order to evaluate whether this deep   learning model architecture catered to morphological generation is able to learn the generalization ability .   We use the encoder - decoder with exact hard monotonic attention model proposed and implemented by   Wu and Cotterell ( 2019 ) .   The performance of the encoder - decoder with exact hard monotonic attention model for the original   shared task setup , the “ wug test”-like setup with or without our best data hallucination augmentation   is presented in Figure 5 . Figure 6 provides detailed comparison between different data augmentation   methods in the “ wug test”-like experimental setup by the encoder - decoder with exact hard monotonic   attention model . We observe that the encoder - decoder with exact hard monotonic attention model has the   same limitation as the Transformer model pointed out in the previous section.748749