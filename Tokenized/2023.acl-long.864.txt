1549415495154961549715498154991550015501155021550315504155051550615507155081550915510ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   9   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . We can not think of any potential risks .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   We used – not created – scientiﬁc artefacts , as described in Section 2 .   /squareB1 . Did you cite the creators of artifacts you used ?   2   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We used the extension of Penn Treebank described in Ficler and Goldberg 2016 ( cited in the pa-   per ) . Unfortunately , the license status of this extension is not clear . The webpage of this extension   ( https://github.com/Jess1ca/CoordinationExtPTB ) requires one to ﬁrst obtain another PTB extension   from a URL that does not seem to be functional ( http://schwa.org/projects/resources/wiki/NounPhrases ) .   In the end we obtained the extension described in Ficler and Goldberg 2016 directly from the authors   and we made sure that our institution obtained the original PTB from LDC , so that no rights were   violated . We felt there was no need ( and no space ) to include the above description of the unclear   license in the paper , but we will be willing to do so if reviewers make this request .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   2 and 9.1   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   We used a version of Penn Treebank , which 1 ) only consists of newspaper texts , 2 ) is widely available   and has been used in countless tasks before , and – for these reasons – does not require any   anonymization .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Not applicable , because we do not make any new artifacts available .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   See Tables 1–3 , fn.8 , and Figure 1.15511C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   No response .   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   No response .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   No response .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   No response .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.15512