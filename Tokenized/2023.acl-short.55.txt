  Hanming Wu , Wenjuan Han , Hui Di , Yufeng Chen , Jinan XuBeijing Jiaotong University , Beijing , ChinaToshiba ( China ) Co. , Ltd. , Beijing , China   21120416@bjtu.edu.cn , wjhan@bjtu.edu.cn   dihui@toshiba.com.cn , yfchen@bjtu.edu.cn , jaxu@bjtu.edu.cn   Abstract   Traditional machine translation evaluation re-   lies on references written by humans . While   reference - free evaluation gets rid of the con-   straints of labor - intensive annotations , it can   pivot easily to new domains and is more scal-   able . In this paper , we propose a reference-   free evaluation approach that characterizes   evaluation as two aspects : ( 1 ) fluency : how   well the candidate translation conforms to nor-   mal human language usage ; ( 2 ) faithfulness :   how well the candidate translation reflects the   source data . We further split the faithfulness   into word - level and sentence - level . Exten-   sive experiments spanning WMT18/19/21 Met-   rics segment - level daRR and MQM datasets   demonstrate that our proposed reference - free   approach , ReFreeEval , outperforms SOTA   reference - free metrics like YiSi-2 , SentSim and   BERTScore - MKD in most language directions .   The code can be found at ReFreeEval Repo .   1 Introduction   Machine translation evaluation has conventionally   relied on reference , where outputs are compared   against translations written by humans . This is   in contrast to the reference - free manner in which   translation quality is directly assessed with the   source text . Reference - free evaluation ( Napoles   et al . , 2016 ; Thompson and Post , 2020 ; Agrawal   et al . , 2021 ) has the potential to free the evaluation   model from the constraints of labor - intensive anno-   tations , allowing it to pivot easily to new domains .   In this way , reference - free evaluation metrics are   substantially more scalable and have lately been in   the spotlight .   The history of reference - free evaluation for MT   can trace back to “ QE as a Metric ” track ofWMT2019 Metrics Task ( Ma et al . , 2019 ) . YiSi-   2 ( Lo , 2019 ) and XBERTScore ( Zhang * et al . ,   2020 ; Leiter , 2021 ) are embedding - based meth-   ods that adopt contextual word embeddings to cal-   culate the lexical similarity between the source   and candidate translation words . Quality estima-   tion ( Fonseca et al . , 2019 ) system metrics such   as UNI+ ( Yankovskaya et al . , 2019 ) and COMET-   QE ( Rei et al . , 2020a , 2021 ) also leverage contex-   tual word embeddings and feed them into a feed-   forward network . However , they are trained to   regress on human scores that are expensive to col-   lect , and gross discrepancies exist when different   humans are asked to label the scores .   More challenging but worthwhile , we focus on   dispensing with references as well as human scores .   Nevertheless , embedding - based methods are lim-   ited to token - level semantic similarity while ne-   glecting sentence - level faithfulness ( Song et al . ,   2021 ) . Besides , it ’s difficult for word embeddings   to discriminate matched word pairs from random   ones ( Zhao et al . , 2020a ) .   In addition , current reference - free evaluation   methods rarely take fluency into account . For the   unfluent candidates whose content is roughly con-   sistent with the source , the embedding - based met-   rics can hardly discriminate and provide accurate   evaluation scores . Moreover , the general goal of   evaluation metrics is to estimate not only the se-   mantic equivalence between source and candidate   but also the general quality ( i.e. , fluency and nat-   uralness ) ( Banchs et al . , 2015 ; Feng et al . , 2020 ;   Yuan et al . , 2021 ) .   In this work , we propose a holistic approach ( i.e. ,   ReFreeEval ) to enhance the evaluation model in   aspects of fluency and faithfulness , meanwhile   on both word and sentence levels . With regard   to fluency , we pose a data augmentation method   and train a fluency discrimination module . For   word - level faithfulness , we adopt a self - guided623contrastive word - alignment method . For sentence-   level faithfulness , we execute knowledge distilla-   tion with SBERT ( Reimers and Gurevych , 2019 ) to   capture more fine - grained semantics . Our method   builds on the framework of XBERTScore . Ex-   tensive experiments spanning WMT18/19/21 Met-   rics ( Ma et al . , 2018 , 2019 ; Freitag et al . , 2021 )   segment - level daRR and MQM datasets demon-   strate that our proposed reference - free approach ,   ReFreeEval , outperforms SOTA reference - free   metrics like YiSi-2 , SentSim and BERTScore-   MKD in most language directions .   2 Approach   Reference - free evaluation of MT can be charac-   terized as two aspects : ( 1 ) fluency : how well it   conforms to normal human language usage ; and ( 2 )   faithfulness : how well the translated text reflects   the source data . We assess faithfulness at different   granularity : word level and sentence level . Figure 1   is the illustration of our ReFreeEval method .   2.1 Sentence - Level Fluency   We explore a data augmentation method to perturb   the fluency of target sentences with noise which   is difficult to be identified . Then we train a flu-   ency discrimination module with contrastive learn-   ing ( Gao et al . , 2021 ; Zhang et al . , 2021 ; Wu et al . ,   2022 ; Wang et al . , 2022 ) to distinguish fluent sam-   ples from perturbed samples ( namely , challenging   negative samples ) .   Data Augmentation Using Clause Permutation   A complex or compound sentencehas two or more   clauses and relative clauses that are joined together   with conjunctions or punctuation . As logical rela-   tions exist between these clauses , we manipulate   and permute the clauses separated by punctuation ,   instead of words . In this way , the meaning is pre-   served inside the clauses , meanwhile , the sentence   is often unfluent and unnatural . Similar to complex   and compound sentences , for a simple sentence   with only one clause , we randomly split it into two   fragments and permute the two fragments . Com-   pared to permutation on the token level , clause-   level permutation has less influence on sentence   fluency and semantic change . The clause - basedpermutation method brings perturbed samples that   are more challenging and hard to be recognized .   Fluency Discrimination We denote a source and   target sentence in parallel data as xandy . Per-   turbed samples augmented from yareˆy,ˆy , ... , ˆy .   A reliable metric has the ability to give the original   fluent target ya higher evaluation score than those   kperturbed unfluent samples .   As for the score , we adopt the same calculation   measure as BERTScore but replace the pre - trained   monolingual model ( Devlin et al . , 2019 ; Liu et al . ,   2019 ) with a cross - lingual model ( Devlin et al . ,   2019 ; Conneau et al . , 2019 ) to do reference - free   evaluation ( Zhou et al . , 2020 ; Song et al . , 2021 ) de-   nominated as XBERTScore ( Leiter , 2021 ) . We use   9th layer of XLM - Roberta - Base to extract contex-   tual word embeddings . Here we only use F   as evaluation score between source xand target-   sideyorˆy , which is represented as spx , yqor   spx,ˆyq . Then we can obtain word - level faith-   fulness scores spx , yq , spx,ˆyq , ... , spx,ˆyqof   pk`1qpairs .   In order to discriminate fluent sentences from   perturbed ones according to these scores , we treat   the original target and its corresponding perturbed   samples as opposite and assign them 1/0 hard la-   bels . The cross - lingual model which produces   XBERTScore is trained to classify target - side sen-   tences with a cross - entropy loss function . The ob-   jective function on Ntraining samples is as fol-   lows :   L“´1   Nÿloge   e`ře(1 )   2.2 Word - Level Faithfulness   As for word - level faithfulness , each word in the   source sentence should have a corresponding cross-   lingual representation in the target sentence and   each word in the target sentence should be an accu-   rate translation of its source word . This motivates   us to do word - alignment training to enhance word-   level evaluation .   This module shares similar architecture with   sentence - level fluency where word embeddings are   derived from 9th layer of XLM - Roberta - Base .   We take the same steps as ( Dou and Neubig ,   2021 ) to extract alignments . First , we compute the   dot product between source and target word embed-   dings to obtain the similarity matrix S. Then Sis   normalized in source and target dimensions . And624   we get source - to - target alignment matrix Sand   target - to - source alignment matrix S. A source/-   target token and a target / source token whose sim-   ilarity value in alignment matrix S / Sexceed   threshold care regarded as aligned . The bidirec-   tional alignment matrix Ais deduced :   A“pSącq˚pSącq ( 2 )   A“1means xandyare aligned . Dou   and Neubig ( 2021 ) also propose the self - training   objective to align words with this bidirectional   alignment , which improves alignment performance   most .   Based on this objective , we adopt a self - guided   contrastive cross - lingual word - alignment method .   By contrast , we not only pull semantic aligned   words to have closer contextual representations but   also push unrelated words away ( Luo et al . , 2021 ;   Su et al . , 2022 ; Meng et al . , 2022 ) , which encour-   ages the model to discriminate matched word em-   beddings from semantically unrelated ones .   The source token and target token are deemed to   be unrelated if their similarity value is low . In our   method , these unmatched pairs constitute negative   samples and are pushed away . Moreover , we set   threshold cto further restrict the negative sam-   ples . The unmatched pairs whose similarity value   is lower than care discarded from negatives as   this unmatched relation can be easily distinguished   by the model . In this way , we can control the diffi-   culty of negative samples and only preserve those   indistinguishable ones ( hard negatives ) to train the   model .   B“pSącq˚pSącq ( 3)B“1means xandyare aligned or a part of   hard negatives , which are preserved to train .   In Figure 1 , the dark blue positions mean bidi-   rectional alignment while the light blue positions   are hard negative examples .   Finally , based on two dimensions of source and   target , the positive and negative samples mentioned   above , we construct a self - guided contrastive learn-   ing objective function on the word level as follows :   L“´1   mÿř 1pA“1qe   ř 1pB“1qe(4 )   L“´1   nÿř 1pA“1qe   ř 1pB“1qe(5 )   L“L`L ( 6 )   2.3 Sentence - Level Faithfulness   The main idea is to improve sentence - level faith-   fulness evaluation . Concretely , we distill sentence-   level semantic meaning from SBERT into the word-   level shared model .   We use SBERT to extract semantically meaning-   ful sentence embeddings . Sentence semantic sim-   ilarity between xandyis calculated with cosine-   similarity between sentence embeddings xandy :   spx , yq“x¨y   } x}}y}(7 )   The semantic similarity reflects the sentence-   level faithfulness from target to source . Then   we can obtain sentence - level faithfulness scores   spx , yq , spx,ˆyq , ... , spx,ˆyq . We use KL-   divergence as the objective function to reduce the625discrepancy between sentence - level and word - level   similarity :   L“ÿspx , yqlogspx , yq   spx , yq(8 )   In this distillation module , SBERT plays a role   of a teacher . Sentence - level semantic knowledge is   distilled into the word - level shared model through   these sentence - level faithfulness scores . In this   way , evaluation is no longer limited to word level   but incorporated sentence semantics .   On the other hand , SBERT plays a role as a   corrector . It is unreasonable that a disturbed sample   with slightly changed semantics is considered to be   completely contrary to the original sentence . We   correct the binary classification and convert the 0/1   discrete value in the fluency discrimination module   to continuous variables .   For sentence - level training , we combine fluency   with faithfulness . This joint architecture is moti-   vated by ( Ren et al . , 2021 ) . The objective is :   L“L`αL ( 9 )   αis a hyper - parameter to control the weight that   the sentence - level faithfulness module accounts for .   3 Experiment   3.1 Setup   Datasets We train and evaluate on four language   pairs : English ØChinese and English ØGerman .   For training , we use the datasets following   Awesome - Align ( Dou and Neubig , 2021 ) . The   En - Zh training dataset is collected from the Ts-   inghuaAlignerwebsite and En - De training data is   Europarl v7 corpus . For evaluation , we use the   segment - level daRR dataset of WMT18/19 and   MQM dataset of WMT21 Metrics Task . Details   about datasets are introduced in Appendix C.1 .   Embeddings We use the 9th layer of XLM-   Roberta - Base to extract contextual word em-   beddings . This follows the default setting of   BERTScore . For sentence embeddings , we adopt   xlm - r - bert - base - nli - stsb - mean - tokens modelthe   same as SentSim . Baselines For reference - based metrics , we   choose sentBLEU ( Papineni et al . , 2002 ) and YiSi-   1 ( Lo , 2019 ) . For reference - free metrics , we   choose XBERTScore ( Leiter , 2021 ) , YiSi-2 ( Lo ,   2019 ) , SentSim ( Song et al . , 2021 ) and BERTScore-   MKD ( Zhang et al . , 2022 ) . Most results of base-   line models are reported in the original paper ( Ma   et al . , 2018 , 2019 ; Freitag et al . , 2021 ; Zhang et al . ,   2022 ) . We also implement experiments that have   not been reported , such as XBERTScore , SentSim   and BERTScore - MKD .   Training Process ForReFreeEval , sentence-   level module is first trained . Then word - level faith-   fulness module is trained based on the best check-   point of sentence - level training . Training details   are in Appendix C.3 .   Evaluation Measures For WMT18/19 segment-   level evaluation , Kendall ’s Tau - like formulation is   used to measure the scores against daRR .   τ“|Concordant |´|Discordant |   |Concordant |`|Discordant |(10 )   For WMT21 segment - level evaluation , conven-   tional Kendall - tau statistic is used to measure the   correlation between our scores and MQM scores .   3.2 Results   The main results are displayed in Table 1 , 2 , 3 . First ,   we observe that fluency , word - level faithfulness ,   and sentence - level faithfulness module improve the   evaluation performance respectively . We also find   that the main improvement comes from sentence-   level fluency indicating that XBERTScore as a   token - level evaluation metric lacks sentence - level   knowledge . Then , the ensemble model combining   the advantages of the three modules achieves even   better results . And compared with some reference-   based baselines it achieves comparable results or   even outperforms them . More details of experimen-   tal results are in Appendix C.4 .   4 Conclusion   We propose a reference - free evaluation approach   ReFreeEval that comprehensively considers three   aspects : fluency , word - level faithfulness , and   sentence - level faithfulness . Extensive experiments   spanning datasets from WMT18/19/21 demonstrate   the superiority of each module designed for each626   aspect . ReFreeEval , combining the above three   modules , achieves a higher correlation with human   judgments , outperforming current SOTA reference-   free metrics like YiSi-2 , SentSim and BERTScore-   MKD in most language directions .   Limitations   In this section , we discuss some limitations of our   method and future work based on the limitations .   First , the enhancement of the word - level module   is not as strong as the remedy of the sentence - level   module . Our word - level module solely achieves   improvement compared with XBERTScore but   does n’t improve as much as the sentence - level mod-   ule . The main reason is that the XBERTScore   framework lacks sentence - level semantic knowl-   edge . Besides , our word - level self - guided con-   trastive method does n’t resort to external informa-   tion and only consolidates the alignment already   existing in the pre - trained language model . Second ,   ReFreeEval performs comparably with baseline   models on language pairs involving German . We   guess it is due to the evaluation of QE . Ma et al .   ( 2019 ) mention that the evaluation results across   all language pairs are unstable in “ QE as a Metric ”   track and ca n’t explain yet .   In the future , we ’ll further explore valuable ex-   ternal information on word level . And we ’ll try   to explore discrepancies among language pairs to   optimize the results . In addition , our simple but   effective data augmentation method - clause per-   mutation does n’t rely on rules or toolkits , which   is an initial attempt at modeling fluency . It could   benefit from further refinement such as language-   specific knowledge , syntactic and semantic parsing   to recognize clauses . We ’ll conduct an in - depth   investigation into further work .   Acknowledgements   The research work described in this paper has been   supported by the National Key R&D Program of   China ( 2020AAA0108001 ) and the National Na-   ture Science Foundation of China ( No . 61976015 ,   61976016 , 61876198 and 61370130 ) . Wenjuan   Han is supported by the Talent Fund of Beijing   Jiaotong University ( 2023XKRC006 ) . The authors627would like to thank the anonymous reviewers for   their valuable comments and suggestions to im-   prove this paper . We would like to express our   sincere gratitude to Hui Huang for guidance before   this research . We are also grateful to Chunyou Li ,   Yu Xiang and Yu Zhang for their assistance during   internship .   References628629   A Related Work   A.1 Reference - based Evaluation for MT   According to matching features , reference - based   evaluation methods can be categorized as follows :   ( 1)n - gram(e.g . BLEU ( Papineni et al . , 2002 )   and CHRF ( Popovi ´ c , 2015 ) ) ; ( 2 ) edit distance(e.g .   TER ( Snover et al . , 2006 ) and EED ( Stanchev   et al . , 2019 ) ) ; ( 3 ) word embedding(e.g . YiSi ( Lo ,   2019 ) and BERTScore ( Zhang * et al . , 2020 ) ) ;   ( 4)predictor - estimator model ( Kim et al . , 2017)(e.g .   COMET ( Rei et al . , 2020a ) ) . n - gram matching630Sentence DA XBERTScore ReFreeEval   SRC但也有顾客认为，网站退款服务不是百分之百   完美 。   REFNonetheless , some customers felt that website re-   fund services are not perfect .   MT1But there are also customers who believe the site   refund service is not 100 per cent perfect.1.1059 0.8993 0.9249   MT2But also some customers believe that website re-   funds money the service is not 100 % perfect.-1.5038 0.9031 0.8680   metrics are restricted to surface form and neglect   semantic meaning .   Instead , embedding - based metrics adopt word   embedding to explore word - level semantic mean-   ing . WMDo ( Chow et al . , 2019 ) builds on Word   Mover ’s Distance ( Kusner et al . , 2015 ) to mea-   sure the similarity of candidate and reference . It   also introduces a word order penalty to take flu-   ency into account . YiSi-1 aggregates the weighted   lexical similarity to evaluate translation quality .   BERTScore calculates the token - level semantic   similarity between candidate translation tokens and   reference tokens . DA - BERTScore ( Zhan et al . ,   2021 ) takes translation difficulty into account and   assigns difficulty weightings to each token in refer-   ence .   COMET leverages contextual word embeddings   of the source sentence , MT hypothesis , and refer-   ence ( or human post - edition ) extracted from pre-   trained cross - lingual models . The embeddings are   combined and fed into a feed - forward network . It ’s   a quality estimation system and is trained with hu-   man assessments(DA , HTER , MQM ) .   A.2 Reference - Free Evaluation for MT   As reference is costly to be collected in practice ,   reference - free metrics attract more attention . Re-   cent studies have explored evaluating translation   quality only based on the source text .   YiSi-2 calculates similarities between cross-   lingual word embeddings for aligned source and   candidate translation words and outputs an F-   measure statistic as the metric score . Zhao et al .   ( 2020b ) propose to re - align vector spaces and cou-   ple the semantic faithfulness scores with GPT-   based fluency testing .   OpenKiWi - XLMR ( Moura et al . , 2020 ) and   COMET - QE ( Rei et al . , 2020b ) are quality estima-   tion systems from “ QE as a Metric ” task ( Mathuret al . , 2020 ) . They remove reference at the input   but still require human assessments to train .   As reference - based BERTScore has achieved   outstanding performance , many recent reference-   free evaluation methods build on BERTScore .   XBERTScore ( Leiter , 2021 ) adopts the cross-   lingual pre - trained language model to evaluate   only based on source sentence without reference .   SentSim ( Song et al . , 2021 ) combines semantic   sentence similarity with token - level BERTScore .   BERTScore - MKD ( Zhang et al . , 2022 ) also uses   sentence embeddings to achieve cross - lingual word   embedding alignment by multilingual knowledge   distillation .   B Case Study   From Table 4 , we can see there is a significant dif-   ference between the golden truth DA of MT1 and   MT2 . And the quality of MT1 is much better than   MT2 . But XBERTScore evaluates incorrectly and   assigns MT1 with a lower score than MT2 . Though   MT2 is translated word by word which means poor   fluency , almost all words in MT2 can be aligned   with source . As XBERTScore method is evaluated   on word - level matching , it can be easily confused .   The model trained with our holistic approach can   make up for this shortage and discriminate the flu-   ency problem .   C Experimental Details   C.1 Data Analysis   Following the data setting of awesome - align ( Dou   and Neubig , 2021 ) , we use the following paral-   lel corpora to fine - tune our model . The English-   Chinese(En - Zh ) dataset is collected from the Ts-   inghuaAligner webset and Englist - German(En - De )   dataset is the Europarl v7 corpus . We only adopt   a multilingual setting but use less data . We ran-631domly sample 20k parallel sentence pairs from each   dataset and mix them together .   In the word - level faithfulness module , we di-   rectly use mixed data to train . In the sentence - level   fluency and faithfulness module , as only the target   is perturbed , we randomly select 1/3 mixed data   and swap the source and target in order to attend to   all three languages .   To evaluate our method , we choose segment-   level evaluation datasets of WMT Metrics Task .   Two types of human assessments are included .   Segment - level Metrics datasets of WMT18/19 use   daRR(direct assessment relative ranking ) as ground   truth and WMT21 use MQM(multidimensional   quality metrics ) as ground truth .   C.2 Details of Sentence - Level Faithfulness   Before applying KL - divergence , the word - level and   sentence - level similarity scores are processed as   follows .   spx , yq“logpe   řeq ( 11 )   spx , yq“e   ře(12 )   C.3 Training Details   Our model is fine - tuned based on the 9th layer   of XLM - Roberta - Base . We implement our model   with Pytorch ( Paszke et al . , 2019 ) , Transform-   ers ( Wolf et al . , 2020 ) and BERTScore ( Zhang *   et al . , 2020 ) package . We use AdamW opti-   mizer ( Loshchilov and Hutter , 2017 ) . The model   is trained on up to 3 GeForce RTX 2080 Ti GPUs .   For sentence - level training , the hyperparame-   ter settings are displayed in Table 5 . We mainly   search αP t0,1,5,10,20,30,40,50,100,500u .   The training process is on a single GPU with gra-   dient accumulation . We evaluate the model with   classification accuracy every 100 steps and save the   checkpoint with the highest accuracy .   For word - level training , the hyperparameter set-   tings are displayed in Table 6 . We search batch   sizePt8,10,15,16,24,28,32,48u , learning rateP   t1e´5,5e´6,3e´6,1e´6,2e´6,5e´7,1e´7u   andcPt1e´5,1e´10,1e´15,1e´20,1e ´   30,1e´50u . For dataset of WMT18/19 the train-   ing process is on 3 GPUs and the batch size on   each GPU is 5 . Specifically , for WMT21 MQM   dataset the batch size is 32 and the learning rate   is 2e-6 . The training is on 4 GPUs and the batchsize on each GPU is 8 . The code of this module   is implemented based on awesome - align ( Dou and   Neubig , 2021 ) . This word - level faithfulness train-   ing continues on the basis of the best checkpoint of   sentence - level training .   Hyperparameters Values   Epoch 1   Evaluation Step 100   Batch Size 10   Learning Rate 1e-6   Warmup Steps 1000   α 30   k 7   Random Seed 42   Hyperparameters Values   Epoch 1   Batch Size 15(32 )   Learning Rate 1e-6(2e-6 )   Warmup Steps 200   c 1e-3   c 1e-20   Random Seed 42   C.4 Details of Experimental Results   In Section 3 , as we want to demonstrate the im-   provement of our ReFreeEval in multilingual set-   ting of all language directions , we report results   corresponding to the highest “ average ” of all lan-   guage pairs for each dataset .   Table 7 , 8 , 9 are the best results of each language   direction of WMT18/19/21 dataset.632   D Analysis   D.1 Analysis of Data Augmentation   We compare our clause permutation with token-   level data augmentation methods shuffling and rep-   etition . The results are displayed in Table 10 .   For the fluency module alone , our clause - based   augmentation method performs much better than   the others , which suggests that our method provides   more proper and valuable fluency information than   others . As for sentence - level faithfulness , we com-   pare the variation of sentence semantic similarity   in Table 11 . The disturbance caused by token shuf-   fling is too great while our clause permutation is   small . The obvious disturbance is easy to be distin-   guished and learned . While the disturbance caused   by our method can hardly be distinguished by sen-   tence similarity thus only this module is not enough .   However , with the clause permutation method , the   combination of both fluency and sentence - level   faithfulness outperforms others a lot . This veri-   fies that our clause - based augmentation method is   effective .   Based on the linguistic definition of clauses , our   clause permutation approach can effectively incor-   porate perturbation to continuity and smoothness ,   which constitute the essence of fluency . This ap-   proach is simple and intuitive , making it a suitable   choice for the preliminary step for more in - depth   investigations about realistic perturbations .   D.2 Balance between Fluency Discrimination   and Faithfulness Distillation   For sentence - level training , we adjust the hyper-   parameter αto balance fluency and faithfulness . A small αmeans the sentence - level training   mainly focuses on classification , which may ne-   glect the semantic meaning of perturbed samples   as we explained in section2.3 . While a large α   weakens the effect of hard classification labels , the   soft similarity is also not enough for sentence - level   training .   From Table 12 , we can conclude that only by   keeping the balance between hard fluency discrim-   ination and soft faithfulness distillation can we   achieve excellent experimental results .   D.3 Control over Difficulty of Negative   Samples in Word - Level Faithfulness   We experiment with different settings of threshold   cin word - level faithfulness to observe the influ-   ence of the difficulty of negative samples .   A small creduces the difficulty of contrastive   learning . This setting includes negative samples   whose unmatched relations can be easily distin-   guished . While a large crestricts the negative   samples extremely , which may lose some useful   information . The results in Table 13 indicate that   properly controlling the difficulty of negative sam-   ples can lead to great performance on the whole .   However , for En - De , a small threshold is benefi-   cial to improve the results . This may be because   negatives without strict limitations are harmful to   contrastive learning due to specific language fea-   tures of En - De .   E Significance Test   Our experimental results above are based on the   model training in a single run with random seed   42 . In this section , we implement the statis-   tical significance test following ( Dror et al . ,   2018 ) to further compare the performance of our   ReFreeEval with a strong baseline SentSim . We   run both the models 10 times with random seed   P t19,27,42,55,76,80,99,153,178,200u . The   p - value of the statistical test is displayed in Ta-   ble 14 . As we can see , the p - value on each language   pair is well below the significance level of 0.05 ,   which indicates that the results of our ReFreeEval   are significantly better than SentSim.633Data Aug Method Model Zh - En En - Zh De - En En - De   fluency 0.1371 0.2503 0.3733 0.4751   Permutation sent - fa 0.1169 0.1759 0.3529 0.4319   sent - level 0.1798 0.2749 0.4144 0.5817   fluency 0.1055 0.1815 0.3491 0.3749   Shuffle sent - fa 0.0809 0.0720 0.3100 0.3034   sent - level 0.1469 0.2238 0.3729 0.4757   fluency 0.1106 0.1586 0.3359 0.4048   Repetition sent - fa 0.1305 0.1979 0.3642 0.4552   sent - level 0.0654 0.0716 0.2846 0.2847   α Zh - En En - Zh De - En En - De   0 0.1425 0.2564 0.3774 0.4951   5 0.1636 0.2538 0.3971 0.5656   10 0.1664 0.2522 0.3979 0.5692   50 0.1800 0.2798 0.4133 0.5843   100 0.1608 0.2613 0.3918 0.5524   500 0.1277 0.2001 0.3615 0.4628634ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section : Limitations   /squareA2 . Did you discuss any potential risks of your work ?   No potential risks   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section : Abstract and Section1 : Introduction   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Mainly in Section3 : Experiments and Appendix D : Experimental Details . Also Section1 : Introduction   and Section2 : Approach mention models .   /squareB1 . Did you cite the creators of artifacts you used ?   Section Reference and footnotes of Section2 and Section 3 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Section Reference and footnotes of Section2 and Section 3 .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section2 and Section3   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Our data does n’t have these information .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section3 and Appendix D   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section3 and Appendix D.1   C / squareDid you run computational experiments ?   Section3 and AppendixE / F   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix D.3635 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section3 and Appendix D.3   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Appendix F   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix D.3   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.636