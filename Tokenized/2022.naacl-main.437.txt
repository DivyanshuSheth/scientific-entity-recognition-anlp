  Shuliang Liu , Xuming Hu , Chenwei Zhang , Shu’ang Li , Lijie Wen , Philip S. YuTsinghua UniversityAmazonUniversity of Illinois at Chicago   { liusl19,hxm19,lisa18}@mails.tsinghua.edu.cn   cwzhang@amazon.com ; wenlj@tsinghua.edu.cn ; psyu@uic.edu   Abstract   Unsupervised relation extraction aims to ex-   tract the relationship between entities from   natural language sentences without prior in-   formation on relational scope or distribution .   Existing works either utilize self - supervised   schemes to refine relational feature signals by   iteratively leveraging adaptive clustering and   classification that provoke gradual drift prob-   lems , or adopt instance - wise contrastive learn-   ing which unreasonably pushes apart those sen-   tence pairs that are semantically similar . To   overcome these defects , we propose a novel   contrastive learning framework named HiURE ,   which has the capability to derive hierarchi-   cal signals from relational feature space using   cross hierarchy attention and effectively opti-   mize relation representation of sentences un-   der exemplar - wise contrastive learning . Exper-   imental results on two public datasets demon-   strate the advanced effectiveness and robust-   ness of HiURE on unsupervised relation ex-   traction when compared with state - of - the - art   models . Source code is available here .   1 Introduction   Relation Extraction ( RE ) aims to discover the se-   mantic ( binary ) relation that holds between two   entities from plain text . For instance , “ Kissel   was born in Adrian ... " , we can extract a re-   lation /people / person / place_of_birth   between the two head - tail entities . The extracted re-   lations could be used in various downstream appli-   cations such as information retrieval ( Corcoglioniti   et al . , 2016 ) , question answering ( Bordes et al . ,   2014 ) , and dialog systems ( Madotto et al . , 2018 ) .   Existing RE methods can achieve decent results   with manually annotated data or human - curated   knowledge bases . While in practice , human an-   notation can be labor - intensive to obtain and hardto scale up to newly created relations . Lots of ef-   forts are devoted to alleviating the impact of human   annotations in relation extraction . Unsupervised   Relation Extraction ( URE ) is especially promising   since it does not require any prior information on   relation scope and distribution . The main challenge   in URE is how to cluster semantic information of   sentences in the relational feature space .   Simon et al . ( 2019 ) adopted skewness and dis-   persion losses to enforce the relation classifier to   be confident in the relational feature prediction   and ensure all relation types can be predicted aver-   agely in a minibatch . But it still requires the exact   number of relation types in advance , and the rela-   tion classifier could not be improved by obtained   clustering results . Hu et al . ( 2020 ) encoded rela-   tional feature space in a self - supervised method   that bootstraps relational feature signals by leverag-   ing adaptive clustering and classification iteratively .   Nonetheless , like other self - training methods , the   noisy clustering results will iteratively result in the   model deviating from the global minima , which is   also known as the gradual drift problem ( Curran   et al . , 2007 ; Zhang et al . , 2021a ) .   Peng et al . ( 2020 ) leveraged contrastive learning   to obtain a flat metric for sentence similarity in   a relational feature space . However , it only con-   siders the relational semantics in the feature space   from an instance perspective , which will treat each   sentence as an independent data point . As scal-   ing up to a larger corpus with potentially more   relations in a contrastive learning framework , it   becomes more frequent that sentence pairs shar-   ing similar semantics are undesirably pushed apart   in a flat relational feature space . Meanwhile , we   observe that many relation types can be organized   in a hierarchical structure . For example , the rela-   tions /people / person / place_of_birth   and / people / family / country share the   same parent semantic on /people , which means   that they belong to the same semantic cluster from5970   a hierarchical perspective . Unfortunately , these two   relations will be pushed away from each other in   an instance - wise contrastive learning framework .   Therefore , our intuitive approach is to allevi-   ate the dilemma of similar sentences being pushed   apart in contrastive learning by leveraging the hi-   erarchical cluster semantic structure of sentences .   Nevertheless , traditional hierarchical clustering   methods all suffer from the gradual drift problem .   Thereby , we try to exploit a new approach of hier-   archical clustering by combining propagation clus-   tering and attention mechanism . We first define   exemplar as a representative instance for a group   of semantically similar sentences in certain clus-   tering results . Exemplars can be in different gran-   ularities and organized in a hierarchical structure .   In order to enforce relational features to be more   similar to their corresponding exemplars in all par-   ent granularities than others , we propose HiURE ,   a novel contrastive learning framework for URE   that combines both the instance - wise and exemplar-   wise learning strategies , to gather more reasonable   relation representations and better classification re-   sults .   The proposed HiURE model is composed of two   modules : Contextualized Relation Encoder and   Hierarchical Exemplar Contrastive Learning . As   shown in Figure 1 , the encoder module leverages   the pre - trained BERT model to obtain two aug-   mented entity - level relational features of each sen-   tence for instance - wise contrastive learning , while   the learning module retrieves hierarchical exem-   plars in a top - down fashion for exemplar - wise con-   trastive learning and updates the features of sen-   tences iteratively according to the hierarchy . These   updated features could be utilized to optimize theparameters of encoders by a combined loss func-   tion noted as Hierarchical ExemNCE ( HiNCE ) in   this work . To summarize , the main contributions   of this paper are as follows :   •We develop a novel hierarchical exemplar con-   trastive learning framework HiURE that in-   corporates top - down hierarchical propagation   clustering for URE .   •We demonstrate how to leverage the seman-   tic structure of sentences to extract hierarchi-   cal relational exemplars which could be used   to refine contextualized entity - level relational   features via HiNCE .   •We conduct extensive experiments on two   datasets and HiURE achieves better perfor-   mance than the existing state - of - the - art meth-   ods . This clearly shows the superior capability   of our model for URE by leveraging different   types of contrastive learning . Our ablation   analysis also shows the impacts of different   modules in our framework .   2 Proposed Model   The proposed model HiURE consists of two mod-   ules : Contextualized Relation Encoder and Hier-   archical Exemplar Contrastive Learning . As illus-   trated in Figure 1 , the encoder module takes natural   language sentences as input , where named entities   are recognized and marked in advance , then em-   ploys the pre - trained BERT ( Devlin et al . , 2019 )   model to output two contextualized entity - level fea-   ture sets HandHfor each sentence based on   Random Span . The learning module takes these   features as input , and aims to retrieve exemplars   that represent a group of semantically similar sen-   tences in different granularities , denoted as C. We5971leverage these exemplars to iteratively update rela-   tional features of sentences in a hierarchy and con-   struct an exemplar - wise contrastive learning loss   called Hierarchical ExemNCE which enforces the   relational feature of a sentence to be more similar   to its corresponding exemplars than others .   2.1 Contextualized Relation Encoder   The Contextualized Relation Encoder aims to ob-   tain two relational features from each sentence   based on the context information of two given en-   tity pairs for instance - wise contrastive learning . In   this work , we assume named entities in the sen-   tence have been recognized in advance .   For a sentence x= [ w , .. , w]withTwords   where each wrepresents a word and two entities   Head and Tail are mentioned , we follow the label-   ing schema adopted in Soares et al . ( 2019 ) and   argument xwith four reserved tokens to mark the   beginning and the end of each entity . We introduce   [ H],[H],[T],[T]to represent the start or end   position of head or tail entities respectively and   inject them to x :   x=/bracketleftbig   w , ... , [ H ] , ... , w , ... , [ H ] , ... , w , ... ,   w , ... , [ T ] , ... , w , ... , [ T ] , ... , w / bracketrightbig   ( 1 )   where xwill be the input token sequence for the   encoder and Span subscript indicates the Random   Span words . Considering the relational features   between entity pairs are normally embraced in the   context , we use pre - trained BERT ( Devlin et al . ,   2019 ) model to effectively encode every tokens in   the sentence along with their contextual informa-   tion , and get the token embedding b = f(w ) ,   where i∈[1 , T]including the special tokens in x   andb∈R , where brepresents the dimension   of the token embedding .   We utilize the outputs bcorresponding to [ H ]   and[T]as the contextualized entity - level features   instead of using sentence - level marker [ CLS ] to   get embedding for target entity pair . For contrastive   learning purposes , we randomly select Pwords as   Random Span from all the context words in the   whole sentence except for those entity words and   special tokens to augment the entity - level features   asb , where multiple different Random Span   selections lead to different semantically invariant   embedding of the same sentence . For every selec-   tion , we concatenate the embedding of the two en-   tity and PRandom Span words together to derivea fixed - length relational feature h∈R :   h= [ b , b , b , ... , b ] ( 2 )   where his the output of the Contextualized   Relation Encoder which can be denoted as   f(x , Head , Tail , Span ) . The Random Span strat-   egy can get sentence - level enhanced relational fea-   tures to construct positive samples directly and ef-   fectively , and its simplicity highlights the role of   subsequent modules .   2.2 Hierarchical Exemplar Contrastive   Learning   In order to adaptively generate more positive sam-   ples other than sentences themselves to introduce   more similarity information in contrastive learning ,   we design hierarchical propagation clustering to ob-   tain multi - level cluster exemplars as positive sam-   ples of corresponding instances . We assume the   relation hierarchies are tree - structured and define   hierarchical exemplars as representative relational   features for a group of semantically similar sen-   tences with different granularities . The exemplar-   wise contrastive learning encourages relational fea-   tures to be more similar to their corresponding ex-   emplars than other exemplars .   The process is completed through Hierarchical   Propagation Clustering ( HPC ) to generate cluster   results of different granularities and Hierarchical   Exemplar Contrastive Loss ( HiNCE ) to optimize   the encoder . The main procedure of HPC consists   of Propagation Clustering and Cross Hierarchy At-   tention ( CHA ) , as is elaborated in Algorithm 1 ,   which will be explained in detail below .   Propagation Clustering   We use propagation clustering to obtain hierar-   chical exemplars in an iterative , top - down fashion .   Traditional clustering methods such as k - means   cluster data points into specific cluster numbers ,   however , these methods could not utilize hierar-   chical information in the dataset and require the   specific cluster number in advance . Propagation   clustering possesses the following advantages : 1 ) It   considers all feature points as potential exemplars   and uses their mutual similarity to extract potential   tree - structured clusters . 2 ) It neither requires the   actual number of target relations in advance nor the   distribution of relations . 3 ) It will not be affected   by the quality of the initial point selection .   In practice , propagation clustering exchanges   real - valued messages between points until a high-   quality set of exemplars and corresponding clusters5972Algorithm 1 Hierarchical Propagation Clustering   Input : Encoder outputs H={h , h , ... , h } ,   Hierarchical cluster layers L   Output : Hierarchical clusters results CH←H , C ← [ ] Initialize { s|i , j∈[1 , n]}by Eq . 3∀i̸=j : p= min ( s ) , p= median ( s)ps=/braceleftig   p|p = p+·(l−1 ) , l∈[1 , L]/bracerightigforlin[1 , L]do Update { s}according to Hby Eq . 3 Set diagonal to preference s = p for all iterations do Update { r}and{a}by Eq . 4 and 5 ˆc=(ˆc , . . . , ˆc),ˆc = argmax(a+r ) Exemplar set E={e|e = h,ˆc∈ˆc } ifChanges of Ehave converged then break end if end forC.add(E ) H←(H , E)by Eq . 8end forreturn C   are generated . Inspired by Frey and Dueck ( 2007 ) ,   we adopt similarity sto measure the distance   between points iandj , responsibility rto indicate   the appropriateness for jto serve as the exemplar   foriand availability ato represent the suitability   forito choose jas its exemplar :   s=−∥h−h∥(3 )   r = s−max / parenleftbig   s+a / parenrightbig   ( 4 )   a=      /summationtextmax / parenleftbig   0 , r / parenrightbig   , j = i   min / bracketleftigg   0 , r+/summationtextmax / parenleftbig   0 , r / parenrightbig / bracketrightigg   , j̸=i   ( 5 )   where randawill be updated through the prop-   agation iterations until convergence ( Lines 8 - 15 )   and a set of cluster centers , which is called ex-   emplar , will be chosen as E(Line 11 ) . Then we   wish to find a set of Lconsecutive layers of cluster-   ing , where the points to be clustered in layer lare   closer to the corresponding exemplar of layer l−1 .   We perform propagation clustering Ltimes ( Lines   5 - 18 ) with different preferences ( Lines 2 - 4 ) to gen-   erateLdifferent layers of clustering result , where a   larger preference leads to more numbers of clusters   ( Moiane and Machado , 2018 ) . The Hyperparam-   eter Analysis part provides a detailed explanation   about how to select Land the reason for building   the preference sequence psaccording to the for-   mula in Line 4 .   Cross Hierarchy Attention   The traditional hierarchical clustering method ei-   ther merge fine - grained clusters into coarse - grained   one or split coarse cluster into fine - grained ones ,   which will both cause the problem of error accu-   mulation . Meanwhile , note that the preference se-   quence leads to different cluster results in a hier-   archical way but lost the interaction information   between adjacent levels in propagation clustering .   Based on this intuition , we introduce the CHA   mechanism to leverage signals from coarse - grained   exemplars to fine - grained clusters .   Formally , we derive a CHA matrix Aat layer l   where the element at ( j , k)is obtained by a scaled   softmax :   α = exp(λe·e )   /summationtextexp(λe·e)(6 )   where λis a trainable scalar variable , not a hyper-   parameter ( Luong et al . , 2015 ) . The attention   weight αreflects the proximity between exem-   plarjand exemplar kin layer land measures the   influence and interactions to corresponding data   points between these exemplars . Typically , exem-   plars that are visually close to each other would   have higher attention weights . Then we derive at-   tended point representation at layer l+ 1by taking   the attention weighted sum of its corresponding5973exemplar from other exemplars :   ˆh=/summationdisplayαe ( 7 )   where eis the exemplar of h. The attended repre-   sentation aggregates signals from other exemplars   weighted by how close they are to exemplar eand   transfer the signals from layer ltol+ 1 . They   reflect how likely a neighboring cluster is relevant   or the point will get close to it . Then we combine   the attended representation with the original one to   obtain the CHA based embedding h , defined as :   h = h+λˆh ( 8)   where λis not a hyper - parameter , but a weight-   ing variable to be automatically trained . As illus-   trated in Figure 2 , the CHA mechanism helps data   points to get closer with corresponding exemplars   in previous layer and thus perform better in the   current layer .   Hierarchical Exemplar Contrastive Loss   Given a training set X={x , x , ... , x}ofnsen-   tences , Contextualized Relation Encoder can ob-   tain two augmented relational features for each   input sentences by randomly sampling spans twice   for the same entity pair . We do this for all   sentences and obtain H={h , h , ... , h}and   H={h , h , ... , h } . Traditional instance - wise   contrastive learning treats two features as a nega-   tive pair as long as they are from different instances   regardless of their semantic similarity . It updates   encoder by optimizing InfoNCE ( Oord et al . , 2018 ;   Peng et al . , 2020 ):   L = /summationdisplay−logexp(h·h / τ)/summationtextexp(h·h / τ )   ( 9 )   where handhare positive samples for instance   i , while hincludes one positive sample and J−1   negative samples for other sentences , and τis a   temperature hyper - parameter ( Wu et al . , 2018 ) .   Compared with the traditional instance - wise con-   trastive learning which unreasonably pushes apart   many negative pairs that possess similar semantics ,   we employ the inherent hierarchical structure in   relations . As illustrated in Figure 1 , we perform   the HPC algorithm iteratively at each epoch to uti-   lize hierarchical relational features . Note that the   relational feature hwill be updated in each batch   while training , but the exemplars will not be re-   trieved until the epoch is finished . To maintainthe invariance of exemplars and avoid represen-   tation shift problems with the relational features   in an epoch , we need to smoothly update the pa-   rameters of the encoder to ensure a fairly stable   relational feature space . In practice , we construct   two encoders : Momentum Encoder fand Propul-   sion Encoder f , both of which is a instance of the   Contextualized Relation Encoder . θis updated by   contrastive learning loss and θis a moving average   of the updated θto ensure a smoothly update of   relational features ( He et al . , 2020 ) . We leverage   HPC on the momentum features h = f(x)to   obtain C(Line 19 ) , which contains Llayers of clus-   ter results with cexemplars respectively , where   cis the number of clusters at layer l. In order to   enforce the relational features more similar to their   corresponding exemplars compared to other exem-   plars ( Caron et al . , 2020 ; Li et al . , 2021 ) , we define   exemplar - wise contrastive learning as ExemNCE :   L = −/summationdisplay1   L / summationdisplaylogexp(h·e / τ)/summationtextexp(h·e / τ )   ( 10 )   where j∈[1 , c],eis the corresponding exem-   plar of instance iat layer landqindicates all the   exemplars from 1tocat layer l. As we have   explicitly constrained handeinto approximate   feature space , so the temperature parameter τcan   be shared here . The difference between InfoNCE   and ExemNCE is described in the second part of   Figure 2 , where the solid line represents positive   while the dashed line represents negative .   Furthermore , we add InfoNCE loss to retain the   local smoothness which could help propagation   clustering . Overall , our objective named Hierarchi-   cal ExemNCE is defined as :   L = L + L ( 11 )   After we update Propulsion Encoder fwith   HiNCE , the Momentum Encoder fcan be   propulsed by :   θ←m·θ+ ( 1−m)·θ(12 )   where m∈[0,1)is a momentum coefficient . The   momentum update in Eq . 12 makes θevolve more   smoothly than θespecially when mis closer to 1 .   3 Experiments   We conduct extensive experiments on real - world   datasets to prove the effectiveness of our model for5974Unsupervised Relation Extraction tasks and give   a detailed analysis of each module to show the   advantages of HiURE . Implementation details and   evaluation metrics are illustrated in Appendix A   and B respectively .   3.1 Datasets   Following previous work ( Simon et al . , 2019 ; Hu   et al . , 2020 ; Tran et al . , 2020 ) , we employ NYT+FB   to train and evaluate our model . The NYT+FB   dataset is generated via distant supervision , align-   ing sentences from the New York Times corpus   ( Sandhaus , 2008 ) with Freebase ( Bollacker et al . ,   2008 ) triplets . We follow the setting in Hu et al .   ( 2020 ) ; Tran et al . ( 2020 ) and filter out sentences   with non - binary relations . We get 41,685 labeled   sentences containing 262 target relations ( including   no_relation ) from 1.86 million sentences .   There are two more further concerns when we   use the NYT+FB dataset , which are also raised   by Tran et al . ( 2020 ) . Firstly , the development   and test sets contain lots of wrong / noisy labeled   instances , where we found that more than 40 out   of 100 randomly selected sentences were given   the wrong relations . Secondly , the development   and test sets are part of the training set . Even un-   der the setting of unsupervised relation extraction ,   this is still not conducive to reflecting the perfor-   mance of models on unseen data . Therefore , we   follow Tran et al . ( 2020 ) and additionally evalu-   ate all models on the test set of TACRED ( Zhang   et al . , 2017 ) , a large - scale crowd - sourced relation   extraction dataset with 42 relation types ( including   no_relation ) and 18,659 relation mentions in the   test set .   3.2 Baselines   We use standard unsupervised evaluation metrics   for comparisons with other eight baseline algo-   rithms : 1 ) rel - LDA ( Yao et al . , 2011 ) , generative   model that considers the unsupervised relation ex-   traction as a topic model . We choose the full rel-   LDA with a total number of 8 features for compari-   son . 2 ) MARCH ( Marcheggiani and Titov , 2016 )   proposed a discrete - state variational autoencoder   ( V AE ) to tackle URE . 3 ) UIE ( Simon et al . , 2019 )   trains a discriminative RE model on unlabeled in-   stances by forcing the model to predict each rela-   tion with confidence and encourages the number   of each relation to be predicted on average , where   two base models ( UIE - PCNN and UIE - BERT ) are   considered . 4 ) SelfORE ( Hu et al . , 2020 ) is a self - supervised framework that clusters self - supervised   signals generated by BERT adaptively and boot-   straps these signals iteratively by relation classifi-   cation . 5 ) EType ( Tran et al . , 2020 ) uses one - hot   vector of the entity type pair to ascertain the im-   portant features in URE . 6 ) MORE ( Wang et al . ,   2021 ) utilizes deep metric learning to obtain rich   supervision signals from labeled data and drive the   neural model to learn semantic relational represen-   tation directly . 7 ) OHRE ( Zhang et al . , 2021b )   proposed a dynamic hierarchical triplet objective   and hierarchical curriculum training paradigm for   open relation extraction . 8) EIURE ( Liu et al . ,   2021 ) is the state - of - the - art method that intervenes   in the context and entities respectively to obtain   the underlying causal effects of them . Since most   of the baseline methods do not exactly match the   dataset and experimental setup of our method , the   baselines are reproduced and adjusted to the same   setting to ensure a fair comparison .   3.3 Results   Since most baseline methods adopted the setting by   clustering all samples into 10 relation classes ( Si-   mon et al . , 2019 ; Hu et al . , 2020 ; Tran et al . , 2020 ;   Liu et al . , 2021 ) , we adjust the pin Algorithm 1 to   get the same results for fair comparison , and name   this setting HiURE w. 10 clusters . Although 10   relation classes are lower than the number of true   relation types in the dataset , it still reveals impor-   tant insights about models ’ ability to tackle skewed   distribution .   Table 1 demonstrates the average performance   and standard deviation of the three runs of   our model in comparison with the baselines on   NYT+FB and TACRED . We can observe that   EIURE achieves the best performance among all   the baselines , which is considered the previous   state - of - the - art method . The proposed HiURE out-   performs all baseline models consistently on B   F1 , V - measure F1 , and ARI . HiURE on average   achieves 3.4 % higher in BF1 , 2.9 % higher in   V - measure F1 , and 3.9 % higher in ARI on two   datasets when comparing with EIURE . The stan-   dard deviation of HiURE is particularly lower than   other baseline methods , which validates its robust-   ness . Furthermore , the performance of HiURE on   TACRED exceeds all the baseline methods by at   least 2.1 % . These performance gains are likely   from both 1 ) higher - quality manually labeled sam-   ples in TACRED and 2 ) an improved discriminative5975   power of HiURE considering the variation and se-   mantic shift from NYT+FB to TACRED .   Effectiveness of HPC . HPC considers all data   points and uses their mutual similarity to find the   most suitable points as exemplars for each cluster ,   these exemplars could update the instances in their   own clusters and transfer the relational features   from high - level relations to base - level through the   cross hierarchy attention . From Table 1 , HiURE   w/o HPC , which uses k - means instead of the pro-   posed hierarchical clustering , gives 4.7 % less per-   formance in average over all metrics when compar-   ing with HiURE .   Effectiveness of Cross Hierarchy Attention . In   order to explore how CHA helps data points to ob-   tain the semantics of exemplars as training signals   in HPC , Figure 3(a ) illustrates the log loss values   of HiNCE during the training epochs . Based on the   loss curve , using Cross Hierarchy Attention leads   to a consistently lowered loss value , which implies   that it provides high - quality signals to help train a   better relational clustering model .   Considering that our exemplars correspond to   specific data points and relations , we further show   the hierarchical relations the model derived from   the dataset . From Figure 4 , we can observe a three-   layer exemplars structure the model derives from   the NYT+FB dataset without any prior knowledge .   The high - level relations and base - level relations   belonging to an original cluster convey similar rela-   tion categories , which demonstrates the rationality   of exemplars in relational feature clustering . As5976   the number of exemplars between different layers   increases , some exemplars are adaptively replaced   with more fine - grained ones in the base - level layer .   Note that the approach in this paper works best   only when the relational structure in the dataset   is hierarchical . Other cases , such as graph struc-   tures or binary structures , are untested and may not   perform optimally .   Effectiveness of HiNCE . The main purpose of   HiNCE is to leverage exemplar - wise contrastive   learning in addition to instance - wise . HiNCE   avoids the pitfall where many instance - wise neg-   ative pairs share similar semantics but are unde-   sirably pushed apart in the feature space . We first   conduct an ablation study to demonstrate the effec-   tiveness of this module . From Table 1 , HiURE w/o   HiNCE gives us 6.3 % less performance averaged   over all metrics . Then we report the average per-   formance of BF1 , V - measure F1 , and ARI on the   two datasets changing with epochs , which reflects   the quality and purity of the clusters generated by   HiURE . From Figure 3(b ) , compared to InfoNCE   alone , training on HiNCE can improve the perfor-   mance as training epochs increase , indicating that   better representations are obtained to form more   semantically meaningful clusters .   Visualize Hierarchical Contextualized Features .   To further intuitively show how tree - structured hier-   archical exemplars help learn better contextualized   relational features on entity pairs for URE , we visu-   alize the contextual representation space Rafter dimension reduction using t - SNE ( Maaten and   Hinton , 2008 ) . We randomly choose 400 relations   from TACRED dataset and the visualization results   are shown in Figure 5 .   From Figure 5 ( a ) , we can see that HiURE can   give proper clustering results to the higher - level   relational features generated by propagation clus-   tering , where features are colored according to their   clustering labels . In order to explore how our mod-   ules utilize high - level relation features to guide the   clustering of base - level relations , we preserve the   color series of the corresponding high - level clus-   tering relation labels , while base - level clustering   relation labels with different shapes to get Figure 5   ( b ) ( c ) ( d ) . HiURE in ( b ) learns denser clusters and   discriminitaive features . However , HiURE without   ExemNCE in ( c ) is difficult to obtain the semantics   of the sentences without exemplar - wise informa-   tion , which makes the clustering results loose and   error - prone . When Hierarchical Propagation Clus-   tering is not applied as ( d ) , k - means is adopted   to perform clustering on the high - level relational   features , which could not use exemplars to update   relational features or mutual similarity between   feature points . On that occasion , HiURE w/o HPC   gives the results where the points between clus-   ters are more likely to be mixed . The outcomes   revealed above prove the effectiveness of HiURE   to obtain the semantics of sentences while distin-   guishing between similar and dissimilar sentences .   Hyperparameter Analysis . We have explicitly   introduced two hyperparameters Pin the encoder   andLin the HPC algorithm . We first study the   number of Random Span words Pwhich affects   the fixed - length of relation representation in Eq .   2 by changing Pfrom 1 to 4 and report the av-   erage performance of BF1 , V - measure F1 , and   ARI on NYT+FB and TACRED . From Table 2 , the   fluctuation results indicate that both information   deficiency and redundancy of relation representa-   tions will affect the model ’s performance . Using   short span words will introduce less - information   relational features so that is hard to transfer repre-   sentations from a large scale of sentences , while   long span words will cause high computational   complexity and lead to information redundancy .   Then , we study the level of Lhierarchical layers   as well as the way of building preference sequences   to form them , so as to discover the most suitable   tree - structured hierarchical relations for the data   distribution . We change Lfrom 2 to 5 with fixed5977   top preference pand bottom preference pto   get the effect of Land report the average perfor-   mance in Table 2 . The fluctuation here implies   that fewer layers fail to transfer more information   while more layers may cause exemplar - level infor-   mation conflicts between different coarse - grained   layers . ( Moiane and Machado , 2018 ) has shown   that the minimum and median value of the similar-   ity matrix are the best preferences for propagation   clustering , so we manually adjust the preference   sequence between them multiple times with L= 3   and get the average results as 3+M to compare with   the automatically generated ones by Line 3 - 4 in   HPC . The results show that the bottom layer is not   so sensitive to the preference sequence as long as it   is reasonable , which proves the practicability and   effectiveness of the equation in Line 4 .   4 Related Work   Unsupervised relation extraction has received at-   tention recently ( Simon et al . , 2019 ; Tran et al . ,   2020 ; Hu et al . , 2020 ) , due to the ability to discover   relational knowledge without access to annotations   or external resources . Unsupervised models either   1 ) cluster the relation features extracted from the   sentence , or 2 ) make more assumptions as learning   signals to discover better relational representations .   Among clustering models , an important mile-   stone is the self - supervised learning approach   ( Wiles et al . , 2018 ; Caron et al . , 2018 ; Hu et al . ,   2020 ) , assuming the cluster assignments as pseudo-   labels and a classification objective is optimized .   However , these works heavily rely on a frequently   re - initialized linear classification layer which in-   terferes with representation learning . Zhan et al .   ( 2020 ) proposes Online Deep Clustering that per-   forms clustering and network update simultane-   ously rather than alternatingly to tackle this con-   cern , however , the noisy pseudo labels still affect   feature clustering when updating the network ( Hu   et al . , 2021a ; Li et al . , 2022b ; Lin et al . , 2022 ) .   Inspired by the success of contrastive learning   in computer vision tasks ( He et al . , 2020 ; Li et al . ,2021 ; Caron et al . , 2020 ) , instance - wise contrastive   learning in information extraction tasks ( Peng et al . ,   2020 ; Li et al . , 2022a ) , and large pre - trained lan-   guage models that show great potential to encode   meaningful semantics for various downstream tasks   ( Devlin et al . , 2019 ; Soares et al . , 2019 ; Hu et al . ,   2021b ) , we proposed a hierarchical exemplar con-   trastive learning schema for unsupervised relation   extraction . It has the advantages of supervised   learning to capture high - level semantics in the re-   lational features instead of exploiting base - level   sentence differences to strengthen discriminative   power and also keeps the advantage of unsuper-   vised learning to handle the cases where the num-   ber of relations is unknown in advance .   5 Conclusion   In this paper , we propose a contrastive learning   framework model HiURE for unsupervised rela-   tion extraction . Different from conventional self-   supervised models which either endure gradual   drift or perform instance - wise contrastive learning   without considering hierarchical relation structure ,   our model leverages HPC to obtain hierarchical   exemplars from relational feature space and fur-   ther utilizes exemplars to hierarchically update re-   lational features of sentences and is optimized by   performing both instance and exemplar - wise con-   trastive learning through HiNCE and propagation   clustering iteratively . Experiments on two pub-   lic datasets show the effectiveness of HiURE over   competitive baselines .   6 Acknowledgement   We thank the reviewers for their valuable com-   ments . The work was supported by the National   Key Research and Development Program of China   ( No . 2019YFB1704003 ) , the National Nature Sci-   ence Foundation of China ( No . 62021002 and   No . 71690231 ) , NSF under grants III-1763325 , III-   1909323 , III-2106758 , SaTC-1930941 , Tsinghua   BNRist and Beijing Key Laboratory of Industrial   Bigdata System and Application .   References59785979A Implementation Details   In the encoder phase , we set the number Pof ran-   domly selected words in the [ Span ] to 2 , the rea-   son for which is illustrated in parameter analysis .   Therefore the output entity - level features hand   hpossess the dimension of 4·b , where b=   768 . We use the pretrained BERT - Base - Cased   model to initialize both the Momentum Encoder   and Propulsion Encoder respectively , and use   AdamW ( Loshchilov and Hutter , 2017 ) to optimize   the loss . The encoder is trained for 20 epochs   with 1e−5learning rate . In the HPC phase , we   set the numbers of layers Lto 3 after parameter   analysis and the maximum iterations at Line 8 to   400 to make sure the algorithm terminates in time   and make the converge condition as Enot change   for 10 iterations . We set temperature parameter   τ= 0.02and momentum parameter m= 0.999   following ( He et al . , 2020 ) and adjust the number of   negative samples Jto512to accommodate smaller   batches .   B Evaluation metrics   We follow previous works and use B(Bagga   and Baldwin , 1998 ) , V - measures ( Rosenberg and   Hirschberg , 2007 ) and Adjusted Rand Index ( ARI )   ( Hubert and Arabie , 1985 ) as our end metrics . B   uses precision and recall to measure the correct rate   of assigning data points to its cluster or clustering   all points into a single class . We use V - measures   ( Rosenberg and Hirschberg , 2007 ) to calculate ho-   mogeneity and completeness , which is analogous   toBprecision and recall . These two metrics pe-   nalize small impurities in a relatively “ pure ” cluster   more harshly than in less pure ones . We also report   the F1 value , which is the harmonic mean of Hom .   and Comp . Adjusted Rand Index ( ARI ) ( Hubert   and Arabie , 1985 ) measures the similarity of pre-   dicted and golden data distributions . The range of   ARI is [ -1,1 ] . The larger the value , the more consis-   tent the clustering result is with the real situation.5980