  Zhijian Hou , Wanjun Zhong , Lei Ji , Difei Gao , Kun Yan ,   Wing - Kwong Chan , Chong - Wah Ngo , Mike Zheng Shouand Nan DuanCity University of Hong KongSun Yat - sen UniversityMicrosoft Research AsiaNational University of SingaporeBeihang UniversitySingapore Management University   Abstract   This paper tackles an emerging and challeng-   ing problem of long video temporal ground-   ing ( VTG ) that localizes video moments related   to a natural language ( NL ) query . Compared   with short videos , long videos are also highly-   demanded but less explored , which brings new   challenges in higher inference computation cost   and weaker multi - modal alignment . To address   these challenges , we propose CONE , an ef-   ficient COarse - to - fiNE alignment framework .   CONE is a plug - and - play framework on top   of existing VTG models to handle long videos   through a sliding window mechanism . Specif-   ically , CONE ( 1 ) introduces a query - guided   window selection strategy to speed up infer-   ence , and ( 2 ) proposes a coarse - to - fine mech-   anism via a novel incorporation of contrastive   learning to enhance multi - modal alignment for   long videos . Extensive experiments on two   large - scale long VTG benchmarks consistently   show both substantial performance gains ( e.g. ,   3.13%− − − → 6.87 % on MAD ) and state - of - the-   art results . Analyses also reveal higher effi-   ciency as the query - guided window selection   mechanism accelerates inference time by 2x   on Ego4D - NLQ and 15x on MAD while keep-   ing SOTA results . Codes have been released at   https://github.com/houzhijian/CONE .   1 Introduction   Video temporal grounding ( Anne Hendricks et al . ,   2017 ; Gao et al . , 2017 ) aims to locate specific mo-   ments in an untrimmed video relevant to a textual   user query . This is a crucial task in multi - modal   video understanding and can be applied to many   practical applications such as video retrieval ( Xu   et al . , 2016 ) , video editing , and video question an-   swering ( Lei et al . , 2018 , 2020a ) .   Temporal grounding for long videos is espe-   cially highly - demanded and emerging due to theFigure 1 : An example of long video temporal grounding ,   which requires coarse - to - fine multi - modal alignment .   flourishing growth of online videos in quantity and   length . However , limited by previously available   datasets ( Gao et al . , 2017 ; Krishna et al . , 2017 ; Lei   et al . , 2021 ) , existing works ( Zhang et al . , 2020a , b )   mainly deal with relatively short videos ranging   from 0.5 to 2.5 minutes on average . Recently ,   Ego4D ( Grauman et al . , 2022 ) and MAD ( Sol-   dan et al . , 2022 ) datasets have been created and   attempted to deal with long video , which spans   from several minutes to hours .   Early attempts for long video grounding extends   existing VTG models for short videos through   sparse sampling ( Grauman et al . , 2022 ) or a sliding   window adaptation ( Soldan et al . , 2022 ) . On the   one hand , existing models for short videos typi-   cally downsample videos to a fixed - length frame   sequence , which leads to temporal information loss   for long videos ( i.e. , fewer visible frames via sparse   sampling ) . Besides , the window - based adaptation   methods divide the long video into candidate win-   dows and inference on every window , which leads   to high inference computational cost . On the other   hand , massive moment candidates from long videos   make their precise multi - modal alignment with the   NL query more challenging , which leads to con-   textual information loss ( i.e. , weaker alignment   to fine - grained contents , like objects , actions , and8013scenes ) . As the motivating example shown in Fig .   1 , accurate grounding requires a coarse - grained   localization of relevant video segments ( e.g. “ in   the room " v.s. “ outdoor " ) , and fine - grained align-   ment to detailed object and action in frames ( e.g. ,   “ women " and “ holding " ) . In conclusion , long video   length poses two new challenges : ( 1 ) higher com-   putational cost during inference on the numerous   windows of the entire long video ; ( 2 ) weaker multi-   modal alignment due to the abundance of moment   candidates .   To address these challenges , we propose CONE ,   a COarse - to - fiNE alignment framework for long   video temporal grounding . First , we slice the arbi-   trary long video into candidate windows . Then , we   employ a query - guided window selection strategy   for efficiency and further introduce a coarse - to-   fine mechanism for effectiveness . Specifically , the   query - guided window selection strategy efficiently   reduces the sizeable window number of the long   video to a modest amount via efficient alignment   score computation . Moreover , the coarse - to - fine   alignment mechanism consists of three modules to   gradually align the multi - modal inputs via multi-   scale granularity : ( 1 ) window ( coarse granularity )   selection to reliably select semantically relevant   candidate windows ; ( 2 ) window ( coarse)-proposal   ( fine ) joint contrastive learning to generate candi-   date proposals considering both inter - window and   intra - window relations ; ( 3 ) proposal ( fine granu-   larity ) ranking to further accurately sort out the   perfect proposal . Notably , we incorporate con-   trastive learning into this mechanism . On the one   hand , we utilize the powerful multi - modal align-   ment ability of contrastive vision - text pre - trained   models ( e.g. , CLIP ) , which computes matching   scores between video frames and textual query for   both window selection and proposal ranking stages .   On the other hand , we select a contrasting negative   window and design an inter - window contrastive   loss during training for the joint learning stage .   With this coarse - to - fine design , CONE has the   following advantages : ( 1 ) higher efficiency in han-   dling long video inputs with less temporal informa-   tion loss ; ( 2 ) more accurate multi - modal alignment   with less contextual information loss . We evalu-   ate CONE on two large - scale long video ground-   ing benchmarks and achieve both the state - of-   the - art results and significant performance boosts   ( 3.13 % →6.87 % on MAD , and 10.46 % →13.46 %   on Ego4D - NLQ in terms of R1@IoU=0.3 ) . Furtheranalysis shows that the window selection strategy   shortens the inference speed by 2x for Ego4D - NLQ   and 15x for MAD compared to inference on every   window , without sacrificing its performance .   Contributions Our work presents two major con-   tributions to the long video temporal grounding   field . ( 1 ) We propose a novel coarse - to - fine align-   ment framework that utilizes a pipeline of { win-   dow slicing and selection , proposal generation and   ranking } , resulting in state - of - the - art performance   and high efficiency on two representative bench-   marks . ( 2 ) We introduce a novel approach for in-   corporating contrastive learning into multi - modal   alignment .   2 Related Work   Video Temporal Grounding . Current models for   this task typically fall into two categories based   on the usage of proposals . Proposal - free methods   predict start / end timestamps directly , bypassing   the generation of proposals ( Ghosh et al . , 2019 ;   Zeng et al . , 2020 ; Zhang et al . , 2021c ) . Con-   versely , Proposal - based methods merge proposal   generation and ranking within an end - to - end frame-   work ( Zhang et al . , 2020b ; Lei et al . , 2021 ; Cao   et al . , 2021 ) . An excellent survey paper provides   further details on these models ( Zhang et al . , 2022 ) .   However , these models are predominantly designed   for relatively short videos , which leads to sub-   stantial information loss when adapted directly to   long - form settings . Regarding long - form video   grounding , VSLNet - L ( Zhang et al . , 2021b ) ex-   tends VSLNet ( Zhang et al . , 2020a ) with a multi-   scale split - and - concat mechanism to address per-   formance degradation . Still , the multi - scale mecha-   nism adds computation cost during inference , and   the sparse sampling approach suffers from sig-   nificant temporal information loss for hour - long   videos . Authors of Ego4D and MAD adapt 2D-   TAN ( Zhang et al . , 2020b ) and VLG - Net ( Soldan   et al . , 2021 ) into simple window - based baselines ,   yet these models lack the ability for coarse - to - fine   alignment , affecting the final results .   Furthermore , other studies have focused on dif-   ferent issues . For example , NaQ ( Ramakrishnan   et al . , 2023 ) addresses data scarcity with an effec-   tive data augmentation strategy , while DeNet ( Zhou   et al . , 2021 ) tackles the ground - truth bias problem   with a de - coupling and de - bias strategy . There are   also other related tasks involving language ground-   ing with video corpus input ( Lei et al . , 2020b ; Hou8014et al . , 2021 ) or spatial - temporal output ( Yang et al . ,   2022 ) , but they fall outside the scope of this paper .   Long - form Video Modeling . Long - form video   modeling is an emerging topic recently investigated   in ( multi - modal ) video understanding , including   classification , segmentation , localization , and re-   trieval . The common challenges of long videos are   modeling long - range temporal dependency , effi-   ciency issues , and accurate multi - modal alignment   ( if language is involved ) . To ease long - range de-   pendency issues , some works explore either fea-   ture memory banks ( Wu et al . , 2019 ; Feng Cheng ,   2022 ) , tracked object - level representations ( Wu   and Krahenbuhl , 2021 ) , bayesian non - parametric   model ( Qiu et al . , 2023 ) , or structured state - space   sequence layers ( Islam and Bertasius , 2022 ) . To   alleviate efficiency issues , other works ( Lin et al . ,   2022b ) exploit dense audio sampling as additional   information to enable sparse video frame sampling .   In contrast , CONE mitigates both efficiency issues   via query - guided window selection strategy and   multi - modal alignment issues via novel contrastive   learning incorporation .   Contrastive Learning . Contrastive learning has   been widely studied in vision ( Misra and Maaten ,   2020 ; He et al . , 2020 ) , language ( Gao et al . , 2021b ;   Meng et al . , 2021 ) , and multi - modal fields ( Liang   et al . , 2020 ; Yan et al . , 2021 ; Wang et al . , 2022 ) .   Some recent VTG works also attempt to adopt con-   trastive learning . They mainly focus on frame - level   contrastive loss via hard negative mining ( Zheng   et al . , 2022 ) or contrasting ground - truth frame with   non - ground - truth frame ( Nan et al . , 2021 ; Zhang   et al . , 2021a ) . In contrast , we further propose   window - level contrastive learning to repel the nega-   tive windows . Moreover , CONE also jointly incor-   porates contrastive vision - text pre - trained models   for accurate alignment , because those contrastive   models show strong multi - modal alignment abil-   ity ( Luo et al . , 2021 ; Xu et al . , 2021 ) derived from   pre - training large - scale vision - text pairs .   3 Task Definition   Since never - ending video streams in real applica-   tions have a higher demand for long videos , in this   paper , we study the task of video temporal ground-   ing ( VTG ) in a more challenging setting with long   video inputs . Taking a natural language ( NL ) query   Qand a long video Vas the inputs , the task of   VTG requires the system to locate the matched mo-   ment Mfrom the video Vrelevant to the queryQ. Formally , video V= [ v , v , . . . , v]is a se-   quence of uniformly sampled video frames , and L   denotes the length of the sampled frames . The NL   query Q= [ q , q , . . . , q]is a sequence of tokens   with sentence length L. The moment Mis a sub-   sequence of Vthat is relevant to Q. The long - form   video setting is more challenging because of the   natural demands for more computation and time to   process the entire video . Moreover , the accurate   multi - modal alignment between each vandQis   also harder when Lincreases .   4 Approach   We present the proposed CONE for long video   temporal grounding . As shown in Fig . 2 , we first   slice the long video into several fixed - length video   windows via a sliding window mechanism ( § 4.1 ) .   Then , we propose a coarse - to - fine mechanism for   efficient and effective multi - modal alignment . At   the coarse - grained window level , we introduce a   query - guided window selection strategy ( § 4.2 )   to accelerate inference and select semantic relevant   windows . At the window - proposal joint learning   level , we rely on the existing VTG work for short   videos to generate reliable candidate proposals and   conduct both intra - window and proposed inter-   window contrastive learning ( § 4.3 ) to assign each   proposal score . At the fine - grained proposal level ,   we further accurately rank candidate proposals with   afine - grained ranking ( § 4.4 ) mechanism .   4.1 Window - based Video Slicing   To flexibly handle long videos without decreasing   the sample rate and alleviate temporal information   loss , we first slice the entire video Vinto several   video windows W. A sliding window with window   length Lis used to be slid on the entire video   to derive a set of Nfixed - length video windows   W= [ v , v , ... , v ] , where wis the   start index of window i. Concretely , we slide the   window by increasing wwith window stride L/2 .   Intuitively , not every window is correlated with the   NL query , so we refer the positive window to the   window overlapping with the ground - truth moment ,   and the negative window otherwise .   4.2 Coarse - grained Window Selection   Lengthy video input is sliced into a sequence of   windows during inference . If the number of win-   dows is N , the model needs to conduct the whole   encoding - prediction process for Ntimes , which8015   will become computationally costly with increased   video length , especially when the model has enor-   mous parameters . Therefore , it is necessary to   reduce the inference computation cost by reliably   filtering windows irrelevant to the natural language   descriptions . We propose a query - guided window   selection strategy via contrastive vision - text pre-   trained models .   Vision - Text Contrastive Model . CLIP ( Radford   et al . , 2021 ) and EgoVLP ( Lin et al . , 2022a ) are   pre - trained with large - scale vision - text pairs via   contrastive learning , aiming to align the visual rep-   resentation with its related text representation . So   they excel at multi - modal alignment , and efficient   dot - product score computation provides higher   matching efficiency . We utilize the pre - trained   model to compute the video features Vand the   text features Qbeforehand .   V= [ v , v , . . . , v ]   Q= [ q , q , q , . . . , q](1 )   where [ CLS ] is a special token at the beginning   of text tokens . The multi - modal matching score   a = v·q is computed via the efficient dot   product between jvideo feature and the text fea-   ture . And the window - level matching score Ais   the maximum score of all the frames in window i :   A= max ( [ a , a , ... , a ] ) ( 2 )   We rank all windows with Aand select the top- k   windows for inference . Thus , we reduce the num-   ber of candidate windows from Nto a constant k   to guarantee a controllable computation cost in an   efficient and reliable manner.4.3 Window ( Coarse)-Proposal ( Fine )   Joint Contrastive Learning   Since there are many excellent works in the VTG   literature for short video input , we target CONE as   a flexible plug - and - play framework on top of the   existing proposed - based model . Existing models   function as the base model to generate reliable can-   didate moment proposals for further processing .   In our scenario , the base model takes window W   and the NL query Qas inputs and outputs several   moment proposal candidates . Each of them has   a moment proposal ( p)and corresponding score   ( s ) , respectively .   Nevertheless , the training of the base model typi-   cally focuses only on the positive window and con-   ducts intra - window relation learning but neglects   plenty of negative windows for long videos . In real   practice , negative windows are the majority during   inference , which results in a discrepancy between   training and inference . To mitigate the discrepancy ,   we further design inter - window contrastive learning   to distinguish the positive and negative windows .   As a result , the overall training loss consists of two   parts : ( 1 ) the intra - window loss of the base model ,   and ( 2 ) the proposed inter - window contrastive loss .   Concretely , each training instance has both a   positive window and a random contrasting negative   window from the long video . We discriminate the   negative window from the positive window through   proposal - level comparison . Proposals in the neg-   ative window should be assigned with minimized   scores compared with positive proposals ( e.g. , the   IoU with ground truth is large than 0.7 ) in the posi-8016   tive window , as follows ,   L=−/summationdisplaylog(s)−/summationdisplaylog(1−s )   ( 3 )   where Lis the proposed contrastive loss , pis   each positive proposal from the positive window   Wandpis each proposal from the negative win-   dowW , sis the corresponding proposal score .   This mechanism can be generalized to different   existing proposal - based models with different intra-   window losses . To show the generalization ability   of CONE , we test on two kinds of base models : 2D-   TAN ( Zhang et al . , 2020b ) and Moment - DETR(Lei   et al . , 2021 ) due to their available codes and supe-   rior performances . Please refer to Appendix A.2   for comprehensive training details .   4.4 Fine - grained Proposal Ranking   With the increased length of video inputs , the fine-   grained attention between each video frame and   the text query will be weakened by many other   perturbed frames , resulting in contextual informa-   tion loss . To remedy this issue , we propose a fine-   grained ranking strategy to conduct accurate pro-   posal ranking utilizing proposal matching scores   computed by contrastive vision - text pre - trained   models ( described in § 4.2 ) . Note that we sim-   ply re - use the pre - computed video frames and text   query features as in Eq . ( 1 ) .   Visual Adapter . With a lightweight visual   adapter on top of CLIP , we exploit adapter - based   tuning to adapt the representations from the gen-   eral contrastive model to the data distribution of   the current downstream task . Inspired by Gao   et al . ( 2021a ) , our main idea is to add an addi-   tional bottleneck layer to learn the task - adaptive   visual features and conduct residual - style blend-   ing with the original pre - trained features . The   lightweight adapter complies with a 2 - layer FFN   followed by ReLU . The iadapted visual feature   is:ˆv = Adapter ( v ) + v. Then , the proposal   feature for the jproposal is computed with the   mean pooling of all the adapted video features in   it : h = Mean ( [ ˆv , . . . , ˆv])For adapter training , we denote the positive pro-   posal ( with feature h ) as the ground - truth one ,   and the negative proposals are the other in the same   batch . We follow the standard contrastive learning   and use the NCE loss :   L = −/summationdisplay(logexp(h·q)/summationtextexp(h·q))(4 )   Note that we also use the adapted visual feature   to compute the window - level matching score in   § 4.2 to improve window selection quality .   Ranking Score Computation . Finally , we aim   to conduct a fine - grained ranking for proposals . For   thejproposal , the final ranking score is fused   with two components : ( 1 ) proposal scores sgener-   ated from the previous module and ( 2 ) fine - grained   matching scores mcomputed by adapted CLIP-   based proposal feature : m = h·q . The for-   mer captures the correlation between proposals via   the sophisticated contextual model design , while   the latter focuses on fine - grained content match-   ing between frames in the proposal and the textual   query . We perform min - max normalization for   these two types of scores for a more stable score   fusion . The final ranking score ris the sum of two   normalized scores :   ˜s = MinMax ( [ s , s , ... , s ] ) ,   ˜m = MinMax ( [ m , m , ... , m ] )   r= ˜s+ ˜m(5 )   where Nis the total candidate proposal number .   5 Experiments   We conduct experiments to explore the effective-   ness of CONE from the following aspects : ( 1 )   model comparison with SOTA methods ( § 5.3 ) ;   ( 2 ) ablation study to analyze the impact of each   component and different variants ( § 5.4 ) ; ( 3 ) ef-   ficiency analysis of acceleration with window se-   lection ( § 5.5 ) and ( 4 ) qualitative analysis ( § 5.6 ) .   Implementation details are given in Appendix A.1 .   5.1 Dataset   We conduct comprehensive experiments on two rep-   resentative large - scale benchmarks on long video8017   temporal grounding : Ego4D - NLQ ( Grauman et al . ,   2022 ) and MAD ( Soldan et al . , 2022 ) . The data   statistics are summarized in Table 1 .   Ego4D - NLQ is a subtask of the Ego4D dataset .   Ego4D is a large - scale egocentric video understand-   ing benchmark , where 931 camera wearers world-   wide record their daily activities in hundreds of sce-   narios . The unedited videos involved have variant   lengths ranging from 3.5 min . to 20 min . The NL   query is designed to retrieve the relevant moment   from the episodic memory of camera wearers , and   involves 13 question types for locating different   types of information .   MAD is a large - scale long video temporal   grounding benchmark on full - length movie videos .   The video duration is magnificent longer than pre-   vious benchmarks at an average length of 110.8   min ( ranging from 47 min . to 202 min ) . The tex-   tual queries in the training set are derived from   translated movie audio descriptions from profes-   sional narrators . The annotations in the evaluation   set are of higher quality derived from the LSMDCdataset ( Rohrbach et al . , 2017 ) with more strict   manual refinement . The NL queries are cleaner   and the ground - truth moments have preciser tem-   poral boundaries .   5.2 Experimental Settings   5.2.1 Evaluation Metric .   For consistent comparison , we follow the previ-   ous baseline evaluation setting and use the metric   Recall @katIoU = θ(R@ k ) . It is the percentage   of queries , having at least one prediction among   the top- kpredictions , whose temporal IoU with   ground - truth is larger than the threshold θ(0.3 or   0.5 ) . Note that there is only one ground - truth an-   swer for each query in both datasets .   5.2.2 Visual and Textual Features .   For MAD dataset , we use CLIP ( Radford et al . ,   2021 ) to extract the visual and textual features ,   which are then utilized in the inputs of the window   selection , proposal generation , and fine - grained   ranking stages . The MAD authors extract the visual   features every 0.2s ( 5fps ) . For Ego4D - NLQ dataset ,   we adopt the egocentric contrastive pre - trained   model , i.e. , EgoVLP ( Lin et al . , 2022a ) , to extract   visual and textual features , because CLIP is trained   with third - person image - text pairs and has the do-   main adaptation gap for first - person videos . We ex-   tract the visual feature every 16 frames ( 1.875fps ) .   5.3 Model Comparison   5.3.1 Baselines .   We compare CONE to the following methods :   ( 1 ) the window - based adaptation of proposal-   based models , i.e. , 2D - TAN ( Zhang et al . , 2020b ) ,   VLG - Net ( Soldan et al . , 2021 ) and Moment-   DETR ( Lei et al . , 2021 ) ; ( 2 ) the sparse sampling-   based proposal - free model VSLNet ( Zhang et al . ,   2020a ) ; ( 3 ) the two - stage model CLIP ( Radford   et al . , 2021 ) , which first generates offline proposals8018and then ranks those proposals with CLIP matching   scores .   Results on Ego4D - NLQ . Table 3 reports the per-   formance comparison on the validation and the   blind test set of Ego4D - NLQ . Regarding all the   metrics , CONE outperforms these baselines by a   large margin . Take R1@IoU=0.3 and R5@IoU=0.3   as examples , the absolute performance gains are   +3.31 % and +11.49 % on the val . set , and +3 % and   +6.92 % on the blind test set , respectively . CONE   also achieves consistent performance gains using   different features and base models , showing a better   generalization ability .   Results on MAD . The main results of MAD   are shown in Table 2 , and demonstrate that   CONE achieves state - of - the - art performances   with obvious boost ( e.g. , 3.13%− − − − → 6.87 %   and 9.85%− − − − → 16.11 % improvement on   R1@IoU=0.3 and R5@IoU=0.3 ) .   From the two tables , the SOTA performance on   two benchmarks demonstrates the effectiveness of   CONE . We speculate the outstanding results are   brought by the following reasons : ( 1 ) CONE pro-   cesses the entire video without decreasing sample   rate , which alleviates temporal information loss ;   ( 2 ) the coarse - to - fine mechanism enables the bet-   ter alignment ability of relevant proposals to the   NL query and reduces contextual information loss .   The lower results on the MAD benchmark also ver-   ify that the grounding task indeed becomes more   challenging with longer videos .   Regarding base model selection , we observe that ,   compared with Moment - DETR , the performance   of 2D - TAN is worse in Ego4D - NLQ but compa-   rable in the MAD dataset . This gives the clues   that the base model might not be influential to the   final results given more training samples for hour-   level videos . Because of the lower parameter and   GLOPs number of Moment - DETR , from now on ,   CONE adopts it as the default base model .   5.4 Model Analysis   Ablation studies are shown in table 4 to unveil the   effectiveness of each component in CONE . We   consider three components : ( a ) contrastive loss   ( § 4.3 ) , which is eliminated by only training on pos-   itive windows without our contrastive loss ; ( b ) fine-   grained ranking fusion ( § 4.4 ) can be removed by   taking only the proposal score from the proposal   generation module for ranking ; ( c ) visual adapter   ( § 4.4 ) is removed by using general CLIP - based   features for matching score computation . Note that   the full CONE model refers to the first row , and the   baseline model ( i.e. , the window - based adaptation   of Moment - DETR ) refers to the last row .   From the table 4 , we highlight the following find-   ings for Ego4D - NLQ dataset ( MAD shows similar   trends ): ( 1 ) Ablating visual adapter leads to a per-   formance drop from 14.15 % to 12.62 % , which in-   dicates that domain - adaptation of visual features is   essential in modelling task - specific semantic vari-   ance . ( 2 ) Eliminating fine - grained ranking also   harms the performance ( row 2 vs. row 3 ) , show-   ing that capturing fine - grained semantic alignment   benefits accurate grounding . ( 3 ) Further removing   contrastive loss ( row 3 vs. row 4 ) leads to a sig-   nificant performance drop of 3.36 % , which reveals   that identifying the inter - window semantic variance   is critical for reliable grounding .   Influence of window length . Figure 3 exhibits   how different window lengths affect the overall per-   formance of CONE . We observe that increasing the   window length indeed brings performance variance .   Longer window length ( more visible frames in a   single window ) can model the interaction between   more frames , but can also weaken the multi - modal   attention between the NL query and every single   frame as the performance drops significantly with   the largest window size . We find a better trade-   off between window length and performance , i.e. ,   nearly 48 seconds ( 90 video features ) for Ego4D-   NLQ and nearly 25 seconds ( 125 video features )   for MAD through this analysis.80195.5 Efficiency Analysis   Figure 4 shows inference speed with respect to   different window numbers after the window selec-   tion stage on the val . sets of Ego4D - NLQ and   MAD . We observe that the overall inference time   is approximately linearly reduced with smaller win-   dow numbers , and the performance of CONE is   relatively stable when the filtered window number   becomes 10 for both datasets . Surprisingly , we   also observe that CONE achieves the optimal per-   formance ( shown as the box with value in Fig . 4 )   at a modest size of window number rather than the   full size . Those observations show that our query-   guided window selection strategy can reliably filter   the irrelevant windows and largely improve the ef-   ficiency of long video temporal grounding .   Concretely , the average window numbers for   full videos ( before filtering ) are 23.3 and 588 for   Ego4D - NLQ and MAD , respectively . If we set   the filtered window number to 10 for Ego4D - NLQ   and 30 for MAD ( better trade - off values obtained   from Fig . 4 ) , the total numbers of windows for   inference are reduced by 2.3x and 19.6x , with a   marginal performance variance ( R@1 ) by -0.1 %   and +0.2 % for Ego4D - NLQ and MAD benchmarks ,   respectively . In real practice , we consider the effect   of implementation methods and window selection   time . When CONE is inferred using one P100   GPU , its running time ( w/o feature extraction and   post - processing time , because both time costs are   the same for all methods ) is largely reduced ( 74   s− − →36 s and 276 min.− − → 18 min . ) on the val . set   of Ego4D - NLQ and MAD .   Furthermore , we also conduct an inference com-   parison between VSLNet and CONE on Ego4D-   NLQ . Using the official Ego4D released code ,   VSLNet takes approximately 12 seconds to infer   the Ego4D validation split and achieves a Recall@1   score of 10.84 at IoU=0.3 . In contrast , CONE , withthe extreme setting of selecting only the top 1 win-   dow , takes about 10.5 seconds to infer the Ego4D   validation split and achieves a Recall@1 score of   12.4 at IoU=0.3 . It demonstrates that CONE strikes   an optimal tradeoff between accuracy and speed   compared to sparse sampling - based baselines .   5.6 Qualitative Analysis   Fig . 5 shows two success examples to analyze the   effect of contrastive learning andfine - grained   ranking . We observe that baseline Moment - DETR   ( a ) is not capable of repelling negative windows and   tends to give an equally high score to the proposal   in the negative window , thus it wrongly ranks the   correct prediction to a much lower position ( e.g. ,   54 in Example A ) . Further adding inter - window   contrastive learning mechanism ( b ) , the rank po-   sition of the ground truth moment is somewhat   improved , but it still lacks fine - grained matching   to detailed contents to push the perfect proposal   into the first place . For example , in Query - A , the   most essential object is the vegetable rather than   the bag ; In Query - B , the query requires the precise   scene “ in the park " and the objects “ two women "   and “ ribbons " . Finally , adding fine - grained ranking   ( c ) , the full CONE successfully locates the ground-   truth moment relevant to the textual query . More   qualitative examples with both success and failure   cases are shown in Appendix B.   6 Conclusions   We present CONE , a COarse - to - fiNE alignment   framework for long video temporal grounding .   CONE jointly achieves state - of - the - art perfor-   mances on two benchmarks and high efficiency   while keeping high frame sample rate . Through   our experiments , we show the proposed coarse - to-   fine mechanism via contrastive learning plays an   important role in boosting performance . Regarding   efficiency , the introduced query - guided window se-   lection strategy largely accelerates inference by 2x   and 15x on Ego4D - NLQ and MAD benchmarks ,   respectively , with even slight performance gains .   Since CONE can be generalized with different ex-   isting proposal - based models , we hope it can be   used to improve the model efficiency and perfor-   mance for temporal grounding on long videos .   Limitations   Currently , CONE is mainly implemented for   proposal - based models as they can generate ex-8020   plicit moment proposals for the introduced inter-   window contrastive learning . In contrast , proposal-   free methods ( e.g. , VSLNet ( Zhang et al . , 2020a ) )   directly predict the start / end timestamps without   explicit proposals . In the future , it is worthwhile   to explore how to incorporate the coarse - to - fine   alignment mechanism with proposal - free methods   to enhance the generalization ability of CONE .   Furthermore , CONE falls short on the ground-   truth moment case whose duration is longer than   the adopted video window duration . To ease this   issue , future work can explore adaptive - duration   window slicing to ensure complete containment of   scenes and events within windows or some rule-   based proposal merging techniques .   Ethics Statement   The present study was conducted in accordance   with ethical principles . This study involved the   analysis using publicly available data and did not   involve any human participants , and potential risks   about credentials or privacy . Therefore , no ethical   clearance was required and there were no potential   risks associated with the conduct of this research .   Acknowledgements   We thank the anonymous reviewers for their insight-   ful feedback . This research was partially supported   by CityU MF_EXT project number 9678180 .   References802180228023A Model Details   A.1 Implementation Details   During training , we perform parameter optimiza-   tion via AdamX and set the learning rate to 1e-4   for the base model and 1e-5 for the visual adapter .   We set the batch size to 32 and adopt the early   stopping strategy . During inference , we use Non-   Maximum Suppression ( NMS ) with a threshold of   0.5 as post - processing .   Moment - DETR Base Model . The experiments   are conducted on one P100 GPU . For the hyperpa-   rameters of network architecture , we set the hidden   sizedto 256 , the transformer layer number in the   encoder / decoder to 2 , and the moment query num-   ber to 5 . As a result , the total parameters of CONE   are 4.35 M ( 4.22 M Moment - DETR + 0.13 M visual   adapter ) . We set the window length to 90 video   features ( 48 seconds ) and 125 video features ( 25   seconds ) for the Ego4D - NLQ and MAD datasets ,   respectively . We train Ego4D - NLQ for 150 epochs   and MAD for 30 epochs , and the training time is   about 3 hours for the Ego4D - NLQ and 18 hours   for the MAD dataset . During inference , we set   the filtered window number to 10 and 30 for the   Ego4D - NLQ and MAD datasets , respectively .   2D - TAN Base Model . The experiments are con-   ducted on two V100 GPUs . For the hyperparame-   ters of network architecture , we set the hidden size   dto 256 , the convolution network layer to 4 , and   the kernel size to 9 . As a result , the total parameters   of CONE are 23.99 M ( 23.86 M 2D - TAN + 0.13 M   visual adapter ) . We set the window length to 64   video features ( 34.1 seconds ) and 128 video fea-   tures ( 25.6 seconds ) for the Ego4D - NLQ and MAD   datasets , respectively . We train Ego4D - NLQ for 90   epochs and MAD for 6 epochs , and because of 6x   more model parameters than Moment - DETR , the   training time is quite larger , i.e. , about 18 hours for   the Ego4D - NLQ and 3 days for the MAD dataset .   During inference , we set the filtered window num-   ber to 5 and 15 for the Ego4D - NLQ and MAD   datasets , respectively .   A.2 Training Loss Details   Moment - DETR Base Model . The original loss   of Moment - DETR consists of three parts : moment   localization , classification , and saliency losses . Lo-   calization loss requires ground truth moment to   measure the discrepancy with the predictions andcan not be used in contrastive loss . Thus , we de-   sign two - level contrastive losses based on two other   losses : ( 1 ) proposal - level loss and ( 2 ) frame - level   loss with a randomly sampled negative window .   For the proposal - level contrastive loss ( L ) ,   proposals in the negative window are assigned   with minimized scores compared to positive pro-   posal ( i.e. , the optimal proposal selected from the   Hungarian algorithm ) from the positive window , as   follows ,   L=−/summationdisplaylog(s)−/summationdisplaylog(1−s)(6 )   where pis the positive proposal from the posi-   tive window Wandpis each proposal from   the negative window W , sis the corresponding   proposal score .   For the frame - level loss L , we set the average   saliency scores for frames located in the positive   window is larger than the maximum saliency score   of frames in the negative window over a margin δ :   where S ( ) is its saliency scoring function . So the   overall contrastive loss is L = L+L.   2D - TAN Base Model . The original loss of 2D-   TAN consists of the binary cross - entropy loss ,   which learns to align each proposal score with its   scaled IoU value . Thus , we assign each proposal in   the negative window with minimized scores com-   pared to the positive proposals ( i.e. , the IoU with   ground truth is large than 0.7 ) from the positive   window , as follows ,   L=−/summationdisplaylog(s)−/summationdisplaylog(1−s)(8 )   where pis the positive proposal from the posi-   tive window Wandpis each proposal from   the negative window W , sis the corresponding   proposal score . And the overall contrastive loss is   L = L.   Overall Loss . In total , our training loss ( L ) con-   sists of three parts : ( 1 ) original training loss of the   base model ; ( 2 ) contrastive loss to discriminate the   negative window versus the positive window ; ( 3 )   adapter loss ( shown in Eq . 4 ) to tune visual repre-   sentations from general pre - training to the current   downstream task :   L = L+λ×L+λ×L ( 9)8024   where λandλ are loss weight hyperparam-   eters to control the loss value . During training , we   setλto 1 and λ to 0.2 .   B More Qualitative Examples   We show four success cases in Figure 6 and four   failure cases of our model CONE in Figure 7.80258026ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   section 6 page 9   /squareA2 . Did you discuss any potential risks of your work ?   section 6 page 9   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   experiments section 5   /squareB1 . Did you cite the creators of artifacts you used ?   experiments section 5   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Ethics Statement   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Ethics Statement   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   table 1   C / squareDid you run computational experiments ?   experiments section 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   appendix A8027 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   appendix A   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   experiments section 5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Not applicable . Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.8028