th   Ruoyao Wang , Peter Jansen , Marc - Alexandre Côté , Prithviraj AmmanabroluUniversity of Arizona , Tucson , AZMicrosoft Research MontréalAllen Institute for AI , Seattle , WA   { ruoyaowang,pajansen}@arizona.edu   macote@microsoft.com , raja@allenai.org   Abstract   We present S W , a benchmark to   test agents ’ scientific reasoning abilities in a   new interactive text environment at the level of   a standard elementary school science curricu-   lum . Despite the transformer - based progress   seen in question - answering and scientific text   processing , we find that current models can not   reason about or explain learned science con-   cepts in novel contexts . For instance , mod-   els can easily answer what the conductivity of   a known material is but struggle when asked   how they would conduct an experiment in a   grounded environment to find the conductivity   of an unknown material . This begs the question   of whether current models are simply retriev-   ing answers by way of seeing a large number of   similar examples or if they have learned to rea-   son about concepts in a reusable manner . We   hypothesize that agents need to be grounded in   interactive environments to achieve such rea-   soning capabilities . Our experiments provide   empirical evidence supporting this hypothesis —   showing that a 1.5 million parameter agent   trained interactively for 100k steps outperforms   a 11 billion parameter model statically trained   for scientific question - answering and reasoning   from millions of expert demonstrations .   1 Introduction   Question answering ( QA ) has seen rapid progress   recently . Standardized elementary and middle   school science exams have served as a challenge   task for QA ( Clark et al . , 2018 ) , as these questions   require combining science - domain knowledge with   world knowledge in complex reasoning procedures   to solve . As large language models have toppled   these benchmarks ( Clark et al . , 2020 ; Khashabi   et al . , 2020 ; Xu et al . , 2021a ) , the focus has shifted   away from simply answering questions toward pro-   ducing human - readable explanations for a model ’s   Table 1:11279answers ( Jansen et al . , 2018 ; Yang et al . , 2018 ; Xie   et al . , 2020 ; Valentino et al . , 2021 ; Xu et al . , 2021b ;   Lamm et al . , 2021 ; Aggarwal et al . , 2021 ) .   While language models are able to produce com-   pelling answers ( Zoph et al . , 2022 ) or explanations   ( Jansen et al . , 2021 ) to science questions , are they   simply retrieving ( or shallowly assembling ) these   answers , or can they understand and use the knowl-   edge they output in a meaningful way ? Also , how   can we evaluate if a model ’s explanation is correct ?   In this work we explore these two questions by   reframing science exam question answering into   an interactive task where agents must complete   elementary science experiments in a simulated text-   based environment called S W ( see   Table 1 ) . Instead of simply answering a ques-   tion ( e.g. , Q : “ What will happen to an ice cube   when placed on a stove ? ” , A : “ it will melt ” ) , the   agent must demonstrate its capacity to combine   declarative scientific knowledge with the procedu-   ral knowledge required to correctly complete the   experiment in the virtual environment . Similarly ,   the sequence of virtual actions an agent performs   can serve as a form of procedural ( “ how ” ) expla-   nation to the question , that can be directly evalu-   ated in the virtual environment for correctness ( e.g. ,   whether the actions led to the ice cube melting ) .   The contributions of this work are :   1.We construct S W , a complex in-   teractive text environment , with simulation en-   gines for thermodynamics , electrical circuits ,   chemistry reactions , and biological processes .   2.We implement 30 benchmark tasks across 10   topics spanning the elementary science cur-   riculum , including changes of state of matter   and the role of pollinators when growing fruit .   3.We evaluate 5 state - of - the - art reinforcement   learning and language model agents on this   benchmark , empirically showing that they per-   form poorly on tasks ( e.g. , melting ice ) that   5grade students can perform with ease .   2 Related Work   Science - domain Inference : Standardized science   exams are a challenging task for question answer-   ing due to their diverse knowledge and inference   requirements ( Clark et al . , 2013 ; Jansen et al . , 2016 ;   Boratko et al . , 2018 ; Clark et al . , 2018 ) . Top per-   forming models can answer more than 90 % of   multiple - choice questions correctly ( Clark et al . ,   2020 ) , typically through the use of large language   models ( Khashabi et al . , 2020 ; Zoph et al . , 2022 ) .   A number of corpora of structured and semi-   structured science exam explanations exist for train-   ing the explanation - generation task ( Jansen et al . ,   2018 ; Xie et al . , 2020 ; Khot et al . , 2020 ; Dalvi   et al . , 2021 ) . Evaluating explanations is challeng-   ing , and typically done by comparing generated   explanations to a single gold explanation . This   has been shown to substantially underestimate ex-   planation generation performance by up to 40 %   ( Jansen et al . , 2021 ) . While others have worked   to mitigate this by generating multiple alternate   explanations for each question , this is expensive ,   and has only been demonstrated for adding one   or two additional explanations per question ( Inoue   et al . , 2020 ; Jhamtani and Clark , 2020 ) . Here , we   propose a partial solution to this evaluation prob-   lem by treating the action sequences of agents as   structured manner explanations for how to solve a   task . These action sequences can be directly run   in the S W simulator to automatically   determine their correctness ( i.e. , whether they ac-   complish the task ) , independent of any variations   in their solution methods . For example , whether an   agent melts ice by using a stove , building a camp-   fire , or leaving the ice out on a kitchen counter ,   the end result is the same and directly measurable   through the formal semantics of the simulator .   Environments : Interactive text environments are   becoming a vehicle for research in natural lan-   guage processing ( see Jansen ( 2021 ) for a review ) ,   primarily because of their reduced development   costs relative to 3D environments , combined with   their ability to easily implement high - level tasks   with large action spaces . In spite of these bene-   fits , implementation costs can still be substantial   for large complex environments , and text agents   are frequently evaluated on a small set of exist-11280ing interactive fiction games such as Zork ( Lebling   et al . , 1979 ) using an unified interface like Jeri-   cho ( Hausknecht et al . , 2020 ) . A few purpose - built   environments provide simple tasks for studying   text agents , typically on procedurally generated   pick - and - place or object - combination tasks ( e.g. ,   cooking , Yin and May , 2019 ) . Kitchen Cleanup   ( Murugesan et al . , 2020b ) and TextWorld Common   Sense ( Murugesan et al . , 2020a ) require agents to   tidy up one or more rooms in a house environment   by putting objects in their typical locations ( e.g. , a   hatshould be placed on the hat rack ) , testing an   agent ’s declarative knowledge of common object   locations with the procedural knowledge required   for this pick - and - place task . The closest existing   interactive text environment to S W   is TextLabs ( Tamari et al . , 2021 ) which simulates   chemistry wet - lab protocols with actions such as   pipetting andcentrifuging . Compared to these ex-   isting environments , S W is generally   a larger and more dynamic environment , populated   with more complex objects with greater depth of   physical simulation . This simulation depth enables   more complex tasks associated with elementary sci-   ence ( e.g. , thermodynamics , electrical circuits , etc . )   to be tested , and a greater variety of solutions .   Simulators : Nearly all text - based world simu-   lations are currently implemented as Z - machine   games ( Infocom , 1989 ; Nelson , 2014 ) , frequently   through higher - level application - specific languages   ( such as Inform7 , Nelson , 2006 ) that compile to   Z - machine code . TextWorld ( Côté et al . , 2018 ) cre-   ates environments using linear - logic statements that   specify action preconditions and postconditions   ( Martens , 2015 ) and generate Inform7 code . Exist-   ing tooling is designed for simpler simulations than   S W , with primarily agent - centered   state changes that make modeling autonomous   physical processes ( e.g. , thermodynamics ) difficult .   As such , in this work we build a novel simulator to   model physical processes in text environments .   Agents : A variety of agent models have been   proposed for reasoning in interactive text envi-   ronments . Most approaches frame reasoning as   a partially - observable Markov decision process   ( POMDP ) , and model inference using reinforce-   ment learning ( RL ) . This includes RL - based mod-   els that learn a policies to pick relevant actions   from lists of candidate actions ( He et al . , 2016 ) ,   or models that mix RL with knowledge graphs   ( Ammanabrolu and Hausknecht , 2020 ) or languagemodels ( Yao et al . , 2020 ) to aid in next - action se-   lection . Action selection has also been modelled   using case - based reasoning ( Atzeni et al . , 2021 ) , or   directly as a sequence - prediction imitation learning   task , using language models trained on gold action   sequences to predict the next action given the cur-   rent state ( Torabi et al . , 2018 ; Ammanabrolu et al . ,   2021 , 2022 ) . In general , agent performance on   solving interactive fictions is still modest , with only   easier games close to completion ( Jansen , 2021 ) .   In this work , we benchmark state - of - the - art   agents on S W as well as introduce   novel agents . We empirically show that these el-   ementary science tasks are difficult for current   agents , and also that smaller and simpler agents   can outperform billion - scale parameter language   models trained on gold sequences , highlighting the   difficulty of this task for transformer - based models .   3 S W   S W is a simulation of the world ab-   stracted through a complex interactive text environ-   ment in English with many objects , actions , and   simulation engines . The framework consists of 40k   lines of S ( speed ) with a P interface .   TheS W environment contains 10   interconnected locations ( see Figure 1 ) , populated   with up to 200 types of objects , including devices ,   instruments , plants / animals , electrical components ,   substances , containers , and common environment   objects such as furniture , books , and paintings .   The S W action space contains 25   high - level actions , including both science - domain   actions ( e.g. , using ) and common   actions ( e.g. , moving , opening containers , pick-   ing up items ) , with approximately 200k possible   action - object combinations per step ( though only   a limited subset of these will be meaningful ) . See   Appendix A for details about S W , in-   cluding the object model , actions , and input parser .   3.1 Simulation Engines   S W supports actions commonly   found in interactive text environments – for ex-   ample , objects can be moved or examined , foods   can be eaten , and books can be read . In addition ,   the environment contains a number of elementary   science - domain specific processes that either occur   automatically ( e.g. , thermodynamics ) or are cou-   pled to actions ( e.g. , devices , mixing chemicals).11281Those simulation enginesare :   Thermodynamics : All objects have temperatures   and other thermal properties based on their materi-   als . All objects within a container are considered in   thermal contact with each other , and transfer heat   energy using a simplified conductive heat model .   The proportion of heat transferred between objects   at each step is mediated by the object ’s thermal   conduction coefficient , allowing thermal conduc-   tors ( like metal pots ) and insulators ( like ceramics )   to be modelled . Every material has phase transition   points ( i.e. , melting point , boiling point ) andcom-   bustion points populated based on the best - known   or approximate physical values for those materials .   Objects that move past these thresholds will change   state of matter ( e.g. , from a solid to a liquid ) , or be-   gin a combustion process that ultimately ends in the   object turning to ash unless its fire is put out . Con-   vective heat transfer is also modelled in the form   of heat sources ( e.g. , , ) and heat sinks   ( e.g. , , ) that transfer heat energy to   or from objects . Rooms also transfer ambient heat   energy to / from the objects they contain .   Electricity : The simulator models simple series   electrical circuits , where electrically - powered de-   vices ( e.g. , , ) can be powered by   being connected to electrical energy sources ( e.g. , , ) through electrical conduc-   tors ( nominally , ) . Polarized and unpolarized   components are modelled , with each object having   exactly two terminals ( anode and cathode for polar-   ized ; terminals 1 and 2 for unpolarized ) . Connec-   tion happens through explicit terminal - to - terminal   connection actions ( e.g. , ) . Every non - electrical   object in S W has virtual unpolarized   terminals , allowing circuits to be build with valid   electrical conductors ( e.g. , using a in   place of a ) , and for the agent to build circuits   that test conductivity by ( for example ) observing if   a light bulb illuminates when a plastic versus metal   fork is used in the circuit .   Devices : Many objects are also considered devices ,   that can be activated or deactivated by the agent   ( e.g. , , ) , or may have   environment - specific conditions to being activated(e.g . , a will only activate if it is prop-   erly electrically connected ; a will   only produce power if it is outside ) . Objects can   also be used with other objects in specific contexts   ( e.g. , a , to measure an object ’s tem-   perature ; a , to dig soil from the ground ) .   Chemistry : A subset of specific chemical reac-   tions are modelled , where mixing a set of sub-   stances together in a container will produce a re-   sultant substance ( e.g. , and mix to pro-   duce ) . Common chemical reactions   taught in elementary science ( e.g. , water reactions ,   rust , food reactions , paint mixing ) are modelled .   Life Stages : Living things ( plants and animals )   progress through life stages ( e.g. , seed , seedling ,   juvenile plant , adult plant , reproducing plant , dead   plant ) . Progression through life stages happens   over time by continuing to meet the needs of that   living thing ( e.g. , , ) . If the needs are   not met ( e.g. , a plant is not watered , is removed   from soil , or becomes too hot ) , then it dies .   Reproduction and Genetics : Living things can   have genes that express traits ( e.g. , flower colour ,   seed shape , leaf size ) . Genes are inherited from   the alleles of both parents , and genotype is deter-   mined at the time of reproduction using a Punnett   square . Phenotype ( expressed , visible traits ) are   determined based on which genes are dominant   versus recessive . Currently , traits are only popu-   lated for selected plants to reproduce Mendelian-   genetics experiments . Plants reproduce by exchang-   ing pollen ( containing their genes ) between flowers ,   typically by a pollinator ( such as a ) . Pollinated   flowers eventually wilt and turn into fruits contain-   ing seeds of genetic descendants .   Friction ( Inclined Plane ): Forces are a significant   part of an elementary science curriculum , but dif-   ficult to incorporate without 2D or 3D simulation .   S W models the forces of gravity and   friction in the specific context of 1 - dimensional in-   clined plane experiments . Objects placed at the top   of an inclined plane will slide down the plane at a   speed proportional to the plane ’s angle , and the fric-   tion coefficient of its surface material . The position   is described to the agent ( e.g. , “ an inclined plant   with a block 60 % of the way down the plane ” ) , al-   lowing experiments to determine either the relative   angle or friction coefficients of different inclined   planes based on the speed the object moves down   a given plane.11282Containers : Containers can be always open ( e.g. ,   a ) or closeable ( e.g. , a ) . Ob-   jects contained inside containers are not visible   until the container is open . Some effects spread be-   yond a container – for example , a wooden cupboard   with a hot object inside may combust , causing other   objects in the kitchen to also increase temperature .   4 Experiments   To understand how contemporary approaches to   text agents perform at S W tasks , we   benchmark a selection of recent architectures .   Tasks . To support our goal of generating a di-   verse set of tasks , we identified a candidate set   of 10 broad science exam topics from the list of   400 fine - grained science curriculum topics of Xu   et al . ( 2020 ) . Topics were chosen that would be   amenable to text - based simulation , and that did not   have critical fine - grained spatial reasoning require-   ments , and include : changes of state , temperature   measurement , electrical circuits , friction , object   classification , chemical mixtures , plants and polli-   nators , life spans , life stages , and Mendelian genet-   ics . Each topic was further divided into between   2 and 4 specific tasks for agents to perform , pro-   ducing a total of 30 science - domain tasks . These   topics and tasks are described in Appendix B.2 .   To prevent overfitting and encourage generaliza-   tion , each subtask contains between 10 and 1400   parametric variations ( with 7200 total variations   across all 30 subtasks ) . Variations change criti-   cal task objects ( e.g. , the specific substance to be   melted ) , the agent ’s starting location in the environ-   ment , as well as randomly vary the contents of the   environment itself ( e.g. , whether the living room   contains a bookshelf , or a painting , or both ) .   Train , Development , Test sets : For a given sub-   task , variations are split into 50 % training , 25 %   development , and 25 % test sets . Variations are   sorted such that critical unseen variations ( e.g. , sub-   stances , animals , or plants unseen during training )   are found in development and test sets .   Goals and Rewards . To reduce reward sparsity ,   each task includes between 2 and 15 optional sub-   goals ( such as turning on the stove , orthe substance   increasing in temperature by 10C ) that help nudge   agents in the direction of canonical solutions , if   desired . Meeting required and optional subgoals in-   creases the agent ’s score on a given subtask . Scores   for all tasks are normalized to between 0 and 1.Oracle Agents To support imitation learning , we   provide gold trajectories from 30 hand - coded ora-   cles on all subtasks and variations . For tractability   these solutions represent canonical solution meth-   ods ( e.g. , using a stove to boil water ) , rather than   all possible solution methods that lead to the goal   state ( e.g. , building a campfire to boil water ) .   Learning Agents . An interactive text envi-   ronment can be cast as a partially observable   Markov decision process ( POMDP ) defined by   ⟨S , T , A , R , O , Ω , γ⟩representing the set of pos-   sible states ( S ) , conditional transition probabili-   ties between states ( T ) , available text commands   ( A ) , reward function ( R∶S×A→R ) , set of   possible text observations ( O ) , observation condi-   tional probabilities ( Ω∶S→O ) , and discount   factor ( γ∈[0,1 ] ) . The goal for a learning agent   is then to learn a policy π(o)→athat chooses   or generates a text command a∈Agiven the   text observation o∈Ω(s)of state s∈Sthat   maximizes the expected discounted sum of re-   wards E[∑γR(s , a ) ] . In S W ,   the agent is also provided with a task description d.   To provide a fair comparison , all models were   run using identical experiment configurations when   possible . Reinforcement learning models were run   with identical training regiments ( 8 environment   threads at 100k steps per thread ) . Training episodes   reset after meeting an end state ( success or failure ) ,   or after reaching a maximum number of steps ( we   used 100 in all experiments ) . Additional model   details are provided in Appendix C.   Random Baseline : At each time step t , this base-   line randomly chooses an action afrom the set of   valid actions Aobtained from the simulator .   DRRN ( He et al . , 2016 ): The Deep Reinforcement   Relevance Network ( DRRN ) learns separate rep-   resentations of the observation space and action   space of an environment , then trains a policy that   selects from Athe action that is the most rele-   vant given the current text observation o(which   also includes the description of the current room   oand the current agent inventory o ) and that   would lead to an increased reward . The DRRN is   a strong baseline with near state - of - the - art perfor-   mance on many medium - to - hard interactive text   environments ( Hausknecht et al . , 2020 ) .   KG - A2C ( Ammanabrolu and Hausknecht , 2020 ):   This model represents the state space with a knowl-   edge graph built dynamically from the text observa-   tions ousing OpenIE triples ( Angeli et al . , 2015)11283such as ( , , ) , while   the action space is represented using action tem-   plates with placeholders ( e.g. , ) obtained   from S W . The model learns a policy   that selects relevant action templates then populates   them with objects from the knowledge graph .   CALM ( GPT2 ) ( Yao et al . , 2020 ): We collect   transcripts of expert demonstrations for the train   variations of the tasks using the oracle agents , then   use them to fine - tune a pre - trained language model   ( GPT-2 , Radford et al . ( 2019 ) ) . At runtime , the   language model is provided with the current ob-   servation o , last observation o , and last action   a , then generates a shortlist of 30 possible ac-   tions to take . This shortlist serves as input to an   RL model similar to the DRRN , which re - ranks the   actions and chooses the next action to perform .   Behavior Cloning ( Torabi et al . , 2018 ): We follow   the methodology of Ammanabrolu et al . ( 2021 ) in   adapting the popular imitation learning method of   behavior cloning from observations to text agents .   We used the same transcripts of demonstrations as   the CALM ( GPT2 ) agent to extract 211,092 train-   ing examples with ( d , o , a , o)as inputs and   aas targets . We fine - tune a transformer - based   text - to - text model with a T5 architecture ( Raf-   fel et al . , 2020 ) initialized with the weights of a   Macaw ( Tafjord and Clark , 2021 ) model designed   to answer science questions .   At test time , the agent performs zero - shot infer-   ence online in the simulator by generating a fixed   number of actions with beam search on the unseen   test variations for each task . Despite training on   a large number of demonstrations , the generated   actions are often invalid or not useful — resulting in   zero scores . Thus , we treat the beam search ’s out-   put as a ranked - list and run the highest - ranked ac-   tion appearing in A , similar to the language - model-   to - valid - action aligner of Huang et al . ( 2022 ) .   Text Decision Transformer : Inspired by the De-   cision Transformer ( Chen et al . , 2021 ) , we cre-   ate a novel text game agent that models the entire   POMDP trajectory as a sequence and has the ability   to potentially predict actions that maximize future   long term expected reward . We again used the   same transcripts of demonstrations as the two pre-   vious agents to extract 224,902 training examples   with(d , o,ˆR , a , o,ˆR)as inputs and a   as targets . Here ˆRis the returns - to - go ( i.e. , sum   of future rewards ) ˆR=∑rwhere ris thefuture reward obtained by the expert at step t —   enabling models to predict actions that maximize   future expected rewards . The architecture , pre-   training , parameter sizes , and test inference are   otherwise similar to the behavior cloning agent .   Both the Behavior Cloning and Text Decision   Transformer agents learn to perform S -   W tasks offline from demonstrations once   pre - trained for scientific QA . They use the prevail-   ing paradigm for achieving state - of - the - art on many   language benchmarks ( e.g. , QA ( Khashabi et al . ,   2020 ; Tafjord and Clark , 2021 ) , language under-   standing ( Raffel et al . , 2020 ; Brown et al . , 2020 ) ) .   Results . Performance for all agents across each   S W task is shown in Table 2 . Over-   all , these tasks are challenging for current models ,   with the best model ( DRRN ) achieving an aver-   age score of 0.17 across all 30 subtasks . Models   that rely on the valid action detection aid gener-   ally perform better than those that learn to generate   valid actions in addition to learning what actions   to pick to increase task performance . All models   relying on large language model for action selec-   tion ( CALM , BC - T5 , TDT - T5 ) generally achieve   low performance as they tend to generate few valid   actions in their candidate action lists .   Figure 2 shows episode reward curves for four   selected tasks ( with reward curves for all tasks in-   cluded in Appendix C ) . These four tasks include   the current best - performing task ( Task 4 - 2 : Find a   non - living thing ) , which requires an agent to focus   on any non - living thing in the environment , pick it   up , and place it in a specific container ( typically in a   different location than the agent ’s starting location ) .   Most RL models quickly solve the majority of this   task , but struggle with picking up and moving the   object to the final container . In contrast , other more   open - ended tasks ( such as Task 1 - 4 , which requires   agents to perform any state - of - matter change to   a specific substance ) are performed poorly by all   models . Finally , S W includes pairs   of identical tasks where one can be solved by re-   trieving some critical component of the answer ,   while the other requires conducting the experimen-   tal procedure successfully . For example , in Task   3 - 3 , an agent could look up that a is an   electrical conductor and solve the task with com-   paratively fewer steps then in its paired Task 3 - 4 ,   where the substance name is randomly generated   ( e.g. , ) and the experiment   must be completed to get the answer . We do not yet11284   observe this behavior with the agents under exami-   nation . They generally struggle with commonsense   level tasks ( e.g. , navigation ) and are unable to reach   a point where the language models ( either GPT-2   in CALM , or T5 initialized with Macaw in BC and   TDT ) are able to leverage their internal knowledge   to solve these tasks through retrieval.5 Discussion   Elementary science tasks are challenging for   text agents . With top - performing agents reach-   ing normalized average scores of 0.17 across tasks ,   performance on S W is comparable   to the current best - performing agents on medium-   difficulty interactive fiction games such as Zork11285(Ammanabrolu et al . , 2020 ; Yao et al . , 2021 ) . Much   as in interactive fiction games , examining agent tra-   jectories reveals that while agents appear to strug-   gle with science - domain inference procedures such   ashow to heat a substance orhow to grow a seed ,   they also currently lack a fluency with common-   sense skills such as navigating the environment or   storing liquids in containers . This underscores the   need for models that can incorporate commonsense   and science - domain knowledge , and integrate that   declarative knowledge into actionable procedures   to progress towards goals in the environment .   Larger models are not necessarily better . While   larger models generally perform better in question   answering tasks ( e.g. , Raffel et al . , 2020 ) , here   we observe that larger models do not always in-   crease performance . Our best - performing model ,   the DRRN , has only 1.5 million parameters – four   orders of magnitude less than the T5 models . Both   models also receive the same number of gradient   updates ( 10 ) with respect to S W   training tasks — though the T5 models have the   added benefit of pre - training both from science   exam QA and a large number of expert demon-   strations . This underscores that how a model   approaches modeling state spaces and action se-   quences may be more important than the scope of   its pre - training . Online , interactive training enables   models such as the DRRN and KG - A2C to per-   form tasks requiring long action sequences more   efficiently in terms of both samples and parameters .   Limitations of agents and environments . While   agents still find text environments challenging , it   is important to recognize that even this modest   performance is achieved through a number of sim-   plifying properties . For example , because agents   frequently generate plausible but invalid actions , all   but two agents benchmarked here depend on S- W ’s valid action detection aid at test   time , substantially simplifying their search prob-   lem in the action space . Similarly , while S -   W achieves a high environment fidelity for   a text simulation , this is still tempered by prag-   matic concerns , such as generating comparatively   short descriptions of environments that can fit into   the sequence lengths of most transformer models .   As such , even environments with complex physi-   cal , chemical , and biological processes underlyingtheir simulations ( such as S W ) still   ultimately must limit the vividness of their descrip-   tions , until these technical limitations in modelling   can be surpassed . Hybrid environments ( e.g. , Shrid-   har et al . , 2020 ) that concurrently model the same   environment as both a high - fidelity 3D world and   comparatively low - fidelity text - based simulation   have shown that text environments can be used to   provide useful task pre - training that can transfer   back to the 3D environment with relatively low   simulation compute cost .   Explanations as action sequences . Explanations   take on a variety of roles ( Lombrozo , 2006 ; Gilpin   et al . , 2018 ) , from detailed human - readable descrip-   tions of classification processes that describe how   a decision was made ( e.g. , Ribeiro et al . , 2016 ) , to   higher - level appeals to scientific processes to ex-   plain why an answer is correct ( e.g. , Jansen et al . ,   2018 ; Dalvi et al . , 2021 ) . Here , the action pro-   cedures generated by agents act as manner expla-   nations for how to solve a particular task – but   while they describe how to accomplish something ,   they do n’t explain at a high - level why those ac-   tions accomplish the task . For example , action   sequences lack high - level goal information such as   “ melting a substance requires heating it , so first the   agent needs to heat the substance with a heating   device , like a stove . ” . Similar to how cuing agents   to answer contextual questions can help improve   their task performance ( Peng et al . , 2021 ) , cuing   agents to generate these explanatory scaffolds may   help future agents increase task performance , while   structuring their action sequence explanations for   better human interpretability .   6 Conclusion   Despite recent progress in both interactive text   agents and scientific text processing via transform-   ers , current models are unable to reason about   fundamental science concepts in a grounded and   reusable manner — calling into question how much   they are actually understanding the tasks at hand .   To better measure such scientific reasoning abilities ,   we introduce S W , an interactive text   environment derived from an elementary school   science curriculum — with tasks ranging from elec-   trical conductivity to Mendelian genetics .   We evaluate three state - of - the - art reinforcement   learning text game agents : DRRN , KG - A2C ,   and CALM ; and further introduce two large - scale   transformer - based agents inspired by recent ad-11286vances such as Behavior Cloning and the Decision   Transformer and trained for scientific reasoning   inS W . While we find that overall   performance on unseen tasks that require using   science - domain knowledge is low across all agents ,   our results also suggest that agents that learn in-   teractively in a grounded environment are more   sample and parameter efficient than large language   models that learn offline by reading text from static   sources . The best agent performance is still modest   — and on - par with medium - difficulty interactive fic-   tion environments such as Zork — highlighting the   need for agents that can integrate declarative scien-   tific and world knowledge with procedural action   sequences in virtual environments .   7 Broader Impacts   Interactive text environments can provide a faster   and cheaper alternative to 3D environments to   teach agents how to plan via sequential deci-   sion making . They allow better control over the   level of abstraction desired to approach a task   ( i.e. , , vs. , , , , ) . We be-   lieve making a plan in this abstract language space   is simpler and more interpretable .   With respect to risks , we consider this current   work as exploratory only . ScienceWorld is primar-   ily intended for training agents to learn reasoning   capabilities in the science domain , with limited im-   mediate utility to human science students . Agents   trained on ScienceWorld should not be used to pro-   vide advice for the real world . The environment   in ScienceWorld has been made safer compared to   the real world . For instance , the agent ca n’t acci-   dentally burn itself while boiling a substance on   a campfire , and its actions should not be taken as   demonstrations of safe procedures for students .   Acknowledgements   This work supported in part by National Science   Foundation ( NSF ) award # 1815948 to PJ , Google   Cloud Compute , and the Allen Institute for AI . The   authors would also like to thank Liwei Jiang , Jack   Hessel , and Oyvind Tafjord for their very timely   technical assistance and advice , giving us the abil-   ity to train our larger , transformer - based agents .   We ’d also like to thank Michal Guerquin for tech-   nical assistance with the project website . References11287112881128911290A S W Description   S W is a simulation of the world ab-   stracted through a complex interactive text envi-   ronment with many objects ( Sec . A.2 ) , actions   ( Sec . A.3 ) , and simulation engines ( Sec . A.4 ) . The   object - oriented ( Sec . A.1 ) simulator is written in   S and offers a P interface to interact   with it . S W ’s flexibility makes it sim-   ple to create new science - domain tasks ( Sec . B.2 )   and to evaluate the correctness of agents ’ solutions   ( App . B.4 ) .   A.1 Object Model   Objects in S W are represented using   an object - oriented model and are implemented as   classes . S W objects can be thought   of as collections of sets of properties ( e.g. , material   properties , life properties , device properties , etc . ) .   All objects implement common functions , such as   those that produce textual descriptions of the ob-   ject , or that provide one or more possible referents   for the object based on its current state ( e.g. , the in the solid state could generate   the referents , , and ,   each of which could be used by the agent to refer   to that object in an action ) . Similar to Z - machine   games ( Infocom , 1989 ) , objects are stored in an ob-   ject tree representing the object ’s current container   ( its immediate parent object in the tree ) , and any   objects it contains ( child nodes in the tree ) .   A.2 Environment and Objects   S W is composed of a map of 10 lo-   cations centered around a house theme ( , , , , , , etc . ) , as shown in Figure 1 . While the   rooms and how they interconnect is static , the envi-   ronment is randomly populated with different com-   binations of relevant contextual items each time it   is initialized – for example , in one run , the living   room may have a with three . In   other runs , the bookcase may have different books ,   no books , or not be present in the environment .   This parametric variation discourages agents from   memorizing the specific environment , and encour-   ages robustness in task performance .   The environment is populated with up to 195   specific types of objects ( excluding variations of   those objects that change names or task proper-   ties , i.e. , and belong to   the same object type ) . This includes 23 animals , 11   plants , 25 substances , 10 canonical liquid contain-   ers ( like or ) , 13 electrical   components ( such as , , ,   and ) , 16 devices ( including a , , and ) , and 15 common   pieces of furniture . To support these objects , the   simulator includes a variety of other properties , in-   cluding ( for example ) plant / animal life cycles , and   80 material properties ( including water , glass , iron ,   and wood ) that pure substances or physical objects   ( e.g. , ) can be made from .   A.3 Action Space   The simulator implements 25 actions , shown in   Table 3 , including generic actions common in in-   teractive text environments ( e.g. , opening a door ,   moving to a location ) , as well as science - domain   specific actions ( e.g. , connecting electrical compo-   nents , chemically mixing items , pouring liquids ) .   Five actions take two arguments , 16 take one argu-   ment , and four actions take zero arguments . Given   the approximately 200 possible objects ( exclud-   ing parametric variations ) in S W , the   action space can naively be estimated to be approx-   imately 200,000 possible unique action possibil-   ities at each step , though only a small subset of11291these would be meaningful . Similar to the Jericho   framework , the simulator can provide valid action   detection as an aid to agents ( such as the DRRN )   that require selecting their next action from a list   of possible known - valid actions at a given step .   Input Parser At each step , an input parser at-   tempts to parse user or agent input into a single   unique action . Actions are specified as templates   that can take on a variety of surface forms ( e.g. , or ) , and that   include placeholders for object referents . At run-   time , the parser examines all valid referents for   visible objects from the agent ’s point of view , and   if a given input string can produce more than one   valid action , the parser will ask for clarification .   A.4 Simulation Engines   S W supports actions commonly   found in interactive text environments – for ex-   ample , objects can be moved or examined , foods   can be eaten , and books can be read . In addition ,   the environment contains a number of elementary   science - domain specific processes that either occur   automatically ( e.g. , thermodynamics ) or are cou-   pled to actions ( e.g. , using devices , mixing chemi-   cals ) . Those simulation engines are :   Thermodynamics : All objects have temperatures   and other thermal properties based on their materi-   als . All objects within a container are considered in   thermal contact with each other , and transfer heat   energy using a simplified conductive heat model .   The proportion of heat transferred between objects   at each step is mediated by the object ’s thermal   conduction coefficient , allowing thermal conduc-   tors ( like metal pots ) and insulators ( like ceramics )   to be modelled . Every material has phase transition   points ( i.e. , melting point , boiling point ) andcom-   bustion points populated based on the best - known   or approximate physical values for those materials .   Objects that move past these thresholds will change   state of matter ( e.g. , from a solid to a liquid ) , or be - gin a combustion process that ultimately ends in the   object turning to ash unless its fire is put out . Con-   vective heat transfer is also modelled in the form   of heat sources ( e.g. , , ) and heat sinks   ( e.g. , , ) that transfer heat energy to   or from objects . Rooms also transfer ambient heat   energy to / from the objects they contain .   Electricity : The simulator models simple series   electrical circuits , where electrically - powered de-   vices ( e.g. , , ) can be powered by   being connected to electrical energy sources ( e.g. , , ) through electrical conduc-   tors ( nominally , ) . Polarized and unpolar-   ized components are modelled , with each object   having exactly two terminals ( either an anode and   cathode for polarized components , or terminals 1   and 2 for unpolarized components ) . Connection   happens through explicit terminal - to - terminal con-   nection actions ( e.g. , ) . Every non - electrical ob-   ject in S W has virtual unpolarized   terminals , allowing circuits to be build with valid   electrical conductors ( e.g. , using a in   place of a ) , and for the agent to build circuits   that test conductivity by ( for example ) observing if   a light bulb illuminates when a plastic versus metal   fork is used in the circuit .   Devices : Many objects are also considered devices ,   that can be activated or deactivated by the agent   ( e.g. , , ) , or may have   environment - specific conditions to being activated   ( e.g. , a will only activate if it is prop-   erly electrically connected ; a will   only produce power if it is outside ) . Objects can   also be used with other objects in specific contexts   ( e.g. , a , to measure an object ’s tem-   perature ; a , to dig soil from the ground ) .   Chemistry : A subset of specific chemical reac-   tions are modelled , where mixing a set of sub-   stances together in a container will produce a re-   sultant substance ( e.g. , and mix to pro-   duce ) . Common chemical reactions   described in elementary science questions ( e.g. ,   water reactions , rust , food reactions , paint mixing )   are modelled .   Life Stages : Living things ( plants and animals )   progress through life stages ( e.g. , seed , seedling ,   juvenile plant , adult plant , reproducing plant , dead   plant ) . Progression through life stages happens   over time by continuing to meet the needs of that11292living thing ( e.g. , , ) . If the needs are   not met ( e.g. , a plant is not watered , is removed   from soil , or becomes too hot ) , then it dies .   Reproduction and Genetics : Living things can   have genes that express traits ( e.g. , flower colour ,   seed shape , leaf size ) . Genes are inherited from   the alleles of both parents , and genotype is deter-   mined at the time of reproduction using a Punnett   square . Phenotype ( expressed , visible traits ) are   determined based on which genes are dominant   versus recessive . Currently , traits are only popu-   lated for selected plants to reproduce Mendelian-   genetics experiments . Plants reproduce by exchang-   ing pollen ( containing their genes ) between flowers ,   typically by a pollinator ( such as a ) . Pollinated   flowers eventually wilt and turn into fruits contain-   ing seeds of genetic descendants .   Friction ( Inclined Plane ): Forces are a significant   part of an elementary science curriculum , but dif-   ficult to incorporate without 2D or 3D simulation .   S W models the forces of gravity and   friction in the specific context of 1 - dimensional in-   clined plane experiments . Objects placed at the top   of an inclined plane will slide down the plane at a   speed proportional to the plane ’s angle , and the fric-   tion coefficient of its surface material . The position   is described to the agent ( e.g. , “ an inclined plant   with a block 60 % of the way down the plane ” ) , al-   lowing experiments to determine either the relative   angle or friction coefficients of different inclined   planes based on the speed the object moves down   a given plane .   Containers : Containers can be always open ( e.g. ,   a ) or closeable ( e.g. , a ) . Ob-   jects contained inside containers are not visible   until the container is open . Some effects spread be-   yond a container – for example , a wooden cupboard   with a hot object inside may combust , causing other   objects in the kitchen to also increase temperature .   B Tasks and Competencies   To support our goal of generating a diverse set of   tasks , we identified a candidate set of 10 broad sci-   ence exam topics from the list of 400 fine - grained   science curriculum topics of Xu et al . ( 2020 ) . Top-   ics were chosen that would be amenable to text-   based simulation , and that did not have critical   fine - grained spatial reasoning requirements . These   topics and tasks are described in Section B.2.Subtasks and Masked Objects : Each of the 10   broad curriculum topics is further subdivided into   between 2 and 4 specific subtasks that test specific   reasoning capacities ( e.g. , melting , boiling , and   freezing subtasks for the change - of - state task ) , or   ask the agent to perform the same task but with   names of critical task objects masked . Some tasks   are possible to partially solve by looking up crit-   ical task information ( e.g. , knowing that are a dominant trait of pea plants for   the Mendelian genetics task ) . We include two   versions of tasks , one with using masked names   ( e.g. , growing instead of a ) while simultaneously randomly generating   the properties of those objects to provide an instru-   ment to measure when agents are solving tasks by   performing the experimental procedure , and when   they are directly looking up answers .   Task Formats : Task goals are structured with the   broad goal of both ( a ) accomplishing a task , and   ( b ) doing so intentionally . Tasks typically include   preliminary subgoals where the agent must signal   their intent to perform the task on a specific object   by first “ focusing ” on the object they intend to per-   form the task with ( e.g. , for a boiling task , focusing   on water they intend to boil ) before they perform   the task .   Tasks take on two main forms : Perform a task :   the agent must directly perform a task , that pro-   duces some measurable change in the environ-   ment ( e.g. , growing a fruit through pollination ) that   can be directly measured as an end - state . Forced-   choice : The agent must perform a task that requires   making an inference ( e.g. , whether an object is   an electrical conductor or insulator ) , and provide   their answer by placing the task object in a spe-   cific container ( i.e. , an “ answer box ” ) if the object   is conductive , and a different container if it is an   insulator .   Task Variations : To prevent overfitting and en-   courage generalization , each subtask contains be-   tween 10 and 1400 parametric variations of that   subtask ( with 7200 total variations across all 30 sub-   tasks ) . Variations change critical task objects ( e.g. ,   the specific substance to be melted ) , the agent ’s   starting location in the environment , as well as ran-   domly vary the contents of the environment itself   ( e.g. , whether the living room contains a bookshelf ,   or a painting , or both ) .   Task Simplifications : Agents find different com-   petencies that S W tests to be chal-11293lenging . Tasks can be made easier by enabling   any of 5 environment simplifications ( or choosing   “ easy ” mode , which enables all the simplifications ) .   Examples of simplifications include a teleport ac-   tion that lets agents instantly move to any location ,   and having all containers open by default .   B.1 Scoring and Evaluation Protocol   Goals and Reward Shaping : Each subtask con-   tains a small number of method - agnostic required   goals to be met ( such as focusing on the substance   to melt , and causing that substance ’s state of matter   to change from a solid to a liquid for the melting   task ) . In addition , to make rewards less sparse   for agents learning these tasks , each task includes   between 2 and 15 optional subgoals ( such as turn-   ing on the stove , orthe substance increasing in   temperature by 10C ) that help nudge agents in the   direction of canonical solutions , if desired . Meet-   ing required and optional subgoals increases the   agent ’s score on a given subtask . Scores for all   tasks are normalized to between 0 and 1 .   B.2 Specific Tasks   Changes of State : The agent must find a named   substance ( e.g. , ) , and change the state of mat-   ter of that substance ( solid , liquid , gas ) using the   heating and cooling devices ( e.g. , , )   available in the environment . Subtasks require spe-   cific phase changes ( melting , boiling , freezing , or   the agent ’s choice ) . Variations change the sub-   stance , and ablate common devices ( e.g. , the stove   becomes disabled ) so that the agent must find alter-   nate methods of heating or cooling .   Measurement Instrument : The agent must find   a and use it to measure the tempera-   ture of a named object . In two additional subtasks ,   the agent must use the thermometer to measure the   melting point of a named substance by heating it   and continually monitoring the temperature . An-   swers are modelled as a forced - choice task , where   the agent is given a predetermined temperature   threshold at the start of the task ( e.g. , 50C ) , and   must focus on one answer box if the melting point   is above the threshold , and the other answer box   if the melting point is below the threshold . Varia-   tions change the substance to be measured , and the   preset temperature threshold .   Electrical Circuits : The agent must build a work-   ing series electrical circuit by connecting vari-   ous electrical components including power sources(e.g . , , , ) , dif-   ferent coloured , and active components ( e.g. , , ) . Subtasks include ( a ) powering a   named component , ( b ) powering using renewable   versus nonrenewable energy , and ( c ) measuring   whether named or unknown substances are elec-   trically conductive . Variations change available   components , colours of wire , and specific task ob-   jects required to be used in the circuit .   Classification : In four subtasks , the agent must   find an object in the environment belonging to a   specific category ( living things , non - living things ,   plants , or animals ) , and place it in an answer box .   Variations change the location and description of   the answer box .   Growing plants : The agent must grow a named   plant ( e.g. , a ) from seed . To do this ,   they must place the and in a ,   and provide regular water as the plant progresses   through life stages into adulthood . Failure to water   appropriately causes the plant to perish . In a sub-   task , the agent must grow a fruit by growing several   plants , then releasing pollinators ( i.e. , ) that   cross - pollinate flowers on the plants , which will   eventually produce fruits . Variations change seed   type , and soil location ( either provided in the pot ,   provided in the room , or must be gathered outside   using a shovel ) .   Chemistry : The agent must create a specific sub-   stance by mixing two or more input substances in   a container . In the generic subtask , a recipe docu-   ment that can be read by the agent is provided some-   where in the environment . In two paint - themed sub-   tasks , the agent is given primary colours of paint   ( red , green , yellow ) , and must mix secondary ( e.g. ,   orange ) or tertiary ( e.g. , orange - yellow ) colours   through several steps . Variations change the re-   quired output substance .   Life Spans : In three task variations , the agent   must find 3 animals in the environment , then select   either the shortest - lived ( e.g. , ) , longest - lived   ( e.g. , ) , or shortest - then - longest lived ( bee-   then - turtle ) . Variations change which animals are   populated in the environment from a list of long ,   medium , and short - lived animals .   Life Stages : The agent must find a named plant or   animal , and focus on its life stages , from earliest   ( e.g. , seed ) to latest ( e.g. , reproducing adult plant ) .   Variations change the plant or animal involved.11294Forces : The agent must use inclined planes and   masses ( e.g. , a ) for experiments about forces .   In one subtask the agent is given two planes , and   must determine which has the steepest or shallow-   est angle based on the time the block takes to move   down the plane . In two other subtasks , the agent   must find which of two planes has a surface of high-   est or least friction , from either named ( e.g. , plastic ,   steel ) or unknown surfaces . The agent can measure   time internally ( in terms of number of steps for   a block to fall ) , or measure this explicitly with a   provided stopwatch . Variations change inclined   plane angles ( first task ) or surface material types   ( remaining tasks ) .   Mendelian Genetics : The agent must determine   whether a named trait ( e.g. , white flower colour )   is a dominant or recessive trait in a plant . Two   seeds are provided ( one with the trait as domi-   nant , one recessive ) , and the agent must grow two   generations of plants and count how often it ob-   serves a given trait in successive generations to   determine whether it is dominant or recessive . Sub-   tasks change whether the plant is known ( the , as in Mendel ’s experiments ) or a randomly   generated plant , while variations change the trait   under investigation .   B.3 Commonsense Competencies   In addition to science - domain competencies , the   agent must demonstrate fluency with common-   sense knowledge and procedures to complete tasks   successfully . Agents must know the locations of   common objects ( e.g. , comes from a , is typically found in a ) , the   affordances of common objects ( a can be   turned on to create , acan be used to   carry a liquid ) , navigation ( a world is made of dis-   crete rooms that can be traversed through doors ) ,   containers need to be opened to observe or use their   contents , and so forth .   B.4 Scoring and Evaluation Protocol   Goals and Reward Shaping : Each subtask con-   tains a small number of method - agnostic required   goals to be met ( such as focusing on the substance   to melt , and causing that substance ’s state of matter   to change from a solid to a liquid for the melting   task ) . In addition , to make rewards less sparse   for agents learning these tasks , each task includes   between 2 and 15 optional subgoals ( such as turn-   ing on the stove , orthe substance increasing intemperature by 10C ) that help nudge agents in the   direction of canonical solutions , if desired . Meet-   ing required and optional subgoals increases the   agent ’s score on a given subtask . Scores for all   tasks are normalized to between 0 and 1 .   Train , Development , Test sets : For a given sub-   task , variations are split into 50 % training , 25 %   development , and 25 % test sets . Variations are   sorted such that critical unseen variations ( e.g. , sub-   stances , animals , or plants unseen during training )   are found in development and test sets .   B.5 Task Simplifications   Agents find different competencies that S -   W tests to be challenging . Tasks can be made   easier by enabling any of 5 environment simplifi-   cations ( or choosing “ easy ” mode , which enables   all the simplifications ) . Examples of simplifica-   tions include a teleport action that lets agents in-   stantly move to any location ; self - watering flower-   ing flower pots that mean plants do not have to be   frequently watered ; and having all containers open   by default .   C Experiment Details   C.1 Reinforcement Learning Models   For each reinforcement learning model , we ran   8 environment threads at 100k steps per thread .   Training episodes reset after meeting an end state   ( success or failure ) , or after reaching 100 steps . For   KG - A2C and CALM , the training episode will also   reset if stuck by invalid actions for 100 steps ( in-   valid actions are not counted by the environment ) .   We did evaluation on the test variations every 1000   steps per environment thread . We randomly chose   10 test variations and run 1 episode of testing for   each chosen variation during each evaluation pe-   riod and reported the average score of the 10 % test   steps scores .   DRRN : We use the DRRN architecture from , with   embedding size and hidden size set as 128 . The   learning rate we use to train DRRN is 0.0001 .   The memory size is 100k , and priority fraction is   0.50 . The tokenizer for the input text is a uni - gram   subword tokenizer model adapted from Kudo   ( 2018 ) .   KG - A2C : We make two major changes to the orig-   inal KG - A2C model to function with S -   W . ( 1 ) We replace the OpenIE knowledge11295graph extractor with a heuristic extractor . The   heuristic extractor uses regular expressions to parse   the text of the “ look around ” and “ agent inventory ”   information into ( subject , relation , object ) triples .   This heuristic functionally extracts the ground truth   knowledge graph representation of the observable   environment for the agent . ( 2 ) We change the KG-   A2C agent to generate object types instead of refer-   ences to specific objects . After selecting the action   template and object type fillers that the agent has   the highest confidence in , we ground those object   types ( e.g. apple ) with specific object referents in   the agent ’s current visible environment . If more   than one referent meets that type , one is chosen   at random . The learning rate we use to train the   KG - A2C agent is 0.003 and the tokenizer used is   the same as the DRRN agent .   CALM - GPT2 : Following the original CALM pa-   per ( Yao et al . , 2020 ) , we use a 12 - layer , 768-   hidden , 12 - head GPT-2 model . We use the de-   fault pre - trained weight offered by the Hugging-   face Transformers library ( Wolf et al . , 2020 ) . We   fine - tune this GPT-2 model on complete action se-   quences generated by the oracle agents . The GPT-2   input prompt is formed as “ [ CLS ] d[SEP ] o[SEP ]   o[SEP ] o[SEP ] o[SEP ] a[SEP ] ” and   targets to predict a , where dstands for the task de-   scription and o , o , o , andastand for the ob-   servation ( excluding the look around and inventory   information ) , the output of a “ look around ” action   at the agent ’s current location , the agent inventory ,   and the action at time step t. We use beam search   for generation , generating 16 beams representing   candidate actions for the agent to select from . We   set the diversity penalty to 50.0 to encourage the   GPT-2 model to generate different actions . For the   GPT-2 training we use a batch size of 12 and train   for 20 epochs with a learning rate of 0.00002 . The   learning rate we use to train the CALM agent is   0.0001 and the input tokenizer is the same as that   used in the original GPT-2 paper .   Due to the high computation cost of the CALM   model , and modest overall performance , perfor-   mance for each task is the average of only 3 ran-   dom seeds instead of the 5 used for training the   DRRN and KGA2C models . During development ,   a small error was found in the prompt . Pilot ex-   periments suggested this resulted in a negligible   ( ±0.01 ) change in performance , so the full experi-   ments ( requiring up to 6000 GPU hours ) were not   regenerated . Episode reward curves : Episode reward curves   for each model across all 30 subtasks in S -   W are shown in Figure 3 .   C.2 Behavior Cloning and Text Decision   Transformer   The T5 models used to train both of these models   are initialized with weights and tokenizers from   the trained Macaw-11b model released at . They are trained   on a v3 - 32 TPU pod with a batch size of 16 and   32 - way model parallelism for 100k gradient update   steps .   At inference time , we use the model to gener-   ate actions given the observation of current and   previous step . We use beam search with a beam   size 16 to get the top 16 generations . We set the   diversity penalty to 50.0 to encourage diversity in   generation . For each subtask , we run one episode   on each of its test variations and report the average   score . We do not update weights of the T5 model   during evaluation .   C.3 Resources1129611297C.4 Impact of Model Size and Pre - training   Methodology on Performance   To examine the effect of model size on behav-   ior cloned and decision transformer model perfor-   mance , we ran two model sizes for the T5 models ,   shown in Table 5 . We first observe that pre - training   specifically for scientific question answering on a   curated dataset ( Macaw ) outperforms T5 general   language model pre - training . Further , we note that   T5 - Large and Macaw - Large , with 14 times fewer   parameters ( 770 m each ) , out - perform the larger   11 billion parameter models by approximately a   factor of two . These results suggest that while S- W can benefit from external scientific   knowledge , it may also present an inverse scaling   problem , where increasing model size can some-   times decrease overall task performance . However ,   these results are only suggestive of an inverse scal-   ing problem rather than a concrete demonstration .   Due to the high cost of training and inference for   these models , we ca n’t currently rule out that these   differences may be due to hyperparameter differ-   ences . We leave this verification as future work .   C.5 Attribution   Graphical visualization makes use of RPG game   assets developed by @Noiracide.11298