  Liangke GuiBorui WangQiuyuan Huang   Alex HauptmannYonatan BiskJianfeng GaoCarnegie Mellon UniversityYale UniversityMicrosoft Research   { liangkeg , alex , ybisk}@cs.cmu.edu   borui.wang@yale.edu , { qihua , jfgao}@microsoft.com   Abstract   The primary focus of recent work with large-   scale transformers has been on optimizing the   amount of information packed into the model ’s   parameters . In this work , we ask a complemen-   tary question : Can multimodal transformers   leverage explicit knowledge in their reasoning ?   Existing , primarily unimodal , methods have   explored approaches under the paradigm of   knowledge retrieval followed by answer predic-   tion , but leave open questions about the qual-   ity and relevance of the retrieved knowledge   used , and how the reasoning processes over   implicit and explicit knowledge should be in-   tegrated . To address these challenges , we pro-   pose a - Knowledge Augmented Transformer   ( KAT ) - which achieves a strong state - of - the-   art result ( +6 % absolute ) on the open - domain   multimodal task of OK - VQA . Our approach   integrates implicit and explicit knowledge in   an encoder - decoder architecture , while still   jointly reasoning over both knowledge sources   during answer generation . Additionally , ex-   plicit knowledge integration improves inter-   pretability of model predictions in our analysis .   Code and pre - trained models are released at   https://github.com/guilk/KAT .   1 Introduction   There has been a revival of interest in knowledge-   intensive tasks which require an external knowl-   edge source for humans to perform . Many applica-   tions in real - world scenarios , such as autonomous   AI agents , need to seamlessly integrate implicit   ( i.e. , commonsense ) and explicit knowledge ( e.g. ,   Wikidata ) to answer questions . In this work , we   investigate how to effectively integrate implicit and   explicit knowledge for reasoning . Tasks like Out-   side Knowledge Visual Question Answering ( OK-   VQA ) ( Marino et al . , 2019 ) require that models use   knowledge not present in the input to answer ques - Figure 1 : Examples of knowledge - based VQA that re-   quires external knowledge . Success on this task requires   not only visual recognition , but also logical reasoning   to incorporate external knowledge about the world .   tions , making it an ideal test bed for investigating   this implicit - explicit knowledge trade - off .   Consider the examples from OK - VQA shown in   Figure 1 . To answer the question in the left exam-   ple , the system needs to both ground organism to   bird through explicit knowledge and then apply the   implicit knowledge birds evolved from reptiles to   answer the question . Similarly for the question in   the right example , the system needs to recognize   boats and harbor and requires the implicit knowl-   edge anchors are used to stop boats from moving .   A key challenge here is to accurately link image   content to abstract external knowledge . There have   been a number of recent developments demonstrat-   ing the feasibility of incorporating external knowl-   edge into Question Answering models ( Wang et al . ,   2017b ; Li et al . , 2020b ; Marino et al . , 2021 ; Wu   et al . , 2022 ; Garderes et al . , 2020 ) . Existing meth-   ods first retrieve external knowledge from external   knowledge resources , such as DBPedia ( Auer et al . ,   2007 ) and ConceptNet ( Liu and Singh , 2004 ) be-   fore jointly reasoning over the retrieved knowledge   and image content to predict an answer .   However , most existing approaches have several   drawbacks . First , explicit knowledge retrieved us-   ing keywords from questions or image tags may be956too generic , which leads noise or irrelevant knowl-   edge during knowledge reasoning . Second , exist-   ing work mainly focuses on explicit knowledge   which is often in the form of encyclopedia articles   or knowledge graphs . While this type of knowl-   edge can be useful , it is insufficient to answer many   knowledge - based questions . As shown in Figure 1 ,   questions require the system to jointly reason over   explicit and implicit knowledge , which is analo-   gous to the way humans do . To address these   challenges , we propose an approach , KAT , to ef-   fectively integrate implicit and explicit knowledge   during reasoning . The main contributions of our   work are as follows :   i ) Knowledge extraction . We adopt two novel   methods for knowledge extraction that significantly   improve the quality and relevance of extracted   knowledge : for implicit knowledge , we design   new prompts to extract both tentative answers and   supporting evidence from a frozen GPT-3 model ;   for explicit knowledge , we design a contrastive-   learning - based explicit knowledge retriever using   the CLIP model , where all the retrieved knowledge   are centered around visually - aligned entities .   ii ) Reasoning in an encoder - decoder trans-   former . We design a novel reasoning module   inKAT to perform jointly reasoning over explicit   and implicit knowledge during answer generation ,   which is trained by using an end - to - end encoder-   decoder transformer architecture .   iii ) OK - VQA performance . KAT sets a new state   of the art on the challenging OK - VQA ( Marino   et al . , 2019 ) benchmark , and significantly outper-   forms existing approaches .   2 Related Work   Vision - Language Transformer . Multimodal   transformers have made significant progress   over the past few years , by pre - trained on large-   scale image and text pairs , then finetuned on   downstream tasks . VisualBERT ( Li et al . , 2019 ) ,   Unicoder - VL ( Li et al . , 2020a ) , NICE ( Chen   et al . , 2021b ) , and VL - BERT ( Su et al . , 2020 )   propose the single - stream architecture to work   on both images and text . ViLBERT ( Lu et al . ,   2019 ) and LXMERT ( Tan and Bansal , 2019 )   propose a two - stream architecture to process   images and text independently and fused by a   third transformer in ta later stage . While these   models have shown to store in - depth cross - modal   knowledge and achieved impressive performanceon knowledge - based VQA ( Marino et al . , 2021 ;   Wu et al . , 2022 ; Luo et al . , 2021 ) , this type of   implicitly learned knowledge is not sufficient to   answer many knowledge - based questions ( Marino   et al . , 2021 ) . Another line of work for multimodal   transformers , such as CLIP ( Radford et al . , 2021 )   or ALIGN ( Jia et al . , 2021 ) , aligns visual and   language representations by contrastive learning .   These models achieve state - of - the - art performance   on image - text retrieval tasks . Different from   existing work that uses multimodal transformers as   implicit knowledge bases , we focus primarily on   how to associate images with external knowledge .   Importantly , our model only relies on multimodal   transformers learned by contrastive learning which   do not require any labeled images . This makes our   model more flexible in real - world scenarios .   Knowledge - based VQA . Some Knowledge-   based visual language tasks requires external   knowledge beyond the image to answer a ques-   tion . Early exploration , such as FVQA ( Wang   et al . , 2017a ) , creates a fact - based VQA dataset   by selecting a fact ( e.g. , < Cat , CapableOf , Climb-   ingTrees > ) from a fixed knowledge base . A recent   Outside Knowledge VQA ( OK - VQA ) dataset is a   more challenging dataset , covering a wide range of   knowledge categories . In our work , we focus on   OK - VQA due to its large - scale knowledge - based   questions as well as its open - ended nature .   Recent approaches have shown a great potential   to incorporate external knowledge for knowledge-   based VQA . Several methods explore aggregat-   ing the external knowledge either in the form   of structured knowledge graphs ( Garderes et al . ,   2020 ; Narasimhan et al . , 2018 ; Li et al . , 2020b ;   Wang et al . , 2017a , b ) , unstructured knowledge   bases ( Marino et al . , 2021 ; Wu et al . , 2022 ; Luo   et al . , 2021 ) , and neural - symbolic inference based   knowledge ( Chen et al . , 2020 ; West et al . , 2021 ) .   In these methods , object detectors ( Ren et al . , 2015 )   and scene classifiers ( He et al . , 2016 ) are used to   associate images with external knowledge . Fur-   ther , external APIs , such as Google ( Wu et al . ,   2022 ; Luo et al . , 2021 ) , Microsoft ( Chen et al . ,   2021a ; Yang et al . , 2022 ) and OCR ( Luo et al . ,   2021 ; Wu et al . , 2022 ) are used to enrich the asso-   ciated knowledge . Finally , pre - trained transformer-   based language models ( Chen et al . , 2021a ; Yang   et al . , 2022 ) , or multimodal models ( Wu et al . ,   2022 ; Luo et al . , 2021 ; Wu et al . , 2022 ; Garderes   et al . , 2020 ; Marino et al . , 2021 ) are leveraged as957implicit knowledge bases for answer predictions .   Different from previous approaches , Our work   aims to develop a single , unified architecture ,   by jointly reasoning over explicit and implicit   knowledge to augment generative language models .   While part of our approach is similar to PICa ( Yang   et al . , 2022 ) which considers GPT-3 as implicit   knowledge base , our model takes one step further   by showing that how explicit and implicit knowl-   edge can be integrated during knowledge reasoning .   Another similar work Vis - DPR ( Luo et al . , 2021 )   collects a knowledge corpus from training set by   Google Search which is specific to a certain dataset .   Our proposed model is more generic by collecting   entities from Wikidata and not limited to the train-   ing set .   Open - Domain Question Answering ( ODQA ) .   ODQA is the NLP task of answering general do-   main questions , in which the evidence is not given   as input to the system . Several approaches ( Chen   et al . , 2017 ; Karpukhin et al . , 2020 ) propose to   predict the answers by first retrieving support doc-   ument from Wikipedia , before extracting answers   from the retrieved document . Recent works ( Izac-   ard and Grave , 2020 ; Lewis et al . , 2020b ) combine   text retrieval models with language generative mod-   els which achieve state - of - the - art performance on   knowledge - intensive natural language processing   tasks . Similar to these works as part of our method ,   we extend this framework to VQA domain and   show the effectiveness of aggregating explicit and   implicit knowledge for knowledge - based VQA .   3 Method   3.1 Overview   When humans reason about the world , they process   multiple modalities and combine external and inter-   nal knowledge related to these inputs . Inspired by   this idea , we introduce a new KAT approach . The   overview of the proposed KAT model is shown in   Figure 2 . We define the knowledge from explicit   knowledge bases as the explicit knowledge , and   the knowledge stored in large - scale language mod-   els as the implicit knowledge ( i.e. , implicit com-   monsense knowledge ) . We describe the retrieval   method of our explicit knowledge ( § 3.2 ) and the   retrieval method of our implicit knowledge ( § 3.3 ) .   Next , we introduce the details of our knowledge   reasoning module which jointly reasons over both   explicit and implicit knowledge ( § 3.4).Problem Formulation . We apply our KAT on   OK - VQA task in this paper . Formally , given a   training dataset D={(v , q , a ) } , where v   denotes the itraining image ; sis the total num-   ber of the training images ; qandarepresent the   iquestion and its corresponding answer , respec-   tively . We use a sequence - to - sequence model that   is composed of an encoder and a decoder , which   is a comparison method of T5 ( Raffel et al . , 2020 )   or BART ( Lewis et al . , 2020a ) . Let θbe the pa-   rameters of the model pthat needs to be trained .   Unlike previous approaches that treat this task as   a classification problem ( Wu et al . , 2022 ; Marino   et al . , 2021 ) , our model is to take vandqas inputs   and generate the answer ain an auto - regressive   manner . It should be noted that our proposed model   tackles a more challenging problem . As the gen-   erated answer may contain an arbitrary number of   words from the entire vocabulary .   3.2 Explicit Knowledge Retrieval   3.2.1 Explicit Knowledge Extraction   Given an image vand corresponding question q ,   it is important to ground image regions with fine-   grained descriptions , which is conducive to under-   standing both the image content and the question   with the referred items . Existing approaches ( Rad-   ford et al . , 2021 ; Jia et al . , 2021 ) on OK - VQA apply   object detectors to generate image tags which are   used for explicit knowledge retrieval . Such image   tags can be generic and have a limited vocabulary   size , leading noise or irrelevant knowledge . Mo-   tivated by the recent progress of visual - semantic   matching approaches ( Radford et al . , 2021 ; Jia   et al . , 2021 ) , we leverage a contrastive - learning-   based model to associate image regions with exter-   nal knowledge bases .   Similar to the previous work ( Marino et al . , 2021 ;   Luo et al . , 2021 ) which uses a subset of exter-   nal knowledge , we construct an explicit knowl-   edge base that covers the 8 categories of animals ,   vehicles and other common objects from Wiki-   data ( Vrandecic and Krotzsch , 2014 ) . The details   can be found in Section 3.2.2 . We denote the con-   structed knowledge base as K. Each knowledge   entry efromKis a concatenation of the entity and   its corresponding description .   The goal of our explicit knowledge retriever is   to index all knowledge entries in d - dimensional   dense representations by a dense encoder E ( · ) ,   such that it can efficiently retrieve the top mknowl-958   edge entries relevant to each input image . Given   an image v , we use a sliding window with a stride   to generate Nimage regions { v , ... , v } . Then   an image encoder E(·)is applied to map each   patch to a d - dimensional dense representation , and   retrieves kknowledge entries from Kwhose rep-   resentations are closest to the patch - level represen-   tation . To define the similarity score between the   image region vand the entity e , we use the inner   product of their normalized representations :   sim(v , e ) = E(e)E(v ) . ( 1 )   In total , we retrieve the top N×kknowledge en-   tries relevant to image v. We keep top- mknowl-   edge entries ranked by similarity scores as explicit   knowledge source x.   In principle , the image and knowledge entry en-   coders can be implemented by any multimodal   transformer . We use the CLIP model ( ViT - B/16   variant ) ( Radford et al . , 2021 ) in our work and take   the[CLS ] as representations . We pre - extract rep-   resentations of the knowledge entries in the knowl-   edge base Kusing the entity encoder Eand   index them using FAISS ( Johnson et al . , 2019 ) .   The qualitative example for the extracting explicit   knowledge model is presented in Appendix A.   3.2.2 Knowledge Base Construction   We use the English Wikidata ( Vrandecic and   Krotzsch , 2014 ) dump from Sep. 20 , 2021 as   the explicit knowledge source base which contains   95,870,584entities . Each data item is stored ina structured format constituted of property - value   pairs . Properties are objects and have their own   Wikidata pages with labels , aliases , and descrip-   tions . We extract a subset that covers common   objects in real - world scenarios . We remove all   entities whose string labels or corresponding de-   scriptions are empty or non - English . This results   in a total of 423,520entity triplets in the end ( e.g. ,   < Q2813 , Coca - Cola , carbonated brown colored   soft drink > ) ( See Table 1 ) .   Subclass Number   Role ( Q214339 ) 162,027   Point of interest ( Q960648 ) 85,900   Tool ( Q39546 ) 78,621   Vehicle ( Q42889 ) 44,274   Animal ( Q729 ) 18,581   Clothing ( Q11460 ) 17,711   Company ( Q891723 ) 12,173   Sport ( Q349 ) 4,233   Total 423,520   3.3 Implicit Knowledge Retrieval   While our explicit knowledge retriever focuses   on semantic matching between image regions and   knowledge entries , it lacks implicit commonsense   knowledge ( e.g. ,Lemons are sour ) which is usu-   ally stored in large - scale language models ( Brown   et al . , 2020 ) . In this section , we retrieve implicit959knowledge with supporting evidence by prompting   from a large - scale pre - trained language model .   We design our implicit knowledge retriever with   inspirations from the previous work ( Yang et al . ,   2022 ) . We leverage GPT-3 as an implicit language   knowledge base and treat VQA as an open - ended   text generation task . For each image - question pair ,   we first convert the image vinto a textual de-   scription Cvia a state - of - the - art image caption-   ing model ( Li et al . , 2020c ) , and then construct   a carefully designed text prompt consisting of a   general instruction sentence , the textual descrip-   tionC , the question , and a set of context - question-   answer triplets taken from the training dataset that   are semantically most similar to the current image-   question pair ( see Figure 7 in Appendix B for a   concrete example ) . We then input this text prompt   to the GPT-3 model in its frozen version and ob-   tain the output from GPT-3 as the tentative answer   candidate to the current image - question pair .   To gain deeper insights from the implicit knowl-   edge coming out of GPT-3 and its rationale , we   design another prompt to query GPT-3 for support-   ing evidence behind the tentative answer candidate   that it generates . More specifically , for each image-   question pair ( v , q ) , and for a tentative answer a   generated by GPT-3 , we construct the prompt in   the form of : “ ( question q ) ? ( answer a ) . This is   because ” to query GPT-3 for supporting evidence   ( see Figure 6 in Appendix B for a concrete exam-   ple ) . We finally compile both the tentative answers   and the corresponding supporting evidence from   GPT-3 as implicit knowledge source x.   3.4 KAT Model   As showed in the Figure 2 , the explicit knowl-   edge entries are from an image , which are con-   cerned with semantic matching of the image re-   gions . These knowledge entries could be noisy or   irrelevant to its corresponding question . Moreover ,   some of the supporting evidence prompted from   GPT-3 is generic or not related to image content .   Simple concatenation of different knowledge may   introduce noise during model training . We design   a knowledge reasoning module with inspirations   from the previous work ( Karpukhin et al . , 2020 ) .   Our knowledge reasoning module encodes each   question and knowledge pair separately , and jointly   reason over both explicit and implicit knowledge   when generating an answer . Encoder . We concatenate question qwith each   knowledge as a question - knowledge pair . Firstly ,   we add sentinel tokens question : , entity :   anddescription : before the question , the   retrieved entity , and its description separately .   Similarly , we add sentinel tokens question : ,   candidate : andevidence : before the ques-   tion , the tentative answer , and its evidence . Sec-   ondly , we use an embedding layer followed by a   sequence of encoder layers to encode the question-   knowledge pairs separately . We average the token   embeddings of each question - knowledge pair from   the last encoder layer , which results in an embed-   ding matrix of explicit knowledge X∈R   and implicit knowledge X∈R , where d ,   mandpare the embedding dimension , the num-   ber of explicit knowledge x , and the number of   implicit knowledge x , respectively .   Reasoning Module . To jointly reason over im-   plicit and explicit knowledge , we concatenate the   embeddings of explicit and implicit knowledge   form a global representation X∈R. The   cross - attention module takes the global represen-   tation Xof the encoder as the input . Let H∈R   be the output of the previous self - attention layer of   the decoder . By definition ( Vaswani et al . , 2017 ) ,   the scaled dot - product attention can be expressed   as :   Q = softmax ( QK   √   d)V , ( 2 )   where queries Q , keys K , and values Vare com-   puted by applying linear transformations : Q=   WH , K = WX , V = WX . The attended   representation Qis a weighted sum of the values ,   and implies that our model performs a joint rea-   soning over explicit and implicit knowledge when   generating answers .   Decoder . We feed the embeddings of explicit and   implicit knowledge to a sequence of decoder layers   for answer generation . We train our model with a   cross - entropy loss :   L=−/summationdisplaylogp(y|y , x;x),(3 )   where yis predicted autoregressively.960Method Knowledge Resources Acc ( % ) Q only ( Marino et al . , 2019 ) - 14.93   Vanilla T5 - 18.56   MLP ( Marino et al . , 2019 ) - 20.67   BAN ( Marino et al . , 2019 ) - 25.1   MUTAN ( Marino et al . , 2019 ) - 26.41BAN+AN ( Marino et al . , 2019 ) Wikipedia 25.61   BAN+KG - AUG ( Li et al . , 2020b ) Wikipedia+ConceptNet 26.71   MUTAN+AN ( Marino et al . , 2019 ) Wikipedia 27.84   ConceptBERT ( Garderes et al . , 2020 ) ConceptNet 33.66   KRISP ( Marino et al . , 2021 ) Wikipedia+ConceptNet 38.35   Vis - DPR ( Luo et al . , 2021 ) Google Search 39.2   MA VEx ( Wu et al . , 2022 ) Wikipedia+ConceptNet+Google Images 39.4PICa - Base ( Yang et al . , 2022 ) Frozen GPT-3 ( 175B ) 43.3   PICa - Full ( Yang et al . , 2022 ) Frozen GPT-3 ( 175B ) 48.0   KAT - explicit ( w/ reasoning ) Wikidata 44.25   KAT - implicit ( w/ reasoning ) Frozen GPT-3 ( 175B ) 49.72   KAT ( w/o reasoning ) Wikidata+Frozen GPT-3 ( 175B ) 51.97   KAT ( single ) Wikidata+Frozen GPT-3 ( 175B ) 53.09   KAT ( ensemble ) Wikidata+Frozen GPT-3 ( 175B ) 54.41   4 Experiment   4.1 Dataset   OK - VQA ( Marino et al . , 2019 ) is currently the   largest knowledge - based VQA dataset , The ques-   tions are crowdsourced from Amazon Mechani-   cal Turkers and require outside knowledge beyond   the images in order to be answered correctly . The   dataset contains 14,031images and 14,055ques-   tions covering a variety of knowledge categories .   We follow the standard evaluation metric recom-   mended by the VQA challenge ( Antol et al . , 2015 ) .   4.2 Implementation Details   For the knowledge reasoning module , we initialize   our model with the pre - trained T5 model ( Raffel   et al . , 2020 ) . We compare two model sizes , base   and large , each containing 220Mand770Mpa-   rameters respectively . We fine - tune the models on   OK - VQA dataset , using AdamW ( Loshchilov and   Hutter , 2019 ) . We use a learning rate of 3e−5   to warm up for 2Kiterations and train for 10 K   iterations . Limited by the computational resources , we set the number of retrieved entities to 40 . The   model is trained with a batch size of 32 , using   16 V100 GPUs with 32Gb of memory each . Un-   less otherwise specified , all results reported in this   paper as KAT use this model which we found to   perform best . We evaluate our predictions with   ground - truth after normalization . The normaliza-   tion step consists of lowercasing , and removing arti-   cles , punctuation and duplicated whitespace ( Chen   et al . , 2017 ; Lee et al . , 2019 ) . To be consistent with   previous work ( Marino et al . , 2021 ) , we train our   model with 3different random seeds and use the   average results for the leaderboard submission .   4.3 Comparison with Existing Approaches   We compare our model against existing approaches   on the OK - VQA dataset and the results are summa-   rized in Table 2 . Our model outperforms state - of-   the - art methods by significant margins . We com-   pare our model with existing approaches from two   aspects . ( 1 ) If we only consider using explicit   knowledge , our model achieves 44.25 % which is   4.85 % and5.9%higher than MA VEx and KRISP,961respectively . Our model uses contrastive - learning-   based model to extract knowledge , leaving head-   room by incorporating supervised pre - trained mod-   els , such as pre - trained object detectors . It should   be noted that our proposed model is working on a   more challenging problem . As the generated an-   swer could contain an arbitrary number of words   from the entire vocabulary . Our model is slightly   better than PICa - Base which is a plain version of   PICa - Full without example engineering . It implies   that our single , unified architecture can effectively   associate images with the explicit knowledge base .   ( 2 ) If we take the implicit knowledge from GPT-   3 as the additional input , our model outperforms   PICa - Full by 6.41 % which indicates it is important   to integrate knowledge of different types when gen-   erating answers . The detailed comparison can be   found in Table 3 .   5 Ablation Study   To unpack the performance gain and understand   the impact of different components , we ablate and   compare different model architectures , types of   knowledge and the number of explicit knowledge .   Specifically , as shown in Table 3 , our KAT - large   shows a consistent improvement over using KAT-   base . This larger model has more capacity for   implicit knowledge reasoning . The integration of   explicit and implicit knowledge achieves a perfor-   mance gain of ∼4 % , supporting the intuition that   these two types of knowledge provide complemen-   tary pieces of knowledge .   5.1 Effectiveness of Knowledge Reasoning   To verify the effectiveness of our knowledge reason-   ing module , we use a KAT without the knowledge   reasoning module which is denoted as KAT ( w/o   reasoning ) . This model concatenates explicit andMethod Accuracy ( % )   KAT ( w/o reasoning ) 51.97   KAT 54.41   implicit knowledge as a sentence and adopts a max-   imum length of 256 tokens . We train this variant   with the same parameter settings . As shown in Ta-   ble 4 , simply concatenating knowledge sources is   2.43 % lower than our proposed model . It indicates   that KAT ( w/o reasoning ) may introduce noise to   relevant knowledge during encoding . Our model   adaptively attend different knowledge sources for   answer generation that can reduce the influence of   irrelevant knowledge .   5.2 Extracting Explicit Knowledge   From Figure 3 we can see , the performance of   our model is directly affected by the size of re-   trieved explicit knowledge . When only consider-   ing the implicit knowledge ( i.e. , the number of   retrieved entities is 0 ) , our model achieves 47.6 %   which is slightly worse than PICa - Full baseline . It   indicates that solely increasing model complexity   can not improve the performance . This also demon-   strates the importance of explicit knowledge . Our   model shows a consistent improvement by incor-   porating more explicit knowledge . While a more962   extensive knowledge set may include more distract-   ing knowledge , retrieved knowledge entries can   share either visually or semantically similar knowl-   edge as the relevant ones . Thus this can massively   reduce the search space and/or reduce spurious am-   biguity .   We compare different explicit knowledge re-   trieval module . Though ViT/16 has a large classifi-   cation improvement over ResNet-50 ( e.g. ,6.9%on   ImageNet ) ( Radford et al . , 2021 ) , there is a less gap   between these two backbones . As the number of re-   trieved entities increases , our knowledge reasoning   module can further migrate this gap by adaptively   attending to different explicit knowledge .   5.3 Category Results on OK - VQA   Here we present quantitative analyses to illustrate   how explicit and implicit knowledge influence the   final predictions . Based on the types of knowledge   required , questions in OK - VQA are categorized   into 11 categories and the accuracy results of each   category are reported in Table 5 . We re - train our   model under the same settings with only either   explicit or implicit knowledge , denoted as “ exp ”   and “ imp ” respectively .   For most categories , the model using only ex-   plicit knowledge performs worse than that using   only implicit knowledge . As implicit knowledge   comes from the results of state - of - the - art object   detection , image captioning models and support-   ing evidence by prompting GPT-3 . While explicit   knowledge is retrieved based on semantic match-   ing between images and entities from knowledge   bases , it contains richer but more distracting knowl-   edge . Note that using explicit knowledge performs   better for category “ Brands , Companies , and Prod - ucts " and “ Weather and Climate " . It indicates that   accurately recognizing objects with fine - grained   descriptions in the images is important for these   categories to answer corresponding questions .   5.4 Qualitative Analysis   Analyzed in previous sections , jointly reasoning   over both knowledge sources during answer gener-   ation improves the explicit - only and implicit - only   models by large margins . Figure 4 shows two ex-   amples comparing answers generated by different   models along with retrieved knowledge . The left   example shows that while explicit knowledge re-   trieved from the knowledge base contains the nec-   essary knowledge entries for reasoning , it fails to   generate the answer which requires the relation be-   tween bench and Coca Cola logos . On the other   side , implicit knowledge retrieved from GPT-3 can   only infer the bench is painted red , failing to rec-   ognize its logo . By jointly considering both knowl-   edge sources , our model can associate the color of963Coca Cola logo with the painted color of the bench   which derives the correct answer . The right ex-   ample shows that though explicit knowledge does   not contain the right knowledge entries , it provides   visually similar descriptions of this sport which fur-   ther constrains the search space of our model and   verifies the correctness of the implicit knowledge .   6 Conclusion   This paper takes a step towards understanding the   complementary role of implicit knowledge gained   from continuing to scale models and explicit knowl-   edge from structured knowledge bases . Impor-   tantly , it appears that there is headroom in both   directions ( i.g . improving retrieval and reasoning ) .   Our conceptually simple yet effective approach for   knowledge - based VQA makes these relationships   explicit while still achieving a significant improve-   ment against state - of - the - art results . Additional   challenges remain , for example how best to align   image regions with meaningful external semantics   deserves and how to efficiently and accurately inte-   grate multiple knowledge bases .   Acknowledgement   We are especially grateful to Jianwei Yang , Daniel   McDuff , Dragomir Radev , Harkirat Behl , Hao   Chen , Chunyuan Li , Baolin Peng , Kezhen Chen ,   Tejas Srinivasan for their for the early insightful dis-   cussions , suggestion , and their pointers to the mod-   eling generation and literature . We thank Zhe Gan ,   Zhengyuan Yang , Lijuan Wang from cognition ser-   vice team of Microsoft for their work and their gen-   erous helps and feedback for the project . We appre-   ciate Subhojit Som from Turing team of Microsoft   for his enormous support and encouragement . The   authors gratefully acknowledge Kenneth Marino   from DeepMind , and Roozbeh Mottaghi from the   AllenAI for their comments , supporting and helps   of the work . This research was supported in part by   the Defense Advanced Research Projects Agency   ( DARPA ) under contract number D17PC00340 and   also supported by the US DARPA KAIROS Pro-   gram No . FA8750 - 19 - 2 - 1004 .   References964965Appendix   A Figure of Explicit Knowledge   In this section , we show one example Figure 5 to   extract explicit knowledge from an image , which   use the CLIP model to conduct the explicit knowl-   edge retrieval with the image and a wiki knowledge   base .   B Examples of Prompts of Implicit   Knowledge   In this Section B of the Appendix , we show two   concrete examples ( Figure 6 and Figure 7 ) for the   prompts that constructed to query GPT-3 for im-   plicit knowledge in our experiments :   C Analysis on More Examples   In this section , we showcase more predictions from   variants of our model . As shown in Figure 8 , we   analyze the predictions based on different type of   knowledge from several aspects :   Effectiveness of explicit knowledge retriever .   Our explicit knowledge retriever can retrieve fine-   grained knowledge entries from the explicit knowl-   edge base , such as golden retriever ( a fine - grained   breed of dogs ) , cucumber sandwich ( a specific type   of sandwich ) and Macbook Pro ( a specific model   of Apple products ) . These fine - grained entities are   hardly obtained from existing object detection mod-   els , which can constraint the search space of our   model and are beneficial to our answer generation   process .   Effectiveness of implicit knowledge retriever .   Our implicit knowledge retriever can retrieve sup-   porting evidence from GPT-3 , such as Thomas : the   train is named after the man who designed it . and   Refrigerator : the refrigerator is used to keep food   cold . These kinds of knowledge are highly related   to commonsense knowledge which needs further   inference based on entities and provide comple-   mentary explanation to explicit knowledge .   Answer generation & classification . As most   previous work on OK - VQA task , such as KRISP or   MA VEx method , implement OK - VQA as a classi-   fication task . The prediction vocabulary is dataset-   specific and assumes the training and test set are   sharing a similar vocabulary . The limitation of   these methods is the generalization ability . Our pro-   posed KAT model treats OK - VQA as an open - end   generation task . From these examples we found ,   our model can generate answers like Iphone orHer-   cules that are visually and semantically reasonable .   Our proposed novel KAT model using the explicit966967and implicit knowledge is designed to enhance se-   mantic alignment and generate representations with   stronger knowledge - awareness.968