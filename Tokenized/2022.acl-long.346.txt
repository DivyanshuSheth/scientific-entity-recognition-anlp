  Tu VuBrian LesterNoah ConstantRami Al - RfouDaniel Cer   Google Research   University of Massachusetts Amherst   { ttvu,brianlester,nconstant,rmyeid,cer}@google.com   tuvu@cs.umass.edu   Abstract   There has been growing interest in parameter-   efÔ¨Åcient methods to apply pre - trained lan-   guage models to downstream tasks . Build-   ing on the approach of Lester   et al . ( 2021 ) , which learns task - speciÔ¨Åc soft   prompts to condition a frozen pre - trained   model to perform different tasks , we propose a   novel prompt - based transfer learning approach   called : SoftPrompt Transfer .   Ô¨Årst learns a prompt on one or more source   tasks and then uses it to initialize the prompt   for a target task . We show thatsig-   niÔ¨Åcantly boosts the performance of across many tasks . More remarkably ,   across all model sizes , matches or out-   performs standard ( which Ô¨Åne-   tunes all model parameters ) on the benchmark , while using up to 27,000    fewer task - speciÔ¨Åc parameters . To understand   whereis most effective , we conduct a   large - scale study on task transferability with   26 tasks in 160 combinations , and demon-   strate that many tasks can beneÔ¨Åt each other   via prompt transfer . Finally , we propose an   efÔ¨Åcient retrieval approach that interprets task   prompts as task embeddings to identify similar   tasks and predict the most transferable source   tasks for a novel target task .   1 Introduction   The past few years have seen the rapid develop-   ment of ever larger pre - trained language models ,   where it has repeatedly been shown that scaling   up the model size is a key ingredient for achiev-   ing the best performance ( Devlin et al . , 2019 ; Raf-   fel et al . , 2020 ; Brown et al . , 2020 ) . While this   trend has continued to push the boundaries of pos-   sibility across various benchmarks , the sheer   size of these models presents a challenge for their   practical application . For 100B+ parameter mod-   els , Ô¨Åne - tuning and deploying a separate instanceFigure 1 : Ourapproach ‚Äî which transfers a   prompt learned from a mixture of source tasks ( here , ) onto target tasks ‚Äî outperforms vanilla ( Lester et al . , 2021 ) and ( Brown et al . ,   2020 ) on by a large margin , matching or   outperforming across all model sizes .   At the model size , even outperforms , which Ô¨Åne - tunes the entire model   on the mixture before Ô¨Åne - tuning it on individ-   ual tasks . See Appendix A for full results .   of the model for each downstream task would be   prohibitively expensive . To get around the infeasi-   bility of Ô¨Åne - tuning , Brown et al . ( 2020 ) propose , where every downstream task is   cast as a language modeling task and the frozen pre-   trained model performs different tasks by condition-   ing on manual text prompts provided at inference   time . They demonstrate impressive few - shot perfor-   mance with a single frozen model , although   its performance depends highly on the choice of the   prompt ( Zhao et al . , 2021 ) and still lags far behind   state - of - the - art Ô¨Åne - tuning results .   More recent work explores methods for learn-   ingsoft prompts ( Liu et al . , 2021b ; Qin and Eis-   ner , 2021 ; Li and Liang , 2021 ; Lester et al . , 2021 ) ,   which can be seen as additional learnable parame-   ters injected into the language model . Lester et al .   ( 2021 ) propose , a simple method5039   that learns a small task - speciÔ¨Åc prompt ( a sequence   of tunable tokens prepended to each example ) for   each downstream task during adaptation to condi-   tion the frozen language model to perform the task .   Strikingly , as model capacity increases , becomes competitive with ,   which Ô¨Åne - tunes the entire model on each down-   stream task . Nevertheless , at smaller model sizes   ( below 11B parameters ) , there are still large gaps   between and .   In this paper , we propose : SoftPrompt   Transfer , a novel transfer learning approach in the   context of prompt tuning.Ô¨Årst trains a prompt   on one or more source tasks , and then uses the re-   sulting prompt to initialize the prompt for a target   ( downstream ) task . Our experiments show thatoffers signiÔ¨Åcant improvements over across tasks and model sizes . For instance ,   on the benchmark ( Wang et al . , 2019b ) ,   we obtain +10.1 and +2.4 point average accuracy   improvements using the(220 M parame-   ter ) and ( 11B parameter ) models ( Raffel   et al . , 2020 ) , respectively . More importantly ,   is competitive with or outperforms   across all model sizes ( see Figure 1 ) .   Motivated by these results , we investigate trans-   ferability between tasks , through the lens of soft   task prompts . Our goal is to answer two questions :   ( a)For a given target task , when does initializing   the prompt from a source task boost performance ?   ( b)Can we use task prompts to efÔ¨Åciently predict   which source tasks will transfer well onto a novel   target task ? To answer ( a ) , we conduct a system-   atic study of themodel using 26 tasks in   160 combinations of source and target tasks . Our   results indicate that many tasks can beneÔ¨Åt eachother via prompt transfer . To address ( b ) , we inter-   pret the learned task prompts as task embeddings to   construct a semantic space of tasks and formalize   the similarity between tasks . We design an efÔ¨Åcient   retrieval algorithm that measures task embedding   similarity , allowing practitioners to identify source   tasks that will likely yield positive transfer .   To summarize , our main contributions are :   ( 1 ) We propose , a novel prompt - based trans-   fer learning approach , and show that scale is not   necessary for to match the perfor-   mance of ; on ,   matches or beats across all model   sizes . ( 2 ) We conduct a large - scale and systematic   study on task transferability , demonstrating con-   ditions under which tasks can beneÔ¨Åt each other   via prompt transfer . ( 3 ) We propose an efÔ¨Åcient re-   trieval method that interprets task prompts as task   embeddings to construct a semantic space of tasks ,   and measures task embedding similarity to identify   which tasks could beneÔ¨Åt each other . ( 4 ) To fa-   cilitate future work on prompt - based learning , we   will release our library of task prompts and pre-   trained models , and provide practical recommenda-   tions for adapting our library to practitioners   at .   2 Improving with   To improve performance of on a   target task , introduces source prompt tuning ,   an intermediate training stage between language   model pre - training and target prompt tuning ( Fig-   ure 2 , left ) , to learn a prompt on one or more source   tasks ( while still keeping the base model frozen),5040which is then used to initialize the prompt for the   target task . Our approach retains all the compu-   tational beneÔ¨Åts of : for each target   task , it only requires storing a small task - speciÔ¨Åc   prompt , enabling the reuse of a single frozen pre-   trained model across all tasks . In this section , we   present a genericapproach where a single   transferred prompt is reused for all target tasks .   In ¬ß 3 , we explore a targeted approach that retrieves   different source prompts for different target tasks .   2.1 Experimental setup   Our frozen models are built on top of the pre-   trainedcheckpoints of all sizes : , , , , with 60 M , 220 M , 770 M , 3B , and   11B parameters , respectively . In our experiments   with , we leverage the LM adapted version of , which was found to be easier to optimize for ( Lester et al . , 2021 ) .   2.1.1 Baselines   We compareto the following baselines : : The vanilla prompt tuning ap-   proach of Lester et al . ( 2021 ) , where an indepen-   dent prompt is directly trained on each target task . : We   compare prompt tuning approaches to , the standard Ô¨Åne - tuning approach ( Devlin   et al . , 2019 ; Raffel et al . , 2020 ) , where all model   parameters are Ô¨Åne - tuned on each target task sep-   arately . For an apples - to - apples comparison , we   include , a more competi-   tive baseline that Ô¨Årst Ô¨Åne - tunes the entire model   on the same mixture of source tasks used for   before Ô¨Åne - tuning it on individual target tasks .   2.1.2 Evaluation datasets   We study downstream performance on a diverse set   of tasks from the ( Wang et al . , 2019c ) and ( Wang et al . , 2019b ) benchmarks . We   train for a Ô¨Åxed number of steps and report results   on the validation set associated with each dataset .   2.1.3 Data for source prompt tuning   As with language model pre - training , the choice of   training data is crucial for successful prompt trans-   fer . To investigate the impact of source training   data on downstream performance , we compare a   diverse set of source tasks .   A single unsupervised learning task : We Ô¨Årst   consider training the prompt on a fraction of the(Colossal Clean Crawled Corpus ) dataset ( Raf-   fel et al . , 2020 ) using the ‚Äú preÔ¨Åx LM ‚Äù objective   discussed in Raffel et al . ( 2020 ) . Although this   task was used to pre - train our frozenmodels al-   ready , it could still be helpful for learning a general-   purpose prompt .   A single supervised learning task : Alterna-   tively , we can train the prompt using a supervised   task . We use either ( Williams et al . , 2018 ) or(Rajpurkar et al . , 2016 ) as a single source   task . was shown to be helpful for many   sentence - level classiÔ¨Åcation tasks ( Phang et al . ,   2019 ) , whilewas found to generalize well   totasks ( Talmor and Berant , 2019 ) .   A multi - task mixture : So far , we have consid-   ered using a single source task . An alternative   approach is multi - task training . Within ‚Äôs uniÔ¨Åed   text - to - text framework , this simply corresponds to   mixing different datasets together . We explore mix-   ing datasets from differentbenchmarks or fam-   ilies of tasks , including , , natural   language inference ( ) , paraphrasing / semantic   similarity , sentiment analysis , question answering   ( ) on ( Fisch et al . , 2019 ) , commonsense   reasoning on ( Lourie et al . , 2021 ) , ma-   chine translation , summarization , and natural lan-5041guage generation on ( Gehrmann et al . , 2021 ) .   We create a mixture of source tasks from each of   the benchmarks / families of tasks above , and a   mixture comprising all datasets ( C4 + 55 labeled   datasets ) , using the examples - proportional mixing   strategy in Raffel et al . ( 2020 ) with an artiÔ¨Åcial   dataset size limit K= 2examples .   2.1.4 Training details   We closely follow the training procedure in Lester   et al . ( 2021 ) . SpeciÔ¨Åcally , the only new parameters   introduced during both source and target prompt   tuning are a shared prompt 2Rprepended   to each ( embedded ) input sequence , where L , E   are the prompt length and the embedding size , re-   spectively . In all cases , we set L= 100 tokens   and tune the prompt for a Ô¨Åxed number of steps   S.WhileSis set to 30 K in Lester et al . ( 2021 ) ,   we Ô¨Ånd that additional tuning is helpful on large   datasets . As such , we set Sto2= 262;144 , fol-   lowing Raffel et al . ( 2020 ) , with the exception of   ablation experiments ( rows ‚Äú  longer tuning ‚Äù ) in   Table 1 which use S= 30 K. For source prompt   tuning , the prompt token embeddings are initial-   ized from sampled vocabulary ( i.e. , the 5,000 most   common tokens ) . During target prompt tuning , we   save a checkpoint every 500 steps and report re-   sults on the checkpoint with the highest validation   performance . Appendix C contains training details   for and model tuning approaches .   2.2 Effect of   We compare the results ofand other ap-   proaches in Table 1 and Figure 1 . Below , we sum-   marize and analyze each of our Ô¨Åndings in detail.signiÔ¨Åcantly improves performance and   stability of : Our results on the and benchmarks with   ( Table 1 ) suggest that prompt transfer provides   an effective means of improving performance for . For example , the best - performing   variant ofoutperforms the vanillaapproach on both and by a   substantial margin , obtaining +4.4 and +10.1 point   average accuracy improvements , respectively . Our   ablation study indicates that longer tuning is also an   important ingredient for achieving the best perfor-   mance , and is complementary to prompt transfer .   Additionally , when longer tuning is omitted , we   observe thatimproves stability across runs .   Within , we can compare the effectiveness   of different source mixtures ( see Table 1 ) . Source   prompt tuning on performs best on both and , obtaining average scores of   82.8 and 73.2 , respectively . Interestingly , unsuper-   vised source prompt tuning on(the same task   used to pre - train our frozen models ) still yields con-   siderable improvements , even outperforming using for tasks . Using oras a single source dataset is also particularly   helpful across target tasks . Other source mixtures   can lead to signiÔ¨Åcant gains , with some families of   tasks ( e.g. ,and paraphrasing / semantic similar-   ity ) showing more beneÔ¨Åt than others . Mixing all   the datasets together does not yield the best results ,   possibly due to task interference / negative transfer   issues , where achieving good performance on one   or more source tasks can hurt performance on a   target task.5042helps close the gap with   across all model sizes : Figure 1 shows our results across model sizes ( see Ap-   pendix A for full results ) . As shown in Lester   et al . ( 2021 ) , becomes more com-   petitive with scale , and at the size , it nearly   matches the performance of . How-   ever , at smaller model sizes , there are still large   gaps between the two approaches . We show thathelps close these gaps and even exceeds ‚Äôs performance by a large margin at sev-   eral model sizes , while retaining all the computa-   tional beneÔ¨Åts conferred by . Finally ,   at the size , achieves the best average   score of 91.2 , +1.1 points better than the strong baseline , despite having   27,000fewer task - speciÔ¨Åc parameters .   As a Ô¨Ånal test of ‚Äôs effectiveness , we submit-   ted our model ‚Äôs predictions to the   leaderboard , achieving a score of 89.2 . This far   exceeds all previous submissions using parameter-   efÔ¨Åcient adaptation , such as ( 71.8 ) , and al-   most matches fully Ô¨Åne - tuned ( 89.3),de-   spite tuning 27,000 fewer parameters . To the   best of our knowledge , is the Ô¨Årst parameter-   efÔ¨Åcient adaptation approach that is competitive   with methods that tune billions of parameters . See   Appendix D for details .   3 Predicting task transferability   So far , we have seen that soft prompt transfer can   signiÔ¨Åcantly boost the performance of prompt tun-   ing , but it is critical to pick the right source tasks for   transfer . For instance , through an extensive search ,   we found that and provide excellent   source tasks for transferring to individual   and tasks . But what about a resource-   constrained scenario where a user is not able to   exhaustively search over a set of source tasks ? Can   wepredict which tasks will best transfer onto a   novel target task without testing them one by one ?   To investigate this , we conduct a large - scale em-   pirical study with 26 tasks . We Ô¨Årst measure   transferability across all task combinations ( ¬ß 3.1 ) .   Next , we show that by interpreting task prompts   as task embeddings , we can construct a seman-   tic space of tasks , wherein similar tasks cluster   together ( ¬ß 3.2 ) . Based on this observation , we pro-   pose a retrieval algorithm ( ¬ß 3.3 ) that leverages task   embedding similarity to choose which source tasks   to use for a given novel target task ( Figure 2 , right ) .   Our proposed approach can eliminate 69 % of the   source task search space while keeping 90 % of the   best - case quality gain .   3.1 Measuring transferability   We study a diverse set of 16 source datasets and   10 target datasets ( see Table 2).We consider   all 160 possible source - target pairs , and perform   transfer from each source task to each target task .   All source tasks are data - rich or have been shown   to yield positive transfer in prior work . To simulate   a realistic scenario , we use low - resource tasks ( less   than 10 K training examples ) as target tasks.5043   To limit computational costs , we usein   all of our task transferability experiments . We per-   form 262;144prompt tuning steps on each source   task . The prompt checkpoint with the highest   source task validation performance is selected to   initialize prompts for target tasks . Since the target   datasets are small , we only perform 100 K prompt   tuning steps on each target task . We repeat each   experiment three times with different random seeds .   Other training details match ¬ß 2.1.4 .   Tasks beneÔ¨Åting each other via prompt trans-   fer : Figure 3 shows a heatmap of our results ( see   Appendix E for full results ) . In many cases , prompt   transfer provides a signiÔ¨Åcant gain on the target   task . The transfer!yields the largest   relative error reduction of 58.9 % ( from an average   score of 92.7 to 97.0 ) , followed by !   ( 29.1 % ) and ! ( 20.0 % ) . Using the   best source prompt ( out of 48 ) for each target task   dramatically improves the average score across our   10 target tasks from 74.7 to 80.7 . Overall , our re-   sults show effective transfer from large source tasks   that involve high - level reasoning about semantic re-   lationships among sentences ( e.g. , ) , or when   the source and target tasks are similar ( e.g. , ! ) . Interestingly , positive transfer can occur   between relatively dissimilar tasks ( e.g. ,   ! , ! , ! ) .   3.2 DeÔ¨Åning task similarity through prompts   Since only prompt parameters are updated dur-   ing prompt tuning on speciÔ¨Åc tasks , the learned   prompts likely encode task - speciÔ¨Åc knowledge .   This suggests that they could be used to reason   about the nature of tasks and their relationships . To   test this idea , we interpret task prompts as task em-   beddings and construct a semantic space of tasks .   More concretely , we deÔ¨Åne a task ‚Äôs embedding as   the prompt checkpoint after training for 10 K steps   on that task . Note that using early checkpoints   allows for quick computation of task embeddings   for novel target tasks . We estimate the similarity   between two tasks t;tby measuring the similar-   ity between their corresponding task embeddings   e;e , using the following metrics : : We   compute the cosine similarity between the average   pooled representations of the prompt tokens :   sim(t;t ) = cos(1   LXe;1   LXe ) ;   wheree;edenote the respective prompt tokens   ofe;e , andcosdenotes the cosine similarity . : We   compute the average cosine similarity between ev-   ery prompt token pair ( e;e ):   sim(t;t ) = 1   LXXcos(e;e):5044Task embeddings capture task relationships :   Figure 4 shows a hierarchically - clustered heatmap   of cosine similarities between the task embed-   dings using the metric . We observe that our learned task   embeddings capture many intuitive task relation-   ships . SpeciÔ¨Åcally , similar tasks group together   into clusters , including ( , , and ; and ) , sentiment analysis   ( , , and ) , ( and;and ) , semantic similarity ( and ) ,   paraphrasing ( and ) , and commonsense   reasoning ( , , and ) . We note that , which is antask   built from thedataset , is not closely linked   to ; this suggests that our task embeddings   are more sensitive to the type of task than domain   similarity . Interestingly , they also capture the un-   intuitive case of ‚Äôs high transferability to . Additionally , task embeddings that are de-   rived from different prompts of the same task have   high similarity scores ( see Appendix F ) .   3.3 Predicting transferability via similarity   We leverage our task embeddings to predict and   exploit task transferability . SpeciÔ¨Åcally , we explore   methods to predict the most beneÔ¨Åcial source tasks   for a given target task and then make use of the   source task prompts to improve performance on the   target task . To enlarge our set of source prompts ,   we use the prompts from each of the three different   prompt tuning runs on each source task , resulting in   48 source prompts . Given a target task twith task   embeddinge , we rank all the source prompts    with associated embeddings ein descending order   by similarity , sim(e;e ) . We denote the ranked   list of source prompts as  , whererdenotes the   rank ( r= 1;2 ; : : : ; 48 ) . We experiment with three   methods for using the ranked source prompts::We select the top- ksource   prompts and use each of them individually to ini-   tialize the target prompt . This procedure requires   prompt tuning ktimes on the target task t. The best   individual result is used for evaluating the effec-   tiveness of this method . : We initialize the tar-   get prompt with a weighted average of the top - k   source promptsP  so that we only per-   form prompt tuning on the target task tonce . The   weights  are computed as :   = sim(e;e)Psim(e;e ) ;   whereedenotes the corresponding task embed-   ding of. : We Ô¨Årst identify   the source tasks whose prompts are in the top - k   prompts and mix their datasets and the target   dataset together , using the examples - proportional   mixing strategy of Raffel et al . ( 2020 ) . Then , we   perform source prompt tuning on this multi - task   mixture and use the Ô¨Ånal prompt checkpoint to ini-   tialize the target prompt .   We report the average score across all target   tasks achieved by each method . For comparison ,   we measure the absolute and relative improvements   over ‚Äî prompt tuning on each target task   from scratch ( i.e. , without any prompt transfer ) .   Additionally , we include ‚Äî the oracle re-   sults achieved by a brute - force search to identify5045the best possible out of 48 source prompts for each   target task .   Correlation between task similarity and task   transferability : Figure 5 shows how the relative   error reduction on a target task changes as a func-   tion of the similarity between the source and target   task embeddings . Overall , we observe a signiÔ¨Å-   ca nt positive correlation between task embedding   similarity and task transferability on four ( out of   10 ) target tasks , including ( p < 0:001 ) ,   ( p < 0:001 ) , ( p < 0:01 ) , and(p < 0:05 ) ,   while it is less signiÔ¨Åcant on the other tasks . In   some cases ( e.g. , on ) , we observe a large rel-   ative error reduction ( 19.0 % , achieved by a source   prompt of ) despite a low cosine similarity   ( 0.4 ) . This suggests that factors other than task   similarity ( data size , task difÔ¨Åculty , domain sim-   ilarity , etc . ) may also play a role in determining   transferability .   Retrieving targeted source tasks via task em-   beddings is helpful : Table 3 compares differ-   ent methods for identifying which source prompts   could be beneÔ¨Åcial for a given target task . Over-   all , our results show the effectiveness of . Simply choosing the source prompt with   the highest task embedding similarity to the target   task using   improves over the baseline by a large margin ( from   an average score of 74.7 to 76.7 , a 12.1 % average   relative error reduction ) . Trying all the top-3 ( out   of 48 ) source prompts for each target task yields an   average score of 77.5 . With larger values of k , we   can retain most of the beneÔ¨Åts of oracle selection   ( 80 % of the gain in terms of average score with   k= 9 and 90 % with k= 15 ) , while still elimi-   nating over 2/3 of the candidate source prompts . has similar average per-   formance towithk= 1 , but achieves   lower variance . Thus , this may be an appealing al-   ternative toin scenarios where trying   multiple prompt tuning runs on the target task is   computationally prohibitive . Finally , also provides a means of obtaining   strong performance with an average score of 77.8 ,   even outperformingwithk3 .   4 Related Work   Parameter - efÔ¨Åcient transfer learning : Large-   scale pre - trained language models have been shown   to exhibit remarkable performance on many NLP   tasks ( Devlin et al . , 2019 ; Liu et al . , 2019b ; Yang   et al . , 2019 ; Lan et al . , 2020 ; Raffel et al . , 2020 ;   Brown et al . , 2020 ; He et al . , 2021 ) . To improve   practical applicability of these models , early work   introduces compression techniques ( Sanh et al . ,   2019 ; Jiao et al . , 2020 ; Fan et al . , 2020 ; Sanh et al . ,   2020 ) to obtain lightweight models . Other work ex-   plores updating only small parts of the model ( Za-   ken et al . , 2021 ) or task - speciÔ¨Åc modules , such as   adapters ( Houlsby et al . , 2019 ; Karimi Mahabadi   et al . , 2021 ) or low - rank structures ( Mahabadi et al . ,   2021 ; Hu et al . , 2021 ) , while keeping the rest of   the model Ô¨Åxed .   Recently , Brown et al . ( 2020 ) demonstrate im-   pressive few - shot performance with ,   where their model is conditioned on a manual   text prompt at inference time to perform differ-   ent tasks . Several efforts have since focused on   developing prompt - based learning approaches with   carefully handcrafted prompts ( Schick and Sch√ºtze ,   2021 ) , prompt mining and paraphrasing ( Jiang5046et al . , 2020b ) , gradient - based search for improved   prompts ( Shin et al . , 2020 ) , and automatic prompt   generation ( Gao et al . , 2021 ) . The use of hard   prompts , however , was found to be sub - optimal and   sensitive to the choice of the prompt ( Zhao et al . ,   2021 ; Liu et al . , 2021b ) . As such , more recent work   has shifted toward learning soft prompts ( Liu et al . ,   2021b ; Qin and Eisner , 2021 ; Li and Liang , 2021 ;   Lester et al . , 2021 ) , which can be seen as learn-   able parameters injected into the model . We refer   readers to Liu et al . ( 2021a ) for a recent survey on   prompt - based learning research .   In concurrent work , Gu et al . ( 2021 ) also explore   the effectiveness of prompt transfer . Their method   uses hand - crafted pre - training tasks tailored to spe-   ciÔ¨Åc types of downstream tasks , being less extensi-   ble to novel downstream tasks . In contrast , we use   existing tasks as source tasks and show that prompt   transfer can confer beneÔ¨Åts even when there are   mismatches ( e.g. , in task type or input / output for-   mat ) between the source and target .   Task transferability We also build on existing   work on task transferability ( Wang et al . , 2019a ;   Liu et al . , 2019a ; Talmor and Berant , 2019 ; Pruk-   sachatkun et al . , 2020 ; Vu et al . , 2020 , 2021 ) . Prior   work shows effective transfer from data - rich source   tasks ( Phang et al . , 2019 ) , those that require com-   plex reasoning and inference ( Pruksachatkun et al . ,   2020 ) , or those that are similar to the target task ( Vu   et al . , 2020 ) . There have also been efforts to predict   task transferability ( Bingel and S√∏gaard , 2017 ; Vu   et al . , 2020 ; Poth et al . , 2021 ) . Vu et al . ( 2020 )   use task embeddings derived from either the input   text or the diagonal Fisher information matrix of   the model , while Poth et al . ( 2021 ) explore adapter-   based alternatives . Here , our use of the same model   ( without task - speciÔ¨Åc components ) with a unifying   text - to - text format allows us to more easily model   the space of tasks . Additionally , prompt - based task   embeddings are comparatively cheaper to obtain .   5 Limitations & Future work   As other parameter - efÔ¨Åcient adaptation methods   ( see ¬ß 4 ) may outperform in speciÔ¨Åc   situations , it would be interesting to test whether an   approach similar tocould extend successfully   to these methods . At the same time , we believe that has its own merits . As pre - trained   language models become larger and larger , some   advantages of over other methods   are : ( 1 ) Among current methods with learnableparameters , is the most parameter   efÔ¨Åcient , requiring less than 0.01 % task - speciÔ¨Åc pa-   rameters for most model sizes . ( 2 )   is simpler than other methods , as it does not mod-   ify the internal model architecture ( cf . the method of Li and Liang ( 2021 ) , which   adds a preÔ¨Åx to each layer of both the Transformer   encoder and decoder ) ; as such , al-   lows mixed - task inference and facilitates transfer   learning between tasks . ( 3 ) As model capacity in-   creases , becomes more competitive   with ; to the best of our knowledge ,   this has not been shown for other methods . ( 4 ) Soft   prompts could possibly be interpreted as natural   language instructions .   Additionally , since our prompt - based task em-   bedding approach does not capture all of the factors   that inÔ¨Çuence task transferability , we leave further   exploration of other task embedding methods to   future work .   6 Conclusion   In this paper , we study transfer learning in the con-   text of prompt tuning . We show that scale is not   necessary for to match the perfor-   mance of . On , our   approach matches or even exceeds the performance   of by a large margin across model   sizes while being more parameter - efÔ¨Åcient . Our   large - scale study on task transferability indicates   that tasks can beneÔ¨Åt each other via prompt transfer   in various scenarios . Finally , we demonstrate that   task prompts can be interpreted as task embeddings   to formalize the similarity between tasks . We pro-   pose a simple yet efÔ¨Åcient retrieval approach that   measures task similarity to identify which source   tasks could confer beneÔ¨Åts to a novel target task .   Taken as a whole , we hope that our work will spur   more research into prompt - based transfer learning .   Acknowledgements   We thank Mohit Iyyer , Sebastian Ruder , Kalpesh   Krishna , Thang Luong , Quoc Le , and the members   of the Descartes team and the UMass NLP group   for helpful discussion and feedback . We would   also like to thank Grady Simon , Lucas Dixon , Slav   Petrov , Nader Akoury , Haw - Shiuan Chang , Kather-   ine Thai , Marzena Karpinska , and Shufan Wang for   their comments on this manuscript . Finally , we are   grateful to Vamsi Aribandi for his work on prepro-   cessing several datasets used in our experiments.5047References504850495050505150525053Appendices   A Full results for Figure 1   Table 4 shows the performance of different model   tuning and prompt tuning methods ( described   in ¬ß 2.1.1 ) on the benchmark .   B Source datasets used in our   experiments in ¬ß 2   Figure 6 displays the datasets used in our   experiments in ¬ß 2 . In addition to theunlabeled   dataset ( Raffel et al . , 2020 ) , we use 55 labeled   datasets . These datasets come from common   benchmarks / families of tasks , namely :    ( Wang et al . , 2019c ) , including(Warstadt et al . , 2019 ) , ( Socher   et al . , 2013 ) , ( Dolan and Brockett ,   2005 ) , ( Iyer et al . , 2017 ) , ( Cer   et al . , 2017 ) , ( Williams et al . , 2018 ) , ( Wang et al . , 2019c ) , and ( Dagan   et al . , 2005 , et seq . ) .    ( Wang et al . , 2019b ) , including(Clark et al . , 2019),(De Marn-   effe et al . , 2019 ) , ( Roemmele et al . ,   2011),(Khashabi et al . , 2018),(Zhang et al . , 2018),,(Pile-   hvar and Camacho - Collados , 2019 ) , and ( Levesque et al . , 2012 ) .   Natural language inference ( ) , including ( Nie et al . , 2020),,(Yin et al . ,   2021 ) , , , , and ( Bowman   et al . , 2015 ) .   Paraphrasing / semantic similarity , including(Parekh et al . , 2021 ) , , , and .   Sentiment analysis , including(Hu and Liu ,   2004 ) , ( Demszky et al . , 2020),(Go et al . , 2009 ) , , and(Zhang et al . , 2015 ) .   Question answering ( ) on ( Fisch   et al . , 2019 ) , including ( Ra-   jpurkar et al . , 2016),(Trischler   et al . , 2017 ) , ( Joshi et al . ,   2017 ) , ( Dunn et al . , 2017),(Yang et al . , 2018 ) , and ( ( Kwiatkowski et al . ,   2019 ) ) .   Commonsense reasoning on ( Lourie et al . , 2021 ) includ-   ing   ( Bhagavatula et al . , 2020 ) , ( Huang et al . , 2019 ) , ( Zellers et al . , 2019 ) , ( Bisk   et al . , 2020),(Sap et al . , 2019 ) ,   and ( Sakaguchi et al . , 2020 ) .   Machine translation , including(Bojar et al . , 2014),(Bojar   et al . , 2015 ) , and(Bojar et al . ,   2016 ) .   Summarization , including ( Zhang and   Tetreault , 2019),(Kornilova and Ei-   delman , 2019 ) , ( Hermann   et al . , 2015 ; See et al . , 2017),(Ladhak et al . , 2020 ) , ( Graff   et al . , 2003 ; Rush et al . , 2015),(Fabbri et al . , 2019 ) , ( Grusky   et al . , 2018),(Gliwa et al . , 2019 ) ,   and(Narayan et al . , 2018 ) .   Natural language generation on ( Gehrmann et al . , 2021 ) , including(Lin et al . , 2020 ) , ( Nan   et al . , 2021 ) , ( Du≈°ek et al . , 2019 ) , ( Rastogi et al . , 2020 ) , ( Gardent   et al . , 2017),(Jiang et al . , 2020a ) , , and .   C Additional training details   For , following Lester et al . ( 2021 ) ,   we initialize the prompt tokens with embeddings   that represent an enumeration of the output classes5054   with a back off to sampled vocabulary to Ô¨Åll any   remaining prompt positions .   For model tuning approaches , we use the de-   fault hyperparameters for(Raffel et al . , 2020 ) ,   i.e. , learning rate 0.001 , Adafactor optimizer with   pre - training parameter states restored , and dropout   probability 0.1 . To improve the model tuning base-   lines , we perform a sweep over the batch size hy-   perparameter and select 2tokens per batch , fol-   lowing Lester et al . ( 2021 ) .   D Details of our submission   Table 5 shows the performance of our submission , along with several strong   competitors from the public leader-   board . Apart from the human baseline , the top-7   submissions all tune > 3B parameters directly on the   Ô¨Ånal tasks . Only three previous sub-   missions use parameter efÔ¨Åcient adaptation , in the   sense of tuning < 1 M parameters on the Ô¨Ånal tasks ;   all other submissions tune > 50 M parameters . Oursubmission achieves a score of 89.2 ,   which far exceeds all other parameter - efÔ¨Åcient   adaptation methods , including , which ben-   eÔ¨Åts from over 10more frozen parameters ( al-   though it uses no tuned parameters ) . Compared to ( Hambardzumyan et al . , 2021 ) , ourap-   proach tunes 16more parameters ( 410 K vs. 25 K ) ,   and beneÔ¨Åts from 50 more frozen parameters .   To the best of our knowledge , is the Ô¨Årst   parameter - efÔ¨Åcient adaptation approach that is com-   petitive with methods that tune billions of param-   eters . Most notably , ‚Äôs performance almost   matches that of fully Ô¨Åne - tuned ( 89.3 ) , de-   spite building on the same underlying model , and   tuning 27,000fewer parameters . We note thatoutperformson three of eight   tasks ( namely , , , ) .   E Task transferability results   The full results of our task transferability exper-   iments can be found in Table 6 . We show that   in many cases , initializing the prompt to that of a5055source task can provide signiÔ¨Åcant gain on a target   task . Table 7 displays positive transfers with more   than 10 % relative error reduction on the target task .   F Task embedding similarity   In Figure 7 , we show a clustered heatmap of cosine   similarities between the task embeddings of the   26 tasks we study in our task transferability   experiments . For each task , we include the result-   ing task embeddings from all the three different   prompt tuning runs on the task . As can be seen , our   task embeddings capture task relationships : similar   tasks cluster together . Additionally , task embed-   dings that are derived from different prompts of the   same task are linked together .   G Correlation between task similarity   and task transferability   Figure 8 shows how the relative error reduction on   a target task changes as a function of the similarity   between the source and target task embeddings.5056505750585059