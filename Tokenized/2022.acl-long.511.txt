  Mobashir Sadat and Cornelia Caragea   Computer Science   University of Illinois Chicago   msadat3@uic.edu cornelia@uic.edu   Abstract   Existing Natural Language Inference ( NLI )   datasets , while being instrumental in the ad-   vancement of Natural Language Understand-   ing ( NLU ) research , are not related to scien-   tiﬁc text . In this paper , we introduce SNLI ,   a large dataset for NLI that captures the for-   mality in scientiﬁc text and contains 107;412   sentence pairs extracted from scholarly papers   on NLP and computational linguistics . Given   that the text used in scientiﬁc literature dif-   fers vastly from the text used in everyday lan-   guage both in terms of vocabulary and sen-   tence structure , our dataset is well suited to   serve as a benchmark for the evaluation of sci-   entiﬁc NLU models . Our experiments show   that SNLI is harder to classify than the exist-   ing NLI datasets . Our best performing model   with XLNet achieves a Macro F1 score of only   78:18 % and an accuracy of 78:23 % showing   that there is substantial room for improvement .   1 Introduction   Natural Language Inference ( NLI ) or Textual En-   tailment ( Bowman et al . , 2015 ) aims at recogniz-   ing the semantic relationship between a pair of   sentences — whether the second sentence entails   the ﬁrst sentence , contradicts it , or they are seman-   tically independent . NLI was introduced ( Dagan ,   Glickman , and Magnini , 2005 ) to facilitate the eval-   uation of Natural Language Understanding ( NLU )   that signiﬁcantly impacts the performance of many   NLP tasks such as text summarization , question   answering , and commonsense reasoning .   To date , several NLI datasets are made available   ( Bowman et al . , 2015 ; Williams , Nangia , and Bow-   man , 2018 ; Marelli et al . , 2014 ; Dagan , Glickman ,   and Magnini , 2005 ) . These datasets have not only   been instrumental for developing and evaluating   NLI models but also have been useful in advanc-   ing many other NLP areas such as : representation   learning ( Conneau et al . , 2017 ) , transfer learning(Pruksachatkun et al . , 2020 ) and multi - task learn-   ing ( Liu et al . , 2019a ) .   However , despite their usefulness , none of the   existing NLI datasets is related to scientiﬁc text   that is found in research articles . The vocabulary   as well as the structure and formality used in sen-   tences in scientiﬁc articles are very different from   the sentences used in the everyday language . More-   over , the scientiﬁc text captured in research papers   brings additional challenges and complexities not   only in terms of the language and its structure but   also the inferences that exist in it which are not   available in the existing NLI datasets . For example ,   a sentence can present the reasoning behind the   conclusion made in the previous sentence , while   other sentences indicate a contrast or entailment   with the preceding sentence . These inferences are   crucial for understanding , analyzing , and reason-   ing over scientiﬁc work ( Luukkonen , 1992 ; Kuhn ,   2012 ; Hall , Jurafsky , and Manning , 2008 ) . There-   fore , ideally , the scientiﬁc language inference mod-   els should be evaluated on datasets which capture   these inferences and the particularities seen only in   scientiﬁc text .   To this end , we seek to enable deep learning for   natural language inference over scientiﬁc text by   introducing SNLI , a large dataset of 107;412   sentence pairs extracted from scientiﬁc papers re-   lated to NLP and computational linguistics ( CL )   and present a comprehensive investigation into the   inference types that occur frequently in scientiﬁc   text . To capture the inference relations which are   prevalent in scientiﬁc text but are unavailable in   the existing NLI datasets , we introduce two new   classes — C andR . We   create SNLI by harnessing cues in our data in   the form of linking phrases between contiguous   sentences , which are indicative of their semantic re-   lations and provide a way to build a labeled dataset   using distant supervision ( Mintz et al . , 2009 ) . Dur-7399   ing training , we directly utilize these ( potentially   noisy ) sentence pairs , but to ensure a realistic eval-   uation of the NLI models over scientiﬁc text , we   manually annotate 6,000 sentence pairs . These   clean pairs are used in two splits , 2,000 pairs for   development and hyper - parameter tuning and 4,000   pairs for testing . Table 1 shows examples from our   dataset corresponding to all of our four classes .   We evaluate SNLI by experimenting with tra-   ditional machine learning models using lexical   and syntactic features , neural network models —   BiLSTM , CBOW , CNN , and pre - trained language   models — BERT ( Devlin et al . , 2019 ) , SciBERT   ( Beltagy , Lo , and Cohan , 2019 ) , RoBERTa ( Liu   et al . , 2019b ) , and XLNet ( Yang et al . , 2019 ) . Our   ﬁndings suggest that : ( 1 ) SNLI is harder to clas-   sify than other datasets for NLI ; ( 2 ) Lexical fea-   tures are not enough for a model to achieve satisfac-   tory performance on SNLI and deep semantic   understanding is necessary ; ( 3 ) SNLI is well   suited for evaluating scientiﬁc NLI models ; and ( 4 )   Our best performing model based on XLNet shows   78:18 % Macro F1 and 78:23 % accuracy illustrat-   ing that SNLI is a challenging new benchmark .   2 Related Work   To date , several datasets exist for NLI of varying   size , number of labels , and degree of difﬁculty .   Dagan , Glickman , and Magnini ( 2006 ) introduced   the RTE ( Recognizing Textual Entailment ) dataset   of text - hypothesis pairs from the general news do-   main and considered two labels : entailment or no-   entailment ( i.e. , a hypothesis is true or false given   a text ) . The RTE dataset is paramount in develop - ing and advancing the entailment task . The SICK   ( Sentences Involving Compositional Knowledge )   dataset introduced by Marelli et al . ( 2014 ) was cre-   ated from two existing datasets of image captions   and video descriptions . SICK consists of sentence   pairs ( premise - hypothesis ) labeled as : entailment ,   contradiction , or neutral . Despite being instrumen-   tal in the progress of NLI , both RTE and SICK   datasets are less suitable for deep learning models   due to their small size .   In recent years , SNLI ( Bowman et al . , 2015 ) and   MNLI ( Williams , Nangia , and Bowman , 2018 ) are   the most popular datasets for training and evalu-   ating NLI models , in part due to their large size .   Similar to SICK , SNLI is derived from an im-   age caption dataset where the captions are used   as premises and hypotheses are created by crowd-   workers , with each sample being labeled as : en-   tailment , contradiction , or neutral . MNLI is cre-   ated in a similar fashion to SNLI except that the   premises are extracted from sources such as face-   to - face conversations , travel guides , and the 9/11   event , to make the task more challenging and suit-   able for domain adaptation . More recently , Nie   et al . ( 2020 ) released ANLI which was created in   an iterative adversarial manner where human anno-   tators were used as adversaries to provide sentence   pairs for which the state - of - the - art models make   incorrect predictions . Unlike the datasets speciﬁc   to classifying the relationships between two sen-   tences , Zellers et al . ( 2018 ) combined NLI with   commonsense reasoning to introduce a new task   of predicting the most likely next sentence from   a number of options along with their new dataset7400called SWAG which was also created with an ad-   versarial approach . However , different from ANLI ,   theSWAG approach was automatic . All these   datasets have been widely used for evaluating NLU   models and many of them appear in different NLU   benchmarks such as GLUE ( Wang et al . , 2018 )   and S GLUE ( Wang et al . , 2019 ) .   Heretofore , Khot , Sabharwal , and Clark ( 2018 )   created the only NLI dataset related to science .   Their dataset , STwas derived from a school   level science question - answer corpus . As a result ,   the text used in STis very different from the   type of text used in scientiﬁc papers . Furthermore ,   the sentence pairs in STare classiﬁed into   one of two classes : entailment or no - entailment .   Thus , STdoes not cover all the inference re-   lationships necessary to understand scientiﬁc text .   In other lines of research , discourse cues , e.g. ,   linking phrases have been previously used to ex-   tract inter - sentence and/or inter - clause semantic re-   lations in discourse parsing ( Hobbs , 1978 ; Webber   et al . , 1999 ; Prasad et al . , 2008 ; Jernite , Bowman ,   and Sontag , 2017 ; Nie , Bennett , and Goodman ,   2019 ) , causal inference ( Do , Chan , and Roth , 2011 ;   Radinsky , Davidovich , and Markovitch , 2012 ; Li   et al . , 2020 ; Dunietz , Levin , and Carbonell , 2017 )   and why - QA ( Oh et al . , 2013 ) . However , none of   the aforementioned bodies of research investigates   these relations in scientiﬁc text , nor do they exploit   the discourse cues to create NLI datasets . Further-   more , discourse parsing studies a broader range of   semantic relations , many of which are unrelated to   the task of NLI while causal inference and why-   QA are limited to only cause - effect relations . In   contrast to these tasks , we focus on the semantic   relations which are either relevant to the task of   NLI or highly frequent in scientiﬁc text and lever-   age linking phrases to create the ﬁrst ever scientiﬁc   NLI dataset , which we call SNLI .   3 SNLI : A New Corpus for NLI   In order to better understand the inter - sentence rela-   tionships that exist in scientiﬁc text , we started the   process of creating our dataset by perusing through   scientiﬁc literature with the intent of ﬁnding clues   that are revealing of those relationships . We found   that to have a coherent structure , authors often use   different linking phrases in the beginning of sen-   tences , which is indicative of the relationship with   the preceding sentence . For example , to elaborate   or make something speciﬁc , authors use linkingphrases such as “ In other words ” or “ In particular , ”   which indicate that the sentence supports or entails   the previous sentence . We also found that some   linking phrases are used to indicate additional rela-   tionships that are prevalent in scientiﬁc text but are   not captured in the existing NLI datasets . For in-   stance , when a sentence starts with “ Therefore ” or   “ Thus , ” it indicates that the sentence is presenting   a conclusion to the reasoning in the previous sen-   tence . Similarly , the phrase “ In contrast ” is used to   indicate that the sentence is contrasting what was   said in the previous sentence .   Therefore , inspired by the framework of dis-   course coherence theory ( Hobbs , 1978 ; Webber   et al . , 1999 ; Prasad et al . , 2008 ) that character-   izes the inferences between discourse units , we   extend the NLI relations commonly used in prior   NLI work — entailment , contradiction , and seman-   tic independence — to a set of inference relations   that manifest in scientiﬁc text — contrasting , reason-   ing , entailment , and semantic independence ( § 3.1 ) .   In order to create a large training set with mini-   mal manual effort , we employ a distant supervi-   sion method based on linking phrases that are com-   monly used in scientiﬁc writing and are indicative   of the semantic relationship between adjacent sen-   tences ( § 3.2 ) . We avoid the noise incurred by the   distant supervision method in our development and   test sets by manually annotating these sets ( § 3.3 ) .   3.1 Inference Classes   We deﬁne the inference classes used to create our   dataset in this section .   3.1.1 C   Our C class is an extension of   theC class in the existing NLI   datasets . With this class , in addition to contradict-   ing relations between sentences in a pair , we aim   to capture inferences that occur when one sentence   mentions a comparison , criticism , juxtaposition , or   a limitation of something said in the other sentence .   We can see an example of a sentence pair from our   C class in Table 1 . Here , the authors   discuss how their work differs from the other work   mentioned in the ﬁrst sentence thereby making a   comparison between the two works .   3.1.2 R   The examples where the ﬁrst sentence presents the   reason , cause , or condition for the result or con-   clusion made in the second sentence are placed in7401   ourR class . In Table 1 , we can see an   example where the authors mention that they use a   multi - reference corpus for evaluation in the second   sentence and provide the reason behind it in the   ﬁrst sentence .   3.1.3 E   OurE class includes the sentence pairs   where one sentence generalizes , speciﬁes or has an   equivalent meaning with the other sentence . An   example from this class can be seen in Table 1 .   In the example , the second sentence is specify-   ing the proposed direction mentioned in the ﬁrst   sentence making the pair suitable for our E - class .   3.1.4 N   The N class includes the sentence pairs   which are semantically independent . We can see   an example from this class in Table 1 . Here , the   ﬁrst sentence discusses the span of the literature   of a particular topic , whereas the second sentence   mentions the challenges of handling abstract words   in certain tasks . Therefore , the sentences are se-   mantically independent of each other .   3.2 Training Set Creation   We construct our training set from scientiﬁc papers   on NLP and computational linguistics available   in the ACL Anthology , published between 2000   and 2019 ( Bird et al . , 2008 ; Radev , Muthukrishnan ,   and Qazvinian , 2009 ) . For extracting textual data   from the PDF papers , we use GROBIDwhich is   a popular tool for parsing PDF ﬁles . We employ   the following distant supervision technique on the   extracted text to select and label the sentence pairs .   We create a list of linking phrases which are   indicative of the semantic relationship between thesentence they occur in and the respective previous   sentence . We then group these linking phrases   into three classes based on the type of relationship   indicated by each of them . The linking phrases   and their assigned class can be seen in Table 2 .   We select the sentences which start with any of   these phrases from each paper and include them   in our dataset as hypotheses or second sentences ;   we include their respective preceding sentences   as the premises or ﬁrst sentences . Each sentence   pair is labeled based on the class assigned to the   linking phrase present in the second sentence , e.g. ,   if the second sentence starts with “ In contrast ” , the   sentence pair is labeled as C . After   assigning the labels , we delete the linking phrases   from the second sentence of each pair to ensure   that the models can not get any clues of the ground   truth labels just by looking at them . We also pair   a large number of randomly selected sentences for   our N class using three approaches :   •BR : Two completely random sen-   tences which do not contain any linking   phrases are extracted ( both from the same pa-   per ) and are paired together .   •F R : First sentence is random ; sec-   ond sentence is selected randomly from the   other three classes ( both from the same paper ) .   •S R : Second sentence is random ;   ﬁrst sentence is selected randomly from the   other three classes ( both from the same paper ) .   Our choice for including the last two approaches   above was to make the dataset more challenging .   3.3 Benchmark Evaluation Sets Creation   To create our development and test sets , we start   by extracting and labeling sentence pairs using the   same distant supervision approach described in the   previous section from the papers published in 2020   which are available in the ACL anthology . We then   manually annotate a subset of these sentence pairs   in order to make SNLI a suitable benchmark for   evaluation . The annotation process is completed in   two steps , as described below .   First , we manually clean the data by ﬁltering   out the examples which contain too many math-   ematical terms and by completing the sentences   that are broken due to erroneous PDF extraction by   looking at the papers they are from . The second   step of the annotation process is conducted in an7402   iterative fashion . In each iteration , we randomly   sample a balanced subset from the cleaned set of   examples created in the previous step and present   the sentence pair from each example to three expert   annotators . To avoid a performance ceiling due to   lack of context , the annotators are instructed to la-   bel each example based only on the two sentences   in each example . If the label is not clear from the   context available in the two sentences , the instruc-   tion is to label them as unclear . The label with the   majority of the votes from annotators is then cho-   sen as the gold label . No gold label is assigned to   the examples ( 5 % ) which do not have a majority   vote . The examples for which the gold label agrees   with the label assigned based on the linking phrase   are selected to be in our benchmark evaluation   set . We continue the iterations of sampling a bal-   anced set of examples and annotating them until   we have at least 1;500examples from each class in   the benchmark evaluation set . In total , 8;044sen-   tence pairs — 2;011from each class are annotated   among which 6;904have an agreement between   the gold label and the label assigned based on the   linking phrase . Therefore , these 6904 examples are   selected to be in the benchmark evaluation set . The   percentage of overall agreement and the class - wise   agreement between the gold labels and the labels   assigned based on the linking phrases are reported   in the last column of Table 3 . The Fleiss - k score   among the annotators is 0:62which indicates that   the agreement among the annotators is substantial   ( Landis and Koch , 1977 ) .   We randomly select 36 % of the papers in our   benchmark evaluation set to be in our development   set and the rest of the papers are assigned to the   test set . This is done based on our decision to have   at least 500 samples from each class in the devel-   opment set and 1000 samples from each class inthe test set . Splitting the dataset into train , develop-   ment and test sets at paper level instead of sentence   pair level is done to prevent any information leak-   age among the data splits caused by sentences from   one paper being in more than one split .   3.4 Data Balancing   Because of the differences in the frequency of oc-   currence of the linking phrases related to different   classes , our initial dataset was unbalanced in all   three splits . In contrast , the examples in the re-   lated datasets such as SNLI ( Bowman et al . , 2015 )   andMNLI ( Williams , Nangia , and Bowman , 2018 )   are almost equally distributed across their classes .   Therefore , for a fair comparison , we balance our   dataset by downsampling the top three most fre-   quent classes to the size of the least frequent class   in each split . We can see the number of examples   in each class of our SNLI dataset in Table 3 .   3.5 Data Statistics   A comparison of key statistics of SNLI with four   related datasets is also shown in Table 3 .   Dataset Size Although the total size of our   dataset is smaller than SNLI andMNLI , SNLI   is still large enough to train and evaluate deep learn-   ing based NLI models .   Sentence Lengths From Table 3 , we can see that   the average number of words in both premise and   hypothesis is higher in SNLI compared with the   other datasets . This reﬂects the fact that sentences   used in scientiﬁc articles tend to be longer than the   sentences used in everyday language .   Sentence Parses Similar to the related datasets ,   we parse the sentences in SNLI by using the   Stanford PCFG Parser ( 3.5.2 ) ( Klein and Manning,7403   2003 ) . We can see that 97 % of both ﬁrst and sec-   ond sentences have parses with an ‘S ’ root which   is higher than the sentences in SNLI and very com-   petitive with the other datasets . This illustrates that   most of our sentences are syntactically complete .   Token Overlap We report the average percent-   age of tokens occurring in hypotheses which over-   lap with the tokens in their premises ( Table 3 ) . We   observe that the overlap percentage in SNLI is   much lower compared to the other datasets . There-   fore , our dataset has low surface - level lexical pat-   terns revealing the relationship between sentences .   4 SNLI Evaluation   We evaluate our dataset by performing three sets of   experiments . First , we aim to understand the difﬁ-   culty level of SNLI compared to related datasets   ( § 4.1 ) . Second , we investigate a lexicalized clas-   siﬁer to test whether simple similarity based fea-   tures can capture the particularities of our rela-   tions and potentially perform well on our dataset   ( § 4.2 ) . Third , we experiment with traditional ma-   chine learning models , neural network models and   transformer based pre - trained language models to   establish strong baselines ( § 4.3 ) .   4.1 SNLI vs. Related Datasets   To evaluate the difﬁculty of SNLI , we compare   the performance of a BiLSTM ( Hochreiter and   Schmidhuber , 1997 ) based classiﬁer on our dataset   and four related datasets : SICK , SNLI , MNLI   andST . The architecture for this model is   similar to the BiLSTM model used by Williams ,   Nangia , and Bowman ( 2018 ) . Precisely , the sen-   tence level representations SandSare derived   by sending the embedding vectors of the words in   each of the sentences in a pair through two sep-   arate BiLSTM layers and averaging their hidden   states . The context vector Sis calculated using   the following equation :   S= [ S;S;S  S;S S ] ( 1)Here , the square brackets denote a concatenation   operation of vectors and  and are element - wise   multiplication and subtraction operators , respec-   tively . Sis sent through a linear layer with Relu   activation which is followed by a softmax layer to   obtain the ﬁnal output class .   Implementation details We pre - process the in-   put sentences by tokenizing and stemming them   using the NLTK tokenizerand Porter stemmer ,   respectively . Any stemmed token which occurs less   than two times in the training set is replaced with   an [ UNK ] token . We use 300D Glove embeddings   ( Pennington , Socher , and Manning , 2014 ) to rep-   resent the tokens which are allowed to be updated   during training . The hidden size for the BiLSTM   models is 300 . The batch size is set at 64 and the   models are trained for 30 epochs where we opti-   mize a cross - entropy loss using Adam optimizer   ( Kingma and Ba , 2014 ) with an initial learning   rate of 0:001 . We employ early stopping with a   patience size 10 where the Macro F1 score of the   development set is used as the stopping criteria .   Since SICK does not have a development split , we   randomly select 10 % of its training examples to   be used as the development set . Similarly , since   MNLI does not have a publicly available test split ,   we consider its development split as the test split   and we randomly select 10;000samples from   the training set to be used as the development set .   We can see the performance of this model on   different datasets in Table 4 . We ﬁnd the following :   SNLI is more challenging than other related   datasets . The BiLSTM model shows a much   lower performance for SNLI compared with the   other datasets . These results indicate that the task   our dataset presents is more challenging compared   to other datasets . As we have seen in Table 3 , there   is a substantial amount of discrepancy in sentence   lengths between SNLI and the other datasets .   The longer sentences in our dataset make it harder   for the models to retain long distance dependencies ,   which result in lower performance . Furthermore ,   our dataset has low surface - level lexical cues and   exhibits complex linguistic patterns that require a   model to be less reliant on lexical cues but instead   learn deep hidden semantics from text.7404   4.2 Lexical Similarity vs. Semantic   Relationship   To verify that the examples in our dataset can not   be classiﬁed based only on syntactic and lexical   similarities , we explore a simple lexicalized clas-   siﬁer similar to ( Bowman et al . , 2015 ) . We train a   classiﬁer using different combinations of the fol-   lowing features : ( 1 ) the second sentence ’s BLEU   ( Papineni et al . , 2002 ) score with respect to the ﬁrst   sentence with an n - gram range of 1 to 4 ; ( 2 ) the   difference in length between the two sentences in   a pair ; ( 3 ) overlap of all words , just nouns , verbs ,   adjectives , or adverbs - both the actual number and   the percentage over possible overlaps ; and ( 4 ) un-   igrams and bigrams from the second sentence as   indicator features . We compare the performance of   these models on our dataset and the SICK dataset   because given the small size of SICK , this is espe-   cially suitable for this kind of models . The results   can be seen in Table 5 . We observe the following :   Semantic understanding is required to perform   well on SNLI . The lexicalized model fails to   achieve satisfactory results on SNLI even when   all features are combined . Both Macro F1 and   accuracy are much lower for our dataset than SICK .   This means that without actually understanding the   content in the sentences in SNLI , a model can not   successfully predict their relationship .   4.3 SNLI Baselines   To establish baselines on our dataset , we consider   three types of models : a traditional machine learn-   ing model , neural network models , and pre - trained   language models .   Traditional Machine Learning Model We con-   sider the lexicalized classiﬁer using all four features   described in § 4.2 as a baseline on our dataset .   Neural Network Models We experiment with   three neural models to get the sentence level rep-   resentations for each sentence in a pair : ( a ) BiL - STM - word embeddings are sent through a BiL-   STM layer and the hidden states are averaged ; ( b )   CBOW - word embedding vectors are summed ; ( c )   CNN - 64 convolution ﬁlters of widths [ 3 , 5 , 9 ]   on the word embeddings are applied , the outputs   of which are mean pooled to get a single vector   representation from the ﬁlters of each of the three   widths . These three vectors are then concatenated   to get the sentence level representation .   For all three models , the sentence level represen-   tations are combined as in Eq . 1 . The obtained   representations are ﬁrst sent through a linear layer   with Relu activation followed by softmax for clas-   siﬁcation ( i.e. , project them with a weight matrix   W2R ) . The hyperparameters and other im-   plementation details are the same as for the BiL-   STM model described in § 4.1 .   Pre - trained Language Models We ﬁne - tune   four transformer based pre - trained language mod-   els : ( a ) BERT ( Devlin et al . , 2019 ) - pre - trained by   masked language modeling ( MLM ) on BookCor-   pus ( Zhu et al . , 2015 ) and Wikipedia ; ( b ) SciBERT   ( Beltagy , Lo , and Cohan , 2019 ) - a variant of BERT   pre - trained with a similar procedure but exclusively   on scientiﬁc text ; ( c ) RoBERTa ( Liu et al . , 2019b ) -   an extension of BERT which was pre - trained using   dynamic masked language modeling , i.e. , unlike   BERT , different words were masked in each epoch   during training . It was also trained for a longer   period of time on a larger amount of text compared   with BERT ; and ( d ) XLNet ( Yang et al . , 2019 ) -   pre - trained with a “ Permutation Language Mod-   eling ” objective instead of MLM . We employ the   base variants of each of these models using the hug-   gingface transformers library . The input sequence   for these models is derived by concatenating the   two sentences in a pair with a [ SEP ] token in be-   tween . The [ CLS ] token is then projected with a   weight matrix W2Rby sending it as the input   to a softmax layer to get the output class . We ﬁne-   tune each transformer based model for 5epochs   where we minimize the cross - entropy loss using   Adam optimizer ( Kingma and Ba , 2014 ) with an   initial learning rate of 2e 5 . Early stopping with   a patience size 2 is employed .   The experiments are run on a single Tesla V10   GPU . The transformer based models took approx-   imately four hours to train and the traditional ma-   chine learning and neural network models were   trained in less than one hour . We run each experi-   ment three times with different random seeds and7405   report the average and standard deviation of the F1   scores for each of the four classes , their Macro av-   erage and overall accuracy in Table 6 . Our ﬁndings   are discussed below .   Transformer based models consistently outper-   form the traditional models The transformer   based models have a very high performance gap   with the traditional lexicalized and neural models .   Their better performance can be attributed to their   superior design for capturing the language seman-   tics and their pre - training on large amounts of texts .   More sophisticated pre - training methods lead   to better performance RoBERTa and XLNet   are created by addressing different limitations of   BERT . Both of these models show a better perfor-   mance than BERT on our dataset . Therefore , the   progress made in these two models for better NLU   capability is reﬂected by the results on SNLI .   This proves that SNLI can be used as an addi-   tional resource for tracking the progress of NLU .   Pre - training on domain speciﬁc text helps to im-   prove classiﬁcation performance The results   show that SciBERT consistently outperforms   BERT on SNLI . This is because unlike BERT ,   SciBERT was pre - trained exclusively on scientiﬁc   text . Hence , it has a better capability to under-   stand the text in the scientiﬁc domain . We see that   RoBERTa and XLNet show slightly better perfor-   mances than SciBERT despite being pre - trained   on non - scientiﬁc text , just like BERT . However ,   it should be noted that these differences in per-   formance are not statistically signiﬁcant . More-   over , both RoBERTa and XLNet were created by   modifying the training procedure of BERT to fur-   ther improve the performance , whereas SciBERT   is just a plain BERT model pre - trained on scien-   tiﬁc text . Even without any modiﬁcations to the   training procedure , SciBERT is able to perform   similarly to these models proving the advantage of   pre - training on domain speciﬁc text and suitability   of our dataset for evaluating scientiﬁc NLI models .   5 Analysis   Research has shown that some stylistic and annota-   tion artifacts are present ( only in the hypotheses ) in   NLI datasets created using crowdsource annotators   ( Gururangan et al . , 2018 ) . To verify that the models   do not learn similar spurious patterns in our dataset   and predict the labels without understanding the   semantic relation between the sentences , we start   our analysis by experimenting with only the second   sentence as the input to BERT and SciBERT mod-   els . Next , to intuitively understand the errors made   by the models , we perform a qualitative analysis   of the predictions made by the SciBERT model   on100randomly selected examples from our test   set . Finally , we show that the N examples   extracted with F R andS R   approaches are harder to classify than the examples   extracted with BR .   Spuriosity Analysis A comparison between the   only second sentence models and the models with   both sentences concatenated as the input can be   seen in Table 7 . Clearly , as we can see from the7406   Extraction Approach   table , there is a substantial amount of performance   decrease when only the second sentence is used as   input . Therefore , in order to perform at the optimal   level , both sentences are required for the models to   make the correct inference by learning the semantic   relation between them .   Qualitative Error Analysis We ﬁnd that a ma-   jor reason behind the wrong predictions is a lack   of domain speciﬁc knowledge . For example , in   the ﬁrst sentence pair in Table 8 , without the do-   main knowledge that the number of parameters in a   model affects the performance , one will not be able   to make the correct inference . We also ﬁnd that   the model is prone to making mistakes for longer   sentences . This issue is exempliﬁed by the second   sentence pair in Table 8 .   Neutral Class Performance Analysis We can   see a plot of the accuracy shown by SciBERT on   N pairs of our test set extracted with differ-   ent approaches in Figure 1 . Indeed , the examples   in which one sentence comes from one of the other   three classes are harder to classify.6 Conclusion & Future Directions   In this paper , we introduced SNLI , the ﬁrst nat-   ural language inference dataset on scientiﬁc text   created with our novel data annotation method . We   manually annotated a large number of examples to   create our benchmark test and development sets .   Our experiments suggest that SNLI is harder to   classify than existing NLI datasets and deep se-   mantic understanding is necessary for a model to   perform well . We establish strong baselines and   show that our dataset can be used as a challenging   benchmark to evaluate the progress of NLU models .   In the future , we will leverage knowledge bases to   improve the models ’ ability to understand scientiﬁc   text . We make our code and the SNLI dataset   available to further research in scientiﬁc NLI .   Acknowledgements   This research is supported by NSF CAREER award   1802358 and NSF CRI award 1823292 to Cornelia   Caragea . Any opinions , ﬁndings , and conclusions   expressed here are those of the authors and do   not necessarily reﬂect the views of NSF . We thank   AWS for computing resources . We also thank our   anonymous reviewers for their constructive feed-   back , which helped improve our paper .   References740774087409