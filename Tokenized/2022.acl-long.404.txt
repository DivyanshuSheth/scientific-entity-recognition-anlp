  Weijie Chen , Yongzhu Chang , Rongsheng Zhang , Jiashu Pu   Guandan Chen , Le Zhang , Yadong Xi , Yijiang Chenand Chang SuSchool of Informatics , Xiamen University , Xiamen , ChinaFuxi AI Lab , NetEase Inc. , Hangzhou , China   Abstract   Simile interpretation ( SI ) and simile genera-   tion ( SG ) are challenging tasks for NLP be-   cause models require adequate world knowl-   edge to produce predictions . Previous works   have employed many hand - crafted resources   to bring knowledge - related into models , which   is time - consuming and labor - intensive . In   recent years , pre - trained language models   ( PLMs ) based approaches have become the de-   facto standard in NLP since they learn generic   knowledge from a large corpus . The knowl-   edge embedded in PLMs may be useful for   SI and SG tasks . Nevertheless , there are few   works to explore it . In this paper , we probe   simile knowledge from PLMs to solve the SI   and SG tasks in the uniﬁed framework of sim-   ile triple completion for the ﬁrst time . The   backbone of our framework is to construct   masked sentences with manual patterns and   then predict the candidate words in the masked   position . In this framework , we adopt a sec-   ondary training process ( Adjective - Noun mask   Training ) with the masked language model   ( MLM ) loss to enhance the prediction diver-   sity of candidate words in the masked position .   Moreover , pattern ensemble ( PE ) and pattern   search ( PS ) are applied to improve the quality   of predicted words . Finally , automatic and hu-   man evaluations demonstrate the effectiveness   of our framework in both SI and SG tasks .   1 Introduction   The simile , which is a special type of metaphor ,   is deﬁned as a ﬁgurative expression in which two   fundamentally different things are explicitly com-   pared , usually using “ like ” or “ as ” ( Israel et al . ,   2004 ; Zeng et al . , 2020 ) . It is widely used in litera-   ture because it can inspire the reader ’s imagination   ( Paul , 1970 ) by giving a vivid and unexpected anal-   ogy between two objects with similar attributes . Figure 1 : In the form of triple , the tasks of Simile In-   terpretation and Simile Generation can be uniﬁed into   Simile Triple Completion .   A simile sentence usually contains three key ele-   ments : the tenor , the attribute and the vehicle ,   which can be deﬁned in the form of a triple ( tenor ,   attribute , vehicle ) ( Song et al . , 2021 ) . For example ,   the simile sentence “ Love is as thorny as rose ” can   be extracted as the triple ( love , thorny , rose ) , where   the tenor is “ love ” , the vehicle is “ rose ” , and the   attribute is “ thorny ” . Note that a simile triple can   produce different simile sentences with different   templates . For the example triple above , the simile   sentences can be also constructed as “ love is thorny   like rose " with the pattern “ tenor isattribute like   vehicle " .   The study of simile is beneﬁt to many down-   stream tasks , like sentiment analysis ( Rentoumi   et al . , 2012 ) , question answering ( Zheng et al . ,   2020 ) , writing polishment ( Zhang et al . , 2021 ) and   creative writing ( Gero and Chilton , 2019 ) . Simile   interpretation ( SI ) ( Qadir et al . , 2016 ; Su et al . ,   2016 ) and simile generation ( SG ) ( Yu and Wan ,   2019 ) are the two important tasks in the study of   simile ( Tong et al . , 2021 ) . The SI task is to ﬁnd   suitable attributes as a mediator between the tenor5875and vehicle . Likewise , the SG task is to select a   proper vehicle for the tenor with the given attri-   bution . And these two tasks can be uniﬁed into   the form of simile triple completion ( STC ) ( Song   et al . , 2021 ) as shown in Figure 1 .   Previous works on the SI and SG tasks relied on   a limited training corpus or labor - intensive knowl-   edge base , which leads to an upper limit on the   diversity of results . ( Song et al . , 2021 ) collected   sentences containing comparator words from a Chi-   nese essays corpus and manually annotated them to   obtain the simile triple . Some works ( Stowe et al . ,   2021 ; Gero and Chilton , 2019 ; Veale et al . , 2016 )   relied on a knowledge base such as ConceptNet ,   FrameNet , which are scarce to other languages   because it is time - consuming and labor - intensive   to construct such a knowledge base . It is notable   that pre - trained language models ( PLMs ) ( Devlin   et al . , 2019 ; Radford et al . , 2019 ) have made signif-   icant progress recently in many NLP tasks since it   learns generic knowledge such as grammar , com-   mon sense from a large corpus ( Davison et al . ,   2019 ; Liu et al . , 2021a , b ) . Considering the suf-   ﬁcient existence of simile in the large corpus , it ’s   reasonable to assume that PLMs are equipped with   rich knowledge of similes during the pre - training   stage . However , few works have explored directly   probing the knowledge of simile from the PLMs .   In this paper , we propose a uniﬁed framework to   solve the SI and SG tasks by mining the knowledge   in PLMs , which does not require ﬁne - labeled train-   ing data or knowledge graphs . The backbone of   our method is to construct masked sentences with   manual patterns from an incomplete simile triple ,   and then use language models with MLM heads   to predict the masked words over the task - speciﬁc   vocabulary . We take the Kwords with the highest   probability as the result words . However , there are   problems with this crude approach . Firstly , the pre-   dicted words should be creative and surprised for   the simile sentence . On the contrary , the PLMs tend   to predict common words ( e.g. , good , bad ) with a   higher probability . To address this issue , we in-   troduce a secondary pre - training stage - Adjective-   Noun mask Training ( ANT ) , where only the noun   or adjective contained in the amod dependencies   ( Nivre et al . , 2017 ) could be masked in the MLM   training process and the number of words masked   times are limited . Secondly , the words predictedby MLM have a preference for different patterns .   For this reason , we employ a pattern ensemble to   obtain high - quality and robust results . Finally , we   also introduce a prompt - search method to improve   the quality of the simile component predictions .   Our main contributions are as follows :   •We propose a uniﬁed framework to solve both   the simile interpretation ( SI ) and simile gener-   ation ( SG ) tasks based on pre - trained models .   To the best of our knowledge , it is the ﬁrst   work to introduce pre - trained language mod-   els to unify these tasks .   •We propose a secondary pre - training stage   that effectively improves the prediction diver-   sity . Further , we employ the pattern - ensemble   and pattern - search approaches to obtain better   results .   •We compare our models on both automated   metrics and manual measures , and the results   show that our approach outperforms the base-   lines in terms of diversity and correctness .   2 Related Work   2.1 Simile Interpretation and Generation   Simile interpretation and simile generation are the   two main directions of the simile study ( Yu and   Wan , 2019 ) . The SI task ( Shutova , 2010 ; Su et al . ,   2017 ) aims at ﬁnding a suitable attribute when   given the tenor and vehicle , while the SG task   ( Yu and Wan , 2019 ) is to ﬁnd a proper vehicle   when given the tenor and its attribute . For sim-   ile interpretation , some works ( Zheng et al . , 2020 ;   Bar et al . , 2018 ; Xiao et al . , 2016 ; Gagliano et al . ,   2016 ; Qadir et al . , 2016 ) applied word vectors   to decide which attribute words can ﬁt into the   tenor and vehicle domains and some other works   ( Gero and Chilton , 2019 ; Stowe et al . , 2021 ) intro-   duced knowledge base ( Baker et al . , 1998 ; Speer   et al . , 2017 ) to help ﬁnd intermediate attributes .   For simile generation , some works focused on   constructing limited training corpus to ﬁnetune a   sequence - to - sequence model ( Lewis et al . , 2020 )   by pattern - based ( Zhang et al . , 2021 ; Bollegala and   Shutova , 2013 ) or knowledge - based approaches   ( Chakrabarty et al . , 2020 , 2021 ; Stowe et al . , 2021 ) .   There are also some works ( Abe et al . , 2006 ;   Hervás et al . , 2007 ; Zheng et al . , 2020 ) that focused   more on the relationships between concepts ( i.e. ,5876   tenor and vehicle ) and attribute . However , our pa-   per carries out the task of simile interpretation and   generation uniformly in the form of simile triples .   And instead of extracting the simile triples from the   limited corpus using designed templates or a hand-   crafted knowledge base , we probe simile - related   knowledge from PLMs .   2.2 Explore knowledge from PLMs   Pre - trained language models such as Bert and   GPT ( Devlin et al . , 2019 ; Radford et al . , 2019 )   are trained on the large - scale unlabeled corpus .   Many recent works ( Manning et al . , 2020 ; Ettinger ,   2020 ; Petroni et al . , 2019 ; Shin et al . , 2020 ; Ha-   viv et al . , 2021 ; Jiang et al . , 2020 ; Zhong et al . ,   2021 ; Wang et al . , 2022a , b ; Li and Liang , 2021 )   focused on exploring the rich knowledge embed-   ded in these PLMs . Manning et al . ( 2020 ) and   Ettinger ( 2020 ) learned the syntactic and semantic   knowledge from PLMs . Among these works , one   branch of works(Petroni et al . , 2019 ; Shin et al . ,   2020 ; Haviv et al . , 2021 ; Jiang et al . , 2020 ) de-   signed discrete patterns to explore the common   sense and world knowledge embedded in PLMs . In   addition , some works ( Zhong et al . , 2021 ; Li and   Liang , 2021 ) probed knowledge by searching the   best - performing continuous patterns in the space   of embedding . Inspired by the above works , in   this paper , we probe the knowledge of simile in   these pre - trained models and further apply pattern   ensemble and pattern search to improve the results.3 Backbone   3.1 Simile Triple Completion   As shown in Figure 1 , the simile triple complete   consists of two tasks : simile interpretation ( SI )   and simile generation ( SG ) . Each simile sentence   can be abstracted into the form of a triple . There-   fore , we deﬁne a triple : ( T;A;V ) , whereT , V   are mainly nouns or noun phrases and represent   the tenor and vehicle in the simile sentence , re-   spectively . Ais the attribute in simile sentences ,   which is an adjective . If the Ais None in the triple ,   i.e.(T;None;V ) , we deﬁne it as the simile in-   terpretation task . Similarly , if the Vis None , i.e.   ( T;A;None ) , this will be the task of simile gener-   ation .   3.2 Masked Language Model   The masked language model ( MLM ) ( Devlin et al . ,   2019 ; Taylor , 1953 ) randomly masks the words in   the input sentence and feeds the masked sentence   into the pre - trained models to make predictions   by other visible words . For example , given a sen-   tences= [ w;w;:::;w;:::;w ] , where thew   means thei - th word in the sentence . We can ran-   domly mask sand feed the masked sequence esinto   the PLMs e.g. BERT ( Devlin et al . , 2019 ) to obtain   the masked words by Equation :   es = f(s;i;v ) ( 1 )   P = f(es ) ( 2)5877where thevmeans the V ocabulary for pre-   trained models , and the idenotes the position of   the masked word in Equation 1 . The is the pa-   rameters of PLMs in Equation 2 . We can select the   word corresponding to the maximum probability in   Pas the output of the model .   3.3 Probe Simile Knowledge with MLM   To probe the simile knowledge in pre - trained   masked language models , the intuitive solution is :   ( 1 ) Construct a sentence that contains the simile   triple in Section 3.1 with the given pattern . ( 2 )   Mask the attributeAor vehicleVin this simile sen-   tence . ( 3 ) Predict the words in the masked position   with MLM . For example , when given a pattern The   Tis asAasV , the input sentence of MLM is The   Tis as [ MASK ] asVfor the SI task while TheT   is asVas [ MASK ] for the SG task .   To formulate this problem , we deﬁne the pattern   function asp (  ) , where  2fSG;SIg . The pre-   trained MLM is denoted as Mand the predicted   distribution Qover vocabulary Vcan be formu-   lated as :   Q(wjp (  ) ) = exp(M(wjp (  ) ) ) Pexp(M(wjp (  ) ) ) ( 3 )   4 Method   In this section , we will introduce our proposed   method of probing simile knowledge from pre-   trained models . Our method ﬁrst introduces a sec-   ondary pre - training stage - Adjective - Noun mask   Training ( ANT ) based on pre - trained language   models to acquire diverse lexical - speciﬁc words .   Then two modules of pattern ensemble and pattern   search are used to obtain the high - quality predic-   tions . The framework of our method is shown in   Figure 2 in detail .   4.1 Adjective - Noun Mask Training ( ANT )   For the MLM task , pre - trained models prefer to   output high - frequency words as candidate words   since the objective of the training is to minimize   the cross - entropy loss ( Gehrmann et al . , 2019 ) .   However , the components of simile triples are usu-   ally nouns or adjectives and the simile sentences   are appealing due to their creativity and unexpect-   edness . Therefore , to predict more diverse andspeciﬁc words of simile component , we introduce   a secondary pre - training stage - Adjective - Noun   mask Training ( ANT ) that ﬁne - tune the pre - trained   model with specially designed datasets . First , we   utilize trankit ( Nguyen et al . , 2021 ) to construct   the training set by selecting sentences from Book-   Corpus ( Zhu et al . , 2015 ) that contains amodde-   pendencies ( Nivre et al . , 2017 ) . Second , we mask   a word at the end of amod relation , instead of ran-   domly masking , and all words are masked no more   than5times . Finally , the pre - trained model is ﬁne-   tuned on the constructed dataset with MLM loss .   In this way , the pre - trained model will avoid the   bias to high - frequency words and have a higher   probability of generating diverse and novel words .   4.2 Pattern Ensemble ( PE )   Since words predicted by MLM have a preference   for different patterns and only using one pattern   is insufﬁcient , we apply the pattern ensemble to   obtain better performance where different types of   patterns are designed as shown in Table 1 . Speciﬁ-   cally , the class I describes the relationship between   the three - elementT , VandA. However , the simi-   les tend to highlight an obvious attribute between   tenor and vehicle ( Israel et al . , 2004 ) . We further   design the class II and class III to ﬁnd the attribute   corresponding to the tenor and vehicle , respectively .   Finally , the attributes of simile sentences are some-   times omitted and thus the class IV is designed to   deal with this case . Additionally , we also design   three patterns for each class to obtain high - quality   and robust results .   The output distribution Qof pattern ensemble   can be formulated as   Q(wjP ) = 1   jPjXlog(Q(wjp (  ) ) ) ( 4 )   wherePis the set of patterns p (  ) for speciﬁc   task  . Note that though we design four classes   of patterns in Table 1 , some classes of patterns are   not required for the SI or SG task . Speciﬁcally ,   The patterns of Class IV are not used for the SI   task because the attribute Ais missed in Class IV .   Likewise , the patterns of Class III are not used for   the SG task due to the lack of vehicle V.5878   4.3 Pattern Search ( PS )   The prediction of pattern ensemble in Section 4.2   is averaged by adding up the output distributions   of all the patterns . Conversely , the hand - designed   patterns are heuristic , which may lead to subopti-   mal results . Therefore , it is worth studying how   these patterns can be combined to obtain better per-   formance . To solve this problem , we introduce an   approach of pattern search ( PS ) to ﬁnd the best com-   bination of different patterns . Speciﬁcally , given   a simile datasetD , we calculate Equation 4 on   Dby iterating all subsets of the patterns . Finally ,   we select the optimal subset pas the input of   MLM to predict simile components .   5 Experiments   5.1 Dataset   Dataset for ANT : We constructed our train set of   ANT from BookCorpus . We ﬁrst extracted the sen-   tences with length less than 64 and then masked   nouns or adjectives in them based on amod depen-   dencies ( Nivre et al . , 2017 ) . Meanwhile , we limited   the frequency of masked words to less than 5 . Fi-   nally , we got 98k sentences as the dataset of ANT ,   which contains 68k noun - masked sentences and   30k adjective - masked sentences .   Dataset for PE and PS : We evaluate our   method on the dataset proposed in ( Roncero and   de Almeida , 2015 ) . As the samples in Table 2 ,   there are multiple attributes for each ( T , V ) pair .   For example , the pair of ( anger , ﬁre ) has the at-   tributes of dangerous , hot , and red . In addition , we   followed the previous work ( Xiao et al . , 2016 ) to   ﬁlter the dataset by reversing simile triples with   attribute frequencies greater than 4 . Eventually , we   obtain the train set with 533 samples and the test   set with 145 samples . Notice that the train set is theTriple Frequency   ( Anger , Dangerous , Fire ) 8   ( Anger , Hot , Fire ) 8   ( Anger , Red , Fire ) 5   ( Love , Beautiful , Rainbow ) 10   ( Love , Beautiful , Melody ) 2   ( Love , Beautiful , Rose ) 9   Din Section 4.3 used for the pattern search and   the test set is used for evaluating all the approaches   in this paper .   5.2 Implementation Details   Details for ANT : In adjective - nouns mask training ,   we utilized Adam as our optimizer and the learning   rate is 5e-5 . The batch size is set to 32 and the max   sequence length is set to 64 , respectively . Further ,   we utilize the Bert - Largewith 340 M parameters   as the basic model to perform adjective - nouns mask   training and the number of training epoch is 3 .   Vehicle Filtering : For simile generation , we ﬁl-   ter the predicted vehicles that are similar to the   tensor by calculating the semantic similarity with   Glove embedding . For instance , given the sentence   “ The child is as tall as [ MASK ] " , we will ﬁlter out   the word “ father " as its vehicle due to not meeting   the simile deﬁnition . To solve this problem , we   compute the similarity score of the tenor and vehi-   cle and ﬁlter the predicted vehicle whose score is   above the threshold 0.48.5879   5.3 Evaluating the effectiveness of ANT   In this section , we will demonstrate that ANT could   improve the diversity of predicted words for both   the SI and SG tasks . We compare the predicted   results of MLM ( i.e. , Bert ) before and after ANT ,   which use the patterns “ The Tis as [ MASK ] asV "   for the SI task and “ The Tis asAas [ MASK ] " for   the SG task .   Metric : We evaluate the diversity of the MLM   predictions by calculating the proportion of unique   words in the predicted Top Kresults on the test set .   It can be formulated as   p@K = Num ( Unique _ words )   KN(5 )   where theNum ( Unique _ words ) means the   number of unique words , and the N represents size   of the test set .   Result : To illustrate the effectiveness of ANT ,   We evaluate the results on the test set based on   Equation 5 . As shown in Table 3 , the diversity of   predicted words signiﬁcantly improves after ANT   for different p@K , speciﬁcally about 100 % im-   provement for the SI task and about 50 % for the SG   task . Additionally , Figure 3 plots the percentage   of samples on the test set , where a given common   word ( e.g. , good , big , strong ) appears in the list   of the topk= 15;25predicted words . We can   observe that the frequency of common words de-   creases signiﬁcantly after ANT . For example , the   frequency of the common word good decreases   from72:37 % to1:32 % whenk= 15 .   5.4 Evaluating the effectiveness of PE and PS   5.4.1 Baselines   We compare the proposed approaches with the fol-   lowing baseline :   ( 1)Meta4meaning ( Xiao et al . , 2016 ) : It uses   the trained LSA vector representation according to   the degree of abstraction and salience imbalance   to select appropriate attributes . ( 2 ) GEM ( Bar   et al . , 2018 ) : A method calculates the cosine simi-   larity and normalized PMI between each attribute   and tensor / vehicle based on Glove representing to   obtain the best attribute with ranking . ( 3 ) Bert ( De-   vlin et al . , 2019 ) : Directly use pre - trained MLM   to predict the simile component with a single pat-   tern as Section 3.3 . In this paper , we utilize the   bert - large - uncased as the basic pre - trained MLM .   ( 4)ConScore ( Zheng et al . , 2020 ) : A connecting   score is proposed to select an attribute word Afor   TandV.   Our proposed approaches are denoted as :   ( 1)ANT : Perform Adjective - Noun mask Train-   ing based on a pre - trained MLM with the datasets   mentioned in Section 5.1 . ( 2 ) ANT+PE : Based on   ANT , the output distribution over vocabulary is pre-5880Task Method MRR R@5 R@10 R@15 R@25 R@50   Meta4meaning N / A 0.221 0.303 0.339 0.397 0.454   GEM 0.312 0.198 0.254 0.278 0.405 0.562   ConScore 0.078 0.076 0.138 0.172 0.269 0.386   Bert 0.266 0.338 0.428 0.448 0.538 0.641   ANT 0.245 0.310 0.407 0.455 0.510 0.614   ANT+PE 0.241 0.331 0.400 0.448 0.552 0.628SI   ANT+PS+PE 0.270 0.379 0.490 0.524 0.579 0.655   ConScore 0.036 0.055 0.09 0.103 0.145 0.200   Bert 0.064 0.076 0.124 0.159 0.207 0.283   ANT 0.049 0.069 0.117 0.145 0.186 0.303   ANT+PE 0.036 0.034 0.083 0.097 0.131 0.172SG   ANT+PS+PE 0.095 0.124 0.145 0.159 0.214 0.290   dicted by average on all the corresponding patterns   in Table 1 . ( 3 ) ANT+PS+PE : Based on ANT , ﬁrst   the pattern search is to decide which patterns in   Table 1 are applied , and then the pattern ensemble   is used over these selected patterns .   5.4.2 Metrics   We use both automatic evaluation and human eval-   uation to compare our approaches with baselines .   Automatic Evaluation :   ( 1 ) Mean Reciprocal Rank ( MRR ): average on   the reciprocal of the ranking rof label words in   the predicted candidates , denoted as   MRR = 1   NX1   r(6 )   ( 2)R@K : the percentage of the label words ap-   pear in the top Kpredictions . Note that , followingprevious works ( Xiao et al . , 2016 ; Bar et al . , 2018 ) ,   we consider a predicted word as the correct an-   swer if it is a synonym of label word n in WordNet   ( Miller , 1992 ) . It can be formulated as   cor(w ) =(   1w2Synonyms ( L )   0w = 2Synonyms ( L)(7 )   R@K=1   NXPcor(w )   K(8 )   whereKdenotes the list of predicted words , L   denotes the list of label words and Synonyms ( L )   represents the synonyms of a word .   Human Evaluation : To further prove our ap-   proaches are better than baselines , human evalu-   ation is used to evaluate the quality of predicted   simile triples from three levels ( 0 , 1 , 2 ) . 0 - The   triple is unacceptable . 1 - The triple is acceptable .   2 - The triple is acceptable and creative . Given a   simile triple , annotators need to score it according   to their subjective judgment and each triple is an-   notated by three annotators independently . We use   the average score of three annotators as the quality   of a simile triple .   5.4.3 Results   Automatic and Human Evaluation : The results   of both automatic and human evaluation are shown   in Table 4 and Table 5 . The agreement between   annotators is measured using Fleiss ’s kappa (Ran-   dolph , 2005 ) . The value is 0.68 ( substantial agree-   ment ) for the SI task and 0.48 ( moderate agree-   ment ) for the SG task .   From the results , we can conclude5881Task Subset of Patterns MRR R@5 R@10 R@15 R@25   SIfp;pg 0.100 0.126 0.184 0.233 0.281   fp;p;p;p;p;pg 0.095 0.107 0.171 0.203 0.268   fp;p;p;p;p;p;pg0.095 0.099 0.163 0.206 0.274   fp;p;pg 0.094 0.094 0.163 0.203 0.261   SGfp;pg 0.056 0.068 0.105 0.135 0.159   fp;pg 0.056 0.071 0.092 0.120 0.154   fp;p;pg 0.052 0.06 0.105 0.128 0.163   fp;p;pg 0.052 0.058 0.096 0.116 0.137   .   ( 1)For both SI and SG tasks , our proposed ap-   proaches ( i.e. , ANT , ANT+PE , ANT+PS+PE )   signiﬁcantly outperform the baselines on both   automatic and human evaluations . It proves   that our methods not only enhance the diver-   sity of predicted simile components in Section   5.3 but also their quality .   ( 2)Pre - trained MLM - based methods ( i.e. , Bert ,   ANT , ANT+PE and ANT+PS+PE ) perform   better than the traditional methods ( i.e. , GEM ,   Meta4meaning , ConScore ) . It shows the po-   tential of pre - trained models in probing simile   knowledge .   ( 3)Compared ANT with Bert , we found that   though ANT improves the diversity of pre-   dicted words in Table 3 , the average scores   on automatic and human evaluations decrease   because the simile knowledge is not involved   in the ANT training process . However , our   proposed PE and PS compensate for the per-   formance .   ( 4)The scores of automatic evaluation metrics on   the SI task are remarkably higher than the SG   task . Yet , the scores of human evaluation met-   rics are signiﬁcantly lower than on the SG task .   We conjecture that this may be because the   list of candidate words of attribute predicted   by SI are smaller than that of the vehicle for   the SG task . For example , given the SI sample   “ ( Cloud , None , Cotton ) ” , the attribute words   are almost restricted to the physical proper-   ties of the vehicle , such as “ Soft ” , while the   choices of vehicle words are more varied and   unexpected given the SG sample “ ( Cloud , soft ,   None ) ” such as “ cotton , silk , towel" . Discussion for PS : Compared ANT+PS+PE to   ANT+PE , it can be included that pattern search   brings a great improvement to the results on both   automatic and human evaluations . To have a deeper   insight into PS , the pattern subsets with high per-   formance are listed in Table 6 . For the SI task ,   the optimal multi - pattern combination is fp;pg ,   which support the hypothesis proposed by ( Ortony ,   1979 ) considers that the highlighted attribute of a   simile triple is more salient in the vehicle domain   despite it is commonly shared by both tenor and   vehicle domains . Speciﬁcally , pattern pbelonging   to the Class I , models the relationship of all three   simile components while the pattern pbelonging   to Class II requires the candidate words to be the   salient attribute of the vehicle . Similarly , for SG   task , optimal multi - pattern combination is fp;pg ,   which is also a combination of the Class I pattern   and the Class II pattern .   6 Conclusion and Future work   In this paper , from the perspective of simile triple   completion , we propose a uniﬁed framework to   solve the SI and SG tasks by probing the knowl-   edge of the pre - trained masked language model .   The backbone of our method is to construct masked   sentences with manual patterns from an incomplete   simile triple , and then use language models with   MLM heads to predict the masked words . More-   over , a secondary pre - training stage ( the adjective-   noun mask training ) is applied to improve the di-   versity of predicted words . Pattern ensemble ( PE )   and pattern search ( PS ) are further used to improve   the quality of predicted words . Finally , automatic   and human evaluations demonstrate the effective-   ness of our framework in both SI and SG tasks .   In future work , we will continue to study how5882to mine broader or complex knowledge from pre-   trained models , such as metaphor , common sense   and we expect more researchers to perform related   research .   Acknowledgements   This work is supported by the Key Research and   Development Program of Zhejiang Province ( No .   2022C01011 ) and National Natural Science Foun-   dation of China ( Project 61075058 ) . We would   like to thank the anonymous reviewers for their   excellent feedback .   References58835884   A More results of Pattern Search   The more results of Pattern Search are shown in   Table 7 .   B More Prediction   Some results are shown in Table 8 and Table 9.5885Task Subset of Patterns MRR R@5 R@10 R@15 R@25 R@50   fp;pg 0.100 0.126 0.184 0.233 0.281 0.375   fp;p;p;p;p;pg 0.095 0.107 0.171 0.203 0.268 0.377   fp;p;p;p;p;p;pg 0.095 0.099 0.163 0.206 0.274 0.373   fp;p;pg 0.094 0.094 0.163 0.203 0.261 0.366   fp;p;p;p;pg 0.094 0.111 0.165 0.214 0.265 0.373   fp;p;p;p;pg 0.093 0.109 0.171 0.212 0.283 0.368   fp;p;p;p;p;p;pg 0.093 0.090 0.152 0.205 0.263 0.338   fp;p;p;pg 0.093 0.113 0.167 0.210 0.280 0.371   fp;p;p;p;pg 0.093 0.111 0.178 0.218 0.272 0.371   fp;p;p;p;p;pg 0.093 0.105 0.173 0.216 0.283 0.370   fp;p;p;p;p;p;p;pg0.093 0.096 0.156 0.210 0.261 0.347   fp;p;p;p;p;pg 0.093 0.098 0.159 0.223 0.265 0.368   fp;p;p;pg 0.092 0.101 0.163 0.203 0.274 0.366   fp;p;p;pg 0.092 0.099 0.171 0.225 0.285 0.362   fp;p;p;p;pg 0.092 0.105 0.173 0.218 0.280 0.360   fp;p;p;p;p;pg 0.092 0.099 0.158 0.216 0.274 0.360   fp;p;p;p;p;p;pg 0.092 0.094 0.159 0.210 0.270 0.355   fp;p;pg 0.092 0.105 0.169 0.216 0.280 0.381   fp;p;p;p;pg 0.092 0.096 0.173 0.220 0.276 0.368   fp;p;p;p;pg 0.092 0.107 0.165 0.208 0.281 0.371   fp;p;pg 0.091 0.116 0.180 0.220 0.283 0.385   fp;p;p;pg 0.091 0.111 0.174 0.229 0.278 0.364   fp;p;p;pg 0.091 0.103 0.176 0.216 0.291 0.366SI   fp;p;p;p;pg 0.091 0.099 0.165 0.205 0.270 0.358   fp;pg 0.056 0.068 0.105 0.135 0.159 0.223   fp;pg 0.056 0.071 0.092 0.120 0.154 0.225   fp;p;pg 0.052 0.060 0.105 0.128 0.163 0.218   fp;p;pg 0.052 0.058 0.096 0.116 0.137 0.197   fp;p;pg 0.052 0.064 0.094 0.114 0.137 0.203   fp;p;pg 0.050 0.058 0.079 0.099 0.141 0.186   fp;p;pg 0.049 0.051 0.086 0.105 0.131 0.197   fp;p;pg 0.048 0.058 0.096 0.114 0.144 0.208   fp;p;pg 0.048 0.051 0.094 0.109 0.135 0.199   fp;p;p;pg 0.048 0.049 0.092 0.120 0.148 0.208   fp;p;p;pg 0.048 0.054 0.090 0.111 0.137 0.214   fp;p;p;pg 0.048 0.062 0.088 0.105 0.128 0.188   fp;p;pg 0.047 0.062 0.090 0.105 0.133 0.197   fp;p;p;pg 0.047 0.051 0.084 0.113 0.146 0.184   fp;p;p;pg 0.047 0.054 0.083 0.113 0.141 0.188   fp;p;p;pg 0.046 0.058 0.088 0.109 0.133 0.206   fp;p;p;p;pg 0.046 0.054 0.083 0.096 0.131 0.188   fp;pg 0.046 0.053 0.081 0.099 0.122 0.171   fp;p;p;p;pg 0.046 0.058 0.079 0.094 0.114 0.171   fp;p;pg 0.045 0.053 0.084 0.101 0.139 0.208   fp;p;p;p;pg 0.045 0.060 0.084 0.099 0.118 0.169   fp;p;p;p;pg 0.045 0.047 0.083 0.116 0.137 0.184   fp;p;p;pg 0.045 0.049 0.079 0.101 0.133 0.189SG   fp;p;p;pg 0.045 0.045 0.077 0.096 0.133 0.1865886Triple Score   ( anger , burning , ﬁre ) 2.00   ( cities , humid , jungles ) 2.00   ( clouds , ﬂuffy , cotton ) 2.00   ( deserts , hot , ovens ) 2.00   ( exams , tough , hurdles ) 2.00   ( families , powerful , fortresses ) 2.00   ( ﬁngerprints , accurate , portraits ) 2.00   ( highways , crooked , snakes ) 2.00   ( love , pure , ﬂower ) 2.00   ( anger , blazing , ﬁre ) 1.67   ( love , romantic , melody ) 1.67   ( money , valuable , oxygen ) 1.67   ( obligations , binding , shackles ) 1.67   ( teachers , creative , sculptors ) 1.67   ( time , important , money ) 1.67   ( tv , addicted , drug ) 1.67   ( wisdom , inﬁnite , ocean ) 1.67   ( desks , messy , junkyards ) 1.33   ( eyelids , close , curtains ) 1.33   ( god , benevolent , parent ) 1.33   ( music , soothing , medicine ) 1.33   ( skating , relaxing , ﬂying ) 1.33   ( friendship , lovely , rainbow ) 1.00   ( life , challenging , journey ) 1.00   ( love , sweet , ﬂower ) 1.00   ( love , fragile , rose ) 1.00   ( pets , annoying , kids ) 1.00   ( television , attractive , candy ) 1.00   ( women , quiet , cats ) 1.00   ( trust , secure , glue ) 0.67   ( tv , harmful , drug ) 0.67   ( tree trunks , weak , straws ) 0.67   ( trees , sturdy , umbrellas ) 0.67   ( winter , long , death ) 0.33   ( tongues , spicy , ﬁre ) 0.33   ( typewriters , obsolete , dinosaurs ) 0.00   ( time , quick , snail ) 0.00   ( trees , long , umbrellas ) 0.00   ( tv , ineffective , drug ) 0.00   ( tv , unreliable , drug ) 0.00Triple Score   ( clouds , white , cream ) 2.00   ( friendship , colorful , jewelry ) 2.00   ( love , colorful , coral ) 2.00   ( love , shiny , pearl ) 2.00   ( skating , relaxing , noon ) 2.00   ( tv , addictive , drug ) 2.00   ( dreams , clear , crystal ) 1.67   ( friendship , colorful , sunrise ) 1.67   ( love , addictive , coke ) 1.67   ( love , colorful , sunrise ) 1.67   ( music , cure , lullaby ) 1.67   ( clouds , white , pearl ) 1.33   ( dreams , clear , glass ) 1.33   ( exams , challenging , boxing ) 1.33   ( friendship , colorful , pottery ) 1.33   ( knowledge , important , faith ) 1.33   ( love , addictive , alcohol ) 1.33   ( love , colorful , lavender ) 1.33   ( music , cure , art ) 1.33   ( clouds , white , dove ) 1.00   ( desks , messy , nightmare ) 1.00   ( desks , messy , storage ) 1.00   ( highways , long , march ) 1.00   ( knowledge , important , time ) 1.00   ( love , addictive , poison ) 1.00   ( love , colorful , perfume ) 1.00   ( love , colorful , silk ) 1.00   ( music , cure , time ) 1.00   ( skating , relaxing , outdoors ) 1.00   ( typewriters , ancient , legend ) 1.00   ( cities , crowded , blast ) 0.67   ( knowledge , important , intuition ) 0.67   ( love , colorful , neon ) 0.67   ( clouds , white , bone ) 0.33   ( friendship , colorful , lightning ) 0.33   ( love , addictive , spice ) 0.33   ( cities , crowded , hell ) 0.00   ( clouds , white , steel ) 0.00   ( dreams , clear , stone ) 0.00   ( exams , challenging , robotics ) 0.005887