  Jian Liu , Chen Liang , Jinan Xu , Haoyan Liuand Zhe ZhaoBeijing Jiaotong UniversityTencent AI Lab   { jianliu , 21120367 , jaxu}@bjtu.edu.cn   { haoyanliu , nlpzhezhao}@tencent.com   Abstract   Document - level event argument extraction   aims to identify event arguments beyond sen-   tence level , where a signiﬁcant challenge is   to model long - range dependencies . Focusing   on this challenge , we present a new chain rea-   soning paradigm for the task , which can gen-   erate decomposable ﬁrst - order logic rules for   reasoning . This paradigm naturally captures   long - range interdependence due to the chains ’   compositional nature , which also improves in-   terpretability by explicitly modeling the rea-   soning process . We introduce T - norm fuzzy   logic for optimization , which permits end - to-   end learning and shows promise for integrating   the expressiveness of logical reasoning with   the generalization of neural networks . In ex-   periments , we show that our approach outper-   forms previous methods by a signiﬁcant mar-   gin on two standard benchmarks ( over 6 points   in F1 ) . Moreover , it is data - efﬁcient in low-   resource scenarios and robust enough to de-   fend against adversarial attacks .   1 Introduction   Identifying event arguments ( i.e. , participants of   an event ) is a crucial task for document - level event   understanding ( Ebner et al . , 2020 ; Li et al . , 2021 ) .   In this task , the major challenge is to model long-   range dependencies between event triggers and ar-   guments , as an event expression can span multiple   sentences ( Ebner et al . , 2020 ; Liu et al . , 2021 ; Li   et al . , 2021 ) . Consider the event expressed by a   trigger detonated ( type= Attack ) in Figure 1 . To   locate its argument Tartus ( semantic role= Place ) ,   a model should capture a large context window   of three sentences and 178 words to support the   reasoning process .   Currently , it still remains an open problem for   effectively capturing such dependencies ( Liu et al . ,   2021 , 2022c ) . Prior research has proposed to modelFigure 1 : Illustration of the document - level EAE task   ( /CIRCLE ) and our chain - of - reasoning paradigm ( /squaresolid ) .   beyond - sentence clues by incorporating hierarchi-   cal encoding mechanisms ( Du and Cardie , 2020a ) ,   generative paradigms ( Li et al . , 2021 ; Ma et al . ,   2022 ; Du et al . , 2022 ) , and document - level induc-   tive bias ( Wei et al . , 2021 ; Pouran Ben Veyseh et al . ,   2022 ; Liu et al . , 2022b ) . Nevertheless , such meth-   ods do not explicitly characterize the reasoning   patterns underlying the document context , which   potentially suffers sub - optimal performance . In ad-   dition , most previous methods are not interpretable   because they rely on black - box neural networks .   In this paper , we propose a new chain - of-   reasoning paradigm to address document - level   event argument extraction ( EAE ) . As indicated at   the bottom of Figure 1 , our method seeks to de-   scribe the global argument-ﬁnding process via a   chain of local inference steps . For example , we   may use the following chain to locate Tartus : det-   onated−−−−→ Arzunah Bridge−−−−−−→ Tartus .   This chain - of - reasoning paradigm has three clear   beneﬁts over previous approaches : First , it natu-   rally captures long - distance dependencies owing to   the compositional structure of the reasoning chain.9570Second , it involves only local reasoning , which is   conceptually easier than performing global reason-   ing directly . Third , it improves interpretability as   the reasoning processes are visible .   Our approach formalizes the reasoning chain   as ﬁrst - order logic ( FOL ) rules ( Cresswell and   Hughes , 1996 ) . Concretely , let RL(T , ? ) be the   query for an event argument fulﬁlling the semantic   roleRL(e.g . ,Place ) regarding an event trigger T.   We formalize the query as the following FOL rule :   RL(T,?)←r(T , B)∧ ... ∧r(B , ? )   where the body of the rule ( on the right ) consists of   conjunctive propositions with low - level predicates   { r}and intermediary clue entities { B } . We   build a model to automatically generate the rule   based on the document context , and then trans-   form the rule into a reasoning chain to locate the   event argument . Nevertheless , it is generally chal-   lenging to optimize with FOL rules owing to their   discrete nature ( Qu et al . , 2021a ) . Inspired by work   that augments neural networks with FOLs ( Li and   Srikumar , 2019 ; Ahmed et al . , 2022 ) , we present   T - Norm fuzzy logic for relaxation ( Hajek , 1998 ) ,   which leads to an end - to - end training regime .   We verify the effectiveness of our method on two   benchmarks ( Ebner et al . , 2020 ; Li et al . , 2021 ) . Ac-   cording to the results , our approach delivers promis-   ing results with this chain reasoning paradigm , such   as yielding a 6 - point improvement in F1 over mod-   els trained using large - scale external resources ( §   6.1 ) . Interestingly , in addition to the performance   boost , our approach demonstrates decent robust-   ness , particularly in low - resource scenarios and   defending against adversarial noises ( § 7.2 ) . Lastly ,   we evaluate the interpretability of our methodology   using a thorough case study ( § 7.3 ) .   In conclusion , our contributions are three - fold :   •We introduce a new chain - of - reasoning   paradigm for document - level EAE , demon-   strating clear advantages in capturing long-   range dependencies and enhancing inter-   pretability . As a seminal study , our work may   motivate more studies in this research line .   •We introduce T - Norm fuzzy logic , which re-   laxes discrete FOL rules for document - level   EAE into differentiable forms ; it also demon-   strates the prospect of combining the expres-   siveness of logical reasoning with the general-   ization capabilities of neural networks.•We report state - of - the - art performance on two   benchmarks , and we have made our code avail-   ablefor future exploration .   2 Related Work   Document - Level EAE . Extracting event argu-   ments in a document context is a vital step in   document - level event extraction ( Grishman , 2019 ;   Ebner et al . , 2020 ) . Earlier efforts on this problem   explore the MUC-4 benchmark ( Chinchor , 1991 ;   Huang and Riloff , 2012 ) , also known as “ template   ﬁlling ” because the entire document is about one   event . Recent research has focused on events with   lexical triggers , intending to extract all arguments   for a trigger - indicated event ( Ebner et al . , 2020 ; Li   and Srikumar , 2019 ) . For capturing the document   context effectively , prior studies have explored hi-   erarchical encoding mechanisms , generative per-   spectives ( Li et al . , 2021 ; Du et al . , 2022 ; Ma et al . ,   2022 ) , document - level inductive biases ( Wei et al . ,   2021 ; Pouran Ben Veyseh et al . , 2022 ) , and external   resources ( Du and Cardie , 2020b ; Liu et al . , 2020 ;   Xu et al . , 2022 ; Liu et al . , 2022a ) . Nonetheless ,   such methods do not explicitly model the under-   lying reasoning process for capturing long - range   dependencies , which therefore risks achieving sub-   optimal performance . In addition , these methods   are not interpretable because they employ neural   networks with black - box architectures . In contrast   to the previous study , we investigate employing   a chain - of - reasoning paradigm to explain the rea-   soning process , which can effectively model long-   range context while retaining interpretability .   Reasoning with FOL Rules . First - order logic   ( FOL ) rules can encode declarative knowledge and   play a crucial role in symbolic reasoning ( Cresswell   and Hughes , 1996 ) . In the era of deep learning , sev-   eral studies have examined the integration of FOL   rules with neural networks for reasoning ( termed   neural - symbolic approaches ) , with applications in   knowledge base inference ( Qu et al . , 2021b ) , text   entailment ( Li and Srikumar , 2019 ) , question an-   swering ( Wang and Pan , 2022 ) , and others ( Medina   et al . , 2021 ; Ahmed et al . , 2022 ) . Our approach   is inspired by the work on knowledge base infer-   ence , which , to the best of our knowledge , is the   ﬁrst attempt to incorporate FOL rules for reasoning   in the context of document - level EAE . Compared   to other methods , we investigate the prospect of9571   generating rules using neural networks automati-   cally instead of employing expert - written rules as   in ( Li and Srikumar , 2019 ; Wang and Pan , 2022 ) .   Additionally , unlike those based on reinforcement   learning ( Qu et al . , 2021b ) , we use T - norms for   rule relaxation , resulting in an end - to - end training   paradigm with a more stable learning procedure .   3 Approach   Figure 2 presents the overview of our approach ,   with an example for extracting the argument of a   Place role for the event detonated . Let D=   { w , · · · , T , · · · , w}be a document with N   words and an event trigger T , and let RL(T , ? ) be a   query for the event argument of a semantic role RL .   Instead of directly performing the reasoning that   may involve high - level processes , our approach rep-   resents the query as a FOL rule with conjunctive   propositions and low - level predicates { r } :   RL(T,?)←r(T , B)∧ ... ∧r(B , ? )   In this way , the body of the rule suggests a rea-   soning chain : T−→B−→ · · · B−→ ? .   We utilize a two - predicate formulation , speciﬁcally   RL(T,?)←r(T , B)∧r(B , ? ) , to explain our   method , and we describe general cases in § 4 .   3.1 Clue Entity Set Generation   In the ﬁrst step of our method , we create a set of   entities from which one may be chosen as an inter-   mediary clue entity to form the reasoning chain ( re-   garding our two - predicate structure ) . We broaden   the notion of “ entity ” to include any single word   in the document for incorporating verb - based cues .   To limit the size of the set , we give each word ascore derived from BERT representations ( Devlin   et al . , 2019 ) . For example , the score for wis :   s = exp(wh+b)/summationtextexp(wh+b)(1 )   where his the representation of w , andwand   bare model parameters . We rank all words based   on the sores and select Kwith the highest sores to   form the set , denoted by B={b } .   To facilitate training and testing , we addition-   ally generate an argument candidate set . In this   case , we do not utilize the broad deﬁnition of en-   tity because an event argument is deﬁned to be a   noun entity ( Walker and Consortium , 2005 ; Ahn ,   2006 ) . When ground - truth entities are available   ( such as in WikiEvents ( Li et al . , 2021 ) ) , we con-   sider the candidate set to be the ground - truth entity   set ; otherwise , we use external toolkitsto recog-   nize entities . We denote the argument candidate set   byA={a } .   3.2 FOL Rule Generation   Given the entity candidate set Band the argument   candidate setA , the next step is to generate two   predicates and select related candidates in the sets   to form the rule . Here we explain our method for   generating predicates regarding a particular entity-   argument pair ( B∈B , A∈A ) , and we show   metrics for ranking the rules generated by different   candidate pairs in§4 .   Predicate Representations . In our approach ,   we assume that there are Matomic predicates with9572indecomposable semantics , represented by a pred-   icate setR={R } . We give each predicate a   d - dimensional vectorized representation and derive   a matrix representation U∈RforR. For the   semantic role RL , we also give it a d - dimensional   representation , indicated by r∈R.   Learning Role - Predicate Associations . Given   the representations , we ﬁrst learn a role - to-   predicate association that indicates which predi-   cates are likely to be generated based on the role   solely and disregarding context . We employ auto-   regressive learning and generate a probability vec-   tora∈Rindicating the distribution of the   ﬁrst predicate rover the predicate set R :   a= softmax ( UWr ) ( 2 )   where W∈Ris a parameter . To learn the   distribution of the second predicate r , we ﬁrst   update the role ’s representation by integrating the   impact of the ﬁrst predicate :   r = r+aWU ( 3 )   and then compute a probability vector a∈R :   a= softmax ( UWr ) ( 4 )   where W∈RandW∈Rare pa-   rameters to learn . We can set randras predi-   cates with the highest probability in aanda ,   respectively . However , such an approach always   generates the same predicates for a semantic role   and has a pretty poor performance ( 7.1 ) . As a solu-   tion , we introduce a mechanism for re - ranking the   predicates based on contexts .   Context - Dependent Predicate Generation .   LetXandYbe two entities . We ﬁrst compute   a probability vector v∈Rdenoting the   compatibility of ( X , Y ) with each predicate R∈R   to form a proposition R(X , Y ):   v= softmax ( W(h⊕h ) ) ( 5 )   where handhare representations of XandY ,   ⊕is a concatenation operator , and W∈R   is a model parameter . We combine integrate the   compatibility probabilities with the role - predicate   association probabilities for ﬁnal predicate genera-   tion . Speciﬁcally , for an event trigger T , a certainentity B∈Band argument candidate A∈A , we   generate the following two predicates :   r= arg max(a⊙v·s·s)(6 )   r= arg max(a⊙v·s·s)(7 )   where⊙is an element - wise multiplication operator   andsindicates the scoreof an entity Xselected   to be in the candidate clue entity set B(Eq . ( 1 ) ) .   In this way , the generated FOL rule is RL(T , A)←   r(T , B)∧r(B , A ) , suggesting a reasoning path   to reach the event argument A : T−→B−→A.   4 Optimization and Generalization   Optimization with FOL rules is typically challeng-   ing due to their discrete nature ( Qu et al . , 2021a ) .   Here we present T - Norm fuzzy logic for relaxation ,   which yields an end - to - end learning process .   T - Norm Fuzzy Logic for Relaxation . T - Norm   fuzzy logic generalizes classical two - valued logic   by admitting intermediary truth values between 1   ( truth ) and 0 ( falsity ) . For our generated FOL rule   RL(T , A)←r(T , B)∧r(B , A ) , we set the truth   values of r(T , B)andr(B , A)to be the corre-   sponding scores in Equation ( 6 ) and ( 7 ) , denoted   bypandprespectively . Then , following the   Łukasiewicz T - Norm logic , the conjunction of two   propositions corresponds to :   p(r(T , B)∧r(B , A ) ) = min(p , p ) ( 8)   where we re - write it as a metric : M(T , B , A ) =   p(r(T , B)∧r(B , A))and use it for rule ranking   and optimization . Particularly , we enumerate each   entity - argument pair ( B , A)∈B×A , and denote   the one with the highest score by ( ˆB,ˆA ) . We then   derive the following loss for optimization :   J(Θ)=−logexp(M(T,ˆB,ˆA))/summationtextexp(M(T , B , A ) )   ( 9 )   where Θindicates the overall parameter set ( In the   training time , the ground - truth argument is known ,   and we can directly set the optimal argument to the   ground - truth ) . Even though our method considers   each candidate entity and argument , we show with   parallel tensor operations , our method runs rivalry   as effectively as prior methods ( see Appendix A.1).9573Generalization to General Cases . We explain   our method using a structure two - predicate struc-   ture , but it is easy to adapt it for general   cases with any number of predicates . Now   assume a n - predicate structure . We ﬁrst   learn a sequence of role - predict association vec-   torsa , a,···,ausing an auto - regressive   regime similar to E.q . ( 3 ) and ( 4 ) . Then , we re - rank   and generate predicates r , r , · · · , rto form the   logic rule . For optimization , we drive the following   metric , p(r∧r∧···∧ r ) = min(p , p,···,p ) ,   which is similar to E.q . ( 8) to perform rule ranking   and model training .   5 Experimental Setups   Benchmarks and Evaluations . We conduct ex-   periments using two document - level EAE bench-   marks : RAMS ( Ebner et al . , 2020 ) and WikiEvents   ( Li et al . , 2021 ) . The RAMS benchmark de-   ﬁnes 139 event types and 59 semantic roles and   gives 7,329 annotated documents ; The WikiEvents   benchmark deﬁnes 50 event types and 59 seman-   tic roles and provides 246 annotated documents .   The detailed data statistics are shown in Table 1 .   Following ( Ebner et al . , 2020 ; Liu et al . , 2021 ) ,   we adopt the type constrained decoding ( TCD )   setup for evaluation , which assumes the events   triggers and their types are known . We employ   Span - F1 on RAMS and Head - F1 and Coref - F1 on   WikiEvents as evaluation metrics , where Head - F1   only examines the head word in an argument and   Coref - F1 also takes co - reference linkages between   arguments into account ( Du and Cardie , 2020a ; Li   et al . , 2021 ; Wei et al . , 2021 ; Ma et al . , 2022 ) .   Implementations . In our approach , we use   BERT to learn the contextualized word rep-   resentations ( Devlin et al . , 2019 ) . The hyper-   parameters are tuned using the development set .   Finally , the size of the entity candidate set Kis   set to 40 , selected from the range [ 20 , 30 , 40 , 50 ] ,   whereas the size of the argument candidate set is   determined automatically by the external entity rec-   ognizer . The number of predicates Mis set to 20   out of [ 10 , 15 , 20 , 25 ] options . For optimization ,   we use the Adam optimizer ( Kingma and Ba , 2015 )   with a batch size of 10 from [ 5 , 10 , 15 , 20 ] and a   learning rate of 1e-4 from [ 1e-3 , 1e-4 , 1e-5 ] .   Baselines . For comparison , we consider the fol-   lowing four categories of methods : 1 ) Traditional   approaches , such as BIOLabel ( Shi and Lin , 2019),Dataset Split # Trigger # Arg . # Entity   RAMSTrain 7,329 17,026 123,127   Dev . 924 2,188 13,305   Test 871 2,023 30,345   WikiEv . Train 3,241 4,552 64,171   Dev . 345 428 5,968   Test 365 566 7,044   which views the task as a sequential labeling prob-   lem . 2 ) Global encoding methods , such as QAEE   ( Du and Cardie , 2020b ) and DocMRC ( Liu et al . ,   2021 ) , which form the task as a document - based   question - answering problem , and MemNet ( Du   et al . , 2022 ) , which uses a memory to store global   event information . 3 ) Generative methods , such   as BART - Gen ( Li et al . , 2021 ) , which proposes a   sequence - to - sequence paradigm for argument ex-   traction , and PAIE ( Ma et al . , 2022 ) , which em-   ploys a set generation formulation . 4 ) Methods   using extra supervisions , for example , FEAE ( Wei   et al . , 2021 ) , which adopts frame - related knowl-   edge , and TSAR ( Xu et al . , 2022 ) , which utilizes   abstract meaning representation ( AMR ) resources .   6 Experimental Results   In this section , we present the key results , separated   by the overall performance and results of capturing   long - range dependencies .   6.1 Overall Performance   Table 2 and 3 display the performance of different   models on RAMS and WikiEvents , respectively .   By adopting the chain - of - reasoning paradigm , our   approach outperforms previous methods by signif-   icant margins and achieves state - of - the - art perfor-   mance — 56.1 % in F1 on RAMS and 72.3 % in   Head - F1 and Coref - F1 on WikiEvents . Notably ,   our model uses no external resources for training ,   yet it outperforms previous models trained with   extensive external resources by over 6 % in F1 on   RAMS and 4 % in Head - F1 ( 7 % in Coref - F1 ) on   WikiEvents . In addition , we discover that the main   improvement derives from improved recall , sug-   gesting that learning the reasoning logic rule fa-   cilitates locating arguments that were difﬁcult for   previous global reasoning methods.9574Model P R F1   BIOlabel ( Shi and Lin , 2019 ) 39.9 40.7 40.3   QAEE ( Du and Cardie , 2020b ) 42.4 44.9 43.6   DocMRC ( Liu et al . , 2021 ) 43.4 48.3 45.7   MemNet ( Du et al . , 2022 ) 46.2 47.0 46.6   BART - Gen ( Li et al . , 2021 ) 42.1 47.3 44.5   PAIE ( Ma et al . , 2022 ) - - 49.5   FEAE ( Wei et al . , 2021 ) 53.1 42.7 47.4   TSAR ( Xu et al . , 2022 ) - - 48.1   Our Method 54.8 57.5 56.1   Model P R F1 F1   BIOLabel ( 2019 ) 55.2 52.3 53.7 56.7   QAEE ( 2020b ) 54.3 53.2 56.9 59.3   DocMRC ( 2021 ) 56.9 51.6 54.1 56.3   MemNet ( 2022 ) 57.2 51.8 54.4 58.8   BART - Gen ( 2021 ) 54.0 51.2 52.6 65.1   PAIE ( 2022 ) - - 66.5 -   TSAR ( 2022 ) - - 68.1 66.3   Our Method 73.5 71.2 72.372.3   w/o GT Entity 68.8 70.4 69.6 65.6   6.2 Addressing Long - Range Dependencies   We then assess the ability of different models to   handle long - range dependencies , which is crucial   for the document - level task . Table 4 and 5 show   results on different argument - trigger distance d —   accordingly , our model achieves remarkable perfor-   mance for addressing long - range dependencies , for   example , yielding 10.9 % , 15.7 % , and 6.7 % abso-   lute improvement in F1 for d=-1,d=1 , and d=2 on   RAMS , respectively . The insight behind the effec-   tiveness is that by adopting the chain - of - reasoning   paradigm , our method can utilize clue entities to re-   duce the distance between triggers and arguments ,   which therefore facilitates learning with long con-   text . Nevertheless , we also note that our method   yields relatively poor performance when the argu-   ment is two sentences prior to the trigger ( d=-2 ) .   One possible reason is that our reasoning chain al-   ways starts with the trigger and we do not deﬁneArgument - Trig . Distance   Model -2 - 10 12   BIOLabel ( 2019 ) 14.0 14.0 41.2 15.7 4.2   DocMRC ( 2021 ) 21.0 20.3 46.6 17.2 12.2   BART - Gen ( 2021 ) 17.7 16.8 44.8 16.6 9.0   PAIE ( 2022 ) 21.7 27.3 54.7 29.4 25.4   FEAE ( 2021 ) 23.7 19.3 49.2 25.0 5.4   TSAR ( 2022 ) 24.3 21.9 49.6 24.6 11.9   Our Method 15.0 38.2 59.8 45.1 32.1   Argument - Trig . Distance   Model < = -1 0 > = 1   BIOLabel ( 2019 ) 34.4 54.6 31.5   DocMRC ( 2021 ) 31.5 56.2 40.0   BART - Gen ( 2021 ) 64.5 67.5 39.4   PAIE ( 2022 ) 68.8 69.5 41.3   Our Method 70.5 75.0 44.1   reverse predicates , which may limit its ﬂexibility .   We leave addressing these issues for further work .   7 Discussion   We conduct a series of detailed studies to further   verify the effectiveness of our model . To ease dis-   cussion , we use the RAMS benchmark as a case .   7.1 Ablation Study   We perform an ablation study to analyze the inﬂu-   ence of different components .   Impact of Predicate Generation . Table 6 con-   trasts our method with methods employing vari-   ous predicate generation strategies : 1 ) “ w/o Pred-   icate Generation ” , which generates the reasoning   path directly without predicate generation ( in other   words , it only cares if there is a relationship be-   tween two variables or not , but not the speciﬁc   relationship ) . 2 ) “ w/o Role Association ” , which   removes the role - predicate association learning pro-   cess in which a predicate is determined purely by   the two variables . 3 ) “ w/o CTX Re - Rank ” , which9575Model P R F1 ∆   Full Approach 54.8 57.5 56.1 -   w/o Predicate Gen. 42.7 25.7 32.2 23.9 ↓   w/o Role Asso . 39.7 41.1 40.4 15.7 ↓   w/o CTX Re - Rank 54.2 50.4 52.2 3.9 ↓   Rule ’s Length P R F1   One ( Strict ) 12.0 36.3 18.1   Two ( Strict ) 37.0 38.5 37.8   Three ( Strict ) 38.0 35.4 36.7   Two ( Adaptive ) 54.8 57.5 56.1   Three ( Adaptive ) 52.9 58.6 55.6   Two ( Ensemble ) 53.4 55.7 54.5   Three ( Ensemble ) 52.6 57.0 54.7   omits the context - dependent predicate re - ranking   process in which the predicates are completely gen-   erated by the role . According to the results , predi-   cate generation is essential for reasoning ; without   it , performance drops signiﬁcantly ( 23.9 % in F1 ) .   In addition , the semantic of the role is essential for   predicate generation ; without it , performance falls   by 15.7 % in F1 . Lastly , learning context - dependent   predicate re - ranking is advantageous , resulting in a   3.9 % absolute improvement in F1 .   Ablation on the Rule ’s Length . Table 7 exam-   ines the effect of predicate count in a LOC rule ,   where N ( Strict ) denotes that we adopt a rule with N   predicates precisely , N ( Adaptive ) denotes that we   adopt a rule with N predicates at most and consider   the prediction with the greatest score adaptively , N   ( Ensemble ) indicates that we ensemble the results   by summing the ﬁnal score of an argument . The   results demonstrate that mandating a ﬁxed number   of predicates leads to poor performance , whereas   providing the option to choose varying numbers of   predicates results in excellent performance . This   also implies that the argument-ﬁnding process does   involve different reasoning patterns . In addition ,   we do not notice an advantage of N ( Adaptive ) over   N ( Ensemble ) , indicating that FOL rules may not   facilitate ensemble .   Ablation on the Amount of Predicates . Figure   3 examines the effect of the number of predicates   on the ﬁnal performance based on the RAMS de-   velopment set , as well as their joint effect with the   length of the rule ( we use the Adaptive setting ) . Ac-   cording to the results , our method is insensitive to   the number of predicates and consistently achieves   high performance when the number of predicates   is more than 15 . In addition , we demonstrate that   the number of predicates can be lowered when the   rule length is increased ( e.g. , from two to three ) .   This makes sense , as a longer rule implies a longer   reasoning chain , which already has a high degree   of intrinsic expressivity . In contrast , for a 1 - length   FOL rule , the performance is always unsatisfactory   even if we increase the number of predicates to   increase their diversity .   7.2 Robustness Evaluation   Given that our approach uses FOL rules to capture   the essential reasoning pattern , it might be more   robust than previous methods to perform reasoning .   We validate this assumption by analyzing its perfor-   mance in low - resource scenarios and for defending   against adversarial attacks ( Jia and Liang , 2017 ) .   Performance in Low - Resource Scenarios . Fig-   ure 4 compares different models in low - resource   conditions , which show models training on only   partial training data ( we report 5 - run average to   against randomness ) . Clearly , our approach con-   sistently outperforms other methods , and remark-   ably , in extremely low resource settings ( less than   5 % training data ) , it outperforms PAIE based on   prompting with large pre - trained language models   and TSAR based on external resources , indicating   its effectiveness and generalizability in learning   FOL rules for reasoning . The performance im-   proves as more training data becomes available.9576   Defending Against Adversarial Attacks . Fig-   ure 5 shows results in defending adversarial attacks   by injecting three forms of noises in a testing ex-   ample . ATK1 : we randomly replace a word in the   sentence that contains the trigger with the slot sym-   bol [ BLANK ] ; ATK2 : we put the corrupted sen-   tence “ The answer is [ BLANK ] " after the sentence   that contains the trigger . ATK3 : we insert a sen-   tence “ The argument of the [ ROLE ] is [ BLANK ] “   after the sentence that contains the trigger , where   [ ROLE ] is replaced by the semantic role on which   we focus . Two settings are considered : Attack   ( Random ) , where the slot is ﬁlled with an argumentthat fulﬁlls the same role in other instances . Attack   ( Gold ) , where the slot is ﬁlled with the ground-   truth argument , but we consider it an error if the   model predicts the argument in the slot to be the   answer since the injected sentence is unrelated to   the context . The results show that our approach is   excelled at defending against adversarial attacks ,   especially with the Attack ( Random ) setting ( see   Figure 5(a ) ) . One reason is that our method forces   predicting arguments that have semantic relations   with other entities in the document context , so it   is less affected by the isolated injected arguments .   Defending the attacks with ground - truth arguments   is more challenging ( Figure 5(b ) ) , but our method   still achieves the best overall performance .   7.3 Interpretability and Case Study   Table 8 examines the interpretability of our method   using a case study . By analyzing cases 1 ) , 2 ) , and   3 ) , we suggest that our method can generate spe-   ciﬁc and context - dependent reasoning rules for the   same semantic role . In addition , the reasoning pat-   terns for cases 2 ) and 3 ) are similar , where rmay   be interpreted as an Attacker predicate and r   as aLocatedIn predicate . Case 4 ) generates   the same predicate ras cases 2 ) and 3 ) , which   may be interpreted as a Committer predicate for   the payment event ; it shares a similar semantic as   Attacker to an Attack event in cases 2 ) and   3 ) . Case 5 ) indicates that our method can capture   extremely distant dependencies .   8 Conclusion   In conclusion , we present a new chain reasoning   paradigm for document - level EAE , demonstrating   clear beneﬁts in capturing long - range dependen-9577cies and improving interpretability . Our method   constructs a ﬁrst - order logic rule to represent an   argument query , with T - Norm fuzzy logic for end-   to - end learning . With this mechanism , our ap-   proach achieves state - of - the - art performance on   two benchmarks and demonstrates decent robust-   ness for addressing low - resource scenarios and de-   fending against adversarial attacks . In future work ,   we seek to extend our methodology to other tasks   requiring modeling of long - range dependencies ,   such as document - level relation extraction .   9 Limitations   One limitation of our method is that when there   are rules of different lengths , the ﬁnal result is   decided by ensemble , not by building a model to   generate a single rule with the best length . The   second way is more natural and important because   ﬁguring out the length of the rule is also a key part   of symbolic reasoning . However , it requires more   parameterization ( for example , the length of the   rule could be a parameter ) and a more advanced   way to optimize . The investigation of the above   method is left for future works .   Acknowledgments   This work is supported by the National Natural   Science Foundation of China ( No.62106016 ) , the   Open Projects Program of the State Key Laboratory   of Multimodal Artiﬁcial Intelligence Systems , and   the Tencent Open Fund .   References9578   A Appendix   A.1 Parallelization and Training Time   We show methods for parallelizing our approach to   identify the optimal entity - argument pair and com-   pare our approach to others in real training time .   Given an event trigger Twith a query RL(T , ? ) , a9579Model Time ( minutes )   BIOlabel ( Shi and Lin , 2019 ) 7.5   QAEE ( Du and Cardie , 2020b ) 28.0   DocMRC ( Liu et al . , 2021 ) 55.0   FEAE ( Wei et al . , 2021 ) 56.3   BART - Gen ( Li et al . , 2021 ) 14.0   PAIE ( Ma et al . , 2022 ) 11.1   Our Method 33.5   candidate entity set B={b}of size K , and an   argument candidate set A={b}of size L , we   ﬁrst compute the predicate comparability for the   event trigger Twith each candidate entity in Bas   follows :   V= softmax ( W(h⊕H ) ) ( 10 )   where the concatenation operator of the vector   h∈Rand the matrix H∈Ris per-   formed by ﬁrst broadcasting the vector to the same   dimension as the matrix , followed by an element-   wise concatenation operation . This results in a M   byKmatrix : V∈R. To unify illus-   tration , we add an extra dimension to Vto   represent the event trigger , which thus makes it a   high - order tensor V∈R. In a similar   fashion , we compute the predicate comparability   for each entity - argument pair in BandAand obtain   a high - order tensor V∈R :   V= softmax ( W(H⊕H ) ) ( 11 )   where the concatenation operator of two matrices is   implemented by ﬁrst broadcasting each matrix into   a dimension of KbyLbydand then concatenating   each element individually .   Given VandV , we can apply a soft-   max operatoron their ﬁrst dimension to identify   the best-ﬁtting predicates for each trigger - entity   and entity - argument pair and only keep the max-   imum values as their scores . Suppose the results   are two matrices O∈RandO∈Rfor   VandVrespectively . We then apply the   T - Norm relaxation for the conjunction operator as   follows :   O= min ( ˆO,ˆO ) ( 12)Noise Type Document with Noise   ATK1 ( Rand . ) [ S1][S2][S3 ] : ... their homes and   Paris have been damaged , burned   or destroyed ... [ S4][S5 ]   ATK2 ( Rand . ) [ S1][S2][S3 ] The answer is   Paris . [ S4][S5 ]   ATK3 ( Rand . ) [ S1][S2][S3 ] The argument of   the place is Paris . [ S4][S5 ]   ATK1 ( Arg . ) [ S1][S2][S3 ] : ... their homes   andAleppo have been damaged   ... [ S4][S5 ]   ATK2 ( Arg . ) [ S1][S2][S3 ] The answer is   Aleppo . [ S4][S5 ]   ATK3 ( Arg . ) [ S1][S2][S3 ] The argument of   the place is Aleppo . [ S4][S5 ]   where ˆO∈RandˆO∈R , which   have the same dimension , are the tensor broadcast-   ing results of OandOrespectively , and min   indicates an element - wise minimization operator .   Finally , by examining the element with the high-   est value in O , the optimal entity and argument   pair can be determined . For example , if Ois   the element with the highest values , then ( B , A )   corresponds to the optimal entity - argument pair .   In Table 9 , we compare the real training time   for each model on the RAMS dataset . All experi-   ments are conducted on a 16G - memory NVIDIA   Tesla P100 - SXM2 Card to ensure a fair compari-   son . From the results , we can see that our model   maintains a comparable time to earlier methods   such as QAEE and is faster than many models such   as FEAE and DocMRC , where FEAE has two base   models for knowledge distillation and DocMRC   uses external data to pretrain the model .   A.2 Cases of Adversarial Examples   In this section , we provide a speciﬁc adversarial   example to enhance comprehension . Our original   document with annotations for the event trigger   ( which is underlined ) and an argument ( which is in   bold ) fulﬁlling a semantic role of Place is :   “ [ S1 ] People we meet who are displaced took   shelter in schools , in unﬁnished buildings and other   facilities , some of which are simply skeleton in-   frastructure . [ S2 ] Most people with whom I spoke9580have been displaced for at least two to three years .   [ S3 ] Many of them see no prospect of returning   home any time soon , either because ﬁghting is still   going on , or because for many of them , their homes   and land have been damaged , burned or destroyed .   [ S4 ] Every single family is affected , and most com-   munities in Aleppo , and beyond , have reached the   limit of their endurance . [ S5 ] Aid workers have   said there is just enough fuel to keep generators ,   bakeries , and hospitals running for a month . ”   We show the generated noisy examples in Table   10 , and note that if the model identiﬁes the noisy   arguments presented in the table as the result , it   should be counted as an error.9581ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 9   /squareA2 . Did you discuss any potential risks of your work ?   Section 9   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Section 6 and 7   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 59582 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   No response .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 6 and 7   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 5   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.9583