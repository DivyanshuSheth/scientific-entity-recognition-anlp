  Qianyu He , Yikai Zhang , Jiaqing Liang ,   Yuncheng Huang , Yanghua Xiao , Yunwen ChenShanghai Key Laboratory of Data Science , School of Computer Science , Fudan UniversitySchool of Data Science , Fudan UniversityDataGrand Inc. , Shanghai , China   { qyhe21 , ykzhang22 , yunchenghuang22}@m.fudan.edu.cn ,   { liangjiaqing , shawyh}@fudan.edu.cn , chenyunwen@datagrand.com   Abstract   Similes play an imperative role in creative   writing such as story and dialogue generation .   Proper evaluation metrics are like a beacon   guiding the research of simile generation ( SG ) .   However , it remains under - explored as to what   criteria should be considered , how to quantify   each criterion into metrics , and whether the   metrics are effective for comprehensive , effi-   cient , and reliable SG evaluation . To address   the issues , we establish HAUSER , a holistic and   automatic evaluation system for the SG task ,   which consists of five criteria from three per-   spectives and automatic metrics for each cri-   terion . Through extensive experiments , we   verify that our metrics are significantly more   correlated with human ratings from each per-   spective compared with prior automatic metrics .   Resources of HAUSER are publicly available at   https://github.com/Abbey4799/HAUSER .   1 Introduction   Similes play a vital role in human expression , mak-   ing literal sentences imaginative and graspable . For   example , Robert Burns famously wrote “ My Luve   is like a red , red rose ” to metaphorically depict the   beloved as being beautiful . In this simile , “ Luve ”   ( a.k.a . topic ) is compared with “ red rose ” ( a.k.a . ve-   hicle ) via the implicit property “ beautiful ” and the   event “ is ” . Here , topic , vehicle , property , and event   are four main simile components ( Hanks , 2013 ) .   As a figure of speech , similes have been widely   used in literature and conversations ( Zheng et al . ,   2019 ; Chakrabarty et al . , 2022 ) .   Simile generation ( SG ) is a crucial task in natu-   ral language processing ( Chakrabarty et al . , 2020 ;   Zhang et al . , 2021 ; Lai and Nissim , 2022 ) , with   the aim of polishing literal sentences into simi-   les . In Fig . 1 , the literal sentence “ He yelps and   howls . ” is polished into a simile by inserting the   phrase “ like a wolf ” , resulting in “ He yelps and   howls like a wolf ” . The ability to generate simi-   les can assist various downstream tasks , such as   making the generations more imaginative in story   or poet generation task ( Tartakovsky and Shen ,   2018 ; Chakrabarty et al . , 2022 ) and the generated   response more human - like in dialogue generation   task ( Zheng et al . , 2019 ) .   Automatic evaluation is critical for the SG task   since it enables efficient , systematic , and scalable   comparisons between models in general ( Celikyil-   maz et al . , 2020 ) . However , existing studies are in-   adequate for effective SG evaluation . Task - agnostic   automatic metrics ( Papineni et al . , 2002 ; Zhang   et al . , 2019 ; Li et al . , 2016 ) are widely adopted   for SG evaluation ( Zhang et al . , 2021 ; Lai and Nis-   sim , 2022 ) , which have several limitations : ( 1 ) The   simile components should receive more attention   than other words during SG evaluation ( e.g. “ he ”   and “ wolf ” in Fig . 1 ) , while there are no automatic   metrics that consider the key components . ( 2 ) The   SG task is open - ended , allowing for multiple plau-   sible generations for the same input ( Chakrabarty   et al . , 2020 ) ( e.g. the howling man can be com-   pared to “ wolf ” , “ buffalo ” , or “ tiger ” in Fig . 1 ) .   Hence , the metrics based on word overlap with a   few references are inadequate to accurately mea-12557   sure the overall quality of generated similes . As   shown in Fig . 1 , the commonly used metric BLEU   deems the second candidate as the highest quality ,   as it has more overlapped words with the only ref-   erenced groundtruth , while human deems the first   candidate as the most coherent one . ( 3 ) The exist-   ing metrics are inadequate to provide fine - grained   and comprehensive SG evaluation , considering that   the creative generation tasks have distinct criteria   for desired generations ( Celikyilmaz et al . , 2020 ) ,   such as novelty and complexity for story genera-   tion ( Chhun et al . , 2022 ) and logical consistency   for dialogue generation ( Pang et al . , 2020 ) .   However , establishing a comprehensive , effi-   cient , and reliable evaluation system for SG is non-   trivial , which raises three main concerns : ( 1 ) What   criteria should be adopted to evaluate the SG task   in a comprehensive and non - redundant fashion ? ( 2 )   How to quantify each criterion into a metric thus en-   abling efficient and objective SG evaluation , given   that the human evaluation of creative generation   task is not only time - consuming but also subjective   and blurred ( Niculae and Danescu - Niculescu - Mizil ,   2014 ; Celikyilmaz et al . , 2020 ) ? ( 3 ) Whether the   proposed metrics are effective in providing useful   scores to guide actual improvements in the real-   world application of the SG model ?   In this paper , we establish HAUSER , aHolistic   and AUtomatic evaluation system for Simile   gEneRation task , consisting of five criteria ( Tab . 1 ):   ( 1 ) The relevance between topic and vehicle , as the   foundation of a simile is to compare the two via   their shared properties ( Paul , 1970 ) . ( 2 ) The logical   consistency between the literal sentence and gen-   erated simile , since the aim of SG task is to polish   the original sentence without altering its seman-   tics ( Tversky , 1977 ) . ( 3 ) The sentiment consistency   between the literal sentence and generated simile , since similes generally transmit certain sentiment   polarity ( Qadir et al . , 2015 ) . ( 4,5 ) The creativity   andinformativeness of the simile , since novel sim-   iles or those with richer content can enhance the   literary experience ( Jones and Estes , 2006 ; Ron-   cero and de Almeida , 2015 ; Addison , 2001 ) . Over-   all , these five criteria can be categorized into three   perspectives : quality ( which considers relevance ,   logical , and sentiment consistency jointly ) , creativ-   ity , and informativeness . We further quantify each   criterion into automatic metrics ( Fig . 2 ) and prove   their effectiveness through extensive experiments .   To the best of our knowledge , we are the first to   systematically investigate the automatic evaluation   of the SG task . To summarize , our contributions are   mainly three - fold : ( 1 ) We establish a holistic and   automatic evaluation system for the SG task , con-   sisting of five criteria based on linguistic theories ,   facilitating both human and automatic evaluation   of this task . ( 2 ) We design automatic metrics for   each criterion , facilitating efficient and objective   comparisons between SG models . ( 3 ) We conduct   extensive experiments to verify that our metrics are   significantly more correlated with human ratings   than prior metrics .   2 Related Work   2.1 Simile Generation Task   There are two primary forms of the simile genera-   tion ( SG ) task : simile triplet completion and literal   sentence polishing . For simile triplet completion , a   model receives simile components , topic and prop-   erty , and is required to generate the vehicle ( Ron-   cero and de Almeida , 2015 ; Zheng et al . , 2019 ;   Chen et al . , 2022 ; He et al . , 2022 ) . For literal sen-   tence polishing , a model receives a literal sentence   and is expected to convert it into similes ( Zhang12558   et al . , 2021 ; Stowe et al . , 2020 ; Chakrabarty et al . ,   2020 ; Lai and Nissim , 2022 ) . We focus on the latter .   However , prior works mainly adopt task - agnostic   automatic metrics to evaluate the SG task , raising   concern as to whether the claimed improvements   are comprehensive and reliable .   2.2 Automatic Evaluation for NLG Systems   Existing automatic metrics for Natural Language   Generation ( NLG ) evaluation can be categorized   into task - agnostic and task - specific metrics . Task-   agnostic metrics can be applied to various NLG   tasks , which generally focus on the coherence of   generations ( Papineni et al . , 2002 ; Zhang et al . ,   2019 ) , including n - gram - based metrics ( Papineni   et al . , 2002 ; Lin , 2004 ; Denkowski and Lavie ,   2014 ) and embedding - based metrics ( Zhang et al . ,   2019 ; Zhao et al . , 2019 ) . There are also many met-   rics for evaluating the diversity of generations ( Li   et al . , 2016 ; Zhu et al . , 2018 ; Tevet and Berant ,   2021 ) . Task - specific metrics are proposed to evalu-   ate NLG systems on specific tasks ( Tao et al . , 2018 ;   Dhingra et al . , 2019 ; Ren et al . , 2020 ) . Specifically ,   various works systematically study the evaluation   of the creative generation task ( Pang et al . , 2020 ;   Tevet and Berant , 2021 ; Chhun et al . , 2022 ) . Dif-   ferent from these works , we revisit SG evaluation ,   propose holistic criteria based on linguistic theo-   ries , and design effective automatic metrics for it .   3HAUSER for SG evaluation   We establish HAUSER , a holistic and automatic eval-   uation system for SG evaluation , containing five   criteria from three perspectives , and further design   automatic metrics for each criterion ( Fig . 2).3.1 Quality   We measure the overall quality of generated simi-   les using three criteria : relevance , logical consis-   tency , sentiment consistency . The key simile com-   ponents - topic and vehicle - should be relevant , as   the foundation of a simile is to compare the two via   their shared properties ( relevance ) ( Paul , 1970 ) . In   Tab . 1 , comparing “ raindrops ” to “ tears ” is more   coherent than to “ arrows ” . Additionally , the gen-   erated simile should remain logically consistent   with the original sentence ( logical consistency ) , as   the SG task aims to polish the plain text without   changing its semantics ( Tversky , 1977 ) . In Tab . 1 ,   comparing “ Stefan ” to “ dancer ” better depicts his   controlled and easy movement than to “ lightning ” .   Furthermore , as similes generally transmit certain   sentiment polarity ( Qadir et al . , 2015 ) , the gener-   ated simile should enhance the sentiment polarity   of the original sentence ( sentiment consistency ) . In   Tab . 1 , the vehicle “ thunderous wave ” enhances the   positive polarity of the original sentence , while the   vehicle “ earthquake ” brings a negative sentiment   polarity in opposition to the original sentence .   3.1.1 Relevance   For the relevance score , if the components of one   simile are relevant , they tend to co - occur in sim-   ile sentences ( Xiao et al . , 2016 ; He et al . , 2022 )   and possess shared properties ( Paul , 1970 ; Tver-   sky , 1977 ) . Hence , obtaining the relevance score   requires large - scale simile sentences as references ,   as well as knowledge about the properties ( adjec-   tives ) of each simile component . For a simile s , the   relevance score is defined as follows:12559where there are mtopic - vehicle pairs extracted   from simile s , each denoted as ( t , v).Γ(t , v)is   the set of similes containing ( t , v)as simile compo-   nents , each denoted as e. P(t , v)is the probability   that the simile components ( t , v)share properties   in the context of the simile sentence e.   An effective way to obtain the frequency infor-   mation Γ(t , v)and property knowledge P(t , v)is   to utilize the large - scale probabilistic simile knowl-   edge base MAPS - KB ( He et al . , 2022 ) , which con-   tains millions of simile triplets in the form of ( topic ,   property , vehicle ) , along with frequency and two   probabilistic metrics to model each triplet . Specif-   ically , the probabilistic metric Plausibility is calcu-   lated based on the confidence score of the simile   instance ( topic , property , vehicle , simile sentence )   supporting the triplet , indicating the probability   that the topic and vehicle share the property . The   relevance score rcan be calculated as follow :   where Gis the set of triplets ( t , p , v ) containing   the ( t , v ) pair in MAPS - KB , with preferring to the   property . nandPare the metrics provided by   MAPS - KB , where nandPdenote the frequency   and the plausibility of the triplet respectively .   It is noticed that the metric is not coupled with   MAPS - KB , as the frequency information can be ob-   tained by referencing a large set of simile sentences   and the property knowledge can be contained via   other knowledge bases . More methods are beyond   the scope of this paper . However , we addition-   ally provide a method to approximate the relevance   score . If we assume the probability that the simile   components ( t , v)share properties in each sentence   is 1 , the relevance score can be approximated as :   where n(t , v)denotes the number of samples that   contain the simile components ( t , v)in large - scale   simile sentences . We discuss the effects of the   referenced dataset size in Sec . 4.2.1 .   3.1.2 Logical Consistency   The literal sentence and the generated simile that   are logically inconsistent generally exhibit contra - dictory logic . Hence , for a generated simile , we   input the < literal text ( l ) , simile ( s ) > sentence pair   into existing pre - trained Multi - Genre Natural Lan-   guage Inference ( MNLI ) model , which determines   the relation between them is entailment , neutral , or   contradiction . The logical consistency score cof   this simile is defined as follows ( Pang et al . , 2020 ):   where P(h = c)represents the probability   that the model predicts the relation of the sentence   pair < l , s > to be contradiction ( denoted as c ) .   3.1.3 Sentiment Consistency   Better similes tend to enhance the sentiment po-   larity of the original sentence ( Qadir et al . , 2015 ) .   Hence , we first apply the model fine - tuned on the   GLUE SST-2 datasetto classify each simile as be-   ing either positive ornegative . Then , the sentiment   consistency score cis defined as follows :   where ais the sentiment polarity of the literal   sentence ( positive ornegative ) predicted by the   model . P(h = a)andP(h = a)denote the   probabilities that the model predicts the sentiment   polarity of the simile sand the literal sentence lto   bea , respectively .   It is noticed that different < topic , vehicle > pairs   within a sentence may have distinct sentiment po-   larities , such as < She , scared rabbit > and < I , bird >   in the simile “ If she escapes like a scared rabbit , I   will fly like a bird to catch her . ” . Directly inputting   text containing multiple topic - vehicle pairs into the   sentiment classification model will result in infe-   rior performance . Therefore , for each simile , only   the text from the beginning up to the first vehicle   is input into the model ( i.e. “ If she escapes like a   scared rabbit ” in the given example ) , and for each   literal sentence , the text from the beginning up to   the first event ( i.e. “ If she escapes ” in the given   example ) is input into the model.125603.1.4 Combination   Since the aim of the SG task is to polish the plain   text , the quality of similes generated from differ-   ent texts can not be compared . Therefore , the   normalized score among the simile candidates for   each original text is utilized . Suppose there are   msimile candidates S={s , s , ... , s}for the   literal text l , the original relevance scores of Ris   R={r , r , ... , r}respectively . The normalized   relevance score rofsis formulated as follows :   which ranges from 0 to 1 . Then , the normalized   logical and sentiment consistency score c , cfor   each simile sare obtained in the same manner .   Finally , the quality for simile sis defined as the   weighted combination of three parts as follows :   where α , β , and γare hyperparameters .   3.2 Creativity   Creative similes can provide a better literary experi-   ence ( Jones and Estes , 2006 ) . In Tab . 1 , comparing   “ sarcasm ” to “ vitriol ” is less common than to “ fire ” ,   yet it better conveys the intensity of a person ’s sar-   casm . Hence , we design creativity score .   Previous studies mainly evaluate the creativity   of text generation tasks via human evaluation ( Sai   et al . , 2022 ) , since measuring the creativity of open-   ended text is a relatively difficult task ( Celikyilmaz   et al . , 2020 ) . Although there have been many works   evaluating the diversity of open - ended text gener-   ation ( Li et al . , 2016 ; Zhu et al . , 2018 ; Tevet and   Berant , 2021 ) , these metrics are not suitable for   measuring the creativity of the text . Because the   diversity metrics take a set of generated text as in-   put and output one score , while a creativity metric   is required to measure each text individually and   output a set of corresponding scores .   Different from other open - ended generation   tasks , the components of the generated similes en-   able us to evaluate creativity automatically . Ac-   cording to linguists , the creativity of a simile is   determined by vehicles ( Pierce and Chiappe , 2008 ;   Roncero and de Almeida , 2015 ) . Intuitively , the   generated simile may be less creative if its extractedtopic - vehicle pair co - occurs frequently , or if many   topics are compared to its vehicle in the corpus .   Therefore , we adopt large - scale corpora as refer-   ences when designing our creativity metric . The   creativity score of sis calculated as follows :   where there are mvehicles extracted from the   simile s , each denoted as v. Ndenotes the fre-   quency of the vehicles appearing in the similes in   the corpora . The log transformation aims to reduce   the influence of extreme values .   An effective way to obtain the adequate fre-   quency information Nis to utilize the million-   scale simile knowledge base MAPS - KB , where the   Ncan be defined as follows :   Gis the set of triplets containing the vehicle vin   MAPS - KB , ndenotes the frequency of the triplet .   It is noticed that the metric is not coupled with   MAPS - KB , as Ncan also be obtained by count-   ing the samples containing the vehicle vin large-   scale simile sentences . The method of obtaining   the simile sentences is beyond the scope of this   paper . Nevertheless , we discuss the effects of the   referenced dataset size in Sec . 4.2.2 .   3.3 Informativeness   The vehicle with richer content can create a more   impact and vivid impression(Addison , 2001 ) . In   the example from Tab . 1 , the addition of the word   “ angry ” makes the similes more expressive . There-   fore , we design the metric informativeness to mea-   sure the content richness of the vehicles .   Intuitively , the more words a vehicle contains ,   the richer its content will be . Hence , for a   given simile s , we adopt the average length of   the extracted vehicles to be the informativeness   score(Chakrabarty et al . , 2020 ; Zhang et al . ,   2021 ) , defined as I=/summationtextlen(v ) , where   there are mvehicles extracted from simile s.   4HAUSER Analysis   In this section , we conduct experiments to verify   the effectiveness of our automatic metrics.12561   4.1 Experiment Setup   4.1.1 Simile Generation   The existing datasets for the SG task are either   Chinese ( Zhang et al . , 2021 ) , limited to the simile   triplet completion ( Roncero and de Almeida , 2015 ;   Chen et al . , 2022 ) , or having all vehicles located at   end of the sentence ( Chakrabarty et al . , 2022 ; Lai   and Nissim , 2022 ) , which are not practical for En-   glish simile generation in a real - world application .   To bridge the gap , we construct a large - scale En-   glish dataset for SG task based on simile sentences   from ( He et al . , 2022 ) , which contains 524k sim-   ile sentences labeled with topic and vehicle . The   output decoder target is the simile sentence sand   the input encoder source is srewritten to drop the   comparator “ like ” and the vehicle . For example ,   given s= “ The idea resounded like a thunderclap   throughout the land . ” , the encoder source would   be “ The idea resounded throughout the land . ” . In   particular , we remove the simile sentences whose   event is a linking verb ( e.g. be , seem , turn ) as they   would be meaningless after the vehicle is removed .   The final train , validation and test sets contain 139k ,   2.5k , and 2.5k sentence pairs , respectively .   Based on our constructed dataset , we fine-   tune a pre - trained sequence - to - sequence model ,   BART ( Lewis et al . , 2020 ) , for the SG task , which   has been demonstrated to be an effective framework   for various figurative language generation ( Zhang   and Wan , 2021 ; Chakrabarty et al . , 2022 ; He et al . ,   2022 ; Lai and Nissim , 2022 ) . The experiments are   run on RTX3090 GPU and the implementation of   BART is based on the HuggingFace Transformers .   The experiments are run with a batch size of 16 , a   max sequence length of 128 , and a learning rate of   4e-5 for 10 epochs .   4.1.2 Evaluation Dataset Construction   Firstly , we randomly sample 50 literal sentences   from the test set and adopt the trained SG model   to generate five candidates for each one . Then , for   each perspective , three raters are asked to rate each   simile from 1 to 5 , where 1 denotes the worst and   5 denotes the best . Since evaluating the quality   of generated similes is subjective and blurred ( Nic-   ulae and Danescu - Niculescu - Mizil , 2014 ) , we re-   move the simile - literal sentence pairs if ( 1 ) raters   argue that the pairs lack context and are difficult   to rate ( e.g. “ Nobody can shoot . ” ) or ( 2 ) some   raters rate them as low quality ( quality score of 1-   2 ) , while others rate them as high quality ( scores of   4 - 5 ) ( Niculae and Danescu - Niculescu - Mizil , 2014 ) .   Moreover , we measure the inter - rater agreement   by holding out the ratings of one rater at a time ,   calculating the correlations with the average of the   other rater ’s ratings , and finally calculating the av-   erage or maximum of all the held - out correlations   ( denoted as “ Mean ” and “ Max ” , respectively ) . The   inter - rater agreement before and after applying the   filtering strategies is shown in Tab . 2 . Overall , the   final inter - rater agreement ensures the reliability   of our evaluation of automatic metrics and the fil-   tering strategies improve the inter - rater agreement   generally . We finally get 150 simile candidates   generated from 44 literal sentences .   4.2 Results   4.2.1 Quality   We compare our quality metric with the follow-   ing automatic metrics : ( 1 ) BLEU ( Papineni et al . ,12562   2002 ) calculates the precision of n - gram matches ,   ( 2)RougeL ( Lin , 2004 ) is a recall - oriented metric ,   ( 3)METEOR ( Denkowski and Lavie , 2014 ) pro-   poses a set of linguistic rules to compare the hypoth-   esis with the reference , ( 4 ) BERTScore ( Zhang   et al . , 2019 ) calculates the cosine similarity be-   tween the BERT embeddings , ( 5 ) Perplexity ( Pang   et al . , 2020 ) measures the proximity of a language   model , the inverse of which is utilized .   Correlations with Human Ratings . Tab . 3   shows the correlation coefficients between auto-   matic metrics and human ratings . Firstly , our met-   rics are significantly more correlated with human   ratings than prior automatic metrics . Moreover , all   the sentence - level metrics , which consider the se-   mantics of the entire sentence , perform worse than   almost all the n - gram - level metrics , which compare   the n - grams between the hypothesis and the refer-   ence , which reveals that simile components need   to be specifically considered during SG evaluation .   According to the visualized correlation result in   Fig . 3 , datapoints from prior automatic metrics tend   to scatter at 0 or 1 , while the datapoints from our   metric are distributed closer to the fitter line , prov-   ing that our metric can better measure the quality .   Recommendation Task . We compare the   rankings given by automatic metrics with hu-   man rankings . We adopt the following met-   rics : Hit Ratio at rank K ( HR@K ( K=1,3 ) ) , Nor-   malized Discounted Cumulative Gain at rank   K ( NDCG@K ( K=1,3 ) ) , and Mean Reciprocal   Rank ( MRR ) . From Tab . 4 , our metric achieves   significant improvement compared to other metrics ,   indicating that our metric can yield more accurate   rankings for quality . Also , the n - gram - level metrics   generally outperform sentence - level metrics , which   is consistent with the result in Tab . 3 .   Ablation Study . To investigate the importance   of different sub - metrics in quality metric , we com-   pare the correlation between quality metric and hu-   man ratings after removing each sub - metric individ-   ually . From Tab . 3 , the removal of any sub - metric   leads to a decline in performance , which proves   the effectiveness of each sub - metric . Among three   components , the removal of the relevance results   in the largest performance drop , which reveals that   relevance is the most important sub - metric .   The Effects of Hyperparameters . Since differ-   ent sub - metrics have varying levels of importance ,   we study the correlation results when gradually in-   creasing the weight of relevance component and de-   creasing the weight of sentiment consistency com-   ponent ( as in Tab . 5 ) . From Fig . 4 ( left ) , increasing   the weight of the relevance component consistently   results in improved performance , peaking at the   combination [ 7 ] ( α , β , γ = 3/6,2/6,1/6 ) , before   eventually causing a decline in performance . This   reveals that although relevance is the most impor-   tant sub - metric , too much weight on it can be detri-   mental .   The Effects of Referenced Dataset Size . We   sample different numbers of simile sentences   from ( He et al . , 2022 ) as references for relevance12563   score and study the correlation between the quality   metric and human ratings . From Fig . 4 ( right ) ,   correlations grow linearly with exponential growth   in referenced dataset size , indicating that using   datasets larger than 100k will improve the correla-   tion coefficients . Moreover , the performance at the   peak surpasses the prior automatic metrics , proving   the effectiveness of our approximation method .   4.2.2 Creativity   We compare our creativity metric with the follow-   ing automatic metrics : ( 1 ) Perplexity which is   often utilized to measure diversity as well ( Tevet   and Berant , 2021 ) , ( 2 ) Self - BLEU ( Zhu et al . ,   2018 ) calculates the BLEU score of each gener-   ation against all other generations as references , ( 3 )   Distinct n - grams(Dist ) ( Tevet and Berant , 2021 ) ,   which is the fraction of distinct n - grams from all   possible n - grams across all generations .   Correlations with Human Ratings . From   Tab . 6 , our metric creativity is significantly more   correlated with human evaluation scores compared   with prior diversity metrics . According to the visu-   alized correlation result in Fig . 5 , the prior diversity   metrics have either wide confidence intervals ( Per-   plexity , Dist ) or scattered datapoints ( self - BLEU ) ,   whereas our creativity metrics exhibit stronger lin-   ear correlation and narrower confidence intervals   ( Creativty w/ Log ) , implying higher reliability .   Recommendation Task . We compare the rank-   ings given by automatic metrics with human rank-   ings . According to Tab . 7 , our creativity metric   outperforms prior automatic metrics , which proves   our metric can better measure the creativity of sim-   ile candidates given a literal sentence , which is   consistent with the results in Tab . 6 .   Ablation Study . According to Tab . 6 , removing   the log transformation leads to significant perfor-   mance drops . According to the visualized correla-   tion result in Fig . 5 , the datapoints are distributed   closer to the fitter line and exhibit narrower confi-   dence intervals after applying the log transforma-   tion , which further proves that log transformation   is essential for our creativity metric .   The Effects of Referenced Dataset Size . Ac-   cording to Fig . 6 ( left ) , the correlation coeffi-   cients increase continuously and eventually con-   verge as the number of referenced sentences in-   creases . Moreover , the performance after conver-   gence is comparable to that given by the creativity   metric based on the simile KB . The trend reveals   that our metric referencing 10k similes can achieve   a promising correlation with human ratings .   4.2.3 Informativeness   The Pearson and Spearman correlation coefficients   between our informativeness metric and human rat-   ings are 0.798 and 0.882 , respectively . According   to Fig . 6 ( right ) , the strong linear correlation be-   tween the metric and human ratings proves that our12564   informativeness metric is simple yet quite effective .   4.2.4 Relation between Metrics   We present pair - wise correlations between the three   automatic metrics in Tab . 8 and also visualize them   in Fig . 7 . Among the three metrics , creativity cor-   relates with informativeness moderately , mainly   because shorter vehicles tend to be less creative   than longer ones . The correlations of all other pair-   wise metrics are relatively weak . Thus , it is evident   that the three metrics are independent of each other   and it is necessary to measure each one of them to   obtain a holistic view of SG evaluation .   5HAUSER Application   We perform a case study to prove that our designed   automatic metrics are effective for various meth-   ods . Here , we apply our metrics to a retrieval   method ( Zhang et al . , 2021 ) ( denoted as BM25 ) , which utilizes the 20 context words around the in-   sertion position given by groundtruth to retrieve the   5 most similar samples based on the BM25 ranking   score from the training set , and adopts the vehicles   from these samples to be those of simile candidates .   This method ensures the diversity of generated sim-   iles . The method introduced in Sec . 4.1 is denoted   asOurs . Given the candidates generated by each   method , we rerank them using a weighted combi-   nation of quality , creativity , and informativeness   rankings obtained by HAUSER , with a ratio of 2:2:1 .   From Tab . 11 in Appendix , the candidates gen-   erated by various methods can be more correlated   with human rankings after being ranked by our met-   rics , thus proving the generality of our metrics . It   is noticed that the insertion position for BM25 is   provided by the groundtruth , while the insertion   position for Ours is predicted by the model , thus   proving the effectiveness of our generation method .   6 Conclusion   In this work , we systematically investigate the eval-   uation of the Simile Generation ( SG ) task . We   establish a holistic and automatic evaluation sys-   tem for the SG task , containing five criteria from   three perspectives , and propose holistic automatic   metrics for each criterion . Extensive experiments   verify the effectiveness of our metrics .   Acknowledgements   This research is funded by the Science and Tech-   nology Commission of Shanghai Municipality   Grant ( No . 22511105902 ) , Shanghai Munici-   pal Science and Technology Major Project ( No .   2021SHZDZX0103 ) , National Natural Science   Foundation of China ( No . 62102095 ) .   Limitations   We analyze the limitations of our work as follows .   Firstly , although applying a million - scale simile   knowledge base or large - scale simile sentences as   reference makes our designed metric significantly12565more correlated with humans than prior reference-   based metrics ( e.g. BLEU , Rouge , BERTScore ) ,   our metrics are still reference - based and rely on   the quality and scale of referenced data . We have   discussed the effect of referenced dataset size in   our paper and will design reference - free metrics to   further complement our metrics in future work . Ad-   ditionally , since our metrics utilize a million - scale   simile knowledge base or large - scale simile sen-   tences as references , the efficiency of our method is   slightly lower than the automatic metrics based on   a few references . Nevertheless , this limitation does   not prevent our metrics from performing systematic   and scalable comparisons between SG models .   Ethical Considerations   We provide details of our work to address potential   ethical considerations . In our work , we propose   holistic and automatic metrics for SG evaluation   and construct an evaluation dataset to verify their   effectiveness ( Sec . 4.1 ) . All the data sources used   in our evaluation dataset are publicly available . The   details about human ratings , such as the instruc-   tions provided to raters , are provided in Appx . A.   In our case study ( Sec . 5 ) , the human rankings are   discussed by three raters . We protect the privacy   rights of raters . All raters have been paid above   the local minimum wage and consented to use the   evaluation dataset for research purposes covered   in our paper . Our work does not raise any ethical   considerations regarding potential risks and does   not involve the research of human subjects .   References1256612567A Human Ratings   The instructions given to raters are detailed as fol-   lows :   1.All raters are provided with the necessary   background information on similes and the   simile generation task , including the defini-   tion of similes , the main simile components ,   and the motivation of our proposed criteria .   2.To ensure the quality of ratings , all the raters   label a small set of 20 samples to reach an   agreement on the labeling criteria for each   metric before the formal labeling .   3.For each perspective ( i.e. quality , creativity ,   informativeness ) , three raters are asked to rate   each simile from 1 to 5 , where 1 denotes the   worst and 5 denotes the best . The examples of   our human ratings are provided in Tab . 10 .   4.During the rating , raters are asked to specif-   ically label the simile - literal sentence pairs   which lack context and are thus difficult to   rate ( e.g. “ Nobody can shoot . ” ) .   B NDCG Formulation   In our setting , the optimal rankings are human   rankings . Hence , given msimile candidates S=   { s , s , ... , s } , the NDCG@k given by each auto-   matic metric is defined as follows :   where OandOrepresent the score list given   by humans and each automatic metric respectively ,   O[j]denote the score of s , I(i)denotes the index   of the i - th largest score in O.   C The Implementation of Prior Metrics   We report the packages used to implement prior   automatic metrics in Tab . 9 . For the metric denoted   with an asterisk ( * ) , we apply the corresponding   package to implement the key parts , based on the   definition from the cited papers . The formulation   of NDCG in our setting is provided in Appx . B.   The rest of the metrics are entirely implemented by   us according to the cited papers .   D The Details of MAPS - KB   MAPS - KB ( He et al . , 2022 ) is a million - scale prob-   abilistic simile knowledge , containing 4.3 million   simile triplets from 70 GB corpora , along with   frequency and two probabilistic metrics , plausi-   bility andtypicality , to model each triplet . The   simile triplet is in the form of ( topic , property ,   vehicle ) ( t , p , v ) .   In our paper , we specifically adopt the frequency   andplausibility information from MAPS - KB to im-   plement our relevance metric . With regard to plau-   sibility , it evaluates the quality of simile triplets   based on the confidence score of their supporting   simile instances ( simile sentence , topic , property ,   vehicle ) ( s , t , p , v ) . In each simile instance , the   topic and vehicle are extracted from the simile sen-   tence , while the property is generated via genera-   tive commonsense model COMET ( Bosselut et al . ,   2019 ) and prompting the PLMs . MAPS - KB adopt   thenoisy - or model to measure the plausibility of   the triplet ( t , p , v ) , which is defined as follows :   where S(s , t , p , v ) = P(p|s , t , v)is the confi-   dence score of each simile instance during gen-   eration and ηis the number of simile instances   supporting the simile triplet ( t , p , v).125681256912570ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   the “ Limitations ” Section .   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   the “ Abstract ” Section and the Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   I adopt the free api from text - davinci-003 to polish the language of the whole paper .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4,5   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4,5 and Appendix A , C , D   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Section 4 , 5 , the “ Ethical Consideration ” Section , and Appendix C , D   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 3 , 4 , 5 , and Appendix D   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   the “ Ethical Consideration ” Section   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4 , Appendix A   C / squareDid you run computational experiments ?   Section 4 , 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4 , 512571 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3 , 4 , Appendix C , D   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 4   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix A   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   the “ Ethical Consideration ” Section .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   the “ Ethical Consideration ” Section .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.12572