  Ehsan AghazadehMohsen FayyazYadollah Yaghoobzadeh   School of Electrical and Computer Engineering ,   College of Engineering ,   University of Tehran , Tehran , Iran   feaghazade1998 , mohsen.fayyaz77 , y.yaghoobzadeh g@ut.ac.ir   Abstract   Human languages are full of metaphorical ex-   pressions . Metaphors help people understand   the world by connecting new concepts and   domains to more familiar ones . Large pre-   trained language models ( PLMs ) are therefore   assumed to encode metaphorical knowledge   useful for NLP systems . In this paper , we in-   vestigate this hypothesis for PLMs , by probing   metaphoricity information in their encodings ,   and by measuring the cross - lingual and cross-   dataset generalization of this information . We   present studies in multiple metaphor detection   datasets and in four languages ( i.e. , English ,   Spanish , Russian , and Farsi ) . Our extensive ex-   periments suggest that contextual representa-   tions in PLMs do encode metaphorical knowl-   edge , and mostly in their middle layers . The   knowledge is transferable between languages   and datasets , especially when the annotation is   consistent across training and testing sets . Our   ﬁndings give helpful insights for both cogni-   tive and NLP scientists .   1 Introduction   Pre - trained language models ( PLMs ) ( Peters et al . ,   2018 ; Devlin et al . , 2019 ) , are now used in almost   all NLP applications , e.g. , machine translation ( Li   et al . , 2021 ) , question answering ( Zhang et al . ,   2020 ) , dialogue systems ( Ni et al . , 2021 ) , and sen-   timent analysis ( Minaee et al . , 2020 ) . They have   sometimes been referred to as “ foundation models ”   ( Bommasani et al . , 2021 ) due to their signiﬁcant   impact on research and industry .   Metaphors are important aspects of human lan-   guages . In conceptual metaphor theory ( CMT )   ( Lakoff and Johnson , 2008 ) , metaphor is deﬁned as   a cognitive phenomenon associating two different   concepts or domains . This phenomenon is built in   cognition and expressed in language . The creativ-   ity and problem solving ( i.e. , generalization to newFigure 1 : An illustration of our probing and gener-   alization scenarios for metaphorical knowledge .   problems ) depend on the analogies and metaphors   a cognitive system , like our brain , relies on . Mod-   eling metaphors is therefore essential in building   human - like computational systems that can relate   emerging concepts to the more familiar ones .   So far , there has been no comprehensive analysis   of whether and how PLMs represent metaphori-   cal information . We intuitively assume that PLMs   must encode some information about metaphors   due to their great performance in metaphor detec-   tion and other language processing tasks . Con-   ﬁrming that experimentally is a question that we   address here . Speciﬁcally , we aim to know whether   generalizable metaphorical knowledge is encoded   in PLM representations or not . The outline of our   work is presented in Figure 1 .   We ﬁrst do probing experiments to answer ques-   tions such as : ( i ) with which accuracies and ex-   tractablities do different PLMs encode metaphor-   ical knowledge ? ( ii ) how deep is the metaphori-   cal knowledge encoded in PLM multi - layer repre-   sentations ? We take two probing methods , edge   probing ( Tenney et al . , 2019b ) and minimum de-   scription length ( V oita and Titov , 2020 ) , and apply   them to four metaphor detection datasets , namely   LCC ( Mohler et al . , 2016 ) , TroFi ( Birke and Sarkar ,   2006 ) , VUA pos , and VUA Verbs ( Steen , 2010).2037To better estimate the generalization of   metaphorical knowledge in PLMs , we design two   setups in which testing comes from a different   distribution than training data : cross - lingual and   cross - dataset metaphor detection . Each setup can   reveal important information on whether or not the   metaphorical knowledge is encoded consistently   in PLMs . Four languages ( English , Farsi , Russian   and Spanish ) and four datasets ( LCC , TroFi , VUA   pos , and VUA Verbs ) are considered in these gen-   eralization experiments .   In summary , this paper makes the following con-   tributions :   •For the ﬁrst time , and through careful probing   analysis , we conﬁrm that PLMs do encode   metaphorical knowledge .   •We show that metaphorical knowledge is en-   coded better in the middle layers of PLMs .   •We evaluate the generalization of metaphori-   cal knowledge in PLMs across four languages   and four dataset sources , and ﬁnd out that   there is considerable transferability for the   pairs with consistent data annotation even if   they are in different languages .   2 Related Work   Metaphor detection using PLMs . The   metaphor detection task ( Mason , 2004 ; Birke   and Sarkar , 2007 ; Shutova et al . , 2013 ) is a good   ﬁt for analyzing the metaphorical knowledge .   Using PLMs for metaphor detection has been   common in recent years , setting new state - of-   the - art results , indicating implicitly that PLMs   represent metaphorical information . Choi et al .   ( 2021 ) introduce a new architecture that integrates   metaphor detection theories with BERT . They   use the deﬁnitions as well as example usages of   words jointly with PLM representations . Similarly ,   Song et al . ( 2021 ) presents a new perspective on   metaphor detection task by framing it as relation   classiﬁcation , focusing on the verbs . These   approaches beat the earlier work of using PLMs   ( Su et al . , 2020 ; Chen et al . , 2020 ; Gong et al . ,   2020 ) , RNN - based ( Wu et al . , 2018 ; Mao et al . ,   2019 ) and feature - based approaches ( Turney et al . ,   2011 ; Shutova et al . , 2016 ) . Note that our goal is   not to compete with these models , but to probe and   analyze the relevant knowledge in PLMs . Tsvetkov et al . ( 2014 ) present cross - lingual   metaphor detection models using linguistic fea-   tures and word embeddings . Bilingual dictionaries   map different languages . Their datasets are quite   small ( ˜1000 training and ˜200 testing examples ) ,   making them unsuitable for a robust evaluation .   However , this paper still remains as the only cross-   lingual evaluation of metaphor detection , to the   best of our knowledge . Here , using multilingual   PLMs , we perform zero - shot cross - lingual trans-   fer for metaphor detection . Our goal is to test if   PLMs represent metaphorical knowledge transfer-   able across languages .   Probing methods in NLP . Probing is an analyti-   cal tool used for assessing linguistic knowledge in   language representations . In probing , the informa-   tion richness of the representations is inspected by   the quality of a supervised model in predicting lin-   guistic properties based only on the representations   ( K¨ohn , 2015 ; Gupta et al . , 2015 ; Yaghoobzadeh and   Sch¨utze , 2016 ; Conneau et al . , 2018 ; Tenney et al . ,   2019b , a ; Yaghoobzadeh et al . , 2019 ; Hewitt and   Manning , 2019 ; Zhao et al . , 2020 ; Belinkov , 2022 ) .   Here , we apply probing to perform our study on   whether metaphorical knowledge is present in PLM   representations , and whether that is generalizable   across languages and datasets .   A popular probing method introduced by Tenney   et al . ( 2019b ) is edge probing ( Figure 2 ) . They   propose a suite of span - level tasks , including POS   tagging and coreference resolution . Despite the   widespread use of edge probing and other conven-   tional probes , the question of whether the probing   classiﬁer is learning the task itself rather than iden-   tifying the linguistic knowledge raises concerns .   An Information - theoretic view can solve this is-   sue ( V oita and Titov , 2020 ) by reformulating prob-   ing as a data transmission problem . They consider   the effort needed to extract linguistic knowledge in   addition to the ﬁnal quality of the probe , showing   that this approach is more informative and robust   than normal probing methods . We employ both   edge and MDL probing in this work .   Probing multilingual PLMs . The application of   probing methods in NLP is extended to multilin-   gual PLMs as well ( Pires et al . , 2019 ; Eichler et al . ,   2019 ; Ravishankar et al . , 2019a , b ; Choenni and   Shutova , 2020 ) . Choenni and Shutova ( 2020 ) in-   troduce probing tasks for typological features of   multiple languages in multilingual PLMs . Ravis-2038hankar et al . ( 2019a , b ) extend the probing tasks   of Conneau et al . ( 2018 ) , to a few other lan-   guages . Pires et al . ( 2019 ) study the generaliza-   tion of multilingual - BERT across languages when   performing cross - lingual downstream tasks . Here ,   as part of our study , we probe the generalization   of metaphorical knowledge in XLM - R ( Conneau   et al . , 2020 ) , a notable multilingual PLM .   Out - of - distribution generalization . There has   been no earlier work on studying or evaluating out-   of - distribution generalization in metaphor detec-   tion . This generalization refers to scenarios where   testing and training sets come from different distri-   butions ( Duchi and Namkoong , 2018 ; Hendrycks   et al . , 2021 , 2020 ) . Here , we have scenarios where   testing and training data are in different languages   or domains / datasets . These are challenging eval-   uation scenarios for the generalization of encoded   information ( metaphoricity in our case ) .   3 Inspecting Metaphorical Knowledge in   PLMs   Metaphors are used frequently in our everyday lan-   guage to convey our thoughts more clearly . There   are related theories in linguistics and cognitive sci-   ence . Following linguistic theories , metaphoric-   ity is mostly annotated using metaphor identiﬁ-   cation procedure ( MIP ) . MIP identiﬁes a word   in a given context as a metaphor if it has a ba-   sic or literal meaning that contrasts with its con-   text words . Based on conceptual metaphor the-   ory ( CMT ) ( Lakoff and Johnson , 2008 ) , one target   domain ( e.g. , ARGUMENT ) is explained using a   source domain ( e.g. , WAR ) . The source domain is   usually more concrete or physical , while the tar-   get is more abstract . Relating these two theories ,   metaphors are expressed in language connecting   two contrasting domains . For example , in “ We   won the argument ” , the domain of ARGUMENT   is linked to the domain of WAR by using the word   “ won ” . The word “ won ” is a “ metaphor ” here since   its primary domain contrasts with its contextual   domain . The same word “ won ” in a sentence like   “ The Allies won the war ” refers to its literal mean-   ing and therefore is not a metaphor . The task of   metaphor detection is deﬁned to do this classiﬁca-   tion of “ literal ” and “ metaphor ” .   Accordingly , when designing a metaphor detec-   tion system , to ﬁgure out if a token is a metaphor   in a particular context , we assume following a pro-   cess like : ( i ) ﬁnding if the token has multiple mean - Figure 2 : Probing architecture for metaphors em-   ployed in edge probing and MDL probing .   ings in different domains , including a more basic ,   concrete , or body - related meaning . For example ,   “ ﬁght ” , “ win ” and “ mother ” satisfy this condition .   ( ii ) ﬁnding if the source domain of the token con-   trasts with the target domain . Here the contrast   is important and ﬁnding the exact domains might   not be necessary . The source domain , in which its   literal / basic meaning resides , is a non - contextual   attribute , while the target domain is mainly found   using the contextual clues ( WAR and ARGUMENT   for “ won ” in the above example ) .   Here , we use the metaphor detection datasets   annotated based on these theories and analyze   the PLM representations to see if they encode   metaphorical knowledge and if the encoding is   generalizable . To do so , we ﬁrst probe PLMs for   their metaphorical information , generally and also   across layers . This gives us intuition on how well   metaphoricity is encoded and how local or con-   textual that is . Then , we test if the knowledge   of metaphor detection can be transferred across   languages and if multilingual PLMs capture that .   Finally , the generalization of metaphorical knowl-   edge across datasets is examined to see if the theo-   ries and annotations followed by different datasets   are consistent , and if PLMs learn generalizable   knowledge rather than dataset artifacts .   3.1 Probing   Here , we aim to answer general questions about   metaphors in PLMs : do PLMs encode metaphori-   cal information and , if so , how it is distributed in2039their layers . We do not attempt to achieve the best   metaphor detection results but to analyze layers of   PLMs to test if they contain the necessary infor-   mation to perform this task . In trying to answer   this question , we apply probing methods , discussed   as follows , to focus on the representation itself by   freezing the PLM parameters and training classi-   ﬁers on top .   We hypothesize that metaphorical information   does exist in PLM layers and more in the middle   layers . As we discussed earlier , metaphor detection   depends on contrast prediction between source and   target domains of a token . We assume that this pre-   diction is made mainly based on the initial layers   of PLM representations of either the token itself   or its context or both . In higher layers of PLMs ,   the representations are dominated by contextual   information , making it hard to retrieve the source   domain , and so , reasoning about the contrast of the   source and target domains becomes difﬁcult .   Methods We employ edge probing ( Tenney et al . ,   2019b ) and MDL ( V oita and Titov , 2020 ) . Edge   probing consists of a classiﬁer in which word repre-   sentations obtained from PLMs are fed as inputs af-   ter projecting to 256 - dimensional vectors ﬁrst . The   quality of the classiﬁer illustrates how well the rep-   resentations encode a speciﬁc linguistic knowledge .   This method is designed for span - level tasks , i.e. ,   the classiﬁer can only access the representations   of a limited part of the input sentence speciﬁed in   the dataset . Edge probing has two pooler sections   for making ﬁxed - sized vectors ; one pools represen-   tations across the words in the span and the other   pools representations across the layers .   The Minimum Description Length ( MDL ) prob-   ing is based on information theory and combines   the quality of the classiﬁer and the amount of effort   needed to achieve this quality .   V oita and Titov ( 2020 ) propose two methods for   computing MDL : “ variational coding ” and “ online   coding . ” The former computes the complexity of   the classiﬁer with a Bayesian model . In the latter ,   the classiﬁer is trained gradually on different por-   tions of the dataset , and the code length will be the   sum of the cross - entropies , each for a data portion .   V oita and Titov ( 2020 ) show that the two methods ’   results are consistent with each other . Accordingly ,   we opted for the “ online coding ” method since it is   more straightforward in implementation . Since the   code length is related to the size of the dataset N ,   we report the “ compression ” , which is equal to 1for a random classiﬁer and larger for better models ,   and is deﬁned as : compression = See   extra details in V oita and Titov ( 2020 ) .   3.2 Generalization   To see if PLMs encode generalizable metaphori-   cal knowledge , we evaluate them in settings where   testing and training data are in different distribu-   tions . We explore transferability analysis across   languages and datasets as two sources of distribu-   tion . We explain each in the following sections .   3.2.1 Cross - lingual   Multilingual encoders project the representations   in multiple languages into a shared space so that   semantically similar words and sentences across   languages end up close to each other . If we use a   multilingual PLM model , and our classiﬁer shows   that representations in language Sare informative   about metaphoricity , what happens if we apply this   classiﬁer to the representations in language T ? We   hypothesize that if the representation is rich in both   languages , the annotation of metaphor is consis-   tent , and the concept of metaphor is transferable   across languages , then the classiﬁer would be able   to predict metaphoricity in language Tfrom what   it learns in S.   When testing cross - lingual generalization , the   linguistic and cultural differences of metaphoricity   are important as well . We assume that metaphors   are conceptualized in a similar process across lan-   guages , and metaphor detection is deﬁned consis-   tently . The lexicalization is , of course , different ,   but that is something that multilingual PLMs are   supposed to handle to some extent .   3.2.2 Cross - dataset   When training and testing on the same distribution ,   any learning model often uses heuristics and an-   notation biases . The consequence is the recurring   overestimation of the capabilities of PLMs in doing   hard tasks . This might be the case for our probing   experiments as well . Therefore , another generaliza-   tion dimension we consider is cross - dataset trans-   fer , i.e. , training on dataset Sand testing on dataset   T.SandTcould be annotated by different peo-   ple with possibly different goals in mind , and their   raw sentences could come from different domains .   However , they must be annotated for the same task   of metaphor detection .   In our case , the four datasets discussed more   in§4.1 differ in their distribution of the candidate2040   Table 1 : Examples of sentences , spans , and target labels for each probing dataset .   Table 2 : Statistics of the datasets . We label - balance   each to have 50 % metaphor . Number of instances   for train / dev / test sets and the types of POS are   given as well . N : Noun , V : Verb , ALL : Noun , Verb ,   Adjective , Adverb .   POS types ( e.g. , TroFi is only verbs , but LCC is   not ) . Further , the annotation process is different   as each follows its own guidelines . However , the   essential task of metaphor detection , i.e. , distin-   guishing metaphor and literal usages , is the same   for all . Therefore , we expect some transferability   across datasets but with differences aligned with   their mismatches .   4 Experimental Setup and Results   4.1 Datasets and Setup   Datasets We use four metaphor detection   datasets in our study . The annotations of LCC   ( Mohler et al . , 2016 ) are done mostly on web   crawled data as well as news corpora . It provides   metaphoricity scores including 0 as no , 2 as con-   ventional , and 3 as clear metaphor . We use the   examples with score 0 as literal , and others as   metaphor .   TroFi dataset ( Birke and Sarkar , 2006 ) consistsof metaphoric and literal usages of 51 English verbs   from WSJ . VUA ( Steen , 2010 ) corpus consists of   words in the academic , ﬁction , and news subdo-   mains of the British National Corpus ( BNC ) . The   authors published two versions : VUA POS and   VUA Verbs .   LCC contains annotations in four languages : En-   glish , Russian , Spanish , and Farsi . The other three   datasets , TroFi , VUA Verbs and VUA POS , are   in English only . We have label - balanced all the   datasets to get a more straightforward interpreta-   tion of results ( the accuracy of a fair - coin random   baseline is 50 % in all cases ) and have split the   datasets to train / dev / test sets with ratios of 0.7 /   0.1 / 0.2 .   The statistics of the datasets are shown in Ta-   ble 2 . Example sentences with the corresponding   annotations can be seen in Table 1 .   Setup In implementing the edge probe , we use   batch size = 32 and learning rate = 5e-5 and train for   ﬁve epochs in all experiments . For the MDL probe ,   the same structure of edge probing is employed .   We apply a logarithm to the base two instead of the   natural logarithm in cross - entropy loss to have all   the obtained code lengths in bits ( see extra details   in V oita and Titov 2020 ) . Our experiments are done   using the GPUs provided by Google Colab free and   pro .   4.2 Probing Results   Here , BERT ( Devlin et al . , 2019 ) , RoBERTa ( Liu   et al . , 2019 ) , and ELECTRA ( Clark et al . , 2020 )   represent our PLMs . Due to our resource limi-   tations , we conduct all experiments on the base   version of the models ( 12 layers , 768 hidden size ,   110 M parameters ) implemented in HuggingFace ’s   Transfomers ( Wolf et al . , 2020 ) . We employ edge   probing for evaluating overall metaphorical knowl-2041Baseline BERT RoBERTa ELECTRA   Dataset Acc . Comp . Acc . Comp . Acc . Comp . Acc . Comp .   LCC ( en ) 74.86 1.0588.25 1.8588.06 1.9689.30 2.05   TroFi 67.34 1.0168.58 1.0768.46 1.0968.07 1.08   VUA POS 65.92 1.0380.32 1.4381.72 1.4883.03 1.51   VUA Verbs 65.97 1.0478.29 1.2878.88 1.3479.96 1.31   Table 3 : Edge probing accuracy results for various metaphoricity datasets in BERT , RoBERTa , and   ELECTRA . Baseline is a randomly initialized BERT . The edge probing results are the average of three   runs . The compression result is the best across layers , and the subscript denotes the best layer .   Figure 3 : MDL compression across layers of three   PLMs in four metaphor detection datasets . Higher   number means better quality and extractability .   edge in our selected PLMs , and MDL for the layer-   wise comparisons . MDL is shown to be more effec-   tive for layer - wise probing ( Fayyaz et al . , 2021 ) .   Table 3 shows the edge probing accuracy and   MDL probing compression results for our three   PLMs . Accordingly , RoBERTa and ELECTRA are   shown to encode metaphorical knowledge better   than BERT on both metrics . This is consistent with   their better performance on various tasks , acquired   by having better pre - training objectives and / or en-   joying more extensive pre - training data . The higher   probing quality of ELECTRA ’s representations , is   also consistent with Fayyaz et al . ( 2021 ) results   on various linguistic knowledge tasks , including   dependency labeling , named entity recognition , se - mantic role labeling , and coreference resolution .   MDL probing compression across layers is   demonstrated in Figure 3 . We see the numbers   increase mostly at the ﬁrst 3 to 6 layers , depend-   ing on the dataset , but it decreases afterwards .   In other words , metaphorical information is more   concentrated in the middle layers , where the repre-   sentations are relatively contextualized but not as   much as higher layers . To put this in perspective ,   we can consider Tenney et al . ( 2019a ) and Fayyaz   et al . ( 2021 ) where the best layers for various lin-   guistic knowledge tasks in BERT are within 4 and   9 . This shows that metaphor detection in PLM   representations can be resolved earlier than some   basic linguistic tasks .   In§3.1 , we elaborated a hypothesis that the pro-   cess of detecting metaphors is not very deep since   what it needs to do is mainly contrast prediction   between source and target domains , and the deep   layers do not represent the source domain well . Our   reported probing results conﬁrm that metaphor de-   tection is not deep in PLM layers . To further evalu-   ate our reasoning , we probe the domain knowledge   in PLM representations across layers . We employ   LCC ’s annotation of source and target domains ,   and run a similar MDL probing on different PLMs   but for domain prediction . The obtained results ,   shown in Figure A.1 in appendix , demonstrate that   the source domain information is represented in the   initial layers ( 2 - 6 ) , conﬁrming that the source do-   main is dominated by other information in higher   layers . On the other hand , target domain informa-   tion generally increases across layers . Therefore ,   the middle layers can be the best place for contrast-   ing source and target domains.2042Train Lang   en es fa ruen 85.14 ( 65.37 ) 79.31 ( 52.71 ) 77.59 ( 50.22 ) 80.51 ( 52.40 )   es 79.40 ( 53.17 ) 84.59 ( 66.09 ) 76.70 ( 50.32 ) 79.68 ( 53.32 )   fa 75.70 ( 50.07 ) 75.29 ( 52.65 ) 81.04 ( 65.91 ) 77.14 ( 50.36 )   ru 83.92 ( 53.25 ) 80.54 ( 51.48 ) 76.61 ( 51.05 ) 88.36 ( 67.98 )   Table 4 : Cross - lingual metaphor detection accuracies after ﬁve epochs of training for XLM - R and ( its   random version ) . For each test language , we bold its in - distribution ( e.g. , en ! en ) , and underline the best   out - of - distribution ( e.g. , ru ! en ) numbers .   4.3 Generalization Experiments   As our PLMs , we use XLM - R ( Conneau et al . ,   2020 ) for cross - lingual and BERT for cross - dataset   experiments . To compare the cross - lingual and   cross - dataset transferability , in § 4.3.3 , we employ   the same setup , including using XLM - R as PLM for   both . The results in § 4.3.1 and 4.3.2 are not compa-   rable . We apply the same edge probing architecture   as in the probing experiments . We sometimes refer   to both language and dataset as distribution .   We run two experiments for each case of a source   distribution Sand a target distribution T : one with   the PLM and one with a randomized version of   the PLM where weights are set to random val-   ues . Randomly initialized Transformers with the   same architecture as PLMs are common baselines   in the community . The difference between the   two gives evidence about the helpfulness of the   encoded knowledge gained during pre - training in   doing the task . When S = T , this effect is mea-   sured for in - distribution and when S6 = T , for   out - of - distribution generalization . Comparing re-   sults of in - distribution ( e.g. , training and testing on   English data ) and out - of - distribution ( e.g. , training   on Spanish and testing on English ) setups demon-   strates how generalizable the metaphorical knowl-   edge in PLM is and how consistent the annotations   are .   4.3.1 Cross - lingual   The four LCC datasets corresponding to four lan-   guages are used here . We subsample from the   datasets to have the same number of examples in   the training sets , i.e. , 12,238 which is the size of   the Russian training set . The results are shown in   Table 4 . The random baseline is acquired using a   randomly initialized XLM - R.   We observe that XLM - R signiﬁcantly outper-   forms the random , conﬁrming that metaphoricalknowledge learned during the pre - training is trans-   ferable across languages . This considerable trans-   ferability can be attributed to the ability of XLM - R   to build language - universal representations useful   for metaphoricity transfer . Moreover , the innate   similarities of metaphors in distinct languages can   contribute to higher transferability , despite the lexi-   calization differences . E.g. , analogizing a concept   to a tool ( en ) occurs the same way in other lan-   guages like instrumento ( es),(fa ) and инстру-   мент ( ru ) . Finally , the constraints of the dataset   producers in , for instance , keeping the languages in   relatively similar target and source domains , could   be inﬂuential . ( See Figures A.2 and A.3 ) .   An interesting observation is that training on   Russian shows the best out - of - distribution results   when testing on other languages . We analyze this   further . First , we observe that LCC(ru ) has almost   the closest target domain distribution to all other   languages ( See Table A.2 in Appendix ) .   Second , the reported results can also be inﬂu-   enced by the amount of data from each of these   languages in the pre - training data of XLM - R. Rus-   sian has the second largest size after English ( Con-   neau et al . , 2020 ) . Finally , for English , the higher-   resource language with closer target domain distri-   bution , we ﬁnd out that there are considerable num-   ber of examples in the LCC(en ) related to “ GUNS ”   and “ CONTROL OFGUNS ” . These domains are   not covered in other LCC datasets ( See Figure A.3   in Appendix ) .   4.3.2 Cross - dataset   Similar to the cross - lingual evaluations , here we   have four datasets used as sources and targets . We   set the train size of each to the minimum of all , i.e. ,   3,838 . For each pair , we run two experiments : one   with randomized and one with pre - trained BERT   as our PLM . Results are shown in Table 5.2043Train Dataset   LCC(en ) TroFi VUA POS VUA VerbsLCC(en ) 84.26 ( 54.93 ) 62.04 ( 50.05 ) 70.35 ( 50.69 ) 70.37 ( 50.14 )   TroFi 59.49 ( 50.58 ) 68.73 ( 64.96 ) 55.38 ( 49.45 ) 59.67 ( 53.68 )   VUA POS 62.23 ( 51.47 ) 55.29 ( 50.47 ) 76.86 ( 56.01 ) 71.6 ( 53.47 )   VUA Verbs 60.20 ( 50.88 ) 54.55 ( 51.73 ) 72.6 ( 56.01 ) 75.21 ( 60.03 )   Table 5 : Cross - dataset edge probing accuracy results on BERT is shown in pairs : pre - trained model   and , in the parenthesis , the randomly initialized model . We set the training size to the minimum among   datasets , i.e. , TroFi . For each test dataset , we bold its in - distribution ( e.g. , VUA Verbs ! VUA Verbs ) ,   and underline the best out - of - distribution ( e.g. , VUA POS ! VUA Verbs ) numbers .   PLM is much better than random in all out-   of - distribution cases , suggesting the presence of   generalizable metaphorical information . As ex-   pected , VUA Verbs and POS achieve the best re-   sults when mutually tested , because , apart from   the POS , they have the same distribution . VUA   datasets and LCC(en ) show good transferability ,   but the gap with in - distribution results is still con-   siderable ( > 13 % absolute ) . VUA Verbs is the   best source for TroFi , likely because of the POS   match between them . Overall , apart from the two   VUA datasets , the gap between in- and out - of-   distribution performance is large .   The random PLM accuracies range from about   54%-64 % and 50%-56 % for in- and out - of-   distribution cases . We hypothesize that this drop in   the out - of - distribution is related to the annotation   biases , which a randomly initialized classiﬁer can   leverage better when testing and training sets are   from the same distribution . When the sets have   different distributions , the biases do not transfer   well .   4.3.3 Comparing cross - dataset and   cross - lingual   Table 6 : Comparing cross - dataset and cross - lingual   scenarios using the same model ( XLM - R ) , train-   ing size , testing set , i.e. , LCC(en ) , and different   training sources .   As additional transferability analysis , we com-   pare cross - lingual and cross - dataset results , byusing XLM - R and evaluating different training   sources on LCC(en ) test set . We make the size   of each train set to be the same ( 3,838 ) . The results   are shown in Table 6 , where the ﬁrst and second   rows belong to cross - lingual and cross - dataset , re-   spectively . To base our results , we include the   in - distribution result of training on LCC(en ) , i.e. ,   82.31 % .   Clearly , there is a substantial gap between cross-   lingual and cross - dataset accuracies . The annota-   tion guideline is consistent in the LCC language   datasets , while for the cross - dataset settings , we   have datasets that differ in many aspects , including   annotation procedure and deﬁnitions , covered part-   of - speeches ( e.g. , Troﬁ and VUA Verbs vs. LCC   and VUA POS ) and sentence lengths ( LCC : 25.9 ,   VUA : 19.4 , Troﬁ : 28.3 ) .   5 Discussion and Conclusion   Metaphors are important in human cognition , and   if we seek to build cognitively inspired or plausi-   ble language understanding systems , we need to   work more on their best integration in the future .   Therefore , any work in this regard is impactful .   Our probing experiments showed that PLMs do   in fact represent the information necessary to do   the task of metaphor detection . We assume this   information is related to metaphorical knowledge   learned during pre - training . Further , the layer - wise   analysis conﬁrmed our hypothesis that middle lay-   ers are more informative .   Even though our probing experiments did show   that metaphorical knowledge is present in PLMs ,   it was still unclear if this knowledge is generaliz-   able beyond the training data . So , to probe the   probe and evaluate generalization , we ran cross-   lingual and cross - dataset experiments . Our results   showed that the transferability across languages2044works quite well for the four languages in LCC an-   notation . However , when the deﬁnitions and anno-   tations were inconsistent across different datasets ,   the cross - dataset results were not satisfactory .   Overall , we conclude that metaphorical knowl-   edge does exist in PLM representations and in mid-   dle layers mainly , and it is transferable if the an-   notation is consistent across training and testing   data . We will explore more the cross - lingual trans-   fer of metaphors and the impact of cross - cultural   similarities in the future . Also , the application   of metaphorical knowledge for text generation is   something important that we will address .   Acknowledgements   We would like to thank the anonymous reviewers   and action editors who helped us greatly in improv-   ing our work with their comments .   References204520462047   A Appendices   Figure A.1 : MDL probing compression across   layers for source and target domain detection for   LCC(en ) dataset .   en es fa ru   en 0.0000   es 0.1622 0.0000   fa 0.1851 0.1688 0.0000   ru 0.1833 0.2239 0.2244 0.0000   Table A.1 : Jensen – Shannon divergence between   source domain frequency distribution of different   languages . The datasets are the same ones used in   cross - lingual experiments where train set sizes are   set to 12,238 . Bold denotes the closest distributions   and underline denotes the furthest distributions.en es fa ru   en 0.0000   es 0.4116 0.0000   fa 0.5004 0.2148 0.0000   ru 0.4291 0.1209 0.2141 0.0000   Table A.2 : Jensen – Shannon divergence between   target domain frequency distribution of different   languages . The datasets are the same ones used in   cross - lingual experiments where train set sizes are   set to 12,238 . Bold denotes the closest distributions   and underline denotes the furthest distributions.2048Language Sentence Annotations   faScore : 3.0   Src Concept : WAR(3.0 )   Target Concept : DEMOCRACY   Polarity : NEUTRAL   Intensity : 1.0   es[atorado]en la [ deuda]p´ublica   y sin avances en Estado de DerechoScore : 3.0   Src Concept : BARRIER(3.0 )   Target Concept : DEBT   Polarity : NEGATIVE   Intensity : 2.0   ruМировые [ деньги][мечутся ] ,   не зная , куда вложиться .Score : 3.0   Src Concept : MOVEMENT(3.0 )   Target Concept : MONEY   Polarity : NEGATIVE   Intensity : 2.0   Table A.3 : Examples of sentences , spans , and annotations for LCC dataset in Farsi , Spanish , and Russian.2049Figure A.2 : Source domain frequency in training   set of cross - lingual datasets . Figure A.3 : Target domain frequency in training   set of cross - lingual datasets.2050