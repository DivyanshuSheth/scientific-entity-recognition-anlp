  Wen Wu , Chao Zhang , Philip C. WoodlandDepartment of Engineering , University of Cambridge , Cambridge , UKDepartment of Electrical Engineering , Tsinghua University , Beijing , China   { ww368 , pcw}@eng.cam.ac.uk ; cz277@tsinghua.edu.cn   Abstract   In automatic emotion recognition ( AER ) , la-   bels assigned by different human annotators to   the same utterance are often inconsistent due   to the inherent complexity of emotion and the   subjectivity of perception . Though determin-   istic labels generated by averaging or voting   are often used as the ground truth , it ignores   the intrinsic uncertainty revealed by the incon-   sistent labels . This paper proposes a Bayesian   approach , deep evidential emotion regression   ( DEER ) , to estimate the uncertainty in emotion   attributes . Treating the emotion attribute la-   bels of an utterance as samples drawn from an   unknown Gaussian distribution , DEER places   an utterance - specific normal - inverse gamma   prior over the Gaussian likelihood and predicts   its hyper - parameters using a deep neural net-   work model . It enables a joint estimation of   emotion attributes along with the aleatoric and   epistemic uncertainties . AER experiments on   the widely used MSP - Podcast and IEMOCAP   datasets showed DEER produced state - of - the-   art results for both the mean values and the   distribution of emotion attributes .   1 Introduction   Automatic emotion recognition ( AER ) is the task   that enables computers to predict human emotional   states based on multimodal signals , such as au-   dio , video and text . An emotional state is de-   fined based on either categorical or dimensional   theory . The categorical theory claims the exis-   tence of a small number of basic discrete emo-   tions ( i.e. anger and happy ) that are inherent in   our brain and universally recognised ( Gunes et al . ,   2011 ; Plutchik , 2001 ) . Dimensional emotion the-   ory characterises emotional states by a small num-   ber of roughly orthogonal fundamental continuous-   valued bipolar dimensions ( Schlosberg , 1954 ; Nico-   laou et al . , 2011 ) such as valence - arousal and ap-   proach – avoidance ( Russell and Mehrabian , 1977;Russell , 1980 ; Grimm et al . , 2007 ) . These dimen-   sions are also known as emotion attributes , which   allow us to model more subtle and complex emo-   tions and are thus more common in psychological   studies . As a result , AER includes a classification   approach based on emotion - class - based labels and   a regression approach based on attribute - based la-   bels . This paper focuses on attribute - based AER   with speech input .   Emotion annotation is challenging due to the in-   herent ambiguity of mixed emotion , the personal   variations in emotion expression , the subjectivity   in emotion perception , etc . Most AER datasets use   multiple human annotators to label each utterance ,   which often results in inconsistent labels , either as   emotion categories or attributes . This is also a typi-   cal manifestation of the intrinsic data uncertainty ,   also referred to as aleatoric uncertainty ( Matthies ,   2007 ; Der Kiureghian and Ditlevsen , 2009 ) , that   arises from the natural complexity of emotion data .   It is common to replace such inconsistent labels   with deterministic labels obtained by majority vot-   ing ( Busso et al . , 2008 , 2017 ) or ( weighted ) av-   erages ( Ringeval et al . , 2013 ; Lotfian and Busso ,   2019 ; Kossaifi et al . , 2019 ; Grimm and Kroschel ,   2005 ) . However , this causes a loss of data sam-   ples when a majority agreed emotion class does n’t   exist ( Majumder et al . , 2018 ; Poria et al . , 2018 ;   Wu et al . , 2021 ) and also ignores the discrepancies   between annotators and the aleatoric uncertainty in   emotion data .   In this paper , we propose to model the uncer-   tainty in emotion attributes with a Bayesian ap-   proach based on deep evidential regression ( Amini   et al . , 2020 ) , denoted deep evidential emotion re-   gression ( DEER ) . In DEER , the inconsistent hu-   man labels of each utterance are considered as ob-   servations drawn independently from an unknown   Gaussian distribution . To probabilistically estimate   the mean and variance of the Gaussian distribution ,   a normal inverse - gamma ( NIG ) prior is introduced,15681which places a Gaussian prior over the mean and an   inverse - gamma prior over the variance . The AER   system is trained to predict the hyper - parameters   of the NIG prior for each utterance by maximising   the per - observation - based marginal likelihood of   each observed label under this prior . As a result ,   DEER not only models the distribution of emo-   tion attributes but also learns both the aleatoric   uncertainty and the epistemic uncertainty ( Der Ki-   ureghian and Ditlevsen , 2009 ) without repeating   the inference procedure for sampling . Epistemic   uncertainty , also known as model uncertainty , is as-   sociated with uncertainty in model parameters that   best explain the observed data . Aleatoric and epis-   temic uncertainty are combined to induce the total   uncertainty , also called predictive uncertainty , that   measures the confidence of attribute predictions .   As a further improvement , a novel regulariser is   proposed based on the mean and variance of the   observed labels to better calibrate the uncertainty   estimation . The proposed methods were evaluated   on the MSP - Podcast and IEMOCAP datasets .   The rest of the paper is organised as follows .   Section 2 summarises related work . Section 3 in-   troduces the proposed DEER approach . Sections 4   and 5 present the experimental setup and results   respectively , followed by the conclusion .   2 Related Work   There has been previous work by AER researchers   to address the issue of inconsistent labels . For emo-   tion categories , a single ground - truth label can be   obtained as either a continuous - valued mean vector   representing emotion intensities ( Fayek et al . , 2016 ;   Ando et al . , 2018 ) , or as a multi - hot vector obtained   based on the existence of emotions ( Zhang et al . ,   2020 ; Ju et al . , 2020 ) . Recently , distribution - based   approaches have been proposed , which consider the   labels as samples drawn from emotion distributions   ( Chou et al . , 2022 ; Wu et al . , 2022b ) .   For emotion attributes , annotators often assign   different values to the same attribute of each ut-   terance . Davani et al . ( 2022 ) proposed a multi-   annotator model which contains multiple heads to   predict each annotator ’s judgement . This approach   is computationally viable only when the number of   annotators is relatively small . The method requires   sufficient annotations from each annotator to be   effective . Deng et al . ( 2012 ) derived confidence   measures based on annotator agreement to build   emotion - scoring models . Han et al . ( 2017 , 2021)proposed predicting the standard deviation of the   attribute label values as an extra task in the multi-   task training framework . Dang et al . ( 2017 , 2018 )   included annotator variability as a representation of   uncertainty in a Gaussian mixture regression model .   These techniques take the variance of human an-   notations either as an extra target or as an extra   input . More recently , Bayesian deep learning has   been introduced to the task , which models the un-   certainty in emotion annotation without explicitly   using the variance of human annotations . These   include the use of Gaussian processes ( Atcheson   et al . , 2018 , 2019 ) , variational auto - encoders ( Srid-   har et al . , 2021 ) , Bayesian neural networks ( Prabhu   et al . , 2021 ) , Monte - Carlo dropout ( Sridhar and   Busso , 2020b ) and sequential Monte - Carlo meth-   ods ( Markov et al . , 2015 ; Wu et al . , 2022a ) .   So far , these methods have not distinguished   aleatoric uncertainty from epistemic uncertainty   which are defined in the introduction . Our proposed   DEER approach can simultaneously model these   two uncertainties . In addition , our approach is   more generic . It has no limits on the number of   annotators , the number of annotators per utterance ,   and the number of annotations per annotator , and   thus can cope with large crowd - sourced datasets .   3 Deep Evidential Emotion Regression   3.1 Problem setup   In contrast to Bayesian neural networks that place   priors on model parameters ( Blundell et al . , 2015 ;   Kendall and Gal , 2017 ) , evidential deep learn-   ing ( Sensoy et al . , 2018 ; Malinin and Gales , 2018 ;   Amini et al . , 2020 ) places priors over the likelihood   function . Every training sample adds support to a   learned higher - order prior distribution called the   evidential distribution . Sampling from this distri-   bution gives instances of lower - order likelihood   functions from which the data was drawn .   Consider an input utterance xwithMemotion   attribute labels y , . . . , yprovided by multiple   annotators . Assuming y , . . . , yare observa-   tions drawn i.i.d . from a Gaussian distribution with   unknown mean µand unknown variance σ , where   µis drawn from a Gaussian prior and σis drawn   from an inverse - gamma prior :   y , . . . , y∼ N(µ , σ )   µ∼ N(γ , συ ) , σ∼Γ(α , β )   where γ∈R , υ > 0 , and Γ(·)is the gamma func-   tion with α > 1andβ > 0.15682Denote { µ , σ}and{γ , υ , α , β } asΨandΩ.   The posterior p(Ψ|Ω)is a NIG distribution , which   is the Gaussian conjugate prior :   p(Ψ|Ω ) = p ( µ|σ,Ω ) p(σ|Ω )   = N(γ , συ ) Γ(α , β )   = β√υ   Γ(α)√   2πσ / parenleftbigg1   σ / parenrightbigg   · exp / braceleftbigg   −2β+υ(γ−µ )   2σ / bracerightbigg   Drawing a sample Ψfrom the NIG distribution   yields a single instance of the likelihood function   N(µ , σ ) . The NIG distribution therefore serves   as the higher - order , evidential distribution on top   of the unknown lower - order likelihood distribution   from which the observations are drawn . The NIG   hyper - parameters Ωdetermine not only the loca-   tion but also the uncertainty associated with the   inferred likelihood function .   By training a deep neural network model to out-   put the hyper - parameters of the evidential distri-   bution , evidential deep learning allows the uncer-   tainties to be found by analytic computation of the   maximum likelihood Gaussian without the need   for repeated inference for sampling ( Amini et al . ,   2020 ) . Furthermore , it also allows an effective esti-   mate of the aleatoric uncertainty computed as the   expectation of the variance of the Gaussian distribu-   tion , as well as the epistemic uncertainty defined as   the variance of the predicted Gaussian mean . Given   an NIG distribution , the prediction , aleatoric , and   epistemic uncertainty can be computed as :   Prediction : E[µ ] = γ   Aleatoric : E[σ ] = β   α−1,∀α > 1   Epistemic : Var[µ ] = β   υ(α−1),∀α > 1   3.2 Training   The training of DEER is structured as fitting the   model to the data while enforcing the prior to cali-   brate the uncertainty when the prediction is wrong .   3.2.1 Maximising the data fit   The likelihood of an observation ygiven the eviden-   tial distribution hyper - parameters Ωis computed   by marginalising over the likelihood parameters Ψ :   p(y|Ω ) = /integraldisplayp(y|Ψ)p(Ψ|Ω ) dΨ   = E[p(y|Ψ)](1)An analytical solution exists in the case of placing   an NIG prior on the Gaussian likelihood function :   p(y|Ω ) = Γ(1/2 + α )   Γ(α)/radicalbiggυ   π(2β(1 + υ ) )   · /parenleftbig   υ(y−γ)+ 2β(1 + υ)/parenrightbig   = St / parenleftbigg   y|γ , β(1 + υ )   υ α / parenrightbigg   ( 2 )   where St(t|r , s)is the Student ’s t - distribution   evaluated at twith location parameter r , scale pa-   rameter s , andνdegrees of freedom . The predicted   mean and variance can be computed analytically as   E[y ] = γ , Var[y ] = β(1 + υ )   υ(α−1)(3 )   Var[y]represents the total uncertainty of model   prediction , which is equal to the summation of the   aleatoric uncertainty E[σ]and epistemic uncer-   tainty Var[µ]according to the law of total variance :   Var[y ] = E[Var[y|Ψ ] ] + Var [ E[y|Ψ ] ]   = E[σ ] + Var [ µ ]   To fit the NIG distribution , the model is trained   by maximising the sum of the marginal likelihoods   of each human label y. The negative log likeli-   hood ( NLL ) loss can be computed as   L(Θ ) = −1   M / summationdisplaylog p ( y|Ω ) ( 4 )   = −1   M / summationdisplaylog / bracketleftbigg   St / parenleftbigg   y|γ , β(1 + υ )   υ α / parenrightbigg / bracketrightbigg   This is our proposed per - observation - based NLL   loss , which takes each observed label into consid-   eration for AER . This loss serves as the first part   of the objective function for training a deep neural   network model Θto predict the hyper - parameters   { γ , υ , α , β } to fit all observed labels of x.   3.2.2 Calibrating the uncertainty on errors   The second part of the objective function regu-   larises training by calibrating the uncertainty based   on the incorrect predictions . A novel regulariser   is formulated which contains two terms : Land   Lthat respectively regularises the errors on the   estimation of the mean µand the variance σof   the Gaussian likelihood.15683The first term Lis proportional to the error   between the model prediction and the average of   the observations :   L(Θ ) = Φ|¯y−E[µ]|   where | · |is L1 norm , ¯y=/summationtextyis the   averaged label which is usually used as the ground   truth in regression - based AER , and Φis an uncer-   tainty measure associated with the inferred poste-   rior . The reciprocal of the total uncertainty is used   asΦin this paper , which can be calculated as   Φ = 1   Var[y]=υ(α−1 )   β(1 + υ )   The regulariser imposes a penalty when there ’s an   error in prediction and dynamically scales it by di-   viding by the total uncertainty of inferred posterior .   It penalises the cases where the model produces an   incorrect prediction with a small uncertainty , thus   preventing the model from being over - confident .   For instance , if the model produces an error with a   small predicted variance , Φis large , resulting in a   large penalty . Minimising the regularisation term   enforces the model to produce accurate prediction   or increase uncertainty when the error is large .   In addition to imposing a penalty on the mean   prediction as in Amini et al . ( 2020 ) , a second term   Lis proposed in order to calibrate the estima-   tion of the aleatoric uncertainty . As discussed in   the introduction , aleatoric uncertainty in AER is   shown by the different emotional labels given to   the same utterance by different human annotators .   This paper uses the variance of the observations to   describe the aleatoric uncertainty in the emotion   data . The second regularising term is defined as :   L(Θ ) = Φ|¯σ−E[σ]|   where ¯σ=/summationtext(y−¯y ) .   3.3 Summary and implementation details   For an AER task that consists of Nemo-   tion attributes , DEER trains a deep neural net-   work model to simultaneously predict the hyper-   parameters { Ω , . . . , Ω}associated with the N   attribute - specific NIG distributions , where Ω=   { γ , υ , α , β } . A DEER model thus has 4Nout-   put units . The system is trained by minimising thetotal loss w.r.t . Θas :   L(Θ ) = /summationdisplayϵL(Θ ) ( 5 )   L(Θ ) = L(Θ )   + λ[L(Θ ) + L(Θ ) ] ( 6 )   where ϵis the weight satisfying / summationtextϵ= 1,λ   is the scale coefficient that trades off the training   between data fit and uncertainty regulation .   At test - time , the predictive posteriors are Nsep-   arate Student ’s t - distributions p(y|Ω),p(y|Ω )   , . . . , p(y|Ω ) , each of the same form as derived   in Eqn . ( 2 ) . Apart from obtaining a distribution   over the emotion attribute of the speaker , DEER   also allows analytic computation of the uncertainty   terms , as summarised in Table 1 .   4 Experimental Setup   4.1 Dataset   The MSP - Podcast ( Lotfian and Busso , 2019 ) and   IEMOCAP datasets ( Busso et al . , 2008 ) were used   in this paper . The annotations of both datasets   useN= 3with valence , arousal ( also called acti-   vation ) , and dominance as the emotion attributes .   MSP - Podcast contains natural English speech from   podcast recordings and is one of the largest publicly   available datasets in speech emotion recognition .   A seven - point Likert scale was used to evaluate   valence ( 1 - negative vs 7 - positive ) , arousal ( 1 - calm   vs 7 - active ) , and dominance ( 1 - weak vs 7 - strong ) .   The corpus was annotated using crowd - sourcing .   Each utterance was labelled by at least 5 human   annotators and has an average of 6.7 annotations   per utterance . Ground - truth labels were defined   by the average value . Release 1.8 was used in   the experiments , which contains 73,042 utterances15684from 1,285 speakers amounting to more than 110   hours of speech . The average variance of the labels   assigned to each sentence is 0.975 , 1.122 , 0.889   for valence , arousal , and dominance respectively .   The standard splits for training ( 44,879 segments ) ,   validation ( 7,800 segments ) and testing ( 15,326   segments ) were used in the experiments .   The IEMOCAP corpus is one of the most widely   used AER datasets . It consists of approximately   12 hours of English speech including 5 dyadic con-   versational sessions performed by 10 professional   actors with a session being a conversation between   two speakers . There are in total 151 dialogues   including 10,039 utterances . Each utterance was   annotated by three human annotators using a five-   point Likert scale . Again , ground - truth labels were   determined by taking the average . The average   variance of the labels assigned to each sentence is   0.130 , 0.225 , 0.300 for valence , arousal , and dom-   inance respectively . Unless otherwise mentioned ,   systems on IEMOCAP were evaluated by training   on Session 1 - 4 and testing on Session 5 .   4.2 Model structure   The model structure used in this paper follows the   upstream - downstream framework ( wen Yang et al . ,   2021 ) , as illustrated in Figure 1 . WavLM ( Chen   et al . , 2022 ) was used as the upstream model , which   is a speech foundation model pre - trained by self-   supervised learning . The BASE+ versionof the   model was used in this paper which has 12 Trans-   former encoder blocks with 768 - dimensional hid-   den states and 8 attention heads . The parameters of   the pre - trained model were frozen and the weighted   sum of the outputs of the 12 Transformer encoder   blocks was used as the speech embeddings and fed   into the downstream model .   The downstream model consists of two 128-   dimensional Transformer encoder blocks with 4-   head self - attention , followed by an evidential layer   that contains four output units for each of the three   attributes , which has a total of 12 output units . The   model contains 0.3 M trainable parameters . A Soft-   plus activatonwas applied to { υ , α , β } to ensure   υ , α , β > 0with an additional +1 added to αto en-   sureα > 1 . A linear activation was used for γ∈R.   The proposed DEER model was trained to simulta-   neously learn three evidential distributions for the   three attributes . The weights in Eqn . ( 5)were set as   ϵ=ϵ=ϵ= 1/3 . The scale coefficients were   set to λ = λ = λ= 0.1for Eqn . ( 6 ) .   A dropout rate of 0.3 was applied to the trans-   former parameters . The system was implemented   using PyTorch and the SpeechBrain toolkit ( Ra-   vanelli et al . , 2021 ) . The Adam optimizer was used   with an initial learning rate set to 0.001 . Training   took∼8 hours on an NVIDIA A100 GPU .   4.3 Evaluation metrics   4.3.1 Mean prediction   Following prior work in continuous emotion recog-   nition ( Ringeval et al . , 2015 , 2017 ; Sridhar and   Busso , 2020a ; Leem et al . , 2022 ) , the concordance   correlation coefficient ( CCC ) was used to evaluate   the predicted mean . CCC combines the Pearson ’s   correlation coefficient with the square difference   between the mean of the two compared sequences :   ρ=2ρ σσ   σ+σ+/parenleftbig   µ−µ/parenrightbig ,   where ρis the Pearson correlation coefficient be-   tween a hypothesis sequence ( system predictions )   and a reference sequence , where µandµare   the mean values , and σandσare the variance   values of the two sequences . Hypotheses that are   well correlated with the reference but shifted in   value are penalised in proportion to the deviation .   The value of CCC ranges from -1 ( perfect disagree-   ment ) to 1 ( perfect agreement).15685   The root mean square error ( RMSE ) averaged   over the test set is also reported . Since the average   of the human labels , ¯y , is defined as the ground   truth in both datasets , ¯ywere used as the reference   in computing the CCC and RMSE . However , using   ¯yalso indicates that these metrics are less informa-   tive when the aleatoric uncertainty is large .   4.3.2 Uncertainty estimation   It is common to use NLL to measure the uncertainty   estimation ability ( Gal and Ghahramani , 2016 ;   Amini et al . , 2020 ) . NLL is computed by fitting   data to the predictive posterior q(y ) .   In this paper , NLL(avg ) defined as −log q(¯ y )   and NLL(all ) defined as −/summationtextlog q ( y )   are both used . NLL(avg ) measures how much the   averaged label ¯yfits into the predicted posterior   distribution , and NLL(all ) measures how much ev-   ery single human label yfits into the predicted   posterior . A lower NLL indicates better uncertainty   estimation .   5 Experiments and Results   5.1 Effect of the aleatoric regulariser L   First , by setting L= 0 in the total loss , an ablation   study of the effect of the proposed extra regularis-   ing term Lis performed . The results are given in   the ‘ L= 0 ’ rows in Table 2 . In this case , only   Lis used to regularise Land the results are   compared to those trained using the complete loss   defined in Eqn . ( 6 ) , which are shown in the ‘ L   in Eqn . ( 6 ) ’ rows . From the results , Limproves   the performance in CCC and NLL(all ) , but not in   NLL(avg ) , as expected.5.2 Effect of the per - observation - based L   Next , the effect of our proposed per - observation-   based NLL loss defined in Eqn . ( 4),L , is com-   pared to an alternative . Instead of using L ,   ¯L=−log p(¯ y|Ω )   is used to compute the total loss during training ,   and the results are given in the ‘ L=¯L ’   rows in Table 2 . While Lconsiders the like-   lihood of fitting each individual observation into   the predicted posterior , ¯Lonly considers the   averaged observation . Therefore , it is expected   that using ¯Linstead of Lyields a smaller   NLL(avg ) but larger NLL(all ) , which have been   validated by the results in the table .   5.3 Baseline comparisons   Three baseline systems were built :   •A Gaussian Process ( GP ) with a radial basis   function kernel , trained by maximising the   per - observation - based marginal likelihood .   •A Monte Carlo dropout ( MCdp ) system with   a dropout rate of 0.4 . During inference , the   system was forwarded 50 times with different   dropout random seeds to obtain 50 samples .   •An ensemble of 10 systems initialised and   trained with 10 different random seeds .   The MCdp and ensemble baselines used the same   model structure as the DEER system , except that   the evidential output layer was replaced by a stan-   dard fully - connected output layer with three output   units to predict the values of valence , arousal and15686   dominance respectively . Following prior work ( Al-   Badawy and Kim , 2018 ; Atmaja and Akagi , 2020b ;   Sridhar and Busso , 2020b ) , the CCC loss ,   L= 1−ρ   was used for training the MCdp and ensemble base-   lines . The CCC loss was computed based on the   sequence within each mini - batch of training data .   The CCC loss has been shown by previous stud-   ies to improve the continuous emotion predictions   compared to the RMSE loss ( Povolny et al . , 2016 ;   Trigeorgis et al . , 2016 ; Le et al . , 2017 ) . For MCdp   and ensemble , the predicted distribution of the emo-   tion attributes were estimated based on the obtained   samples by kernel density estimation .   The results are listed in Table 3 . The proposed   DEER system outperforms the baselines on most of   the attributes and the overall values . In particular ,   DEER outperforms all baselines consistently in the   NLL(all ) metric .   5.4 Cross comparison of mean prediction   Table 4 compares results obtained with those pre-   viously published in terms of the CCC value . Pre-   vious papers have reported results on both version   1.6 and 1.8 of the MSP - Podcast dataset . For com-   parison , we also conducted experiments on version   1.6 for comparison . Version 1.6 of MSP - Podcast   database is a subset of version 1.8 and contains   34,280 segments for training , 5,958 segments for   validation and 10,124 segments for testing . For   IEMOCAP , apart from training on Session 1 - 4 and   testing on Session 5 ( Ses05 ) , we also evaluated   the proposed system by a 5 - fold cross - validation   ( 5CV ) based on a “ leave - one - session - out ” strategy .   In each fold , one session was left out for testing and(a ) Aleatoric uncertainty(b ) Epistemic uncertainty(c ) Total uncertainty   the others were used for training . The configuration   is speaker - exclusive for both settings . As shown in   Table 4 , our DEER systems achieved state - of - the-   art results on both versions of MSP - Podcast and   both test settings of IEMOCAP .   5.5 Analysis of uncertainty estimation   5.5.1 Visualisation   Based on a randomly selected subset test set of   MSP - Podcast version 1.8 , the aleatoric , epistemic   and total uncertainty of the dominance attribute   predicted by our proposed DEER system are shown15687   in Figure 2 .   Figure 2 ( a ) shows the predicted mean ±square   root of the predicted aleatoric uncertainty ( E[µ]±/radicalbig   E[σ ] ) and the average label ±the standard de-   viation of the human labels ( ¯y±¯σ ) . It can be seen   that the predicted aleatoric uncertainty ( blue ) over-   laps with the label standard deviation ( grey ) and   the overlapping is more evident when the mean   predictions are accurate ( i.e. samples around index   80 - 100 ) .   Figure 2 ( b ) shows the predicted mean ±square   root of the predicted epistemic uncertainty ( E[µ]±/radicalbig   Var[µ ] ) . The epistemic uncertainty is high when   the predicted mean deviates from the target ( i.e.   samples around index 40 - 50 ) while low then the   predicted mean matches the target ( i.e. samples   around index 80 - 100 ) .   Figure 2 ( c ) shows the predicted mean ±square   root of the total epistemic uncertainty ( E[y]±/radicalbig   Var[y ] ) which combines the aleatoric and epis-   temic uncertainty . The total uncertainty is high   either when the input utterance is complex or the   model is not confident .   5.5.2 Reject option   A reject option was applied to analyse the uncer-   tainty estimation performance , where the system   has the option to accept or decline a test sample   based on the uncertainty prediction . Since the eval-   uation of CCC is based on the whole sequence   rather than individual samples , its computation   would be affected when the sequence is modified(a ) MSP - Podcast ( b ) IEMOCAP   by rejection ( Wu et al . , 2022a ) . Therefore , the   reject option is performed based on RMSE .   Confidence is measured by the total uncertainty   given in Eqn . ( 3 ) . Figure 3 shows the performance   of the proposed DEER system with a reject option   on MSP - Podcast and IEMOCAP . A percentage of   utterances with the largest predicted variance were   rejected . The results at 0 % rejection corresponds   to the RMSE achieved on the entire test data . As   the percentage of rejection increases , test coverage   decreases and the average RMSE decreases show-   ing the predicted variance succeeded in confidence   estimation . The system then trades off between the   test coverage and performance .   6 Conclusions   Two types of uncertainty exist in AER : ( i ) aleatoric   uncertainty arising from the inherent ambiguity of   emotion and personal variations in emotion expres-15688sion ; ( ii ) epistemic uncertainty associated with the   estimated network parameters given the observed   data . This paper proposes DEER for estimating   those uncertainties in emotion attributes . Treating   observed attribute - based annotations as samples   drawn from a Gaussian distribution , DEER places   a normal - inverse gamma ( NIG ) prior over the Gaus-   sian likelihood . A novel training loss is proposed   which combines a per - observation - based NLL loss   with a regulariser on both the mean and the vari-   ance of the Gaussian likelihood . Experiments on   the MSP - Podcast and IEMOCAP datasets show   that DEER can produce state - of - the - art results in   estimating both the mean value and the distribu-   tion of emotion attributes . The use of NIG , the   conjugate prior to the Gaussian distribution , leads   to tractable analytic computation of the marginal   likelihood as well as aleatoric and epistemic uncer-   tainty associated with attribute prediction . Uncer-   tainty estimation is analysed by visualisation and   a reject option . Beyond the scope of AER , DEER   could also be applied to other tasks with subjective   evaluations yielding inconsistent labels .   Limitations   The proposed approach ( along with other meth-   ods for estimating uncertainty in inconsistent an-   notations ) is only viable when the raw labels from   different human annotators for each sentence are   provided by the datasets . However , some multiple-   annotated datasets only released the majority vote   or averaged label for each sentence ( i.e. Poria   et al . , 2019 ) .   The proposed method made a Gaussian assump-   tion on the likelihood function for the analytic com-   putation of the uncertainties . The results show that   this modelling approach is effective . Despite the   effectiveness of the proposed method , other distri-   butions could also be considered .   Data collection processes for AER datasets vary   in terms of recording conditions , emotional elicita-   tion scheme , and annotation procedure , etc . This   work was tested on two typical datasets : IEMO-   CAP and MSP - Podcast . The two datasets are both   publicly available and differ in various aspects :   •IEMOCAP contains emotion acted by pro-   fessional actors while MSP - Podcast contains   natural emotion .   •IEMOCAP contains dyadic conversations   while MSP - Podcast contains Podcast record-   ings.•IEMOCAP contains 10 speakers and MSP-   Podcast contains 1285 speakers .   •IEMOCAP contains about 12 hours of speech   and MSP - Podcast contains more than 110   hours of speech .   •IEMOCAP was annotated by six professional   evaluators with each sentence being annotated   by three evaluators . MSP - Podcast was an-   notated by crowd - sourcing where a total of   11,799 workers were involved and each work   annotated 41.5 sentences on average .   The proposed approach has been shown effective   over both datasets . We believe the proposed tech-   nique should be generic . Furthermore , although   validated only for AER , the proposed method could   also be applied to other tasks with disagreements   in subjective annotations such as hate speech detec-   tion and language assessment .   Ethics Statement   In tasks involving subjective evaluations such as   emotion recognition , it is common to employ mul-   tiple human annotators to give multiple annotations   to each data instance . When annotators disagree ,   majority voting and averaging are commonly used   to derive single ground truth labels for training su-   pervised machine learning systems . However , in   many subjective tasks , there is usually no single   “ correct ” answer . By enforcing a single ground   truth , there ’s a potential risk of ignoring the valu-   able nuance in each annotator ’s evaluation and their   disagreements . This can cause minority views to be   under - represented . The DEER approach proposed   in this work could be beneficial to this concern as it   models uncertainty in annotator disagreements and   provides some explainability of the predictions .   While our method helps preserve minority per-   spectives , misuse of this technique might lead to   ethical concerns . Emotion recognition is at risk of   exposing a person ’s inner state to others and this in-   formation could be abused . Furthermore , since the   proposed approach takes each annotation into con-   sideration , it is important to protect the anonymity   of annotators .   Acknowledgements   Wen Wu is supported by a Cambridge International   Scholarship from the Cambridge Trust . This work   has been performed using resources provided by15689the Cambridge Tier-2 system operated by the Uni-   versity of Cambridge Research Computing Service   ( www.hpc.cam.ac.uk ) funded by EPSRC Tier-2   capital grant EP / T022159/1 .   The MSP - Podcast data was provided by The   University of Texas at Dallas through the Multi-   modal Signal Processing Lab . This material is   based upon work supported by the National Sci-   ence Foundation under Grants No . IIS-1453781   and CNS-1823166 . Any opinions , findings , and   conclusions or recommendations expressed in this   material are those of the author(s ) and do not nec-   essarily reflect the views of the National Science   Foundation or The University of Texas at Dallas .   References1569015691   A Derivation of the predictive posterior   Since NIG is the Gaussian conjugate prior ,   p(Ψ|Ω ) = N(γ , συ ) Γ(α , β )   = β√υ   Γ(α)√   2πσ / parenleftbigg1   σ / parenrightbigg   · exp / braceleftbigg   −2β+υ(γ−µ )   2σ / bracerightbigg   its posterior p(Ψ|D)is in the same parametric fam-   ily as the prior p(Ψ|Ω ) . Therefore , given a test   utterance x , the predictive posterior p(y|D)has15692   the same form as the marginal likelihood p(y|Ω ) ,   where Ddenotes the training set .   p(y|D ) = /integraldisplay   p(y|Ψ)p(Ψ|D ) dΨ ( 7 )   p(y|Ω ) = /integraldisplay   p(y|Ψ)p(Ψ|Ω ) dΨ ( 8)   In DEER , the predictive posterior and posterior   are both conditioned on Ω , written as p(y|D,Ω )   andp(Ψ|D,Ω)to be precise . Also , the informa-   tion of Dis contained in ΩsinceΩ=f(x )   andˆΘis the optimal model parameters obtained by   training on D. Then the predictive posterior can be   written as p(y|Ω ) . Given the conjugate prior , the   predictive posterior in DEER can be computed by   directly substituting the predicted Ωinto the ex-   pression of marginal likelihood derived in Eqn . ( 2 ) ,   skipping the step of calculating the posterior .   B Fusion with text modality   This appendix presents bi - modal experiments   that incorporate text information into the DEER   model . Transcriptions were obtained from a pub-   licly available automatic speech recognition ( ASR )   model “ wav2vec2 - base-960h"which fine - tuned   the wav2vec 2.0 ( Baevski et al . , 2020 ) model on   960 hours Librispeech data ( Panayotov et al . , 2015 ) .   Transcriptions were first encoded by a RoBERTa   model ( Liu et al . , 2019 ) and fed into another two-   layer Transformer encoder . As shown in Figure 4 ,   outputs from the text Transformer were concate-   nated with the outputs from the audio Transformer   encoder and fed into the evidential output layer .   Results are shown in Table 5 . Incorporating text   information improves the estimation of valence but   not necessarily for arousal and dominance . Similar   phenomena were observed by ( Triantafyllopoulos   et al . , 2022 ) . A possible explanation is that text   is effective for sentiment analysis ( positive or neg-   ative ) but may not be as informative as audio to   determine a speaker ’s level of excitement . CCC   for dominance improves more for IEMOCAP than   MSP - Podcast possibly because IEMOCAP is an   acted dataset and the emotion may be exaggerated   compared with MSP - Podcast which contains natu-   ral emotion.15693ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   " Limitations ” section   /squareA2 . Did you discuss any potential risks of your work ?   " Ethics statement ” section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract , introduction , and conclusions   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4 & 5   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Section 4   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   The resources used are aligned with their intended use ( in Section 4 ) .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No known offensive content and identiﬁers . The database providers ensured the datasets are suitable   for research use .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4.1   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4.1   C / squareDid you run computational experiments ?   Section 4 & 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4.215694 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4.2   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Human annotators were used in the corpora provided but no new human annotations collected .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.15695