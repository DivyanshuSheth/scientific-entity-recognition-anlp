  Peng Cui Mrinmaya Sachan   Department of Computer Science , ETH Zürich   { peng.cui , mrinmaya.sachan}@inf.ethz.ch   Abstract   Adaptive learning aims to provide customized   educational activities ( e.g. , exercises ) to ad-   dress individual learning needs . However , man-   ual construction and delivery of such activities   is a laborious process . Thus , in this paper , we   study a novel task of adaptive andpersonalized   exercise generation for online language learn-   ing . To this end , we combine a knowledge trac-   ing model that estimates each student ’s evolv-   ing knowledge states from their learning history   and a controlled text generation model that gen-   erates exercise sentences based on the student ’s   current estimated knowledge state and instruc-   tor requirements of desired properties ( e.g. , do-   main knowledge and difficulty ) . We train and   evaluate our model on real - world learner in-   teraction data from Duolingo and demonstrate   that LMs guided by student states can gener-   ate superior exercises . Then , we discuss the   potential use of our model in educational ap-   plications using various simulations . These   simulations show that our model can adapt to   students ’ individual abilities and can facilitate   their learning efficiency by personalizing learn-   ing sequences .   1 Introduction   Adaptive learning technologies which continuously   monitor student progress to dynamically adjust the   level or type of learning materials based on the in-   dividual ’s abilities are quite popular ( Becker et al . ,   2018 ) . Empirical studies have shown various bene-   fits of adaptive learning , such as improved student   learning outcomes ( Bailey et al . , 2018 ; Holthaus   et al . , 2019 ) , lower dropout rates ( Daines et al . ,   2016 ) , and increased instructor satisfaction ( Yarnall   et al . , 2016 ) . Despite their effectiveness , designing   adaptive systems is challenging as it usually in-   volves planning a series of exercises that is person-   alized and adaptive to each student , which requiresFigure 1 : We first assess student knowledge states from   their learning history and then generate exercises based   on estimated states and instructor control of desired   properties including domain knowledge ( vocabulary )   and difficulty levels ( expected error numbers ) .   diverse exercise planning as well as an understand-   ing of the student learning process .   On the other hand , powered by advances in neu-   ral NLP , works have been done for automatically   generating text - based exercises or questions for   educational purposes in second language learn-   ing ( Heck and Meurers , 2022 ; Perez and Cuadros ,   2017 ) , mathematics ( Polozov et al . , 2015 ; Zhou   and Huang , 2019 ; Wang et al . , 2021 ) , and com-   puter science ( Susanti et al . , 2017 ) . Nevertheless ,   how to apply these approaches in adaptive systems   remains an open question . First , existing meth-   ods largely rely on pre - defined question templates   or specified information sources ( e.g. , a passage ) ,   thereby resulting in limited knowledge coverage   and low question difficulty control , and as a conse-   quence , do not meet each student ’s individual and   nuanced learning needs . Besides , they are usually   designed to generate standalone exercises , whereas   adaptive learning systems usually require a con-   tinuous supply of exercises . Another related line   of research studies exercise recommendation to   customize learning content based on individual ca-10184pabilities and goals ( Wu et al . , 2020 ; Huang et al . ,   2022 ) . However , these systems are limited by the   diversity of the exercise pool .   To address the above limitations , we study the   task of exercise generation in the context of adap-   tive learning , where we hypothesize that a student ’s   dynamic knowledge state holds the key to gener-   ating adaptive andpersonalized exercises . Specif-   ically , we ground our study in the domain of lan-   guage learning to create exercise sentences for   translation , of which Figure 1 illustrates the overall   process . We start with an assumption about the dy-   namics between exercise difficulty , vocabulary , and   a student ’s knowledge state ( § 3 ) . Then , we propose   an approach ( § 4 ) that marries knowledge tracing   ( KT ; Corbett and Anderson ( 1994 ) ) , a technique for   estimating students ’ mastery states of knowledge   components from their learning history , with a con-   trolled text generation model that generates the next   exercise based on instructor requirements , such as   specified domain knowledge andtarget difficulty .   We further explore various strategies to adapt the   generation of exercises based on students ’ chang-   ing knowledge states . In doing this , our model not   only supports personalized generation where the   instructor ( or the system ) can express some desired   properties of the generated exercises but is also   adaptive to each student ’s learning progress .   We conduct extensive experiments on real - world   student learning data from Duolingo , a popular   online language learning platform that offers struc-   tured and individualized learning content . Our re-   sults ( § 5 ) show that pre - trained LMs can help KT   assess student language knowledge while student   states estimated by KT can guide LMs to generate   adaptive and personalized exercises . We further   discuss the potential use of our model in educa-   tional applications with simulations . The simu-   lations show that our model can dynamically ad-   just exercise difficulty to match individual learning   progress and facilitate their learning efficiency by   customizing exercise sequences .   2 Related Work   Adaptive Learning technologies that dynamically   monitor student progress and adjust the course con-   tent based on an individual ’s abilities have demon-   strated various benefits in education ( Becker et al . ,   2018 ) . Such systems usually consist of three core   components : ( 1 ) a domain model which refers tothe content and structure of the topic to be taught ,   ( 2 ) a learner model which repeatedly measures and   updates learner characteristics , and ( 3 ) an adap-   tion model which combines information from the   domain and learner model to offer adaptive in-   structions ( Vagale and Niedrite , 2012 ; Imhof et al . ,   2020 ) . In this study , we build the learner model   based on the KT technique and combine the domain   and adaption model into an LM which generates   learning content adaptively based on user features   captured by the learner model .   Knowledge Tracing ( Corbett and Anderson , 1994 )   is the technique to estimate students ’ knowledge   mastery sfrom their practiced exercises ( e ) and   responses ( r ):   s = f((e , r),(e , r ) , ... , ( e , r)).(1 )   Early KT approaches model fas variants of lo-   gistic regression , such as Item Response Theory   ( IRT ) and Additive Factor Model ( AFM ) ( Cen et al . ,   2008 ) , or probabilistic models such as Bayesian   Knowledge Tracing ( Corbett and Anderson , 1994 )   and its variants ( Yudelson et al . , 2013 ; Käser et al . ,   2017 ) . These approaches heavily rely on their as-   sumptions of the learning process which are of-   ten incomplete . In recent years , neural networks   have become the dominant method in this area .   Piech et al . ( 2015 ) proposed the first Deep Knowl-   edge Tracing model based on Recurrent Neural Net-   works . After that , various architectures have been   applied to model different characteristics of learn-   ing , such as self - attention ( Pandey and Karypis ,   2019 ; Shin et al . , 2021 ) , memory networks ( Ab-   delrahman and Wang , 2019 ) , and graph neural net-   works ( Tong et al . , 2020 ) .   Exercise Generation . Previous exercise genera-   tion approaches for language learning primarily   retrieve and manipulate text to create fixed types   of exercises , such as gap fill and multiple - choice   exercises ( Agarwal and Mannem , 2011 ; Perez and   Cuadros , 2017 ; Heck and Meurers , 2022 ) , which   are limited by the richness of the corpus . Besides   them , some Question Generation ( QG ) approaches   have been proposed for educational purposes ( Zhao   et al . , 2022 ; Wang et al . , 2021 ) . While some of   them allow for user control of certain question prop-   erties , they do not consider learners ’ individual and   dynamic learning needs and progress . Thus , they   can not achieve the goal of adaptive learning . Re-   cently , Srivastava and Goodman ( 2021 ) proposed   an adaptive question generation model that con-   nects question difficulty with student knowledge.10185However , it neither models students ’ fine - grained   knowledge states nor provides control over domain   knowledge . Consequently , it is insufficient for prac-   tical use .   Controlled Text Generation ( CTG ) methods aim   to steer text generation toward certain attributes .   Existing CTG approaches can be broadly clas-   sified into three types : directly training a class-   conditional language model ( CCLM ) ( Keskar et al . ,   2019 ; Ziegler et al . , 2019 ; Ficler and Goldberg ,   2017 ) , guiding a model via an attribute discrimi-   nator ( Dathathri et al . , 2020 ; Liu et al . , 2020 ) , or   manipulating decoder ’s logits ( also referred to as   weighted decoding ) ( Holtzman et al . , 2018 ; Yang   and Klein , 2021 ) . This study explores difficulty   and lexical control in generating language learn-   ing exercises . Additionally , we seek to adapt the   model ’s controllability to different users by build-   ing the dependency between control signals and   individual states .   3 Problem Formalization   LetH={(e , r ) , ... , ( e , r)}be a student ’s   learning history consisting of nexercises and re-   sponses . Here , e={w , ... , w}is an exer-   cise sentence for translation and r∈ { 0,1 }   is the correctness label for each word in e. We   generate the next exercise ebased on :   •C : knowledge components that should   be involved in e. In language learning , we   consider a word as a knowledge component ,   and therefore C={c , ... , c|c∈   V}is a subset of vocabulary Vthat should be   included in the output . In general , the knowl-   edge components can be user or system de-   fined based on the current learning material .   •s : a student ’s knowledge state for the   knowledge components ( the vocabulary ) af-   terninteractions . scan be formalized   as a|V|-dimensional vector with each entry   between 0 and 1 indicating the mastery proba-   bility of that word .   •d : the expected difficulty ofe . We   use individual performance to estimate prob-   lem difficulty . For a particular student , the   difficulty of an exercise is defined as the ex-   pected number of word errors the student   would make in translating it . Given the above setting , we formalize our task as :   e= arg maxP(e|s , d , C),(2 )   where esatisfies the following constraints :   ∀c∈C:∃i , e = c , ( 3 )   d=/summationdisplay(1−s[w ] ) , ( 4 )   corresponding to word constraint anddifficulty con-   straint , respectively . Here , s[w]represents the   correct probability of translating word w ; therefore ,   the sum of { 1−s[w]| , w∈e}is the expected num-   ber of errors in translating e , which can be seen as   a measure of the difficulty of e.   Our task is distinct from previous CTG works   in two aspects : 1 ) our control is dynamic ; student   states acting as control are also learnable ; 2 ) there   is a strong dependency among control signals ( Eqs .   3 and 4 ) , which is non - trivial to learn . Note that in   this work , we measure difficulty via student perfor-   mance and only consider vocabulary knowledge in   defining sfor simplicity . Other definitions of sen-   tence difficulty ( e.g. , definitions that incorporate   other types of linguistic knowledge such as syntax )   can be explored in future work .   4 Methodology   Our model is illustrated in Figure 2 . We first em-   ploy a knowledge tracer T(§ 4.1 ) to estimate a   student ’s time - varying knowledge states . Then , we   build an LM - based exercise generator G(§ 4.2 ) to   create exercises based on estimated states and spec-   ified difficulty and knowledge components ( words ) .   We jointly optimize the two modules with an in-   consistency loss ( § 4.3 ) at training and apply a con-   strained decoding strategy ( § 4.4 ) at inference . Fi-   nally , we discuss how our model can accommodate   personalized learning recommendation algorithms   on the fly ( § 4.5 ) .   4.1 Knowledge Tracing   The goal of our knowledge tracing model Tis to   estimate a student ’s latest knowledge state s   given previous interactions H. We adopt the   deep knowledge tracing ( DKT ) model proposed   by Piech et al . ( 2015 ) . We concatenate past exer-   cises as a word sequence e={w , ... , w }   and past responses as a label sequence r=   { r , ... , r } , where wandrrepresent the   jthword or label of the ithexercise . Then we10186   convert the two sequences into word embeddings   ⃗eand label embeddings ⃗rand send them to   an LSTM encoder to predict the next state s :   h= LSTM ( ⃗e+⃗r;h ) , ( 5 )   s = sigmoid ( W∗h+ b ) . ( 6 )   The model is trained to predict the binary word la-   bels of the next exercise using the estimated knowl-   edge state . The cross - entropy loss for a single stu-   dent ’s history of Ninteractions is computed as :   L=/summationdisplay / summationdisplayCE(r , s[w ] ) . ( 7 )   We adopt the regularization strategy proposed   by Yeung and Yeung ( 2018 ) to stabilize training :   L=/summationdisplay / summationdisplay|s−s|,(8 )   where Lensures that only the states of relevant   knowledge components are updated , and Lpe-   nalizes the vibration . The final objective of Tis   L = L+λ∗L+λ∗Lwithλfor balance .   4.2 Controllable Exercise Generator   Our exercise generator Gis fine - tuned from a pre-   trained LM . Specifically , we generate an exercise   ebased on a student ’s current knowledge state s ,   target words C , and expected difficulty d(we drop   the interaction index to reduce clutter ) . We param-   eterize the inputs as follows :   x= [ f(s);f(d);Emb ( c , ... , c)],(9)where knowledge state sand scalar difficulty dare   projected to control vectors via two feedforward   layers fandf , and Care mapped to word em-   beddings . The training objective for generating a   single exercise is defined as :   L=−/summationdisplaylogP ( w|w , ... , w , x).(10 )   During training , we sample a proportion of words   from reference exercises as Cand calculate dif-   ficulty dfrom ground - truth correctness labels ,   whereas states sare estimated by T. At inference ,   dandCcan be determined by instructors or the sys-   tem , allowing automated and human intervention .   4.3 Joint Learning with Inconsistency Loss   We jointly optimize the knowledge tracer Tand   exercise generator Gwith an inconsistency loss   inspired by Cui and Hu ( 2021 ) , enabling the two   modules to learn from each other . Concretely , after   generating an exercise e , we calculate its difficulty   using input state svia Eq . 4 , which should be as   close to the input difficulty das possible :   L=|d−/summationdisplay(1−s[w])| . ( 11 )   Since the second term is non - differentiable due to   theargmax operation involved in producing e , we   replace it with " soft " tokens :   L=|d−/summationdisplay(1−p⊙s)| , ( 12 )   where p = softmax ( o / τ)is the tdistribution   normalized from its logits o∈Rwith a temper-   ature parameter τ , and⊙represents dot product .   For the generator G , this loss constrains the gen-   eration toward the target difficulty . For T , the LM   distributions pprovide similarity information be-   tween vocabulary words . This is analogous to the   relationship of knowledge components , which has   been shown helpful in knowledge tracing ( Tong   et al . , 2020 ) . The final objective of our model is   L = L+γL+γL.   4.4 Lexical Difficulty Constrained Decoding   We propose a beam search - based decoding algo-   rithm to enforce the constraints introduced in § 3 .   At each step , we update the beam according to :   Y= argtopklogP ( y|x ) + /summationdisplayαF(y ) ,   ( 13)10187where Yis the set of decoded hypotheses in step   tandkis the beam size . The first term is the stan-   dard objective of beam search and the second term   is a weighted combination of additional scoring   functions in terms of the satisfaction of different   constraints . We formulate our constraints Fin Eqs .   3 and 4 as :   F(y ) = /summationdisplayI(c , y),andF(y ) = −|d−h(y)| ,   corresponding to the satisfaction of word constraint   and difficulty constraint , respectively . I(c , y)is a   Boolean predicate indicating whether word cis   included in sequence yandh(y)calculates its dif-   ficulty via Eq . 4 .   Succinctly , the decoding algorithm works in   three steps . First , we expand the current khy-   potheses to k× |V| candidates . Then , we prune   the search space by dropping candidates that are   not in the top- klist of any scoring functions F.   Finally , we rescore the pruned candidates based   on the full objective ( Eq . 13 ) and select the k - best   ones to update the beam .   However , we found that greedily applying Fin   the rescoring step would bias the decoder toward   sequences with difficult words in the earlier steps .   Drawing inspiration from Lu et al . ( 2022 ) , we use   lookahead heuristics that incorporate future esti-   mates into the decoding process . Concretely , to   score a subsequence y , we first greedily decode   the next l+ 1 steps " soft " tokens ( i.e. , distribu-   tions ): ˜ y=[p , ... , p ] . Then , we combine the   constraint satisfaction of decoded yand the esti-   mated future ˜ y :   ˜F(y)=/summationdisplaymax ( I(c , y),maxP(y = c ) ) ,   ˜F(y ) = −|d−h(y)−/summationdisplay1−p⊙s| .   The procedure of our decoding algorithm is in Ap-   pendix A.   4.5 Plug - and - Play Personalized Generation   Our model can be flexibly plugged into an existing   personalized learning recommendation algorithm   to automatically generate novel and customized ex-   ercises . We showcase this functionality using the   E Mcurriculum planning strategy de-   rived from DKT . Given a student ’s current state s ,   we can calculate the expected knowledge state after   practicing a new exercise eusing our KT model T :   ˜s=/summationdisplayP(r)∗ T(s,(e , r)),(14 )   whereT(·)computes the updated knowledge state   given a new interaction ( e , r ) . The probability of la-   bel sequence ris computed from sassuming con-   ditional independence P(r ) = /producttextP(r ) , where   P(r ) = s[e].E Mscores ebased   on how well it can improve a student ’s average   knowledge state , i.e. , F(e ) = ˜s−s , where   sdenotes mean of the vector . We incorporate F   into the decoding objective ( Eq . 13 ) and call it   E M - G.   In principle , our model can accommodate dif-   ferent recommendation algorithms with different   ranking functions F. The key benefit is that our   model can generate novel exercises , while retrieval-   based systems can only select exercises from an   existing pool .   5 Experimental Results and Analysis   We experiment on the English track of Duolingo   Second Language Acquisition Modeling ( SLAM )   dataset ( Settles et al . , 2018 ) , which contains about   1 million interactions of 2.6k learners over the first   30 days of learning a second language . For each   student , we use the first 80 % of interactions for   training , and the subsequent and the last 10 % for   validation and testing , respectively . Details of the   dataset and experimental setup are in Appendix B.   We first evaluate the ability of the KT model to   estimate student knowledge states in § 5.1 . Then ,   we analyze the effectiveness of the exercise gener-   ator in § 5.2 . Lastly , we showcase the superiority   of our model in two educational scenarios with   simulation experiments in § 5.3.10188   5.1 Knowledge Tracing Evaluation   We use the standard AUC ( ROC ) as the metric   of knowledge tracing in accordance with Settles   et al . ( 2018 ) . We denote our DKT model jointly   trained with the LM - based exercise generator as   DKTand compare it with the following base-   lines : 1 ) Ensemble ( Osika et al . , 2018 ) which is one   of the winning methods of the SLAM challenge   that combines a RNN and a GBDT classifier . We   reimplement this model to use texts only as input   and remove other side features , such as response   time . We do this because we are interested in its   performance in a general setting where we do not   assume the availability of diverse side information ;   2 ) the standard DKT ( Piech et al . , 2015 ) which is   trained only with the KT loss L. We use it to ver-   ify whether jointly learning with an LM can help   predict student language knowledge .   We present the results in Table 1 , where we can   see that DKT outperforms the Ensemble model   when only text features are used , and our best   model DKToutperforms DKT on all met-   rics . We hypothesize the performance gain comes   from the word similarity information entailed in   the output distributions pof the LM . This can be   regarded as the relationship between knowledge   components , which is demonstrated effective in   knowledge tracing ( Tong et al . , 2020 ) . To verify   this , we tune the temperature τwhich controls the   sparsity of output distributions : τ→0produces a   sparse distribution that is too assertive and provides   little relationship information , while τ→ ∞ pro-   duces a uniform distribution where all words are   evenly related . The results in the second section   of Table 1 suggest that a medium τimproves the   performance , while a small ( τ=1 ) or large ( τ=5 )   is harmful , particularly for predicting unseen data . The broader message from this observation is that   the knowledge encoded in pre - trained LMs has the   potential to improve knowledge tracing in the do-   main of language learning . We also conduct an   analysis of the influence of regularization terms Eq .   8 , detailed in Appendix C.   5.2 Exercise Generation Evaluation   The main results of exercise generation are pre-   sented in Table 2 , which are split according to   whether the exercises are seen in the training set .   Evaluation metrics include reference - based BLEU   ( Papineni et al . , 2002 ) and METEOR ( Banerjee   and Lavie , 2005 ) , KC - Coverage which is the per-   centage of target knowledge components ( words )   that appear in the outputs , D - MAE which is the   mean absolute error between the input difficulty   and output difficulty , Invalid which is the percent-   age of exercises that have grammar errors detected   using an automatic tool . Since we generate ex-   ercises for language learning , we expect a valid   exercise to be grammatically correct . We analyze   the performance from the following aspects .   Lexical Controllability . We first examine the lex-   ical controllability of our model , which is crucial   for generating personalized exercises for language   learning . We compare our model with two base-   lines:1 ) EGwhich generates the next exercise   based on the student ’s historical interactions ; and   2 ) AGQwhich generates the next exercise   based on historical interactions and a target diffi-   culty . The two baselines perform poorly on BLEU ,   METEOR , and KC - Coverage metrics , particularly10189   for unseen data . This indicates that they can not pre-   dict the accurate content of the next exercise based   on historical data or difficulty information , possi-   bly because there is no strong connection within a   sequence of exercises or such connection can not   be captured by an LM . We note that EGperforms   well on the validness metric . However , upon in-   specting its results , we found the model almost only   copies exercises from history , with less than 0.02 %   novel generations . The same issue is observed in   AQGwhere more than 90 % exercises are repet-   itive . We follow Srivastava and Goodman ( 2021 )   to improve its novelty using a repetition penalty   during the generation , but this results in far more   invalid exercises ( 1.7 % ) . In comparison , our model   achieves a better balance between generalization   ability and fluency .   Effect of Student Modeling . To investigate   whether student modeling helps exercise genera-   tion , we build two baselines without student knowl-   edge states : 1 ) EGwhich conditions generation   on target KCs ( words ) only , and 2 ) EGon both   target words and difficulty . The former variant   can be considered a keyword - to - text generation   model , while the latter imposes additional diffi-   culty control . Our full model APEGsignif-   icantly outperforms both of them , which proves   our aforementioned hypothesis that a student ’s dy-   namic knowledge states must be considered in gen-   erating adaptive and personalized exercises . An   interesting observation is that incorporating diffi-   culty control improves the performance on unseen   data , indicating the model to some degree learns   generalizable difficulty information . Nevertheless ,   our further analysis shows the model is not adap-   tive to students of different abilities , which will be   discussed in § 5.3 .   Ablation Study . The key challenge of our task is   to learn the dependency between student knowl-   edge , vocabulary , and exercise difficulty ( Eqs . 3   and 4 ) . To understand which parts of our model   contribute to this goal , we build two ablated vari-   ants by removing the joint learning strategy ( § 4.3 )   and the constrained decoding algorithm ( § 4.4 ) , re-   spectively . As shown in the second section of Table   2 , the search - based method is slightly better than   the learning - based method , while combining them   leads to the best performance .   We further explore the effect of the lookahead   strategy on difficulty constraints . Table 3 presents   the ablation results on the validation set , where we   can see lookahead strategy improves both genera-   tion quality and controllability . To understand how   it works , we measure the distribution of difficulty   in different regions of exercise sentences . Such   distribution is computed as the accumulated word   difficulty in four equally sized segments of 2000   sampled sentences . As shown in Figure 3 , the dif-   ficult words of reference exercises are largely con-   centrated in the 2and4quarter . Our decoding   algorithm with lookahead produces a similar result ,   while removing lookahead would bias the distribu-   tion toward 2and3quarter . This confirms our   assumption that naively applying Fwould greed-   ily select difficult words in the early steps , which   is not the distribution of reference exercises . Our   decoding algorithm avoids this issue by estimating   the future and therefore achieves better results .   Upper Bound Analysis . When we train our model ,   we use ground - truth difficulty dand target words   Cobtained from references ; however , the student   states sare estimated from the KT model . We   conduct an upper bound analysis to understand the   influence of the accuracy of son the generation   performance . Since a student ’s actual mastery of   every vocabulary word is not available , we choose   to replace the ground - truth difficulty levels dwith   those estimated from s. As shown in the last sec-   tion of Table 2 , all metrics are considerably boosted   when the inconsistency between states sand diffi-   culty dis eliminated . This again proves the effect10190   of incorporating student states and explains how   such information comes to play : the knowledge   states explicitly convey the dynamics between con-   trol signals d , C , and target exercises e , which is   non - trivial to learn by the model itself .   Case Study . We provide a few cases in Table 4 .   We can see our model can dynamically adjust the   exercise content according to specified words , tar-   get difficulty , as well as students ’ different mastery   states of the vocabulary . The exercises generated   for advanced students ( avg . state = 0.65 ) are gener-   ally more difficult than for poor students ( avg . state   = 0.32 ) under the same input difficulty .   5.3 Educational Applications   In this subsection , we showcase the potential appli-   cations of our model in two educational scenarios   with simulation experiments .   5.3.1 Adaptive Difficulty Calibration   A crucial requirement for adaptive learning sys-   tems is to dynamically adjust the difficulty of   learning items to match each student ’s learningprogress ( Becker et al . , 2018 ) . However , previ-   ous difficulty - controlled question generation ap-   proaches are mainly based on inherent problem dif-   ficulty , independent of individual abilities ( Susanti   et al . , 2017 ; Kumar et al . , 2019 ) . Ideally , our model   can achieve this goal by learning the dependency   between difficulty and student knowledge states .   To verify this , we generate 50 additional exercises   of specified difficulties for each student after their   existing interactions . At each step , we construct in-   put by sampling a target word from the vocabulary   and a difficulty level from a uniform distribution   [ 1,3 ] . We compare our full model APEG   with its variant EGwhich achieves the best dif-   ficulty controllability for unseen data . This baseline   can be considered a vanilla non - adaptive difficulty-   controlled exercise generation model .   In this simulation , we are interested in whether   the difficulty controllability of our model can adapt   to students of various knowledge levels . To this end ,   we rank students based on their average knowledge   states sand split the result accordingly . As shown   in Figure 4 , the difficulty controllability of the base-   line is not reliable across different groups . In par-   ticular , it tends to generate harder ( up to 2×d )   exercises for the bottom 10 percentile students but   easier ( up to×d ) ones for the top 10 percentile   students , although it performs well for the interme-   diate 80 percentile students . In comparison , our   adaptive model is also slightly biased toward the in-   termediate group but much more consistent than the   baseline , with less than 20 % fluctuations on aver-   age . Besides , we can see from the shadows that the   baseline experiences huge variances at each step ,   indicating it is not adaptive to different knowledge   states , even though the students within a group are   at a similar level.10191   5.3.2 Improving Learning Efficiency   We now examine whether our model can be used to   improve student learning efficiency by personaliz-   ing exercise sequences . To this end , we customize   30 continuous exercises for 50 sampled students   using our proposed E M - G(§ 4.5 )   and the original E M. Both of them aim   to maximize the expected knowledge state of the   next step ˜s . For the former , at each step , we   first find the best single word that can maximize   ˜sand then generate the next exercise based on   the selected word and a fixed difficulty of 1 . For   the latter , we directly select the best exercise from   the pool . We update students ’ knowledge states   after each practice and repeat this process until we   collect 30 exercises . We compare the change in   ˜sto measure which strategy is more efficient in   improving students ’ knowledge .   The simulation results are shown in Figure   5 . We also include a randomly selected exercise   sequence as a lower bound , which turns out to harm   student learning most of the time . The decrease in   knowledge state is possibly caused by overly diffi-   cult exercises which would lead to wrong answers   and reduce the predicted probability . Under the   same practice opportunities , exercises generated   byE M - Glead to faster knowledge   growth than those selected by E M.   Upon further inspection , we found about 70 % of   them are unseen in the corpus . This explains the   efficiency of E M - Gas it can create   novel exercises targeting individual needs on the   fly while E Mis limited by the pool.5.3.3 Qualitative Discussions on Simulation   Our simulations are based on the DKT model . We   note that some previous studies have observed in-   consistencies between DKT behaviors and the hu-   man learning process ( Shen et al . , 2021 ) . Thus ,   we adopt a simple regularization approach ( Eqs .   5 and 6 ) to alleviate such inconsistencies ( Yeung   and Yeung , 2018 ) , which we found can reduce the   variance of simulation results and improve KT per-   formance ( Appendix C ) .   A popular argument regarding the relationship   between the difficulty of learning content and stu-   dent outcomes is that the level of difficulty should   be set just above the learner ’s current knowledge ,   i.e. ,d≈0.5(Settles and Meeder , 2016 ; Gallego-   Durán et al . , 2018 ) . During the simulations , we   found E Mdoes not follow this heuris-   tic but tends to generate relatively easy exercises   ( d < 0.3mostly ) repeatedly using certain words ,   consistent with the finding in Tschiatschek et al .   ( 2022 ) . One possible reason is that easier exercises   are more likely to produce correct answers , which   in turn increases the averaged predicted probability   of DKT ( i.e. , estimated knowledge state ) .   Nevertheless , the above observations do not in-   fluence our conclusion as the superiority of our   model comes from its ability to adapt to students ’   knowledge ( § 5.3.1 ) and generate customized exer-   cises targeting individual needs ( § 5.3.2 ) , indepen-   dent of the simulation policy .   6 Conclusion   We propose an adaptive and personalized exercise   generation model combining recent advances in   knowledge tracing and controllable generation us-   ing pre - trained LMs . Our approach works by learn-   ing the dynamics between exercise difficulty and   student vocabulary knowledge in the domain of lan-   guage learning . Experimental results on real - world   language learning data from Duolingo demonstrate   that our model can generate adaptive and person-   alized exercises needed in an Educational setting .   We further showcase our model ’s applicability in   Education with simulation studies .   Ethics Statement   The learner data used in this study are anonymized   by Settles et al . ( 2018 ) and , to the best of our knowl-   edge , do not contain sensitive information . We   foresee no further ethical or privacy concerns with   the work.10192Limitations   We state the limitations of this work from the fol-   lowing aspects . First , we make an initial assump-   tion about the dynamics between exercise difficulty ,   vocabulary , and student knowledge . While we be-   lieve our assumption is sensible in the domain of   language learning , we acknowledge that we make   some simplifications for the ease of modeling . For   example , we measure difficulty using individual   performance , whereas a better way could be com-   bining it with inherent problem difficulty , e.g. , text   complexity . Besides , we only consider vocabulary   mastery in defining student knowledge and predict-   ing their performance . Exploring more dimensions   of language knowledge ( e.g. , syntax ) might lead to   a finer - grained personalization . Second , our model   relies on student learning logs to estimate their real-   time knowledge states . This model might face the   cold start problem when dealing with insufficient   history . Though it is beyond the scope of this study ,   techniques like computerized adaptive testing can   be used to combat this problem . Lastly , due to the   lack of a real learning environment , we discuss the   educational promise of our model with simulation   experiments . In the future , a user study can be   incorporated to validate our conclusions .   References101931019410195A Decoding Algorithm   Algorithm 1 Pseudo - code for our Lexical Diffi-   culty Constrained Decoding   B Experimental Setup   B.1 Dataset Details   The statistics of our dataset are summarized in Ta-   ble 5 . Each interaction records a target sentence ,   per - token correctness labels of the student ’s re-   sponse , and meta information such as user nation-   ality and response time . We group interactions by   user_id ( anonymous ) in temporal order to obtain   per - student interaction sequences . Refer to Settles   et al . ( 2018 ) for more descriptions of the dataset .   B.2 Implementation Details   We implement our models using the Transform-   ers library ( Wolf et al . , 2020 ) . Our knowledge   tracing model is a three - layer LSTM with a hid-   den size of 100 . We train it for 10 epochs with   the regularization weights λ= 0.5 , λ= 0.1 ,   selected on the validation set . For the exercise   generator , we fine - tune a pre - trained BART - base(Lewis et al . , 2020 ) for up to 10 epochs . An early   stop strategy is applied when the loss on the vali-   dation set does not decrease for three continuous   epochs . We first train the DKT and exercise gener-   ator separately until both of them converge . Then ,   we jointly optimized the two models with hypear-   ameters : γ= 1 , γ= 0.8 , τ= 2 . During genera-   tion , we set the beam size to 4 . The weights αfor   word and difficulty constraints are set to 0.1 and   0.5 as the word constraint is easy to achieve in our   experiments . We use Nvidia Tesla A100 with 40   GB of GPU memory for training and inference . On   a single GPU , one training epoch of the exercise   generator takes about 30 minutes , and that of DKT   takes about 7 minutes when they are separately   trained . Joint training takes a longer time , about an   hour for one epoch . We report the average results   over three runs .   C Influence of Regularization in KT   To inspect the influence of regularization terms   ( Eq . 8) on the KT performance , we conduct a grid   search for λandλon the validation set . As can   be seen from Table 6 and Table 7 , Lconsistently   improves exercise - level performance at the cost of   sacrificing word - level performance , whereas L   with a suitable weight ( λ= 0.3 ) can improve   both in most cases . This suggests the students ’   knowledge states transit gradually over time . We   choose λ= 0.5 , λ= 0.1for the best balance .   λAUC λ0.0 0.1 0.3 0.5   0.0 79.51 79.50 79.57 79.53   0.1 79.44 79.45 79.49 79.52   0.3 79.42 79.40 79.44 79.36   0.5 79.32 79.43 79.41 79.30   λAUC λ0.0 0.1 0.3 0.5   0.0 70.89 70.98 70.85 71.15   0.1 71.04 71.02 71.06 71.23   0.3 71.41 71.31 71.43 71.31   0.5 71.41 71.48 71.45 71.4510196ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Ethical and Privacy Considerations   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   3   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Appendix B.1   /squareB1 . Did you cite the creators of artifacts you used ?   Appendix B.1   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Appendix B.1   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Appendix B.1   C / squareDid you run computational experiments ?   Appendix B.2   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix B.210197 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix B.2   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Appendix B.2   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix B.2   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.10198