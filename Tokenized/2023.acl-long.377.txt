  Rongxin Zhu Jianzhong Qi Jey Han Lau   School of Computing and Information Systems   The University of Melbourne   rongxinz1@student.unimelb.edu.au , { jianzhong.qi,laujh}@unimelb.edu.au   Abstract   A series of datasets and models have been   proposed for summaries generated for well-   formatted documents such as news articles . Di-   alogue summaries , however , have been under   explored . In this paper , we present the first   dataset with fine - grained factual error annota-   tions named DSF . We define fine-   grained factual error detection as a sentence-   level multi - label classification problem , and we   evaluate two state - of - the - art ( SOTA ) models on   our dataset . Both models yield sub - optimal re-   sults , with a macro - averaged F1 score of around   0.25 over 6 error classes . We further propose an   unsupervised model EDR via candi-   date ranking using pretrained encoder - decoder   models . Our model performs on par with the   SOTA models while requiring fewer resources .   These observations confirm the challenges in   detecting factual errors from dialogue sum-   maries , which call for further studies , for which   our dataset and results offer a solid foundation .   1 Introduction   Factual inconsistency in abstractive summariza-   tion — a phenomenon where model - generated sum-   maries contain facts that are inconsistent with the   source document — is a widely known problem   and has been studied extensively in the document   summarization community . An example is shown   in Figure 1 , where the source document is a di-   alogue — the type of documents that this paper   focuses on .   Existing work covers topics on factual inconsis-   tency including error typology and factuality an-   notations of state - of - the - art neural summarization   models ( Maynez et al . , 2020 ; Huang et al . , 2020 ;   Pagnoni et al . , 2021 ; Goyal and Durrett , 2021 ; Fab-   bri et al . , 2021 ; Gao and Wan , 2022 ; Tang et al . ,   2022a ) , automatic factual error detectors ( Wang   et al . , 2020 ; Goyal and Durrett , 2020 ; KryscinskiFigure 1 : Example summaries that are factually consis-   tent and inconsistent with a source dialogue .   et al . , 2020 ; Durmus et al . , 2020 ; Zeng et al . , 2021 ;   Scialom et al . , 2021 ) , methods to correct factual   errors in summaries ( Cao et al . , 2020 ; Dong et al . ,   2020 ; Chen et al . , 2021a ) and methods to produce   factually more consistent summaries ( Zhao et al . ,   2020 ; Cao and Wang , 2021 ; Tang et al . , 2022b ;   Zhu et al . , 2021 ; Aralikatte et al . , 2021 ; Chen et al . ,   2021b ; Balachandran et al . , 2022 ) . Almost all of   these works focus on news summarization based on   two datasets : CNN / D M(Hermann et al . ,   2015 ; Nallapati et al . , 2016 ) and XS(Narayan   et al . , 2018 ) .   Dialogue summarization ( cf Figure 1 ) , which   aims to produce a condensed version of a dialogue   while maintaining its salient information , is equally   important due to its application to summarizing   meeting transcripts ( Li et al . , 2019 ; Zhu et al . , 2020 ;   Zhong et al . , 2022 ) , daily conversations ( Chen and   Yang , 2020 ; Liu and Chen , 2021 ; Feng et al . , 2021 ) ,   customer service dialogues ( Liu et al . , 2019 ; Zou   et al . , 2021 ) and medical dialogues ( Joshi et al . ,   2020 ; Krishna et al . , 2021 ) . However , factual con-   sistency in dialogue summarization is under ex-   plored as there are currently no benchmark datasets   that contain fine - grained error categories . This pa-   per aims to fill in this gap .   To investigate factual consistency in dialogue6825summarization , we release DSF with   fine - grained sentence - level annotations regarding   factual consistency for 475 model summaries   ( 1,340 sentences ) from six neural dialogue sum-   marization models on two popular datasets : SAM-   S(Gliwa et al . , 2019 ) and QMS(Zhong   et al . , 2021 ) . We adopt a two - dimensional typology   that considers the semantic roles and verifiability   of error spans separately .   We formulate factual error detection as a   sentence - level multi - label classification task and   useDSF to evaluate two state - of - the - art   factual error detection models designed for docu-   ment summarization . As there are no existing error   detection model for fine - grained error categories ,   we adapt the two binary classification models to fit   to our task . Empirical results show that they do n’t   work well on the task , indicating its difficulty and   the domain gap between document summarization   and dialogue summarization .   We then propose two models : BM and   EDR .BM is a multi - class clas-   sification model trained on synthetic data , which   is created by corrupting sentences from reference   summaries ( Kryscinski et al . , 2020 ) . EDR- is a simple unsupervised model that can   leverage any pretrained encoder - decoder model to   detect factual errors . Given a model - generated   summary sentence containing a span of interest   for error detection , EDR computes log   likelihood scores for the sentence and its vari-   ants containing replacement spans fetched from   the source dialogue . The scores are computed as   BARTS ( Yuan et al . , 2021 ) , which will be   explained in 4.2 . We compare the scores of the   sentences to determine if the span of interest and   hence the summary sentence contains a factual er-   ror . We run experiments with T5(Raffel et al . ,   2020 ) , BART ( Lewis et al . , 2020 ) and PEGA-   SUS ( Zhang et al . , 2020 ) , fine - tuned either on   news summarization or dialogue summarization ,   as the encoder - decoder for EDR . The   results show that BM andEDR   performs on par with the adapted state - of - the - art   models in terms of macro - averaged F1 .   Motivated by the strong complementarity be-   tween models , we further present two ensemble   models combining the four models above . The re-   sults , while exceeding those of the individual mod-   els , are still far from indicating a practical model   for factual error detection over dialogue summaries . This calls for further studies , for which our dataset   and results form a solid foundation .   To summarise , this paper makes the following   contributions :   •We annotate and present DSF , the   first dataset with fine - grained sentence - level   factual errors for dialogue summarization , pro-   viding rich annotation including error classes ,   erroneous spans and explanation .   •We investigate the effectiveness of adapting   state - of - the - art factual error detection mod-   els for document summarization on model-   generated dialogue summaries , demonstrating   the difficulty of the task .   •We propose BM , a weakly-   supervised multi - class classifier and   EDR , an unsupervised factual   error detector that requires no human labeled   data for training and can leverage existing   pre - trained encoder - decoder models . Both   models perform on par with adapted SOTA   factual error detection models for document   summarization .   •Our experiments and analyses reveal the   strengths and weaknesses of different factual   error detection models , and point out future   directions to improve them .   2 Related Work   Error typology and datasets . There are a few   existing datasets on factual errors . Some of them   use binary ( factually consistent or inconsistent ) la-   bels ( Kryscinski et al . , 2020 ; Wang et al . , 2020 )   and 5 - point Likert Scale labels ( Fabbri et al . , 2021 ;   Gao and Wan , 2022 ) , which require lower efforts   to annotate , but they do not provide information on   how and where factual errors were made . To sup-   port fine - grained analysis , multi - class and multi-   dimensional typologies are designed . Pagnoni   et al . ( 2021 ) propose a linguistically motivated an-   notation framework that covers semantic frame er-   rors , discourse errors and content verifiability er-   rors . Goyal and Durrett ( 2021 ) use a 2 - dimensional   typology , where content verifiability and semantic   error types are considered separately . Cao et al .   ( 2022 ) focus on hallucinations and consider both   factual and non - factual hallucination . Tang et al .   ( 2022a ) unify different error types from previous6826works into a hierarchical taxonomy . These datasets   mostly focus on news summaries .   DialSummEval ( Gao and Wan , 2022 ) is another   popular dataset that contains annotation on fac-   tual consistency of model - generated dialogue sum-   maries . The core difference of our work is that   we consider fine - grained error categories and the   text span ( i.e. , starting and ending position ) of an   error . Thus it provides a more elaborate , diagnos-   tic assessment as to what and where goes wrong   when a summary is not factually consistent . In   comparison , DialSummEval only considers coarse-   grained assessment of factuality using 5 - point Lik-   ert Scale ( Joshi et al . , 2015 ) , without specifying the   actual error type ( e.g. , entity error ) .   Factual error detection models . Most popular   factual error detectors are based on either textual-   entailment or question - answering ( QA ) .   Textual - entailment - based models are generally   binary classifiers that take as input the source doc-   ument and a model - generated summary . For exam-   ple , Kryscinski et al . ( 2020 ) train binary factual   error classifiers using synthetic data . Zeng et al .   ( 2021 ) use a gradient - based adversarial method to   improve model accuracy . Goyal and Durrett ( 2020 )   leverage dependency - level entailment achieving   better performance and interpretability .   QA - based models first generate questions from   a model - generated summary ( or source dialogue ) ,   and then answer those questions based on its source   dialogue ( or a model - generated summary ) . The   factual consistency is decided by the similarity be-   tween the ground truth answer and the predicted   answer . For example , Wang et al . ( 2020 ) ; Durmus   et al . ( 2020 ) use a precision - oriented method that   generates questions from model - generated sum-   maries and answer them using the source document .   Scialom et al . ( 2019 ) instead generate questions   from a source document and answer them using   the summary , making it a recall - oriented method .   Scialom et al . ( 2021 ) combine recall and precision-   oriented techniques into a single framework . Fab-   bri et al . ( 2022 ) refine the model component design   and obtain a QA - based method that outperforms   textual - entailment - based methods .   Our unsupervised method EDR com-   pares a span ( e.g. , a person name ) in a model-   generated sentence with candidates ( e.g. , other peo-   ple ’s names in the dialogue ) and decide the factual   consistency of the span based on its rank among   candidates . It achieves comparable macro F1 withadapted SOTA factual error detectors for document   summarization but requires no labelled resources .   3 The DSF Dataset   This section presents our DSF dataset   and procedures to construct the dataset .   3.1 Data Source   To cover dialogues from different domains , we se-   lected two popular datasets SAMS(Gliwa et al . ,   2019 ) and QMS(Zhong et al . , 2021 ) . SAM-   Scontains daily conversations and gold sum-   maries . QMScomes with queries and answers   based on meeting transcripts . The answers to each   query can be seen as a summary to an aspect of the   meeting transcript .   For both SAMSandQMS , we randomly   sampled 60 dialogues and their summaries in its   test split . ForQMS , we only chose queries   whose gold utterances contain no more than 700   tokens according to Bert tokenizer . We manu-   ally filtered out dialogues with sensitive contents   ( e.g. , dirty words and potential bias on gender or   race ) . More statistics on the dataset can be found   in Appendix Table 5 and Table 6 .   3.2 Summary Generation Models   We generally choose models with publicly acces-   sible pretrained model checkpoints or generated   outputs instead of training models ourselves .   On SAMS , we use five models :   BART ( Lewis et al . , 2020 ) , PEGASUS ( Zhang   et al . , 2020 ) , S - BART ( Chen and Yang , 2021 ) ,   CONDIGSUM ( Liu et al . , 2021 ) and GPT-   3(Brown et al . , 2020 ) . For S - BART and   CONDIGSUM , we obtain model outputs from   the original papers . For BART andPEGASUS ,   we generate output by running their pre - trained   models . ForGPT-3 , we fine - tune curie over   SAMSum dataset and generate summaries using   the official API .   OnQMS , we use three models : PEGASUS ,   BART andDialogLM ( Zhong et al . , 2022 ) . Since   we only focus on specific queries ( i.e. , queries that6827   only ask about an aspect of a meeting , instead of   summarizing the whole meeting ) , which is a sub-   set of the original dataset , we fine - tuned them us-   ing specific queries only . The fine - tuned models   achieve ROUGE scores that are better or compa-   rable to state - of - the - art models on the complete   dataset .   3.3 Typology of Factual Errors   Motivated by Goyal and Durrett ( 2021 ) ; Pagnoni   et al . ( 2021 ) , we adopt a 2 - dimensional typology   that treats semantic role and content verifiability of   error spans separately .   On the semantic role dimension , we consider six   error classes Entity Error ( EntE ) , Predicate Er-   ror ( PredE ) , Circumstance Error ( CirE ) , Coref-   erence Error ( CorefE ) , Link Error ( LinkE ) and   Others , with definitions and examples shown in Ta-   ble 1 . EntE , PredE , CirE are semantic frame errors ,   and CorefE , LinkE are discourse errors . When   a sentence in the summary does not contain any   factual error , we label it as No Error .   For content verifiability , we consider IntrinsicError ( i.e. , the error span consists of tokens from   the source dialogue ) and Extrinsic Error ( i.e. , the   error span consists of tokens not mentioned in the   source dialogue ) , a.k.a . hallucinations . This dimen-   sion is only defined for EntE , PredE and CirE.   3.4 Annotation Procedure   We recruited 12 workers for the annotation task ,   including nine PhD students majored in natural   language processing and three Master ’s students   majoring in linguistics and information technology .   All annotators are fluent English speakers . We take   an in - house annotation approach because a trial on   Amazon Mechanical Turk did not yield meaningful   results , even though high - quality crowd - sourced   workers were sourced through strict constraints .   The 12 annotators form six pairs randomly where   each pair annotates 10 dialogues from each dataset .   The annotation is done in three stages : pilot   study , full annotation and annotation adjudication .   An annotation task involves analysing a dialogue   and the summaries generated by all corresponding   models . During the pilot study , annotators are re-   quired to go through the definition and examples   for each error class to learn the labelling typology .   Then , they will work on two pilot tasks , which are   the same for all workers . For each task , a source di-6828alogue and a model - generated summary are shown   at the same time , and the annotator needs to label   any factual errors in each individual sentence in the   summary . When all sentences in the summary are   done , another summary generated by a different   model will be shown . Models are anonymized and   their generations are shown in random order .   During the full annotation stage , we assign each   annotator 10 tasks from each dataset , which are dif-   ferent from the tasks in pilot study . The annotations   are only done for the semantic role dimension .   In the adjudication stage , the two annotators of   a pair along with an annotation quality controller   ( one of the authors of this paper ) go through the   annotations to resolve any disagreements , and de-   tailed notes were taken for reaching the final deci-   sions ( which is released as part of the dataset as   it can be useful for future analysis ) . Annotation   mistakes are also corrected in this process . In the   end , a total of 1340 sentences ( 99.7 % ) with agreed   annotations were obtained , while the rest of the   sentences were discarded because no agreement   can be made .   Note that the annotations on the content veri-   fiability dimension are manually created by the   annotation quality controller based on the detailed   meeting notes of the last stage . It is a product of   a post - annotation process because the original an-   notators did not explicitly label the error type as   extrinsic or intrinsic . Instead , the annotators mark   anExtrinsic Error for all error spans that are not   mentioned in the source dialogue . The annotation   quality controller takes this information and further   split them into EntE , PredE and CirE based on the   semantic role of an error span , and assign Intrinsic   Error to all original EntE , PredE and CirE , thus   obtaining a 2 - dimensional annotation .   3.5 Inter - annotator Agreement   We use Cohen ’s Kappa ( McHugh , 2012 ) to evaluate   the inter - annotator agreement . The scores in each   group before adjudication are as follows . We first   evaluate the agreement for binary label by merging   all error types into a single negative class . The   scores are 0.39 , 0.44 , 0.57 , 0.59 , 0.43 , 0.51 . For   multi - class label , the scores are 0.34 , 0.33 , 0.44 ,   0.31 , 0.31 , 0.25 . After adjudication we have full   agreement for all instances ( as explained in Sec-   tion 3.4 ) .   3.6 Results on the Summarization Models   We summarize the performance results of the sum-   marization models as derived from the annotations   in this subsection . Figure 2 and Figure 3 show the   factual error class distribution of the summarization   models evaluated on SAMSand QMS .   Overall , 33.3%and41.9%sentences in model-   generated summaries contain one or more factual   errors in SAMSand QMS , respectively .   The average number of errors for a factually in-   consistent sentence is 1.14 . This indicates a broad   existence of factual errors in the model - generated   summaries , thus emphasizing the importance to   resolve factual errors in dialogue summarization .   Semantic frame errors ( i.e. , EntE , PredE and   CirE ) are more frequent than discourse errors ( i.e. ,   CorefE and LinkE ) overall , while their distributions   are not the same on both datasets . SAMShas a   higher portion of factually inconsistent sentences   caused by semantic frame errors ( 76.9 % ) than QM-   Shas ( 58.9 % ) , while QMShas a higher por-   tion of discourse errors ( 24.0 % ) than SAMS   ( 11.3 % ) . We observe two main reasons for this6829discrepancy . First , the sentences in QMSare   longer and exhibit more complex discourse struc-   tures , especially causal relations , which can be chal-   lenging for models to summarize . Second , models   fine - tuned on QMStend to copy large chunks   of the input dialogue . Many pronouns are directly   copied from the source dialogue without proper   context , causing Coreference Errors ( CorefE ) .   Among the different summarization models ,   BART andPEGASUS have been evaluated on   both datasets where BART generates summaries   with fewer factual errors consistently . On SAM-   S,24.0%of the sentences generated by BART   contain factual errors , which is the fewest , while   the highest portion is reported by GPT-3 , i.e. ,   58.7%.CONDIGSUM andS - BART are vari-   ants of BART that achieve better ROUGE scores   than BART using contrastive learning and dia-   logue structure information , respectively . Our re-   sults reveal that both models produced more sen-   tences with factual errors than BART did , indi-   cating that improvement in ROUGE may not help   with the factual consistency of summaries . This   result emphasizes the importance of more bench-   mark datasets for dialogue summarization model   evaluation . On QMS , BART is still the best ,   while D LMproduced the highest proportion   of sentences with factual errors .   On the content verifiability dimension , models   onQMSproduce more extrinsic errors than on   SAMSum . A potential reason is that reference sum-   maries in QMScontain more tokens outside   the source dialogue . For SAMS , all models are   mainly dominated by intrinsic errors , while GPT-3   produces more extrinsic errors than intrinsic ones .   4 Detecting Factual Errors   In this section , we automate factual error detec-   tion in model - generated summaries . We first adapt   two state - of - the - art factual error detection models   from document summarization . We then propose a   weakly supervised multi - class classifier and a sim-   ple yet effective unsupervised model that can utilize   any pretrained encoder - decoder model to identify   factual errors . Finally , we present ensemble - based   models combining all techniques above .   Problem statement . We formulate factual er-   ror detection as a sentence - level multi - label clas-   sification task , i.e. , given an input dialogue and   a sentence from a model - generated summary , we   classify whether the sentence contains any ( seman - Dependency Arc Types Error Class   nsubj , obj , obl : agent , iobj , dobj ,   nmod , vocative , appos , nummod ,   compound , amod , det , clf , flatEntE   obl : tmod , advmod CirE   aux PredE   other arc types Others   tic role ) factual errors as outlined in Section 3.3 .   4.1 Adapted State - of - the - Art Models   DAE ( Goyal and Durrett , 2020 ) is based   on dependency - level entailment , which predicts   whether a dependency arc in a model - generated   sentence is entailed by the input document ( e.g. ,   a dialogue in our problem ) . To adapt it to our   problem , we design rules to map from dependency   arc types to our factual error classes , as shown in   Table 2 . Given a summary sentence , we use the   trained DAE provided by the authors to predict   dependency arcs in the sentence . The union of all   factual error classes corresponding to the types of   the predicted erroneous dependency arcs will be   used as our factual error predictions . Note that not   all factual error classes have corresponding depen-   dency arc types and hence not all error classes can   be detected by this model .   QAFactEval ( Fabbri et al . , 2022 ) is a QA - based   factual error detector . Given a question genera-   tion model ( QG ) and a question answering model   ( QA ) , which are trained on existing datasets for the   question answering task , it works as follows : ( 1 )   Question - worthy spans ( s ) , which are noun phrases   and named entities , are extracted from a model-   generated summary . ( 2 ) For each s , a question is   generated by QG based on sand the summary . ( 3 )   The QA model predicts an answer abased on the   question and the source document . ( 4 ) The similar-   ity between sandais measured by some metric .   ( 5 ) The factual consistency of the summary is made   based on the similarity scores for all sin it .   We use the learned metric LERC ( QuIP ) men-   tioned in the paper and report a factual error if the   similarity score between sandais smaller than   a threshold T(a hyper - parameter ) . Question-   worthy spans of different semantic roles correspond6830   to our semantic role - based factual error classes , as   outlined in Algorithm 1 in Appendix . We obtain the   semantic role of a question - worthy span by a pre-   trained structured prediction model in AllenNLP   2.9.3 .   W -S -C is a   multi - class classifier that we construct . It takes   as input a source dialogue and a generated sum-   mary sentence to predict factual error classes in   the sentence , motivated by Kryscinski et al . ( 2020 ) .   We create synthetic training data by corrupting sen-   tences in reference summaries as follows .   For Entity Error , Circumstance Error and Coref-   erence Error , we replace named entities or pro-   nouns with those randomly picked from the same   category . For Predicate Error , we replace verbs   with other randomly chosen verbs . We match   the form ( e.g. , tense ) of the selected verbs to the   original one . Negative replacements for all above   classes are extracted from either the source dia-   logue or the whole dataset . For Link Error , we   replace a discourse marker corresponding to causal   relation ( e.g. , because ) with another one indicat-   ing a reversed causal relation ( e.g. , so ) . More de-   tails on our synthetic data generation are in Ap-   pendix A.3.1 .   We use cross entropy loss to train the classifier ,   which is based on BERT ( Devlin et al . , 2019 ) with   a linear layer on top of [ CLS ] representation for   classification . We concatenate the source dialogue   and a sentence , delimited by [ SEP ] , as input.4.2 EDR   Here , we present our proposed unsupervised model ,   EDR . Given a generated summary sen-   tence , it first identifies a set of spans of interest   ( SOI ) which may correspond to factual errors . For   each SOI , EDR replaces it with differ-   ent candidate spans and calculates a score for each   span including the SOI . The factuality of the SOI   is then decided based on its score among the scores   of all candidate spans . Figure 4 summarizes the   workflow of EDR . Below we detail core   steps of EDR : ( 1 ) SOI identification , ( 2 )   candidate span generation , ( 3 ) span scoring and   ( 4)ranking - based factual error detection .   Span of interest identification . An SOI is a   snippet in a sentence for factual error classifica-   tion . We consider noun phrases , named entities   and verbs as SOIs , which are obtained using spaCy   3.1.4.We obtain the semantic roles of the SOIs   like for QAFE , which will be used to de-   cide the error class of an SOI later .   Candidate span generation . For each SOI , we   create a set of candidate spans that can potentially   replace it in the model generated summary sen-   tence . For a named entity SOI , the candidate spans   are entities of the same named entity class ( e.g. ,   PERSON ) of the SOI extracted from the input di-   alogue . For the PERSON class , in particular , we   include all speaker names on top of all other PER-   SON named entities extracted . For a verb SOI , we   extract all verbs from the input dialogue according6831   to the Part - of - Speech tags and match the form ( e.g. ,   tense ) with the SOI . For a noun phrase SOI , all   noun phrases from the input dialogue are consid-   ered as candidate spans . All candidate spans are   extracted using spaCy 3.1.4 .   Span scoring . LetDbe an input dialogue and   Sbe a generated summary sentence with ntokens   { w , w , · · · , w , w } , which includes a candi-   date span or an SOI , denoted by c. We adopt a   encoder - decoder model Mto calculate a sentence   score for Sconditioned on Das follows , which   is used as the score of span c , denoted by score .   Mcan be any pre - trained encoder - decoder model ,   such as a summarization model .   score=1   n / summationdisplaylogp(w|w , D ) ( 1 )   Intuitively , the score is the average log likelihood   of each token winS , conditioning on the previous   tokens in S(i.e . , w ) and D. Here , wis the   starting token of the decoder .   Ranking - based factual error detection . Given   a set of candidate spans C={c , c , · · · , c}of   an SOI , we form |C|sentences by replacing the   SOI with each of the candidate spans . We calculate   span scores for the SOI and the candidate spans ,   and rank the spans by their scores in descending   order . If the SOI has a rank larger than a threshold   T(a hyper - parameter ) , we report it as erroneousand determine its error class based on its semantic   role , as summarized in Algorithm 1 ( cf . Appendix ) .   The same process is repeated for all SOIs in S. The   union of all error classes detected for the SOIs is   the final factual error classes predicted for S.   4.3 Ensemble Modeling   We further build two simple ensemble models   based on the four models above : Most Freq uent   Voting ( FV ) and Logistic regression   ( L ) .FV takes all predicted er-   ror classes from the four models above and uses   the class(es ) with the largest frequency as the fi-   nal prediction . For L , we train a logistic   regression model for each factual error class that   takes the binary outputs from the four models above   as features . We use the union of all factual error   classes predicted by the different logistic regression   models as the final prediction .   4.4 Experiments   To evaluate the models described in the last section ,   we perform 5 - fold cross validation ( Stone , 1978 )   using DSF.Implementation details and   parameter settings are discussed in Appendix A.3 .   We record the F1 scores ( mean and standard devia-   tion ) of the models on each error class in Table 3.6832Results : All models can detect EntE signifi-   cantly and consistently better than the other classes .   Different models show advantage on different er-   ror classes , while no model can outperform all the   others on all error classes .   QAFE performs the best on EntE ( 0.45 )   and CirE ( 0.23 ) but poorly on the other error   classes . The reason is that only named entities   and noun phrases are treated as question - worthy   spans . Future work may consider question - worthy   spans of different types , such as verbs and dis-   course markers , to cover more error classes .   DAE performs well on EntE and Others , while   it suffers on CirE , PredE and CorefE. The main   reason is that not all error classes are covered in the   rules mapping from dependency arc to error class .   Since a dependency arc is related to two words , rule   designing is not easy . Future work may leverage   learned models to predict error class automatically .   BM shows the best results on CorefE   ( 0.29 ) but poor performance on CirE , PredE and   Others , despite its high performance on synthetic   validation dataset ( 0.98accuracy ) . It indicates the   difference between synthetic and real factual errors .   Our proposed model EDR using dif-   ferent pretrained encoder - decoder models generally   exhibits strong results on EntE , PredE and CorefE ,   while more improvements need to be done on CirE   and Others . Among all variants of EDR ,   PEGASUS- performs on par with QAFE- in terms of macro - averaged F1 score , while it   does not require question generation and question   answering models .   The two ensemble models improve on the micro   and macro - averaged F1 , indicating complementar-   ity among the models . For most error classes , the   ensemble models usually have the best or second   best performance .   Overall , none of the models yielded a particu-   larly high F1 score for any error class . It shows   that fine - grained factual error detection in dialogue   summaries is a challenging problem which calls for   further studies , for which our results and dataset   will serve as a solid foundation .   5 Conclusions   We created a fine - grained multi - faceted dataset   named DSF on factual consistency of   dialogue summarization . DSF offers in-   sights into how and where current neural summa-   rization models fail when they produce factuallyinconsistent details in dialogue summaries . It can   also serve as a testbed for automating factual error   detection . Our proposed error detection method ,   EDR , is shown to perform on par with   state - of - the - art models even though it requires no   labelled training data . That said , we ultimately   found that even ensembling several error detec-   tion methods do not produce results that are good   enough for practical use , indicating opportunities   for future research in this area .   6 Limitations   EDR is only tested on DSF .   Further tests on more datasets are required to estab-   lish its general applicability .   7 Ethics Statement   This study is conducted under the guidance of the   ACL code of Ethics . We manually filtered out   potential offensive content and removed all infor-   mation related to the identification of annotators .   The annotators are all fairly paid based on the Aus-   tralian minimum wage . The annotation protocol is   approved under Human Ethics LNR Application   with reference number 2022 - 24233 - 30104 - 3 .   Acknowledgements   This research was undertaken using the LIEF HPC-   GPGPU Facility hosted at the University of Mel-   bourne . This Facility was established with the as-   sistance of LIEF Grant LE170100200 . We want to   thank Gisela Vallejo , Han Sun , Miao Li , Rui Xing ,   Wei Gao , Yanchuan Chang , Yulia Otmakhova ,   Zheng Wei Lim , Zhexi Li , Zhuohan Xie for their   help in the annotation .   References683368346835   A Implementation Details   A.1 Cross Validation Settings   We randomly split DSF into 5 portions   with equal number of examples and keep the splits6836consistent across all models . Each time we take   one portion as the test set and combine the other   four portions for training or validation , or both .   The details for the evaluation of each model are   described below .   •BM , DAE andFV : there   is no hyper - parameter to tune . The model   is only evaluated on different test sets for 5   times .   •QAFE andEDR : they are   unsupervised models so no training is needed .   Each time the four portions are combined as   validation set for hyper - parameter tuning .   •L : since this model requires super-   vised training , we combine the four portions ,   shuffle it and further split it into training set   and validation set , following a ratio of 7:3 .   The validation set is used for hyper - parameter   tuning .   A.2 Summary Generation Models   GPT-3 : we use a batch size of 64 and fine - tune it   for 2 epochs . During inference , temperature is set   to1.0andmax_tokens is set to 100 . The finetuned   model achieves 41.7 and 15.9 on ROUGE-1 and   ROUGE-2 .   D LM : we finetune MingZhong / DialogLED-   large-5120 proposed in the original paper . We   finetune it for 5 epochs using a batch size of 32 ( per-   device batch size is 2 , gradient accumulation is 16 )   and learning rate 3×10 . The fine - tuning takes   30 minutes . The finetuned model achieves 38.48   and13.70on ROUGE-1 and ROUGE-2 , which are   higher than 34.50and9.92reported in the original   paper .   PEGASUS : we finetune google / pegasus-   cnn_dailymail for 5 epochs using a batch size of 32   ( per - device batch size is 2 , gradient accumulation   is 16 ) and learning rate 3×10 . The fine - tuning   takes 15 minutes . The finetuned model achieves   33.56and11.35on ROUGE-1 and ROUGE-2 .   BART : we finetune facebook / bart - large - cnn for 5   epochs using a batch size of 32 ( per - device batch   size is 2 , gradient accumulation is 16 ) and learning   rate3×10 . The fine - tuning takes 25 minutes .   The finetuned model achieves 40.46and14.93on   ROUGE-1 and ROUGE-2.All original models come from huggingface   model hub . The fine - tuning for BART ,   PEGASUS and D LM is conducted us-   ingrun_summarization.py from Transformers   4.14.0 .   During training , the input is the concatenation   of the query and its relevant utterances , which is a   subset of the whole meeting transcript . Utterances   are concatenated as a long string , the query and   utterances are delimited by “ || ” .   A.3 Error Detection Models   A.3.1 W -S -C   To obtain corrupted reference sentences with Entity   Error , Coreference Error and Predicate Error , we   first extract named entities , noun phrases and verbs   using spaCy 3.1.4 , then get their semantic roles like   forQAFE in Section 4.1 . We finally map   from semantic role to factual error class according   to Algorithm 1 .   We generate 80k negative examples for each er-   ror class , among which 75k are used for training   and 5k for validation . For EntE , PredE and CirE ,   the negative replacements for half of the data come   from the same dialogue , while another half of the   data uses negative replacements extracted from the   whole dataset excluding the dialogue correspond-   ing to the sentence . In this case we include both   intrinsic and extrinsic negative replacements . Sen-   tences from reference summaries are used for No   Error .   We use run_glue.py from Transformers 4.14.0   for model training . The pretrained model we use   for BERT is bert - base - uncased .   We tune batch size among 16 , 32 , 64 and 128 .   The best value is 64 according to the accuracy on   validation set ( 98.24 % ) . The model is trained for 8   epochs and evaluated every 500 steps . The learning   rate we use is 3×10 . The training takes 8 hours   on a Tesla V100 GPU with 32GM RAM .   A.3.2 EDR   The details of the pretrained models that we use   are as follows :   BART- -:facebook / bart - large - cnn   BART- - : lidiya / bart - large - xsum-   samsum   PEGASUS-:google / pegasus - cnn_dailymail6837PEGASUS- : transformersbook / pegasus-   samsum   T5- - : sysresearch101 / t5 - large-   finetuned - xsum - cnn   T5- - : We fine - tune it using   run_summarization.py from Transformers 4.14.0   based on sysresearch101 / t5 - large - finetuned - xsum-   cnn . The final batch size is 2 with a gradient   accumulation steps of 16 ( i.e. , the conceptual   batch size is 2×16 = 32 ) . The model is trained   for 8 epochs on a single NVIDIA A100 ( 40 G )   GPU , taking 5 hours . We choose the batch size 32   among [ 8 , 16 , 32 ] because it produces the highest   ROUGE-1 and ROUGE-2 on validation set .   A.3.3 DAE   We use the trained classifier provided by the au-   thors of the DAE modeland process each sen-   tence in a model - generated summary separately . A   dependency arc is considered as erroneous if the   predicted probability for the positive class is less   than0.5 .   A.3.4 QAFE   We use the model provided by the authorsand   retrieve the similarity score between ground truth   answers and predicted answers from logs , given   by the learned model LERC ( QuIP ) . We tune the   threshold Tamong [ 0.5,1.0,1.5,2.0 ] and choose   0.5as the final value , as it produces the highest   macro - averaged F1 score .   The process to map from semantic role to factual   error class is outlined in Algorithm 1 .   A.3.5 EDR   We tune T ( i.e. , the rank threshold ) among [ 1 , 2 , 3 ,   4 , 5 , 6 , 7 , 8 , 9 , 10 ] and choose the smallest value   that achieves the highest macro - averaged F1 on   the validation set . The best T values for different   pre - trained models are as follows :   • BART- - : 2   • BART- - : 2   • PEGASUS- : 3   • PEGASUS- : 2   • T5- - : 3   • T5- - : 3Algorithm 1 Semantic Role to Factual Error Class .   arg0 to arg5 are core semantic roles such as subject   and object . ‘ ARGM ’ is the prefix for non - core se-   mantic roles such as ARGM - TMP ( temporal modi-   fier ) . V represents ‘ verb ’ .   Require : s ▷ a Span - of - Interest   Require : sr ▷ the semantic role of s   pronouns ←[i , we , us , you , he , him , she , her ,   it , they , them , this , that , these , those , myself ,   yourself , himself , herself , ourselves , yourselves ,   themselves ]   ifsrin[arg0,arg1,arg2,arg3,arg4,arg5]then   ifs∈pronouns then   Return CorefE   else   Return EntE   end if   else if srcontains ‘ ARGM ’ then   Return CirE   else if sr=‘V’then   Return PredE   else   Return Others   end if   To avoid repeated encoding for the same dialogue ,   which corresponds to multiple sentences for factual   error detection , we cache the encoded representa-   tion in encoder and reuse them to improve inference   speed .   The experiments are conducted on a single   Nvidia V100 GPU with 16GM RAM . The infer-   ence over a full pass of our dataset takes around   40 hours with a batch size of 1 . The computaional   overhead can be reduced by ( 1 ) reducing the num-   ber of Span of Interest ( SOI ) in a sentence , and   ( 2 ) reducing the number of candidates , especially   for noun phrases . We also tried distilled encoder-   decoder models , but the results are sub - optimal .   A.3.6 Ensemble Learning   For ensemble models ( i.e. , FV andL- ) , the best EDR is chosen based   on the performance on the validation set , which is   30 % of the four portions combined except the test   set , as introduced in A.1 .   During the training of L , we upsam-   ple the minority class to match the number of the   majority class for each logistic regression model   corresponding to different factual error types.6838Dataset # Mod # Summ # Sen Domain Annotation Typology   FactCC   ( Kryscinski et al . , 2020)10 / 1,434 news binary ( consistent , inconsistent )   QAGS   ( Wang et al . , 2020)2 474 / news binary ( consistent , inconsistent )   SummEval   ( Fabbri et al . , 2021)44 12,800 / news 5 - point Likert scale   Polytope   ( Huang et al . , 2020)10 1,500 / news multi - class   Cao’22   ( Cao et al . , 2022)1 800 / news multi - class   Maynez’20   ( Maynez et al . , 2020)5 500 / news binary ( intrinsic , extrinsic )   Frank   ( Pagnoni et al . , 2021)8 2,250 4,942 news multi - class   Goyal’21   ( Goyal and Durrett , 2021)3 50 / news multi - dimensional , multi - class   CLIFF   ( Cao and Wang , 2021)2 600 / news multi - class   ConFIT   ( Tang et al . , 2022b)4 76 / dialogue multi - class   DialSummEval   ( Gao and Wan , 2022)13 4,200 dialogue 5 - point Likert Scale   DSF ( ours ) 6 475 1,340 dialogue multi - dimensional , multi - class   B Data Annotation   B.1 Error Typology   For CorefE , if a reference comes without an-   tecedents in the input dialogue , we ignore the error   in the summary .   B.2 Annotation Tool   We modify a web application developed originally   for FRANK ( Pagnoni et al . , 2021)to fit to our   task . Specifically , we replace the example article   and model summaries with an example dialogue   and manually composed summaries to help explain   different error types . We also add an input field for   error span annotation in the main page . Screenshots   are shown in Figures 6 , 7 , 8 and 9 .   For in - house annotation , we deploy the web ap-   plication on Firebaseand provide with annotators   URLs to the tasks directly . B.3 Annotation Procedure   The initial annotation by all annotators follows the   typology proposed by Pagnoni et al . ( 2021 ) , which   includes two additional classes : Out - of - Article Er-   ror ( i.e. , Extrinsic Error in our paper ) and Grammar   Error . We merge Grammar Error to Others , and   treat Extrinsic Error as a separate dimension , as   outlined in 3.3 .   B.4 Payments to Annotators   All our annotators are volunteers . We pay 100 AUD   to each annotator . The annotation task begins after   they agree to the amount of payment .   B.5 Demographic Characteristics of   Annotators   1 annotator come from Colombia , 1 annota-   tor comes from Russia , 1 annotator come from   Malaysia , 9 annotators come from China . There   are 6 female and 6 male annotators.6839B.6 Consent from Annotators   We show the consent form in the annotation web ap-   plication . Annotation can only begin after consent   form is received from annotators .   C Case Study   As shown in Figure 5 , our EDR suc-   cessfully identifies an error of the span “ The team ”   because its rank is larger than the threshold T= 3 .   Since the semantic role of the span is arg0 , the   model predicts Entity Error according to Algo-   rithm 1 . On the right - side example , EDR   fails to report the error of “ muchroom picking ” , al-   though the factual consistent span “ horse racing ”   is ranked at the top among candidates . The rea-   son is that Tis too large . For future work , we   may design error identification methods using SOI-   specific thresholds rather than a universal threshold   for all SOIs .   D Potential Risks   The factual error detection models we propose ,   which are BM andEDR , do   not produce satisfactory performance to be used   for real applications . We do not advise people to   use them directly in real applications as factual er-   ror detectors for dialogue summarization without   further improvements .   E Intended Use of Existing Artifacts   The SAMSum ( Gliwa et al . , 2019 ) dataset is   shared on terms of the AttributionNonCommercial-   NoDerivatives 4.0 International ( CC BY - NC - ND   4.0 ) license . We provide additional information   ( i.e. , model - generated summaries and human anno-   tations ) without modifying the original data ( i.e. ,   dialogues and reference summaries).Data source SAMS QMS   # Exs 757 583   T 148.4 355.7   U 12.3 16.2   T 11.3 25.7   S 2.6 3.2   T / 14.9   Error Type Frequency   No Error 853   EntE 256   PredE 106   CirE 48   CorefE 62   LinkE 41   Others 426840684168426843ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   section Limitations between conclusion and References   /squareA2 . Did you discuss any potential risks of your work ?   Appendix D   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Introduction   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Our created artifacts are provided in an anonymous github repository . The artifacts we use are   mentioned in Section 3.1 ; Section 4.1 ; Appendix A.2 and A.3 .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3.1 ; Section 4.1 ; Appendix A.2 and A.3 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   The license is included in the anonymous github repositories .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Appendix E.   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   The end of Section 3.1 mentions how we ﬁlter out offensive contents . The Ethics Statments section   mentions that we protect annotators privacy .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   The documentation of our data and code are provided in the anonymous github repositories , men-   tioned in the footnote of abstract .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Introduction , Table 5 , Table 4 , Appendix A.3.16844C / squareDid you run computational experiments ?   Section 4.4 , Appendix A   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A.2 , A.3   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix A.2 , A.3   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Figure 2 , Figure 3 , Table 3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4.2 , Appendix A.3.1   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 3.4   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Figure 6 , 7 , 8 and 9 in Appendix .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Appendix B.4 , Ethics Statement .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Appendix B.6 .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Ethics Statement   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Appendix B.5.6845