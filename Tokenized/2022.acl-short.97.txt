  Xing Wu , Chaochen Gao , Meng Lin , Liangjun Zang , Songlin HuInstitute of Information Engineering , Chinese Academy of Sciences , Beijing , ChinaSchool of Cyber Security , University of Chinese Academy of Sciences , Beijing , ChinaKuaishou Technology , Beijing , China   { gaochaochen,linmeng,zangliangjun,husonglin}@iie.ac.cn   wuxing@kuaishou.com   Abstract   Before entering the neural network , a token is   generally converted to the corresponding one-   hot representation , which is a discrete distri-   bution of the vocabulary . Smoothed represen-   tation is the probability of candidate tokens   obtained from a pre - trained masked language   model , which can be seen as a more infor-   mative substitution to the one - hot representa-   tion . We propose an efficient data augmenta-   tion method , termed text smoothing , by con-   verting a sentence from its one - hot represen-   tation to a controllable smoothed representa-   tion . We evaluate text smoothing on different   benchmarks in a low - resource regime . Exper-   imental results show that text smoothing out-   performs various mainstream data augmenta-   tion methods by a substantial margin . More-   over , text smoothing can be combined with   those data augmentation methods to achieve   better performance . Our code are available at   https://github.com/caskcsg/TextSmoothing .   1 Introduction   Data augmentation is a widely used technique , es-   pecially in the low - resource regime . It increases   the size of the training data to alleviate overfit-   ting and improve the robustness of deep neural   networks . In the field of natural language process-   ing ( NLP ) , various data augmentation techniques   have been proposed . One most commonly used   method is to randomly select tokens in a sentence   and replace them with semantically similar tokens   to synthesize a new sentence ( Wei and Zou , 2019 ;   Kobayashi , 2018 ) . ( Kobayashi , 2018 ) proposes   contextual augmentation to predict the probabil-   ity distribution of replacement tokens by using the   LSTM language model and sampling the replace-   ment tokens according to the probability distribu-   tion . ( Wu et al . , 2019a , b ) uses BERT ’s ( Devlin   et al . , 2018 ) masked language modeling ( MLM)Figure 1 : The blue part demonstrates the use of text   smoothing data augmentation for downstream tasks , and   the red part directly uses the original input .   task to extend contextual augmentation by consid-   ering deep bi - directional context . ( Kumar et al . ,   2020 ) further propose to use different types of trans-   former based pre - trained models for conditional   data augmentation in the low - resource regime .   MLM takes masked sentences as input , and typ-   ically 15 % of the original tokens in the sentences   will be replaced by the [ MASK ] token . Before   entering MLM , each token in sentences needs to   be converted to its one - hot representation , a vec-   tor of the vocabulary size with only one position   is 1 while the rest positions are 0 . MLM outputs   the probability distribution of the vocabulary size   of each mask position . Through large - scale pre-   training , it is expected that the probability distri-   bution is as close as possible to the ground - truth   one - hot representation . Compared with the one-   hot representation , the probability distribution pre-   dicted by pre - trained MLM is a “ smoothed ” repre-   sentation , which can be seen as a set of candidate   tokens with different weights . Usually , most of the   weights are distributed on contextual - compatible   tokens . Multiplying the smooth representation by   the word embedding matrix can obtain a weighted   summation of the word embeddings of the candi-   date words , termed smoothed embedding , which   is more informative and context - rich than the one-871hot ’s embedding obtained through lookup opera-   tion . Therefore , the use of smoothed representation   instead of one - hot representation as the input of   the model can be seen as an efficient weighted data   augmentation method . To get the smoothed rep-   resentation of all the tokens of the entire sentence   with only one forward process in MLM , we do not   explicitly mask the input . Instead , we turn on the   dropout of MLM and dynamically randomly dis-   card a portion of the weight and hidden state at   each layer .   An unneglectable situation is that some tokens   appear more frequently than others in similar con-   texts during pre - training , which will cause the   model to have a preference for these tokens . This is   harmful for downstream tasks such as fine - grained   sentiment classification . For example , given “ The   quality of this shirt is average . " , the “ average " to-   ken is most relevant to the label . The smoothed   representation through the MLM at the position   of “ average " is shown in Figure 2 . Although the   probability of “ average " is the highest , more proba-   bilities are concentrated on tokens conflict with the   task label , such as “ high " , “ good " or “ poor ” . Such   a smoothed representation is hardly a good aug-   mented input for the task . To solve this problem ,   ( Wu et al . , 2019a ) proposed to train label embed-   ding to constraint MLM predict label compatible   tokens . However , under the condition of low re-   sources , it is not easy to have enough label data   to provide supervision . Inspired by the practical   data augmentation method mixup ( Zhang et al . ,   2017 ) in the computer vision field , we interpolate   the smoothed representation with the original one-   hot representation . Through interpolation , we can   enlarge the probability of the original token , and   the probabilities are still mostly distributed on the   context - compatible words , as shown in the figure   2 .   We combine the two stages as text smooth-   ing : obtaining a smooth representation through   MLM and interpolating to constrain the represen-   tation more controllable . To evaluate the effect   of text smoothing , we perform experiments with   low - resource settings on three classification bench-   marks . In all experiments , text smoothing achieves   better performance than other data augmentation   methods . Further , we are pleased to find that text   smoothing can be combined with other data aug-   mentation methods to improve the tasks further . To   the best of our knowledge , this is the first method to   improve a variety of mainstream data augmentation   methods .   2 Related Work   Various NLP data augmentation techniques have   been proposed and they are mainly divided into two   categories : one is to modify raw input directly , and   the other interferes with the embedding ( Miyato   et al . , 2016 ; Zhu et al . , 2019 ) . The most commonly   used method to modify the raw input is the token   replacement : randomly select tokens in a sentence   and replace them with semantically similar tokens   to synthesize a new sentence . ( Wei and Zou , 2019 )   directly uses the synonym table WordNet(Miller ,   1998 ) for replacement . ( Kobayashi , 2018 ) proposes   contextual augmentation to predict the probabil-   ity distribution of replacement tokens with two   causal language models . ( Wu et al . , 2019a ) extends   contextual augmentation with BERT ’s masked lan-   guage modeling ( MLM ) to consider bi - directional   context . ( Gao et al . , 2019 ) softly augments a ran-   domly chosen token in a sentence by replacing   its one - hot representation with the distribution of   the vocabulary provided by the causal language   model in machine translation . Unlike ( Gao et al . ,   2019 ) , we use MLM to generate smoothed repre-   sentation , which considers the deep bi - directional   context more adequately . And our method has bet-   ter parallelism , which can efficiently obtain the   smoothed representation of the entire sentence in   one forward process . Moreover , we propose to con-   strain smoothed representation more controllable   through interpolation for classification tasks .   3 Our Method   3.1 Smoothed Representation   We use BERT as a representative example of   MLM . Given a downstream task dataset , namely   D={t , p , s , l } , where Nis the number of872   instances , tis the one - hot encoding of a text ( a   single sentence or a sentence pair ) , pis the posi-   tional encoding of t , sis the segment encoding   oftandlis the label of this instance . We feed   the one - hot encoding t , positional encoding pas   well as the segment encoding sinto BERT , and   fetch the output of the last layer of the transformer   encoder in BERT , which is denoted as :   − →t = BERT ( t ) ( 1 )   where− →t∈ Ris a 2D dense vector   in shape of [ sequence_len , embedding_size ] . We   then multiply− →twith the word embedding ma-   trixW∈ Rin BERT , to get the   MLM prediction results , which is defined as :   MLM ( t ) = softmax ( − →tW ) ( 2 )   where each row in MLM ( t)is a probability distri-   bution over the token vocabulary , representing the   context - compatible token choices in that position   of the input text learned by pre - trained BERT .   3.2 Mixup Strategy   The mixup ( Zhang et al . , 2017 ) is defined as :   ˜x = λx+ ( 1−λ)x ( 3 )   ˜y = λy+ ( 1−λ)y ( 4 )   where ( x , y)and(x , y)are two feature - target   vectors drawn at random from the training data , and   λ∈[0,1 ] . In text smoothing , the one - hot repre-   sentation and smoothed representation are derived   from the same raw input , their lables are identical   and the interpolation operation will not change the   label . So the mixup operation can be simplified to :   et = λ·t+ ( 1−λ)·MLM ( t ) ( 5 )   where tis the one - hot representation , MLM ( t)is   the smoothed representation , etis the interpolated   representation and λis the balance hyperparameter   to control interpolation strength . In the downstream   tasks , we use interpolated representation instead of   the original one - hot representation as input .   4 Experiment   4.1 Baseline Approaches   EDA ( Wei and Zou , 2019 ) consists of four simple   operations : synonym replacement , random inser-   tion , random swap , and random deletion .   Back Translation ( Shleifer , 2019 ) translate a sen-   tence to a temporary language ( EN - DE ) and then   translate back the previously translated text into the   source language ( DE - EN ) .   CBERT ( Wu et al . , 2019a ) masks some tokens   and predicts their contextual substitutions with pre-   trained BERT .   BERTexpand , BERTprepend ( Kumar et al . ,   2020 ) conditions BERT by prepending class labels   to all examples of given class . “ expand " a the label   to model vocabulary , while “ prepend " without .   GPT2context ( Kumar et al . , 2020 ) provides a   prompt to the pre - trained GPT model and keep-   ing generating until the EOS token .   BARTword , BARTspan ( Kumar et al . , 2020 ) con-   ditions BART by prepending class labels to all ex-   amples of given class . BARTword masks a single   word while BARTspan masks a continuous chunk .   4.2 Experiment Setting   Our experiment strictly follows the settings in the   ( Kumar et al . , 2020 ) paper on three text classifica-   tion datasets downloaded from the links .   SST-2 ( Socher et al . , 2013 ) is a movie reviews sen-   timent classification task with two labels .   SNIPS ( Coucke et al . , 2018 ) is a task of over   16,000 crowd - sourced queries distributed among 7   user intents of various complexity.873   TREC ( Li and Roth , 2002 ) contains six question   types collected from 4,500 English questions .   We randomly subsample 10 examples per class   for each experiment for both training and develop-   ment set to simulate a low - resource regime . Data   statistics of the three datasets are shown in Table   1 . Following ( Kumar et al . , 2020 ) , we replace nu-   meric class labels with their text versions .   We first compare the effects of text smoothing   and baselines data augmentation methods on dif-   ferent datasets in a low - resource regime . Then we   further explore the effect of combining text smooth-   ing with each baseline method . Considering that   the amount of data increases to 2 times after com-   bination , we expand the data used in the baseline   experiments to the same amount for the fairness of   comparison . All experiments are repeated 15 times   to account for stochasticity and results are reported   as Mean ( STD ) accuracy on the full test set .   4.3 Experimental Results   As shown in Table2 , text smoothing brings the   largest improvement to the model on the three   datasets compared with other data augmenta-   tion methods . The previously best method is   BARTspan , which is exceeded by Text smoothing   with 1.17 % in average . Moreover , we are pleased to find that text   smoothing can be well combined with various data   augmentation methods , further improving the base-   line data augmentation methods . As shown in Ta-   ble3 , text smoothing can bring significant improve-   ments of 5.98 % , 2.79 % , 2.39 % , 2.92 % , 2.17 % ,   6.48 % , 3.21 % , 3.03 % to EDA , BackTrans , CBERT ,   BERTexpand , BERTprepend , GPT2context , BART-   word , and BARTspan , respectively . To the best of   our knowledge , this is the first method to improve a   variety of mainstream data augmentation methods .   5 Conclusoins   This article proposes text smoothing , an effective   data augmentation method , by converting sentences   from their one - hot representations to smoothing   representations . In the case of a low data regime ,   text smoothing is significantly better than various   data augmentation methods . Furthermore , text   smoothing can further be combined with various   data augmentation methods to obtain better perfor-   mance .   References874875