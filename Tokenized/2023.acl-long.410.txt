  Myles Foley , Ambrish Rawat , Taesung Lee ,   Yufang Hou , Gabriele Picco , Giulio ZizzoImperial College London , IBM Research   m.foley20@imperial.ac.uk   { ambrish.rawat , yhou}@ie.ibm.com   { taesung.lee , gabriele.picco , giulio.zizzo2}@ibm.com   Abstract   The wide applicability and adaptability of gen-   erative large language models ( LLMs ) has en-   abled their rapid adoption . While the pre-   trained models can perform many tasks , such   models are often ﬁne - tuned to improve their   performance on various downstream applica-   tions . However , this leads to issues over viola-   tion of model licenses , model theft , and copy-   right infringement . Moreover , recent advances   show that generative technology is capable of   producing harmful content which exacerbates   the problems of accountability within model   supply chains . Thus , we need a method to in-   vestigate how a model was trained or a piece of   text was generated and what their pre - trained   base model was . In this paper we take the ﬁrst   step to address this open problem by tracing   back the origin of a given ﬁne - tuned LLM to its   corresponding pre - trained base model . We con-   sider different knowledge levels and attribution   strategies , and ﬁnd that we can correctly trace   back 8 out of the 10 ﬁne tuned models with our   best method .   1 Introduction   Recent advancements in pre - trained large language   models ( LLMs ) have enabled the generation of   high quality texts that humans have difﬁculty iden-   tifying as machine generated ( Wahle et al . , 2022 ) .   While these pre - trained models can perform many   tasks in the zero - shot or few - shot settings ( Brown   et al . , 2020 ; Schick and Schütze , 2021 ) , such mod-   els are often ﬁne - tuned to improve their perfor-   mance on various downstream applications ( Peters   et al . , 2019 ; Pfeiffer et al . , 2020 ) . As of May 2023 ,   there are more than 209,000 models hosted on Hug-   gingfaceand more than 12,000 of them belong to   the “ text generation ” category . Many generation   models are ﬁne - tuned from the open - access pre-   trained base models such as XLNet ( Yang et al . , 2019 ) , BART ( Lewis et al . , 2020 ) , or GPT - J ( Wang   and Komatsuzaki , 2021 ) whose training typically   requires signiﬁcant computational resources .   While the proliferation of text generation mod-   els has led to the performance improvement for   a wide range of downstream applications such as   text summarization and dialogue systems , it has   also been repeatedly shown that these pre - trained   or ﬁne - tuned LLMs can facilitate the creation and   dissemination of misinformation at scale ( Wei-   dinger et al . , 2021 ) , and the manipulation of public   opinion through false “ majority opinions ” ( Mann ,   2021 ) . In response , laws like the EU ’s Digital   Services Act ( DSA)aim at tackling these issues   by enforcing procedural accountability and trans-   parency for responsible use of AI - based technolo-   gies . These growing demands for AI forensics re-   quire the development of methods for establishing   model ownership , protecting intellectual property ,   and analyzing the accountability of any violations .   In this work , we systematically investigate LLM   attribution , a novel task recently proposed at the   ﬁrst “ Machine Learning Model Attribution Chal-   lenge ( MLMAC ) ” , which aims to link an arbitrary   ﬁne - tuned LLM to its pre - trained base model using   information such as generated responses from the   models . Through LLM attribution , regulatory bod-   ies can trace instances of intellectual property theft   or inﬂuence campaigns back to the base model .   However , determining attribution for ﬁne - tuned   LLMs can be challenging as base models often   have similar architectures and overlapping training   data . For instance , TP(Gao et al . , 2020a ) ,   a large data set that consists of 22 smaller , high-   quality datasets , with a total size of 825 GB , was in-   cluded into the training data for both GPT - J ( Wang   and Komatsuzaki , 2021 ) and OPT ( Zhang et al . ,   2022 ) .74237424access to the models in BandFand the amount of   resources available for developing the method . In   general , we assume that the developer of an attri-   bution system can only query the LLMs as a black   box to obtain the generated responses , and has lim-   ited access to models in F. We speculate this to   be true for real - world settings where the producers   of pre - trained base models , maintainers of model   zoos , or an external auditor are incentivised to de-   velop such attribution systems . In such scenarios ,   they may only have limited access to the API of   ﬁne - tuned models which will typically be owned   by a third - party . Other constraints may arise from   the amount of resources available for developing at-   tribution systems . For instance , an external auditor   may not have the domain expertise or computation   resources to beneﬁt from the insights from other   ﬁne - tuning pipelines . Similarly , the developer is   assumed to have no knowledge of the ﬁne - tuning   datasets and approaches used to obtain the models   inF , as in these cases attribution may be easily   achieved by replicating the setup locally and com-   paring the obtained models with those in F. In   addition to these assumptions , we consider the fol-   lowing two knowledge levels available with the   developer of an attribution system .   •Universal knowledge K : This allows the   developer access to universal knowledge   about models in B. This allows the analysis   by a human expert , as well as computing the   perplexity of the input . Moreover , the devel-   oper can build an additional set of ﬁne - tuned   models A , or even the capability to train such   models . This enables building a supervised   attributor using Aas a training dataset .   •Restricted knowledge K : We do not have   access to A , and can only query the models in   Bas a black box to get the responses .   4 Attribution Methods   We approach the LLM attribution problem as a   classiﬁcation task . Essentially , LLM attribution re-   quires identifying the certain robust or latent char-   acteristics of a pre - trained base model within the   given ﬁne - tuned model . The ﬁne - tuned model may   retain unique aspects in the pre - training data like   events and vocabulary of a speciﬁc time period , or   the ﬂuency of the base model in a certain domain .   In particular , as shown in Figure 2we build   a classiﬁer htesting a response for each pre-   trained base model mto decide if a given ﬁne-   tuned model mretains the characteristics of m ,   following the one - vs - rest ( mor others ) scheme .   Then , we aggregate the result to pick the top-1 base   model with the majority voting method . In other   words , we take msuch that / summationtexth(m(p ) )   is maximized , where Pis a set of prompts .   The task can be further broken down into two   steps for each base model mand its classiﬁer   hincluding ( 1 ) characterizing the target base   model mand representing the input to the classi-   ﬁer ( Section 4.1.1 ) , ( 2 ) selecting the prompts ( Sec-   tion4.1.2 ) , and ( 3 ) designing the classiﬁer ( Sec-   tion4.2 ) .   4.1 Model Characterization and Input   Representation   In this step , we characterize an LLM ( ﬁne - tuned or   base model ) , and prepare the input to the classiﬁer   h. One piece of evidence of attribution lies in ex-   ploiting the artefacts of a pre - trained LLM that are   expected to persist through the ﬁne - tuning process   and are inherited by their ﬁne - tuned counterparts .   For instance , a distinctive feature of RoBERTa ( Liu   et al . , 2019 ) is the sequence length limit of 512   which is often inherited by its ﬁne - tuned versions .   The task characteristics and associated training data   may also help distinguish different LLMs . For ex-   ample , LLMs trained for speciﬁc tasks like chat   bots or code generation will have characteristically   different output spaces . They may also have unique   aspects in their training data like a speciﬁc lan-   guage or markers such as data collected over spe-   ciﬁc time period .   While feature engineering can extract a usable   set of features , it is prone to bias , and less adaptable ,   and it also requires deep knowledge about B. Thus ,   we leverage the embeddings of the prompts and   responses to learn and exploit such knowledge.74254.1.1 Input Representation   Our goal is to train a classiﬁer to capture the corre-   lations between an arbitrary response and the base   model m. For example , with a prompt p , this   could capture the relationship between a response   m(p)andm . Similarly , we can capture the rela-   tionship between a response m(p)andmwhere   mis obtained by ﬁne - tuning m. Assuming that   such correlations are preserved in a base model and   ﬁne - tuned model pair , we use it to determine the   attribution of a ﬁne - tuned LLM .   Given a set of prompts p , . . . , p , there are   multiple ways to prepare them for the classiﬁer .   We can apply the target base model , or ﬁne - tuned   model to get the responses , and concatenate the   prompt and its response . Speciﬁcally , we list the   input representations we consider as follows :   • Base model only ( I ): “ pm(p ) ”   • Fine - tuned model only ( I ): “ pm(p ) ”   •Base model + ﬁne - tuned model ( I ): “ p   m(p)<SEP > pm(p ) ”   •Separate embeddings for base model and ﬁne-   tuned model .   We embed these concatenated sentences   using BERT computed by a best -base -   multilingual -cased modelexcept for   the last approach that embeds the components   separately for margin - based classiﬁer TripletNet   described in Section 4.2 . Note that all reference to   a ﬁne - tuned model mduring training are actually   sampled from another set Aof ﬁne - tuned models   under Kassumption as we assume only sparse   access to m. Also , Itakes the responses from   mduring prediction to test if the responses share   the same characteristics that this classiﬁer learned   about m.   4.1.2 Prompt Selection   While many corpora to pre - train LLMs provide   prompts , they might not be all useful to predict   the base model . Thus , we aim to test and se-   lect prompts with more distinctive outcome . Our   prompt selection strategy is driven to help best   characterise base models . We ﬁrst collect the list   of datasets used in training each base model , identi-   fying unique aspects of datasets that can help iden-   tify a base model . Intuitively , one might expectsuch unique prompts or ‘ edge cases ’ to bring out   the distinctive aspects in the subsequent ﬁne - tuned   models . Speciﬁcally , we ﬁrst identify the unique   categories of prompts ( e.g. different languages )   present in different datasets and sample from this   set .   More speciﬁcally , we consider three approaches :   a small set ( P1 ) ofedge cases that are distinct to   each corpus , a naive collection ( P2 ) of prompts ,   and reinforcement learning to select a subset ( P3 )   from the edge cases .   While the naive collection of the 10,000 prompts   from ThePile corpus and manually selecting a set   of prompts unique to each training dataset is clear ,   we can also use reinforcement learning to optimize   the selection using the classiﬁcation result . More   speciﬁcally , we train an agent for each hthat can   supply prompts for attribution inference . During   the training episodes , the agent is rewarded for   prompts whose responses lead to correct attribution .   The reinforcement learning setup for this problem   is deﬁnes as follows :   •State . A feature space consisting of the   classiﬁcation of the prompt , and an embed-   ding of the prompt response computed by   best - base - multilingual - cased .   •Action . Selecting one of the prompts from   P1 .   •Reward . Using a sparse reward function we   reward ( +1 ) for correct classiﬁcation and pe-   nalise ( -10 ) for incorrect classiﬁcation .   •Episode . 20 possible actions .   At the start of each episode we are able to randomly   select one of the models that the classiﬁer was   trained on , thus the RL agent learns to generalise   to a variety of different models . We implement the   RL agent using the Proximal Policy Optimisation   ( PPO ) method ( Schulman et al . , 2017 ) .   We can use these collected prompts in a few   different ways . A simplistic approach is using each   setP1,P2orP3individually . Another approach   P1+P2 trains the classiﬁer with P2 , and then ﬁne-   tune with P1to leverage both of them ( P3is already   a subset of P2 ) and we ﬁnd this is promising in our   experiments . See Appendix Dfor details of the   prompts used from TPfor this combination   approach.74264.2 Classiﬁer Architecture   We consider a one vs rest setup where for each base   model mwe train a binary classiﬁer h : Σ→   { 0,1}which takes as input a response s∈Σ , op-   tionally with additional tokens , and predicts a score   that reﬂects its association to the based model m.   Single embeddings prepared in Section 4.1.1 can   be straightforwardly used in a simple classiﬁer . We   ﬁne - tune the BERT model used for the embedding   to make the binary prediction with cross - entropy   loss . Given the one - vs - rest approach the positive   samples for an hare repurposed as negative ones   for the rest of the classiﬁers hform∈B\{m } .   The best average score thus obtained is used to es-   tablish the attribution for m.   We also consider TripletNet ( Wei et al . , 2021 )   based classiﬁers that use a margin - based loss func-   tion using the separate embeddings of the base   and ﬁne - tuned model responses . The TripletNet   is able to make predictions by taking in a single   sentence , computing the output embedding , and   ﬁnding the closest embedding from the training set   and using the label of the training sentence as a   prediction . The cosine distance between the anchor   input , positive example , and negative example are   then computed as the loss . We adopt the margin   parameter 0.4 from the original paper ( Wei et al . ,   2021 ) .   5 Experiments   5.1 Experiment Setup   For training the attribution models hwe make   use of popular text corpora including : GitHub ,   The BigScience ROOTS Corpus ( Laurençon et al . ,   2022 ) , CC-100 ( Conneau et al . , 2020 ) , Reddit   ( Hamilton et al . , 2017 ) , and TP(Gao et al . ,   2020b ) .   We also use a variety of prompt sizes for attribu-   tion ( 150 to 10,000 ) , and datasets ( IMDB Reviews   ( Maas et al . , 2011 ) , GLUE ( Wang et al . , 2018 ) ,   Tajik OSCAR ( Abadji et al . , 2022 ) , and Amazon   Multilingual ( Keung et al . , 2020 ) .   To provide a wide didactic range of models for   our approaches we utilise 10 pre - trained LLMs   to create Band corresponding ﬁne - tuned mod-   els ( Table 1 ): Bloom ( Scao and et al . , 2022 ) ,   OPT ( Zhang et al . , 2022 ) , DialoGPT ( Zhang et al . ,   2020 ) , DistilGPT2 ( Sanh et al . , 2020 ) , GPT2   ( Radford et al . , 2019 ) , GPT2 - XL ( Radford et al . ,   2019 ) , GPT - NEO ( Black et al . , 2021 ) , CodeGen   ( Nijkamp et al . , 2023 ) , XLNET , MultiLingual-   MiniLM ( Wang et al . , 2021 ) . These models provide   different architectures , parameter sizes , and tasks   to offer a variety of model behaviors .   We consider a one - to - one mapping from BtoF   ( andA ) , thus FandAcontain ten models each . We   utilise open - source models that are implemented   in the Huggingface library to form the sets of F   andA. We select AandFsuch that the ﬁne - tuning   dataset , and model conﬁguration are known to us ,   of these we select the most popular by number of   downloads . We provide further details of these in   Appendix B.   We take the top-1 result for each mas men-   tioned in Section 4and check its correctness . We   use F1 and ROC curves as additional metrics .   These are calculated using prompt - level attribu-   tion calculated per m(as in Figure 8) , and we use   an average per h(as in Figure 3 ) . Each of the   attributors hdescribed is ran once to determine   the attribution of mtom . Training is conducted   using a single NVIDIA A100 GPU .   5.2 Compared Approaches   We consider different conﬁgurations for BERT clas-   siﬁers based on the input representations I , Ior   I , and the prompts used P1,P2,P3orP1+P2   described in Section 4.1.1 .   We also consider the margin classiﬁer TripleNet   ( Section 4.2 ) , and the following heuristic ap-   proaches .   •Perplexity : A measure of how conﬁdent a   model is at making predictions , this can be7427leveraged for measuring attribution by com-   puting the perplexity of mrelative to the re-   sponse of mto prompt p.   •Heuristic Decision Tree ( HDT ) : Using K   we can use knowledge of Bto create a series   of discriminative heuristics to categorise F   as used by the winning solution to the ﬁrst   MLMAC .   •Exact Match : Attribute responses mtom   when both models respond the same to a   prompt . Using the argmax of these attribu-   tions to attribute mtom .   For detailed descriptions of the heuristic ap-   proaches , please refer to Appendix A.   5.3 Attribution Accuracy   Here , we examine the attribution abilities of the   compared approaches shown in Table 2 . Under K   conditions the baselines of Perplexity and HDT are   only able to correctly attribute 1 and 5 models re-   spectively . Perplexity fails to capture the subtly of   attribution , as repetitive responses lead to lower per-   plexity and so incorrect attribution . The HDT par-   ticularly fails to account for overlap in pre - training   and ﬁne - tuning . For instance , DialoGPT - Large and   m(ﬁne - tuned version of distilgpt2 ) respond in   similar short sentences that leads to incorrect attri-   bution . The TripletNet baseline performs poorly ,   only correctly attributing 3 of the models . Both   BERT based attributors are able to attribute more   models correctly in comparison to the baselines .   Examining the models at Kshows similar per-   formance . The exact match correctly attributes   5 models and BERT+ Iidentiﬁes 6 models .   BERT+ I+P1 + P2attributor is the most success-   ful by correctly attributing 8 models . Note that this   model is the most expensive to train as we have to   query a large number of prompts .   We compare the ROC curves for BERT based   attributor deﬁned under each Kin Figure 3 . We   provide plots of hin each variant in Appendix C.   It is interesting to note that the models under K   have shallower curves than their Kcounterparts ,   yet these Kmodels lead to the same or higher   number of correct attributions . This is likely due   to the ‘ noise ’ that gets added to responses of A   from their separate ﬁne - tuning task , T. This noise   moves the responses of mfurther from m(and   by extent m ) . As such responses from mare   closer to mthanm . This makes the attributors   predict more negative samples correctly under K   as there is greater disparity in response between   mandm , leading to a higher AUC ; but also to   miss - attribution of mat inference . Hence , it is   unsurprising that the pretrained Khas the low-   est AUC of any model , yet it leads to the highest   attribution accuracy in Table 2as it is trained on   responses of mwhich is closer in the latent space   to responses of mthanm .   Lesson Learned : Even under reduced knowl-   edge level , pre - training was found to be the   factor that contributed to the highest attribu-   tion performance .   5.4 Effects of Prompt usage   The number of prompts available to an attributor for   classiﬁcation can have an inﬂuence on the attribu-   tion performance : we hypothesize that increasing   the number of prompts used results in a clearer   signal as to the ﬁnetuned to base model attribution .   We train BERT attributors under the Kcon-7428   dition , as the Kpretrained model performed the   strongest . For these experiments we do not use RL   prompt selection .   The results of this experiment are shown in Fig-   ure4 . By increasing the number of prompts that   a classiﬁer is able to use for classiﬁcation , we see   that there is an improvement in the AUC , with di-   minishing returns from 6,000 prompts onward .   Increasing the number of prompts improves the   AUC , yet does not lead to direct improvement in   terms of the attribution accuracy as shown in Ta-   ble3 . In fact , increasing the number of prompts   used for classiﬁcation leads to a highly variable   performance . None of the models that directly use   these prompts ( 150 - 10 K prompts from the pile )   are able to improve or even match that of the pre-   trained model using 150 prompts from Table 1 .   Lesson Learned : Increasing the number of   prompts for attribution does not lead to reli-   able improvements in the number of models   correctly attributed .   5.5 Effects of pretraining attributors   We next aim to investigate how the size of the pre-   training data effects the performance of the attri-   bution , as while using increasingly large data for   direct attribution may not improve performance ,   Section 5.3shows that using it as pretraining data   does improve attribution .   To this end each model discussed in Section 5.4   is ﬁnetuned under K , varying the size of pretrain-   ing data from 150 prompt responses to 10,000 .   We report the results of the experiment in Fig-   ure5 . In Figure 5awe see that the ﬁnetuned models   are able to improve over the equivalent models in   Figure 4 . Yet they do not improve on the AUC of   models trained under Kconditions .   We see from Figure 5bthat increasing the num-   ber of prompts minimally improves the precision   and recall of attribution , with little correlation be-   tween number of prompts , even of a varied set like   TP . Whilst these pretrained-ﬁnetuned attrib-   utors are able to improve on the precision of the   attributor using manual selected prompts , however   they are unable to improve on the recall .   What is most important for this task , however , is   the ability of attribution , hence we also determine   the model attributions for each model in Table 4 .   The models that have been pretrained on a larger   number are able to outperform the Kmodel of   Section 5.3attributing 8 models correctly in the the   models pretrained on 4k and 6k prompts .   Lesson Learned : Pretraining attributors is   vital to improve the attribution performance .   However , this has to diminishing returns in   terms of correct attributions and AUC .   5.6 Effects of Finetuning on Attribution   The type and duration of the ﬁnetuning conducted   on a base model Bcan effect attribution perfor-   mance . To investigate this we use of two base   models : distilgpt2 and Multilingual - MiniLM and   ﬁnetune them using three datasets : IMDB ( Maas   et al . , 2011 ) , GLUE ( Wang et al . , 2018 ) , Amazon7429   reviews Multilingual ( Keung et al . , 2020 ) , and the   Tajik language subset of OSCAR ( Abadji et al . ,   2022 ) .   Using such datasets more closely models the re-   alistic attack scenario where common pre - training   prompt sets are used in an attempt to determine at-   tribution , and ﬁne - tuning datasets are often propri-   etary and/or unique to the application . Conducting   experiments in this scenario in a controlled setting   allows us to study the effect of ﬁnetuning on attri-   bution in terms of ( a ) number of epoch and ( b ) size   of dataset .   Effect of Finetuning Epochs : Firstly , we study   the effect of the number of ﬁnetuning epochs has   on attribution . Figure 6shows the F1 score of the   MLMini and distilgpt2 attributors when trying to   attribute the ﬁnetuned base models .   The MLMini attributor is greatly affected ini-   tially by MLMini being ﬁnetuned on IMDB , how - ever as with the model ﬁnetuned on Amazon re-   views there is an increase in attribution perfor-   mance with increasing ﬁnetuning epochs . Con-   versely , the MLMini model ﬁnetuned on GLUE   MNLI had minimal change in performance only   with anomalous increased F1 score at epoch 6 .   However , when trying to attribute MLMINI ﬁne-   tuned with the Tajik subset of OSCAR we see that   the F1 score is signiﬁcantly worse . We speculate   that AMAZON and IMDB datasets are similar to   the pretraining dataset of MLMini ( CC-100 ) and   that the AMAZON reviews , with its 6 languages ,   are the most similar to this . In fact , the CC-100   is likely to have an overlap in the data distribu-   tion of all three of these datasets as all are openly   available . As there is no Tajik in CC-100 it is out-   of - distribution ( OOD ) of MLMINI ’s pretraining   dataset , which leads to the poor performance in   attribution .   With the attributor for distilgpt2 there is poor   performance in all datasets regardless of the num-   ber of epochs . This follows due to the ﬁnetuning   datasets being OOD relative the the pretraining   data of distilgpt2 which used the OpenWebTextCor-   pus . As OpenWebTextCorpus is mostly in English ,   ﬁnetuning in other languages such as those in the   AMAZON dataset , makes attribution harder .   Lesson Learned : The attribution performance   is dominated by the similarity of the ﬁne-   tuning dataset to the pre - training dataset ,   rather than the amount of ﬁne - tuning con-   ducted .   Effects of Dataset Size : In addition to the num-   ber of ﬁnetuning epochs we consider the overall7430   sizeof the ﬁnetuning set on attribution . We report   the results of using a ﬁxed 10 epochs and varying   the ﬁnetuning dataset size in Figure 7 . We can   see similar effects as in Figure 6 , that the OOD   datasets for Distilgpt2 lead to poor F1 scores , and   consequently , poor attribution results .   For MLMINI we see similar performance on   IMDB and AMAZON ( two of the in - distibution   datasets ) with an increased F1 as the dataset size   increases . When ﬁnetuning on OSCAR and GLUE   the F1 score shows a minimal correlation with   dataset size . This again follows from Figure 6 .   OSCAR is OOD for MLMINI , which makes attri-   bution signiﬁcantly harder . Similarly GLUE offers   the most varied dataset making attribution harder   and giving lower F1 .   Lesson Learned : Training on a richer dataset   broadly improves results if it is within distri-   bution .   Effects of Dataset : Across Figures 6and7we   see the effect of different ﬁnetuning datasets on the   ability to attribute to base models .   We can observe the effect of the ﬁnetuning   datasets on the ability to attribute to base models in   Figures 6and7 . These ﬁgures show the distribution   of the dataset greatly affects attribution . Finetuning   datasets that are completely out of distribution in   relation to the original pre - training dataset severely   impact attribution performance . This is particularly   apparent in MLMINI where ﬁnetuning on OSCAR   leads to poor attribution performance in Figure 6   and7 .   Both base models ﬁnetuned with GLUE also   make attribution harder . We reason that this is dueto the broad range of prompts that are not typical   of a ﬁnetuning dataset . This leads the model to   produce generic responses to the targeted prompts   used for attribution .   Lesson Learned : The most signiﬁcant impact   on attribution is the distribution and variety   of the ﬁnetuing dataset .   6 Conclusion   In this work we have taken initial steps in the LLM   attribution problem . We study LLM attribution in   KandKsettings which limit access to Band   Fto different levels . We argue this prevents trivial   solutions in white - box settings , and provides an   interesting and realistic study of LLM attribution .   We have considered a variety of different LLMs   that are trained on different datasets , and for dif-   ferent purposes . We postulate that the 10 differ-   ent LLMs provide a didactic range of models for   LLM attribution . In our experiments , we have used   pre - existing LLMs that have been ﬁne - tuned by the   open - source community to demonstrate the applica-   bility of our methodology . To mitigate the potential   for bias this causes , we have tried out best to ensure   the ﬁne - tuning task and dataset of such models is   known . In addition , we ﬁne - tune a subset of these   models in an ablation study , which demonstrates   the effect that such ﬁne - tuning has on LLM attri-   bution in a controlled environment . Our ablation   study also studies the effect that OOD ﬁne - tuning   datasets have on attribution . This mitigates the ef-   fect of only ﬁne - tuning within distribution ( of the   pre - training data ) .   Overall , our work contributes to the growing   understanding of LLM attribution , laying the foun-   dation for future advancements and developments   in this domain.7431Limitations   We have considered a variety of different LLMs   in order to study attribution . However we have   only considered a small sample of the different   LLM architectures and training strategies . This   has been with a view to using a small but diverse   set of LLMs . Of these 10 base models , we tested   our approach to attribution on a controlled set of   ﬁne - tuned models . While a study that considers a   wider variety and larger scale of ﬁne - tuned models   would be beneﬁcial to the problem of attribution ,   the computation resources limited our study .   Furthermore , in our assumptions in this work   we consider that there is a one - to - one mapping be-   tween mandm . However , this is not necessarily   the case . There could be an m - to - nmapping and   also a model may be present in one set , but not the   other .   We believe there is rich space for further research   in this area that can address these limitations , and   further develop the problem of attribution .   Ethics Statement   In the discussion we have highlighted how the tech-   niques for attributing ﬁne - tuned models to their   pre - trained large language models can be used as a   tool to mitigate issues such as violation of model   licenses , model theft , and copyright infringement ,   but this is only a subset of the issues related to   authorship attribution . The increasing quality and   credibility of LLM generated text has recently high-   lighted ethical issues such as plagiarismor the   banning of users for submitting AI generated re-   sponses to answer questions . Even within the sci-   entiﬁc community discussions are arising related   to topics such as the authorship of papers or codes ,   who owns what is it generated ? Many AI con-   ferences have banned the submission of entirely   self - generated scientiﬁc papers .   These are some examples of controversial sit-   uations , but the use of AI - generated content has   ethical implications in several domains that depend   on the speciﬁc context and application . It is there-   fore crucial , as a ﬁrst step to tackle these ethical   issues , to ensure that any AI - generated contentsare clearly labeled as such and are not presented as   original work without proper attribution ( whether   it ’s a person or a base model ) .   Acknowledgements   This work was supported by European Union ’s   Horizon 2020 research and innovation programme   under grant number 951911 – AI4Media.7432References743374347435   A Heuristic Approaches   A.1 Perplexity   Using the response of Fwe can calculate the per-   plexity of Brelative to F. This can then be used   as a measure of how conﬁdent Bis in predicting   F , where a lower perplexity would indicate higher   conﬁdence and attribution . In our initial experi-   ments , we found this to be loose approximation of   similarity between models in BandF. Moreover ,   this approach assumed stronger access which is   typically not available in real - world settings as we   discussed in Section 3 .   Perplexity is a measure of how well a model is   able to predict a sample . It has previously been   used in analogous settings for extracting training   data from language models ( Carlini et al . , 2021 ;   Mireshghallah et al . , 2022 ) to determine if a model   is conﬁdent in its prediction of a sample . It is pos-   sible to leverage this for the purpose of attributing   FtoB. By collecting responses of Fto prompts   we can calculate the perplexity of Brelative to F.   Thus we can take the perplexity score as a measure   of how conﬁdent Bis in predicting the response of   F , we would expect lower perplexity to be an in-   dication of higher conﬁdence and therefore higher   chances of attribution .   A.2 Heuristic Decision Tree   When it comes to generalisation , many LLMs share   an equal footing owing to the massive size and in-   tensive training backing their capabilities . How-   ever , when examined closely there are distinctive   features that set them apart which can be detected   via static or dynamic inspection of the model . For   instance , LLMs with a larger number of parameters   tend to take longer for inference . Similarly , length   of response varies across LLMs , and some are   prone to repetition ( such as XLNET ( Mohamad Za-   mani et al . , 2022 ) ) . The task characteristics and   associated training data may also help distinguishdifferent LLMs . For example , LLMs trained for   speciﬁc tasks like chat bots or code generation   will have characteristically different output spaces .   They may also have unique aspects in their training   data like a speciﬁc language or markers such as   data collected over speciﬁc time period . Much like   watermarking , these can be used to craft prompts   that can help reveal these unique artefacts .   While in principle many of these heuristics can   be used for attribution , the practical development   of such systems faces a range of challenges . First ,   these properties may not be preserved across the   ﬁne tuning process and therefore provide no mean-   ingful insight for attribution . Second , these heuris-   tics require a high level of expertise and knowledge   which may not always be available . An external   auditor working with the restricted knowledge of   Kmay not be able to develop such solutions .   Third , many of the properties of models in Fcan   be easily obfuscated by the exposed API . For exam-   ple it is fairly easy to normalise response times or   post - process the responses to account for repetition .   Moreover , an API may be simultaneously backed   by multiple different models which would make the   attribution even more challenging . Finally , LLMs   often have overlapping datasets which can dilute   many of the subtleties underlying these heuristics .   This limits the applicability and scalability of such   approaches for larger collections of BandF.   B Fine - tuned model Details   Here we provide details of the ﬁne - tuned LLMs we   use in sets AandF. Each of the LLMs is an open   source implementation hosted on the Huggingface ,   we provide the link to the ﬁne - tuned model . In   Table 5we show set Fas FT models 0 - 9 inclusive ,   and set Afrom 10 - 19 inclusive . For each model   we also provide the dataset used to ﬁne - tune each   of the LLMs .   C AUC Curves   We provide the ﬁnegrained plots of how each indi-   vidual hdid in each experiment . Figure 8shows   the results from the experiment that measures the   attribution accuracy under different Kas discussed   in Section 5.3 . Figure 9details the effect of using   a different number of prompts for attribution under   K , as discussed in Section 5.4 . Finally Figure 10   shows the effect of varying the number of prompts   for pretaining h(Section 5.5).7436743774387439   D The Pile subset   We make use of a 10,000 prompt subset of The   Pile ( Gao et al . , 2020b ) , in Table 6we report the   distrubtion of the smaller datasets present in The   Pile.7440ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   7   /squareA2 . Did you discuss any potential risks of your work ?   7   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   5   /squareB1 . Did you cite the creators of artifacts you used ?   5 , B   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No , the artifacts used all have open source licences , and are freely avaliable at their respective   citations .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   We make use of data used to train/ﬁne - tune language models , this is consistent with their intended   use . We do n’t discuss this use explicity relative to the intended us , but do discuss the use of these   artifacts for ﬁnetuning purposes in Section 5 , B.   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No as none of the data used contains information that names or uniquely identiﬁes individual people   or offensive content .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   B , D   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   B7441C / squareDid you run computational experiments ?   5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   5   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   We utilised the BERT using default parameter values used in the original paper and then used this   model to develop a classiﬁer .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   5 , B   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Left blank.7442