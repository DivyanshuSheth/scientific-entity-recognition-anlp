  Gary Ang   Singapore Management University   gary.ang.2019@phdcs.smu.edu.sgEe - Peng Lim   Singapore Management University   eplim@smu.edu.sg   Abstract   Most works on financial forecasting use infor-   mation directly associated with individual com-   panies ( e.g. , stock prices , news on the com-   pany ) to predict stock returns for trading . We   refer to such company - specific information as   local information . Stock returns may also be   influenced by global information ( e.g. , news   on the economy in general ) , and inter - company   relationships . Capturing such diverse informa-   tion is challenging due to the low signal - to-   noise ratios , different time - scales , sparsity and   distributions of global and local information   from different modalities . In this paper , we pro-   pose a model that captures both global and lo-   cal multimodal information for investment and   risk management - related forecasting tasks . Our   proposed Guided Attention Multimodal Multi-   task Network ( GAME ) model addresses these   challenges by using novel attention modules   to guide learning with global and local infor-   mation from different modalities and dynamic   inter - company relationship networks . Our ex-   tensive experiments show that GAME outper-   forms other state - of - the - art models in several   forecasting tasks and important real - world ap-   plication case studies .   1 Introduction   Forecasting stock prices or returns is an important   task in trading . Such forecasts can also be used   in investment and risk management applications   such as portfolio allocation and risk forecasting .   Stock returns in financial markets are influenced by   large volumes of textual information from diverse   sources , e.g. , news , blogs , social media . Such tex-   tual information can be directly associated with a   specific company ( local ) , e.g , a company ’s CEO   stepping down ; or relevant to multiple companies   ( global ) , e.g. , disruptions in supply chains due   to export curbs in key countries , airline industry   bankruptcies . In this paper , articles with company   tags are treated as local information . All articles aretreated as global information as any article could   be potentially relevant to a company .   Direct and indirect relationships between com-   panies also serve as channels through which the   effects of information from both global and local   textual and numerical information propagate and in-   fluence stock returns , e.g. , a disruption in company   A could affect all its suppliers ; a scandal involv-   ing company A ’s CEO may affect company B if   the CEO is a member of company B ’s board . We   illustrate such diverse information and effects in   Figure 1 .   Apart from low signal - to - noise ratios in finan-   cial time - series due to market forces , there are other   challenges in modeling such diverse information .   Time scales of information from different modal-   ities are of different granularity , e.g. , numerical   financial information may be available daily , while   publication of financial text happens at irregular   times . Companies ’ local financial news are typi-   cally sparse and long - tailed , e.g. , a company may   not be in the news for an extended period of time ,   but suddenly becomes the focus of many news re-   ports in a short period due to a scandal . Local   textual information may also be noisy with regards   to its relevance to the company ’s stock returns , e.g. ,   a news article on a company ’s HR practices may   have little effect on its stock returns , whereas a   news article on a sector ’s outlook can have a sig-   nificant effect on the company ’s stock returns even   without any mention of the company .   More research on financial forecasting is re-   quired to address such challenges . Most exist-   ing works model financial information of a single   modality ( Ding et al . , 2015 ; Ziniu et al . , 2018 ; Du   and Tanaka - Ishii , 2020 ; Sawhney et al . , 2021b ) ,   and do not model the effects of inter - company re-   lationships . Some works ( Feng et al . , 2019 ; Xu   et al . , 2021 ; Sawhney et al . , 2021a ) model both uni-   modal financial information and the effects of inter-   company relationships . There are however few6313   works capturing multimodal financial information   and inter - company relationships ( Ang and Ee - Peng ,   2021 ; Sawhney et al . , 2020b , a ) . Ang and Ee - Peng   ( 2021 ) utilizes both numerical and global textual   information , as well as inter - company relationships   but does not address challenges related to captur-   ing global and local multimodal information . Most   works also focus on a single task - forecasting stock   returns for trading . Another equally important set   of forecasting tasks which has many investment   and risk management applications involves similar   challenges . It involves a multivariate multitask set-   ting , where there is a need to manage the returns   and risks of financial portfolios that comprise many   stocks ( multivariate ) , and make investment and risk   decisions based on multiple forecasts ( multitask ):   forecast stock i ) mean returns and ii ) risks ( volatili-   ties ) over a future horizon to balance potential re-   turns and risks when making investment decisions ,   as well as forecast iii ) correlations between stocks   in portfolios over a future horizon .   To address financial data challenges in multi-   task settings , we propose the Guided Attention   Multimodal Multitask N etwork ( GAME ) model .   Our key idea is to use attention to guide learning   between information from different sources and   modalities . GAME incorporates several important   components : i ) guided latent cross - attention learn-   ing between modalities of different time - scales and   sparsity ; ii ) graph - guided representation learning   based on inter - company relationships with dynamic   weights learnt from multimodal information ; and   iii ) guided cross - attention learning between global   and local information . GAME is trained on multi-   ple tasks - forecasting means , volatilities and cor-   relations over a future horizon , which could beused for portfolio allocation and risk management .   While existing works for financial forecasting cap-   ture either local or global information , or network   information with either global or local informa-   tion , GAME jointly captures global , local and net-   work information . Compared to existing works   that utilize transformers for time - series forecast-   ing ( Zerveas et al . , 2021 ) , GAME proposes novel   cross - attention mechanisms that enable i ) more ef-   fective modelling of local information of different   lengths and granularity from different modalities   by first encoding such information to a common la-   tent representation ; and ii ) the extraction of global   information by leveraging such local information .   Hence , our key contributions are as follows :   •To our knowledge , this is the first work to   propose a model for capturing global and lo-   cal information from multiple modalities for   multivariate multitask financial forecasting ;   •We propose an attention - based module that   encodes multimodal information of different   sequence lengths and time granularity to a la-   tent representation space for efficient mutually   guided cross - attention learning ;   •We design a graph encoding module that   uses inter - company relationships to propa-   gate multimodal information across compa-   nies ; and dynamically updates relationship   weights with learnt importances ;   •We design an attention - based module that uses   cross - attention between local and global infor-   mation to guide learning of relevant global   information;6314•We train the model on multiple forecasting   tasks to lower the risk of over - fitting , and   demonstrate the effectiveness of GAME on   forecasting tasks and real - world applications   against state - of - the - art baselines on real - world   datasets .   2 Related Work   As this work involves time - series forecasting and   network learning , we review key related works in   these areas .   Time Series Forecasting . Classical methods   ( Box and Jenkins , 1990 ; Bollerslev , 1986 ) are com-   monly applied to time - series forecasting . How-   ever , they are designed for numerical data but not   unstructured financial text . Deep learning mod-   els have been increasingly applied to time - series   forecasting . They include feed - forward networks   ( Yoojeong et al . , 2019 ; Ding et al . , 2015 ; Oreshkin   et al . , 2020 ) , convolutional neural networks ( Pan-   tiskas et al . , 2020 ; Borovykh et al . , 2017 ; Wan   et al . , 2019 ) , recurrent neural networks ( Flunkert   et al . , 2020 ; Qin et al . , 2017 ; Liu et al . , 2020 ) , and   transformers ( Wu et al . , 2020 ; Zerveas et al . , 2021 ) .   A detailed review of these works can be found in   Lim and Zohren ( 2021 ) ; Jiang ( 2021 ) ; Torres et al .   ( 2021 ) .   Time - series Transformer ( TST ) ( Zerveas et al . ,   2021 ) is a recent model based on the transformer   encoder architecture designed for numerical inputs .   StockEmbed ( SE ) ( Du and Tanaka - Ishii , 2020 ) is   designed for global textual features , while Finan-   cial News and Tweet Based Time Aware Network   ( FAST ) ( Sawhney et al . , 2021b ) is designed for   local textual features . To encode sequences of tex-   tual features , SE utilizes bidirectional GRUs , while   FAST utilizes Time - aware LSTMs ( Baytas et al . ,   2017 ) . These works are designed for information   from a single modality , do not model the effects   of company - to - company relationships , and do not   address the challenges of capturing global and local   multimodal information .   Network Learning . Graph neural networks   ( GNN ) compose messages based on network fea-   tures , and propagate them to update the embed-   dings of nodes and/or edges over multiple neural   network layers ( Gilmer et al . , 2017 ) . In particular ,   Graph Convolutional Network ( GCN ) ( Kipf and   Welling , 2017 ) aggregates features of neighboring   nodes and normalizes aggregated representations   by node degrees . Graph Attention Network ( GAT)(Veli ˇckovi ´ c et al . , 2018 ) assigns neighboring nodes   with different importance weights during aggrega-   tion . Such GNNs are designed for static networks   with static node attributes and can not be directly   applied to networks where attributes are evolving   time series .   A few recent works extend GNNs to predic-   tion tasks on financial time - series data ( Ang and   Ee - Peng , 2021 ; Feng et al . , 2019 ; Sawhney et al . ,   2020b , a , 2021a ) . Relational Stock Ranking ( RSR )   ( Feng et al . , 2019 ) uses LSTM to generate out-   put embeddings for numerical time - series data   of companies before feeding the latter to learn   company embeddings in a network using a GCN-   based model , but does not consider textual informa-   tion . Knowledge Enriched Company Embedding   ( KECE ) ( Ang and Ee - Peng , 2021 ) captures nu-   merical and global textual information and uses a   GAT - based model to capture inter - company rela-   tionships but does not address the challenges of   capturing global and local multimodal information .   RSR and KECE also do not learn the dynamic im-   portance of inter - company relationships .   3 Guided Attention Multimodal   Multitask Network Model   GAME represents companies in a network G=   ( V , E , X ) , where Vrepresents a set of company   nodes , Erepresents relationships between com-   panies , Xrepresents sequences of multimodal at-   tributes . Given a time step t , we define numerical   features X(t ) = [ x(t−K ) , ... , x(t−   1)]to be the sequence of numerical price - related   data associated with company vover a window   ofKtime steps up to t−1 . Textual news fea-   tures include local and global textual features , i.e.   X={X , X } . The pre - encoded lo-   cal news textual features directly associated with a   company vwithin the same window are denoted   asX ( t ) = [ x(t−K ) , · · · , x(t−   K ) , x(t−K+ 1 ) , · · · , x(t−1)].S=   M×Kand assumes Mnews articles are cap-   tured for each company at each time - step . Where   there are less than Marticles for any company   at any given time - step , we add PAD values of   zero to the sequence ( Devlin et al . , 2019 ) . We   denote pre - encoded global news features over the   window period KasX(t ) = [ x(t−   K ) , ... , x(t−1 ) ] , with varying number of   news articles binned into each time step .   As shown in Figure 2 , GAME first encodes6315   bothX(t)∈RandX(t)∈   R , where d , dare embedding di-   mension sizes , to a common latent sequence length   Land dimension dby using latent attention - based   encoders inspired by Jaegle et al . ( 2021a ) , where   L≪Kas part of the Guided Latent Cross-   Attention Learning step . We introduce guided   cross - attention to enable information from one   modality to guide the attention learning of an-   other . In the next Dynamic Graph - Guided Atten-   tion Learning step , representations of both modal-   ities are used to discover and update importance   weights of inter - company relationships before ap-   plying dynamic graph convolutions . A latent de-   coder inspired by Jaegle et al . ( 2021b ) then de-   codes the numerical and local textual representa-   tions to the original sequence lengths KandS.   In the Guided Global - Local Attention Learning   step , we use the decoded local representations to   guide the attention extraction of the sequence of   global textual features relevant to each company   v. The resultant representations are then com-   bined and sequentially encoded with a transformer ,   followed by attention - based temporal and multi-   modal fusion . Finally , GAME generates forecasts   of means , volatilities and correlations of financial   returns over a selected future horizon of Ktime-   steps , i.e. the means , volatilities and correlations   ofY(t ) = [ y(t ) , ... , y(t+   K ) ] , where y(t ) = ( price ( t)−price ( t−   1))/price ( t−1)andprice ( t)denote the percent - age return and stock price at time step trespectively .   We further elaborate on GAME modules below .   Guided Latent Cross - Attention Learning .   This step addresses the challenge of learning   information from modalities of different se-   quence lengths , degrees of sparsity and distribu-   tions , specifically X(t)∈Rand   X(t)∈R. For X(t ) , we first   project the inputs to common dimension dand   add a learnt time vector ( Kazemi et al . , 2019 ;   Godfrey and Gashler , 2018 ) . The time vector is   learned from the time - stamps T(t)corresponding   to the inputs . In this paper , we use day of week ,   week and month of year for T(t ) , and further   include seconds of day for T(t)as these are   most relevant to the respective inputs . The time   vector P(t)∈Ris learned by com-   bining functional forms and learnable weights and   could be viewed as a time - sensitive version of posi-   tional encodings used in transformers ( Vaswani   et al . , 2017 ) . For GAME , the empirically cho-   sen components used to generate the time vectors   areΦ = sigmoid ( Linear ( T(t)))and   Φ = cos(Linear ( T(t ) ) ) , which enable   the model to extract non - linear and seasonality-   based temporal patterns . We then concatenate   these components and project them : P(t ) =   Linear ( [ Φ||Φ ] ) . The output of the pro-   jection and addition of time vectors step is :   H(t)∈R. For the latent encoding   step , we introduce latent units L ∈R. We re-6316peatLby|V|times to get R , and apply   linear layers to generate queries from L , and keys   and values from H(t ) . That is , Q(t ) =   Linear(L),K(t ) = Linear(H(t ) ) ,   V(t ) = Linear(H(t ) ) . We then ap-   ply scaled dot - product attention ˜H(t ) =   softmax ( Q(t)·K(t))V(t)/√   d. To   elaborate , the dot - product between Q(t)∈   RandK(t)∈Rgives us at-   tention weights of dimensions |V| ×L×K. We   use these attention weights to map V(t)∈   Rto˜H(t)∈R. The same   set of steps is repeated for X(t)to ob-   tain ˜H(t ) . Hence , after the latent encod-   ing step , both ˜H(t)and˜H(t)have the   same sequence length Land dimension d , i.e.   ˜H(t),˜H(t)∈R , and share a   common latent space due to the common L.   In the next guided cross - attention step , infor-   mation from each of the modalities guide attention   learning of the other . Sharing a common latent   space facilitates mutually guided learning between   the modalities and is more efficient as L≪K≪   S. For this step , we generate queries , keys , and   values from the numerical and local text represen-   tations : ˜Q(t ) = Linear(˜H(t ) ) ,   ˜K(t ) = Linear(˜H(t ) ) ,   ˜V(t ) = Linear(˜H(t ) ) ,   ˜Q(t ) = Linear(˜H(t ) ) ,   ˜K(t ) = Linear(˜H(t ) ) ,   ˜V(t ) = Linear(˜H(t ) ) .   Queries of one modality are used to guide the learn-   ing of the other modality as follows :   Dynamic Graph - Guided Attention Learn-   ing . We then utilize inter - company relation-   ships Eto guide learning . While these rela-   tionships do not frequently change ( e.g. , com-   mon sector relationships ) , their importances vary   across time . Hence , we discover dynamic re-   lationship weights with the dynamic attention-   based edge weights discovery ( DW ) module .   We concatenate and project ˜H(t)and   ˜H(t)with a linear layer to obtain : ˜H(t ) =   Linear [ ˜H(t)||˜H(t ) ] . We then gener-   ate : Q(t ) = Linear(˜H(t));K(t ) = Linear(˜H(t ) ) . To learn the importance of   inter - company relationships in a dynamic manner ,   we compute attention weights :   where W∈R. As we carry out   this operation in the latent space with dimen-   sionL , W(t)∈R. We then repeat   the adjacency matrix corresponding to the inter-   company relationships EbyLtimes to get A(t)∈   Rand compute the Hadamard product be-   tween A(t)andW(t):˜A(t ) = A(t)⊙W(t ) .   This results in the weighted adjacency tensor   ˜A(t)∈Rwith ˜A(t)∈Rrepresenting   the weighted relational edges between asset iand   jacross latent dimension L. Next , in the dynamic   network convolution step , we utilize the encoded   company representations ˜H(t)and the weighted   adjacency tensor ˜A(t)as inputs to a weighted dy-   namic graph convolution step to encode network   representations of companies . For company v , we   compute its network representations Z(t)∈R   across Ldimension by aggregating representations   from its neighbors N(i , t)based on ˜A(t ) , j∈V :   Across all assets , we obtain Z(t)∈R. We   adopt this approach instead of other GNNs for com-   putational efficiency as it allows us to apply graph   convolution across multiple dimensions in parallel .   Guided Global - Local Attention Learning . We   then apply latent decoding to decode the represen-   tation Z(t)from the latent dimension Lto the origi-   nal sequence length KandSfor the numerical and   local text modalities respectively . To decode the nu-   merical information , the numerical representations   after the projection and addition of time vectors   H(t)are used as queries to decode the keys   and values of the representation Z(t ) . We generate :   Q(t ) = Linear(H(t ) ) , K(t ) =   Linear(Z(t ) ) , V(t ) = Linear(Z(t ) ) ,   and apply scaled dot - product attention :   To elaborate , the dot - product between   Q(t)∈RandK(t)∈R   gives us attention weights of dimensions   |V| ×K×L. We then use these attention6317weights to map V(t)∈Rto   Z(t)∈R. Similarly , to decode   the local textual representation , the queries   of the local textual representations after   the projection and addition of time vectors   H(t)are used to decode the keys and   values of Z(t ) . We generate : Q(t ) =   Linear(H(t ) ) , K(t ) =   Linear(Z(t ) ) , V(t ) = Linear(Z(t ) ) ,   and again apply scaled dot - product attention :   resulting in Z(t)∈R. The global-   local guided cross - attention step uses the decoded   Z(t)to guide the learning of global textual fea-   tures relevant to each company vfromX(t ) .   We utilize Z(t)instead of Z(t)as we   extract global textual features for each time - step   t−kin window Krather than S.Z(t)also   contains information relating to Z(t)due   to the prior guided latent cross - attention learning   step . For each time step t−kin window K , we   generate Q(t−k ) = Linear(Z(t−   k ) ) , K(t−k ) = Linear(X(t−   k ) ) , V(t−k ) = Linear(X(t−   k ) ) . We apply scaled dot - product attention :   Z(t−k ) = softmax ( K(t−k ) ·   W·Q(t−k)/√   d)·V(t−k )   where W∈Ris an inner weight shared   across all time steps t−kto improve attention ex-   traction of global textual information . Across the   window period , we get Z(t)∈R.   Sequential Encoding and Fusion . Transformer   encoders ( Vaswani et al . , 2017 ) are then used   to encode the resultant sequence of representa-   tions : Z(t ) = TransformerEnc ( Z(t ) ) ,   Z(t ) = TransformerEnc ( Z(t ) ) , and   Z(t ) = TransformerEnc ( Z(t ) ) . The   transformer encoded sequence of representations   are combined with temporal attention fusion , which   weights contributions of each time step t−k   based on its importance . A non - linear transfor-   mation is applied to the respective representa-   tions , say Z(t−k ) , to obtain scalar α(t−k )   for each time step t−kin the window of t :   α(t−k ) = Wtanh(WZ(t−k ) + b ) ,   where WandWare learnable weight ma-   trices and bis the bias vector . We normalize   eachα(t−k)to obtain the weights : β(t−k ) = . We then fuse the sequence   of representations : Z(t ) = Pβ(t−   k)Z(t−k ) , where Z(t)∈R. This   temporal attention fusion step is repeated across K   time - steps for Z(t)to obtain Z(t)∈   Rand across Stime - steps for Z(t)to   obtain Z(t)∈R. The representations   from the three modalities are then fused with mul-   timodal attention fusion . We denote each of the   modalities as r , for a total of R= 3 modalities   for the numerical , local textual and global tex-   tual modalities respectively . A non - linear trans-   formation is applied to the representations to ob-   tain scalars s(r ) = Wtanh(W¯Z(t ) + b ) ,   where WandWare learnable weight ma-   trices and bis the bias vector . Parameters are   shared across modalities . We normalize the scalars   with a softmax function to obtain the weights :   β= , which are used to fuse rep-   resentations across the three modalities : Z(t ) = PβZ(t ) , where Z(t)∈R.   Forecasting and Loss Functions . We   use fully connected layers to generate fore-   casts of means and volatilities of stock returns   over the selected horizon period [ t , t+K ] :   ˆY ( t ) = FC(Z(t ) ) ; and ˆY(t ) =   FC(Z(t ) ) . To forecast correlations of as-   set returns over the horizon period [ t , t+K ] ,   we use weights from linear layers in DW :   Q(t ) = Linear(Z(t));K(t ) =   Linear(Z(t ) ) . This allows what was learnt   in the DW step to be utilized here : ˆY ( t ) =   FC(tanh ( ) ) . We then compute   losses between the forecasts above and respective   ground - truths , i.e. actual means , volatilities and   correlations over the horizon [ t , t+K](see Ap-   pendix A.2 for ground - truth definitions ) with root   mean squared loss ( RMSE ) , and use total losses as   the training objective :   We do not weight the losses differently as we want   the model to perform equally well on all three tasks.6318   4 Experiments   Datasets . We conduct experiments with four   datasets , comprising global and local textual infor-   mation of news articles from financial news portals   - Investing news ( IN ) and Benzinga news ( BE ) ; and   numerical information of daily stock market price-   related information of two stock markets - NYSE   ( NY ) and NASDAQ ( NA ) from 2015 to 2019 . The   coverage of these datasets - across five years , more   than 1.5 m articles and 2,000 companies - is more   extensive than most existing works and provides   strong assurance to our experiment findings . Fol-   lowing Ang and Ee - Peng ( 2021 ) , we utilize rela-   tionships between companies extracted from Wiki-   data knowledge graphs for the inter - company re-   lationships Efrom Wikidata dumps dated 7 Jan.   2019 . Companies such as Google , Apple and Mi-   crosoft are present within the Wikidata KG as enti-   ties , and relationships between them , e.g. , Alpha-   bet as a parent company of Google ( first - order ) ,   both Apple and Microsoft are producing computer   hardware ( second - order ) , can be extracted from   Wikidata . We use a pre - trained Wikipedia2Vec   ( Yamada et al . , 2020 ) model to pre - encode textual   news to capture the rich knowledge present within   the Wikipedia knowledge base ( see Table 1 and   Appendix A.1 for more details on datasets ) .   Tasks and Metrics . We compare GAME with   state - of - the - art baselines on three predictive tasks :   forecasting of i ) means , ii ) volatilities , and   iii ) correlations of stock price percentage re - turns . We use RMSE , mean absolute error ( MAE )   and symmetric mean absolute percentage error   ( SMAPE ) as metrics . RMSE and MAE are com-   mon scale - dependent metrics used to evaluate fore-   casting performance with RMSE being more sen-   sitive to outliers than MAE . SMAPE is a scale-   independent metric that gives equal importance   to under- and over - forecasts required in our eval-   uation context ( see Appendix A.3 for more de-   tails on SMAPE ) . Datasets are divided into non-   overlapping training / validation / test sets in the ra-   tios 0.7/0.15/0.15 for experiments .   Baselines and Settings . We compare GAME   against state - of - the - art baselines ( see Section 2 ):   TST ( Zerveas et al . , 2021 ) that captures numeri-   cal information ; SE(Du and Tanaka - Ishii , 2020 )   that captures global textual information ; FAST   ( Sawhney et al . , 2021b ) that captures local tex-   tual information ; RSR ( Feng et al . , 2019 ) that cap-   tures numerical information and inter - company re-   lationships ; and KECE ( Ang and Ee - Peng , 2021 )   that captures numerical , global textual information   and inter - company relationships . We add fully-   connected layers to baselines for them to forecast   means , volatilities and correlations of percentage   stock returns . We set the window period K= 20   days ; and horizon period K= 10 .K= 20 corre-   sponds to a trading month , and K= 10 days corre-   sponds to a global regulatory requirement for VaR   computations , which we examine in the case - study   ( in Section 6 ) . Following Sawhney et al . ( 2021b),6319we set Mfor local news text sequences to be 10 .   We empirically set Lto 16 . Dimensions of hidden   representations are fixed at 100 across all models .   Models are implemented in Pytorch and trained for   100 epochs on a 3.60GHz AMD Ryzen 7 Windows   desktop with NVIDIA RTX 3090 GPU and 64 GB   RAM . Training GAME , which has 1.01e6 parame-   ters , takes around two hours on the IN datasets and   nine hours on the BE datasets ( see Appendix A.4   for more details on settings ) .   Results . Table 2 sets out the results of the fore-   casting experiments . Across all tasks , GAME out-   performs all baselines . On the task of forecasting   means , dispersion in model performances for IN   datasets is more narrow than for BE datasets . On   the tasks of forecasting volatilities andforecasting   correlations , baseline models ( RSR , KECE ) that   perform better for BE datasets utilize textual and   relational information . Performance differences   between GAME and baselines are more significant   for the larger BE datasets than for the IN datasets   due to the larger volume of news textual informa-   tion . Differences in performances between GAME   and baselines are more pronounced for volatilities   and correlations forecasting than means forecasting   as these are harder tasks that require the model to   capture global and local news effects and the prop-   agation of news effects between companies , which   are key features of the GAME model .   5 Ablation Studies   Table 3 shows the results of ablation studies for   GAME on IN - NY . We observe similar sensitivities   for other datasets . When we exclude the guided co-   attention module ( w/o . guided co - attn . ) , the drop   in performance is more significant for volatility and   correlation forecasting tasks , while performance   decline is more significant for the correlation fore-   casting task when we exclude the dynamic graph-   guided attention module ( w/o . graph - guided enc . ) .   When we vary the multi - task aspect of GAME by   training on mean , volatility or correlation forecast   losses only ( i.e. w. mean loss only , w. vol . loss   only , w. corr . loss only ) , we see significant drops   in performance , even on tasks that correspond to   the training loss , e.g. , performance of mean fore-   casts when we train only on mean loss is poorer   than when we train GAME with multiple tasks .   6 Application Case Studies   We use model forecasts for investment and risk   management applications to evaluate the quality   of forecasts . Portfolio allocation optimizes the   proportion of capital invested in each stock in a   portfolio by finding an optimal set of investment   weights Wthat maximize portfolio returns while   minimizing portfolio risk . We use model forecasts   as optimization inputs to find Wthat maximizes   risk - adjusted returns in a future horizon . Value - at-   Risk ( VaR ) ( Linsmeier and Pearson , 2000 ) is a key   measure of risk used in financial institutions that   measures potential losses in a pre - defined horizon   with a probability of p% , e.g. , 10 day 95 % VaR of   $ 1 m means a 5 % probability of losses exceeding   $ 1 m over a 10 day horizon . When realized losses   exceed forecasted VaR , we call it a VaR breach .   We use model forecasts to compute 10 day 95 %   portfolio VaR forecasts , and evaluate model perfor-   mances by the total number of VaR breaches . De-   tails on computation methodologies are provided   in Appendix A.5 . Table 4 depicts results for the IN-   NY / IN - NA datasets . For portfolio allocation , port-   folios constructed using GAME ’s forecasts achieve   highest average risk - adjusted returns . For VaR ,   GAME out - performs baselines with significantly   less VaR breaches . Baselines utilizing textual infor-   mation or inter - company relationships ( SE , FAST ,   RSR and KECE ) generally perform better.6320   7 Conclusion and Future Work   In this paper , we designed GAME , a model that cap-   tures global and local multimodal information with   modules that i ) enable mutual guidance between   modalities with different time - scales , sparsity and   distributions ; ii ) propagation of multimodal infor-   mation between companies via real - world relation-   ships with dynamic weights to guide learning ; iii )   guided attention learning between global and lo-   cal information to extract relevant global informa-   tion ; and iv ) was trained in a multivariate multi-   task setting . The model performs strongly on three   forecasting tasks and two real - world applications ,   demonstrating the value of guided attention learn-   ing for global and local multimodal information .   The datasets used are more extensive than most   similar works and provide strong assurance on the   validity of the results across different companies   and textual information . Future work could extend   GAME to capture information from other modali-   ties ( e.g. , audio , visual ) , textual sources ( e.g. , Twit-   ter , Reddit ) , and inter - company relationships ( e.g. ,   DBPedia , GDELT ) . In relation to the societal im-   pact of this work , we see opportunities for GAME   to support better investment and risk management   decisions , and also benefit a range of real - world   applications , such as investment portfolio alloca-   tion and risk management , as we demonstrated in   our paper . We should however recognize that mod-   els such as GAME generate forecasts based on   past historical patterns that may not always hold in   the future , particularly for non - stationary financial   time - series . Hence , model risk management , e.g. ,   monitoring significant changes in input information   and model performance , is particularly important to   avoid negative impacts , such as investment losses . Acknowledgements   This research is supported by the National Research   Foundation , Singapore under its Strategic Capabili-   ties Research Centres Funding Initiative . Gary Ang   is supported by a Monetary Authority of Singapore   Postgraduate Scholarship . Any opinions , findings   and conclusions or recommendations expressed in   this material are those of the author(s ) and do not   reflect the views of National Research Foundation ,   Singapore , nor the Monetary Authority of Singa-   pore .   References632163226323A Appendix   A.1 Datasets   The four datasets ( across two news article sources   and two stock markets ) differ in the companies   covered and news sources as depicted in Table 1   in the main paper . The news article sources are :   i ) Investing news datasets ( IN ) ; and ii ) Benzinga   news datasets ( BE ) . The datasets contain news   articles and commentaries collected from Investing   and Benzinga investment news portals , which are   drawn from a wide range of mainstream providers ,   analysts and blogs , such as Seeking Alpha and   Zacks .   We also collected daily stock market price-   related information - opening , closing , low & high   prices , and trading volumes - of two stock markets -   NYSE ( NY ) and NASDAQ ( NA ) - from the Center   for Research in Security Prices . We filter out stocks   from NYSE and NASDAQ that are not traded in   the respective time periods and not mentioned in   any articles for the respective news article sources .   GAME can be used for datasets that contain more   stocks , i.e. , even those that are not mentioned in   any news articles , as it captures both global and   local textual news information as well as numerical   information . However , we restrict the experiments   to stocks that are mentioned in the articles for a fair   comparison with models such as FAST that are de-   signed to only capture local textual news informa-   tion , i.e. they can not capture any news information   not associated with any specific companies .   For inter - company relationships , we use Wiki-   data , one of the largest and most active collab-   oratively constructed KGs . Companies such as   Google , Apple and Microsoft are present within   the Wikidata KG as entities , and relationships be-   tween them , e.g. , Alphabet as a parent company   of Google ( first - order ) , both Apple and Microsoft   are producing computer hardware ( second - order ) ,   can be extracted from Wikidata . We extracted in-   stances of 57 first and second - order relationship-   types identified by ( Feng et al . , 2019 ) from the   Wikidata dumps dated 7 Jan. 2019 . The earliest   Wikidata dumps were from 2014 . We used Wiki-   data dumps from 7 Jan. 2019 and not earlier as we   found that knowledge graphs extracted from earlierWikidata dumps were too sparse to be useful for   our experiments . We did not use Wikidata dumps   that were more recent so that the starting date of   the test sets will be after the 7 Jan. 2019 date of the   Wikidata dump used to construct the KG .   A.2 Ground - truth Definitions   ForY(t ) = [ y(t ) , ... , y(t+   K)]over a horizon of Ktime - steps , the ground-   truth labels for means and volatilities are defined   as follows :   where µ=Y ( t ) . For correlations between   any two assets iandj :   where x = y ( t+k ) , x = y ( t+   k ) .   A.3 Metrics   SMAPE is defined as :   where nis the number of observations . We choose   SMAPE instead of mean absolute percentage error   ( MAPE ) as SMAPE gives equal importance to both   under- and over - forecasts required in this evalua-   tion context while MAPE favors under - forecast .   A.4 Settings   To train GAME , we chose the window and hori-   zon periods K= 20 andK= 10 days based   on experiments with different periods K , K∈   { 5,10,20,60}which correspond to a trading week ,   fortnight , month and quarter . Differences in per-   formance between GAME and baselines were gen-   erally consistent across all window and horizon   periods . Hence , we set the window period K= 20   days ; and horizon period K= 10 asK= 20   corresponds to a trading month , and K= 10 days   corresponds to a global regulatory requirement for   VaR computations , which we examined in the case-   study ( see Section 6 of the paper ) . For the latent6324L , we chose the latent dimension L= 16 based on   experiments with different periods L∈ { 8,16,32 } .   ForK= 20 andK= 10 , we found that L= 16   led to the best overall performance , and enabled   more efficient scaled dot - product operations than   if we had chosen larger values for L. An Adam   optimizer with a learning rate of 1e-3 with a cosine   annealing scheduler is used . Models are imple-   mented in Pytorch and trained for 100 epochs on   a 3.60GHz AMD Ryzen 7 Windows desktop with   NVIDIA RTX 3090 GPU and 64 GB RAM . Train-   ing GAME , which has 1.01e6 parameters , takes   around two hours on the IN datasets and nine hours   on the BE datasets .   A.5 Methodology for Application Case   Studies   In this section , we describe the detailed methodol-   ogy for portfolio allocation and VaR risk measure-   ment used in this paper .   A.5.1 Portfolio Allocation Methodology   Investment portfolio allocation is an important task   for many financial institutions . The aim of invest-   ment portfolio allocation is to optimize the propor-   tion of capital invested in each stock in a portfolio ,   by finding an optimal set of weights Wthat de-   termine how much capital to invest in each stock ,   so that portfolio returns can be maximized while   minimizing portfolio risk . In this paper , we adopt   the risk aversion formulation ( Fabozzi et al . , 2007 )   of the mean - variance risk minimization model by   Markowitz ( 1952 ) , which models both portfolio   return and risk . Under the risk aversion formula-   tion , the classical mean - variance risk minimization   model by Markowitz ( 1952 ) is re - formulated to   maximize the risk - adjusted portfolio return :   max(Wµ−λWΣW ) ( 12 )   subject to W1= 1.λ , known as the Arrow - Pratt   risk aversion index , is used to express risk pref-   erences and is typically set from 2 to 4 ( Fabozzi   et al . , 2007 ) . In this paper , we set λ= 2 for the   experiments . Higher values of λ= 2will reduce   returns across all models , but the relative differ-   ences between models were generally consistent .   In this paper , we use the forecasted means of asset   returns for µ , i.e.˜µ=ˆY ( t ) ; and compute Σ   with the forecasted volatilities and correlations of   asset returns :   ˜Σ = D(t)·ˆY ( t)·D(t ) ( 13)where D(t)is the diagonal matrix filled with   ˆY(t)along the diagonals . We choose to   forecast correlations of asset returns over the se-   lected horizon period [ t , t+K ] , instead of di-   rectly forecasting co - variances as the co - variances   need to be positive semi - definite ( PSD ) so that   the matrix is invertible , which is important for ap-   plications such as portfolio allocation . Forecast-   ing co - variances directly does not guarantee PSD ,   but forecasting volatilities and correlations sepa-   rately and computing the co - variance matrix using   the volatilities and correlations with the formula :   ˜Σ = D(t)·ˆY ( t)·D(t ) , where D(t)is the   diagonal matrix filled with ˆY(t)along the   diagonals , allows the co - variance matrix to be PSD .   This application can be viewed as a predictive   task as we are using the returns from the window   period t−Ktot−1to make forecasts over the   future horizon ttot+Kand using these forecasts   to determine the resultant weights Wto invest in   stocks over the future horizon ; and then measuring   the portfolio returns realized in this future hori-   zon : E = WR , where Ris a vector   of realized percentage stock returns over the future   horizon .   Given that the aim is to maximize portfolio re-   turns while minimizing portfolio risk ( volatility ) ,   we choose risk - adjusted realized portfolio returns   over the selected future period as the evaluation   metric , defined as : ˜E= , where σ(E )   is portfolio return volatility , defined as the one stan-   dard deviation of the portfolio returns over the same   future period . For this application , the datasets   are similarly divided into non - overlapping train-   ing / validation / test sets in the ratios 0.7/0.15/0.15 ,   and we evaluate performance based on the average   of the risk - adjusted realized portfolio returns across   the test set .   A.5.2 Value - at - Risk ( VaR ) Measurement   Methodology   VaR is a key measure of risk used in financial insti-   tutions for the measurement , monitoring and man-   agement of financial risk . Financial regulators re-   quire important financial institutions such as banks   to measure and monitor their VaR over a 10 day   horizon and maintain capital based on this VaR as   loss buffers . Exchanges may also collect margins   from individual investors based on the VaR of their   investment portfolios . VaR measures the loss that   an institution may face in the pre - defined horizon   with a probability of p% , for e.g. , if the 10 day632595 % VaR is $ 1,000,000 , it means that there is a 5 %   probability of losses exceeding $ 1,000,000 over a   10 day horizon . Whenever the realized losses ex-   ceed the VaR , it is regarded as a VaR breach . More   formally , we define VaR as :   Pr[E≤ −V aR(p ) ] = p ( 14 )   where Eis the realized portfolio value and the   minus sign is added to VaR as we are dealing with   losses , i.e. the probability of realized portfolio   value ( i.e. losses ) being more negative than nega-   tive VaR. For this application , the portfolio is con-   structed based on the approach described for the   portfolio allocation application at each time - step .   This mimics a real - world scenario where finan-   cial institutions continually update their portfolios   based on market conditions . VaR can be computed   as a multiple of the portfolio ’s volatility :   V aR = ϕ(p)×σ ( 15 )   where ϕis the inverse cumulative distribution func-   tion of the standard normal distribution , for e.g. if   p= 95 % thenϕ(p ) = 1 .645 . In the classical ap-   proach , σis the historical portfolio volatility over   a pre - defined window period . To evaluate the base-   line models , we instead use the forecasted portfolio   volatility ˜σ = p   ˜Σwhere ˜Σis computed using   the forecasted volatilities and correlations of asset   returns as defined in equation ( 13 ) . Similar to the   portfolio allocation application , this can also be   viewed as a predictive task as we are using the re-   turns from the window period t−Ktot−1to make   forecasts over the future horizon ttot+Kand   using these forecasts to determine the VaR in the   future horizon . We evaluate model performances   by counting the total number of 95 % VaR breaches ,   i.e. where the realized portfolio loss exceeds the   forecasted VaR in the testing dataset ( using the   same training / validation / test sets as the portfolio al-   location application ) . We choose the 95 % VaR for   our experiments as it is a common confidence level   used by banks to monitor their risks . Models that   are able to make accurate forecasts of VaR should   have less VaR breaches.6326