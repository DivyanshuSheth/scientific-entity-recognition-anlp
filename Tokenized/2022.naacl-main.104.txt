  Rujun HanHong ChenYufei TianNanyun PengUniversity of Southern CaliforniaTokyo UniversityUniversity of California , Los Angeles   rujunhan@usc.edu ; chen@nlab.ci.i.u-tokyo.ac.jp   yufeit@ucla.edu ; violetpeng@cs.ucla.edu   Abstract   Stories or narratives are comprised of a se-   quence of events . To compose interesting sto-   ries , professional writers often leverage a cre-   ative writing technique called ﬂashback that in-   serts past events into current storylines as we   commonly observe in novels and plays . How-   ever , it is challenging for machines to generate   ﬂashbacks as it requires solid understanding   of event temporal order ( e.g. feeling hungry   /angbracketleftbefore / angbracketrighteat , not vice versa ) , and the creativ-   ity to arrange storylines so that earlier events   do not always appear ﬁrst in narrative order .   Two major issues in existing systems exacer-   bate the challenges : 1 ) temporal bias in pre-   training and story datasets that leads to mono-   tonic event temporal orders ; 2 ) lack of explicit   guidance that helps machines decide where   to insert ﬂashbacks . We propose to address   these issues using structured storylines to en-   code events and their pair - wise temporal rela-   tions ( /angbracketleftbefore / angbracketright,/angbracketleftafter / angbracketrightand / angbracketleftvague / angbracketright ) astempo-   ral prompts that guide how stories should un-   fold temporally . We leverage a Plan - and - Write   framework enhanced by reinforcement learn-   ing to generate storylines and stories end - to-   end . Evaluation results show that the proposed   method can generate more interesting stories   with ﬂashbacks while maintaining textual di-   versity , ﬂuency and temporal coherence .   1 Introduction   Flashback is a popular creative writing technique   that brings the readers from the present moment   to the past via inserting earlier events in order to   provide background or context of the current narra-   tive ( Pavis , 1998 ; Kenny , 2004 ; Gebeyehu , 2019 ) .   For example , in Figure 1a , the “ GHOST ” in Shake-   speare ’s play Hamlet instruments a ﬂashback by   interrupting the main narrative and describing a   historical event to the audience that Hamlet ’s father   was killed by the current king rather than a snake .   Figure 1 : ( a ) ﬂashback ( b ) temporal v.s. narrative order .   Flashback , by manipulating the event temporal   orders in narrative structure , can arouse readers ’   emotions such as surprise , suspense , and curios-   ity ( Brewer and Lichtenstein , 1981 , 1982 ) . These   emotions stimulate readers ’ interests and eventually   contribute to the satisfaction of reading ( Tan , 1996 ;   Alwitt , 2002 ) , which improves the interest level of   a story . The example in Figure 1a injects historical   events in the middle of the narrative . This arrange-   ment of events can surprise readers and therefore ,   makes the story more interesting than a straightfor-   ward storyline where the past events are shown in   the beginning .   Similarly , consider the pair of two - sentence sto-   ries in Figure 1b . Both stories are composed of the   same events with the temporal order “ lost con-   sciousness”/angbracketleftbefore / angbracketright“woke up in the hospital . ” In   Story ( 1 ) , seeing [ e1 ] , readers can make a relatively   easy educated guess of [ e2 ] , but it is more subtle in   Story ( 2 ) as there are many different ways to end up   in a hospital . By showing the ending event ﬁrst , the   ﬂashback in Story ( 2 ) creates suspense that makes   the following sentences less predictable , and thus   arouses readers ’ curiosity and makes the reading   more interesting .   While human writers are capable of maneuver-1450ing event temporal orders to compose coherent and   interesting stories , it remains challenging for ma-   chines . The challenge is partially attributed to data   bias . Ning et al . ( 2018a ) shows that the pattern in   Story ( 1 ) is dominant in human - written texts , where   neighboring events with /angbracketleftbefore / angbracketrighttemporal relations   ( i.e. , narrative order indicates temporal order )   occur 60−70 % of the time . This is also man-   ifested in our experiments with vanilla language   models amplifying this ratio and producing more   than 80%/angbracketleftbefore / angbracketrightrelations for neighboring events   in the generated stories . Furthermore , current state-   of - the - art story generation systems that incorporate   event representations usually assume event tem-   poral order follows narrative order ( Goldfarb-   Tarrant et al . , 2020 ; Lin et al . , 2021 ) . There are   no explicit prompts in these systems that help de-   termine when ﬂashback should be used , leaving   models to produce dull stories consisting of event   sequences with monotonic /angbracketleftbefore / angbracketrightrelations .   To facilitate more effective ﬂashback , we pro-   pose to incorporate temporal prompts in an end-   to - end story generation framework inspired by the   Plan - and - Write paradigm ( Yao et al . , 2019 ; Xu   et al . , 2020 ; Goldfarb - Tarrant et al . , 2020 ) , where   machines ﬁrst learn to plot a storyline , and then gen-   erate the story based on the storyline . Speciﬁcally ,   we encode predeﬁned event temporal prompts in   structured storylines . As the bottom block of Fig-   ure 2 shows , a structured storyline contains two   components : 1 ) event representations where an   event trigger ( “ grabbed ” ) and two arguments ( “ she ”   and “ the dog ” ) are extracted from the original story   sentences ; and 2 ) temporal prompts : the tempo-   ral order between neighboring events , e.g. event   1 : ( “ she ” , “ grabbed ” , “ the dog ” ) is /angbracketleftafter / angbracketrightevent   2 : ( “ white snow ” , “ blanketed ” , “ the ground ” ) . By   training our storyline generation model with these   predeﬁned pair - wise temporal relations , models   capture how neighboring events are temporally re-   lated to each other ; while during storyline decoding ,   supplying predeﬁned temporal prompts can guide   models to generate reasonable narratives with de-   sirable event temporal orders .   Prior works ( Fan et al . , 2019 ; Goldfarb - Tarrant   et al . , 2020 ) build the storyline and story models   separately , which creates a discrepancy where gold   storylines are used during training , but predicted   storylines are used during inference . To mitigate   this training - inference discrepancy , we leverage re-   inforcement learning ( RL ) to train our systems end - to - end . It enables the story model to train on gen-   erated storylines and updates the storyline model   with the feedback from the story model . Our exper-   imental results show that the RL - based models can   leverage temporal prompts more effectively , re-   sulting in more effective ﬂashback generation and   more interesting stories .   We summarize the contributions of this paper   as follows : 1 ) To facilitate effective ﬂashback , we   propose to leverage structured storylines with tem-   poral prompts to arrange events in story genera-   tion . 2 ) We integrate reinforcement learning in our   story generation pipeline , which can help models   better leverage temporal prompts . 3 ) We test our   framework on two open - domain story datasets and   show more effective ﬂashbacks and increased inter-   est level while maintaining ﬂuency and temporal   commonsense in the generated stories . To our best   knowledge , this is a pioneering study on ﬂashbacks   inneural story generation .   2 Background and Task Deﬁnitions   In this section , we describe the key components :   events and temporal prompts in our proposed   structured storylines , and then deﬁne the Plan - and-   Write generation task .   Event Representation . Following the deﬁni-   tions of ACE ( 2005 ) , we deﬁne an event as a trigger   word and its arguments . In this work , we simplify   the representation by leveraging semantic role la-   beling ( SRL ) tools ( Shi and Lin , 2019 ) to parse two   arguments as shown in Figure 2 . We only consider   one event per story sentence and denote the k - th   event in story iase . We leave more complicated   representations for future study .   Temporal Prompts . Letr={r}denotes the   set of temporal relations between the k - th and the   ( k+1)-th event in story i. Ifkindexes the last event ,   ris not deﬁned . Following the event relation   deﬁnition of ( Ning et al . , 2018b ) , we use events ’   start time to evaluate temporal order .   Structured Storyline . Figure 2 provides a sto-   ryline consisting of ﬁve event representations ex-   tracted from our data . More formally , let S=   { e , e, ... e, ... e}indicates a storyline with   nevents . Encoding temporal prompts , Sbe-   comesS={e , r , e , r ... e , r, ... e } .   Note that in this work , ris provided as prede-   ﬁned prompts rather than predicted as e.   Story . Our ultimate goal is to generate ﬂashbacks   in stories . We denote the story associated with the1451   storylineSasY.   Plan - and - Write is a two - stage framework that   ﬁrst generates storyline ˆSgiven some input x(e.g .   title , leading sentence ) , and then generate ˆYbased   onˆS. Again , rare given as predeﬁned prompts   whereaseare to be predicted as part of the story-   line generation shown in Figure 2 .   3 Framework for FlashBack Generation   In this section , we ﬁrst provide an overview of   the Plan - and - Write story generation system and   introduce a vanilla version of the end - to - end train-   ing method . Then we describe the details of   our key contribution of leveraging event tempo-   ral prompts to generate ﬂashbacks . After that ,   we discuss pretraining structured storylines with   self - labeled data and incorporating reinforcement   learning to jointly train our end - to - end models .   3.1 Plan - and - Write Models   In order to provide better explainability and con-   trollability over the machine generated stories , re-   cent research efforts ( Yao et al . , 2019 ; Xu et al . ,   2020 ; Goldfarb - Tarrant et al . , 2020 ) explore divid-   ing story generation into two steps : 1 ) from input   or preﬁx , x , we ﬁrst produce a storyline , S ; 2 )   based on the storyline , we generate a story , Y. We   describe the details below .   Storyline Model . Letαdenote the parameters   of the storyline model , per sample training loss can   be computed asL=−logp(S|x , α ) .   Story Model . Letβdenote the parameters of the   story model , per sample training loss can be com-   puted asL=−logp(Y|x , S , β).Inference . Note thatSabove is the gold story-   line extracted from Y. At the inference time , we   do not haveS , and have to replace it with ˆS , the   predicted storyline . This results in a discrepancy   between the training and inference time .   End - to - end Training . Instead of using gold sto-   rylineSto train a story model , we can take ˆSas   its input . Now the per sample training loss for the   story model becomes L=−logp / parenleftBig   Y|x,ˆS , θ / parenrightBig   ,   whereθindicates the end - to - end story model pa-   rameters . End - to - end training can alleviate the gap   between the training and inference time , and poten-   tially lead to more consistent stories .   3.2 Structured Storyline Construction   As Figure 2 shows , for a story sentence , we ﬁrst use   the SRL tool to parse its trigger tand two argu-   mentsaanda . We then convert this represen-   tation into a textual form : “ t;a;a / angbracketlefteoe / angbracketright ” ,   where “ ; ” separates two event components , and   /angbracketlefteoe / angbracketrightindicates event ending . For example , the   parsedt , aandain the story sentence “ she   grabbed the dog and ran outside ” are “ grabbed , ”   “ she ” and “ the dog ” respectively . They are concate-   nated into a ﬁnal textual representation as “ grabbed   ; she ; the dog / angbracketlefteoe / angbracketright . ”   Depending on the experimental setup , we may   use no or only the leading event as input , x. In-   spired by the mask prediction design in Devlin et al .   ( 2019 ) ; Liu et al . ( 2019 ) ; Lewis et al . ( 2020 ) , we   represent the remaining missing events in the in-   puts as “ /angbracketleftmask / angbracketright;/angbracketleftmask / angbracketright;/angbracketleftmask / angbracketright;/angbracketlefteoe / angbracketright , ” where   /angbracketleftmask / angbracketrightindicates either event trigger word or argu-1452Algorithm 1 RL - based End - to - end Training   ments to be predicted by the storyline model .   3.3 Temporal Prompt Encoding   Temporal prompts are used to generate ﬂashbacks .   As we mentioned in Section 2 , we encode a se-   quence of predeﬁned event temporal prompts   r={r}in storyline for k∈{1,n−1}to help   models determine whether the next event mention   ( in narrative order ) should start earlier or later than   its preceding event mention . We use temporal rela-   tion extraction tools to annotate all rin our exper-   imental data . Speciﬁcally , we use ECONET ( Han   et al . , 2021b ) ﬁnetuned on the MATRES dataset   ( Ning et al . , 2018b ) to predict the temporal rela-   tion between neighboring events . The context   and the locations of a pair of event trigger words   are fed into ECONET to predict their temporal or-   der . The temporal prompt set consists of / angbracketleftbefore / angbracketright ,   /angbracketleftafter / angbracketrightand / angbracketleftvague / angbracketright(capturing undetermined tem-   poral order ) , and are ﬁxed in S. Note that / angbracketleftvague / angbracketright   indicates undetermined temporal order due to the   ambiguity of the context ( Cassidy et al . , 2014 ; Ning   et al . , 2018b ) and it does not suggest the context is   poor or the relations are wrong . As shown in Fig-   ure 2 , we replace the end - of - event token /angbracketlefteoe / angbracketrightwith   temporal prompts in storylines , except for the last   event which does not have a next event . With the   prompt - augmented storylines , S , we can re - write   the storyline loss as L=−logp(S|x , r , α ) ,   and story loss asL=−logp / parenleftBig   Y|x,ˆS , θ / parenrightBig   .   3.4 Storyline Pretraining   Using intermediate pretraining to adapt original   pretrained language models has been shown to be   effective for a variety of downstream tasks such as   information extraction ( Joshi et al . , 2020 ) , question-   answering ( Khashabi et al . , 2020 ; Garg et al . , 2020 )   and commonsense reasoning ( Zhou et al . , 2021 ) .   To capture more diverse event sequences and facili-   tate better story generation , we explore pretrainingstoryline model with SRL extracted storyline from   BookCorpus dataset ( Zhu et al . , 2015 ) , and use   learnedαto initialize storyline models .   3.5 RL - based End - to - end Model   The end - to - end model described in Sec . 3.1 allows   the story model to train with the generated story-   lines and hence alleviate the gap between train-   ing and inference . However , this workﬂow still   lacks a mechanism that enables the storyline model   to adjust with the feedback from the story model .   The challenges of training storyline and story mod-   els jointly originate from decoding storylines as   inputs for the story model , which involves non-   differentiable token selections . Thus , the ﬁnal loss   Lcannot be directly back - propagated into the sto-   ryline model .   To overcome this barrier , we adopt reinforce-   ment learning ( RL ) , speciﬁcally , the REINFORCE   algorithm ( Williams and Peng , 1991 ) in our end-   to - end training . Let R = R(x , r ) . The expected   reward with respect to the storyline model can be   written as E[R ] = E[R·log ( p(S|x , r , α ) ) ] .   The gradient to update the storyline model is   ∇J = E[R·∇log ( p(S|x , r , α ) ) ] , which   can be approximated with sampling techniques .   Motivated by Xu et al . ( 2018 ) , we use negative   loss of the story model to construct rewards , that   is , R=−L.In other words , smaller loss from   the story model is associated with larger reward .   Algorithm 1 summarizes the overall method .   4 Experimental Setup   In this section , we start by describing our research   objectives , then we describe our data , evaluation   metrics , experimental designs and implementation   details aiming to achieve these objectives .   The overall research objective is to measure   the impact of using temporal prompts in struc-   tured storylines . Speciﬁcally , can /angbracketleftafter / angbracketrightsuccess-   fully induce ﬂashbacks ? If so , does that contribute   to the interest level of the generated stories while   maintaining the overall quality of the texts ?   4.1 Datasets .   ROCStories ( Mostafazadeh et al . , 2016a ) and Writ-   ingPrompts ( Fan et al . , 2018 ) are our experimen-   tal datasets . We ensured all reported results us-   ing the same test data as the baseline systems ( Xu   et al . , 2020 ) and ( Goldfarb - Tarrant et al . , 2020 ) . For1453pretraining data , we use BookCorpus ( Zhu et al . ,   2015 ) . Appendix B shows all details of data splits   and pre - processing process .   4.2 Temporal Prompts Constructions   ECONET was ﬁnetuned three times with differ-   ent random seeds , so we take the consensus vote   from three models . If there is any disagreement ,   we label the temporal order as /angbracketleftvague / angbracketright . We bench-   mark ECONET ’s annotation performances in Ap-   pendix G , which shows it provides highly accurate   temporal relations . For human evaluations specif-   ically , we consider two prompt settings in order   to gauge different impacts of /angbracketleftafter / angbracketright . 1 ) for ROC-   Stories , all structured storylines consist of exactly   four predeﬁned temporal prompts created follow-   ing Sec 3.3 . We randomly sample stories with one   /angbracketleftafter / angbracketrightprompt from the test data . We will show   later in the analysis that vanilla language models   would generate more than 80 % event pairs with   /angbracketleftbefore / angbracketrightrelations for ROCStories ; /angbracketleftafter / angbracketrightprompt   should bring this ratio down if it is effective . 2 ) for   WritingPrompts , since the number of events is not   ﬁxed , we randomly sample test stories generated   with / angbracketleftafter / angbracketrightprompts for evaluation .   4.3 Automatic Evaluation Metrics   We use automatic metrics to evaluate the textual   quality of stories . We report Ref . PPL : reference   stories ’ perplexity in our models and Gen. PPL :   generated stories ’ perplexity scored by GPT-2 ( Rad-   ford et al . , 2019 ) . For diversity , we report Distinct   Ratio ( % ): overall vocabulary : token number ratio .   We also report standard BLEU-3 andROUGE .   4.4 Human Evaluation Metrics   We rely on human annotators to analyze the ef-   fectiveness of ﬂashback generations . We request   18 MTurkers who succeeded in our previous anno-   tation tasks ( Han et al . , 2021a ) to evaluate stories   produced by our compared models . We host a small   qualiﬁcation round followed by a large annotation   task . Only 10 workers are qualiﬁed , and we only   consider their annotations . Eventually , we collect   106 and 77 sets of valid annotations for ROCStories   and WritingPrompts .   Temporal diversity . The dominance of / angbracketleftbefore / angbracketright   relation in our data can make models biased to-   ward generating stories with more /angbracketleftbefore / angbracketrightrela-   tions . Therefore , we are interested to see how in-   serting an / angbracketleftafter / angbracketrightprompt can help increase the per-   centage of non-/angbracketleftbefore / angbracketrightevent relations in the gen-   erated stories . Let ˆRindicate the percentage of aparticular relation annotated by MTurkers . We cal-   culate the entropy of the set { ˆR},∀r∈{/angbracketleftbefore / angbracketright ,   /angbracketleftafter / angbracketright,/angbracketleftvague / angbracketright}to measure temporal diversity .   Accuracy measures the percentage of /angbracketleftafter / angbracketrightbe-   ing correctly incorporated in the generated stories   labeled by human annotators . We used a relaxed   version by counting annotated /angbracketleftvague / angbracketrightas correct   too , as / angbracketleftvague / angbracketrightcan potentially be /angbracketleftafter / angbracketright . Both   accuracy andtemporal diversity can show the ef-   fectiveness of generating ﬂashbacks using / angbracketleftafter / angbracketright .   Temporal coherence indicates if the event se-   quence in a generated story aligns with an anno-   tator ’s temporal commonsense .1 and 0 corre-   spond to yes and no , respectively .   Interest level . Precisely deﬁning interest level is   difﬁcult as it is a broad concept . So we focus on   theunexpectedness component of cognitive inter-   est . As pointed out by Behrooz ( 2019 ) , unexpect-   edness can be further explained as how predictive   an event is , which is closely related to ﬂashback   generation . Therefore , we deﬁne an interesting   event as 1 ) being unexpected or surprising and 2 )   being logical according to the context and general   commonsense .   For the compared models , we ask annotators   to provide ranks between 1 to K for the gener-   ated stories , with K indicating the most interest-   ing story and 1 indicating the least interesting one .   We encourage workers to provide different scores   for all compared stories , but equal scores are al-   lowed . The max score K depends on the number   of compared models , 5 for ROCStories and 4 for   WritingPrompts . We provide detailed instructions   in the interface shown in the appendix . Crucially ,   interest level is separately annotated from other   metrics and we ensure annotators do not see the   same set of stories in both tasks .   4.5 Compared Models   Baselines . Xu et al . ( 2020 ) , denoted as MEGA-   TRON , is chosen as the baseline as it outperforms   previous systems such as Guan et al . ( 2020 ) on   ROCStories . We also compare with T -   BART ( Lin et al . , 2021 ) as it is pretrained with tem-   poral ordering and event inﬁlling tasks . For Writ-   ingPrompts , we compare with C P- ( Goldfarb - Tarrant et al . , 2020 ) as it also1454   adopts the Plan - and - Write workﬂow as well as   structured event representations . Appendix D de-   scribes more details of baseline systems .   We describe our own model variants below ,   1.V -Guses the parameters of a pre-   trained language model ( LM ) , speciﬁcally BART-   base ( Lewis et al . , 2020 ) , to initialize both the story-   line and story models . Its workﬂow is illustrated in   the upper block of Figure 2 . Since no information   other than the preﬁx ( ﬁrst sentence , prompt , etc . )   is used to generate the story , we denote this model   as vanilla LM generation or V -G.   2.S -P enhances V -   Gby using a structured storyline of events to   encode temporal prompts , which is associated   with the workﬂow of the bottom block of Figure 2 .   3.P .For ROCStories data only , we initialize the storyline model of S -   P with the pretrained parameters .   4.RLuses the same inputs as S -   P . The difference is that reinforcement   learning is used to train storyline and story models   jointly . As Algorithm 1 shows , RL - based model   is trained following the same forward workﬂow as   S -P , but during backpropaga-   tion , the storyline models ’ parameters are updated .   5 Results and Analysis   The main results for ROCStories and Writing-   Prompts are shown in Table 1 and Table 3 respec-   tively . Examples of generated stories can be found   in Table 2 and Table 6 for ROCStories and Ta-   ble 11 in the appendix for WritingPrompts . We   organize our discussions and analysis in the follow-   ing sections by answering the four research ques-   tions . Q1 ) Can our proposed models ( with tem-   poral prompts ) produce stories with good textual   quality ? Q2 ) Are our proposed models effective   at generating ﬂashbacks ? Q3 ) Can our proposed   models maintain event temporal coherence in sto-   ries ? Q4)How do our proposed models contribute   to stories ’ interest levels ?   5.1 Textual Quality   We measure the textual quality of stories using a   wide range of automatic evaluation metrics .   Perplexity . For ROCStories , all three model vari-   ants can improve Ref . PPL against V -G   andT -BART while maintaining good   Gen. PPL . The weak Gen. PPL ofMEGATRON   may be attributed to its sentence - by - sentence gen-   eration pipeline , whereas our models generate an   entire story in an integrated step . For Writing-   Prompts , both model variants improve Gen. PPL   over V -Gand C P   while maintaining good Ref . PPL .   Token Diversity . For ROCStories , RL - based   model improves the V -Gby 0.18 per1455   Distinct Ratio .MEGATRON achieves the high-   est token diversity as it incorporates external   knowledge - bases that make the generated stories   contain novel tokens . For WritingPrompts , we   observe longer stories are associated with poorer   scores . However , the large increases in Distinct   Ratio suggest that the token usages in our proposed   models are diverse .   BLEU and ROUGE.For ROCStories , the pro-   posed models perform on - par with V -   GandT -BART while outperforming   MEGATRON , which generates the shortest stories   among all compared models . For WritingPrompts ,   C P performs the best partially   due to its usage of BART - large models .   The overall performances across these three   types of automatic metrics suggest that using tem-   poral prompts in the Plan - and - Write framework   can produce stories with high textual quality .   5.2 Effectiveness on Flashback Generation   The second research question probes the effective-   ness of using temporal prompts on generating   ﬂashbacks . For ROCStories , all models can gen-   erate stories with the same number of events / sen-   tences as the gold stories . This allows annotators   to judge pairwise event relations in the generated   stories and help us check whether the generated   events have relations truthfully reﬂecting the tem-   poral prompts used . Accuracy is the perfect met-   ric for this . As Table 1 shows , the ﬁnal RL - based   model achieves the highest score , which indicates   the strongest effectiveness of generating ﬂashbacks .   However , temporal prompts are not used in the   baselines and V -G. So we compute an   approximate measures of effectiveness , temporal   diversity , which indicates how many non- /angbracketleftbefore / angbracketright   relations / angbracketleftafter / angbracketrightprompt can induce . Table 1 shows   that S -P , P and   RL - based models can help improve V -   Gwith more than 80 % generated /angbracketleftbefore / angbracketrightre-   lations . MEGATRON achieves the highest score   due to the largest amount ( 29 % ) of /angbracketleftvague / angbracketrightre-   lations ( complex or undetermined ) annotated by   MTurkers shown in Figure 3 , which is associated   with its lowest temporal coherence score .   For WritingPrompts , stories are long and can   contain dialogues or short phrases without events   at all . These make the sentence or event alignments   between the gold and generated stories worse than   ROCStories , i.e. e , rmay not correspond to   the k - th sentence in Y. Therefore , accuracy can-   not be computed . To obtain an approximate met-   ric , we use the tool described in Sec . 3.3 to an-   notate neighboring event temporal relations in the   generated test stories for all the compared mod-   els . Slightly different from temporal diversity ,   we calculate the total number of machine anno-   tated / angbracketleftafter / angbracketrightrelations , denoted as ˆNin each ˆS.   Let{N}denote the number of /angbracketleftafter / angbracketrighttemporal   prompts extracted in gold stories . We compute   thePearson correlation ( Benesty et al . , 2009 ) be-   tween the sets{ˆN}and{ˆN}as the measure .   As Table 3 shows , for C P and   V -Gwithout temporal prompts , the   correlations are weak ; whereas when temporal   prompts are used in both S -P   andRL - based models , the correlations are strong .   Although using models ’ temporal annotations for   the generated stories is not as precise as human   annotations , the large differences in correlation pro-   vide another piece of evidence that our proposed   methods are effective at generating ﬂashbacks .   5.3 Temporal Coherence   Generating ﬂashbacks requires a system to dis-   rupt the monotonic /angbracketleftbefore / angbracketrightsequence , which is the   dominant temporal pattern generated by V -   G(see Figure 3 ) . In other words , ﬂashbacks with   at least one / angbracketleftafter / angbracketrightare minor patterns that can be1456hard to learn in our data , which may result in event   sequences violating our temporal commonsense .   Thus , we need to check that stories generated with   ﬂashbacks maintain good temporal coherence . As   shown in Table 1 and 3 , our proposed models with   temporal prompts can achieve on - par or slightly   lower scores , suggesting little trade - off of tempo-   ral coherence in generating ﬂashbacks . We will   discuss this more in the error analysis ( Sec . 6 ) .   5.4 Contributions to the Interest Level   As we can observe in Table 1 , the impact of tempo-   ral diversity andcoherence on the interest level   appears to be complex . To better understand the   dynamics among these metrics , we run ordinary   least square regressions ( OLS ) ( Kenney and Keep-   ing , 1965 ) by setting interest level as the target   variable and temporal diversity , coherence and   the number of / angbracketleftafter / angbracketrights as predictors . Since all of   these metrics apply to each of the compared stories ,   the total instances are 530 and 308 for ROCStories   and WritingPrompts , respectively .   As Table 4 shows , for ROCStories , holding other   metrics constant , adding 1 unit to temporal coher-   ence anddiversity leads to a 0.609 increase and   0.532 decrease of the interest level . The former   result implies that a story lacking event temporal   coherence tends to be less interesting . The latter   result suggests that increasing temporal diversity   may lead to less interesting stories , which we hy-   pothesize could be attributed to two factors : 1 )   /angbracketleftbefore / angbracketrightis dominant in ROCStories , and by using   /angbracketleftafter / angbracketrightas prompt , we force models to generate rela-   tions less seen in data . 2 ) Figure 3 shows temporal   diversity can increase with more /angbracketleftvague / angbracketrightrelations .   Since / angbracketleftvague / angbracketrightis an undetermined temporal relation   even for our annotators , it could make the storyline   confusing and thus lead to less interesting stories .   The coefﬁcient for the number of /angbracketleftafter / angbracketrightindicators   is positive with strong statistical signiﬁcance . It   suggests that holding the other two metrics con-   stant , adding the number of /angbracketleftafter / angbracketrightindicators by 1   contributes to 0.387 increases of the interest level .   For WritingPrompts , although we are not able   to conclude that the estimates are statistically sig-   niﬁcant , the coefﬁcients have the same signs as   ROCStories . Also , we observe that the p - value of   the number of / angbracketleftafter / angbracketrightindicators is much lower than   the other two variables , which implies a relatively   stronger ( positive ) impact .   Since temporal prompts in human evaluations   all contain at least one /angbracketleftafter / angbracketright , these results show   that when / angbracketleftafter / angbracketrightprompt successfully produces   event pairs with / angbracketleftafter / angbracketrightrelation in the ﬁnal sto-   ries , it makes stories more interesting . Now , we   can answer the ﬁnal research question : improving   temporal diversity can help interest level when   /angbracketleftafter / angbracketrightprompts are effective at generating /angbracketleftafter / angbracketright   relations in stories ; that is , when ﬂashbacks truly   work , stories become more interesting .   6 Error Analysis   In Table 1 , we observe that our ﬁnal models can not   outperform VANILLA - GEN forTemporal Co-   herence and fall behind MEGATRON forTempo-   ral Diversity . We show examples below to provide   feasible explanations .   Temporal Coherence . Table 5 shows three com-   parisons between our ﬁnal RL - based model and   theVANILLA - GEN baseline . In all three pairs ,   human evaluators mark VANILLA - GEN ’s gen-   erated stories as temporally coherent but not for   RL ’s stories . As we can see in these examples , the   red highlighted events temporally contradict the   preceding context . In Input 1 , the context shows   “ I have listened to the new album ” so “ ca n’t wait   to listen to the new album ” should n’t follow the   former event . For Input 2 , if “ Anna had gotten her   hair permed , ” it does make sense that the event   “ went to apply it ( the perm ) ” follows . In Input 3 , if   the ofﬁcer “ gave [ a ] ticket ” already , it does quite   make sense to give “ a warning ” afterward as it is   a weaker penalty than a “ ticket . ” However , we ob-   serve that in all three cases , /angbracketleftafter / angbracketrightrelations are   successfully reﬂected ( ﬂashback ) . Since stories   with / angbracketleftafter / angbracketrightrelations are minority cases in the data ,   we hypothesize that they have not been perfectly   learned by our proposed models . We leave more   rigorous investigation for future research efforts .   Temporal Diversity . Table 10 in the appendix   shows three comparisons between our ﬁnal RL-   based model and the MEGATRON baseline . In all   three pairs , MEGATRON ’s generated stories are   more temporally diverse based on the predicted   relations provided by human evaluators . How-   ever , MEGATRON ’s stories are either contradic-1457   tory ( Input 1 ) , incoherent ( Input 2 ) or repetitive   ( Input 3 ) , resulting in higher ambiguous event re-   lations , i.e./angbracketleftvague / angbracketrightannotations ( consistent with   Figure 3 ) . Therefore , despite lower temporal di-   versity , our proposed models can still produce sto-   ries with higher quality , which is demonstrated via   other metrics such as Temporal Coherence and   Interest Level .   7 Related Work   Generating ﬂashbacks has been studied in a   few prior works . Bae and Young ( 2008 ) is one   of the early efforts proposing a planning - based ap-   proach to generate ﬂashbacks to evoke surprise in   the readers . Follow - up works proposed a cognitive-   based model that ﬁnds the best location in the orig-   inal stories to insert a past event ( Wu et al . , 2016 ) .   Our work differs from this line of research by using   temporal prompts with pretrained language models   to generate integrated ﬂashback in stories . Hoek   et al . ( 2014 ) studies ﬂashback in game narrative   generation , which is remotely related to our work .   Plan - and - Write framework has been shown to   be an effective method to enhance the explainabil-   ity and controllability of story generation . Yao et al .   ( 2019 ) enables machines to produce a sequence ofkeywords prior to generating stories . Follow - up   works leverage commonsense or external knowl-   edge to enhance the quality of stories ( Guan et al . ,   2020 ; Xu et al . , 2020 ; Tan et al . , 2021 ) . Goldfarb-   Tarrant et al . ( 2020 ) is one of our compared works   that incorporates SRL extracted event representa-   tions in storylines and train models with several   event - related decoding objectives . Our work differs   from it by explicitly encoding temporal prompts   in event plots that facilitates ﬂashback .   Structured representation such as discourse   structure ( Guan et al . , 2021 ) , story keywords ( Peng   et al . , 2018 ; Goldfarb - Tarrant et al . , 2019 ) and even-   t / plot graph ( Ammanabrolu et al . , 2019 , 2021 ) have   been widely used in story generation to enable mod-   els to output diverse stories , but they are remotely   related to our ﬂashback generation task .   Reinforcement learning has also been explored   in two - stage story generation such as Xu et al .   ( 2018 ) and Tambwekar et al . ( 2019 ) . Our moti-   vation of using RL - based generation is to enhance   the effectiveness of temporal prompts .   Event temporal reasoning helps the construc-   tion of the temporal prompts . It has been studied   in story understanding ( Han et al . , 2019b ) , infor-   mation extraction ( Ning et al . , 2017 ; Han et al . ,   2019c , a , 2020 ; Ma et al . , 2021 ) , QA ( Ning et al . ,   2020 ; Zhou et al . , 2019 ) , and event generation ( Lin   et al . , 2021 ; Li et al . , 2021 ) . Our proposed method   is the pioneer work to introduce event temporal   prompts inﬂashback generation .   8 Conclusions   We propose to generate ﬂashbacks in stories by en-   coding temporal prompts in structured storylines .   Experimental and evaluation results show our pro-   posed systems can produce ﬂuent , temporally co-   herent , and more interesting stories . Future work   can focus on improving temporal prompts so that   they handle minority cases ( i.e. prompts with one   or more / angbracketleftafter / angbracketrightindicators ) more effectively . De-   signing more powerful rewards in reinforcement   learning is another promising research direction .   Acknowledgments   We thank the PlusLab members and the anony-   mous reviewers for their constructive feedback .   This work is supported in part by IARPA , via Con-   tract No . 2019 - 19051600007 , the DARPA Machine   Common Sense ( MCS ) program under Coopera-   tive Agreement N66001 - 19 - 2 - 4032 , and a CISCO   research award.1458References1459146014611462   A Additional Generated Stories   Please see Table 6 and Table 11 for more examples   on ROCStories and WritingPrompt respectively .   B Data   ROCStories ( Mostafazadeh et al . , 2016a ) con-   tains 5 - sentence stories . Following ( Xu et al . ,   2020 ) , we split data into 88,344/4,908/4,909 for   train / validation / test sets .   WritingPrompt ( Fan et al . , 2018 ) contains   30,335 pairs of prompts and stories . With an av-   erage of more than 700 words per story , Writing   Prompts are much longer than ROCStories . These   stories are also much less structured as some di-   alogues and short phrases may be included . To   speed up our experiments , we select stories with a   maximum of 500 words , resulting in a total numberof 96,488 training and 5,784 validation prompt-   story pairs , respectively . For the test set , we use the   1,000 prompt - story pairs provided by the baseline   paper ( Goldfarb - Tarrant et al . , 2020 ) for reporting   automatic evaluation results .   Pretraining Data . As we mention in Section 3.4 ,   we pretrain storyline models for ROCStories . To   be consistent with ROCStories inputs , we divide   BookCorpus data ( Zhu et al . , 2015 ) into 5 consecu-   tive sentences and ﬁlter out those with noisy tokens .   We randomly select 1 million such 5 - sentence text   spans and extract their storylines following Sec-   tion 3.2 .   C More Details for Evaluation Metrics   Automatic evaluation metrics are used to mea-   sure textual quality of stories . We report 1 ) Ref .   PPL : reference stories ’ perplexity in a model ; 2 )   Gen. PPL : generated stories ’ perplexity scored   by GPT-2 ( Radford et al . , 2019 ) , i.e. we feed   the generated stories into GPT-2 to compute per-   plexity scores . For diversity scores , we found   our models implemented by Huggingface ( Wolf   et al . , 2020 ) can achieve nearly 0 Repeat-3 and   100 % Distinct-3 scores , so we follow Goldfarb-   Tarrant et al . ( 2020 ) to compute the overall vocab-   ulary : token number ratio , which we denote as 3 )   Distinct Ratio ( % ) . We also report standard 4 )   BLEU-3 and 5 ) ROUGEscores .   D More Details for Baseline Models   MEGATRON - CNTRL Xu et al . ( 2020 ) , de-   noted as MEGATRON for brevity , is chosen as the   baseline as it outperforms previous systems such as   Guan et al . ( 2020 ) on ROCStories . We do not per-   form delexicalization that replaces names and en-   tities with [ MALE ] , [ FEMALE ] and[NEUTRAL ]   tokens , as we found our models work well by rec-   ognizing names and entities . When conducting   evaluations , we try our best to map these special   tokens back to their original texts by using the   given ﬁrst sentence . For rare undetermined cases ,   we manually examine the generated stories and   swap in names or entities that make the most sense   in the context . To be fair , we compare with the   124M - parameter version .   ContentPlanning ( Goldfarb - Tarrant et al . , 2020 )   is chosen as the baseline for WritingPrompt , as it   also adopts the Plan - and - Write workﬂow as well   as structured event representations . However , their   models are based on BART - large and do not train1463with an end - to - end framework . They use 65 % of   the original training data and also ﬁlter out samples   with non-[WP ] prompts . Our ﬁnal training data is   about 2/3 of theirs .   TemporalBART ( Lin et al . , 2021 ) is designed   for two event temporal relation related tasks : tem-   poral ordering and event inﬁlling . Although Tem-   poralBART does not tackle story generation di-   rectly , it encodes event temporal information via   pretraining tasks . So we consider TemporalBART   as another baseline model by initialing the sto-   ryline model with their parameters and training   theS -P workﬂow on ROCSto-   ries .   E Reproduction Check List   We ﬁnetune BART - base . For ROCStories , hyper-   parameters are learning rate : 5e ; batch size : 10 .   We use 3 random seeds : ( 5,9998,20016 ) and re-   port the average performances for all end - to - end   models . For Writing Prompts , hyper - parameters   are learning rate : 1e ; batch size : 64 ; gradient   accumulation : 8 .   For ROCStories , we were able to ﬁnetune on a   single Nvidia GTX2020 GPU with 11 G memory ,   and training time is 3 - 4 hours per epoch . For Writ-   ingPrompt , we have to use a much larger Nvidia   A100 GPU with 40 G memory , and the training time   is 20 hours per epoch . We train all models for 10   epochs and save the model with the best evaluation   perplexity . All reproduction details can be found   in the separately submitted code .   F Perplexity and Event Coverage   Trade - off   One caveat of using end - to - end training is that there   is no guarantee that the generated events will ap-   pear in the ﬁnal stories ; whereas in two - stage mod-   els , the story model learns a mapping from refer-   ence storylines to stories , which leads to a higher   coverage rate of the generated events . To provide a   potential solution , we experiment with the mixture-   training method proposed by Zhang et al . ( 2019 ) ,   p=µ   µ+ exp(e/µ )   where p controls the ratio of reference storylines   used in training and eis the training step . Here the   larger the hyper - parameter µ , the slower p decays   to 0 as training proceeds .   In Figure 4 , we show the trade - off between per-   plexity andµvalues by training our ﬁnal RL - based   models . When µis nearly zero , it corresponds   to always using predicted storylines , and hence a   smaller predicted event coverage rate in the ﬁnal   stories . When µgets larger , eventually making p   close to 1 , the training corresponds to always using   gold storylines , which leads to a very high event   coverage rate , but relatively poor perplexity ( still   much stronger than two - stage results ) . We leave   the search for optimal µfor future research .   G Benchmark Event Temporal Relation   Annotations   The experimental results in the main text demon-   strate the effectiveness of using temporal prompts .   Here , we further show that the tool to produce tem-   poral prompts , i.e. ECONET , provides reliable   event temporal relation annotations . We bench-   mark ECONET ’s performances using CaTeRS   ( Mostafazadeh et al . , 2016b ) , which annotates 4   types of temporal relations for event pairs in a   small amount of ROCStories . However , CaTeRS ’s   annotations are based on event time interval rather   than event start time as used in MATRES , which   ECONET is ﬁnetuned on .   In Tabel 7 , we provide a mapping from   CaTeRS ’s temporal relations to MATRES labels.1464The only non - unique mapping is /angbracketleftOverlap / angbracketright . In   other words , when ECONET predicts /angbracketleftbefore / angbracketrightfor   a CaTeRS sample / angbracketleftOverlaps / angbracketright , we have to manually   examine whether it is correct or not . We found   that when ECONET predicts /angbracketleftbefore / angbracketrightfor CeTeRS   data , the precision rate is 65.53 % due to a large   amount of / angbracketleftOverlaps / angbracketrightevent pairs being predicted   as / angbracketleftbefore / angbracketright . But we emphasize here that this low   number is caused by label mismatch as shown   in Table 7 , which does not truthfully reﬂect the   ECONET ’s accuracy .   To have a better understanding , we randomly   selected 20 such pairs and manually examine their   temporal relations in the context and found that   90 % of such pairs are indeed correctly predicted   by ECONET . Adjusting for this factor , the preci-   sion rate for the annotated /angbracketleftbefore / angbracketrightrelation would   be 92.07 % , indicating highly accurate predictions .   We do not claim the ﬁnal accuracy is 92.07 % ,   but simply argue that the annotations provided by   ECONET are helpful as our main experimental   results demonstrate .   H Two - stage Model Results   Two - stage Model . As we mentioned in Sec-   tion 3 , another way to implement Plan - and - Write   framework is to train storyline and story models   separately with gold input and outputs , and replace   story models ’ inputs with the storyline model ’s pre-   dictions during inference . We found this variant ’s   performances fall far behind other compared mod-   els . So we do not use them in human evaluations   and simply show their automatic evaluation results   in Table 8 .   I Storyline Model Results   Our primary goal is to improve temporal control   in ﬁnal stories . However , as aforementioned , the   motivation for using reinforcement learning is to   allow the storyline model to adapt together with   the story model . In Table 9 , we compare storyline   prediction performances between the vanilla and   the RL - based end - to - end models . These results   suggest that our ﬁnal framework combining struc-   tured storylines , temporal prompts , pretraining and   RL also helps storyline models ﬁt reference story-   lines better , resulting in lower perplexity , higher   overlapped scores and better diversity .   J Error Analysis   Table 10 shows examples for the temporal diversity   in Section 6 .   K Potential Risks   Since our models deal with open - domain gener-   ation , it is conceivable that the generated stories   could contain biases , malicious languages , and hal-   lucinations . We refer readers to the work in fairness   and fact checking to address these issues.146514661467146814691470