  Wenting Zhaoand Konstantine Arkoudasand Weiqi Sunand Claire CardieCornell UniversityAmazon Alexa AI   { wzhao,cardie}@cs.cornell.edu , { arkoudk,weiqisun}@amazon.com   Abstract   Task - oriented parsing ( TOP ) aims to convert   natural language into machine - readable repre-   sentations of specific tasks , such as setting an   alarm . A popular approach to TOP is to apply   seq2seq models to generate linearized parse   trees . A more recent line of work argues that   pretrained seq2seq models are better at gener-   ating outputs that are themselves natural lan-   guage , so they replace linearized parse trees   with canonical natural - language paraphrases   that can then be easily translated into parse   trees , resulting in so - called naturalized parsers .   In this work we continue to explore naturalized   semantic parsing by presenting a general re-   duction of TOP to abstractive question answer-   ing that overcomes some limitations of canon-   ical paraphrasing . Experimental results show   that our QA - based technique outperforms state-   of - the - art methods in full - data settings while   achieving dramatic improvements in few - shot   settings .   1 Introduction   Task - oriented parsing ( TOP ) takes an utterance as   input and generates an unambiguous specification   of a task that can be executed by machine ( Gupta   et al . , 2018 ) . Traditional approaches to TOP treat   the task as an instance of slot filling ( Liu and Lane ,   2016 ) , first classifying the intent of the utterance   as a whole and then tagging tokens with slot la-   bels that identify relevant entities ( such as numbers ,   persons , dates or times , organizations , and so on ) .   However , this approach only works for simple ut-   terances that have flat rather than compositional   semantics . That is , slot - filling approaches can not   produce nested ( or “ hierarchical ” ) meaning repre-   sentations , such as the one shown in Fig 1 , where   slots and intents can be composite and contain other   slots ( or intents ) as proper parts .   A straightforward way to handle compositional   semantics is to formulate TOP as a seq2seq prob - lem , where the input sequence is the utterance   and the output sequence is a linearized represen-   tation of a semantic tree ( shown in the bottom   part of Fig 1 ) . The recent development of high-   performing transformer - based pretrained language   models ( PLMs ) ( Lewis et al . , 2020 ; Raffel et al . ,   2020 ; Brown et al . , 2020 ) that can be fine - tuned   on specific tasks ( such as a particular TOP dataset )   has made this formulation feasible .   This approach has achieved state - of - the - art per-   formance on a variety of TOP datasets ( Rongali   et al . , 2020 ; Aghajanyan et al . , 2020 ; Chen et al . ,   2020 ) . However , the output sequences are typically   not expressed in natural language but rather in a   mixture of natural language and special symbols   ( such as [ SL : ) that were not seen during pretrain-   ing and have no meaning to the PLMs . Depending   on the task , the output sequences may hardly con-   tain any natural language at all ( for example , if the   outputs are SQL queries ) . It would seem reason-   able to conjecture that performance would improve   if we could reformulate TOP as a more conven-   tional NLP task in which both the inputs and the   outputs are expressed in natural language , as such   a reformulation might be better able to leverage the   PLM ’s pretraining .   To bridge this gap , Shin et al . ( 2021 ) reduced   TOP to a canonical - paraphrasing task . They first   fine - tune a PLM to map a natural utterance uto   another canonical utterance u , where canonical   utterances belong to a controlled fragment of the   relevant natural language . Then ucan be translated   into the desired meaning representation ( semantic   tree ) via a context - free grammar that can parse   all and only canonical utterances . However , this   approach has a major limitation : canonical frag-   ments can be defined and parsed by hand - written   grammars only in closed - world domains where the   set of underlying entities is fixed and known in   advance ( e.g. , in a domain where people ask ques-   tions about basketball players , all of whom are4418   statically known ) . That condition rarely holds in   task - oriented domains , and therefore this method is   not applicable to datasets like Topv2 . Moreover , as   input utterances grow more structurally complex ,   the associated canonical utterances become much   longer , and generating long sequences is known to   be challenging for PLMs ( Guo et al . , 2018 ) .   In this paper we focus on scenarios where it is   not viable to specify a canonical grammar with a   fixed set of rules . We instead propose to formulate   TOP as abstractive question answering , in such a   way that answering all questions for a given ut-   terance allows us to reconstruct the parse tree of   that utterance . Specifically , we introduce single-   turn QA ( ST QA for short ) , which poses one single   query for a given utterance , and multi - turn QA ( MT   QA ) , which dynamically constructs multiple ques-   tions for each utterance , depending on previous   answers . Because single - turn QA asks only one   question , it has lower latency ; however , the model   must generate a longer text representing the entire   parse tree . By contrast , multi - turn QA generates   shorter answers that are more straightforward to   parse ; however , all questions need to be answered   correctly , and if there are dependencies betweenquestions , they ca n’t be run within the same batch .   We study these two approaches and their tradeoffs   in both full - data and low - resource settings .   To summarize our contributions :   1.We propose a general reduction of composi-   tional TOP to abstractive QA and introduce   two specific variants : single - turn QA and   multi - turn QA , each with unique benefits .   2.We train the abstractive QA models with a   masked span prediction ( MSP ) objective , one   of the pretraining objectives of the seq2seq   model , which is shown to yield very substan-   tial improvements in few - shot scenarios .   3.We evaluate ST QA and MT QA on two pub-   lic datasets , Topv2 ( Chen et al . , 2020 ) and   Pizza ( Arkoudas et al . , 2021 ) , and show that   our results improve on the state of the art   by 3 % on full - data Topv2 , 28 % for few - shot   Topv2 , and 7 % on few - shot Pizza .   2 Related Work   Task - oriented parsing has been extensively stud-   ied in the literature . The most prevalent approach   is seq2seq modeling , which maps utterances to   their meaning representations , typically expressed   as a mixture of natural language and tokens such   as brackets and artificial intent and slot identi-   fiers ( Rongali et al . , 2020 ; Zhou et al . , 2021 ; Agha-   janyan et al . , 2020 ; Shrivastava et al . , 2021 ; Man-   simov and Zhang , 2021 ) ; we take this approach as   our main baseline .   Few - shot semantic parsing has also attracted   wide interest . Chen et al . ( 2020 ) applied a differ-   ent training paradigm ; they assumed there are sev-   eral source domains with labeled data and adopted   a first - order meta - learning algorithm , Reptile , to   train their model .   Shin et al . ( 2021 ) argued that PLMs are bet-   ter suited for directly generating natural language   rather than task - specific meaning representations ,   and thus they fine - tune PLMs to generate canonical   paraphrases , which can then be parsed by a context-   free grammar to produce the corresponding seman-   tic trees . They further improve performance by aug-   menting input sequences with similar examples as   prompts . Rongali et al . ( 2022 ) push that direction   further by leveraging small amounts of unannotated   data . We use canonical paraphrasing as one of our4419baselines . As we already noted in the introduction ,   canonical paraphrasing is not widely applicable in   open - world task - oriented parsing ; our QA - based   approach overcomes that limitation . Desai et al .   ( 2021 ) applied modifications to the linearized trees   to make them more natural .   Inspired by the recent success of QA - driven ap-   proaches to a wide range of NLP tasks , such as   dialogue state tracking ( Gao et al . , 2019 ) , named   entity recognition ( Li et al . , 2020 ) , and multi - task   learning ( McCann et al . , 2018 ; Du et al . , 2021 ) ,   Namazifar et al . ( 2021 ) framed semantic parsing as   an extractive QA task . This limits its scope to non-   compositional semantic structures . Our work is   the first to use QA for semantic parsing of arbitrar-   ily nested and complex meaning representations .   Moreover , in contrast to previous approaches , our   formulation usually results in fewer questions .   3 Reducing TOP to Abstractive QA   We now present a general method for reducing   compositional TOP to abstractive QA . Given an   utterance , our goal is to recover its semantic parse   tree by asking questions and parsing the answers   returned by a QA model . For this to succeed , all   questions associated with an input utterance need   to be answered correctly . At a high level , we for-   mulate questions so as to build the required parse   tree in a top - down , left - to - right fashion : We first   ask a question to determine the root node , and then   we recursively proceed towards the leaves . We start   by describing multi - turn QA ; the single - turn case   is discussed in Section 3.3 .   Each QA instance is a triple consisting of a con-   text , a question , and an answer . We use the parse   tree in Fig . 1 to illustrate the corresponding ( multi-   turn ) sequence of QA triples shown in Fig . 2 . The   context is the utterance provided by the user plus   general information about the domain and/or state   from previous turns ; the question corresponds to   a particular node of the parse tree ; and the answer   provides the content of that node . We first extract   the top - level intent ( get directions ) , then the corre-   sponding slots ( destination ) , then the value of each   slot ( destination is the nearest parking near S Beri-   tania Street ) , and then we start to recursively repeat   this process on the phrase the nearest parking near   S Beritania Street ( by asking what is the intent in-   cluded in that utterance segment ) . We note that our   system is able to handle negative answers : If there   is n’t a nested intent in an utterance segment , the   system returns none .   3.1 State Tracking   The triples we have described are processed inde-   pendently from one another . To provide richer in-   formation to the QA model , we include previous an-   swers as additional context ( shown in italicized red   font in Fig . 2 ) . We represent a previous question-   answer pair by combining them into a declarative   sentence , and we stack all previous states together .   This essentially encodes all parse - tree ancestors in   natural language , which can potentially help the   QA system resolve ambiguities . For example , the   nested intent in the nearest parking near S Berita-   nia Street could be different depending on whether4420this was a source or a destination .   3.2 Incorporating Domain Metadata   A given domain has predefined semantics , i.e. , a   fixed number of intents , each of which has an asso-   ciated set of slots that describe important aspects   of user requests . We incorporate metadata relevant   to the current node as additional context ( shown in   blue bold font in Fig . 2 ) . This effectively reduces   the space of possible answers . For instance , while   a domain may have a large number of intents , the   intents that can appear at a particular node ( e.g. ,   at the root level ) may be substantially fewer . And   because we explicitly list all possible intents , the   QA system can simply copy and paste the appro-   priate tokens , which is easier than searching over   the entire vocabulary .   3.3 Single - turn QA vs Multi - turn QA   We note that there is no need to limit ourselves   to one question for every node . For the example   of Fig . 2 , for instance , we do n’t ask one question   for each child node of IN : GET_LOCATION indi-   vidually ( e.g. , what is the first / second / third slot ) ;   instead , we simply ask “ what are the slots ? ” and   the answer should be “ location modifier , location   category . ” Another example is when we ask “ what   is the location modifier ? ” , the answer being “ near-   est ; near S Beritania Street . ”   On the extreme side , we could ask one ques-   tion that would return an answer representing the   entire parse tree , and this becomes somewhat sim-   ilar to canonical paraphrasing ( Shin et al . , 2021 ) .   However , canonical paraphrasing assumes there is   concrete grammar that specifies a controlled frag-   ment of natural language ( all and only the canonical   utterances ) , which can be used to map sentences   from that fragment into parse trees . That assump-   tion often fails in open - world TOP domains ; for   example , when someone asks for directions , the   destination could be expressed by an unbounded   number of phrases ( my parent ’s house , a restaurant   that satisfies an arbitrary set of constraints , etc . )   that can not be specified a priori by a closed - form   grammar .   In the single - turn part of Fig . 2 , we show how   we compress an entire parse tree into a single QA   triple . The bold blue context is again encoding the   domain ’s metadata . From left to right , the answer   explicitly lists the relevant intents , their associated   slots , and the tokens corresponding to the slots in a   top - down direction . We deal with nested intents byrecursively adding new sentences , which start with   the tokens under which the intent is nested .   While our approach is more general than canon-   ical paraphrasing , we still prefer canonicalization   when possible , as the corresponding fragments tend   to be more easily learnable . In the case of Pizza ,   for instance , a canonical grammar can be defined   fairly straightforwardly . We illustrate the use of   such canonical utterances in combination with our   QA approach in Fig . 3 . For multi - turn QA , we ask   one question for each order in the utterance , and the   answer is the order ’s canonical paraphrase . We also   include the previous answers when asking about   the next order , to prevent the QA system from re-   peating the same orders . For single - turn QA , we   ask only one question for all orders . We also in-   clude the canonical paraphrasing formulation for   comparison .   Thus , when canonical representations exist ,   single - turn QA and paraphrasing are similar ; the   main difference is that our formulation always in-   cludes a context . However , we reiterate that canon-   icalization is often not viable in TOP .   In summary , the general principle guiding the   design of multi - turn interactions is that we first   ask questions about intents , then we ask questions   about their slots and slot values , and then repeat   the process if we detect a nested intent in a slot .   As for single - turn QA , we only ask one question   and ensure that the answer encodes the entire tree .   When canonical grammars exist in a given domain ,   they can be used to train single - turn QA systems in   a straightforward way .   We experimentally evaluate the two QA variants   and show how one may be preferred over the other   under different settings .   3.4 Using MSP Objectives   Chada and Natarajan ( 2021 ) have shown that fine-   tuning pretrained seq2seq models to perform QA   tasks with too few examples leads to much de-   graded performance , while training a QA model   with a loss function directly aligned with the pre-   training objective performs better . Inspired by   this observation , we explore the following change   to our QA formulation : instead of making QA a   separate downstream task , we treat it as one of   the pretraining tasks — masked span prediction , for   which the models are trained to generate the entire   masked span given one unique mask token ( Raffel   et al . , 2020 ) . Accordingly , instead of asking the4421 : : :   model questions and having it generate arbitrary   answers , we rephrase the question - answer pair as a   declarative sentence where the answer is masked .   Thus , the model now has to denoise and recover the   masked segment . We show an example of this ap-   proach in Fig . 3 . The Q ’s are the original questions ,   whereas the primed Q ’s in red are the declarativesentences with the answers masked out . The an-   swers to both Q ’s and primed Q ’s are the same , as   mask tokens cover the exact answers .   3.5 Converting Answers to Parse Trees   Single - turn QA . In single - turn QA , each sen-   tence of the answer concerns up to a fixed number   of levels in the parse tree . Take the Topv2 instance   in the bottom part of Fig.2 as a running example ,   where each sentence contains three levels : The first   level is an intent node , the second level is its slots ,   and the third level is the value of each slot . In   the first sentence , which always corresponds to the   root intent , we take what follows the phrase “ The   user wants to ” as the intent , in the part before the   first comma ; the rest of the sentence is of the form   “ where SisV ” , where Sis the slot and Vis the   value . We note that one slot could have multiple   values separated by semicolons , and for each value   we create one slot node . In the case where an intent   node does n’t have slot children , the sentence sim-   ply stops after the first part . To expand subsequent   sentences into tree nodes , we make these sentences   start with “ The intent in [ subutterance ] is ” , so we   can traverse the parse tree to find the “ [ subutter-   ance ] ” node and expand the tree from there .   Multi - turn QA . In the multi - turn model , recon-   structing the parse tree is more straightforward .   We categorize all questions into three groups : the   first asking the intent for a ( sub-)utterance , the sec-   ond asking the slots that appear in an intent , and   the third asking for the value given to a slot . We   parse the questions for an utterance sequentially   and identify the group to which each question be-   longs . When the question asks for the top - level   intent , we build the root node . When it asks for   the intent of a sub - utterance , we traverse the parse   tree to look for the leaf node containing the exact   text and add an intent child node ( with the text as   a node attribute ) from there . For a question from   the second group , we find the intent node whose   attribute has the same sub - utterance and append   the slots as its children . When the question asks   for a slot value , we traverse the parse tree again ,   find which slot node they belong to , and add it as a   child node .   A final note for both single- and multi - turn QA :   When we detect invalid entities generated by the   QA models , we stop parsing and simply count such   an instance as an incorrect output.44224 Experiments   We evaluate our method against two state - of - the-   art seq2seq techniques , one generating linearized   parse trees and the other generating canonical para-   phrases . We chose Topv2 and Pizza because they   are the only two task - oriented datasets we are aware   of with nested meaning representations that can not   be produced by slot - filling approaches . We investi-   gate both multi- and single - turn QA . In the former ,   a question aims to recover one or several nodes in   the parse tree ; in single - turn QA , we only ask one   question to reconstruct the entire tree . Additionally ,   we perform ablation analysis to evaluate the contri-   bution of each component , as described in Sec . 3 .   We show that our method achieves superior perfor-   mance , particularly on those TOPv2 utterances that   have more nested semantics .   4.1 Datasets   The Topv2 Dataset . The Topv2 dataset ( Chen   et al . , 2020 ) is a collection of queries produced by   crowdsourced workers and intended for smart voice   assistants . Topv2 has compositional queries with   hierarchical meaning representations and extends   the original TOP dataset ( Gupta et al . , 2018 ) with   six additional domains . We present statistics for   each domain in Table 1 . The eight domains vary   widely , including the number of samples ( ranging   from 13k to 31k ) , the number of slots ( from 5 to   33 ) , and the portion of flat utterances . Accordingly ,   when we convert Topv2 instances to multi - turn   QA instances , the average number of questions per   instance varies a lot across the domains . We test our   QA approach on Topv2 under the full - data setting   for all domains , and select four domains to study in   a few - shot setting , with only 10 samples per intent   and slot ( 10SPIS ) .   The Pizza Dataset . Pizza is a new TOP   dataset ( Arkoudas et al . , 2021 ) consisting of com-   plex utterances that order pizzas and drinks . Pizza   consists of 2.4 M training examples that are syn-   thetically generated from a CFG , along with 348   dev examples and 1357 test examples generated   and annotated by MTurk workers . Although the   training set is large , we focus on few - shot settings   with 30 , 50 , and 100 examples randomly drawn   from the dev set . The low - resource setting is in-   deed challenging — the dev set has only 107 unique   slot values , whereas the test set has 180 , requiring   models to generalize well.4.2 Evaluation Metric   We use a standard metric for evaluating TOP per-   formance : unordered exact match accuracy ( abbre-   viated as EM ) , which does a node - to - node compari-   son between the generated parse tree and the golden   parse tree , modulo sibling order . EM does n’t take   partially correct parses into account , so given a   reference and a hypothesis , EM is either 0 or 1 .   4.3 Implementation Details   We use the T5 - large model ( Raffel et al . , 2020 ) as   the backbone of our QA framework . We choose a   learning rate between 5e-6 and 5e-4 , a batch size   from { 32 , 64 , 96 , 128 } for the full - data setting , and   a batch size from { 8 , 16 , 24 } for the few - shot set-   ting . We search for the best set of hyperparameters   with 16 random trials for each configuration . With   full data , we train single - turn models for 10 epochs   and multi - turn models for 30 epochs to select the   best - performing checkpoint on validation . For few-   shot learning , we train for 3000 steps and make a   checkpoint every 100 steps . We used 8 Tesla V100   GPUs with 32 GB memory for all our training .   4.4 Baselines   We consider two baselines . The first applies a   seq2seq model trained on logical forms ( LFs ) ex-   pressed in the “ TOP - decoupled format , ” which re-   moves from the parse tree all text that does not   appear in a leaf slot ( Aghajanyan et al . , 2020 ) . For   this baseline , we use the BART - Large model de-   scribed by Lewis et al . ( 2020 ) , because this model   is commonly used for TOP ( Rongali et al . , 2020 ;   Aghajanyan et al . , 2020 ; Shrivastava et al . , 2021 ) ,   and we also use T5 - large , because that is the back-   bone of our QA implementation . This allows us   to eliminate any benefit that may be derived from   the architecture itself . The second baseline method   we consider generates canonical paraphrases of the   original utterances ( Shin et al . , 2021 ) , and we use   T5 - large to allow for a direct comparison .   Note that for Topv2 we only compare our   method against the first baseline , since that dataset   does n’t have a canonical representation . Pizza does ,   so for that dataset we compare our method against   both baselines ( LFs in Top - decoupled notation and   canonical paraphrases ) . Since we ’re not aware   of any existing results on the baseline methods   for individual Topv2 domains and for Pizza , we   implemented both with HuggingFace ( Wolf et al . ,   2020 ) and performed hyperparameter tuning with4423   the same computation budget given to our method .   4.5 Main Results   We first present Topv2 results for both full - data   and few - shot settings . The baseline methods , la-   beled as BART / T5 LF , are trained on linearized   Top - decoupled trees . We include four variants of   the QA approach : ST QA and MT QA , as well as   MSP ST QA and MSP MT QA ( the same as ST   and MT QA , except that the models are trained   on the the masked span prediction objective ) . We   report the full - data EM scores in Table 2 and the   10SPIS EM scores in Table 3 . The relative gain is   computed between the best QA approach and the   best baseline approach in each domain .   In the full - data setting , all four QA variants out-   perform the baselines , with MSP ST QA having a   slight overall edge , exceeding the best baseline   method by 3.07 absolute points . We have the   largest relative gain ( 6.19 % ) in reminder . In gen-   eral , we see smaller improvements in the flatter   domains : music , weather , timer , alarm are the four   domains with the smallest semantic - tree depths ,   and relative gains for these domains are below 3 % .   We see that different QA approaches are close   to each other when we have enough data ; ST QA   performs only marginally better than MT QA . How-   ever , ST QA has shorter latency and may therefore   be practically preferable .   We choose four representative domains to per-   form low - resource experiments . For 10SPIS , MSP   MT QA is a clear winner over the other approaches ,   improving the best baseline method by 17.16 ab-   solute points . ST QA has the worst performance .   Our explanation is that ST QA requires the genera-   tion of long texts ( longer than the Top - Decoupled   LFs ) , and it is too challenging to learn a complex   new task with only 10 instances per intent and slot . Therefore , in a few - shot setting , the benefit of hav-   ing short answers is much clearer . Additionally ,   it is worth noting that having an objective that is   well - aligned with the pretraining stage provides a   significant benefit . It improves MT QA by nearly 5   absolute points on average , and it is a game changer   for ST QA , in that with this one modification ST   QA achieves competitive performance in three out   of four domains .   We also test whether the inclusion of previous   answers and metadata into the context helps in few-   shot scenarios ( the difference may be negligible   with full data ) . We remove answers to previous   questions ( “ w/o state ” ) and metadata ( “ w/o meta-   data ” ) from MT QA and report the scores obtained   from both changes . The results suggest that both   state and metadata make strong contributions to   performance . Excluding prior answers has a more   negative impact on average .   We note that our QA approach is very competi-   tive against state - of - the - art results in the literature .   For instance , the best reported result for 10SPIS   reminder is 61.47 ( Chen et al . , 2020 ) , which was   achieved by first pretraining the model with six   source domains . We improved that result by 9 ab-   solute points while training only with the target   domain ’s data .   We next report results on Pizza , where we com-   pare our approach against both LF - trained base-   lines ( with LFs expressed in TOP - decoupled nota-   tion ) and canonical paraphrases . Table 4 summa-   rizes results for three low - resource settings . We   obtain the greatest improvement with 30 training   examples , for a relative gain exceeding 8 % . Consis-   tently with the Topv2 results , as more training ex-   amples become available the gap between the QA   approaches and the baselines shrinks . Even though   T5 Canonical and ST QA are similar , the results   show that including context information about an   order is beneficial , especially when there are fewer   training examples . MT QA has a slight edge over   ST QA here , as asking more questions mitigates   the burden of generating longer and more complex   sequences .   4.6 Additional Analysis : Depth vs. Accuracy   We investigate how our approach performs on full-   data Topv2 as a function of semantic depth . We   chose to perform the analysis on navigation and4424   timer because navigation has the greatest average   tree depth , and we want to explain why our method   gave the lowest improvements on timer .   We present the breakdown in Table 5 , whereLF EM shows EM scores from the best baseline   method generating TOP - decoupled trees , and QA   EM shows EM scores from the best QA variant . For   utterances with shallow semantics , generating TOP-   decoupled trees outperforms QA ; but as we move   towards deeper semantic structures , the benefit of   using a more naturalized representation becomes   evident . Indeed , for both navigation and timer , QA   performs best at the deepest level . Thus , for prac-   tical purposes , when the goal is to build a system   that achieves high accuracy across all utterance   groups , it is worth considering a combination of   conventional LF generation for shallow utterances   and a QA model for more complex utterances . We   define the length of an utterance to be the number   of words the utterance has . Table 5 lists the aver-   age utterance length for each depth , and shows that   input size is a good proxy for semantic complexity   ( and could thus be used to quickly decide between   the two approaches ) .   5 Conclusion   We have presented a reduction of compositional   task - oriented parsing to abstractive QA , whereby   parse tree nodes are recovered by posing queries to   a QA model . We have also proposed to train QA   models with the MSP ( masked span prediction )   objective , to better leverage the massive amount of   linguistic knowledge gained during pretraining . We   experimentally evaluated single - turn QA and multi-   turn QA on two public datasets , both in full - data   and in few - shot settings , with and without MSP ,   and showed that they consistently outperform a   number of powerful baseline techniques , including   canonical paraphrasing , in both settings . The MSP   variants perform best on average , with particularly   dramatic improvements obtained in the few - shot   setting.4425References44264427