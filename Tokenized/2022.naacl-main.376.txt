  Rong Ye , Mingxuan Wang , Lei LiByteDance AI Lab , University of California , Santa Barbara   { yerong , wangmingxuan.89}@bytedance.com   leili@cs.ucsb.edu   Abstract   How can we learn uniﬁed representations   for spoken utterances and their written text ?   Learning similar representations for seman-   tically similar speech and text is important   for speech translation . To this end , we pro-   pose ConST , a cross - modal contrastive learn-   ing method for end - to - end speech - to - text trans-   lation . We evaluate ConST and a variety of   previous baselines on a popular benchmark   MuST - C. Experiments show that the proposed   ConST consistently outperforms the previous   methods , and achieves an average BLEU of   29.4 . The analysis further veriﬁes that ConST   indeed closes the representation gap of dif-   ferent modalities — its learned representation   improves the accuracy of cross - modal speech-   text retrieval from 4 % to 88 % . Code and   models are available at https://github .   com / ReneeYe / ConST .   1 Introduction   End - to - end speech - to - text translation ( E2E ST ) be-   comes important in many internet products and   real applications . An E2E ST system accepts au-   dio signals as the input and generates the target   translation using a single model . Compared with   the conventional cascade ST models , E2E ST mod-   els have achieved almost comparable ( Bentivogli   et al . , 2021 ) or even superior ( Ansari et al . , 2020 ;   Potapczyk and Przybysz , 2020 ; Xu et al . , 2021 )   performance .   The performance of an E2E ST model is still re-   stricted for languages with relatively small parallel   data , compared to text machine translation ( MT ) .   Existing approaches for ST focus on using addi-   tional data from MT and automatic speech recog-   nition ( ASR ) . This can be realized through pre-   training approaches ( Zheng et al . , 2021 ; Dong et al . ,   2021b , a ) or multi - task training frameworks ( Tang   et al . , 2021b ; Ye et al . , 2021 ; Han et al . , 2021 ) .   Figure 1 : Illustration of representations for speech and   transcript text ( projected to 2D ) . ( a ) representations   learned by existing models . Pairs of speech and text   representations are distant . ( b ) an ideal representation   that we expect , where different modalities with same   meaning should stay close to each other .   Different from the data perspective , this paper   investigates the bottleneck of E2E ST from the   neural representation perspective . We believe that   when the representation of audio input is similar to   its corresponding textual representation , it is easier   for information to transfer from MT to ST , thus   improving speech translation performance .   We analyze Transformer models for speech trans-   lation and observe a noticeable modality gap be-   tween encoder representations of speech and text   from existing ST models ( as in Figure 1a . Sec . 6   has more details ) . An ideal representation should   satisfy : if the content of the speech and tran-   scription are similar , their encoded representations   should likewise be close to each other ( as in Fig-   ure 1b ) . Nevertheless , how to learn uniﬁed and   aligned speech - text representations ?   Inspired by the recent progress of contrastive   learning approaches in cross - lingual ( Lample and   Conneau , 2019 ; Pan et al . , 2021 ) and cross - modal   vision - and - language domains ( Li et al . , 2021 ; Zhou   et al . , 2020 ; Dong et al . , 2019 ) , we designed a sim-   plecontrastive learning method for ST(ConST )   to learn the representations that meet the afore-   mentioned conditions explicitly . On the one hand ,   our model inherits the advantages of the previous5099multi - task learning methods . On the other hand ,   it reduces the gap between the representations of   speech and its corresponding transcription .   Our contributions are as follows .   •We develop ConST for speech translation , a   cross - modal contrastive learning method , on top   of the multi - task training framework .   •Our experiments on the MuST - C benchmark   show that ConST achieves an average BLEU   score of 29.4 , outperforming the best previous   baselines .   •We demonstrate that ConST indeed learns simi-   lar representations for two modalities and better   retrieves text with speech input .   2 Related Work   End - to - end ST To alleviate the error propaga-   tion in the cascaded ST systems and to make the   deployment simpler , Bérard et al . ( 2016 ) ; Weiss   et al . ( 2017 ) proposed to use an end - to - end archi-   tecture to directly translate speech into text in an-   other language , without the intermediate transcrip-   tion . Kano et al . ( 2017 ) ; Berard et al . ( 2018 ) ; In-   aguma et al . ( 2020 ) ; Wang et al . ( 2020a ) ; Zhao et al .   ( 2021a ) implemented several off - the - shelf encoder-   decoder E2E - ST models , such as BiLSTM ( Greff   et al . , 2016 ) and Speech - Transformer ( Dong et al . ,   2018 ) . However , training an end - to - end speech   translation model is difﬁcult because we need to   design a cross - modal cross - language model , mean-   while , the speech - transcription - translation super-   vised data for speech translation is signiﬁcantly   less than that of MT and ASR . Methods , like data   augmentation ( Park et al . , 2019 ; Pino et al . , 2020 ;   Chen et al . , 2021 ) , pre - training ( Weiss et al . , 2017 ;   Berard et al . , 2018 ; Bansal et al . , 2019 ; Wang   et al . , 2020b ; Alinejad and Sarkar , 2020 ; Dong   et al . , 2021a ; Zheng et al . , 2021 ) , self - training ( Pino   et al . , 2020 ; Wang et al . , 2021a ) , utilizing self-   supervised pre - trained audio representation ( Wu   et al . , 2020 ; Han et al . , 2021 ; Ye et al . , 2021 ; Wang   et al . , 2021a ) , are proved to be effective . Mean-   while , some work has shown that the encoder-   decoder model with a single encoder can not en-   code speech information well . For example , Dong   et al . ( 2021b ) ﬁrst proposed a second encoder to   further extract semantic information of the speech   sequence . Xu et al . ( 2021 ) proposed a stacked   acoustic - and - textual encoder and introduced large-   scale out - of - domain data . Also , multi - task frame-   works ( Le et al . , 2020 ; Tang et al . , 2021b ; Ye et al . ,2021 ) are widely applied to further enhance the ro-   bustness for ST . As a cross - modal task , some work   has noted the problem of the modality gap . ( Han   et al . , 2021 ) designed a ﬁx - size semantic memory   module to bridge such a gap , from the neuroscience   perspective . However , we ﬁnd that this approach   actually sacriﬁces the effect of MT . So in this pa-   per , we propose a simple yet effective contrastive   learning method to bridge the gap and to improve   ST performance .   Cross - modal grounding learning This paper at-   tempts to address the problem in speech translation   from the perspective of cross - speech - text repre-   sentation learning . We are also inspired by cross-   modal representation learning in the acoustic word   embedding ( AWE ) ( Palaskar et al . , 2019 ; Kam-   per et al . , 2020 ; Hu et al . , 2020 ) and the visual-   language pre - training ( VLP ) ( Wu et al . , 2019 ; Lu   et al . , 2019 ; Chen et al . , 2020b ; Li et al . , 2021 )   tasks . These works usually focus on enhancing tex-   tual representations with acoustic or visual infor-   mation , in other words , grounding learning . In this   work , we consider the its dual form , i.e. , grounding   speech representations using text .   Contrastive learning Our method is motivated   by the recent success in contrastive representa-   tion learning . The contrastive learning method   was ﬁrst proposed to learn representations from   unlabeled datasets ( hence the term , self - supervised   learning ) by telling which data points are similar   or distinct , especially in the ﬁeld of computer vi-   sion ( Chopra et al . , 2005 ; Gutmann and Hyvärinen ,   2010 ; Schroff et al . , 2015 ; Sohn , 2016 ; Oord et al . ,   2018 ; Chen et al . , 2020a ; Grill et al . , 2020 ) . Khosla   et al . ( 2020 ) extended the self - supervised batch con-   trastive approach to the fully - supervised setting and   proposed a supervised contrastive learning method .   In speech processing , representative methods fo-   cused on speaker identiﬁcation ( Ravanelli and Ben-   gio , 2018 ) , speech recognition ( Schneider et al . ,   2019 ) , and audio representation learning ( van den   Oord et al . , 2018 ; Baevski et al . , 2020 ) . In the   NLP area , the contrastive framework is used for   sentence representation learning ( Fang et al . , 2020 ;   Shen et al . , 2020 ; Gao et al . , 2021 ; Wu et al . , 2021 ;   Yan et al . , 2021 ; Fu et al . , 2022 ) , machine transla-   tion ( Pan et al . , 2021 ) , and summarization ( Wang   et al . , 2021b ; Cao and Wang , 2021 ) . Very recently ,   contrastive learning is also applied to learning a uni-   ﬁed representation of image and text ( Dong et al . ,   2019 ; Zhou et al . , 2020 ; Li et al . , 2021 ) . Moti-5100   vated by the contrastive learning frameworks in   cross - lingual and cross - modal topics , we introduce   a similar idea in speech translation .   3 The ConST Approach   An end - to - end speech translation model directly   translates audio sequence s= ( s, ... ,s)to   the text y= ( y, ... ,y)in the target language .   Speech translation corpus D={(s , x , y)}pro-   vides transcript x= ( x, ... ,x)in the source   language , as well .   In this section , we present the overall speech   translation model and cross - modal contrastive   learning . We also provide several feasible strate-   gies to construct more positive and negative pairs   to enhance the contrastive learning .   3.1 Model Framework   We use the same model architecture as XSTNet ( Ye   et al . , 2021 ) . Our model consists four sub - modules :   a speech encoder , a word embedding layer , a Trans-   former Encoder and a Transformer decoder ( Fig-   ure 2 ) . It is designed to take either speech or a   sentence as input , and to output either source tran-   script or target translation text . Such architecture   enables a universal framework for multiple tasks ,   including ST , MT and ASR .   Thespeech encoder module ( S - Enc ) is designed   to extract low - level features for speech signals . It   contains Wav2vec2.0 ( Baevski et al . , 2020 ) and   two additional convolutional layers . The input is   raw waveform signal sampled at 16kHz . Each con-   volutional layer has a stride of 4 and dchannels . In   total , it shrinks the time dimension by a factor of 4.Denote a = S - Enc ( s)as the audio representation   of the speech,|a|/lessmuch|s| .   Parallel to the speech encoder is the word em-   beeding layer . It is the same as word embedding   for text translation .   Both the speech encoder and word embedding   layer are connect to Transformer encoder and then   passed to the Transformer decoder . The Trans-   former encoder and decoder are using the same   conﬁguration as the original ( Vaswani et al . , 2017 ) .   To explain , the Transformer encoder further ex-   tracts the high - level semantic hidden representation   of two modalities . The Transformer decoder gener-   ates the word sequences ( transcription and transla-   tion ) for ST , MT and ASR tasks . Since our model   has a complete Transformer encoder - decoder as   a sub - module , this makes it possible to pre - train   using large - scale extra MT parallel data .   Previous work has shown that multi - task learn-   ing on ST , MT and ASR improves translation per-   formance ( Indurthi et al . , 2020 ; Tang et al . , 2021b ;   Ye et al . , 2021 ) . Our training loss consists of the   following elements .   L = L+L+L+λL ( 1 )   where   L=−/summationdisplaylogP(y|s )   L=−/summationdisplaylogP(x|s )   L=−/summationdisplaylogP(y|x)5101The ﬁrst three elements are cross - entropy losses   on < speech , target text > , < speech , source text >   and < source text , target text > pairs . These pairs   are built from the triplet ST data . We also intro-   duce a cross - modal contrastive loss term L(see   Section 3.2 for details ) . It aims to bring the repre-   sentation between the speech and textual transcrip-   tion modalities closer ( its effect will be analyzed in   detail in Section 6 ) . λis a tuned hyper - parameter   of the weighted contrastive loss term .   3.2 Cross - modal Contrastive Learning   As mentioned in the beginning , since we need to   produce similar representations for the speech and   transcript sharing the same semantic meanings , we   propose cross - modal contrastive learning method   to bring their representations closer together . The   main idea of cross - modal contrastive learning is   to introduce a loss that brings speech and its cor-   responding transcript ( positive example ) near to-   gether while pushing irrelevant ones ( negative ex-   amples ) far apart .   Given a positive example of such a speech-   transcript pair ( s , x ) , we randomly pick a set of   N−1transcripts{x}from the same batch   as negative examples . For speech sand its tran-   script x , we ﬁrst average them in terms of the time   dimension ,   u = MeanPool ( S - Enc ( s ) ) ( 2 )   v = MeanPool ( Emb(x ) ) ( 3 )   and apply the multi - class N - pair contrastive   loss ( Sohn , 2016 ):   L=−/summationdisplaylogexp(sim(u , v)/τ)/summationtextexp(sim(u , v(x))/τ )   ( 4 )   whereA={x}∪{x},τis the temperature   hyper - parameter , and sim is the cosine similarity   functionsim(a , b ) = ab//bardbla / bardbl / bardblb / bardbl . In the imple-   mentation , negative examples { x}are from   the same training batch of data ( Figure 2(b ) ) .   3.3 Mining Hard Examples for Contrastive   Learning   To further enhance the contrastive learning , we   introduce three strategies to mine additional hard   examples . These strategies are at input and rep-   resentation ( gray shaded modules in Figure 2(a ) ) .   Speciﬁc schematic illustrations of each operations   are shown in Figure 3 .   Span - Masked Augmentation We mask consec-   utive segments of an original audio waveform se-   quence sto obtain a new modiﬁed speech s. We   takesas an input to the model , and compute   the contrastive loss on its original corresponding   transcript . We randomly sample without replace-   ment all time steps in the original waveform of   the speech to be the starting indices with a prob-   abilityp , and then we set the sub - sequence M   successive time steps to be blank . In the exper-   iment , we tried multiple conﬁgurations , and found   p= 0.25,M = 3600 the best , resulting in a   masked span of 0.225second . Since the masked   speech fragment is very short , we consider the   masked speech and the original transcript to be   positive pairs , and the remaining transcripts in the   same batch to be negative pairs .   Word Repetition The word repetition strategy ran-   domly replicates some words ( or sub - words ) in the   original sentences , with two advantages for improv-   ing representation robustness . First , as the length   of the sentence is shorter than that of its audio   representation , randomly repeating the words in   the sentence is a simple yet useful technique to   increase the length . Second , repeating words does   not change the semantics and is suitable as an ex-   tra positive example of the corresponding speech .   Speciﬁcally , given sentence x , each sub - word to-   kenxcan be duplicated kmore times , resulting   in the duplicated sentence x , wherek= 0,1,2 , ...   andk∼Poisson ( 1 ) . We regard xas the additional   positive example for the speech sand the samples   with the same operation in the same batch as the   negative examples .   Cut - off strategy Recent studies on natural lan-   guage understanding and generation have proved   cut - off strategy to be successful ( Shen et al . , 2020 ;   Yan et al . , 2021 ) . We analogize a similar idea to the   cut - off approach for speech representation . We en-5102tirely erase a slice of the T×drepresentation matrix   along each dimension and set the erased terms to 0 .   Here , we present two variants : sequence cut - off ,   which erases some sequence dimension , and fea-   ture cut - off , which erases some feature dimension .   Note that there is a difference between cut - off and   dropout . Dropout randomly sets some elements to   0 , while cut - off is a dimensional “ block " dropout .   Similarly , we treat the cut - off audio representation   and the original transcribed sentence as positive   pairs , and the rest sentences in the same batch as   negative pairs .   4 Experiments   4.1 Experimental Setups   ST datasets We conduct experiments on   all the translation directions in MuST - C   dataset(Di Gangi et al . , 2019 ): English ( En ) to   German ( De ) , Spanish ( Es ) , French ( Fr ) , Italian   ( It ) , Dutch ( Nl ) , Portuguese ( Pt ) , Romanian ( Ro )   and Russian ( Ru ) . As one of the largest ST   benchmarks , MuST - C contains more than 385   hours of TED talks for each direction .   MT datasets We also introduce external WMT   datasets ( Bojar et al . , 2016 ) for En - De / Es / Fr / Ro / Ru   andOPUS100 datasets ( Zhang et al . , 2020 ) for En-   It / Nl / Pt directions , as the expanded setup .   Table 8 ( in Appendix . A ) lists the statistics of all   the datasets included .   Model Conﬁgurations The Wav2vec2.0 in the S-   Enc is only pre - trained on Librispeech ( Panayotov   et al . , 2015 ) speech without any downstream ﬁne-   tuning . Two layers of CNNs after the Wav2vec2.0   are set to kernel size 5 , stride size 2 and hidden   size 512 . The Transformer follows the base con-   ﬁguration , with 6 layers of encoder and decoder ,   hidden sized= 512 , 8 attention heads , and 2048   FFN hidden states . We use pre - layer normalization   for stable training . The model with the above con-   ﬁgurations has a total of about 150 M parameters .   Experiment Details We evaluate case - sensitive   detokenized BLEU using sacreBLEU(Post , 2018 )   on MuST - C tst - COMMON set . In the analysis ,   we also report the ChrF++ score(Popovi ´ c , 2017)and the learning - based BLEURT score . We use   the raw 16 - bit 16kHz mono - channel speech input .   We jointly tokenize the bilingual textusing Sen-   tencePiece ( Kudo and Richardson , 2018 ) , with a   vocabulary size of 10k , which is the same as Ye   et al . ( 2021 ) ’s setup . For the training loss , we set   contrastive temperature τ= 0.02and weight of   contrastive term λ= 1.5for German and Dutch ,   andλ= 1.0for the other languages .   Appendix B contains more detailed settings and   explanations for the baseline models in Table 1 .   Appendix C shows the experiments on the choice   of the hyper - parameters .   4.2 Main Results   Comparison with end - to - end ST models Table 1   shows the main results . Since many existing works   regard “ leveraging external data ” to be one of their   model ’s features , their strong performances are   largely predicated on the utilization of auxiliary   data , especially large - scale MT data . For a rela-   tively fair comparison , we investigate two cases :   ( 1 ) without external MT data and ( 2 ) with exter-   nal MT data . Without the external MT data , our   method already gains an average improvement of   0.5 BLEU over the previous best models . Also   when speech data is introduced for pre - training ,   our method works better than others ( Self - training ,   W - Transf . and XSTNet ) . When extra MT data are   introduced , our method also outperforms SOTA by   an average of 0.6 BLEU . Among the benchmark   models , with the same goal of closing two modal-   ity gaps , Chimera ( Han et al . , 2021 ) constructed   an extra ﬁxed - length shared semantic space . How-   ever , the shared memory with ﬁxed size actually   compromises the MT performance , while our con-   trastive learning approach is more straightforward   and effective .   Comparison with cascaded ST systems We com-   pare our method with several cascade baselines ,   where Ye et al . ( 2021 ) and Xu et al . ( 2021 ) provided   two strong cascade systems trained using MuST-   C and external ASR and MT data ( LibriSpeech ,   WMT , and Opensubtitles ) . From Table 2 , we ﬁnd   that as an end - to - end model , ConST can outper-   form these strong cascade models . In Appendix 7 ,   we provide a case study to show such improvement.5103   5 Analysis   5.1 Is contrastive loss effective ?   With the same model architecture and the same pre-   training + ﬁne - tuning procedure , the main differ-   ence between ConST and XSTNet ( Ye et al . , 2021 )   is whether we use the contrastive loss term during   the ﬁne - tuning or not . Comparing the BLEU results   of w/o and w/ external MT data situations in Ta-   ble 1 , we ﬁnd that ConST further improves 0.5and   0.6BLEU scores in terms of eight translation direc-   tions on average , which proves the effectiveness of   the cross - modal contrastive learning . By gradually   removing each losses in Eq . ( 1 ) , Table 3 shows the   improvements bringing by the multi - task learning   and the contrastive learning . For En - De translation   direction , contrastive learning can bring an averageExternal MT   Conﬁg . without with   ConST 25.7 28.3   −L−L 24.6 27.0   −L−L−L 23.6 26.3   improvement of 0.9 BLEU over the baseline mod-   els by only optimizing L(corresponding to the   last row of the Table 3 ) , and multi - task learning can   lead to a further improvement of about 1.2 BLEU   on top of that .   5.2 Which layer to contrast on ?   An intriguing question is which representations   should be considered in the contrastive loss func-   tion . In the method part ( Section 3.2 ) , we use aver-   aged audio representation ufor speech s(Eq.(2 ) )   and averaged lexical embedding vfor the transcript   x(Eq.(3 ) ) , denoted as low - level repr . . Whereas   inspired by a recent study in multilingual MT ( Pan   et al . , 2021 ) , we also provide an alternative con-   trastive loss as a comparison , whose speech and   text features are average - pooled semantic repre-5104   sentations derived from the Transformer encoder ,   denoted as high - level repr . .   Table 4 shows that contrastive learning using   the low - level representations ( Line 1 ) is better   than using the high - level ones ( Line 2 ) . On the   other hand , although the performance of Line 2 is   relatively inferior , it still outperforms the multi - task   model without the contrastive loss ( Line 3 ) . The   detailed analysis of possible explanations will be   shown in Section 6.2 .   5.3 Is contrastive loss better than other   losses ?   Our goal for introducing the contrastive loss term   ( denoted as CTR Loss ) is to close the distance be-   tween speech and text representations . Whereas ,   there are other options to achieve this goal , such as   L2 loss and CTC loss .   •L2 Loss : Without introducing any negative sam-   ples , L2 loss directly reduces the Euclidean dis-   tance between the representations of two modali-   ties by minimizing L=/bardblu−v / bardbl . L2 loss can   be viewed as an implementation based on the   idea of knowledge distillation ( Heo et al . , 2019 ;   Dong et al . , 2021b ) .   •CTC Loss : The connectionist temporal classiﬁ-   cation ( CTC ) loss ( Graves et al . , 2006 ) is com-   monly used in speech - related tasks ( Xu et al . ,   2021 ; Dong et al . , 2021b ) . Unlike contrastive   loss that cares about the representation , CTC   loss connects the two modalities by establishing   speech - text alignment and maximizing p(x|a ) = /summationtext / producttextp(π|a ) , where Πis the set of   all valid alignments .   Compared to the other two ways of bridging the   modality gap , L2 and CTC loss , is the contrastive   loss term better ? The answer is yes according to the   results in Table 5 . Our explanation is that informa-   tion on the negative samples beneﬁts the contrastive   loss , bringing the the distance between the speech   and its corresponding transcription closer while   pushing the distance to the irrelevant text farther .   5.4 Analysis on the hard example mining   strategies   In Section 3.3 , we proposed four methods to mine   the hard examples for contrastive learning , namely   span - masked augmentation ( SMA ) , word repeti-   tion ( Rep ) , sequence cut - off ( SCut ) , and feature   cut - off ( FCut ) . In this section , we study how effec-   tive these methods are , and to do so , we consider   the BLEU performances of their 15 combinations   ( Figure 4 ) . Note that “ Original ” means the original   contrastive loss in Eq.(4 ) without any additional   hard examples mining operation , and the diagonal   in the heat map represents only one strategy used .   For an easy and fair comparison , we set the weight   of the contrastive term to 1.0 uniformly . We have   the following observations .   All the hard examples mining methods are ef-   fective . All the BLEU scores in Figure 4 exceed   the strong multi - task model trained without con-   trastive learning ( 27.1 ) . Among all the strategies ,   the combination of the original and SCut reaches5105   the best result ( 28.3 ) , and is better than the model   without any expanded operations ( p<0.01 ) . Gen-   erally , to ﬁnd the best model , we suggest adopting   multiple strategies and choosing the best check-   point on the dev - set .   The combinations of the hard examples min-   ing methods and the “ original ” have relatively   better performances . We argue that we need   the original positive and negative examples to   give more accurate representations ( without any   dropout ) for contrastive learning . On the contrary ,   without the help of “ original ” loss , the performance   with both sequence cut - off and feature cut - off is   the worst in Figure 4 , probably because too much   information is lost by superimposing the two .   6 Why does cross - modal contrastive   learning work ? — Analysis on the   Modality Gap   As mentioned earlier , the existing multi - task train-   ing models can not address the speech - text modality   gap . Does ConST reduce the representation gap   between speech and text ?   6.1 Visualization of Representation   Does the speech - text modality gap exist without   explicitly bridging the two ? Speech - text modality   gapmeans the discrepancy between the audio repre-   sentations and transcription sentence embeddings .   To visualize it , we plot the bivariate kernel den-   sity estimation ( Parzen , 1962 ) ( KDE ) contour of   their dim - reduced features , where T - SNE ( Van der   Maaten and Hinton , 2008 ) is used to reduce the   dimension into two ( Figure 5 ) . Ideally , if the rep-   resentations of speech and its corresponding tran - script are similar , their KDEs will be similar , and   thus the contour lines will overlap as much as pos-   sible . However , Figure 5(a ) is the KDE contour of   the multi - task framework without any explicit mod-   eling to bring two modalities together ( Ye et al . ,   2021 ) . It shows that the representations are so dis-   similar that they are organically divided into two   clusters , i.e. speech - text modality gap exists .   Does ConST reduce the modality gap ? As   shown in Figure 5(b ) , compared to the baseline   model without contrastive learning , ConST with   cross - modal contrastive learning is able to bring   representations of different modalities much closer .   This means that the audio representation contains   more linguistic information similar to that of the   textual transcription , which is more advantageous   for the downstream ST generation through the   shared Transformer encoder anddecoder .   6.2 Cross - modal Retrieval   How good is the cross - modal representation   space learned from ConST ? To answer this ques-   tion , we conduct a retrieval experiment , i.e.ﬁnd-   ing the nearest ( smallest cosine similarity ) tran-   script based on the speech representation . We com-   pare ConST model with the baseline without cross-   modal contrastive learning and report the top-1 re-   trieval accuracy using ( 1 ) the low - level represen-   tations and ( 2 ) the high - level semantic representa-   tions , in Table 7 .   When retrieving the text using low - level rep-   resentations , our method gains a substantial 79 %   increase over the baseline . In addition , we ﬁnd that   without explicit contrastive modeling , the baseline   can achieve retrieval accuracy of more than 94 % ac-   cording to the semantic representations outputted   from the Transformer encoder . We believe that   such high accuracy is automatically learned from   the triple - supervised data itself under the multi - task   learning framework . With such a degree of cross-   modal alignment , if we construct the contrastive   loss with semantic representations , its gain to the   ST performance turns out to be limited , which   exactly corroborates the ﬁndings in Section 5.2 –   low - level representations are preferred in the cross-   modal contrastive learning .   7 Case Analysis   In this section , we use several cases that ConST   generates . We compare our model with the cas-   caded model and the previous end - to - end model,5106   Representations CTR loss Acc .   low - level repr.× 9.4   /check 88.6   high - level repr.× 94.7   /check 95.0   XSTNet ( Ye et al . , 2021 ) .   For this ﬁrst case , the cascaded system fails to   give a right translation due to the mis - punctuation   issue ( klingt is a verb ) , while the end - to - end model ,   XSTNet and ConST translate correctly . For the sec-   ond case , the previous end - to - end XSTNet model   can not accurately translate the phrase “ started ex-   ploring this idea of ” , which performs worse than   the cascaded one . Whereas ConST successfully   conveys the meaning of “ this idea ” , and translates   more accurately than XSTNet . We believe this im-   provement comes from the cross - modal contrastive   learning .   8 Conclusion   In this paper , we propose ConST , a simple yet ef-   fective contrastive learning framework bridging the   speech - text representation gap and facilitating theST with limited data . We also provide feasible hard   example mining methods to learn robust representa-   tions . The results on the MuST - C ST dataset prove   the effectiveness of the method .   9 Broader Impact   This work improves the performance of ST tasks on   public datasets by learning speech representations   that are more similar to text representations , but   the model is far from being achieved for industrial-   grade implementations . In real scenarios , for exam-   ple , the original voice is noisier and the distribution   of speech lengths is more complex than in the pub-   lic dataset , which can not be handled by an end - to-   end model alone . The shortcoming of this model is   that it still needs a certain amount of labeled data   for training , especially < speech , transcription > to   learn better speech representation , and for the more   than 7,000languages and dialects in the world ,   most of them do not have corresponding transla-   tions or even transcriptions , our method does not   work in untranscribed scenarios . In this paper , we   focus on the improvement brought by the better   speech representation on the ST task , and obtained   good results with hundreds of hours of speech data .   We hope that our work achieves better results using   more data ( e.g. raw speech , raw text , ASR , MT   data ) in the future.5107References5108510951105111A Statistics of all datasets   ST ( MuST - C ) MT   En→ hours # sents name # sents   De 408 234 K WMT16 4.6 M   Es 504 270 K WMT13 15.2 M   Fr 492 292 K WMT14 40.8 M   It 465 258 K OPUS100 1.0 M   Nl 442 253 K OPUS100 1.0 M   Pt 385 211 K OPUS100 1.0 M   Ro 432 240 K WMT16 0.6 M   Ru 489 270 K WMT16 2.5 M   B Experimental Details   Training and Implementation Details We use   Adam optimizer ( β= 0.9,β= 0.98 ) with learn-   ing rate = 1eand warmup 25k steps during the   ST training . We also implement the expanded set-   ting with the introduction of external WMT to train   the Transformer module . In the pre - training stage ,   we set the learning rate = 7eand warmup 4000   steps . For robust training , we set label smoothing to   0.1 , and dropout rate to 0.1 . The hyper - parameters   for different data augmentation methods are as fol-   lows : for masked audio span strategy , we set mask-   ing probability p= 0.25and masking span length   M= 3600 frames ; for both sequence and feature   cut - off , we set the cut - off dropout rate as 0.1 . We   save the checkpoint with the best BLEU on dev - set   and average the last 10 checkpoints . For decoding ,   we use a beam size of 10 and length penalty 0.7for   German , 1.0for French , and 0.4for Russian . We   train the models in 8 Nvidia Tesla V100 GPUs for   each experiment . We use Fairseq ( Ott et al . , 2019 )   as the code - base for our implementation .   Baseline Models In Table 1 , we compared   our method with end - to - end baseline models   whose audio inputs are 80 - channel log Mel-ﬁlter   bank , including : FairseqST ( Wang et al . , 2020a ) ,   NeurST ( Zhao et al . , 2021a ) , Espnet ST ( In-   aguma et al . , 2020 ) , Dual - decoder Transformer ( Le   et al . , 2020 ) , SATE ( Xu et al . , 2021 ) , Speech-   former ( Papi et al . , 2021 ) , self training ( Pino et al . ,   2020 ) and mutual learning ( Zhao et al . , 2021b )   method , STAST ( Liu et al . , 2020b ) , bi - KD ( In-   aguma et al . , 2021 ) , MLT method ( Tang et al . ,   2021b ) , Lightweight Adaptor ( Le et al . , 2021 ) ,   JT - S - MT ( Tang et al . , 2021a ) , FAT - ST ( Zhenget al . , 2021 ) , TaskAware ( Indurthi et al . , 2021 ) ,   and STPT ( Tang et al . , 2022 ) . We also compare   our method to baseline models that have pretrained   Wav2vec2.0 as a module , including :   •W - Transf . ( Ye et al . , 2021 ): the model has the   same structure as ours , but is only trained on   < speech , translation > parallel data .   •Chimera - ST ( Han et al . , 2021 ): the model that   builds a shared semantic memory for both audio   and text modalities .   •XSTNet ( Ye et al . , 2021 ): the model has the   same structure as ours , and adopted a multi - task   ﬁne - tuning strategy .   •STEMM ( Fang et al . , 2022 ): the model that   bridges the modality representation gap by mini-   mizing the Jensen – Shannon divergence between   the original speech representation and the mani-   fold mix - up representation .   C The Choice for Hyper - parameters   Inﬂuence of Temperature In the contrastive loss ,   the temperature hyper - parameter is provided to con-   trol the smoothness of the distribution normalized   by softmax operation . A high temperature helps   to smooth the distribution , making it more difﬁcult   for the model to distinguish between positive and   negative samples ( corresponding to correct tran-   scriptions and other transcriptions in this work ) ,   while the low temperature behaves just the opposite .   We choose several temperature hyper - parameters   ranging from 0.01to0.5 , and Figure 6 shows their   BLEUs on the test and dev sets . We ﬁnd that ( 1 )   the choice of the temperature does not drastically   affect the ﬁnal BLEU score , and ( 2 ) we recommend   that the temperature τbe set between 0.02 and 0.05   to ensure a relatively good ST performance . In the   experiment , we use τ= 0.02.5112Inﬂuence of Contrastive Loss Weight The total   loss we optimize , Eq.(1 ) , is a linear combination of   the multi - task cross - entropy losses Land the   contrastive termL. To investigate how much   the contrastive terms affect BLEU , we ﬁx its tem-   peratureτ= 0.02 , adjust the values of its loss   weightλfrom 0.1 to 2.0 , performed three experi-   ments for each value , and test the average BLEU on   En - De tst - COMMON set . Figure 7 depicts the per-   formances . First , all objective functions containing   L , even if their weights λtake different values ,   are apparently better than the baseline model with   LonlyL. Then , the best BLEU score is   achieved at loss weight λ= 1.5 , corresponding   to the results in Table 1 . And when analyzing the   effect of data augmentation strategies ( Section 5.4 ) ,   since we need to consider the combination between   them , which is more complicated . Therefore , we   set the loss weight to 1.0uniformly for simplicity .   In general , we recommend that the weight hyper-   parameter takes a value between 0.8and1.5 .   D Data Scale for Fine - tuning   The experiments in the main paper show that our   model can perform well by introducing external   MT data pre - training . Here , we simulate the sce-   nario with plenty of MT and speech data and lim-   ited ST triple - labeled data , and does ConST have   the ability of low - resource learning ? In the ex-   periment , we reduce the labeled ST data to 1 , 10 ,   and 100 hours , corresponding to sentence counts of   about 500 , 5k , and 50k sentences . For a fair com-   parison , we use the same MT pre - trained Trans-   former module as in the main paper . We ﬁnd   the contrastive loss particularly helpful when the   amount of speech data is extremely small , like only1 hour of speech . Second , the multi - task training   strategy is also very effective in improving the ro-   bustness of the model performance . We also ﬁnd   that by using easily accessible MT and speech pre-   training , our model could reach the previous base-   line results without pre - training using only 1/4of   the original data , i.e.100hours of labeled ST data.5113