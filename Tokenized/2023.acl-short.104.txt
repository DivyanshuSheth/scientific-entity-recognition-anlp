  Nguyen Duc - Thang†Hoang Thanh - Tung†Quan Tran‡   Huu - Tien Dang †Nguyen Ngoc - Hieu †Anh Dau †Nghi Bui †   †FPT Software AI Center ‡Adobe Research   { nguyenducthang8a2 , htt210 , quanthdhcn}@gmail.com   Abstract   Influence functions ( IFs ) are a powerful tool for   detecting anomalous examples in large scale   datasets . However , they are unstable when ap-   plied to deep networks . In this paper , we pro-   vide an explanation for the instability of IFs and   develop a solution to this problem . We show   that IFs are unreliable when the two data points   belong to two different classes . Our solution   leverages class information to improve the sta-   bility of IFs . Extensive experiments show that   our modification significantly improves the per-   formance and stability of IFs while incurring   no additional computational cost .   1 Introduction   Deep learning models are data hungry . Large mod-   els such as transformers ( Vaswani et al . , 2017 ) ,   BERT ( Devlin et al . , 2019 ) , and GPT-3 ( Brown   et al . , 2020 ) require millions to billions of training   data points . However , data labeling is an expensive ,   time consuming , and error prone process . Popular   datasets such as the ImageNet ( Deng et al . , 2009 )   contain a significant amount of errors - data points   with incorrect or ambiguous labels ( Beyer et al . ,   2020 ) . The need for automatic error detection tools   is increasing as the sizes of modern datasets grow .   Influence function ( IF ) ( Koh and Liang , 2017 )   and its variants ( Charpiat et al . , 2019 ; Khanna et al . ,   2019 ; Barshan et al . , 2020 ; Pruthi et al . , 2020 ) are a   powerful tool for estimating the influence of a data   point on another data point . Researchers leveraged   this capability of IFs to design or detect adversarial   ( Cohen et al . , 2020 ) , poisonous ( Koh et al . , 2022 ;   Koh and Liang , 2017 ) , and erroneous ( Dau et al . ,   2022 ) examples in large scale datasets . The intu-   ition is that these harmful data points usually have   a negative influence on other data points and this   influence can be estimated with IFs .   Basu et al . ( 2021 ) empirically observed that IFs   are unstable when they are applied to deep neu - ral networks ( DNNs ) . The quality of influence   estimation deteriorates as networks become more   complex . In this paper , we provide empirical and   theoretical explanations for the instability of IFs .   We show that IFs scores are very noisy when the   two data points belong to two different classes but   IFs scores are much more stable when the two data   points are in the same class ( Sec . 3 ) . Based on   that finding , we propose IFs - class , variants of IFs   that use class information to improve the stability   while introducing no additional computational cost .   IFs - class can replace IFs in anomalous data detec-   tion algorithms . In Sec . 4 , we compare IFs - class   and IFs on the error detection problem . Experi-   ments on various NLP tasks and datasets confirm   the advantages of IFs - class over IFs .   2 Background and Related work   We define the notations used in this paper . Let   z= ( x , y)be a data point , where x∈ X is   the input , y∈ Y is the target output ; Z=/braceleftbig   z / bracerightbigbe a dataset of ndata points ; Z=   Z\zbe the dataset Zwith zremoved ; f :   X → Y be a model with parameter θ;L=/summationtextℓ(f(x),y ) = /summationtextℓ(z;θ)be   the empirical risk of fmeasured on Z , where   ℓ:Y × Y → Ris the loss function ; ˆθ=   arg minLandˆθ= arg minLbe the   optimal parameters of the model ftrained on Z   andZ. In this paper , fis a deep network and   ˆθis found by training fwith gradient descent on   the training set Z.   2.1 Influence function and variants   The influence of a data point zon another data   point zis defined as the change in loss at z   when zis removed from the training set   s=ℓ(z;ˆθ)−ℓ(z;ˆθ ) ( 1 )   The absolute value of smeasures the strength of   the influence of zonz . The sign of sshow1204the direction of influence . A negative smeans   that removing zdecreases the loss at z , i.e.z   is harmful to z.shas high variance because   it depends on a single ( arbitrary ) data point z.   To better estimate the influence of zon the entire   data distribution , researchers average the influence   scores of zover a reference set Z   s=1   |Z|/summationdisplays = L− L(2 )   sis the influence of zon the reference set Z.   Zcan be a random subset of the training set or   a held - out dataset . Naive computation of sre-   quires retraining fonZ. Koh and Liang ( 2017 )   proposed the influence function ( IF ) to quickly es-   timate swithout retraining   s≈IF(z , z )   ≈1   n∇ℓ(z;ˆθ)H∇ℓ(z;ˆθ)(3 )   where H=/is the Hessian at ˆθ . Exact   computation of His intractable for modern net-   works . Koh and Liang ( 2017 ) developed a fast al-   gorithm for estimating H∇ℓ(z;ˆθ)and used   only the derivatives w.r.t . the last layer ’s parameters   to improve the algorithm ’s speed . Charpiat et al .   ( 2019 ) proposed gradient dot product ( GD ) and gra-   dient cosine similarity ( GC ) as faster alternatives to   IF . Pruthi et al . ( 2020 ) argued that the influence can   be better approximated by accumulating it through   out the training process ( TracIn ) . The formula for   IFs are summarized in Tab . 3 in Appx . A.   IFs can be viewed as measures of the similarity   between the gradients of two data points . Intu-   itively , gradients of harmful examples are dissimi-   lar from that of normal examples ( Fig . 1 ) .   2.2 Influence functions for error detection   In the error detection problem , we have to detect   data points with wrong labels . Given a ( potentially   noisy ) dataset Z , we have to rank data points in   Zby how likely they are erroneous . Removing   or correcting errors improves the performance and   robustness of models trained on that dataset .   Traditional error detection algorithms that use   hand designed rules ( Chu et al . , 2013 ) or simple   statistics ( Huang and He , 2018 ) , do not scale well   to deep learning datasets . Cohen et al . ( 2020 ) ;   Dau et al . ( 2022 ) used IFs to detect adversarial and   erroneous examples in deep learning datasets . Dauet al . ( 2022 ) used IFs to measure the influence of   each data point z∈ Z on a clean reference set   Z. Data points in Zare ranked by how harmful   they are to Z. Most harmful data points are re-   examined by human or are removed from Z(Alg . 2   in Appx . A ) . In this paper , we focus on the error   detection problem but IFs and IFs - class can be used   to detect other kinds of anomalous data .   3 Method   3.1 Motivation   Basu et al . ( 2021 ) attributed the instability of   IFs to the non - convexity of DNNs and the errors   in Taylor ’s expansion and Hessian - Vector product   approximation . In this section , we show that the   learning dynamics of DNNs makes examples from   different classes unrelated and can have random   influence on each other .   Pezeshkpour et al . ( 2021 ) ; Hanawa et al . ( 2021 )   empirically showed that IFs with last layer gradi-   ent perform as well as or better than IFs with all1205layers ’ gradient and variants of IF behave simi-   larly . Therefore , we analyze the behavior of GD   with last layer ’s gradient and generalize our results   to other IFs . Fig . 1 shows the last layer ’s gradi-   ent of an MLP on a 3 - class classification problem .   In the figure , gradients of mislabeled data points   have large magnitudes and are opposite to gradients   of correct data points in the true class . However ,   gradients of mislabeled data points are not neces-   sarily opposite to that of correct data points from   other classes . Furthermore , gradients of two data   points from two different classes are almost per-   pendicular . We make the following observation .   A mislabeled / correct data point often has a very   negative / positive influence on data points of the   same ( true ) class , but its influence on other classes   is noisy and small .   We verify the observation on real - world datasets .   ( Fig . 2 ) . We compute GD scores of pairs of clean   data points from 2 different classes and plot the   score ’s distribution . We repeat the procedure for   pairs of data points from each class . In the 2 - class   case , GD scores are almost normally distributed   with a very sharp peak at 0 . That means , in many   cases , a clean data point from one class has no   significant influence on data points from the other   class . And when it has a significant effect , the effect   could be positive or negative with equal probability .   In contrast , GD scores of pairs of data points from   the same class are almost always positive . A clean   data point almost certainly has a positive influence   on clean data points of the same class .   Our theoretical analysis shows that when the two   data points have different labels , then the sign of   GD depends on two random variables , the sign of   inner product of the features and the sign of inner   product of gradients of the losses w.r.t . the logits .   And as the model becomes more confident about   the labels of the two data points , the magnitude   of GD becomes smaller very quickly . Small per-   turbations to the logits or the features can flip the   sign of GD . In contrast , if the two data points have   the same label , then the sign of GD depends on   only one random variable , the sign of the inner   product of the feature , and the GD ’s magnitude   remains large when the model becomes more confi-   dent . Mathematical details are deferred to Appx . D.   3.2 Class based IFs for error detection   Our class based IFs for error detection is shown in   Alg . 1 . In Sec . 3.1 , we see that an error has a veryAlgorithm 1 Class based influence function for   error detection .   Require : Z=/braceleftbig   z / bracerightbig : a big noisy datasetC : number of classesZ=/braceleftbig   z / bracerightbig : clean data from class kZ=/uniontextZ : a clean reference datasetf : a deep model pretrained on Zsim ( · , · ): a similarity measure in Tab . 3   Ensure : ˆZ : data points in Zranked by scoreforz∈ Z do fork= 1 , ... , C do s= end for s= min(s)end forˆZ = sort(Z , key = s , ascending = True)return ˆZ   strong negative influence on correct data points   in the true class , and a correct data point has a   positive influence on correct data points in the true   class . Influence score on the true class is a stronger   indicator of the harmfulness of a data point and   is better at differentiating erroneous and correct   data points . Because we do not know the true class   ofzin advance , we compute its influence score   on each class in the reference set Zand take the   minimum of these influence scores as the indicator   of the harmfulness of z(line 8 - 11 ) . Unlike the   original IFs , IFs - class are not affected by the noise   from other classes and thus , have lower variances   ( Fig . 4 in Appx . A ) . In Appx . A , we show that our   algorithm has the same computational complexity   as IFs based error detection algorithm .   4 Experiments   4.1 Error detection on benchmark datasets   Experiment setup We evaluate the error detec-   tion performance of IFs - class on 2 NLP tasks , ( 1 )   text classification on IMDB ( Maas et al . , 2011 ) ,   SNLI ( Bowman et al . , 2015 ) , and BigCloneBench   ( Svajlenko et al . , 2014 ) datasets , and ( 2 ) NER on   the CoNLL2003 ( Tjong Kim Sang and De Meul-   der , 2003 ) dataset . For text classification tasks ,   we detect text segments with wrong labels . For   the NER task , we detect tokens with wrong en-   tity types . We use BERT ( Devlin et al . , 2019 ) and   CodeBERT ( Feng et al . , 2020 ) in our experiments .   Implementation details are located in Appx . B. To1206   create benchmark datasets Z ’s , we inject random   noise into the above datasets . For text classification   datasets , we randomly select p%of the data points   and randomly change their labels to other classes .   For the CoNLL - NER dataset , we randomly select   p%of the sentences and change the labels of r%   of the phrases in the selected sentences . All to-   kens in a selected phrase are changed to the same   class . The reference set Zis created by randomly   selecting mclean data points from each class in   Z. To ensure a fair comparison , we use the same   reference set Zfor both IFs and IFs - class algo-   rithms . Models are trained on the noisy dataset   Z. To evaluate an error detection algorithm , we   select top q%most harmful data points from the   sorted dataset ˆZand check how many percent of   the selected data points are really erroneous . In-   tuitively , increasing qallows the algorithm to find   more errors ( increase recall ) but may decrease the   detection accuracy ( decrease precision ) .   Our code is available at   https://github.com/Fsoft-AIC/   Class - Based - Influence - Functions .   Result and Analysis Because results on all datasets   share the same patterns , we report representative   results here and defer the full results to Appx . C.   Fig . 3(a ) shows the error detection accuracy on   the SNLI dataset and how the accuracy changes   withq . Except for the GC algorithm , our class-   based algorithms have higher accuracy and lower   variance than the non - class - based versions . When   qincreases , the performance of IFs - class does not   decrease as much as that of IFs . This confirms that   IFs - class are less noisy than IFs . Class information   fails to improve the performance of GC . To under-   stand this , let ’s reconsider the similarity measure   sim ( · , · ) . Let ’s assume that there exist some clean   data points z∈ Zwith a very large gradient∇ℓ(z ) . If the similarity measure does not nor-   malize the norm of ∇ℓ(z ) , then zwill have   the dominant effect on the influence score . The   noise in the influence score is mostly caused by   these data points . GC normalizes both gradients ,   ∇ℓ(z)and∇ℓ(z ) , and effectively removes   such noise . However , gradients of errors tend to be   larger than that of normal data points ( Fig . 1 ) . By   normalizing both gradients , GC removes the valu-   able information about magnitudes of gradients of   errors∇ℓ(z ) . That lowers the detection perfor-   mance . In Fig . 3(a ) , we see that the performance of   GC when q≥15 % is lower than that of other class-   based algorithms . Similar trends are observed on   other datasets ( Fig . 6 , 7 , 8 in Appx . C ) .   Fig . 3(b ) shows the change in detection accuracy   as the level of noise pgoes from 5%to20 % . For   each value of p , we set qto be equal to p. Our   class - based influence score significantly improves   the performance and reduces the variance . We note   that when pincreases , the error detection problem   becomes easier as there are more errors . The detec-   tion accuracy , therefore , tends to increase with pas   shown in Fig . 3(b ) , 9 , 10 .   Fig . 3(c ) shows that GD - class outperforms GD   on all entity types in CoNLL2003 - NER . The per-   formance difference between GD - class and GD is   greater on the MISC and ORG categories . Intu-   itively , a person ’s name can likely be an organiza-   tion ’s name but the reverse is less likely . Therefore ,   it is harder to detect that a PER or LOC tag has   been changed to ORG or MISC tag than the reverse .   The result shows that IFs - class is more effective   than IFs in detecting hard erroneous examples.12074.2 The effect of data on error detection   algorithms   We study the effect of the size and the cleanliness   of the reference set on the performance of error   detection algorithms .   The size of the reference set . We changed the   size of classes in the reference set from 10 to 1000   to study the effect of the reference set ’s size on   the detection performance . We report the mean   performance of GD and GC algorithms in Tab . 1 .   We observe no clear trend in the performance as   the size of the reference set increases . Our conjec-   ture is that gradients of clean data points from the   same class have almost the same direction . Averag-   ing the gradient direction over a small set of data   points already gives a very stable gradient direc-   tion . Therefore , increasing the size of the reference   set does not have much impact on detection perfor-   mance .   The cleanliness of the reference set . The result   of GD and GD - class on SNLI dataset when the ref-   erence set is a random ( noisy ) subset of the training   set is shown in table 2 . When the reference set is   noisy , the error detection performance of IF algo-   rithms decreases significantly . IF - class algorithms   are much more robust to noise in the reference set   and their performance decreases only slightly . This   experiment further demonstrates the advantage of   IFs - class over IFs algorithms .   5 Conclusion   In this paper , we study influence functions and   identify the source of their instability . We give a   theoretical explanation for our observations . We   introduce a stable variant of IFs and use that to de-   velop a high performance error detection algorithm .   Our findings shed light of the development of new   influence estimators and on the application of IFs   in downstream tasks.1208Limitations   Our paper has the following limitations   1.Our class - based influence score can not im-   prove the performance of GC algorithm . Al-   though class - based version of GD , IF , and   TracIn outperformed the original GC , we aim   to develop a stronger version of GC . From the   analysis in Sec . 4 , we believe that a partially   normalized GC could have better performance .   In partial GC , we normalize the gradient of the   clean data point zonly . That will remove   the noise introduced by ∥∇ℓ(z)∥while   retaining the valuable information about the   norm of ∇ℓ(z ) .   Ethics Statement   Our paper consider a theoretical aspect of influence   functions . It does not have any biases toward any   groups of people . Our findings do not cause any   harms to any groups of people.1209References1210   A Additional algorithms and formula   IF ∇ℓ(z;ˆθ)H∇ℓ(z;ˆθ )   GD / angbracketleftbig   ∇ℓ(z),∇ℓ(z)/angbracketrightbig   GC cos / parenleftbig   ∇ℓ(z),∇ℓ(z)/parenrightbig   TracIn / summationtextη / angbracketleftbig   ∇ℓ(z),∇ℓ(z)/angbracketrightbig   Computational complexity of error detection   algorithms   The inner for - loop in Alg . 1 calculates Cinfluence   scores . It calls to the scoring function sim()exactlyAlgorithm 2 Influence function based error detec-   tion ( Dau et al . , 2022 )   Require : Z=/braceleftbig   z / bracerightbig : a big noisy datasetZ=/braceleftbig   z / bracerightbig : a clean reference datasetf : a deep model pretrained on Zsim ( · , · ): a similarity measure in Tab . 3   Ensure : ˆZ : data points in Zranked by scoreforz∈ Z do s=/summationtextsim(∇ℓ(z),∇ℓ(z))end forˆZ = sort(Z , key = s , ascending = True)return ˆZ   |Z|=mtimes . The complexity of the inner for-   loop in Alg . 1 is equal to that of line 6 in Alg . 2 .   Thus , the complexity of Alg . 1 is equal to that of   Alg . 2 .   B Implementation details   B.1 Experiment setup   We used standard datasets and models and experi-   mented with 5 different random seeds and reported   the mean and standard deviation . A Nvidia RTX   3090 was used to run our experiments . Models   are trained with the AdamW optimizer ( Loshchilov   and Hutter , 2019 ) with learning rate η= 5e−5 ,   cross entropy loss function , and batch - size of 16.1211The epoch with the best classification accuracy on   the validation set was used for error detection .   Our source code and guidelines were attached to   the supplementary materials .   B.2 Datasets   IMDB ( Maas et al . , 2011 ) The dataset includes   50000 reviews from the Internet Movie Database   ( IMDb ) website . The task is a binary sentiment   analysis task . The dataset contains an even num-   ber of positive and negative reviews . The IMDB   dataset is split into training , validation , and test   sets of sizes 17500 , 7500 , and 25000 . The IMDB   dataset can be found at https://ai.stanford .   edu/~amaas / data / sentiment/   SNLI dataset ( Standart Natural Language Infer-   ence ) ( Bowman et al . , 2015 ) consists of 570k sen-   tence pairs manually labeled as entailment , con-   tradiction , and neutral . We convert these labels   into numbers . It is geared towards serving as a   benchmark for evaluating text representational sys-   tems . This dataset is available at https://nlp .   stanford.edu/projects/snli/ .   BigCloneBench ( Svajlenko et al . , 2014 ) is a huge   code clone benchmark that includes over 6,000,000   true clone pairs and 260,000 false clone pairs from   10 different functionality . The task is to predict   whether two pieces of code have the same seman-   tics . This dataset is commonly used in language   models for code ( Feng et al . , 2020 ; Lu et al . , 2021 ;   Guo et al . , 2020 ) . This dataset is available at https :   //github.com / clonebench / BigCloneBench   CoNLL2003 ( Tjong Kim Sang and De Meulder ,   2003 ) is one of the most influential corpora for   NER model research . A large number of publica-   tions , including many landmark works , have used   this corpus as a source of ground truth for NER   tasks . The data consists two languages : English   and German . In this paper , we use CoNLL2003   English dataset . The sizes of training , validation ,   and test are 14,987 , 3,466 , and 3,684 sentences   correspond to 203,621 , 51,362 , and 46,435 tokens ,   respectively . The dataset is available at https :   //www.clips.uantwerpen.be / conll2003 / ner/   B.3 Models   BERT ( Devlin et al . , 2019 ) stands for Bidirec-   tional Encoder Representations from Transformers ,   is based on Transformers . The BERT model in   this paper was pre - trained for natural language pro-   cessing tasks . We use BERT for IMDB and SNLI   datasets . At the same time , we also use the BERTmodel for the NER problem on the CoNLL2003   dataset .   CodeBERT ( Feng et al . , 2020 ) is a bimodal pre-   trained model for programming and natural lan-   guages . We use CodeBERT for BigCloneBench   dataset .   C Additional results   C.1 3 - class classification experiment   We train a MLP with 2 input neurons , 100 hidden   neurons in the first hidden layer , 2 hidden neurons   in the second hidden layer , and 3 output neurons   with SGD for 1000 epochs . The activation function   is LeakyReLU and the learning rate is η= 1e−3 .   The last layer has 6 parameters organized into a   3×2matrix . The gradient of the loss with respect   to the last layer ’s parameters is also organized into   a3×2matrix . We visualize 3 rows of the gradient   matrix in 3 subfigures ( Fig . 5 ) .   C.2 Result on IMDB , SNLI , BigCloneBench ,   and CoNLL2003   To ensure a fair comparison between our class-   based algorithm and algorithm 2 , we use the same   reference dataset Zfor both algorithms . The ref-   erence dataset Zconsists of Cclasses . We have   C= 2for the IMDB dataset , C= 3for the SNLI   dataset , C= 2for the BigCloneBench dataset , and   C= 5 for the CoNLL2003 - NER dataset . From   each of the Cclasses , we randomly select m= 50   k= 1 , ... , C clean data points to form Z. We tried   varying mfrom 10 to 1000 and observed no sig-   nificant changes in performance.121212131214DExplanation of the observation in Sec . 3   Let ’s consider a classification problem with cross   entropy loss function   ℓ(ˆy , y ) = /summationdisplayylog ˆy   where dis the number of classes . Let z= ( x , y )   be a data point with label k , i.e. y= 1 , y=   0∀i̸=k . The model fis a deep network with   last layer ’s parameter W∈R , where dis   the number of hidden neurons . Let u∈Rbe the   activation of the penultimate layer . The output is   computed as follow   a = Wu   ˆy = δ(a )   where δis the softmax output function . The deriva-   tive of the loss at zw.r.t . Wis   ∂ℓ(z )   ∂W=∇ℓ(z)u(4 )   =    ∇ℓ(z)u   ...   ∇ℓ(z)u    ( 5 )   The gradient ∇ℓ(z)is   ( ∇ℓ)=∂ℓ   ∂a(6 )   = ∂ℓ   ∂ˆy∂ˆy   ∂a(7 )   = /bracketleftig······/bracketrightig   ×      ··· ............ ··· ............ ···   (8 )   = /bracketleftig······/bracketrightig   ( 9 )   We go from Eqn . 8 to Eqn . 9 by using the following   fact   ∂ℓ   ∂ˆy=/braceleftigg   0ifi̸=kifi = kWe also have   ∂ˆy   ∂a=/braceleftigg   ˆy(1−ˆy)ifi = k   −ˆyˆyifi̸=k   Substitute this into Eqn . 9 we have   ∇ℓ=   −ˆy   ...   1−ˆy   ...   −ˆy      Because 1−ˆy=/summationtextˆy,1−ˆyis much greater   than ˆyin general . Substitute this into Eqn . 5 , we   see that the magnitude of the k - th row is much   larger than than of other rows . We also note that   the update for the k - th row of Whas the opposite   direction of the updates for other rows .   Let ’s consider the inner product of the gradients   of two data points zandzwith label kandk .   Let ’s consider the case where k̸=kfirst .   vec / parenleftbigg∂ℓ(z )   ∂W / parenrightbigg   vec / parenleftbigg∂ℓ(z )   ∂W / parenrightbigg   =   ( ∇ℓ∇ℓ)(uu ) ( 10 )   Intuitively , the product ∇ℓ∇ℓis small be-   cause the large element ∇ℓ= 1−ˆyis multi-   plied to the small element ∇ℓ= ˆyand the large   element ∇ℓ= 1−ˆyis multiplied to the small   element ∇ℓ= ˆy . To make it more concrete ,   let ’s assume that ˆy = α≈1andˆy==β   fori̸=k . We assume the same condition for ˆy .   ∇ℓ∇ℓ= ( ˆy−1)ˆy+ ( ˆy−1)ˆy   + /summationdisplayˆyˆy   = ( d−2)β−2(d−1)β   = −dβ   = −d(1−α )   ( d−1)(11 )   α≈1implies 1−α≈0andβ≈0 . Eqn . 11   implies that as the model is more confident about   the label of zandz , the product ∇ℓ∇ℓtends   toward 0 at a quadratic rate . The means , as the train-   ing progresses , data points from different classes1215become more and more independent . The gradients   of data points from different classes also become   more and more perpendicular .   The sign of the gradient product depends on   the sign of ∇ℓ∇ℓanduu . The signs of   ∇ℓ∇ℓanduuare random variables that de-   pend on the noise in the features uanduand the   weight matrix W. If the model fcannot learn   a good representation of the input then the fea-   tureuand the sign of uucould be very noisy .   sign(uu)is even noisier if zandzare from dif-   ferent classes . Because / vextendsingle / vextendsingle∇ℓ∇ℓ/vextendsingle / vextendsingleis small , a   tiny noise in the logits aandacan flip the sign of   ∇ℓ∇ℓand change the direction of influence .   We now consider the case where k = k. When   k = k,∇ℓ∇ℓis always positive . The sign of   the gradient product only depends on uu . That   explains why the product of gradients of data points   from the same class is much less noisy and almost   always is positive .   Furthermore , the magnitude of ∇ℓ∇ℓis   larger than that in the case k̸=kbecause the   large element 1−ˆyis multiplied to the large el-   ement 1−ˆy . More concretely , under the same   assumption as in the case k̸=k , we have   ∇ℓ∇ℓ= ( 1−ˆy)(1−ˆy ) + /summationdisplayˆyˆy   = ( 1−α)+ ( d−1)β(12 )   From Eqn . 12 , we see that when k = k , the magni-   tude of ∇ℓ∇ℓis approximately dtimes larger   than that when k̸=k.1216ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations section   /squareA2 . Did you discuss any potential risks of your work ?   Ethics Statement   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Not applicable . Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Not applicable . Left blank .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Left blank.1217 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.1218