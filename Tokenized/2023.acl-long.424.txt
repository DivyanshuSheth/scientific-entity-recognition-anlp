  Weiwei SunHengyi CaiHongshen ChenPengjie Ren   Zhumin ChenMaarten de RijkeZhaochun RenShandong University , Qingdao , ChinaJD.com , Beijing , ChinaUniversity of Amsterdam , Amsterdam , The Netherlands   { sunnweiwei,hengyi1995}@gmail.com   ac@chenhongshen.com,m.derijke@uva.nl   { renpengjie,chenzhumin,zhaochun.ren}@sdu.edu.cn   Abstract   In open - domain question answering , due to the   ambiguity of questions , multiple plausible an-   swers may exist . To provide feasible answers   to an ambiguous question , one approach is   to directly predict all valid answers , but this   can struggle with balancing relevance and di-   versity . An alternative is to gather candidate   answers and aggregate them , but this method   can be computationally costly and may neglect   dependencies among answers . In this paper ,   we present AmbigPrompt to address the imper-   fections of existing approaches to answering   ambiguous questions . Specifically , we inte-   grate an answering model with a prompting   model in an iterative manner . The prompt-   ing model adaptively tracks the reading pro-   cess and progressively triggers the answering   model to compose distinct and relevant an-   swers . Additionally , we develop a task - specific   post - pretraining approach for both the answer-   ing model and the prompting model , which   greatly improves the performance of our frame-   work . Empirical studies on two commonly-   used open benchmarks show that Ambig-   Prompt achieves state - of - the - art or competitive   results while using less memory and having   a lower inference latency than competing ap-   proaches . Additionally , AmbigPrompt also per-   forms well in low - resource settings . The code   are available at : https://github.com/   sunnweiwei / AmbigPrompt .   1 Introduction   Recent years have witnessed substantial advances   in open - domain question answering ( QA ) sys-   tems ( Karpukhin et al . , 2021 ; Lewis et al . , 2020 ;   Izacard and Grave , 2021b ) , which aim to find the   answer for the given question from a large knowl-   edge corpus ( Chen et al . , 2017 ) . While a dominat-   ing scenario is the single - answer QA setting , i.e. ,   only one exact answer is required for a given ques-   tion ( Karpukhin et al . , 2021 ) , this work focusesFigure 1 : An example of an open - domain question , a   subset of its evidential Wikipedia passages and multiple   answers they lead to .   on the more realistic scenario of Multi - answer QA ,   where multiple plausible answers are associated   with a user - issued question ( Min et al . , 2020 ) , given   that questions posed by humans are often open-   ended and ambiguous .   A natural approach for answering ambiguous   open - domain questions would be to fine - tune a pre-   trained answer generation model , e.g. , T5 ( Raffel   et al . , 2020 ) , using supervised data of the form   ( evidential passages , question , all plausible an-   swers ) ( Min et al . , 2020 , 2021 ) . However , this   approach often leads to sub - optimal solutions since   it requires the model to balance the relevance   and diversity of the generated multiple answers   within a single - round decoding procedure , which   is non - trivial . To manage the relevance - diversity   trade - off , another approach is to decompose multi-   answer QA into candidate answer prediction and   answer post - processing . This typically requires a   high - capacity model with billions of parameters7669to construct candidate answers and sophisticated   answer aggregation pipelines to obtain the final re-   sults ( Shao and Huang , 2022 ; Gao et al . , 2021b ) ,   incurring high computational costs . In addition ,   this approach suffers from the dilemma of having   to predict diverse candidate answers before know-   ing which answer has been predicted , which is   unnatural and intricate . For example , in Figure 1 ,   given the question “ Which movie was both directed   and screenwritten by Kamal Haasan ? , ” with the   existence of the answer Vishwaroopam , the model   excludes its eponymous translation version Vishwa-   roop and deduces that Vishwaroopam II is another   potential answer .   When facing an ambiguous question , people are   capable of providing multiple valid answers by in-   trospectively composing new content on the basis   of what has already been devised , usually in an   iterative manner . Inspired by this observation , in   this paper , we conceptualize AmbigPrompt as an   approach to mimic this mechanism by iteratively   guiding the answering model with a lightweight   prompting model . As shown in Figure 2 , this   prompting model steers the answering model to   progressively generate valid answers whose con-   tent the prompting model will then condition on for   the next - round prompt construction . Essentially ,   our proposed framework comprises two key com-   ponents : ( i ) an encoder - decoder answering model   and ( ii ) an interleaving answer - conditional prompt-   ing model . By conditioning on preceding generated   contents , the proposed framework introspectively   perceives which answer has been predicted before   updating the hidden activation for the generation   of subsequent answers . Furthermore , we devise   a task - adaptive post - pretraining strategy , in which   pseudo multi - QA training instances are constructed   to facilitate the training of the proposed framework .   We carry out extensive experiments on the Am-   bigQA ( Min et al . , 2020 ) and WebQSP ( tau Yih   et al . , 2016 ) datasets . The results demonstrate that   AmbigPrompt attains superior performance despite   having a significantly smaller parameter scale , 14   times less than state - of - the - art models . Further-   more , as a lightweight approach , AmbigPrompt   improves the answer relevance and diversity with a   tiny fraction of the memory footprint and inference   latency of competing approaches . Notably , Ambig-   Prompt achieves the best performance in the low-   resource setting . The effectiveness of the proposed   method is also verified by ablation experiments and   analytical experiments .   In summary , this paper makes the following con-   tributions : ( i ) We propose AmbigPrompt , which   tackles ambiguous question answering by iterative   prompting . ( ii ) We propose an interleaving answer –   conditional prompting model to generate mean-   ingful continuous prompts . ( iii ) Experiments on   multi - QA datasets verify the effectiveness of the   proposed approach .   2 Preliminaries   2.1 Problem formalization   Formally , given an open - domain question q , a   multi - answer question answering ( QA ) model is   required to make use of ( multiple pieces of ) ev-   idence from a large - scale text corpus Ω(e.g . ,   Wikipedia ) to find multiple plausible answers A=   { a , a , . . . , a } , where adenotes one answer and   we suppose there are nanswers . The QA model   aims to infer p(A|q,Ω ) . In open - domain QA , the   QA model typically follows a two - step pipeline ,   comprising passage retrieval andanswer gener-   ation . In the passage retrieval step , a retrieval   model p(C|q,Ω)retrieves mevidence passages   C={c , c , . . . , c}according to the question q   from Ω. In the answer generation step , an answer-   ing model p(A|q , C)reads the evidential passages   and finds the answers to the question .   2.2 Answering model   We use Fusion - in - Decoder ( FiD ) as a basic single-   answer answering model ( Izacard and Grave ,   2021b ) . In particular , FiD has an encoder - decoder   architecture . FiD first concatenates each retrieved7670passage with the question with a [ SEP ] token :   X={x , x , . . . , x } , x = q[SEP ] c(1 )   where we use Xto denote the concatenated se-   quence . Then , for each x , the FiD encoder Enc   encodes it to x :   X= Cat ( { x , x , . . . , x}),x= Enc ( x)(2 )   where Cat denotes a concatenation function . Fi-   nally , the decoder Dec attends to the representa-   tions of all passages and generates an answer a :   p(a|q , C ) = Dec ( X ) ( 3 )   2.3 Prompt - tuning   Prompt - tuning adapts pre - trained transformer mod-   els to downstream tasks by optimizing continu-   ous prompting vectors ( Li and Liang , 2021 ; Liu   et al . , 2022 ) . Suppose xis the input sequence   of the model , we denote Q(x),K(x),V(x)as   the query , key , and value representations of xin   thej - th attention layer in the transformer encoder .   Prompt - tuning prepends learnable prompting vec-   torsEtoK(x)andV(x)to modify the atten-   tion distribution as well as the output xof the j - th   layer as follows :   x= Attn ( Q(x),Cat(E , K(x ) ) ,   Cat(E , V(x))),(4 )   where xdenotes the output of layer j , Attn(·)rep-   resents the attention operation in the transformer ,   andCat(·)is the concatenation function .   3 AmbigPrompt   Conventionally , the question answering model gen-   erates the desired answer given the input context   in a single pass ( Izacard and Grave , 2021b ) . While   it suffices to tackle the single - answer QA scenario ,   managing ambiguous questions with multiple an-   swers can be more nuanced – the answering model   is required to balance the relevance and diversity   of the generated answers in a single pass , and pre-   cisely modeling dependencies among the answers   can be non - trivial . In this paper , we propose Am-   bigPrompt , a question - answering model that an-   swers ambiguous questions via iterative prompting ,   inferring more accurate answers progressively . Fig-   ure 2 gives an overview of the proposed method . Overall , AmbigPrompt decomposes the gener-   ation of answers Ainto multiple steps instead of   one single pass , i.e. ,   p(A|q , C ) = /productdisplayp(a|ϕ(a ) , q , C ) , ( 5 )   where adenotes the set of answers that have been   generated at time t , and ϕ(·)denotes a prompting   model that generates prompt vectors for answer   generation at the t - th step . The prompting model   shares parameters with the answering model , al-   lowing for seamless integration . AmbigPrompt   iteratively composes a new answer a , conceiving   the prompt of previous answers , i.e. , ϕ(a ) , and   appends ato the answers set , till all feasible an-   swers are found .   The proposed framework is optimized in a two-   stage manner : task - adaptive post - pretraining and   prompt - based tuning . In the former stage , the   model is trained on a large synthesized multi-   answer QA dataset , while in the latter stage , the   model is tuned on the annotated multi - answer QA   dataset . We first detail the prompting model ( § 3.1 )   and the iterative question answering procedure   ( § 3.2 ) , and then introduce the optimization scheme   ( § 3.3 ) .   3.1 Retrospective prompting mechanism for   answer generation   To capture intricate dependencies among answers ,   we devise an interleaving answer - conditional   prompting model ϕ(a ) , which generates the   prompt vector E=ϕ(a)conditioned on an-   tecedent generated answers a , as depicted in Fig-   ure 3 . Specifically , the prompting model ϕis a   transformer encoder that shares the same parame-   ters with the encoder of the answering model . ϕ   processes the ain three steps :   ( 1)Templating answers . First , ais transformed   into a text sequence e = T(a)using a tem-   plateT. Here we use semicolons to splice   answers .   ( 2)Generating prompts . Then , given the answer   sequence eand context X(i.e . , the concate-   nated question and passages in Eq . 1 ) , the   prompting model ϕcomputes the hidden ac-   tivations Eof each layer jvia cross - attending   the contextual representation X :   E= Attn ( Q(e),Cat(K(e),X ) ,   Cat(V(e),X)),(6)7671   where Q(e),K(e ) , and V(e)denote the   query , key , and value representations of e   in the j - th attention layer in the prompting   model ; X= Cat ( { x , x , . . . , x } )   denotes the concatenated context representa-   tions of the ( j−1)-th layer in the answering   model . We write Efor the last layer output of   the prompting model .   ( 3)Prompting answering model . Finally , the   generated prompt Eis prepended to the atten-   tion layer of the encoder Enc of the answering   model as in Eq . 4 . Meanwhile , the decoder   Dec of answering model attends to Cat(E , X )   and generates the target answer a :   p(a|ϕ(a ) , q , C ) = Dec(Cat ( E , X)).(7 )   Capturing long - range dependencies among derived   answers via a retrospective prompting mechanism   enables the answering model to compose new   contents grounding on what has already been de-   vised , and thus the model is able to strike a good   relevance - diversity balance for answering ambigu-   ous questions .   3.2 Answering ambiguous questions via   iterative prompting   Given the input context , i.e. , the question and re-   trieved evidential passages , AmbigPrompt itera-   tively performs attention operations over the input   context and the generated answers , addressing the   answer generation and prompt construction inter-   actively . The key is to pass the attention activa - tions between the prompting model and answer-   ing model so that they can inspect each other ’s   internal states and make harmonious predictions .   Specifically , we start from an empty answer set and   progressively append newly generated answers to   it . As depicted in Figure 2 , in each iteration , we   first use the previously generated answer sequence   to obtain the introspective prompts , and then in-   terwoven the resultant prompting vectors into the   answering model to predict the next answer . Our al-   gorithm terminates if the model reaches the [ EOI ]   token .   3.3 Optimization   To enhance the pre - training model towards multi-   answer QA , one straightforward approach is to   leverage a question - answering dataset such as   NQ ( Kwiatkowski et al . , 2019 ) for domain - adaptive   pre - training ( Min et al . , 2021 ) . However , the effec-   tiveness of such a trivial approach is limited to the   inherent defect of the one - pass prediction process ;   that is , the lack of the modeling capability of the   interactions between answer generation and answer   perception , which is critical to achieving superior   performance in multi - QA scenarios . To explic-   itly align the pre - training objective to task - specific   preferences , we further propose to conduct task-   adaptive post - pretraining on pseudo multi - answer   QA dataset , and then finetune the proposed model   using the task data .   Task - adaptive post - pretraining . We first pre-   train the model on NQ , in which only one answer   A={a}is labeled for each question q. To ex-7672plicitly characterize the pretraining stage as the   efforts for finding which part of preceding answers   to interact with regarding the input context , we   construct the pseudo multi - answer dataset ˆAfor   post - pretraining the proposed framework to mimic   the iterative question answering process . Specifi-   cally , we first train an auxiliary reader g(a|q , c ) ,   which learns to find an answer from the passage   cgiven a question q. Then , we use this auxil-   iary reader to generate a pseudo answer for each   retrieved passage in C :   ˆA={ˆa| ∀i∈[1 , m],ˆa∼g(a|q , c)},(8 )   where ˆAdenotes the pseudo - answer set of q.   Then , we aggregate the generated answers to   construct the previously known answers ain   Eq . 5 . In particular , we randomly sample tanswers   from ˆAand filter out those that are equivalent to   the ground - truth answer a ; we denote the sampled   set as ˆa . With the pseudo answers , we define the   post - pretraining objective as :   L=−logp(a|ϕ(ˆa ) , q , C ) , ( 9 )   where the number of answers in ˆa , i.e. , t , is sam-   pled from a Bernoulli distribution .   Prompt - based fine - tuning . We fine - tune the pre-   trained model on downstream multi - answer QA   datasets . Specifically , in multi - answer QA , nan-   swersA={a , a , . . . , a}corresponding to a   question qare provided . The model is tuned by the   following objective :   L=−logp(a|ϕ(a ) , q , C ) , ( 10 )   where t∈[1 , n]is sampled from a Bernoulli distri-   bution . Since Ais unordered , we shuffle Awhen   constructing the aandato improve the robust-   ness . Besides , we explicitly optimize the model to   generate [ EOI ] to stop the iteration . Specifically ,   we define a parameter α∼ U(0,1)and a thres-   holdλ , which controls the propensity of generating   [ EOI ] . Ifα < λ , we replace the aandaas   [ EOI ] andA , respectively .   4 Experimental Setup   4.1 Datasets   We evaluate AmbigPrompt on the AmbigQA ( Min   et al . , 2020 ) and WebQSP ( tau Yih et al . , 2016 )   datasets . AmbigQA : AmbigQA is constructed   to address the ambiguity of questions in open-   domain QA . It samples 14,042 questions from   NQ - Open ( Kwiatkowski et al . , 2019 ) , and asks   annotators to search for , navigate and read multi-   ple Wikipedia pages to find as many answers as   possible . WebQSP : WebQSP consists of questions   from Google Suggest API , originally from Berant   et al . ( 2013 ) . The answer is a set of distinct enti-   ties in Freebase ; we use the modified versions by   Min et al . ( 2021 ) , which recasts WebQSP as textual   question answering based on Wikipedia .   The statistical details of these two datasets and   NQ are shown in Table 1 .   4.2 Evaluation metrics   Following previous studies ( Min et al . , 2020 ) , we   adopt F1 as the evaluation metric , which measures   the precision and recall between the ground - truth   answers and the predicted answers . The test set is   further divided into two subsets : fullandmulti . The   fullsubset evaluates the model on all the questions   in the test set , while the multi subset evaluates the   model on the questions with multiple answers ( i.e. ,   n > 1 ) . To assess the computational efficiency   of various approaches , we also report the number   of parameters , average latency , and peak memory   usage during model inference . All the models are   tested on the same device . We estimate the latency   and memory usage of those baselines without pub-   lic code using randomly initialized models since   these metrics are independent of their parameters   given a fixed number of encoded tokens and decod-   ing length .   4.3 Baselines   The following models are adopted as baselines :   DPR ( Karpukhin et al . , 2021 ): A dual - encoder is   trained using contrastive loss for passage retrieval ,   and a BERT - based reader is used for answer ex-   traction . SpanSeqGen ( Min et al . , 2020 ): DPR   reranks the passages , and a BART - based generator   is used for answer generation . FiD ( Izacard and   Grave , 2021b ): The retrieved passages are encoded   by a T5 encoder independently , and the represen-   tations are then concatenated and fed into the T5   Decoder to generate answers . Refuel ( Gao et al . ,7673   2021b ): A question disambiguation module is pro-   posed to generate disambiguated questions . The   disambiguated questions are then used to find more   answers . JPR ( Min et al . , 2021 ): JPR is a pas-   sage reranker that reranks the passages using an   autoregressive model . With the additional rerank-   ing stage , JPR selects ten diverse passages from   100 retrieved passages and uses a T5 - 3B FiD an-   swering model to compose answers in one pass .   RECTIFY ( Shao and Huang , 2022 ): RECTIFY   proposes the recall - then - verify framework , which   separates the reasoning process of each answer . An   answering model operates on each passage to re-   call surplus answers . Then , a sophisticated verifier   based on T5 - 3B FiD verifies each answer with an   aggregation module .   We divide the baseline models into two cate-   gories depending on the number of parameters of   the models : ( i ) high - capacity baselines that use   large models with billions of parameters , while   requiring more computational resources and mem-   ory ; ( ii ) comparable low - capacity baselines that   use low - capacity models with a similar number   of parameters and computational effort as Ambig-   Prompt , which can be reasonably compared with   AmbigPrompt .   4.4 Implementation details   We choose T5 - Base ( Raffel et al . , 2020 ) as the back-   bone of the answering model . Regarding the pas-   sage retrieval model , we fine - tune the pre - trained   model from Gao and Callan ( 2021 ) on the NQ   dataset ( See Appendix C for details ) . The retrieval   corpus is the English Wikipedia on 12/20/2018,and the documents are split into chunks with 100   words following Karpukhin et al . ( 2021 ) . We set   m=100 , λ=0.5 , the batch size to 32 , and the model   is trained using the AdamW optimizer ( Loshchilov   and Hutter , 2017 ) with a constant learning rate of   5e−5 . We train the model up to 5k steps on on 4   V100 - 16 G GPUs and choose the hyperparameters   and checkpoints on the validation set .   5 Experimental Results   5.1 Main results   Table 2 reports the evaluation results on AmbigQA   and WebQSP . Based on the results , we have three   main observations .   First , AmbigPrompt achieves comparable perfor-   mance to the state - of - the - art . Specifically , Ambig-   Prompt obtains 48.7 F1 on the fulltest set and 38.8   F1 on the multi test set , which exceeds all baselines   except RECTIFY . The improvements are particu-   larly significant on the multi test set ; AmbigPrompt   improves 1.2 % over JPR and 1.5 % over Refuel . Be-   sides , compared with FiD , which concatenates all   the answers in Awith[SEP ] and generates them   in one pass , the proposed method , which benefits   from the iterative design and answer - conditional   prompting mechanism , achieves 3 % and 5 % im-   provements on fullandmulti of AmbigQA . Similar   results can also be observed on WebQSP .   Second , AmbigPrompt uses fewer resources   compared to previous high - capacity models . Am-   bigPrompt uses a lightweight model with 220M7674   parameters . Still , AmbigPrompt achieves superior   performance compared to the high - capacity mod-   els , e.g. , JPR , that use 3B parameters . The state - of-   the - art model RECTIFY uses 6B parameters ( 3B   for the answering model and 3B for the verifier ) ,   which is 27×as much as ours , significantly increas-   ing the training and inference overhead . Similar   results are witnessed in terms of latency . In par-   ticular , RECTIFY is 29×slower than our model   due to the heavy design of the answering model   and verifier . Refuel ’s iterative passage retrieval and   clarifying question generation procedure results   in a32.6×latency compared with our approach .   Finally , the comparison of peak memory usage   also confirms our approach ’s lightweight nature .   The lightweight design allows our approach to be   adapted to academically accessible devices and re-   duces the carbon footprint for model training and   deployment .   Third , we find that AmbigPrompt achieves   a better resource - performance balance . In Fig-   ure 4 ( a ) , we display the existing methods under   the speed - performance coordinate system . Note   that we place RECTIFY with different sizes ( i.e. ,   latency ) on the diagram according to Shao and   Huang ( 2022 ) . AmbigPrompt improves the op-   timal latency - performance curve ( the dashed lines ) ,   especially on the multi - answer test set , demonstrat-   ing the effectiveness of our approach in answering   ambiguous questions .   5.2 Low - resource setting   Figure 4 ( b ) shows the results under different   training data sizes to investigate the effectiveness   of the proposed method in the low - resource set-   ting . The proposed method achieves favorable re-   sults for different data sizes . Remarkably , Ambig-   Prompt achieves promising performance with little   data , surpassing the fully supervised high - capacity   model JPR on a multi - answer test set . This result   suggests that the proposed prompting mechanism   can better elicit the capabilities of the pre - trained   model and effectively adapt the model trained on   single - answer QA data to multi - answer scenarios .   5.3 Ablation study   To understand the contribution of each component   of AmbigPrompt , we conduct an ablation study .   The results are listed in Table 3 . The compared   variants and the findings are :   W / o task - adaptive pre - training . The models are   trained only on multi - QA data with L. A notable   performance decline can be seen . This observation   suggests that task - adaptive pre - training is an impor-   tant contributor to the model ’s performance since   the size of multi - answer QA data is small .   W / o prompting model . We remove the prompting   model in this variant and instantiate the learnable   prompt vector to each step tseparately , like Liu   et al . ( 2021a ) . The performance drops by about   3 % and 4 % on the two datasets , respectively . The   results verify the effectiveness of the proposed   answer - conditional prompting mechanism .   W / o interleaving prompting . We remove the in-   teraction mechanism between the prompting model   and answering model , i.e. , the FiD encoder en-   codes the eandXindependently without cross-   attention . The results drop by about 2 % and 2 %   on two datasets , respectively , which reveals that   enabling the answering model to generate new an-   swers conditioned on the introspective prompts ef-   fectively improves the model ’s performance.7675   5.4 Analytical experiments   Conceptually , our proposed framework Ambig-   Prompt equips the FiD model with the ability to   progressively compose the answers using retro-   spective prompts , i.e. , iterative prompt learning .   To further analyze the capability of such an iter-   ative prompt learning approach in managing the   relevance - diversity trade - off , we present the F1 ,   precision , recall , and average answer numbers of   AmbigPrompt and FiD model variants in Figure 5 .   In particular , FiD - multi denotes a variant of FiD   in which we reduce the generation probability of   the end - of - sequence token < /s > to ensure that the   number of generated answers is approximately the   same as AmbigPrompt . We see that FiD - multi ob-   tains comparable recall but gets significantly lower   precision . In contrast , AmbigPrompt generates   more answers than FiD without sacrificing preci-   sion , indicating that the designed iterative prompt-   ing mechanism induces the model with a superior   ability to manage the trade - off between relevancy   and diversity for ambiguous question answering .   6 Related work   6.1 Ambiguous question answering   In open - domain QA , given a question about any   topic , the model finds the answer from a large   knowledge corpus ( Chen et al . , 2017 ) . Typically ,   a retrieval model and an answering model are em-   ployed . The two modules can be trained sepa-   rately ( Karpukhin et al . , 2021 ; Izacard and Grave ,   2021b ; Qu et al . , 2021 ) or jointly ( Lee et al . , 2022 ;   Lewis et al . , 2020 ; Izacard and Grave , 2021a ) . Am-   biguity is inherent to open - domain QA ; especially   when exploring new topics , it can be difficult to   ask questions that have a single , unambiguous an-   swer ( Min et al . , 2020 ; Rubin et al . , 2022 ) . Min   et al . ( 2020 ) identify the challenge of multi - answer   QAand collect the dataset AmbigQA . Based on   that , Min et al . ( 2021 ) propose an autoregressivepassage reranking model JPR , which reranks the   top - retrieved passages and improves their diversity .   Gao et al . ( 2021b ) propose a round - trip prediction   approach , where clarification questions are gen-   erated and fed back into the model to find more   answers . Shao and Huang ( 2022 ) propose a recall-   and - verify framework , where surplus answers are   generated first , and a verifier model then deter-   mines each candidate answer . Compared with exist-   ing methods , we propose a lightweight yet effective   approach to answering ambiguous questions by it-   erative prompting .   6.2 Prompt - based learning   Prompt - based learning has received much atten-   tion recently ( Liu et al . , 2021a ) . Existing stud-   ies on prompt - based learning mainly focus on dis-   crete and continuous prompts . The former designs   text - based prompts ( Jiang et al . , 2020 ; Gao et al . ,   2021a ; Schick and Schütze , 2021 ) , while the latter   prepend a learnable prompt vector to word em-   beddings ( Lester et al . , 2021 ; Liu et al . , 2021b )   or attention layers ( Li and Liang , 2021 ; Liu et al . ,   2022 ) . Prompt - based learning has demonstrated ad-   vantages in low - parameter tuning ( He et al . , 2022 )   and few - shot / zero - shot performance ( Brown et al . ,   2020 ; Wei et al . , 2022a ) . We propose an iterative   prompting method for multi - answer QA based on   answer - conditional continuous prompts .   6.3 Iterative generation   Iterative generation ( a.k.a . progressive generation )   aims to decompose a challenging generation task   into multiple steps and progressively produce the   target sequence . Iterative generation has been ap-   plied to the tasks of machine translation ( Lee et al . ,   2018 ) , controllable text generation ( Casas et al . ,   2020 ; Zhang et al . , 2020 ) , storytelling ( Hua and   Wang , 2020 ; Tan et al . , 2021 ) , data - to - text ( Kasner   and Dusek , 2020 ) , etc . Recently , Wang et al . ( 2022 )   introduced an iterative prompting framework to pro-   gressively elicit knowledge from language models   for commonsense reasoning and multi - hop ques-   tion answering tasks ( Qi et al . , 2019 ; Xiong et al . ,   2021 ) . Compared to existing work , we propose   an answer - conditional prompting model and an ef-   fective task - specific pre - training scheme for multi-   answer QA.76767 Conclusions   In this paper , we have proposed AmbigPrompt for   multi - answer QA . AmbigPrompt is a simple yet ef-   fective model that answers ambiguous questions by   iterative prompting . We have proposed an answer-   conditional prompting model for prompt genera-   tion , and a task - adaptive post - pretraining scheme   for model training . Extensive experiments sug-   gest that AmbigPrompt achieves comparable per-   formance as high - capacity models and achieves the   best results in a low - resource setting .   Limitations   The limitations of this paper include the absence   of experiments on large language models . Previ-   ous studies have shown that using high - capacity   pre - trained language models can significantly im-   prove the accuracy of answers but also entails an   increase in computational overhead . Due to ( aca-   demic ) limitations of computational resources , this   paper employs a low - capacity T5 model for exper-   iments . Our experiments have suggested that the   proposed iterative prompting method that works   with the low - capacity model can achieve compara-   ble results with baseline methods equipping with   large models .   In future work , we would like to scale up the pro-   posed model to improve the model ’s performance .   Recent research on large language models ( LLMs )   has shown that they can learn from few examples   and reason well . We believe that it is worth ex-   ploring ways to enhance the prompting of LLMs   to improve their completeness when responding   to ambiguous questions and reduce model halluci-   nation in generation ( OpenAI , 2023 ; Zhao et al . ,   2023 ; Sun et al . , 2023b , a ) . Another direction worth   exploring in the future is the application in low-   resource scenarios , such as low - resource languages .   Low - resources in our study are characterized by   limited multi - answer - QA annotations , which aims   to examine how data size impacts model perfor-   mance . Other low - resource languages may behave   differently with less training data and large mod-   els ( Xue et al . , 2020 ; Sun et al . , 2021 ) . Besides ,   we would like to explore more effective prompting   methods , such as chain - of - thought prompting ( Wei   et al . , 2022b ) .   Ethics Statement   The paper has proposed a question - answering   model , which is intended to answer factoid open - domain questions . The model - predicted answers   still have a considerable amount of misinformation .   Besides , the proposed models rely on pre - trained   question - answering models , which are trained on   large - scale web data that is known to contain biased   or discriminatory content .   Acknowledgements   This work was supported by the National   Key R&D Program of China with grant No .   2020YFB1406704 , the Natural Science Founda-   tion of China ( 62272274 , 61972234 , 62072279 ,   62102234 , 62202271 ) , the Natural Science Foun-   dation of Shandong Province ( ZR2022QF004 ) , the   Key Scientific and Technological Innovation Pro-   gram of Shandong Province ( 2019JZZY010129 ) ,   the Fundamental Research Funds of Shandong Uni-   versity , the Hybrid Intelligence Center , a 10 - year   program funded by the Dutch Ministry of Educa-   tion , Culture and Science through the Netherlands   Organization for Scientific Research , https://   hybrid-intelligence-centre.nl .   All content represents the opinion of the authors ,   which is not necessarily shared or endorsed by their   respective employers and/or sponsors .   References767776787679   A Results on NQ   Table 4 lists the exact match ( EM ) score of the   baselines and AmbigPrompt on single - answer QA   benchmark , NQ - Open test . We see that the high-   capacity models ( e.g. , JPR ) , which benefit from   large language models like T5 - 3B , achieve better   EM score . However , in the multi - answer QA task ,   the models need to focus not only on the precision   of answers , but also on the diversity of answers   ( i.e. , recall rate ) . In AmbigQA , we can see that the   proposed model outperforms JPR , indicating its   superior ability to recall multiple feasible answers .   B Zero - shot evaluation on AmbigQA   We also test the proposed model and baselines on   AmbigQA in zero - shot setting following Min et al .   ( 2020 ) . In zero - shot evaluation , the models are   trained using partial supervision only ( i.e. , single-   answer NQ - Open ( Kwiatkowski et al . , 2019 ) ) , and   are evaluated on multi - answer data AmbigQA . This   setting provides a practical application where only   single - answer datasets are available . Note that   the zero - shot evaluation on AmbigQA allows the   model to tune some hyper - parameters ( e.g. , thresh-   old of generation probability ( Min et al . , 2020 ) )   using development data , which may make the set-   ting not zero - shot in the strictest sense .   The compared models are ( 1 ) DPR and SpanSe-   qGen , in which the models trained on NQ-   Open are adopted to predict multiple answers   via a thresholding strategy ( Min et al . , 2020 ) .   ( 2 ) FiD with various decoding methods , in   which FiD trained on NQ - Open produces mul-   tiple answers through ( a ) Nucleus sampling   with{p=0.8 , t=0.8 } ; ( b ) Top - k sampling with   { k=40 , t=0.8 } ; and ( c ) Diverse beam search with{b=3 , t=0.8,diversity_penalty = 0.5 } . We also   evaluate FiD with greedy decoding that gener-   ates one answer for each question as the default   setting of FiD. ( 3 ) AmbigPrompt , in which the   FiD answering model prompted by our proposed   answer - conditional prompting model is trained on   NQ - Open with our task - adaptive post - pretraining   method and produces multiple answers through   iterative prompting .   The results are listed in Table 5 . FiD series out-   perform DPR and SpanSeqGen as they utilize more   passages that potentially cover more feasible an-   swers . FiD with nucleus sampling obtains the best   results among different decoding methods . Ambig-   Prompt achieves the best zero - shot performance on   AmbigQA and also outperforms high - capacity su-   pervised baselines JPR on the multi - answer subset .   C Retrieval results   We train the dense retrieval model on NQ - Open   using in - batch negatives with batch size 64 . The re-   trieval model is initialized from CoCondenser ( Gao   and Callan , 2021 ) . Our retrieval corpus is the En-   glish Wikipedia from 12/20/2018 . Table 6 lists   the retrieval results on NQ - Open and AmbigQA .   In NQ - Open , we use Recall@k ( R@k for short )   as the metric , which considers retrieval to be suc-   cessful if at least one answer is included in the   top - k ranked passages . In AmbigQA , we use MRe-   call@k ( MR@k for short ) as the metric , which   considers retrieval to be successful if all answers or   at least k answers in the answer set Aare covered   by the top - k ranked passages . From the results , we   see that our retrieval model achieves comparable   results against baseline retrieval models , but un-   derperforms reranking models such as KPR and   MonoT5 .   D Case study   We present some examples in Table 7 and Table 8.76807681ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   On page 9 , Section " Limitations " .   /squareA2 . Did you discuss any potential risks of your work ?   On page 9 , Section " Ethics Statement " .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Section 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4 and 57682 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.7683