  Badr AlKhamissiFaisal Ladhak   Srini Iyer Ves Stoyanov Zornitsa Kozareva Xian Li Pascale Fung   Lambert Mathias Asli Celikyilmaz   Meta AIMona Diab   Abstract   Hate speech detection is complex ; it relies on   commonsense reasoning , knowledge of stereo-   types , and an understanding of social nuance   that differs from one culture to the next . It   is also difﬁcult to collect a large - scale hate   speech annotated dataset . In this work , we   frame this problem as a few - shot learning task ,   and show signiﬁcant gains with decomposing   the task into its " constituent " parts . In addi-   tion , we see that infusing knowledge from rea-   soning datasets ( e.g. ATOMIC ) improves   the performance even further . Moreover , we   observe that the trained models generalize to   out - of - distribution datasets , showing the supe-   riority of task decomposition and knowledge   infusion compared to previously used methods .   Concretely , our method outperforms the base-   line by 17.83 % absolute gain in the 16 - shot   case .   1 Introduction   Disclaimer : Due to the nature of this work , some   examples contain offensive text and hate speech .   This does not reﬂect authors ’ values , however our   aim is to help detect and prevent the spread of such   harmful content .   The task of automatically detecting Hate Speech   ( HS ) is becoming increasingly important given the   rapid growth of social media platforms , and the   severe social harms associated with the spread of   hateful content . However , building good systems   for automated HS detection is challenging due to   the complex nature of the task . It requires the sys-   tem to understand social nuance , such as which   groups are being targeted by the hateful content .   Prior work has shown that even humans can not   achieve a high agreement on whether or not a social   media post constitutes HS ( Rahman et al . , 2021 ) .   In this work , we explore whether decompos-   ing HS detection into subtasks that correspondFigure 1 : Hate Speech decomposed into several sub-   tasks leading to better results .   to the deﬁnitional criteria of what constitutes HS   ( i.e. the offensiveness of a post and whether it tar-   gets a group or an individual ) ( Davidson et al . ,   2017 ; Mollas et al . , 2020a ) would lead to sys-   tems that are more accurate and robust . In par-   ticular , we show that task decomposition leads to   more sample - efﬁcient systems for HS detection , by   showing improved results in the few - shot setting .   Moreover , we demonstrate that infusing common-   sense knowledge by ﬁne - tuning on the ATOMIC   ( Hwang et al . , 2021 ) and StereoSet ( Nadeem et al . ,   2021 ) datasets improve performance even further .   Speciﬁcally , we observe an absolute improvement   of17.83 % in the 16 - shot case over the baseline   model ( § 5.2 ) . Further , we show that the resulting   models are more robust , and are able to achieve   better performance than baseline methods in out-   of - distribution settings ( § 5.3 ) .   Explainability is an important aspect for being   able to identify and ﬁx failure modes of HS systems   ( Attanasio et al . , 2022 ) . To that end , we show that   task decomposition of HS detection moves us a   step closer towards explainable systems , allowing   us to identify the problematic subtasks that may be2109the bottleneck for improving overall performance   for HS detection . We then show that explicitly   targeting to improve the problematic subtask leads   to improved overall performance ( § 5.1.1 ) .   The remainder of the paper is structured along   four axes . We ﬁrst present the importance of ( 1 )   Task Decomposition and ( 2 ) Knowledge Infu-   sion for training better few - shot HS detection mod-   els . Speciﬁcally , we compare our method against   a baseline that only outputs a binary prediction for   HS . Thus showing the signiﬁcance of decompos-   ing the prediction into several subtasks and pre-   ﬁnetuning the model on two different reasoning   datasets that instill in the model a degree of com-   monsense reasoning and knowledge of stereotypes .   We evaluate our experiments across 10 seeds each   of which uses a different sampled dataset with sizes   ranging from 16 to 1024 samples . Our method ,   TK , show signiﬁcant improvementsover the   baseline . The trained models can also ( 3 ) General-   izebetter to three out - of - distribution datasets , and   are more ( 4 ) Robust to training data and hyperpa-   rameters . We demonstrate this by measuring the   variance across different seeds , data partitions and   hyperparameters and show that it is signiﬁcantly   smaller for the TKmodels compared to the   baseline .   2 Few - Shot Hate Speech Detection   Collecting a high quality large - scale dataset for HS   is difﬁcult since it is a relatively rare phenomenon ,   which makes it hard to sample social media posts   containing HS without relying on keywords that   may be indicative of it ( Rahman et al . , 2021 ) . How-   ever , despite being rare , its effects are of signiﬁcant   harm . Further , since HS is a complex phenomenon ,   relying on keywords may result in datasets with low   coverage that are not effective in capturing more   subtle forms of HS ( ElSherief et al . , 2021 ) . This   further results in building models that are less gen-   eralizable and can exhibit racial biases ( Davidson   et al . , 2019 ; Sap et al . , 2019 ) .   Motivated by these , we frame HS detection as a   few - shot learning task , where the model is given a   limited number of examples to learn what consti-   tutes HS , and explore whether we can build robust   HS detection models that can generalize well in   cases where we do not have a lot of training data . In   particular , we show that our trained TKmodelsare more generalizable by measuring the perfor-   mance on out - of - distribution HS datasets , and are   more robust by measuring the variance in perfor-   mance of HS detection across different randomly   sampled few - shot datasets and hyperparameters .   3 Datasets   SBIC ( Sap et al . , 2020 ) . We use the Social Bias   Inference Corpus ( SBIC ) to construct few - shot   training and validation sets . This corpus includes   posts from several online social media platforms ,   such as Reddit , Twitter , etc . , along with the annota-   tions for the offensiveness , targeted group as well   as the implications to further explain what stereo-   type is being implied by the post . While the dataset   does not have explicit labels for whether or not a   post is HS , we derive it using the annotations for   offensiveness and group detection , i.e. a post is   considered HS if it contains offensive / derogatory   language that is expressed towards a targeted group   ( see Figure 2 ) . This is consistent with the deﬁnition   of HS used by prior work ( Davidson et al . , 2017 ;   Mollas et al . , 2020a ) .   To construct few - shot training sets , we perform   a stratiﬁed sampling of data from the SBIC corpus ,   up to a target size n. We sampleexamples con-   taining inoffensive posts , examples containing   offensive , but non - HS posts . The remaining bud-   get ofsamples is used for posts containing HS ,   spread evenly across different targeted groups to en-   sure diversity . We create datasets of varying sizes   from 16samples up to 1024 , ensuring each smaller   dataset is a proper subset of the larger datasets . We   sample 10different datasets for each target size n   using 10different random seeds .   3.1 Reasoning Datasets   ATOMIC(Hwang et al . , 2021 ) This is a com-   monsense knowledge graph containing 1.33 M   inferential knowledge tuples in textual format   that are not readily available in pretrained LMs .   ATOMICencodes different social and physical   aspects of the everyday experience of human life .   In this work , we ﬁnd that training on human read-   able templates in - place of each tuple vastly im-   proves the downstream SBIC few - shot HS perfor-   mance . Examples of such templates are shown in   the Appendix ( § A.1 ) .   StereoSet ( Nadeem et al . , 2021 ) This dataset   was developed to measure stereotype bias in LMs .   Speciﬁcally , it contains 17k sentences that measure2110   biases across four different domains : gender , pro-   fession , race and religion . In this work , we only   ﬁnetune task - decomposed models on a subset of   StereoSet . In particular we only use stereotypes   that belong to the intersentence task since we found   that it results in better HS detection models . More   details on the StereoSet training can be found in   the Appendix ( § A.2 ) .   3.2 Out - of - Distribution Datasets   In addition , to test the generalizability of our mod-   els , we use the following three corpora to evaluate   out - of - distribution performance :   HateXplain ( Mathew et al . , 2021 ) . This corpus   includes posts from Twitter and Gab along with   the HS labels ( i.e. hate , offensive or normal ) , tar-   get community ( i.e. victim of the HS or offensive   speech ) and the rationales ( i.e. spans from the posts   that affected the annotator ’s decision ) . In our work ,   we convert the HS labels into a binary HS / non - HS   label , to measure performance for HS detection .   HS18 ( de Gibert et al . , 2018 ) . This corpus con-   sists of sentences from posts on Stormfront , a white   supremacist forum , along with labels for HS .   Ethos ( Mollas et al . , 2020b ) . This corpus ( com-   piled by researchers at the Aristotle University of   Thessaloniki ) consists of comments from social   media platforms ( YouTube and Reddit ) with binary   labels for HS , as well as a ﬁner - grained categoriza-   tion of the type of HS . We use the binary labels   in our work to measure the performance for HS   detection . Number of examples in test set of each   evaluation dataset is shown in Table 1.Dataset # Examples   HateXplain 1,924   HS18 9,916   Ethos 998   4 Experimental Setup   4.1 Task Decomposition   HS detection is a complex and subjective task , and   prior work has shown that it is hard to get high   agreements between humans about whether or not   a post constitutes HS ( Sanguinetti et al . , 2018 ; Assi-   makopoulos et al . , 2020b ) . Therefore recent efforts   on hate speech annotation have turned to more ﬁne-   grained , hierarchical annotation schemes that break   HS detection into subtasks that correspond to the   deﬁnitional criteria of what constitutes HS , leading   to higher agreement scores than reported by prior   work ( Assimakopoulos et al . , 2020b ; Mathew et al . ,   2021 ; Sap et al . , 2020 ; Rahman et al . , 2021 ) .   Motivated by these ﬁndings , we treat HS detec-   tion as a conditional generation task , since that   allows us to represent classiﬁcation and genera-   tion subtasks in a uniﬁed framework . The model   is given the set of tokens in a post as input and   is tasked with generating the inferences related   to the HS subtasks . Table 2 shows the lineariza-   tion scheme that we use to train our models . The   baseline model is tasked with generating a binary   prediction for HS , whereas the task - decomposed   model has to generate the predictions for the sub-   tasks , in order , before making the prediction for   HS . Speciﬁcally , the model predicts ﬁrst if the post   is offensive , then whether it is targeting a group   or an individual or neither , and following that it2111Input Output   Baseline Post : { POST } Hate speech ? { HS }   TK Post : { POST } Offensive ? { OFF } Target implication ? { GD } Targeted minori-   ties ? { GI , ... , GI N } Hate speech ? { HS }   predicts the identities of the targeted groups ( e.g.   disabled people ) . This forces the task decomposed   model to reason about the subtasks before decid-   ing whether or not a post constitutes HS . For all   sets of experiments , we ﬁnetune the pre - trained   BART model ( Lewis et al . , 2020 ) provided   by the HuggingFace library ( Wolf et al . , 2019 ) for   the task of HS detection . The results are shown in   Table 3 .   4.2 Knowledge Infusion   In the following set of experiments we ask whether   incorporating commonsense knowledge and stereo-   types into the model show signiﬁcant improve-   ments on the few - shot HS detection task .   ATOMICTo answer this question , we ﬁrst   ﬁnetune BART on the ATOMICdataset , where   each tuple is converted into a natural language state-   ment using human readable templates ( see § A.1 ) .   The resulting model achieves similar performance   on the held - out ATOMICtest set as the one re-   ported in Hwang et al . ( 2021 ) . Following that , we   further ﬁnetune the resulting model on both the   baseline and the task decomposed data from SBIC   as described in§4.1 . Results are shown in Table 4 .   StereoSet Here , we ﬁnetune both BART and our   model ﬁnetuned on ATOMICon stereotypical   sentences from the StereoSet dataset . Speciﬁcally ,   we similarly treat it as a conditional generation task ,   where we predict the context based on the sentence ,   bias type and target group ( see Appendix § A.2 ) .   5 Results   In this section , we report the mean binary F1 - scores   on the testing set across 10different seeds for each   dataset size for the HS detection task . Speciﬁcally ,   we compare the Baseline model , which directly   predicts a binary label of whether the post is HS or   not , with the TKmodel that employ task de - composition and knowledge infusion as described   in the previous section .   5.1 Task Decomposition   Table 3 shows the effect of task decomposition on   the HS detection task . In particular , we compare   the baseline model with a model that uses only   theOffensiveness andGroup Detection tasks as it ’s   subtasks before predicting the HS label . This is   referred to as the Minimal Decomposition model   since it uses the minimal constituents that we used   to derive the HS label . Following that , we add   theGroup Identiﬁcation to the subtasks and ob-   serve signiﬁcant improvements over the Baseline   in the few - shot setting . However , as the dataset   size increases beyond 512samples , the observed   differences in the mean are no longer signiﬁcant   ( see Figure 3c ) .   5.1.1 Fine - Grained Error Analysis   Task decomposition allows us to perform ﬁner   grained error analysis to identify failure modes of   the HS model . Speciﬁcally , we analyze whether   Offensiveness classiﬁcation or Group Detection is   more challenging for the model to learn . Figure 3a   shows the performance of the model for the Offen-   siveness subtask , while Figure 3b shows the perfor-   mance for Group Detection subtask with varying   number of training samples .   We observe that the overall performance for   Group Detection subtask consistently lags behind   Offensiveness prediction , especially when we have   fewer examples in the training data . We note that   the model is able to achieve a reasonable perfor-   mance ( ∼68 % ) for Offensiveness prediction even   in the few - shot regime . This suggests that Group   Detection subtask is the bottleneck in improving   the performance for HS classiﬁcation , and in order   to improve further we need our model to be more   accurate for this subtask .   Given these ﬁndings , we further explore whether2112Model 16 32 64 128 256 512 1024   Baseline 45.31 53.23 56.41 60.12 64.37 70.29 73.95   Minimal Decomposition 50.79 56.12 56.46 59.78 64.94 67.83 69.95   + Group Identiﬁcation 58.89 61.77 68.03 70.25 70.28 70.65 72.76   adding more ﬁne - grained information related to   groups would help improve the Group Detection   subtask . In particular , we additionally require the   task - decomposed model to generate the group that   is being targeted by the offensive content in the post .   Figure 3b shows the performance of the model in   predicting Group Detection when we require the   subtask model to identify the groups that are being   targeted . We see that incorporating this subtask   signiﬁcantly improves the performance for Group   Detection in the few - shot setting . Figure 3c shows   that this further translates to improved HS detection   in the same regime .   5.2 Knowledge Infusion   Table 4 show the binary F1 - scores across 10 seeds   on the SBIC testing - set for the HS detection task us-   ing different knowledge infused models for both the   baseline and task - decomposed datasets ( referred as   Subtasks ) . The ﬁrst row show the BART model   without any knowledge infusion ( same as the one   reported in Table 3 ) . The following row show the   results when we ﬁnetune the pre - trained BART on   StereoSet . It can be seen that this result is the best   performance in the 32 - shot regime . In the third   row we ﬁnetune BART on the natural languageversion of the ATOMICdataset . This increases   performance most noticeably in the 16 - shot regime .   The ﬁnal row shows the results when we further   ﬁnetune the model ﬁnetuned on ATOMICon   StereoSet , it consistently improves the performance   even further from the 64 - shot setting to the 512-   shot setting . In all models , the difference between   the baseline and subtasks model is signiﬁcant in   most cases until we reach 512 training examples . It   can be seen that knowledge infusion alone does not   seem to consistently improve performance over the   BART baseline model , however when combined   with subtask decomposition , it leads to the best re-   sults overall . This would imply that the reasoning   knowledge helps the model to better understand   relationships among subtasks .   5.3 Generalizability   Figure 4 compares the OOD performance of the   baseline model with subtasks models that were   trained with different degrees of knowledge infu-   sion . Note that all models were trained on the SBIC   data and evaluated for each of the three datasets in   a zero - shot manner , i.e. we do not perform any fur-   ther dataset speciﬁc ﬁnetuning . Similar to above ,   each point represents the mean F1 scores across2113Model 16 32 64 128 256 512 1024   BARTBaseline 45.31 53.23 56.41 60.12 64.37 70.29 73.95   Subtasks 58.89 61.77 68.03 70.25 70.28 70.65 72.76   + StereoSetBaseline 53.30 54.68 54.17 61.41 67.69 71.25 73.68   Subtasks 42.86 66.17 69.01 70.06 70.14 72.14 72.64   + ATOMICBaseline 44.76 49.60 64.89 69.38 70.09 72.32 73.97   Subtasks 63.14 62.01 67.96 70.94 70.16 72.29 72.96   + StereoSetBaseline 44.75 47.47 56.18 62.88 66.38 69.77 71.50   Subtasks 59.74 63.28 70.08 70.99 70.57 72.36 73.80   10different runs for the given dataset size . We   see that for all three datasets , the TKmodels   consistently outperforms the baseline , and shows   signiﬁcantly better zero - shot generalizability in the   few - shot setting . However , we note though that   HateXplain has a slight distribution shift ( since it ’s   also built based on Twitter as SBIC ) , and interest-   ingly it can be seen that only the results from Ethos   and HS18 ( in Figure 4 ) are signiﬁcantly different   from the binary ( baseline ) prediction model . This   implies that our model is indeed better at generaliz-   ing to out - of - distribution data .   5.4 Robustness   Here , we show that the TKmodels are less sen-   sitive to changes in hyperparameters and training   dataset selection . Speciﬁcally , Table 5 show the   average standard deviation across different hyper-   parameters on the validation set as a function of the   number of training samples . It can be seen that the   subtasks model is signiﬁcantly more robust than   the baseline model .   6 Analysis   In this section , we explore whether the order of the   subtasks matter or not , the scale of the model and   if adding an additional subtask that requires the   model to generate natural language explanations   as to why the post is considered HS helps improve   the overall HS performance . The results reported   here were done across only 5 seeds . In addition ,   we show a few examples that the baseline model   was not able to capture but the TKmodel got   correctly . # of Samples Baseline TK   16 15.75 2.44   32 14.15 0.55   64 13.71 0.87   128 16.44 1.66   256 21.20 2.69   512 17.29 3.36   1024 15.72 3.20   6.1 Does Order Matter ?   Table 6 shows the importance of the order of the   HS label in the sequence of subtasks . It can be seen   that placing HS at the end gets the best result when   the number of training samples are 64or less .   6.2 Do Implications Matter ?   Here , we add an additional subtask with the goal   of generating an implied stereotype in natural lan-   guage for a HS post or ‘ None ’ otherwise . For   example , given the following post : “ How do   you make a Muslim ’s phone explode ? Set it to   airplane mode . ” The implied stereotype would   be : Muslims are terrorists . Therefore ,   the model is tasked with generating such a sentence   before predicting the HS label . Table 7 shows the   HS detection performance before and after adding   the implications subtask in comparison with the2114   Order 16 32 64 128 256 512 1024   OFF GD GI HS 55.60 62.31 68.47 69.22 69.64 70.49 71.69   OFF GD HSGI 54.67 60.36 68.02 67.65 68.66 70.13 70.92   OFF HSGD GI 54.53 56.14 67.02 68.59 71.37 72.47 72.39   HSOFF GD GI 51.64 62.04 64.48 69.45 70.33 71.33 72.20   GD GI OFF HS 38.28 27.11 31.32 53.98 52.12 60.69 67.20   # of Samples Baseline TK + Impl   16 52.67 58.21 57.02   32 52.71 64.47 59.98   64 57.60 70.93 65.50   128 60.01 71.25 67.89   256 66.81 71.62 69.22   512 69.41 72.59 70.12   1024 74.72 74.09 71.45   baseline across 5 seeds for a given training set size .   It can be seen that although adding the implied   stereotype to the list of subtasks pushes the model   to performing better than the baseline when the   number of samples is less than 512 , it still falls   short to the TKmodel without the implication .   The reason behind this might be because the im-   plications were noisy and sometimes too generic ,   which is why it might have resulted in performance   degradation . Further , we believe that scaling up the   model to be an order or two magnitude larger will   enable a better utilization of the implications.6.3 Does Scale Matter ?   We train BART using the same task decom-   position and knowledge infusion methods reported   earlier . We ﬁnd that the results do not fully trans-   fer to smaller models . Speciﬁcally , the results are   only better when the training examples are 16 to 64 ,   otherwise the baseline model surpasses the TK   model . Showing that scale does matter . The results   are shown in Table 9 in the Appendix ( § A.3 ) .   6.4 Error Analysis   Figure 5 shows the percentage of false positives   ( FPs ) and false negatives ( FNs ) across 10 runs on   the testing set for the 16 to 128 - shot case . It can   be seen that the difference in performance between   both models is largely due to a considerable de-   crease in the percentage of FPs for the TK   model as we increase the number of samples , while   the percentage of FNs stay almost constant . How-   ever , the percentage of FNs are always less than the   FPs , which is a desirable property for HS models as   FPs can be more tolerated ( i.e. over enforcement )   than FNs ( i.e. under enforcement ) , since the latter   will lead to more propagation of hateful content .   As discussed in Section 5.1.1 , predicting the group   identity in the TKmodel enables the model   to correctly detect whether the post is targeting a2115   group or an individual , and we can see that clearly   in posts where the baseline model got a FN and the   TKmodel got the correct result . For example ,   the baseline model was n’t able to predict this post   as HS : “ Your approval is so worthless you should   pay people to take it @user Call me sexist but I do   not approve of female football presenters ” , while   the subtasks model identiﬁed correctly that it tar-   getsWomen leading to a correct prediction . On   the other hand , this is an example that the baseline   predicted correctly while the TKmodel did   not : “ What do you get when an epileptic falls into   a garden ? A seizure salad . ” The reason for this is   the TKmodel predicted that this post targets   anIndividual and not a Group .   7 Related Work   Social media provides a platform for users to con-   nect with people all over the world and engage   in ways that were not previously possible . Re-   cent surveys show that 41 % of internet users ex-   perienced some form of harassment online , with a   third of these cases being identity - related ( i.e. race ,   gender , sexual orientation , etc . ) ( V ogels , 2021 ;   League , 2020 ) . The sheer scale of content shared   on social media platforms makes manual modera-   tion untenable and necessitates automated methods   for detecting hateful content ( Halevy et al . , 2022 ) .   This has led to an increased interest in automated   hate speech detection , both in terms of collecting   corpora ( Poletto et al . , 2021 ) as well as improved   methods for hate speech detection ( Schmidt and   Wiegand , 2017 ; Fortuna and Nunes , 2018 ) .   Early work in hate speech detection has treated   the problem as a binary classiﬁcation task , requir-   ing annotators to simply indicate whether or not   a given post constitutes hate speech ( Waseem and   Hovy , 2016 ; Davidson et al . , 2017 ; Founta et al . ,   2018 ) . However , recent work has shown that elicit-   ing binary judgments for hate speech is unreliableand leads to poor inter - annotator agreement ( San-   guinetti et al . , 2018 ; Assimakopoulos et al . , 2020a ) .   This has lead to increased work in collecting hate   speech annotation with more complex annotation   schemas . Zampieri et al . ( 2019 ) propose a three-   level annotation schema that identiﬁes both the type   and target of offensiveness in social media posts .   Another line of work proposes a hierarchical anno-   tation schema where the task of determining hate   speech is broken down into subtasks , in an effort to   eliminate some of the subjectivity ( Assimakopou-   los et al . , 2020a ; Sap et al . , 2020 ) . Rahman et al .   ( 2021 ) combine established information retrieval   techniques with task decomposition and annota-   tor rationale , in order to create a higher quality   dataset for hate speech detection . While the afore-   mentioned studies explore the idea of task decom-   position in improving annotation consistency , our   work instead looks at the role of task decomposi-   tion in building more robust , generalizable models   for few - shot hate speech detection .   The focus of the limited prior work on few - shot   hate speech detection has been to explore zero-   shot / few - shot crosslingual transfer from a source   language ( such as English ) with sufﬁcient hate   speech data to a target language with limited data   ( Stappen et al . , 2020 ; Nozza , 2021 ) . In contrast ,   our work explores how task decomposition and   knowledge infusion can help even when there is   not sufﬁcient hate speech data in English .   8 Conclusion   In this work , we propose TK , a method to train   language models for detecting HS in the few - shot   setting . We show that it signiﬁcantly outperforms   comparable baseline models that predicts the HS   label directly instead of decomposing it into its con-   stituent parts . We further show that task decompo-   sition not only improves the performance , but also   allows for ﬁne - grained inspection of the model’s2116behavior . Since HS is a complex phenomenon that   requires a set of reasoning skills not readily avail-   able in such pre - trained models , we pre-ﬁnetune the   BART on both the ATOMICand StereoSet   datasets to equip the model with commonsense rea-   soning and knowledge of stereotypes that we show   leads to further improvement in HS detection per-   formance . We show that the TKmodels gener-   alize better to three out - of - distribution datasets in   the few - shot setting as well as being signiﬁcantly   more robust to training setups . We further analyze   the model ’s behavior in terms of the order in which   the HS labels appears , the scale of the model and   the performance when adding an additional subtask   that explains the implied stereotype of the post .   In future work , we plan on investigating the role   of task decomposition , knowledge infusion and the   additional subtask of explaining the implication   behind the post in large language models as well   as explore the TKmethod in low - resource lan-   guages , where it is expected to be most beneﬁcial .   9 Limitations   We note a few limitations of our work : ( 1 ) in our   experiments we compared our task - decomposed   model to standard models as baselines . It will   be valuable for future work to compare our mod-   els against other models of similar scale trained   using multi - task learning in a similar manner to   ( AlKhamissi and Diab , 2022 ) , where each clas-   siﬁcation head is subtask - speciﬁc and trained us-   ing categorical cross - entropy on the corresponding   number of classes . However , that would require cat-   egorizing the group identities into a discrete num-   ber of classes . ( 2 ) To the best of our knowledge   there is no literature that uses the SBIC dataset in a   few - shot hate speech setting , therefore we resorted   to the baseline with binary prediction using the   same conditional generation framework . Future   work should compare with such models . ( 3 ) The   datasets used in this work are mostly looking at   HS from a western perspective and are only in En-   glish . Different languages and societies may have   subtleties which may affect the performance of HS   systems . Even though we believe that our work   is generalizable beyond the English language , we   have not evaluated this , and we encourage future   work to look beyond the settings we explored in   this paper . ( 4 ) We follow prior work and determine   hate - speech labels based on majority vote which   might silence the voice of minority groups , whichis especially problematic in this context . In the fu-   ture we hope to model dissenting opinions between   the annotators similar to recent work ( Gordon et al . ,   2022 ) .   References21172118   A Appendix   A.1 ATOMICHuman Readable Templates   The ATOMICknowledge graph consists of tuples   in the form of / angbracketleftHead , Relation , Tail / angbracketright , where   Head andTail are provided in textual format .   However , we found that training BART on this   dataset directly led to sub - optimal results ; where   it is tasked to predict the Tail given the Head   andRelation . Instead we replaced each relation   with a human readable template as shown in Table   8 , where the Head is injected in place of { 0 } and   the model is tasked with predicting the Tail .   A.2 StereoSet Training   Nadeem et al . ( 2021 ) designed two types of Con-   text Association Tests ( CATs ): intrasentence and   intersentence to measure the degree of stereotyp-   ical bias in language models at the sentence level   and discourse level respectively . In this work , we   only consider the latter and only include stereotyp-   ical sentences ( i.e. omit the anti - stereotype exam-   ples ) . In the pre-ﬁnetuning step of StereoSet , the   BART model is tasked with predicting the expla-   nation of a given stereotype conditioned on the tar-   geted group , bias type and stereotype . The dataset   is linearized according to the following format :   Input : " Target : { TARGET_GROUP } Bias Type :   { BIAS_TYPE } Sentence : { STEREOTYPE } Con-   text ? " and the Output : is " { EXPLANATION } " .   For example : Input : " Target : { Cape Verde } Bias   Type : { race } Sentence : { They rely on the ports . }   Context ? " ; Output : " { Cape Verde has a lot of wa-   ter . } " .   A.3 Scale Experiments   In this analysis , we investigate the role of scale in   TK . Table 9 shows that the results are only   better from the 16 to the 64 - shot case , suggesting   that scale does play a role in the proposed method . Relation Human Readable Template   ObjectUse { 0 } is used for { 1 }   AtLocation You are likely to ﬁnd { 0 } in { 1 }   MadeUpOf { 0 } is made up of { 1 }   HasProperty { 0 } is { 1 }   CapableOf { 0 } can { 1 }   Desires { 0 } wants { 1 }   NotDesires { 0 } does not want { 1 }   isAfter Something that happens after { 0 }   is { 1 }   HasSubEvent Something you might do while   { 0 } is { 1 }   isBefore Something that happens before   { 0 } is { 1 }   HinderedBy { 0 } is hindered by { 1 }   Causes Sometimes { 0 } causes { 1 }   xReason { 0 } . The reason for PersonX do-   ing this is { 1 }   isFilledBy { 0 } can be ﬁlled by { 1 }   xNeed But before { 0 } , PersonX needed   { 1 }   xAttr { 0 } is seen as { 1 }   xEffect As a result of { 0 } , PersonX will   { 1 }   xReact As a result of { 0 } , PersonX feels   { 1 }   xWant After { 0 } , PersonX would want   { 1 }   xIntent Because of { 0 } , PersonX wanted   { 1 }   oEffect as a result of { 0 } , others will { 1 }   oReact as a result of { 0 } , others would   feel { 1 }   oWant as a result of { 0 } , others would   want { 1}2119Model 16 32 64 128 256 512 1024   Baseline 53.49 51.47 58.28 66.70 68.24 70.09 71.30   TK 55.23 60.29 63.68 65.10 67.52 68.67 69.662120