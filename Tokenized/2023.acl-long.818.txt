  Nitay Calderon   Technion – IITSubhabrata Mukherjee   Microsoft ResearchRoi Reichart   Technion – IITAmir Kantor   Microsoft   Abstract   Modern Natural Language Generation ( NLG )   models come with massive computational and   storage requirements . In this work , we study   the potential of compressing them , which is   crucial for real - world applications serving mil-   lions of users . We focus on Knowledge Distilla-   tion ( KD ) techniques , in which a small student   model learns to imitate a large teacher model ,   allowing to transfer knowledge from the teacher   to the student . In contrast to much of the pre-   vious work , our goal is to optimize the model   for a specific NLG task and a specific dataset .   Typically in real - world applications , in addi-   tion to labeled data there is abundant unlabeled   task - specific data , which is crucial for attaining   high compression rates via KD . In this work ,   we conduct a systematic study of task - specific   KD techniques for various NLG tasks under   realistic assumptions . We discuss the special   characteristics of NLG distillation and particu-   larly the exposure bias problem . Following , we   derive a family of Pseudo - Target ( PT ) augmen-   tation methods , substantially extending prior   work on sequence - level KD . We propose the   Joint - Teaching method , which applies word-   level KD to multiple PTs generated by both   the teacher and the student . Finally , we vali-   date our findings in an extreme setup with no   labeled examples using GPT-4 as the teacher .   Our study provides practical model design ob-   servations and demonstrates the effectiveness   of PT training for task - specific KD in NLG .   1 Introduction   Modern Natural Language Generation ( NLG ) sys-   tems are based on pre - trained Language Models   ( LMs ) , which are gradually achieving remarkable   milestones ( Raffel et al . , 2020 ; Brown et al . , 2020 ;   OpenAI , 2023 ) . Alongside the impressive advances   in applications such as Neural Machine Transla - tion ( NMT ) , Summarization , chatbots , such mod-   els have also become increasingly larger , deeper ,   slower , and more complex . The massive storage   requirements and high computational complexity   of NLG models discourage their deployment in   real - life . As such , there is a growing demand in   the industry for compressing such models while   preserving their performance .   Model compression methods typically either   prune less informative parameters ( LeCun et al . ,   1989 ) or use knowledge distillation ( KD ) ( Hinton   et al . , 2015 ; Kim and Rush , 2016 ) to transfer knowl-   edge from a larger model ( the teacher ) to a smaller   model ( the student ) . In generation tasks , KD can   be applied at the word - level , by training the student   to mimic the teacher ’s next token distribution , or   at the sequence - level , by training the student on   Pseudo - Targets ( PTs ) generated by the teacher .   Although KD research is extensive ( Gou et al . ,   2021 ; Gupta and Agrawal , 2022 ; Treviso et al . ,   2022 ; Xu and McAuley , 2022 ) , most works focus   on Natural Language Understanding ( NLU ) tasks ,   task - agnostic language modeling , or specific gen-   eration tasks ( e.g. , NMT ) . Additionally , KD works   for NLG typically consider large datasets with hun-   dreds of thousands of labeled examples , and ignore   unlabeled data ( Shleifer and Rush , 2020 ; Wang   et al . , 2021a ; Li et al . , 2022 ; Zhang et al . , 2022a ) .   In more realistic scenarios , however , the num-   ber of labeled examples is limited , alongside an   abundance of unlabeled data ( Oliver et al . , 2018 ;   Calderon et al . , 2022 ) that may contribute to KD .   To bridge these gaps , in this paper we conduct a   systematic study of KD for NLG , considering a   variety of tasks : Summarization , Question Gen-   eration , Abductive Reasoning , Style Transfer and   Simplification , in a more realistic setup .   Our realistic setup follows 5 criteria that are par-   ticularly attractive for a broad range of NLP prac-   titioners : ( 1 ) Only several thousand labeled exam-   ples are available for training ( Medium - resource),14632as annotation is costly or labor - intensive , especially   for NLG . This is in contrast to research setups   where labeled datasets can be very large . ( 2 ) Large   amounts of unlabeled data are available , as is of-   ten the case in industrial setups where unlabeled   data is collected during the life - cycle of the prod-   uct ; ( 3 ) Off - the - shelf models are used , which is   more practical than training models from scratch ;   ( 4 ) Inference - time efficiency is our goal , mean-   ing high compression rate ; ( 5 ) One - time computa-   tional training resources are negligible , compared   to inference - time , allowing extensive use of PTs .   Recently , huge LMs with excellent generative   capacity , such as GPT-4 ( OpenAI , 2023 ) have been   presented . While it is tempting to focus our re-   search on them , we focus on small to medium size   LMs in a fine - tuning setup . This choice is because   utilizing a huge LM as a teacher is often infeasible ,   e.g. , due to their high financial costs or when the   data can not be sent to external servers because of   privacy constraints . Furthermore , research suggests   that using mediator - teachers aids the distillation   process ( Mirzadeh et al . , 2020 ) , as might be the   case in distillation from a huge LM to a medium   fine - tuned teacher and finally to a small student .   For an extended discussion , see § 7 .   Our work hence focuses on a medium size fine-   tuned teacher and we assume there are several thou-   sand labeled examples for its fine - tuning . Despite   the above limitations , applying huge LMs in some   valuable setups is still possible . Therefore , we also   consider the distillation of one such model ( GPT-4 ) ,   although this is not our main focus .   We start our study by comparing architectural   ( Encoder - decoder vs. Decoder - only ) , pruning and   KD design decisions , discussing the tradeoff be-   tween computational resources and task perfor-   mance . We focus on practical measures like la-   tency andthroughput , which is important for batch-   offline applications and is typically overlooked .   We next provide the first exposure bias perspec-   tive for KD which motivates PT augmentation .   This bias derives from teacher forcing when the   LM conditions on ground - truth tokens during train-   ing , while at inference time it conditions on previ-   ously generated tokens ( Ranzato et al . , 2016 ) . As   the distillation progresses , the student ’s predictions   gradually become similar to its teacher ’s , and there-   fore training with PTs can alleviate exposure bias .   We propose extensions of the common practice   of generating a single mode approximation PT viabeam search , instead , we suggest sampling multi-   ple PTs to facilitate higher exposure to conditional   distribution factors . Additionally , we generate PTs   for unlabeled data and demonstrate their effective-   ness . Moreover , we propose a novel KD technique   termed Joint - Teaching , which applies word - level   KD to PTs generated by both the teacher and the   student . This technique aims to implicitly and ex-   plicitly address the student exposure bias , ground   the learning and teach it to correct its mistakes .   Finally , we extend the scope of our study by   working solely with limited unlabeled examples .   Due to the absence of labeled examples , fine - tuning   the teacher is infeasible , leading us to depend   on huge LMs with zero - shot capabilities . Con-   sequently , we investigate whether our KD findings   from our realistic setup ( which involves a fine-   tuned teacher ) remain applicable to the new ex-   treme setup . To this end , we show how to success-   fully distill GPT-4 , a huge Decoder - only model ,   into a small Encoder - decoder model ( T5 - small ) ,   which also has a different tokenizer .   Our main empirical findings ( § 5 ) are : ( 1 )   Encoder - decoder architectures outperform their   Decoder - only counterparts in task - specific fine-   tuning for NLG ; ( 2 ) Decoder pruning substantially   outperforms encoder pruning when considering   both latency and task performance ; and ( 3 ) PTs can   be used much more effectively compared to what   was suggested in previous work and this yields sub-   stantially improved task performance on a much   reduced computational cost .   2 Background and Related Work   2.1 Natural Language Generation   Modern LMs based on Transformers leverage two   primary architectures for text generation : Encoder-   decoder ( ED ) ( Vaswani et al . , 2017 ) and Decoder-   only ( DO ) ( Radford et al . , 2019 ) . While ED models   are more popular for classification , summarization ,   and NMT , DO models excel on open - text genera-   tion and zero / few - shot setups ( Wang et al . , 2022 ) .   Nonetheless , the increasing popularity of mas-   sive DO models like GPT-3 and PaLM ( Brown   et al . , 2020 ; Chowdhery et al . , 2022 ) with impres-   sive generation capabilities , has led to the question   ofwhether ED models are still relevant for NLG ?   In § 5 and in the appendix ( § B ) we discuss and   demonstrate the differences between these two ar-   chitectures . We show in line with recent work of   Tay et al . ( 2022 ) that ED models outperform DO14633models in task - specific fine - tuning for conditional   NLG . In light of this observation , we focus on KD   only for ED models in the rest of the paper .   Text generation is a structured prediction prob-   lem where the goal is to sample a text ˆyfrom the   distribution learned by the LM , conditioned on the   input text x. The training objective of the LM is to   minimize the negative log - likelihood ( NLL ) of the   training dataset , by factorizing −logP(y|x)into   −/summationtextlogP(y|x , y ) . At inference time , the   LM generates one token at a time according to the   conditional distribution : P(y|x,ˆy ) . The selec-   tion of the next token is handled by the decoding   method . Beam search , which aims to find the most   likely target , is the the de - facto standard ( Zarrieß   et al . , 2021 ) . Alternatively , it is possible to frame   decoding as sampling , as we do in this work .   2.2 Exposure Bias   LMs learn the distribution P(y|x , y)at the train-   ing phase by conditioning on the ground truth y.   This is known as teacher forcing which makes the   training efficient and stable but also creates a mis-   match at inference time , since the LM conditions   on its previous predictions ˆy . This discrepancy   between training and inference is called exposure   bias . Potential side - effect is that a single error   during generation may have a cascading effect by   causing a deviation from the ground truth distri-   bution and resulting in an accumulation of errors   ( Arora et al . , 2022 ) . Many works link exposure bias   to generalization , hallucinations , and degeneration   ( Schmidt , 2019 ; Chiang and Chen , 2021 ) .   Recent works attempted to address exposure   bias , most of which focused on open - text genera-   tion and NMT ( Schmidt , 2019 ; Wang and Sennrich ,   2020 ; Hormann and Sokolov , 2021 ) . Other works   addressed this problem by applying reinforcement   learning techniques ( Ranzato et al . , 2016 ) or by   scheduled sampling which replace ground truth to-   kens with generated tokens ( Bengio et al . , 2015 ;   Liu et al . , 2021b ) . However , it leads to training   with inaccurate and noisy signals ( Xu et al . , 2019 ) .   In contrast to other works which study this problem   in a general setting , in KD setting the teacher can   be used to mitigate the student exposure bias by   utilizing PTs and reliable signals from it . This is   the first work to discuss exposure bias in KD .   2.3 Compression and Knowledge Distillation   There has been extensive research on model com-   pression on techniques such as parameter sharing , pruning , quantization and factorization ( Gou et al . ,   2021 ; Gupta and Agrawal , 2022 ; Treviso et al . ,   2022 ; Xu and McAuley , 2022 ) . Pruning ( LeCun   et al . , 1989 ) aims to discard unimportant weights of   a pre - trained or fine - tuned LM , making it more ef-   ficient while preserving performance . Usually , the   pruning is structured , and complete blocks , rows ,   or layers are removed according to their magni-   tude , changes during training ( Sanh et al . , 2020 ) ,   or causal effect ( Rotman et al . , 2021 ) .   Typically there is a performance gap between   the original and the compressed model , which can   be closed by applying Knowledge distillation ( KD )   ( Hinton et al . , 2015 ) – a technique for transfering   knowledge from a large trained model ( teacher T )   to a smaller one ( student S ) , by training the student   to mimic the teacher ’s predictions or features . KD   can be divided into two categories : task - agnostic ,   where the goal is to mimic a pre - trained LM ’s be-   havior , and task - specific , where the distillation is   performed on a fine - tuned LM . Generally , there   are three levels of KD : word - level ( or class - level ) ,   inner - level , and sequence - level ( only in NLG ):   Word - level KD , also known as Logits KD ( Hin-   ton et al . , 2015 ; Kim and Rush , 2016 ) . In this   method , the student learns to match the teacher ’s   distribution over the next token at each position ,   by minimizing the KL divergence between the dis-   tribution of the student P(y|x , y)and the dis-   tribution of its teacher P(y|x , y ) . There are   variations like Noisy KD ( Liu et al . , 2021a ) where   noise is injected during KD by applying dropout to   the teacher , Wang et al . ( 2021a ) which applies KD   only for carefully selected examples , etc .   Inner - level KD aims to mimic additional inner   features of the teacher , for example , Jiao et al .   ( 2020 ) leverages hidden states of the teacher to   train the student . Wang et al . ( 2020 ) and Wang   et al . ( 2021b ) proposed Attention - relations KD   which trains the student to mimic the relation ma-   trix ( scaled dot - product ) of the self - attention states .   Sequence - level KD is commonly used for NMT   ( Kim and Rush , 2016 ; Kim et al . , 2019 ; Kasai et al . ,   2020 ) . In this approach , the teacher generates PTs   for inputs in the original dataset , and student is   trained to predict them . Usually , the teacher gener-   ates a single PT using beam search , which is known   as “ mode approximation ” of P(y|x ) .   Gou et al . ( 2021 ) and Gupta and Agrawal ( 2022 )   present a detailed overview of KD techniques . No-   tably , most works in NLP explore task - agnostic KD14634   for encoder - only models ( Sanh et al . , 2019 ; Jiao   et al . , 2020 ; Wang et al . , 2021b ) or focus on NMT   ( Kim and Rush , 2016 ; Kasai et al . , 2020 ; Wang   et al . , 2021a ) . Shleifer and Rush ( 2020 ) focused   on high - resource summarization , and compared   three KD strategies : pruning and fine - tuning , logits   KD , and mode approximation PTs . Unlike these   works , we perform a more systematic study of task-   specific KD for a variety of NLG tasks in realistic   setups . Moreover , we focus on PTs and propose   extensions to demonstrate their effectiveness .   3 Methods   3.1 Research Design   Our research design illustrated in Figure 1 has eight   stages . At each stage , we examine different model-   ing decisions and continue to the next stage after   selecting the best technique according to the perfor-   mance on the development set ( to avoid performing   selection on the test set ) . We linearly examine one   aspect at a time since the alternative ( combinatorial   choices ) is too expensive . Our study starts with   architectural designs ( stages 1 - 2 ) , continues with   comparing different KD strategies ( stages 3 - 4 ) and   proceeds to explore the usage of PTs as augmenta-   tion strategies for KD ( stages 5 - 8) .   3.2 Architectures and Pruning   In the spirit of our realistic setup , we consider off-   the - shelf LMs and experiment with two model fam-   ilies for each architecture type ( see § 4.2 ) . In ap-   pendix § B we discuss the differences between ED   ( Encoder - Decoder ) and DO ( Decoder - only ) archi-   tectures ( stage 1 ) and show that ED models outper-   form DO models on task - specific tuning for NLG .   Following that , we present results only for ED in   § 5 . In stage 2 , we examine the effect of pruning ,   by discarding complete model layers . In the caseof ED , layers can be dropped either from the en-   coder or decoder components , resulting in different   impacts on the task or computational performances .   3.3 Objectives   As discussed in § 2.3 , various works proposed dif-   ferent training strategies for KD . In stage 3 we   perform a comparison between three popular KD   objectives ( baselines ) , which do not involve PTs :   ( 1)Logits KD – which is the most common and   the simplest technique ; ( 2 ) Noisy KD – which   showed promising results for summarization in self-   distillation setup ; and ( 3 ) Attention - Relations KD   ( combined with Logits KD ) – which is the SOTA   technique for Encoder - only models .   As suggested by Mukherjee and Awadallah   ( 2020 ) , following the end of the KD stage , we   also perform an end - to - end fine - tuning stage on   the ground truth labels . This stage is extremely   cheap since a teacher is not required .   3.4 Pseudo - Targets ( a.k.a sequence - level KD )   Pseudo - Targets ( PTs ) are predictions generated by   the teacher that can be utilized for training the stu-   dent . Word - level or Inner - level KD can be com-   bined with sequence - level KD ( e.g. , by applying   Logits KD to PTs ) . In stage 4 we investigate the   impact of augmenting the labeled data with PTs   when fine - tuning the student ( sequence - level KD )   or when using the objective from stage 3 .   Although various works demonstrated the effec-   tiveness of PTs ( Kim and Rush , 2016 ; Shleifer and   Rush , 2020 ) , their use of PTs was limited to a sin-   gle PT per labeled training example , generated with   mode approximation beam search . In this paper we   demonstrate that the use of PTs can be much more14635   extensive : We generate multiple PTs per training   example , increase their diversity with sampling-   based rather than mode approximation generation ,   and generate PTs for both labeled and unlabeled   examples , which are much more abundant by na-   ture . Our experiments demonstrate that each of   these extensions yields substantial improvements   in the quality of the resulting student model . We   next touch on each of these extensions .   Unlabeled data In our setup unlabeled data is avail-   able in abundance . Since in autoregressive NLG the   LM learns to condition on the targets ( y ) , PTs   are essential for utilizing unlabeled data ( inputs   without corresponding targets ) . From a general-   ization perspective , exposing the model to more   inputs , and consequently to more P(y|x,ˆy)fac-   tors , should help the student generalize beyond   the labeled data distribution . Indeed , many works   in various NLP fields have shown that unlabeled   data is effective for generalization ( Xie et al . , 2020 ;   Mukherjee and Awadallah , 2020 ; Calderon et al . ,   2022 ) . In stage 5 we examine its importance .   Multiple PTs We further explore alternatives to the   common practice of generating a single PT with   beam search ( mode approximation ) . Unlike clas-   sification , NLG is a structured prediction problem   and multiple candidates can form a correct solution .   Therefore , we can generate multiple PTs resulting   in stronger exposure to the teacher ’s knowledge .   We explore the impact of multiple PTs in stage 6 .   Sampling PTs Beam search is not the only way   to generate PTs . In fact , it has been demonstrated   to produce generic texts that lack diversity ( Finkel   et al . , 2006 ; Gimpel et al . , 2013 ) . A simple alterna - tive that can produce more diverse and surprising   texts is sampling ( Roberts et al . , 2020 ; Holtzman   et al . , 2020 ) . Moreover , controlling the temperature   of the logits can increase the diversity of the PTs   even further ( Tevet and Berant , 2021 ) . We compare   these decoding techniques in stage 7 .   Motivation for these Extensions Compared to a   single mode approximation PT , sampling multiple   PTs for both the labeled and unlabeled examples   should add more variability to the student training   and cover a larger portion of the learnable distribu-   tion , which are known to improve generalization .   Furthermore , these extensions expose the student   to more of the teacher ’s knowledge .   Additionally , we also provide an exposure bias   motivation . During the distillation the student ’s pre-   dictions gradually become similar to its teacher ’s   predictions : ˆy∼ˆy . Therefore , we can expect   that training the student with diverse PTs may mit-   igate its exposure bias , which occurs at inference   when it conditions on ˆy , and not on the ground-   truth distribution . In addition , PTs of unlabeled   examples can help mitigate this bias as the stu-   dent is getting exposed to the teacher ’s knowledge   rather than the gold standard . Moreover , multiple   and diverse PTs results in extensive exposure to   additional P(y|x,ˆy)factors . Therefore , we hy-   pothesize that sampling multiple PTs will improve   the student compared to a mode approximation PT .   3.5 Joint - Teaching   As mentioned above , training with PTs generated   by the teacher may implicitly mitigate the student   exposure bias . On the other hand , we can try to   mitigate this bias explicitly by training the student   while conditioning on its predictions ( i.e. generate   PTs with the student and use them for training ) .   Generally , this can be unstable since the student   may learn from its own mistakes . Fortunately , in   KD we have a strong oracle : the teacher . By apply-   ing word - level KD on ˆy , the teacher can teach   the student how to continue its generated sequence   correctly and prevent a cascading effect .   Nevertheless , this relies on the reasonable as-   sumption that the teacher models P(y|x,ˆy)bet-   ter than the student . In Figure 2 we present a single   setup analysis that supports this assumption : At   almost any stage of the student training , continuing   the generation with the teacher results in better pre-   dictions . Moreover , as the student becomes more   similar to the teacher , we can expect the teacher to   model P(y|x,ˆy)even better , which makes the14636word - level signals more reliable . This is also sup-   ported by Figure 2 : As the distillation progresses ,   the teacher continuations keeps getting better .   Following that , we propose a novel KD method   which addresses the exposure bias implicitly and   explicitly namely Joint - Teaching : Apply word-   level KD on PTs generated by both the teacher   and the student . In our experiment we randomly   use the student ’s PTs for 50 % of the training steps .   Instage 7 we compare training only with the stu-   dents ’ PTs or the teachers ’ PTs to Joint - Teaching ,   demonstrating the superiority of the latter .   4 Experimental Setup   In this section we describe our four NLG tasks and   datasets , the participating models and the evalua-   tion procedures . URLs of the code and datasets , as   well as implementation details and hyperparameter   configurations are described in § D. Additionally ,   a comparison between ED and DO architectures   ( stage 1 ) is provided in § B.1 ; theoretical and em-   pirical complexity analyses are provided in § B.2 .   4.1 Tasks and Datasets   We selected four English - to - English core NLG   tasks , which are part of several NLG benchmarks   and surveys ( Fu et al . , 2018 ; Gehrmann et al . , 2021 ,   2022 ; Khashabi et al . , 2021 ; Erdem et al . , 2022 ; Jin   et al . , 2022 ) . We built a new realistic experimen-   tal setup , in which the ratio of labeled to unlabeled   data is 1:4 , and the amount of labeled data is reason-   able . For each task ( excluding Shake7 ) we keep the   original assignment of each example to its train - test   splits . The exact numbers are provided in Table 1 .   Summarization ( XSUM40 ) We use the XSUM   dataset ( Narayan et al . , 2018 ) for the abstractive   summarization task . The task of the NLG model   is to generate an introductory sentence ( summary )   for a given news article .   Question Generation ( SQuAD17 ) We use the   SQuAD dataset ( Rajpurkar et al . , 2016 , 2018 ) for   the question generation task . Given a Wikipedia   document and an answer to the question , the task   of the NLG model is to generate the question .   Abductive Reasoning ( ART10 ) We use the αNLG   ( also known as ART ) dataset ( Bhagavatula et al . ,   2020 ) for abductive reasoning generation task . The   task of the NLG model is to generate a plausible   explanation for two given observations .   Style Transfer and Simplification ( Shake7 ) We   construct a new dataset for the well - explored style   transfer task ( which is also a simplification task ) of   translating Shakespeare ’s texts to modern English .   We combined pairs of Shakespearean and modern   English texts from Shakespeare ’s plots ( taken from   Xu et al . ( 2012 ) ; Jhamtani et al . ( 2017 ) ) , with other   texts written by Shakespeare ( Karpathy , 2015 ) and   created a parallel style transfer dataset , see § D.1 .   4.2 Models and Pruning   Decoder - only We use the GPT2 - family models   ( Radford et al . , 2019 ): GPT2 , GPT2 - M , and GPT2 - L ;   and the recent OPT - family models ( Zhang et al . ,   2022b ): OPT-125 M andOPT-350 M .   Encoder - decoder We use the T5 - family models   ( Raffel et al . , 2020 ): T5 - S andT5 - L ; and the BART-   family models ( Lewis et al . , 2020 ): BART-6:6   ( base version ) and BART - L .   Pruning We apply pruning only for the pre - trained   BART-6:6 model ( thus our study also includes a   non - pruned student , T5 - S ) , and consider two types   of pruning : Encoder pruning and decoder prun-   ing . Following Shleifer and Rush ( 2020 ) , in both   pruning types we keep only the first and last lay-   ers , resulting in two models : BART-2:6 ( pruned   encoder ) and BART-6:2 ( pruned decoder ) .   In the KD stages ( 3 - 8) we use two student-   teacher pairs : T5 - S andT5 - L , and a pair with a   pruned student : BART-2:6 andBART - L .   4.3 Evaluation   Task Performance We report on various metrics   that focus on different aspects , resulting in a more   holistic evaluation of the models . To this end ,   we focus on the lexical similarity metrics , BLEU   and ROUGE , the semantic equivalence metric   BERTScore ( BS , Zhang et al . ( 2020 ) ) and the sta-   tistical modeling metric Perplexity ( PPL ) , which is   measured by the average NLL of the ground truth   targets . To make the result tables more readable ,   we report the average ROUGE ( of the F1 scores   for R-1/2 / L ) , and the F1 score for BS . Notice that   in § D we specify for each task the appropriate met-   ric we use for the development set . In § E we report   the scores of all the metrics.14637   Computational Performance For measuring the   computational performance of the models , we re-   port the number of parameters , the memory of the   models and the number of floating - point operations   ( FLOPs ) . These measures are device - agnostic and   may not be well correlated with the actual perfor-   mance in practice , which depends on the device ,   implementation , and hardware utilization of the   accelerators ( Ma et al . , 2018 ; Hadidi et al . , 2019 ) .   Therefore , we also report practical measurements   such as the latency of generating a single output ,   which is important for real - time applications , and   thethroughput , which is the maximum number of   examples that can be processed in a minute , and is   important for offline batched applications .   5 Results   The complete results are provided in § E. Table 2   reports the results of fine - tuned models ( stages 1-   2 ) . Table 3 reports the results of the KD stages   ( 3 - 8) as follows : For each student - teacher pair and   dataset , we calculate the fraction of their perfor-   mance gap that is compensated for by using distil-   lation as opposed to only fine - tuning the student   model:% , where KD , TandSare the task   scores of the distilled student , its teacher and the   student baseline ( fine - tuned ) , respectively . Then ,   we report for each dataset the average fraction of   the closed gap over four metrics and two student-   teacher pairs . We also report the number of wins   within 32 setups ( 4 datasets , 4 metrics , 2 pairs ) .   S1 : Encoder - decoder models outperform   Decoder - only models in task - specific tuning for   NLG . We present our results in Table 2 . For a   detailed analysis of Encoder - decoder ( ED ) and   Decoder - only ( DO ) models , we refer readers to   Appendix § B , which reports several interesting the - oretical and empirical insights . Nevertheless , it is   worth noting here that ED models , such as T5 - L ,   can have twice the number of layers and parame-   ters of DO models , such as GPT2 - M orOPT-350 M .   However , despite the higher number of parame-   ters , ED models have roughly the same FLOPs and   comparable latency and throughput .   Regarding task performance , our experiments   demonstrate that ED models consistently outper-   form DO models across all datasets and models ,   regardless of their size . Presumably , a better induc-   tive bias is injected by applying self - attention ( and   not autoregressive - attention ) to the conditioned in-   put sequence . This finding is particularly relevant   for NLP practitioners who aim to develop a spe-   cialized in - house model for a specific NLG task .   We hence continue to the KD stages only with ED   models ( T5 and BART ) .   S2 : It is better to prune layers from the decoder .   In stage 2 , we examine whether it is better to prune   encoder or decoder layers . To this end , we prune   BART-6:6 and report the results at the bottom of   Table 2 . First , notice that pruning decoder layers   greatly impacts the latency given the autoregressive   nature of NLG tasks , making BART-6:2 two times   faster than BART-6:6 . For comparison , pruning   encoder layers does not affect the latency ( see the   discussion in § B.2 ) . On the other hand , BART-2:6   has a higher throughput than BART-6:2 , mainly   because of the long input in some tasks which is   processed by the encoder . Notice , however , that   the improvement of BART-6:2 in latency is more   substantial than its throughput degradation .   Second , BART-6:2 outperforms BART-2:6 in ev-   ery task metric ( and dataset ) , being competitive   toBART-6:6 . Moreover , for tasks with long in-   puts ( e.g. , summarization or question generation,14638   see § E ) , the depth of the encoder is critical and   the pruned - encoder BART-2:6 underpeforms . As a   rule of thumb , our results suggest that it is better   to prune layers of the decoder . Besides reducing   the model latency , it has a smaller impact on task   performance . In the following stages we use two   student - teacher pairs : T5 - S andT5 - L , and a pair   with a pruned student , BART-6:2 andBART - L .   S3 : Use Logits KD as the main training objec-   tive . In stage 3 we compare different KD objec-   tives . As seen in Table 3.A , Logits , Noisy and   Attention - Relations KD techniques are competi-   tive , and the quality of the method depends on the   task . Even though Noisy KD has more wins than   Logits KD , the PPL metric accounts for 8 of the   14 wins . Since Logits KD is the best - performing   method according to the average performance on   the development set , we continue to the next PT   stages with it . Our results demonstrate the impor-   tance of KD : applying Logits KD closes more than   34.4 % of the student - teacher gap , on average .   S4 : Combine Logits KD and PTs . In stage 4 we   examine three methods : using Logits KD only on   the labeled examples , fine - tuning the student with   PTs ( Sequence - level KD ) or combining them . Thecorresponding rows in Table 3 show that sequence-   level KD underperforms Logits KD . However , their   combination results in a better student in 22 setups   and achieves a higher development score , and there-   fore , we use this strategy in the subsequent stages .   S5 : Unlabeled data should be utilized . Generat-   ing PTs for the unlabeled inputs may help extract   more of the knowledge embodied in the teacher ,   allowing the student to generalize better . In stage 5   we explore this hypothesis . According to Table 3.C ,   utilizing unlabeled data greatly boosts the perfor-   mance and closes an additional 19 % of the gap . To   the best of our knowledge , this is the first study that   shows this in KD for NLG . In the next stages , we   generate PTs for the labeled and unlabeled inputs .   S6 : Exposing the student to multiple PTs helps .   By comparing the rows of Single PT and K - Beams   in Table 3.D , it can be seen that exposing the stu-   dent to multiple targets and covering a larger por-   tion of learnable distribution closes an additional   6.4 % of the gap on average .   S7 : Sampling is better than Beam - Search for   generating PTs . Table 3.D also shows that gen-   erating PTs with sampling is typically better than   beam search , and closes another 5.2 % of the gap   on average . We observe that high sampling tem-   perature is competitive , although its effect depends   on the task and model . High sampling works bet-   ter for T5 - S , while sampling without temperature   works better for BART-6:2 ( and on average ) . Fur-   ther research could investigate a larger range of   temperatures and other diversity - oriented decod-   ing methods . Nevertheless , this is the first study   that challenges the traditional mode - approximation   practice , and show that generating multiple PTs via   sampling significantly improves NLG distillation .   S8 : Joint - Teaching improves the student . The   results in Table 3.E support two of our hypotheses ,   which we discuss in § 3.5 . The first is that PTs   generated only by the student are less valuable for   its training than PTs generated by teacher . The   second is that the combination of the two types of   PTs ( by Joint - Teaching ) can be more effective for   KD than using only PTs generated by the student   or teacher . Our Joint - teaching approach wins 25   out of 32 times and closes another 5.7 % of the gap .   Final Compression Results . The final compres-   sion results ( after stage 8) are provided in Table 4 .   We attempt to achieve high compression rates :   T5 - KD andBART - KD reduce 92 % and 75 % of their14639   teachers ’ parameters , respectively . This results in   great computational performance improvements .   Our distilled models reduce the latency of their   teachers by a factor of 3.7 . In addition , T5 - KD has a   10 times higher throughput , and BART - KD has dou-   ble the throughput of its teacher . Our study shows   that KD allows model compression and drastically   improves the task performance compared to the   fine - tuned baseline . In most setups , our recipe for   KD closes more than 75 % of the student - teacher   gap . Surprisingly , in some of the tasks like Shake7   the distilled model outperforms its teacher . Finally ,   we also conduct a human evaluation to examine the   relatively lower performance of our KD method   on the ART10 dataset ( see appendix § F ) . Our hu-   man evaluation results show that the distilled model   ( T5 - KD ) closes 72 % of the gap , and this is in - line   with the performance on other datasets .   5.1 Extreme setup : KD with GPT-4   In the final phase , we explore the transferability of   our KD conclusions to an extreme setup which in-   volves only limited unlabeled examples . As labeled   examples are unavailable , fine - tuning the teacher   becomes impractical , leading to the reliance on a   huge LM with zero - shot capabilities as the teacher ,   and this poses new challenges : ( 1 ) The teacher is   a huge Decoder - only model ( since this is the stan-   dard for zero - shot learning ) while the student is an   Encoder - decoder model ; ( 2 ) The teacher and the   student have different tokenizers and ( 3 ) Querying   the teacher is financially costly , limiting its usage .   We utilize GPT-4 ( OpenAI , 2023 ) as our teacher   andT5 - S as the student . The prompt of GPT-4   consists of three labeled demonstrations . Due to   its high cost , we conduct experiments only for theSQuAD17 ( 3000 examples ) and the Shake7 ( 1500   examples ) datasets , and with the following base-   lines and methods : ( a ) The GPT-4 teacher ; ( b ) T5 - S   training with ground - truth ( GT ) labels ; ( c ) Student   fine - tuning with a single PT ; ( d ) Fine - tuning with   multiple ( five ) PTs ; ( e ) Student training with Logits   KD and a single PT ( f ) Logits KD with multiple   PTs ; More details are provided in § C.   Our results in Table 7 ( appendix § C.2 ) are mixed :   Generating multiple PTs outperforms a single PT ,   but Logits KD only helps in the SQuAD17 dataset .   Future research is called for as we attribute this   result to challenges in aligning the tokenizers .   6 Conclusion   In this paper , we present a general KD recipe for   NLG . To this end , we conduct a systematic study   on various tasks and evaluate the impact of different   modeling decisions on computational and task per-   formance of distilled models . Our results suggest   that using ED models as students , pruning decoder   layers , combining Logits KD and PTs via sampling   and Joint - Teaching achieve high compression rates   while maintaining competitive performance .   Nevertheless , our recipe is based on average per-   formance and may depend on the task , model , or   setup . The teacher - student performance gap that   still exists demonstrate the need for further research .   For example , high - temperature PTs seem to be less   effective for BART , and further exploration of dif-   ferent hyperparameters or methods for increasing   PT diversity may be necessary . Integrating a smart   selection of training examples or PTs ( Wang et al . ,   2021a ) , refining Joint - Teaching with curriculum   learning or scheduling ( Liu et al . , 2021b ) are some   future research directions.146407 Limitations   Using a medium size fine - tuned teacher .   With recent advances in huge LM such as GPT-   4 and their extraordinary generation capabilities ,   one may wonder about the relevance of this work   which mainly focuses on a medium size fine - tuned   teacher . Although we show the distillation of a   huge LM ( GPT-4 ) , it is often infeasible .   First , when the data can not be sent to external   servers because of privacy constraints or when the   domain is unique or specific ( e.g. , in national se-   curity settings or human conversations ) , huge LMs   that can not be fine - tuned may be less effective .   Second , we have distinguished between two   types of costs : computational and financial . While   training a student model with a medium - size fine-   tuned teacher may take a few days , the entire pro-   cess is feasible since training time is typically not a   limited resource . In contrast , generating PTs with a   huge LM like GPT-4 can easily cost ( many ) dozens   of thousands of dollars . This financial cost is of-   ten prohibitive , particularly when training a gen-   eral high - quality student or several domain - specific   ones . While it is possible to utilize a huge LM   to obtain a limited number of labeled examples ,   relying on it for generating PTs for abundant un-   labeled data is not feasible . Therefore , a medium   size teacher is needed .   Furthermore , research suggests that using medi-   ator / assistant teachers aids the distillation process   ( Mirzadeh et al . , 2020 ; Wang et al . , 2020 ) , as might   be the case in distillation from a huge LM to a   medium size fine - tuned teacher , and finally to a   small student . Considering the aforementioned rea-   sons , our study holds significant relevance as it em-   phasizes the importance of the distillation process   with a medium size teacher , regardless of whether   the data is generated manually or by a huge LM .   The scope of our realistic setup . While our results   demonstrate the effectiveness of KD for various   English - to - English NLG tasks , for the tasks that   were part of the study , the output length is relatively   short compared to the input ( e.g. , Summarization   and Question Generation ) or has a similar length   ( Abductive Reasoning , Style Transfer and Simplifi-   cation ) . The results may differ for tasks with much   longer output lengths or for non - English - to - English   tasks such as NMT , data - to - text ( e.g. , table - to - text ) ,   multilingual , or multi - modality tasks .   In addition , the results are applicable to our re-   alistic task - specific setups , and some findings mayvary in high - resource scenarios or when unlabeled   data is unavailable . Although these scenarios may   be less relevant to NLP application developers , they   are commonly studied in academic research .   Computational training costs . Another limita-   tion of our research is that we did not consider   the computational costs of the KD stages . The   training time comparison between the methods was   therefore overlooked . This is because we assumed   that one - time resource usage for training could be   neglected compared to the accumulated inference   cost of a deployed model .   However , it is worth noting that generating PTs   with the teacher for all the training and unlabeled   examples is computationally expensive ( it could   take one to a few days , depending on the number of   unlabeled examples ) . Furthermore , Joint - Teaching   can also be computationally heavier than other KD   methods , as the student generates PTs during the   training process ( although the student is fast ) .   In addition , different training objectives also   have different costs , with some methods being   more computationally intensive than others ( e.g. ,   Attention - Relation is more costly than Logits KD ) .   Finally , the distillation process can be long , and   multiple epochs are required until the student con-   verges - in some setups , we trained the student for   more than a few days .   Utilizing huge LMs . Certain limitations arise in   our extreme setup , which involves the costly uti-   lization of huge LMs ( GPT-4 ) provided by external   companies like OpenAI . First , the comparison with   the Joint - Teaching method is not conducted due to   the need for repeated costly querying of the teacher   model to extract its logits every time a PT is gener-   ated with the student . Nevertheless , extracting the   logits of the teacher PTs ( for Logits KD ) and gen-   erating multiple PTs is approximately equivalent to   generating a single PT . This is because the prompt ,   consisting of many tokens , is processed only once ,   and the marginal cost of generating multiple ( rela-   tively short ) PTs is low .   Another limitation arises from relying on exter-   nal companies to enable logit extraction ( for Logits   KD ) and there is no assurance that this feature will   be supported . For instance , in the chat versions :   ChatGPT and GPT-4 , logits are not accessible . In   this work , we rely on an internal version of GPT-4 ,   which allows us to extract its logits . Fortunately , we   demonstrate that even without Logits KD , achiev-   ing a strong student model is possible.14641Acknowledgements   We would like to thank the area chair , the review-   ers , the members of the Microsoft MSAI team , and   theNLP@Technion team for their valuable feed-   back and advice . Roi Reichart has been partially   supported by the VATAT grant on data science .   References14642146431464414645   A Study Methods   In this section , we formally describe the objec-   tives and methods we consider in our study and   discuss in § 3 . A description of the notations is pro-   vided in Table 5 . In addition , for each method we   mention the stage in which we examine it and its   corresponding name in the results Table 3 . More   implementation details including hyperparameters   are provided in § D.   Conditional Language Modeling ( fine - tuning )   Stages 1 and 2 . “ Fine - tune ” in Table 3.A.   The objective of the autoregressive LM is to   minimize the Negative Log Likelihood ( NLL ) of   the training dataset :   L(x , y ) = −logP(y|x )   = −/summationdisplaylogP(y|x , y )   Notice that in our experiments we also conduct   a fine - tuning stage for 10 epochs on the labeled   data after the distillation stage of the following KD   methods .   Logits KD ( a.k.a Word - Level KD )   Stage 3 . “ Logits ” in Table 3.A and 3.B.   The objective of the student is to minimize the   KL divergence ( or the Cross - Entropy ) of the next   token distribution of the student and the teacher :   L(x , y ) =   −/summationdisplayKL(P(y|x , y)||P(y|x , y ) )   Noisy KD   Stage 3 . “ Noisy ” in Table 3.A.   For more details see Liu et al . ( 2021a ) and § D.   Attention Relation KD   Stage 3 . “ Att - Rel ” in Table 3.A.   For more details see Wang et al . ( 2020 ) , Wang   et al . ( 2021b ) and § D.   Fine - tune + PTs ( a.k.a . sequence - Level KD )   Stage 4 . “ Seq - lvl ” in Table 3.B.14646   For each labeled input x , we use the teacher   to generate a single mode approximation PT via   beam search : ˆy . Then we fine - tune the student by   minimizing L(x,ˆy ) .   Notice that in our experiments we actually mini-   mizeL(x,ˆy)+L(x , y ) , i.e. , an interpolation   between the ground truth target and the PT . We find   this interpolation to work better than using only the   PT . Kim and Rush ( 2016 ) proposed another inter-   polation , by selecting the most similar PT to the   ground truth from a set of KPTs generated by   beam search .   Logits KD + PTs   Stage 4 and 5 . “ Logits+Seq ” in Table 3.B and   “ Labeled ” in Table 3.C.   Same as “ Fine - tune + PTs ” , but we train the   student to minimize : L(x,ˆy ) . Following the   note above , we actually minimize the interpolation :   L(x,ˆy ) + L(x , y)(this is also the case for   the following methods ) .   Logits KD + PTs for unlabeled inputs   Stages 5 and 6 . “ + Unlabeled ” in Table 3.C and   “ Single PT ” in Table 3.D.   Same as “ Logits KD + PTs ” , but we also gen-   erate a single mode approximation PT for each   unlabeled input .   Logits KD + Multiple PTs   Stage 6 . “ K - Beams ” in Table 3.D.   We use the teacher to generate KPTs for ev-   ery labeled or unlabeled input , using beam search   with a beam size of K. We kept all the final K   beams ( sequences ) , Y , and used them to distill   the student by minimizing:/summationtextL(x,ˆy).This technique can be viewed as generating the   top - Kmode approximations . In our experiments   we use a different single PT for each input at every   epoch ( i.e. , if we generate KPTs for each input , it   takes Kepochs until the student sees all of them ) .   We mainly do it for a fair comparison between the   different methods ( see § D for additional details ) .   Logits KD + Sampling Multiple PTs   Stage 7 and 8 . “ Sampling ” in Table 3.D and “ Only   Teacher ” in Table 3.E.   Same as “ Logits KD + Multiple PTs ” , but rather   than generating PTs via beam search , we sample   them . Notice that in every distillation epoch a dif-   ferent single PT is sampled .   Logits KD + High Temperature Sampling of   Multiple PTs   Stage 7 . “ H - Sampling ” in Table 3.D.   Same as “ Logits KD + Sampling Multiple PTs ” ,   but we apply softmax temperature adjustment to   the next token distribution when we sample PTs .   High temperature values cause the next token dis-   tribution to be more flat ( and increase its entropy ) .   Therefore , high - temperature sampling generates   more diverse and surprising PTs ( Tevet and Berant ,   2021 ) . We use τ= 1.5 in our experiments .   Logits KD + Student PTs   Stage 8 . “ Only Student ” in Table 3.E.   Same as “ Logits KD + Sampling Multiple PTs ” ,   but instead of generating PTs with the teacher , we   use the student to generate PTs . We generate PTs   on - the - fly since the student is continuously updated   during training . In other words , for every training   input , we use the student to sample a student PT   ˆy . Then , we calculate L(x,ˆy)and update the   student weights . The process is repeated for every   input until the student finishes the training .   Joint - Teaching   Stage 8 . “ Joint - Teaching ” in Table 3.E.   This method combines “ Logits KD + Sampling   Multiple PTs ” and “ Logits KD + Student PTs ” .   Accordingly , we generate a PT for every training   input using either the teacher or the student . The   student is trained to minimize :   αL(x,ˆy ) + ( 1 −α)L(x,ˆy )   Where in our experiments α= 0.5 , since we find   it to work nicely . However , in future extensions of   this method , αcan also be a scheduled variable or   a variable that depends on the student ’s learning.14647B Language Models Architectures   As discussed in § 3 , the first stage ( stage 1 ) of our   study is to select the backbone architecture of the   NLG model . In this section , we thoroughly dis-   cuss and demonstrate the differences between the   two common transformer architectures for NLG :   Encoder - decoder ( ED ) models and Decoder - only   ( DO ) models . We start by providing a background   on these architectures in § B.1 . Following that , in   § B.2 we present a theoretical and empirical com-   plexity analysis . Finally , in Subsection § B.3 we   compare various off - the - shelf LMs from different   families by fine - tuning them on several NLG tasks   in the realistic setups we consider in this work .   An important note : We acknowledge that the   generation capabilities of huge LMs such as GPT-   3 , GPT-4 , and PaLM are exceptional . We do not   claim that Encoder - decoder models outperform   huge Decoder - only models . We consider fine - tuned   small or medium - sized LMs since our teachers and   students are such . In this case , Encoder - decoders   are preferable for task - specific fine - tuning of NLG .   B.1 Transformer Background   Modern LMs are based on the Multi - layer Trans-   former architecture ( Vaswani et al . , 2017 ) . The   core building block of the Transformer is Atten-   tion , which processes the sequence by replacing   each token with a weighted average of the rest   of the sequence ( self - attention ) , the preceding to-   kens ( autoregressive - attention ) , or another input   sequence ( cross - attention ) . For text generation ,   there are two dominant types of models : Encoder-   decoder ( ED ) ( Vaswani et al . , 2017 ; Raffel et al . ,   2020 ; Lewis et al . , 2020 ) and Decoder - only ( DO )   ( Radford et al . , 2019 ; Zhang et al . , 2022b ) .   ED models , which consist of two components   ( an encoder and a decoder ) , process inputs and tar-   gets ( outputs ) independently , with different parame-   ter sets : The encoder processes the inputs with self-   attention layers and passes its output to the decoder .   Then , the decoder autoregressively generates the   target token by token by applying autoregressive-   attention and cross - attention ( with the output of   the encoder ) . On the other hand , DO models con-   sist of autoregressive - attention layers that process   inputs and targets together . Typically , the target   sequence is concatenated to the input sequence   ( sometimes , with a separation token between them ,   such as “ TL;DR ” for summarization ) .   Notice that in contrast to the DO model , theencoder component represents each token of the   input sequence by sharing information from all the   tokens in the input ( via self - attention ) , while the   DO model represents an input token by sharing   information only from its preceding tokens ( via   autoregressive - attention ) . Another difference be-   tween the two architectures is that each layer of   the decoder component of the ED model , applies   cross - attention to the target tokens by conditioning   on the last hidden states of the input tokens . This is   in contrast to the decoder layers of the DO model   which apply autoregressive - attention to the target   inputs by conditioning on the same layer hidden   states of the input tokens .   ED and DO models differ not only in the ar-   chitecture but also in the pre - training objectives .   Whereas DO models are trained with an autoregres-   sive language modeling objective ( given previous   tokens , predict the following one ) , ED models are   trained with a masked language modeling objective   ( given a sequence with masked spans , predict the   missing tokens ) .   As a result of these differences ( encoder com-   ponent , attention mechanisms , and training objec-   tives ) , the models exhibit different inductive biases ,   which affect their performance . While ED models   are more popular for classification , summarization ,   and NMT tasks , DO models excel on open - text   generation and zero - shot or few - shot learning ( Raf-   fel et al . , 2020 ; Wang et al . , 2022 ) . Furthermore ,   the two architectures have different computational   complexities ( see the discussion in the next sub-   section , § B.2 ) . Nonetheless , the increasing popu-   larity of huge DO models like GPT-3/4 and PaLM   ( Brown et al . , 2020 ; Chowdhery et al . , 2022 ; Ope-   nAI , 2023 ) , which have impressive generation ca-   pabilities , has led to the question of “ whether ED   models are still relevant for NLG ” , a question that   we aim to answer in the first stage of our study .   To build an NLG system , it is necessary to select   an architecture that meets its needs . In the spirit   of our realistic setup , we compare various off - the-   shelf ED and DO LMs from different families , and   show that ED models outperform DO models in   conditional generation tasks . These findings are in   line with the recent work of Tay et al . ( 2022 ) , which   in contrast to us , trained from scratch LMs . For the   DO architecture , we use the GPT2 - family models   ( Radford et al . , 2019 ): GPT2 , GPT2 - M , and GPT2 - L ;   and the recent OPT - family models ( Zhang et al . ,   2022b ): OPT-125 M andOPT-350 M . For ED models14648   we use the same models which are described in   the main paper : T5 - family models ( Raffel et al . ,   2020 ): T5 - S andT5 - L ; and the BART - family mod-   els ( Lewis et al . , 2020 ): BART-6:6 ( base version )   andBART - L .   B.2 Complexity Analysis   For the theoretical complexity analysis , we assume   all the transformer models have the same hidden   size dimension and ignore it in our analysis . We   consider two types of models : ED with Eencoder   layers and Ddecoder layers , and a DO model with   Ddecoder layers . The input and target lengths are   mandn , respectively . For decoding , we assume   that hidden states of previously generated tokens   are cached and re - used ( i.e. , for the i - th token , the   decoder layers perform operations only for it ) . We   do not discuss space complexity , as it depends on   the exact implementation ( Rabe and Staats , 2021 )   and memory utilization of the device . Therefore ,   we do not connect the throughput measure to theo-   retical analysis .   A single encoder layer has a quadratic time com-   plexity in terms of input length O(m ) . An ED   decoder layer consists of a causal - attention and   cross - attention and therefore has a time complexity   ofO(n(m+n ) . Thus , ED model has a complexity   ofO(mE+n(m+n)D ) . Since we concatenate   the input and the target for DO models , a single   decoder layer of a DO has a time complexity of   O((m+n ) ) . Thus , a DO model has a complex-   ity of O(mD+n(m+n)D ) . This suggests that   an ED model can have the same time complex-   ity as a DO model ( when E =D ) while havingdouble parameters because the encoder and the de-   coder layers do not share parameters ( excluding   cross - attention weights , which account for a small   portion of the total weights ( Raffel et al . , 2020 ) ) .   Note that the number of floating - point operations   ( FLOPs , see subsection § 4.3 ) , is compatible with   the theoretical complexity . As a result , it is possi-   ble to verify the observation above – an ED model   with double the number of layers and parameters as   a DO model should have roughly the same number   of FLOPs . Consider Table 3.2 and take for example   GPT2 - M , which has 24 decoder layers , and compare   it toT5 - L which has 48 layers ( both of them have   the same number of heads and the same hidden di-   mension , see Table 6 ) . Indeed , they have the same   number of FLOPs . On the other hand , there are   differences in the practical measures . While the   latency of GPT2 - M is smaller than T5 - L , its memory   footprint is larger , which results in smaller through-   puts . This highlights the complex nature of the   connection between the theoretical and the prac-   tical measures ( e.g. , FLOPs and latency ) , which   depend on the device , implementation , and hard-   ware utilization that enable parallelism .   Now , compare models from the same family but   with different sizes . As can be seen , the ratio be-   tween the latencies of the models does not reflect   the large compression rate between their sizes . For   example , T5 - L is 12 times larger than T5 - S , how-   ever , it is only 3.7 times faster . Likewise , GPT2 - L   is 6 times larger than GPT2 , but is only 2.9 times   faster . On the other hand , the throughput reflects   much better the size differences . This demonstrates   the complex relationship between architectural de-   cisions and computational measurements and sug-   gests that architectural decisions should be taken   according to the ( specific ) task and system needs .   We next present a big Onotion when assuming   that operations can be parallelized ( as in GPUs ) .   This notation reflect better the latency : a practical   measure of the time for generating a single target   example . With full parallelism , the complexity of   processing the input in a single encoder layer ( for   ED ) is reduced from O(m)toO(m)(see Kasai   et al . ( 2020 ) ) . The same is true for the DO decoder   layer when it processes the input since it is capable   of processing all of it at the same time . However ,   since the target is generated by one token at a time   ( autoregressive ) , the processing complexity in each   layer that processes the target remains O(n(m+   n ) ) . As a result , the time complexity of ED is14649   O(mE+n(m+n)D)and of DO is O(mD+   n(m+n)D ) , which is equal to the ED complexity   when E = D. Nevertheless , there are differences   in practical measurements .   The theoretical analysis when allowing paral-   lelism sheds light on two observations that come   up from the practical analysis . The first one is   that the length of the target has a higher impact on   the latency , than the length of the input . This is   expected in the autoregressive generation process ,   where the relationship between the complexity and   the input length is linear , while quadratic for the   target length . This is supported by Table 8 : for   all models , altering only the input size minimally   affects the latency .   The second observation is pruning decoder lay-   ers has a higher impact on the latency than pruning   encoder layers . This is also expected since each   decoder layer contributes O(n(m+n))to the total   latency complexity , whereas a single encoder layer   contributes O(m ) . This is verified in Table 8 and   in Figure 3 : the encoder pruned model , BART-2:6 ,   has roughly the same latency as its full version ,   BART-6:6 . Conversely , the decoder pruned model   BART-6:2 has a smaller latency from both .   The behavior of throughput is more complex   than latency . While the pruned decoder model   consistently has a smaller latency regardless of   input length ( as shown in Figure 3 ) , the pruned   encoder ( BART-2:6 ) has a higher throughput than   the pruned decoder ( BART-6:2 ) for longer inputs ,   as indicated by the crossover at around 0.8 on the   X - axis in Figure 3 .   B.3 Task Performance Analysis   In this subsection , we discuss the differences in   task performance between off - the - shelf ED and DOmodels , which are finetuned on our four datasets .   The average results ( over the four tasks ) are pro-   vided in Table 2 . For all datasets and models , ED   models outperform DO models . Presumably , a bet-   ter inductive bias is injected to the ED models : ( 1 )   By applying self - attention ( and not autoregressive-   attention ) to the conditioned input sequence ; ( 2 )   By the fact that in contrast to the DO model , the   decoder component of the ED model attends to the   last hidden states of the conditioned input sequence   from its first layer . This is unlike the DO model ,   where each layer applies attention to hidden states   of the same layer .   Our results for conditional generation tasks in a   finetuning setup are in line with other works ( Raffel   et al . , 2020 ; Tay et al . , 2022 ) which trained LMs   from scratch . This finding is particularly relevant   for NLP practitioners who aim to develop a special-   ized in - house model for a specific NLG task . Our   findings also raise the question of why huge lan-   guage models , such as GPT-3 and PaLM ( Brown   et al . , 2020 ; Chowdhery et al . , 2022 ) are DO , and   Wang et al . ( 2022 ) answer it by showing that DO   models excel in zero and few - shot setups . Indeed ,   in the final part of our study , which involves an ex-   treme setup where labeled data is unavailable , we   use GPT-4 , a Decoder - only model with zero - shot   capabilities , to generate PTs . Our equivocal results   lead us to continue only with ED models ( T5 and   BART ) for our compression study ( stages 1 - 8) .   C KD without Labeled Data   In the final phase of our study , we intend to explore   the possibility of scaling up our experimental setup .   This is accomplished by working with only a lim-   ited number of unlabeled examples and without   any labeled examples . We refer to this setup as   extreme setup . It is important to note that unlike   the realistic setup , which incorporates a medium-   sized labeled dataset , the extreme setup poses a   challenge for fine - tuning a teacher model due to   the lack of labeled examples . In that case , we need   to utilize as our teacher a huge LM , such as GPT-4 ,   which has zero - shot and few - shot capabilities and   can generate plausible PTs .   The main goal of this phase is to investigate   the transferability of the KD conclusions from our   realistic setup to the extreme setup , which pos-   sess the following differences since it involves a   huge zero - shot LM as the teacher : ( 1 ) The teacher   is a Decoder - only model ( since this is the stan-14650dard architecture for zero - shot and few - shot LMs )   and the student is an Encoder - decoder model ( fol-   lowing our findings that they outperform Decoder-   only models , see § 5 ) ; ( 2 ) The teacher and the stu-   dent have different tokenizers , which means that a   sequence - alignment algorithm is needed to perform   Logits KD ; ( 3 ) Unlike the realistic setup where the   computational training cost could be neglected , in   the extreme setup we assume that querying the   huge teacher is financially costly and therefore we   limit its usage .   The third difference above impacts the design   choice of the extreme setup , and we limit the num-   ber of unlabeled data to a few thousand . In addition ,   we do not consider the Joint - Teaching method due   to its high cost compared to other methods . This   is because it requires querying the teacher every   time we generate a PT with the student ( to extract   the teacher logits ) . However , notice that extracting   the logits of GPT-4 and generating multiple PTs is   approximately equivalent to generating a single PT .   This is because the prompt and the input , consist-   ing of many tokens , are processed only once , and   the marginal cost of generating multiple ( relatively   short ) PTs is low .   C.1 Experimental Setup   Models and datasets We utilize GPT-4 as our   teacher model and T5 - S as the student model . For   generating PTs with GPT-4 , we use a prompt that   contains a task instruction and three demonstrations   of labeled examples ( few - shot learning ) .   We consider two NLG tasks : ( 1 ) Question Gen-   eration – we use the SQuAD17 dataset and sample   3000 , 250 and 500 examples as the train , devel-   opment and test , respectively ; ( 2 ) Simplification   and style transfer – we use the Shake7 dataset and   sample 1500 , 250 and 350 examples as the train ,   development , and test , respectively . Notice that   both the training and development sets do not con-   tain any labeled data . Only the test set includes   labeled data , which is used for evaluation purposes .   Methods and baselines We present the test results   for the following baselines and methods : ( a ) The   GPT-4 teacher ; ( 2 ) A T5 - S model which is trained   using ground - truth ( GT ) targets to compare with   the GPT-4 teacher ; ( c ) Student fine - tuning with a   single PT ; ( d ) Student fine - tuning with multiple   PTs ; ( e ) Student training with a single PT and Log-   its KD ; ( f ) Student training with multiple PTs and   Logits KD .   We train each model ( except GPT-4 ) using four   learning rates : [ 0.003 , 0.001 , 0.0005 , 0.0003 ] . In   addition , as we explain § C.2 , we also include re-   sults when we train the models of ( c)-(f ) using   golden targets ( ground - truth labels ) for the devel-   opment set .   Tokenizers Alignment In the extreme setup , the   teacher is a Decoder - only model ( GPT-4 ) , and the   student is an Encoder - decoder model ( T5 - S ) that   does not share the same tokenizer . Therefore , to   perform Logits KD , where the probabilities of the   next token ’s logits are used for distillation , two   types of token alignment are required : ( 1 ) Match-   ing each token in the teacher ’s tokenized PT se-   quence with its corresponding token in the stu-   dent ’s tokenized PT sequence ; ( 2 ) Matching the   tokens from the teacher ’s logits to tokens from the   student ’s vocabulary .   For example , consider the black and blue arrows   in Figure 4 . These arrows demonstrate the first   type of match , where we align the tokens of the to-   kenized PT sequences . Additionally , some tokens   might be inserted ( c ) or deleted ( f ) . Similar to Fu   et al . ( 2023 ) , we use the well - known dynamic pro-   gramming Needleman – Wunsch algorithm ( Needle-   man and Wunsch , 1970 ) for sequence alignment to   find this mapping . The output of the algorithm is   a sequence of edit operations : match , replacement ,   insertion , and deletion . We consider two tokens as   a match if the algorithm determines them as such   or if they are replaced and one is a prefix of the   other . For instance , the blue arrows in Figure 4   represent a match via replacement .   The OpenAI API allows us to extract only the   probability distribution over the top five tokens at   each decoding step . However , their probability is   usually close to 1 . We align the top five tokens to   the student ’s vocabulary by performing an exact14651   match . Then , we apply softmax to the logits to   make their probabilities sum to one . For example ,   ( a ) and ( b ) in Figure 4 present such an alignment .   Notice that some of the tokens of the teachers are   omitted ( “ Robsert ” and “ |<endoftext| > ” ) . In case   the student token does not have a match in the   teacher ’s top five tokens , we determine its proba-   bility as one . For example , ( c ) and ( e ) in Figure 4 :   ( c ) “ s ” is a token that is inserted and therefore its   probability is one ; and ( e ) the token “ 2010 ” does   not appear in the top five tokens ( d ) , and therefore   its probability is one .   The second type of alignment we need to per-   form is matching the teacher ’s logits of the next   token prediction to tokens from the student ’s vocab-   ulary . The OpenAI API allows us to extract only   the probability distribution over the top five tokens   at each decoding step . However , the sum of their   probabilities is usually close to 1 . We align the top   five tokens with the student ’s vocabulary by per-   forming an exact match . Then , we apply softmax   to the logits to ensure their probabilities sum up to   one . For example , ( a ) and ( b ) in Figure 4 demon-   strate such an alignment . Note that some tokens   from the teacher are omitted ( e.g. , " Roberts " and   " |<endoftext| > " ) . If the student ’s token does not   have a match in the teacher ’s top five tokens , we   assign its probability as one . For instance , in Fig-   ure 4 , ( c ) " s " is an inserted token , so its probability   is one ; and ( e ) the token " 2010 " does not appear in   the top five tokens ( d ) , hence its probability is one .   C.2 Results   In Table 7 , we present the results of the extreme   setup . We do not include computational perfor-   mance metrics as OpenAI does not detail the exact   architecture of GPT-4 . We find the results vary   greatly between different initializations and learn-   ing rates . The observed difference in performance   can be primarily attributed to the unique extreme   setup . The limited number of training instances andthe distinct distribution of PTs , generated by GPT-   4 rather than a fine - tuned model contribute to this   variation . Additionally , the discrepancy between   the development set , consisting of PTs , and the   test set , containing ground - truth targets , negatively   affect model selection ( Fu et al . , 2023 ) .   To address the issue of variability , we present   the average scores over different learning rates .   Additionally , we include the results from exper-   iments conducted with development sets that con-   tain ground - truth ( GT ) targets ( as shown in the four   left columns of Table 7 ) . Indeed , the correlation   between the development score and the test score   is considerably low ( 0.06 and 0.12 for the SQuAD17   andShake7 datasets , respectively ) when the de-   velopment sets are PTs , in contrast to the higher   correlations observed when the development sets   are GTs ( 0.57 and 0.66 ) .   As depicted in Table 7 , the overall trends in   the left four columns ( development set with GTs )   align with those in the right four ( development set   with PTs ) . The results are mixed : for the SQuAD17   dataset , Logits KD with multiple PTs outperforms   the other methods , which is in line with the conclu-   sions from the realistic setup . Surprisingly , incor-   porating Logits KD has a positive effect only when   there are multiple PTs . In the Shake7 dataset , Log-   its KD does not improve the student . We believe   this is due to the difficulties with aligning the tok-   enizers and call for further research . Nevertheless ,   another conclusion from the realistic setup , which   also holds in the extreme setup , is that generating   multiple PTs is preferable over a single PT .   D Additional Implementation Details   Our experiments are conducted in the PyTorch   framework . Models are trained on a machine   equipped with 4 Nvidia Tesla v100 GPUs ( for   XSUM40 andSQuAD17 ; in that case , we use DDP   training ) or with Nvidia GeForce RTX 4080 ( for   ART10 andShake7 ) .14652Training We optimize our model with the   AdamW optimizer , with a weight decay of 1e−5 ,   ε= 1e−8,100warmup steps , and a linear learning   rate scheduler . We use the largest batch size that   fits the GPU for every dataset and model . However ,   for a fair comparison , we accumulate the gradients   and update the model every 96 training examples   for any experiment ( same number of gradient up-   dates ) . For BART models we apply half - precision   training and inference .   The validation metric for XSUM40 is ROUGE-2   ( F1 ) and for SQuAD17 , ART10 andShake7 isBLEU .   In addition , for XSUM40 we use “ summarize : ” as a   prefix for T5 models and “ TL;DR ” as a suffix for   DO models . For SQuAD17 we use “ ask : ” as a prefix   and suffix for T5 and DO models respectively . For   ART10 we use “ explain : ” as a prefix and suffix for   T5 and DO models respectively , and for Shake7 ,   we use “ modern : ” as a prefix and suffix for T5 and   DO models respectively .   Fine - tuning We examine multiple learning rates   for each model and dataset as follows : for our   student models , T5 - S andBART-6:6 , and smaller   Decoder - only models , GPT2 , OPT-125 M – we search   within 8 different learnings rates in the range of   [ 5e−2,1e−5]and train the models for 35 epochs .   For our teacher models , T5 - L andBART - L and the   remaining decoder - only models–6 learning rates in   [ 5e−3,1e−6]and 20 epochs . For fine - tuning the   decoder - only models , we concatenate the input and   the target , separated by a task suffix , and calculate   the loss only on the target tokens .   Evaluation We evaluate every model two times   at each epoch ( at the middle and the end ) and select   the best checkpoint according to the development   set performance ( see § 4.1 for more details about   the measure used in each dataset ) . For computa-   tional reasons , when we evaluate the model on the   development set , we generate predictions for no   more than 1K. We use DeBERTa - base model ( He   et al . , 2021 ) , fine - tuned on the MNLI dataset as the   backbone model for calculating BSs .   Knowledge Distillation For computational rea-   sons , the learning rate which is used for training   the student is selected according to the develop-   ment performances in the fine - tuning stage ( and   reported on Tables 9 , 10 , 11 , 12 ) . Since we have   observed that the convergence time of the students   in KD setups is slow ( unlike fine - tuning ) , we train   the models for 192 epochs but stop the training ifthere is no improvement in the performance for 16   epochs ( 32 evaluation steps ) . We note that all of   the experiments were stopped before the last epoch .   Following Mukherjee and Awadallah ( 2020 ) ; Xu   et al . ( 2022 ) , we perform a fine - tuning stage for   10 epochs after selecting the best checkpoint dur-   ing the KD stage . The final checkpoint is selected   either from the KD or fine - tuning stages .   For Logits KD we minimize the KL divergence   between the student and the teacher logits . We also   tried using Label - Smoothing ( Müller et al . , 2019 )   with different scaling temperatures , however , it did   not help the distillation . For Sequence - Level KD   we fine - tune the student on pseudo - targets gener-   ated with beam search in addition to the original   ground truth targets . For Noisy KD we only apply   noise to the teacher ’s logits ( as it is shown to be   more important than applying noise to the input ,   and since we do n’t focus on input manipulations in   this study ) . For Attention - Relations KD we distill   relations from the last encoder and last decoder lay-   ers and scale the weights of the loss components to   1 at the start of the training .   Pseudo Targets For generating pseudo targets   we use nucleus sampling with P= 0.95 , for high   temperature sampling we use τ= 1.5 . When gen-   erating pseudo targets with the teacher using beam   search , we use a beam size of 16 . For sampling ,   we generate 48 pseudo targets ( these are the largest   sizes that fit on v100 GPU for XSUM40 ) .   In experiments with a single pseudo target , we   select the highest - ranked prediction among the gen-   erated targets using beam search with a beam of   size 16 . We augment the training data with PTs   by adding pairs of input and a single PT for each   labeled or unlabeled example ( depending on the   experiment ) . In experiments with multiple PTs , we   use a different single PT at every epoch ( alterna-   tively , the student could learn from multiple pseudo   targets of the same input on every epoch ) . We do it   for two main reasons : first , we want a fair compar-   ison between experiments with single or multiple   pseudo targets . Second , we have observed that the   ground truth of the labeled data is important – this   way , the student sees more of it as the proportion   of ground truth targets is larger than the alternative .   We use nucleus sampling and for high tempera-   ture sampling we use τ= 1.5 . For computational   reasons , we generate all the teacher PTs once and   reuse them . Conversely , the student PTs are gen-   erated on - the - fly , since the student is continuously14653updated during training . In Joint - Teaching , we gen-   erate PTs with the student in 50 % of the training   steps ( in the remaining 50 % we use the teacher ) .   Computational Profiling All computational pro-   filing experiments are conducted on Nvidia   GeForce RTX 4080 . Following Geifman ( 2020 ) ,   we do a GPU warmup for 10 steps and then av-   erage 100 computational measurements . For ev-   ery dataset , we use the maximum input and target   length as reported in Table 1 . FLOPs are measured   for a full forward step . Latency and memory foot-   print are measured for generating a single example .   For measuring Throughput , which is the maximum   number of examples the model can process in a   minute , we find the maximum batch size that does   not exceed 16 GB during the generation , and then   measure the throughput .   D.1 The Shake7 Dataset   We construct a new dataset for the well - explored   style transfer task ( which is also a simplification   task ) of translating Shakespeare ’s texts to modern   English . We combined three existing datasets : two   parallel datasets of Shakespeare ’s original texts and   their modern versions ( Xu et al . , 2012 ; Jhamtani   et al . , 2017 ) and a third dataset containing only   unlabeled texts from Shakespeare ’s plots that are   not part of the other two datasets ( Karpathy , 2015 ) .   A particular advantage of this dataset is that it con-   sists of publically available datasets , while many   other datasets for thesimplification task are not pub-   lic . Moreover , in this dataset we have access to   both labeled ( original alongside modern texts ) and   unlabeled ( original texts ) data . Additionally , the   labels ( modern English texts ) are of very high qual-   ity as experts produce them . Finally , the task of   this dataset is harder than other style transfer and   simplification cases since the difference between   the original text and the simplified version is not   limited to a small number of words . We hope this   dataset will contribute to the NLP community .   D.2 URLs of Code and Data   •Code Repository - code and datasets :   github.com/nitaytech/KD4Gen .   •HuggingFace ( Wolf et al . , 2020 ) - code   and pretrained weights for language models ,   tokenizers , and datasets : huggingface.co/ .   huggingface.co/docs/accelerate/index .   •Torchprofile - for measuring FLOPs :   github.com/zhijian-liu/torchprofile .E Additional Results   In this section , we report additional details and the   complete results of our study . Table 6 presents a   full description of the architecture of the models   used in our study . Table 8 provide full results of our   computational measurements . In Tables 9 , 10 , 11   and 12 we report on the performances of every ex-   periment we conduct , for XSUM40 , SQuAD17 , ART10   andShake7 datasets , respectively.146541465514656   F Human Evaluation for ART10   In this section , we aim is to examine the relatively   lower performance of our KD method on the ART10   dataset when compared to other datasets . Accord-   ingly , the abductive reasoning task and the ART10   dataset in particular , are a unique case where au-   tomatic evaluation is hard to perform due to the   large number of diverse potential solutions ( See the   examples in § F.2 ) . Therefore , we assume that the   fraction of the student - teacher gap closed by the   distilled model may have been underestimated .   To validate our assumption , we conducted a   human evaluation involving two annotators . We   randomly selected 50 input examples and gener-   ated outputs using the student baseline ( T5 - S ) ,   the teacher ( T5 - L ) , and the final distilled model   ( T5 - KD ) . The annotators were asked to rate the gen-   erated texts on a five - level scale ( see § F.1 below ) .   The inter - annotator agreement achieved by our an-   notators was Kendall ’s τ=0.52 . According to Ta-   ble 13 , which presents the average rating for each   model , we find that the distilled model closes 72 %   of the student - teacher gap . This result is much   greater than the 50 % estimated by the automatic   evaluation , and is in - line with the performance of   the distilled model on other datasets .   F.1 Human Evaluation – Instructions   You will be presented with two texts referred to as   “ The Observations ” . These observations occur in a   specific order , with the first happening before the   second . Your task is to assess four “ explanations ”   of the second observation , which aim to explain   why it occurred , given that the first observation had   already occurred . A good “ explanation ” should be   clear and provide a plausible account of what hap-   pened between the two observations . You should   rate each explanation using this five - level scale :   1.The explanation is nonsensical or contains   many grammatical errors .   2.The explanation is not related to the observa-   tions or repeating the observations.3.The explanation is related to the observations   but does not explain them .   4.The explanation is related to the observations   but only partially explains them .   5.The explanation fully explains the observa-   tions .   F.2 Generated Examples   Observation 1 : I went to a rap show for the first   time in my life .   Observation 2 : Now I ’m avid rap and hip hop   listener .   Reference : I really enjoyed the show .   T5 - S : I went to a hip hop show .   T5 - L : I fell in love with rap and hip hop .   T5 - KD : The rap show was very good .   Observation 1 : Allison wanted to renew her vows   with Tom .   Observation 2 : Yeah even had a new baby .   Reference : Allison and Tom did it and felt more   love .   T5 - S : Tom had a baby .   T5 - L : Allison proposed to Tom .   T5 - KD : Allison asked Tom to marry her .   Observation 1 : Today I decided to learn how to   make bread .   Observation 2 : I noticed I made an error in my   measurements and started over .   Reference : I accidentally put in twice as much salt   as needed .   T5 - S : I went to the grocery store to learn how to   make bread .   T5 - L : I did n’t follow the recipe exactly .   T5 - KD : I did not follow the instructions carefully .   Observation 1 : Tommy called on the girl who sat   next to him in class .   Observation 2 : Tommy decided to ask the girl for   a date .   Reference : The girl was very beautiful and kind .   T5 - S : Tommy asked the girl for a date .   T5 - L : Tommy liked the girl a lot .   T5 - KD : The girl said she liked Tommy.14657ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   7   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Not relevant to this work , as it is a general model compression / knowledge distillation   for NLG .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract , 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Appendix   /squareB1 . Did you cite the creators of artifacts you used ?   4 , Appendix   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   4 , Appendix   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   4   C / squareDid you run computational experiments ?   4 , Appendix   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   4 , Appendix14658 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   4 , Appendix   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   5 , Appendix   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.14659