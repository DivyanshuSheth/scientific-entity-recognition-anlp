  Ce Zheng , Xudong Chen , Runxin Xu , Baobao ChangThe MOE Key Laboratory of Computational Linguistics , Peking University , ChinaSchool of Software and Microelectronics , Peking University , China   { zce1112zslx,xdc}@pku.edu.cn   runxinxu@gmail.com ; chbb@pku.edu.cn   Abstract   Frame semantic parsing is a fundamental   NLP task , which consists of three subtasks :   frame identification , argument identification   and role classification . Most previous stud-   ies tend to neglect relations between differ-   ent subtasks and arguments and pay little at-   tention to ontological frame knowledge de-   fined in FrameNet . In this paper , we pro-   pose a Knowledge - guided Incremental seman-   tic parser with Double - graph ( K ) . We first   introduce Frame Knowledge Graph ( FKG ) , a   heterogeneous graph containing both frames   and FEs ( Frame Elements ) built on the frame   knowledge so that we can derive knowledge-   enhanced representations for frames and FEs .   Besides , we propose Frame Semantic Graph   ( FSG ) to represent frame semantic structures   extracted from the text with graph structures .   In this way , we can transform frame seman-   tic parsing into an incremental graph construc-   tion problem to strengthen interactions be-   tween subtasks and relations between argu-   ments . Our experiments show that Kout-   performs the previous state - of - the - art method   by up to 1.7 F1 - score on two FrameNet datasets .   Our code is availavle at https://github .   com / PKUnlp - icler / KID .   1 Introduction   The frame semantic parsing task ( Gildea and Juraf-   sky , 2002 ; Baker et al . , 2007 ) aims to extract frame   semantic structures from sentences based on the   lexical resource FrameNet ( Baker et al . , 1998 ) . As   shown in Figure 1 , given a target in the sentence ,   frame semantic parsing consists of three subtasks :   frame identification , argument identification and   role classification . Frame semantic parsing can   also contribute to downstream NLP tasks such as   machine reading comprehension ( Guo et al . , 2020 ) ,   relation extraction ( Zhao et al . , 2020 ) and dialogue   generation ( Gupta et al . , 2021).Figure 1 : Given the target receive in this sentence , the   frame identification is to identify the frame Receiving   evoked by it ; the argument identification is to find the   arguments ( He , the book , ... ) of this target ; the role   classification is to assign frame elements ( Recipient ,   Theme , ... ) as semantic roles to these arguments .   FrameNet is an English lexical database , which   defines more than one thousand hierarchically-   related frames to represent situations , objects or   events , and nearly 10 thousand FEs ( Frame Ele-   ments ) as frame - specific semantic roles with more   than 100,000 annotated exemplar sentences . In ad-   dition , FrameNet defines ontological frame knowl-   edge for each frame such as frame semantic re-   lations , FE mappings and frame / FE definitions .   The frame knowledge plays an important role in   frame semantic parsing . Most previous approaches   ( Kshirsagar et al . , 2015 ; Yang and Mitchell , 2017 ;   Peng et al . , 2018 ) only use exemplar sentences and   ignore the ontological frame knowledge . Recent   researches ( Jiang and Riloff , 2021 ; Su et al . , 2021 )   introduce frame semantic relations and frame def-   initions into the subtask frame identification . Dif-   ferent from previous work , we construct a hetero-   geneous graph named Frame Knowledge Graph   ( FKG ) based on frame knowledge to model multi-   ple semantic relations between frames and frames ,   frames and FEs , as well as FEs and FEs . Further-   more , we apply FKG to all subtasks of frame se-   mantic parsing , which can fully inject frame knowl-   edge into frame semantic parsing . The knowledge-   enhanced representations of frames and FEs are   learned in a unified vector space and this can also   strengthen interactions between frame identifica-   tion and other subtasks .   Most previous systems neglect interactions be-4998   tween subtasks , they either focus on one or two   subtasks ( Hermann et al . , 2014 ; FitzGerald et al . ,   2015 ; Marcheggiani and Titov , 2020 ) of frame se-   mantic parsing or treat all subtasks independently   ( Das et al . , 2014 ; Peng et al . , 2018 ) . Furthermore ,   in argument identification and role classification ,   previous approaches process each argument sepa-   rately with sequence labeling strategy ( Yang and   Mitchell , 2017 ; Bastianelli et al . , 2020 ) or span-   based graphical models ( Täckström et al . , 2015 ;   Peng et al . , 2018 ) . In this paper , we propose Frame   Semantic Graph ( FSG ) to represent frame semantic   structures and treat frame semantic parsing as a   process to construct this graph incrementally . With   graph structure , historical decisions of parsing can   guide the current decision of argument identifica-   tion and role classification , which highlights inter-   actions between subtasks and arguments .   Based on two graphs mentioned above , we   propose our framework K(Knowledge - guided   Incremental semantic parser with Double - graph ) .   FKG provides a static knowledge background for   encoding frames and FEs while FSG represents   dynamic parsing results in frame semantic parsing   and highlights relations between arguments .   Overall , our contributions are listed as follow :   •We build FKG based on the ontological   frame knowledge in FrameNet . FKG incorpo-   rates frame semantic parsing with structured   frame knowledge , which can get knowledge-   enhanced representations of frames and FEs .   •We propose FSG to represent the frame se-   mantic structures . We treat frame semantic   parsing as a process to construct the graph in-   crementally . This graph focuses on the target-   argument and argument - argument relations . We evaluate the performance of Kon two   FrameNet datasets : FN 1.5 and FN 1.7 , the results   show that the Kachieves state - of - the - art on these   datasets by increasing up to 1.7 points on F1 - score .   Our extensive experiments also verify the effective-   ness of these two graphs .   2 Ontological Frame Knowledge   Frame semantics relates linguistic semantics to   encyclopedic knowledge and advocates that one   can not understand the semantic meaning of one   word without essential frame knowledge related to   the word ( Fillmore and Baker , 2001 ) . The frame   knowledge of a frame contains frame / FE defini-   tions , frame semantic relations and FE mappings .   FrameNet defines 8 kinds of frame semantic re-   lations such as Inheritance , Perspective_on and   Using ; for any two related frames , the FrameNet   defines FE mappings between their FEs . For exam-   ple , the frame Receiving inherits from Getting and   the FE Donor ofReceiving is mapped to the FE   Source ofGetting . Each frame or FE has its own   definition and may mention other FEs .   We propose two ways of reasoning about frame   semantic parsing : inter - frame reasoning and intra-   frame reasoning in Figure 2 . Frame knowledge   mentioned above can guide both ways of reasoning .   The frame semantic relation between Receiving   andGetting and FE mappings associated with it   allow us to learn from the left sentence when pars-   ing the right sentence because similar argument   spans of two sentences will have related FEs as   their roles . The FE definitions reflect dependen-   cies between arguments . The definition of Role   in frame Receiving mentions Theme andDonor ,   which reflects dependencies between argument the   book and argument as a gift .49993 Task Formulation   Given a sentence S = w , . . . , wwith a target   span tinS , the frame semantic parsing aims to   extract the frame semantic structure of t. Suppose   that there are karguments of tinS : a , . . . , a ,   subtasks can be formulated as follow :   •Frame identification : finding an f∈ F   evoked by target t , where Fdenotes the set of   all frames in the FrameNet .   •Argument identification : finding the bound-   aries iandifor each argument a=   w , . . . , w.   •Role classification : assigning an FE r∈ R   to each a , where Rdenotes the set of all   FEs of frame f.   4 Method   Kencodes all frames and FEs to knowledge-   enhanced representations via frame knowledge   graph encoder ( section 4.1 ) . For a sentence with   a target , contextual representations of tokens are   derived from the sentence encoder ( section 4.2 ) .   Frame semantic parsing is regarded as a process   to build FSG incrementally from an initial target   node to complete FSG . Frame identification finds a   frame evoked by the target and combines the target   with its frame into the initial node of FSG ( section   4.3.1 ) . Argument identification ( section 4.3.2 ) and   role classification ( section 4.3.3 ) for each argument   is based on the current snapshot of partial FSG con-   sidering all historical decisions . Section 4.4 tells   how frame semantic graph decoder encodes partial   FSG to its representation and how it expands FSG   incrementally , which is also shown in Figure 4 .   4.1 Frame Knowledge Graph Encoder   FKG is an undirected multi - relational heteroge-   neous graph , and Figure 3 shows a subgraph of   FKG . Its nodes contain both frames and FEs and   there are four kinds of relations in FKG : frame - FE ,   frame - frame , inter - frame FE - FE and intra - frame   FE - FE relations . The following will show how we   extract these relations from frame knowledge :   Frame - FE : we connect a frame with its FEs so that   we can learn representations of frames and FEs in   a unified vector space to strengthen interactions   between frame identification and other subtasks .   Frame - frame andinter - frame FE - FE : these two   kinds of relations are frame semantic relations and   FE mappings respectively and here we ignore re-   lation types of frame semantic relations . They can   both guide inter - frame reasoning in Figure 2 .   Intra - frame FE - FE : If the definition of an FE   mentions another FE in the same frame , they will   have intra - frame FE - FE relations with each other .   This relation can help with intra - frame reasoning   and strengthen interactions between arguments .   The frame knowledge graph encoder aims to get   knowledge - enhanced representations of nodes in   FKG via an RGCN ( Schlichtkrull et al . , 2018 ) mod-   ule . We use Fto represent all frames in FrameNet   andRto represent all FEs of frame f. In ad-   dition , we use R=/uniontextRto represent all   FEs in the FrameNet . Let 0 , . . . , |F| − 1denote   all frames and |F| , . . . , |F|+|R| − 1denote all   FEs . Moreover , we introduce a special dummy   node indexing |F|+|R|into FKG . So the vectors   y , . . . , y∈Rdenote the representations of all   nodes in FKG , where M=|F|+|R| .   For each node i , we take a randomly initialized   embedding y∈Ras the input feature of the   RGCN module . Then we can get representations   of all frames and FEs :   y , . . . , y= RGCN / parenleftig   y , . . . , y / parenrightig   ( 1 )   The RGCN module models four kinds of rela-   tions : Frame - FE , intra - frame FE - FE , frame - frame   and inter - frame FE - FE . For better modeling inter-   frame relations between FEs , we also fuse name   information into representations of FEs . The FEs   whose names are the same will share the same em-   beddings , i.e. for i , j≥ |F| , y = yif the   name of iis the same as j.5000   4.2 Sentence Encoder   The sentence encoder converts tokens of the sen-   tence S = w , . . . , wto their representations   h , . . . , h∈R.   We use LSTM ( Hochreiter and Schmidhuber ,   1997 ) and GCN ( Kipf and Welling , 2016 ) to model   both sequential structure and dependency structure :   α , . . . , α= BiLSTM ( e , . . . , e)(2 )   β , . . . , β= GCN ( α , . . . , α , T)(3 )   edenotes the embedding of word w. We get   contextual representations hby adding βtoα .   We follow previous studies ( Marcheggiani and   Titov , 2020 ; Bastianelli et al . , 2020 ) to use syn-   tax structures like dependency tree TofShere .   Furthermore , we use boundary information   ( Wang and Chang , 2016 ; Cross and Huang , 2016 ;   Ouchi et al . , 2018 ) to represent spans like s=   w , . . . , wbased on token representations because   we need to embed spans into the vector space of   FKG in frame identification and role classification :   Q(i , j ) = FFN ( ( h−h)⊕(h+h ) ) ( 4 )   The dimension of Q(i , j)isd . The⊕denotes   concatenation operation and FFN denotes Feed For-   ward Network .   4.3 Frame Semantic Parsing   4.3.1 Frame Identification   A frame f∈ F will be identified based on the   target t , representations of tokens h , . . . , hand representations of frames y , . . . , ywith   a scoring module . The target t = w , . . . , w   will be embedded to the vector space of all frames   asγ∈R. We can calculate dot product nor-   malized similarities between γand all frames   Y= ( y , . . . , y)∈Rto get the prob-   ability distribution of f. For short , we use πto   denote Q(i , i ):   γ= tanh ( FFN ( π ) ) ( 5 )   P(f|S , t ) = softmax / parenleftig   Y·γ / parenrightig   ( 6 )   4.3.2 Argument Identification   Based on g , the representation of current snapshot   of FSG G , we need to find an argument a=   w , . . . , w. We use pointer networks ( Vinyals   et al . , 2015 ) to identify its start and end positions   iandiseparately via an attention mechanism ,   which is more efficient than traditional span - based   model ( Chen et al . , 2021 ) . Take ias example :   ρ= FFN ( g ) ( 7 )   P(i|S , G ) = softmax / parenleftig   H·ρ / parenrightig   ( 8)   H= ( h , . . . , h)∈Rrepresents the   output of the sentence encoder , and ρ∈Ris   used to find the start position of argument span a.   4.3.3 Role Classification   Based on ganda , we embed ainto the vec-   tor space of FEs as γ∈R. Similar to frame   identification , we calculate dot product normal-   ized similarities between γand all FEs Y=5001(y , . . . , y)∈Rto get the con-   ditional probability distribution of rgiven aand   G.   γ= FFN ( π⊕g ) ( 9 )   P(r|S , G , a ) = softmax / parenleftig   Y·γ / parenrightig   ( 10 )   4.4 Frame Semantic Graph Decoder   We propose FSG to represent the frame semantic   structure of tin the sentence Sand we treat the   frame semantic parsing as a process to construct   FSG incrementally . Intermediate results of FSG are   partial FSGs representing all historical decisions ,   which highlights interactions between arguments .   Suppose that there are karguments of target t :   a , . . . , awith their roles r , . . . , r. For τ-   th snapshot of FSG G , it contains τ+1nodes : one   target node ( t , f)andτargument nodes ( if exist )   ( a , r ) , . . . , ( a , r ) . The target node will be   connected with all argument nodes . The indices of   nodes in Gdepend on the order in which they are   added into the graph , 0 denotes the target node and   1 , . . . , τ denotes ( a , r ) , . . . , ( a , r ) .   We encode Gto its representation g :   g= Maxpooling ( z , . . . , z ) ( 11 )   z , . . . , z= GCN / parenleftig   z , . . . , z , G / parenrightig   ( 12 )   z=/braceleftbiggπ⊕y , j = 0   π⊕y , j= 1 , . . . , τ(13 )   where iandidenotes indices of fandrin   FKG , and π = Q(i , i ) . The GCN module is to   encode partial FSG .   Based on the representation gof each snapshot   G , Kpredicts boundary positions of argument   aand assign an FE ras its semantic role ( section   4.3.2,4.3.3 ) . The Gwill be updated to Gwith   the new node ( a , r)until the ris the special   dummy node in FKG . Figure 4 shows how to find   a new node and add it into the FSG .   5 Training and Inference   5.1 Training   We train Kwith all subtasks jointly by opti-   mizing the loss function Lsince representations   of frames and FEs are learned in a unified vector   space .   L=−logP(f = f|S , t ) ( 14 )   L=−/summationdisplaylogP(i = I|S , G)(15 )   L=−/summationdisplaylogP(r = r|S , G , a)(16 )   L = λL+λ(L+L ) + λL(17 )   where fis the gold frame and I , I , rare   gold labels of argument a.ris “ Dummy ” ,   indicating the end of the parsing . We force our   model to identify arguments in a left - to - right order ,   i.e.ais the leftmost argument in S. We use gold   frame in the initial node of FSG : G= ( t , f )   while other nodes are predicted autoregressively :   G = G+ ( ˆa,ˆr),ˆr∈ R.   5.2 Inference   Kpredicts frame and all arguments with their   roles in a sequential way . We use probabilities   above with some constraints : 1 . We use lexicon fil-   tering strategy : for a target t , we can use the lemma   ℓof it to find a subset of frames F⊂ F so that   we can reduce the searching space ; 2 . Similarly , we   takeRinstead of Ras the set of candidate FEs ; 3 .   In argument identification , we will mask spans that   are already selected as arguments , and ishould be   no less than i.   6 Experiment   6.1 Datasets   We evaluate Kon two FrameNet datasets : FN   1.5 and FN 1.7.FN 1.7 is an extension version   of FN 1.5 , including more fine - grained frames and   more instances . FN 1.5 defines 1019 frames and   9634 FEs while FN 1.7 defines 1221 frames and   11428 FEs . We use the same splits of datasets as   Peng et al . ( 2018 ) , and we also follow Kshirsagar   et al . ( 2015 ) ; Yang and Mitchell ( 2017 ) ; Peng et al.5002   ( 2018 ) ; Chen et al . ( 2021 ) to include exemplar in-   stances as training instances . As Kshirsagar et al .   ( 2015 ) states that there exists a domain gap between   exemplar instances and original training instances ,   we follow Chen et al . ( 2021 ) to use exemplar in-   stances as pre - training instances and further train   our model in original training instances . Table 1   shows the numbers of instances in two datasets .   6.2 Empirical Results   We compare Kwith previous models ( see ap-   pendix C ) on FN 1.5 and FN 1.7 . We focus on   three metrics : frame acc , arg F1 and full structure   F1.Full structure F1 shows the performance of   models on extracting full frame semantic structures   from text , frame acc denotes accuracy of frame   identification and arg F1 evaluates the results of   argument identification and role classification with   gold frames . All metrics are evaluated in test set .   Table 2 shows results on FN 1.5 . For a fair   comparison , we divide models into three parts :   the first part of models do not use exemplar in-   stances as training data ; the second part of mod-   els use exemplar instances without any pretrained   language models ; the third part of models use pre-   trained language models . K(GloVe ) uses GloVe(Pennington et al . , 2014 ) as word embeddings and   K(BERT ) fine - tunes pretrained language model   BERT ( Devlin et al . , 2019 ) to encode word repre-   sentations . Kachieves state - of - the - art of these   metrics under almost all circumstances , and we also   train our model with multiple runs , which shows   K(GloVe + exemplar ) and K(BERT ) outper-   forms previous state - of - the - art models by 1.4 and   1.3 full structure F1 - score averagely . There is an   exception that our model with BERT does not out-   perform Su et al . ( 2021 ) on frame identification   accuracy and we find that the number of train , val-   idation and test instances reported by them are a   little bit smaller than ours . Results on FN 1.7 and   statistical analysis of our model with multiple runs   are listed in appendix C.   It is worth noting that Kachieves much higher   recall than other models . We attribute this to the   incremental strategy of building FSG . By construct-   ing FSG incrementally , Kcan capture relations   between arguments and identify arguments that are   hard to find in other models .   6.3 Ablation Study   To prove the effectiveness of double - graph archi-   tecture , we conduct further experiments with K   on FN 1.5 . Table 3 shows ablation study on double-   graph architecture . w/o FSG uses LSTM instead   of our frame semantic graph decoder . It takes a5003   sequence of arguments and their roles that have al-   ready been identified as input to predict the next ar-   gument . FSG performs better than LSTM because   it captures target - argument and argument - argument   relations and can model long - distance dependen-   cies . w/o FKG directly uses input vectors of frame   knowledge graph encoder , and results also show   that knowledge - enhanced representations are bet-   ter than randomly initialized embeddings . We also   test the influence of double - graph structure with   pre - trained language models , the results shows the   double - graph structure is still effective and useful   even with the pre - trained language models .   FKG is a multi - relational heterogeneous graph .   The ablation study on structures of FKG is shown   in Table 4 . In addition , we evaluate the perfor-   mance of FI w/o FKG , which identifies frames   with a simple linear classification layer instead of   FKG , and the results prove that FKG strengthens   interactions between frame identification and role   classification .   In addition , we explore the effectiveness of name   information of FEs . Whether the name informa-   tion is used in previous work is unclear and some   BIO - based approaches like Marcheggiani and Titov   ( 2020 ) are likely to use name information by re-   garding FEs with the same name as the same label   in role classification . To the best of our knowledge ,   we are the first one to study the effectiveness of   name information . As shown in Table 5 , the names   of FEs provide rich information for frame semantic   parsing and shared embedding strategy can make   good use of the name information . Further ablation   study of FKG is conducted under the circumstance   without name information , the performance will   drop 0.9 points if we remove FKG too , showing   that knowledge - enhanced representations are im-   portant no matter whether we share embeddings   for FEs with the same names or not .   6.4 Transfer learning ability of FKG   As we have discussed in Figure 2 , if frame B is   related to frame A , a sentence with frame A can   contribute to parsing another sentence with frame   B by inter - frame reasoning . Frame - frame and inter-   frame FE - FE relations of FKG can guide Kto   learn experience from other frames .   To confirm that FKG has ability of transfer learn-   ing , we design zero ( few)-shot learning experi-   ments on FN 1.7 . Target word getcan evoke multi-   ple frames in FrameNet , and we choose instances5004including target getwith three frames ( Arriving ,   Getting andTransition_to_state ) as test instances .   We remove all instances with target getfrom train   and development instances , and can selectively add   few ( or zero ) instances including other targets with   these three frames into train and development sets .   We then compare the performance of Kwith   Kw / o FKG under zero - shot and few - shot cir-   cumstances . If FKG has ability of transfer learning ,   Kwith FKG can learn experience from other   related frames like Receiving and its performance   will not be influenced so much by the sparsity of   labels .   Table 6 shows the results of our experiments .   K= 0 indicates zero - shot learning while K=   { 4,16,32}indicates few - shot learning . Kwith-   out FKG performs much worse in zero - shot learn-   ing . As the number of instances that can be seen   in training grows up , the performance of Kwith   FKG gets a steady increase while the performance   ofKwithout FKG increases rapidly . Results   verify our assumption that even with few train in-   stances FKG can guide inter - frame reasoning with   its structure and allow models to learn experience   from other seen frames .   7 Related Work   Frame semantic parsing has caught wide attention   since it was released on SemEval 2007 ( Baker et al . ,   2007 ) . The task is to extract frame structures de-   fined in FrameNet ( Baker et al . , 1998 ) from text .   From then on , a large amount of systems are ap-   plied on this task , ranging from traditional machine   learning classifiers ( Johansson and Nugues , 2007 ;   Das et al . , 2010 ) to fancy neural models like re-   current neural networks ( Yang and Mitchell , 2017 ;   Swayamdipta et al . , 2017 ) and graph neural net-   works ( Marcheggiani and Titov , 2020 ; Bastianelli   et al . , 2020 ) .   A lot of previous systems neglect interactions   between subtasks and relations between arguments .   They either focus on one or two subtasks ( Hermann   et al . , 2014 ; FitzGerald et al . , 2015 ; Marcheggiani   and Titov , 2020 ) of frame semantic parsing or treat   all subtasks independently ( Das et al . , 2014 ; Peng   et al . , 2018 ) . Täckström et al . ( 2015 ) propose an ef-   ficient global graphical model , so they can enumer-   ate all possible argument spans and treat the assign-   ment as the Integer Linear Programming problem .   Later systems like FitzGerald et al . ( 2015 ) ; Peng   et al . ( 2018 ) follow this method . Swayamdiptaet al . ( 2017 ) ; Bastianelli et al . ( 2020 ) use sequence-   labeling strategy , and Yang and Mitchell ( 2017 ) in-   tegrate these two methods with a joint model . Only   few approaches like Chen et al . ( 2021 ) model inter-   actions between subtasks , which use the encoder-   decoder architecture to predict arguments and roles   sequentially . However , the sequence modeling of   Chen et al . ( 2021 ) does not consider structure infor-   mation and is not good at capturing long - distance   dependencies . We use graph modeling to enhance   structure information and strengthen interactions   between target and argument , argument and argu-   ment .   Only a few systems utilize linguistic knowledge   in FrameNet . Kshirsagar et al . ( 2015 ) use FE map-   pings to share information in FEs . In frame identi-   fication , Jiang and Riloff ( 2021 ) encode definitions   of frames and Su et al . ( 2021 ) use frame identi-   fication and frame semantic relations . However ,   they do not utilize ontological frame knowledge   in all subtasks while we construct a heterogeneous   graph containing both frames and FEs . Besides ,   our model does not need extra encoders to encode   definitions , which reduces parameters of the model .   Some systems also treat constituency parsing or   other semantic parsing tasks like AMR as a graph   construction problem . Yang and Deng ( 2020 ) use   GCN to encode intermediate constituency tree to   generate a new action on the tree . Cai and Lam   ( 2020 ) construct AMR graphs with the Transformer   ( Vaswani et al . , 2017 ) architecture .   8 Conclusion   In this paper , we incorporate knowledge into frame   semantic parsing by constructing Frame Knowl-   edge Graph . FKG provides knowledge - enhanced   representations of frames and FEs and can guide   intra - frame and inter - frame reasoning . We also   propose frame semantic graph to represent frame   semantic structures . We regard frame semantic   parsing as an incremental graph construction prob-   lem . The process to construct FSG is structure-   aware and can utilize relations between arguments .   Our framework Knowledge - guided Incremental se-   mantic parser with Double - graph ( K ) achieves   state - of - the - art on FrameNet benchmarks . How-   ever , how to utilize linguistic knowledge better is   still to be resolved . Future work can focus on better   modeling of ontological frame knowledge , which   will be useful for frame semantic parsing and trans-   fer learning in frame semantic parsing.5005Acknowledgements   This paper is supported by the National Science   Foundation of China under Grant No.61936012 ,   61876004 and the National Key Research and   Development Program of China under Grant No .   2020AAA0106700 .   References50065007   A Model Details   A.1 Graph Convolutional Network   Graph convolution is introduced in Kipf and   Welling ( 2016 ) . A GCN layer is defined as fol-   low :   f(H , G ) = σ / parenleftig   ˜D˜A˜DHW / parenrightig   ( 18 )   where Hdenotes hidden representations of nodes   inl - th layer , σis the non - linear activation function   ( e.g. ReLU ) and Wis the weight matrix . ˜Dand˜A   are separately the degree and adjacency matrices   for the graph G. From a node - level perspective , a   GCN layer can be also formalized as follow :   h = σ   /summationdisplay1   chW    ( 19 )   where N(i)is the set of neighbors of node i , and   c=/radicalbig   |N(j)|/radicalbig   |N(i)| .   By stacking Ldifferent GCN layers , we get final   GCN module GCN ( H , G ) .   A.2 Relational Graph Convolutional Network   Type information of graph edges is ignored in GCN ,   and RGCN ( Schlichtkrull et al . , 2018 ) is proposed   to model relational data from which we can benefit   to model the multi - relational graph FKG . Different   edge types use different weights and only edges   of the same relation type rare associated with the   same projection weight W. From a node ’s view :   h = σ   hW+/summationdisplay / summationdisplay1   chW      ( 20 )   where Ndenotes the set of neighbor indices of   nodeiunder relation r∈Randcis a normaliza-   tion constant i.e. |N| .   InK , we use tanh as activation function of   RGCN for normalization because we need to cal-   culate normalized dot product similarities between   frames / FEs and target / arguments .   A.3 Encoding Dependency Tree   We follow previous studies ( Marcheggiani and   Titov , 2020 ; Bastianelli et al . , 2020 ) to use syn-   tax structures like dependency tree TofSinKHyper - parameter Value   batch size 32   learning rate 6e-5/1e-4   lr decay 0.6 per 30 epochs   optimizer Adam   pretrain epochs 10/20/ 30/40/50   epoch 100   activation function ReLU   FFN Layers 2   LSTM Layers 2   GCN Layers 1/ 2   d , d 512 , 256   λ , λ , λ 0.1 , 0.3 , 0.3   because syntax structure is proved beneficial to se-   mantic parsing . We use Stanza ( Qi et al . , 2020 ) ,   an open - source python NLP toolkit to parse de-   pendency syntactic structure for instances , and   depGCN ( Marcheggiani and Titov , 2017 ) to en-   code the syntactic structure . We simplify depGCN   by ignoring directions and labels of edges in de-   pendency tree , which means if token iis head or   dependent of token j , we will have A = A= 1   in adjacency matrix AofT.   In addition , if we use BERT as encoder , the to-   kens are sub - word level and the adjacency matrix   will be a little bit different . Specifically , if token   iis the sub - word of some word u , token jis the   sub - word of some word vanduis head or depen-   dent word of v , we will have A = A= 1 in   adjacency matrix A.   B Hyper - parameter Setting   For replicability of our work , we list hyper-   parameter settings of K(GloVe ) and K   ( BERT ) in Table 7 and 8 . We use the development   set to manually tune the optimal hyper - parameters   based on Full structure F1 . The values of hyper-   parameters finally selected are in bold . Token em-   beddings we use in K(GloVe ) are the same as   Chen et al . ( 2021 ) , including word , lemma and   POS tag embeddings with a binary type embedding   to distinguish whether a token is a target or not .   C Experiment Details   C.1 Models   We compare Kwith following baselines:5008Hyper - parameter Value   bert version bert - base - uncased   batch size 16   learning rate 6e-6/1e-5   optimizer BertAdam   pretrain epochs 10/20/ 30/40/50   epoch 100   activation function ReLU   FFN Layers 2   LSTM Layers 2   GCN Layers 2   d , d 512 , 256   λ , λ , λ 0.1 , 0.3 , 0.3   SEMAFOR : a widely - used open - resource statisti-   cal model proposed by Das et al . ( 2010 , 2014 ) .   SEMAFOR ( HI ) : an improved version of SE-   MAFOR using exemplar instances and hierarchy   features ( FE mappings ) proposed by Kshirsagar   et al . ( 2015 )   Hermann et al . ( 2014 ) : a neural network-   based model learning representations of words and   frames .   Täckström et al . ( 2015 ) : identifying arguments   with a global graphical model .   FitzGerald et al . ( 2015 ) : an extension of Täck-   ström et al . ( 2015 ) learning neural representations   of frames and FEs .   open - SESAME : a syntax - free open - resource se-   mantic parser proposed by Swayamdipta et al .   ( 2017 ) .   Swayamdipta et al . ( 2018 ) : an extension version   ofopen - SESAME with multi - task and exemplar   instances .   Yang and Mitchell ( 2017 ) : a joint model integrat-   ing both sequential and relational models .   Peng et al . ( 2018 ) : a joint model using latent   structure variables .   Chen et al . ( 2021 ) : a joint encoder - decoder model   predicting arguments and roles sequentially .   Marcheggiani and Titov ( 2020 ) : a GCN - based   model over constituency trees .   Bastianelli et al . ( 2020 ) ( JL ) : a GCN - based model   encoding syntactic constituency path . JL denotes   joint learning on all subtasks of frame semantic   parsing .   Kalyanpur et al . ( 2020 ) : a T5 - based model   treating frame semantic parsing as a sequence - to-   sequence generation task .   Jiang and Riloff ( 2021 ) : a sentence - pair bert-   based model using frame definitions .   Su et al . ( 2021 ) : a BERT - based model for frame   identification using both frame identification and   frame semantic relations .   C.2 Empirical Results on FN 1.7   Table 9 , 10 , 11 list our results with comparing mod-   els . Koutperforms previous state - of - the - art ex-   cept Su et al . ( 2021 ) . FN 1.7 is the up - to - date   extension version of FN 1.5 containing more fine-   grained frames and FEs . However , there are only   few models reporting their results on FN 1.7 and we   hope that future work on frame semantic parsing   can be more focused on FN 1.7 .   C.3 Time Costs of FKG   The FKG is built over the full FrameNet containing   more than 10,000 nodes while the intra - frame and   inter - frame relations make the graph larger . Since   we need to encode the full FKG when parsing a sin-   gle sentence , it ’s necessary to explore the time costs   of FKG . Results are shown in Table 12 and we can   find the time encoding FKG is approximately 20 %   in the whole runtime and may slightly hurt the effi-   ciency of our models . However , in inference time,5009Model Time cost ( % )   K(GloVe ) 27.17   K(BERT ) 18.85   the representations of nodes in FKG are fixed and   we can load the representations offline to reduce   the inference time .   C.4 Statistical Analysis of Kon FN 1.5   For evaluating solidity of our model , we train K   with five random seeds . The average performances   with deviation and results of significance testing   are listed in Table 13 . The significance testing is to   show whether our model significantly outperforms   previous state - of - the - art , and we do not conduct   significance testing for K(BERT ) because we do   not outperform Su et al . ( 2021 ) on frame accuracy .   Allp - values are less than 0.05 and even some p-   values are less than 1e-3 , which proves the solidity   of our model.50105011