  Xiangci Li Biswadip Mandal Jessica Ouyang   Department of Computer Science   University of Texas at Dallas   Richardson , TX 75080   lixiangci8@gmail.com ,   { Biswadip . Mandal , Jessica.Ouyang}@UTDallas.edu   Abstract   Academic research is an exploratory activity   to discover new solutions to problems . By this   nature , academic research works perform litera-   ture reviews to distinguish their novelties from   prior work . In natural language processing , this   literature review is usually conducted under the   “ Related Work ” section . The task of related   work generation aims to automatically generate   the related work section given the rest of the   research paper and a list of papers to cite . Prior   work on this task has focused on the sentence as   the basic unit of generation , neglecting the fact   that related work sections consist of variable   length text fragments derived from different   information sources . As a first step toward a   linguistically - motivated related work genera-   tion framework , we present a Citation Oriented   Related Work Annotation ( CORWA ) dataset   that labels different types of citation text frag-   ments from different information sources . We   train a strong baseline model that automatically   tags the CORWA labels on massive unlabeled   related work section texts . We further suggest   a novel framework for human - in - the - loop , iter-   ative , abstractive related work generation .   1 Introduction   Academic research is an exploratory activity to   solve problems that have never been solved before .   By this nature , each academic research work must   sit at the frontier of its field and present novel contri-   butions that have not been addressed by prior work ;   in order to convince readers of the novelty of the   current work , the authors must compare against the   prior work . While the format may vary among dif-   ferent fields , in natural language processing ( NLP ) ,   this literature review is usually conducted under   the “ Related Work ” section . Since each paper must   review the relevant prior work in its field , which   is shared among papers on the same topic or task ,   many related work sections in a given field can   be similar in both content and format . Therefore , Figure 1 : An example of CORWA labels displayed using   the BRAT interface ( Stenetorp et al . , 2012 ) .   it is a natural motivation to develop a system for   generating related work sections automatically .   The task of automatic related work generation   is that of generating the related work section of a   target paper given the rest of the target paper and a   set of papers to cite . Prior works ( Hoang and Kan ,   2010 ; Hu and Wan , 2014 ; Chen and Zhuge , 2019 ;   Wang et al . , 2019 ; Xing et al . , 2020 ; Ge et al . , 2021 ;   Luu et al . , 2021 ; Chen et al . , 2021 ) mostly simplify   related work generation as a general summariza-   tion task , generating related work sections using   sentence - level models . This approach ignores the   nature of the related work section , which consists of   variable - length text fragments derived from differ-   ent information sources . These text fragments refer   to different cited papers , and they range in length   from a few words to multiple sentences . There are   also non - citation , supporting sentences that serve   various discursive roles , such as introducing new   topics , transitioning between topics , or reflecting   on the current work . We argue it is necessary to dis-5426tinguish these heterogeneous text fragments , rather   than treating related work sections as concatena-   tions of homogeneous sentences .   In addition to the heterogeneous information   sources for related work section sentences , the writ-   ing styles of these sentences also vary . Khoo et al .   ( 2011 ) classify literature reviews to be integrative   or descriptive , depending on whether they focus on   high - level ideas or provide more detailed informa-   tion on specific studies . However , this document-   level classification scheme was intended as a de-   scriptive , information science study of related work   sections , and it has not been previously used in   automatic related work generation .   Inspired by these observations , as a first step   towards linguistically - motivated related work gen-   eration , we present a Citation Oriented Related   Work Annotation ( CORWA ) dataset of related work   sections from NLP papers . We distinguish text   fragments from different information sources by   tagging each sentence with discourse labels and   identifying the spans of tokens belonging to each   citation . We further distinguish citations that give   detailed explanations of cited papers and those that   illustrate high - level concepts .   Our main contributions are as follows : ( 1 ) We   collect a CORWA dataset that decomposes the re-   lated work section with three inter - related annota-   tion tasks — discourse tagging , citation span detec-   tion , and citation type recognition — and demon-   strate the significance of CORWA with analyses   from multiple perspectives ( § 3 ) . ( 2 ) We propose a   strong baseline model that automatically tags the   CORWA annotation scheme on massive unlabeled   related work section texts ( § 4 ) . ( 3 ) We show that   citation spans are a better target than citation sen-   tences with two example tasks ( § 5 ) . ( 4 ) We discuss   a novel framework for human - in - the - loop , iterative ,   abstractive related work generation ( § 6 ) .   2 Related Work   Extractive Related Work Generation . Early re-   lated work generation systems employed the ex-   tractive summarization approach . Hoang and Kan   ( 2010 ) pioneered the task , developing rules to se-   lect sentences following a topic hierarchy tree that   was assumed to be given as input . Hu and Wan   ( 2014 ) grouped sentences into topic - biased clus-   ters with PLSA , modeled sentence importance with   SVR , and applied a global optimization framework   to select sentences . Chen and Zhuge ( 2019 ) se - lected sentences from papers that co - cited the same   cited papers as the target paper in order to cover a   minimum Steiner tree constructed from the paper ’s   keywords . Wang et al . ( 2019 ) extracted Cited Text   Spans ( CTS ) , the matched text spans in the cited   paper that are most related to a given citation . How-   ever , these extractive approaches aim to maximally   cover the citation texts with the extracted sentences ,   thus mostly ignoring the reference type citations   that are concise and abstractive ( § 3.1.3 ) .   Abstractive Related Work Generation . Re-   cently , Xing et al . ( 2020 ) extend the pointer-   generator ( See et al . , 2017 ) to take two text inputs ,   allowing them to recover a masked citation sen-   tence given its neighboring context sentences . Ge   et al . ( 2021 ) encode the citation context , cited pa-   per ’s abstract , and citation network and train their   model with multiple objectives : sentence salience   score regression of the cited paper ’s abstract , func-   tional role classification of the citation sentence ,   and citation sentence generation . Chen et al . ( 2021 )   propose a relation - aware , multi - document encoder   to generate a related work paragraph given a set   of cited papers . Luu et al . ( 2021 ) fine - tune GPT2   ( Radford et al . , 2019 ) on scientific texts and ex-   plore several techniques for representing docu-   ments , such as using extracted named entities .   All of the works described above focus on the   generation aspect , while neglecting dataset collec-   tion ; their datasets are mostly extracted automati-   cally . Moreover , the datasets are not reused , though   they are publicly available , because these works   all use slightly different problem definitions , and   thus the models are not directly comparable ( Li   and Ouyang , 2022 ) . In this work , we focus on   collecting a dataset that is widely applicable to var-   ious related work generation settings , rather than   proposing another incomparable approach .   3 CORWA Dataset   In this work , we limit our scope to publications   from the NLP domain for ease of automatically ex-   tracting the related work section ; existing work on   related work generation has also focused on NLP   in the past . We build our dataset on top of the NLP   partition of the S2ORC dataset ( Lo et al . , 2020 ) ,   a large - scale corpus of scientific papers derived   from LTEX source code and PDF files . We extract   the related work section by matching the section   titles . Because not all papers cited in the extracted   related work sections are available in S2ORC , we5427prioritize annotating related work sections where   the majority of their cited papers are available .   3.1 Annotation Scheme   Our CORWA dataset decomposes the related work   section with three inter - related annotation tasks :   discourse tagging , citation span detection , and cita-   tion type recognition .   3.1.1 Discourse Tagging   Each sentence in a related work section has a spe-   cific role and information source . Some may be   general topic or transition sentences ; some summa-   rize one or multiple prior works in detail , while oth-   ers describe the general relationship among prior   works at a high level . Our discourse tagging task   tags the role of each related work sentence with   one of six labels : { single_summ , multi_summ , nar-   rative_cite , reflection , transition , other } .   Single Document Summarization . Sin-   gle_summ refers to sentences that summarize one   single cited work in detail . Most typically , this   includes sentences with explicit citation marks , as   when a work is mentioned for the first time . We   also include the following cases : ( 1 ) follow - up   sentences without explicit citation marks that de-   scribe the same paper as a preceding single_summ   sentence , and ( 2 ) sentences containing multiple   citations that heavily focus on one of those works .   Multi - Document Summarization . Multi_summ   refers to sentences that summarize multiple prior   works of equal importance . As with single_summ ,   we include the case of follow - up sentences without   explicit citation marks that continue describing the   same group of prior works discussed in a preceding   multi_summ sentence .   Narrative Citation . In contrast to single_summ   andmulti_summ , narrative citation ( narrative_cite )   refers to citation sentences that do not summarize   specific cited works in detail , but rather convey   high - level observations from the authors of the cur-   rent work . Narrative_cite sentences may contain   general statements about the field or task , or the au-   thors ’ comments on or comparisons of prior works .   Reflection . In addition to describing prior works ,   authors discuss how they relate to the current   work , highlighting the authors ’ novel contributions .   These reflection sentences focus on the current   work , instead of prior works . Transition . Non - citation sentences in related   work sections serve as topic introductions or tran-   sitions from one topic to another . We label these   supplemental sentences that do not belong to any   of the above cases as transition sentences .   Other . The related work sections in our dataset   are extracted automatically using heuristics based   on section titles , and there are occasionally some   errors in section boundary detection ; we label those   sentences that are not actually part of the related   work section as other .   3.1.2 Citation Span Detection   In order to understand sentences that describe prior   work , it is crucial to recognize the token - level map-   ping between the citation text and the cited paper(s ) .   Our citation span detection task identifies the span   of text whose information is directly derived from a   specific cited paper . For example , if a cited paper is   explained with a summary , its citation span covers   the entire summary , which may range from part of   a sentence to a few consecutive sentences ; if a cited   paper is mentioned with an explicit citation , but is   not described or discussed at all , then the citation   span is just the citation mark .   In constructing the dataset , we find that a single   citation rarely spans across paragraph boundaries   without a new explicit citation mark , so we require   our spans to be bounded by paragraph boundaries .   3.1.3 Citation Type Recognition   Our citation type recognition task indicates whether   a cited work is discussed in detail or used to illus-   trate a high - level concept . We label these types of   citations as dominant andreference , respectively .   Dominant . These citations are discussed in de-   tail , usually via summarization of their content , and   are often longer than reference citations .   Reference . These citations are not discussed in   detail . They frequently appear in narrative_cite   sentences , but may also appear in single_summ and   multi_summ sentences when they are not the main   focus of the sentence , and thus it is not sufficient   to depend on the sentence - level discourse tags to   distinguish them . For example , in Figure 1 , line   5 , the pointer - generator network ( See et al . , 2017 )   is cited for reference as part of a longer dominant   citation span . Reference citations tend to be more   abstractive than dominant citations.5428   3.2 Annotation Process and Agreement   Two graduate students from our university ’s Com-   puter Science Department , manually annotated   927 related work sections . They first annotated 23   related work sections from scratch , after which we   incrementally trained a transformer - based tagging   model ( Vaswani et al . , 2017 ) ( § 4 ) to assist the an-   notation process , asking the annotators to correct   the model ’s predictions , rather than performing   manual annotation from scratch . We split the 362   annotated related work sections from papers pub-   lished in 2019 and later as our test set and all 565   earlier papers as the training set .   Since each related work section is labeled by a   single annotator , we calculate agreement by sam-   pling 50 related work sections from the test set and   asking the other annotator to re - annotate them from   scratch . We obtain strong agreement on all tasks   ( Cohen ’s κof 0.824 , 0.965 and 0.878 for discourse   tagging , citation type recognition , and citation span   detection , respectively ; citation type recognition   and citation span detection are converted to token-   level labels for agreement calculation ) .   The automated , correction - based annotation pro-   cess is much faster than annotating from scratch   and allows us to collect a much larger annotated   dataset . As a trade - off , the annotations may be bi-   ased by the model ’s predictions if the annotators   fail to notice any incorrect predictions . This may   explain why our model performance reported in   § 4.2 is higher than the inter - annotator agreement .   3.3 Analysis of CORWA   The tasks of discourse tagging , citation span detec-   tion , and citation type recognition , capture distinct   but overlapping perspectives of information .   3.3.1 Relations among CORWA Subtasks   We investigate the relationships among the   CORWA subtasks by calculating the co - occurrence   distributions of discourse labels and citation span   types . A citation span is considered dominant if it   contains any dominant citations , and reference oth-   erwise . Figure 2 shows that dominant -type spans   ( average of 34.5 tokens ) are significantly longer   than reference -type spans ( average of 8.2 tokens ) .   Table 1 shows the count of each discourse label ,   the conditional probability and the joint probability   of discourse labels and citation span types . Sin-   gle_summ with dominant span , multi_summ with   dominant span , and narrative_cite with reference   span are the most frequent combinations . These   observations make intuitive sense , since dominant -   type spans describe cited papers in detail , often   taking the form of a summary , while reference -type   spans are highly abstracted , making them more   likely to be mixed into narrative -type sentences   that discuss high - level ideas , often encompassing   multiple cited papers . This difference is analogous   toinformative versus indicative summaries , where   the former serves as a surrogate for the document ,   and the latter characterizes what the document is   about ( Kan et al . , 2001 ) .   3.3.2 Related Work Writing Styles   Integrative or Descriptive ? As Khoo et al .   ( 2011 ) note , authors may describe the same cited   paper in two different styles : descriptive , which ex-5429   plicitly summarizes the cited paper , or integrative ,   which describes and comments on the cited paper   in a narrative form . We examine the ratio of summa-   rization ( both single_summ andmulti_summ ) and   narrative sentences ( narrative_cite ) in related work   paragraphs ( Figure 3 ) . The CORWA discourse la-   bels capture writing style differences among papers :   34.6 % of related work section paragraphs only con-   tainsummarization sentences , resembling Khoo et   al . ’s descriptive literature review , while 32.1 % of   paragraphs contain only narrative sentences , re-   sembling an integrative literature review . Interest-   ingly , 33.3 % of paragraphs mix both styles and are   neither purely descriptive nor purely integrative .   Frequent Discourse Label Subsequences . Sci-   entific discourse is used by paper authors to pro-   mote their ideas ( Li et al . , 2021a ) . We analyze the   patterns of CORWA discourse labels to uncover   how authors promote their ideas using a mix of   sentence types . We apply the rule - based PrefixS-   pan ( Han et al . , 2001 ) and Gap - Bide ( Li and Wang ,   2008 ) algorithms to extract frequent discourse la-   bel subsequences . We identify six typical subse-   quences , shown in Supplementary Tables 7 and 8 .   For example , the pattern of single_summ followed   byreflection compares the cited paper to the cur-   rent work , usually without directly criticizing the   cited paper , while single_summ followed by tran-   sition is the more impersonal pattern for criticism   of a cited paper , where authors tend to avoid direct   comparison with the current work .   4 Joint Related Work Tagger   To help propagate our CORWA annotations to mas-   sive unlabeled related work sections , we build a   joint related work tagger baselinethat is trained   on the three annotation tasks , discourse tagging , ci-   tation span detection , and citation type recognition ,   via multi - task learning ( Caruana , 1997 ) .   4.1 Model Design   Figure 4 shows the model architecture of our joint   related work tagger . We encode related work sec-   tions using a transformer - encoder ( Vaswani et al . ,   2017 ) paragraph by paragraph , as we enforce the   independence of paragraphs in CORWA citation   span annotations . We decode citation span labels   and citation type labels token by token , while our   discourse tagging task uses the paragraph - level   sentence tagging mechanism proposed by Li et al .   ( 2021b ) . Because the three sub - tasks of CORWA   are inter - related , we use multi - task learning to   jointly train the tagger by sharing the encoder   across tasks .   4.1.1 Paragraph Encoder   We experiment with several pre - trained   transformer - encoders ( Devlin et al . , 2019 ;   Beltagy et al . , 2019 ; Liu et al . , 2019 ; Beltagy et al . ,   2020 ) , and eventually focus on SciBERT ( Beltagy   et al . , 2019 ) , which is a variant of the BERT model   ( Devlin et al . , 2019 ) that is trained on a scientific   corpus with domain - specific tokenization schemes ,   including NLP papers .   4.1.2 Task - specific Decoders   Citation Span Detection & Citation Type Recog-   nition . We use the BIO 2tagging scheme ( Sang   and Veenstra , 1999 ) for the citation span detection   and citation type recognition tasks ; we use B , I ,   Ofor citation span detection and five labels — B-   Dominant , I - Dominant , B - Reference , I - Reference , 5430   andO — for citation type recognition . We use   a two - layer feed - forward network to decode the   encoded paragraph - level token embeddings to the   output sequence of BIO 2tags .   Discourse Tagging . We apply Li et al . ( 2021b ) ’s   paragraph - level sentence tagging approach for the   discourse labels : a simple attention mechanism is   used to aggregate token embeddings , sentence by   sentence , into sentence encodings , before decoding   the sentence encodings into discourse labels using   a two - layer multi - layer feed - forward network .   4.1.3 Multi - task Learning   We use cross - entropy loss on all three CORWA sub-   tasks . We balance the relative importance of the   sub - tasks by taking a weighted sum of the sub - task   losses of discourse tagging , citation span detection ,   and citation type recognition { L , L , L } :   L = γL+γL+γL ( 1 )   where { γ , γ , γ } are tuned hyper - parameters ;   their values are given in Table 3 .   4.2 Experiments   We perform five - fold cross - validation to tune the   model hyper - parameters . Table 2 shows the strong   performance of the model . We use the joint re-   lated work tagger to automatically label the unanno-   tated 11,465 related work sections remaining in the   S2ORC NLP partition and then use this distantly-   supervised data to further boost the model ’s per-   formance . For the citation span detection and ci-   tation type recognition tasks , we use a token - level   F1 score . Our final , distantly - supervised joint re-   lated work tagger achieves more than 0.9 test F1   on all three tasks , indicating the high quality of   the model ’s predictions . This model can be used   to propagate our labels on the unannotated related   work sections to create a very large training set for   future work .   5 Spans as an Alternative to Sentences   We argue that the citation spans annotated in   CORWA are a better alternative to the citation sen-   tences that have previously been used for the tasks   of ROUGE - based retrieval and citation text genera-   tion .   5.1 Queries for Relevant Sentence Retrieval   Citations focus on a small portion of the content   in cited papers , and this focus is not explicitly   recorded in the citation network . A popular ap-   proach for determining relevant sentences retrieves   sentences from the cited papers by comparing the   similarity between the gold citation sentence and   candidate sentences in the cited paper ( Cao et al . ,   2015 ; Yasunaga et al . , 2017 , 2019 ; Ge et al . , 2021 ) .   Figure 5 compares the distribution of the top-1 av-   erage of ROUGE-1 and ROUGE-2 recall scores   ( Lin , 2004 ) of retrieved sentences from cited papers   using citation spans with those using citation sen-   tences . There is no significant difference between   the average ROUGE scores of dominant spans and   sentences containing dominant citations , which is5431   reasonable because dominant spans are often full   sentences anyway . In contrast , the average score   ofreference spans is significantly higher than that   of sentences containing reference -type citations ;   reference spans are shorter and contain highly con-   centrated key information derived from their cited   papers . Thus , using CORWA citation spans as   queries for ROUGE - based cited sentence retrieval   is superior for reference -type citations and compa-   rable for dominant -type citations .   5.2 Span - based Related Work Generation   Existing neural network - based , abstractive related   work generation systems generate citation sen-   tences given the surrounding context sentences   ( Xing et al . , 2020 ; Ge et al . , 2021 ; Luu et al . , 2021 )   or generate entire paragraphs containing multiple   citations ( Chen et al . , 2021 ) . These task settings   neglect the fact that the citation text corresponding   to a cited paper is not necessarily in the form of a   sentence , but could be a portion of a sentence or a   block of multiple sentences . Our span - based anno-   tation scheme identifies the citation tokens that are   directly derived from the cited papers .   As Figure 6 shows , reference spans are not full   sentences , while dominant spans can cover multi-   ple sentences . For reference -type citations , using   a full sentence as the generation target includes   potentially unrelated tokens outside the citation   span that do not refer to the cited paper . For domi-   nant - type citations , using a single sentence as the   generation target can result in 1 ) information loss   when not all sentences describing the cited paper   are included in the target , and the model never   learns to generate them , or 2 ) information leak   when sentences that actually describe the cited pa - per are used as context sentences instead of target   sentences . Thus , we propose a span - level citation   text generation task and present a pilot study using   a Longformer - Encoder - Decoder ( LED ) ( Beltagy   et al . , 2020 ) baseline model .   5.2.1 Experimental Setting   The common Transformer - based language models   ( Devlin et al . , 2019 ; Liu et al . , 2019 ; Lewis et al . ,   2020 ; Raffel et al . , 2020 ) have a limited input win-   dow size ( typically 512 or 1024 tokens ) , which   presents a major challenge for tasks like related   work generation that use multiple long documents   as inputs . LED ( Beltagy et al . , 2020 ) addresses   this challenge by using a local self - attention mech-   anism , rather than global self - attention , handling   in input context windows of up to 16k tokens . We   present an LED - based baseline model for the cita-   tion span generation task .   We first pretrain the LED - base model on the   masked language modeling ( MLM ) task ( Devlin   et al . , 2019 ) using related work sections from   S2ORC papers in the computer science domain ,   as well as on the cross - document language model-   ing ( CDLM ) task ( Caciularu et al . , 2021 ) , which   aligns masked citation sentences with their context   sentences and the full text of their cited papers . We   further pretrain the LED encoder with the three   CORWA sub - tasks ( Supplementary Table 6 ) . All   pretraining strictly excludes the texts from test set .   For the citation span generation task , we input   the concatenation of { the target paper ’s introduc-   tion ( following Luu et al . ( 2021 ) ) , the partial related   work paragraph excluding the target citation span ,   and the concatenation of { explicit citation mark ,   title , and abstract } of each cited paper in the target   span } ; the generation target is the ground truth ci-   tation span from CORWA . We provide the explicit   citation mark ( e.g. Devlin et al . , 2018 ) because it   is simple to extract but can not be inferred from the   paper text alone . Just as a human reader may re-   member the content of the frequently cited papers   or the research topics of frequently cited authors ,   so the citation mark tokens may carry information   about the cited paper and its authors .   In addition to the CORWA training set , we use   the distantly supervised labels predicted by our   joint related work tagger ( § 4.2 ) for training . We use   the default hyper - parameters of the Huggingface   LED implementation ( Wolf et al . , 2020).5432   5.2.2 Experimental Results   As Table 4 shows , the ROUGE scores of our   LED - base models for citation span / sentence gen-   eration are similar to previous sentence - level ci-   tation text generation models ( Xing et al . , 2020 ;   Ge et al . , 2021 ) , and our pretraining improves the   citation span generation performance . Compared   to sentence - level generation , span - level generation   has lower scores for dominant citations , but higher   scores for reference citations . However , because   the span- and sentence - level tasks have different   generation targets , their scores can not be directly   compared .   We perform a human evaluation following the   setting of Xing et al . ( 2020 ) ; Ge et al . ( 2021 ) . We   sample 15 instances each for dominant andref-   erence citations and compare their corresponding   span- and sentence - based generation outputs , as   well as the gold spans from the original related   work sections . Each citation text is rated by three   NLP graduate students who are fluent in English   on a 1 ( very poor ) to 5 ( excellent ) point scale , with   respect to four aspects : fluency ( whether a citation   span / sentence is fluent ) , relevance ( whether a cita-   tion span / sentence is relevant to the cited paper(s ) ) ,   coherence ( whether a citation span / sentence is co-   herent within its context ) , and overall quality .   Table 5 shows human evaluation results , with   moderate inter - annotator agreement ( Kendall ’s τ   of 0.298 , 0.205 , and 0.172 among three annotators ) .   All citation texts are judged to be highly fluent .   Interestingly , in previous studies ( Xing et al . ,2020 ; Ge et al . , 2021 ) the scores of gold sentences   are higher than those of generated texts , but our   gold spans have a significantly lower relevance   scores than the generated spans . This is likely be-   cause the gold spans contain information derived   from the body sections of the cited papers , which   are not provided to either the models or to the hu-   man judges . As a result , some gold spans appear   to be irrelevant to the human judges , echoing our   earlier finding in § 5.1 that citation spans contain   more focused information . This observation also   suggests that gold citation spans are not necessarily   the best target for all task settings .   We also see that , while dominant sentences and   spans receive similar scores , the reference sen-   tences have lower relevance scores than the spans .   This result makes sense because reference citation   spans are short and focused , so the full sentences   include tokens unrelated to the cited paper(s ) . Over-   all , the generated spans are rated slightly higher   than the generated sentences by the human judges ,   confirming that span - level citation text generation   is preferable to sentence - level generation .   6 Toward Full Related Work Generation   Existing extractive related work generation systems   ( Hoang and Kan , 2010 ; Hu and Wan , 2014 ; Chen   and Zhuge , 2019 ; Wang et al . , 2019 ) select sen-   tences from the target paper and/or the cited papers ,   which can be concatenated to form a full related   work section ; neural network - based , abstractive   related work generation systems generate individ-   ual citation sentences ( Xing et al . , 2020 ; Ge et al . ,   2021 ; Luu et al . , 2021 ) or paragraphs ( Chen et al . ,   2021 ) . However , none of these prior works address   the ordering of the extracted / generated sentences or   the grouping of sentences into paragraphs , nor are   they able to produce rhetorical sentences to smooth   the transitions between citations . No prior work   bridges the gap from generating individual citation   texts to generating a full related work section .   We suggest a bottom - up , iterative approach to   generate full related work sections . The process   would begin with generating citation spans under5433the settings proposed in § 5.2 . Then , multiple gener-   ated citation spans would be aggregated and rewrit-   ten into citation text blocks in either the summa-   rization ornarrative style . These blocks would be   further aggregated and rewritten into paragraphs by   generating transition andreflection sentences .   Generating and rewriting in this pipeline fashion   has the following benefits : ( 1 ) It mitigates the prac-   tical issue of computational resource limitations ,   given that state - of - the - art models do not perform   well on long text generation . ( 2 ) The auxiliary in-   puts , such as citation functions or discourse tags ,   may vary for each stage of generation . ( 3 ) As a   practical system to assist researchers , it is crucial to   allow user involvement in the iterative generation   process . Due to the large search space , consisting   of multiple valid related work section candidates   with different writing styles , it is extremely chal-   lenging to precisely generate a satisfying text with a   one - shot , end - to - end system . A human - in - the - loop   approach allows the user to significantly prune the   search space and simultaneously reduces the error-   propagation issue caused by the pipeline design .   7 Other Related Tasks   7.1 Scientific Document Understanding   Besides summarization , scientific document under-   standing also plays an important role in related   work generation .   Citation Analysis . Citations are the core of re-   lated work sections . There has been a line of re-   search on citation analysis , including citation func-   tion ( Teufel et al . , 2006 ; Dong and Schäfer , 2011 ;   Jurgens et al . , 2018 ; Tuarob et al . , 2019 ) , citation   intent ( Cohan et al . , 2019 ; Lauscher et al . , 2021 ) ,   citation sentiment ( Athar , 2011 ; Athar and Teufel ,   2012 ; Ravi et al . , 2018 ; Vyas et al . , 2020 ) , etc .   These studies annotate citations with different la-   beling schemes to study the various usages and   purposes of citations .   Discourse Analysis . Scientific discourse analy-   sis studies the rhetorical components of clauses ,   sentences , or text spans that are not limited to ci-   tations , uncovering how authors persuade expert   readers with their claims . There is a significant   amount of prior work proposing discourse schemes   and developing models for discourse tagging for   scientific articles ( Teufel and Moens , 1999 , 2002 ;   Hirohata et al . , 2008 ; Liakata , 2010 ; Liakata et al . ,   2012 ; Guo et al . , 2010 ; De Waard and Maat , 2012;Burns et al . , 2016 ; Dernoncourt and Lee , 2017 ;   Huang et al . , 2020 ; Li et al . , 2021a ) .   Our CORWA discourse tagging task focuses on   distinguishing the source of the information in each   related work sentence , which is complementary to   the discourse tagging work listed above .   7.2 Cited Text Span   AbuRa’ed et al . ( 2020 ) extend Hoang and Kan   ( 2010 ) ’s RWSData dataset by annotating the Cited   Text Span ( CTS ) ( Wang et al . , 2019 ) . They an-   notate the specific sentences in cited papers that   each citation in the target paper is based on . For   each cited paper , they further collect a set of papers   that co - cite this cited paper . Jaidka et al . ( 2018 ,   2019 ) propose the CL - Scisumm shared task , which   includes identifying the CTS in reference papers   for each citation instance . This shared task pro-   vides a valuable dataset for the precise generation   of citation texts from a CTS , in contrast to most   recent work , which uses the cited paper ’s abstract   or introduction .   7.3 Studies of Literature Reviews   From an information studies perspective , Khoo   et al . ( 2011 ) largely classify literature reviews into   two styles : integrative and descriptive . Descrip-   tive literature reviews summarize individual studies   and provide detailed information on each , such as   methods , results , and interpretation ; integrative lit-   erature reviews provide fewer details of individual   studies , instead focusing on synthesizing ideas and   results extracted from these papers . Jaidka et al .   ( 2010 , 2011 , 2013 ) analyze the properties of these   two types of literature reviews .   8 Conclusion   We present the CORWA dataset of three inter-   related annotation tasks : discourse tagging , citation   span detection , and citation type recognition . We   demonstrate the significance of CORWA with anal-   yses from multiple perspectives , such as writing   style and discourse patterns . We propose a strong   baseline model that can automatically propagate   the CORWA annotation scheme to massive unla-   beled related work sections . Furthermore , we show   that citation spans are a better alternative to citation   sentences for both the relevant sentence retrieval   and citation generation tasks . Finally , we discuss   a novel framework for human - in - the - loop iterative   abstractive related work generation.5434References543554365437A Appendix   A.1 Training Configurations   For the joint related work tagger training , we use   GeForce GTX 1080 11 GB GPUs . The training   process lasts 2.5 hours on a single GPU using Hug-   gingface ’s ( Wolf et al . , 2020 ) SciBERT , BERT - base   or Roberta - base as the paragraph encoders , and it   lasts 6.5 hours using LED - base encoder . We train   the models for 15 epochs . It takes approximately   one week to run the hyper - parameter search using   five - fold cross - validation for all language models ,   using 8 GPUs in total .   For training the citation span generation model ,   we use Tesla V100s - PCIE-32 GB GPUs . The train-   ing process lasts for 2 days on a single GPU . We   run the training for a maximum of 3 epochs with   early stopping based on the validation loss .   A.2 Ethical Considerations   We present a new dataset that is derived from the   S2ORC dataset ( Lo et al . , 2020 ) , which is released   under CC BY - NC 2.0 license . The Huggingface   models ( Wolf et al . , 2020 ) we develop upon are   released under Apache License 2.0 .   Our annotators were compensated for their work   at a rate of double the minimum wage in our local   area.543854395440