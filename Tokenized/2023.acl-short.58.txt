  Yahan Yang   University of Pennsylvania   yangy96@seas.upenn.eduSoham Dan   IBM Research   soham.dan@ibm.com   Dan Roth   University of Pennsylvania   danroth@seas.upenn.eduInsup Lee   University of Pennsylvania   lee@cis.upenn.edu   Abstract   Recently it has been shown that state - of - the - art   NLP models are vulnerable to adversarial at-   tacks , where the predictions of a model can be   drastically altered by slight modifications to the   input ( such as synonym substitutions ) . While   several defense techniques have been proposed ,   and adapted , to the discrete nature of text adver-   sarial attacks , the benefits of general - purpose   regularization methods such as label smooth-   ing for language models , have not been studied .   In this paper , we study the adversarial robust-   ness provided by label smoothing strategies   in foundational models for diverse NLP tasks   in both in - domain and out - of - domain settings .   Our experiments show that label smoothing   significantly improves adversarial robustness   in pre - trained models like BERT , against vari-   ous popular attacks . We also analyze the rela-   tionship between prediction confidence and ro-   bustness , showing that label smoothing reduces   over - confident errors on adversarial examples .   1 Introduction   Neural networks are vulnerable to adversarial at-   tacks : small perturbations to the input , which do   not fool humans ( Szegedy et al . , 2013 ; Goodfel-   low et al . , 2014 ; Madry et al . , 2017 ) . In NLP   tasks , previous studies ( Alzantot et al . , 2018 ; Jin   et al . , 2019 ; Li et al . , 2020 ; Garg and Ramakr-   ishnan , 2020 ) demonstrate that simple word - level   text attacks ( synonym substitution , word inser-   tion / deletion ) easily fool state - of - the - art models ,   including pre - trained transformers like BERT ( De-   vlin et al . , 2019 ; Wolf et al . , 2020 ) . Further , it has   recently been shown models are overconfidenton   examples which are easy to attack ( Qin et al . , 2021 )   and indeed , such over - confident predictions plaguemuch of modern deep learning ( Kong et al . , 2020 ;   Guo et al . , 2017 ; Nguyen et al . , 2015 ; Rahimi et al . ,   2020 ) . Label smoothing is a regularization method   that has been proven effective in a variety of ap-   plications , and modalities ( Szegedy et al . , 2016 ;   Chorowski and Jaitly , 2017 ; Vaswani et al . , 2017 ) .   Importantly , it has been shown to reduce overcon-   fident predictions and produce better confidence   calibrated classifiers ( Muller et al . , 2019 ; Zhang   et al . , 2021 ; Dan and Roth , 2021 ; Desai and Durrett ,   2020 ; Huang et al . , 2021 ; Liu and JaJa , 2020 ) .   In this work , we focus on the question : does   label smoothing also implicitly help in adversarial   robustness ? While there has been some investi-   gation in this direction for adversarial attacks in   computer vision , ( Fu et al . , 2020 ; Goibert and   Dohmatob , 2019 ; Shafahi et al . , 2019 ) , there is   a gap in understanding of whether it helps with   discrete , text adversarial attacks used against NLP   systems . With the increasing need for robust NLP   models in safety - critical applications and a lack of   generic robustness strategies , there is a need to   understand inherent robustness properties of pop-   ular label smoothing strategies , and the interplay   between confidence and robustness of a model .   In this paper , we extensively study standard la-   bel smoothing and its adversarial variant , cover-   ing robustness , prediction confidence , and domain   transfer properties . We observe that label smooth-   ing provides implicit robustness against adversarial   examples . Particularly , we focus on pre - trained   transformer models and test robustness under vari-   ous kinds of black - box and white - box word - level   adversarial attacks , in both in - domain and out - of-   domain scenarios . Our experiments show that label   smoothing ( 1 ) improves robustness to text adver-   sarial attacks ( both black - box and white - box ) , and   ( 2 ) mitigates over - confident errors on adversarial   textual examples . Analysing the adversarial exam-657ples along various quality dimensions reveals the   remarkable efficacy of label smoothing as a simple   add - on robustness and calibration tool .   2 Background   2.1 Text Adversarial Attacks   Our experiments evaluate the robustness of text   classification models under three state - of - the - art   text adversarial attacks TextFooler ( black - box ) ,   BAE ( black - box ) and SemAttack ( white - box ) ,   described below . For a particular victim NLP   model and a raw text input , the attack produces   semantically - similar adversarial text as output . Im-   portantly , only those examples are attacked , which   are originally correctly predicted by the victim   model . The attacks considered are word - level , i.e.   they replace words in a clean text with their syn-   onyms to maintain the meaning of the clean text ,   but change the prediction of the victim models .   •TextFooler ( TF ) : ( Jin et al . , 2019 ) proposes   an attack which determines the word impor-   tance in a sentence , and then replaces the im-   portant words with qualified synonyms .   •BAE : ( Garg and Ramakrishnan , 2020 ) uses   masked pre - trained language models to gener-   ate replacements for the important words until   the victim model ’s prediction is incorrect .   •SemAttack ( SemAtt ) : ( Wang et al . , 2022 )   introduces an attack to search perturbations in   the contextualized embedding space by formu-   lating an optimization problem as in ( Carlini   and Wagner , 2016 ) . We specifically use the   white - box word - level version of this attack .   2.2 Label Smoothing   Label Smoothing is a modified fine - tuning proce-   dure to address overconfident predictions . It intro-   duces uncertainty to smoothen the posterior distri-   bution over the target labels . Label smoothing has   been shown to implicitly calibrate neural networks   on out - of - distribution data , where calibration mea-   sures how well the model confidences are aligned   with the empirical likelihoods ( Guo et al . , 2017 ) .   •Standard Label Smoothing ( LS ) ( Szegedy   et al . , 2013 ; Muller et al . , 2019 ) constructsa new target vector ( y ) from the one - hot   target vector ( y ) , where y= ( 1−α)y+   α / K for aKclass classification problem . α   is a hyperparameter selection and its range is   from 0 to 1 .   •Adversarial Label Smoothing ( ALS ) ( Goib-   ert and Dohmatob , 2019 ) constructs a new tar-   get vector ( y ) with a probability of 1−α   on the target label and αon the label to which   the classification model assigns the minimum   softmax scores , thus introducing uncertainty .   For both LS and ALS , the cross entropy loss is   subsequently minimized between the model pre-   dictions and the modified target vectors y , y.   3 Experiments   In this section , we present a thorough empirical   evaluation on the effect of label smoothing on ad-   versarial robustness for two pre - trained transformer   models : BERT and its distilled variant , dBERT ,   which are the victim models . We attack the vic-   tim models using TF , BAE , and SemAttack . For   each attack , we present results on both the standard   models and the label - smoothed models on various   classification tasks : text classification and natural   language inference . For each dataset we evaluate   on a randomly sampled subset of the test set ( 1000   examples ) , as done in prior work ( Li et al . , 2021 ;   Jin et al . , 2019 ; Garg and Ramakrishnan , 2020 ) . We   evaluate on the following tasks , and other details   about the setting is in Appendix A.8 :   •Text Classification : We evaluate on movie re-   view classification using Movie Review ( MR )   ( Pang and Lee , 2005 ) and Stanford Sentiment   Treebank ( SST2 ) ( Socher et al . , 2013 ) ( both   binary classification ) , restaurant review clas-   sification : Yelp Review ( Zhang et al . , 2015a )   ( binary classification ) , and news category clas-   sification : AG News ( Zhang et al . , 2015b )   ( having the following four classes : World ,   Sports , Business , Sci / Tech ) .   •Natural Language Inference : We investigate   two datasets for this task : the Stanford Natu-   ral Language Inference Corpus ( SNLI ) ( Bow-   man et al . , 2015 ) and the Multi - Genre Natural   Language Inference corpus ( MNLI ) ( Williams   et al . , 2018 ) , both having three classes . For   MNLI , our work only evaluates performance658   on the matched genre test - set in the OOD set-   ting presented in subsection 3.2 .   3.1 In - domain Setting   In the in - domain setting ( iD ) , the pre - trained trans-   former models are fine - tuned on the train - set for   each task and evaluated on the corresponding test-   set . For each case , we report the clean accuracy , the   adversarial attack success rate ( percentage of mis-   classified examples after an attack ) and the average   confidence on successfully attacked examples ( on   which the model makes a wrong prediction).Ta-   ble 1 shows the performance of BERT and dBERT ,   with and without label - smoothing . We choose la-   bel smoothing factor α= 0.45for standard label-   smoothed models in our experiments .   We see that label - smoothed models are more ro-   bust for every adversarial attack across different   datasets in terms of the attack success rate , which   is a standard metric in this area ( Li et al . , 2021 ;   Lee et al . , 2022 ) . Additionally , the higher confi-   dence of the standard models on the successfully   attacked examples indicates that label smoothing   helps mitigate overconfident mistakes in the adver-   sarial setting . Importantly , the clean accuracy re-   mains almost unchanged in all the cases . Moreover ,   we observe that the models gain much more robust-   ness from LS under white - box attack , comparedto the black - box setting . We perform hyperparam-   eter sweeping for the label smoothing factor αto   investigate their impact to model accuracy and ad-   versarial robustness . Figure 1 shows that the attack   success rate gets lower as we increase the label   smooth factor when fine - tuning the model while   the test accuracy is comparable . However , when   the label smoothing factor is larger than 0.45 , there   is no further improvement on adversarial robust-   ness in terms of attack success rate . Automatic   search for an optimal label smoothing factor and   its theoretical analysis is important future work .   We also investigate the impact of adversarial la-   bel smoothing ( ALS ) and show that the adversarial   label smoothed methods also improves model ’s ro-   bustness in Table 2.659   3.2 Out - of - Domain setting   We now evaluate the benefits of label smoothing   for robustness in the out - of - domain ( OOD ) setting ,   where the pre - trained model is fine - tuned on a par-   ticular dataset and is then evaluated directly on a   different dataset , which has a matching label space .   Three examples of these that we evaluate on are   the Movie Reviews to SST-2 transfer , the SST-2 to   Yelp transfer , and the SNLI to MNLI transfer .   In Table 3 , we again see that label - smoothinghelps produce more robust models in the OOD set-   ting although with less gain compared to iD setting .   This is a challenging setting , as evidenced by the   significant performance drop in the clean accuracy   as compared to the in - domain setting . We also see   that the standard models make over - confident er-   rors on successfully attacked adversarial examples ,   when compared to label - smoothed models .   3.3 Qualitative Results   In this section , we try to understand how the gener-   ated adversarial examples differ for label smoothed   and standard models . First we look at some qualita-   tive examples : in Table 4 , we show some examples   ( clean text ) for which the different attack schemes   fails to craft an attack for the label smoothed model   but successfully attacks the standard model .   We also performed automatic evaluation of the   quality of the adversarial examples for standard and   label smoothed models , adopting standard metrics   from previous studies ( Jin et al . , 2019 ; Li et al . ,   2021 ) . Ideally , we want the adversarial sentences to   be free of grammar errors , fluent , and semantically   similar to the clean text . This can be quantified   using metrics such as grammar errors , perplexity ,   and similarity scores ( compared to the clean text ) .   The reported scores for each metric are computed   over only the successful adversarial examples , for   each attack and model type.660Table 5 shows that the quality of generated adver-   sarial examples on label smoothed models is worse   than those on standard models for different metrics ,   suggesting that the adversarial sentences generated   by standard models are easier to perceive . This   further demonstrates that label smoothing makes it   harder to find adversarial vulnerabilities .   4 Conclusion   We presented an extensive empirical study to inves-   tigate the effect of label smoothing techniques on   adversarial robustness for various NLP tasks , for   various victim models and adversarial attacks . Our   results demonstrate that label smoothing imparts   implicit robustness to models , even under domain   shifts . This first work on the effects of LS for text   adversarial attacks , complemented with prior work   on LS and implicit calibration ( Desai and Durrett ,   2020 ; Dan and Roth , 2021 ) , is an important step   towards developing robust , reliable models . In the   future , it would be interesting to explore the combi-   nation of label smoothing with other regularization   and adversarial training techniques to further en-   hance the adversarial robustness of NLP models .   5 Limitations   One limitation of our work is that we focus on ro-   bustness of pre - trained transformer language mod-   els against word - level adversarial attacks , which   is the most common setting in this area . Future   work could extend this empirical study to other   types of attacks ( for example , character - level and   sentence - level attacks ) and for diverse types of ar-   chitectures . Further , it will be very interesting to   theoretically understand how label smoothing pro-   vides ( 1 ) the implicit robustness to text adversarial   attacks and ( 2 ) mitigates over - confident predictions   on the adversarially attacked examples .   6 Ethics Statement   Adversarial examples present a severe risk to ma-   chine learning systems , especially when deployed   in real - world risk sensitive applications . With the   ubiquity of textual information in real - world appli-   cations , it is extremely important to defend against   adversarial examples and also to understand the ro-   bustness properties of commonly used techniques   like Label Smoothing . From a societal perspective ,   by studying the effect of this popular regulariza-   tion strategy , this work empirically shows that it   helps robustness against adversarial examples inin - domain and out - of - domain scenarios , for both   white - box and black - box attacks across diverse   tasks and models . From an ecological perspec-   tive , label smoothing does not incur any additional   computational cost over standard fine - tuning em-   phasizing its efficacy as a general - purpose tool to   improve calibration and robustness .   Acknowledgements   Research was sponsored by the Army Research   Office and was accomplished under Grant Number   W911NF-20 - 1 - 0080 . This work was supported by   Contract FA8750 - 19 - 2 - 0201 with the US Defense   Advanced Research Projects Agency ( DARPA ) .   The views expressed are those of the authors and   do not reflect the official policy or position of the   Department of Defense , the Army Research Office   or the U.S. Government . This research was also   supported by a gift from AWS AI for research in   Trustworthy AI .   References661662A Appendix   •A.1Pictorial Overview of the Adversarial At-   tack Framework   •A.2Description of the Evaluation Metrics   •A.3Details of Automatic Attack Evaluation   •A.4 Additional results on Movie Review   Dataset   •A.5 Additional white - box attack on label-   smoothed models   •A.6Additional results for α= 0.1   •A.7Additional results on ALBERT model   •A.8Dataset overview and expertiment details   •A.9Attack success rate versus label smooth-   ing factors for different attacks ( TextFooler   and SemAttack )   •A.10 Average number of word change versus   Confidence   A.1 Overview of the Framework   A.2 Evaluation Metrics   The followings are the details of evaluation metrics   from previous works ( Lee et al . , 2022 ; Li et al . ,   2021 ):   Clean accuracy =   Attack Succ . Rate =   where successful adversarial examples are derived   from correctly predicted examples   Adv Conf = 663A.3 Attack evaluation   We performed automatic evaluation of adversarial   attacks against standard models and label smoothed   models following previous studies ( Jin et al . , 2019 ;   Li et al . , 2021 ) . Following are the details of the   metrics we used in Table 5 :   Perplexity evaluates the fluency of the input using   language models . We use GPT-2 ( Radford et al . ,   2019 ) to compute perplexity as in ( Li et al . , 2021 ) .   Similarity Score determines the similarity between   two sentences . We use Sentence Transformers   ( Reimers and Gurevych , 2019 ) to compute sen-   tence embeddings and then calculate cosine sim-   ilarity score between the clean examples and the   corresponding adversarially modified examples .   Grammar Error The average grammar error in-   crements between clean examples and the corre-   sponding adversarially modified example .   A.4 Additional results on Movie Review   Dataset   Here we provide results of movie review datasets   ( Pang and Lee , 2005 ) under in - domain setting .   A.5 Additional results on an additional   white - box attack   In this section , we use another recent popular white-   box attack named Gradient - based Attack ( Guo   et al . , 2021 ) . This is a gradient - based approach that   searches for a parameterized word - level adversar-   ial attack distribution , and then samples adversarial   examples from the distribution . We run this attack   on standard and label smoothed BERT models and   the results are listed below .   We observe that the label smoothing also help   with adversarial robustness against this attack   across four datasets under iD setting . The results   also show that , similar to SemAttack , the grad-   based attack benefits more from label smoothing   compared to black - box attacks like TextFooler and   BAE .   A.6 Additional results of α= 0.1   Table 8 and 9 are the additional results to show   when label smoothing α= 0.1 , how the adversarial   robustness of fine - tuned language models changes   under iD and OOD scenarios .   Table 10 are the additional results for adversarial   label smoothing α= 0.1 .   A.7 Additional results on ALBERT   In this section , we include experiment results for   standard ALBERT and label smoothed ALBERT   in Table 11 . We observe that the label smoothing   technique also improves adversarial robustness of   ALBERT model across different datasets.664   A.8 Dataset Overview and Experiments   Details   We use Huggingface ( Wolf et al . , 2020 ) to load the   dataset and to fine - tune the pre - trained models . All   models are fine - tuned for 3 epochs using AdamW   optimizer ( Loshchilov and Hutter , 2017 ) and the   learning rate starts from 5e−6 . The training and   attacking are run on an NVIDIA Quadro RTX 6000   GPU ( 24 GB ) . For both BAE and Textfooler attack ,   we use the implementation in TextAttack ( Morris   et al . , 2020 ) with the default hyper - parameters ( Ex-   cept for AG_news , we relax the similarity threshld   from 0.93 to 0.7 when using BAE attack ) . The Se-   mAttack is implemented by ( Wang et al . , 2022 )   while the generating contextualized embedding   space is modified from ( Reif et al . , 2019 ) . The re-   ported numbers are the average performance over 3   random runs of the experiment for iD setting , and   the standard deviation is less than 2 % .   A.9 Attack success rate versus label   smoothing factors   As mentioned in Section 3.1 , we plot the attack suc-   cess rate of BAE attack versus the label smoothing   factors . Here , we plot the results for the TextFooler   and SemAttack in Figure 3 and 4 , and observe the   same tendency as we discussed above .   We also plot the attack success rate of665   BAE / TextFooler attack versus the adversarial la-   bel smoothing factors in Figure 5 and 6 .   We additionally plot the clean accuracy versus   the label smoothing factor in Figure 7 , and find out   that there is not much drop in clean accuracy with   increasing the label smoothing factors .   A.10 Average number of word change versus   Confidence   Word change rate is defined as the ratio between   the number of word replaced after attack and the   total number of words in the sentence . Here we plot   the bucket - wise word change ratio of adversarial   attack versus confidence , and observe that the word   change rate for high - confident examples are higher   for label smoothed models compared to standard   models in most cases . This indicates that it is more   difficult to attack label smoothed text classification   models . Also note that there is the word change   rate is zero because there is no clean texts fall into   those two bins .   Moreover , we bucket the examples based on666   the confidence scores , and plot the bucket - wise   attack success rate ( of the BAE attack on the Yelp   dataset ) versus confidence in Figure 10 and Figure   11 . We observe that the label smoothing technique   improves the adversarial robustness for high confi-   dence score samples significantly . In future work ,   we plan to investigate the variations of robustness in   label - smoothed models as a function of the model   size.667ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 5 .   /squareA2 . Did you discuss any potential risks of your work ?   Section 6 .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Yes . Abstract and section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3 , and appendix A.8 .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3 and appendix A.8 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 3 .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Appendix A.8   C / squareDid you run computational experiments ?   Section 3 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A.8668 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 3.1 and Appendix A.8 .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 3 and Appendix A.8 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3 and Appendix A.3 , A.8 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.669