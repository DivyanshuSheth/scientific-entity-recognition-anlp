  Xin Cheng , Shen Gao , Lemao Liu , Dongyan Zhao , Rui YanWangxuan Institute of Computer Technology , Peking UniversityCenter for Data Science , Peking UniversityTencent AI LabSchool of Computer Science and Technology , Shandong UniversityState Key Laboratory of Media Convergence Production Technology and SystemsBeijing Institute of General Artificial Intelligence ( BIGAI)Gaoling School of Artificial Intelligence , Renmin University of China   chengxin1998@stu.pku.edu.cn shengao@sdu.edu.cn   zhaody@pku.edu.cn redmondliu@tencent.com ruiyan@ruc.edu.cn   Abstract   Retrieval - augmented Neural Machine Trans-   lation models have been successful in many   translation scenarios . Different from previ-   ous works that make use of mutually similar   but redundant translation memories ( TMs ) , we   propose a new retrieval - augmented NMT to   model contrastively retrieved translation mem-   ories that are holistically similar to the source   sentence while individually contrastive to each   other providing maximal information gains in   three phases . First , in TM retrieval phase , we   adopt a contrastive retrieval algorithm to avoid   redundancy and uninformativeness of similar   translation pieces . Second , in memory encod-   ing stage , given a set of TMs we propose a   novel Hierarchical Group Attention module to   gather both local context of each TM and global   context of the whole TM set . Finally , in train-   ing phase , a Multi - TM contrastive learning ob-   jective is introduced to learn salient feature of   each TM with respect to target sentence . Ex-   perimental results show that our framework ob-   tains improvements over strong baselines on   the benchmark datasets .   1 Introduction   Translation memory ( TM ) is basically a database   of segmented and paired source and target texts   that translators can access in order to re - use previ-   ous translations while translating new texts ( Chris-   tensen and Schjoldager , 2010 ) . For human trans-   lators , such similar translation pieces can lead   to higher productivity and consistency ( Yamada ,   2011 ) . For machine translation , early works mainly   contributes to employ TM for statistical machine   translation ( SMT ) systems ( Simard and Isabelle ,   2009 ; Utiyama et al . , 2011 ; Liu et al . , 2012 , 2019 ) .   Recently , as neural machine translation ( NMT )   model ( Sutskever et al . , 2014 ; Vaswani et al . , 2017 )   has achieved impressive performance in manyFigure 1 : An example of Greedy Retrieval andCon-   trastive Retrieval . The similarity score is computed by   edit distance detailed in Section 3.1 . And the target side   of TM is omitted for brevity .   translation tasks , there is also an emerging inter-   est ( Gu et al . , 2018 ) in retrieval - augmented NMT   model .   The key idea of retrieval - augmented NMT   mainly includes two steps : a retrieval metric is used   to retrieve similar translation pairs ( i.e. , TM ) , and   the TM is then integrated into an NMT model . In   the first step , a standard retrieval method greedily   chooses the most similar translation memory one   by one solely based on similarity with the source   sentence ( namely Greedy Retrieval ) . This method   would inevitably retrieve translation memories that   are mutually similar but redundant and uninforma-   tive as shown in Figure 1 . Intuitively , it is promis-   ing to retrieve a diverse translation memory which   would offer maximal coverage of the source sen-   tence and provide useful cues from different as-   pects . Unfortunately , empirical experiments in Gu   et al . ( 2018 ) show that a diverse translation mem-   ory only leads to negligible improvements . As a3591result , greedy retrieval is adopted in almost all later   studies ( Cao and Xiong , 2018 ; Xia et al . , 2019 ;   Xu et al . , 2020 ; He et al . , 2021 ; Cai et al . , 2021 ;   Khandelwal et al . , 2020 ) .   This paper aims to ask an important question   whether diverse translation memories are bene-   ficial for retrieval - augmented NMT . To this end ,   we propose a powerful retrieval - augmented NMT   model called Contrastive Memory Model which   takes into account diversity in translation mem-   ory from three ways . Specifically , ( 1 ) during   TM retrieval , inspired by Maximal Marginal Rel-   evance ( MMR ) ( Carbonell and Goldstein , 1998 ) ,   we introduce a conceptually simple while empir-   ically useful retrieval method called Contrastive   Retrieval to find informative translation memories .   The core is to retrieve a cluster of translation mem-   ories that are similar to the source sentence while   contrastive to each other keeping inner - cluster uni-   formity in the latent semantic space , as shown in   Figure 1 . ( 2 ) In TM encoding , given multiple trans-   lation memories , the local and global information   should both be captured by the translation model .   Separately encoding ( Gu et al . , 2018 ; He et al . ,   2021 ; Cai et al . , 2021 ) or treating them as a long   sequence ( Xu et al . , 2020 ) would inevitably lose   such hierarchical structure information . Thus , to   facilitate the direct communication between dif-   ferent translation memories for local information   and gather the global context via message passing   mechanism , we propose a Hierarchical Group At-   tention ( HGA ) module to encode the diverse memo-   ries . ( 3 ) In the model training phase , to learn salient   and distinct features of each TM with respect to   target sentence , we devise a novel Multi - TM Con-   trastive Learning objective ( MTCL ) , which further   contributes to a uniformly distributed translation   memory cluster by forcing representation of every   translation memory to approach the sentence to be   translated while keep away from each other .   To verify the effectiveness of our framework , we   conduct extensive experiments on four benchmark   datasets , and observe substantial improvement over   strong baselines , proving that diverse translation   memories is indeed useful to NMT . Our main con-   tributions are :   •We answer an important question about   retrieval - augmented NMT , i.e. , is diverse   translation memory beneficial for retrieval-   augmented NMT ?   •We propose a diverse - TM - aware frameworkto improve a retrieval - augmented NMT sys-   tem from three ways including TM retrieval ,   TM encoding and model training .   •We conduct extensive experiments on four   translation directions , observing substantial   performance gains over strong baselines with   greedy retrieval .   2 Related Work   TM - augmented NMT Augmenting neural ma-   chine translation model with translation memories   is an important line of work to boost the perfor-   mance of the NMT model with non - parametric   method . Feng et al . ( 2017 ) stores memories of   infrequently encountered words and utilizes them   to assist the neural model . Gu et al . ( 2018 ) uses an   external memory network and a gating mechanism   to incorporate TM . Cao and Xiong ( 2018 ) uses   an extra GRU - based memory encoder to provide   additional information to the decoder . Xia et al .   ( 2019 ) adopts a compact graph representation of   TM and perform additional attention mechanisms   over the graph when decoding . Bulté and Tezcan   ( 2019 ) and Xu et al . ( 2020 ) directly concatenate   TM with source sentence using cross - lingual   vocabulary . Zhang et al . ( 2018 ) augments the   model with an additional bonus given to outputs   that contain the collected translation pieces . There   is also a line of work that trains a parametric   retrieval model and a translation model jointly ( Cai   et al . , 2021 ) and achieves impressive results .   Recently , with rapid growth of computational   power , a more fine grained token level translation   memories are use in Khandelwal et al . ( 2020 ) . This   approach gives the decoder direct access to billions   of examples at test time , achieving state - of - the - art   result even without further training .   Contrastive Learning The key of contrastive learn-   ing ( Hadsell et al . , 2006 ; Mikolov et al . , 2013 )   is to learn effective representation by pulling se-   mantically close neighbors together and pushing   apart non - neighbors . Chen et al . ( 2020 ) and He   et al . ( 2020 ) show that contrastive learning can   boost the performance of self - supervised and semi-   supervised learning in computer vision tasks . In   natural language processing , Word2Vec ( Mikolov   et al . , 2013 ) uses noise - contrastive estimation to   learn better word representation . Gao et al . ( 2021 )   adopts contrastive learning with a simple token   level dropout to greatly advance the state - of - the - art3592sentence embeddings . Liu and Liu ( 2021 ) uses con-   trastive loss to post - rank generated summaries and   achieves promising results in benchmark datasets .   Lee et al . ( 2020 ) and Pan et al . ( 2021 ) also use con-   trastive learning in translation tasks and observe   consistent improvements .   3 Proposed Framework   Preliminary Assuming we are given a source   sentence x={x , ... , x}and its correspond-   ing target sentence y={y , ... , y}where s , t   are their respective length . For a TM - augmented   NMT model , a set of similar translation pairs   M={(x , y)}are retrieved based on cer-   tain criterion Cand NMT models the conditional   probability of target sentence yconditioned on both   source sentence xand translation memories Min   a left - to - right manner :   P(Y = y|X = x ) = /productdisplayP(y|y , ... y;x;M )   ( 1 )   Overview Given a source sentence xand infor-   mative translation memories M , the translation   model defines the conditional probability similar   to the Equation 1 . At the high level , our frame-   work , as shown in Figure 2 , consists of contrastive   retrieval , which searches a diverse translation mem-   ory , source encoder which transforms source sen-   tence xinto dense vector representations z , mem-   ory encoder with hierarchical group attention mod-   ule to jointly encode |M|translation memories   into a series dense representation zanddecoder   which attends to both zandzand generates tar-   get sentence yin an auto - regressive fashion , and   contrastive learning which effectively trains the   memory encoder as well as source encoder and de-   coder . Among all these five modules , contrastive   memory ( § 3.1 ) , memory encoder ( § 3.3 ) and con-   trastive learing ( § 3.5 ) are key in our framework   compared with existing work of TM - augmented   NMT .   3.1 Contrastive Retrieval   In this stage , following previous work ( Gu   et al . , 2018 ) we first employ an off - the - shelf   full - text search engine , Apache Lucene , to get   a preliminary translation memory set K=   { ( x , y)}(|K| ≫ |M|)for every source sen-   tence . Notice that both source sentence xandtranslation memory set Kare from training set   D={(x , y ) } , which means we do not in-   troduce any extra data during training . Then to be   directly comparable with previous works ( Gu et al . ,   2018 ; He et al . , 2021 ) as discussed in Section 5   and considering the core of our method is similar-   ity function - agnostic as detailed below , we adopt a   sentence - level similarity function :   sim(x , x ) = 1−D(x , x )   max(|x|,|x|)(2 )   where Dis the edit distance between two sen-   tences and |x|is the length of x. Specifically , we   would select |M|translation memories incremen-   tally and in every step we do not only measure the   similarity between current translation memory and   the source sentence but also take into considera-   tion the edit distance with those already retrieved   ones balanced by a hyperparameter α(namely con-   trastive factor ) . Different from MMR ( Carbonell   and Goldstein , 1998 ) , we treat retrieved translation   memories as a whole and take the average similar-   ity score as a penalty term :   arg max[sim(x , x)−α   |M|/summationdisplaysim(x , x ) ]   ( 3 )   where Mis the post - ranked translation memory   set . Finally , for every source sentence x , by ig-   noring the source side of Mdue to information   redundancy we have translation memories M=   { y } .   3.2 Source Encoder   For a source sentence x={x , ... , x } , our   source encoder is built upon the standard Trans-   former ( Vaswani et al . , 2017 ) architecture com-   posed of a token embedding layer , a sinusoidal po-   sitional embedding Layer and stacked transformer   encoder layers . Specifically we prepend a < bos >   token to source sentence and get the dense vector   representation zas follows :   z = SrcEnc ( < bos > , x , ... x ) ( 4 )   3.3 Memory Encoder   Given a set of translation memories , the local con-   text of each TM and the global context of the whole   TM set should be captured by the model to fully   utilize this hierarchical structure information . Sepa-   rately encoding ( Gu et al . , 2018 ; Cai et al . , 2021 ) or   treating them as a long sequence ( Xu et al . , 2020)3593   would inevitably mask the model with this kind   of local and global schema . In this section , to   facilitate the direct communication between dif-   ferent translation memories for local information   and gather the global context via message passing   mechanism , we propose a Hierarchical Group At-   tention ( HGA ) module . Formally , given a cluster   of translation memories M={y } , where   eachy={y , ... , y}is composed of nto-   kens , for each ywe would like to create a fully   connected graph G= ( V , E)where Vis   the token set . To facilitate inter - memory commu-   nication , we also create a super node vby con-   necting it with all other nodes ( namely trivial node )   in that graph and then connect all super nodes to-   gether contributing to information flow among dif-   ferent translation memories in a hierarchical way   as shown in Figure 2 . Then we adopt multi - head   self attention mechanism ( Vaswani et al . , 2017 ) as   message passing operator ( Gilmer et al . , 2017 ) . For   every node vin the graph , their hidden state in   time step t+ 1is updated by the hidden states of   its neighbours ϕ(v)in time step t :   v|=SelfAttn ( ϕ(v| ) , v| ) ( 5)To be computationally efficient , we use mask mech-   anism to block communication between nodes in   different graphs . For each trivial node vinG ,   they update their hidden states by attending to all   trivial nodes as well as super node v. For v ,   it does not only exchange information within the   graph G , but also communicate with all other su-   per nodes { v } . To stabilize training , we also   add residual connection and feed - forward Layer   after HGA module . After stacking multiple layers ,   we get dense representation of translation memo-   ries :   z = MemEnc ( Concate { y } ) ( 6 )   where |m|is the total length of |M|translation   memories and z∈R.   3.4 Fusing TM in Decoding   To better incorporate the information from both   source sentence zand translation memories z ,   we introduce a multi - reference decoder architec-   ture . For a target sentence y , we get a hidden   representation h={h , ... , h}after token embed-   ding layer and masked self - attention layer , then we3594use a cross attention layer to fuse information from   source sentence :   ˆh = CrossAttn ( Add&Norm ( h ) , z , z ) ( 7 )   Then for translation memories , we employ another   cross attention layer :   h = CrossAttn ( Add&Norm ( ˆh ) , z , z)(8 )   After stacking multiple decoder layers , to further   exploit translation memories , we apply a copy mod-   ule ( See et al . , 2017 ; Gu et al . , 2016 ) using the at-   tention score from the second cross attention layer   in the last sub - layer of decoder as a probability of   directly copying the corresponding token from the   translation memory . Formally , with t−1previous   generated tokens and hidden state h , the decoder   computes t - th token probability as :   p(y| · ) = ( 1 −p)p(y ) + p / summationdisplayα 1   ( 9 )   where p = σ(MLP ( h , y , α⊗z)),αis   the attention score , ⊗is a Hadamard product and   1is the indicator function .   3.5 Multi - TM Contrastive Learning   The key of contrastive learning is to learn ef-   fective representation by pulling semantically   close neighbors together and pushing apart non-   neighbors ( Hadsell et al . , 2006 ; Mikolov et al . ,   2013 ) . As indicated in ( Lee et al . , 2020 ) , sim-   ply choosing in - batch negatives would yield mean-   ingless negative examples that are already well-   discriminated in the embedding space and would   even cause performance degradation in translation   task ( Lee et al . , 2020 ) , which also holds true in   our preliminary experiments . So how to devise   effective contrastive learning objective for a trans-   lation model with a cluster of translation memories   to learn salient features with respect to the target   sentence remains unexplored and challenging .   In this work , to make every translation mem-   ory learn distinct and useful feature representa-   tions with respect to current target sentence , we   propose a novel Multi - TM Contrastive Learning   ( MTCL ) objective which do not simply treat in-   batch samples as negative but instead keep aligned   with the principle of our contrastive retrieval , mak-   ing every translation memory approach the ground   truth translation while pushing apart from eachother . Formally , given a source sentence x , its cor-   responding target sentence yand translation mem-   ories M={y } . The goal of MTCL is to   minimize the following loss :   L = −/summationdisplayloge   /summationtexte(10 )   where sim(y , y)is the cosine similarity between   the representation of target sentence yand transla-   tion memory ygiven by memory encoder andτis   a temperature hyperparameter which controls the   difficulties of distinguishing between positive and   negative samples ( Pan et al . , 2021 ) . Notice that the   representation of each translation memory is the su-   per node vgiven by HGA module in Section 3.3 ,   which communicates with both intra - memory and   inter - memory nodes . Intuitively , by maximizing   the softmax term e , the contrastive loss   would force the representation of each translation   memories to approach the ground truth while push   apart from each other , delivering a uniformly dis-   tributed representation around the target sentence   in latent semantic space . In MTCL , all negative   samples are not from in - batch data but are differ-   ent translation memories for one source sentence ,   which make up of non - trivial negative samples and   help the model to learn the subtle difference be-   tween multiple translation memories .   During the training phase , the model can be op-   timized by jointly minimizing the MTCL loss and   Cross Entropy loss as shown :   L = L+λL ( 11 )   where λis a balancing coefficient to measure the   importance of different objectives in a multi - task   learning scenario ( Sener and Koltun , 2018 ) .   4 Experimental Setup   4.1 Dataset and Evaluation   We use the JRC - Acquis ( Steinberger et al . , 2006 )   corpus to evaluate our model . This corpus is a   collection of parallel legislative text of European   union Law applicable in the EU member states .   Highly related and well structured data make this   corpus an ideal test bed to evaluate the proposed   TM - augmented translation model . Following pre-   vious work , we use the same split of train / dev / test   set as in ( Gu et al . , 2018 ; Xia et al . , 2019 ; Cai   et al . , 2021 ; Xu et al . , 2020 ; He et al . , 2021 ) . For   evaluation , we use SacreBLEU .35954.2 Implementation Details   Our model is named Contrastive Memory   Model ( CMM ) . To implement CMM , we use trans-   former as building block of our model . Specifically ,   we adopt the base configuration and the default op-   timization configuration as in Vaswani et al . ( 2017 ) .   We use joint BPE encoding ( Sennrich et al . , 2016 )   with vocab size 35000 . We also adopt label smooth-   ing as 0.1 in all experiments . The number of to-   kens in every batch is 10000 , which includes both   source sentence and translation memories . The   memory size and contrastive factor is set to be 5   and 0.7 across all translation directions . The con-   trastive temperature τis{0.1,0.08,0.05,0.15}for   Es→En , En →Es , De →En and En →De directions .   The balancing factor λis set to be 1 .   4.3 Baselines   CMM is compared with the following baselines :   •Vaswani et al . ( 2017 ): this is the original imple-   mentation of base transformer .   •Gu et al . ( 2018 ): this is a pioneer work of in-   tegrating translation memories into NMT system   using an external memories networks to separately   encode every translation memory   •Xu et al . ( 2020 ): this paper augments source   sentence with concatenation of TM and euqip the   model with different language embedding ( FM ) .   •Xia et al . ( 2019 ): this work uses a compact graph   to encode translation memories and is also based   on transformer architecture .   •Zhang et al . ( 2018 ): this work equips a NMT   model with translation pieces and extra bonus given   to outputs that contain the collected translation   pieces .   •Cai et al . ( 2021 ): this model first retrieves transla-   tion memories by source side similarity and adopts   a dual encoder architecture .   •He et al . ( 2021 ): this model incorporates one   most similar translation memory with proposed   example layer .   In addition , considering that Gu et al . ( 2018 )   is based on Memory Network and RNN architec-   ture , to be fairly compared with transformer based   model , we re - implement two more direct baselines   ( i.e. , BaseGreedy andT - Ada ) on top of Trans-   former with the same configuration as our CMM .   Specifically , in both baselines the original Mem-   ory Network is replaced by a transformer encoder   sharing weights with source encoder . BaseGreedy   employs greedy retrieval and it does not take diver-   sity of TM into account . In contrast , T - Ada adopts   adaptive retrieval , which finds the translation mem-   ories via maximizing the token coverage of source   sentence , and it promotes the diversity in retrieved   memory to some extent as CMM .   5 Experiment Results   5.1 Main results   Is diverse translation memory helpful ? We   make a comparison with the direct baseline T - Ada   because both the proposed CMM and T - Ada pro-   mote the diversity in translation memory . As shown   in Table 1 , T - Ada yields modest gains ( about +0.2   BLEU points on average ) over BaseGreedy on four   translation tasks , which is in line with the results in   Gu et al . ( 2018 ) on the RNN architecture . We con-   jecture that it is because Adaptive Retrieval only   partially maximize the word coverage while ne-   glecting the overall semantics of the whole sen-   tence thus injecting undesirable noise into the re-   trieval phase . In contrast , the proposed CMM takes   both token - level coverage and sentence - level sim-   ilarity into consideration and consistently outper-   forms T - Ada , gaining about 0.5 - 1.4 BLEU points   on four tasks in translation quality with smaller   TM size and lower latency in both training and   inference phase . This fact shows the following   findings : 1 ) NMT augmented with diverse trans-   lation memory can yield consistent improvements   in translation quality ; 2 ) how to model and learn   the diverse translation memory is important in addi-3596SystemEs→En En →Es De →En En →De   Dev Test Dev Test Dev Test Dev Test   Vaswani et al . , 2017 †64.08 64.63 62.02 61.80 60.18 60.16 54.65 55.07   Gu et al . , 2018 57.62 57.27 60.28 59.34 55.63 55.33 49.26 48.80   Zhang et al . , 2018 63.97 64.30 61.50 61.56 60.10 60.26 55.54 55.14   Xu et al . , 2020 * 66.44 65.90 - - - - - -   Xia et al . , 2019 66.37 66.21 62.50 62.76 61.85 61.72 57.43 56.88   He et al . , 2021(@s ) 67.23 67.26 - - - - - -   Cai et al . , 2021(#2 ) 66.98 66.48 63.04 62.76 63.62 63.85 57.88 57.53   CMM 67.48 67.76 63.84 64.04 64.22 64.33 58.94 58.69   tion to promoting diversity in translation memory .   Because of the potential problem of high BLEU   test ( Callison - Burch et al . , 2006 ) , we conduct an-   other two experiments . First , We use metrics other   than BLEU to evaluate our high BLEU systems .   We compare our model CMM and BaseGreedy in   JRC / EsEn dataset . We use both model - free and   model - based metrics as shown in Table 4 . A clear   patent here is that our higher - BLEU model CMM   outperforms BaseGreedy model in all these met-   rics . Second , we disengage our model from high   BLEU range by picking the hard sentences from   the test set of JRC / EsEn according to the sentence-   level BLEU for a vanilla Transformer model . The   evaluation results for top-25 % , top-50 % , top-75 %   hardest subsets are shown in Table 5 . We can see   that the proposed CMM still outperforms baselines   on the top-25 % subset whose BLEU is in the range   of 30s .   Comparing with other baselines Since our   CMM involves the heuristic metric ( i.e. , TF - IDF   and normalized edit distance ) for retrieval , we first   compare our methods with other works using the   same retrieval metric . The result is presented in   Table 2 . As can be seen , our method yields con-   sistent better results than all other baseline mod-   els across four tasks in terms of BLEU . Substan-   tial improvement by an average 3.31 BLEU points   and up to 4.29 in En →De direction compared with   transformer baseline model demonstrates the effec-   tiveness of incorporating translation memories into   NMT model . In comparison with previous works   either using greedy retrieval ( Gu et al . , 2018 ; Zhang   et al . , 2018 ; Xia et al . , 2019 ; Cai et al . , 2021 ) , which   introduces redundant and uninformative translation   memories , or using top1 similar translation mem-   ory ( Xu et al . , 2020 ; He et al . , 2021 ) , which causes   omission of potentially useful cues , our framework   equipped with contrastive translation memories can   deliver consistent improvement in both develop-   ment set and test set among four translation direc-   tions .   Unlike the above work , there is also another   line of work that retrieve translation memory with   a learnable metric . Cai et al . ( 2021 ) proposes a3597   powerful framework ( namely T - Para ) which jointly   trains the retrieval metric and translation model in   an end - to - end fashion , leading to state - of - the - art   performance in translation quality . We also com-   pare our method with this strong model and result   is shown in Table 3 . Notice that our model gives   comparable results with T - Para , which is actually   remarkable considering that our model has much   smaller model size and training latency . In particu-   lar , our work about contrastive translation memory   is orthogonal to Cai et al . ( 2021 ) and it is promis-   ing to apply our idea into their framework , which   remains a future work .   5.2 Analysis   Ablation Study We also implement several   variants of our framework : ( 1 ) T w/o MTCL : this   model uses the same model configuration as CMM   but without MTCL loss . ( 2 ) T w/o HGA : in this   setting , |M|translation memories are concatenated   together as a long sequence without Hierarchical   Group Attention module . ( 3 ) T - Greedy : this model   replaces the Contrastive Retrieval in CMM by   Greedy Retrieval . ( 4 ) T - MMR : this model replaces   theContrastive Retrieval in CMM by Maximal   Marginal Relevance ( Carbonell and Goldstein ,   1998 ) while the setting of translation model keeps   the same as CMM . The result is shown in Table 6   and we have the following observations . Simply re-   placing Contrastive Retrieval byGreedy Retrieval   or MMR while keeping the setting of translation   model unchanged yields worse results than our   model which demonstrates that the informative   translation memories serve as key ingredient in a   TM - augmented NMT model . Interestingly , direct   removal of HGA module while maintaining MTCL   objective ( i.e. , T w/o HGA ) gives consistent worse   results in four translation directions . We suspect   that a pull - and - push game brought by contrastive   learning causes performance degradation without   modeling the fine - grained interaction among   multiple translation memories . Combining HGA   and MTCL , which facilitates communication   between different translation memories and helps   the model to learn the subtle difference between   them , performs better than all other baseline   models revealing the fact that properly designed   contrastive learning objective and HGA module is   complementary to each other .   Memory Size and Contrastive Factor To ver-   ify the effectiveness of fusing multiple contrastive   translation memories , we choose En →De dataset   and make the following experiments in both TM   retrieval and TM fusion stage : In retrieval stage , we   explore the contrastive factor αwhich is supposed   to decide the degree of currently retrieved transla-   tion memory contrasting to those already retrieved .   A larger αindicates that the retrieved translation   memories are less similar to the source sentence   while more contrastive to each other . And in fusion   stage , the size |M|of translation memories is con-   sidered . The effect of different αis shown in Figure   3 . The random point is the result of a NMT model   with|M|randomly retrieved translation memories   and it even underperforms a non - TM translation   model ( Vaswani et al . , 2017 ) shown in Table 2 . We3598assume it is due to much noise injected by random   memories . When contrastive factor αis set to 0 , it   is essentially greedy retrieval , and an important ob-   servation is that the translation quality of our model   increases with the αuntil it drops at some certain   point . We suspect that too large αwould yield mu-   tually contrastive TM that divert too much from the   original source sentence . Similar phenomenon can   be verified in the Figure 3 , when TM size equals to   0 , it is a non - TM translation model delivering worst   result while too large TM size also hurts the model   performance which is also observed in Bulté and   Tezcan ( 2019 ) ; Xia et al . ( 2019 ) .   To further demonstrate the intuition behind our   framework , we randomly sample 1,000 examples   from test sets of En →De and En →Es directions   and use t - SNE ( Van der Maaten and Hinton , 2008 )   to visualize the sentence embedding of translation   memories and target sentence encoded by our   CMM . The result is shown in Figure 4 and one   interesting observation is that although the target   side of testset is never exposed to the model ,   the representation of translation memories are   uniformly distributed around the target sentence in   the latent semantic space .   6 Conclusion   In this work , we introduce an approach to incorpo-   rate contrastive translation memories into a NMT   system . Our system demonstrates its superiority   in retrieval , memory encoding and training phases .   Experimental results on four translation datasets   verify the effectiveness of our framework . In the fu-   ture , we plan to exploit the potential of this general   idea in different retrieval - generation tasks .   7 Limitations   This paper propose a framework for Retrieval-   augmented Neural Machine Translation and it re-   lies on holistically similar but mutually contrastive   translation memories which makes it work mostly   for corpora in the same domain . How to apply this   general idea to other scenario like low resource   NMT remains a future challenge .   8 Acknowledgement   This work was supported by National Natural   Science Foundation of China ( NSFC Grant No .   62122089 and No . 61876196 ) . Rui Yan is sup-   ported by Tencent Collaborative Research Fund . References359936003601