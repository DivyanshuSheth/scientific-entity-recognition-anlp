  Wangchunshu Zhou , Canwen Xu , Julian McAuleyETH ZurichUniversity of California , San Diegowangchunshu.zhou@inf.ethz.ch,{cxu,jmcauley}@ucsd.edu   Abstract   Intermediate - task transfer can benefit a wide   range of NLP tasks with properly selected   source datasets . However , it is computation-   ally infeasible to experiment with all interme-   diate transfer combinations , making choosing   a useful source task a challenging problem . In   this paper , we anticipate that task - specific pa-   rameters updated in parameter - efficient tuning   methods are likely to encode task - specific in-   formation . Therefore , such parameters can be   predictive for inter - task transferability . Thus ,   we propose to exploit these efficiently tuned   parameters as off - the - shelf task embeddings   for the efficient selection of source datasets for   intermediate - task transfer . We experiment with   11 text classification tasks and 11 question an-   swering tasks . Experimental results show that   our approach can consistently outperform exist-   ing inter - task transferability prediction methods   while being conceptually simple and computa-   tionally efficient . Our analysis also reveals that   the ability of efficiently tuned parameters on   transferability prediction is disentangled with   their in - task performance . This allows us to   use parameters from early checkpoints as task   embeddings to further improve efficiency .   1 Introduction   Thepretraining then fine - tuning paradigm ( Peters   et al . , 2018 ; Devlin et al . , 2019 ; Radford et al . ,   2018 , 2019 ; Brown et al . , 2020 ; Lewis et al . , 2020 ;   Raffel et al . , 2019 ) has substantially improved the   state - of - the - art on a wide range of natural language   processing ( NLP ) tasks . In this paradigm , we first   pretrain a large language model on large - scale cor-   pora in a general domain , and then fine - tune the   pretrained model to be a task - specific model on   the target dataset . In addition to directly trans-   ferring from a general pretrained language model , Figure 1 : The workflow of using efficiently tuned param-   eters as task embeddings . The yellow boxes represent   tunable parameters in Transformer layers .   prior work ( Phang et al . , 2018 ) also shows that   intermediate - task transfer , i.e. , fine - tuning on in-   termediate source tasks before the target task , can   further improve target task performance . However ,   the success of intermediate - task transfer heavily   relies on the selection of a proper source dataset   while an inappropriate source dataset often leads   to performance degradation compared to plain fine-   tuning . Therefore , some recent works ( Vu et al . ,   2020 ; Poth et al . , 2021 ) investigate methods to ef-   ficiently predict inter - task transferability without   actually trying out all intermediate - task combina-   tions .   The current state of the art ( Vu et al . , 2020 )   on predicting inter - task transferability is built on   Task2Vec ( Achille et al . , 2019 ) , which considers   the Fisher information matrix of a model fine-   tuned on a task as the “ task embedding ” , and pre-   dicts inter - task transferability by computing the   cosine similarity between the task embedding of   the source and target tasks . Despite empirically per-   forming well , this approach requires fine - tuning the   full model and ( inefficiently ) computing the Fisher   matrix of the model . Moreover , the resulting task   embeddings generally have a high dimensionality   similar to the size of the underlying model . There-   fore , intermediate task selection , which requires   storing task embeddings for each source / target task ,   can be space - consuming , especially when experi-5007menting with large language models .   In this work , we opt for parameter - efficient tun-   ing approaches ( Houlsby et al . , 2019 ; Li and Liang ,   2021 ; Guo et al . , 2021 ; Hu et al . , 2022 ; Zaken   et al . , 2022 ) for the efficient and accurate pre-   diction of inter - task transferability . Our key in-   sight is that task - specific parameters updated in   parameter - efficient tuning methods are likely to en-   code high density task - specific information since   they are used as a query for retrieving task - related   knowledge in a frozen pretrained language model .   Therefore , we propose to directly use task - specific   parameters learned via parameter - efficient tuning   on source / target datasets as task embeddings , as   shown in Figure 1 . Compared to task embed-   dings obtained by calculating the Fisher matrix   of the fine - tuned model ( Achille et al . , 2019 ; Vu   et al . , 2020 ) , efficiently tuned parameters are of   much lower dimensionality and do not suffer from   noise from uninformative weights in the model   parameters , thus leading to more accurate trans-   ferability prediction . Also , our method only re-   quires parameter - efficient tuning on the tasks and   stores task - specific parameters , making both com-   puting and storing task embeddings more efficient .   Moreover , with the development of open - source   parameter - efficient tuning platforms like Adapter-   Hub ( Pfeiffer et al . , 2020 ) , we can easily access   off - the - shelf parameters of the source and target   datasets downloaded from the model zoo and then   compute the similarity between the downloaded   parameters .   We empirically verify the effectiveness of our   approach by experimenting with 11 text classifi-   cation tasks and 11 question answering tasks , fol-   lowing Vu et al . ( 2020 ) . Our results show that our   approach consistently outperforms existing inter-   task transferability prediction methods while being   simpler and more efficient . In addition , we find that   the ability of efficiently tuned parameters on trans-   ferability prediction is not strongly correlated with   their in - task performance . Therefore , task - specific   parameters tuned with a relatively small number   of steps are already highly predictive for inter - task   transferability , allowing us to further improve the   efficiency of intermediate task selection .   2 Related Work   Prior work ( Phang et al . , 2018 ) shows that posi-   tive transfer can be elicited by training a model   on intermediate source tasks before fine - tuning onthe target task . However , the choice of an appro-   priate source task is crucial for effective transfer .   Phang et al . ( 2018 ) show that the size of the source   dataset is an good prior for source task selection .   Pruksachatkun et al . ( 2020 ) propose to use task re-   quiring complex reasoning and inference as source   tasks . Besides these heuristics , a number of work   also focuses on systematic prediction of interme-   diate task transferability . Vu et al . ( 2020 ) propose   to used T2Vto construct task embeddings   based on the input text or Fisher information ma-   trix of a fine - tuned model . Poth et al . ( 2021 ) fur-   ther extend similar ideas for adapter - based trans-   fer learning . More recently , Vu et al . ( 2021 ) ex-   plore prompt - based transfer and propose to use   prompt similarity as a predictor for prompt trans-   ferability to select proper soft prompts for initial-   ization . This can be viewed as a special case of   our proposed method where the parameter - efficient   tuning method is restricted to vanilla prompt tun-   ing ( Lester et al . , 2021 ) and the transfer method   is restricted to prompt transfer instead of general   intermediate - task transfer .   3 Methodology   3.1 Parameter - Efficient Tuning   Parameter - efficient tuning only updates a small   portion of parameters in a large pretrained model .   In this paper , we experiment with three types of   parameter - efficient tuning : Prompt Tuning ( Liu   et al . , 2021 ) , Bias Tuning ( Zaken et al . , 2022 ) , and   Low - Rank Tuning ( Hu et al . , 2022 ) .   Prompt Tuning We experiment with P - Tuning   v2 ( Liu et al . , 2021 ) . Specifically , P - Tuning v2   implements a prompt tuning method by introduc-   ing additional attention prefix matrices K=   { k. . .k}andV={v . . .v}for each Trans-   former layer , where nis a hyperparameter control-   ling the added prefix length ; kandvare vectors   with dimension d;dis the hidden size of the   Transformer model .   For each Transformer layer , the added vectors   are concatenated with the original key and value   matrices to be K = K⊕KandV = V⊕V ,   where KandVare the original key and value in   each layer ’s attention block . Then , the new scaled   dot - product attention is calculated by replacing the   original KandVwith the new KandV , respec-   tively.5008Bias Tuning BitFit ( Zaken et al . , 2022 ) simply   updates all bias terms bin all linear layers h=   Wx+bin each Transformer layer .   Low - Rank Tuning LoRA ( Hu et al . , 2022 ) in-   jects trainable rank decomposition matrices into   each layer of the Transformer model . For each   linear layer h = Wx where W∈R , the   forward pass is modified to h = Wx+BAx ,   where B∈R , A∈R , and the rank   r≪min(d , k ) .   3.2 Tuned Parameters as Task Embeddings   After parameter - efficient tuning , we concatenate all   tuned parameters in each Transformer layer and av-   erage them across all layers to obtain a vector as a   representation for a task , namely TunedParameters   asTaskEmbedding ( TuPaTE ) . Following Vu et al .   ( 2020 ) , we calculate the cosine similarity between   the embeddings of a given targeted task and the can-   didate source tasks . Then , we rank the candidate   source tasks in descending order by the similarity   scores .   4 Experiments   4.1 Datasets   Following Vu et al . ( 2020 ) , we conduct experiments   with 11 tasks of text classification or regression   ( CR ) and 11 tasks of question answering ( QA ) .   Note that Vu et al . ( 2020 ) also includes 11 tasks of   sequence labeling . We do not include those datasets   since most of them are not publicly available . The   list of datasets can be found in Appendix A. To   be consistent with Vu et al . ( 2020 ) , we use two   metrics to evaluate the performance of the task   embeddings : ( 1 ) the average rank ρof the source   task with the highest absolute transfer gain ; ( 2 )   Normalized Discounted Cumulative Gain ( NDCG ) ,   which is a widely used metric for evaluating the   quality of the entire ranking , instead of focusing on   the highest rank as ρdoes .   4.2 Baselines   We use the following methods as baselines : ( 1 )   DS(Vu et al . , 2020 ) is a simple baseline   that ranks all source tasks by the number of training   examples . ( 2 ) C G ( Bingel and Søgaard ,   2017 ; Vu et al . , 2020 ) is a baseline that uses the   gradients of the loss curve of BERT for each task .   It is originally proposed in Bingel and Søgaard   ( 2017 ) for predicting gains from multi - task learn-   ing and adapted by Vu et al . ( 2020 ) for predictingMethod#Tuned Embedding   Param . Dim .   TE 110 M 110 M   PT 184 K 15.4 K   LRA 300 K 25.0 K   BF 100 K 8.3 K   transferability . ( 3 ) TE(Vu et al . , 2020 )   averages sentence representations over the entire   dataset . The sentence representation is obtained   by averaging the hidden states in the last layer of   BERT . ( 4 ) TE(Vu et al . , 2020 ) represents   tasks based on the Fisher information matrix . It is   adapted from the task embedding originally pro-   posed in Achille et al . ( 2019 ) for meta - learning .   4.3 Training Details   We apply P - Tuning v2 , BitFit , and LoRA on BERT-   base for fine - tuning on the aforementioned datasets .   For each method , we adopt the default hyperparam-   eters from their corresponding papers . Specifically ,   for P - Tuning v2 , we use a prefix length of 20 and   search the learning rate from { 1e-2 , 1e-3 } ; For   LoRA , we set LoRA ’s rto 8 and αto 8 , and search   a learning rate from { 5e-4 , 2e-4 } ; For BitFit , we   search a learning rate from { 1e-4 , 4e-4 } . We train   all models with a batch size of 32 for 20 epochs   on all datasets . We use the parameters tuned for 2   epochs as “ early ” task embeddings and those corre-   sponding to the best validation set performance as   “ late ” task embeddings . We compare the number   of tunable parameters and the final task embedding   dimensions in Table 1 . We can see that TuPaTE   has a significantly lower dimensionality compare to   theTEbaseline . We also include an ensem-   ble of the three efficient tuning methods ( denoted   as “ 3 E ” ) , by averaging the inter - task   similarity scores of each model .   4.4 Experimental Results   We present the main results in Table 2 . We find that   TuPaTE with different parameter - efficient tuning   methods consistently outperforms prior works in-   cluding TEandTE . Interestingly ,   the performance improvement is larger in F→   L andL →L settings . We   conjecture that this is because in limited resource5009   settings , parameter - efficient tuning methods gen-   erally perform much better than full fine - tuning ,   which is used in the TEmethod . More-   over , we find that PT andBFoutper-   form LRAin all settings . We suspect this is be-   cause the amount of tunable parameters in LRA   is much larger than PT andBF . Also ,   the ensemble of three methods achieve even better   performance than only using one approach .   4.5 Analysis   We conduct additional experiments in the in - class   setting on classification / regression tasks to bet-   ter understand how TuPaTE works . We first an-   alyze the correlation between the in - task perfor-   mance ( e.g. , accuracy ) and transferability predic-   tion ability of efficiently tuned parameters . We   train TuPaTE with 5 random combinations between   searchable hyperparameters and random seeds , and   present the correlation in Table 3 . We observe that   there is only a weak correlation between in - task   performance and transferability prediction results ,   indicating that the ability of efficiently tuned pa-   rameters to encode task - related information is dis-   entangled with their final in - task performance . This   also shows the robustness of TuPaTE with respect   to hyperparameters .   The fact that in - task performance only corre-   lates weakly with transferability prediction moti-   vates us to explore whether early checkpoints of   efficiently tuned parameters can be used for trans-   ferability prediction . From Table 4 , we find that5010early checkpoints are also effective task embed-   dings . This allows us to reduce the computation   cost by around 90 % while substantially outperform-   ing the TEbaseline .   5 Conclusion   In this paper , we show that efficiently tuned parame-   ters are highly predictive for inter - task transferabil-   ity and thus can be used as off - the - shelf task em-   beddings for source task selection in intermediate-   task transfer learning . Our empirical investigation   with three parameter - efficient tuning methods on   22 NLP tasks demonstrates that our approach out-   performs prior works on inter - task transferability   prediction despite being more efficient .   Limitations   We select three representative works for three types   of parameter - efficient tuning . However , there are   other parameter - efficient tuning methods that we   have not investigated . Although we believe our   conclusion can generalize to other methods , we   will conduct more experiments to confirm for future   work .   Ethics Statement   We propose to use efficiently tuned parameters   as task embedding , only for predicting the perfor-   mance of intermediate transfer learning . Thus , we   do not anticipate any major ethical concern .   Acknowledgements   We would like to thank the anonymous reviewers   for their insightful comments . This project is partly   supported by NSF Award # 1750063 .   References501150125013A List of Datasets   Task |Train|   Text classification / Regression ( CR )   SNLI ( Bowman et al . , 2015 ) 570k   MNLI ( Williams et al . , 2018 ) 393k   QQP ( Iyer et al . , 2017 ) 364k   QNLI ( Wang et al . , 2019 ) 105k   SST-2 ( Socher et al . , 2013 ) 67k   SciTail ( Khot et al . , 2018 ) 27k   CoLA ( Warstadt et al . , 2019 ) 8.5k   STS - B ( Cer et al . , 2017 ) 7k   MRPC ( Dolan and Brockett , 2005 ) 3.7k   RTE ( Dagan et al . , 2005 ) 2.5k   WNLI ( Levesque , 2011 ) 634   Question Answering ( QA )   SQuAD-2 ( Rajpurkar et al . , 2018 ) 162k   NewsQA ( Trischler et al . , 2017 ) 120k   HotpotQA ( Yang et al . , 2018 ) 113k   SQuAD-1 ( Rajpurkar et al . , 2016 ) 108k   DuoRC - p ( Saha et al . , 2018 ) 100k   DuoRC - s ( Saha et al . , 2018 ) 86k   DROP ( Dua et al . , 2019 ) 77k   WikiHop ( Welbl et al . , 2018 ) 51k   BoolQ ( Clark et al . , 2019 ) 16k   ComQA ( Abujabal et al . , 2019 ) 11k   CQ ( Bao et al . , 2016 ) 2k5014