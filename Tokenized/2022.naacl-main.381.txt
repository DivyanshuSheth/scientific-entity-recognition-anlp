  Yifan Chen , Qi Zeng , Dilek Hakkani - Tur , Di Jin , Heng Ji , Yun YangUniversity of Illinois Urbana - ChampaignAmazon Alexa AI{yifanc10 , qizeng2 , hengji , yy84}@illinois.edu{hakkanit , djinamzn}@amazon.com   Abstract   Transformer - based models are not efficient in   processing long sequences due to the quadratic   space and time complexity of the self - attention   modules . To address this limitation , Linformer   and Informer reduce the quadratic complex-   ity to linear ( modulo logarithmic factors ) via   low - dimensional projection and row selection ,   respectively . These two models are intrinsically   connected , and to understand their connection   we introduce a theoretical framework of ma-   trix sketching . Based on the theoretical analy-   sis , we propose Skeinformer to accelerate self-   attention and further improve the accuracy of   matrix approximation to self - attention with col-   umn sampling , adaptive row normalization and   pilot sampling reutilization . Experiments on   the Long Range Arena benchmark demonstrate   that our methods outperform alternatives with   a consistently smaller time / space footprint .   1 Introduction   Transformer ( Vaswani et al . , 2017 ) utilizes softmax   self - attention modules to capture the dependency   between tokens in a sequence and has been widely   used in various Natural Language Processing tasks .   The time and space complexity of the dot - product   self - attention is quadratic in the input sequence   length , which restricts the largest sequence length   and batch size . To adapt transformers to long se-   quences , documents have to be truncated , chunked   using a sliding window , or processed in parallel on   multiple GPUs . These additional operations usu-   ally cause the loss of long - range dependency and   introduce additional computational costs .   In this paper , we focus on efficient self - attention   methods ( Xiong et al . , 2021 ; Qiu et al . , 2020 ;   Zaheer et al . , 2020 ; Beltagy et al . , 2020 ; Kitaevet al . , 2020a ; Roy et al . , 2021 ) , among which Lin-   former ( Wang et al . , 2020b ) and Informer ( Zhou   et al . , 2020 ) are two representative approaches to   reducing the O(n)self - attention to an /tildewideO(n)oper-   ation ( /tildewideO(·)means O(·)modulo poly - log terms and   nis the sequence length ) in both space and time   complexity . Linformer forms a low - rank factoriza-   tion of the original attention by decomposing it into   smaller attentions , while Informer allows each key   to only attend to a certain number of queries .   To better understand self - attention , we intro-   duce a theoretical framework , sketching ( Woodruff ,   2014 ) , to help explain the key ideas in Informer and   Linformer from the perspective of matrix approxi-   mation . Specifically , sketching methods replace the   original matrix Bwith its random sketch BS to   reduce computations . In Section 3.3 we introduce   some concrete instances of commonly used distri-   butions for constructing the random sketching ma-   trixS. Furthermore , taking matrix approximation   as a general guideline , we recognize the deficiency   in Informer and Linformer , that they either do not   fully utilize the information in the value matrix V ,   or deviate from the original self - attention output .   This guideline also motivates us to propose Skein-   former through the theoretical analysis under the   sketching framework .   To improve the approximation accuracy in terms   of the original attention output , Skeinformer ap-   plies sub - sampling sketching to reduce time com-   plexity and exploits the information from the value   matrix Vwith column sampling . Skeinformer   also incorporates an adaptive row normalization   step , which approximates the un - selected rows by   a vector with all elementsand has significantly   boosted the performance of Informer . In addition ,   we introduce a simple yet effective step , pilot sam-   pling reutilization , which reuses the computation   from pilot sampling to improve both approximation   accuracy and training efficiency . Our experiments   on the LRA benchmark show that Skeinformer con-5187sistently uses less space and time while achieving   better accuracy than most baseline methods .   In summary , our contributions are twofold :   •We introduce sketching as a theoretical frame-   work for analyzing and developing efficient   transformers . Specifically , the randomized   sketching theory covers these two methods   from the perspective of approximate matrix   multiplication . This framework connects the   studies on efficient transformers and random-   ized sketching theory , so that future develop-   ment in efficient transformers and sketching   can benefit each other .   •We propose Skeinformer as a straightforward   product of the sketching framework to acceler-   ate the training and inference of transformers .   Skeinformer consists of three components :   the initial column sampling that incorporates   the information from the value matrix Vinto   the sampling probabilities , the adaptive row   normalization that fills un - selected columns   with the averaged selected columns , and the   pilot sampling re - utilization .   2 Related Work   The ability to process long sequences is criti-   cal for many Natural Language Processing tasks ,   including Document Summarization ( Xiao and   Carenini , 2019 ; Huang et al . , 2021 ) , Question An-   swering ( Wang et al . , 2020a ) , Information Ex-   traction ( Li et al . , 2021 ; Du and Cardie , 2020 ;   Ebner et al . , 2020 ; Du et al . , 2022 ) , and Ma-   chine Translation ( Bao et al . , 2021 ) . However , the   quadratic computational cost of self - attention in   transformer - based models limits their application   in long - sequence tasks . Recent methods have been   proposed to accelerate attention computation by se-   lectively attending to a subset of the tokens or with   low - rank matrix approximation ( Tay et al . , 2020b ) .   Selective attention methods limit the scope of   matrix operation with sparse attention patterns or   column / row sampling methods . BlockBERT ( Qiu   et al . , 2020 ) introduces sparse block structures into   the attention matrix . Sparse Transformer ( Child   et al . , 2019 ) introduces dilated patterns . Big   Bird ( Zaheer et al . , 2020 ) proposes a combina-   tion of random , window , and global attention .   Longformer ( Beltagy et al . , 2020 ) combines lo-   cal windowed attention with task - motivated global   attention . The most related work to ours is In - former ( Zhou et al . , 2020 ) , which allows each key   to only attend to the top queries under the Kullback-   Leibler divergence based sparsity measurement .   Low - rank attention matrix approximation meth-   ods are based on the assumption of low - rank   structure in the full self - attention matrix . Lin-   former ( Wang et al . , 2020b ) compresses the   size of the key and value matrices by the John-   son – Lindenstrauss transform ( Johnson and Lin-   denstrauss , 1984 ) . Performer ( Choromanski et al . ,   2020 ) recognizes the attention score matrix as an   empirical Gaussian kernel matrix and constructs   a low - rank projection for both the query and key   matrices through random Fourier features ( Rahimi   and Recht , 2007 ) . Nyströmformer ( Xiong et al . ,   2021 ) instead utilizes Nyström method ( Williams   and Seeger , 2000 ; Drineas and Mahoney , 2005 )   to approximate the attention score matrix . Sky-   former ( Chen et al . , 2021 ) replaces the softmax   structure with a Gaussian kernel and adapts the   Nyström method to accelerate the computation .   Some other methods follow a similar principle   to decompose the attention score matrix , although   they are not necessarily aiming to approximate the   original self - attention itself . The representative   methods include Linear Transformer ( Katharopou-   los et al . , 2020 ) , which claims that the exponential   transform of the dot - product in the softmax opera-   tion can be replaced by the direct matrix multipli-   cation of the projected query and key matrices , and   Reformer ( Kitaev et al . , 2020b ) , which forces the   query and key matrices to be identical and applies   locality - sensitive hashing ( LSH ) ( Har - Peled et al . ,   2012 ) to simplify the computation of the attention   score matrix . Those methods are effective alterna-   tives of the original self - attention , while they do   not fall into the scope of matrix approximation . We   spare the discussion of those methods in this paper .   3 Sketching Framework   3.1 Problem Formulation   Given an input sequence X∈R , where n   is the sequence length and d is the embedding   dimension , the dot - product attention for a single   attention head in transformer ( Vaswani et al . , 2017 )   is defined as   Attention ( Q , K , V ) = softmax / parenleftbiggQK   √p / parenrightbigg   V   where Q = XW , K = XW , andV=   XW.W , W , W∈Rare the query,5188Algorithm 1 : Skeinformer .   key , and value weight metrics that linearly project   the input Xof dimension d to an output tensor   of dimension p.   To ease the future analysis , the softmax term   can be rewritten into DA , where A:=   exp ( QK/√p ) , andDis a diagonal matrix whose   diagonal is exp(QK/√p)·1(1is a size- nvector   with all elements being 1 ) .   3.2 Sketching Methods   Beyond current attempts to accelerate self-   attention , research in the random matrix approx-   imation community can be potentially applied to   fast attention . Among the theoretical frameworks ,   the sketching method ( Woodruff , 2014 ) is the most   comprehensive one as its general concept can in-   corporate many different approaches .   The core idea of the sketching method is to re-   place an original matrix B∈Rwith its ran-   dom sketch BS , where S∈Ris a random   sketching matrix . In practice , to apply the sketch-   ing method we plug an identity matrix into the   original expression , and then formally replace the   identity matrix with the product SS , as the dis-   tribution of Sis usually designed to satisfy the   constraint that   E(SS ) = I. ( 1 )   Common methods to construct a sketching   matrix include sub - Gaussian maps ( Vershynin ,   2010 ; Halko et al . , 2011 ) , subsampled randomized   Hadamard transform ( SRHT ) ( Ailon and Chazelle ,   2006 ; Lu et al . , 2013 ; Yang et al . , 2017 ) , sparse   oblivious subspace embeddings ( Cohen et al . ,2016 ) , very sparse random projection ( Li et al . ,   2006 ) , accumulative sketching ( Chen and Yang ,   2021 ) , and sub - sampling sketching ( Monte Carlo   algorithms ) ( Drineas et al . , 2006 ) . Specifically ,   Informer and Linformer , two efficient transformer-   based methods mentioned above , can be under-   stood as applications of sub - sampling sketching   and sub - Gaussian maps , respectively . We further   elaborate the connections in the next subsection .   3.3 Sketching in Self - attention Approximation   A naïve step in applying sketching method to ap-   proximate the self - attention output DAV is to   construct a random sketch of the un - normalized   attention score matrix A , the bottleneck in compu-   tation . Informer and Linformer construct two types   of sketches , ASandASrespectively .   3.3.1 Informer   Informer selects dimportant rows of DA ,   though deterministically , to represent DA . This   process can be related to a sketched approximation   DSSA , where Sis a sub - sampling matrix   defined as follows :   Definition 3.1 ( Sub - sampling matrix ) .Consider   a discrete distribution which draws iwith prob-   ability p>0,∀i∈[n ] . For a random matrix   S∈R , ifShas independent and identically   distributed ( i.i.d . ) columns and each column S   isewith probability p , where eis the i - th   column of the n - by - nidentity matrix I , thenSis   called a sub - sampling matrix with sub - sampling   probabilities { p } .   Some researchers in the field of approximate ma-5189trix multiplication have provided a practical guide-   line for the choice of the sub - sampling probabilities   { p}inS. Specifically for the matrix multipli-   cation BC of two arbitrary matrices BandC ,   Drineas et al . ( 2006 ) approximate it with BSSC   and set the sampling probability pinSpropor-   tional to the product ∥B∥∥C∥ , where B   is the i - th column in matrix BandCis the i - th   row in matrix C. For the product DA , the prob-   ability in sketching will be p= , where   ais the j - th element of the i - th row in matrix A.   The above sampling probability choice { p }   is highly related to the sparsity measurement used   in Informer , which is M= ln . Here   pis the ratio between the quadratic mean and the   arithmetic mean of { a};Mis the logarithm   of the ratio between the arithmetic mean and the   geometric mean . It is clear that Mwill increase   withpas these two ratios will both be large when   { a}are highly non - uniform . We conclude   that in Informer , the main idea to select the rows   with high sparsity measurement can be taken as a   special variant of the sub - sampling method above   with probabilities { p } .   3.3.2 Linformer   Another type of sketch ASis mentioned ( but not   finally used ) in Linformer . The sketching matrix   Stakes a form different from sub - sampling . The   construction of Sin Linformer is motivated by   Johnson - Lindenstrauss ( JL ) transform , which ap-   plies the sketching matrix Ssatisfying the ( ε , δ)-JL   guarantee :   Definition 3.2 ( Oblivious Johnson - Lindenstrauss   guarantee ( Johnson and Lindenstrauss , 1984 ) ) .   A distribution DoverRsatisfies “ oblivious   Johnson - Lindenstrauss guarantee " if for some   ε , δ∈(0,1/2 ):   ∀b∈R , P / parenleftbig / vextendsingle / vextendsingle∥Sb∥− ∥b∥/vextendsingle / vextendsingle > ε∥b∥/parenrightbig   < δ .   ( 2 )   Specifically , a matrix with i.i.d . Gaussian ele-   ments can meet the above requirement . It has been   proven ( Johnson and Lindenstrauss , 1984 ) that with   d = O(εlog(1 / δ ) ) , a Gaussian sketching matrix   Scan satisfy the oblivious ( ε , δ)-JL guarantee . To   extend the conclusion from a single vector b∈R   to a matrix B∈R , the size dstill needs to   suffer from an additional lognterm ( Vershynin ,   2010 ) , which matches the bound in sub - sampling   sketching ( Drineas et al . , 2006 , Theorem 1).However , the direct use of Gaussian sketch-   ing matrix , i.e. the approximation DASSV   ( Wang et al . , 2020b , Eqn . ( 5 ) ) , requires the   computation of the whole matrix A. To   avoid this computational burden , Linformer re-   places the form of sketching method with   softmax / parenleftbig   ( QK/√p)S / parenrightbig   SV , which sacrifices   the accuracy for efficiency in some tasks as shown   in later experimental results .   4 Methodology : Skeinformer   Based on the previous discussion , we observe that   Informer omits the information from the value ma-   trixV , and Linformer deviates from the usual   sketching form for efficiency . To address these   issues and fully exploit the power of sketching ,   we strengthen the attention approximation with the   following components .   In Section 4.1 , we introduce column importance   sampling , which allows the information incorpora-   tion from Vto accelerate the matrix multiplication   ( compared to JL transform ) ; in Section 4.2 and   Section 4.3 , we leverage the sampled columns to   perform the row normalization and reuse the pilot   row sampling , which further improves the approxi-   mation and makes the training more stable .   We describe the proposed method Skeinformer   in Algorithm 1 and verify its performance on ma-   trix approximation in Section 5 . We also provide   complexity analysis in Section 4.5 to show that our   method enjoys the same O(nlogn)complexity as   other methods .   4.1 Column Sampling   The row selection in Informer has been derived as   a special variant of the sketching method and can   be further improved by utilizing the information   fromV , in a form similar to Linformer :   DASSV ,   whereSabove is a sub - sampling matrix defined in   Definition 3.1 with sampling probabilities   p∝ ∥(DA)∥∥V∥ , i = 1,2 , . . . , n.   We remark that using the sub - sampling sketching   in this way can both circumvent the computation   burden of Gaussian sketching , and also allow the   incorporation of the information from V.   AsSformally samples some columns from   DA , we name the procedure as column sam-   pling in our method . The performance regarding5190   the Frobenius norm loss of the approximate matrix   multiplication can be guaranteed by the following   proposition :   Proposition 1 ( Adapted from Theorem 1 ( Drineas   et al . , 2006 ) ) .Suppose the attention score matrix   B:=DA∈R , the value matrix V∈   R , the number of sampled columns d∈Z   such that 1≤d≤n , and the sampling probabil-   ities{p}are such that / summationtextp= 1and such   that for a quality coefficient β∈(0,1 ]   p≥β∥B∥∥V∥/summationtext∥B∥∥V∥,∀i∈[n ] . ( 3 )   Construct a sub - sampling matrix S∈Rwith   sub - sampling probabilities { p}as in Defini-   tion 3.1 , and let BSSVbe an approximation to   BV . Letδ∈(0,1)andη= 1+/radicalbig   ( 8 / β ) log(1 /δ ) .   Then with probability at least 1−δ ,   ∥BV−BSSV∥≤η   βd∥B∥∥V∥.(4 )   Remark . Proposition 1 guides Informer and our   method to pick the important rows and columns of   the attention score matrix B. In self - attention , it is   feasible to compute the norm ∥V∥of each row   inVwithO(n)time , assuming the dimension pin   each head is fixed and independent of n. However ,   similar to Informer , it is inefficient to exactly com-   pute the ℓ-2 norm of each column in the n - by - n   matrix B , and we need pilot sampling as well to   estimate the norm of the columns in B. We show   thatO(logn)samples in the pilot sampling are suf-   ficient to guarantee the quality coefficient β≥/radicalig   with high probability by the following lemma . ( See   proof in Appendix . )   Lemma 1 . Assume for any i∈[n],∥B∥/nis   uniformly lower bounded by a constant C , whereB:=DA . For another constant δ∈(0,1/2 ) ,   we uniformly sample dindices { j}from [ n ]   with replacement , and let dbe a constant multiple   oflog(n / δ ) . Then with probability at least 1−δ ,   the estimated sub - sampling probabilities   ˆp:=(/summationtextb)∥V∥   /summationtext(/summationtextb)∥V∥,∀i∈[n],(5 )   satisfy the constraints ( 3 ) with β=/radicalig , where   bis the element of Bfrom the j - th row and i - th   column .   This lemma states the sub - sampling weights   used in our proposed method . Its computation only   requires accesses to { B}obtained from the   pilot sampling , and thus has greatly reduced the   time cost . Combining the preceding lemma and   Proposition 1 , we conclude that with the sampling   probabilities { ˆp}estimated by O(logn)pilot   samples , the sampled dimportant columns suffice   to capture the essence of the original output BV .   We conclude this subsection with a remark that the   theoretical result that sub - sampling sketching can   well approximate the original self - attention , indeed   matches the rank collapse phenomenon observed   by Dong et al . ( 2021 ) that self - attention can be well   approximated by a low - rank matrix .   4.2 Adaptive Row Normalization   In addition to the theoretical guarantee of the sub-   sampling sketching method , we identify an impor-   tant component behind Informer , row normaliza-   tion , which implicitly fills the un - selected rows   with . The experiments in Section 5 reveal that   even the rank - one pure row normalization bench-   mark11V , as an ablation , will have acceptable   spectral norm loss ∥DAV−11V∥. There-   fore , we incorporate adaptive row normalization5191to provide an even better attention score distribu-   tion in each row . It fills un - selected columns with   the averaged selected columns . Moreover , from   the model training perspective , it allows the whole   value matrix Vin Skeinformer to participate in   the computation ( compared to only using the sub-   sampling sketch SV ) , and thus can improve the   efficiency of updating Wduring the training .   Specifically , in adaptive row normalization any   row in the matrix Acan be divided into two parts ,   the exactly computed elements in the selected   columns with indices { j}⊂[n]and the other   elements in the un - selected columns . For the latter ,   in each row , we set all the un - selected elements   as the geometric mean of the selected ones , con-   sidering the exponentiation in softmax . We then   perform row normalization based on the above con-   struction , in which the i - th diagonal element in D   is estimated as   ˆd=/summationdisplaya+ ( n−d)(/productdisplaya),(6 )   where each ais the corresponding element in ma-   trixA. Next we normalize rows composed of exact   elements in the selected columns , and the other   elements estimated with the mean value above . We   comment that though the component of adaptive   row normalization makes the proposed method in-   applicable to Proposition 1 , it benefits the perfor-   mance on matrix approximation and avoid the cost   to compute the diagonal normalization matrix D.   ( c.f . Section 5 )   4.3 Pilot Sampling Reutilization   Since we have already computed Bin pilot sam-   pling step ( defined in Ln . 3 of Algorithm 1 ) , we   can exactly reproduce the drows in the original   self - attention output with an additional product   BVinO(nlogn)time . This allows for more   precise approximation with little cost . In addition ,   the computation of those rows involves the whole   key matrix K , which benefits the training of the   parameters W.   4.4 Implementation Details   Applying the sub - sampling - based methods requires   the support for padding masks commonly used in   Natural Language Processing tasks . However , a   naïve implementation of Algorithm 1 will result in   the unnecessary sampling of the padding tokens .   Therefore , we count the number of the unpaddedtokens m , and only perform the pilot sampling   within the certain range [ m ] . After the matrix B   is computed , we set its columns belonging to the   padded part to be all zero , so that the probability   ˆpof choosing column ifrom the padded part will   be zero and the column will not be sampled in the   later importance sampling . Similar modifications   can also be applied to Informer to enable its appli-   cations in NLP tasks in Section 6 .   4.5 Complexity Analysis   With the mild assumption in Lemma 1 , we claim   that our method can have an O(nlogn)time and   space complexity . The claim is shown by the fol-   lowing analysis of the complexity , which heavily   relies on the notations in Algorithm 1 .   First , we point out that the row / column retriev-   ing operation after index sampling can be imple-   mented by only forming a view and thus the cost   is negligible . For Line 1∼4 in Algorithm 1 ,   the time complexity of the uniform pilot sampling   isO(d ) = O(logn ) , while the computation of   the matrix Band the corresponding probabilities   { ˆp}costsO(nd ) = O(nlogn)time and space .   For Lines 5∼7 , with probabilities { ˆp } , the   importance sampling takes O(n+dlogn ) = O(n )   time , and similar to the computation above it takes   O(nlogn)time and space to obtain AandR.   For Lines 8∼10 , it is clear that the three vectors   g , d , andvcan be computed in O(nlogn)time .   As for the last step in Line 11 , since it just requires   the matrix product involving a diagonal matrix , we   can finish the computation also in O(nlogn)time   and space . In summary , the total time and space   complexity for Algorithm 1 is at most O(nlogn ) ,   much lower than the O(n)complexity for the orig-   inal softmax self - attention .   Remark . The complexity above is derived based   on the high probability bound in Proposition 1 ,   which is different than the derivation by some pre-   vious methods to claim the linear O(n)complexity .   5 Approximation Evaluation   As a preliminary justification of our proposed meth-   ods , we compute the spectral norm loss , a com-   mon metric for approximate matrix multiplication ,   to evaluate the effect of different models to ap-   proximate the original self - attention . We compare   the spectral norms of the differences between the   outputs from vanilla self - attention and other fast   attention methods given the same input Q , K , V.5192Specifically we compute ∥BV−R∥ , where   B:=DAis the attention score matrix in the   original method , and Ris the output of each ap-   proximation method .   To construct the inputs Q , K , V , we first truncate   the raw text from Wikitext-2 dataset ( Merity et al . ,   2017 ) into sequences of length 512 . Then we trans-   form the input XintoQ , K , Vwith the query , key ,   and value weight metrices from a pretrained model   or a randomly initiated model .   We report the spectral norm loss of different   sketching - based methods in Figure 1 . The results   are averaged over 768trials , and the error bars   in the figure represent the standard error of the   reported values . For size d(x - axis ) , either the num-   ber of columns / rows selected or the projection di-   mension , it is set in the range from 2to2 .   V - Mean uses a rank - one matrix11Vto ap-   proximate the original self - attention , and thus its   approximation error does not change with the size   d. V - Mean can also be seen as an ablation for the   row normalization step ( equivalent to adaptive row   normalization without sub - samples ) . We observe   the row normalization step greatly contributes to   the approximation of self - attention that involves   a softmax structure . Among the candidates , Ske-   informer tends to have the smallest spectral norm   loss , especially when dis large . It attains a higher   accuracy than Informer and Linformer regarding   the matrix approximation performance .   6 Experiment   6.1 Benchmark   We experiment on Long Range Arena ( LRA )   benchmark ( Tay et al . , 2020a ) , including ListOps   ( Nangia and Bowman , 2018 ) , Text Classification   ( Maas et al . , 2011 ) , Document Retrieval ( Radev   et al . , 2013 ) , Pathfinder ( Linsley et al . , 2018 ) ,   and Image Classification ( Krizhevsky et al . , 2009 ) .   LRA is designed for long - context scenarios and   more appropriate for evaluating efficient transform-   ers comparing to GLUE ( Mutton et al . , 2007 ) with   shorter input context . Following ( Xiong et al . ,   2021 ) we use a 2 - layer transformer model with 64   embedding dimensions , 128 hidden dimensions ,   and 2 attention heads for all experiments . More   details can be found in Appendix .   6.2 Baseline Methods   We compare our method with the standard   quadratic self - attention ( Vaswani et al . , 2017 ) , BigBird ( Zaheer et al . , 2020 ) , Linformer ( Wang et al . ,   2020b ) , Informer ( Zhou et al . , 2020 ) , Performer   ( Choromanski et al . , 2020 ) , and Nyströmformer   ( Xiong et al . , 2021 ) . In addition to their vanilla   implementations , we compare with standard self-   attention without dropout ( since most fast attention   methods do not have this component ) , Linformer   with unreduced Johnson - Lindenstrauss Transform   ( the original form that Linformer deviates from ) ,   and Informer with padding masks .   Ablation studies include replacing the column   sampling with uniform sampling , disabling the   adaptive row normalization or replacing it with the   simple row normalization implemented in Informer ,   and disabling the pilot sampling reutilization .   For clarification , deep transformers or pretrained   language models are not appropriate baselines .   Training a deep transformer from scratch requires   large computational resources and much more data   to converge , and therefore is not adopted by previ-   ous work . A shallow transformer structure , on the   other hand , has been justified by previous work to   be enough for fair comparison in attention accelera-   tion performance . Pretrained models are trained for   token - level text - based tasks , and are not suitable   for image pixel sequences ( as in Pathfinder and Im-   age Classification ) , character sequences ( as in Text   Classification ) and math operation sequences ( as   in ListOps ) .   6.3 Results   We conclude the results in Table 1 and Table 2 with   the following observations :   Most / tildewideO(n)attention acceleration methods   have comparable or better performance with   vanilla attention . After all models converge to   their long - time limits , Linformer tends to have   worse performance possibly due to the violation of   the sketching form , while Skeinformer has the best   overall performance .   While surprising , those approximation methods   tend to outperform the original transformer in most   tasks . We speculate the reason behind this phe-   nomenon is that a good approximation can recover   the main signals in the original self - attention ma-   trix , and also restrain the noise via the sparse /   low - rank structure . A similar phenomenon can be   found in CNN ( Sanyal et al . , 2018 ) , that a low - rank   regularizer , such as SVD , applied to the representa-   tion of the intermediate layers can allow the model   to have lower prediction errors . This speculation5193   motivates us to turn to some theoretical framework   for matrix approximation to better analyze the fast   attention methods , which will potentially benefit   transformer pruning , compression and distillation .   Skeinformer has the comparable general per-   formance in terms of time / space complexity and   classification accuracy . A long transformer is con-   sidered efficient when it ( 1 ) reduces space complex-   ity and supports larger sequence length and larger   batch size , ( 2 ) reduces time complexity with less   training time per step and less total time to con-   verge , and ( 3 ) shows comparable performance with   vanilla softmax without much loss from approxi-   mation .   For convergence efficiency , Skeinformer effi - ciently converges to the long - time limit . Regarding   the training efficiency , we focus on how soon the   model can attain the stationary distribution of its   long - time limit ( He et al . , 2019 ) . The loss decay   plot on ListOps in Appendix shows significant dif-   ferences in the convergence rate of each method in   addition to classification accuracy .   Though our method does not always outperform   others ( with the fastest convergence or the highest   accuracy ) , but we remark that Skeinformer at-   tains the best accuracy - efficiency trade - off based   on experimental results . On the opposite , some   model converges fast but gets stuck in a local opti-   mum , like Linformer in some cases.51947 Conclusion   We conclude in this paper that sketching can be ap-   plied as a theoretical framework for analyzing fast   attention models , through which we are able to rec-   ognize the potential improvements upon previous   work . Theoretical results are provided to guarantee   the high accuracy of the approximation to the origi-   nal self - attention by our proposed Skeinformer . We   empirically validate the contributions of the compo-   nents in Skeinformer , including column sampling ,   adaptive row normalization and pilot sampling re-   utilization , with extensive comparisons with vari-   ous baseline and ablation methods .   Acknowledgement   We thank the anonymous reviewers for their helpful   suggestions . This research is based upon work sup-   ported by U.S. NSF grant DMS-1810831 , DARPA   AIDA Program No . FA8750 - 18 - 2 - 0014 and U.S.   DARPA KAIROS Program No . FA8750 - 19 - 2-   1004 . The views and conclusions contained herein   are those of the authors and should not be inter-   preted as necessarily representing the official poli-   cies , either expressed or implied , of DARPA , or   the U.S. Government . The U.S. Government is   authorized to reproduce and distribute reprints for   governmental purposes notwithstanding any copy-   right annotation therein .   References519551965197A Further experiment Details   A.1 Implementation Details   As it is not realistic to exhaustively fine - tune all   models and search for the best performance under   limited computation resources , we instead replace   the self - attention module in transformer with the   various drop - in attention methods and keep other   experimental settings the same . Following ( Xiong   et al . , 2021 ) we use a 2 - layer transformer model   with 64 embedding dimensions , 128 hidden dimen-   sions , and 2 attention heads for all experiments .   Mean pooling is used in all classifiers .   For comparable computation complexity , we   control the number of features used in all methods ,   which leads to 256as the number of features in Ske-   informer , 256askin Linformer , 256as the number   of landmarks in Nyströmformer , ( 256 / logn)as   the factor in Informer , and 256as the number of   features in Performer . Additionally , the number of   random blocks and block size in Big Bird are by   default 3and64 , under which setting Big Bird will   visit640·nelements in the attention matrix while   other models visit 256·nelements . A clearer com-   plexity evaluation on the FLOPs of each method is   provided in Appendix .   We use Adam optimizer ( Kingma and Ba , 2015 )   with a learning rate of 1e−4 . Batch size is selected   conditioned on the memory requirements of Skein-   former , which leads to 128for Text Classification ,   256for ListOps , 64for Document Retrieval , 512   for Pathfinder and 256for Image . For methods   reporting out - of - memory errors , we apply gradi-   ent accumulation and report the accumulated steps .   Instead of setting a fixed epoch number , we train   all models until convergence with a stopping strat-   egy ( if better performance is not observed for 10   checking steps on the validation set we will stop   the training process ) .   We conduct all experiments on one Tesla V100   SXM2 16 GB . For numerical consistency , all ex-   periment results are averaged across three random   runs .   A.2 LRA Dataset   We evaluate on five classification tasks in LRA   benchmark ( Tay et al . , 2020a ) , excluding Pathfider-   X , which fails all baseline models .   ListOps ( Nangia and Bowman , 2018 ): This 10-   label classification task requires the models to parse   a sequence of length 2kof numbers and operatorsand evaluates their capacity of modeling hierarchi-   cally structured long sequences .   Text Classification on IMDb review   dataset(Maas et al . , 2011 ): This byte - Level   binary classification task requires the model to   analyze the sentiment of a sequence of length 4k   by composing the unsegmented characters into   higher - level meaningful units .   Document Retrieval on AAN dataset ( Radev   et al . , 2013 ): This byte - Level binary classifica-   tion task requires the model to compress long se-   quences of length 4kinto representations for simi-   larity score calculation in a two - tower setup with-   out cross - attention .   Pathfinder on CIFAR-10 dataset(Linsley et al . ,   2018 ): This binary classification task requires the   model to decide whether two points are connected   by a dashed path on an image represented as a pixel   sequence of length 4k , and exams their capacity to   capture long - range spatial dependency .   Image Classification ( Krizhevsky et al . , 2009 ):   This 10 - label classification task requires the models   to learn the spatial relations between the flattened   input pixels of length 1k .   A.3 Validation Loss   We present the loss decay plots on all tasks in Fig-   ure 2 . In the first subplot for the text classification   task , we note all the methods quickly overfit the   dataset . In all the other plots , our methods show   the ability to both efficiently converge to the long-   time limit and find better local minima with lower   validation loss .   A.4 FLOPs   We conclude in this subsection the floating point   operations ( FLOPs ) of each candidate model ( ex-   cluding the ablation models ) . To ease the notation ,   given the sequence length n , we fix p= 32 , d=   256 . Assuming the matrices Q , K , Vare given   and omitting the non - leading term , we report the   FLOPs of each model in Table 3 . We additionally   comment that Reformer is excluded from the table   since its FLOPs are not fixed and depend on the fre-   quency of collision after hashing of tokens , which   changes with the input sequence .   A.5 Hyper - parameter Sensitivity   Figure 3 shows the accuracy and training time for   Skeinformer using different batch sizes ( 64,128 )   and learning rates ( 1e−3,1e−4,1e−5 ) on text5198   Models FLOPs   Standard 2np   Big Bird 5ndp   Performer 3ndp   Nystromformer 4ndp   Linformer 4ndp   Informer 3ndp   Skeinformer 4ndp   classification . The results are averaged across ran-   dom trials . We observe that smaller learning rate   offers slower convergence but to a better point .   B Proof of Lemma 1   Proof . For each column B , we first define a dis-   crete random variable X , that with probability , X = b,∀j∈[n ] , where bis the j - th el-   ement in B. Since all the elements in Bare   bounded ( within the range [ 0,1 ] ) due to the row   normalization in softmax , we infer that for any   i∈[n ] , X∈[0,1]is a sub - Gaussian random vari - able with parameter σ=(Wainwright , 2019 ) .   Combine the conclusion with the assumption that   EX≤C , we have   X   EX∼sub - Gaussian / parenleftbigg   σ=1   4C / parenrightbigg   . ( 7 )   Then we uniformly sample dindexes { j } ’s   with replacement , and we estimate the squared   norm of each column with the unbiased estima-   torY=/summationtextb . We remark Yhas the same   distribution as / summationtextX , where X ’s are   di.i.d . copies of X. Therefore through a linear   transform of Equation ( 7 ) we can derive that   Y   nEX∼sub - Gaussian / parenleftbigg   σ=1   4Cd / parenrightbigg   .(8 )   Notice different Y ’s may not be independent   since they all rely on the same drows in B. How-   ever , we can still apply the maximal sub - Gaussian   inequality ( Boucheron et al . , 2013 ) to have :   P / braceleftbigg   max / vextendsingle / vextendsingle / vextendsingle / vextendsingleY   nEX−1 / vextendsingle / vextendsingle / vextendsingle / vextendsingle>1   2 / bracerightbigg   ≤2ne.(9 )   If the high probability bound holds that   max / vextendsingle / vextendsingle / vextendsingle−1 / vextendsingle / vextendsingle / vextendsingle≤ , we directly have that   our estimators Y∈[∥B∥,∥B∥],∀i∈   [ n ] . In that case , the estimated sub - sampling   probabilities satisfy that   ˆp = Y∥V∥   /summationtextY∥V∥≥/radicalbig   1/2∥B∥∥V∥/radicalbig   3/2∥B∥∥V∥   = /radicalbigg   1   3p,∀i∈[n ] ,   where p ’s are the optimal probabilities defined in   the main paper .   In that case , to prove the lemma it suffices to   pick a big enough sub - sample size dsuch that the   right - hand side of Inequality ( 9 ) is smaller than δ .   Simply solving the inequality leads to the desired   result d≥log ( ) . ♢ 5199