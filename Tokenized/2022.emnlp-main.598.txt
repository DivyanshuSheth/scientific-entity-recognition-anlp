  Abhilasha Ravichander   Carnegie Mellon University   aravicha@cs.cmu.eduMatt Gardner   Microsoft Semantic Machines   mattgardner@microsoft.comAna Marasovi ´ c   University of Utah   ana.marasovic@utah.edu   Abstract   The full power of human language - based com-   munication can not be realized without negation .   All human languages have some form of nega-   tion . Despite this , negation remains a challeng-   ing phenomenon for current natural language   understanding systems . To facilitate the fu-   ture development of models that can process   negation effectively , we present C QA ,   the first English reading comprehension dataset   which requires reasoning about the implica-   tions of negated statements in paragraphs . We   collect paragraphs with diverse negation cues ,   then have crowdworkers ask questions about   theimplications of the negated statement in   the passage . We also have workers make three   kinds of edits to the passage — paraphrasing the   negated statement , changing the scope of the   negation , and reversing the negation — resulting   in clusters of question - answer pairs that are dif-   ficult for models to answer with spurious short-   cuts . C QA features 14,182 question-   answer pairs with over 200 unique negation   cues and is challenging for current state - of-   the - art models . The best performing model   onC QA(U QA-2 - 3B ) achieves   only 42 % on our consistency metric , well be-   low human performance which is 81 % . We   release our dataset , along with fully - finetuned ,   few - shot , and zero - shot evaluations , to facili-   tate the development of future NLP methods   that work on negated language .   1 Introduction   Negation is fundamental to human communication .   It is a phenomenon of semantic opposition , relat-   ing one expression to another whose meaning is in   some way opposed . Negation supports key proper-   ties of human linguistic systems such as contradic-   tion and denial ( Horn , 1989).Despite the prevalence of negation , processing   it effectively continues to elude models . Here are   just a few of the many recently reported failures :   “ The model [ BERT - Large trained on SQuAD ] does   not seem capable of handling ... simple examples   of negation ” ( Ribeiro et al . , 2020 ) . “ We find that   indeed the presence of negation can significantly   impact downstream quality [ of machine translation   systems ] ” ( Hossain et al . , 2020a ) . “ State - of - the - art   models answer questions from the VQA ... correctly ,   but struggle when asked a logical composition in-   cluding negation ” ( Gokhale et al . , 2020 ) . How can   NLU systems meet this long - standing challenge ?   To facilitate systems that can process negation   effectively , it is crucial to have high - quality eval-   uations that accurately measure models ’ compe-   tency at processing and understanding negation .   In this work , we take a step toward this goal by   contributing the first large - scale reading compre-   hension dataset , C QA , focused on reasoning   about negated statements in language .   The three - stage annotation process we develop   to construct C QAis illustrated in Fig . 1 . We   first collect passages from English Wikipedia that   contain negation cues , including single- and multi-   word negation phrases , as well as affixal negation .   In the first stage , crowdworkers make three types   of modifications to the original passage : ( 1 ) they   paraphrase the negated statement , ( 2 ) they modify   the scope of the negated statement ( while retaining   the negation cue ) , and ( 3 ) they undo the negation .   In the second stage , we instruct crowdworkers to   ask challenging questions about the implications   of the negated statement . The crowdworkers then   answer the questions they wrote previously for the   original and edited passages .   This process resulted in a dataset of 14,182 ques-   tions , covering a variety of negation cue types and   over 200 unique negation cues , as well as a con-8729   trastive dataset , with passages that are lexically   similar to each other but that may induce differ-   ent answers for the same questions . To perform   well on C QA , models must be able to reason   about the implications of negated statements in text .   In addition to accuracy , the contrastive nature of   C QAenables us to measure the consistency   of models — i.e. , the extent to which models make   correct predictions on closely - related inputs .   We extensively benchmark baseline models on   C QA in three training data regimes : us-   ing all training examples , using only a small frac-   tion ( few - shot ) , or not using any examples ( zero-   shot ) . We show that C QA is challenging   for current models . Finetuning U -QA-   3B(Khashabi et al . , 2022)—which was trained   on 20 QA datasets — on C QA , achieves the   best result of 73.26 % compared to human accu-   racy of 91.49 % . Further , we find that models are   largely inconsistent ; the best model achieves a con-   sistency score of only 42.18 % ( 40 % below human   consistency ) . This very low consistency score   demonstrates that handling negation phenomena   is still a major unresolved issue in NLP , along   with sensitivity to contrasting data more generally .   The dataset and baselines are available at https://   github.com/AbhilashaRavichander/CondaQA .2 C QA Data Collection   This section describes our goals in constructing   C QA and our data collection procedure .   Design Considerations Our goal is to evaluate   models on their ability to process the contextual   implications of negation . We have the following   four desiderata for our question - answering dataset :   1.The dataset should include a wide variety of   ways negation can be expressed .   2.Questions should be targeted towards the im-   plications of a negated statement , rather than   the factual content of what was or was n’t   negated , to remove common sources of spuri-   ous cues in QA datasets ( Kaushik and Lipton ,   2018 ; Naik et al . , 2018 ; McCoy et al . , 2019 ) .   3.The dataset should feature contrastive groups :   passages that are closely - related , but that may   admit different answers to questions , in or-   der to reduce models ’ reliance on potential   spurious cues in the data and to enable more   robust evaluation ( Kaushik et al . , 2019 ; Gard-   ner et al . , 2020 ) .   4.Questions should probe the extent to which   models are sensitive to how the negation is   expressed . In order to do this , there should be   contrasting passages that differ only in their   negation cue or its scope.8730Dataset Construction Overview We generate   questions through a process that consists of the   following steps , as shown in Figure 1 :   1.We extract passages from Wikipedia which   contain negated phrases .   2.We show ten passages to crowdworkers , and   allow them to choose a passage they would   like to work on .   3.Crowdworkers make three kinds of edits to   the passage : ( i ) paraphrasing the negated state-   ment , ( ii ) changing the scope of the negation ,   ( iii ) rewriting the passage to include an af-   firmative statement in place of the negated   statement . For all three kinds of edits , the   crowdworkers modified the passage as appro-   priate for internal consistency .   4.Crowdworkers ask questions that target the   implications of a negated statement in the pas-   sage , taking passage context into account .   5.Crowdworkers provide answers to the con-   structed questions for the Wikipedia passage ,   as well as the three edited passages .   Further , we validate the development and test   portions of C QA to ensure their quality .   Passage Selection We extract passages from a   July 2021 version of Wikipedia that contain ei-   ther single - word negation cues ( e.g. , ‘ no ’ ) or multi-   word negation cues ( e.g. , ‘ in the absence of ’ ) . We   use negation cues from ( Morante et al . , 2011 ; van   Son et al . , 2016 ) as a starting point which we ex-   tend . Our single - word negation cues include affixal   negation cues ( e.g. , ‘ il - legal ’ ) , and span several   grammatical categories including :   1.Verbs : In this novel , he took pains to avoid the   scientific impossibilities which had bothered   some readers of the " Skylark " novels .   2.Nouns : In the absence of oxygen , the citric   acid cycle ceases .   3.Adjectives : Turning the club over to man-   agers , later revealed to be honest people , still   left Wills in desperate financial straits with   heavy debts to the dishonest IRS for taxes .   4.Adverbs : Nasheed reportedly resigned invol-   untarily to forestall an escalation of violence ;   5.Prepositions : Nearly half a century later , after   Fort Laramie had been built without permis-   sion on Lakota land .   6.Pronouns : I mean , nobody retires anymore .   7.Complementizers : Leave the door ajar , lest   any latecomers should find themselves shut   out.8.Conjunctions : Virginia has no ‘ pocket veto ’   and bills will become law if the governor   chooses to neither approve nor veto legisla-   tion .   9.Particles : Botham did not bat again .   Crowdworker Recruitment We use the Crow-   daq platform ( Ning et al . , 2020 ) to recruit a small   pool of qualified workers to contribute to C - QA . We provide instructions , a tutorial and a   qualification task . Workers were asked to read the   instructions , and optionally to also go through the   tutorial . Workers then took a qualification exam   which consisted of 12 multiple - choice questions   that evaluated comprehension of the instructions .   We recruit crowdworkers who answer > 70 % of the   questions correctly for the next stage of the dataset   construction task . In total , 36 crowdworkers con-   tributed to C QA . We paid 8 USD / HIT , which   could on average be completed in less than 30 min-   utes . Each HIT consisted of choosing a passage ,   making edits to the passage , creating questions , and   answering those questions .   Contrastive Dataset Construction We use Ama-   zon Mechanical Turk to crowdsource question-   answer pairs about negated statements . Each ques-   tion is asked in the context of a negated statement   in a Wikipedia passage .   In the first stage of the task , we show crowd-   workers ten selected passages of approximately the   same length and let them choose which to work on .   This allows crowdworkers the flexibility to choose   passages which are easy to understand , as well as   to choose passages which are conducive to making   contrastive edits ( for example , it may be difficult   to reverse the negation in a passage about ‘ Gödel ’s   incompleteness theorems ’ ) .   After selecting a passage , crowdworkers make   three kinds of edits to the original Wikipedia pas-   sage ( Fig . 1 ): ( 1 ) they rewrite the negated state-   ment such that the sentence ’s meaning is preserved   ( P E ) ; ( 2 ) they rewrite the negated   statement , changing the scope of the negation   ( S E ) ; and ( 3 ) they reverse the negated   event ( A E ) . We ask crowdwork-   ers to additionally make edits outside of the negated   statement where necessary to ensure that the pas-   sage remains internally consistent .   In the second stage of the task , the crowdworker   asks at least three questions about the implications   of the negated statement in the original Wikipedia8731passage . We encourage the construction of good   questions about implications by providing several   examples of such questions , as well as by sending   bonuses to creative crowdworkers , ranging from   10$-15$. Crowdworkers can group these questions ,   to indicate questions that are very similar to each   other , but admit different answers .   In the final stage of this task , crowdworkers pro-   vide answers to the questions , in context of the   Wikipedia passages as well as for the three edited   passages . The answers to the questions are required   to be either Yes / No / Don’t Know , a span in the ques-   tion , or a span in the passage . Following best prac-   tices for crowdsourcing protocols described in the   literature ( Nangia et al . , 2021 ) , we provide person-   alized feedback to each crowdworker based on their   previous round of submissions , describing where   their submission was incorrect , why their submis-   sion was incorrect , and what they could have sub-   mitted instead . In all , we provide over 15 iterations   of expert feedback on the annotations . We collect   this data over a period of ∼seven months .   Data Cleaning and Validation In order to esti-   mate human performance , and to construct a high-   quality evaluation with fewer ambiguous examples ,   we have five verifiers provide answers for each   question in the development and test sets . Crowd-   workers were given passages , as well as the pas-   sage edits and questions contributed in the previ-   ous stage of our task . In each HIT , crowdwork-   ers answered 60 questions in total , spanning five   passage sets . We found there was substantial inter-   annotator agreement ; for the test set we observed   a Fleiss ’ κof 63.27 for examples whose answers   are Yes / No / Don’t know ( 97 % of examples ) , 62.75   when answers are a span in the question ( 2 % of   examples ) , and 48.54 when answers were indicated   to be a span in the passage ( 1 % of examples ) . We   only retain examples in the test and development   sets where at least four annotators agreed on the   answer . However , since this procedure results in   few questions with ‘ do n’t know ’ as the answer , we   include an additional stage where we ( the authors )   manually verify and include questions where ‘ do n’t   know ’ was the answer provided by the question au-   thor . As a result , we discard 1,160 instances from   the test set , and 270 from the development set .   3 C QA Data Analysis   In this section , we provide an analysis of the pas-   sages , questions , edits , and answers in C QA .   Descriptive statistics are provided in Table 1 .   Negation Cues Negation is expressed in many   complex and varied ways in language ( Horn , 1989 ) .   To characterize the distribution of types of negated   statements in C QA , we analyze the negation   cues in Wikipedia passages that annotators could   select . Figures 2 and 4 ( Appendix ) show that the   distribution over these cues and their grammatical   roles is considerably diverse . Moreover , there are   219 unique cues in C QAand 75 novel cues   in the test set that are unseen in the training data .   This is a substantially wider range of negation cues   than what is included in prior work ; see Appendix   A for a detailed comparison.8732   Commonsense inferences We assess common-   sense inferences types required to answer C - QAquestions . We sample 100 questions from   the test set and manually annotate the dimensions   of commonsense reasoning required to answer   them . Table 2 shows some of these reasoning types   ( the full version in Table 12 in the Appendix ) .   Editing Strategies Recall that the passages with   negated statements are sourced from Wikipedia and   crowdworkers make three kinds of edits ( Fig . 1 ) .   Through a qualitative analysis of the data , we iden-   tify commonly employed edit strategies ( Tables 3   and 13 ) . We also analyze to what extent edits cause   an answer to change . We find that the affirmative   edits change the answers of 77.7 % of questions   from the original Wikipedia passage , and the scope   edits change the answer of 70.6 % of questions .   Potential edit artifacts Because we had crowd-   workers edit Wikipedia paragraphs , a potential con-   cern is that the edited text could be unnatural and   give spurious cues to a model about the correct an-   swer . We ran two tests to try to quantify potentialbias in this edited data . First , we trained a BERT   model ( Devlin et al . , 2019 ) to predict the edit type   given just the passage . The model performs only a   little better than random chance ( 34.4 % ) , most of   the improvement coming from the ability to some-   times detect affirmative edits ( where the negation   cue has been removed ) . Second , we compared the   perplexity of the original paragraphs to the perplex-   ity of the edited paragraphs , according to the GPT   language model ( Radford et al . , 2018 ) , finding that   they are largely similar . Details for both of these   experiments are in Appendix B.   4 Baseline Performance on C QA   We now evaluate state - of - the - art models ’ abilities   to solve instances of C QA . We evaluate mod-   els that we train either on the entire C QA   training data or few examples , as well as zero - shot   models . We use two classes of metrics :   Accuracy The percentage of predictions which   match the ground truth answer . If the answer is a   span , this metric measures whether the prediction8733   matches the ground truth answer exactly .   Group Consistency C QA ’s dense annota-   tions enable us to study model robustness through   group consistency . We wish to measure whether   a model correctly captures how the presence of   negated phrases influences what can be inferred   from a paragraph . Measuring this requires vary-   ing ( and sometimes removing ) the negated phrases   and seeing how the model responds ( see Table 14   in the Appendix ) ; it is only by looking at consis-   tency across these perturbations that we can tell   whether a model understands the phenomena in   question ( Gardner et al . , 2020 ) . Specifically , for   a group of minimally - different instances , consis-   tency measures whether the prediction matches the   ground truth answer for every element in that group .   We consider two types of groups : ( a ) Question-   level consistency : each group is formed around a   question and the answers to that question for the   original Wikipedia passage , as well as the three   edited passage instances ( A ) , ( b ) Edit - level con-   sistency : each group is formed around a ques-   tion , the answers to that question for the original   Wikipedia passage , and only one of the edited pas-   sages ( P , S - , and A ) . Tocompute consistency , we use the 5,608 questions   in the test set that have ( passage , answer ) pairs for   all four edit types ( excluding any question where at   least one passage was removed during validation ) .   4.1 Models and Controls   The baseline models that we benchmark on C - QAare listed in Table 4 . We categorize them   based on whether they use ( a ) all of the training   data we provide ( full finetuned ) , ( b ) a small frac-   tion of the available training data ( few - shot ) , ( c ) no   training data ( zero - shot ) , and on ( d ) whether they   measure dataset artifacts ( controls ) .   Forfull finetuning , we train and evaluate three   BERT - like models : BERT ( Devlin et al . , 2019 ) ,   RoBERTa ( Liu et al . , 2019 ) , and DeBERTa ( He   et al . , 2021b , a ) , in addition to UnifiedQA - v2   ( Khashabi et al . , 2022 ) , a T5 variant ( Raffel et al . ,   2020 ) that was further specialized for QA by train-   ing the model on 20 QA datasets . More information   about these models is given in Appendix C.1 . We   study Base , Large , and 3B sizes of UnifiedQA - v2 .   Each fully - finetuned model is trained with 5 ran-   dom seeds , and we report the average performance   across seeds on the entire test set .   In the few - shot setting with 8–9 shots , we evalu-   ate UnifiedQA - v2-{Base , Large , 3B } ( Khashabi8734et al . , 2022 ) , GPT-3 ( davinci ; Brown et al . ,   2020 ) , and a version of InstructGPT(Ouyang   et al . , 2022 ) known as text - davinci-002 ; hence-   forth referred to as /agcInstructGPT . We additionally   prompt / agcInstructGPT with chain of thoughts ( CoT ;   Wei et al . , 2022 ) as this should be beneficial for rea-   soning tasks . We do prompt - based finetuning of   UnifiedQA - v2 ( i.e. , change its parameters ) and in-   context learning of the GPT models ( i.e. , we do not   change their parameters ) . Besides these models , in   thezero - shot setting , we also evaluate UnifiedQA-   v2 - 11B and FLAN - T5 - 11B ( Chung et al . , 2022 ) , a   T5 variant that was further trained with instruction   finetuning and CoT data . Details of few- and zero-   shot settings are given in Appendix C.2 . Due to the   cost of the OpenAI API and sensitivity of few - shot   learning to the choice of few examples ( Zhao et al . ,   2021 ; Logan IV et al . , 2022 ; Perez et al . , 2021 ) , we   evaluate few- and zero - shot models as follows . We   split the train / test sets into five disjoint sets , sample   9 shots from each train subset , evaluate models on   such five train - test splits , and report the average   performance across them . On average each test   split contains 1448 instances .   We evaluate heuristic baselines to measure the   extent to which models can use data artifacts to   answer C QAquestions . These baselines can   answer questions correctly only if there is bias in   the answer distribution given a question or other   metadata since they do not get paragraphs . We train   U QA-2- on just : ( i ) ( question , an-   swer ) pairs , ( ii ) ( question , edit type , answer ) triples   where the edit type denotes whether the passage   was a paraphrase , scope edit , etc . , and ( iii ) ( ques-   tion , negation cue , answer ) triples . We find these   baselines do little better than just answering “ No ” .   Human Performance We measure human per-   formance on C QAdevelopment and test sets .   Every question was answered by five crowdwork-   ers . To evaluate human performance , we treat each   answer to a question as the human prediction in   turn , and compare it with the most frequent answer   amongst the remaining answers . For questions   where the gold answer was decided by experts ( § 2 ) ,   we treat each answer as the human prediction and   compare it to the gold answer . Human accuracy is   91.94 % , with a consistency score of 81.58 % .   5 Results   Model performance on C QAis given in Ta-   ble 4 . The best performing model is fully finetunedU QA-2 - 3B with an accuracy of 73.26 %   and overall consistency of 42.18 % , where the esti-   mated human accuracy is 91.94 % and consistency   81.58 % . This gap shows that C QAquestions   are both answerable by humans , and challenging   for state - of - the - art models .   We create a contrastive dataset to be able to mea-   sure consistency as measuring models ’ ability to   robustly predict answers across small input per-   turbations can provide a more accurate view of   linguistic capabilities ( Gardner et al . , 2020 ) . Here ,   there is a gap of ∼40 % in consistency between hu-   mans and the best model . Models are most robust   to paraphrase edits : if a model answers a question   correctly for the original passage , it is likely to be   robust to changes in how that negation is expressed .   We observe that the heuristic - based baselines ex-   hibit low consistency , suggesting the consistency   metric may be a more reliable measure than accu-   racy to evaluate models ’ ability to process negation .   Thus , mainstream benchmarks should consider in-   cluding consistency as a metric to more reliably   measure progress on language understanding .   Few- and zero - shot baselines do not match   fully finetuned models ’ performance , but con-   siderably improve over the majority baseline .   For UnifiedQA - v2 in particular , this suggests   that some reasoning about implications of nega-   tion is acquired during pretraining . Surprisingly ,   UnifiedQA - v2 few - shot performance is worse than   zero - shot . While this behavior has been reported   for in - context learning with GPT-3 ( Brown et al . ,   2020 ; Xie et al . , 2022 ) , we did not expect to ob-   serve this for a finetuned model . UnifiedQA - v2-   3B finetuned with a few examples is comparable to   /agcInstructGPT ( text - davinci-002 ; at least 175B   parameters ) with in - context learning . Chain - of-   thought prompting ( CoT ) notably improves the   performance of /agcInstructGPT , especially in terms   of the most challenging metrics : scope and affir-   mative consistency . In the zero - shot setting , the   11B version of UnifiedQA - v2 performs the best ,   while the base version of only 220 M parameters   is comparable to /agcInstructGPT . UnifiedQA - v2-   11B is also better than FLAN - T5 - XXL ( a 11B-   parameter model as well ) . Given that UnifiedQA-   v1 ( Khashabi et al . , 2020 ) has been effective for   tasks beyond QA ( Bragg et al . , 2021 ; Marasovi ´ c   et al . , 2022 ) , this result suggests that UnifiedQA8735   models are strong but overlooked baselines in re-   cent works on large - scale models .   6 Analysis   While examining model errors , we find   U QA-2- has a negative correlation   with question length ( Figure 7 in Appendix D ) . Hu-   mans can still reliably answer such long questions   that are frequent in C QA . We also analyze   the performance of U QA-2-   across answer types , finding that : ( i ) the model   performs best when the answer is “ No ” , ( ii ) it   almost never predicts “ Do n’t know ” , and ( iii )   its performance on span extraction questions   is in - between those two extremes ( Figure 8 in   Appendix D ) . U QA-2 - 3B exhibits similar   behavior , with improved performance on questions   which admit “ Do n’t know ” as an answer .   We analyze questions across the Wikipedia pas - sages and the passages with edited scopes , with   the focus on : ( i ) instances where the true an-   swer does not change with the edited scope and   the model should be stable , and ( ii ) instances   where the true answer does change and the model   should be sensitive to the edit . We find that when   the fully - finetuned U QA-2 - 3B ( the best-   performing model ) answers the question correctly   for the Wikipedia passage , it only produces the an-   swer correctly for 63.23 % of questions where the   scope edit induces a different answer . In contrast ,   the model answers questions correctly for 91.03 %   of the instances where the answer does not change   with the scope edit . This suggests the model is   not sensitive to changes of the scope of negated   statements .   We also analyze to what extent U QA-2 - 87363Bdistinguishes between negated statements and   their affirmative counterparts . We examine model   predictions for 1080 sample pairs where the answer   changes when the negation is undone . For 43.52 %   of these , the model changes its predictions . This   suggests , in contrast to previous work ( Kassner   and Schütze , 2020 ; Ettinger , 2020 ) , that models are   sensitive to negated contexts to some extent .   7 Related Work   In Aristotle ’s de Interpretatione , all declarative   statements are classified as either affirmations or   negations used to affirm or contradict the occur-   rence of events ( Ackrill et al . , 1975 ) . Negation is   expressed through a variety of formulations ( Horn ,   1989 ) and is prevalent in English corpora ( Hos-   sain et al . , 2020c ) . Despite that , evidence from   multiple tasks that require language understanding   capabilities — such as NLI ( Naik et al . , 2018 ) , senti-   ment analysis ( Li and Huang , 2009 ; Zhu et al . ,   2014 ; Barnes et al . , 2019 ) , paraphrase identifi-   cation ( Kovatchev et al . , 2019 ) , machine transla-   tion ( Fancellu and Webber , 2015 ; Hossain et al . ,   2020a ) , and QA ( Ribeiro et al . , 2020 ; Sen and Saf-   fari , 2020)—identify negation as a challenging se-   mantic phenomenon for models . Hossain et al .   ( 2022 ) analyze negation in 8 NLU datasets and   conclude : “ new corpora accounting for negation   are needed to solve NLU tasks when negation is   present ” . We expect C QA will help .   Negation Annotations Jiménez - Zafra et al .   ( 2020 ) overview datasets with negation as the main   phenomenon and mention the following : BioScope   ( Vincze et al . , 2008 ) , ProbBank Focus ( Blanco and   Moldovan , 2011 ) , ConanDoyle - neg ( Morante and   Daelemans , 2012 ) , SFU Review(Konstantinova   et al . , 2012 ) , NEG - DrugDDI ( Bokharaeian and   Díaz , 2013 ) , NegDDI - Drug ( Bokharaeian et al . ,   2014 ) , and DT - Neg ( Banjade and Rus , 2016 ) .   These datasets are small ( < 4 K ) and annotated   with different schemes and guidelines as there is   no established formalism for negation due to its   complexity — the case when the QA format is use-   ful ( Gardner et al . , 2019a ) . There are datasets fo-   cused on negation cue / scope / focus detection , or   negated event recognition ( Morante and Blanco ,   2012 ; Reitan et al . , 2015 ; Fancellu et al . , 2017 ; He   et al . , 2017 ; Li and Lu , 2018 ; Hossain et al . , 2020b ) .   Jiménez - Zafra et al . ( 2020 ) assert that the lack of   large datasets remains a major obstacle . Probing Negation Ettinger ( 2020 ) introduces a   dataset of 72 sentences for probing understanding   of negation . Kassner and Schütze ( 2020 ) analyze   factual knowledge in the presence of negation . Sev-   eral works have recently constructed challenge sets   that focus on negation for existing NLI datasets   ( Cooper et al . , 1996 ; Dagan et al . , 2005 ; Giampic-   colo et al . , 2007 ) . Hartmann et al . ( 2021 ) introduce   a multilingual dataset for probing negation based   on XNLI / MNLI ( Conneau et al . , 2018 ; Williams   et al . , 2018 ) . Hossain et al . ( 2020c ) analyze nega-   tion in three existing NLI datasets and find they   are unsuitable for studying how NLI models han-   dle negation . They introduce a new benchmark of   4.5 K instances based on 1.5 K seed instances from   the three NLI datasets . Geiger et al . ( 2020 ) con-   struct a dataset targeting the interaction between   lexical entailment and negation , finding that mod-   els trained on general - purpose NLI datasets do not   perform well , but finetuning with their dataset is   sufficient to address this failure . In contrast to sev-   eral of these works , we contribute training data and   find that simply finetuning on these examples is not   sufficient to address the challenges in C QA .   See Appendix § A for a detailed comparison .   Improving Negation Understanding Efforts to   improve models ’ negation abilities that can be stud-   ied on C QAare : unlikelihood training ( Hos-   seini et al . , 2021 ) , NLI data ( Kim et al . , 2019 ) , com-   monsense knowledge ( Jiang et al . , 2021 ) , multitask-   ing ( Moore and Barnes , 2021 ) , extra MLM ( Khan-   delwal and Sawant , 2020 ; Truong et al . , 2022 ) .   8 Conclusion   Negation supports key properties of human linguis-   tic systems such as the ability to distinguish be-   tween truth and falsity . We present C QA , a   QA dataset that contains 14,182 examples to evalu-   ate models ’ ability to reason about the implication   of negated statements . We describe a procedure for   contrastive dataset collection that results in chal-   lenging questions , present a detailed analysis of   the dataset , and evaluate a suite of strong base-   lines in fully - finetuned , few - shot , and zero - shot set-   tings . We evaluate models on both their accuracy   and consistency , and find that this dataset is highly   challenging — even the best - performing model is   18 points lower in accuracy than our human base-   line , and about 40 points lower in consistency . We   expect that C QAwill facilitate NLU systems   that can handle negation.8737Limitations   In this work , we contribute C QA , a dataset   to facilitate the development of models that can   process negation . Though C QAcurrently   represents the largest NLU dataset that evaluates   a model ’s ability to process the implications of   negation statements , it is possible to construct a   larger dataset , with more examples spanning dif-   ferent answer types . Further , C QA is an   English dataset , and it would be interesting to ex-   tend our data collection procedures to build high-   quality resources in non - English languages . Fi-   nally , while we attempt to extensively measure and   control for artifacts in C QA , it is possible   that the dataset has hidden artifacts that we did not   study .   Acknowledgements   The authors are grateful to Aakanksha Naik , Aditya   Potukuchi , Rajat Kulshreshtha , Shruti Rijhwani ,   and Ashu Ravichander for feedback on the crowd-   sourcing task , and to Noah Smith and Jena Hwang   for valuable discussions related to this project . The   authors would also like to thank all members of the   AllenNLP team and the three anonymous reviewers   for their valuable feedback .   References873887398740874187428743A Extended Comparison to Prior   Negation Datasets   In this section , we complement the discussion in § 7   on how C QAdiffers from existing datasets   focused on negation . A detailed comparison is   given in Table 5 .   Our goal with constructing C QA is to   contribute a high - quality and systematic evalua-   tion that will facilitate future models that can ad-   equately process negation . To this end , we aim   to construct a benchmark where artifacts are care-   fully mitigated ( Gardner et al . , 2020 ) , that is large   enough to support robust evaluation , and that cov-   ers competencies any NLU system needs for ad-   equate processing of negation . For example , the   ability to recognize the implications of negated   statements , distinguish them from their affirmative   counterparts , and identify their scope . As such ,   main properties that C QAhas compared to   prior datasets focused on negation are :   1.It is the first English reading - comprehension   dataset that targets how models process   negated statements in paragraphs ( Gardner   et al . , 2019b ) .   2.It features three types of contrastive inputs   to test a model ’s sensitivity to the presence   of negation , its exact scope , and the way it   is phrased . As such , it is the first contrastive   dataset for studying negation .   3.It is substantially larger in size to facilitate   robust evaluation .   4.It contains diverse forms of negation . Prior   work constructing negation - based challenge   sets for NLI models have largely constructed   instances by using ‘ not ’ as the only negation   cue ( Hossain et al . , 2020c ; Naik et al . , 2018 ) .   Hartmann et al . ( 2021 ) extend this and include   66 English negation cues in their NLI chal-   lenge set . Our dataset consists of over 200   negation cues . Figures 3a and 3b illustrate the   distribution of negation cues in the dataset by   Hartmann et al . ( 2021 ) and C QA , re-   spectively . C QAis less skewed toward   a few negation cues such as “ not ” , “ never ” ,   “ no ” , etc .   5.All examples are manually constructed by   well - trained crowdworkers rather than by us-   ing rules and templates .   6.It includes a rigorous validation procedure by   several crowdworkers to mitigate examples   being incorrect or ambiguous .   B Analysis of C QA   Commonsense Inferences We provide a catego-   rization of the types of commonsense inferences   required to answer C QAquestions . These   categories are presented in Table 12 .   Edit Strategies We provide a set of edit strate-   gies that were employed by crowdworkers to make   paraphrase and scope edits . These edits are given   in Table 13 .   Question / Passage Overlap An issue with some   NLU datasets is that simple heuristics based on8744   lexical overlap are sufficient to achieve high per-   formance ( Weissenborn et al . , 2017 ; Naik et al . ,   2018 ) . We measure the lexical overlap between   C QAquestions and passages and find that is   considerably lower than many prior QA datasets .   Specifically , the average overlap between questions   words and passage words is 0.52 , which is lower   compared to SQuAD 1.0 ( Rajpurkar et al . , 2016 )   ( 0.63 ) , SQuAD 2.0 ( Rajpurkar et al . , 2018 ) ( 0.63 ) ,   RACE ( Lai et al . , 2017 ) ( 0.67 ) , and Quoref ( Dasigi   et al . , 2019 ) ( 0.72 ) .   Distribution of grammatical categories of nega-   tion cues We analyze the distribution over gram-   matical categories for single - word negation cues in   C QA . We use the NLTK library ( Bird et al . ,   2009 ) to identify part - of - speech tags for these cues .   These results are shown in Figure 4 .   Model sensitivity to edits One potential issue   with the dataset may be that models find it trivial to   distinguish between edited passages and leverage   this information to answer questions . To evalu-   ate whether models can easily distinguish between   the original passages and edited versions , we train   BERT ( Devlin et al . , 2019 ) on the task of identify-   ing whether a passage is sourced from Wikipedia   or is an edited passage produced by a crowdworker .   We expect it should be simple for these models to   distinguish between the Wikipedia passages and   the affirmative edits , as the model can simply rely   on the presence or absence of a negation cue . We   observe that as expected , models are somewhat   able to distinguish the original Wikipedia passages   from affirmative edits , but are largely unable to   discriminate between the original passage and the   paraphrase and scope edits ( Table 6 ) .   Naturalness of edits New edits made by crowd-   workers may contain unnatural sentences or linguis-   tic constructs . To quantify this and to exclude the   possibility that model performance degrades only   due to the unnaturalness of the edits , we compare   the perplexity assigned by the OpenAI - GPT lan-8745   guage model ( Radford et al . , 2018 ) to the edited   passages and the original Wikipedia passages , find-   ing that they are largely similar ( Table 7 ) .   Consistency Groups We provide data statistics   on the instances that are used to compute consis-   tency metrics on the dataset . There are 5,608 in-   stances in the dataset that are included in consis-   tency groups , and thus there are 1,402 “ groups ”   to compute question - level consistency . and each   edit - level consistency metric .   C Model Training Details   All models we evaluate on C QA are pre-   trained transformer - based language models . We   test them in three training settings : ( ii ) finetuned   on the entire training data ( § C.1 ) , ( ii ) finetuned on   a few examples ( few - shot ; § C.2 ) , and ( iii ) without   training ( zero - shot ; § C.2 ) .   C.1 Fully Finetuned   We train all fully - finetuned model with five seeds   and report the average performance across them .   For every seed , we evaluate the model with the best   validation accuracy on the entire test set .   BERT ( Devlin et al . , 2019 ) BERT is pretrained   with masked language modeling ( MLM ) and a next-   sentence prediction objective . Since a majority   of the questions have Yes / No / Don’t know as the   answer , we finetune BERT and other BERT - like   models ( see below ) in a multi - class classification   setting . We train all BERT - like models in this fash-   ion . In our experiments , we BERT - Large . We train   with a learning rate of 1e-5 for 10 epochs . RoBERTa ( Liu et al . , 2019 ) RoBERTa is a more   robustly pretrained version of BERT . In our experi-   ments , we use RoBERTa - Large .   DeBERTa ( He et al . , 2021b , a ) DeBERTa has   a disentangled attention mechanism and it is pre-   trained with a version of MLM objective that uses   the content and position of the context words . In   our experiments , we use DeBERTa - v2 - XLarge and   DeBERTa - v3 - Large .   UnifiedQA ( Khashabi et al . , 2020 , 2022 ) Uni-   fiedQA is built on top of the T5 architecture ( Raffel   et al . , 2020 ) by further training it on 20 QA datasets .   We use UnifiedQA - v2 and finetune it with a learn-   ing rate of 5e-5 for 5 epochs . In the fully - finetuned   setting , we study Base , Large , and 3B versions of   UnifiedQA - v2 .   C.2 Few - shot and Zero - Shot   Unlike fully - finetuned models , we evaluate few-   and zero - shot models on 5 train - test splits due to   the cost of the OpenAI API . Evaluation on multiple   disjoint splits of test data ( that in union form the   entire test set ) with different choices of shots allows   us to consider in our evaluation the sensitivity of   few - shot learning to the choice of few examples . If   the cost was not a concern , we would use five sets   of few training examples and the entire test set .   GPT-3 ( davinci ; Brown et al . , 2020 ) This is the   original GPT-3 model trained using only the stan-   dard LM objective . Its maximum input sequence   length is ∼2 K tokens which allows to fit on aver-   age 8–9 C QAtraining examples . Thus , we   use this number of shots for few - shot experiments .   To benchmark GPT models , we use the OpenAI   API ( in October 2022 ) . We show one prompt for   few - shot GPT models in Fig . 5 .   /agcInstructGPT ( text - davinci-002 ; Ouyang   et al . , 2022 ) This GPT variant does not come   with a corresponding paper and little is known   about it . It has recently been confirmed that   it is an Instruct model , but unlike the origi-   nal InstructGPT(text - davinci-001 ; Ouyang   et al . , 2022 ) it is not derived from GPT-3   ( davinci ) .InstructGPThas been trained on   the data that includes “ prompts submitted to earlier   versions of the InstructGPT models on the OpenAI   API Playground ” . InstructGPTis finetuned with8746   reinforcement learning from human feedback ( Sti-   ennon et al . , 2020 ) . text - davinci-002 has two   times longer maximum input sequence length than   davinci suggesting that the overall model size is   notably larger too . This also means we can fit more   examples in the context , but we do not find that to   improve text - davinci-002 ’s performance ; see   Table 9 . It has been reported on social media   thattext - davinci-002 has notably stronger per-   formance than text - davinci-001 , but where do   these improvements come from is publicly un-   known .   Chain - of - Thoughts ( CoT ) prompting ( Wei et al . ,   2022 ) This type of prompting makes the model   explain its prediction before providing it . When   it was introduced , CoT prompting demonstrated   benefits for math and commonsense reasoning .   Since then , Suzgun et al . ( 2022 ) report that CoT   prompting gives substantial improvements for a   hard subset of the BIG - Bench tasks ( Srivastava   et al . , 2022).This makes it a promising prompt   for our proposed task of reasoning about implica-   tions of negation . The suggested way to conduct   CoT prompting ( and how we use it in this paper ) is   as follows :   •Input : { task_description }   { task_examples } { test_instance } An-   swer : Let ’s think step by step .   •Output : { explanation } So the answer is{answer }   One of the authors wrote explanations for all shots   in each split ( 45 explanations in total ) in few hours .   In Figure 6 , we show an example of a CoT prompt   we use for “ InstructGPT ” ( text - davinci-002 ) .   FLAN - T5 ( Chung et al . , 2022 ) FLAN - T5 is a   T5 variant that is further trained with instruction   finetuning that includes CoT prompting , on over   1.8 K tasks . We prompt FLAN - T5 in the zero - shot   setting by constructing each test instance as fol-   lows :   •Input : Passage : { passage } \nQuestion :   { question } \nGive the rationale before an-   swering .   •Output : { explanation } So the answer is   { answer } .   This output form is the most common , but the   model sometimes generates “ ( final ) answer is ” , “ ( fi-   nal ) answer : ” , etc . , instead of “ So the answer is ” .   UnifiedQA - v2 ( Khashabi et al . , 2022 ) We also   evaluate UnifiedQA - v2 in a few- and zero - shot set-   tings in addition to fully training it . We construct   instances following how they are constructed for   training UnifiedQA - v2 :   •Input : { passage } \n{question }   •Output : { answer }   We normalize and lowercase passages , questions ,   and answers . We manually choose hyperparame-   ters following Bragg et al . ( 2021 ) and keep them   fixed .   Which few examples to select ? C QA ’s   unique structure raises the question of which 8–98747   examples to use for few - shot learning :   1 . Randomly selected ,   2.Random without affirmative paragraphs to in-   clude more paragraphs with negation cues ,   3.Two groups of two questions and correspond-   ing 4 paragraphs ( original and three edited ) ,   4.Three groups of two questions and corre-   sponding 3 paragraphs ( original , scope- and   paraphrase - edited ; no affirmative ) .   We hypothesize that the last two options could be   beneficial for consistency of few - shot models . We   prompt davinci with 1st and 3rd options , and de-   pending which is better we evaluate 2nd or 4th   ( i.e. , the better option without affirmative para-   graphs ) . Contrary to our expectations , we find that   the 1st option works better than 3rd , as well as bet-   ter than the 2nd option ; see Table 8 . Therefore , for   each training split , we sample 9 paragraph - question   pairs randomly ( sometimes only 8 fit in the context )   and use these samples for all few - shot experiments .   D Model analysis   Model performance stratified by passage type   In Table 10 , we report the accuracy of model pre-   dictions corresponding to the type of passage : i.e   whether the question was asked on the original   Wikipedia passage , its paraphrase edit , its scope   edit or the affirmative edit . When we compare   those results with those in Table 4 , we observe   that UnifiedQA - v2 shows largely similar QA per-   formance in terms of accuracy on these different   passage types , despite having very different consis-   tency scores with the original passage . In contrast ,   GPT-3 and /agcInstruct - GPT in the few - shot setting   perform better on the original Wikipedia passages   and their paraphrased versions than on the scope   and affirmative edits , possibly suggesting that these   models work best on passages that are available on-   line .   Model performance by question length In Fig-   ure 7 , we show model performance stratified by   question length . We observe that longer questions   are more difficult for U QA-2 - L but   U QA-2 - 3B appears to exhibit similar QA   performance on some of these long questions .   Model performance by answer type In Figure   8 , we show results of model performance stratified8748   by answer type ( Figure 8) .   Variance in model performance We report the   standard deviation of UnifiedQA - V2 models com-   puted over the results from five seeds , as well as the   standard deviation of GPT-3 and /agcInstruct - GPT in   few - shot and zero - shot settings computed over five   splits . These are shown in Table 11 .   Novelty of negation cues We compare the perfor-   mance of fully - finetuned UnifiedQA - v2 Large/3B   on Wikipedia passages where the negation cue has   occurred in the training data , with the performance   for novel negation cues . We find that model accu-   racy for UnifiedQA - V2 - Large is 68.03 when the   negation cue is unseen ( has not been the cue in   the negated statement that crowdworkers construct   questions around in the training data ) , and 70.45   when it has appeared before in the training data .   Similarly , UnifiedQA - V2 - 3B ’s accuracy is 74.38   and 73.73 for unseen and seen cues respectively .   This suggests that the novelty of the negation cue   is not a major factor of difficulty for UnifiedQA - v2   once it has been finetuned on the entire training8749data .   E Crowdsourcing Interface Templates   We include an example of the annotation interface   we showed to crowdworkers . Figure 9 shows a   sample of each stage of our task.875087518752875387548755