  Carl Edwards * , Tuan Lai * , Kevin Ros , Garrett Honke , Kyunghyun Cho , Heng JiUniversity of Illinois Urbana - ChampaignX , the Moonshot FactoryNew York University , Genentech   { cne2 , tuanml2 , kjros2 , hengji}@illinois.edu   ghonk@google.com , kyunghyun.cho@nyu.edu   Abstract   We present MolT5 – a self - supervised learn-   ing framework for pretraining models on a   vast amount of unlabeled natural language   text and molecule strings . MolT5 allows for   new , useful , and challenging analogs of tradi-   tional vision - language tasks , such as molecule   captioning and text - based de novo molecule   generation ( altogether : translation between   molecules and language ) , which we explore for   the first time . Since MolT5 pretrains models on   single - modal data , it helps overcome the chem-   istry domain shortcoming of data scarcity . Fur-   thermore , we consider several metrics , includ-   ing a new cross - modal embedding - based met-   ric , to evaluate the tasks of molecule caption-   ing and text - based molecule generation . Our   results show that MolT5 -based models are able   to generate outputs , both molecules and cap-   tions , which in many cases are high quality .   1 Introduction   Imagine a future where a doctor can write a few   sentences describing a specialized drug for treating   a patient and then receive the exact structure of   the desired drug . Although this seems like science   fiction now , with progress in integrating natural   language and molecules , it might well be possi-   ble in the future . Historically , drug creation has   commonly been done by humans who design and   build individual molecules . In fact , bringing a new   drug to market can cost over a billion dollars and   take over ten years ( Gaudelet et al . , 2021 ) . Re-   cently , there has been considerable interest in us-   ing new deep learning tools to facilitate in silico   drug design – a field often called cheminformatics   ( Rifaioglu et al . , 2018 ) . Yet , many of these experi-   ments still focus on molecules and their low - level   properties such as logP ( the octanol - water parti-   tion coefficient ) ( Bagal et al . , 2021 ) . In the future , Figure 1 : An example output from our model for the   molecule generation task . The left is the ground truth ,   and the right is a molecule generated from the given   natural language caption .   we foresee a need for a higher - level control over   molecule design , which can easily be facilitated by   natural language .   In this work , we pursue an ambitious goal of   translating between molecules and language by   proposing two new tasks : molecule captioning   and text - guided de novo molecule generation . In   molecule captioning , we take a molecule ( e.g. , as   a SMILES string ) and generate a caption that de-   scribes it ( Figure 2 ) . In text - guided molecule gener-   ation , the task is to create a molecule that matches   a given natural language description ( Figure 1 ) .   These new tasks would help to accelerate research   in multiple scientific domains by enabling chem-   istry domain experts to generate new molecules   and better understand them using natural language .   While our proposed molecule - language tasks   share some similarities with vision - language tasks ,   they have several inherent difficulties that separate   them from existing vision - language analogs : 1 )   creating annotations for molecules requires signif-   icant domain expertise , 2 ) thus , it is significantly   more difficult to acquire large numbers of molecule-375   description pairs , 3 ) the same molecule can have   many functions and thus be described in very dif-   ferent ways , which causes 4 ) existing evaluation   measures based on reference descriptions , such as   BLEU , to fail to adequately evaluate these tasks .   To address the issue of data scarcity ( i.e. , diffi-   culties 1 and 2 ) , we propose a new self - supervised   learning framework named MolT5 ( Molecular T5 )   that is inspired by the recent progress in pretrain-   ing multilingual models ( Devlin et al . , 2019 ; Liu   et al . , 2020 ) . MolT5 first pretrains a model on a   vast amount of unlabeled natural language text and   molecule strings using a simple denoising objec-   tive . After that , the pretrained model is finetuned   on limited gold standard annotations . Furthermore ,   to adequately evaluate models for molecule cap-   tioning or generation , we consider various kinds   of metrics and also adopt a new metric based on   Text2Mol ( Edwards et al . , 2021 ) . We repurpose   this retrieval model for assessing the similarity be-   tween the ground truth molecule / description and   the generated description / molecule , respectively .   To the best of our knowledge , there is no work   yet on molecule captioning or text - guided molecule   generation . The closest existing work to molecule   captioning falls within the scope of image caption-   ing ( Vinyals et al . , 2015 ) . However , molecule cap-   tioning is arguably much more challenging due to   the increased linguistic variety in possible captions   ( Figure 2 ) . A molecule could be described with an   IUPAC name , with one of many different synthetic   routes from known precursor molecules , in termsof the properties ( e.g. carcinogenic or lipophilic ) ,   with the applications of the molecule ( e.g. a dye ,   an antipneumonic , or an antifungal ) , or in terms of   its functional groups ( e.g. “ substituted by hydroxy   groups at positions 5 and 7 and a methyl group at   position 8 ” ) , among other methods .   In summary , our main contributions are :   1.We propose two new tasks : 1 ) molecule cap-   tioning , where a description is generated for   a given molecule , and 2 ) text - based de novo   molecule generation , where a molecule is gen-   erated to match a given text description .   2.We consider multiple evaluation metrics for   these new tasks , and we adopt a new cross-   modal retrieval similarity metric based on   Text2Mol ( Edwards et al . , 2021 ) .   3.We propose MolT5 : a self - supervised learn-   ing framework for jointly training a model on   molecule string representations and natural   language text , which can then be finetuned on   a cross - modal task .   2 Tasks   With the ambitious goal of bi - directional translation   between molecules and language , we propose two   new novel tasks : molecule captioning ( Section 2.1 )   and text - based molecule generation ( Section 2.2 ) .   2.1 Molecule Captioning   For any given molecule , the goal of molecule cap-   tioning is to describe the molecule and what it does .   An example is shown in Figure 2 . Molecules are376often represented as SMILES strings ( Weininger ,   1988 ; Weininger et al . , 1989 ) , a linearization of   the molecular graph which can be interpreted as   a language for molecules . Thus , this task can be   considered an exotic translation task , and sequence   to sequence models serve as excellent baselines .   2.2 Text - Based de Novo Molecule Generation   The goal of the de novo molecule generation task   is to train a model which can generate a variety   of possible new molecules . Existing work tends   to focus on evaluating the model coverage of the   chemical space ( Polykovskiy et al . , 2020 ) . Instead ,   we propose generating molecules based on a nat-   ural language description of the desired molecule –   this is essentially swapping the input and output   for the captioning task . An example of this task is   shown in Figure 1 . Recent work , such as DALL · E   ( Ramesh et al . , 2021 , 2022 ) , which generates im-   ages from text , has shown the ability to seamlessly   integrate multiple properties , such as chairs and   avocados , in an image . This points towards similar   applications in the molecule generation domain via   the usage of natural language .   3 Evaluation Metrics   3.1 Text2Mol Metric   Since we are considering new cross - modal tasks   between molecules and text , we also introduce a   new cross - modal evaluation metric . This is based   on Text2Mol ( Edwards et al . , 2021 ) , which aims to   train a retrieval model to rank molecules given their   text descriptions . Since the ranking function uses   cosine similarity between embeddings , a trained   model can be repurposed for evaluating the similar-   ity between the ground truth molecule / description   and the generated description / molecule ( respec-   tively ) . To this end , we first train a base multi - layer   perceptron ( MLP ) model from Text2Mol . This   model is then used to generate similarities of the   candidate molecule - description pairs , which can be   compared to the average similarity of the ground   truth molecule - description pairs . We also note that   negative molecule - description pairs have an aver-   age similarity of roughly zero .   3.2 Evaluating Molecule Captioning   Traditionally , captioning tasks have been evaluated   by natural language generation metrics such as   BLEU ( Papineni et al . , 2002 ) , ROUGE ( Lin , 2004 ) ,   and METEOR ( Banerjee and Lavie , 2005 ) . Un - like captioning tasks such as COCO ( Chen et al . ,   2015 ) , which has several captions per image , in   our task we only have one reference caption . This   makes these metrics less effective , especially be-   cause there are many non - overlapping ways to   describe a molecule . Nevertheless , for compari-   son , we still report these scores ( e.g. , aggregated   sentence - level METEOR scores ) .   3.3 Evaluating Text - Based de Novo Molecule   Generation   Considerable interest has grown in applying deep   generative models to de novo molecule generation .   Because of this , a number of metrics have been   proposed , such as novelty and scaffold similarity   ( Polykovskiy et al . , 2020 ) . However , many of these   metrics do not apply to our problem – we want   our generated molecule to match the input text   instead of being generally diverse . Instead , we   consider metrics which measure the distance of   the generated molecule to either the ground truth   molecule or the ground truth description , such as   our proposed Text2Mol - based metric .   We employ three fingerprint metrics : MACCS   FTS , RDK FTS , and Morgan FTS , where FTS   stands for fingerprint Tanimoto similarity ( Tani-   moto , 1958 ) . MACCS ( Durant et al . , 2002 ) , RDK   ( Schneider et al . , 2015 ) , and Morgan ( Rogers and   Hahn , 2010 ) are each fingerprinting methods for   molecules . The fingerprints of two molecules are   compared using Tanimoto similarity ( also known   as Jaccard index ) , and the average similarity over   the evaluation dataset is reported . See ( Campos   and Ji , 2021 ) for more details . We also report ex-   act SMILES string matches , Levenshtein distance   ( Miller et al . , 2009 ) , and SMILES BLEU scores .   Preuer et al . ( 2018 ) propose Fréchet ChemNet   Distance ( FCD ) , which is inspired by the Fréchet   Inception Distance ( FID ) ( Heusel et al . , 2017 ) .   FCD is based on the penultimate layer of a network   called “ ChemNet ” , which was trained to predict   the activity of drug molecules . Thus , FCD takes   into account chemical and biological information   about molecules in order to compare them . This al-   lows molecules to be compared based on the latent   information required to predict useful properties   rather than a string - based metric .   In the case of models which use SMILES strings ,   generated molecules can be syntactically invalid .   Therefore , we also report validity as the percent   of molecules which can be processed by RDKIT377   ( Landrum , 2021 ) as in ( Polykovskiy et al . , 2020 ) .   4 MolT5 – Multimodal Text - Molecule   Representation Model   We can crawl a massive amount of text from the   Internet . For example , Raffel et al . ( 2020 ) built a   Common Crawl - based dataset that contains over   700 GB of reasonably clean and natural English   text . On the other hand , over a billion molecules   are also available from public databases such as   ZINC-15 ( Sterling and Irwin , 2015a ) . Inspired   by the progress in large - scale pretraining ( Ramesh   et al . , 2021 ) , we propose a new self - supervised   learning framework named MolT5 ( Molecular T5 )   to leverage the vast amount of unlabeled natural   language text and molecule strings .   Figure 3 shows an overview of MolT5 . We first   initialize an encoder - decoder Transformer model   ( Vaswani et al . , 2017 ) using one of the public check-   points of T5.1.1 , an improved version of T5 ( Raf - fel et al . , 2020 ) . After that , we pretrain the model   using the “ replace corrupted spans ” objective ( Raf-   fel et al . , 2020 ) . More specifically , during each   pretraining step , we sample a minibatch compris-   ing both natural language sequences and SMILES   sequences . For each sequence , some words in the   sequence are randomly chosen for corruption . Each   consecutive span of corrupted tokens is replaced by   a sentinel token ( shown as [ X ] and [ Y ] in Figure 3 ) .   Then the task is to predict the dropped - out spans .   Molecules ( e.g. represented as SMILES strings )   can be thought of as a language with a very unique   grammar . Then , intuitively , our pretraining stage   essentially trains a single language model on two   monolingual corpora from two different languages ,   and there is no explicit alignment between the two   corpora . This approach is similar to how some mul-   tilingual language models such as mBERT ( Devlin   et al . , 2019 ) and mBART ( Liu et al . , 2020 ) were pre-   trained . As models such as mBERT demonstrate ex-378cellent cross - lingual capabilities ( Pires et al . , 2019 ) ,   we also expect models pretrained using MolT5 to   be useful for text - molecule translation tasks .   After the pretraining process , we can finetune   the pretrained model for either molecule caption-   ing or generation ( depicted by the bottom half of   Figure 3 ) . In molecule generation , the input is a   description , and the output is the SMILES repre-   sentation of the target molecule . On the other hand ,   in molecule captioning , the input is the SMILES   string of some molecule , and the output is a caption   describing the input molecule .   5 Experiments and Results   5.1 Data   Pretraining Data As described in Section 4 , the   pretraining stage of MolT5 requires two monolin-   gual corpora : one consisting of natural language   text and the other consisting of molecule representa-   tions . We use the “ Colossal Clean Crawled Corpus ”   ( C4 ) ( Raffel et al . , 2020 ) as the pretraining dataset   for the textual modality . For the molecular modal-   ity , we directly utilize the 100 million SMILES   strings used in Chemformer ( Irwin et al . , 2021 ) .   As these strings were selected from the ZINC-15   dataset ( Sterling and Irwin , 2015b ) , we refer to this   pretraining dataset as ZINC from this point .   Finetuning and Evaluation Data We use   ChEBI-20 ( Edwards et al . , 2021 ) as our gold stan-   dard dataset for finetuning and evaluation . It con-   sists of 33,010 molecule - description pairs , which   are separated into 80/10/10 % train / validation / test   splits . We use ChEBI-20 to finetune MolT5 - based   models and to train baseline models . Many cap-   tions in ChEBI-20 contain a name for the molecule   at the start of the string ( e.g. , “ Rostratin D is an   organic disulfide isolated from ... ” ) . To force the   models to focus on the semantics of the descrip-   tion , we replace the molecule ’s name with " The   molecule is [ ... ] " ( e.g. , “ The molecule is an organic   disulfide isolated from ... ” ) .   5.2 Baselines   Any sequence - to - sequence model is applicable to   our new tasks ( i.e. , molecule captioning and gener-   ation ) . We implement the following baselines :   1.RNN - GRU ( Cho et al . , 2014 ) . We implement   a 4 - layer GRU recurrent neural network . The   encoder is bidirectional.2.Transformer ( Vaswani et al . , 2017 ) . We train   a vanilla Transformer model consisting of six   encoder and decoder layers .   3.T5(Raffel et al . , 2020 ) . We experiment with   three public T5.1.1 checkpoints : small , base ,   and large . We finetune each checkpoint for   molecule captioning or molecule generation   using the t5x framework ( Roberts et al . , 2022 ) .   We train the baseline models on ChEBI-20 us-   ing SMILES representations for the molecules .   Molecule captioning and generation are trained   with molecules as input / output and text as out-   put / input . More information about the baselines   and the hyperparameters is in the appendix .   5.3 Pretraining Process   We first initialize an encoder - decoder Transformer   model using a public checkpoint of T5.1.1 ( either   t5.1.1.small , t5.1.1.base , ort5.1.1.large ) . We then   pretrain the model on the combined dataset of C4   and ZINC ( i.e. , C4+ZINC ) for 1 million steps .   Each step uses a batch size of 256 evenly split   between text and molecule sequences . After this ,   we finetune the pretrained model on ChEBI-20 for   either molecule captioning or generation . The num-   ber of finetuning steps is 50,000 .   5.4 Molecule Captioning   Table 1 shows the overall molecule captioning re-   sults . The pretrained models , either T5 or MolT5 ,   are considerably better at generating realistic lan-   guage to describe a molecule than the RNN and   Transformer baselines . The RNN is more capable   of extracting relevant properties from molecules   than the Transformer , but it generally produces un-   grammatical outputs . On the other hand , the Trans-   former produces grammatical outputs , but they tend   to repeat the same properties , such as carcinogenic ,   regardless of whether they apply . For this reason ,   the Text2Mol scores are much lower for the Trans-   former model , since its outputs match the given   molecule much less frequently . We speculate that   the ChEBI-20 dataset is too small to effectively   train a Transformer without large - scale pretraining .   We find that our additional pretraining of MolT5   results in a reasonable increase over T5 in cap-   tioning performance on both the traditional NLG   metrics and our Text2Mol metric for each model   size . Finally , we refer the reader to Section H in379   the appendix for information about the statistical   significance of our results .   Several examples of different models ’ outputs   are shown in Figure 4 and Appendix Figure 9 . In   ( 1 ) , MolT5 ’s description matches best , identify-   ing the molecule as a “ GDP - L - galactose ” . MolT5   is usually able to recognize what general class   of molecule it is looking at ( e.g. cyclohexanone ,   maleate salt , etc . ) . In general , all models often look   for the closest compound they know and base their   caption on that . The argon atom , example ( 2 ) with   SMILES ‘ [ 39Ar ] ’ , is not present in the training   dataset bonded to any other atoms ( likely because   it is an inert noble gas ) . All models recognize that   ( 2 ) is a single atom , but they are unable to describe   it . In ( 3 ) , the models try to caption a histological   dye . MolT5 captions the molecule as an azure his-   tological dye , which is very close to the ground   truth “ brilliant cresyl blue ” , while T5 does not.5.5 Text - Based de novo Molecule Generation   In the molecule generation task , the pretrained   models also perform much better than the RNN   and Transformer ( Table 2 ) . Although it is well   known that scaling model size and pretraining data   leads to significant performance increases ( Kaplan   et al . , 2020 ) , it was still surprising to see the results .   For example , a default T5 model , which was only   pretrained on text data , is capable of generating   molecules which are much closer to the ground   truth than the RNN and which are often valid . This   trend also persists as language model size scales ,   since T5 - large with 770 M parameters outperforms   the specifically pretrained MolT5 - small with 60 M   parameters . Still , the pretraining in MolT5 slightly   improves some molecule generation results , with   especially large gains in validity . Finally , Section H   in the appendix has information about the statistical   significance of our results .   We show results for the models in Figure 5 and380   also in Figures 6 , 7 , and 8 in Appendix F , which   we number by input description . Compared to T5 ,   MolT5 is better able to understand instructions for   manipulating molecules , as shown in examples ( 3 ,   4 , 6 , 7 , 16 , 18 , 21 ) . In many cases , MolT5 obtains   exact matches with the ground truth ( 2 , 3 , 4 , 6 , 7 , 8 ,   10 , 12 , 17 , 20 , 21 ) . ( 3 ) is an interesting case , since it   shows that MolT5 can understand crystalline solids   like hydrates . ( 2 ) is another interesting example ;   it is the longest SMILES string , at 474 characters ,   which MolT5 is able to generate an exact match   for . MolT5 understands peptides and can produce   them from descriptions ( 2,15,17 ) . It also shows this   ability for saccharides ( 6 , 21 ) and enzymes ( 8,20 ) .   MolT5 is able to understand rare atoms such as   Ruthenium ( 5 ) . However , in this case it still misses   the atom ’s charge . Some example descriptions ,   such as ( 1 ) , lack details so the molecules generated   by MolT5 may be interesting to investigate.5.6 Probing the Model   We conduct probing tests on the model for certain   input properties , which are shown in Appendix J.   Often , the model will generate molecules that it   knows matches the input description from the fine-   tuning data . It also creates solutions from these   as well by adding various ions ( e.g. " .[Na+ ] " ) . In   some cases , it generates molecules not appearing in   finetuning data ( sometimes successfully sometimes   not ) . For example , given the input “ The molecule   is a corticosteroid . ” , the first molecule generated is   a well known corticosteroid called corticosterone .   The fifth molecule generated is not present in the   PubChem database . Based on a structure similarity   search , it is most closely related to the androgenic   steroid Fluoxymesterone and the corticosteroid Hy-   drocortisone.3816 Related Work   6.1 Multimedia Representation   Much recent work on multimedia representations   falls into training large vision - language models ( Su   et al . , 2020 ; Lu et al . , 2019 ; Chen et al . , 2020 ) .   CLIP ( Radford et al . , 2021 ) trains a zero - shot   image classifier by using natural language labels   which can be easily extended . A modification of   CLIP ’s contrastive loss function , which follows   ( Sohn , 2016 ) , is applied by Text2Mol ( Edwards   et al . , 2021 ) for cross - modal retrieval between   molecule and text pairs . Edwards et al . ( 2021 )   also released the ChEBI-20 dataset of molecule-   description pairs , which is used for training and   evaluation in this paper . Vall et al . ( 2021 ) lever-   age a contrastive loss between bioassay descrip-   tions and molecules to predict activity between   the two . Sun et al . ( 2021 ) uses cross - modal at-   tention with molecule structures to improve chem-   ical entity typing . Zeng et al . ( 2022 ) pretrain a   language model to learn a joint representation be-   tween molecules and biomedical text via entity   linking which they use for tasks such as relation   extraction , molecule property prediction , and cross-   modal retrieval like Text2Mol . Unlike our work ,   they do not explore generating text nor molecules .   Vaucher et al . ( 2020 ) create a dataset of chemical   equations and associated action sequences in natu-   ral language . Vaucher et al . ( 2021 ) then leverage   this dataset to train a BART model which can plan   chemical reaction steps . Their natural language   generation is constrained to the specific reaction   steps in their dataset – the main purpose of their   model is to create the steps for a reaction rather   than describing molecules .   6.2 Image Captioning and Text - Guided Image   Generation   Image captioning has been studied extensively ( Pan   et al . , 2004 ; Lu et al . , 2018 ; Hossain et al . , 2019 ;   Stefanini et al . , 2021 ) . Many recent studies tend   to pretrain Transformer - based models on massive   text - image corpora ( Li et al . , 2020 ; Hu et al . , 2022 ) .   Work has also been done in the biomedical domain   ( Pavlopoulos et al . , 2019 ) , a close cousin of the   chemistry domain , where tasks tend to be focused   on diagnosis of various image types such as x - rays   ( Demner - Fushman et al . , 2016 ) .   The reverse problem , text - guided image gener-   ation , has proven considerably more challenging   ( Khan et al . , 2021 ) . Several attempts have usedGAN - based methods ( Reed et al . , 2016 ; Zhang   et al . , 2017 ; Xu et al . , 2018 ) . Recent work has   shown remarkable results . DALL · E ( Ramesh et al . ,   2021 , 2022 ) can seamlessly fuse multiple concepts   together to generate a realistic image .   6.3 Molecule Representation   Molecule representation has been a long - standing   problem in the field of cheminformatics . Tradi-   tionally , fingerprinting methods have been a pre-   ferred technique to featurize molecule structural   representations ( Rogers and Hahn , 2010 ; Cereto-   Massagué et al . , 2015 ) . These approaches do not   allow representations to be learned from data . In re-   cent years , advances in machine learning and NLP   have been applied to this problem . A popular in-   put for these algorithms has been SMILES strings   ( Weininger , 1988 ; Weininger et al . , 1989 ) , which   are a computer - readable linearization of molecule   graphs . Jaeger et al . ( 2018 ) use the Morgan fin-   gerprinting algorithm to convert each molecule   into a ‘ sentence ’ of its substructures , to which it   applies the Word2vec algorithm ( Mikolov et al . ,   2013a , b ) . Duvenaud et al . ( 2015 ) use neural meth-   ods to learn fingerprints . Other advances such as   BERT ( Devlin et al . , 2019 ) have also been ap-   plied to the domain , such as MolBERT ( Fabian   et al . , 2020 ) and ChemBERTa ( Chithrananda et al . ,   2020 ) , which use SMILES strings as inputs to pre-   train a BERT - esque model . Work has been done   to use the molecule graph structure and known re-   actions for learning representations ( Wang et al . ,   2022 ) . Schwaller et al . ( 2021b ) trains a BERT   model to learn representations of chemical reac-   tions . Schwaller et al . ( 2021a ) leverages unsuper-   vised representation learning with Transformers   to extract an organic chemistry grammar . Unlike   existing work , MolT5 ’s molecule representations   allow for translation between molecules and natural   language .   There has been particular interest in training gen-   erative models for de novo molecule discovery . Ba-   gal et al . ( 2021 ) apply a GPT - style decoder for   this task . Lu and Zhang ( 2022 ) apply a T5 model   to SMILES strings for multitask reaction predic-   tion problems . MegaMolBARTtrains a BART   model on 500 M SMILES strings from the ZINC-   15 dataset ( Sterling and Irwin , 2015b)3827 Conclusions and Future Work   In this work , we propose MolT5 , a self - supervised   learning framework for pretraining models on a   vast amount of unlabeled text and molecule strings .   Furthermore , we propose two new tasks : molecule   captioning and text - guided molecule generation ,   for which we explore various evaluation methods .   Together , these tasks allow for translation between   natural language and molecules . Using MolT5 , we   are able to obtain high scores for both tasks .   8 Broader Impacts   Our proposed model and tasks will have the fol-   lowing broader impacts . 1 ) It will help to democ-   ratize molecular AI , allowing chemistry experts   to take advantage of new AI technologies for dis-   covering new life - changing drugs by interacting in   the natural language , because it is most natural for   humans to provide explanations and requirements   in natural language . 2 ) Text - based molecule gen-   eration enables the ability to generate molecules   with specific functions ( such as taste ) rather than   properties , enabling the next generation of chem-   istry where custom molecules are used for each   application . Specifically - designed molecular solu-   tions have the potential to revolutionize fields such   as medicine and material science . 3 ) Our models ,   whose weights we will release , will allow further   research in the NLP community on the applications   of multimodal text - molecule models .   8.1 Risks   MolT5 , like other large language models , can   potentially be abused . First , there may be bi-   ases learned by the model due to its large - scale   training data . These biases may affect what   type of molecules are generated when the model   is prompted about certain diseases . Thus , any   molecules discovered by usage of MoLT5 should   strictly evaluated by standard clinical processes   before being considered for medicinal use . An-   other risk is that the model may be used to dis-   cover potentially dangerous molecules instead of   beneficial ones . It is difficult to predict what ex-   act molecules may be discovered via usage of our   work . However , while there is this unfortunate po-   tential for misuse of the technology , knowledge   of dangerous molecule ’s existence and structure is   generally not harmful due to the requisite techni-   cal knowledge and laboratory resources required to   synthesize them in any meaningful quantity . Over - all , we believe these downsides are outweighed   by the benefits to the research and pharmaceutical   communities .   9 Limitations   Since this work focuses on a new application for   large language models , many of the same limita-   tions apply here . Namely , the model is trained on   a large dataset collected from the Internet , so it   may contain unintended biases . One limitation of   our model is using SMILES strings – recent work   ( Krenn et al . , 2020 ) proposes a string representa-   tion with validity guarantees . In practice , we found   this to work poorly with pretrained T5 checkpoints   ( which were important from a computational per-   spective ) . We also note that some compounds in   ChEBI-20 can cause validity problems in the de-   fault SELFIES implementation . We leave further   investigation of this to future work . Finally , we   stress that MolT5 was created for research pur-   poses and generated molecules should not be used   for medical purposes without careful evaluation by   standard clinical testing first .   Acknowledgement   We would like to thank Martin Burke for his helpful   discussion . This research is based upon work sup-   ported by the Molecule Maker Lab Institute : an AI   research institute program supported by NSF under   award No . 2019897 and No . 2034562 . The views   and conclusions contained herein are those of the   authors and should not be interpreted as necessarily   representing the official policies , either expressed   or implied , of the U.S. Government . The U.S. Gov-   ernment is authorized to reproduce and distribute   reprints for governmental purposes notwithstand-   ing any copyright annotation therein.383References384385386387A Baselines and Hyperparameters   Any sequence - to - sequence model is applicable to   our new tasks ( i.e. , molecule captioning and gener-   ation ) . We implement the following baselines :   1.RNN - GRU ( Cho et al . , 2014 ) . We implement   a 4 - layer GRU recurrent neural network with   a hidden size of 512 . We use a learning rate   of 1e-4 and a batch size of 128 for molecule   generation . For caption generation , a batch   size of 116 is used . The number of training   epochs is 50 . Additionally , the encoder is   bidirectional . For training , teacher forcing is   used 50 % of the time , and gradient clipping   to 50 is applied .   2.Transformer ( Vaswani et al . , 2017 ) . We train   a vanilla Transformer model consisting of six   encoder and decoder layers . The number of   training epochs is 40 , the batch size is 16 , and   the learning rate is 1e-4 . We use a linear decay   with a warmup of 400 steps .   3.T5(Raffel et al . , 2020 ) . We experiment   with three public T5.1.1 checkpoints : small ,   base , and large . We finetune each checkpoint   for molecule captioning or molecule genera-   tion using the open - sourced t5x framework   ( Roberts et al . , 2022 ) . The number of training   steps is set to be 50,000 . The dropout rate   is set to be 0.0 for the small and base mod-   els , and it is set to be 0.1 for the large model .   For other hyperparameters , we use the default   values provided by the t5x framework .   We train the baseline models on the ChEBI-   20 dataset using SMILES representations for the   molecules . Molecule captioning and generation are   trained with molecules as input / output and text as   output / input . Sequences are limited to 512 tokens   for input and output . During inference , a beam   decoder with a beam size of 5 is used .   On the RNN and vanilla Transformer models ,   we use a character - split vocabulary for SMILES .   For the text vocabulary , we use SciBERT ’s 31,090-   token vocabulary ( Beltagy et al . , 2019 ) .   B Reproducibility Checklist   The programs , trained models , and resources will   be made publicly available . For training the RNN   and Transformer baselines , we use NVIDIA TeslaV100 GPUs . For pretraining and finetuning T5-   related models , we use TPUs .   When testing on a MacBook Pro that has no   access to GPUs , the average inference time of our   MolT5 - Base molecule generation model is 2.24   seconds / query . The average inference time of our   large MolT5 - Base molecule captioning model is   9.86 seconds / query .   C Decoding with Huggingface Model   For ease of adoption , we converted our original   models trained using the t5x framework ( Roberts   et al . , 2022 ) to HuggingFace - based models ( Wolf   et al . , 2019 ) . We will release the converted models   on HuggingFace ( HF ) Hub . Due to implementation   differences , the HF - based models produce slightly   different outputs from the original models . There-   fore , we also report the numbers of the HF - based   models in Table 3 and Table 4 .   D High Validity Molecule Generation   To increase the validity score of the molecule gener-   ation models , we consider a high - validity decoding   strategy . We use diverse beam search ( Vijayakumar   et al . , 2016 ) with a beam width and beam group of   30 and a diversity penalty of 0.5 . Then , we use RD-   Kit ( Landrum , 2021 ) to select the first valid beam .   On rare occasions , the beam size exceeds memory   limitations , so we iteratively reduce the beam size   by 5 for that input and try again . In Table 4 , MolT5-   Small - HV , MolT5 - Base - HV , and MolT5 - Large - HV   denote models that use this decoding process .   E Ablations   We perform ablations on MolT5 - Small pretraining .   For molecule captioning ( Table 5 ) , pretraining on   both C4 and ZINC is clearly more beneficial than   pretraining only on C4 or only on ZINC .   For molecule generation , at first glance , pretrain-   ing on C4+ZINC seems not to outperform pretrain-   ing only on C4 ( Table 6 ) . However , note that except   for BLEU , Exact , Levenshtein , and Validity , other   metrics in Table 6 are computed using only syn-   tactically valid molecules . Table 7 shows the nor-   malized molecule generation results . After normal-   ization , we see that pretraining on C4+ZINC out-   performs pretraining only on C4 or only on ZINC   according to most metrics . Finally , pretraining only   on ZINC increases the validity score substantially .   However , this leads to decreased similarity of the   generated molecules to the ground truths.388   F More Examples389390391392393 G Testing Model Diversity with Retrieval   To test the diversity of generations , we apply   a Text2Mol ( Edwards et al . , 2021 ) cross - modal   retrieval model to the entire generated set of   molecules or descriptions . In the case of molecules ,   we first take the molecules generated for our test   set . We consider these molecules as our corpus and   then use the descriptions ( which were used to gen-   erate the molecules in the first place ) as our queries .   So , for each query we look at the rank of its gener-   ated molecule ( the highest rank is 1 ) . This process   tests whether the Text2Mol retrieval model can dif-   ferentiate between the generated ( valid ) molecules .   Doing so means it can retrieve a specific molecule   when given the description used to generate it . If   the generative model did not sufficiently take the   descriptions into consideration , then the retrieval   model wo n’t be able to distinguish between gen-   erated molecules and the scores will be very low   ( such as the transformer model , which frequently   generates the same molecule / caption ) .   As an example , consider that we have 10 descrip-   tions of molecules . For each description , we use a generative model   to generate a molecule . Now , we treat these 10   generated molecules as our corpus . Using our re-   trieval model , we now consider each description as   a query and try to retrieve the molecule that was   generated from that description . If the retrieval   model performs poorly , that means the molecules   which were generated are difficult to distinguish   from one another . By using this method with dif-   ferent generative models , we measure the relative   diversity of generated molecules along with how   well the generated molecules match the description .   Results are reported in Tables 8 and 9 for retriev-   ing generated molecules from descriptions and for   retrieving generated descriptions from molecules ,   respectively . We use the same Text2Mol model   for retrieval here as in the Text2Mol metric . For   description of metrics , see ( Edwards et al . , 2021 ) .   Results indicate that MolT5 model generations are   sufficiently distinct to be retrievable . In contrast ,   the outputs of the captioning transformer are essen-   tially indistinguishable for the retrieval model.394H Statistical Significance   To strengthen the quantitative results , we conducted   statistical tests between T5 - Large and MolT5-   Large . For molecule captioning , we carried out   paired t - tests . The computed p - values and test   statistics are :   •For ROUGE-1 , the p - value is 1.53e-22 . The   test statistic is -9.841 .   •For ROUGE-2 , the p - value is 3.27e-26 . The   test statistic is -10.683 .   •For ROUGE - L , the p - value is 3.58e-21 . The   test statistic is -9.509 .   •For METEOR , the p - value is 2.02e-21 . The   test statistic is -9.57 .   •For Text2Mol , the p - value is 1.053e-29 . The   test statistic is -11.431 .   Note that for every metric above , the higher   the score , the better the performance . Since all   the test statistics are negative and the p - values   are extremely small , MolT5 - Large produces   significant improvements over T5 - Large on the   task of molecule captioning .   For molecule generation , we conducted in-   dependent t - tests to compare between T5 - Large   and MolT5 - Large :   •For MACCS FTS , the p - value is 0.008 . The   test statistic is -2.652 .   •For RDK FTS , the p - value is 0.0092 . The test   statistic is -2.604 .   •For Morgan FTS , the p - value is 0.0153 . The   test statistic is -2.426 .   •For Levenshtein , the p - value is 0.064 . The   test statistic is 1.8544704091978725 .   •For Text2Mol , the p - value is 0.168 . The test   statistic is -1.376724743237994 .   Note that for Levenshtein , the lower the score , the   better the performance . We see that the test statis-   tics for all metrics except Levenshtein is negative .   In addition , while the p - values now are typically   larger than the ones computed for molecule caption-   ing , the p - values for molecule generation are still   reasonably small . Therefore , we can still conclude   that MolT5 - Large also produces significant im-   provements over T5 - Large on the task of molecule   generation .   I NLP Capabilities of MolT5   We finetune our MolT5 - based models on some   GLUE tasks and see similar results for MolT5 and   T5 . For example , our finetuned MolT5 - base modelachieved an accuracy score of 95.6 % on SST-2 . For   comparison , T5 - base achieved a score of 95.2 % .   Since our self - supervised learning framework uses   a large amount of natural language text in addi-   tion to SMILES string , it is reasonable that our   MolT5 - based models still possess “ typical ” NLP   capabilities .   J Model Probing Tests   To generate a variety of output molecules given   a single input , we employ diverse beam search   ( Vijayakumar et al . , 2016 ) with a beam width and   beam group of 30 and a diversity penalty of 0.5 .   The goal of these tests ( shown in the following   figures ) is to explore molecule outputs given very   specific desired properties . Note that these brief   input descriptions are out - of - distribution from the   finetuning data . In the following figures , the top 10   valid molecules are shown for each prompt ( order :   left to right , top to bottom).395396397398399400401402403404405406407408409410411412413