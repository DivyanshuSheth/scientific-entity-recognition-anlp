  Jind ˇrich Helcland Barry Haddowand Alexandra BirchSchool of Informatics , University of EdinburghFaculty of Mathematics and Physics , Charles University   { jhelcl,bhaddow,a.birch}@ed.ac.uk   Abstract   Efficient machine translation models are com-   mercially important as they can increase infer-   ence speeds , and reduce costs and carbon emis-   sions . Recently , there has been much interest   in non - autoregressive ( NAR ) models , which   promise faster translation . In parallel to the   research on NAR models , there have been suc-   cessful attempts to create optimized autore-   gressive models as part of the WMT shared   task on efficient translation . In this paper , we   point out flaws in the evaluation methodology   present in the literature on NAR models and   we provide a fair comparison between a state-   of - the - art NAR model and the autoregressive   submissions to the shared task . We make the   case for consistent evaluation of NAR models ,   and also for the importance of comparing NAR   models with other widely used methods for im-   proving efficiency . We run experiments with   a connectionist - temporal - classification - based   ( CTC ) NAR model implemented in C++ and   compare it with AR models using wall clock   times . Our results show that , although NAR   models are faster on GPUs , with small batch   sizes , they are almost always slower under   more realistic usage conditions . We call for   more realistic and extensive evaluation of NAR   models in future work .   1 Introduction   Non - autoregressive neural machine translation   ( NAR NMT , or NAT ; Gu et al . , 2018 ; Lee et al . ,   2018 ) is an emerging subfield of NMT which fo-   cuses on increasing the translation speed by chang-   ing the model architecture .   The defining feature of non - autoregressive mod-   els is the conditional independence assumption on   the output probability distributions ; this is in con-   trast to autoregressive models , where the output   distributions are conditioned on the previous out-   puts . This conditional independence allows one   to decode the target tokens in parallel . This cansubstantially reduce the decoding time , especially   for longer target sentences .   The decoding speed is assessed by translating a   test set and measuring the overall time the process   takes . This may sound simple , but there are various   aspects to be considered that can affect decoding   speed , such as batching , number of hypotheses in   beam search or hardware used ( i.e. , using CPU or   GPU ) . Decoding speed evaluation is a challeng-   ing task , especially when it comes to comparabil-   ity across different approaches . Unlike translation   quality , decoding speed can be measured exactly .   However , also unlike translation quality , different   results are obtained from the same system under   different evaluation environments . The WMT Ef-   ficient Translation Shared Task aims to evaluate   efficiency research and encourages the reporting   of a range of speed and translation quality values   to better understand the trade - off across different   model configurations ( Heafield et al . , 2021 ) . In this   paper , we follow the emerging best practices de-   veloped in the Efficiency Shared Task and directly   compare with the submitted systems .   In the development of NAR models , modeling   error and its subsequent negative effect on transla-   tion quality remains the biggest issue . Therefore ,   the goal of contemporary research is to close the   performance gap between the AR models and their   NAR counterparts , while maintaining high decod-   ing speed . Considering these stated research goals ,   the evaluation should comprise of assessing trans-   lation quality as well as decoding speed .   Translation quality is usually evaluated by scor-   ing translations of an unseen test set either us-   ing automatic metrics , such as BLEU ( Papineni   et al . , 2002 ) , ChrF ( Popovi ´ c , 2015 ) or COMET   ( Rei et al . , 2020 ) , or using human evaluation . To   prevent methods from eventually overfitting to a   single test set , new test sets are published each   year as part of the WMT News Translation Shared   Task . In contrast , translation quality evaluation in1780NAR research is measured almost exclusively on   the WMT 14 English - German test set , using only   BLEU scores . Automatic evaluation of transla-   tion quality remains an open research problem , but   current research advises against relying on a sin-   gle metric , and especially against relying on only   BLEU ( Mathur et al . , 2020 ; Kocmi et al . , 2021 ) .   In our experiments , we follow the recent best prac-   tices by using multiple metrics and recent test sets .   In this paper , we examine the evaluation method-   ology generally accepted in literature on NAR   methods , and we identify a number of flaws . First ,   the results are reported on different hardware ar-   chitectures , which makes them incomparable , even   when comparing only relative speedups . Second ,   most of the methods only report latency ( decoding   with a single sentence per batch ) using a GPU ; we   show that this is the only setup favors NAR models .   Third , the reported baseline performance is usually   questionable , both in terms of speed and transla-   tion quality . Finally , despite the fact that the main   motivation for using NAR models is the lower time   complexity , the findings of the efficiency task are   ignored in most of the NAR papers .   We try to connect the separate worlds of NAR   and efficient translation research . We train non-   autoregressive models based on connectionist tem-   poral classification ( CTC ) , an approach previously   shown to be effective ( Libovický and Helcl , 2018 ;   Ghazvininejad et al . , 2020 ; Gu and Kong , 2021 ) .   We employ a number of techniques for improving   the translation quality , including data cleaning and   sequence - level knowledge distillation ( Kim and   Rush , 2016 ) . We evaluate our models following a   unified evaluation methodology : In order to com-   pare the translation quality with the rest of the NAR   literature , we report BLEU scores measured on the   WMT 14 test set , on which we achieve state - of-   the - art performance among ( both single - step and   iterative ) NAR methods ; we also evaluate the trans-   lation quality and decoding speed of our models in   the same conditions as the efficiency task .   We find that despite achieving very good results   among the NAT models on the WMT 14 test set ,   our models fall behind in translation quality when   measured on the recent WMT 21 test set using   three different automatic evaluation metrics . More-   over , we show that GPU decoding latency is the   only scenario in which non - autoregressive models   outperform autoregressive models .   This paper contributes to the research commu - nity in the following aspects : First , we point out   weaknesses in standard evaluation methodology of   non - autoregressive models . Second , we link the   worlds of non - autoregressive translation and op-   timization of autoregressive models to provide a   better understanding of the results achieved in the   related work .   2 Non - Autoregressive NMT   The current state - of - the - art NMT models are au-   toregressive – the output distributions are condi-   tioned on the previously generated tokens ( Bah-   danau et al . , 2016 ; Vaswani et al . , 2017 ) . The de-   coding process is sequential in its nature , limiting   the opportunities for parallelization .   Non - autoregressive models use output distribu-   tions which are conditionally independent of each   other , which opens up the possibility of paralleliza-   tion . Formally , the probability of a sequence y   given the input xin a non - autoregressive model   with parameters θis modeled as   p(y|x ) = /productdisplayp(y|x , θ ) . ( 1 )   Unsurprisingly , the independence assumption in   NAR models has a negative impact on the trans-   lation quality . The culprit for this behavior is the   multimodality problem – the inability of the model   to differentiate between different modes of the joint   probability distribution over output sequences in-   side the distributions corresponding to individual   time steps . A classic example of this issue is the   sentence “ Thank you ” with its two equally proba-   ble German translations “ Danke schön ” and “ Vie-   len Dank ” ( Gu et al . , 2018 ) . Because of the inde-   pendence assumption , a non - autoregressive model   can not assign high probabilities to these two trans-   lations without also allowing for the incorrect sen-   tences “ Vielen schön ” and “ Danke Dank ” .   Knowledge distillation ( Kim and Rush , 2016 )   has been successfully employed to reduce the nega-   tive influence of the multimodality problem in NAR   models ( Gu et al . , 2018 ; Saharia et al . , 2020 ) . Syn-   thetic data tends to be less diverse than authentic   texts , therefore the number of equally likely trans-   lation candidates gets smaller ( Zhou et al . , 2020 ) .   A number of techniques have been proposed for   training NAR models , including iterative methods   ( Lee et al . , 2018 ; Ghazvininejad et al . , 2019 ) , aux-   iliary training objectives ( Wang et al . , 2019 ; Qian   et al . , 2021 ) , or latent variables ( Gu et al . , 2018;1781Lee et al . , 2018 ; Kaiser et al . , 2018 ) . In some form ,   all of the aforementioned approaches use explicit   target length estimation , and rely on one - to - one   correspondence between the output distributions   and the reference sentence .   A group of methods that relax the requirement of   the strict one - to - one alignment between the model   outputs and the ground - truth target sequence in-   clude aligned cross - entropy ( Ghazvininejad et al . ,   2020 ) and connectionist temporal classification ( Li-   bovický and Helcl , 2018 ) .   The schema of the CTC - based model , as pro-   posed by Libovický and Helcl ( 2018 ) , is shown   in Figure 1 . The model extends the Transformer   architecture ( Vaswani et al . , 2017 ) . It consists   of an encoder , a state - splitting layer , and a non-   autoregressive decoder . The encoder has the same   architecture as in the Transformer model . The   state - splitting layer , applied on the encoder out-   put , linearly projects and splits each state into   kstates with the same dimension . The decoder   consists of a stack of Transformer layers . Unlike   the Transformer model , the self - attention in the   non - autoregressive decoder does not use the causal   mask , so the model is not prevented from attending   to future states . Since the output length is fixed   tok - times the length of the source sentence , the   model is permitted to output blank tokens . Dif-   ferent positions of the blank tokens in the output   sequence represent different alignments between   the outputs and the ground - truth sequence . Connec-   tionist temporal classification ( Graves et al . , 2006 )   is a dynamic algorithm that efficiently computes   the standard cross - entropy loss summed over all   possible alignments .   We choose the CTC - based architecture for our   models because it has been previously shown to be   effective for NAR NMT ( Gu and Kong , 2021 ; Sa-   haria et al . , 2020 ) and performs well in the context   of non - autoregressive research . It is also one of the   fastest NAR architectures since it is not iterative .   3 Evaluation Methodology   The research goal of the non - autoregressive meth-   ods is to improve translation quality while main-   taining the speedup brought by the conditional in-   dependence assumption . This means that careful   thought should be given to both quantifying the   speed gains and the translation quality evaluation .   The speed - vs - quality trade - off can be characterized   by the Pareto frontier . In this section we discuss   the evaluation from both perspectives .   Translation Quality . In the world of non-   autoregressive NMT , the experimental settings   are not very diverse . The primary language pair   for translation experiments is English - German ,   sometimes accompanied by English - Romanian to   simulate the low - resource scenario . These lan-   guage pairs , along with the widely used test sets   – WMT 14 ( Bojar et al . , 2014 ) for En - De and   WMT 16 ( Bojar et al . , 2016 ) for En - Ro – became   the de facto standard benchmark for NAR model   evaluation .   A common weakness seen in the literature is the   use of weak baseline models . The base variant of   the Transformer model is used almost exclusively   ( Gu et al . , 2018 ; Gu and Kong , 2021 ; Lee et al . ,   2018 ; Ghazvininejad et al . , 2020 ; Qian et al . , 2021 ) .   We argue that using weaker baselines might lead to   overrating the positive effects brought by proposed   improvements . Since the baseline autoregressive   models are used to generate the synthetic parallel   data for knowledge distillation , the weakness is   potentially further amplified in this step .   Evaluation is normally with automatic metrics   only , and often only BLEU is reported . In light of   recent research casting further doubt on the relia-   bility of BLEU as a measure of translation quality   ( Kocmi et al . , 2021 ) , we argue that this is insuffi-   cient.1782Decoding Speed . The current standard in eval-   uation of NAR models is to measure translation   latency using a GPU , i.e. , the average time to trans-   late a single sentence without batching . Since the   time depends on the hardware , relative speedup is   usually reported along with latency .   This is a reasonable approach but we need to   keep in mind the associated difficulties . First , the   results achieved on different hardware architectures   are not easily comparable even when considering   the relative speedups . We also note that the relative   speedup values should always be accompanied by   the corresponding decoding times in absolute num-   bers . Sometimes , this information is missing from   the published results ( Qian et al . , 2021 ) .   We argue that measuring only GPU latency dis-   regards other use - cases . In the WMT Efficiency   Shared Task , the decoding speed is measured in   five scenarios . The speed is reported using a GPU   with and without batching , using all 36 CPU cores   ( also , with and without batching ) , and using a sin-   gle CPU core without batching . In batched decod-   ing , the shared task participants could choose the   optimal batch size . Our results in Section 5 show   that measuring latency is the only one that favors   NAR models , and as the batch size increases , AR   models quickly reach higher translation speeds .   4 Experiments   We experiment with non - autoregressive models for   English - German translation . We used the data pro-   vided by the WMT 21 News Translation Shared   Task organizers ( Akhbardeh et al . , 2021 ) .   As our baseline model , we use the CTC - based   NAR model as described by Libovický and Helcl   ( 2018 ) . We use stack of 6 encoder and 6 decoder   layers , separated by the state splitting layer which   extends the state sequence 3 times .   We implement our modelsin the Marian toolkit   ( Junczys - Dowmunt et al . , 2018 ) . For the CTC loss   computation , we use the warp - ctc library ( Amodei   et al . , 2016 ) .   4.1 Teacher Models   For training our baseline autoregressive models , we   closely follow the approach of Chen et al . ( 2021 ) .   The preparation of the baseline models consists of   three phases – data cleaning , backtranslation , and   the training of the final models . Data Raw size Cleaned size   Parallel – clean 3.9 3.1   Parallel – noisy 92.0 84.6   Monolingual – En 93.1 91.0   Monolingual – De 149.9 146.2   We train the teacher models on cleaned parallel   corpora and backtranslated monolingual data . For   the parallel data , we used Europarl ( Koehn , 2005 ) ,   the RAPID corpus ( Rozis and Skadi nš , 2017 ) , and   the News Commentary corpus from OPUS ( Tiede-   mann , 2012 ) . We consider these three parallel   datasets clean . We also use noisier parallel datasets ,   namely Paracrawl ( Bañón et al . , 2020 ) , Common   Crawl , WikiMatrix ( Schwenk et al . , 2019 ) , and   Wikititles . For backtranslation , we used the mono-   lingual datasets from the News Crawl from the   years 2018 - 2020 , in both English and German .   We clean the parallel corpus ( i.e. both clean and   noisy portions ) using rule - based cleaning . Addi-   tionally , we exclude sentence pairs with non - latin   characters . and we apply dual cross - entropy filter-   ing on the noisy part of the parallel data ( Junczys-   Dowmunt , 2018 ) . We train Transformer base mod-   els in both directions on the clean portion of the   parallel data . Then , we select the best - scoring 75 %   of sentence pairs for the final parallel portion of the   training dataset .   For backtranslation ( Sennrich et al . , 2016 ) , we   train four Transformer big models on the cleaned   parallel data in both directions . We then use them   in an ensemble to create the synthetic source side   for the monolingual corpora . We add a special   symbol to the generated sentences to help the mod-   els differentiate between synthetic and authentic   source language data ( Caswell et al . , 2019 ) .   We use hyperparameters of the Transformer big   model , i.e. model dimension 1,024 , feed - forward   hidden dimension of 4,096 , and 16 attention heads .   For training , we use the Adam optimizer ( Kingma   and Ba , 2014 ) with β , βandϵset to 0.9 , 0.998   and 10respectively . We used the inverted square-   root learning rate decay with 8,000 steps of linear1783warm - up and initial learning rate of 10 .   The teacher models follow the same hyperpa-   rameter settings as the models for backtranslation ,   but are trained with the tagged backtranslations in-   cluded in the data . As in the previous case , we train   four teacher models with different random seeds   for ensembling .   Similar to creating the backtranslations , we use   the four teacher models in an ensemble to create   the knowledge - distilled data ( Kim and Rush , 2016 ) .   We translate the source side of the parallel data , as   well as the source - language monolingual data . We   do not translate back - translated data . Thus , the   source side data for the student models is authen-   tic , and the target side is synthetic , created by the   teacher models .   4.2 Student Models   We train five variants of the student models with dif-   ferent hyperparameter settings . The “ Large ” model   is our baseline model – the same number of layers   as the teacher models , 6 in the encoder , followed by   the state splitting layer , and another 6 layers in the   decoder . The “ Base ” model has the same number   of layers with reduced dimension of the embed-   dings and the feed - forward Transformer sublayer ,   to match the Transformer base settings . We also   try reducing the numbers of encoder and decoder   layers . We shrink the base model to 3 - 3 ( “ Small ” ) ,   2 - 2 ( “ Micro ” ) , and 1 - 1 ( “ Tiny ” ) architectures .   We run the training of each model for three   weeks on four Nvidia Pascal P100 GPUs .   5 Results   In this section , we try to view the results of the   NAR and efficiency research in a shared perspec-   tive . We evaluate our models and present results   in terms of translation quality and decoding speed .   We compare the results to the related work on both   non - autoregressive translation and model optimiza-   tion .   Translation Quality . The research on non-   autoregressive models uses the BLEU score ( Pap-   ineni et al . , 2002 ) measured on the WMT 14 test   set ( Bojar et al . , 2014 ) as a standard benchmark for   evaluating translation quality . We use Sacrebleu   ( Post , 2018 ) as the implementation of the BLEU   score metric . Using a single test set for the whole   volume of research on this topic may however pro-   duce misleading results . To bring the evaluationEn→De De →En   Saharia et al . ( 2020 ) 28.2 31.8   Gu and Kong ( 2021 ) 27.2 31.3   Qian et al . ( 2021 ) 26.6 31.0   Large 28.4 31.3   Base 23.7 30.3   Small 23.6 29.1   Micro 25.0 27.5   Tiny 20.3 21.7   up to date with the current state - of - the - art transla-   tion systems , we also evaluate our models using   COMET ( Rei et al . , 2020)and BLEUscores on   the recent WMT 21 test set . The same test set was   used in the WMT 21 Efficiency Task .   Table 2 shows the BLEU scores of our NAR   models on the WMT 14 test set . We show the re-   sults of the five variants of the NAR models and   we include three of the best - performing NAR ap-   proaches from the related work . We see from the   table that using BLEU , the “ Large ” model scores   among the best NAR models on the WMT 14 test   set . As the NAR model size decreases , so does   the translation quality , with the notable exception   of the En →De “ Micro ” model , which outperforms   the “ Base ” model consistently on different test sets .   In Table 3 , we report the automatic evaluation   results of our AR and NAR models on the multi-   reference WMT 21 test set ( Akhbardeh et al . , 2021 ) .   We compare our NAR models to the AR large   teacher models from Section 4.1 , an AR base model   trained on the original clean data , and an AR base   student model trained on the distilled data . Follow-   ing Heafield et al . ( 2021 ) , we use references A , C ,   and D for English - German translation .   We see that there is a considerable difference in   the translation quality between the NAR models   and the AR large teacher model . This difference   grows with beam search and ensembling applied   on the AR decoding , techniques not usually used   with NAR models because of the speed cost . We1784En→De COMET BLEU   AR – Large 0.4110 50.5   + beam 0.4053 50.8   + ensemble 0.4332 52.2   AR – Base 0.3881 47.9   + beam 0.3873 48.0   Student AR – Base 0.4550 51.6   NAR models   Large 0.1485 47.8   Base -0.0521 41.8   Small -0.0752 41.9   Micro -0.0083 43.5   Tiny -0.3333 34.7   also note that when we train an AR base model   on the distilled data , it outperforms the NAR large   model by a considerable margin .   Another thing we notice is the enormous differ-   ence in the COMET scores between the AR and   NAR models . The AR base models achieve compa-   rable BLEU scores to the NAR large models , but   differ substantially in the COMET score . From   a look at the system outputs , we hypothesize that   the NAR systems produce unusual errors which   BLEU does not penalise as heavily as COMET .   This might suggest that NAR models would rank   poorly in human evaluation relative to their autore-   gressive counterparts , despite the reasonable BLEU   score values . Another reason might be that the dif-   ferent errors of NAR models are causing a domain   mismatch between the COMET training data and   the data being evaluated .   Decoding speed . We follow the decoding time   evaluation methodology of the WMT 21 Efficient   Translation Shared Task ( Heafield et al . , 2021 ) . We   recreate the hardware conditions that were used   in the task . For the GPU decoding measurements ,   we use a single Nvidia Ampere A100 GPU . The   CPU evaluation was performed on a 36 - core CPU   Intel Xeon Gold 6354 server from Oracle cloud . To   amortize the various computation overheads , the   models submitted to the shared task are evaluated   on a million sentence benchmark dataset .   We measure the overall wall time to translate theModel Latency ( ms )   Gu et al . ( 2018 ) 39   Wang et al . ( 2019 ) 22   Sun et al . ( 2019 ) 37   Ours – Large 14   shared task dataset with different batching settings   on both the GPU and the 36 - core CPU . The decod-   ing times are shown in Figures 2 and 4 for the GPU   and CPU times , respectively . We do not report the   single - core CPU latencies as the decoding speed   of the NAR models falls far behind the efficient   AR models in this setup and the translation of the   dataset takes too long .   We can see that in case of GPU decoding that   all models benefit from having larger batch sizes .   However , the non - autoregressive models are much   faster when the batch size is small . We also ran the   evaluation on an Nvidia Pascal P100 GPU , which   showed that when the batch size is large enough ,   autoregressive models eventually match the speed   of non - autoregressive models . We show the decod-   ing times on the Pascal GPU in Figure 3 . In Table   4 , we compare the latencies measured on the Pas-   cal GPU to some of the related NAR approaches   that report results on this GPU type . Due to imple-   mentation reasons , the maximum batch size for our   NAR models is around 220 sentences .   Comparison with Efficient AR Models . In Ta-   ble 5 , we present a comparison on the million sen-   tence test set with “ Edinburgh base ” , one of the   leading submissions in the WMT 21 efficiency task   ( Behnke et al . , 2021 ) , which uses the deep encoder   – shallow decoder architecture ( Kasai et al . , 2021 ) .   First , we see that using three different evaluation   metrics ( ChrF , COMET , and BLEU ) , our models   lag behind the Edinburgh base model . In line with   our previous observation , we see a considerable   drop in the COMET score values . In terms of   decoding speed , the only scenario in which the   non - autoregressive model is better is on GPU with   batch size 1 . This is in line with our intuition that   the parallelization potential brought by the GPU is   utilized more efficiently by the NAR model . On178502,0004,0006,0008,00010,00012,00014,000   1 2 4 8 16 32 64 128   Batch size ( sentences)AR Large   AR Base   Large   Base   Small   Micro   Tiny   05,00010,00015,00020,00025,00030,000   1 2 4 8 16 32 64 128   Batch size ( sentences)AR Large   AR Base   Large   Base   Small   Micro   Tiny   02,0004,0006,0008,00010,00012,00014,000   1 2 4 8 16 32 64 128   Batch size ( sentences)AR Big   AR Base   Large   Base   Small   Micro   Tiny1786Translation quality Decoding time ( seconds )   Edinburgh base ( Behnke et al . , 2021 ) 61.5 0.527 55.3 140 16,851 500   AR – Large ( teacher ) 59.2 0.411 50.5 1,918 > 24h 9,090   AR – Base ( student ) 59.5 0.455 51.6 1,465 > 24h 2,587   NAR – Large 58.6 0.149 47.8 782 7,020 7,434   NAR – Micro 57.3 -0.008 43.5 311 2,322 897   one hand , larger batches open up the paralleliza-   tion possibilities to AR models . On the other hand ,   limited parallelization potential ( in form of CPU   decoding ) reduces the differences between AR and   NAR models . The batch size of the Edinburgh base   model was 1,280 in the batched decoding setup .   6 Conclusions   In this paper , we challenge the evaluation methodol-   ogy adopted by the research on non - autoregressive   models for NMT .   We argue that in terms of translation quality ,   the evaluation should include newer test sets and   metrics other than BLEU ( particularly COMET   and ChrF ) . This will provide more insight and put   the results into the context of the recent research .   From the decoding speed perspective , we should   bear in mind various use - cases for the model   deployment , such as the hardware environment   or batching conditions . Preferably , the research   should evaluate the speed gains across a range of   scenarios . Finally , given that the latency condi-   tion – translation of one sentence at a time on a   GPU – already translates too fast to be perceived   by human users of MT , there is currently no com-   pelling scenario that warrants the deployment of   NAR models .   Acknowledgments   This project received funding from the Euro-   pean Union ’s Horizon 2020 research and innova-   tion programmes under grant agreements 825299   and 825303 ( GoURMET , Bergamot ) , and from   the Czech Science Foundation grant 19 - 26934X   ( NEUREM3 ) of the Czech Science Foundation .   Our work has been using data provided by the   LINDAT / CLARIAH - CZ Research Infrastructure , supported by the Ministry of Education , Youth   and Sports of the Czech Republic ( Project No .   LM2018101 ) .   References1787178817891790