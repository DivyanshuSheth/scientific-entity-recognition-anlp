  Nan Hu , Zirui Wu , Yuxuan Lai , Xiao Liu , Yansong FengWangxuan Institute of Computer Technology , Peking University , ChinaThe MOE Key Laboratory of Computational Linguistics , Peking University , ChinaDepartment of Computer Science , The Open University of China   { hunan,ziruiwu,erutan,lxlisa,fengyansong}@pku.edu.cn   laiyx@ouchn.edu.cn   Abstract   Different from previous fact extraction and ver-   ification tasks that only consider evidence of a   single format , FEVEROUS brings further chal-   lenges by extending the evidence format to both   plain text and tables . Existing works convert   all candidate evidence into either sentences or   tables , thus often failing to fully capture the   rich context in their original format from the   converted evidence , let alone the context in-   formation lost during conversion . In this pa-   per , we propose a Dual Channel Unified For-   mat fact verification model ( DCUF ) , which uni-   fies various evidence into parallel streams , i.e. ,   natural language sentences and a global evi-   dence table , simultaneously . With carefully-   designed evidence conversion and organization   methods , DCUF makes the most of pre - trained   table / language models to encourage each ev-   idence piece to perform early and thorough   interactions with other pieces in its original   format . Experiments show that our model can   make better use of existing pre - trained models   to absorb evidence of two formats , thus outper-   forming previous works by a large margin . Our   code and models are publicly available .   1 Introduction   The task of fact extraction and verification aims to   extract evidence and verify a given claim . Previ-   ous efforts focus on dealing with text format ev-   idence from unstructured documents ( Nie et al . ,   2019 ; Zhong et al . , 2020 ; Kruengkrai et al . , 2021 )   or evidence from a single given table ( Chen et al . ,   2020 ; Yang et al . , 2020 ; Eisenschlos et al . , 2020 ) .   Recently , Aly et al . ( 2021 ) propose a new realis-   tic setting , FEVEROUS , i.e. , fact extraction and   verification over unstructured and structured infor-   mation . In FEVEROUS , models should not only   extract evidence sentences / table cells from millionsFigure 1 : An excerpt example from FEVEROUS .   of passages , but also combine the evidence in dif-   ferent formats to verify a given claim .   Previous works on FEVEROUS generally con-   vert all evidence pieces into either plain text ( Aly   et al . , 2021 ; Saeed et al . , 2021 ; Malon , 2021 ) or   several tables ( Bouziane et al . , 2021 ) . However ,   format conversions inevitably lose rich context in-   formation for the converted evidence , thus may   mislead the subsequent encoding and interaction   steps . For example , in Figure 1 , the entire top two   rows are indispensable to understand the table cell   Won . It is difficult to identify all related context   cells and design a general conversion method to   render them into sentences , but these connections   can be easily caught by pre - trained table models   ( Herzig et al . , 2020 ; Yin et al . , 2020 ) . On the other   side , identifying / re - organizing crucial elements in   a sentence to construct a table is also challenging .   Simply inserting a whole sentence in a table cell   ( Bouziane et al . , 2021 ) will make the new cells   much larger ( and unique ) than normal ones , thus   can not make the most of general pre - trained table   models ( Herzig et al . , 2020 ; Yin et al . , 2020 ) as we5232expect .   Considering the inevitable expense in format   conversion , we believe that each evidence in its   original format can contribute necessary informa-   tion to final verification , thus should be better en-   coded in its original format . This further indicates   that we should design both sentence - to - table and   table(cell)-to - sentence conversion methods to ob-   tain all evidence in both formats , and maintain two   parallel encoders to absorb the two formats , respec-   tively . An advantage of doing so is to maximally   encourage early interaction , which proves more ef-   fective than pair - wise encoding ( Tymoshenko and   Moschitti , 2021 ; Jiang et al . , 2021 )   When converting table evidence into sentences ,   previous works either convert table cells to a   concatenation of key - value pairs ( Aly et al . ,   2021 ; Malon , 2021 ) , or construct sentences in a   coordinate - description style ( Kotonya et al . , 2021a ) .   They pay less attention to the conventional orga-   nization of tables structures . We observe that , in   a table , the column headers usually represent the   types / properties and the row headers often denote   entities or scopes . We argue that one should con-   sider these conventions to convert a table cell ev-   idence into more natural sentences , and later pre-   trained language models will be able to better cap-   ture the contextualized semantics of the table cells   from generated sentences . On the other hand , exist-   ing pre - trained table models are trained to analyze   one table at one time , while previous evidence con-   version methods produce several small tables for   one instance . It would be necessary to properly or-   ganize all evidence in one table so that pre - trained   table models can allow the most interactions among   all evidence pieces .   In this paper , we propose a dual channel uni-   fied format verification model ( DCUF ) to allow   each evidence piece encoded in its original for-   mat and maximally maintain its original rich con-   text , while encouraging further interactions with   other evidence . DCUF converts each evidence into   both textual and tabular formats in respective chan-   nels , and apply corresponding pre - trained models   to learn the representations for the final verifica-   tion . With the dual channel setting and carefully de-   signed evidence conversion methods , DCUF makes   better use of pre - trained language / table models to   perform early and thorough interactions among all   evidence and also between the claim and evidence .   In summary , we make the following contribu - tions in this paper : ( 1 ) we propose DCUF , a novel   model to maintain various evidence in two unified   formats and allow each evidence piece to interact   with other evidence in its best form . Experiments   show that DCUF outperforms all previous works   in literature . ( 2 ) we propose a context - aware evi-   dence conversion method that can properly orga-   nize evidence of different formats , which fits cur-   rent pre - trained language / table models hence take   their most advantage to obtain accurate and focused   representations .   2 Our Model   The FEVEROUS task can be formalized as , given   a claim qand Wikipedia dump , a model is asked to   find the evidence set consisting of sentences Sand   table cells C , and predict the veracity label of the   claim accordingly . The veracity label set includes   “ SUPPORTS ” , “ REFUTES ” and “ NOT ENOUGH   INFORMATION ” .   2.1 Model Overview   Following the widely adopted fact verification   pipeline ( Thorne et al . , 2018 ; Aly et al . , 2021 ) ,   we take three steps to solve the FEVEROUS task   ( i ) retrieving pages from the Wikipedia dump ; ( ii )   extracting evidence from the retrieved pages , and   ( iii ) verifying the claim according to extracted evi-   dence .   Specifically , for the document retrieval step , we   narrow the search space with an information re-   trieval model DRQA ( Chen et al . , 2017 ) and then re-   rank the retrieved pages . For the evidence retrieval   step , we design a multi - turn cell selector to extract   sentence evidence and table evidence respectively ,   and select evidence cells from tables . Finally , we   propose a Dual Channel Unified Format verifica-   tion model ( DCUF , shown in Figure 2 ) for the veri-   fication step . DCUF converts evidence to a unified   table / sentence format with carefully - designed evi-   dence conversion and re - organization methods in   each channel , and combine dual - channel encodings   to make the final prediction .   2.2 Document Retrieval   An efficient and effective document retriever is   required since the Wikipedia dump containing mil-   lions of pages . We first narrow the search space to   several hundred pages ( m ) with an efficient infor-   mation retrieval method based on TF - IDF , namely ,   DRQA ( Chen et al . , 2017 ) . A RoBERTa - based5233   re - ranker ( Saeed et al . , 2021 ) and a BM25 - based   re - ranker are then applied in parallel to re - rank   themdocument candidates . We combine the   results of two re - rankers and keep top mdocu-   ments since BM25 focuses more on entity matching   and RoBERTa - based re - ranker pays more attention   to the overall sentence structure . The document   scores are calculated as the sum of their rankings in   the two re - rankers . Documents with lower scores   have higher priority .   We further notice that the first several words   of a claim always contain the page titles needed .   We therefore derive a position - aware sub - sequence   matching to strengthen the page retriever . We also   remove pages with a long Wikipedia title starting   with a specified year that is not contained in the   claim .   2.3 Evidence Retrieval   We use DRQA ( Chen et al . , 2017 ) to extract ksen-   tences S={s}andntables T={t}from   the retrieved pages , respectively . Then we select   cells from the extracted tables . Many instances   in the FEVEROUS dataset require evidence cells   from more than one table , and each retrieved table   has different relevance score to the claim . However ,   the widely - used cell extractor ( Aly et al . , 2021 ) re-   serves cells from only one table in their implemen-   tation .   We thus propose a Multi - turn Cell Selector   ( MCS ) , which retrieves cells from all evidence ta-   bles and consider the importance of the retrieved   tables . A basic cell selector concatenates a given   claim qand a flattened candidate evidence tabletand feeds it into a sequence tagger to decide   whether each cell in the table should be selected .   MCS implements this procedure in a multi - turn   manner , since each table has a different relevance   score to the given claim . In the first turn , MCS   selects the table most related to the given claim and   feeds it to the basic cell selector . All cells with a   selection score larger than the threshold gwill be   added to the cell evidence set C. In the second   turn , all tables ranked second in Tare the input to   the basic selector . If the number of cells in Chas   not reached the upper limit , MCS adds the newly   selected cells in the second turn to C. MCS repeats   this procedure for nloops and we will get the cell   evidence set C={c}.ois the number of   cells selected as evidence for the jinstance .   2.4 Unified Format Encodings   Since the evidence can be of two formats , tex-   tual format and tabular ( or cell ) format , we con-   vert each evidence of one format to another , so   that we will get a unified representation of all   evidence and could easily assemble them . We   propose two conversion methods , i.e. , cells - to-   sentences and sentences - to - tables for the original   tabular evidence and textual evidence , respectively .   We carefully design the evidence conversion and   re - organization methods to make converted evi-   dence natural , thus take better advantage of the   pre - trained language / table models .   2.4.1 Text Format Encoding   We consider the table conventions , i.e. , row headers   in general tables usually represent attribute types,5234and convert table cells into natural sentences , thus   make better use of pre - trained language models to   perform early interaction over the claim and all   evidence .   There are two types of tables on the Wikipedia   pages , ( i ) general tables and ( ii ) Infoboxes . For   general tables , we ignore header cells selected by   the evidence retriever and convert content cells se-   lected into text format . For each cell , we identify   its row header cell and column header cell . We find   that most general - typed tables are column tables ,   which means they only contain column headers .   However , the first column of a table , in many cases ,   indicates the object name and others are attributes .   Therefore , if a cell does not have an explicit header   cell , we choose the first cell of the same row to be   its row header cell . We observe that the row header   cell for a general table always indicates the object   name , the column header cell indicates the attribute   type , and the cell itself is the attribute value for the   object . We thus form the corresponding text for   a cell in a general table as “ < column header > for   < row header > is < cell value > . ” For Infoboxes , it is   a different story . The row header is always the at-   tribute type , the column header is the field that the   attribute belongs to , and the Wikipedia title is the   object name . Therefore , the text representation of   each cell in an Infobox is formulated as “ < column   header > : < row header > of < Wikipedia title > is   < cell value > ” . As shown in Figure 3 , “ 21 - 24 min-   utes ” is a selected cell , its row header “ Running   Time ” is the attribute type , and the header cell “ Pro-   duction ” is the field which the attribute “ Runing   time ” belongs to . Thus , the cell will be converted   to “ Production : Running time of The Simpsons is   21 - 24 minutes . ”   Claim verification on a set of evidence contain-   ing many cells often requires operations on cells   in the same column , such as maximum and sum-   mary ( Aly et al . , 2021 ) . Therefore , we pack the   texts from cells in the same column together . These   texts are joined by semicolons and form a piece of   column text . Each column is converted into a sen-   tence . As shown in Figure 3 , the cells “ December   17 , 1989 ” and “ October 11 , 1990 ” are jointly con-   verted to one sentence , “ Season premiere for 1 is   December 17 , 1989 ; Season premiere for 2 is Octo-   ber 11 , 1990 . ” The given claim , the extracted text   evidence , and the column texts are concatenated   together to form the input to the text format evi-   dence encoder , separated by the separator “ < /s > ” inthe RoBERTa model . Column texts from the same   table are adjacent , and sentences from the same   Wikipedia page are arranged together .   2.4.2 Table Format Encoding   As shown in the right part of Figure 3 , we pro-   pose to construct a single evidence table containing   all evidence candidates , both textual and tabular ,   since most existing table pre - training models are   designed to and trained to analyze one single table   at one time . Making better use of the pre - training   models helps to perform early and thorough evi-   dence interaction .   The method of converting text format evidence   to tabular format is straightforward . We group the   evidence sentences from the same Wikipedia page .   If there are nevidence sentences from the same   Wikipedia page , we construct a table of n+ 1rows   and 1 column . The only cell in the first row is a   header cell containing the Wikipedia title . And   for other rows , each row has one cell , and in that   cell is a piece of sentence evidence from that page .   Assuming there are evidence sentences from m   pages , we will get mtable units after this step .   As the pre - training table model has a capacity   limit , we crop irrelevant cells first to compress the   extracted tables . To be precise , rows and columns   containing no selected cells are removed . After   that , we add one cell row to the top of each cropped   table , this cell contains the title of the Wikipedia   page from which the table is extracted . If there   arenextracted tables , we will get ncropped table   from this step .   We get a global evidence table by stacking the   mtables from sentences and ntables from tabular   evidence , as illustrated in Figure 3 . Then , we feed   the claim and the global evidence table to a pre-   trained table model , TAPAS ( Herzig et al . , 2020 ) ,   and get the tabular format evidence representation .   2.4.3 Dual - Channel Verdict Prediction   The final verdict prediction is based on the dual   channel encoding . Therefore , each evidence can   be encoded in its original format while interacting   with all evidence pieces and the claim .   We concatenate the text format evidence encod-   inghand the tabular format evidence encoding   hto obtain a joint format encoding for predic-   tion . With a feed - forward network and a softmax   layer , we obtain the veracity probability distribu-   tion of the claim and the predicted label is the one5235   with the largest probability :   h= [ h;h ] ( 1 )   p ( y|q , S , T , C ) = Softmax ( FNN ( h ) ) ( 2 )   ˆy = argmaxp ( y|q , S , T , C ) ( 3 )   where p ( y|S , T , C ) represents the probability of   each alternative label ygiven the claim q , evidence   sentences Sand evidence tables T.   To strengthen the model ’s ability to predict the   veracity label with the evidence set containing irrel-   evant pieces , we construct two instances for each   claim in the training set . One is the claim with the   gold evidence provided by the FEVEROUS dataset ,   and the other is the claim with extracted evidence   pieces from previous evidence extraction steps . We   use the cross - entropy loss function :   L=−1   N / summationdisplaylog ( p ( ˆ y = y|q , S , T , C ) ) ( 4 )   where yis the true veracity label of the iinstance .   Nis the size of the training set .   3 Experiments   We evaluate our models on the FEVEROUS dataset ,   where each claim is annotated with a gold veracity   label and several gold evidence sets . Any one of   the evidence sets is sufficient to verify the claim .   More details about the FEVEROUS dataset are in   Appendix .   The FEVEROUS dataset provides two official   metrics , namely label accuracy ( Acc . ) and FEVER-   OUS score . Label accuracy calculates the ratioof the instances whose veracity label is correctly   predicted . FEVEROUS score is the ratio of the   instances whose veracity label is correct and the   extracted evidence set is sufficient . Here , sufficient   evidence sets are defined as the evidence sets cov-   ering one of the gold evidence sets provided in the   FEVEROUS dataset . Note that there are at most 25   table cells and 5 sentences to calculate the scores .   3.1 Implementation Details   In the document retrieval step , the number of pages   retrieved by the BM25 - based retriever mis 150 .   We keep the top 5 pages for evidence extraction af-   ter page re - ranking . For evidence retriever , the top   k=5 sentences and top n=2 tables are extracted   from the retrieved pages . The gate of cell selection   gis 0.25 and a maximum of 25 cells are selected   in total for each claim .   We use an Adam optimizer ( Kingma and Ba ,   2015 ) with a linear learning rate scheduler . The   rate of warm - up steps is 20 % . The peaking learning   rate for parameters in pre - trained language models   is10and10for other parameters . The batch   size is 24 , implemented with gradient accumulation   techniques . DCUF takes about 5 hours to run on a   single NVIDIA A100 Tensor Core GPU ( 40 GB ) .   The RoBERTa - based re - ranker is initialized   from the hugging face checkpointwithout further   fine - tuning . The TAPAS checkpoint is fine - tuned   with a table fact verification task , Tabfact ( Chen5236   et al . , 2020 ) . Same as the baselines , the sentence   evidence encoder is RoBERTa - large tuned with   several NLI and verification tasks .   Document Retriever Details For BM25 re-   reranker , all page candidates of every 200 adjacent   instances are merged to build the BM25 index for   these instances . Each document in the Wikipedia   dump is represented by the concatenation of its title   and the first 64 words of its content for the BM25   re - ranker . We use NLTKto remove stop words   and lemmatize the remains . For position - aware en-   tity matching , if a sub - sequence , with more than   two words , in the first ten words of a given claim is   a page title in the Wikipedia dump and it is not in   themdocuments we replace the page of the lowest   priority with it .   3.2 Main Results   The overall performance of our model on the de-   velopment set and the test set are shown in Table 1 .   We get an increase of 5.77 % on the FEVEROUS   score and 7.91 % on the accuracy over the previ-   ous best model FaBULOUS ( Bouziane et al . , 2021 )   on the development set . For the test set , the in-   crease is 6.96 % and 7.14 % in Feverous score and   label accuracy , respectively . These results suggest   the effectiveness of our proposed DCUF model .   The evidence format of a global evidence table is   consistent to the input of pre - trained table models .   Thus , DCUF can make better use of the internal   ability of pre - trained models than previous works   which concatenate linearized tables or max - pool   lots of claim - table pair encoding ( Bouziane et al . ,2021 ) . Moreover , DCUF also performs better than   another well - performing model , CARE ( Kotonya   et al . , 2021b ) . DCUF converts cells to meaningful   sentences that are similar to the inputs of PLMs   pre - training stage , which makes better use of the   PLMs ability .   We also conduct experiments with the gold evi-   dence to investigate the effectiveness of our ver-   ification model . The results are shown in Ta-   ble 2 . DCUF obtains an increase of 2.88 % on   accuracy over the previous best result and 3.68 %   over RoBERTa - based models . With all evidence   candidates in the same format and preserving con-   text information , our system can make better use of   the pre - trained language / table models and perform   early and thorough interactions among evidence   and between the claim and evidence .   Evidence Extraction Results The document re-   trieval results are shown in Table 3 . In Table 3 ,   experiments show that both BM25 re - ranker and   RoBERTa re - ranker can improve the document re-   trieval quality to a great extent compared to vanilla   DrQA page retriever , with whole evidence set recall   improvement of 8.63 % and 12.56 % respectively .   The combination of them can further enlarge this   gap to 16.22 % . We find that the average number of   pages in the merged set of top-5 BM25 re - ranked5237   results and top-5 RoBERTa re - ranked results is   7.75 in the development set . It proves that these   two re - rankers tend to focus on different aspects   when evaluating the correlation of the given claim   and a page candidate . The rule - based enhancement   methods , namely matching position - aware entities   and removing pages with unmatched year , bring   a a further improvement of 3.10 % . It indicates   that document retrievers should take the word posi-   tions in the given claim into consideration . Without   any training procedure or Dense Retriever ( which   is time - consuming ) , we get a whole set recall of   84.82 % when retrieving 5 pages for each claim .   Table 4 shows the evidence extraction results .   With MCS , we achieve an increase of 29.71 % on   cell recall and 13.22 % on the overall evidence re-   call over FaBULOUS . The result indicates that con-   sidering only one table is not enough and we should   pay attention to the relevance scores of the input ta-   bles especially when the cell selector is somewhat   weak .   3.3 Ablation Study   We evaluate the effect of each part of DCUF with   a collection of ablation experiments . The Experi-   ment settings are as follows . ( 1 ) w/o Table Format   Encoding We only use the unified text format ev-   idence encoding for verdict prediction . ( 2 ) w/o   Table Format Encoding We only use the unified   table format evidence encoding for verdict predic-   tion . ( 3 ) w/o Dual - Channel Predictor We use   the verdict predictor in the baseline to predict the   veracity label .   The results are shown in Table 5 . The FEVER-   OUS score and accuracy drop consistently when we   remove the unified table format encoding or the uni-   fied text format encoding . Especially , the FEVER-   OUS score drops by 3.34 % when only using the   unified tabular encoding for prediction . With only   unified text format encoding , the FEVEROUS   score drops by 0.61 % , which may contribute to   pre - trained table models , such as TAPAS , being   still weaker compared to pre - trained language mod-   els . However , with our carefully - designed context-   aware unified format conversion , verdict prediction   upon one format encoding outperforms all previous   results . To relieve the effect of different evidence   extraction methods , we train the verdict prediction   model in the baseline with our evidence extraction   results . We see a great drop , with accuracy drop-   ping of 14.51 % and FEVEROUS score by 7.86 % ,   which proves that our combined unified format ver-   dict prediction model can keep the context needed   when converting the evidence format and help the   extracted evidence perform early interaction in a   unified format .   4 Analysis5238Few - shot Results Figure 4 shows the FEVER-   OUS score on the development set when we train   unified DCUF , DCUF without text format encoding   and DCUF without table format encoding verdict   predictors on 1 % , 5 % and 10 % instances of the   training set respectively . We find that , training   on only 10 % instances , all of the three settings   outperform previous SOTA results , with the dual   channel predictor achieving the best FEVEROUS   score of 33.9 % . Meanwhile , with only 1 % of the   training set , namely , 713 instances , the unified text   format predictor outperforms previous SOTA result   by 0.9 % . These improvements may contribute to   our carefully - designed format conversion methods .   The text format evidence converted from cells is   similar to sentences in natural language when keep-   ing much context information in the conversion   procedure . And the concatenated claim - evidence   string is the same as previous fact verification tasks   on which the RoBERTa checkpoint is fine - tuned .   Meanwhile , a given claim , and a single global evi-   dence table are consistent to the input requirements   in the pre - training step of TAPAS . With few new pa-   rameters introduced and an input form strictly com-   plying with the requirements of pre - trained mod-   els and previous single format fact - checking tasks ,   DCUF could make the most of the pre - training   stage and , thus , learn well in the few - shot setting .   We also find that when training with only 1 %   instances , the unified text format predictor outper-   form other two settings . As the number of training   instances increase , dual - channel predictor learns   how to combine information from the two channel ,   thus achieving better results .   Error Analysis We find that when converting   cells to text with rule - based methods , there are   inevitably noise or not fluent sentences introduced .   One problem is caused by the latent header cells .   As shown in Table 6(a ) , the cell “ Team ” is selected   but not explicitly marked as a header cell , so a   meaningless sentence “ Senior career * : Years for   Aramais Yepiskoposyan is Team . ” is derived from   this cell . Meanwhile , although the header cells in-   dicating the attribute type are usually nouns , there   are some exceptions . Infobox Table 6(b ) is an ex-   ample . The selected cell “ Sir Roger Manwood ”   is converted to a fluent and meaningful evidence   sentence “ Information : Founder for Sir Roger Man-   wood ’s School is Sir Roger Manwood . ” . However ,   “ Information : Established for Sir Roger Manwood ’s   School is 1563 ; 457 years ago . ” is not a sentence   that conforms to English grammar . And some-   times , the latent row header which indicates the   object name is confusing . For example , the cell   “ Marcia Wallace ” in Table 6(c ) is converted to a   sentence “ Actor for 179 is Marcia Wallace . ” With-   out the header “ Episodes ” , what “ 179 ” refers to is   confusing . Assessing and polishing the converted   sentence may help to solve the problems presented   above .   5 Related Works   Fact Verification over Unstructured Evidence   Thorne et al . ( 2018 ) proposed FEVER , a large-   scale dataset of claims based on Wikipedia arti-   cles . Language models have better performance   compared to other methods e.g. ESIM - based mod-   els(Hanselowski et al . , 2018 ; Nie et al . , 2019 ) .   BERT - based models make the prediction based   on collected evidence in a direct aggregating   rule(Soleimani et al . , 2020 ) or a graph - based ap-   proach(Zhou et al . , 2019 ; Zhong et al . , 2020 ) .   Fact Verification over Structured Evidence   Benchmarks for fact verification on structured   evidence are built on tables collected from   Wikipedia(Chen et al . , 2020 ) or scientific arti-   cles(Wang et al . , 2021 ) . Many previous works   search latent programs as an intermediary to rea-   son over the given table . They directly encode   programs ( Chen et al . , 2020)or construct heteroge-   neous graphs ( Shi et al . , 2020 ; Yang et al . , 2020 )   with the claim , the table and the programs . Another5239way is to linearize the input table and perform table   pre - training ( Chen et al . , 2020 ) and add additional   table - aware embeddings ( Herzig et al . , 2020 ; Eisen-   schlos et al . , 2020 ) to enhance the table encoding .   However , in these datasets , the evidence is only   one given table , and models are not requested to   find out the evidence cells explicitly .   Fact Verification over Structured and Unstruc-   tured Evidence FEVEROUS ( Aly et al . , 2021 ) is   the first dataset of fact verification on structured and   unstructured evidence . Many previous works fol-   low the baseline settings and convert all evidence to   text format to perform evidence interaction . They   transform each cell to header - value pairs ( Aly et al . ,   2021 ; Malon , 2021 ) or in a cell location indication   type ( Kotonya et al . , 2021a ) . They pay less atten-   tion to making the converted text more consistent   with natural language expressions or identifying   what the context cells represent . Bouziane et al .   ( 2021 ) propose to convert all evidence to tables .   They simply convert each sentence to a 2 - cell table   with the Wikipedia title and itself instead of pack-   ing closely - tied evidence and building a global evi-   dence table . There are also works focusing on the   first two steps two improve the final results . Saeed   et al . ( 2021 ) propose to add a document re - ranker   to strengthen the document retrieval . Multi - hop   Dense Retriever ( Bouziane et al . , 2021 ) and T5   generator ( Malon , 2021 ) are introduced to better   extract multi - hop evidence .   6 Conclusion   In this paper , we propose DCUF , a dual channel   unified format model for fact verification over struc-   tured and unstructured data . With context - aware   evidence format conversion , DCUF gets a unified   text format representation of all evidence and a   global evidence table of them at the same time .   The dual channel design helps us make the most   of existing pre - trained language / table models to   encourage all evidence pieces to interact with each   other in their best forms as early as possible . Exper-   iments show that , with dual - channel unified format   encoding , our proposed DCUF achieves state - of-   the - art performance and also comparable results in   few - shot settings .   Acknowledgements   This work is supported in part by NSFC   ( 62161160339 ) . We would like to thank the anony-   mous reviewers and action editors for their helpfulcomments and suggestions . For any correspon-   dence , please contact Yansong Feng .   References5240   A Statistics of the FEVEROUS dataset   The FEVEROUS is an open - domain English   dataset . It contains 87,026 claims , and the claim   length is 225.3 on average . Each claim averagely   needs 1.4 sentences and 3.3 cells ( 0.8 tables ) to be   verified . 34,963 instances need only text format ev-   idence , 28,760 only table format and 24,667 need a   combination of the two formats . There are 49,115   instances labeled SUPPORTS , 33,669 labeled Re-   futes and the rest 4,242 instances are labeled NEI .   Detailed label and evidence distributions are shown   in Table 7.52415242