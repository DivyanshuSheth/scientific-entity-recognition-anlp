  Pedro Henrique MartinsZita MarinhoAndr√© F. T. MartinsInstituto de Telecomunica√ß√µesDeepMindInstitute of Systems and RoboticsLUMLIS ( Lisbon ELLIS Unit ) , Instituto Superior T√©cnicoUnbabel   Lisbon , Portugal   pedrohenriqueamartins@tecnico.ulisboa.pt ,   zmarinho@google.com , andre.t.martins@tecnico.ulisboa.pt .   Abstract   Transformers are unable to model long - term   memories effectively , since the amount of com-   putation they need to perform grows with   the context length . While variations of efÔ¨Å-   cient transformers have been proposed , they   all have a Ô¨Ånite memory capacity and are   forced to drop old information . In this paper ,   we propose the1 - former , which extends the   vanilla transformer with an unbounded long-   term memory . By making use of a continuous-   space attention mechanism to attend over the   long - term memory , the 1 - former ‚Äôs attention   complexity becomes independent of the con-   text length , trading off memory length with   precision . In order to control where pre-   cision is more important , 1 - former main-   tains ‚Äú sticky memories , ‚Äù being able to model   arbitrarily long contexts while keeping the   computation budget Ô¨Åxed . Experiments on   a synthetic sorting task , language modeling ,   and document grounded dialogue generation   demonstrate the1 - former ‚Äôs ability to retain   information from long sequences .   1 Introduction   When reading or writing a document , it is impor-   tant to keep in memory the information previously   read or written . Humans have a remarkable ability   to remember long - term context , keeping in mem-   ory the relevant details ( Carroll , 2007 ; Kuhbandner ,   2020 ) . Recently , transformer - based language mod-   els have achieved impressive results by increasing   the context size ( Radford et al . , 2018 , 2019 ; Dai   et al . , 2019 ; Rae et al . , 2019 ; Brown et al . , 2020 ) .   However , while humans process information se-   quentially , updating their memories continuously ,   and recurrent neural networks ( RNNs ) update a   single memory vector during time , transformers do   not ‚Äì they exhaustively query every representation   associated to the past events . Thus , the amountof computation they need to perform grows with   the length of the context , and , consequently , trans-   formers have computational limitations about how   much information can Ô¨Åt into memory . For exam-   ple , a vanilla transformer requires quadratic time to   process an input sequence and linear time to attend   to the context when generating every new word .   Several variations have been proposed to address   this problem ( Tay et al . , 2020b ) . Some propose   using sparse attention mechanisms , either with   data - dependent patterns ( Kitaev et al . , 2020 ; Vyas   et al . , 2020 ; Tay et al . , 2020a ; Roy et al . , 2021 ;   Wang et al . , 2021 ) or data - independent patterns   ( Child et al . , 2019 ; Beltagy et al . , 2020 ; Zaheer   et al . , 2020 ) , reducing the self - attention complexity   ( Katharopoulos et al . , 2020 ; Choromanski et al . ,   2021 ; Peng et al . , 2021 ; Jaegle et al . , 2021 ) , and   caching past representations in a memory ( Dai   et al . , 2019 ; Rae et al . , 2019 ) . These models are   able to reduce the attention complexity , and , conse-   quently , to scale up to longer contexts . However , as   their complexity still depends on the context length ,   they can not deal with unbounded context .   In this paper , we propose the 1 - former ( inÔ¨Ånite   former ; Fig . 1 ): a transformer model extended with   an unbounded long - term memory ( LTM ) , which   allows the model to attend to arbitrarily long con-   texts . The key for making the LTM unbounded   is a continuous - space attention framework ( Mar-   tins et al . , 2020 ) which trades off the number   of information units that Ô¨Åt into memory ( basis   functions ) with the granularity of their represen-   tations . In this framework , the input sequence is   represented as a continuous signal , expressed as   a linear combination of Nradial basis functions   ( RBFs ) . By doing this , the 1 - former ‚Äôs attention   complexity isO(L+LN)while the vanilla   transformer ‚Äôs isO(L(L+L ) ) , whereLand   L correspond to the transformer input size and   the long - term memory length , respectively ( details   in ¬ß 3.1.1 ) . Thus , this representation comes with5468two signiÔ¨Åcant advantages : ( i ) the context can be   represented using a number of basis functions N   smaller than the number of tokens , reducing the   attention computational cost ; and ( ii ) Ncan be   Ô¨Åxed , making it possible to represent unbounded   context in memory , as described in ¬ß 3.2 and Fig . 2 ,   without increasing its attention complexity . The   price , of course , is a loss in resolution : using a   smaller number of basis functions leads to lower   precision when representing the input sequence as   a continuous signal , as shown in Fig . 3 .   To mitigate the problem of losing resolution , we   introduce the concept of ‚Äú sticky memories ‚Äù ( ¬ß 3.3 ) ,   in which we attribute a larger space in the LTM ‚Äôs   signal to regions of the memory more frequently   accessed . This creates a notion of ‚Äú permanence ‚Äù in   the LTM , allowing the model to better capture long   contexts without losing the relevant information ,   which takes inspiration from long - term potentiation   and plasticity in the brain ( Mills et al . , 2014 ; Bamji ,   2005 ) .   To sum up , our contributions are the following :   ‚Ä¢We propose the1 - former , in which we ex-   tend the transformer model with a continuous   long - term memory ( ¬ß 3.1 ) . Since the attention   computational complexity is independent of   the context length , the 1 - former is able to   model long contexts .   ‚Ä¢We propose a procedure that allows the model   to keep unbounded context in memory ( ¬ß 3.2 ) .   ‚Ä¢We introduce sticky memories , a procedure   that enforces the persistence of important in-   formation in the LTM ( ¬ß 3.3 ) .   ‚Ä¢We perform empirical comparisons in a syn-   thetic task ( ¬ß 4.1 ) , which considers increas-   ingly long sequences , in language modeling   ( ¬ß 4.2 ) , and in document grounded dialogue   generation ( ¬ß 4.3 ) . These experiments show   the beneÔ¨Åts of using an unbounded memory .   2 Background   2.1 Transformer   A transformer ( Vaswani et al . , 2017 ) is composed   of several layers , which encompass a multi - head   self - attention layer followed by a feed - forward   layer , along with residual connections ( He et al . ,   2016 ) and layer normalization ( Ba et al . , 2016 ) .   Let us denote the input sequence as   X= [ x;:::;x]2R , whereLis theinput size and eis the embedding size of the   attention layer . The queries Q , keysK , and values   V , to be used in the multi - head self - attention   computation are obtained by linearly projecting   the input , or the output of the previous layer , X ,   for each attention head h :   Q = XW ; K = XW;V = XW ;   ( 1 )   whereW;W;W2Rare learnable   projection matrices , d== , andHis the num-   ber of heads . Then , the context representation   Z2R , that corresponds to each attention   headh , is obtained as :   Z = softmaxQKp   d   V ; ( 2 )   where the softmax is performed row - wise . The   head context representations are concatenated to   obtain the Ô¨Ånal context representation Z2R :   Z= [ Z;:::;Z]W ; ( 3 )   whereW2Ris another projection matrix   that aggregates all head ‚Äôs representations .   2.2 Continuous Attention   Continuous attention mechanisms ( Martins et al . ,   2020 ) have been proposed to handle arbitrary con-   tinuous signals , where the attention probability   mass function over words is replaced by a probabil-   itydensity over a signal . This allows time intervals   or compact segments to be selected .   To perform continuous attention , the Ô¨Årst step   is to transform the discrete text sequence rep-   resented by X2Rinto a continuous signal .   This is done by expressing it as a linear combina-   tion of basis functions . To do so , each x , with   i2f1;:::;Lg , is Ô¨Årst associated with a position   in an interval , t2[0;1],e.g . , by settingt = i = L.   Then , we obtain a continuous - space representation   X(t)2R , for anyt2[0;1]as :   X(t ) = B ( t ) ; ( 4 )   where ( t)2Ris a vector of NRBFs , e.g. ,    ( t ) = N(t;; ) , with2[0;1 ] , and   B2Ris a coefÔ¨Åcient matrix . Bis obtained   with multivariate ridge regression ( Brown et al . ,   1980 ) so that X(t)xfor eachi2[L ] , which   leads to the closed form ( see App . A for details ):   B = XF(FF+I)=XG ; ( 5)5469whereF= [ ( t ) ; : : : ; ( t)]2Rpacks the   basis vectors for the Llocations . As G2R   only depends of F , it can be computed ofÔ¨Çine .   Having converted the input sequence into a con-   tinuous signal X(t ) , the second step is to attend   over this signal . To do so , instead of having a   discrete probability distribution over the input se-   quence as in standard attention mechanisms ( like   in Eq . 2 ) , we have a probability density p , which   can be a Gaussian N(t;; ) , whereand   are computed by a neural component . A unimodal   Gaussian distribution encourages each attention   head to attend to a single region , as opposed to   scattering its attention through many places , and   enables tractable computation . Finally , having p ,   we can compute the context vector cas :   c = EX(t)   : ( 6 )   Martins et al . ( 2020 ) introduced the continuous   attention framework for RNNs . In the following   section ( ¬ß 3.1 ) , we will explain how it can be used   for transformer multi - head attention .   3 InÔ¨Ånite Memory Transformer   To allow the model to access long - range context ,   we propose extending the vanilla transformer with   a continuous LTM , which stores the input embed-   dings and hidden states of the previous steps . We   also consider the possibility of having two mem-   ories : the LTM and a short - term memory ( STM ) ,   which consists in an extension of the transformer ‚Äôs   hidden states and is attended to by the transformer ‚Äôs   self - attention , as in the transformer - XL ( Dai et al . ,   2019 ) . A diagram of the model is shown in Fig . 1 .   3.1 Long - term Memory   For simplicity , let us Ô¨Årst assume that the long-   term memory contains an explicit input discrete se-   quenceXthat consists of the past text sequence ‚Äôs   input embeddings or hidden states , depending on   the layer(we will later extend this idea to an un-   bounded memory in ¬ß 3.2 ) .   First , we need to transform Xinto a continuous   approximation X(t ) . We compute X(t)as :   X(t ) = B ( t ) ; ( 7 )   where ( t)2Rare basis functions and coef-   Ô¨ÅcientsB2Rare computed as in Eq . 5 ,   B = XG . Then , we can compute the LTM   keys , K2R , and values , V2R , for   each attention head h , as :   K = BW ; V = BW ; ( 8)   whereW;W2Rare learnable projection   matrices . For each query qfori2f1;:::;Lg ,   we use a parameterized network , which takes as   input the attention scores , to compute 2]0;1 [   and2R :   =sigmoid   aneKqp   d   ( 9 )   =softplus   aneKqp   d   :( 10 )   Then , using the continuous softmax transforma-   tion ( Martins et al . , 2020 ) , we obtain the probability   densitypasN(t;; ) .   Finally , having the value function V(t)given   asV(t ) = V ( t);we compute the head - speciÔ¨Åc   representation vectors as in Eq . 6 :   z = E[V ] = VE [ ( t ) ] ( 11 )   which form the rows of matrix Z2R   that goes through an afÔ¨Åne transformation ,   Z= [ Z;:::;Z]W.   The long - term representation , Z , is then   summed to the transformer context vector , Z , to   obtain the Ô¨Ånal context representation Z2R :   Z = Z+Z ; ( 12 )   which will be the input to the feed - forward layer.5470   3.1.1 Attention Complexity   As the1 - former makes use of a continuous-   space attention framework ( Martins et al . , 2020 )   to attend over the LTM signal , its key matrix   sizeK2Rdepends only on the number   of basis functions N , but not on the length   of the context being attended to . Thus , the   1 - former ‚Äôs attention complexity is also indepen-   dent of the context ‚Äôs length . It corresponds to   O(L(L+L ) + LN)when also using   a short - term memory and O(L+LN)when   only using the LTM , both  O ( L(L+L ) ) ,   which would be the complexity of a vanilla trans-   former attending to the same context . For this rea-   son , the1 - former can attend to arbitrarily long   contexts without increasing the amount of compu-   tation needed .   3.2 Unbounded Memory   When representing the memory as a discrete se-   quence , to extend it , we need to store the new hid-   den states in memory . In a vanilla transformer , this   is not feasible for long contexts due to the high   memory requirements . However , the 1 - former   can attend to unbounded context without increasing   memory requirements by using continuous atten-   tion , as next described and shown in Fig . 2 .   To be able to build an unbounded representation ,   we Ô¨Årst sample Mlocations in [ 0;1]and evaluate   X(t)at those locations . These locations can simply   be linearly spaced , or sampled according to the   region importance , as described in ¬ß 3.3 .   Then , we concatenate the corresponding vectors   with the new vectors coming from the short - term   memory . For that , we Ô¨Årst need to contract this   function by a factor of  2]0;1[to make room forthe new vectors . We do this by deÔ¨Åning :   X(t ) = X(t=  ) = B ( t=  ): ( 13 )   Then , we can evaluate X(t)at theMlocations   0t;t;:::;t  as :   x = B ( t=  ) ; form2[M ] ; ( 14 )   and deÔ¨Åne a matrix X= [ x;x;:::;x]2   Rwith these vectors as rows . After that , we   concatenate this matrix with the new vectors X ,   obtaining :   X = h   X;Xi   2R:(15 )   Finally , we simply need to perform multivari-   ate ridge regression to compute the new coefÔ¨Å-   cient matrix B2R , viaB = XG , as in   Eq . 5 . To do this , we need to associate the vec-   tors inXwith positions in [ 0 ;  ] and inX   with positions in ]  ; 1]so that we obtain the matrix   G2R. We consider the vectors posi-   tions to be linearly spaced .   3.3 Sticky Memories   When extending the LTM , we evaluate its current   signal atMlocations in [ 0;1 ] , as shown in Eq . 14 .   These locations can be linearly spaced . However ,   some regions of the signal can be more relevant   than others , and should consequently be given a   larger ‚Äú memory space ‚Äù in the next step LTM ‚Äôs sig-   nal . To take this into account , we propose sampling   theMlocations according to the signal ‚Äôs relevance   at each region ( see Fig . 6 in App . B ) . To do so ,   we construct a histogram based on the attention   given to each interval of the signal on the previ-   ous step . For that , we Ô¨Årst divide the signal into5471Dlinearly spaced bins fd;:::;dg . Then , we   compute the probability given to each bin , p(d )   forj2f1;:::;Dg , as :   p(d)/XXZN(t;;)dt ; ( 16 )   whereHis the number of attention heads and L   is the sequence length . Note that Eq . 16 ‚Äôs integral   can be evaluated efÔ¨Åciently using the erf function :   ZN(t;; ) = 1   2   erfbp   2    erfap   2   :   ( 17 )   Then , we sample the Mlocations at which the   LTM ‚Äôs signal is evaluated at , according to p. By   doing so , we evaluate the LTM ‚Äôs signal at the re-   gions which were considered more relevant by the   previous step ‚Äôs attention , and will , consequently   attribute a larger space of the new LTM ‚Äôs signal to   the memories stored in those regions .   3.4 Implementation and Learning Details   Discrete sequences can be highly irregular and ,   consequently , difÔ¨Åcult to convert into a continuous   signal through regression . Because of this , before   applying multivariate ridge regression to convert   the discrete sequence Xinto a continuous signal ,   we use a simple convolutional layer ( with stride =   1andwidth = 3 ) as a gate , to smooth the sequence :   ~X = sigmoid ( CNN(X ) )  X : ( 18 )   To train the model we use the cross entropy loss .   Having a sequence of text Xof lengthLas input ,   a language model outputs a probability distribution   of the next word p(xjx;:::;x ) . Given a   corpus ofTtokens , we train the model to minimize   its negative log likelihood :   L= Xlogp(xjx;:::;x):(19 )   Additionally , in order to avoid having uniform   distributions over the LTM , we regularize the con-   tinuous attention given to the LTM , by minimizing   the Kullback - Leibler ( KL ) divergence , D , be-   tween the attention probability density , N(; ) ,   and a Gaussian prior , N(; ) . As different   heads can attend to different regions , we set =    , regularizing only the attention variance , andget :   L = XXD(N(;)jjN(; ) )   ( 20 )   = XX1   2    log       1   :( 21 )   Thus , the Ô¨Ånal loss that is minimized corre-   sponds to :   L = L+L ; ( 22 )   whereis a hyperparameter that controls the   amount of KL regularization .   4 Experiments   To understand if the 1 - former is able to model   long contexts , we Ô¨Årst performed experiments on a   synthetic task , which consists of sorting tokens by   their frequencies in a long sequence ( ¬ß 4.1 ) . Then ,   we performed experiments on language modeling   ( ¬ß 4.2 ) and document grounded dialogue genera-   tion ( ¬ß 4.3 ) by Ô¨Åne - tuning a pre - trained language   model .   4.1 Sorting   In this task , the input consists of a sequence of   tokens sampled according to a token probability   distribution ( which is not known to the system ) .   The goal is to generate the tokens in the decreasing   order of their frequencies in the sequence . One   example can be :   1 2 1 3 1 0 3 1 3 2|{z}<>1 3 2 0   To understand if the long - term memory is being   effectively used and the transformer is not only   performing sorting by modeling the most recent   tokens , we design the token probability distribution   tochange over time : namely , we set it as a mixture   of two distributions , p=  p+ ( 1   ) p , where   the mixture coefÔ¨Åcient  2[0;1]is progressively   increased from 0 to 1 as the sequence is generated .   The vocabulary has 20 tokens and we experiment   with sequences of length 4,000 , 8,000 , and 16,000.5472   Baselines . We consider the transformer - XL   ( Dai et al . , 2019 ) and the compressive transformer   ( Rae et al . , 2019 ) as baselines . The transformer - XL   consists of a vanilla transformer ( Vaswani et al . ,   2017 ) extended with a short - term memory which is   composed of the hidden states of the previous steps .   The compressive transformer is an extension of the   transformer - XL : besides the short - term memory , it   has a compressive long - term memory which is com-   posed of the old vectors of the short - term memory ,   compressed using a CNN . Both the transformer - XL   and the compressive transformer require relative   positional encodings . In contrast , there is no need   for positional encodings in the memory in our ap-   proach since the memory vectors represent basis   coefÔ¨Åcients in a predeÔ¨Åned continuous space .   For all models we used a transformer with 3 lay-   ers and 6 attention heads , input size L= 1;024   and memory size 2,048 . For the compressive trans-   former , both memories have size 1,024 . For the   1 - former , we also consider a STM of size 1,024   and a LTM with N= 1;024basis functions , hav-   ing the models the same computational cost . Fur-   ther details are described in App . C.1 .   Results . As can be seen in the left plot of Fig . 3 ,   the transformer - XL achieves a slightly higher   accuracy than the compressive transformer and   1 - former for a short sequence length ( 4,000 ) . This   is because the transformer - XL is able to keep al-   most the entire sequence in memory . However ,   its accuracy degrades rapidly when the sequence   length is increased . Both the compressive trans - former and1 - former also lead to smaller accura-   cies when increasing the sequence length , as ex-   pected . However , this decrease is not so signiÔ¨Åcant   for the1 - former , which indicates that it is better   at modeling long sequences .   Regression error analysis . To better understand   the trade - off between the 1 - former ‚Äôs memory pre-   cision and its computational efÔ¨Åciency , we ana-   lyze how its regression error and sorting accuracy   vary when varying the number of basis functions   used , on the sorting task with input sequences of   length 8,000 . As can be seen in the right plot of   Fig . 3 , the sorting accuracy is negatively correlated   with the regression error , which is positively cor-   related with the number of basis functions . It can   also be observed , that when increasing substantially   the number of basis functions the regression error   reaches a plateau and the accuracy starts to drop .   We posit that the latter is caused by the model hav-   ing a harder task at selecting the locations it should   attend to . This shows that , as expected , when in-   creasing1 - former ‚Äôs efÔ¨Åciency or increasing the   size of context being modeled , the memory loses   precision .   4.2 Language Modeling   To understand if long - term memories can be used to   extend a pre - trained language model , we Ô¨Åne - tune   GPT-2 small ( Radford et al . , 2019 ) on Wikitext-   103 ( Merity et al . , 2017 ) and a subset of PG-19   ( Rae et al . , 2019 ) containing the Ô¨Årst 2,000 books   ( 200 million tokens ) of the training set . To do   so , we extend GPT-2 with a continuous long - term   memory ( 1 - former ) and a compressed memory   ( compressive transformer ) with a positional bias,5473   based on Press et al . ( 2021 ) .   For these experiments , we consider transform-   ers with input size L= 512 , for the compressive   transformer we use a compressed memory of size   512 , and for the1 - former we consider a LTM with   N= 512 Gaussian RBFs and a memory threshold   of 2,048 tokens , having the same computational   budget for the two models . Further details and   hyperparameters are described in App . C.2 .   Results . The results reported in Table 1 show that   the1 - former leads to perplexity improvements on   both Wikitext-103 and PG19 , while the compres-   sive transformer only has a slight improvement   on the latter . The improvements obtained by the   1 - former are larger on the PG19 dataset , which   can be justiÔ¨Åed by the nature of the datasets : books   have more long range dependencies than Wikipedia   articles ( Rae et al . , 2019 ) .   4.3 Document Grounded Dialogue   In document grounded dialogue generation , besides   the dialogue history , models have access to a doc-   ument concerning the conversation ‚Äôs topic . In the   CMU Document Grounded Conversation dataset   ( CMU - DoG ) ( Zhou et al . , 2018 ) , the dialogues are   about movies and a summary of the movie is given   as the auxiliary document ; the auxiliary document   is divided into parts that should be considered for   the different utterances of the dialogue . In this   paper , to evaluate the usefulness of the long - term   memories , we make this task slightly more chal-   lenging by only giving the models access to the   document before the start of the dialogue .   We Ô¨Åne - tune GPT-2 small ( Radford et al . , 2019 )   using an approach based on Wolf et al . ( 2019 ) .   To allow the model to keep the whole document   on memory , we extend GPT-2 with a continuous   LTM ( 1 - former ) with N= 512 basis functions .   As baselines , we use GPT-2 , with and without ac-   cess ( GPT-2 w/o doc ) to the auxiliary document ,   with input size L= 512 , and GPT-2 with a com-   pressed memory with attention positional biases   ( compressive ) , of size 512 . Further details and   hyper - parameters are stated in App . C.3 .   To evaluate the models we use the metrics : per-   plexity , F1 score , Rouge-1 and Rouge - L ( Lin ,   2004 ) , and Meteor ( Banerjee and Lavie , 2005 ) .   Results . As shown in Table 2 , by keeping   the whole auxiliary document in memory , the   1 - former and the compressive transformer are   able to generate better utterances , according to   all metrics . While the compressive and 1 - former   achieve essentially the same perplexity in this task ,   the1 - former achieves consistently better scores   on all other metrics . Also , using sticky memo-   ries leads to slightly better results on those metrics ,   which suggests that attributing a larger space in the   LTM to the most relevant tokens can be beneÔ¨Åcial .   Analysis . In Fig . 4 , we show examples of ut-   terances generated by 1 - former along with the   excerpts from the LTM that receive higher atten-   tion throughout the utterances ‚Äô generation . In these   examples , we can clearly see that these excerpts   are highly pertinent to the answers being generated .   Also , in Fig . 5 , we can see that the phrases which   are attributed larger spaces in the LTM , when using   sticky memories , are relevant to the conversations .   5 Related Work   Continuous attention . Martins et al . ( 2020 ) in-   troduced 1D and 2D continuous attention , using   Gaussians and truncated parabolas as densities .   They applied it to RNN - based document classi-   Ô¨Åcation , machine translation , and visual question   answering . Several other works have also proposed   the use of ( discretized ) Gaussian attention for natu-   ral language processing tasks : Guo et al . ( 2019 )   proposed a Gaussian prior to the self - attention   mechanism to bias the model to give higher atten-   tion to nearby words , and applied it to natural lan-   guage inference ; You et al . ( 2020 ) proposed the use5474   of hard - coded Gaussian attention as input - agnostic   self - attention layer for machine translation ; Dubois   et al . ( 2020 ) proposed using Gaussian attention as a   location attention mechanism to improve the model   generalization to longer sequences . However , these   approaches still consider discrete sequences and   compute the attention by evaluating the Gaussian   density at the token positions . Farinhas et al . ( 2021 )   extend continuous attention to multimodal densi-   ties , i.e. , mixtures of Gaussians , and applied it to   VQA . In this paper , we opt for the simpler case ,   an unimodal Gaussian , and leave sparse and multi-   modal continuous attention for future work .   EfÔ¨Åcient transformers . Several methods have   been proposed that reduce the transformer ‚Äôs at-   tention complexity , and can , consequently , model   longer contexts . Some of these do so by perform-   ing sparse attention , either by selecting pre - deÔ¨Åned   attention patterns ( Child et al . , 2019 ; Beltagy et al . ,   2020 ; Zaheer et al . , 2020 ) , or by learning these   patterns from data ( Kitaev et al . , 2020 ; Vyas et al . ,   2020 ; Tay et al . , 2020a ; Roy et al . , 2021 ; Wang   et al . , 2021 ) . Other works focus on directly re-   ducing the attention complexity by applying the   ( reversed ) kernel trick ( Katharopoulos et al . , 2020 ;   Choromanski et al . , 2021 ; Peng et al . , 2021 ; Jae-   gle et al . , 2021 ) . Closer to our approach are the   transformer - XL and compressive transformer mod-   els ( Dai et al . , 2019 ; Rae et al . , 2019 ) , which extend   the vanilla transformer with a bounded memory . Memory - augmented language models . RNNs ,   LSTMs , and GRUs ( Hochreiter et al . , 1997 ; Cho   et al . , 2014 ) have the ability of keeping a memory   state of the past . However , these require backprop-   agation through time which is impractical for long   sequences . Because of this , Graves et al . ( 2014 ) ,   Weston et al . ( 2014 ) , Joulin and Mikolov ( 2015 )   and Grefenstette et al . ( 2015 ) proposed extending   RNNs with an external memory , while Chandar   et al . ( 2016 ) and Rae et al . ( 2016 ) proposed efÔ¨Å-   cient procedures to read and write from these mem-   ories , using hierarchies and sparsity . Grave et al .   ( 2016 ) and Merity et al . ( 2017 ) proposed the use   of cache - based memories which store pairs of hid-   den states and output tokens from previous steps .   The distribution over the words in the memory is   then combined with the distribution given by the   language model . More recently , Khandelwal et al .   ( 2019 ) and Yogatama et al . ( 2021 ) proposed using   nearest neighbors to retrieve words from a key-   based memory constructed based on the training   data . Similarly , Fan et al . ( 2021 ) proposed retriev-   ing sentences from a memory based on the training   data and auxiliary information . Khandelwal et al .   ( 2019 ) proposed interpolating the retrieved words   probability distributions with the probability over   the vocabulary words when generating a new word ,   while Yogatama et al . ( 2021 ) and Fan et al . ( 2021 )   proposed combining the information at the architec-   ture level . These models have the disadvantage of   needing to perform a retrieval step when generating5475each token / utterance , which can be computation-   ally expensive . These approaches are orthogonal   to the1 - former ‚Äôs LTM and in future work the two   can be combined .   6 Conclusions   In this paper , we proposed the 1 - former : a trans-   former extended with an unbounded long - term   memory . By using a continuous - space attention   framework , its attention complexity is independent   of the context ‚Äôs length , which allows the model   to attend to arbitrarily long contexts while keep-   ing a Ô¨Åxed computation budget . By updating the   memory taking into account past usage , the model   keeps ‚Äú sticky memories ‚Äù , enforcing the persistence   of relevant information in memory over time . Ex-   periments on a sorting synthetic task show that 1-   former scales up to long sequences , maintaining   a high accuracy . Experiments on language model-   ing and document grounded dialogue generation   by Ô¨Åne - tuning a pre - trained language model have   shown improvements across several metrics .   Ethics Statement   Transformer models that attend to long contexts ,   to improve their generation quality , need large   amounts of computation and memory to perform   self - attention . In this paper , we propose an exten-   sion to a transformer model that makes the attention   complexity independent of the length of the con-   text being attended to . This can lead to a reduced   number of parameters needed to model the same   context , which can , consequently , lead to gains in   efÔ¨Åciency and reduce energy consumption .   On the other hand , the 1 - former , as well as the   other transformer language models , can be used on   questionable scenarios , such as the generation of   fake news ( Zellers et al . , 2019 ) , defamatory text   ( Wallace et al . , 2019 ) , or other undesired content .   Acknowledgments   This work was supported by the European Research   Council ( ERC StG DeepSPIN 758969 ) , by the   P2020 project MAIA ( contract 045909 ) , by the   Funda√ß√£o para a Ci√™ncia e Tecnologia through   project PTDC / CCI - INF/4703/2021 ( PRELUNA ,   contract UIDB/50008/2020 ) , by the EU H2020   SELMA project ( grant agreement No 957017 ) , and   by contract PD / BD/150633/2020 in the scope of   the Doctoral Program FCT - PD/00140/2013 NET-   SyS. We thank Jack Rae , Tom Schaul , the SAR - DINE team members , and the reviewers for helpful   discussion and feedback .   References547654775478A Multivariate ridge regression   The coefÔ¨Åcient matrix B2Ris obtained   with multivariate ridge regression criterion so that   X(t)xfor eachi2[L ] , which leads to the   closed form :   B= arg minjjBF Xjj+jjBjj   ( 23 )   = XF(FF+I)=XG ;   whereF= [ ( t ) ; : : : ; ( t)]packs the basis vec-   tors forLlocations andjjjjis the Frobenius   norm . AsG2Ronly depends of F , it can be   computed ofÔ¨Çine .   B Sticky memories   We present in Fig . 6 a scheme of the sticky memo-   ries procedure . First we sample Mlocations from   the previous step LTM attention histogram ( Eq .   16 ) . Then , we evaluate the LTM ‚Äôs signal at the   sampled locations ( Eq . 14 ) . Finally , we consider   that the sampled vectors , X , are linearly spaced   in[0 ;  ] . This way , the model is able to attribute   larger spaces of its memory to the relevant words .   C Experimental details   C.1 Sorting   For the compressive transformer , we consider com-   pression rates of size 2 for sequences of length   4,000 , from 2to6for sequences of length 8,000 ,   and from 2to12for sequences of length 16,000 .   We also experiment training the compressive trans-   former with and without the attention reconstruc-   tion auxiliary loss . For the 1 - former we con-   sider 1,024 Gaussian RBFs N(t ; ~;~)with ~lin-   early spaced in [ 0;1]and~2f:01;:05 g. We set   = 0:75and for the KL regularization we used   = 110and= 0:05 .   For this task , for each sequence length , we cre-   ated a training set with 8,000 sequences and valida-   tion and test sets with 800 sequences . We trained   all models with batches of size 8 for 20 epochs on 1   Nvidia GeForce RTX 2080 Ti or 1 Nvidia GeForce   GTX 1080 Ti GPU with 11Gb of memory , using   the Adam optimizer ( Kingma and Ba , 2015 ) . For   the sequences of length 4,000 and 8,000 we used a   learning rate of 2:510while for sequences of   length 16,000 we used a learning rate of 210 .   The learning rate was decayed to 0 until the end of   training with a cosine schedule . C.2 Pre - trained Language Models   In these experiments , we Ô¨Åne - tune the GPT-2 small ,   which is composed of 12 layers with 12 attention   heads , on the English dataset Wikitext-103 and on   a subset of the English dataset PG19containing   the Ô¨Årst 2,000 books . We consider an input size   L= 512 and a long - term memory with N= 512   Gaussian RBFsN(t ; ~;~)with~linearly spaced   in[0;1]and~2f:005;:01gand for the KL regu-   larization we use = 110and= 0:05 .   We set  = 0:5 . For the compressive transformer   we also consider a compressed memory of size 512   with a compression rate of 4 , and train the model   with the auxiliary reconstruction loss .   We Ô¨Åne - tuned GPT-2 small with a batch size of   1 on 1 Nvidia GeForce RTX 2080 Ti or 1 Nvidia   GeForce GTX 1080 Ti GPU with 11Gb of mem-   ory , using the Adam optimizer ( Kingma and Ba ,   2015 ) for 1 epoch with a learning rate of 510   for the GPT-2 parameters and a learning rate of   2:510for the LTM parameters .   C.3 Document Grounded Generation   In these experiments , we Ô¨Åne - tune the GPT-2   small , which is composed of 12 layers with 12   attention heads , on the English dataset CMU -   Document Grounded Conversations(CMU - DoG.   CMU - DoG has 4112 conversations , being the pro-   portion of train / validation / test split 0.8/0.05/0.15 .   We consider an input size L= 512 and a long-   term memory with N= 512 Gaussian RBFs   N(t ; ~;~)with ~linearly spaced in [ 0;1]and   ~2f:005;:01gand for the KL regularization we   use= 110and= 0:05 . We set   = 0:5 . For the compressive transformer we con-   sider a compressed memory of size 512 with a   compression rate of 3 , and train the model with the   auxiliary reconstruction loss . We Ô¨Åne - tuned GPT-2   small with a batch size of 1 on 1 Nvidia GeForce   RTX 2080 Ti or 1 Nvidia GeForce GTX 1080 Ti   GPU with11Gb of memory , using the Adam   optimizer ( Kingma and Ba , 2015 ) with a linearly   decayed learning rate of 510 , for 5 epochs .   D Additional experiments   We also perform language modeling experiments   on the Wikitext-103 dataset(Merity et al . , 2017)5479   which has a training set with 103 million tokens and   validation and test sets with 217,646 and 245,569   tokens , respectively . For that , we follow the stan-   dard architecture of the transformer - XL ( Dai et al . ,   2019 ) , which consists of a transformer with 16 lay-   ers and 10 attention heads . For the transformer - XL ,   we experiment with a memory of size 150 . For   the compressive transformer , we consider that both   memories have a size of 150 and a compression   rate of 4 . For the1 - former we consider a short-   term memory of size 150 , a continuous long - term   memory with 150 Gaussian RBFs , and a memory   threshold of 900 tokens .   For this experiment , we use a transformer with   16 layers , 10 heads , embeddings of size 410 , and   a feed - forward hidden size of 2100 . For the com-   pressive transformer , we follow Rae et al . ( 2019 )   and use a compression rate of 4 and the attention   reconstruction auxiliary loss . For the 1 - former we   consider 150 Gaussian RBFs N(t ; ~;~)with ~   linearly spaced in [ 0;1]and~2f:01;:05 g. We   set  = 0:5and for the KL regularization we used   = 110and= 0:1 .   We trained all models , from scratch , with   batches of size 40 for 250,000 steps on 1 Nvidia   Titan RTX or 1 Nvidia Quadro RTX 6000 with   24GPU Gb of memory using the Adam opti-   mizer ( Kingma and Ba , 2015 ) with a learning rate   of2:510 . The learning rate was decayed to 0   until the end of training with a cosine schedule .   Results . As can be seen in Table 3 , extending the   model with a long - term memory leads to a better   perplexity , for both the compressive transformer   and1 - former . Moreover , the 1 - former slightly   outperforms the compressive transformer . We can   also see that using sticky memories leads to a some-   what lower perplexity , which shows that it helps   the model to focus on the relevant memories .   Analysis . To better understand whether 1-   former is paying more attention to the older mem-   ories in the LTM or to the most recent ones , we   plotted a histogram of the attention given to each   region of the long - term memory when predicting   the tokens on the validation set . As can be seen in   Fig . 7 , in the Ô¨Årst and middle layers , the 1 - former   tends to focus more on the older memories , while   in the last layer , the attention pattern is more uni-   form . In Figs . 8 and 9 , we present examples of   words for which the 1 - former has lower perplexity   than the transformer - XL along with the attention   given by the1 - former to the last layer ‚Äôs LTM . We   can see that the word being predicted is present sev-5480eral times in the long - term memory and 1 - former   gives higher attention to those regions .   To know whether the sticky memories approach   attributes a larger space of the LTM ‚Äôs signal to   relevant phrases or words , we plotted the memory   space given to each wordpresent in the long-   term memory of the last layer when using and not   using sticky memories . We present examples in   Figs . 10 and 11 along with the phrases / words   which receive the largest spaces when using sticky   memories . We can see in these examples that this   procedure does in fact attribute large spaces to old   memories , creating memories that stick over time .   We can also see that these memories appear to be   relevant as shown by the words / phrases in the   examples .   E Additional examples   In Fig . 12 , we show additional examples of utter-   ances generated by 1 - former along with the ex-   cerpts from the LTM that receive higher attention   throughout the utterances ‚Äô generation .   Additionally , ground truth conversations con-   cerning the movies ‚Äú Toy Story ‚Äù and ‚Äú La La Land ‚Äù ,   for which the sticky memories are stated in Fig . 5 ,   are shown in Tables 4 and 5 , respectively.54815482548354845485