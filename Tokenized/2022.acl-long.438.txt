  Bei Li , Chuanhao Lv , Zefan Zhou , Tao Zhou ,   Tong Xiao , Anxiang Maand Jingbo ZhuSchool of Computer Science and Engineering , Northeastern University , Shenyang , ChinaNiuTrans Research , Shenyang , China   { libei_neu,lch-sy,ZhouZefan_zzf,zhoutao_neu}@outlook.com   { xiaotong,maanxiang,zhujingbo}@mail.neu.edu.cn   Abstract   Previous work on multimodal machine trans-   lation ( MMT ) has focused on the way of in-   corporating vision features into translation but   little attention is on the quality of vision mod-   els . In this work , we investigate the impact   of vision models on MMT . Given the fact that   Transformer is becoming popular in computer   vision , we experiment with various strong   models ( such as Vision Transformer ) and en-   hanced features ( such as object - detection and   image captioning ) . We develop a selective at-   tention model to study the patch - level contribu-   tion of an image in MMT . On detailed probing   tasks , we Ô¨Ånd that stronger vision models are   helpful for learning translation from the visual   modality . Our results also suggest the need of   carefully examining MMT models , especially   when current benchmarks are small - scale and   biased . Our code could be found at .   1 Introduction   Multimodal machine translation ( MMT ) has   emerged as an active Ô¨Åeld of research which mar-   ries the worlds of computer vision ( CV ) and natural   language processing ( NLP ) ( Specia et al . , 2016 ) .   Early models of this kind produce a translation   given the fused representation of both the visual   and textual inputs ( Caglayan et al . , 2016 ; Libovick√Ω   and Helcl , 2017 ; Calixto and Liu , 2017 ) . As ex-   pected , such a paradigm achieves promising BLEU   improvements and inspires the community to fol-   low up .   But soon researchers found that MMT systems   did not act as what they ordinarily designed : the   visual modality contributes to translation little . For   example , it is not harmful to MMT systems when   the input image is irrelevant to the text ( Gr√∂nroos   et al . , 2018 ; Lala et al . , 2018 ) , or even when the   vision features are absent ( Elliott , 2018 ) . More re-   cently , Wu et al . ( 2021 ) have pointed out that theuse of the visual modality is a way of regulariza-   tion for training but not a complement to the text   modality . As another response to the analysis of   MMT , Caglayan et al . ( 2019 ) investigate how the   vision features correlate to the text . They Ô¨Ånd that   the input image helps translation when some of the   input words are masked .   Note that previous work has for the most part   focused on integrating off - the - shelf vision models   ( such as ResNet-50 ) into MMT . The underlying   assumption here is that the existing vision models   are powerful enough to encode the image . This   implicitly ignores the quality of vision models in   representing images . But computer vision is facing   a new trend by moving from CNNs to Transformer   as the backbone model ( Dosovitskiy et al . , 2021 ;   Liu et al . , 2021b ; Carion et al . , 2020 ) . A natu-   ral question that arises is : how will MMT systems   behave if stronger vision models are adopted ?   In this work , we address this question by a sys-   tematic study of using various vision models in   MMT , in particular using the most successful mod-   els in recent studies ( such as Vision Transformer ,   or ViT for short ) . We Ô¨Ånd that the patch method   used in Transformer - based vision models offers an   opportunity to detail the patch - level contribution of   the image . This leads us to develop a selective atten-   tion model to correlate words with image patches .   Beyond this , we introduce object - detection and   image captioning features into MMT for further   improvements of the vision models ( Carion et al . ,   2020 ; Fang et al . , 2021 ) .   Following Caglayan et al . ( 2019 ) ‚Äôs work , we   design more detailed probing tasks to examine   to what degree the visual modality contributes to   MMT . We run an extensive set of experiments on   En - De and En - Fr MMT tasks . Our Ô¨Åndings are   Stronger vision models help . For example ,   ViT can beat ResNet-50 on the probing tasks   though the superiority is not signiÔ¨Åcant on   standard MMT data.6327   Automatic evaluation on current MMT tasks   might not be a good indicator for the effec-   tiveness of MMT models . For example , mod-   els enhanced with object - detection and image   captioning features yield good BLEU scores   on the original MMT task but show modest or   no contributions on the probing tasks .   We hope that the results here can inspire more   research on exploring better vision models and eval-   uation methods for multimodal NLP .   2 Preliminary   We start with a description of the probing tasks .   It is followed by a design of vision features and   a selective attention mechanism for introducing   ViT - like representations into MMT .   2.1 InsufÔ¨Åcient Text Generation   To know how an image contributes to translation , a   way is to mask some of the input words ( call this   insufÔ¨Åcient text ) and force the translation model   to learn from the image . Following the previous   design of color deprivation and entity - based mask-   ing , we present detailed probing tasks which are   complementary to Caglayan et al . ( 2019 ) ‚Äôs work .   In preliminary experiments , we Ô¨Ånd that ‚Äú color ‚Äù ,   ‚Äú character ‚Äù and ‚Äú noun ‚Äù are three kinds of words   which could be complemented according to the   visual modality once the corresponding texts are   masked . The following probing tasks are designed   accordingly .   Color - based Probing In training , all source   words referring to a color are replaced by a spe-   cial token [ Mask_C ] . There are 8;919sentences   involving color words , and nearly one third of them   involve more than one color . It is worth noting that   each color may have two or more translations due   to the rich morphology in German and French . For   example , the English ‚Äú green ‚Äù can be translated to‚Äúgr√ºn ‚Äù , ‚Äú gr√ºne ‚Äù , ‚Äú gr√ºnes ‚Äù , ‚Äú gr√ºner ‚Äù , ‚Äú gr√ºnen ‚Äù and   ‚Äú gr√ºnem ‚Äù in German . We design two criteria to   measure the accuracy of translation . The Ô¨Årst cri-   terion is strict . The correct translation requires   generating the same color and the same gender as   in reference translations . The second criterion is   relaxed and all translations expressing the same   color are correct .   Character - based Probing For character words ,   we choose ‚Äú man ‚Äù , ‚Äú woman ‚Äù , ‚Äú people ‚Äù , ‚Äú men ‚Äù ,   ‚Äú girl ‚Äù and ‚Äú boy ‚Äù . More than 60 % sentences contain   character words in our training data , so they are   reasonable indicators of assessing the ability to in-   fer correct translations from the input image . Here   we use [ MASK_P ] for masking . Note that some   character words have more than two translations ,   e.g. ‚Äú people ‚Äù , we also use the same evaluation   metric with the color - based probing task , including   relaxed and strict two criteria .   Noun - based Probing For more complex scenar-   ios , a sentence can be masked with several kinds of   ambiguous words , such as animals , clothing , and   vehicles , provided by Flickr30 K ( Plummer et al . ,   2015 ) . High - frequency words labeled with noun ( or   nouns ) are more likely to be masked as [ MASK_N ]   ( or [ MASK_NS ] ) ) . See Table 1 for example insufÔ¨Å-   cient text with different numbers of masks .   2.2 Various Vision Features   In addition to ResNet-50 , we choose several   Transformer - based vision models .   General Backbone . Vision Transformer ( ViT )   and Swin Transformer are popular models in   computer vision ( Dosovitskiy et al . , 2021 ; Liu   et al . , 2021b ) . We use ViT with various model   capacities to vary from weak to strong ViT   models .   Object - detection . For pretrained object-   detection vision models , we choose DETR   ( Carion et al . , 2020 ) and QueryInst ( Fang   et al . , 2021 ) for their strong performance.6328Vision Transformer   Image Captioning . For image captioning   models , we choose CATRbecause it is a   Transformer - based image captioning architec-   ture and can be easily implemented on top of   ViT.   We form a number of vision features by combin-   ing the methods described above . More details are   presented in Section 3 .   2.3 Selective Attention   ViT and related models perform in almost the same   way as Transformer in NLP ( Vaswani et al . , 2017 ) .   Unlike the general models in CV , ViT does not   represent the image as a single vector . Instead , it   generates a sequence of patches for image repre-   sentation . An advantage of this design is that we   can use the attention mechanism to correlate image   patches to words . Thus , we present a selective at-   tention model to model the patch - level contribution   of the image . See Figure 1 for the architecture .   Text - only Transformer Transformer follows an   encoder - decoder paradigm ( the purple region in   Figure 1 ) . The encoder is a stack of identical   layers . Each layer consists of a self - attention ( SAN )   block and a feedforward network ( FFN ) block . The   decoder shares a similar design with the encoder ,   but with an additional cross - attention block .   Gated Fusion Gated fusion mechanism is a pop-   ular technique for fusing representations from dif-   ferent sources ( Wu et al . , 2021 ; Zhang et al . , 2020 ;   Lin et al . , 2020 ; Yin et al . , 2020 ) . Given the textinputXand the image input X , the text rep-   resentationHand the image feature Hcan   be deÔ¨Åned as :   H = TransformerEncoder ( X)(1 )   H = WViT(X ) ( 2 )   whereWis a projection matrix to convert the shape   ofViT(X)into that ofH. Note that ViT( )   can be replaced by other vision models , e.g. DETR ,   Swin Transformer and etc . Then , the gate 2   [ 0;1]and the fuzed output are deÔ¨Åned as :   =Sigmoid ( UH+VH)(3 )   H= ( 1 )H+H(4 )   whereUandVare trainable variables . controls   how much visual information is kept . Then , the   fusion vector His fed into the decoder . See   the right side of the pink region in Figure 1 for an   illustration of the gated fusion models .   Selective Attention After obtaining the text and   image representations ( or features ) , we use a single-   head attention network to correlate words with im-   age patches , where the query , key and value are   H , HandH , respectively . Then the se-   lective attention output His deÔ¨Åned to be :   H = Softmax ( QK   pd)V ( 5 )   wheredis the same as the dimension of H   because a single head is used . Then the fused rep-   resentation could be obtained by using Eqs . 3 and   4 and replacing HwithH.6329   3 Experiments   3.1 Datasets   We conducted experiments on the widely used   Multi30 K benchmark ( Elliott et al . , 2016 ) . The   training and validation sets consisted of 29;000and   1;014instances , respectively . We reported the re-   sults on the Test2016 , Test2017 and MSCOCO test   sets ( Elliott et al . , 2017 ) . Note that MSCOCO is   more challenging for MMT models due to the out-   of - domain instances with ambiguous verbs . Fol-   lowing the setup in ( Wu et al . , 2021 ) , we learned   a joint BPE code for 10;000merging operations   for both the source and target languages , resulting   in vocabularies of 9;716and9;548entries for the   En - De and En - Fr tasks .   3.2 Experimental Setups   We followed the Wu et al . ( 2021 ) ‚Äôs work to con-   duct experiments with Transformer - Tiny conÔ¨Ågu-   ration , which is more suited for small datasets like   Multi30K. Note that smaller models even obtain   higher BLEU scores than pervious MMT models .   Similar observations have been discussed when   building context - aware machine translation models   ( Li et al . , 2020 ) . The model consists of 4encoder   and decoder layers . The hidden size is 128and the   Ô¨Ålter size of FFN is 256 . There are 4heads in the   multi - head self - attention mechanism . We set the   dropout as 0:3and the label smoothing as 0:1 .   Our implementation was based on Fairseq ( Ott   et al . , 2019 ) . For training , we used Adam Op-   timizer ( Kingma and Ba , 2015 ) with  = 0:9 ,   = 0:98and= 10 . We adopted the samelearning rate schedule as ( Vaswani et al . , 2017 ) ,   where the learning rate Ô¨Årst increased linearly for   warmup = 2000 steps from 1eto5e . After   the warmup , the learning rate decayed proportion-   ally to the inverse square root of the current step .   Each training batch contained 4;096tokens . We   also adopted the early - stop training strategy ( Zhang   et al . , 2020 ) to avoid the overÔ¨Åtting issue .   For evaluation , we averaged the last 10check-   points for more reliable results . The width of beam   size was set to 5 . The performance was measured   by BLEU and METEOR for all test sets . Also , we   used accuracy for evaluation on the probing tasks .   3.3 Results   Table 2 summarizes the results on standard MMT   data . Each model was evaluated on three test sets   on two language pairs . We see , Ô¨Årst of all , that   the improvements of previous methods ( Rows 2 - 4 )   over the tiny baseline are marginal in terms of both   BLEU and METEOR . This conÔ¨Årms the assump-   tion that the visual features are not fully used if   the text is complete ( Caglayan et al . , 2019 ) . When   switching the vision features from ResNet ( Row.5 )   to ViT ( Row.6 ) , there are no signiÔ¨Åcant BLEU   gains . Then , we test them on the proposed probing   tasks to examine the ‚Äú real ‚Äù contribution to MMT .   Color - based Probing Table 3 shows the accu-   racy on the color - based probing task . We see   that the accuracy improvement of the gated fusion   method is marginal by both restrict and relaxed cri-   teria . However , replacing ResNet with ViT yields   gains of over 8accuracy points across three test6330   sets on En - De task . Similar improvements are ob-   served on the En - Fr task . The Ô¨Ånding here indicates   that stronger vision features are helpful for repre-   senting the visual information . Moreover , selective   attention can make better use of the ViT features ,   achieving over 20accuracy gains on three test sets .   This veriÔ¨Åes the conjecture that the selective atten-   tion can further enhance the fused representation   for the ViT features .   Character - based Probing Table 4 shows simi-   lar results as in Table 3 . ViT with selective at-   tention performs the best on most scenarios , it is   only slightly inferior to Gated Fusion + ViT on the   MSCOCO dataset . While the gated fusion method   with ResNet feature behaves far from desirable .   It even underperforms the text - only Transformer ,   though the text - only Transformer is carefully reg-   ularized . A potential explanation is the character-   based probing task is more challenging than the   color - based probing task because it is more difÔ¨Å-   cult for the model to Ô¨Ånd the correct corresponding   region of the masked character word and provide   useful signals to the text encoder .   Noun - based Probing Figure 2 plots the results   of noun - based masking . It again veriÔ¨Åes the above   conjecture . The histograms in blue and red denotethe results on the En - De and En - Fr tasks , respec-   tively . The ViT features can signiÔ¨Åcantly outper-   form the ResNet features across all masking meth-   ods on the two language pairs . We also observe   that the gap between the ResNet and ViT features is   gradually enlarged as more nouns are masked . This   conÔ¨Årms the results in ( Dosovitskiy et al . , 2021 ) .   4 Analysis   4.1 How Vision Features Improve the MMT   We further explore the impact of model capacity .   Here , we report the results of ViT and Swin Trans-   former because they are strong models in recent   studies . Our conjecture here is that larger ViT / Swin   models can describe the image more accurately ,   which enables the text encoder to receive richer   complementary information . Figure 3 depicts the   BLEU scores in progressive noun masking scenar-   ios . Intuitively , larger ViT and Swin models pro-   vide more complementary knowledge to complete   the insufÔ¨Åcient text representations .   Nevertheless , a counterintuitive phenomenon is   the inferiority of Swin across all scenarios in the   same conÔ¨Åguration , though it outperforms ViT on   most computer vision benchmarks . We attribute   the reason to the short length of the patch sequence .   In patch , ViT has a length of 577 ( 576 sequence6331   segments and a special token CLS ) when the image   resolution and the patch size are 384384and   1616 . However , Swin has a Ô¨Åxed sequence length   ( 49 ) restricted by the shifted window operation .   This leads to more Ô¨Åne - grained local features for   ViT , which is beneÔ¨Åcial to the selective attention   mechanism for extracting more relevant pieces .   4.2 Impact of Learning Objectives   Then , we investigate the impact of the enhanced   vision features on MMT . Previous studies have al-   ready attempted to leverage object - detection fea-   tures ( Zhao et al . , 2020 ; Wang and Xiong , 2021)but the observation here is slightly different . Be-   yond the object - detection pretrained features , we   also take the image captioning task into account .   Rows 11 - 13 in Table 2 summarize the results   of the three enhanced vision features on the stan-   dard MMT data , and Figure 4 depicts the results   on insufÔ¨Åcient texts . Here we choose ViT - Tiny-   based models for comparison due to the similar   model capacity they own . We see that not only   the object - detection ( DETR and QueryInst ) , but   also the image captioning ( CATR ) pretrained fea-6332   tures obtain superior performance compared with   ViT - tiny ( Row 8) when the text is complete . It is   consistent with previous Ô¨Åndings ( Yin et al . , 2020 ;   Zhao et al . , 2020 ) . However , the advantages do   not persist when switching to limited text scenarios .   A possible explanation is that these methods are   sensitive to the quality of the extracted objects . We   leave this as future work .   4.3 Impact of Resolution and Patch Size   It is well - known that higher resolutions are ben-   eÔ¨Åcial to the accuracy improvement in computer   vision tasks ( Dosovitskiy et al . , 2021 ) . Despite   the success of the Transformer architecture , re-   cent studies show that the success of ViT mainly   comes from the successful use of the patch schema   ( Dosovitskiy et al . , 2021 ) . Here , we compare MMT   systems with different resolutions and patch sizes   based on ViT - Base . The results on three prob-   ing tasks ( see Table 5 ) again conÔ¨Årm the above   assumption that Ô¨Åne - grained vision features are   more suited for the selective attention . Also , the   attention map visualized in Figure 5 demonstrates   that high resolution with Ô¨Åne - grained patch schema   can attend to correct regions of the image for each   masked token . For example , both models pay the   right attention to the masked character and noun ,   but the model with low resolution fails to detect theright region of color . The Ô¨Ånding here may shed   light to other multimodal tasks , such as VQA .   4.4 Incongruent Decoding   Incongruent decoding is a widely used manner to   evaluate whether the visual modality contributes   to the text ( Caglayan et al . , 2019 , 2021 ) . Table 6   shows that incongruent decoding causes obvious   BLEU drops except for the ResNet feature . ViT   beats the ResNet with gated fusion . It yields higher   BLEU scores with congruent decoding and exhibits   a larger BLEU drop with incongruent decoding .   We also Ô¨Ånd that the ViT features learned from   scratch are also insensitive to the visual modality .   This is reasonable that the learned vision systems   are not sufÔ¨Åciently strong due to the data scarcity   of Multi30K. Thus the visual modality acts more   like noise signals . In addition , focusing on the   results of pretrained selective attention + ViT , the   gap between congruent and incongruent decoding   gradually becomes larger .   We also investigate whether the ensemble vision   features can help . Concretely , we choose ViT and   CATR to independently generate the fused repre-   sentations with the text feature , and then the ensem-   ble feature is obtained based on them . We see that   the ensemble vision feature performs the best on   the congruent decoding , and achieves the largest6333   BLEU gaps on four masking scenarios compared   with other systems . These results again indicate   that stronger visual contexts indeed help .   4.5 Case Study   Finally , we compare several real cases . We choose   gated fusion ( CNN ) ( Wu et al . , 2021 ) and selective   attention + ViT_Base ( ViT ) for comparison . The   qualitative examples in Table 7 demonstrate that   the visual modality is complementary rather than   redundant if the text is insufÔ¨Åcient . To Ô¨Ågure out   whether the German translation is right or not , we   provide the human - translation results . First , we   see the top half case of Table 7 , ViT can Ô¨Åll in   the masked entities and generate the correct trans-   lations even four entities were masked . Unfortu-   nately , CNN incorrectly judges the man as a woman .   Also , it can not distinguish the right color of shirt   due to the complex background . When given a   more complex image ( the bottom half case ) , it is   still a challenge for ViT to generate the right trans-   lation . The observation here inspires us to design   a more powerful fusion method . Also , the datascarcity problem is a root issue to prevent us from   further improving the cross - modal translation qual-   ity .   5 Related Work   Multimodal machine translation is a cross - domain   task in the Ô¨Åeld of machine translation . Early at-   tempts mainly focused on enhancing the MMT   model by better incorporation of the vision features   ( Calixto and Liu , 2017 ; Elliott and K√°d√°r , 2017 ;   Delbrouck and Dupont , 2017 ) . However , directly   encoding the whole image feature brings additional   noise to the text ( Yao and Wan , 2020 ; Liu et al . ,   2021a ) . To address the above issue , Yao and Wan   ( 2020 ) proposed a multimodal self - attention to con-   sider the relative difference of information between   two modalities . Similarly , Liu et al . ( 2021a ) used a   Gumbel Softmax to achieve the same goal .   Researchers also realize that the visual modality   may be redundant . Irrelevant images have little   impact on the translation quality , and no signiÔ¨Åcant   BLEU drop is observed even the image is absent   ( Elliott , 2018 ) . Encouraging results appeared in6334Caglayan et al . ( 2019 ) ‚Äôs work . They pointed out   that the visual modality is still useful when the lin-   guistic context is scarce , but is less sensitive when   exposed to complete sentences . More recently , Wu   et al . ( 2021 ) attributed the BLEU gain on MMT   tasks to the regularization training , and they again   emphasized the imperative of constructing proper   insufÔ¨Åcient textual input . It is worthy to note that   the proposed probing task is an improved version   based upon previous work ( Caglayan et al . , 2019 ;   Wu et al . , 2021 ) . We also opensource the prepro-   cessed data and the corresponding scripts for the   subsequent researchers to experiment on .   Another line of research is to explore large - scale   cross - modal pretraining models . In this way , the   MMT task is regarded as a downstream task . For   example , CLIP ( Radford et al . , 2021 ) is a general   cross - modal pretraining model , which learns to   perform a wide variety of tasks via natural lan-   guage prompting . Caglayan et al . ( 2021 ) presented   a MMT - speciÔ¨Åc pretraining model which combines   the translation language modeling with masked re-   gion classiÔ¨Åcation objectives . In this work , we   make a systematic study on whether stronger vision   features are helpful . We also extend the research   to enhanced features , such as object - detection and   image captioning , which are complementary to pre-   vious work .   6 Conclusions   In this work , we show that stronger vision features   ( e.g. ViT - like models ) strengthen MMT systems   on three proposed probing tasks . We present a   selective attention method for ViT - based models to   make better use of the patch - level representation .   The result here shows a promising line of research   on developing better vision models for multimodal   tasks . As far as we know , this is the Ô¨Årst attempt   to build MMT systems with Transformer only . In   future work , we are willing to investigate whether   it is possible to use a single set of parameters to   encode the vision and text modalities .   Acknowledgments   This work was supported in part by the National   Science Foundation of China ( Nos . 61732005   and 61876035 ) , the National Key R&D Project   of China ( No . 2019QY1801 ) , the China HTRD   Center Project ( No . 2020AAA0107904 ) and Yun-   nan Provincial Major Science and Technology Spe-   cial Plan Projects ( Nos . 201902D08001905 and202103AA080015 ) . The authors would like to   thank anonymous reviewers for their valuable com-   ments . And thank Yufan Jiang for his helpful ad-   vice to improve the paper .   References633563366337