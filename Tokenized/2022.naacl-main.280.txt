  Uri Berger , Gabriel Stanovsky , Omri Abend , and Lea FrermannSchool of Computer Science and Engineering , The Hebrew University of JerusalemSchool of Computing and Information Systems , University of Melbourne   { uri.berger2 , gabriel.stanovsky , omri.abend}@mail.huji.ac.il   lea.frermann@unimelb.edu.au   Abstract   Recent advances in self - supervised modeling   of text and images open new opportunities for   computational models of child language acqui-   sition , which is believed to rely heavily on   cross - modal signals . However , prior studies   have been limited by their reliance on vision   models trained on large image datasets anno-   tated with a pre - deﬁned set of depicted object   categories . This is ( a ) not faithful to the infor-   mation children receive and ( b ) prohibits the   evaluation of such models with respect to cat-   egory learning tasks , due to the pre - imposed   category structure . We address this gap , and   present a cognitively - inspired , multimodal ac-   quisition model , trained from image - caption   pairs on naturalistic data using cross - modal   self - supervision . We show that the model   learns word categories and object recognition   abilities , and presents trends reminiscent of   those reported in the developmental literature .   We make our code and trained models public   for future reference and use .   1 Introduction   To date , the mechanisms underlying the efﬁciency   with which infants learn to speak and understand   natural language remain an open research question .   Research suggests that children leverage contex-   tual , inter - personal and non - linguistic information .   Visual input is a case in point : when spoken to ,   infants visually perceive their environment , and   paired with the input speech , the visual environ-   ment could help bootstrap linguistic knowledge   ( Tomasello et al . , 1996 ) . Unlike social cues , visual   input has a natural physical representation , in the   form of pixel maps or videos .   Previous multimodal language acquisition stud-   ies either considered toy scenarios with small vo-   cabularies ( Roy and Pentland , 2002 ; Frank et al . ,   2007 ) , or used visual encoders that were pretrainedFigure 1 : Model overview . Given an image - caption   pair ( left ) , both the visual ( top ) and textual encoder   ( bottom ) generate a binary vector indicative of the clus-   ters associated with the current input . The text / visual   model predicts a probability vector over clusters per   word / image . Vectors are mapped to a binary space   using thresholding . The modality - speciﬁc vectors pro-   vide mutual supervision during training ( right ) .   on large labeled data bases such as ImageNet ( Deng   et al . , 2009 ) or Visual Genome ( Krishna et al . ,   2017 ) . This has two drawbacks : ﬁrst , systematic   access to labeled data is a cognitively implausible   assumption in a language acquisition setting ; sec-   ond , imposing a pre - deﬁned categorization system   precludes studying categories that emerge when   learning from unlabeled multimodal data . This   type of setting more closely resembles the data un-   derlying early language learning at a time when the   child has only acquired little conceptual informa-   tion . Although the subject of much psycholinguis-   tic work , the computational study of multimodal   word categories , formed without recourse to man-   ual supervision has been scarcely addressed in pre-   vious work .   We present a model that learns categories as clus-3819   ters of words from large - scale , naturalistic multi-   modal data without any pre - training . Given ( im-   age , caption)-pairs as input , the model is trained to   cluster words and images in a shared , latent space ,   where each cluster pertains to a semantic category .   In a self - supervised setup , a neural image classiﬁer   and a co - occurrence based word clustering module   provide mutual supervision through joint training   with an expectation - maximization ( EM ) style algo-   rithm . Figure 1 illustrates our model .   Our input representation is cognitively plausible   in that we use raw image data ( pixels ) , and train   our model on a comparatively small data set ( Ta-   ble 1 ) . However , we follow previous work ( Kádár   et al . , 2015 ; Nikolaus and Fourtassi , 2021 ) in us-   ing written and word - segmented text input . Young   infants do not have access to such structure , and   this assumption therefore deviates from cognitive   plausibility ( but opens avenues for future work ) .   We show that semantic and visual knowledge   emerges when training on the self - supervised cate-   gorization task . In a zero - shot setup , we evaluate   our model on ( 1 ) word concreteness prediction as a   proxy for noun identiﬁcation , one of the ﬁrst cues   for syntax in infant language acquisition ( Fisher   et al . , 1994 ) ; ( 2 ) visual classiﬁcation and object   segmentation . We also study the emerging latent   word clusters and show that words are clustered   syntagmatically ( De Saussure , 1916 ): words repre-   senting entities that are likely to occur together are   more likely clustered together ( e.g. , dog - bark ) , than   words that share taxonomic categories ( e.g. , dog-   cat ) . This concurs with ﬁndings that young children   acquire syntagmatic categories more readily than   taxonomic categories ( Sloutsky et al . , 2017).2 Background and Related Work   We brieﬂy review previous studies of unimodal   learning without pretraining ( for both text and vi-   sion ) and multimodal learning ( studied mainly in   acquisition implausible settings ) to highlight the   gap that this work addresses .   Unimodal Learning . Self - supervised learning   without pre - training has been extensively studied ,   but predominantly in a unimodal scenario or un-   der cognitively implausible assumptions . For the   text modality , large language models have been de-   veloped in recent years ( e.g. , BERT ; Devlin et al . ,   2019 ) , trained on large unlabeled text corpora ( Ta-   ble 1 ) . A more cognitively motivated model is   BabyBERTa ( Huebner et al . , 2021 ) , a smaller ver-   sion of RoBERTa ( Liu et al . , 2019 ) ( also ) trained   on transcribed child directed speech . In the visual   domain , self - supervision is typically implemented   as contrastive learning , training the model to align   corrupted images with their original counterparts   ( Chen et al . , 2020a ) , with subsequent ﬁne - tuning .   Multimodal Language Learning . Early language   acquisition studies ( Roy and Pentland , 2002 ) con-   sidered toy scenarios with small vocabularies and   used heuristics for image processing . Silberer and   Lapata ( 2014 ) model multi - modal human catego-   rization using human - annotated feature vectors as   input for a multimodal self - supervised autoencoder ,   while we learn the features from raw images .   Unlike our work , recent work on cross - modal   language learning ( Kádár et al . , 2015 ; Chrupała   et al . , 2017 ; Ororbia et al . , 2019 ; Nikolaus and Four-   tassi , 2021 ) typically use Convolutional Neural Net-   works , pre - trained on large labeled data bases like   ImageNet ( Deng et al . , 2009 ) , or alternatively ( e.g. ,   Lu et al . , 2019 ; Chen et al . , 2020b ) use object detec-   tors pre - trained on Visual Genome ( Krishna et al . ,   2017 ) as the visual model .   Few studies assume no prior knowledge of the   grounded modality . Most related to our study is   CLIP ( Radford et al . , 2021 ) , a pre - trained off - the-   shelf model trained to project matching images and   captions to similar vectors . CLIP assumes a multi-   modal joint space which is continuous , unlike our   binary space . Liu et al . ( 2021 ) use CLIP pretrained   encoders to learn cross - modal representations with   a similar training objective as ours . They discretize   the output of the encoders by mapping it to the clos-   est vector from a ﬁnite set of learned vectors , which   can be viewed as a form of categorization . CLIP-3820based works are trained to match entire sentences   to images and have no explicit representation of   words and phrases . We therefore view it as a cogni-   tively less plausible setting than is presented in the   current study . Nevertheless , we include CLIP as a   point of comparison when applicable .   3 Model   Our goal is to learn the meaning of words and   raw images through mutual supervision by map-   ping both modalities to a joint representation . In-   tuitively , given a visual input paired with relevant   text ( approximating a typical learning scenario ) ,   the output for each of the modalities is a binary   vector in{0,1 } , with non - zero dimensions indi-   cating the clustersto which the input is assigned   andNis the total number of clusters ( a predeﬁned   hyper - parameter ) . The clusters are unknown a pri-   ori and are formed during training . The goal is to   assign matching text and image to the same clus-   ters . In order to minimize assumptions on innate   knowledge or pre - imposed categories available to   the language learner , and enable the study of emerg-   ing categories from large - scale multi - modal input   data , we deliberately avoid any pre - training of our   models .   3.1 Visual Encoder   The visual encoder ( Figure 1 , top ) is a randomly   initialized ResNet50 ( He et al . , 2016 ) , without pre-   training . We set the output size of the network to N   ( the number of clusters ) and add an element - wise   sigmoid layer . To produce the binary output and   predict the clusters given an input image , we apply   a hyper - parameter threshold θto the output of the   sigmoid layer .   3.2 Text Encoder   The text encoder ( Figure 1 , bottom ) is a sim-   ple probabilistic model based on word - cluster co-   occurrence , which is intuitively interpretable and   makes minimal structural assumptions . Given a   sentence , the model assigns each word to at most   one cluster . The sentence is assigned to the union   of the clusters to which the words in it are assigned . Formally , given a sentence s=(w , w, ... ,w )   of wordsw , and an assignment of the words   to clustersf:{w, ... ,w}→ { 1, ... ,N}∪{∅ } , the   clusters to which the sentence is assigned are:/braceleftbig   c|if∃ws.t.f(w)=c / bracerightbig . When assigning   words to clusters , we make two simplifying as-   sumptions : ( 1 ) the probability that a word is as-   signed to a speciﬁc cluster is independent of the   linguistic context , meaning that we assign to clus-   ters on the type- rather than the token level ( a rea-   sonable assumption given that children learn single   words ﬁrst ) ; ( 2 ) a single word can not be assigned   to more than one cluster , but it might be assigned   to no cluster at all if it does not have a visual corre-   spondent in the image ( e.g. , function words ) . Under   these assumptions , the encoder estimates P(c|w )   for eachc∈ { 1, ... ,N}and for each w∈V ,   whereVis the vocabulary . If the probability of   assigning a given word in a sentence to any of the   clusters exceeds a hyper - parameter threshold θ , it   is assigned to the cluster with the highest proba-   bility , otherwise it is not assigned to any cluster .   Formally :   f(w ) =       argmaxP(c|w)if maxP(c|w)≥θ   ∅ else   In the next step , we deﬁne the word - cluster associa-   tionsP(c|w ) . We estimate these using Bayes Rule ,   P(c|w ) = P(w|c)P(c )   P(w)(1 )   P(w|c)is deﬁned as the fraction of all predictions   of clustercfrom the visual encoder , in which woc-   curred in the corresponding caption . We instantiate   the prior cluster probability P(c)as uniform over   all clusters .   Finally , for a given word w , we estimate   P(w)=/summationtextP(c)P(w|c ) . Intuitively , we ex-   pect that a concrete word would repeatedly occur   with similar visual features ( of the object described   by that word ) , therefore repeatedly co - occurring   with the same cluster and receiving a high assign-   ment probability with that cluster , whereas abstract   words would co - occur with multiple clusters , there-   fore not being assigned to any cluster.38214 Training   At each training step , the model observes a batch of   ( image , caption)-pairs . We ﬁrst perform inference   with both encoders , and then use the results of each   encoder ’s inference to supervise the other encoder .   Text Encoder . Given the list of clusters predicted   by the visual encoder and a tokenized caption   s={w, ... ,w } , for eachw∈sand for each clus-   tercpredicted by the visual encoder , we increment   count ( c)andcount ( w , c ) . These are needed to   compute the probabilities in equation ( 1 ) .   Visual Encoder . For each input image and corre-   sponding cluster vector predicted by the text en-   coder , we use binary cross entropy loss comparing   the output of the sigmoid layer with the predicted   cluster vector and use Backpropagation to update   the parameters of the ResNet model .   5 Experiments   We trained our model on the 2014 split of   MSCOCO ( Lin et al . , 2014 ) , a dataset of natu-   ralistic images with one or more corresponding   captions , where each image is labeled with a list   of object classes it depicts . MSCOCO has 80 ob-   ject classes , 123 K images and 616 K captions ( split   into 67 % train , 33 % test ) . We ﬁltered out images   that did not contain any labeled objects , and images   that contained objects with a multi - token label ( e.g. ,   “ ﬁre hydrant”).After ﬁltering , we are left with 65   ground - truth classes . The ﬁltered training ( test )   set contains 56 K ( 27 K ) images and 279 K ( 137 K )   captions . We set apart 20 % of the training set for   hyper - parameter tuning .   We trained our model with a batch size of 50   until we observed no improvement in the F - score   measure from Section 5.1 ( 40 epochs ) . Training   took 4 days on a single GM204GL GPU . We used   N=150 clusters , θ=0.08 , andθ=0.5 . The visual   thresholdθwas ﬁrst set heuristically to 0.5to   avoid degenerate solutions ( images being assigned   to all or no clusters initially ) . Then , Nandθ   were determined in a grid search , optimizing the   F - score measure from Section 5.1 . We used spaCy   ( Honnibal and Montani , 2017 ) for tokenization.5.1 Semantic Word Categorization   Background . Semantic word categorization is the   clustering of words based on their semantic fea-   tures . Psycholinguistic studies have shown that   children use semantic word categories to solve lin-   guistic tasks by the end of the second year of life   ( e.g. , Styles and Plunkett , 2009 ) . There is a long   established fundamental distinction between syn-   tagmatic ( words that are likely to co - occur in the   same context ) and paradigmatic relations ( words   that can substitute one another in a context with-   out affecting its grammaticality or acceptability )   ( De Saussure , 1916 ) . Each relation type invokes a   different type of word categories ( syntagmatic rela-   tions invoke syntagmatic categories , or associative   categories ; paradigmatic relation invoke taxonomic   categories ) . Despite an acknowledgement that in-   fants , unlike adults , categorize based on syntag-   matic criteria more readily than on paradigmatic cri-   teria ( “ The Syntagmatic - Paradigmatic shift ” ; Ervin ,   1961 ) , and empirical evidence that syntagmatic cat-   egories might be more important for word learning   than taxonomic categories ( Sloutsky et al . , 2017 ) ,   computational categorization studies and datasets   predominantly focused only on taxonomic hierar-   chies ( Silberer and Lapata , 2014 ; Frermann and   Lapata , 2016 ) .   Setting . Our model ’s induced clusters are created   by using the text encoder to predict , for each word ,   the most likely cluster .   We evaluated induced clusters against a taxo-   nomic and a syntagmatic reference data set . First ,   we followed Silberer and Lapata ( 2014 ) , used the   categorization dataset from Fountain and Lapata   ( 2010 ) , and transformed the dataset into hard cat-   egories by assigning each noun to its most typical   category as extrapolated from human typicality rat-   ings . The resulting dataset contains 516 words   grouped into 41 taxononmic categories . We ﬁl-   tered the dataset to contain only words that occur   in the MSCOCO training set and in the word2vec   ( Mikolov et al . , 2013 ) dictionary , obtaining the ﬁnal   dataset with 444 words grouped into 41 categories .   In order to quantify the syntagmatic nature of the   induced clusters , we used a large dataset of human   word associations , the " Small World of Words "   ( SWOW , De Deyne et al . , 2019 ) . SWOW was com-   piled by presenting a cue word to human partici-   pants and requesting them to respond with the ﬁrst   three words that came to mind . The association3822strength of a pair of words ( w , w)is determined   by the number of participants who responded with   wto cue word w. Prior work has shown that   word associations are to a large extent driven by   syntagmatic relations ( Santos et al . , 2011 ) .   Comparison with other models . We compare   against several word embedding models , where   for each model we ﬁrst induce embeddings , which   we then cluster into K=41 clusters ( the number of   taxonomic gold classes ) using K - Means . We com-   pare against a text - only variant of our modelby   creating a co - occurrence matrix CwhereCis the   number of captions in which tokens i , jin the vo-   cabulary co - occur . The normalized rows of Care   the vector embeddings of words in the vocabulary .   We compare against off - the - shelf word2vec and   BERT embeddings . For BERT , given a word   w , we feed an artiﬁcial context ( “ this is a w ” ) and   take the embedding of the ﬁrst subword of w. We   also include the multi - modal CLIP , using prompts   as suggested in the original paper ( “ a photo of a   w”).Finally , we include a randomized baseline ,   which assigns each word at random to one of 41   clusters . Implementation details can be found in   Appendix A.1 .   Taxonomic categorization . We use the F - score   metric following Silberer and Lapata ( 2014 ) . The F-   value of a ( gold class , cluster)-pair is the harmonic   mean of precision and recall deﬁned as the size   of intersection divided by the number of items in   the cluster and the number of items in the class ,   respectively . The F - score of a class is the maximum   F - value attained at any cluster , and the F - score of   the entire clustering is the size - weighted sum of   F - scores of all classes . We report performance over   ﬁve random restarts for all models .   Results are presented in Table 2 . The text - only   baseline improves results over a random categoriza-   tion algorithm . Our multi - modal model grounded   in visual input improves over its unimodal vari-   ant . Our model is competitive with BERT and is   surpassed by word2vec and CLIP . However , con-   sidering the small model and training data we used   ( see Table 1 ) , our results are competitive . Model F - Score   Random 0.15±0.0032   Text - only 0.26±0.0098   Word2vec 0.40±0.0172   BERT 0.33±0.011   CLIP 0.38±0.0142   Ours 0.33±0.0109   Model MAS   Taxonomic 5.72   Random 4.23 ±1.88   Text - only 5.47 ±0.25   Word2Vec 6.65 ±0.16   BERT 5.75±0.23   CLIP 7.08 ±0.41   Ours 7.45 ±0.33   Syntagmatic categorization . We quantify the syn-   tagmatic nature of a clustering by the mean asso-   ciation strength ( MAS ) of pairs of words in the   SWOW dataset , where association strength of a   pair of words ( w , w)is again number of partic-   ipants who responded with w2 to cue word w1 .   MAS is computed across all word pairs from the   taxonomic dataset in which both words were as-   signed the same cluster by this clustering solution .   Results are presented in Table 3 . The multimodal   models ( ours and CLIP ) outperform all unimodal   models , an indication of the impact of multimodal-   ity on category learning : multimodal word learn-   ing shifts the learner towards syntagmatic relations   more signiﬁcantly than unimodal word learning .   To our knowledge , this is the ﬁrst computational   result to support this hypothesis , shown empiri-   cally in human studies with infants ( Elbers and van   Loon - Vervoorn , 1999 ; Mikolajczak - Matyja , 2015 ) .   Qualitative analysis . Table 4 shows four of the3823Ours 1 skis ; axe ; sled ; parka ; sleigh ; pants ;   gloves   Ours 2 sailboat ; canoe ; swan ; raft ; boat ; yacht ;   duck ; willow ; ship ; drum   Ours 3 train ; bullet ; subway ; tack ; bridge ; trol-   ley   Ours 4 bedroom ; rocker ; drapes ; bed ; dresser ;   sofa ; couch ; piano ; curtains ; cushion ; lamp ;   chair ; fan ; bureau ; stool ; cabin ; book   W2V cluster avocado , walnut , pineap ple ,   grape fruit , coconut , olive , lime , lemon   clusters created by our model and one cluster cre-   ated by word2vec for the taxonomic categorization   dataset . The clusters formed by our algorithm   are syntagmatic , associating words frequently ob-   served together ( e.g. , tokens in cluster 1 are related   tosnow activity , while cluster 2 broadly relates to   water ) . The cluster formed by word2vec embed-   dings is taxonomic ( all tokens are food products ) .   Our results provide initial evidence that syntag-   matic clusters emerge from an unsupervised train-   ing algorithm drawing on simple joint clustering of   words and images .   5.2 Concreteness Estimation   Background . Fisher et al . ( 1994 ) suggest that the   number of nouns in a sentence is among the ear-   liest syntactic cues that children pick up . Conse-   quently , noun identiﬁcation is assumed to be one   of the ﬁrst syntactic tasks learned by infants . We   approximate noun identiﬁcation as concreteness   estimation , since words representing concrete enti-   ties are mostly nouns . Chang and Bergen ( 2021 )   show that while children acquire concrete words   ﬁrst , neural text - based models show no such effect ,   suggesting that multimodality impacts the learning   process . Setting . We evaluate concreteness estimation us-   ing the dataset by Brysbaert et al . ( 2013 ) , which   contains concreteness ratings for 40 K English   words averaged over multiple human annotated   ratings on a scale of 1 to 5 . We estimate the con-   creteness of a word as the maximum probability   with which it was assigned to any cluster . For eval-   uation , we follow Charbonnier and Wartena ( 2019 )   and compute the Pearson correlation coefﬁcient of   our predictions with the ground - truth values . In   addition , we investigate the impact of word fre-   quency on our model ’s predictions by evaluating   the model on subsets of words in the Brysbaert data   of increasing minimum frequency in MSCOCO .   Comparison with other models . First , we com-   pare against supervised SVM regression mod-   els , which have shown strong performance on the   Brysbaert data in prior work ( Charbonnier and   Wartena , 2019 ) . Following their work , we use two   feature conﬁgurations : ( 1 ) POS tags + sufﬁxes ,   ( 2 ) POS tags + sufﬁxes + pre - trained FastText em-   beddings ( Joulin et al . , 2017 ) . We train the SVMs   on the full Brysbaert data .   Second , we compare with a minimally super-   vised text - only model . As in Sec 5.1 , we create   word vector representations from co - occurrence   counts . Next , following prior work ( Turney et al . ,   2011 ) , we select concrete ( abstract ) representative   words by taking the 20 words with the highest ( low-   est ) concreteness value in the Brysbaert data that   occur more than 10 times in the MSCOCO training   set . We predict a word ’s concreteness by comput-   ing its average cosine similarity to the concrete   representative words minus the average of its co-   sine similarity to the abstract representative words .   Results . Figure 2 presents the results in terms of   Pearson correlation when evaluated on words of   varying minimum frequency in MSCOCO . When   considering frequent tokens only , our model pre-   dicts word concreteness with an accuracy higher   than the SVM with POS and sufﬁx features , al-   though additional embedding features improve   SVM performance further . Note that the super-   vised baseline was trained on the full data set , and   hence evaluated on a subset of its training set . Our   multimodal model performs better than its text-   only variant for tokens that occur at least 100 times ,   even though the text - only model has received some   supervision ( by selecting the representative words).3824   5.3 Visual Multi - Label Classiﬁcation   In addition to linguistic knowledge , infants acquire   visual semantic knowledge with little explicit super-   vision , i.e. , they learn to segment and classify ob-   jects . To test whether our model also acquires such   knowledge we evaluated it on the multi - label clas-   siﬁcation task : For each image in the MSCOCO   test set , predict the classes of objects in the image .   In a zero - shot setting , we mapped the induced   clusters to predicted lists of MSCOCO classes as   follows . We ﬁrst provided the name of each class to   our model as text input and retrieved the assigned   cluster , thus obtaining a ( one - to - many ) cluster - to-   classes mapping . Now , for each test image , we used   the visual encoder to predict the assigned cluster(s ) .   The predicted set of MSCOCO classes is the union   of the lists of classes to which the predicted clusters   are mapped .   Comparison with CLIP . We compare our results   against CLIP . To ensure comparability with our   model we use CLIP with ResNet50 . We use CLIP   as a point of comparison to provide perspective on   the capabilities of our model despite differences in   modeling and assumptions . However , we note two   caveats regarding this comparison . First , CLIP was   trained on a much larger training set and has more   parameters than our model ( see Table 1 ) . Second ,   CLIP has only been used for single- ( not multi- )   label classiﬁcation , by inferring encodings of both   input images and prompts representing the ground-   truth classes ( e.g. , “ a photo of a bus ” for the ground   truth class bus ) and assigning the image to the classModel Precision Recall F - Score   Ours   CLIP   with highest cosine similarity to its encoding . We   adapt CLIP to a multi - label setting as follows : In-   stead of assigning the image to the class with the   highest cosine similarity , we take into account the   cosine similarity with all classes for each image .   We consider a class as predicted if its cosine simi-   larity exceeds a threshold , tuned on the MSCOCO   training split .   Results . Table 5 presents the results . As expected ,   CLIP outperforms our model . However , our model   achieves impressive results considering its simplic-   ity , its size , and that CLIP is the current state - of   the - art in self - supervised vision and language learn-   ing . Training a CLIP model of comparable size and   exposed to similar training data as our model is be-   yond the scope of this paper , but an interesting   direction for future work .   5.4 Object Localization   Another important task performed by infants is vi-   sual object localization . To test our model ’s ability   to reliably localize objects in images we use Class   Activation Maps ( CAM ) described by Zhou et al .   ( 2016 ) . Each CAM indicates how important each   pixel was during classiﬁcation for a speciﬁc cluster .   Quantitative analysis . Most previous studies   of zero - shot segmentation ( Bucher et al . , 2019 )   trained on a subset of “ seen ” classes , and evalu-   ated on both seen and unseen classes . We use a   more challenging setup previously referred to as   annotation - free segmentation ( Zhou et al . , 2021 ) ,   where we evaluate our model without any train-   ing for the segmentation task . We use MSCOCO ’s   ground - truth bounding boxes , which are human   annotated and mark objects in the image , for evalu-   ation . Following the original CAM paper , we use a   heuristic method to predict bounding boxes : Given   a CAM , we segment the pixels of which the value is3825   above 50 % of the max value of the CAM and take   the bounding box that covers the largest connected   component in the segmentation map .   We use precision and recall for evaluation . A   pair of bounding boxes is considered a match if   the intersection over union ( IoU ) of the pair ex-   ceeds 0.5 . Given lists of predicted and ground - truth   bounding boxes , we consider each matched pair as   a true positive and a prediction ( ground - truth ) for   which no matching ground - truth ( prediction ) was   found as a false positive ( negative ) . We compare   our model to a random baseline : Sample krandom   bounding boxes ( where kis the number of ground-   truth bounding boxes in the current image ) . This   baseline uses the number of ground - truth bounding   boxes in each image ( our model is not exposed to   this information ) .   The results are presented in Table 6 . Our   model is signiﬁcantly more precise than the ran-   dom baseline , but achieves similar recall : the entire   MSCOCO test split contains a total of 164,750   bounding boxes , while our model predicted 38,237   bounding boxes . This problem could be addressed   by lowering the visual threshold . We leave this   direction for future research .   Qualitative analysis . Fig . 3 shows a selection of   CAMs , plotted as heatmaps and associated with   class predictions ( see Sec . 5.3 ) . The heatmaps ex-   tracted by the model were better when the model   predicted a correct class in the visual classiﬁcation   task ( top six images and bottom left image in Fig   3 ) . In the bottom two images two clusters were pre-   dicted for the same original image , one correct and   one incorrect ( with an , unsurprisigly , meaningless   heatmap ) .   6 Discussion and Conclusion   We proposed a model for unsupervised multimodal   lagnguage acquisition , trained to jointly cluster text   and images . Many of our design choices were   guided by ﬁndings from cognitive studies of in-   fant language acquisition : The joint learning of   multiple modalities ; learning word - level seman-   tics ( e.g. , Fisher et al . , 1994 , suggest that children   ﬁrst learn to identify nouns and use this informa-   tion to learn sentence - level semantics ) ; and cross-   situational learning ( counting how many times each   word co - occurred with each cluster , see Gleitman ,   1990 ) . After training , our model demonstrates capa-   bilities typical of infant language acquisition : Word   concreteness prediction and identiﬁcation and seg-   mentation of objects in a visual scene .   However , we do not stipulate that infants begin   their acquisition of language by clustering words .   It would be interesting to design experiments to   test this hypothesis , e.g. , by connecting our work   with laboratory work on joint word and category   learning ( Borovsky and Elman , 2006 ) , or work on   the emergence of syntagmatic vs. taxonomic cate-   gories in young children ( Sloutsky et al . , 2017 ) .   While our model is cognitively more plausible   compared to previous studies , the gap from a realis-   tic setting of language acquisition is still large : ( 1 )   we assume the language input is segmented into   words ; ( 2 ) the input data , while naturalistic , is not   typical of infants at the stage of language acquisi-3826tion ; ( 3 ) the input only includes the visual and tex-   tual modality , but not , e.g. , pragmatic cues like ges-   ture ; and ( 4 ) the model learns in a non - interactive   setting , whereas physical and social interactions   are considered crucial for language learning , and   learns ( and is evaluated ) in a batch fashion while   human learning is typically incremental ( Frermann   and Lapata , 2016 ) .   In the semantic word categorization and con-   creteness prediction experiments , we compared our   multimodal model to unimodal text - only baselines ,   which we chose to be as similar as possible to our   model . The results suggest that multimodality im-   proves performance on both text tasks . However ,   it is unclear which speciﬁc information is encoded   in the visual modality that beneﬁts these text tasks .   We leave this question for future research .   Syntagmatic categories , although highly intu-   itive in the context of human memory , were not the   subject of many previous computational studies .   We propose to further investigate this type of cat-   egories and its use . One interesting direction is to   combine syntagmatic categories with interactivity :   Given a relevant signal from the environment the   model can cycle through concepts in the syntag-   matic category triggered by the signal , speeding up   the extraction of relevant concepts in real time . One   possible application of this direction is modelling   the construction of ad - hoc categories , described by   Barsalou ( 1983 ) .   While all experiments are conducted in English ,   our setting supports future work on other languages .   The small training set and the nature of the data   ( image - sentence pairs that might be , to some extent ,   collected from the Internet ) allow our model to   be extended to low - resource languages , while the   minimal structural assumptions of the text encoder   may imply some degree of language - agnosticity .   In future work , we plan to improve the cognitive   plausibility of our model by ( 1 ) incorporating typ-   ical input observed by children ( by using videos   taken in real scenes of child language acquisition ,   see Sullivan et al . , 2021 ) ; and ( 2 ) changing the   setting to an interactive one , where the model is   transformed into a goal - driven agent that uses in-   teractivity to learn and produce language .   Acknowledgements   We would like to thank the anonymous reviewers   for their helpful comments and feedback . This   work was supported in part by the Israel ScienceFoundation ( grant no . 2424/21 ) , by a research gift   from the Allen Institute for AI and by the HUJI-   UoM joint PhD program .   Ethical Considerations   We used publicly available resources to train our   model ( Lin et al . , 2014 ) . As with other statistical   methods for word representations , our approach   may capture social biases which manifest in its   training data ( e.g. , MSCOCO was shown to be bi-   ased with respect to gender , Zhao et al . , 2017 ) . Our   code includes a model card ( Mitchell et al . , 2019 )   which reports standard information regarding the   training used to produce our models and its word   clusters .   References38273828   A Appendix   A.1 Implementation details   A.1.1 Training   For the visual encoder , we used the ResNet50 im-   plementation from the torchvision package with   ADAM optimizer and a learning rate of 10.3829A.1.2 Semantic Word Categorization   For clustering of word embeddings , we use the K-   Means implementation in scikit - learn . We use the   word2vec google - news-300 model from the gensim   package , the BERT model from the transform-   ers library and CLIP ’s ofﬁcial implementation .   A.1.3 Concreteness Estimation   Supervised model . For the implementation of the   supervised model described by ( Charbonnier and   Wartena , 2019 ) we used 3 feature types . For POS   tag , we used the WordNet module from the NLTK   package to ﬁnd , for each word , the number of   synsets in each of the 4 possible POS tags ( NOUN ,   VERB , ADJ , ADV ) , and the induced feature vector   was the normalized count vector . In case no synsets   were found the induced feature vector was all zeros .   For sufﬁxes , we collected the 200 most frequent   sufﬁxes of length 1 to 4 characters in the training   set , and the induced feature vector was a 200 - d   binary vector indicating , for each sufﬁx , if it oc-   curs in the current word . For word embeddings , we   used the fastText wiki - news-300d-1 M model . The   ﬁnal feature vector is the concatenation of all the   selected feature vectors : POS+sufﬁx for the ﬁrst   model ( resulting a 204 - d feature vector for each   word ) , and POS+sufﬁx+embedding for the second   model ( resulting a 504 - d feature vector for each   word ) .   Text - only baseline . The 20 concrete representa-   tive words were sand , seagull , snake , snowsuit ,   spaghetti , stairs , strawberry , tiger , tomato , tooth-   brush , tractor , tree , turtle , umbrella , vase , water ,   comb , tire,ﬁretruck , tv .   The 20 abstract representative words were would ,   if , though , because , somewhat , enough , as , could ,   how , yet , normal , ago , so , very , the , really , then ,   abstract , a , an .   A.1.4 Object Localization   To extract Class Activation Mappings , we used the   CAM module from the torchcam package .   A.2 Cluster lists   Following is a list of clusters created by different   clustering algorithm , in a speciﬁc execution .   A.2.1 Our model   Words are sorted by P(c|w ) .   Cluster 1 : doorknob , canaryCluster 2 : trombone , leotards , trumpet , projector ,   cello , harmonica , guitar   Cluster 3 : train , bullet , subway , tack , bridge , trol-   ley   Cluster 4 : bus , ambulance , inn , taxi , level   Cluster 5 : elephant , bear , giraffe , paintbrush , rock ,   fence , chain   Cluster 6 : machete , porcupine , hornet , banana ,   gorilla , apple , turtle , turnip , peach , stick   Cluster 7 : veil , shawl   Cluster 8 : ashtray , mushroom , cheese , spinach ,   olive , tomato , shrimp , rice , pie , chicken , potato ,   broccoli , plate , pan , pepper , asparagus , skillet , peas ,   onions , tuna , salmon , cranberry , lettuce , beans ,   spatula , ladle , dish , crab , corn , cucumber , tray , wal-   nut , plum , box , lobster , cherry , table , shell   Cluster 9 : church , clock , skyscraper , chapel , build-   ing , brick , stone , ﬂea   Cluster 10 : airplane , helicopter , pier , gate   Cluster 11 : bedroom , rocker , drapes , bed , dresser ,   sofa , couch , piano , curtains , cushion , lamp , chair ,   fan , bureau , stool , cabin , book   Cluster 12 : skis , axe , sled , parka , sleigh , pants ,   gloves   Cluster 13 : dishwasher , kettle , toaster , freezer ,   stove , microscope , microwave , oven , fridge , cup-   board , mixer , blender , plug , mittens , grater , pot ,   apron , cabinet , tape , apartment   Cluster 14 : missile , jet , bomb , rocket , drill   Cluster 15 : bouquet , thimble , umbrella , accordion ,   cake , scissors , wrench , jar , pliers , candle , penguin ,   frog , doll , bottle , shield , pig , card   Cluster 16 : zucchini , beets , cabbage , celery ,   cauliﬂower , wheelbarrow , parsley , tongs , shelves   Cluster 17 : grapefruit , tangerine , colander , clamp ,   snail , cantaloupe , pineapple , grape , pear , lemon ,   eggplant , mandarin , garlic , nectarine , basket ,   corkscrew , pyramid , pumpkin , bin , sack , lime , cork ,   orange   Cluster 18 : octopus , kite , crocodile , squid , bal-   loon , butterﬂy , whale   Cluster 19 : surfboard , swimsuit , board , rope   Cluster 20 : hose , hut   Cluster 21 : skateboard , pipe , saxophone , helmet ,   escalator , barrel , broom   Cluster 22 : shotgun , seal , dolphin , car , hoe , ham-   ster , wheel , house   Cluster 23 : sailboat , canoe , swan , raft , boat , yacht ,   duck , willow , ship , drum   Cluster 24 : tortoise , dog , cat , tiger , cheetah   Cluster 25 : hyena3830Cluster 26 : buckle , mug , ruler , envelope , bag , belt ,   cup , camel , pencil , spider , cart , saucer , closet , tri-   pod , carpet   Cluster 27 : crowbar , bathtub , toilet , drain , sink ,   faucet , marble , mirror , basement , tank , bucket ,   door , razor , mat   Cluster 28 : toad , mouse , keyboard , key , desk , type-   writer , stereo , rat , bookcase , telephone , anchor , ra-   dio   Cluster 29 : buzzard , chickadee , ﬁnch , wood-   pecker , grasshopper , worm , sparrow , blackbird , vul-   ture , parakeet , bluejay , hawk , robin , dagger , perch ,   falcon , stork , peacock , pelican , owl , crow , pigeon ,   seagull , ﬂamingo , eagle , vine , birch , beaver , pheas-   ant , raven , goose , squirrel , seaweed , ant , emu , dove ,   cage , crown , shovel   Cluster 30 : horse , racquet , saddle , pony , buggy ,   bat , football , wagon , sword , donkey , ball , fox   Cluster 31 : beetle   Cluster 32 : zebra , ostrich , elk , deer , lion , pen , pin ,   riﬂe , bolts   Cluster 33 : bracelet , fawn , slippers , socks , shoes ,   tap , boots , strainer , jeans , ring   Cluster 34 : whistle , cathedral , wand , thermometer ,   peg , hook , goldﬁsh , lantern , wall , urn , caterpillar ,   chandelier , robe , leopard   Cluster 35 : motorcycle , bike , tractor , truck , trailer ,   tricycle , scooter , jeep , limousine , garage , van , tent ,   crane   Cluster 36 : baton , revolver , violin , tie , bow , cock-   roach , elevator , mink , necklace , blouse , trousers ,   vest , scarf , skirt , gun , gown , dress , shirt , sweater ,   bra , cap , jacket , coat , cape   Cluster 37 : bench , cannon   Cluster 38 : unicycle , groundhog   Cluster 39 : pistol , buffalo   Cluster 40 : clam , pickle , raisin , raspberry , napkin ,   submarine , fork , coconut , strawberry , bread , spoon ,   blueberry , radish , knife , biscuit , cloak , spear , whip ,   avocado , carrot , cottage , turkey , bowl   Cluster 41 : lamb , sheep , raccoon , cow , goat ,   rooster , calf , ox , hatchet , bull , moose , bison , barn ,   rabbit , shed , shack   Cluster 42 : screwdriver , pajamas , comb , hammer ,   brush , alligator   A.2.2 word2vec   Cluster 1 : leopard , hyena , crocodile , canary , lion   Cluster 2 : lobster , tuna , clam , octopus , whale ,   squid , shrimp , seaweed , salmon , crab , dolphin   Cluster 3 : mat , cageCluster 4 : lantern , chandelier , candle , tripod , pro-   jector , lamp   Cluster 5 : sailboat , submarine , raft , yacht , canoe ,   boat , pier , ship   Cluster 6 : avocado , walnut , pineapple , grapefruit ,   coconut , olive , lime , lemon   Cluster 7 : mittens , doll , slippers , pajamas , neck-   lace , socks   Cluster 8 : rock , cottage , tent , gate , house ,   brick , pyramid , rocker , door , bluejay , shed , bench ,   skyscraper , bolts , hut , mirror , key , building , bar-   rel , tape , inn , apartment , cabinet , book , marble ,   drum , shack , umbrella , crane , bureau , garage , shell ,   basement , fan , cathedral , fence , chapel , stone , drill ,   telephone , comb , radio , shield , church , anchor , mi-   croscope , clock , level , board , football , chain , cabin ,   wall , barn , bridge   Cluster 9 : elk , bison , pheasant , beaver , deer ,   moose , goose   Cluster 10 : pig , cow , sheep , goat , ostrich , emu ,   calf , buffalo , bull   Cluster 11 : elevator , train , whistle , limousine , es-   calator , subway , bus , taxi , trolley   Cluster 12 : groundhog , parakeet , fawn , tortoise ,   goldﬁsh , porcupine , fox , cheetah , gorilla , ﬂea , rab-   bit , mink , peacock , rooster , mouse , duck , turtle ,   squirrel , dog , bear , alligator , rat , raccoon , cat ,   ﬂamingo , tiger , hamster , penguin   Cluster 13 : razor , pliers , scissors , crowbar , knife ,   screwdriver , machete   Cluster 14 : keyboard , violin , trumpet , piano , saxo-   phone , guitar , cello , accordion , trombone , harmon-   ica   Cluster 15 : rocket , helicopter , jet , bomb , missile ,   ambulance , airplane   Cluster 16 : jeans , leotards , boots , blouse , skirt ,   bracelet , shirt , swimsuit , shoes , trousers , dress ,   pants , sweater , bra   Cluster 17 : cauliﬂower , spinach , cabbage , broc-   coli , peas , garlic , radish , lettuce , eggplant , cucum-   ber , onions , zucchini , parsley , celery , beans , aspara-   gus , beets   Cluster 18 : sofa , drapes , typewriter , napkin , toi-   let , chair , bathtub , bedroom , bed , doorknob , stool ,   desk , carpet , table , dresser , couch , stereo , curtains   Cluster 19 : strainer , colander   Cluster 20 : kite , balloon , willow   Cluster 21 : corn , pickle , bread , turkey , biscuit ,   dish , cheese , cake , lamb , pepper , pie , rice , chicken   Cluster 22 : frog , spider , toad , ant , worm , cock-   roach , snail , butterﬂy , beetle , hornet , grasshopper,3831caterpillar   Cluster 23 : cannon , bullet , gun , pistol , riﬂe , re-   volver , shotgun   Cluster 24 : bag , kettle , mug , envelope , sack , urn ,   basket , cup , pot , box , card , plate , jar , bucket , bou-   quet , bin , ashtray , tray , bottle   Cluster 25 : bowl , spatula , spoon , blender , ladle ,   grater , tongs , pan , mixer , saucer   Cluster 26 : sleigh , trailer , buggy , wheelbarrow ,   van , wagon , tractor , truck , cart , jeep   Cluster 27 : plum , cherry , birch , cork   Cluster 28 : ﬁnch , falcon , pigeon , hawk , pelican ,   raven , seagull , stork , buzzard , vulture , chickadee ,   sparrow , robin , crow , owl , woodpecker , blackbird ,   eagle , swan   Cluster 29 : racquet , bike , skateboard , skis , unicy-   cle , sled , scooter , motorcycle , helmet , surfboard ,   wheel , saddle , tricycle , car   Cluster 30 : sword , ruler , dagger , spear , baton   Cluster 31 : bookcase , shelves , closet , fridge , cup-   board   Cluster 32 : thermometer , microwave , dishwasher ,   toaster , skillet , stove , oven , freezer   Cluster 33 : vest , coat , jacket , parka , gloves   Cluster 34 : plug , thimble , tap , seal , dove , sink ,   drain   Cluster 35 : shawl , scarf , cap , cloak , veil , gown ,   cape , robe , wand , apron   Cluster 36 : hammer , broom , shovel , pencil ,   hatchet , brush , paintbrush , hoe , wrench , bat , pen   Cluster 37 : clamp   Cluster 38 : pumpkin , vine , grape , raspberry , car-   rot , mandarin , strawberry , pear , banana , apple ,   turnip , nectarine , cantaloupe , orange , mushroom ,   peach , cranberry , tomato , tangerine , raisin , blue-   berry , potato   Cluster 39 : faucet , tank , pipe , hose   Cluster 40 : donkey , ox , pony , horse , camel , ele-   phant , zebra , giraffe   Cluster 41 : bow , belt , tie , stick , buckle , cushion ,   hook , peg , perch , ring , tack , pin , ball , corkscrew ,   fork , whip , rope , crown   A.2.3 BERT   Cluster 1 : crane , vulture , ﬁnch , pigeon , owl , spar-   row , snail , octopus , bat , lobster , crab , mushroom ,   shrimp , shell , squid , perch , hornet , spider , worm ,   butterﬂy , turtle , toad   Cluster 2 : anchor , tack , bow , raft , doll , tray , knife ,   jet , airplane , canoe , car , helicopter , boat , ship , nap-   kin , book , board , card , desk , chair , bed , sword ,   bomb , dagger , spear , rope , bag , pencilCluster 3 : pheasant , woodpecker , parakeet , ostrich ,   caterpillar   Cluster 4 : stork , fawn , hatchet , hyena , raccoon ,   grasshopper   Cluster 5 : pliers , toaster , mittens , strainer , blender ,   freezer , saucer   Cluster 6 : crow , eagle , raven , hawk , dove , pig ,   sheep , fox , dog , cat , peacock , camel , bear , ele-   phant , deer , buffalo , rabbit , dolphin , frog , cow , elk ,   lion , moose , donkey , beaver , squirrel , rat , mouse ,   salmon , goat , calf , whale , leopard , bison , horse ,   bull , crocodile   Cluster 7 : hook , tape , pipe , pyramid , mat , chain ,   drill , balloon , ball , kite , cap , ring , belt , umbrella ,   bin , bucket , barrel , basket , bench , gate , wheel , plug ,   key , stereo , mixer , baton , envelope   Cluster 8 : scooter , sleigh , shawl , sled   Cluster 9 : raisin , raspberry , beets   Cluster 10 : birch , grape , pear , plum , apple , cherry ,   orange , tomato , peach , lemon , peas , beans , pepper ,   carrot , lime , rice , potato , olive , garlic , corn , walnut ,   strawberry , cheese , coconut , mandarin , cabbage ,   banana , vine , willow , onions   Cluster 11 : pelican , chickadee , porcupine , cucum-   ber , cockroach , tortoise   Cluster 12 : trailer , hose , saddle , tractor , ambu-   lance , wagon , taxi , bus , submarine , subway , train ,   elevator , limousine , bike , trolley , motorcycle , jeep ,   truck , yacht , tank , sofa , rocket , missile , cart , helmet   Cluster 13 : skillet , ladle   Cluster 14 : level , building , bridge , pier , house ,   cabin , shield , lantern , marble , sink , apartment , hut ,   basement , wall , cottage , box , rock , door , table ,   cage , fence , brick , lamp , telephone , drain , shed ,   garage , stone , skyscraper , barn , church , cathedral ,   chapel   Cluster 15 : buggy   Cluster 16 : revolver , riﬂe , pistol , shotgun , cannon ,   gun , bullet   Cluster 17 : seagull , sailboat , seaweed   Cluster 18 : urn   Cluster 19 : wheelbarrow , doorknob   Cluster 20 : ruler , shovel , stove , keyboard , micro-   scope , colander , cupboard , bowl , dish , skis , tie ,   pie , bread , cake , toilet , stool , cushion , mirror , tap ,   cabinet , carpet , fork , comb , apron   Cluster 21 : skateboard , surfboard , swimsuit   Cluster 22 : bluejay , blackbird , nectarine , grape-   fruit , tangerine , eggplant , asparagus , cauliﬂower ,   pineapple , cranberry , blueberry , goldﬁsh , ground-   hog , mink , broccoli3832Cluster 23 : ﬂamingo , hoe , racquet , parka   Cluster 24 : violin , accordion , piano , cello , guitar ,   trombone , harmonica , trumpet , saxophone   Cluster 25 : chandelier   Cluster 26 : buzzard   Cluster 27 : hamster   Cluster 28 : gown , robe , bra , scarf , sweater , shirt ,   jacket , skirt , coat , dress , necklace , bouquet , blouse   Cluster 29 : parsley , biscuit , celery   Cluster 30 : goose , duck , falcon , rooster , canary ,   turkey , swan , gorilla , penguin , tiger , zebra , fan , pa-   jamas , pants , jeans , van , pumpkin , tuna , chicken ,   rocker , lamb , ox , pony , ant , ﬂea , beetle , cape , alli-   gator   Cluster 31 : thermometer   Cluster 32 : machete   Cluster 33 : cheetah , giraffe   Cluster 34 : wrench , corkscrew , screwdriver , esca-   lator , tongs   Cluster 35 : thimble , tricycle , unicycle , tripod   Cluster 36 : avocado , lettuce   Cluster 37 : robin , hammer , pin , pen , bolts , scis-   sors , brush , microwave , fridge , oven , drum , bed-   room , curtains , peg , football , wand , mug , pot ,   shoes , trousers , vest , cloak , socks , boots , tent , inn ,   gloves , razor , bracelet , crown , buckle , shack , bot-   tle , sack , plate , broom , candle , cork , dresser , couch ,   bureau , seal , whip , cup , clock , radio , closet , jar ,   shelves , kettle , pan , stick , spoon , veil , whistle   Cluster 38 : crowbar , emu , clam , drapes , zucchini ,   radish , turnip , clamp , projector , bookcase , spatula ,   grater , spinach , pickle   Cluster 39 : paintbrush , typewriter   Cluster 40 : cantaloupe , leotards   Cluster 41 : dishwasher , faucet , slippers , ashtray ,   bathtub   A.2.4 CLIP   Cluster 1 : mittens , doll , rabbit , mouse , squirrel ,   rat , cat , hamster   Cluster 2 : plug , lantern , kettle , mug , thimble , urn ,   cup , candle , pot , blender , jar , book , bucket , toaster ,   bin , bottle , cage , lamp   Cluster 3 : mirror , ashtray , table , tray , mat   Cluster 4 : chandelier , bracelet , basket , unicycle ,   bolts , cap , barrel , tape , drum , umbrella , shell , neck-   lace , stool , bouquet , ring , fan , tack , drill , tele-   phone , wheel , saddle , microscope , clock , whip ,   chain , rope , crown , hose   Cluster 5 : bow , broom , shovel , spatula , spoon ,   ladle , tongs , crowbar , spear , forkCluster 6 : keyboard , typewriter , raft , piano , esca-   lator , comb , sink , drain , accordion   Cluster 7 : pumpkin , bread , biscuit , worm , carrot ,   cheese , orange , tangerine   Cluster 8 : trailer , rocker , bike , gun , box , train ,   motorcycle , projector , van , tractor , radio , bus , truck ,   mixer , taxi , tank , car , ambulance , jeep   Cluster 9 : jeans , leotards , bag , blouse , skirt , sack ,   tie , shirt , swimsuit , trousers , pajamas , socks , dress ,   carpet , veil , gown , pants , sweater , curtains , apron   Cluster 10 : peacock , raven , robin , crow , blackbird   Cluster 11 : elk , fawn , bison , cheetah , leopard ,   deer , emu , hyena , moose , zebra , giraffe   Cluster 12 : donkey , ox , cow , pony , horse , camel ,   sheep , goat , lamb , calf , buffalo , bull   Cluster 13 : thermometer , bullet , stick , tripod , pen-   cil , ruler , peg , brush , paintbrush , hoe , pin , screw-   driver , baton , wand , pen   Cluster 14 : pepper , beans   Cluster 15 : racquet , skateboard , skis , scooter ,   doorknob , surfboard , guitar , board   Cluster 16 : plum , vine , grape , raspberry , cherry ,   olive , cranberry , raisin , blueberry   Cluster 17 : pig , groundhog , porcupine , fox , seal ,   gorilla , beaver , whale , dog , bear , elephant , raccoon ,   salmon , lion , tiger   Cluster 18 : violin , trumpet , saxophone , cello ,   trombone , harmonica   Cluster 19 : parakeet , bluejay , ﬁnch , mink , dove ,   perch , birch , canary , bat , chickadee , sparrow , wood-   pecker   Cluster 20 : sleigh , cannon , buggy , sled , canoe ,   wheelbarrow , limousine , wagon , tricycle , cart , trol-   ley   Cluster 21 : shed , elevator , garage , basement , barn   Cluster 22 : avocado , walnut , pineapple , grapefruit ,   coconut , marble , strawberry , pear , apple , lime , nec-   tarine , cantaloupe , peach , willow , tomato , lemon ,   potato   Cluster 23 : crane , ostrich , pelican , stork , ﬂamingo   Cluster 24 : rock , boots , brick , slippers , shoes , hel-   met , stone , ball , bomb , balloon , football , bra   Cluster 25 : ﬂea   Cluster 26 : radish , turnip , parsley , beets   Cluster 27 : bookcase , shelves , cabinet , bureau ,   closet , desk , dresser , fridge , freezer , stereo , cup-   board   Cluster 28 : cottage , tent , house , pyramid , door ,   skyscraper , card , hut , bedroom , building , inn , apart-   ment , shack , cathedral , chapel , church , level , cabin ,   wall3833Cluster 29 : lobster , spider , octopus , ant , squid ,   shrimp , cockroach , beetle , hornet , crab , grasshop-   per   Cluster 30 : tuna , tortoise , frog , goldﬁsh , toad ,   clam , turtle , mandarin , snail , butterﬂy , alligator ,   crocodile , mushroom , dolphin   Cluster 31 : sailboat , submarine , yacht , boat ,   rocket , helicopter , jet , missile , ship , airplane   Cluster 32 : envelope , bowl , napkin , toilet , bathtub ,   microwave , plate , dishwasher , dish , cake , skillet ,   stove , pan , shield , pie , oven , rice , saucer   Cluster 33 : shawl , vest , scarf , coat , drapes , cloak ,   jacket , parka , cape , robe   Cluster 34 : hammer , belt , razor , sword , pliers , tap ,   key , hatchet , buckle , hook , whistle , pistol , riﬂe , re-   volver , kite , faucet , clamp , scissors , wrench , gloves ,   dagger , knife , anchor , corkscrew , shotgun , machete ,   pipe , cork   Cluster 35 : cauliﬂower , cabbage , garlic , onions   Cluster 36 : turkey , pheasant , falcon , pigeon , hawk ,   rooster , duck , seagull , buzzard , vulture , chicken ,   owl , goose , eagle , penguin , swan   Cluster 37 : gate , pier , fence , subway , bridge   Cluster 38 : spinach , broccoli , lettuce , seaweed   Cluster 39 : sofa , bench , chair , bed , cushion , couch   Cluster 40 : corn , pickle , peas , eggplant , cucumber ,   banana , zucchini , celery , asparagus , caterpillar   Cluster 41 : strainer , grater , colander   A.2.5 Text - only   Cluster 1 : saxophone , buckle , broom , shotgun ,   hatchet   Cluster 2 : grape , pepper , potato , lettuce   Cluster 3 : pipe , racquet , skateboard , tricycle , skis ,   barrel , board , helmet   Cluster 4 : emu , mug , cup   Cluster 5 : eagle , ostrich , thermometer , owl , ele-   phant , octopus , accordion , apple , orange , jet , air-   plane , apartment , umbrella , ox , escalator   Cluster 6 : surfboard , pajamas , swimsuit   Cluster 7 : falcon , crow , pigeon , bluejay , raven ,   hawk , tack , snail , blackbird , mat , peg , tray , cloak ,   submarine , limousine , belt , radish , lobster , biscuit ,   coconut , turnip , napkin , stool , sofa , cushion , couch ,   bench , table , squirrel , perch , seal , vine , pan , saucer ,   envelope , carpet   Cluster 8 : cantaloupe , zucchini , parsley , eggplant ,   pineapple , beets , garlic , cranberry , mandarin , cel-   ery   Cluster 9 : hammer , shovel , crowbar , screwdriver ,   dolphin , drill , hose , bat , wand , riﬂe , sword , bomb ,   cockroach , clamp , gunCluster 10 : wrench , thimble , scissors , pliers , nec-   tarine , spear   Cluster 11 : robin , level , pier , trailer , shield , tractor ,   ambulance , taxi , bus , bike , trolley , car , motorcycle ,   jeep , truck , inn , beetle , garage   Cluster 12 : bolts   Cluster 13 : ﬂamingo , wheelbarrow , machete , por-   cupine , harmonica , hut , tuna , bin , goldﬁsh , hyena ,   ant , willow , bouquet , strainer , missile , tongs   Cluster 14 : birch , vulture , ﬁnch , pin , pen , brush ,   sheep , bear , deer , buffalo , zebra , elk , pyramid ,   cabin , basement , cage , lamb , calf , bison , giraffe ,   ﬂea , grasshopper , barn   Cluster 15 : pheasant , dove , ruler , paintbrush , fan ,   chandelier , piano , drum , cherry , drapes , bedroom ,   curtains , razor , peach , dresser , bureau , rocker , lamp ,   radio , telephone , closet , bookcase , shelves , grater ,   comb   Cluster 16 : frog , keyboard , desk , mouse , key   Cluster 17 : asparagus , cauliﬂower , lemon , peas ,   beans , rice , corn , chicken , shrimp , cabbage ,   salmon , broccoli   Cluster 18 : goose , duck , seagull , woodpecker ,   swan , anchor , fox , sparrow , moose , sailboat , boat ,   ship , yacht , urn , gate , rocket , cannon , cathedral   Cluster 19 : raccoon   Cluster 20 : crane , rooster , stork , parakeet , peli-   can , hook , hoe , pig , cheetah , gorilla , dog , peacock ,   camel , rabbit , penguin , cow , lion , donkey , fridge ,   toaster , guitar , trombone , building , bridge , house ,   chain , lantern , football , raft , scooter , balloon , ball ,   kite , doll , robe , bra , scarf , socks , van , canoe , he-   licopter , tent , necklace , bracelet , ring , crown , car-   rot , banana , shack , wall , bottle , bucket , sack , tank ,   box , basket , rock , door , book , card , candle , cork ,   chair , bed , fence , brick , rat , goat , pony , whale , leop-   ard , bull , mink , spider , worm , tap , rope , wheel ,   clock , projector , blender , tripod , drain , typewriter ,   mixer , cart , jar , shed , bag , freezer , sled , stone , stick ,   spatula , ladle , butterﬂy , alligator , turtle , skyscraper ,   church   Cluster 21 : ashtray   Cluster 22 : revolver   Cluster 23 : mittens , tomato , olive , mushroom ,   cheese , crocodile , spinach , onions   Cluster 24 : chapel   Cluster 25 : clam   Cluster 26 : baton   Cluster 27 : canary , cat , fawn , tiger , shoes , slippers ,   hamster , beaver , groundhog   Cluster 28 : unicycle3834Cluster 29 : subway , train , bullet   Cluster 30 : turkey , violin , cello , bow , cap , gown ,   trousers , parka , sweater , shirt , jacket , skirt , pants ,   shawl , coat , vest , jeans , dress , boots , elevator ,   gloves , tie , pistol , blouse , apron , veil , cape   Cluster 31 : pear , raisin , cucumber , avocado , hor-   net , plug   Cluster 32 : tape , grapefruit , tangerine , plum , rasp-   berry , microscope , colander , bowl , knife , dish ,   pumpkin , crab , lime , pie , walnut , strawberry , bread ,   cake , blueberry , cottage , plate , shell , squid , cater-   pillar , seaweed , whip , pencil , fork , spoon , pickle   Cluster 33 : toad   Cluster 34 : saddle , wagon , buggy , sleigh , horse   Cluster 35 : corkscrew , microwave , stove , oven ,   pot , stereo , skillet , kettle   Cluster 36 : doorknob , marble , dishwasher , faucet ,   cupboard , sink , toilet , mirror , bathtub , cabinet   Cluster 37 : leotards   Cluster 38 : buzzard , chickadee   Cluster 39 : trumpet   Cluster 40 : whistle   Cluster 41 : dagger , tortoise3835