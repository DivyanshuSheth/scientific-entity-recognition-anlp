  Weixi Feng Tsu - Jui Fu Yujie Lu William Yang Wang   UC Santa Barbara   { weixifeng , tsu - juifu , yujielu , william}@cs.ucsb.edu   Abstract   Vision - and - Language Navigation ( VLN ) is a   task to guide an embodied agent moving to   a target position using language instructions .   Despite the significant performance improve-   ment , the wide use of fine - grained instructions   fails to characterize more practical linguistic   variations in reality . To fill in this gap , we in-   troduce a new setting , namely Underspecified   vision - and- Language Navigation ( ULN ) , and   associated evaluation datasets . ULN evaluates   agents using multi - level underspecified instruc-   tions instead of purely fine - grained or coarse-   grained , which is a more realistic and general   setting . As a primary step toward ULN , we   propose a VLN framework that consists of a   classification module , a navigation agent , and   an Exploitation - to - Exploration ( E2E ) module .   Specifically , we propose to learn Granularity   Specific Sub - networks ( GSS ) for the agent to   ground multi - level instructions with minimal   additional parameters . Then , our E2E module   estimates grounding uncertainty and conducts   multi - step lookahead exploration to improve   the success rate further . Experimental results   show that existing VLN models are still brittle   to multi - level language underspecification . Our   framework is more robust and outperforms the   baselines on ULN by „ 10 % relative success   rate across all levels .   1 Introduction   Vision - and - Language Navigation ( VLN ) allows a   human user to command or instruct an embodied   agent to reach target locations using verbal instruc-   tions . For this application to step out of curated   datasets in real - world settings , the agents must gen-   eralize to many linguistic variations of human in-   structions . Despite significant progress in VLN   datasets ( Anderson et al . , 2018b ; Chen et al . , 2019 ;   Ku et al . , 2020 ; Shridhar et al . , 2020 ) and agent   design ( Fried et al . , 2018 ; Li et al . , 2021 ; Min et al . ,Figure 1 : Navigation results of a baseline ( left ) and our   VLN framework ( right ) with multi - level underspecified   instructions ( L - L ) . Trajectories are curved for demon-   stration . Note that the baseline stops early and fails to   reach the target position with L - L. Our agent man-   ages to reach the goal across all levels .   2021 ) , it remains a question whether existing mod-   els are generalized and robust enough to deal with   all kinds of language variations .   For the language input in an indoor environment ,   some datasets focus on long and detailed instruc-   tions with the route description at every step to   achieve fine - grained language grounding ( Ander-   son et al . , 2018b ; Ku et al . , 2020 ) or long - horizon   navigation ( Jain et al . , 2019 ; Zhu et al . , 2020a ) . For   instance , from Room - to - Room ( R2R ) ( Anderson   et al . , 2018b ) , to Room - Across - Room ( RxR ) ( Ku   et al . , 2020 ) , the average instruction length in-   creases from 29 to 129 words . Other datasets   have coarse - grained instructions like REVERIE   ( Qi et al . , 2020 ) or SOON ( Zhu et al . , 2021a ) .   Agents are trained and evaluated on a single granu-6394larity or one type of expression .   In contrast , we propose to evaluate VLN agents   on multi - level granularity to better understand the   behavior of embodied agents with respect to lan-   guage variations . Our motivation is that users are   inclined to give shorter instructions instead of de-   tailed route descriptions because 1 ) users are not   omniscient observers who follow the route and   describe it step by step for the agent ; 2 ) shorter   instructions are more practical , reproducible , and   efficient from a user ’s perspective . 3 ) users tend to   underspecify commands in familiar environments   like personal households . Therefore , we propose   a new setting , namely Underspecified vision - and-   Language Navigation ( ULN ) and associated eval-   uation datasets on top of R2R , namely R2R - ULN   to address these issues . R2R - ULN contains under-   specified instructions where route descriptions are   successively removed from the original instructions .   Each long R2R instruction corresponds to three   shortened and rephrased instructions belonging to   different levels , which preserves partial alignment   but also introduces variances .   As shown in Fig . 1 , the goal of ULN is to facili-   tate the development of a generalized VLN design   that achieves balanced performance across all gran-   ularity levels . As a primary step toward ULN , we   propose a modular VLN framework that consists of   an instruction classification module , a navigational   agent , and an Exploitation - to - Exploration ( E2E )   module . The classification module first classifies   the input instruction as high - level or low - level in   granularity so that our agent can encode these two   types accordingly . As for the agent , we propose   to learn Granularity Specific Sub - networks ( GSS )   to handle both levels with minimally additional pa-   rameters . A sub - network , e.g. , the text encoder , is   trained for each level while other parameters are   shared . Finally , the E2E module estimates the step-   wise language grounding uncertainty and conducts   multi - step lookahead exploration to rectify wrong   decisions that originated from underspecified lan-   guage .   Our VLN framework is model - agnostic and can   be applied to many previous agents that follow a   “ encode - then - fuse ” mechanism for language and vi-   sual inputs . We establish our framework based on   two state - of - the - art ( SOTA ) VLN agents to demon-   strate its effectiveness . We conduct extensive ex-   periments to analyze the generalization of existing   agents and our framework in ULN and the orig - inal datasets with fine - grained instructions . Our   contribution is three - fold :   •We propose a novel setting named Underspec-   ified vision - and - Language Navigation ( ULN )   to account for multi - level language variations   for instructions . We collect a large - scale eval-   uation dataset R2R - ULN which consists of 9k   validation and 4k testing instructions .   •We propose a VLN framework that consists   of Granularity Specific Sub - networks ( GSS )   and an E2E module for navigation agents to   handle both low - level and high - level instruc-   tions .   •Experiments show that achieving consistent   performance across multi - level underspecifi-   cation can be much more challenging to ex-   isting VLN agents . Furthermore , our VLN   framework can improve the success rate by   „ 10 % relatively over the baselines and miti-   gate the performance gap across all levels .   2 Related Work   Language Variations for Multimodal Learning   Natural language input has been an essential com-   ponent of modern multimodal learning tasks to   combine with other modalities such as vision ( An-   tol et al . , 2015 ; Johnson et al . , 2017 ) , speech   ( Alayrac et al . , 2020 ) or gestures ( Chen et al . ,   2021b ) . The effect of language variations has been   studied in many vision - and - language ( V&L ) tasks   ( Bisk et al . , 2016 ; Agrawal et al . , 2018 ; Cirik et al . ,   2018 ; Zhu et al . , 2020b ; Lin et al . , 2021 ) . For in-   stance , referring expression datasets ( Kazemzadeh   et al . , 2014 ; Yu et al . , 2016 ; Mao et al . , 2016 ) con-   tain multiple expressions for the same referring   object . Ref - Adv ( Akula et al . , 2020 ) studies the ro-   bustness of referring expression models by switch-   ing word orders . In Visual Question Answering   ( VQA ) , Shah et al . ( 2019 ) discovers that VQA   models are brittle to rephrased questions with the   same meaning . As for VLN , we characterize the   linguistic and compositional variations in rephras-   ing and dropping sub - instructions from a full in-   struction with complete route descriptions . We also   define three different levels to formalize underspec-   ification for navigational instructions .   VLN Datasets VLN has gained much attention   ( Gu et al . , 2022 ) with emergence of various simula-   tion environments and datasets ( Chang et al . , 2017;6395Kolve et al . , 2017 ; Jain et al . , 2019 ; Nguyen and   Daumé III , 2019 ; Koh et al . , 2021 ) . R2R ( Ander-   son et al . , 2018a ) and RxR ( Ku et al . , 2020 ) provide   fine - grained instructions which guide the agent in   a step - wise manner . FG - R2R ( Hong et al . , 2020a )   and Landmark - RxR ( He et al . , 2021 ) segments the   instructions into action units and explicitly ground   sub - instructions on visual observation . In contrast ,   REVERIE ( Qi et al . , 2020 ) , and SOON ( Zhu et al . ,   2021a ) proposes to use referring expression with   no guidance on intermediate steps that lead to the   final destination . Compared to these datasets , ULN   aims to build an agent that can generalize to multi-   level granularity after training once , which is more   practical for real - world applications .   Embodied Navigation Agents Learning to   ground instructions on visual observations is one   major problem for an agent to generalize to an un-   seen environment ( Wang et al . , 2019 ; Deng et al . ,   2020 ; Fu et al . , 2020 ) . Previous studies demon-   strate significant improvement by data augmenta-   tions ( Fried et al . , 2018 ; Tan et al . , 2019 ; Zhu et al . ,   2021b ; Fang et al . , 2022 ; Li et al . , 2022 ) , designing   pre - training tasks ( Hao et al . , 2020 ; Chen et al . ,   2021a ; Qiao et al . , 2022 ) and decoding algorithms   ( Ma et al . , 2019a ; Ke et al . , 2019 ; Ma et al . , 2019b ;   Chen et al . , 2022 ) . For exploration - based meth-   ods , FAST ( Ke et al . , 2019 ) proposes a searching   algorithm that allows the agent to backtrack to the   most promising trajectory . SSM ( Wang et al . , 2021 )   memorizes local and global action spaces and es-   timates multiple scores for candidate nodes in the   frontier of trajectories . Compared to E2E , Active   VLN ( Wang et al . , 2020 ) is the most relevant work   where they learn an additional policy for multi - step   exploration . However , they define the reward func-   tion based on distances to target locations , while   our uncertainty estimation is based on step - wise   grounding mismatch . Our E2E module is also more   efficient that has fewer parameters and low training   complexity .   3 Underspecification in VLN   Our dataset construction is three - fold : We first ob-   tain underspecified instructions by asking work-   ers to simplify and rephrase the R2R instructions .   Then , we validate that the goals are still reachable   with underspecified instructions . Finally , we verify   that instructions from R2R - ULN are preferred to   R2R ones from a user ’s perspective , which proves   the necessity of the ULN setting . We briefly de-   scribe definitions and our ULN dataset in this sec-   tion with more details in Appendix A.   3.1 Instruction Simplification   We formalize the instruction collection as a sen-   tence simplification task and ask human annotators   to remove details from the instructions progres-   sively . Denoting the original R2R instructions as   Level 0 ( L ) , annotators rewrite each Linto three   different levels of underspecified instructions . We   discover that some components in Lcan be redun-   dant or rarely used in indoor environments , such   as “ turn 45 degrees ” . Therefore , to obtain Level 1   ( L ) from each Linstruction , annotators rewrite   Lby removing any redundant part but keep most   of the route description unchanged . Redundant   components include but are not limited to repeti-   tion , excessive details , and directional phrases ( See   Table 1 ) . As for Level 2 ( L ) , annotators remove   one or two sub - instructions from L , creating a   scenario where the users omit some details in com-   monplaces . We collect Level 3 ( L ) instructions   by giving destination information such as region   label and floor level and ask annotators to write one   sentence directly referring to the object or location   of the destination point .   3.2 Instruction Verification   To ensure that the underspecified instructions pro-   vide a feasible performance upper bound for VLN   agents , we have another group of annotators navi-   gate in an interactive interface from R2R ( Anderson6396   et al . , 2018b ) . As is shown in Table 2 , annotators   achieve a slightly degraded but promising success   rate ( SR ) with L. SPL is a metric that normalizes   SR over the path length . Therefore , the trade - off   for maintaining high SR is to have more explo-   ration steps , resulting in a much lower SPL value .   We also verify that L , iPt1,2,3uare more prac-   tical and efficient choices than L. Table 2 shows   that people prefer underspecified instructions over   full instructions in both aspects , with an increasing   trend as iincreases to 3 .   4 Method   4.1 Overview   In this section , we present our VLN framework   for handling multi - level underspecified language   inputs , which mainly consists of three modules ( see   Figure 2 ) . Given a natural language instruction in   a sequence of tokens , W“pw , . . . wq , the clas-   sification module first categorize language input as   low - level ( L , L , L ) or high - level ( L ) instruc-   tions . To handle these two types accordingly , GSS   learns a sub - network , e.g. , the text encoder , for   each type while the other parameters are shared .   At each step t , we denote the visual observation   O“prv;as , . . . , rv , asqwith visual feature   vand angle feature aofi - th view among all N   views . The history contains a concatenation of   all observations before tmH“pO , . . . , Qq .   GivenW , H , O , the GSS - based agent predicts   a an action aby choosing a navigable viewpoint   fromO. To overcome the reference misalignment   issue , the E2E module predicts a sequence of uncer-   tainty score S“ps , ... , sqand conducts multi-   step exploration to collect future visual informa-   tion .   4.2 Instruction Classification   VLN agents can operate in two different modes ,   fidelity - oriented or goal - oriented , depending on   reward functions ( Jain et al . , 2019 ) or text inputs   ( Zhu et al . , 2022 ) . Agents trained on low - level gran-   ularity encounter performance degradation when   applied to high - level ones , and vice versa . As is   shown in Figure 2 , we propose first to classify the   text inputs into two granularities and then encode   them independently in downstream modules . Our   classification module contains an embedding layer ,   average pooling , and a fully - connected layer to   output binary class predictions .   4.3 Navigation Agent   Base Agent We summarize the high - level frame-   work of many transformer - based agents ( Hao et al . ,   2020 ; Guhur et al . , 2021 ; Moudgil et al . , 2021 )   paramterized as θas shown in Figure 2 . Given   the history H , textX , visual observation O , the   agent first encodes each modality input with en-   coders f , f , f :   X“fpWq , H“fpHq ,   O“fpOq(1 )   HAMT ( Chen et al . , 2021a ) applies ViT ( Doso-   vitskiy et al . , 2020 ) and a Panoramic Trans-   former to hierarchically encode Has a se-   quence of embeddings H“ph , . . . , hqwhile   VLNBERT ( Hong et al . , 2021 ) encodes Has   a state vector H“h . The embedding from   each modality is then fed into a L - layer cross-   modal transformer f , and passed through a cross-   attention first in each layer l :   α“prH;OsWqpXWq   ? d(2 )   where αdenotes the attention weights of   history - visual concatenation on the language em-   beddings , dis the hidden dimension . We omit6397the attention head index for simplicity . For   VLNBERT , it concatenated state Hwith X   instead . The prediction of arelies on either a   two - layer FC network f , or the summation of   attention weights of HonOover all heads :   β“fpOdxq ( 3 )   β“ÿpHWqpOWq   ? d(4 )   a“arg maxpβq ( 5 )   where O , Xare the observation and language to-   kens output from f.   Granularity Specific Sub - network Training an   ensemble of agents or one agent with a mixture of   levels can be inefficient or sub - optimal for ULN .   Instead , we find a sub - network that influences the   agent ’s navigation mode , as shown in Figure 3 . We   identify such sub - network by the following steps :   1.Train an agent θwith full instructions Land   a separate agent θwith the last sentence of   L. Denote θperformance on Las metric   value m.   2.For each of the sub - network , f , f , f ,   andfinθ , load its weights to θand de-   note the performance on Lasmwhere   xP ttext , img , hist , cm uindicates the sub-   network replaced .   3.Find the sub - network with the maximum gain   in metric value on Lafter replacement , i.e.   x“arg maxpm´mq .   After identifying the critical sub - network f ,   we train a new ffrom scratch with the rest of the   model parameters loaded from θand kept frozen .   4.4 Exploitation to Exploration   Multi - level inputs introduces Temporal Reference   Misalignment ( TRM ) . As the agent gradually   shifts its attention to sub - instructions , it lacks a   mechanism to ensure the attended text segments   align with the visual observation transition . Conse-   quently , after several steps , agents can not correctly   ground sub - instructions to visual features . To miti-   gate this issue , we propose an E2E module to esti-   mate step - wise uncertainty and perform multi - step   lookahead to skip the dilemma .   Uncertainty Estimation We evaluate the deci-   sion uncertainty at each step based on the attention   score distribution . TRM changes the distribution   ofα , βand makes them different from the   distribution when full instructions are given . There-   fore , the joint distribution of α , βimplies the   degree of grounding uncertainty . We simply input   the concatenation α , βto a two - layer MLP   f to learn the uncertainty score :   s“fprα;βsq . ( 6 )   sPr0,1sindicates whether the agent is con-   fident for the decision . If sis greater than a   threshold , the agent first explores the environment   before the next step decisions .   Multi - Step Lookahead When score s in-   dicates an uncertain decision , our system calcu-   lates the likelihood of success by exploration and   re - evaluates the action logits . Specifically , the ex-   plorer moves Ksteps forward for each of the top   Ccandidate actions at step t. Since unnormalized   logits incorporate alignment between actions and   instructions , we adopt the attention weights on vi-   sual candidates , i.e. , β . The new action probability   estimation accounts for a weighted sum of the fu-   ture logits sequence with a hyperparameter γ :   a“arg max “   β`ÿγmaxpβq‰   .(7 )   βis the logit value for candidate c , andβ   are the logits after executing greedy action at step   t`i´1 . For parameter efficiency , we utilize the   trained agent as the explorer . Our lookahead heuris-   tic differentiates from Active VLN ( Wang et al . ,   2020 ) as we explicitly quantify the misalignment6398   by uncertainty estimation , while the latter depends   on goal - based reward functions with implicit su-   pervision . Our E2E module is also more efficient   and stable as we spare the need to train a separate   policy for exploration .   State Freeze The history encoding hserves as   an important query to attend to on both visual and   language domains . Due to the underspecified lan-   guage input , the history - attended instruction ad-   vances the transition in the visual scenarios . There-   fore , to calculate βin Equation 7 , we shall   utilize hinstead of hto maintain the attention   on the pending sub - instruction until alignment re-   covers .   5 Experiments   Dataset R2R ( Anderson et al . , 2018b ) contains   over 14k instructions for training , 1k for validation   seen environments ( Val - seen ) , 2.3k for validation   unseen ( Val - unseen ) . As for R2R - ULN , we select   around 1600 longest instructions from the R2R   validation set as L. We assign three different an-   notators to simplify each Land filter low - quality   samples . R2R - ULN includes 3132 instructions for   Val - seen , 6714 for Val - unseen , and 4198 test un-   seen . We train the agent and other modules on R2R   ( train split ) only and evaluate our system on R2R   and R2R - ULN without re - training . To maintain the   ratio between the training and validation set , we   use a subset of 2k Val - seen and 4.5k Val - unseen   from the full R2R - ULN for evaluation as default if   not specified . We randomly sample 30 % L1 , 70 %   L2 and 100 % L3 from the full set based on the   preference results ( Table 2 ) . We also report the   evaluation results in the full set in Appendix . Evaluation Metrics We evaluate the navigation   performance using the standard metrics of R2R :   Trajectory Length ( TL ): the agent ’s navigation path   in meters ; Navigation Error ( NE Ó ): the average dis-   tance between the goal and agent ’s final location ;   Success Rate ( SR Ò ): the ratio of trials that end   within 3 meters to the overall target trials ; Success   weighted by inverse Path Length ( SPL Ò ) ( Ander-   son et al . , 2018a ) .   Implementation Details We adopt full instruc-   tions as low - level samples ( R2R ) and the last sen-   tences of instructions as high - level samples ( R2R-   last ) . We train the classification module and the   agents with these two training sets . For uncer-   tainty estimation training , we applied the chunk-   ing function ( Hong et al . , 2020a ) to randomly   drop sub - instructions from R2R and create pseudo-   underspecified instructions as inputs to a trained   agent . At each step , if the agent ’s action is dif-   ferent from the teacher ’s action , the uncertainty   ground truth label is 1 , else 0 . During inference , an   uncertainty score over 0.5 will initiate multi - step   lookahead . We also limit the lookahead to at most   three times for performance benefit . We explain   this choice in Sec . 5.2.3 .   We train the classification and the agent with a   low - level text encoder for 300,000 iterations with a   learning rate of 1e-2 and 1e-5 . The agent is trained   on a mix of imitation learning , and A2C ( Mnih   et al . , 2016 ) , the same as the baselines . Then we   train the high - level text encoder with other param-   eters fixed for 10,000 iterations . Finally , for the   E2E module , we train the uncertainty estimation   network with a learning rate 1e-4 for 10 epochs .   We directly adopt the trained agent as the explorer6399   for efficiency . All training stages are done on a   single GPU with AdamW optimizer ( Loshchilov   and Hutter , 2018 ) . For E2E , we set γas 1.2 and K   as 1 for the best performance and balance between   SR and SPL .   5.1 Main Results   Comparison with SOTA Agents We first com-   pare our GSS method with the state - of - the - art   ( SOTA ) model HAMT ( Chen et al . , 2021a ) , and   a strong baseline VLNBERT . We mainly con-   sider three training sets for the baselines : R2R only ,   R2R - last only , and R2R+R2R - last . Table 3 shows   that training on a single granularity inevitably de-   grades the agent ’s performance on the other level .   A mixture of these two levels shows a compro-   mised performance across R2R and R2R - ULN val - idation sets . In contrast , our method can achieve   3.8 % absolute SR improvement for R2R - ULN L   while maintaining the performance on R2R. Our   GSS only requires 30 % additional training itera-   tions , 25 % - 38 % extra parameters , and can achieve   slightly better performance .   Greedy - Decoding Agents As is shown in the   top section ( Row 1 - 5 ) of Table 4 , greedy - decoding   agents generally struggle with R2R - ULN instruc-   tions . These agents ’ SR and SPL values gener-   ally drop relatively by 40 - 50 % , indicating a poten-   tial performance degradation and risk due to lan-   guage variations when we deploy these models in   more realistic household environments . Note that   VLNBERT and HAMT are more robust with   only a relatively 30 % decrease in SR . This may   attribute to better transformer architectures and ini-   tialization from large - scale pre - trained models .   Exploration - based Agents We select three   exploration - based agents : FAST - Short ( Ke et al . ,   2019 ) , Active VLN ( Wang et al . , 2020 ) , SSM   ( Wang et al . , 2021 ) . Table 4 shows that FAST   and SSM navigate longer trajectories and achieve   better SR . They are even better than VLNBERT   and HAMT on Val - seen though they underperform   these SOTA agents on R2R.   Our system outperforms the corresponding base-6400   lines by around 2 % in SR and 1.6 % in SPL without   exploration . With the E2E module , our system   gains additional improvement by sacrificing the tra-   jectory length , resulting in lower SPL values . The   improvement from GSS and E2E is consistent , as   shown in Table 5 . Since the target level of GSS is   L , only LSR is improved without E2E. On the   other hand , E2E can improve performance across   all levels by estimating grounding uncertainty and   looking ahead of frontiers .   Comparison on R2R Table 3 shows the evalua-   tion results on R2R Val - unseen . Our classification   module and GSS can achieve similar or better per-   formance than the baselines . We disable E2E since   ULN does not encourage exploration for L , but we   also show the performance with E2E in Appendix   B Table 11 . E2E improves the SR by 1.4 % by   sacrificing path length for VLNBERT while de-   creasing SR by 1.8 % for HAMT . This is potentially   due to inaccurate uncertainty estimation and exces-   sive exploration for full instructions . Considering   the overall performance in R2R and R2R - ULN , our   method still improves the SR by an absolute 1 - 3 %   ( see Appendix B.2 ) .   5.2 Ablation Studies   5.2.1 Component Analysis   We demonstrate the effectiveness of the GSS and   E2E module in our framework in Table 6 . Adding   GSS brings the most significant SR gain in Lby   5.1 % percent . This gain is intuitive as the high-   level sub - network is trained on coarse - grained in-   structions only . Adding the classification module   harms the performance for Lbut improves it for   L. That is because of the 85 % accuracy and po-   tentially some Linstructions being too short . The   high - level sub - network is thus more suitable for   these Linstructions . Secondly , adding the looka-   head heuristic improves the SR by 1 - 2 % consis-   tently across all levels . The state freeze trick is also   beneficial as it further improves the SR by 1 % for   L , L. Finally , we verify that gains from GSS and   E2E are supplementary to each other . The last two   rows show that adding lookahead on top of GSS   can improve 1 - 2 % SR on L , Lbut can not help   withL. With state freeze , we observe an addi-   tional 2 % SR gain in L. The potential reason is   that the lookahead heuristic has overlapping bene-   fits with GSS , but state freeze is a complementary   trick for high - level sub - network .   5.2.2 Exploration Threshold   As is mentioned in Section 4.4 , the E2E module   initiates an exploration when the uncertainty score   sexceeds a threshold . We investigate the ef-   fect of this threshold value on the evaluation results   in Table 7 . A threshold value of 1.0 indicates no   exploration at all . Decreasing the threshold im-   poses more explorations and improves SR by 2 - 3 %   across all levels at the expense of lower SPL . When   the value goes below 0.5 , the improvement in SR is   marginal , while the reduction in SPL is significant .   Therefore , we select 0.5 as our threshold for the   best SR - SPL trade - off in practice .   5.2.3 Exploration Accuracy   Uncertainty Estimation Beyond success - based   metrics , the accuracy of uncertainty estimation is   an important indicator of whether the exploration   steps are necessary . Given an underspecified in-6401struction , we measure the accuracy with teacher   corrections , i.e. , correcting the agent ’s action when   it deviates from the teacher ’s action . Figure 4(a )   shows the accuracy for steps between two consecu-   tive corrections . Interestingly , the accuracy drops   to a minimum between the first and second cor-   rection and increases as the agent moves forward .   Despite that the estimation becomes most inaccu-   rate when the agent makes its second or third false   decision , we will show that these steps are critical   ones to be adjusted .   Early Exploration One interesting question that   arises from the above observation is : should we   seize explorations for the first several uncertainties   and encourage explorations in later steps ? The an-   swer can be revealed by investigating the number   of steps between two teacher corrections . Surpris-   ingly , Figure 4(b ) indicates that Tideviates from   Tat almost every step after the third correction .   Assuming that our one - step lookahead is perfect   as the teacher correction , the phenomenon implies   that our agent has to make an exploration for ev-   ery single step after the third exploration . However ,   such behavior is undesirable as it excessively harms   navigation efficiency . Fig . 4 together implies that   agents must rely on early explorations instead of   later ones . Otherwise , the system error accumulates   exponentially and becomes intractable eventually .   Therefore , our agent relies on early explorations   even though the uncertainty estimation accuracy is   relatively defective .   6 Conclusion   In this work , we consider a new setting , Underspec-   ified vision - and - Language Navigation ( ULN ) . We   collected a large - scale evaluation set , R2R - ULN ,   with multi - level underspecified instructions . We   show that ULN is a reasonable and practical setting .   ULN presents a novel direction where explorations   are necessary and justifiable . As a first step towards   ULN , we propose two novel components to build a   VLN framework , Granularity Specific Sub - network   ( GSS ) and Exploitation - to - Exploration ( E2E ) . Ex-   perimental results show that GSS and E2E effec-   tively mitigate the performance gap across all levels   of instruction . Finally , we believe that ULN is a   more challenging setting for future VLN develop-   ment , and our framework can be further improved   with more sophisticated policy design or language   grounding models.7 Limitations   In this study , we only cover Vision - Language Nav-   igation datasets with English instructions . Instruc-   tions in other languages may characterize different   types of ambiguity or underspecification . Thus , ex-   panding the datasets to multi - lingual ULN based on   datasets like RxR is essential . Secondly , we only   consider indoor environments where the instruc-   tions are generally shorter than outdoor ones due to   shorter path lengths . However , the phenomenon of   underspecification can also be expected outdoors ,   accompanied by other modalities such as hand ges-   tures or hand sketches . We simply assumed that un-   derspecification is a more ubiquitous phenomenon   in the indoor than the outdoor environment , which   may be overturned from additional surveys or ex-   periments . In the future , we hope to expand our   work to multi - lingual instructions and outdoor en-   vironments and combine it with more modalities .   8 Ethical Considerations   For data collection and verification on Amazon Me-   chanical Turk , we select annotators from English-   speaking countries , including the US , CA , UK , AU ,   and NZ . Each HIT for instruction simplification   takes around 1.5 minutes on average to accom-   plish , and we pay each submitted HIT with 0.4 US   dollars , resulting in an hourly payment of 16 US   dollars . As for the instruction following , each HIT   takes around 2 minutes to accomplish , and we pay   each HIT 0.5 US dollars per HIT , resulting in an   hourly payment of 15 US dollars . In addition , we   award each successful navigation attempt with 0.3   US dollars for high - quality verification . As for the   preference assessment , each HIT takes around 1   minute to accomplish , and we pay each HIT 0.3   US dollars , resulting in an hourly payment of 18   US dollars .   9 Acknowledgment   We would like to thank the Robert N. Noyce Trust   for their generous gift to the University of Califor-   nia via the Noyce Initiative . The work is also par-   tially funded by an unrestricted gift from Google .   The writers ’ opinions and conclusions in this publi-   cation are their own and should not be construed as   representing the sponsors ’ official policy , expressed   or inferred.6402References640364046405   A ULN Datasets   A.1 Data Collection   Denote the original R2R instructions as Level 0   ( L ) . We define Level 1 ( L ) instructions as ones   where the redundant part is removed from L.   Level 2 ( L ) instructions are partial route descrip-   tion with some sub - instructions removed from L.   Level 3 ( L ) instructions directly refer to the goal   destination without any intermediate route informa-   tion .   Level 1 : We form the data collection stage as a   sentence simplification task and ask human work-   ers to progressively omit details from the instruc-   tion . For the first step , workers remove redundant   parts from Lto obtain L. Redundancy includes   but is not limited to repetition , excessive details ,   and directional phrases . To make it more human-   centric , we allow the workers to determine the   degree of redundancy . For example , some may   rewrite “ turn 180 degrees ” as “ turn around ” , while   others may delete the whole phrase assuming that   “ turn around ” is still redundant .   Level 2 : Then , one or two phrases containing   objects are removed from L , resulting in partial   route descriptions ( L).Lassumes that humans   tend to ignore intermediate references when the   route is partially visible , or they are familiar with   the environment .   Level 3 : Finally , the third step requires workers   to write one sentence directly referring to the goals   by combining Land providing information like   region label and floor level ( L).Lresembles   instructions in REVERIE ( Qi et al . , 2020 ) but is   restricted to pure navigational instructions while   REVERIE commands the agent to interact with   target objects . As a result , we obtain one triplet   ( L , L , L ) perLinstruction per annotator .   Such progressive simplification design enables   us to control the inter - level alignment as workers   inject no external objects / directions or substitute   existing ones with external ones for each L. It   also preserves intra - level sample variance since   workers simplify the sentences based on subjec-   tive judgments on the degree of redundancy , yet   circumscribed by the definition of levels .   Instruction Following We ask workers to reach   the goal by following instructions and operating in   an interactive WebGL environment . We randomly   sample 250 triplets plus Linstructions from the   validation set . We follow a similar setup as in ( An-   derson et al . , 2018b ) . As is shown in Table 2 , work-   ers can still achieve over 80 % success rate ( SR ) , the   most suitable metric for ULN , on LandLand   maintain a high - quality performance on L. Hence ,   ULN is a feasible setting where agents should ac-   tively make more explorations as the instructions   become less specific.6406   Preference Assessment As is shown in the bot-   tom right of Fig . 5 , we investigate human prefer-   ence on whether the full instructions or our col-   lected ones are more practical and realistic . We   sample 600 L - Linstructions and form each one   with its corresponding Linstruction as a pair ( Lx ,   L ) , xP r1,3s . Given ( Lx , L ) and starting and   goal viewpoint panoramas , workers are asked “ Q1 .   Which one is a more practical expression in daily   life ? ” and “ Q2 . Which one do you prefer to speak   to command the robot , considering applicability   and efficiency ? ” . As shown in Table 2 , the results   demonstrate an increasing trend in choosing shorter   and less specific instructions to reflect more prac-   tical expressions and benefit applicability and effi-   ciency .   A.2 Dataset Statistics   As illustrated in Table 8 , R2R - ULN preserves most   of the trajectories from the R2R validation set . It   addresses the shortage of underspecified instruc-   tions , which is essential for evaluating the gener-   alization of embodied agents to various language   expressions . Instructions of different lengths are   better aligned , making it a better reference to in-   vestigate the correlation between instruction length   and agent performance .   B Supplementary Experiments   B.1 Identifying GSS   Intuitively , this sub - network incorporates into the   cross - modal transformer . The conjecture is that   the cross - attention layers shift the vision - to - text   attention from the first token to the end for low-   level instructions . On the other hand , for high - level   instructions , the visual features may persistently   attend to the goal object tokens . To verify this   intuition ,   We show three additional experimental results   here for supplementary . First , as is shown in Ta-   ble 10 , we identify the critical sub - network by re-   placing part of a low - level agent with that from   a high - level agent and observe the performance   change . For VLNBERT , the most crucial sub-6407   network that impacts its navigation mode is the text   encoder , as replacing it achieves the highest perfor-   mance . Note that replacing the embedding layer   downgrades the performance . We hypothesize that   the high - level agent is trained with a smaller vo-   cabulary due to shorter instructions . As for HAMT ,   the critical sub - network is the cross - modal encoder ,   which aligns with our hypothesis .   B.2 Ablation Studies   R2R Evaluation We also show full performance   results on R2R in Table 11 . Our framework   maintains a comparable performance on R2R for   VLNBERT and even slightly improves the SR   with E2E. However , as for HAMT , our framework   downgrades the SR by 0.6 % . This is due to the im-   perfect classifier that misclassifies some short R2R   instructions as high - level instructions . We empiri-   cally find that the HAMT is more sensitive to clas-   sifier error , indicating better robustness of HAMT   in handling short R2R instructions . It spares the   need to run the high - level sub - network to deal with   those exceptions in R2R.   Combining Multiple Datasets Since there are   existing high - level datasets like REVERIE ( Qi   et al . , 2020 ) and SOON ( Zhu et al . , 2021a ) , there is   also a natural motivation to combine these datasets   during training since these instructions can be seen   as goal instructions ( similar to L ) . We trained   three agents with a combination of R2R - Last and   one of the high - level datasets . For comparison , we   also train an agent with R2R - Last , and the same   amount of speaker - augmented instructions . Note   that the speaker is trained with last sentences as   well , so it is a goal instruction speaker . As is   shown in Table 12 , existing datasets have limited   benefit on Levaluation . REVERIE and SOON in-   structions contain both navigation and localization   orders , making these datasets noisier for pure navi-   gation training and evaluation . We leave it as future   work to consider combining VLN datasets of dif-   ferent settings for one unified , general navigational   agent.6408   Path Length As is shown in Fig . 6 , most greedy-   decoding agents learn a spurious correlation be-   tween instruction length and trajectory length ( TL ) ,   reflected as a decreasing trend in TL from Lto   L. However , SOTA agents are more robust as they   navigate slightly longer for L3 . Exploration - based   agents showed significantly longer TL and increas-   ing trends , which is more robust ( in SR percentage   drop ) and interpretable .   Component Analysis Last but not least , we show   the full ablation table with SPL compared to Table   6 . Our GSS - only framework achieves the highest   SPL while adding the E2E module decreases the   SPL value . This is intuitive as the purpose of E2E   is to improve the success rate by making more   exploration steps , and will result in a smaller SPL   value .   B.3 Trajectory Visualization   We provide visualizations of a set of underspeci-   fied instruction ( L , L , L ) in add to Fig . 1 . As is   shown in Fig . 8 , the base agent fails to go down-   stairs from the beginning and keeps moving around   on the initial floor . Our agent also navigates on the   initial floor for a long time . However , it starts mov-   ing downstairs from step 14 thanks to uncertainty   estimation and active exploration at the previous   step . Finally , it reaches the lounge downstairs . Fig .   9 shows the trajectories of an Linstruction . Simi - larly , the base agent keeps moving around on the   initial floor and fails to go downstairs . However ,   our agent classifies the instruction as high - level and   adopts the high - level GSS to guide the navigation   decisions . Therefore , it reaches the goal with only   seven steps . As for L , the base agent manages to   go downstairs at step 7 but keeps moving down-   stairs and completely misses out on the lounge ,   resulting in a long path . In contrast , our agent with   the high - level GSS reaches the target location with   only seven steps . There is no exploration because   no uncertain steps are identified.6409641064116412