  Zhisong Zhang , Emma Strubell , Eduard Hovy   Language Technologies Institute , Carnegie Mellon University   zhisongz@cs.cmu.edu , strubell@cmu.edu , hovy@cmu.edu   Abstract   In this work , we provide a literature review of   active learning ( AL ) for its applications in natu-   ral language processing ( NLP ) . In addition to a   fine - grained categorization of query strategies ,   we also investigate several other important as-   pects of applying AL to NLP problems . These   include AL for structured prediction tasks , an-   notation cost , model learning ( especially with   deep neural models ) , and starting and stopping   AL . Finally , we conclude with a discussion of   related topics and future directions .   1 Introduction   The majority of modern natural language process-   ing ( NLP ) systems are based on data - driven ma-   chine learning models . The success of these models   depends on the quality and quantity of the avail-   able target training data . While these models can   obtain impressive performance if given enough su-   pervision , it is usually expensive to collect large   amounts of annotations , especially considering that   the labeling process can be laborious and challeng-   ing for NLP tasks ( § 3.2 ) . Active learning ( AL ) , an   approach that aims to achieve high accuracy with   fewer training labels by allowing a model to choose   the data to be annotated and used for learning , is   a widely - studied approach to tackle this labeling   bottleneck ( Settles , 2009 ) .   Active learning has been studied for more than   twenty years ( Lewis and Gale , 1994 ; Lewis and   Catlett , 1994 ; Cohn et al . , 1994 , 1996 ) and there   have been several literature surveys on this topic   ( Settles , 2009 ; Olsson , 2009 ; Fu et al . , 2013 ; Aggar-   wal et al . , 2014 ; Hino , 2020 ; Schröder and Niekler ,   2020 ; Ren et al . , 2021 ; Zhan et al . , 2022 ) . Nev-   ertheless , there is still a lack of an AL survey for   NLP that includes recent advances . Settles ( 2009 )   and Olsson ( 2009 ) provide great surveys covering   AL for NLP , but these surveys are now more than   a decade old . In the meantime , the field of NLP   has been transformed by deep learning . WhileFigure 1 : Counts of AL ( left ) and “ neural ” ( right ) pa-   pers in the ACL Anthology over the past twenty years .   other more recent surveys cover deep active learn-   ing , they are either too specific , focused only on   text classification ( Schröder and Niekler , 2020 ) , or   too general , covering AI applications more broadly   ( Ren et al . , 2021 ; Zhan et al . , 2022 ) . Moreover ,   applying AL to NLP tasks requires specific consid-   erations , e.g. handling complex output structures   and trade - offs in text annotation cost ( § 3 ) , which   have not been thoroughly discussed .   In order to provide an NLP - specific AL survey ,   we start by searching the ACL Anthology for AL-   related papers . We simply search for the keyword   “ active ” in paper titles and then perform manual   filtering . We also gradually include relevant papers   missed by keyword search and papers from other   venues encountered by following reference links   throughout the surveying process . The distribution   of AL - related papers in the ACL Anthology over   the past twenty years is shown in Figure 1 , which   also includes rough counts of works concerning   neural models by searching for the word “ neural ”   in titles . The overall trend is interesting . There is   a peak around the years of 2009 and 2010 , while   the counts drop and fluctuate during the mid-2010s ,   which corresponds to the time when neural models   became prominent in NLP . We observe a renewed   interest in AL research in recent years , which is6166Algorithm 1 A typical active learning procedure .   primarily focused on deep active learning ( Ren   et al . , 2021 ; Zhan et al . , 2022 ) .   1.1 Overview   We mainly examine the widely utilized pool - based   scenario ( Lewis and Gale , 1994 ) , where a pool of   unlabeled data is available and instances are drawn   from the pool to be annotated . Algorithm 1 illus-   trates a typical AL procedure , which consists of a   loop of instance selection with the current model   and model training with updated annotations . The   remainder of this survey is organized correspond-   ing to the main steps in this procedure :   •In § 2 , we discuss the core aspect of AL : Query   strategies , with a fine - grained categorization over   informativeness ( § 2.1 ) , representativeness ( § 2.2 )   and the combination of these two ( § 2.3 ) .   •In § 3 , we cover the two additional important   topics of querying and annotating for NLP tasks :   AL for structured prediction tasks ( § 3.1 ) and the   cost of annotation with AL ( § 3.2 ) .   •In § 4 , we discuss model and learning : the query-   successor model mismatch scenario ( § 4.1 ) and   AL with advanced learning techniques ( § 4.2 ) .   •In § 5 , we examine methods for starting ( § 5.1 )   and stopping ( § 5.2 ) AL .   In § 6 , we conclude with related and future direc-   tions . We also include representative AL works for   various NLP tasks in Appendix A and some other   aspects of AL for NLP in Appendix B.   2 Query Strategies   2.1 Informativeness   Informativeness - based query strategies mostly as-   sign an informative measure to each unlabeled in-   stance individually . The instance(s ) with the high-   est measure will be selected .   2.1.1 Output Uncertainty   Uncertainty sampling ( Lewis and Gale , 1994 )   is probably the simplest and the most commonlyutilized query strategy . It prefers the most un-   certain instances judged by the model outputs .   For probabilistic models , entropy - based ( Shannon ,   1948 ) , least - confidence ( Culotta and McCallum ,   2005 ) and margin - sampling ( Scheffer et al . , 2001 ;   Schein and Ungar , 2007 ) are three typical uncer-   tainty sampling strategies ( Settles , 2009 ) . Schröder   et al . ( 2022 ) revisit some of these uncertainty - based   strategies with Transformer - based models and pro-   vide empirical results for text classification . For   non - probabilistic models , similar ideas can be uti-   lized , such as selecting the instances that are close   to the decision boundary in an SVM ( Schohn and   Cohn , 2000 ; Tong and Koller , 2001 ) .   Another way to measure output uncertainty is   to check the divergence of a model ’s predictions   with respect to an instance ’s local region . If an   instance is near the decision boundary , the model ’s   outputs may be different within its local region . In   this spirit , recent works examine different ways to   check instances ’ local divergence , such as nearest-   neighbour searches ( Margatina et al . , 2021 ) , adver-   sarial perturbation ( Zhang et al . , 2022b ) and data   augmentation ( Jiang et al . , 2020 ) .   2.1.2 Disagreement   Uncertainty sampling usually considers the outputs   of only one model . In contrast , disagreement-   based strategies utilize multiple models and se-   lect the instances that are most disagreed among   them . This is also a widely - adopted algorithm , of   which the famous query - by - committee ( QBC ; Se-   ung et al . , 1992 ) is an example . The disagreement   can be measured by vote entropy ( Engelson and Da-   gan , 1996 ) , KL - divergence ( McCallum and Nigam ,   1998 ) or variation ratio ( Freeman , 1965 ) .   To construct the model committee , one can train   a group of distinct models . Moreover , taking a   Bayesian perspective over the model parameters is   also applicable ( Houlsby et al . , 2011 ) . Especially   with neural models , ( Gal and Ghahramani , 2016 )   show that dropout could approximate inference and   measure model uncertainty . This deep Bayesian   method has been applied to AL for computer vi-   sion ( CV ) tasks ( Gal et al . , 2017 ) as well as various   NLP tasks ( Siddhant and Lipton , 2018 ; Shen et al . ,   2018 ; Shelmanov et al . , 2021 ) .   2.1.3 Gradient   Gradient information can be another signal for   querying , with the motivation to choose the in-   stances that would most strongly impact the model.6167In this strategy , informativeness is usually mea-   sured by the norm of the gradients . Since we do   not know the gold labels for unlabeled instances ,   the loss is usually calculated as the expectation   over all labels . This leads to the strategy of ex-   pected gradient length ( EGL ) , introduced by Set-   tles et al . ( 2007 ) and later applied to sequence label-   ing ( Settles and Craven , 2008 ) and speech recog-   nition ( Huang et al . , 2016 ) . Zhang et al . ( 2017 )   explore a variation for neural networks where only   the gradients of word embeddings are considered   and show its effectiveness for text classification .   2.1.4 Performance Prediction   Predicting performance can be another indicator for   querying . Ideally , the selected instances should be   the ones that most reduce future errors if labeled   and added to the training set . This motivates the   expected error reduction strategy ( Roy and McCal-   lum , 2001 ) , which chooses instances that lead to   the least expected error if added to retrain a model .   This strategy can be computationally costly since   retraining is needed for each candidate .   Recently , methods have been proposed to learn   another model to select instances that lead to the   fewest errors , usually measured on a held - out de-   velopment set . Reinforcement learning and im-   itation learning have been utilized to train such   policy models ( Bachman et al . , 2017 ; Fang et al . ,   2017 ; Liu et al . , 2018a , b ) . This learning - to - select   strategy may have some constraints . First , it re-   quires labeled data ( maybe from another domain )   to train the policy . To mitigate this reliance , Vu   et al . ( 2019 ) use the current task model as an imper-   fect annotator for AL simulations . Moreover , the   learning signals may be unstable for complex tasks ,   as Koshorek et al . ( 2019 ) show for semantic tasks .   A similar and simpler idea is to select the most   erroneous or ambiguous instances with regard to   the current task model , which can also be done   with another performance - prediction model . Yoo   and Kweon ( 2019 ) directly train a smaller model   to predict the instance losses for CV tasks , which   have been also adopted for NLP ( Cai et al . , 2021 ;   Shen et al . , 2021 ) . In a similar spirit , Wang et al .   ( 2017 ) employ a neural model to judge the correct-   ness of the model prediction for SRL and Brantley   et al . ( 2020 ) learn a policy to decide whether expert   querying is required for each state in sequence la-   beling . Inspired by data maps ( Swayamdipta et al . ,   2020 ) , Zhang and Plank ( 2021 ) train a model to   select ambiguous instances whose average correct - ness over the training iterations is close to a pre-   defined threshold . For machine translation ( MT ) ,   special techniques can be utilized to seek erroneous   instances , such as using a backward translator to   check round - trip translations ( Haffari et al . , 2009 ;   Zeng et al . , 2019 ) or quality estimation ( Logacheva   and Specia , 2014a , b ) .   2.2 Representativeness   Only considering the informativeness of individ-   ual instances may have the drawback of sampling   bias ( Dasgupta , 2011 ; Prabhu et al . , 2019 ) and the   selection of outliers ( Roy and McCallum , 2001 ;   Karamcheti et al . , 2021 ) . Therefore , representa-   tiveness , which measures how instances correlate   with each other , is another major factor to consider   when designing AL query strategies .   2.2.1 Density   With the motivation to avoid outliers , density - based   strategies prefer instances that are more represen-   tative of the unlabeled set . Selecting by n - gram   or word counts ( Ambati et al . , 2010a ; Zhao et al . ,   2020b ) can be regarded as a simple way of density   measurement . Generally , the common measure-   ment is an instance ’s average similarity to all other   instances ( McCallum and Nigam , 1998 ; Settles and   Craven , 2008 ) . While it may be costly to calculate   similarities of all instance pairs , considering only   k - nearest neighbor instances has been proposed as   an alternative option ( Zhu et al . , 2008c , 2009 ) .   2.2.2 Discriminative   Another direction is to select instances that are dif-   ferent from already labeled instances . Again , for   NLP tasks , simple feature - based metrics can be uti-   lized for this purpose by preferring instances with   more unseen n - grams or out - of - vocabulary words   ( Eck et al . , 2005 ; Bloodgood and Callison - Burch ,   2010 ; Erdmann et al . , 2019 ) . Generally , similarity   scores can also be utilized to select the instances   that are less similar to the labeled set ( Kim et al . ,   2006 ; Zhang et al . , 2018 ; Zeng et al . , 2019 ) . An-   other interesting idea is to train a model to discrim-   inate the labeled and unlabeled sets . Gissin and   Shalev - Shwartz ( 2019 ) directly train a classifier for   this purpose , while naturally adversarial training   can be also adopted ( Sinha et al . , 2019 ; Deng et al . ,   2018 ) . In domain adaptation scenarios , the same6168motivation leads to the usage of a domain separator   to filter instances ( Rai et al . , 2010 ) .   2.2.3 Batch Diversity   Ideally , only one most useful instance would be   selected in each iteration . However , it is more effi-   cient and practical to adopt batch - mode AL ( Set-   tles , 2009 ) , where each time a batch of instances   is selected . In this case , we need to consider the   dissimilarities not only between selected instances   and labeled ones but also within the selected batch .   To select a batch of diverse instances , there   are two common approaches . 1 ) Iterative selec-   tion collects the batch in an iterative greedy way   ( Brinker , 2003 ; Shen et al . , 2004 ) . In each itera-   tion , an instance is selected by comparing it with   previously chosen instances to avoid redundancy .   Some more advanced diversity - based criteria , like   coreset ( Geifman and El - Yaniv , 2017 ; Sener and   Savarese , 2018 ) and determinantal point processes   ( Shi et al . , 2021 ) , can also be approximated in a sim-   ilar way . 2 ) Clustering - based methods partition   the unlabeled data into clusters and select instances   among them ( Tang et al . , 2002 ; Xu et al . , 2003 ;   Shen et al . , 2004 ; Nguyen and Smeulders , 2004 ;   Zhdanov , 2019 ; Yu et al . , 2022 ) . Since the chosen   instances come from different clusters , diversity   can be achieved to some extent .   For the calculation of similarity , in addition to   comparing the input features or intermediate neu-   ral representations , other methods are also inves-   tigated , such as utilizing model - based similarity   ( Hazra et al . , 2021 ) , gradients ( Ash et al . , 2020 ;   Kim , 2020 ) , and masked LM surprisal embeddings   ( Yuan et al . , 2020 ) .   2.3 Hybrid   There is no surprise that informativeness and repre-   sentativeness can be combined for instance query-   ing , leading to hybrid strategies . A simple combi-   nation can be used to merge multiple criteria into   one . This can be achieved by a weighted sum ( Kim   et al . , 2006 ; Chen et al . , 2011 ) or multiplication   ( Settles and Craven , 2008 ; Zhu et al . , 2008c ) .   There are several strategies to naturally inte-   grate multiple criteria . Examples include ( uncer-   tainty ) weighted clustering ( Zhdanov , 2019 ) , di-   verse gradient selection ( Ash et al . , 2020 ; Kim ,   2020 ) where the gradients themselves contain un-   certainty information ( § 2.1.3 ) and determinantal   point processes ( DPP ) with quality - diversity de-   composition ( Shi et al . , 2021).Moreover , multi - step querying , which applies   multiple criteria in series , is another natural hybrid   method . For example , one can consider first fil-   tering certain highly uncertain instances and then   performing clustering to select a diverse batch from   them ( Xu et al . , 2003 ; Shen et al . , 2004 ; Mirroshan-   del et al . , 2011 ) . An alternative strategy of selecting   the most uncertain instances per cluster has also   been utilized ( Tang et al . , 2002 ) .   Instead of statically merging into one query strat-   egy , dynamic combination may better fit the AL   learning process , since different strategies may ex-   cel at different AL phases . For example , at the   start of AL , uncertainty sampling may be unreliable   due to little labeled data , and representativeness-   based methods could be preferable , whereas in later   stages where we have enough data and target finer-   grained decision boundaries , uncertainty may be   a suitable strategy . DUAL ( Donmez et al . , 2007 )   is such a dynamic strategy that can switch from a   density - based selector to an uncertainty - based one .   Ambati et al . ( 2011b ) further propose GraDUAL ,   which gradually switches strategies within a switch-   ing range . Wu et al . ( 2017 ) adopt a similar idea   with a pre - defined monotonic function to control   the combination weights .   3 Query and Annotation   3.1 AL for Structured Prediction   AL has been widely studied for classification tasks ,   while in NLP , many tasks involve structured pre-   diction . In these tasks , the system needs to output   a structured object consisting of a group of inter-   dependent variables ( Smith , 2011 ) , such as a label   sequence or a parse tree . Special care needs to be   taken when querying and annotating for these more   complex tasks ( Thompson et al . , 1999 ) . One main   decision is whether to annotate full structures for   input instances ( § 3.1.1 ) , or allow the annotation of   only partial structures ( § 3.1.2 ) .   3.1.1 Full - structure AL   First , if we regard the full output structure of an in-   stance as a whole and perform query and annotation   at the full - instance level , then AL for structured pre-   diction tasks is not very different than for simpler   classification tasks . Nevertheless , considering that   the output space is usually exponentially large and   infeasible to explicitly enumerate , querying may   require further inspection .   Some uncertainty sampling strategies , such as6169entropy , need to consider the full output space .   Instead of the infeasible explicit enumeration ,   dynamic - programming algorithms that are simi-   lar to the ones in decoding and inference processes   can be utilized , such as algorithms for tree - entropy   ( Hwa , 2000 , 2004 ) and sequence - entropy ( Mann   and McCallum , 2007 ; Settles and Craven , 2008 ) .   Instead of considering the full output space , top-   kapproximation is a simpler alternative that takes   k - best predicted structures as a proxy . This is also a   frequently utilized method ( Tang et al . , 2002 ; Kim   et al . , 2006 ; Rocha and Sanchez , 2013 ) .   For disagreement - based strategies , the measure-   ment of partial disagreement may be required ,   since full - match can be too strict for structured   objects . Fine - grained evaluation scores can be rea-   sonable choices for this purpose , such as F1 score   for sequence labeling ( Ngai and Yarowsky , 2000 ) .   Since longer instances usually have larger uncer-   tainties and might be preferred , length normaliza-   tionis a commonly - used heuristic to avoid this bias   ( Tang et al . , 2002 ; Hwa , 2000 , 2004 ; Shen et al . ,   2018 ) . Yet , Settles and Craven ( 2008 ) argue that   longer sequences should not be discouraged and   may contain more information .   Instead of directly specifying the full utility of an   instance , aggregation is also often utilized by gath-   ering utilities of its sub - structures , usually along   the factorization of the structured modeling . For ex-   ample , the sequence uncertainty can be obtained by   summing or averaging the uncertainties of all the   tokens ( Settles and Craven , 2008 ) . Other aggrega-   tion methods are also applicable , such as weighted   sum by word frequency ( Ringger et al . , 2007 ) or   using only the most uncertain ( least probable ) one   ( Myers and Palmer , 2021 ; Liu et al . , 2022 ) .   3.1.2 Partial - structure AL   A structured object can be decomposed into smaller   sub - structures with different training utilities . For   example , in a dependency tree , functional relations   are usually easier to judge while prepositional at-   tachment links may be more informative for the   learning purpose . This naturally leads to AL with   partial structures , where querying and annotating   can be performed at the sub - structure level .   Factorizing full structures into the finest-   grained sub - structures and regarding them as the   annotation units could be a natural choice . Typical   examples include individual tokens for sequence   labeling ( Marcheggiani and Artières , 2014 ) , word   boundaries for segmentation ( Neubig et al . , 2011;Li et al . , 2012b ) , syntactic - unit pairs for depen-   dency parsing ( Sassano and Kurohashi , 2010 ) and   mention pairs for coreference ( Gasperin , 2009 ;   Miller et al . , 2012 ; Sachan et al . , 2015 ) . The query-   ing strategy for the sub - structures can be similar   to the classification cases , though inferences are   usually needed to calculate marginal probabilities .   Moreover , if full structures are desired as anno-   tation outputs , semi - supervised techniques such   as self - training ( § 4.2 ) could be utilized to assign   pseudo labels to the unannotated parts ( Tomanek   and Hahn , 2009b ; Majidi and Crane , 2013 ) .   At many times , choosing larger sub - structures   is preferable , since partial annotation still needs   the understanding of larger contexts and frequently   jumping among different contexts may require   more reading time ( § 3.2.1 ) . Moreover , increasing   the sampling granularity may mitigate the missed   class effect , where certain classes may be over-   looked ( Tomanek et al . , 2009 ) . Typical examples   of larger sub - structures include sub - sequences for   sequence labeling ( Shen et al . , 2004 ; Chaudhary   et al . , 2019 ; Radmard et al . , 2021 ) , word - wise   head edges for dependency parsing ( Flannery and   Mori , 2015 ; Li et al . , 2016 ) , neighborhood pools   ( Laws et al . , 2012 ) or mention - wise anaphoric   links ( Li et al . , 2020 ; Espeland et al . , 2020 ) for   coreference , and phrases for MT ( Bloodgood and   Callison - Burch , 2010 ; Miura et al . , 2016 ; Hu and   Neubig , 2021 ) . In addition to increasing granu-   larity , grouping queries can also help to make   annotation easier , such as adopting a two - stage se-   lection of choosing uncertain tokens from uncertain   sentences ( Mirroshandel and Nasr , 2011 ; Flannery   and Mori , 2015 ) and selecting nearby instances in   a row ( Miller et al . , 2012 ) .   For AL with partial structures , output model-   ingis of particular interest since the model needs   to learn from partial annotations . If directly us-   ing local discriminative models where each sub-   structure is decided independently , learning with   partial annotations is straightforward since the an-   notations are already complete to the models ( Neu-   big et al . , 2011 ; Flannery and Mori , 2015 ) . For   more complex models that consider interactions   among output sub - structures , such as global mod-   els , special algorithms are required to learn from   incomplete annotations ( Scheffer et al . , 2001 ; Wan-   varie et al . , 2011 ; Marcheggiani and Artières , 2014 ;   Li et al . , 2016 ) . One advantage of these more com-   plex models is the interaction of the partial labels6170and the remaining parts . For example , considering   theoutput constraints for structured prediction   tasks , combining the annotated parts and the con-   straints may reduce the output space of other parts   and thus lower their uncertainties , leading to better   queries ( Roth and Small , 2006 ; Sassano and Kuro-   hashi , 2010 ; Mirroshandel and Nasr , 2011 ) . More   generally , the annotation of one label can interme-   diately influence others with cheap re - inference ,   which can help batch - mode selection ( Marcheg-   giani and Artières , 2014 ) and interactive correction   ( Culotta and McCallum , 2005 ) .   In addition to classical structured - prediction   tasks , classification tasks can also be cast as struc-   tured predictions with partial labeling . Partial   feedback is an example that is adopted to make   the annotating of classification tasks simpler , espe-   cially when there are a large number of target labels .   For example , annotators may find it much easier to   answer yes / no questions ( Hu et al . , 2019 ) or rule   out negative classes ( Lippincott and Van Durme ,   2021 ) than to identify the correct one .   3.2 Annotation Cost   AL mainly aims to reduce real annotation cost and   we discuss several important topics on this point .   3.2.1 Cost Measurement   Most AL works adopt simple measurements of unit   cost , that is , assuming that annotating each instance   requires the same cost . Nevertheless , the annota-   tion efforts for different instances may vary ( Settles   et al . , 2008 ) . For example , longer sentences may   cost more to annotate than shorter ones . Because   of this , many works assume unit costs to tokens   instead of sequences , which may still be inaccurate .   Especially , AL tends to select difficult and ambigu-   ous instances , which may require more annotation   efforts ( Hachey et al . , 2005 ; Lynn et al . , 2012 ) . It   is important to properly measure annotation cost   since the measurement directly affects the evalua-   tion of AL algorithms . The comparisons of query   strategies may vary if adopting different cost mea-   surement ( Haertel et al . , 2008a ; Bloodgood and   Callison - Burch , 2010 ; Chen et al . , 2015 ) .   Probably the best cost measurement is the actual   annotation time ( Baldridge and Palmer , 2009 ) .   Especially , when the cost comparisons are not that   straightforward , such as comparing annotating data   against writing rules ( Ngai and Yarowsky , 2000 )   or partial against full annotations ( § 3.1 ; Flannery   and Mori , 2015 ; Li et al . , 2016 , 2020 ) , time - basedevaluation is an ideal choice . This requires actual   annotating exercises rather than simulations .   Since cost measurement can also be used for   querying ( § 3.2.2 ) , it would be helpful to be able to   predict the real cost before annotating . This can   be cast as a regression problem , for which several   works learn a linear cost model based on input   features ( Settles et al . , 2008 ; Ringger et al . , 2008 ;   Haertel et al . , 2008a ; Arora et al . , 2009 ) .   3.2.2 Cost - sensitive Querying   Given the goal of reducing actual cost , the query-   ing strategies should also take it into considera-   tion . That is , we want to select not only high-   utility instances but also low - cost ones . A natu-   ral cost - sensitive querying strategy is return - on-   investment ( ROI ; Haertel et al . , 2008b ; Settles   et al . , 2008 ; Donmez and Carbonell , 2008 ) . In   this strategy , instances with higher net benefit per   unit cost are preferred , which is equivalent to divid-   ing the original querying utility by cost measure .   Tomanek and Hahn ( 2010 ) evaluate the effective-   ness of ROI together with two other strategies , in-   cluding constraining maximal cost budget per in-   stance and weighted rank combination . Haertel   et al . ( 2015 ) provide further analytic and empirical   evaluation , showing that ROI can reduce total cost .   In real AL scenarios , things can be much more   complex . For example , there can be multiple an-   notators with different expertise ( Baldridge and   Palmer , 2009 ; Huang et al . , 2017 ; Cai et al . , 2020 ) ,   and the annotators may refuse to answer or make   mistakes ( Donmez and Carbonell , 2008 ) . Being   aware of these scenarios , Donmez and Carbonell   ( 2008 ) propose proactive learning to jointly select   the optimal oracle and instance . Li et al . ( 2017 )   further extend proactive learning to NER tasks .   3.2.3 Directly Reducing Cost   In addition to better query strategies , there are other   ways of directly reducing annotation cost , such as   computer - assisted annotation . In AL , models and   annotators usually interact in an indirect way where   models only query the instances to present to the   annotators , while there could be closer interactions .   Pre - annotation is such an idea , where not only   the raw data instances but also the model ’s best   or top- kpredictions are sent to the annotators to   help them make decisions . If the model ’s pre-   dictions are reasonable , the annotators can sim-   ply select or make a few corrections to obtain the   gold annotations rather than creating from scratch.6171This method has been shown effective when com-   bined with AL ( Baldridge and Osborne , 2004 ; Vla-   chos , 2006 ; Ringger et al . , 2008 ; Skeppstedt , 2013 ;   Cañizares - Díaz et al . , 2021 ) . Post - editing for MT   is also a typical example ( Dara et al . , 2014 ) .   Moreover , the models could provide help at real   annotating time . For example , Culotta and Mc-   Callum ( 2005 ) present an interactive AL system   where the user ’s corrections can propagate to the   model , which generates new predictions for the   user to further refine . Interactive machine transla-   tion ( IMT ) adopts a similar idea , where the anno-   tator corrects the first erroneous character , based   on which the model reproduces the prediction . AL   has also been combined with IMT to further reduce   manual efforts ( González - Rubio et al . , 2012 ; Peris   and Casacuberta , 2018 ; Gupta et al . , 2021 ) .   3.2.4 Wait Time   In AL iterations , the annotators may need to wait   for the training and querying steps ( Line 3 and 4 in   Algorithm 1 ) . This wait time may bring some hid-   den costs , thus more efficient querying and training   would be preferable for faster turnarounds .   To speed up querying , sub - sampling is a simple   method to deal with large unlabeled pools ( Roy   and McCallum , 2001 ; Ertekin et al . , 2007 ; Tsvigun   et al . , 2022 ) . For some querying strategies , pre-   calculating and caching unchanging information   can also help to speed up ( Ashrafi Asli et al . , 2020 ;   Citovsky et al . , 2021 ) . In addition , approximation   withk - nearest neighbours can also be utilized to   calculate density ( Zhu et al . , 2009 ) or search for   instances after adversarial attacks ( Ru et al . , 2020 ) .   To reduce training time , a seemingly reason-   able strategy is to apply incremental training across   AL iterations , that is , continuing training previous   models on the new instances . However , Ash and   Adams ( 2020 ) show that this type of warm - start   may lead to sub - optimal performance for neural   models and many recent AL works usually train   models from scratch ( Hu et al . , 2019 ; Ein - Dor et al . ,   2020 ) . Another method is to use an efficient model   for querying and a more powerful model for final   training . However , this might lead to sub - optimal   results , which will be discussed in § 4.1 .   Another idea to reduce wait time is to simply   allow querying with stale information . Actually ,   batch - mode AL ( § 2.2.3 ) is such an example where   instances in the same batch are queried with the   same model . Haertel et al . ( 2010 ) propose parallel   AL , which maintains separate loops of annotating , training , and scoring , and allows dynamic and pa-   rameterless instance selection at any time .   4 Model and Learning   4.1 Model Mismatch   While it is natural to adopt the same best-   performing model throughout the AL process , there   are cases where the query and final ( successor )   models can mismatch ( Lewis and Catlett , 1994 ) .   Firstly , more efficient models are preferable for   querying to reduce wait time ( § 3.2.4 ) . Moreover ,   since data usually outlive models , re - using AL-   base data to train another model would be desired   ( Baldridge and Osborne , 2004 ; Tomanek et al . ,   2007 ) . Several works show that model mismatch   may make the gains from AL be negligible or even   negative ( Baldridge and Osborne , 2004 ; Lowell   et al . , 2019 ; Shelmanov et al . , 2021 ) , which raises   concerns about the utilization of AL in practice .   For efficiency purposes , distillation can be uti-   lized to improve querying efficiency while keep-   ing reasonable AL performance . Shelmanov et al .   ( 2021 ) show that using a smaller distilled version   of a pre - trained model for querying does not lead   to too much performance drop . Tsvigun et al .   ( 2022 ) combine this idea with pseudo - labeling and   sub - sampling to further reduce computational cost .   Similarly , Nguyen et al . ( 2022 ) keep a smaller   proxy model for query and synchronize the proxy   with the main model by distillation .   4.2 Learning   AL can be combined with other advanced learning   techniques to further reduce required annotations .   Semi - supervised learning . Since AL usually as-   sumes an unlabeled pool , semi - supervised learning   can be a natural fit . Combining these two is not   a new idea : ( McCallum and Nigam , 1998 ) adopt   the EM algorithm to estimate the outputs of un-   labeled data and utilize them for learning . This   type of self - training or pseudo - labeling technique   is often utilized in AL ( Tomanek and Hahn , 2009b ;   Majidi and Crane , 2013 ; Yu et al . , 2022 ) . With a   similar motivation , ( Dasgupta and Ng , 2009 ) use   an unsupervised algorithm to identify the unam-   biguous instances to train an active learner . For   the task of word alignment , which can be learned   in an unsupervised manner , incorporating supervi-   sion with AL can bring further improvements in a   data - efficient way ( Ambati et al . , 2010b , c).6172Transfer learning . AL can be easily combined   with transfer learning , another technique to reduce   required annotations . Utilizing pre - trained models   is already a good example ( Ein - Dor et al . , 2020 ;   Yuan et al . , 2020 ; Tamkin et al . , 2022 ) and con-   tinual training ( Gururangan et al . , 2020 ) can also   be applied ( Hua and Wang , 2022 ; Margatina et al . ,   2022 ) . Moreover , transductive learning is com-   monly combined with AL by transferring learn-   ing signals from different domains ( Chan and Ng ,   2007 ; Shi et al . , 2008 ; Rai et al . , 2010 ; Saha et al . ,   2011 ; Wu et al . , 2017 ; Kasai et al . , 2019 ; Yuan   et al . , 2022 ) or languages ( Qian et al . , 2014 ; Fang   and Cohn , 2017 ; Fang et al . , 2017 ; Chaudhary et al . ,   2019 , 2021 ; Moniz et al . , 2022 ) . In addition to the   task model , the model - based query policy ( § 2.1.4 )   is also often obtained with transfer learning .   Weak supervision . AL can also be combined   with weakly supervised learning . Examples include   learning from inputs and execution results for se-   mantic parsing ( Ni et al . , 2020 ) , labeling based on   identical structure vectors for entity representations   ( Qian et al . , 2020 ) , learning from gazetteers and   dictionaries for sequence labeling ( Brantley et al . ,   2020 ) and interactively discovering labeling rules   ( Zhang et al . , 2022a ) .   Data augmentation . Augmentation is also appli-   cable in AL and has been explored with iterative   back - translation ( Zhao et al . , 2020b ) , mixup for   sequence labeling ( Zhang et al . , 2020 ) and phrase-   to - sentence augmentation for MT ( Hu and Neubig ,   2021 ) . As discussed in § 2.1.1 , augmentation can   also be helpful for instance querying ( Jiang et al . ,   2020 ; Zhang et al . , 2022b ) . Another interesting   scenario involving augmentation and AL is query   synthesis , which directly generates instances to be   annotated instead of selecting existing unlabeled   ones . Though synthesizing texts is still a hard prob-   lem generally , there have been successful applica-   tions for simple classification tasks ( Schumann and   Rehbein , 2019 ; Quteineh et al . , 2020 ) .   5 Starting and Stopping AL   5.1 Starting AL   While there are cases where there are already   enough labeled data to train a reasonable model   and AL is utilized to provide further improvements   ( Bloodgood and Callison - Burch , 2010 ; Geifman   and El - Yaniv , 2017 ) , at many times we are facing   the cold - start problem , where instances need to beselected without a reasonable model . Especially ,   how to select the seed data to start the AL process   is an interesting question , which may greatly influ-   ence the performance in initial AL stages ( Tomanek   et al . , 2009 ; Horbach and Palmer , 2016 ) .   Random sampling is probably the most com-   monly utilized strategy , which is reasonable since   it preserves the original data distribution . Some   representativeness - based querying strategies ( § 2.2 )   can also be utilized , for example , selecting points   near the clustering centroids is a way to obtain rep-   resentative and diverse seeds ( Kang et al . , 2004 ;   Zhu et al . , 2008c ; Hu et al . , 2010 ) . Moreover , some   advanced learning techniques ( § 4.2 ) can also be   helpful here , such as transfer learning ( Wu et al . ,   2017 ) and unsupervised methods ( Vlachos , 2006 ;   Dasgupta and Ng , 2009 ) . In addition , language   model can be a useful tool , with which Dligach and   Palmer ( 2011 ) select low - probability words in the   context of word sense disambiguation and Yuan   et al . ( 2020 ) choose cluster centers with surprisal   embeddings by pre - trained contextualized LMs .   5.2 Stopping AL   When adopting AL in practice , it would be desir-   able to know the time to stop AL when the model   performance is already near the upper limits , before   running out of all the budgets . For this purpose ,   a stopping criterion is needed , which checks cer-   tain metrics satisfying certain conditions . There   can be simple heuristics . For example , AL can   be stopped when all unlabeled instances are no   closer than any of the support vectors with an SVM   ( Schohn and Cohn , 2000 ; Ertekin et al . , 2007 ) or   no new n - grams remain in the unlabeled set for MT   ( Bloodgood and Callison - Burch , 2010 ) . Neverthe-   less , these are specific to the underlying models or   target tasks . For the design of a general stopping   criterion , there are three main aspects to consider :   metric , dataset andcondition .   For the metric , measuring performance on a de-   velopment set seems a natural option . However ,   the results would be unstable if this set is too small   and it would be impractical to assume a large de-   velopment set . Cross - validation on the training set   also has problems since the labeled data by AL is   usually biased . In this case , metrics from the query   strategies can be utilized . Examples include un-   certainty or confidence ( Zhu and Hovy , 2007 ; Vla-   chos , 2008 ) , disagreement ( Tomanek et al . , 2007 ;   Tomanek and Hahn , 2008 ; Olsson and Tomanek,61732009 ) , estimated performance ( Laws and Schütze ,   2008 ) , expected error ( Zhu et al . , 2008a ) , confi-   dence variation ( Ghayoomi , 2010 ) , as well as ac-   tual performance on the selected instances ( Zhu   and Hovy , 2007 ) . Moreover , comparing the predic-   tions between consecutive AL iterations is another   reasonable option ( Zhu et al . , 2008b ; Bloodgood   and Vijay - Shanker , 2009a ) .   Thedataset to calculate the stopping metric re-   quires careful choosing . The results could be un-   stable if not adopting a proper set ( Tomanek and   Hahn , 2008 ) . Many works suggest that a separate   unlabeled dataset should be utilized ( Tomanek and   Hahn , 2008 ; Vlachos , 2008 ; Bloodgood and Vijay-   Shanker , 2009a ; Beatty et al . , 2019 ; Kurlandski and   Bloodgood , 2022 ) . Since the stopping metrics usu-   ally do not rely on gold labels , this dataset could   potentially be very large to provide more stable   results , though wait time would be another factor   to consider in this case ( § 3.2.4 ) .   The condition to stop AL is usually compar-   ing the metrics to a pre - defined threshold . Ear-   lier works only look at the metric at the current   iteration , for example , stopping if the uncertainty   or the error is less than the threshold ( Zhu and   Hovy , 2007 ) . In this case , the threshold is hard   to specify since it relies on the model and the   task . ( Zhu et al . , 2008b ) cascade multiple stopping   criteria to mitigate this reliance . A more stable   option is to track the change of the metrics over   several AL iterations , such as stopping when the   confidence consistently drops ( Vlachos , 2008 ) , the   changing rate flattens ( Laws and Schütze , 2008 ) or   the predictions stabilize across iterations ( Blood-   good and Vijay - Shanker , 2009a ; Bloodgood and   Grothendieck , 2013 ) .   Pullar - Strecker et al . ( 2021 ) provide an empiri-   cal comparison over common stopping criteria and   would be a nice reference . Moreover , stopping AL   can be closely related to performance prediction   and early stopping . Especially , the latter can be of   particular interest to AL since learning in early AL   stages need to face the low - resource problem and   how to perform early stopping may also require   careful considerations .   6 Related Topics and Future Directions   6.1 Related Topics   There are many related topics that could be ex-   plored together with AL . Other data - efficient learn-   ing methods such as semi - supervised and transferlearning are naturally compatible with AL ( § 4.2 ) .   Curriculum learning ( Bengio et al . , 2009 ) , which   arranges training instances in a meaningful order ,   may also be integrated with AL ( Platanios et al . ,   2019 ; Zhao et al . , 2020a ; Jafarpour et al . , 2021 ) .   Uncertainty ( Gawlikowski et al . , 2021 ) , outlier de-   tection ( Hodge and Austin , 2004 ) and performance   prediction ( Xia et al . , 2020 ) can be related to in-   stance querying . Crowdsourcing can be adopted   to further reduce annotation cost ( § B ) . Model ef-   ficiency ( Menghani , 2021 ) would be crucial to re-   duce wait time ( § 3.2.4 ) . AL is a typical type of   human - in - the - loop framework ( Wang et al . , 2021 ) ,   and it will be interesting to explore more human-   computer interaction techniques in AL .   6.2 Future Directions   Complex tasks . AL is mostly adopted for simple   classification , while there are many more complex   tasks in NLP . For example , except for MT , genera-   tion tasks have been much less thoroughly explored   with AL . Tasks with more complex inputs such as   NLI and QA also require extra care when using   AL ; obtaining unlabeled data is already non - trivial .   Nevertheless , preliminary work has shown that AL   can be helpful for data collection for such tasks   ( Mussmann et al . , 2020 ) .   Beyond direct target labeling . In addition to   directly annotating target labels , AL can also be uti-   lized in other ways to help the target task , such as   labeling features or rationales ( Melville and Sind-   hwani , 2009 ; Druck et al . , 2009 ; Sharma et al . ,   2015 ) , annotating explanations ( Liang et al . , 2020 ) ,   evaluation ( Mohankumar and Khapra , 2022 ) and   rule discovery ( Zhang et al . , 2022a ) .   AL in practice . Most AL works simulate anno-   tations on an existing labeled dataset . Though this   method is convenient for algorithm development , it   ignores several challenges of applying AL in prac-   tice . As discussed in this survey , real annotation   cost ( § 3.2.1 ) , efficiency and wait time ( § 3.2.4 ) , data   reuse ( § 4.1 ) and starting and stopping ( § 5 ) are all   important practical aspects which may not emerge   in simulation . Moreover , since the AL process usu-   ally can not be repeated multiple times , how to se-   lect the query strategy and other hyper - parameters   remains a great challenge . It will be critical to ad-   dress these issues to bring AL into practical use   ( Rehbein et al . , 2010 ; Attenberg and Provost , 2011 ;   Settles , 2011 ; Lowell et al . , 2019 ) and make it more   widely utilized ( Tomanek and Olsson , 2009).6174Limitations   There are several limitations of this work . First , we   mainly focus on AL works in the context of NLP ,   while AL works in other fields may also present   ideas that could be utilized for NLP tasks . For ex-   ample , many querying strategies originally devel-   oped with CV tasks could be naturally adopted to   applications in NLP ( Ren et al . , 2021 ) . We encour-   age the readers to refer to other surveys mentioned   in § 1 for additional related AL works . Moreover ,   the descriptions in this survey are mostly brief in   order to provide a more comprehensive coverage   within page limits . We mainly present the works   in meaningful structured groups rather than plainly   describing them in unstructured sequences , and we   hope that this work can serve as an index where   more details can be found in corresponding works .   Finally , this is a pure survey without any exper-   iments or empirical results . It would be helpful   to perform comparative experiments over different   AL strategies , which could provide more meaning-   ful guidance ( Zhan et al . , 2022 ) . We leave this to   future work .   References61756176617761786179618061816182618361846185618661876188A Tasks   In this section , we list representative works for   different NLP tasks . According to the output struc-   tures , the tasks are further categorized into four   groups : classification , sequence labeling , complex   structured prediction , and generation .   Classification denotes the tasks whose output   consists of only one variable . Text classification   that assigns a target label to an input text sequence   is a typical example . Pairwise classification and   word - level classification are also commonly seen   in NLP .   •Text classification : Please refer to the paper   table mentioned in ( § C ) for related works . We   do not list them here since there are too many .   •Pairwise classification : ( Grießhaber et al . , 2020 ;   Bai et al . , 2020 ; Mussmann et al . , 2020 )   •Word sense disambiguation ( WSD ): ( Fujii   et al . , 1998 ; Chen et al . , 2006 ; Chan and Ng ,   2007 ; Zhu and Hovy , 2007 ; Zhu et al . , 2008c ;   Imamura et al . , 2009 ; Martínez Alonso et al . ,   2015 )   Sequence labeling is probably the most com-   monly seen structured prediction task in NLP . It   aims to predict a sequence of labels , among which   there may be interactions and constraints .   •Part - of - speech ( POS ): ( Engelson and Dagan ,   1996 ; Ringger et al . , 2007 ; Haertel et al . , 2008a ;   Marcheggiani and Artières , 2014 ; Fang and   Cohn , 2017 ; Brantley et al . , 2020 ; Chaudhary   et al . , 2021 )   •(Named ) entity recognition ( NER / ER ): ( Shen   et al . , 2004 ; Culotta and McCallum , 2005 ; Kim   et al . , 2006 ; Settles and Craven , 2008 ; Tomanek   and Hahn , 2009b ; Marcheggiani and Artières ,   2014 ; Chen et al . , 2015 ; Li et al . , 2017 ; Shen   et al . , 2018 ; Siddhant and Lipton , 2018 ; Erdmann   et al . , 2019 ; Chaudhary et al . , 2019 ; Brantley   et al . , 2020 ; Hazra et al . , 2021 ; Shelmanov et al . ,   2021 ; Radmard et al . , 2021 )   •Segmentation : ( Ngai and Yarowsky , 2000 ; Sas-   sano , 2002 ; Neubig et al . , 2011 ; Li et al . , 2012b ;   Marcheggiani and Artières , 2014 ; Cai et al . ,   2021 )   •Natural language understanding ( NLU ): ( Ha-   dian and Sameti , 2014 ; Deng et al . , 2018 ; Peshter-   liev et al . , 2019 ; Zhu et al . , 2020)Complex structure prediction in this work de-   notes the structure prediction tasks that are more   complex than sequence labeling , and have explicit   connections ( alignments ) between inputs and out-   puts . They usually aim to extract relational struc-   tures among input elements .   •Parsing : ( Hwa , 2000 ; Tang et al . , 2002 ;   Baldridge and Osborne , 2003 , 2004 ; Hwa , 2004 ;   Reichart and Rappoport , 2009 ; Sassano and   Kurohashi , 2010 ; Atserias et al . , 2010 ; Mir-   roshandel and Nasr , 2011 ; Majidi and Crane ,   2013 ; Flannery and Mori , 2015 ; Li et al . , 2016 ;   Shi et al . , 2021 )   •Semantic role labeling ( SRL ): ( Roth and Small ,   2006 ; Wang et al . , 2017 ; Ikhwantri et al . , 2018 ;   Siddhant and Lipton , 2018 ; Koshorek et al . , 2019 ;   Myers and Palmer , 2021 )   •Coreference : ( Gasperin , 2009 ; Miller et al . ,   2012 ; Laws et al . , 2012 ; Zhao and Ng , 2014 ;   Sachan et al . , 2015 ; Li et al . , 2020 ; Espeland   et al . , 2020 ; Yuan et al . , 2022 )   •Relation - related : ( Roth and Small , 2008 ; Blood-   good and Vijay - Shanker , 2009b ; Mirroshandel   et al . , 2011 ; Fu and Grishman , 2013 ; Cañizares-   Díaz et al . , 2021 ; Mallart et al . , 2021 ; Seo et al . ,   2022 ; Zhang et al . , 2022a )   •Event - related : ( Cao et al . , 2015 ; Shen et al . ,   2021 ; Lee et al . , 2022 )   •Word alignment : ( Ambati et al . , 2010b , c ;   Rocha and Sanchez , 2013 )   •Entity alignment / resolution : ( Kasai et al . ,   2019 ; Liu et al . , 2021 )   Generation refers to the tasks that aim to gen-   erate a sequence of tokens . We differentiate them   from plain structured prediction tasks since there   are usually no explicit alignments between input   and output sub - parts in the supervision and such   alignments are usually implicitly modeled , espe-   cially in recent sequence - to - sequence neural mod-   els . MT is a typical generation task , where we   further separate traditional statistical machine trans-   lation ( SMT ) and recent neural machine translation   ( NMT ) . We also include semantic parsing here ,   since recent works usually cast it as a sequence - to-   sequence generation task .   •SMT : ( Eck et al . , 2005 ; Haffari et al . , 2009 ; Haf-   fari and Sarkar , 2009 ; Ananthakrishnan et al . ,   2010b ; Bloodgood and Callison - Burch , 2010 ;   Ambati et al . , 2010a ; Ananthakrishnan et al . ,   2010a ; González - Rubio et al . , 2012 ; Rocha and6189Sanchez , 2013 ; Logacheva and Specia , 2014a , b ;   Miura et al . , 2016 )   •NMT : ( Peris and Casacuberta , 2018 ; Liu et al . ,   2018b ; Zhang et al . , 2018 ; Zeng et al . , 2019 ;   Zhao et al . , 2020b ; Hu and Neubig , 2021 ; Gupta   et al . , 2021 ; Zhou and Waibel , 2021 ; Hazra et al . ,   2021 ; Mendonça et al . , 2022 )   •Semantic parsing : ( Duong et al . , 2018 ; Ni et al . ,   2020 ; Sen and Yilmaz , 2020 )   •Others : ( Mairesse et al . , 2010 ; Deng et al . , 2018 )   B Other Aspects   We describe some other aspects that are frequently   seen when applying AL to NLP .   Crowdsourcing and Noise . Crowdsourcing is   another way to reduce annotation costs by includ-   ing non - expert annotations ( Snow et al . , 2008 ) . Nat-   urally , AL and crowdsourcing may also be com-   bined with the hope to further reduce cost ( Ambati   et al . , 2010a ; Laws et al . , 2011 ; Yan et al . , 2011 ;   Fang et al . , 2014 ; Zhao et al . , 2020c ) . One spe-   cific factor to consider in this case is the noises   in the crowdsourced data , since noisy data may   have a negative impact on the effectiveness of AL   ( Rehbein and Ruppenhofer , 2011 ) . Cost - sensitive   querying strategies ( § 3.2.2 ) can be utilized to select   both annotators and instances by estimating label-   ers ’ reliability ( Yan et al . , 2011 ; Fang et al . , 2014 ) .   Requiring multiple annotations per instance and   then consolidating is also applicable ( Laws et al . ,   2011 ) . Lin et al . ( 2019 ) provide a framework that   enables automatic crowd consolidation for AL on   the tasks of sequence labeling .   Multiple Targets . In many cases , we may want   to consider multiple targets rather than only one ,   for example , annotating instances in multiple do-   mains ( Xiao and Guo , 2013 ; He et al . , 2021 ; Long-   pre et al . , 2022 ) or multiple languages ( Haffari   and Sarkar , 2009 ; Qian et al . , 2014 ; Moniz et al . ,   2022 ) . Moreover , there may be multiple target   tasks , where multi - task learning ( MTL ) can inter-   act with AL ( Reichart et al . , 2008 ; Ambati et al . ,   2011a ; Rocha and Sanchez , 2013 ; Ikhwantri et al . ,   2018 ; Zhu et al . , 2020 ; Rotman and Reichart , 2022 ) .   In these scenarios with multiple targets , naturally ,   strategies that consider all the targets are usually   more preferable . Reichart et al . ( 2008 ) show that a   query strategy that considers all target tasks obtains   the overall best performance for MTL . Moniz et al .   ( 2022 ) suggest that joint learning across multiplelanguages using a single model outperforms other   strategies such as equally dividing budgets or allo-   cating only for a high - resource language and then   performing the transfer .   Data Imbalance . Imbalance is a frequently oc-   curring phenomenon in NLP and AL can have in-   teresting interactions with it . On the one hand ,   as in plain learning scenarios , AL should take   data imbalance into considerations , with modifica-   tions to the model ( Bloodgood and Vijay - Shanker ,   2009b ) , learning algorithm ( Zhu and Hovy , 2007 )   and query strategies ( Tomanek et al . , 2009 ; Escud-   eiro and Jorge , 2010 ; Li et al . , 2012a ) . On the other   hand , AL can be utilized to address the data imbal-   ance problem and build better data ( Ertekin et al . ,   2007 ; Tomanek and Hahn , 2009a ; Attenberg and   Ertekin , 2013 ; Mottaghi et al . , 2020 ; Mussmann   et al . , 2020 ) .   C Surveying Process   In this section , we provide more details of our sur-   veying process :   •For the ACL Anthology , we search for pa-   pers with the keyword “ active ” in titles ( by   grepping the “ Full Anthology BibTeX file ” ) .   There can be related papers that are missed   from this simple keyword search , but as we   read along the filtered list , we gradually in-   clude the notable missing ones .   •We also include papers outside the ACL An-   thology . First , we look for papers by search-   ing with the key phrase “ active learning ” on   Arxiv ( in the field of cs . CL , excluding those   already appearing in ACL Anthology ) . More-   over , we also collect related works in other   venues , such as AI / ML conferences and jour-   nals . For these venues , we do ( can ) not per-   form extensive searches due to high volume   ( and that many are unrelated to our focus on   NLP ) . We mainly collect related papers in   these adjacent venues by following the refer-   ences from the papers already surveyed .   We also create a table for the related   papers ( with detailed categorizations ) ,   which can be found at this link : https :   //github.com / zzsfornlp / zmsp / blob/   main / msp2 / docs / al4nlp / readme.md .6190