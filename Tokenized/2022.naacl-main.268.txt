  Chenxuan Cui Katherine J. Zhang   LanguageTechnologiesInstitute , CarnegieMellonUniversity   cxcui@cs.cmu.edu , kjzhang@cmu.edu , dmortens@cs.cmu.eduDavid R. Mortensen   Abstract   Coordinate compounds ( CCs ) and elaborate   expressions(EEs)arecoordinateconstructions   common in languages of East and Southeast   Asia . Mortensen ( 2006)claimsthat(1)thelin-   earorderingofEEsandCCsinHmong , Lahu ,   andChinesecanbepredictedviaphonological   hierarchies and ( 2 ) these phonological hierar-   chies lack a clear phonetic rationale . These   claims are significant because morphosyntax   has often been seen as in a feed - forward re-   lationship with phonology , and phonological   generalizations have often been assumed to   be phonetically “ natural ” . We investigate   whether the ordering of CCs and EEs can   be learned empirically and whether computa-   tional models ( classifiers and sequence label-   ingmodels)learnunnaturalhierarchiessimilar   tothosepositedby Mortensen ( 2006 ) . Wefind   that decision trees and SVMs learn to predict   the order of CCs / EEs on the basis of phonol-   ogy , with DTs learning hierarchies strikingly   similartothoseproposedbyMortensen . How-   ever , we also find that a neural sequence la-   beling model is able to learn the ordering of   elaborate expressions in Hmong very effec-   tivelywithoutusinganyphonologicalinforma-   tion . WearguethatEEorderingcanbelearned   through two independent routes : phonology   andlexicaldistribution , presentingamorenu-   ancedpicturethanpreviouswork . [ ISO639 - 3 :   hmn , lhu , cmn ]   1 Introduction   In many languages of East and Southeast Asia ,   there are common constructions in which two   wordsorphrasesarecoordinatedwithoutanovert   marker like a conjunction ( Hanna,2013;Filbeck ,   1996;Johns and Strecker , 1987;Wheatley,1982;   Matisoff,1973;PanandCao , 1972;Watson,1966;   Banker,1964 ) . In coordinate compounds ( CCs ) ,   twowordsarecombinedtoformacompoundwordwhosesemanticsareoftenageneralizationofthose   ofthetwoconjoinedwords . Elaborateexpressions   ( EEs ) are similar , except that they can consist of   two phrases ( rather than words ) and include a re-   peatedword . Takethefollowingexamples :   ( 1)Chinesecoordinatecompounds(CCs )   父母 fùmǔfather - mother ‘ parents ’   花木 huāmùflower - tree ‘ vegetation ’   天地 tiāndìheaven - earth ‘ universe ’   国家 guójiācountry - home‘nation ’   风水 fēngshuǐwind - water ‘ geomancy ’   ( 2)Lahuelaborateexpressions(EEs )   a.ɔ̂   fourcē   cornerɔ̂   fourphɔ̂   side   ‘ ateverycorner ’   b.chɔ   peoplephôʔ   pilechɔ   peopledì   lump   ‘ athrongofpeople ’   c.câ   eatcûʔ   scarcedɔ̀   drinkcûʔ   scarce   ‘ havenothingtoeatordrink ’   Coordinating compounds are found throughout   theworld , withvaryingsemanticrelationshipsbe-   tweenthewholeandtheparts ( Obermüller , 2015 ) .   Elaborate expressions are most common in main-   landSoutheastAsia , wheretheyoccupyaposition   of great prominence . They are often associated   with elevated styles of discourse , but they occur   inallgenresandregisters .   Earlier investigators have claimed the order of   theconstituentwordsinCCsandEEsinsomelan-   guages is predictable by rule . Many of the pro-   posedorderinghierarchiesarebasedonphonology   ( Ting,1975;Dai,1986;Mortensen , 2006 ) . Build-   ingonthisearlierwork , Mortensen ( 2006)posited   that Lahu EE orders could be predicted based on   vowelquality — likeJingpho ( Dai,1986)—andthat   Hmong EE orders could be predicted based on   tone , echoing earlier claims for Chinese and Qe-   Nao ( Ting,1975;Pan and Cao , 1972 ) . These3656tone and vowel scales were , however , not easy   to rationalize in phonetic terms and were used by   Mortensentoargueforaphonologyinwhichstruc-   ture reigns supreme and in which phonetic sub-   stanceplaysonlyanepiphenomenalrole .   Theseclaimshavebeenviewedwithskepticism   for two reasons : morphosyntax has been widely   seenasprovidingtheinputstophonology , notbe-   ing driven by phonology ( Chomsky,1965); and   phonology , since Jakobsonetal . ( 1951)andChom-   sky and Halle ( 1968 ) has usually been seen as   grounded in phonetic categories . Some investi-   gators have claimed that sound patterns that are   not phonetically natural are inherently unlearn-   able . They can exist only as linguistic fossils de-   positedbyahistoryoflanguagechange . Inthispa-   per , weundertaketoinvestigatewhatkindofdata   is needed for ( computational ) learners to acquire   thesepatterns . Wereportthefollowingfindings :   •Even rather simple classifiers like decision   trees can learn to predict the order of EEs in   isolation in over 96 % of cases ( Hmong ) and   79%ofcases(Lahu)usingonlyphonological   information .   •ThedecisiontreesforHmong , Lahu , andChi-   nesemirrorthephonologicalhierarchiespro-   posed for these languages , suggesting that   these hierarchies are empirically robust and   learnablefromtheavailableevidence .   •However , correct and incorrect orderings of   Hmong EEs can be effectively distinguished   in context by a neural sequence labeling   model withoutanyphonologicalinformation ,   suggestingthatlearnerswouldnothavetoac-   quirethephonologicalgeneralizationdirectly   inordertoproducewell - formedEEs .   2 Theoretical Significance   Theexperimentsreportedinthispaperhaveabear-   ingontwoassumptionswidelyheldinphonologi-   caltheory :   1.Truephonologicalgeneralizationsarealways   grounded in phonetic realities ( phonology is   natural )   2.Phonology operates on the outputs of syntax   andmorphology(grammarisserial )   Bothoftheseassumptionshavebeencontested . If   the analysis of EE and CC ordering in Mortensen   ( 2006 ) is sound , neither of these assumptions can   beentirelycorrect.2.1 Phonological patterns and phonetic   substance   Starting even before Prague School phonology , it   was widely assumed that the grammatical cate-   goriesandpatternsmakingreferencetosoundare   coherent in terms of physical ( articulatory , acous-   tic , and psychophysical ) dimensions . The most   common sound patterns found in the world ’s lan-   guages can usually be explained in these terms .   Forexample , inmanylanguagesincludingEnglish ,   if a nasal is followed by a stop , the place of artic-   ulation of the nasal assimilates to that of the stop   ( i[m]possiblevs.i[n]tolerantvs.i[ŋ]glorious ) .   Forphonologicallydistinctivefeatures , suchas   thoserelatedtoplaceofarticulation , thiswascodi-   fiedbyJakobsonetal . ( 1951)andinjectedintogen-   erative phonology by Chomsky and Halle ( 1968 ) .   Even more radical statements about the relation-   ship between phonological form and substance   havebeenmadesincethen ( DoneganandStampe ,   1979;Flemming,2013;Hayes,2011;Doneganand   Stampe,2009;Steriade et al . , 2001 ) . While there   hasneverbeenacompleteconsensusonthematter   ( Fudge,1967;Hyman,1970;HaleandReiss , 2000 ,   2008 ) , it has been widely assumed that phonolog-   ical patterns that are phonetically incoherent can-   not be learned by humans or can be learned only   with difficulty ( Hayes and White , 2013 ) . For ex-   ample , Becker et al . ( 2011 ) claim that language   users do not acquire unnatural statistical patterns   that would allow them to distinguish nouns with   andwithoutlaryngealalternationsbetweenvowel-   initial suffixes ( while acquiring natural ones ) . In   contrast , Hayesetal . ( 2009)arguethatspeakersof   Hungarianmakeuseofunnaturalpatternsindecid-   ingvowelharmonypatterns(whetheraformends   inabilabialstop)buthavealearningbiastowards   natural patterns . Artificial grammar learning ex-   periments have been inconclusive but have sug-   gested that the difficult - to - learn phonological pat-   terns are structurally complex , not phonetically   unnatural ( MoretonandPater , 2012a , b ) .   The phonological ordering generalizations pro-   posed byMortensen ( 2006 ) are structurally quite   simple , but often phonetically incoherent . For   Hmong EEs , ordering follows the hierarchy pre-   sentedinTable 1;anEEwithan 𝐴𝐵𝐴𝐵formis   ordered such that , if 𝐵and 𝐵differ in tone , the3657Order Orthography IPA Description   1 -j ˥˧highfalling   2 -b ˥high   3 -m ˨˩lowcreaky   4 -s ˨low   5 -v ˧˥rising   6 -g ˧˩fallingbreathy   7 - ∅ ˧mid   toneof 𝐵ishigheronthehierarchythanthetone   of 𝐵.   This hierarchy has one phonetically reasonable   aspect — thefirsttwotonesstarthigh(thoughtheir   relative rank seems arbitrary ) . The rest of the hi-   erarchyispuzzling : itgoesfromlowesttolowto   risingtofallingtoneutral . Mortensen’sgeneraliza-   tion for Lahu elaborate expressions would be eas-   iertoreconcilewithphoneticsubstance(thehigher   invowelspaceavowelis , thebetteracandidateit   is for the first position ) were it not that the best   first - position vowel is /o/ , a mid , back , rounded   vowel . The ordering generalizations that have   beenproposedforChinesearesimilarlyarbitrary-   looking — they can be stated in terms of historical   tonal categories ( like the Middle Chinese tones )   butappearincoherentinmodernlects , inwhichthe   phonetic realizations tones have “ wandered ” pho-   neticallytoadramaticdegree .   If it can be shown that these patterns can be   learnedfromnaturalisticdata , thattheyarerobust   predictorsofEEandCCordering , andthatmodels   trainedtodetectcorrectlyorderedEEsandCCsin   runningtextlearntousethiskindofphonological   evidence to assign labels , it would be suggestive ,   thoughnotdefinitive , evidenceagainsttheposition   thatphonologicalconstraintsmustbegroundedin   phoneticsubstance .   2.2 Word order conditioned on phonology   Inmainstreamgenerativelinguistics , grammarhas   usually been viewed as a feed - forward produc-   tion system . While the nature of this pipeline has   changedovervariousrevisionsofthetheory , acon-   sistentthemeisthatphonologyoperatesontheout-   put of syntax ( Chomsky,1965,1981,1995 ) and   that , therefore , syntax should not be sensitive to   phonology . One common version of this theoryis the Y model , of which some of the earliest de-   scriptions are found in Chomsky(1981 ) . In this   model , surfacestructure ( Mary hits John vs. John   is hit by Mary ) is derived first . Then phonetic   form(PF; [ mɛəɹi hɪts dʒɒn ] ) andlogicalform(LF;   ℎ𝑖𝑡(𝑀 𝑎𝑟𝑦 , 𝐽 𝑜ℎ𝑛 ) ) are derived from surface struc-   ture . BecausesurfacestructureisfixedbeforePF ,   syntax should not be sensitive to phonology . In   certainothertheoriesofgrammar , differentlevels   ofrepresentationarecomputedinparallelandare   mutually constraining . An early example of such   a framework is Lexical Functional Grammar ( Ka-   planand Bresnan , 1982;Bresnanet al . , 2015 ) . In   thisclassofframeworks , itisexpectedthatphonol-   ogyshouldbe able to influence word order . The   question of whether and how phonology can af-   fectwordorderissignificantforlargertheoriesof   grammar .   In fact , there is mounting evidence that word   order can be sensitive to phonology . It has long   beensuggestedthatdativeshiftinEnglishissensi-   tivetophonologicalweight ( Ross,1967)although   this claim has also been long contested ( Wasow   and Arnold , 2003 ) . Some newer evidence comes   fromcoordinatecompoundandechoreduplication   constructions in Japanese , Korean , and Jingpho   ( Kwon and Masuda , 2019;Dai,1986 ) . An even   more interesting case comes from Tagalog noun-   adjectiveorder , whichissometimesviewedasbe-   ing free but which is actually sensitive to a set of   phonological constraints ( Shih and Zuraw , 2017 ) .   Even more germane to the current discussion are   the findings of Benor and Levy ( 2006 ) andMor-   ganandLevy ( 2016),whofoundthatphonological   factors are significant predictors of the sequence   ofbinomialexpressions(like son and daughter ) in   English . The current case would enrich the body   of relevant evidence in part because , while these   cases are all instances of “ soft ” statistical tenden-   cies , theHmongorderinggeneralizationisclaimed   tobenearlycategorical(withafew , principled , ex-   ceptions ) .   3 Hypotheses   Basedontheexistingvolumeofwork , wepropose   thefollowinghypotheses:36581.The order of Hmong and Lahu EEs and Chi-   nese CCs can be predicted phonologically   ( outofcontext ) .   2.The “ phonetically unnatural ” phonological   scales proposed by Mortensen ( 2006 ) and   Ting(1975 ) predict the ordering of EEs in   HmongandLahuandCCsinChinese(outof   context ) .   3.These scales can be learned by decision tree   classifiers(outofcontext ) .   4.Phonological information facilitates the   recognition of correctly and incorrectly   orderedHmongEEsincontext .   4 Data   We examine the ordering effects across three lan-   guages : Hmong , Lahu , andChinese(withMiddle   ChineseandMandarinpronunciations ) .   For Hmong , we use a list of 3253 unique elab-   orate expressions extracted from the 12 million-   wordHmongSCHcorpus ( Mortensenetal . , 2022 ) ,   which was manually annotated and validated by   a human expert . All of the EEs are of the form   𝐴𝐵𝐴𝐵where 𝐵𝐵forms a coordinate com-   pound . WealsousetheentirecorpusfortheEEtag-   ging task described in Section 5.2 . For Lahu , we   usealistof1400EEscompiledby Matisoff(1989 ,   2006),whichcontainsboth 𝐴𝐵𝐴𝐵and 𝐵𝐴𝐵𝐴   forms . ForChinese , weusealistof254antonymic   coordinatecompounds 𝐵𝐵recordedinthe Mod-   ern Chinese Dictionary ( Anonymous , 2016 ) . Mid-   dleChinesepronunciationsareretrievedfromWik-   tionary .   5 Experiments   5.1 Learning Hmong , Lahu , and Chinese CC   and EE Ordering with Classifiers over   Phonological Features   We first examine whether the orders in elabo-   rateexpressionsandcoordinatecompoundscanbe   learned by a classifier . This experiment accom-   plishes two goals : 1 ) to reveal the existence and   robustness of the patterns in the phonological or-   dering , and2)togaininsightintothefeaturecom-   binationsthataremostcorrelatedwiththeordering   effects . Experiment We use the EE lists described in   Section4asphraseswiththe attestedordering , and   createan unattested listofEEsbyswitchingtheor-   derof 𝐵and 𝐵(occasionallybothordersareat-   tested , inwhichitisnotincludedinthe unattested   list ) . Wethenformulatethetaskasabinaryclassi-   ficationproblemtopredictwhetheragivenorder-   ingisattestedorunattested .   To examine the degree to which the order can   be predicted by phonology only , we use one - hot   featuresoftheonset , rhyme(vowel)andtonecon-   stituentsineachsyllableasclassificationfeatures .   Wefoundthatone - hotphonemicfeaturesweresuf-   ficientlyexpressive , andthatusingarticulatoryfea-   tures ( Mortensen et al . , 2016 ) did not further im-   prove the performance . In Section 5.3we ana-   lyze the effect of adding word embeddings to the   feature set . For all classification experiments , we   compute the 𝜒statistic on all input features and   select the top 𝐾features that most correlate with   theclasslabel , where 𝐾isdeterminedbyadevel-   opmentset .   We report the result on two types of classifiers :   a decision tree ( DT ) classifier for maximal inter-   pretability , and a support vector machine ( SVM )   with RBF kernel for the best classification per-   formance . We also experimented with multi-   layer perceptron classifiers of varying widths and   depths , but they did not outperform SVM on this   dataset . Sinceotherclassifiersdonotoffertheex-   plainabilityofDTortheperformanceofSVM , we   onlyreportresultsonthesetwomodels .   We split the attested word list into 70%/30 %   train / testsetsbeforeaugmentingitwithunattested   datainordertopreventthesameEEfromappear-   ing in both the train and test sets . However , it   would still be possible for the same ( 𝐵 , 𝐵)to   appear in both train and test sets with different   𝐴words ( repeated words ) . To eliminate this pos-   sibility , we also report results on randomly sam-   pledsubsets ofEEs whereinall ( 𝐵 , 𝐵)pairsare   unique(sothatthereisnocontaminationacrossthe   trainandtestsets ) .   Rule - Based Classification We also test how   well the ordering scales proposed in Mortensen   ( 2006 ) perform as a rule - based classifier , com-   pared to a DT and SVM trained on the dataset .   Thisisequivalenttodirectlyexaminingthedistri-   butional patterns of the ordering effects . Table 23659showstheordersinHmong , LahuandMiddleChi-   neseusedintherule - basedclassifier . Whenthere   isatie , theorderisdeterminedrandomly .   Language Order   HmongTones jbmsvg∅   LahuRhymes ouiɨәɔeɛa   MCTones pingshangquru   ResultsTable3shows the classification accura-   cies for all languages . We report results on two   classifiers , usingtwodifferentsetsoffeatures : fo-   cal constituent for the group of phonemes corre-   spondingtotheorderingrules(rhymeforLahuand   toneforHmongandChinese),and all constituents   foralltheonset , rhyme , andtonephonemes .   Weobservearobustcorrelationbetweenphonol-   ogy and attested orders in all four languages , as   seen by the high accuracy a classifier can attain .   Even on unique ( 𝐵 , 𝐵)pairs , the best classifier   andfeaturesetachieves71%–88%accuracy . This   meansthattheorderingeffectsarenotsimplydue   to frequent ( 𝐵 , 𝐵)pairs skewing the statistics;   rather , the ordering effect is robust across many   ( 𝐵 , 𝐵)pairsinthefourlanguages .   With only the focal constituent feature set , we   observe comparable accuracy between the rule-   basedclassificationandeitherstatisticalclassifier .   This suggests that the degree to which the focal   constituent alone determines EE ordering is no   more than the linear ordering scale proposed by   Mortensen ( 2006).However , when phonemes   fromotherconstituentsareincludedinthefeature   set and an SVM is used , we observe an increase   of3–11%inaccuracy . Thissuggeststheexistence   ofmorecomplexphonologicalinteractionsbeyond   thelinearscaleoverthefocalconstituent .   Visualization of Learned Decision Tree Byex-   amining the learned decision tree , one can derive   a linear hierarchy based on the order of features   on the nobranch , and whether each branching ac-   tion leads to majority attested words or majority   unattested words . We find that phonemes that ap-   pear topmost in the tree ( the most order - definingphonemes)areexactlythoseatthetwoendsofthe   scales proposed by Mortensen ( 2006 ) , and a deci-   siontreeclassifiercanlearnastrikinglysimilarhi-   erarchy , asshowninTable 4 . Detailsonthederiva-   tionandthelearnedtreeareshowninAppendix A.   5.2 Learning Hmong EE Ordering as   Sequence Labeling   Experiment Now we investigate whether mod-   els can learn to recognize elaborate expressions   and their ordering effects in context in a natural-   istic corpus . We limit our experiments to Hmong   in this section due to the unavailability of EE-   annotatedcorporainotherlanguages . TheHmong   dataset is annotated with BIOtags , where a BIII   sequence represents a labeled EE . We train a neu-   ralsequencelabelingmodeltopredictthe BIOtag   ofeachwordinasentence .   Weexperimentwithtwotypesoffeatureextrac-   tors : a bidirectional LSTM and a CNN . We use   both word - level and phoneme - level embeddings ,   followingtheintuitionthatthephonologicallycon-   ditioned ordering helps speakers recognize an EE   structure in context . Implementation details and   hyperparametersaredescribedinAppendix B.   Inadditiontothevanillataggingtask , toinvesti-   gatewhetherthemodelscanlearntheorderingof   EEs in context , we perform an experiment where   theordersof 𝐵and 𝐵areswappedforhalfofthe   EEs , andthetagsfortheswappedEEsarechanged   toB - fakeand I - fake . Thisrendersthetaskmore   difficultasthemodelneedstobothidentifyanEE   incontextandclassifywhethertheorderhasbeen   changed .   To prevent the model from memorizing certain   EEs , wesplitthedataintotrain / val / testsetsbypar-   titioning the list of EEs into disjoint sets , so that   EEs in the validation and test sets do not appear   in the training split . This way , the model is only   given unseen EEs at test time . Furthermore , we   partitiontheEEsintoswap / no - swapsothatoccur-   rencesofeachEEareeitherallswappedorallkept   unchanged .   Baseline Thesimplestbaselinemodelwouldbe   to tag every occurrence of 𝐴𝐵𝐴𝐵(a 4 - gram   where the first and third words are identical ) in   the corpus as an EE without any consideration   of the word or its phonology . Doing so yields   100 % recall but very poor precision , since most   occurrences of 𝐴𝐵𝐴𝐵are not elaborate expres-   sions . Three strategies are employed to improve3660Language Hmong Lahu Mandarin MiddleChinese   Data All Unique All Unique Unique Unique   N 6420 1404 2748 1664 254 251   Rules 88.8 % 85.5 % 68.3 % 66.3 % – 70.7 %   DT(focalconstituent ) 89.0 % 85.0 % 67.2 % 64.3 % 65.3 % 70.4 %   DT(allconstituents ) 96.4 % 85.3 % 79.7 % 67.8 % 68.1 % 75.3 %   SVM(focalconstituent ) 89.1 % 85.4 % 67.3 % 64.4 % 65.3 % 70.4 %   SVM(allconstituents ) 96.7 % 88.3 % 81.9 % 71.3 % 76.1 % 81.0 %   Language Order   HmongLing . jbmsvg∅   Tree jbmvsg∅   LahuLing . ouiɨәɔeɛa   Tree ou ... eɔɛa   MCLing . pingshangquru   Tree pingshangquru   the performance of this baseline : ( 1 ) ensure that   ( 𝐴 , 𝐵 , 𝐵)are proper Hmong syllables parsable   by a regular expression classifier; ( 2 ) set a word   vector similarity threshold between the two CC   words ( 𝐵and 𝐵 ) so that 𝑐𝑜𝑠𝑖𝑛𝑒(𝑣 , 𝑣 ) > 𝛼 ,   sincemanyHmongEEshavethetwoCCwordsof   similar meanings ( Mortensen , 2006);(3 ) ensure   the tonal scale in Table 2is followed between 𝐵   and 𝐵   ResultsWereporttheF1scoreofpredictedtags   on different models in Table 5 . For the baseline   model , allthreestrategiesimprovethetaggingper-   formance , suggestingthatbothsemanticsimilarity   and adherence to the tonal scale are indicators of   being an EE . Despite the reasonable performance   of the baseline , a neural sequence labeling model   is able to beat it substantially , achieving a high   F1 score in the EE tagging task . In particular , a   CNNfeatureextractoroutperformsanLSTMfea-   ture extractor . We hypothesize that this is due toModel F1 Precision Recall   Baseline 41.32 26.15 100.00   + regexparsable 49.24 32.83 100.00   + wv . sim . thresh 60.99 50.29 77.99   + tonalscale 66.66 59.37 76.56   LSTM 74.10 66.12 84.36   + phonemes 73.14 65.39 83.09   LSTM + swap clf . 64.38 57.54 73.29   + phonemes 63.97 56.93 73.17   CNN 90.79 87.36 94.52   + phonemes 90.26 85.98 95.58   CNN + swap clf . 89.01 85.73 92.62   + phonemes 89.26 86.00 92.79   a convolution kernel being able to capture non-   localinteractionsinanEE(i.e . ,identicalfirstand   thirdwords , andsimilarsecondandfourthwords ) ,   whereasthelinearnatureofanLSTMencoderbe-   comesrestrictiveinthistask .   WhenhalfoftheEEsintheformof 𝐴𝐵𝐴𝐵are   changedto 𝐴𝐵𝐴𝐵andtheirtagsaremodifiedto   B - fakeand I - fake(swapclf . rowsinthetable ) ,   themodelisstillabletoachievehighF1scoresthat   are only slightly lower than the unswapped coun-   terpart , even though the Band Itags have split   intotwotypes . Thefactthatincreasingthenumber   of classes does not degrade the performance very   much suggests that the model can learn to distin-   guish attested and unattested orderings very well .   Toquantifythemodel’sabilitytolearnHmongEE   ordering , wecalculateanin - contextclassification   accuracybyexamininghowmanycorrectlyidenti-3661fiedEEsalsohaveacorrectpredictioninwhether   the order has been swapped . We find that the in-   contextclassificationaccuracyis99.1%forLSTM   and99.5%forCNN , whicharebothexceptionally   high . NotethatthisanalysisexcludesEEsthatare   notcorrectidentified(bothfalsepositivesandfalse   negatives ) . Full confusion matrices are shown in   AppendixC.   Interestingly , wefindthataddingphonemelevel   featurestotheinputofeitherLSTMorCNNdoes   notimprovetheperformanceinboththeswapped   and unswapped cases . This result is in contrast   with other similar sequence tagging tasks ( e.g. ,   NER ) , where character level features are found   to improve performance ( Yang et al . , 2018;Kuru   etal . ,2016 ) . Moreimportantly , thisresultpresents   acontrasttotherobustphonologicalpatternsfound   intheprevioussection , asitdemonstratesthatthe   modelisabletotagelaborateexpressionsandclas-   sify their orders successfully without any refer-   encetophonology . Thissuggeststhattheordering   ( 𝐵 , 𝐵)can be predicted not only via phonology ,   butalsoviaword - levelfeaturesthroughtheembed-   dingstrainedwiththetaggingmodel .   Visualization of Word Embeddings It is a   rather perplexing result that a tagging model can   learn the ordering of EEs via word embeddings   only . Figure 1shows the UMAP projection   ( McInnesetal . , 2018)oftwotypesoflearnedem-   beddingsinto2Dspace . Embeddingsfromthetag-   ging model show clear separation between words   that tend to occur first in an EE ( in the 𝐵posi-   tion ) and words that tend to occur second , where   asembeddingstrainedseparatelyontheSkipGram   algorithm ( Mikolov et al . , 2013 ) show no separa-   tion . This suggests that the learned separation is   uniquetothetaggingmodel . However , thereisno   wayforthemodeltomemorizeEEsfromthetrain-   ingset , sincethetestsetcontainsnon - overlapping   EEs . How , then , would the tagging model learn   whatwordstendtooccurfirstandwhatwordstend   tooccursecondinanEE ?   Itappearsthatthemodelisabletolearntheorder   of 𝐵and 𝐵fromtheoccurrencesofthese compo-   nentwordsinthetrainingset . ForanEE 𝐴𝐵𝐴𝐵   in the test set , although 𝐴𝐵𝐴𝐵itself never ap-   pearsinthetrainingset , 𝐵and 𝐵doappeareither   asacoordinatecompound 𝐵𝐵/𝐵𝐵,orasparts   of another EE 𝑋𝐵𝑋𝐵/𝑋𝐵𝑋𝐵in the training   set . AsshowninFigure 2,appearancesofthemin   thesameordergreatlyoutnumbersthoseofthere-   versedorder . Asaresult , themodelmaybeableto   learnwhichwordstendtobe 𝐵or 𝐵fromthese   distributionalpropertiesoftheEEwords .   Tofurtherisolatethiseffect , weperformanex-   periment where the train / test splits are made so   thatevencomponentwordsdonooverlapbetween   them ( so that the box plot in Figure 2would be   completely empty ) . We confirm that the tagging   performancedropconsiderablyinthissetting , with   an average F1 score ( 59.78 ) unable to beat the   strongest baseline . However , even in this setting ,   we do not find phoneme features to contribute to   the tagging performance in a statistically signifi-3662   ca nt way ( F1=61.16 , one - sided Wilcoxon signed-   ranktestp=0.28 ) .   5.3 Learning Hmong EE Ordering with   Classifiers over Word Vectors   Experiment To further investigate to what ex-   tent word embeddings determine the order of   ( 𝐵 , 𝐵)in Hmong EEs , we revisit the out - of-   contextclassificationexperimentpresentedinsec-   tion5.1,thistimeaddingwordvectorfeatures . We   experiment with both SkipGram embeddings and   embeddingsextractedfromtheCNNsequencetag-   gingmodel(withoutswapping ) . Embeddingsfrom   the tagging model are expected to perform better   ontheclassificationtask , sincetheyareoptimized   to detect words contained in EEs in the attested   order . On the other hand , embeddings separately   trainedviaSkipGramaremore“pure,”astheyonly   capture the distributional semantics of the words   withoutadditionalinformation .   ResultsTable6shows the classification accura-   cies using word embedding features , as well as   wordembeddingcombinedwithone - hotphoneme   features . Weobservethatembeddingstrainedwith   thetaggerindeedperformbetterthanthosetrained   viaSkipGram . Whatissurprisingisthatusingem-   bedding features from the tagger alone produces   a classification accuracy comparable to using all   phonemes ( 88 % ) . Moreover , an even higher ac-   curacy can be achieved by combining phoneme   features with embeddings from the tagger . This   suggests that EE ordering in Hmong can be pre - Data All Unique   focalconstituent(tone ) 89.1 % 85.4 %   allconstituents 96.7 % 88.3 %   wv - sg 94.4 % 71.1 %   wv - tagger 96.4 % 88.3 %   allconstituents+wv - sg 96.6 % 88.8 %   allconstituents+wv - tagger 97.1 % 93.8 %   dictedfromtwo independent butmutuallyreinforc-   ingroutes , namelyphonologyandlexicaldistribu-   tion . Eithermethodaloneisagoodpredictorofthe   ordering , butcombiningthetwoachievesthebest   accuracy , because the two routes each offer addi-   tionalinformationthatareimportantinpredicting   theorderingofHmongEEs .   Visualization of Feature Importance Withato-   tal360featuresfrombothphonemesandwordvec-   tors , we can visualize which features the model   find the most important in this classification task   byexaminingtheweightslearnedbythemodel .   Figure3shows the proportion of feature types   thathavethehighestimportancewhenvaryingthe   number of features ( 𝑘 ) . We see that when 𝑘is   small , the model overwhelmingly uses phoneme   features ( especially tones ) to perform classifica-   tion . The test accuracy is impressively 84 % with   only 12 features – nearly 40 % of which are tonal   features . As 𝑘increases , wordembeddingfeatures   starttogainimportance , andthetestaccuracycan   be further improved when word embeddings are   incorporated . By the time when 𝑘reaches 200 ,   theproportionofeachfeaturetypebecomesimilar   to the natural proportion before selection ( dashed   lines ) .   6 Discussion   In this paper , we set out to explore the ways that   the order of words in EEs and CCs in Hmong ,   Lahu , and Chinese can be learned by computa-   tionalmodels . Motivatedbyearlierlinguists’find-3663   ings , we first use phonological features alone to   discriminate between attested and unattested or-   dersofwords . Wefindthatinthecaseofallthree   languages , the order of words can indeed be pre-   dicted phonologically , and the “ phonetically un-   natural”hierarchiesdopredicttheorderingofEEs   and CCs . Furthermore , a decision tree classifier   is able to learn more - or - less the same hierarchies ,   suggestingthatspeakersofthoselanguagescould   inprinciplelearnthelinearhierarchiesthroughex-   posure to the language , and use these hierarchies   todecideonthecorrectorderofwordsinEEsand   CCs . Thesefindingsprovidepositiveevidencefor   hypotheses1–3fromSection 3 . Wethenexplored   the ways models can utilize context and distribu-   tional patterns of words to learn the orders in the   sequence tagging experiments , and we were not   able to find evidence for hypothesis 4 . We were   surprisedtofindthatmodelscanperformwellus-   ingonlywordfeatures , andthataddingphonemes   tothefeaturesetdoesnothelpatall .   The seemingly contradictory results of our in-   vestigationpointinaninterestingdirection . Infor-   mation on which a model could rely to learn the   ordering of these constructions is present redun-   dantlyinphonology(ontheonehand)andinlexi-   calanddistributionalpatterns(ontheother ) . When   allowed to cooperate on a level playing field , em-   beddings and phonology - based features both con - tributetotheidentificationofwell - formedEEsat   a similar level . In other words , while it is possi-   ble that language users mayuse phonological hi-   erarchieslikethoseproposedin Mortensen ( 2006 )   toselectappropriateordersforEEsandCCs , itis   clearly not the case that they must(though they   willperformabitbetteriftheydo ) . Thesephono-   logical hierarchies may have been more order-   defining in the history of the languages , but as   thesequencetaggingexperimentshavesuggested ,   they may also have become fossilized in the lex-   icon and in distributional patterns in the modern   form . Many times , a ( 𝐵 , 𝐵)pair appears abun-   dantlyinmultiplesEEs(as 𝑋𝐵𝑋𝐵),asaCC(as   𝐵𝐵),orinother — morecomplicated — discourse   patterns in the same order , so that language users   couldlearnwhetheragivenwordtendstoappearin   the 𝐵or 𝐵position . Ifataggingmodelcanlearn   a word representation that distinguishes between   𝐵and 𝐵,languageusersmaydothesame .   In a sense , these results should be pleasing   to both the “ structure ” ( Mortensen , Hale , Reiss )   and the “ substance ” ( Hayes , Flemming , Steriade )   camps . They show , once again , that generaliza-   tions about sounds can be robust but phonetically   arbitrary . However , they leave open the possibil-   itythattherelevantsynchronicgeneralizationsare   notactuallyphonological .   7 Future Directions   We have shown two independent routes , namely   phonologyandlexicaldistribution , bywhichcom-   putationalmethodscanpredicttheorderofwords   in Hmong EEs . A language user could probably   dothesame , relyingonbothroutestosomedegree   whentheyneedtoselecttheorderofwordsinEEs .   However , thereisnowaytoknowforsurewithout   conductingapsycholinguisticexperimentwithna-   tive speakers , which would shed light on whether   any of the modeling actually translates to human   cognition . TheChineseandLahucasesalsoraise   interesting questions for future work : does the   sametwo - routemechanismworkforEEsandCCs   in these languages as well ? Answering this ques-   tionwillrequireadditionaldatacollectionandan-   notation , butwillshedsignificantlightonthisthe-   oreticallyimportantissue .   References36643665   A Trained Decision Trees   Figures4,5,6showwhatthetraineddecisiontree   lookslikeinthethreelanguages . Ineachtreenode ,   the top half of the box show the current majority   class , attested(ATT)orunattested(FAKE),aswell   as the number of votes . The bottom half of the   box shows the variable to branch on . As noted   in the main text , a linear ordering can be induced   fromthetreebyfollowingthebranches . Takethe   Hmong tree ( Figure 4 ) as an example . The first   factortosplitonis whether 𝐵has the j tone , and   if the answer is yes , the majority of words are at-   tested ( 255 attested vs 15 unattested / fake ) . This   suggests that jhas a strong tendency to occur in   the 𝐵position , sinceitisthemostdistinguishing   factortospliton . Hence jcanbeplacedasthefirst   toneonthescale . If 𝐵doesnothavethe jtone , the   next question to ask is whether 𝐵has the b tone .   Sincea yesansweragainleadstomajorityattested   words ( 273 attested vs 61 unattested / fake ) , bcan   beplacedsecondonthescale . Thenextthreeques-   tionstoaskconcernwiththe 𝐵word . Sincea yes   answerleadstoattestedwordsinallthreecases , it   suggeststhat ∅,gandshaveatendencytoappear   inthe 𝐵position , hencetheycanbeplacedonthe   endofthescaleinthatorder . Thenexttwofactors   concernwiththe jandbtones , whichhavealready   been placed on the scale , so we skip them . This   processoffollowingtheleftchild(the nobranch )   and placing tones at either end of the scale is re-   peatedlyapplied , yieldingtheinducedlinearscales   showninTable 4.36663667B Implementation Details   B.1 Data   TheHmongcorpusconsistsof740ksentenceswith   a positive rate of around 3.1 % ( i.e. 96.9 % of sen-   tences contain no EEs ) . The EEs are randomly   split into disjoint train and val / test sets with ap-   proximateratiosof91%/4.5%/4.5 % . Toreducethe   possibilitythatcertainsplitsareeasierthanothers ,   threesuchsplitsareindependentlyproduced . The   positive sentences are split into train and val / test   sets according to the EE partitions , and the nega-   tivesentencesaresplitwithapproximateratiosof   91%/4.5%/4.5 % .   B.2 Models   The sequence tagging model consists of a feature   extractor followed by a fully connected layer to   predict the tags : { B , I , O}in the unswapped case   and { B , B - fake , I , I - fake , O } in the swapped   classificationexperiments . Twofeatureextractors   are used : 1 ) an LSTM with bidirectional encod-   ing , and2)aCNN , consistingoffourlayersof1D   convolution , ReLU , Dropout , and BatchNorm .   Whencharacterorphonemelevelfeaturesareused ,   thecharacterembeddingsgothroughaCharCNN   before being concatenated with the word embed-   ding . Details on model configuration is shown in   Table7 . TheLSTMmodelcontainsapproximately   1.4 M parameters and the CNN contains approxi-   mately 1.7 M parameters . Our code is based on   NCRF++ ( YangandZhang , 2018 ) .   Hyperparameter Value   Wordembeddim 100   Charembeddim 30   LSTMhiddendim 100   CNNhiddendim 200   CNNkernelsize 3   CharCNNhiddendim 50   CharCNNkernelsize 3   Dropoutprobability 0.53668B.3 Training and Decoding   Themodelistrainedwithcrossentropylossusing   an SGD optimizer with momentum . Early stop-   ping is used on the F1 score of the validation set ,   with a patience of 10 epochs . During training ,   negative sentences in the training set are down-   sampled to 90 % ( resampled every epoch ) instead   of97%,whichleadsto3xfastertrainingtimebut   minimal impact on performance . Validation and   testsetsareusedintheirentirety . Traininghyper-   parameters are shown in Table 8 . Training typi-   callytakeslessthan2hourstocompleteonasingle   GeForceRTX2080TiGPU .   Hyperparameter Value   Batchsize 64   Learningrate 0.02   SGDmomentum 0.9   Earlystoppingpatience 10   C Confusion Matrices   Figure7showstheconfusionmatricesfortheswap   classification experiments . As mentioned in the   maintext , anin - contextclassificationaccuracycan   be calculated from the tokens that are correctly   identifiedaspartofanEEbutmayormaynothave   a correct prediction of the orders ( i.e. confuses B   with B - fake ) . For example , the in - contextclassi-   ficationaccuracyfortheCNNconfusionmatrixis   𝑎𝑐𝑐 = 439 + 447   439 + 447 + 4= 99.55%3669