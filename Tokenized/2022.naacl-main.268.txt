  Chenxuan Cui Katherine J. Zhang   LanguageTechnologiesInstitute , CarnegieMellonUniversity   cxcui@cs.cmu.edu , kjzhang@cmu.edu , dmortens@cs.cmu.eduDavid R. Mortensen   Abstract   Coordinate compounds ( CCs ) and elaborate   expressions(EEs)arecoordinateconstructions   common in languages of East and Southeast   Asia . Mortensen ( 2006)claimsthat(1)thelin-   earorderingofEEsandCCsinHmong , Lahu ,   andChinesecanbepredictedviaphonological   hierarchies and ( 2 ) these phonological hierar-   chies lack a clear phonetic rationale . These   claims are significant because morphosyntax   has often been seen as in a feed - forward re-   lationship with phonology , and phonological   generalizations have often been assumed to   be phonetically â€œ natural â€ . We investigate   whether the ordering of CCs and EEs can   be learned empirically and whether computa-   tional models ( classifiers and sequence label-   ingmodels)learnunnaturalhierarchiessimilar   tothosepositedby Mortensen ( 2006 ) . Wefind   that decision trees and SVMs learn to predict   the order of CCs / EEs on the basis of phonol-   ogy , with DTs learning hierarchies strikingly   similartothoseproposedbyMortensen . How-   ever , we also find that a neural sequence la-   beling model is able to learn the ordering of   elaborate expressions in Hmong very effec-   tivelywithoutusinganyphonologicalinforma-   tion . WearguethatEEorderingcanbelearned   through two independent routes : phonology   andlexicaldistribution , presentingamorenu-   ancedpicturethanpreviouswork . [ ISO639 - 3 :   hmn , lhu , cmn ]   1 Introduction   In many languages of East and Southeast Asia ,   there are common constructions in which two   wordsorphrasesarecoordinatedwithoutanovert   marker like a conjunction ( Hanna,2013Í¾Filbeck ,   1996Í¾Johns and Strecker , 1987Í¾Wheatley,1982Í¾   Matisoff,1973Í¾PanandCao , 1972Í¾Watson,1966Í¾   Banker,1964 ) . In coordinate compounds ( CCs ) ,   twowordsarecombinedtoformacompoundwordwhosesemanticsareoftenageneralizationofthose   ofthetwoconjoinedwords . Elaborateexpressions   ( EEs ) are similar , except that they can consist of   two phrases ( rather than words ) and include a re-   peatedword . Takethefollowingexamples :   ( 1)Chinesecoordinatecompounds(CCs )   çˆ¶æ¯ fÃ¹mÇ”father - mother â€˜ parents â€™   èŠ±æœ¨ huÄmÃ¹flower - tree â€˜ vegetation â€™   å¤©åœ° tiÄndÃ¬heaven - earth â€˜ universe â€™   å›½å®¶ guÃ³jiÄcountry - homeâ€˜nation â€™   é£æ°´ fÄ“ngshuÇwind - water â€˜ geomancy â€™   ( 2)Lahuelaborateexpressions(EEs )   a.É”Ì‚   fourcÄ“   cornerÉ”Ì‚   fourphÉ”Ì‚   side   â€˜ ateverycorner â€™   b.chÉ”   peoplephÃ´Ê”   pilechÉ”   peopledÃ¬   lump   â€˜ athrongofpeople â€™   c.cÃ¢   eatcÃ»Ê”   scarcedÉ”Ì€   drinkcÃ»Ê”   scarce   â€˜ havenothingtoeatordrink â€™   Coordinating compounds are found throughout   theworld , withvaryingsemanticrelationshipsbe-   tweenthewholeandtheparts ( ObermÃ¼ller , 2015 ) .   Elaborate expressions are most common in main-   landSoutheastAsia , wheretheyoccupyaposition   of great prominence . They are often associated   with elevated styles of discourse , but they occur   inallgenresandregisters .   Earlier investigators have claimed the order of   theconstituentwordsinCCsandEEsinsomelan-   guages is predictable by rule . Many of the pro-   posedorderinghierarchiesarebasedonphonology   ( Ting,1975Í¾Dai,1986Í¾Mortensen , 2006 ) . Build-   ingonthisearlierwork , Mortensen ( 2006)posited   that Lahu EE orders could be predicted based on   vowelquality â€” likeJingpho ( Dai,1986)â€”andthat   Hmong EE orders could be predicted based on   tone , echoing earlier claims for Chinese and Qe-   Nao ( Ting,1975Í¾Pan and Cao , 1972 ) . These3656tone and vowel scales were , however , not easy   to rationalize in phonetic terms and were used by   Mortensentoargueforaphonologyinwhichstruc-   ture reigns supreme and in which phonetic sub-   stanceplaysonlyanepiphenomenalrole .   Theseclaimshavebeenviewedwithskepticism   for two reasons : morphosyntax has been widely   seenasprovidingtheinputstophonology , notbe-   ing driven by phonology ( Chomsky,1965)Í¾ and   phonology , since Jakobsonetal . ( 1951)andChom-   sky and Halle ( 1968 ) has usually been seen as   grounded in phonetic categories . Some investi-   gators have claimed that sound patterns that are   not phonetically natural are inherently unlearn-   able . They can exist only as linguistic fossils de-   positedbyahistoryoflanguagechange . Inthispa-   per , weundertaketoinvestigatewhatkindofdata   is needed for ( computational ) learners to acquire   thesepatterns . Wereportthefollowingfindings :   â€¢Even rather simple classifiers like decision   trees can learn to predict the order of EEs in   isolation in over 96 % of cases ( Hmong ) and   79%ofcases(Lahu)usingonlyphonological   information .   â€¢ThedecisiontreesforHmong , Lahu , andChi-   nesemirrorthephonologicalhierarchiespro-   posed for these languages , suggesting that   these hierarchies are empirically robust and   learnablefromtheavailableevidence .   â€¢However , correct and incorrect orderings of   Hmong EEs can be effectively distinguished   in context by a neural sequence labeling   model withoutanyphonologicalinformation ,   suggestingthatlearnerswouldnothavetoac-   quirethephonologicalgeneralizationdirectly   inordertoproducewell - formedEEs .   2 Theoretical Significance   Theexperimentsreportedinthispaperhaveabear-   ingontwoassumptionswidelyheldinphonologi-   caltheory :   1.Truephonologicalgeneralizationsarealways   grounded in phonetic realities ( phonology is   natural )   2.Phonology operates on the outputs of syntax   andmorphology(grammarisserial )   Bothoftheseassumptionshavebeencontested . If   the analysis of EE and CC ordering in Mortensen   ( 2006 ) is sound , neither of these assumptions can   beentirelycorrect.2.1 Phonological patterns and phonetic   substance   Starting even before Prague School phonology , it   was widely assumed that the grammatical cate-   goriesandpatternsmakingreferencetosoundare   coherent in terms of physical ( articulatory , acous-   tic , and psychophysical ) dimensions . The most   common sound patterns found in the world â€™s lan-   guages can usually be explained in these terms .   Forexample , inmanylanguagesincludingEnglish ,   if a nasal is followed by a stop , the place of artic-   ulation of the nasal assimilates to that of the stop   ( i[m]possiblevs.i[n]tolerantvs.i[Å‹]glorious ) .   Forphonologicallydistinctivefeatures , suchas   thoserelatedtoplaceofarticulation , thiswascodi-   fiedbyJakobsonetal . ( 1951)andinjectedintogen-   erative phonology by Chomsky and Halle ( 1968 ) .   Even more radical statements about the relation-   ship between phonological form and substance   havebeenmadesincethen ( DoneganandStampe ,   1979Í¾Flemming,2013Í¾Hayes,2011Í¾Doneganand   Stampe,2009Í¾Steriade et al . , 2001 ) . While there   hasneverbeenacompleteconsensusonthematter   ( Fudge,1967Í¾Hyman,1970Í¾HaleandReiss , 2000 ,   2008 ) , it has been widely assumed that phonolog-   ical patterns that are phonetically incoherent can-   not be learned by humans or can be learned only   with difficulty ( Hayes and White , 2013 ) . For ex-   ample , Becker et al . ( 2011 ) claim that language   users do not acquire unnatural statistical patterns   that would allow them to distinguish nouns with   andwithoutlaryngealalternationsbetweenvowel-   initial suffixes ( while acquiring natural ones ) . In   contrast , Hayesetal . ( 2009)arguethatspeakersof   Hungarianmakeuseofunnaturalpatternsindecid-   ingvowelharmonypatterns(whetheraformends   inabilabialstop)buthavealearningbiastowards   natural patterns . Artificial grammar learning ex-   periments have been inconclusive but have sug-   gested that the difficult - to - learn phonological pat-   terns are structurally complex , not phonetically   unnatural ( MoretonandPater , 2012a , b ) .   The phonological ordering generalizations pro-   posed byMortensen ( 2006 ) are structurally quite   simple , but often phonetically incoherent . For   Hmong EEs , ordering follows the hierarchy pre-   sentedinTable 1Í¾anEEwithan ğ´ğµğ´ğµformis   ordered such that , if ğµand ğµdiffer in tone , the3657Order Orthography IPA Description   1 -j Ë¥Ë§highfalling   2 -b Ë¥high   3 -m Ë¨Ë©lowcreaky   4 -s Ë¨low   5 -v Ë§Ë¥rising   6 -g Ë§Ë©fallingbreathy   7 - âˆ… Ë§mid   toneof ğµishigheronthehierarchythanthetone   of ğµ.   This hierarchy has one phonetically reasonable   aspect â€” thefirsttwotonesstarthigh(thoughtheir   relative rank seems arbitrary ) . The rest of the hi-   erarchyispuzzling : itgoesfromlowesttolowto   risingtofallingtoneutral . Mortensenâ€™sgeneraliza-   tion for Lahu elaborate expressions would be eas-   iertoreconcilewithphoneticsubstance(thehigher   invowelspaceavowelis , thebetteracandidateit   is for the first position ) were it not that the best   first - position vowel is /o/ , a mid , back , rounded   vowel . The ordering generalizations that have   beenproposedforChinesearesimilarlyarbitrary-   looking â€” they can be stated in terms of historical   tonal categories ( like the Middle Chinese tones )   butappearincoherentinmodernlects , inwhichthe   phonetic realizations tones have â€œ wandered â€ pho-   neticallytoadramaticdegree .   If it can be shown that these patterns can be   learnedfromnaturalisticdata , thattheyarerobust   predictorsofEEandCCordering , andthatmodels   trainedtodetectcorrectlyorderedEEsandCCsin   runningtextlearntousethiskindofphonological   evidence to assign labels , it would be suggestive ,   thoughnotdefinitive , evidenceagainsttheposition   thatphonologicalconstraintsmustbegroundedin   phoneticsubstance .   2.2 Word order conditioned on phonology   Inmainstreamgenerativelinguistics , grammarhas   usually been viewed as a feed - forward produc-   tion system . While the nature of this pipeline has   changedovervariousrevisionsofthetheory , acon-   sistentthemeisthatphonologyoperatesontheout-   put of syntax ( Chomsky,1965,1981,1995 ) and   that , therefore , syntax should not be sensitive to   phonology . One common version of this theoryis the Y model , of which some of the earliest de-   scriptions are found in Chomsky(1981 ) . In this   model , surfacestructure ( Mary hits John vs. John   is hit by Mary ) is derived first . Then phonetic   form(PFÍ¾ [ mÉ›É™É¹i hÉªts dÊ’É’n ] ) andlogicalform(LFÍ¾   â„ğ‘–ğ‘¡(ğ‘€ ğ‘ğ‘Ÿğ‘¦ , ğ½ ğ‘œâ„ğ‘› ) ) are derived from surface struc-   ture . BecausesurfacestructureisfixedbeforePF ,   syntax should not be sensitive to phonology . In   certainothertheoriesofgrammar , differentlevels   ofrepresentationarecomputedinparallelandare   mutually constraining . An early example of such   a framework is Lexical Functional Grammar ( Ka-   planand Bresnan , 1982Í¾Bresnanet al . , 2015 ) . In   thisclassofframeworks , itisexpectedthatphonol-   ogyshouldbe able to influence word order . The   question of whether and how phonology can af-   fectwordorderissignificantforlargertheoriesof   grammar .   In fact , there is mounting evidence that word   order can be sensitive to phonology . It has long   beensuggestedthatdativeshiftinEnglishissensi-   tivetophonologicalweight ( Ross,1967)although   this claim has also been long contested ( Wasow   and Arnold , 2003 ) . Some newer evidence comes   fromcoordinatecompoundandechoreduplication   constructions in Japanese , Korean , and Jingpho   ( Kwon and Masuda , 2019Í¾Dai,1986 ) . An even   more interesting case comes from Tagalog noun-   adjectiveorder , whichissometimesviewedasbe-   ing free but which is actually sensitive to a set of   phonological constraints ( Shih and Zuraw , 2017 ) .   Even more germane to the current discussion are   the findings of Benor and Levy ( 2006 ) andMor-   ganandLevy ( 2016),whofoundthatphonological   factors are significant predictors of the sequence   ofbinomialexpressions(like son and daughter ) in   English . The current case would enrich the body   of relevant evidence in part because , while these   cases are all instances of â€œ soft â€ statistical tenden-   cies , theHmongorderinggeneralizationisclaimed   tobenearlycategorical(withafew , principled , ex-   ceptions ) .   3 Hypotheses   Basedontheexistingvolumeofwork , wepropose   thefollowinghypotheses:36581.The order of Hmong and Lahu EEs and Chi-   nese CCs can be predicted phonologically   ( outofcontext ) .   2.The â€œ phonetically unnatural â€ phonological   scales proposed by Mortensen ( 2006 ) and   Ting(1975 ) predict the ordering of EEs in   HmongandLahuandCCsinChinese(outof   context ) .   3.These scales can be learned by decision tree   classifiers(outofcontext ) .   4.Phonological information facilitates the   recognition of correctly and incorrectly   orderedHmongEEsincontext .   4 Data   We examine the ordering effects across three lan-   guages : Hmong , Lahu , andChinese(withMiddle   ChineseandMandarinpronunciations ) .   For Hmong , we use a list of 3253 unique elab-   orate expressions extracted from the 12 million-   wordHmongSCHcorpus ( Mortensenetal . , 2022 ) ,   which was manually annotated and validated by   a human expert . All of the EEs are of the form   ğ´ğµğ´ğµwhere ğµğµforms a coordinate com-   pound . WealsousetheentirecorpusfortheEEtag-   ging task described in Section 5.2 . For Lahu , we   usealistof1400EEscompiledby Matisoff(1989 ,   2006),whichcontainsboth ğ´ğµğ´ğµand ğµğ´ğµğ´   forms . ForChinese , weusealistof254antonymic   coordinatecompounds ğµğµrecordedinthe Mod-   ern Chinese Dictionary ( Anonymous , 2016 ) . Mid-   dleChinesepronunciationsareretrievedfromWik-   tionary .   5 Experiments   5.1 Learning Hmong , Lahu , and Chinese CC   and EE Ordering with Classifiers over   Phonological Features   We first examine whether the orders in elabo-   rateexpressionsandcoordinatecompoundscanbe   learned by a classifier . This experiment accom-   plishes two goals : 1 ) to reveal the existence and   robustness of the patterns in the phonological or-   dering , and2)togaininsightintothefeaturecom-   binationsthataremostcorrelatedwiththeordering   effects . Experiment We use the EE lists described in   Section4asphraseswiththe attestedordering , and   createan unattested listofEEsbyswitchingtheor-   derof ğµand ğµ(occasionallybothordersareat-   tested , inwhichitisnotincludedinthe unattested   list ) . Wethenformulatethetaskasabinaryclassi-   ficationproblemtopredictwhetheragivenorder-   ingisattestedorunattested .   To examine the degree to which the order can   be predicted by phonology only , we use one - hot   featuresoftheonset , rhyme(vowel)andtonecon-   stituentsineachsyllableasclassificationfeatures .   Wefoundthatone - hotphonemicfeaturesweresuf-   ficientlyexpressive , andthatusingarticulatoryfea-   tures ( Mortensen et al . , 2016 ) did not further im-   prove the performance . In Section 5.3we ana-   lyze the effect of adding word embeddings to the   feature set . For all classification experiments , we   compute the ğœ’statistic on all input features and   select the top ğ¾features that most correlate with   theclasslabel , where ğ¾isdeterminedbyadevel-   opmentset .   We report the result on two types of classifiers :   a decision tree ( DT ) classifier for maximal inter-   pretability , and a support vector machine ( SVM )   with RBF kernel for the best classification per-   formance . We also experimented with multi-   layer perceptron classifiers of varying widths and   depths , but they did not outperform SVM on this   dataset . Sinceotherclassifiersdonotoffertheex-   plainabilityofDTortheperformanceofSVM , we   onlyreportresultsonthesetwomodels .   We split the attested word list into 70%/30 %   train / testsetsbeforeaugmentingitwithunattested   datainordertopreventthesameEEfromappear-   ing in both the train and test sets . However , it   would still be possible for the same ( ğµ , ğµ)to   appear in both train and test sets with different   ğ´words ( repeated words ) . To eliminate this pos-   sibility , we also report results on randomly sam-   pledsubsets ofEEs whereinall ( ğµ , ğµ)pairsare   unique(sothatthereisnocontaminationacrossthe   trainandtestsets ) .   Rule - Based Classification We also test how   well the ordering scales proposed in Mortensen   ( 2006 ) perform as a rule - based classifier , com-   pared to a DT and SVM trained on the dataset .   Thisisequivalenttodirectlyexaminingthedistri-   butional patterns of the ordering effects . Table 23659showstheordersinHmong , LahuandMiddleChi-   neseusedintherule - basedclassifier . Whenthere   isatie , theorderisdeterminedrandomly .   Language Order   HmongTones jbmsvgâˆ…   LahuRhymes ouiÉ¨Ó™É”eÉ›a   MCTones pingshangquru   ResultsTable3shows the classification accura-   cies for all languages . We report results on two   classifiers , usingtwodifferentsetsoffeatures : fo-   cal constituent for the group of phonemes corre-   spondingtotheorderingrules(rhymeforLahuand   toneforHmongandChinese),and all constituents   foralltheonset , rhyme , andtonephonemes .   Weobservearobustcorrelationbetweenphonol-   ogy and attested orders in all four languages , as   seen by the high accuracy a classifier can attain .   Even on unique ( ğµ , ğµ)pairs , the best classifier   andfeaturesetachieves71%â€“88%accuracy . This   meansthattheorderingeffectsarenotsimplydue   to frequent ( ğµ , ğµ)pairs skewing the statisticsÍ¾   rather , the ordering effect is robust across many   ( ğµ , ğµ)pairsinthefourlanguages .   With only the focal constituent feature set , we   observe comparable accuracy between the rule-   basedclassificationandeitherstatisticalclassifier .   This suggests that the degree to which the focal   constituent alone determines EE ordering is no   more than the linear ordering scale proposed by   Mortensen ( 2006).However , when phonemes   fromotherconstituentsareincludedinthefeature   set and an SVM is used , we observe an increase   of3â€“11%inaccuracy . Thissuggeststheexistence   ofmorecomplexphonologicalinteractionsbeyond   thelinearscaleoverthefocalconstituent .   Visualization of Learned Decision Tree Byex-   amining the learned decision tree , one can derive   a linear hierarchy based on the order of features   on the nobranch , and whether each branching ac-   tion leads to majority attested words or majority   unattested words . We find that phonemes that ap-   pear topmost in the tree ( the most order - definingphonemes)areexactlythoseatthetwoendsofthe   scales proposed by Mortensen ( 2006 ) , and a deci-   siontreeclassifiercanlearnastrikinglysimilarhi-   erarchy , asshowninTable 4 . Detailsonthederiva-   tionandthelearnedtreeareshowninAppendix A.   5.2 Learning Hmong EE Ordering as   Sequence Labeling   Experiment Now we investigate whether mod-   els can learn to recognize elaborate expressions   and their ordering effects in context in a natural-   istic corpus . We limit our experiments to Hmong   in this section due to the unavailability of EE-   annotatedcorporainotherlanguages . TheHmong   dataset is annotated with BIOtags , where a BIII   sequence represents a labeled EE . We train a neu-   ralsequencelabelingmodeltopredictthe BIOtag   ofeachwordinasentence .   Weexperimentwithtwotypesoffeatureextrac-   tors : a bidirectional LSTM and a CNN . We use   both word - level and phoneme - level embeddings ,   followingtheintuitionthatthephonologicallycon-   ditioned ordering helps speakers recognize an EE   structure in context . Implementation details and   hyperparametersaredescribedinAppendix B.   Inadditiontothevanillataggingtask , toinvesti-   gatewhetherthemodelscanlearntheorderingof   EEs in context , we perform an experiment where   theordersof ğµand ğµareswappedforhalfofthe   EEs , andthetagsfortheswappedEEsarechanged   toB - fakeand I - fake . Thisrendersthetaskmore   difficultasthemodelneedstobothidentifyanEE   incontextandclassifywhethertheorderhasbeen   changed .   To prevent the model from memorizing certain   EEs , wesplitthedataintotrain / val / testsetsbypar-   titioning the list of EEs into disjoint sets , so that   EEs in the validation and test sets do not appear   in the training split . This way , the model is only   given unseen EEs at test time . Furthermore , we   partitiontheEEsintoswap / no - swapsothatoccur-   rencesofeachEEareeitherallswappedorallkept   unchanged .   Baseline Thesimplestbaselinemodelwouldbe   to tag every occurrence of ğ´ğµğ´ğµ(a 4 - gram   where the first and third words are identical ) in   the corpus as an EE without any consideration   of the word or its phonology . Doing so yields   100 % recall but very poor precision , since most   occurrences of ğ´ğµğ´ğµare not elaborate expres-   sions . Three strategies are employed to improve3660Language Hmong Lahu Mandarin MiddleChinese   Data All Unique All Unique Unique Unique   N 6420 1404 2748 1664 254 251   Rules 88.8 % 85.5 % 68.3 % 66.3 % â€“ 70.7 %   DT(focalconstituent ) 89.0 % 85.0 % 67.2 % 64.3 % 65.3 % 70.4 %   DT(allconstituents ) 96.4 % 85.3 % 79.7 % 67.8 % 68.1 % 75.3 %   SVM(focalconstituent ) 89.1 % 85.4 % 67.3 % 64.4 % 65.3 % 70.4 %   SVM(allconstituents ) 96.7 % 88.3 % 81.9 % 71.3 % 76.1 % 81.0 %   Language Order   HmongLing . jbmsvgâˆ…   Tree jbmvsgâˆ…   LahuLing . ouiÉ¨Ó™É”eÉ›a   Tree ou ... eÉ”É›a   MCLing . pingshangquru   Tree pingshangquru   the performance of this baseline : ( 1 ) ensure that   ( ğ´ , ğµ , ğµ)are proper Hmong syllables parsable   by a regular expression classifierÍ¾ ( 2 ) set a word   vector similarity threshold between the two CC   words ( ğµand ğµ ) so that ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’(ğ‘£ , ğ‘£ ) > ğ›¼ ,   sincemanyHmongEEshavethetwoCCwordsof   similar meanings ( Mortensen , 2006)Í¾(3 ) ensure   the tonal scale in Table 2is followed between ğµ   and ğµ   ResultsWereporttheF1scoreofpredictedtags   on different models in Table 5 . For the baseline   model , allthreestrategiesimprovethetaggingper-   formance , suggestingthatbothsemanticsimilarity   and adherence to the tonal scale are indicators of   being an EE . Despite the reasonable performance   of the baseline , a neural sequence labeling model   is able to beat it substantially , achieving a high   F1 score in the EE tagging task . In particular , a   CNNfeatureextractoroutperformsanLSTMfea-   ture extractor . We hypothesize that this is due toModel F1 Precision Recall   Baseline 41.32 26.15 100.00   + regexparsable 49.24 32.83 100.00   + wv . sim . thresh 60.99 50.29 77.99   + tonalscale 66.66 59.37 76.56   LSTM 74.10 66.12 84.36   + phonemes 73.14 65.39 83.09   LSTM + swap clf . 64.38 57.54 73.29   + phonemes 63.97 56.93 73.17   CNN 90.79 87.36 94.52   + phonemes 90.26 85.98 95.58   CNN + swap clf . 89.01 85.73 92.62   + phonemes 89.26 86.00 92.79   a convolution kernel being able to capture non-   localinteractionsinanEE(i.e . ,identicalfirstand   thirdwords , andsimilarsecondandfourthwords ) ,   whereasthelinearnatureofanLSTMencoderbe-   comesrestrictiveinthistask .   WhenhalfoftheEEsintheformof ğ´ğµğ´ğµare   changedto ğ´ğµğ´ğµandtheirtagsaremodifiedto   B - fakeand I - fake(swapclf . rowsinthetable ) ,   themodelisstillabletoachievehighF1scoresthat   are only slightly lower than the unswapped coun-   terpart , even though the Band Itags have split   intotwotypes . Thefactthatincreasingthenumber   of classes does not degrade the performance very   much suggests that the model can learn to distin-   guish attested and unattested orderings very well .   Toquantifythemodelâ€™sabilitytolearnHmongEE   ordering , wecalculateanin - contextclassification   accuracybyexamininghowmanycorrectlyidenti-3661fiedEEsalsohaveacorrectpredictioninwhether   the order has been swapped . We find that the in-   contextclassificationaccuracyis99.1%forLSTM   and99.5%forCNN , whicharebothexceptionally   high . NotethatthisanalysisexcludesEEsthatare   notcorrectidentified(bothfalsepositivesandfalse   negatives ) . Full confusion matrices are shown in   AppendixC.   Interestingly , wefindthataddingphonemelevel   featurestotheinputofeitherLSTMorCNNdoes   notimprovetheperformanceinboththeswapped   and unswapped cases . This result is in contrast   with other similar sequence tagging tasks ( e.g. ,   NER ) , where character level features are found   to improve performance ( Yang et al . , 2018Í¾Kuru   etal . ,2016 ) . Moreimportantly , thisresultpresents   acontrasttotherobustphonologicalpatternsfound   intheprevioussection , asitdemonstratesthatthe   modelisabletotagelaborateexpressionsandclas-   sify their orders successfully without any refer-   encetophonology . Thissuggeststhattheordering   ( ğµ , ğµ)can be predicted not only via phonology ,   butalsoviaword - levelfeaturesthroughtheembed-   dingstrainedwiththetaggingmodel .   Visualization of Word Embeddings It is a   rather perplexing result that a tagging model can   learn the ordering of EEs via word embeddings   only . Figure 1shows the UMAP projection   ( McInnesetal . , 2018)oftwotypesoflearnedem-   beddingsinto2Dspace . Embeddingsfromthetag-   ging model show clear separation between words   that tend to occur first in an EE ( in the ğµposi-   tion ) and words that tend to occur second , where   asembeddingstrainedseparatelyontheSkipGram   algorithm ( Mikolov et al . , 2013 ) show no separa-   tion . This suggests that the learned separation is   uniquetothetaggingmodel . However , thereisno   wayforthemodeltomemorizeEEsfromthetrain-   ingset , sincethetestsetcontainsnon - overlapping   EEs . How , then , would the tagging model learn   whatwordstendtooccurfirstandwhatwordstend   tooccursecondinanEE ?   Itappearsthatthemodelisabletolearntheorder   of ğµand ğµfromtheoccurrencesofthese compo-   nentwordsinthetrainingset . ForanEE ğ´ğµğ´ğµ   in the test set , although ğ´ğµğ´ğµitself never ap-   pearsinthetrainingset , ğµand ğµdoappeareither   asacoordinatecompound ğµğµ/ğµğµ,orasparts   of another EE ğ‘‹ğµğ‘‹ğµ/ğ‘‹ğµğ‘‹ğµin the training   set . AsshowninFigure 2,appearancesofthemin   thesameordergreatlyoutnumbersthoseofthere-   versedorder . Asaresult , themodelmaybeableto   learnwhichwordstendtobe ğµor ğµfromthese   distributionalpropertiesoftheEEwords .   Tofurtherisolatethiseffect , weperformanex-   periment where the train / test splits are made so   thatevencomponentwordsdonooverlapbetween   them ( so that the box plot in Figure 2would be   completely empty ) . We confirm that the tagging   performancedropconsiderablyinthissetting , with   an average F1 score ( 59.78 ) unable to beat the   strongest baseline . However , even in this setting ,   we do not find phoneme features to contribute to   the tagging performance in a statistically signifi-3662   ca nt way ( F1=61.16 , one - sided Wilcoxon signed-   ranktestp=0.28 ) .   5.3 Learning Hmong EE Ordering with   Classifiers over Word Vectors   Experiment To further investigate to what ex-   tent word embeddings determine the order of   ( ğµ , ğµ)in Hmong EEs , we revisit the out - of-   contextclassificationexperimentpresentedinsec-   tion5.1,thistimeaddingwordvectorfeatures . We   experiment with both SkipGram embeddings and   embeddingsextractedfromtheCNNsequencetag-   gingmodel(withoutswapping ) . Embeddingsfrom   the tagging model are expected to perform better   ontheclassificationtask , sincetheyareoptimized   to detect words contained in EEs in the attested   order . On the other hand , embeddings separately   trainedviaSkipGramaremoreâ€œpure,â€astheyonly   capture the distributional semantics of the words   withoutadditionalinformation .   ResultsTable6shows the classification accura-   cies using word embedding features , as well as   wordembeddingcombinedwithone - hotphoneme   features . Weobservethatembeddingstrainedwith   thetaggerindeedperformbetterthanthosetrained   viaSkipGram . Whatissurprisingisthatusingem-   bedding features from the tagger alone produces   a classification accuracy comparable to using all   phonemes ( 88 % ) . Moreover , an even higher ac-   curacy can be achieved by combining phoneme   features with embeddings from the tagger . This   suggests that EE ordering in Hmong can be pre - Data All Unique   focalconstituent(tone ) 89.1 % 85.4 %   allconstituents 96.7 % 88.3 %   wv - sg 94.4 % 71.1 %   wv - tagger 96.4 % 88.3 %   allconstituents+wv - sg 96.6 % 88.8 %   allconstituents+wv - tagger 97.1 % 93.8 %   dictedfromtwo independent butmutuallyreinforc-   ingroutes , namelyphonologyandlexicaldistribu-   tion . Eithermethodaloneisagoodpredictorofthe   ordering , butcombiningthetwoachievesthebest   accuracy , because the two routes each offer addi-   tionalinformationthatareimportantinpredicting   theorderingofHmongEEs .   Visualization of Feature Importance Withato-   tal360featuresfrombothphonemesandwordvec-   tors , we can visualize which features the model   find the most important in this classification task   byexaminingtheweightslearnedbythemodel .   Figure3shows the proportion of feature types   thathavethehighestimportancewhenvaryingthe   number of features ( ğ‘˜ ) . We see that when ğ‘˜is   small , the model overwhelmingly uses phoneme   features ( especially tones ) to perform classifica-   tion . The test accuracy is impressively 84 % with   only 12 features â€“ nearly 40 % of which are tonal   features . As ğ‘˜increases , wordembeddingfeatures   starttogainimportance , andthetestaccuracycan   be further improved when word embeddings are   incorporated . By the time when ğ‘˜reaches 200 ,   theproportionofeachfeaturetypebecomesimilar   to the natural proportion before selection ( dashed   lines ) .   6 Discussion   In this paper , we set out to explore the ways that   the order of words in EEs and CCs in Hmong ,   Lahu , and Chinese can be learned by computa-   tionalmodels . Motivatedbyearlierlinguistsâ€™find-3663   ings , we first use phonological features alone to   discriminate between attested and unattested or-   dersofwords . Wefindthatinthecaseofallthree   languages , the order of words can indeed be pre-   dicted phonologically , and the â€œ phonetically un-   naturalâ€hierarchiesdopredicttheorderingofEEs   and CCs . Furthermore , a decision tree classifier   is able to learn more - or - less the same hierarchies ,   suggestingthatspeakersofthoselanguagescould   inprinciplelearnthelinearhierarchiesthroughex-   posure to the language , and use these hierarchies   todecideonthecorrectorderofwordsinEEsand   CCs . Thesefindingsprovidepositiveevidencefor   hypotheses1â€“3fromSection 3 . Wethenexplored   the ways models can utilize context and distribu-   tional patterns of words to learn the orders in the   sequence tagging experiments , and we were not   able to find evidence for hypothesis 4 . We were   surprisedtofindthatmodelscanperformwellus-   ingonlywordfeatures , andthataddingphonemes   tothefeaturesetdoesnothelpatall .   The seemingly contradictory results of our in-   vestigationpointinaninterestingdirection . Infor-   mation on which a model could rely to learn the   ordering of these constructions is present redun-   dantlyinphonology(ontheonehand)andinlexi-   calanddistributionalpatterns(ontheother ) . When   allowed to cooperate on a level playing field , em-   beddings and phonology - based features both con - tributetotheidentificationofwell - formedEEsat   a similar level . In other words , while it is possi-   ble that language users mayuse phonological hi-   erarchieslikethoseproposedin Mortensen ( 2006 )   toselectappropriateordersforEEsandCCs , itis   clearly not the case that they must(though they   willperformabitbetteriftheydo ) . Thesephono-   logical hierarchies may have been more order-   defining in the history of the languages , but as   thesequencetaggingexperimentshavesuggested ,   they may also have become fossilized in the lex-   icon and in distributional patterns in the modern   form . Many times , a ( ğµ , ğµ)pair appears abun-   dantlyinmultiplesEEs(as ğ‘‹ğµğ‘‹ğµ),asaCC(as   ğµğµ),orinother â€” morecomplicated â€” discourse   patterns in the same order , so that language users   couldlearnwhetheragivenwordtendstoappearin   the ğµor ğµposition . Ifataggingmodelcanlearn   a word representation that distinguishes between   ğµand ğµ,languageusersmaydothesame .   In a sense , these results should be pleasing   to both the â€œ structure â€ ( Mortensen , Hale , Reiss )   and the â€œ substance â€ ( Hayes , Flemming , Steriade )   camps . They show , once again , that generaliza-   tions about sounds can be robust but phonetically   arbitrary . However , they leave open the possibil-   itythattherelevantsynchronicgeneralizationsare   notactuallyphonological .   7 Future Directions   We have shown two independent routes , namely   phonologyandlexicaldistribution , bywhichcom-   putationalmethodscanpredicttheorderofwords   in Hmong EEs . A language user could probably   dothesame , relyingonbothroutestosomedegree   whentheyneedtoselecttheorderofwordsinEEs .   However , thereisnowaytoknowforsurewithout   conductingapsycholinguisticexperimentwithna-   tive speakers , which would shed light on whether   any of the modeling actually translates to human   cognition . TheChineseandLahucasesalsoraise   interesting questions for future work : does the   sametwo - routemechanismworkforEEsandCCs   in these languages as well ? Answering this ques-   tionwillrequireadditionaldatacollectionandan-   notation , butwillshedsignificantlightonthisthe-   oreticallyimportantissue .   References36643665   A Trained Decision Trees   Figures4,5,6showwhatthetraineddecisiontree   lookslikeinthethreelanguages . Ineachtreenode ,   the top half of the box show the current majority   class , attested(ATT)orunattested(FAKE),aswell   as the number of votes . The bottom half of the   box shows the variable to branch on . As noted   in the main text , a linear ordering can be induced   fromthetreebyfollowingthebranches . Takethe   Hmong tree ( Figure 4 ) as an example . The first   factortosplitonis whether ğµhas the j tone , and   if the answer is yes , the majority of words are at-   tested ( 255 attested vs 15 unattested / fake ) . This   suggests that jhas a strong tendency to occur in   the ğµposition , sinceitisthemostdistinguishing   factortospliton . Hence jcanbeplacedasthefirst   toneonthescale . If ğµdoesnothavethe jtone , the   next question to ask is whether ğµhas the b tone .   Sincea yesansweragainleadstomajorityattested   words ( 273 attested vs 61 unattested / fake ) , bcan   beplacedsecondonthescale . Thenextthreeques-   tionstoaskconcernwiththe ğµword . Sincea yes   answerleadstoattestedwordsinallthreecases , it   suggeststhat âˆ…,gandshaveatendencytoappear   inthe ğµposition , hencetheycanbeplacedonthe   endofthescaleinthatorder . Thenexttwofactors   concernwiththe jandbtones , whichhavealready   been placed on the scale , so we skip them . This   processoffollowingtheleftchild(the nobranch )   and placing tones at either end of the scale is re-   peatedlyapplied , yieldingtheinducedlinearscales   showninTable 4.36663667B Implementation Details   B.1 Data   TheHmongcorpusconsistsof740ksentenceswith   a positive rate of around 3.1 % ( i.e. 96.9 % of sen-   tences contain no EEs ) . The EEs are randomly   split into disjoint train and val / test sets with ap-   proximateratiosof91%/4.5%/4.5 % . Toreducethe   possibilitythatcertainsplitsareeasierthanothers ,   threesuchsplitsareindependentlyproduced . The   positive sentences are split into train and val / test   sets according to the EE partitions , and the nega-   tivesentencesaresplitwithapproximateratiosof   91%/4.5%/4.5 % .   B.2 Models   The sequence tagging model consists of a feature   extractor followed by a fully connected layer to   predict the tags : { B , I , O}in the unswapped case   and { B , B - fake , I , I - fake , O } in the swapped   classificationexperiments . Twofeatureextractors   are used : 1 ) an LSTM with bidirectional encod-   ing , and2)aCNN , consistingoffourlayersof1D   convolution , ReLU , Dropout , and BatchNorm .   Whencharacterorphonemelevelfeaturesareused ,   thecharacterembeddingsgothroughaCharCNN   before being concatenated with the word embed-   ding . Details on model configuration is shown in   Table7 . TheLSTMmodelcontainsapproximately   1.4 M parameters and the CNN contains approxi-   mately 1.7 M parameters . Our code is based on   NCRF++ ( YangandZhang , 2018 ) .   Hyperparameter Value   Wordembeddim 100   Charembeddim 30   LSTMhiddendim 100   CNNhiddendim 200   CNNkernelsize 3   CharCNNhiddendim 50   CharCNNkernelsize 3   Dropoutprobability 0.53668B.3 Training and Decoding   Themodelistrainedwithcrossentropylossusing   an SGD optimizer with momentum . Early stop-   ping is used on the F1 score of the validation set ,   with a patience of 10 epochs . During training ,   negative sentences in the training set are down-   sampled to 90 % ( resampled every epoch ) instead   of97%,whichleadsto3xfastertrainingtimebut   minimal impact on performance . Validation and   testsetsareusedintheirentirety . Traininghyper-   parameters are shown in Table 8 . Training typi-   callytakeslessthan2hourstocompleteonasingle   GeForceRTX2080TiGPU .   Hyperparameter Value   Batchsize 64   Learningrate 0.02   SGDmomentum 0.9   Earlystoppingpatience 10   C Confusion Matrices   Figure7showstheconfusionmatricesfortheswap   classification experiments . As mentioned in the   maintext , anin - contextclassificationaccuracycan   be calculated from the tokens that are correctly   identifiedaspartofanEEbutmayormaynothave   a correct prediction of the orders ( i.e. confuses B   with B - fake ) . For example , the in - contextclassi-   ficationaccuracyfortheCNNconfusionmatrixis   ğ‘ğ‘ğ‘ = 439 + 447   439 + 447 + 4= 99.55%3669