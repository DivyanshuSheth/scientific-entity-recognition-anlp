  Kexin Wang , Nandan Thakur , Nils Reimers , Iryna GurevychUbiquitous Knowledge Processing Lab , Technical University of DarmstadtUniversity of Waterloo , Hugging Face   Abstract   Dense retrieval approaches can overcome the   lexical gap and lead to significantly improved   search results . However , they require large   amounts of training data which is not avail-   able for most domains . As shown in previous   work ( Thakur et al . , 2021b ) , the performance   of dense retrievers severely degrades under a   domain shift . This limits the usage of dense re-   trieval approaches to only a few domains with   large training datasets .   In this paper , we propose the novel unsuper-   vised domain adaptation method Generative   Pseudo Labeling ( GPL ) , which combines a   query generator with pseudo labeling from a   cross - encoder . On six representative domain-   specialized datasets , we find the proposed GPL   can outperform an out - of - the - box state - of - the-   art dense retrieval approach by up to 9.3 points   nDCG@10 . GPL requires less ( unlabeled ) data   from the target domain and is more robust in   its training than previous methods .   We further investigate the role of six recent   pre - training methods in the scenario of domain   adaptation for retrieval tasks , where only three   could yield improved results . The best ap-   proach , TSDAE ( Wang et al . , 2021 ) can be   combined with GPL , yielding another average   improvement of 1.4 points nDCG@10 across   the six tasks . The code and the models are   available .   1 Introduction   Information Retrieval ( IR ) is a central component   of many natural language applications . Tradition-   ally , lexical methods ( Robertson et al . , 1994 ) have   been used to search through text content . However ,   these methods suffer from the lexical gap ( Berger   et al . , 2000 ) and are not able to recognize synonyms   and distinguish between ambiguous words . Recently , information retrieval methods based   on dense vector spaces have become popular to   address these challenges . These dense retrieval   methods map queries and passagesto a shared ,   dense vector space and retrieve relevant hits by   nearest - neighbor search . Significant improvement   over traditional approaches has been shown for   various tasks ( Karpukhin et al . , 2020 ; Xiong et al . ,   2021 ) . This method is also adapted increasingly   by industry to enhance the search functionalities   of various applications ( Choi et al . , 2020 ; Huang   et al . , 2020 ) .   However , as shown in Thakur et al . ( 2021b ) ,   dense retrieval methods require a large amount   of training data to work well . Most importantly ,   dense retrieval methods are extremely sensitive to   domain shifts : Models trained on MS MARCO   perform rather poorly for questions for COVID-19   scientific literature ( Wang et al . , 2020 ; V oorhees   et al . , 2021 ) . The MS MARCO dataset was created   before COVID-19 , hence , it does not include any   COVID-19 related topics and models did not learn   how to represent this topic well in a vector space .   In this work , we present Generative Pseudo La-   beling ( GPL ) , an unsupervised domain adaptation   technique for dense retrieval models ( see Figure 1 ) .   For a collection of paragraphs from the desired   domain , we use an existing pre - trained T5 encoder-   decoder to generate synthetic queries . These input   passages are viewed as the positive passages for the   generated queries . For each generated query , we   retrieve the most similar paragraphs as the neg-   ative passages using an existing dense retrieval   model . We term this step negative mining and term   these negative passages hard negatives . Finally ,   we use an existing cross - encoder to score each   ( query , passage)-pair and train a dense retrieval2345model on these generated , pseudo - labeled queries   using MarginMSE - Loss ( Hofstätter et al . , 2020 ) .   We use publicly available models for query gen-   eration , negative mining , and the cross - encoder ,   which have been trained on the MS MARCO   dataset ( Nguyen et al . , 2016 ) , a large - scale dataset   from Bing search logs combined with relevant   passages from diverse web sources . We evaluate   GPL on six representative domain - specific datasets   from the BeIR benchmark ( Thakur et al . , 2021b ) .   GPL improves the performance by up to 9.3   points nDCG@10 compared to state - of - the - art   model trained solely on MS MARCO . Compared   to the previous state - of - the - art domain - adaptation   method QGen ( Ma et al . , 2021 ; Thakur et al . ,   2021b ) , GPL improves the performance by up to   4.5 nDCG@10 points . Training with GPL is easy ,   fast , and data efficient .   We further analyze the role of six recent pre-   training methods in the scenario of domain adap-   tation for retrieval tasks . The best approach is   TSDAE ( Wang et al . , 2021 ) , that outperforms the   second best approach , Masked Language Model-   ing ( Devlin et al . , 2019 ) on average by 2.5 points   nDCG@10 . TSDAE can be combined with GPL ,   yielding another average improvement of 1.4 point   nDCG@10 .   2 Related Work   Pre - Training based Domain Adaptation . The   most common domain adaptation technique   for transformer models is domain - adaptive pre-   training ( Gururangan et al . , 2020 ) , which continues   pre - training on in - domain data before fine - tuning   with labeled data . However , for retrieval it is often   difficult to get in - domain labeled data and models   are applied in a zero - shot setting on a given corpus .   Besides Masked Language Modeling ( MLM ) ( De-   vlin et al . , 2019 ) , different pre - trained strategies   specifically for dense retrieval methods have been   proposed . Inverse Cloze Task ( ICT ) ( Lee et al . ,   2019 ) generates query - passage pair by randomly se-   lecting one sentence from the passage as the query   and the other part as the paired passage . ConDensor   ( CD ) ( Gao and Callan , 2021 ) applies MLM on top   of the CLS token embedding from the final layer   and the other context embeddings from a previous   layer to force the model to learn meaningful CLS   representation . SimCSE ( Gao et al . , 2021a ; Liu   et al . , 2021 ) passes the same input twice through   the network with different dropout masks and min - imizes the distance of the resulting embeddings ,   while Contrastive Tension ( CT ) ( Carlsson et al . ,   2021 ) passes the input through two different mod-   els . TSDAE ( Wang et al . , 2021 ) uses a denoising   auto - encoder architecture for representation learn-   ing : Words from the input text are removed and   passed through an encoder to generate a fixed - sized   embedding . A decoder must reconstruct the origi-   nal text without noise . As we show in Appendix E ,   just using these unsupervised techniques is not suf-   ficient and the resulting models perform poorly .   So far , ICT and CD have only been studied on   in - domain performance , i.e. a large in - domain la-   beled dataset is available which is used for subse-   quent supervised fine - tuning . SimCSE , CT , and   TSDAE have been only studied for unsupervised   sentence embedding learning . As our results show   in Appendix E , they do not work at all for purely   unsupervised dense retrieval .   If these pre - training approaches can be used for   unsupervised domain adaptation for dense retrieval   was so far unclear . In this work , we transfer the   setup from Wang et al . ( 2021 ) to dense retrieval   and first pre - train on the target corpus , followed   by supervised training on labeled data from MS   MARCO ( Nguyen et al . , 2016 ) . Performance is   then measured on the target corpus .   Query Generation . Query generation has been   used to improve retrieval performances . Doc2query   ( Nogueira et al . , 2019a , b ) expands passages with   predicted queries , generated by a trained encoder-   decoder model , and uses traditional BM25 lexical   search . This performed well in the zero - shot re-   trieval benchmark BeIR ( Thakur et al . , 2021b ) . Ma   et al . ( 2021 ) proposes QGen , that uses a query   generator trained on general domain data to syn-   thesize domain - targeted queries for the target cor-   pus , on which a dense retriever is trained from   scratch . As a concurrent work , Liang et al . ( 2020 )   also proposes the similar method . Following this   idea , Thakur et al . ( 2021b ) views QGen as a post-   training method to adapt powerful MS MARCO   retrievers to the target domains .   Despite the success of QGen , previous methods   only consider the cross - entropy loss with in - batch   negatives , which provides coarse - grained relevance   and thus limits the performance . In this work , we   show that extending this approach by using pseudo-   labels from a cross - encoder together with hard neg-2346   atives can boost the performance by several points   nDCG@10 .   Other Methods . Recently , Xin et al . ( 2021 ) pro-   poses MoDIR to use Domain Adversarial Training   ( DAT ) ( Ganin et al . , 2016 ) for unsupervised do-   main adaptation of dense retrievers . MoDIR trains   models by generating domain invariant represen-   tations to attack a domain classifier . However , as   argued in Karouzos et al . ( 2021 ) , DAT trains mod-   els by minimizing the distance between represen-   tations from different domains and such learning   objective can result in bad embedding space and   unstable performance . For sentiment classification ,   Karouzos et al . ( 2021 ) proposes UDALM based on   multiple stages of training . UDALM first applies   MLM training on the target domain ; and it then ap-   plies multi - task learning on the target domain with   MLM and on the source domain with a supervised   objective . However , as shown in section 5 , we find   this method can not yield improvement for retrieval   tasks .   Pseudo Labeling and Cross - Encoders : Bi-   Encoders map queries and passage independently   to a shared vector space from which the query-   passage similarity is computed . In contrast , cross-   encoders ( Humeau et al . , 2020 ) work on the con-   catenation of the query and passage and predict   a relevance score using cross - attention between   query and passage . This can be used in a re - ranking   setup ( Nogueira and Cho , 2019 ) , where the rele-   vancy is predicted for all query - passage - pairs for   a small candidate set . Previous work has shown   that cross - encoders achieve much higher perfor-   mances ( Thakur et al . , 2021a ; Hofstätter et al . ,   2020 ; Ren et al . , 2021 ) and are less prone to domain   shifts ( Thakur et al . , 2021b ) . But cross - encoders   come with an extremely high computational over-   head , making them less suited for a production set - ting . Transferring knowledge from cross - encoder   to bi - encoders have been shown previous for sen-   tence embeddings ( Thakur et al . , 2021a ) and for   dense retrieval : Hofstätter et al . ( 2020 ) predict   cross - encoder scores for ( query , positive)-pairs and   ( query , negative)-pairs and learns a bi - encoder to   predict the margin between the two scores . This has   been shown highly effective for in - domain dense   retrieval .   3 Method   This section describes our proposed Generative   Pseudo Labeling ( GPL ) method for the unsuper-   vised domain adaptation of dense retrievers . Fig-   ure 1 illustrates the idea of GPL .   For a given target corpus , we generate for each   passage three queries ( cf . Table 3 ) using an T5-   encoder - decoder model ( Raffel et al . , 2020 ) . For   each of the generated queries , we use an exist-   ing retrieval system to retrieve 50 negative pas-   sages . Dense retrieval with a pre - existing model   was slightly more effective than BM25 lexical re-   trieval ( cf . Appendix A ) . For each ( query , posi-   tive , negative)-tuple we compute the margin δ=   CE(Q , P,)−CE(Q , P)withCEthe score as   predicted by a cross - encoder , Qthe query and   P / Pthe positive / negative passage .   We use the synthetic dataset D =   { ( Q , P , P , δ)}with the MarginMSE loss ( Hof-   stätter et al . , 2020 ) for training a domain - adapted   dense retriever that maps queries and passages into   the shared vector space .   Our method requires from the target domain just   an unlabeled collection of passages . Further , we   use use pre - existing T5- and cross - encoder models   that have been trained on the MS MARCO passages   dataset .   Query Generation : To enable supervised train-2347ing on the target corpus , synthetic queries can be   generated for the target passages using a query   generator trained on a different , existing dataset   like MS MARCO . Previous work QGen ( Ma et al . ,   2021 ) used the simple MultipleNegativesRanking   ( MNRL ) loss ( Henderson et al . , 2017 ; van den   Oord et al . , 2018 ) with in - batch negativesto train   the model :   L ( θ ) =   −1   M / summationdisplaylogexp / parenleftbig   τ·σ(f(Q ) , f(P))/parenrightbig   /summationtextexp / parenleftbig   τ·σ(f(Q ) , f(P))/parenrightbig   where Pis a relevant passage for Q;σis a cer-   tain similarity function for vectors ; τcontrols the   sharpness of the softmax normalization ; Mis the   batch size .   MarginMSE loss : MultipleNegativesRanking   loss considers only the coarse relationship between   queries and passages , i.e. the matching passage is   considered as relevant while all other passages are   considered irrelevant . However , the query encoder   is not without flaws and might generate queries that   are not answerableby the passage . Further , other   passages might actually be relevant as well for a   given query , which is especially the case if training   is done with hard negatives as we do it for GPL .   In contrast , MarginMSE loss ( Hofstätter et al . ,   2020 ) uses a powerful cross - encoder to soft - label   ( query , passage ) pairs . It then teaches the dense re-   triever to mimic the score margin between the pos-   itive and negative query - passage pairs . Formally ,   L ( θ ) = −1   M / summationdisplay|ˆδ−δ|(1 )   where ˆδis the corresponding score margin of the   student dense retriever , i.e. ˆδ = f(Q)f(P)−   f(Q)f(P ) . Here the dot - product is usually   used due to the infinite range of the cross - encoder   scores .   This loss is a critical component of GPL , as it   solves two major issues from the previous QGen   method : A badly generated query for a given pas-   sage will get a low score from the cross - encoder , hence , we do not expect the dense retriever to put   the query and passage close in the vector space . A   false negative will lead to a high score from the   cross - encoder , hence , we do not force the dense   retriever to assign a large distance between the cor-   responding embeddings . In section 6.3 , we show   that GPL is a lot more robust to badly generated   queries than the previous QGen method .   4 Experiments   In this section , we describe the experimental setup ,   the datasets used and the baselines for comparison .   4.1 Experimental Setup   We use the MS MARCO passage ranking   dataset ( Nguyen et al . , 2016 ) as the data from the   source domain . It has 8.8 M passages and 532.8 K   query - passage pairs labeled as relevant in the train-   ing set . We select six representative datasets from   the BeIR benchmark as the data from the target   domain ( cf . subsection 4.2 ) . As Table 1 shows ,   a state - of - the - art dense retrieval model , achieving   an MRR@10 of 33.2 points on the MS MARCO   passage ranking dataset , performs poorly on the six   selected domain - specific retrieval datasets when   compared to simple BM25 lexical search .   We use the DistilBERT ( Sanh et al . , 2019 ) for   all the experiments . We use the concatenation of   the title and the body text as the input passage   for all the models . We use a maximum sequence   length of 350 with mean pooling and dot - product   similarity by default . For QGen , we use the de-   fault setting in Thakur et al . ( 2021b ): 1 - epoch   training and batch size 75 . For GPL , we train the   models with 140k training steps and batch size   32 . To generate queries for both QGen and GPL ,   we use the DocT5Query ( Nogueira et al . , 2019a )   generator trained on MS MARCO and generate   queries using nucleus sampling with temperature   1.0,k= 25 andp= 0.95 . To retrieve hard neg-   atives for both GPL and the zero - shot setting of   MS MARCO training , we use two dense retriev-   ers with cosine - similarity trained on MS MARCO :   msmarco - distilbert - base - v3 andmsmarco - MiniLM-   L-6 - v3 from Sentence - Transformers . The zero-   shot performance of these two dense retrievers is   available in Appendix B. We retrieve 50 negatives2348using each retriever and uniformly sample one neg-   ative passage and one positive passage for each   training query to form one training example . For   pseudo labeling , we use the ms - marco - MiniLM - L-   6 - v2cross - encoder . For all the pre - training meth-   ods ( e.g. TSDAE and MLM ) , we train the models   for 100 K training steps and with batch size 8 .   As shown in Section 6 , small corpora require   more generated queries and for large corpora , a   small down - sampled subset ( e.g. 50 K ) is enough   for good performance . Based on these findings ,   we adjust the number of generated queries per pas-   sageqand the corpus size |C|to make the total   number of generated queries equal to a fixed num-   ber , 250 K , i.e. q× |C|= 250 K . In detail , we   first set q>= 3 and uniformly down - sample   the corpus if 3× |C|>250 K ; then we calculate   q=⌈250 K /|C|⌉. For example , the qval-   ues for FiQA ( original size = 57.6 K ) and Robust04   ( original size = 528.2 K ) are 5 and 3 , resp . and   the Robust04 corpus is down - sampled to 83.3K.   QGen and GPL share the generated queries for fair   comparision .   4.2 Evaluation   As our methods focus on domain adaptation to   specialized domains , we selected six domain-   specific text retrieval tasks from the BeIR bench-   mark ( Thakur et al . , 2021b ): FiQA ( financial do-   main ) ( Maia et al . , 2018 ) , SciFact ( scientific pa-   pers ) ( Wadden et al . , 2020 ) , BioASQ ( biomedical   Q&A ) ( Tsatsaronis et al . , 2015 ) , TREC - COVID   ( scientific papers on COVID-19 ) ( Roberts et al . ,   2020 ) , CQADupStack ( 12 StackExchange sub-   forums ) ( Hoogeveen et al . , 2015 ) and Robust04   ( news articles ) ( V oorhees , 2005 ) . These selected   datasets each contain a corpus with a rather specific   language and can thus act as a suitable test bed for   domain adaptation .   The detailed information for all the target   datasets is available at Appendix C. We make   modification on BioASQ and TREC - COVID . For   efficient training and evaluation on BioASQ , we   randomly remove irrelevant passages to make the   final corpus size to 1M. In TREC - COVID , the orig-   inal corpus has many documents with a missing   abstract . The retrieval systems that were used to   create the annotation pool for TREC - COVID often   ignored such documents . This leads to a strongannotation bias on text length for these documents ,   since this dataset contains only titles and abstracts .   Hence , we removed all documents with a missing   abstract from the corpus . The evaluation results   on the original BioASQ and TREC - COVID are   available at Appendix D. Evaluation is done using   nDCG@10 .   4.3 Baselines   Zero - Shot Models : We apply supervised training   on MS MARCO or PAQ ( Lewis et al . , 2021 ) and   evaluate the trained retrievers on the target datasets .   ( a)MS MARCO represents a distilbert - base dense   retrieval model trained with MarginMSE on the   MS MARCO dataset with batch - size 75 for 70k   steps . ( b ) PAQ ( Oguz et al . , 2021 ) represents   MNRL training on the PAQ dataset . ( c ) PAQ + MS   MARCO represents MNRL training on PAQ fol-   lowed by MarginMSE training on MS MARCO . ( d )   TSDAE represents TSDAE ( Wang et al . ,   2021 ) pre - training on MS MARCO followed by   MarginMSE training on MS MARCO . ( e ) BM25   system based on lexical matching from Elastic-   search .   Previous Domain Adaptation Methods : We   include two previous unsupervised domain adapta-   tion methods , UDALM ( Karouzos et al . , 2021 ) and   MoDIR ( Xin et al . , 2021 ) . For UDALM , we apply   MLM training on the target corpus and then apply   the multi - task training of MarginMSE training on   MS MARCO and MLM training on the target cor-   pus . For MoDIR , it starts from the ANCE check-   point and apply domain adversarial training on MS   MARCO and the target dataset . As of writing , the   training code of MoDIR is not public , but domain   adapted models for 5 out of 6 datasets have been   released by the authors .   Pre - Training based Domain Adaptation : We   follow the setup proposed in Wang et al . ( 2021 )   on domain - adapted pre - training : We pre - train the   dense retrievers with different methods on the tar-   get corpus and then continue to train the mod-   els on MS MARCO with MarginMSE loss . The   pre - training methods consist of : ( a ) CD(Gao and   Callan , 2021 ) extracts the hidden representations   from an intermediate layer and applies MLM on   the CLS token representation and these extracted   hidden representations . ( b ) SimCSE ( Gao et al . ,   2021b ; Liu et al . , 2021 ) simply encode the same2349   text twice with different dropout masks in combi-   nation with MNRL loss . ( c ) CT(Carlsson et al . ,   2021 ) is similar to SimCSE but it uses two inde-   pendent encoders to encode a pair of text . ( d )   MLM ( Devlin et al . , 2019 ) uses the default set-   ting in original paper , where 15 % tokens in a text   are sampled to be masked and are needed to be   predicted . ( e ) ICT ( Lee et al . , 2019 ) uniformly   samples one sentence from a passage as the pseudo   query to that passage and uses MNRL loss on the   synthetic data . We follow the setting in Lee et al .   ( 2019 ) and masked out the selected sentence 90 %   of the time . ( f ) TSDAE ( Wang et al . , 2021 ) uses   a denoising autoencoder to pre - train the dense re-   trievers with 60 % random tokens deleted in the   input texts .   Generation - based Domain Adaptation : We   use the training scriptfrom Thakur et al . ( 2021b )   to train QGen models with the default setting . Co - sine similarity is used and the models are fine - tuned   for 1 epoch with MNRL . The default QGen is   trained with in - batch negatives . For a fair com-   parison , we also test QGen with hard negatives as   used in GPL , noted as QGen ( w/ Hard Negatives ) .   Further , We test the combination of TSDAE and   QGen ( TSDAE + QGen ) .   Re - Ranking with Cross - Encoders : We also   include results of the powerful but inefficient   re - ranking methods for reference . Three re-   trievers for the first - phrase retrieval are tested :   BM25 from Elasticsearch , the zero - shot MS   MARCO retriever and the GPL retriever en-   hanced by TSDAE pre - training . We use the cross-   encoder ms - marco - MiniLM - L-6 - v2 from Sentence-   Transformers , which is also the same model used   for pseudo labeling in GPL .   5 Results   Pre - Training based Domain Adaptation:2350The results are shown in Table 1 . Compared   with the zero - shot MS MARCO model , TSDAE ,   MLM and ICT can improve the performance if we   first pre - train on the target corpus and then perform   supervised training on MS MARCO . Among them ,   TSDAE is the most effective method , outperform-   ing the zero - shot baseline by 4.0 points nDCG@10   on average . CD , CT and SimCSE are not able to   adapt to the domains in a pre - training setup and   achieve a performance worse than the zero - shot   model .   To ensure that TSDAE actually learns domain   specific terminology , we include TSDAE   in our experiments : Here , we performed TSDAE   pre - training on the MS MARCO dataset followed   by supervised learning on MS MARCO . This   performs slightly weaker than the zero - shot MS   MARCO model .   We also tested the pre - training methods without   any supervised training on MS MARCO . We find   all of them fail miserably compared to the zero - shot   baseline as shown in Appendix E .   Previous Domain Adaptation Methods : We   test MoDIR on the datasets except Robust04 .   MoDIR performs on - par with our zero - shot MS   MARCO model on FiQA , TREC - COVID and   CQADupStack , while it performs much weaker on   SciFact and BioASQ . An improved training setup   with MoDIR could improve the results .   We also test UDALM , which first does MLM   pre - training on the target corpus , and then runs   multitask learning with MLM objective and super-   vised training on MS MARCO . The results show   that UDALM in this case greatly harms the perfor-   mance by 12.2 points in average , when compared   with the MLM - pre - training approach . We suppose   this is because unlike text classification , the dense   retrieval models usually do not have an additional   task head and the direct MLM training conflicts   with the supervised training .   Generation - based Domain Adaptation : The   results show that the previous best method , QGen ,   can successfully adapt the MS MARCO models   to the new domains , improving the performance   on average by 3.6 points . It performs on par with   TSDAE - based domain - adaptive pre - training . Com-   bining TSDAE with QGen can further improve the   performance by 1.5 points .   When using QGen with hard negatives insteadof random in - batch negatives , the performance de-   creases by 2.5 points in average . QGen is sensitive   to false negatives , i.e. negative passages that are   actually relevant for the query . This is a common   issue for hard negative mining . GPL solves this   issue by using the cross - encoder to determine the   distance between the query and a passage . We give   more analysis in section 7 .   Generative Pseudo Labeling ( GPL , proposed   method ): We find GPL is significantly better on   almost all the datasets compared to other tested   methods , outperforming QGen by up to 4.5 points   ( on BioASQ ) and in average by 2.7 points . One   exception is TREC - COVID , but as this dataset has   just 50 test queries , this difference can be due to   noise .   As a further enhancement , we find that TSDAE-   based domain - adaptive pre - training combined with   GPL ( i.e. TSDAE + GPL ) can further improve the   performance on all the datasets , achieving the new   state - of - the - art result of 52.9 nDCG@10 points   in average . It outperforms the out - of - the - box MS   MARCO model 7.7 points on average .   For the results of GPL on the full 18 BeIR   datasets , please refer to Appendix D. The observa-   tions remain the same .   Re - ranking with Cross - Encoders : Cross-   encoders perform well in a zero - shot setting and   outperform dense retrieval approaches significantly   ( Thakur et al . , 2021b ) , but they come with a sig-   nificant computational cost at inference . TSDAE   and GPL can narrow but not fully close the perfor-   mance gap between the single - stage retrievers and   the re - ranking methods . Due to the much lower   computational costs at inference , the TSDAE +   GPL model would be preferable in a production   setting . For example , as shown in Thakur et al .   ( 2021b ) , the retrieval latency on a 1M - sized corpus   for one query is 14ms and 450ms for dense retriev-   ers ( with the same backbone as ours ) and BM25 +   CE reranking , resp .   6 Analysis   In this section , we analyze the influence of training   steps , corpus size , query generation and choices of   starting checkpoints on GPL .   6.1 Influence of Training Steps   We first analyze the influence of the number of   training steps on the model performance . We eval-   uate the models every 10 K training steps and end2351   the training after 140 K steps . The results for the   change of averaged performance on all the datasets   are shown in Figure 2 . We find the performance of   GPL begins to be saturated after around 100 K steps .   With the TSDAE pre - training , the performance can   be improved consistently during the whole train-   ing stage . For reference , training a distilbert - base   model for 100k steps takes about 9.6 hours on a   single V100 GPU .   6.2 Influence of Corpus Size   We next analyze the influence of different corpus   sizes . We use Robust04 for this analysis , since it   has a relatively large size . We sample 1 K , 10 K ,   50 K and 250 K passages from the whole corpus in-   dependently to form small corpora and train QGen   and GPL on the same small corpus . The results are   shown in Table 2 . We find with more than 10 K pas-   sages , GPL can already significantly outperform   the zero - shot baseline by 2.4 NDCG@10 points ;   with more than 50 K passages , the performance be-   gins to saturate . On the other hand , QGen falls   behind the zero - shot baseline for each corpus size .   6.3 Robustness against Query Generation   Next , we study how the query generation influences   the model performance . First , we train QGen and   GPL on SciFact , FiQA and Robust04 , with 1 up to   50 generated Queries Per Passage ( QPP ) . The re-   sults are shown in Table 3 . We observe that smaller   corpora , e.g. SciFact ( size = 5.2 K ) and FiQA ( size   = 57.6 K ) require more generated queries per pas-   sage than the large one , Robust04 ( size = 528.2 K ) .   For example , GPL needs QPP equal to around 50 ,   5 and 1 for SciFact , FiQA and Robust04 , resp . to   achieve the optimal performance .   The temperatureplays an important role in nu-   cleus sampling , higher values make the generated   queries more diverse , but of lower quality . We   train QGen and GPL on FiQA with different tem-   peratures : 0.1 , 1 , 1.3 , 3 , 5 and 10 . Examples of   generated queries are available in Appendix F. We   generated 3 queries per passage . The results are   shown in Figure 3 . We find the performance of   QGen and GPL both peaks at 1.0 . With a higher   temperature , the next - token distribution will be flat-   ter and more diverse queries , but of lower quality ,   will be generated . With high temperatures , the gen-   erated queries have nearly no relationship to the   passage . QGen will perform poorly in these cases ,   worse than the zero - shot model . In contrast , GPL   performs still well even when the generated queries   are of such low quality .   6.4 Sensitivity to Starting Checkpoints   We also analyze the influence of initialization   on GPL . In the default setting , we start from a   distilbert - model supervised on MS MARCO us-   ing MarginMSE loss . We also evaluate to directly   fine - tune a distilbert - model using QGen , GPL and   TSDAE + GPL . The performance averaged on all   the datasets is shown in Table 4 . We find the MS   MARCO training has relatively small effect on the   performance of GPL ( with 0.3 - point difference in   average ) , while QGen highly relies on the choice2352   of the initialization checkpoint ( with 1.9 - point dif-   ference in average ) .   7 Case Study : Fine - Grained Labels   GPL uses continuous pseudo labels from a cross-   encoder , which can provide more fine - grained in-   formation and is more informative than the simple   0 - 1 labels as in QGen . In this section , we give a   more detailed insight into it by a case study .   One example from FiQA is shown in Table 5 .   The generated query for the positive passage asks   for the definition of “ futures contract ” . Negative   1 and 2 only mention futures contract without ex-   plaining the term ( with low GPL labels / scores be-   low 2.0 ) , while Negative 3 gives the required def-   inition ( with high GPL label / score 8.2 ) . As an   interesting case , Negative 4 gives a partial explana-   tion of the term ( with medium GPL label / score 6.9 ) .   GPL assigns suitable fine - grained labels to differ-   ent negative passages . In contrast , QGen simply   labels all of them as 0 , i.e. as irrelevant . Such differ-   ence explains the advantage of GPL over QGen and   why using hard negatives harms the performance   of QGen in Table 1 .   8 Conclusion   In this work we propose GPL , a novel unsuper-   vised domain adaptation method for dense retrieval   models . It generates queries for a target corpus and   pseudo labels these with a cross - encoders . Pseudo-   labeling overcomes two important short - comings   of previous methods : Not all generated queries are   of high quality and pseudo - labels efficiently detects   those . Further , training with mined hard negatives   is possible as the pseudo labels performs efficient   denoising .   In this work , we also evaluated different   pre - training strategies in a domain - adaptive pre-   training setup : We first pre - trained on the target   domain , then performed supervised training on MS   MARCO . ICT and MLM were able to yield a small   improvement ( by < = 1.5 nDCG@10 points on aver-   age ) , while TSDAE was able to yield a significant   improvement of 4 nDCG@10 points on average .   Other approaches degraded the performance .   Acknowledgments   This work has been funded by the German Re-   search Foundation ( DFG ) as part of the UKP-   SQuARE project ( grant GU 798/29 - 1).2353References235423552356A Performance of Using Different Retrievers for Negative Mining in GPL   The performance of using different retrievers ( BM25 , dense , BM25 + dense and single dense retrievers )   for mining hard negatives in GPL is shown in Table 6 . The results show GPL performs best when using   hard negatives mined by both the two dense retrievers .   B Performance of the Zero - Shot Retrievers in Hard - Negative Mining   The performance of directly using the zero - shot retrievers for hard - negative mining in GPL is shown in   Table 7 . Compared with the strong baseline ( MS MARCO in Table 7 ) trained with MarginMSE , msmarco-   distilbert - base - v3 andmsmarco - MiniLM - L-6 - v3 are much worse in terms of zero - shot generalization on   each dataset . This comparison supports GPL can indeed train powerful domain - adapted dense retrievers   with minimum reliance on choices of the retrievers for hard - negative mining .   C Target Datasets   FiQA is for the task of opinion question answering over financial data . It contains 648 queries and 5.8 K   passages from StackExchange posts under the Investment topic in the period between 2009 and 2017 . The   labels are binary ( relevant or irrelevant ) and there are 2.6 passages in average labeled as relevant for each   query .   SciFact is for the task of verifying scientific claims using evidence from the abstracts of the scientific   papers . It contains 300 queries and 5.2 K passages built from S2ORC ( Lo et al . , 2020 ) , a publicly - available   corpus of millions of scientific articles . The labels are binary and there are 1.1 passages in average labeled   as relevant for each query .   BioASQ is for the task of biomedical question answering . It originally contains 500 queries and 15 M   articles from PubMed . The labels are binary and it has 4.7 passages in average labeled as relevant for   each query . For efficient training and evaluation , we randomly remove irrelevant passages to make the   final corpus size to 1M.   TREC - COVID is an ad - hoc search challenge for scientific articles related to COVID-19 based on the   CORD-19 dataset ( Wang et al . , 2020 ) . It originally contains 50 queries and 171 K documents . The original   corpus has many documents with only a title and an empty body . We remove such documents and the2357final corpus size is 129.2K. The labels in TREC - COVID are 3 - level ( i.e. 0 , 1 and 2 ) and there are 430.8   passages in average labeled as 1 or 2 in the clean - up version .   CQADupStack is a dataset for community question - answering , built from 12 StackExchange subfo-   rums : Android , English , Gaming , Gis , Mathematica , Physics , Programmers , Stats , Tex , Unix , Webmasters   and WordPress . The task is to retrieve duplicate question posts with both a title and a body text given   a post title . It has 13.1 K queries and 457.2k passages . The labels are binary and there are 1.4 passages   in average labeled as relevant for each query . As in Thakur et al . ( 2021b ) , the average score of the 12   sub - tasks is reported .   Robust04 is a dataset for news retrieval focusing on poorly performing topics . It has 249 queries and   528.2 K passages . The labels are 3 - level and there are in average 69.9 passages labeled as relevant for   each query .   The detailed statistics of these target datasets are shown in Table 8 .   We also evaluate the models trained in this work on the original version of BioASQ and TREC - COVID   datasets from BeIR ( Thakur et al . , 2021b ) . The results are shown in Table 9 .   D Results on full BeIR   We also evaluate the models on all the 18 BeIR datasets . We include DocT5Query ( Nogueira et al . , 2019a ) ,   the strong baseline based on document expansion with the T5 query generator ( also used in GPL for query   generation ) + BM25 ( Anserini ) . We also include the powerful zero - shot model TAS - B ( Hofstätter et al . ,   2021 ) , which is trained on MS MARCO with advanced knowledge - distillation techniques into comparison .   Viewing TAS - B as the base model and also the negative miner , we apply QGen and GPL on top of them ,   resulting in TAS - B + QGen andTAS - B + GPL , resp .   The results are shown in Table 9 . We find both DocT5Query and BM25 ( Anserini ) outperform   MS MARCO , TSDAE and QGen , in terms of both average performance and average ( performance )   rank . QGen struggles to beat MS MARCO , the zero - shot baseline and it even significantly harms the   performance on many datasets , e.g. TREC - COVID , FEVER , HotpotQA , NQ . Thakur et al . ( 2021b ) also   observes the same issue , claiming that the bad generation quality on these corpora is the key to the failure   of QGen . On the other hand , GPL significantly outperforms these baselines above , achieving average   performance rank 5.2 and can consistently improve the performance over the zero - shot model on all the   datasets . For TSDAE , TSDAE + QGen and TSDAE + GPL , the conclusion remains the same as in the   main paper .   For the powerful zero - shot model TAS - B , it outperforms QGen and performs on par with TSDAE +   QGen . When building on top of TAS - B , GPL can also yield significant performance gain by up - to 21.5   nDCG@10 points ( on TREC - COVID ) and 4.6 nDCG@10 points on average . This TAS - B + GPL model   performs the best over all these retriever models , achieving the averaged performance rank equal to 3.2 .   However , when applying QGen on top of TAS - B , it can not improve the overall performance but also   harms the individual performance on many datasets , instead.2358   E Performance of Unsupervised Pre - Training   The performance of the unsupervised pre - training methods without access to the MS MARCO data is   shown in Table 10 . We find ICT is the best method , achieving highest scores on all the datasets . However ,   all the unsupervised pre - training methods can not directly yield improvement in performance compared   with the zero - shot baseline .   F Examples of Generated Queries under Different Temperatures   The generation temperature controls the sharpness of the next - token distribution . The examples for one   passage from FiQA are shown in Table 11 Higher temperature results in longer and less duplicate queries   under more risk of generating non - sense texts.23592360