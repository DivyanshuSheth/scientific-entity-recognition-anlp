  Alan Ramponi   Fondazione Bruno Kessler   Trento , Italy   alramponi@fbk.euSara Tonelli   Fondazione Bruno Kessler   Trento , Italy   satonelli@fbk.eu   Abstract   /warning_signWarning : this paper contains content that   may be offensive or upsetting .   Avoiding to rely on dataset artifacts to pre-   dict hate speech is at the cornerstone of ro-   bust and fair hate speech detection . In this pa-   per we critically analyze lexical biases in hate   speech detection via a cross - platform study ,   disentangling various types of spurious and   authentic artifacts and analyzing their impact   on out - of - distribution fairness and robustness .   We experiment with existing approaches and   propose simple yet surprisingly effective data-   centric baselines . Our results on English data   across four platforms show that distinct spuri-   ous artifacts require different treatments to ul-   timately attain both robustness and fairness in   hate speech detection . To encourage research   in this direction , we release all baseline mod-   els and the code to compute artifacts , pointing   it out as a complementary and necessary addi-   tion to the data statements practice .   1 Introduction   Hate speech in online social communities is a se-   rious and pervasive concern , which requires fair   and robust automated approaches to be tackled at   scale . However , despite the great progress in natu-   ral language processing for detecting hate speech ,   current models have shown to be brittle when ap-   plied to real - world data , exhibiting limited out - of-   distribution ( OOD ) robustness ( Vidgen et al . , 2019 )   and perpetuating and amplifying harmful social bi-   ases ( Röttger et al . , 2021 ) . Noticeably , hate speech   detection systems are typically trained on data from   limited language varieties such as individual plat-   forms , which inevitably exhibit differences in writ-   ing norms , language use , and hate targets , hamper-   ing generalization ( Vidgen and Derczynski , 2020 ) .   One of the main reasons for limited robustness   and fairness of mainstream hate speech detection   Table 1 : Posts wrongly labeled as hateful by a ﬁne-   tuned BERT classiﬁer due to the presence of spurious   lexical artifacts ( identity and nonidentity - related ) and   their negative impact (  ) on fairness and robustness .   systems is largely ascribable to spurious statisti-   cal correlations between surface lexical items and   labels in training data , which models exploit to   derive predictions . These biases are commonly   referred to as lexical dataset artifacts , and have   recently attracted attention in the NLP community ,   particularly in natural language inference ( NLI )   studies ( Belinkov et al . , 2019 ; Gururangan et al . ,   2018 ; Poliak et al . , 2018 , inter alia ) . Efforts to   tackle the issue in hate speech detection are instead   rather scattered , and mainly focus on fairness us-   ing datasets from few platforms ( Zhou et al . , 2021 ;   Kennedy et al . , 2020b , inter alia ) , leaving the study   on OOD robustness largely unexplored . We instead   argue that fairness and robustness are strongly in-   tertwined aspects ( Table 1 ) , and thus should be   studied jointly , with the goal to understand to what   extent these two dimensions are related .   Previous work has shown that state - of - the - art   models overly rely on identity words ( e.g. , “ jews ” ,   “ gay ” ) to predict hateful content ( Zhou et al . , 2021 ;   Kennedy et al . , 2020b , inter alia ) , further de-   moting voices of people from already marginal-   ized groups ( Bender et al . , 2021 ) . However , non   identity - related lexical items – such as “ sport ” , “ an-   nouncer ” , and “ football ” in Waseem and Hovy   ( 2016 ) – are also often spuriously associated with   hate speech due to a biased data collection pro-   cess ( Wiegand et al . , 2019 ) , undermining OOD   robustness . Despite the recent trend in minimizing3027topic bias in data sampling , we show that some spu-   rious lexical artifacts still remain highly - predictive   on certain distributions even if data has been sam-   pled in a more attentive fashion ( e.g. , artifacts that   are potentially data- or platform - speciﬁc – Figure 1 ,   highlighted in gray ) .   We argue that disentangling artifacts into ﬁne-   grained categories by means of a cross - platform   analysis may be beneﬁcial to drive a broader de-   biasing of current hate speech models , ultimately   improving both fairness and robustness to out - of-   distribution data . To this purpose , we critically   analyze artifacts in hate speech detection across   multiple platforms and propose simple yet effective   data - centric baselines exploiting spurious lexical   items . We show that although we achieve substan-   tial improvements in OOD fairness by exploiting   spurious identity - related artifacts , this comes at the   cost of robustness . This conﬁrms that fairness and   robustness are strictly interrelated dimensions that   should be studied together in future research .   Contributions To the best of our knowledge , we   are the ﬁrst to ( i ) conduct a thorough investigation   of lexical artifacts across online platforms ; ( ii ) dis-   entangle artifacts into ﬁne - grained categories ; and   ( iii ) propose a viable data - centric approach based   on masking that consistently improves fairness over   allbaselines across allplatforms . To foster future   research on the topic , we also release ( iv ) code to   reproduce all experiments , and ( v ) disaggregated   lexical artifact annotations , more broadly ( vi ) sug-   gesting the inclusion of dataset artifacts in data   statements ( Bender and Friedman , 2018 ) , which   can be easily revealed using our codebase .   2 Lexical Artifacts are notall the Same   We conceptualize dataset artifacts at the lexical   level as emergent correlations between tokens and   labels in input data , consistently to lexical annota-   tion artifacts in NLI ( Gururangan et al . , 2018 ) . As   such , given a target class c , we formally deﬁne lexi-   cal artifactsLas the set of highly - discriminating   tokens forc , which comprise authentic artifacts   A – items that potentially carry useful information   for the class at hand – and spurious artifacts S   – items that are spuriously ( or undesirably ) associ-   ated to the target class – such that L = A∪S. In   the context of hate speech detection , we consider   the hateful class as cunless otherwise speciﬁed and   simplify the notation ( i.e. , from “ · ” to “ · ” ) .   We build our deﬁnitions on top of the categories   of lexical biases by Zhou et al . ( 2021 ) , which origi-   nally identify three bias groups : i)minority identity   mentions which are not offensive , ii)minority iden-   tity mentions which are potentially offensive , and   iii)non - identity mentions which are possibly offen-   sive . We enrich this categorization by introducing   a high - level separation into spurious ( i.e. , group i )   in Zhou et al . ( 2021 ) ) and authentic artifacts ( i.e. ,   group ii)andiii ) ) , and including an additional spu-   rious , non identity - related category ( Section 2.2 ) .   Indeed , given the broad nature of authentic and   spurious artifacts , we further categorize them in   Section 2.1 and 2.2 ( see Figure 1 for an overview ) .   2.1 Authentic artifacts   We deﬁne authentic lexical artifacts Aas the sub-   set of highly - discriminating tokens which poten-   tially convey hatefulness , profanity , or are other-   wise frequently associated with hateful contexts .   Intuitively , Ais the set of artifacts which is likely to   be informative to detect hate speech across distribu-   tions . Authentic artifacts enclose minority identity-   related artifactsAand non - identity artifacts A.   Identity - related ( A)Potentially offensive or   stereotyping terms towards minority identities ( e.g. ,   “ n*gro ” , “ f*ggot ” , “ k*ke ” , “ wh*re ” ) , as well as re-   claimed slurs ( e.g. , “ n*gga ” ) ( Figure 1 , top left ) .   Non - identity related ( A)Swear words and   profanities ( e.g. , “ f*ck ” , “ sh*t ” ) as well as broad3028terms typically associated with hateful contexts   ( e.g. , “ kill ” , “ idiots ” ) ( Figure 1 , bottom right ) .   2.2 Spurious artifacts   Spurious lexical artifacts Sbroadly enclose all to-   kens which we do not expect to be predictive for   the target class at hand . As such , we postulate that   those artifacts are a main reason for insufﬁcient   robustness and fairness of current hate speech de-   tectors , and thus may play a positive role in lexical   debiasing . We speciﬁcally focus on these artifacts   in our experiments . As for authentic artifacts , spu-   rious items can be grouped into minority identity-   related artifactsSand non - identity artifacts S ,   the latter being currently disregarded in research   investigating fairness only ( Zhou et al . , 2021 ) .   Identity - related ( S)Terms describing a social   minority , which are typically associated to hate   speech due to their frequency on offensive state-   ments on online fora ( e.g. , “ muslim ” , “ woman ” ,   “ Islam ” , “ nigerian ” , “ LGBT ” ) ( Figure 1 , top right ) .   Non - identity related ( S)All non - identity to-   kens which are unexpectedly associated to hate   speech , e.g. , due to platform - speciﬁcity , bias in   collection timeframe , etc . ( e.g. , “ people ” , “ RT ” ,   “ streets ” , “ Trump ” , “ yeah ” ) ( Figure 1 , bottom left ) .   3 Data   In this work we focus on hate speech , i.e. , messages   whose content spreads hatred or incites violence ,   or threatens people ’s freedom , dignity and safety ,   and whose target is a protected group , or an indi-   vidual targeted for belonging to such a group and   not for his / her individual characteristics ( Poletto   et al . , 2021 ) . Hate speech typically encompasses   serious cases of offense with severe moral and legal   implications , i.e. , those cases that are of primary   importance for content moderation .   We collect hate speech corpora that meet the fol-   lowing criteria : ( i ) they minimize topic and author   biases in data collection ( Wiegand et al . , 2019 ) , us-   ing alternatives to keyword and user searches such   as pure or boosted random sampling , ( ii ) they per-   tain to different social media platforms , and ( iii )   they follow similar annotation guidelines , where   hate speech is clearly deﬁned and separated from   other types of offensive language . For each corpus   we create hateful andnon - hateful examples . All   datasets follow consistent preprocessing , deduplica-   tion , and anonymization ( Appendices A.1 and A.2).R ( /f1a1)We use the recently introduced   Reddit dataset ( v1.1 ) by Vidgen et al . ( 2021 ) which   preserves a variety of grammar , topic , and style fea-   tures due to a community - based sampling approach .   The corpus contains 27,494 entries annotated fol-   lowing a hate speech taxonomy comprising abu-   sive ( identity - directed , afﬁliation - directed , person-   directed ) and non - abusive labels ( non - hateful slurs ,   counter speech , and neutral ) . We follow the widely   accepted deﬁnition of hate speech as “ abuse target-   ing a protected group or its members for being a   part of that group”(Röttger et al . , 2021 ; Banko   et al . , 2020 ; Vidgen et al . , 2019 , inter alia ) to create   thehateful label from identity - directed examples ,   and the non - hateful label from the remaining exam-   ples . For the purpose of this study , we discard in-   stances marked as requiring previous content to be   interpreted . The ﬁnal dataset after preprocessing   consists of 1,688 hateful and 19,888 non - hateful   examples , for a total of 21,576 unique instances .   T ( /twitter)We select a widely used hate   speech dataset which has been collected follow-   ing a bootstrap random sampling approach ( Founta   et al . , 2018 ) . The dataset consists of 99,996 tweets   annotated as hateful , abusive , spam , and normal .   Similarly to previous work , we discard the spam   category ( Zhou et al . , 2021 ) , forming the hateful   class following the original classiﬁcation provided   by the authors . This led to 3,937 hateful and 70,554   non - hateful examples , for a total of 74,491 tweets .   G ( ) We use the GAB hate corpus   by Kennedy et al . ( 2020a ) , whose data has been   sampled purely randomly due to the frequency of   hate speech of the “ free speech - preserving ” ( Zan-   nettou et al . , 2018 ) GAB social network . The cor-   pus ( v.2021 - 03 - 03 ) consists of 27,546 posts anno-   tated with ( assault on ) human dignity , call for vio-   lence , and vulgarity / offensive labels . Similarly to   previous work , we take the union of human dig-   nity andcall for violence labels for the hateful   class ( Kennedy et al . , 2020b ) , whereas we create   thenon - hateful class from the remaining examples .   We also leverage target annotations and consider   messages towards ideology / political groups as non-   hateful , to ensure consistency among datasets . As3029a result , the ﬁnal dataset is made up of 27,014 mes-   sages : 1,785 hateful and 24,829 non - hateful .   S ( /_595)We use the dataset pertain-   ing to a white supremacist web forum collected   by de Gibert et al . ( 2018 ) following a random sam-   pling procedure . It consists of a total of 10,944   messages with annotations for hate , no - hate , re-   lation , and skip labels . We remove relation and   skipexamples , since they require previous context ,   or they represent spam / content written in other   languages , respectively . We then use the hate exam-   ples for the hateful class , and the no - hate instances   for the non - hateful class . This led to a total of   10,448 examples , 1,192 hateful and 9,256 normal .   4 Disentangling Lexical Artifacts   In order to disentangle lexical artifacts , we ﬁrst   compute the correlation between each token and the   hateful class for each dataset ( Section 4.1 ) , then as-   sessing the cross - distribution indicativeness of each   token ( Section 4.2 ) . For segmenting texts into to-   kens , we rely on the training portion of each dataset   only ( Section 5 ) and employ the WordPiece ( Schus-   ter and Nakajima , 2012 ) subword tokenizer as used   in BERT ( Devlin et al . , 2019).Finally , we perform   lexical artifacts annotation ( Section 4.3 ) following   the categories deﬁned in Section 2 .   4.1 In - distribution artifacts   We follow Gururangan et al . ( 2018 ) and employ   pointwise mutual information ( PMI ; Fano , 1961 )   to compute the discriminativeness of each token to   the target class . Since lexical artifacts are meant   to be used for downstream debiasing , we argue that   tokens should be consistent with inputs to the end   model . As a result , we use tokens as given by the   WordPiece subword tokenizer , the same tokenizer   used by models employed in our experiments ( Sec-   tion 5 ) . Formally , given a token tand a classc , the   PMI is deﬁned as follows :   PMI(t , c ) = logp(t , c )   p(t|·)p(·|c)(1 )   We further apply reweighting to emphasize   highly - discriminative token - class correlations , and   normalize≤0values to zero since negative PMI   scores are known to be unreliable on relatively   small corpora ( Jurafsky and Martin , 2021 , Ch . 6 ) .   Discussion The top 10 tokens on each platform   that are more associated with the hateful class are   presented in Table 2 ( left ) . All platforms exhibit   a variety of lexical artifact types ( cf . Section 2 ) ;   however , we observe clear divergences across dis-   tributions . While artifacts in Stormfront data are   mainly related to race , on Gab the focus is more   on religion . Reddit and Twitter conversations are   instead more varied , with higher occurrence of spu-   rious , non - identity artifacts ( e.g. , “ RT ” , “ people ” ) .   4.2 Cross - distribution artifacts   When datasets from multiple platforms are avail-   able , we hypothesize that leveraging individual   scores makes possible to better identify artifacts .   Given PMI(t , c)the score of a token tfor the   hateful class con a given distribution d∈D   ( e.g. , platform ) , we normalize it in [ 0,1]by ap-   plying a min - max normalization function to en-   able cross - platform score comparability – obtaining   PMI(t , c ) – further applying a logtransforma-   tion to mitigate the skewness of the original PMI   distribution . As a result , the ﬁnal cross - distribution   scoreS(t , c)for each token is given by the average   of the corresponding individual scores :   S(t , c ) = 1   |D|log(PMI(t , c))(2 )   We then sort tokens by descending score , high-   lighting lexical artifacts that are highly discrim-   inating for the hateful class across distributions .   Table 2 ( right ) shows the top 10 tokens after the   cross - platform computation is carried out.3030Discussion As shown in Table 2 ( right ) , cross-   platform importance of tokens for the hateful class   demotes scores ( and thus , ranks ) of lexical artifacts   which are likely to be more indicative on some   platforms only ( e.g. , “ RT ” ) , while consolidating   the informativeness of cross - platform items ( e.g. ,   “ jews ” , “ hate ” , “ # # s ” , “ # # es”,“people ” ) . This con-   ﬁrms our hypothesis that encompassing multiple   platforms is beneﬁcial for capturing lexical items   that are likely to be predictive across distributions .   4.3 Artifacts annotation   In order to disentangle lexical artifacts for further   debiasing , we select the kmost predictive tokens   given by the cross - distribution rank of discrimina-   tiveness ( Section 4.2 ) to be manually annotated .   In our experiments , we set k= 200 as it matches   the subset of tokens which are highly informative   ( ≥0.33).Allktokens have been labeled as poten-   tially hateful and/or related to minority identities by   two annotators – male and female , ﬂuent in English   – with background in linguistics and NLP , and past   experience in hate speech activities with NGOs .   Each annotator was provided with ﬁve examples of   tokens in context for enabling more informed anno-   tation decisions , represented by randomly sampled   posts from the four platforms included in this study .   After annotation , the two annotators were in-   volved in an adjudication session in order to discuss   the cases of disagreement , followed by correction   wherever possible . We calculate the inter - annotator   agreement ( IAA ) score before and after adjudica-   tion using Cohen ’s kappa ( Cohen , 1960 ) . We ob-   tainκ= 0.6887 before andκ= 0.8311 after the   adjudication session , which is high agreement .   Discussion Although some cases of disagree-   ment were easily resolvable ( e.g. , annotation er-   rors ) , we found tokens which are difﬁcult to discern   due to ambiguity – mostly subwords – or due to   real disagreement in the interpretation of the terms .   This is in line with existing works showing that   disagreement in toxicity annotation is inherent to   the task and can not always be solved through ma-   jority voting or adjudication ( Aroyo et al . , 2019 ;   Leonardelli et al . , 2021 ) . Interestingly , this dis-   agreement follows the trend of cross - distribution   rank of artifacts ( Figure 2 ) . We decided to leave   the analysis of annotators ’ disagreement for future   work , and we release these cases as disaggregated   labels . In Table 3 we show the most informative   artifacts by type , whereas the full list of spurious   artifacts used in the experiments is in Appendix B.   5 Experiments   We investigate the impact of spurious lexical ar-   tifacts on fairness and robustness in hate speech   detection . Similarly to previous studies ( Kennedy   et al . , 2020a ; Röttger et al . , 2021 ) , we cast hate   speech detection as a binary classiﬁcation problem ,   where the two classes to be predicted are hateful   andnon - hateful , as deﬁned in Section 3 . We carry   out in - distribution and OOD experiments , namely   training and testing all models on the same or dif-   ferent platform data , respectively . We evaluate per-   formance of models using macro Fscore , whereas   for fairness we use false positive rate ( FPR ) on test   instances containing Smentions , consistently to3031   previous work ( Zhou et al . , 2021 ) .   We outline the experimental setup in Section 5.1 ,   whereas data - centric baselines are presented in Sec-   tion 5.2 . Lastly , we present results and a thorough   discussion in Section 5.3 .   5.1 Experimental setup   For all our experiments , we employ the uncased   BERT - base model ( Devlin et al . , 2019 ) as imple-   mented in the MaChAmp v0.2 toolkit ( van der Goot   et al . , 2021 ) , since it has been shown to achieve   state - of - the - art performance in hate speech detec-   tion ( Tran et al . , 2020 ) . We use default hyperparam-   eters , and perform a grid search to determine the   number of epochs , the learning rate , and the batch   size , using the search space suggested by Devlin   et al . ( 2019 ) – i.e. , [ 2 , 3 , 4 ] for epochs , [ 2e-05 , 3e-   05 , 5e-05 ] for learning rate , and [ 16 , 32 ] for batch   size . We use stratiﬁed 80 % train , 10 % develop-   ment , 10 % test splits for each dataset , selecting the   best model based on the average macro Fscore   on the development test across all platforms . Dur-   ing ﬁne - tuning , we emphasize the minority hate-   ful class using a cross - entropy loss with balanced   class weights . The ﬁnal hyperparameters are : 4 for   epochs , 2e-05 for learning rate , and 16 for batch   size . All experiments have been run on a NVIDIA   Tesla V100 - SXM2 GPU , with a training time rang-   ing from 10 to 40 minutes each . The number of   trainable parameter for all models are ≈110M.   5.2 Baselines   We investigate the impact of spurious identity-   related and non identity - related lexical artifacts on   the robustness and fairness of hate speech detection   by employing the following data - centric baselines . V We ﬁne - tune the BERT - base model on   each corpus , so that the proposed baselines can be   directly compared to a commonly employed setup .   F Swayamdipta et al . ( 2020 ) have   shown that the most ambiguous training data in-   stances promote OOD generalization while pre-   serving in - distribution performance . We thus lever-   age the V model ’s training dynamics to   ﬁlter training data to contain the 33 % most am-   biguous instances only , in line with the subset size   in Swayamdipta et al . ( 2020).Intuitively , those   are instances whose class probabilities ﬂuctuate   frequently across training epochs . We then ﬁne-   tune the BERT - base model on the resulting subset .   This setup is similar in spirit to the one employed   in Zhou et al . ( 2021 ) ; however , we assess it on data   from multiple platforms , also removing duplicate   instances ( Appendix A.2 ) which may potentially   confound the debiasing results .   R Prior to ﬁne - tuning , we naively re-   move any occurrence of spurious lexical artifacts   from training and development data . This matches   previously employed baselines for assessing fair-   ness in hate speech detection ( Kennedy et al . ,   2020b ) . However , since we are also interested in   OOD robustness , we experiment with two removal   variants : one forSand one forSartifacts . We   hypothesize that removing Stokens potentially   improves fairness , whereas removing Stokens   mostly contributes to OOD robustness .   M We propose a novel data - centric debi-   asing alternative based on token masking . Instead   of removing spurious artifacts altogether , we re-   serve a special token in the vocabulary of the model   that we use as replacement for spurious artifacts .   We then ﬁne - tune the model on the masked data .   Intuitively , this way we encourage the model to   blend all artifacts to a single contextualized rep-   resentation that will never appear during testing ,   also avoiding to redistribute the informativeness of   spurious lexical items to surrounding tokens . As   forR , we experiment with SandS   masking variants .   5.3 Results and discussion   In Table 4 we report the results for all baselines   along the in - distribution and out - of - distribution di-   mensions from the lens of fairness and robustness.3032   Since we argue that in - distribution performance is   nota reliable measure for the performance of a hate   speech detection system in the wild , due to space   constraints we here focus on the more realistic yet   more challenging out - of - distribution setup .   Filtering is not a one - size-ﬁts - all solution De-   spite the improvements in OOD generalization on   commonsense reasoning , question answering , and   NLI tasks ( Swayamdipta et al . , 2020 ) , training on   ambiguous instances collected from training dy-   namics is not as effective in hate speech detec-   tion . Instead , our results show that F   leads to mixed results for OOD fairness compared   to the V baseline . This is consistent with   results on Twitter data ( Zhou et al . , 2021 ) , and we   further conﬁrm it is the case also across platforms .   Importantly , we also notice that F has   a detrimental effect on OOD robustness , except   for two cases only ( i.e. , /twitter→/f1a1and→/_595 ) .   This indicates that hate speech detection is a nu-   anced task requiring more targeted approaches than   automated data ﬁltering . RemovingSis not as strong as it has been pre-   viously thought Removing identity terms from   data altogether is a commonly used baseline for   testing downstream fairness ( e.g. , Kennedy et al . ,   2020b ) . Indeed , our results conﬁrm that R- ( S)consistently reduces the FPR on test   instances containing Smentions compared to   theV baseline – with the only exception   of/_595→ . However , it only improves OOD ro-   bustness on→/f1a1and/_595→/twitter . Moreover , it   consistently scores lower than M ( S)on   fairness , as discussed below . This raises the ques-   tion of whether R ( S)should continue to   be used as fairness baseline in future studies .   MaskingSimproves fairness When masking   S , we notice a consistent improvement in fairness   over allapproaches , both in - distribution and out-   of - distribution , on allplatforms . Reduction in FPR   over the V baseline is as large as 3× , as   results for { /twitter;}→/_595and→/twittershow .   Most of the remaining train - test pairs show a 2×   improvement in FPR , also compared to the com-   mon R ( S)baseline . We hypothesize the   improved fairness performance with respect to R-3033 ( S)is due to the way contextualized repre-   sentations are formed during training , as discussed   in Section 5.2 . Despite being surprisingly simple ,   we envision M ( S)as a strong baseline for   future work on fairness in hate speech detection .   Sartifacts are not as useful as SWe ob-   serve that methods exploiting Sartifacts lead to   mixed results . This suggests that while a substan-   tial FPR reduction can be achieved exploiting S   artifacts , robustness calls for more complex debias-   ing strategies to transfer well across distributions .   Fairness comes at the cost of robustness Over-   all , we observe an important trade - off between   fairness and robustness . Data - centric approaches   that achieve a consistently high level of fairness –   namely , M ( S)andR ( S ) – typ-   ically show a decrease in in - distribution and out-   of - distribution performance – with the exception   of / f1a1→/f1a1and→/f1a1forM ( S ) , and→/f1a1and/_595→/twitterforR ( S ) . On one   hand , this suggests that spurious , identity - related   lexical artifacts do play an important role in perfor-   mance across distributions . On the other hand , we   believe this reﬂects the real performance of a proto-   typical model that is substantially fairer , and thus to   which future work in hate speech detection should   be compared to . We argue that M ( S)rep-   resents a starting point to achieve both fairness and   OOD robustness , the latter requiring more complex ,   model - centric debiasing approaches . A summary   of the results over all corpus pairs for each method   is presented in Appendix C.2 .   6 Towards Artifacts Documentation   The practice of data statements ( Bender and Fried-   man , 2018 ) has been recently adopted by the NLP   community as a way to include relevant informa-   tion about the creators , the methodology and possi-   ble biases when a dataset is released . This should   in turn have a positive impact on systems trained   on such data , contributing to a better evaluation of   models ’ generalization and fairness . We propose   that an artifacts statement should be added to this   documentation as a way to contribute to diagnosis   ( and thus mitigation ) of pre - existing bias , which is   also one of the goals of data statements .   In particular , we propose a template for lexical ar-   tifacts documentation and publicly release code to   easily compute ranked correlations between tokens   and target classes of interest for a given annotatedcorpus . To ensure the process of documenting lexi-   cal artifacts will be as smooth as possible – and thus   allows widespread adoption of artifacts statement   in the future – our code automatically generates out-   puts in different formats , from raw text to LaTeX   code for seamless inclusion in publications .   We present the artifacts statement template be-   low , and provide a full example in Appendix D. ) T .Which are the   kmost informative tokens in the corpus for the   class(es ) of interest ? This can be a ranked list of   ( k≥10 ) tokens in plain text or in a tabular format ,   optionally along with associated scores . If there are   multiple classes of interest , top klexical artifacts   for each class should be included . ) C .Different deﬁnitions   for the same class may exist across datasets . This   impacts the annotation , which in turn has an effect   on resulting lexical artifacts . An explicit deﬁnition   of the target class(es ) for which the top lexical   artifacts are computed should be provided here . ) M .The method   used to compute the correlation between tokens and   class(es ) ( e.g. , PMI , interpretability approaches )   in the annotated corpus should be reported here ,   possibly with a link to code . If preprocessing and   deduplication have been performed , they should   be clearly reported . Resources such as full lists of   lexical artifacts can be additionally included .   7 Related Work   The problem of models ’ generalizability related   to hate speech detection has been extensively dis-   cussed in recent works ( Vidgen and Derczynski ,   2020 ; Yin and Zubiaga , 2021 ; Wich et al . , 2021 ) .   Indeed , it has been shown that state - of - the - art per-   formance on this task overestimates the capability   of models to yield the same results over time ( Flo-   rio et al . , 2020 ) or across different domains ( Wie-   gand et al . , 2019 ) . Possible mitigation strategies   include domain adaptation techniques ( Ramponi   and Plank , 2020 ) , augmenting smaller datasets with   a larger dataset from a different domain ( Karan   and Šnajder , 2018 ) , the use of a domain lexi-   con to transfer knowledge across domains ( Pa-   mungkas and Patti , 2019 ) and the ﬁne - tuning of   HateBERT ( Caselli et al . , 2021 ) on the target cor-   pus ( Bose et al . , 2021 ) , among others .   Concerning bias and fairness , several works have   pointed out the presence of bias in hate and abu-   sive language datasets ( Wiegand et al . , 2019 ; Sap3034et al . , 2019 , 2020 ) . This issue has been addressed   in different ways , including functional tests for   hate speech detection models ( Röttger et al . , 2021 ;   Manerba and Tonelli , 2021 ) and post - hoc expla-   nations to measure models ’ bias towards identity   terms ( Kennedy et al . , 2020b ) . As regards bias   mitigation , the task has been addressed through a   number of approaches , e.g. , via adversarial feature   learning ( Vaidya et al . , 2020 ) , by using debiased   word embeddings and gender swap data augmen-   tation ( Park et al . , 2018 ) or by adding non - toxic   examples to better balance the data ( Dixon et al . ,   2018 ) . The work probably most related to ours   is Zhou et al . ( 2021 ) , which presents an analysis of   lexical and dialectal biases in the dataset by Founta   et al . ( 2018 ) . The authors propose lexical bias cate-   gories which we extend in this work ( see Section 2 ) .   However , they focus only on one dataset and on in-   domain bias reduction . Moreover , they start from a   list of “ bad words ” , whereas we compute it from   data . To our knowledge , this is the ﬁrst work advo-   cating for a joint view on fairness and robustness ,   both identiﬁed as critical aspects related to the clas-   siﬁcation of hate speech ( Wich et al . , 2021 ) .   8 Limitations   Our work is a step forward towards a better under-   standing of the bias that can be encoded in hate   speech detection corpora ( Blodgett et al . , 2020 ) .   However , we are aware of some limitations . First ,   all ﬁndings in this work are related to hate speech   datasets in English . With the increasing availability   of hate speech data in languages other than English ,   we aim to investigate our methods on other lan-   guages too . Second , annotated data from multiple   platforms may not be available for some languages ,   and this can limit the cross - distribution computa-   tion of artifacts . Lastly , we acknowledge spurious   statistical correlations may go beyond the token   level . We believe our study is a ﬁrst step towards   contextual debiasing from spurious lexical artifacts ,   and thus can be of inspiration for future studies .   9 Conclusion and Future Directions   This paper investigates the impact of lexical arti-   facts on out - of - distribution fairness and robustness   in hate speech detection , raising awareness on the   interplay between the two dimensions that should   be studied together in future work . We propose a   ﬁne - grained categorization of lexical artifacts and   simple yet effective data - centric baselines , show - ing that while robustness calls for model - centric   approaches , masking spurious identity artifacts is   a viable approach that we argue should be used as   strong baseline for fairness assessment in future   research . In future work we aim to investigate the   role of dialectal biases and non - lexical artifacts , ex-   tending the study on languages other than English .   We release all baseline models , resources , and the   code to compute lexical artifacts , broadly suggest-   ing the inclusion of “ artifacts statement ” as a way   to document potential lexical biases when a dataset   is released , to provide a complementary view to   data statements ( Bender and Friedman , 2018 ) .   Ethical Considerations   The annotation task described in Section 4.3 was   carried out by two researchers regularly employed   at Fondazione Bruno Kessler as part of their work .   Overall , we do not foresee any speciﬁc ethical   concern related to this work . On the contrary , our   goal is to propose artifacts statement as a desir-   able practice for documenting potential biases in   newly released datasets , and improve current debi-   asing methods by distinguishing among different   types of lexical artifacts . However , the ( ﬁnite set   of ) identity - related and offensive tokens consid-   ered in this work are all in English and centered   around Western cultural context . We leave the eval-   uation of our methodology to assess whether there   are language- or more broadly culture - dependent   changes for future work , following recent work on   biases in geo - cultural contexts ( Ghosh et al . , 2021 ) .   Acknowledgements   Part of this work was funded by the PROTEC-   TOR European project ( ISFP-2020 - AG - PROTECT-   101034216 - PROTECTOR ) . This research was also   supported by the KID ACTIONS REC - AG project   ( n. 101005518 ) on “ Kick - off preventIng and re-   sponDing to children and AdolesCenT cyberbul-   lyIng through innovative mOnitoring and educa-   tioNal technologieS ” .   References3035303630373038Appendix   A Data : Additional Details   A.1 Preprocessing and anonymization   We preprocess texts across platforms in a consis-   tent way by anonymizing user mentions , URLs ,   and email addresses with [ USER ] , [ URL ] , and   [ EMAIL ] placeholders , respectively . We seg-   ment hashtags into constituent words using the   wordsegment package , remove newlines , un-   escape HTML tags , and lowercase the texts .   A.2 Deduplication   We found many duplicates in the data for all plat-   forms . We argue that retaining duplicates as done   in most previous work could severely affect the reli-   ability of any bias analysis ( and debiasing method )   and its subsequent conclusions . Speciﬁcally , dupli-   cates can ( i ) skew the distribution of actual artifacts   in the data , overamplifying some lexical items and   demoting others , and ( ii ) result in unfair evalua-   tions due to identical examples falling in multiple   instances of the training , development and test sets ,   also potentially leading to overﬁtting .   Following this intuition , we thus remove dupli-   cate instances after preprocessing . Speciﬁcally ,   we removed 485 duplicate instances from Vid-   gen et al . ( 2021 ) , 10,911 duplicate instances   from Founta et al . ( 2018 ) , 521 from Kennedy et al .   ( 2020a ) , and 255 from de Gibert et al . ( 2018 ) .   Moreover , for the purpose of this work we remove   duplicates whose single instances exhibit opposing   labels , leaving the exploration and exploitation of   annotator disagreement for future work .   B List of Spurious Artifacts   In the following , we provide the list of all spuri-   ous lexical artifacts annotated as SandSas   described in Section 4.3 . All these are the ones that   exhibit full agreement . In our shared repository   we also release all artifacts that exhibit disagree-   ment even after adjudication , in order to encourage   future work on this direction . Identity - related ( S)“white ” , “ black ” , “ jews ” ,   “ women ” , “ jew ” , “ whites ” , “ blacks ” , “ muslim ” ,   “ gay ” , “ muslims ” , “ islam ” , “ woman ” , “ jewish ” ,   “ islamic ” , “ immigrants ” , “ mexican ” , “ asian ” , “ ho-   mosexual ” , “ americans ” , “ lesbian ” , “ homo ” , “ fe-   males ” , “ america ” , “ brown ” , “ israel ” , “ arabs ” ,   “ zionist ” , “ trans ” , “ lgbt ” , “ girl ” , “ hispanic ” ,   “ refugees ” , “ male ” , “ african ” , “ africa ” , “ girls ” , “ in-   dians ” , “ queer ” , “ # # grate ” , “ guy ” .   Non identity - related ( S)“##s ” , “ # # es ” , “ peo-   ple ” , “ country ” , “ # # ing ” , “ anti ” , “ illegal ” , “ bunch ” ,   “ # # t ” , “ kids ” , “ culture ” , “ brain ” , “ # # ly ” , “ # # bt ” ,   “ # # d ” , “ sex ” , “ ho ” , “ # # nt ” , “ countries ” , “ # # ic ” ,   “ # # ers ” , “ liberal ” , “ reason ” , “ # # y ” , “ human ” ,   “ genocide ” , “ # # ed ” , “ # # ists ” , “ wrong ” , “ lives ” ,   “ bad ” , “ god ” , “ # # oc ” , “ lying ” , “ # # ard ” , “ racism ” ,   “ # # e ” , “ # # oid ” , “ # # w ” , “ yeah ” , “ millions ” , “ so-   ciety ” , “ # # g ” , “ leftist ” , “ crime ” , “ sp ” , “ des ” ,   “ # # ist ” , “ # # ry ” , “ mouth ” , “ # # ards ” , “ # # rs ” ,   “ # # ize ” , “ burn ” , “ murdered ” , “ worship ” , “ # # en-   ing ” , “ # # ism ” , “ living ” , “ # # fa ” , “ coming ” , “ call-   ing ” , “ streets ” , “ # # ting ” , “ force ” , “ mis ” , “ # # ss ” ,   “ blame ” , “ typical ” , “ # # pe ” , “ baby ” , “ death ” , “ talk-   ing ” , “ # # gen ” , “ belong ” , “ respect ” , “ di ” , “ # # yp ” ,   “ sexual ” , “ # # less ” , “ mad ” , “ war ” .   C Experiments : Additional Results   C.1 Filtering with different thresholds   In Table 5 we present results for the F - baseline using different sampling thresh-   olds . Speciﬁcally , in addition to using the 33 %   ( 1/3 ) most ambiguous training data instances as   in Swayamdipta et al . ( 2020 ) , we provide full re-   sults using more aggressive ( i.e. , 25 % , 1/4 ) and   less aggressive ( i.e. , 50 % , 1/2 ) ﬁltering thresholds .   We notice mixed results that make hard to deter-   mine which is the best threshold across platforms .   F ( 25 % ) improves OOD robustness on→/f1a1 , and F ( 50 % ) provides best over-   all in - domain performance on . However , M-(S)outperforms all F approaches   according to the FPR metric .   C.2 Average results over all corpus pairs   We provide a summary of the results for all meth-   ods in Table 6 , where we report average scores over   all corpus pairs ( refer to Table 4 for full results ) . On   average , M ( S)improvement in FPR over   theV baseline is as large as 2× , both in-   distribution and out - of - distribution . This comes at3039   the cost of a minimal in - distribution and OOD drop   in macroF(i.e . ,−1.26and−1.95 , respectively ) .   D Lexical Artifacts Statement Example   An example of lexical artifacts statement for the   Reddit dataset ( Vidgen et al . , 2021 ) used in this   study is presented in the following . ) T .We present the   topk= 10 most informative tokens for the hateful   class along with their scores in Table 7 . ) C .Thehateful class is   represented by originally identity - directed labeledexamples in CAD ( Vidgen et al . , 2021 ) , and is de-   ﬁned as “ Content which contains a negative state-   ment made against an identity . An ‘ identity ’ is a so-   cial category that relates to a fundamental aspect of   individuals ’ community , socio - demographics , po-   sition or self - representation [ ... ] . It includes but   is not limited to Religion , Race , Ethnicity , Gen-   der , Sexuality , Nationality , Disability / Ableness and   Class . ” ( Vidgen et al . , 2021 ) . ) M .In order   to compute the correlation between tokens to the   hateful class we employ PMI as implemented   in[this work ] ( code : https://github.com/   dhfbk / hate - speech - artifacts ) . Input   texts have been preprocessed by anonymizing   user mentions , URLs , and email addresses   with [ USER ] , [ URL ] , and [ EMAIL ] place-   holders . Hashtags have been segmented using   wordsegment , and we remove newlines , un-   escape HTML tags , and lowercase texts . Duplicate   instances have been removed after preprocessing .   The full list of lexical artifacts along with asso-   ciated scores is available at https://github .   com / dhfbk / hate - speech - artifacts .3040