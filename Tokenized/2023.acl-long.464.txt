  Chi Chen , Peng Li , Maosong Sun , Yang LiuDept . of Comp . Sci . & Tech . , Institute for AI , Tsinghua University , Beijing , ChinaInstitute for AI Industry Research ( AIR ) , Tsinghua University , Beijing , ChinaBeijing National Research Center for Information Science and TechnologyShanghai Artificial Intelligence Laboratory , Shanghai , China   Abstract   Weakly supervised vision - and - language pre-   training ( WVLP ) , which learns cross - modal   representations with limited cross - modal super-   vision , has been shown to effectively reduce   the data cost of pre - training while maintain-   ing decent performance on downstream tasks .   However , current WVLP methods use only lo-   cal descriptions of images , i.e. , object tags ,   as cross - modal anchors to construct weakly-   aligned image - text pairs for pre - training . This   affects the data quality and thus the effective-   ness of pre - training . In this paper , we pro-   pose to directly take a small number of aligned   image - text pairs as anchors , and represent each   unaligned image and text by its similarities to   these anchors , i.e. , relative representations . We   build a WVLP framework based on the rela-   tive representations , namely RELIT , which   collects high - quality weakly - aligned image-   text pairs from large - scale image - only and   text - only data for pre - training through relative   representation - based retrieval and generation .   Experiments on four downstream tasks show   that RELIT achieves new state - of - the - art re-   sults under the weakly supervised setting .   1 Introduction   Vision - and - language pre - training ( VLP ) ( Chen   et al . , 2020 ; Zhang et al . , 2021 ; Kim et al . , 2021 ;   Radford et al . , 2021 ; Wang et al . , 2022a ) has   received increasing attention in recent years for   its great success on various vision - and - language   tasks , such as visual question answering ( Antol   et al . , 2015 ) , cross - modal retrieval ( Plummer et al . ,   2015 ) , and image captioning ( Lin et al . , 2014 ) .   Different from other foundation models ( Bom-   masani et al . , 2021 ) such as BERT ( Devlin et al . ,2018 ) and MAE ( He et al . , 2022 ) that only require   single - modality data , VLP models rely on large-   scale aligned image - text datasets ( Lin et al . , 2014 ;   Sharma et al . , 2018 ; Ordonez et al . , 2011 ; Krishna   et al . , 2017 ) to bridge the gap between the two   modalities , which requires either extensive manual   annotations or heavy data cleaning processes ( Lin   et al . , 2014 ; Sharma et al . , 2018 ) . The natural dif-   ficulty of obtaining paired data hinders the scale   of cross - modal datasets , while the success of uni-   modal pre - trained models implies the potential to   exploit the unlabeled data for pre - training . There-   fore , besides collecting more paired data , it is a   worthwhile direction to explore how to utilize low-   cost unimodal data with limited cross - modal super-   vision , i.e. , weakly supervised vision - and - language   pre - training ( WVLP ) .   The core challenge of WVLP is to establish the   connection between the two modalities without us-   ing a large number of aligned image - text pairs . Ex-   isting works on WVLP ( Li et al . , 2021b ; Zhou et al . ,   2022 ; Wang et al . , 2022b ; Chen et al . , 2022 ) usually   address this by taking object tags as anchors as they   are in the form of text and cover the information   of the image at the same time . They use tags to   collect weakly - aligned image - text pairs from un-   aligned unimodal data for pre - training and achieve   competitive results compared to standard VLP mod-   els , demonstrating that tags can effectively bridge   the gap between the two modalities .   Despite its success , using object tags as an-   chors suffers from two limitations . First , tags are   merely local descriptions instead of a complete   representation of the whole image and text . Sec-   ond , the vocabulary of tags only includes com-   mon concepts , making it difficult to represent im-   ages with complex semantics ( Zhou et al . , 2022 ) .   These limitations could deteriorate the quality of   the weakly - aligned data ( and possibly pre - trained   models ) based on the object tags . Therefore , to fur-   ther improve the performance of WVLP , we need8341to reconsider the choice of the cross - modal anchors   and find a better approach to measure the alignment   between an image and a text .   Recently relative representation has been   proven to be effective in representation learn-   ing ( Moschella et al . , 2022 ) and zero - shot image   classification ( Norelli et al . , 2022 ) . The main idea   is to represent a data point as its similarities to   a set of selected data points ( anchors ) . We ar-   gue that relative representations can be a good   choice for WVLP because ( 1 ) they are built on   the semantic similarities of well - trained neural   network representations rather than on superficial   human - designed features like tags and ( 2 ) they   are modality - invariant by design because they re-   flect the intrinsic relationships between data points ,   which naturally enables communication between   different modalities .   In this paper , we propose RELIT , a novel relative   representation - based WVLP framework . Instead   of object tags , we directly use a minuscule amount   ( compared to pre - training data ) of available image-   text pairs as anchors , and create a common relative   representation space with respect to the anchors for   unaligned images and text . This allows us to esti-   mate the semantic similarity of any image - text pair   by calculating their distance in the relative repre-   sentation space . In addition , we design two relative   representation - based data collection methods that   can retrieve or generate weakly - aligned image - text   pairs from unaligned unimodal corpora . Experi-   mental results prove the effectiveness of relative   representations in bridging the gap between image   and text modalities . Moreover , our work reveals   a promising research direction to establish cross-   modal alignments by finding and aligning invariant   data structures in different modalities , which may   inspire future works on multimodal pre - training .   Our main contributions are as follows :   •We introduce the idea of relative representa-   tions in WVLP and demonstrate its superiority   over object tags in effectively bridging the gap   between different modalities .   •We propose a relative representation - based   WVLP framework that can both retrieve and   generate weakly - aligned image - text pairs for   learning cross - modal representations .   •Extensive experiments on four diverse vision-   and - language tasks show that our proposedframework outperforms strong WVLP base-   lines and further closes the performance gap   between WVLP and standard VLP .   2 Related Work   Relative Representations . The concept of rel-   ative representations is initially proposed by   Moschella et al . ( 2022 ) . They show that the rel-   ative representations obtained from the representa-   tion spaces of different models are similar , which   enables comparison and alignment between latent   embeddings of different learning models . Norelli   et al . ( 2022 ) explore relative representations in a   multimodal scenario to align images and text for   zero - shot image classification tasks . Specifically ,   they use 1.6 M image - text pairs to build the rela-   tive representation space , which is comparable to   the size of the data used in pre - training . To the   best of our knowledge , our work is the first that   exploits relative representations for weakly super-   vised cross - modal pre - training .   Weakly Supervised Vision - and - Language Pre-   training . Li et al . ( 2021b ) first explore WVLP   with unaligned image and text corpora and use ob-   ject tags directly as pseudo captions for images to   bridge the vision and language modalities . Zhou   et al . ( 2022 ) use tags to retrieve weakly - aligned cap-   tions for each image and then apply multi - granular   alignment tasks on this retrieved dataset . Wang   et al . ( 2022b ) propose the cross - modal CutMix to   replace some grounded words with regions that   have the same tags , and construct a multimodal   view of the text - only sentences for pre - training .   Chen et al . ( 2022 ) introduce an end - to - end frame-   work with a referring expression matching task .   Different from all of these WVLP works that utilize   tags as anchors to provide object - level cross - modal   alignment signals , our work uses relative represen-   tations to capture the overall semantic similarity   between each image and text and demonstrates its   effectiveness in WVLP .   Data Augmentation . Data augmentation has   been extensively employed in various computer   vision ( Zhang et al . , 2018 ; Cubuk et al . , 2018 ) and   natural language processing tasks ( Sennrich et al . ,   2015 ; Guo et al . , 2020 ) . In the area of VLP , Li et al .   ( 2022 ) augment the noisy web - crawled aligned data   by filtering low quality image - text pairs and gen-   erating synthetic captions with an image captioner   fine - tuned on clean image - text pairs . In this work,8342we adopt a similar filter - and - generate process in the   construction of weakly - aligned data for WVLP , but   our relative representation - based pseudo caption   generator is fine - tuned on the text - only dataset .   3 Method   3.1 Relative Representations   Figure 1a provides an illustration of relative rep-   resentations . The basic idea is to represent a data   point as its similarities to other data points ( an-   chors ) . In this work , we consider the relative rep-   resentations with cross - modal anchors , which has   been shown its potential in zero - shot image classi-   fication ( Norelli et al . , 2022 ) .   Formally , given a set of Mcross - modal anchors   A={a , a , . . . , a}where a= ( ˜x,˜y)is an   image - text pair , ˜xis the image and ˜yis the text .   For an image x , a pre - trained image encoder Eis   used to calculate the similarity between xand each   anchor aas :   sim(x , a ) = cos ( E(x ) , E(˜x ) ) ( 1 )   where cos(·,·)is the cosine similarity , and the rela-   tive representation of xis defined as :   r(x ) = ( sim ( x , a ) , . . . , sim(x , a ) ) ( 2 )   Similarly , the relative representation of a text yis   defined as r(y)with a pre - trained text encoder   Eto compute sim(y , a ) = cos ( E(y ) , E(˜y ) ) .   Since the relationship between data points is ob-   jective , the relative representations obtained by dif-   ferent models should be similar , despite their in-   dependent representation spaces ( Moschella et al . ,   2022 ) . In other word , an image and its correspond-   ing text should share similar relative representa-   tions . This allows us to leverage it to construct   weakly - aligned image - text pairs from large - scale   unpaired image and text datasets .   3.2 Weakly - Aligned Image - Text Pairs   Retrieval   While there are no large - scale aligned image - text   pairs available , having a joint input of image and   text , even if they are not aligned , is still necessary   for WVLP ( Zhou et al . , 2022 ; Wang et al . , 2022b ) .   To achieve this , inspired by previous work ( Zhou   et al . , 2022 ) , we construct a weakly - aligned image-   text corpus from the unpaired unimodal corpora by   retrieving semantically related sentences for each   image based on the relative representations . Figure 1b illustrates the process of our weakly-   aligned image - text pairs retrieval method . First   we collect a very small amount of image - text pairs   as cross - model anchors ( denoted by pairs of con-   nected squares in the figure ) . Note that the number   of anchors is negligible compared to the image-   text pairs used in standard VLP , which keeps our   method in a weakly supervised setting . Then , for   all images and text we compute their relative repre-   sentations with respect to the anchors , which only   involves similarity computation within each modal-   ity using unimodal pre - trained encoders . We take   the cosine distance between the relative represen-   tations of each image and text as their semantic   relevance score and retrieve the best matching text   with the highest score for each image to construct   a weakly - aligned image - text pair .   Specifically , we randomly sample Mimage - text   pairs as anchors Afrom an aligned image - text   dataset ( e.g. , Conceptual Captions ( Sharma et al . ,   2018 ) ) D(M≪ |D| ) . Given unaligned im-   age dataset Dand text dataset D , we construct   a retrieved weakly - aligned image - text pair dataset   D={(x,ˆy ) , . . . , ( x,ˆy)}where N=|D|   andˆyis the retrieved caption from Dfor image   xdefined as :   ˆy= argmaxcos(r(x),r(y ) ) ( 3 )   We use the off - the - shelf ViT ( Dosovitskiy et al . ,   2020 ) and Sentence - BERT ( Reimers et al . , 2019 )   to encode images and text , respectively .   Our retrieval method with relative representa-   tions can effectively improve the quality of re-   trieved weakly - aligned dataset compared to tag-   based retrieval . This is because relative represen-   tations tend to capture the overall semantics while   tags describe only local information of the image .   As a result , our method can better measure the   semantic similarities between images and text , es-   pecially in cases where tag - based retrieval fails to   distinguish between images and text that have dif-   ferent semantics but share the same objects .   3.3 Pseudo Caption Generation   Although relative representation - based retrieval   can construct reasonable weakly - aligned image-   text pairs for WVLP , there are still cases where   non - relevant text are retrieved . This could hap-   pen especially when the unaligned unimodal cor-   pora are collected individually and for some images   there are no proper captions in the corpora.8343   To alleviate this problem , we propose to directly   generate pseudo captions for these images . As   shown in Figure 2 , we first adapt a well - trained text   generator to perform conditional text generation   given relative representations . Then , since images   and text share a common relative representation   space , we can directly use this generator to pre-   dict the pseudo caption for an image based on its   relative representation .   Specifically , given the text - only dataset D , for   each text y∈ D , we derive a prefix P∈R   from its relative representations r(y)as :   P= [ r(y)]W+ [ E(˜y ) , . . . , E(˜y)]W   ( 4 )   where E(˜y)∈Ris the encoder output of   the text in the i - th anchor , W∈Rand   W∈Rare two learnable projection matri-   ces . We fine - tune a pre - trained GPT-2 model ( Rad-   ford et al . , 2019 ) to learn to predict ygiven P , and   name the fine - tuned model as Rel2Cap . To further   save computational cost , we only consider the en-   tries in Pthat correspond to the top Kanchors   with the highest similarities as the model input .   After training , the model can be used to pre-   dict the pseudo caption for an image xwith low   quality retrieved captions by constructing an input   prefix Pbased on the relative representations of   the image , i.e. , r(x ) . The definition of Pis sim-   ilar to Equation 4 , except that r(y)is replaced   byr(x ) . We define a quality score s(x,ˆy ) =   cos(r(x),r(ˆy))for each weakly - aligned image-   text pair ( x,ˆy)collected both by retrieval and gen-   eration , and replace the retrieved pair with the gen-   erated one if the latter has a higher quality score .   So far , we have discussed how we collect a   weakly - aligned image - text dataset Dfrom the un-   paired unimodal corpora by relative representation-   based retrieval and generation . Next , we describe   how we use these data for WVLP .   3.4 Pre - training   Model Overview . We use the same model ar-   chitecture as Chen et al . ( 2022 ) that consists of   a vision and a multimodal encoder . For each   weakly - aligned image - text pair , the image is en-   coded with the vision encoder and the outputs are   fed to the multimodal encoder along with the text   embeddings to obtain a multimodal representation .   Such an end - to - end framework has been proven8344to be more effective compared to others that use   region features from external object detectors both   in standard VLP and WVLP . We apply three pre-   training objectives to learn multimodal represen-   tations from the collected weakly - aligned image-   text pairs : masked tag prediction ( MTP ) , masked   language modeling ( MLM ) and image text match-   ing ( ITM ) .   Masked Tag Prediction . This objective aims to   learn object - level cross - modal alignment from the   image - only data and their detected object tags . Fol-   lowing previous works ( Li et al . , 2021b ; Chen et al . ,   2022 ) , we randomly mask out the tags with a prob-   ability of 15 % , and then predict the masked tags   conditioned on the image and other unmasked tags .   Formally , given the image x∈ Dand its detected   object tags t , the MTP objective is defined as :   L=−ElogP(t|t , x ) ( 5 )   where tandtrepresents masked and un-   masked object tags , respectively .   Masked Language Modeling . To better fuse be-   tween the two modalities , the masked language   modeling objective is adopted to learn from the   joint image - text inputs from the weakly - aligned   corpora . Since the weakly - aligned pairs may con-   tain noise in the retrieved or generated text , we only   mask out and predict the noun phrases in the text   inspired by ( Zhou et al . , 2022 ) . The MLM loss is   formulated as :   L = −ElogP(ˆy|ˆy , x)(6 )   where ˆyandˆyare masked and unmasked text .   Image Text Matching . ITM is a commonly   used objective for learning instance - level cross-   modal alignment in VLP , which aims to distinguish   whether an image - text pair is matched semantically .   We random replace the text in half of the image - text   pairs with another text to form training input , and   define the label of each pair as l∈ { 0,1}where 1   indicates the pair is a match . The ITM objective is   to minimize the binary cross - entropy loss :   L=−ElogP(l|x,ˆy ) ( 7 )   whereDis the dataset after random replacement .   Relative Representation - Guided Training . To   further reduce the impact of the noisy image - text   pairs in the weakly - aligned dataset , we apply thequality score s(x,ˆy)of each pair described in Sec-   tion 3.3 to L andLto guide the training to   learn more from high - quality data :   L = −Es(x,ˆy ) logP(ˆy|ˆy , x )   ( 8)   L=−Es(x,ˆy ) logP(l|x,ˆy ) ( 9 )   4 Experiments   4.1 Datasets   We follow previous WVLP works ( Li et al . , 2021b ;   Zhou et al . , 2022 ; Wang et al . , 2022b ; Chen et al . ,   2022 ) and conduct experiments in two different set-   tings , each containing an image - only dataset and   a text - only dataset . The first setting treats images   and text from Conceptual Captions ( CC ) ( Sharma   et al . , 2018 ) as individually collected unimodal   dataset without the alignment information . The   second setting uses images from CC and text from   BookCorpus ( Zhu et al . , 2015 ) , which is a more re-   alistic scenario where images and text are gathered   separately from different sources .   4.2 Implementation Details   Relative Representations . We randomly select   8,192 aligned image - text pairs from CC as an-   chors , yielding relative representations as vectors   of8,192dimensions . To save computational cost ,   inspired by Norelli et al . ( 2022 ) , we only keep the   highest 50dimensions and set the others to 0 .   Weakly - Aligned Data Construction . We imple-   ment the retrieval system with the faiss ( Johnson   et al . , 2019 ) library . For each image we only re-   trieve the text with the best match score . For Rel-   Cap , we fine - tune GPT-2 with a learning rate of 5e-   5 and a batch size of 1,024for5epochs on the text-   only dataset . We generate 5pseudo - captions for   each image using nucleus sampling with p= 0.9   which proved effective in synthetic caption gener-   ation ( Li et al . , 2022 ) , and rank the results with   the quality scores . We also include the weakly-   aligned dataset based on tag - based retrieval in the   pre - training , as described in Zhou et al . ( 2022 ) .   Pre - training . We use the same architecture as   Chen et al . ( 2022 ) which includes a 12 - layer Swin-   Transformer ( Swin B-384/32 ) ( Liu et al . , 2021 )   as the vision encoder and a 12 - layer Transformer   initialized from BERT - base ( Devlin et al . , 2018 )   as the multimodal encoder . For object tags , we8345   utilize the off - the - shelf object detector provided by   VinVL ( Zhang et al . , 2021 ) . We pre - train the model   with a total training step of 150k and a batch size   of512 . We use an AdamW optimizer ( Kingma and   Ba , 2014 ) with an initial learning rate of 3e-5 , and   the warm - up ratio is set to 10 % . The pre - training   takes 3days on 4NVIDIA A100 GPUs .   Downstream Tasks . We follow previous works   and test our pre - trained model on four downstream   V+L tasks , including Visual Question Answer-   ing ( VQAv2 ) ( Goyal et al . , 2017 ) , Natural Lan-   guage for Visual Reasoning ( NLVR ) ( Suhr et al . ,   2018 ) , Visual Entailment ( VE ) ( Xie et al . , 2019 )   and image retrieval ( Flickr30k ) ( Plummer et al . ,   2015 ) . Details of the task settings and the fine-   tuning strategies are in Appendix A.   4.3 Main Results   We first compare our proposed RELIT with previ-   ous methods pre - trained with unaligned images and   text from CC . Note that these baselines only utilize   object tags . Table 1 shows the experimental results   on the downstream tasks . Our method outperforms   previous WVLP methods on all downstream tasks . Specifically , RELIT outperforms previous best re-   sults by 1.8%onNLVRand by 3.8%on the im-   age retrieval task ( Flickr30k ) , both of which benefit   from the instance - level cross - modal alignment ca-   pability of the pre - trained model ( Chen et al . , 2020 ;   Zhou et al . , 2022 ) . This suggests that our relative   representation - based method improves the align-   ment quality of weakly - aligned image - text pairs   compared to previous tag - based approaches , result-   ing in improved cross - modal alignment capability   of the pre - trained model .   When pre - trained with images from CC and text   from BookCorpus , as shown in Table 2 , our pro-   posed RELIT also achieves the best results on all   downstream tasks . This demonstrates that the pro-   posed relative representation - based methods can   effectively mine useful cross - modal alignment in-   formation for multimodal pre - training from image-   only and text - only data , even if they are collected   separately from different sources .   4.4 Ablation Study   We conduct an ablation study to verify the effective-   ness of the proposed relative representation - based   retrieval and generation . Table 3 shows the results.8346   All models are pre - trained on weakly - aligned data   derived from unaligned CC images and text . As we   can see from the table , compared to tag - based re-   trieved data ( Retrv ( Tag ) ) , pre - training with relative   representation - based retrieved data ( Retrv ( Relrep ) )   performs better on downstream tasks . Besides , the   model achieves the best results when the generated   pseudo captions ( Rel2Cap ) are included during pre-   training . We believe this is because the original CC   dataset contains noisy captions , such as alt - texts   that do not describe the image contents , which is   suboptimal for VLP ( Li et al . , 2022 ) . In summary ,   the experimental results demonstrate that both our   retrieval and generation methods contribute to the   performance of the pre - training .   We also compare the performance of the pre-   trained models on downstream tasks with and with-   out relative representation - guided training . As   shown in Table 4 , pre - training with guided training   can consistently improve results across all down-   stream tasks , illustrating that relative representa-   tions can be used to detect noise in the weakly-   aligned data and guide the model to learn from data   with a higher level of alignment .   4.5 Data Quality   We evaluate the quality of different kinds of weakly-   aligned data from unaligned CC images and text ,   and the results are listed in Table 5 . We use CLIP-   Score ( Hessel et al . , 2021 ) to measure the overall   alignment of all weakly - aligned image - text pairs .   As we can see from the table , the data quality of   Retrv ( Relrep ) is significantly higher than that of   Retrv ( Tag ) , which again illustrates the superiority   of relative representations as cross - modal anchors .   In addition , Rel2Cap further improves data qual-   ity by filtering and replacing low - quality pairs in   Retrv ( Relrep ) . The analysis of the data quality   here is consistent with the analysis of pre - training   results in Table 3 , and again proves that our rel-   ative representation - based methods can produce   high quality weakly - aligned data from unaligned   unimodal data .   4.6 Effects of Anchor Selection   The number of anchors has a significant in-   fluence on the effect of relative representa-   tions ( Norelli et al . , 2022 ) . To verify its influence   on the collected weakly - aligned image - text pairs ,   we test the quality of the data retrieved with differ-   ent numbers of anchors on the COCO ( Lin et al . ,   2014 ) dataset . From Figure 3 , we can see that as the   number of anchors increases , the quality of the re-   trieved data also improves . In addition , we evaluate8347   the downstream task accuracy using models pre-   trained on data with varying numbers of anchors .   Specifically , we generate 3 random sets of anchors   for each size , and retrieve the weakly - aligned data   with different sets of anchors . We pre - train mod-   els on each set of the retrieved data with the same   hyperparamters , and fine - tune them on the NLVR   task . The results are shown in Figure 4 . In general ,   the higher the number of anchors , the better the   model performance . We use 8,192anchors in our   final experiments as a trade - off between represen-   tation capability and computational cost . However ,   using more anchors will almost certainly give bet-   ter results due to better quality of the data , which   indicates the scalability of our approach . We leave   more exploration on this for future work .   We also conduct experiments to verify the im-   pact of anchor diversity on data quality . Specifi-   cally , we considered three sampling methods on   the COCO dataset : random , diverse , and non-   diverse . The diverse sampling first performs K-   means clustering on all the data , and selects one   anchor from each cluster . The non - diverse sam-   pling uses a greedy algorithm to select k anchors ,   at each step choosing the data closest to the aver-   age of the anchors already selected . Table 6 lists   the data quality results obtained with different sam - pling methods . In general , diverse anchors lead   to better quality , while random anchors perform   satisfactorily when the number of anchors is large   enough . Non - diverse anchors can result in catas-   trophic data quality .   4.7 Case Study   To explore the reasons for the improvement in data   quality , we show two examples of the compar-   isons between different weakly - aligned image - text   pairs in Figure 6 . In each example , we provide   the ground truth caption of the image and the de-   tected object tags , as well as three weakly - aligned   captions . From these two examples , we can see   that the captions retrieved by tags do have many   of the same tags as the images ( underlined in the   figure ) , but are not good descriptions of the im-   ages . In contrast , our relative representation - based   retrieval and generation methods are able to obtain   captions that are more relevant to the overall seman-   tics of the images . Specifically , in the example in   Figure 6a , our proposed methods successfully iden-   tifies key information in the image such as “ golfer ” ,   which is difficult for tag - based retrieval since there   are no such tag as “ golfer ” . The same thing hap-   pens to Retrv ( Tag ) in Figure 6b , which retrieves a   caption related to “ cat ” instead of “ lynx ” . In this   example , our retrieval method recognizes the ani-   mal in the image as “ cheetah ” , which is close but   not exactly correct , while our generation method   correctly generates a caption related to the correct   concept “ lynx ” . This indicates that our generation   method has the ability to generate pseudo captions   of better quality when the retrieved ones are not   good enough .   In Figure 5 we further visualize the relative rep-   resentations of the image and two retrieved cap-   tions in Figure 6a , which helps understand the   effectiveness of relative representations in align-   ing semantically related image - text pairs . From   the figure we can see that the image and our re-   trieved caption Retrv ( Relrep ) activate the same   group of anchors ( i.e. , have high similarities with   these anchors ) , which makes them close in the   relative representation space . On the other hand ,   Retrv ( Tag ) activates a completely different set of   anchors , which leads to a large distance between it   and the image in the relative representation space .   These observations suggest that ( 1 ) relative rep-   resentations are ( almost ) modality - invariant and   ( 2 ) relative representations can be utilized to effec-8348   tively estimate the cross - modal alignment of data   in different modalities . These properties of the rel-   ative representations make it naturally suitable for   WVLP , which is verified in this paper.5 Conclusion   This paper introduces the idea of relative represen-   tations to weakly - supervised vision - and - language   pre - training and demonstrates its effectiveness in   bridging the gap between the two modalities . We   propose a relative representation - based framework   that can both retrieve and generate weakly - aligned   image - text pairs for pre - training . Experimental re-   sults show that our method outperforms all pre-   vious tag - based approaches under the weakly-   supervised setting . We hope our work will motivate   future work in multimodal pre - training .   Limitations   As this work is mainly focused on weakly super-   vised vision - and - language pre - training , we do not   fully explore the factors that may influence the per-   formance of relative representations , such as the   use of different unimodal encoders and the source   of the anchors . Besides , we only validate the ef-   fectiveness of relative representations in a weakly   supervised setting , while it remains to be explored   whether it is also useful for standard VLP and mul-   timodal learning in other modalities ( e.g. , audio   and video ) . We will further exploit the potential   of relative representations and validate it in more   cross - modal learning scenarios in the future.8349Acknowledgments   This work is supported by the National Key R&D   Program of China ( 2022ZD0160502 ) and the Na-   tional Natural Science Foundation of China ( No .   61925601 , 62276152 , 62236011 ) . We thank Ziyue   Wang , Fuwen Luo , Rui Jiao and Zonghan Yang for   their advices in paper writing .   References83508351A Details of Downstream Tasks   Visual Question Answering ( VQA ) The task of   VQA is to answer questions correctly according to   the given images . We follow previous works ( Yu   et al . , 2019 ; Chen et al . , 2020 ) and formulate VQA   as a classification task with 3,192classes represent-   ing the most frequent answers in the dataset . We   fine - tune the pre - trained model for 10epochs with   a batch size of 256 . We use an AdamW optimizer   with a peak learning rate of 5×10 .   Natural Language for Visual Reasoning   ( NLVR)The objective of NLVRis to decide if   a natural language description is true for a given   pair of images . We follow previous work ( Chen   et al . , 2020 ) to form two image - text pairs as inputs ,   and concatenate the two [ CLS ] outputs of the   model as the final representation for classification .   We fine - tune the model for 10epochs with a batch   size of 128and a peak learning rate of 2.5×10 .   Visual Entailment ( VE ) Given an image and   a text hypothesis , the task of VE is to determine   whether the image implies the hypothesis . This   is formulated as a three - way classification task to   predict whether the logical relationship between   the image and the text is entailment , neutral or   contradiction . For the VE task , we fine - tune the   pre - trained model with a batch size of 64and a   peak learning rate of 1×10for5epochs .   Image Retrieval ( Flickr30k ) We follow pre-   vious works ( Li et al . , 2021b ; Chen et al . ,   2022 ) to conduct the image retrieval task on the   Flickr30k ( Plummer et al . , 2015 ) dataset . We sam-   ple15negative image - text pairs for each positive   pair by replacing its text with randomly sampled   ones . The batch size is set to 512 . We fine - tune the   model with a peak learning rate of 2.5×10for   10epochs .   B Additional Examples   In Figure 7 , we provide more examples of dif-   ferent kinds of weakly - aligned image - text pairs .   From these examples , we can see that our relative   representation - based approaches yield higher qual-   ity weakly - aligned image - text pairs compared to   tag - based retrieval.83528353ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   6   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   4   /squareB1 . Did you cite the creators of artifacts you used ?   4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   4   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   4   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   We only use publicly available datasets that are widely used in previous vision - and - language pre-   training works . Relevant statistics have been fully discussed and can be easily found in these   works .   C / squareDid you run computational experiments ?   4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   48354 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   4   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.8355