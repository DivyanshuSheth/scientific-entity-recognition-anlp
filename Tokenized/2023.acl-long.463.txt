  Navita Goyal   University of Maryland   navita@cs.umd.eduAni Nenkova   Adobe Research   nenkova@adobe.comHal Daumé III   University of Maryland   Microsoft Research   me@hal3.name   Abstract   In the task of entity description generation ,   given a context and a specified entity , a model   must describe that entity correctly and in a   contextually - relevant way . In this task , as well   as broader language generation tasks , the gener-   ation of a nonfactual description ( factual error )   versus an incongruous description ( contextual   error ) is fundamentally different , yet often con-   flated . We develop an evaluation paradigm that   enables us to disentangle these two types of   errors in naturally occurring textual contexts .   We find that factuality and congruity are often   at odds , and that models specifically struggle   with accurate descriptions of entities that are   less familiar to people . This shortcoming of   language models raises concerns around the   trustworthiness of such models , since factual   errors on less well - known entities are exactly   those that a human reader will not recognize .   1 Introduction   Gricean maxims of effective communication   ( Grice , 1975 ) as they pertain to referring expres-   sions ( Dale and Reiter , 1995 ) posit that referring   expressions should not convey false information   and that they should be relevant to context . Factu-   ality andcongruity are thus the two main properties   of pragmatically appropriate referring expressions .   Following these maxims , human - written refer-   ring expressions strongly adhere to the principles   of factuality and congruity ( Dale and Reiter , 1995 ;   Kheirabadi and Aghagolzadeh , 2012 ) . Standard   evaluation practices for referring expression gener-   ation ( Belz et al . , 2009 ; Kang et al . , 2019 ; Cao and   Cheung , 2019 ) , however , only distinguish between   model generated referring expressions being accu-   rate(ground - truth ) versus inaccurate ( not ground-   truth ) , without considering factuality and congruity   of the model outputs . However , this distinction   Figure 1 : Two example contexts with s represent-   ing potential location of a target referring expression .   In the top case , the inaccurate generation is nonfactual   ( not true ) but contextually plausible ( given “ Sochi ” ) ; in   the bottom , the inaccurate generation is factual ( Blake   was both a poet and a painter ) but incongruous .   between factual and contextual errors is important ,   as contextually - relevant factual errors are likely to   be harder for people to identify ; this concern is   supported by evidence that human annotators trust   translations that are fluent / coherent but inadequate   over translations that are adequate but disfluent   ( Martindale and Carpuat , 2018 ; Popovi ´ c , 2020 ) .   In this work , we design an evaluation framework   to study the distribution of factual and contextual   errors in referring expression selection and gen-   eration , for descriptions of people mentioned in   English news articles . In the top example in Fig-   ure 1 , the model generates the description Russian   Olympic Committee Chairman , plausibly based on   the contextual cues ( Sochi ) , leading to factual error   that is contextually relevant . We call such contex-   tual but not factual descriptions nonfactual . In the   bottom example in Figure 1 , the generated descrip-   tionthe English Painter is factually correct ( Wiliam   Blake was both a poet and painter ) , however it is   contextually not relevant . We call such factual but   not contextual descriptions incongruous .   To tease apart these two failure modes , we au-   tomatically construct potentially nonfactual and   incongruous reference texts using article context   and other factual sources and evaluate generated   descriptions against these reference texts ( § 2.1).8322   Additionally , we consider a multiple - choice identi-   fication experiment , with distractors that are non-   factual , incongruous , or both ( § 2.2 , § 2.3 ) . Our   goal is to evaluate the ability of language models   to select between factually and contextually plau-   sible alternatives and to determine which aspects   are more problematic for language models . We   find that models commonly make both factual and   contextual errors .   Because people reading the generated descrip-   tions might fail to recognize factual errors for enti-   ties they are unfamiliar with , we augment our evalu-   ation to distinguish between more and less familiar   entities . We find that models disproportionately   predict nonfactual descriptions for unfamiliar enti-   ties , raising concerns about the trustworthiness of   text where description selection is guided by lan-   guage models ( § 5 ) . We validate our nonfactuality   and incongruity assumptions on the proposed dis-   tractors ( § 6 ) and show that our findings continue to   hold for a completely validated test set . Finally , we   discuss the validity of our proposed evaluations in   measuring what we purport them to measure from   the measurement modeling perspective ( § 7 ) .   2 Referring Expression Generation   Open - book referring expression generation in-   volves two steps : claim selection and description   generation ( Kang et al . , 2019 ) . Claim selection   identifies facts from the target entity ’s WikiData   ( referred as claims ) relevant to the context . De-   scription generation produces the text , conditioned   on the context and relevant claims , for instance us-   ing an auto - regressive encoder ( Kang et al . , 2019 ) .   Traditionally , such generation tasks are evaluated   using text similarity metrics , such as ROUGE ( Lin ,   2004 ) or BLEU ( Papineni et al . , 2002 ) , of the pre - dicted description with respect to the ground - truth   description . However , the failure modes beyond   the ground - truth are usually left unexplored .   Here we develop a framework for evaluating   generated descriptions against various reference   texts , controlled to be accurate , nonfactual , or in-   congruous , to sharpen our understanding of model   errors . We also design multiple choice evaluations   for claim and description identification , with dis-   tractors that are nonfactual , incongruous , or both .   This multiple - choice setup allows more control   over the chosen alternatives to highlight the fac-   tual and contextual preferences in language models .   For instance , in the top - left example in Figure 2 , the   claim identification model identifies Silvio Berlus-   coni as a television presenter rather than as the   Italian prime minister , possibly because the context   text mentions “ television talk show . ”   2.1 Description Generation   The description generation task calls for generating   referring expressions describing people in a text .   For training a description generation model , we   mask the description text in an article and feed the   masked context 𝑡 , along with the target entity 𝑒 , to   a language model as input to generate the descrip-   tion left - to - right . To investigate factual and contex-   tual errors , we evaluate the similarity of generated   description ( 𝑦 ) not only with the ground - truth   ( 𝑦 ) but also with alternative factual , or contex-   tual , reference text that are incongruous ( 𝑦 ) , or   nonfactual ( 𝑦 ) , respectively .   /rhd𝑦 : we take that entity ’s entire Wikipedia text   as an incongruous reference ; this presumably   contains true facts about that entity , but which   are not relevant in the article context .   /rhd𝑦 : we take the descriptions of all other entities   mentioned in the source article ( excluding target8323entity ’s descriptions ) as a nonfactual reference ;   these are very unlikely to be facts about the can-   didate entity , but are definitionally contextually   relevant .   In both cases , we exclude ground - truth descriptions   from incongruous and nonfactual reference texts .   Given these reference texts , we measure preci-   sion of the generated description ( 𝑦 ) with the   𝑦∈ { 𝑦 , 𝑦 , 𝑦}(after excluding stop words )   using BLEU ( unigram ) ( Papineni et al . , 2002 ) ,   ROUGE - L ( Lin , 2004 ) and Bertscore ( Zhang et al . ,   2019 ) . We use precision , instead of recall , as the   generated description and ground - truth tend to be   short ( 1.43 ± 0 .89words and 1.64 ± 0 .85 , respec-   tively ) , whereas the distractor reference texts are   much longer . The recall of distractor reference   is not possible , or even expected . The distractor   reference texts , instead , act as noisy proxy of the   space of incongruous and nonfactual tokens that the   model might generate . We would expect a “ good ”   system to have high precision with the ground - truth   and low precision with either distractor . A high   precision with respect to distractor reference texts   indicates model tendency to generate factually in-   correct or contextually incongruous descriptions .   2.2 Claim Identification   The claim identification task is to find a relevant   Wiki property ( claim ) for an entity ( 𝑒 ) mentioned   in the article given the masked article context ( 𝑡 ) .   We frame claim identification as a multiple - choice   problem where the ground - truth ( 𝑦 ) is taken   from the original text and we automatically   construct incongruous ( 𝑦),nonfactual ( 𝑦 ) ,   and both incongruous and nonfactual ( 𝑦 )   distractors ( see Figure 2 ( top ) ) . For a given text   𝑡and a target entity 𝑒 , we construct alternatives   = { 𝑦 , 𝑦 , 𝑦 , 𝑦}as :   /rhd𝑦 : claims in the target entity ’s Wiki tagged   as relevant to the ground - truth description . If   multiple , we choose the claim with the highest   unigram overlap with the description .   /rhd𝑦 : claims in the target entity ’s Wiki tagged as   irrelevant to the ground - truth description .   /rhd𝑦 : claims pertaining to the other entities in the   article , drawn from the article directly .   /rhd𝑦 : claims pertaining to entities not men-   tioned in the article , drawn from a random other   article .   For𝑦and𝑦 , we use the contextual relevance   mark up from Kang et al . ( 2019 ) . To minimizethe chance that a distractor is accidentally accurate ,   we sample all distractors from respective candidate   sets ensuring zero unigram overlap with the ground-   truth description . We ascertain nonfactuality of   𝑦and𝑦distractors by enforcing zero overlap   with all claim associated with the target entity .   2.3 Description Identification   Description identification aims to identify the   ground - truth description ( 𝑦 ) from distractors . In   contrast with claim identification , which uses   structured Wiki claims as alternatives , descrip-   tion identification uses free - text descriptions in   the article . We construct alternatives =   { 𝑦 , 𝑦 , 𝑦 , 𝑦}as :   /rhd𝑦 : descriptions of 𝑒from another article .   /rhd𝑦 : description of another entity in the article .   /rhd𝑦 : description of an entity not mentioned in   the article .   We ensure that distractor descriptions   ( 𝑦 , 𝑦 , 𝑦 ) have zero unigram overlap   with 𝑦. Also , 𝑦and𝑦do not have any   overlap with any description of the target entity .   For both claim and description identification , the   model takes ( 𝑡 , 𝑒 , 𝑦 ) as input and outputs a proba-   bility 𝑝(𝑦 ) , for each 𝑦∈= { 𝑦 , 𝑦 , 𝑦 , 𝑦 } .   We report the relative frequency that each type of   distractor is highest ranked : T= arg max𝑝(𝑦 ) .   We also compute a mean reciprocal rank ( M ) for   the ground - truth and distractor classes on the held-   out test data . Maverages the reciprocals of the   rank of each class : large Mvalues correspond   to higher ranking alternatives .   For uniform comparison of the generation ex-   periment with the identification experiments , in   addition to reporting the BLEU , ROUGE - L , and   Bertscore of the generated description with respect   to the ground - truth and distractor reference texts ,   we also rank the different classes ( accurate , in-   congruous , and nonfactual ) based on their BLEU ,   ROUGE - L , and Bertscores on each example and   report the TandMof each class . Note , we do   not have a both incongruous and nonfactual class   in the description generation task as the generation   task is free - form , making the scope of description   that is both incongruous and nonfactual too open-   ended to be expressed by some reference text .   3 Entity Familiarity   Our proposed evaluation attempts to disentangle   factual and contextual preferences in language mod-8324   els in generating referring expression for entities   mentions in news articles . However , these pref-   erences need not be uniform across entities . Lan-   guage models may possess disparate information   about different entities , leading to disparate re-   liance on factual and contextual cues . We aim to   understand how factual and contextual errors relate   to the familiarity with the entity being described .   We develop a heuristic for approximating entity   familiarity , adapted from Siddharthan et al . ( 2011 ) .   We tag entities that always appear in article sum-   maries without descriptions ( referring expressions )   asfamiliar and the rest as unfamiliar .   4 Experiment Details   We conduct our experiments on the PoMo dataset   ( Kang et al . , 2019 ) , which contains English news ar-   ticles from CNN , Daily Mail and New York Times ,   along with their summaries . The dataset also iden-   tifies post - modifier description for an entity men-   tioned in the article and a set of claims from target   entity ’s Wikidata . We extract all person entities   mentioned in the article using NER tagging ( Finkel   et al . , 2005 ) . We use claims associated with the tar-   get entity and other entities mentioned in the article   to construct alternatives for claim identification .   For description identification and generation   experiments , we extract referring expressions —   pre - modifier , relative clause , appositive , partici-   ple clause , adjective / adverb clause and preposi-   tional phrase — using the regular expressions of   Stali ¯unait ˙e et al . ( 2018 ) . We use the same set of   referring expressions for familiarity heuristics . We   use a dependency parser ( Manning et al . , 2014 )   to extract the descriptions associated with entities ’   first mention . We identify alternative descriptions   of an entity in other articles based on full name   matches . Within an article ( or summary ) , we match   entities based on first or last name ( Siddharthan   et al . , 2011 ) .   We validate the familiarity heuristic by collect-   ing human annotations on a subset of 200person   entities in the validation set , with 3 - 4annotations   per entity . We ask crowdworkers on AMT ( paid   ∼US$15 / hour on AMT ) if they are familiar with   the given entity . For the entities marked as famil-   iar , we ask annotators to add a short description   of the entity to ensure that annotators perform the   task carefully . We specifically ask the annotators   to not use external search for this task and clarify   that there is no penalty for tagging entities as fa-   miliar or unfamiliar . We use two attention check   questions : one corresponding a well - known entity   ( e.g. , Joe Biden ) and another corresponding to a   made up entity , expecting the response as familiar   and unfamiliar , respectively . We further discard the   annotations where 90 % of entities are marked as   familiar or unfamiliar .   Inter - rater agreement according to the gener-   alized Kappa measure for multiple raters ( Gwet ,   2014 ) is 0.768 , indicating “ substantial agreement ”   ( Viera et al . , 2005 ) . We consider entities tagged   unfamiliar by the majority annotators as unfamil-   iarand the rest as familiar , and measure preci-   sion / recall of our heuristic with respect to this   ground - truth . The proposed heuristic has preci-   sion of 79 % , recall of 64%and F - score of 67 % . This   is close to precision , recall , and F - score of baseline   human annotations comparing one - vs - rest ( 74 % ,   75 % , and 74 % , respectively ) . See Figure 3 for exam-   ples of ( dis)agreement between human annotations   and our heuristics .   Table 1 shows the distribution of examples with   familiar and unfamiliar entities across different   tasks . We apply our evaluation procedure on mod-8325   els that we fine - tune on each task . We consider 3   sequence - to - sequence models for the generation   task : T5 - small , -base ( Raffel et al . , 2020 ) and   BART models ( Lewis et al . , 2020 ) with 60𝑀,220𝑀 ,   and140𝑀parameters , respectively . We consider 3   multiple - choice models for the identification tasks :   BERT ( Devlin et al . , 2019 ) , RoBERTa ( Liu et al . ,   2019 ) and ELECTRA ( Clark et al . , 2020 ) base   models with 110𝑀,130𝑀 , and 110𝑀parameters ,   respectively . See Appendix D for details .   5 Do error types differ for familiar vs   unfamiliar entities ?   Figure 4 shows the generative evaluation with re-   spect to ground - truth and distractor reference texts   across familiarity for descriptions generated by the   fine - tuned T5 - base model . We observe a higher   overlap with nonfactual reference texts for unfamil-   iar entities and a higher overlap with incongruous   reference texts for familiar entities across all eval-   uation metrics . The difference is significant for   BLEU and ROUGE metrics with 𝑝<0.005 , but non-   significant for Bertscore . TandMof classes(accurate , incongruous , or nonfactual ) in descrip-   tion generation task , obtained by ranking the BLEU   overlap between the generated description and ref-   erence texts , yield similar results ( Figure 5 ) . We   observe a higher TandMfor incongruous   reference texts for familiar entities and a higher   TandMfor nonfactual reference texts for   unfamiliar entities . We include results for other   models and metrics in Appendix E.   Controlled evaluation of RoBERTa model fine-   tuned on Claim and Description Identification tasks   ( Figure 5 ) echo the same trends . Nonfactual distrac-   tors are ranked higher than incongruous distractors   for unfamiliar entities , indicating more factual er-   rors . In contrast , incongruous distractors are ranked   higher for familiar entities , indicating more con-   textual errors . Additionally , comparing between   familiar and unfamiliar examples , we find that   rate of incongruous errors is significantly higher   ( 𝑝<{0.005,0.05 } ) for familiar examples than that   for unfamiliar examples . The converse is true for8326factual errors , with a significantly higher ( 𝑝<0.005 )   rate of factual errors for unfamiliar examples than   familiar examples . This reflects that the distribu-   tion of different types of models errors is different   for different entity types , with a disproportionately   higher rate of nonfactual predictions for unfamiliar   entities . Unsurprisingly , we also observe a higher   performance ( accurate class ) for familiar entities   in most of the cases . Our findings are consistent   across models ( Appendix E ) .   6 Are nonfactual and incongruous   alternatives really so ?   Our results highlight model tendency to make fac-   tual or contextual errors insofar as the automatically   extracted distractors and reference texts are faithful   to their associated classes . Our automatic distractor   extraction makes two assumptions : nonfactuality   and incongruity of distractors . For nonfactual dis-   tractors , we assume that a claim or description as-   sociated with other people mentioned in the article   is contextual , but not factual , for the given entity .   This assumption is easy to verify : we consider the   claims and descriptions associated with the target   entity in the PoMo corpus ensuring no overlap with   the facts associated with the target entity .   For the incongruous distractors , on the other   hand , we have assumed that a random alternative   factual claim / description associated with the target   entity that is not present in the current context is   incongruous . However , it is entirely plausible that   alternative factual descriptions are actually some-   times congruous , representing a threat to the valid-   ity of this measurement . To ascertain how reason-   able this assumption is , we conduct a human study .   Because assessing the contextuality of a descrip-   tion is difficult in isolation , we ask annotators to   compare automatically extracted incongruous ( but   factual ) descriptions with the congruous ( ground-   truth ) descriptions . Annotators are shown the ar-   ticle context and the two descriptions and asked   to give their preference on a 5 - point scale ranging   from strongly prefer description 1 tostrongly pre-   fer description 2 . We randomize description 1 and   description 2 as incongruous or ground - truth de-   scriptions . We collect 3annotations for 50samples   from both claim and description identification tasks ,   compensating AMT crowdworkers at US $ 15 / hour .   For the description generation task , we construct   nonfactual reference text as the context excluding   factual description of the entity , to enforce the non-   factuality assumption . To assess the validity of   incongruous reference texts in the description gen-   eration task , we collect human annotations . We   consider the generated descriptions that overlap   with the target entity ’s Wikipedia , aka , the incon-   gruous reference text as incongruous description .   We ask annotators to compare this incongruous   description with the ground - truth congruous de-   scription , as we do not have a generated congruous   description for the same input . We ask the anno-   tators to rate the contextual appropriateness of the   two on a scale of 1–5for50examples ( 3annota-   tions each ) . More details on human annotations   and interface are included in Appendix F.   We observe a moderate inter - rater agreement   of0.63(Fleiss ’ kappa ) . The annotator rating for   congruous and incongruous description is 3.71 ±   0.23and2.28 ± 0 .23(mean ±standard error ) , with   statistically significant effect size of 0.87(Cohen ’s   d ) at 𝑝<0.05 . The inter - rater agreements and effect   sizes for different tasks are shown in Table 2 . We   take the description with annotator rating < 3as   incongruous . We observe a majority agreement of   0.79with our automated annotations .   For the claim and description identification task ,   we construct a gold test set with the subset of exam-   ples where our extracted distractors agree with the   human annotations . We obtain 41and38gold ex-   amples in the claim and description identification ,   respectively . We also internally validate the factual-   ity assumption for the gold set . We manually check   that the incongruous distractors in the gold test set   are factual and the nonfactual distractors are not .   The evaluation on the gold set is limited to identi-   fication tasks . For description generation , human   annotators verify only the generated description   that overlap with incongruous reference text , not8327   the full reference text , which would require more   involved and time - consuming annotations .   Figure 6 shows similar trends on familiar and un-   familiar entities in the human annotated set ( gold   test set ) as seen previously on automatically an-   notated test sets ( Figure 5 ) . Models make more   factual errors for unfamiliar entities and more con-   textual errors for familiar entities ( 𝑝<0.05 ) , both for   claim and description identification tasks .   7 Measurement Validity   Above , we analyzed factual and contextual prefer-   ences in language models on three tasks pertaining   to referring expression generation . Throughout ,   we have shown that the language models we mea-   sure tend to make both factual errors and congruity   errors , and also have shown that these measures   are — in various ways — actually measuring what   we purport them to measure . Formally , we can   conceptualize this from a measurement modeling   perspective ( Messick , 1995 ; Jacobs and Wallach ,   2021 ) , wherein we consider factual and contextual   errors to be unobservable constructs .   In this view , we have proposed three measure-   ment devices as proxies for the unobservable con-   struct . This forms a measurement model , which   we can example from the perspective of measure-   ment validity , considering : face validity ( the extent   to which the measures look plausible ) , content va-   lidity ( the extent to which the measures capturethe substantive nature of the construct ) , convergent   validity ( the extent to which the measure matches   related measures ) , concurrent validity ( the extent   to which the measures distinguish between groups   that it should meaningfully be able to distinguish   between ) , and consequential validity ( the implica-   tion of using the measure ) .   Face validity is inherently subjective in nature .   We pose that the incongruous ( but factual ) and non-   factual ( but contextual ) alternatives respectively en-   code the factual and contextual cues that the models   refer to . Without any external criteria , readers are   often the only judge of the face validity .   Content validity has two key sub - aspects : sub-   stantive validity and structural validity .   /rhdSubstantive validity asserts that the measure ,   fully and only , incorporates the properties related   to the construct . We argue for substantive validity   of the measures based on our design and human   validation ( § 6 ) . By design , nonfactual alternatives   are extracted from the context and do not have   any overlap with the factual information about   the entity captured in Wikipedia or WikiData .   So , to the extent that Wikipedia / WikiData are   correct , these have strong substantive validity .   The incongruous alternatives are extracted from   factual information about the entity and verified   to be incongruous using human validation . They   subsequently point to factual preferences in the   models . This confirms that the measures capture   only the properties related to the construct for both   identification and generation tasks .   In the case of the description generation task ,   however , the nonfactual and incongruous measures   do not fully capture the respective constructs of con-   textual and factual preferences in language models .   The unigram precision metric only accounts for   factual and contextual indicators in the generated   description dictated by the reference texts . Being   open - ended , the reference text for incongruous and   nonfactual generations is really broad . We design   alternative reference texts to cover the range of fac-   tual and contextual cues that the model might parrot   in generating entity descriptions , but the reference   text is not necessarily exhaustive . This points to   weaker substantive validity of the measures in the   description generation task .   /rhdStructural validity is a component of content   validity that asserts that the measure captures the   structure of relationship between the constructs.8328   We note that the identification task measures have   relatively weak structural validity : the probability   assigned to each alternative is relative and a higher   probability to incongruous alternative might stem   from being paired with a low probability nonfac-   tual alternative . On the other hand , the description   generation task measures have a strong structural   validity as the generation task is free - form , so the   model directly outputs the highest probability to-   kens . This represents a trade - off in content validity   of the measures : identification has stronger substan-   tive and generation has stronger structural validity .   Convergent validity considers the degree to   which multiple measures of the same unobserved   construct point in the same direction . Our three   proposed measures have a clear convergent validity .   As seen in the results ( Figure 4 - 6 ) , the measures   point to the same effects across tasks : models de-   fault to contextual cues , leading to factual errors ,   for unfamiliar entities , whereas models tend to be   more factual , even while compromising on the con-   gruity of the description , for familiar entities . This   distinctive trend across familiar and unfamiliar en-   tities also points to the concurrent validity of the   proposed measures .   Concurrent validity asserts that the measure   should be able to distinguish between the meaning-   ful groups . Previous works highlight that language   models acquire information about entities seen dur-   ing pre - training ( Petroni et al . , 2019 ; Kandpal et al . ,   2022 ) . For such entities , we can expect the models   to reference the factual information seen during pre-   training . For unseen entities , on the other hand , we   can expect the model to yield best guess based on   contextual plausibility . Unsurprisingly , the models ’   exposure to certain entities would correlate withthe human familiarity with these entities due to pro-   liferation of online content on respective entities   being a common cause for the two . The proposed   measures ( nonfactual and incongruous alternatives )   are thus able to distinguish models ’ contextual and   factual preferences between potentially seen and   unseen groups , confirming the concurrent validity   of the measures . Table 3 summarizes the validity   of our proposed measures across different tasks .   Consequential validity is an assessment of what   happens if a measure is adopted . The unanimous   trend across various tasks points to a deeper con-   cern for the variability in the priors that models   use for familiar vs unfamiliar entities . Standard   evaluations of tasks miss both what kind of errors   models make and how these errors differ for dif-   ferent populations . This work brings attention to   these problems that usually get hidden under the   rug of accuracy measures . Based on our findings ,   we recommend that downstream tasks requiring   a balance of factual and contextual information   should probe into the model error distributions and   their variances across familiar and unfamiliar enti-   ties . Human evaluation for such tasks should also   account for human biases and limitations .   8 Related Work   Errors in Text Generation . Language models   often generate erroneous information , not sup-   ported by source and/or background documents   ( Ji et al . , 2023 ) , often termed as hallucinations . Pre-   vious works in text generation — abstractive sum-   marization ( Maynez et al . , 2020 ; Pagnoni et al . ,   2021 ) , dialog generation ( Dziri et al . , 2021 ; San-   thanam et al . , 2021 ) , and translation ( Lee et al . ,   2019)—highlight these nonfactual or incongruous   generations , without disentangling the factual and8329contextual errors . Cao et al . ( 2022 ) study errore-   nous generations in summarization , which are fac-   tual but unverifiable from the source text . Although   their work examines contextual errors , they do not   contrast these with factual errors due to the context .   Familiarity Prediction . Previous works have   studied familiarity of the reader with a person   mentioned in news , using linguistic signals in ar-   ticles and summaries ( Siddharthan et al . , 2011 ;   Stali ¯unait ˙e et al . , 2018 ) . Siddharthan et al . ( 2011 )   distinguishes between familiar ( “ hearer - old ” ) and   unfamiliar ( “ hearer - new ” ) based on how people are   referred to in summaries — hearer - old entities are   referred to with name only or title + name , while   hearer - new are referred to with an additional de-   scription . Stali ¯unait ˙e et al . ( 2018 ) further studies   the change in description length over time as enti-   ties evolve from hearer - old to hearer - new .   Knowledge Probing . There have been many dis-   cussions previously around “ knowledge ” encom-   passed in language models . Petroni et al . ( 2019 )   created the LAMA benchmark with ( subject , rela-   tion , object ) fact triplets , along with human - written   templates to elicit these facts . Language models   are designed to inherently focus on the context   for generation . LAMA benchmark ( Petroni et al . ,   2019 ) is specifically designed for factual probing .   As a result , the context , i.e. template or prompt , is   directly linked to the fact in question . More gener-   ally , language models are required to be both cor-   rect and contextually - relevant . Petroni et al . ( 2020 )   improves factual recall by augmenting templates   with relevant contextual information retrieved from   external sources , such as Wikipedia . The task is de-   signed such that context aids factual probing . Our   work deals with naturally occurring contexts .   Factuality and Congruity . Our paper focuses on   the two main properties of referring expressions :   factuality and congruity . These properties are bor-   rowed from Gricean maxims of effective communi-   cation ( Grice , 1975 ): quantity ( give as much infor-   mation as needed , and no more ) , quality ( not to give   information that is false ) , relation ( stay pertinent   to the discussion ) and manner ( be clear / brief ) . We   adapt the maxims of quality and relation into factu-   ality and congruity . We do not focus on the max-   ims of quantity and manner , because these maxims   mainly apply to the use and frequency of referring   expressions . For example , familiar entities are de-   scribed without referring expressions in summariesfor conciseness ( Siddharthan et al . , 2011 ) .   In a different directions , research in event fac-   tuality deals with the interaction between lexical   and syntactic information meant to convey vary-   ing levels of veracity or factuality of events men-   tioned in text ( Nairn et al . , 2006 ; de Marneffe et al . ,   2012 ) . For instance , “ XYZ , the supposed best artist ”   implies that the veracity of XYZ being the best   artist is questionable . In contrast to this line of   research , our work focuses on factuality in terms   of the maxim - of - quality sense alone . We conduct   a preliminary sanity check against lexicosyntactic   triggers for event factuality ( White et al . , 2018 ) to   confirm that referring expressions considered in   our work do not contain any . Extending to broader   pragmatic conditions related to event pragmatics   would be fascinating future work .   9 Conclusion   In this work , we integrate indicators for factual in-   consistencies and contextual incongruities in auto-   mated evaluation in referring expression generation .   Our paper aims at conducting post - hoc analyses of   language models in referring generation task to as-   sess the differences in error types across familiar   vs unfamiliar entities that are not reflected in the   aggregate accuracy numbers alone . Comparisons   with alternative factual and contextual reference   text , in addition to those with ground - truth descrip-   tion , suggest that description generation models   heavily mis - generate incongruous and nonfactual   description . We also show this effect using con-   trolled multiple - choice experiments for claim and   description identification . Further , we show that   language models disproportionately rely on context   when describing less familiar people , resulting in   factually incorrect descriptions .   The tasks discussed in our work are meant to   exemplify the disparities in language model errors   to advocate further research investment in expand-   ing language model evaluation beyond accuracy .   Our work opens avenues for future research on   the effects of augmenting language models with   retrieved information on the factuality and con-   gruity of the generations : do retrieval - augmented   language models still make factual errors and are   these models able to maintain the contextuality of   the generated descriptions . Further , more work is   needed to study to what extent language models   abide by the conversational pragmatics in the wild.8330Limitations   Our work aims to uncover how the distribution of   factual and contextual errors in referring expres-   sion generation varies based on the familiarity of   entities . Our proposed experiments uncover this   effect using pragmatics - driven heuristics . We need   a more general deep - dive into what models “ know "   to estimate how language models handle known   and unknown information differently , in a way that   might even escape human scrutiny . Further , our dis-   cussion is limited to specific semi - artificial tasks .   Although our work reveals an hitherto understud-   ied shortcoming in language models , extending this   diagnosis to general tasks might be non - trivial .   Ethics Statement   In this work , we adapt a pragmatic - driven familiar-   ity heuristic to study factual and contextual errors   for familiar and unfamiliar entities . Our work re-   veals that description generation disproportionately   rely on context for unfamiliar entities leading to   incorrect predictions . Our heuristic for tagging fa-   miliarity is aimed at contrasting model errors . We   do not propose the use of this heuristic to filter   unfamiliar entities in end services .   Acknowledgements   We sincerely thank the reviewers for their helpful   comments and insights . We are very grateful to   Ruiyi Zhang and the members of the CLIP lab at   UMD for their constructive feedback and useful   pointers for this work .   References833183328333A Why First Mention Description ?   The choice of considering entity description associated with entities ’ first mention is inspired by previous   studies in referring expressions ( Siddharthan et al . , 2011 ) . Siddharthan et al . ( 2011 ) find that the first   mentions of entities are generally longer and descriptive , and serve to introduce relevant information   about the entity . Later references tend to be mostly referential . We confirm this property in our data : we   find that only 14 % instances of later mentions of entities , if any , have descriptions . We ignore these later   descriptions to avoid any effect of description style varying across first vs later descriptions .   B Data   The PoMo dataset is built on top of CNN and Daily Mail data ( See et al . , 2017 ) , available under MIT   License with conditions only requiring preservation of copyright and license notices , and Wikidata   available under the Creative Commons Attribution / Share - Alike License . No additional copyright is listed   for the PoMo dataset ( Kang et al . , 2019 ) .   C Evaluation Metrics   For claim and description identification tasks , we use highest ranking prediction Tand Mean Reciprocal   Rank ( M ) evaluation metrics . Tconsiders the highest ranking prediction as 1 and the rest of the   predictions as 0 :   T(𝑐 ) = 1   𝑛   ∑𝕀[𝑐= arg max{𝑝(𝑦 ) } ] ( 1 )   where 𝑐∈ { ⋆,inc , nf , both}is accurate , nonfactual , incongruous , or both class , 𝑝(𝑦)is the prediction   probability of the class 𝑐in the 𝑖sample , and 𝕀is the indicator function . Mean Reciprocal Rank , on the   other hand , takes into account the rank order of different classes in each test example :   M(𝑐 ) = 1   𝑛   ∑1   rank(2 )   where rankis the rank of alternative from class 𝑐in the 𝑖test sample .   For description generation , we consider three generative metrics : BLEU ( Papineni et al . , 2002 ) ,   ROUGE - L ( Lin , 2004 ) , and Bertscore ( Zhang et al . , 2019 ) precision . For each example , we calculate the   generative metrics for the generated description with respect to the accurate reference text ( ground - truth   description ) and the incongruous and nonfactual reference texts as described in § 2.1 . We can consider the   set of reference texts as = { 𝑟 , 𝑟 , 𝑟 } . Let 𝑠(𝑦 , 𝑟)be generative score of the generated description   𝑦with respect to the reference text 𝑟for the 𝑖sample , where 𝑐∈ { ⋆,inc , nf } . We further rank the   three classes — accurate , incongruous , nonfactual — based on their generative scores on each example and   calculate Tand Mas :   T=1   𝑛   ∑𝕀[𝑐= arg max{𝑠(𝑦 , 𝑟 ) } ] ( 3 )   and   M(𝑐 ) = 1   𝑛   ∑1   rank , ( 4 )   where rankis the rank of the reference text from class 𝑐in the 𝑖test sample .   D Experiment Details   We fine - tune BERT ( Devlin et al . , 2019 ) , RoBERTa ( Liu et al . , 2019 ) , and ELECTRA ( Clark et al . , 2020 )   base models with 110𝑀,130𝑀 , and 110𝑀parameters ( 𝐴=12,𝐻=768and𝐿=12 ) , respectively , for claim   and description identification in multiple - choice setting to predict the accurate alternative . For description8334   generation , we fine - tune a T5 - small and T-5 base model ( Raffel et al . , 2020 ) with 60𝑀parameters ( 𝐴=8 ,   𝐻=512,𝐿=12 ) and 220𝑀parameters ( 𝐴=12,𝐻=768,𝐿=12 ) , respectively , and a BART base model   with 140𝑀parameters ( 𝐴=12,𝐻=768,𝐿=12 ) . We use a learning rate of 2 × 10with huggingface   implementation of Adam optimizer ( Loshchilov and Hutter , 2019 ) with a weight decay of 0.01 . The   models are trained for 3 epochs and take about 2 - 3hours each to train .   E Results   Table 4 - 5 show the evaluation of description generation and claim and description identification tasks   across different models . We observe an accuracy of 78%,81 % , and 84%on claim identification task and   accuracy of 67%,71 % , and 36%on description identification task for BERT , RoBERTa , and ELECTRA   models , respectively . Presumably ELECTRA model performs distinctly worse on description identification   due to lack of hyper - parameter tuning , which we keep same across all models for ease of experimentation .   We find that models rank incongruous or nonfactual distractors at top in between 4 − 27 % test examples .   We also note that the accuracy in description identification task is on average lower than the accuracy on   the claim identification task . The lower accuracy in description identification task reflects the difficulty of   identifying appropriate free - text descriptions , as opposed to structured claims .   For description generation , we observe best performance for T5 - base model with a BLEU , ROUGE - L   and Bertscore of 0.46,0.52 , and 0.71with respect to the ground - truth description . Ranking the ground-8335   truth and distractor classes based on the generative metrics ( BLEU , ROUGE - L , Bertscore ) , we observe   an accuracy of 56%on average for the T5 - base model ( that is , precision is highest with respect to the   ground - truth description in 56%of test examples ) , with 15%and28%incongruous and nonfactual errors   on average , respectively .   Figure 7 - 8 show the comparison between factual and contextual errors across familiar vs unfamiliar   entities for different models . The results are consistent with § 5 . Even though , the difference for Bertscore   is non - significant , we observe significant and consistent difference between familiar and unfamiliar sets   based on Tand Mof classes ranked by Bertscore .   F Human Annotations   We conduct two human annotation studies : verifying familiarity heuristics and task verification . For   familiarity annotations , we consider 200person entities in our validation set and have 3 − 4 annotations   per entity where user mark the entities as being familiar or unfamiliar . For the entities marked as familiar ,   we ask users to add a short description of who the person is to ensure that users are engaging with the task .   We specifically ask the users to not use external search for this task and clarify that there is no penalty for   tagging entities as familiar or unfamiliar ( 9 ) . We set a small time window for the task , about 20second   per entity ) to dissuade users from taking aid of any resources . We internally discard the annotations   where 90%of entities are marked as familiar or unfamiliar . We further use two attention check questions :   one corresponding a well - known entity ( e.g. , Joe Biden ) and another corresponding to a made up entity ,   expecting the response as familiar and unfamiliar respectively . We collect annotations from English   speaking participants in North America region as the news source ( CNN ) employed in our study is U.S.   centric . We do not collect any other demographic information from the participants .   The second set of human annotations aim to verify the congruity assumption of incongruous distractor   and reference text in claim & description identification and description generation tasks respectively . We8336   sample incongruous and congruous descriptions for 50 test examples in each task ( details of incongruous   and congruous description in § 6 ) . We ask 3annotators to compare the two descriptions and rate them on   a scale of 1 − 5 from Strongly Prefer description 1 toStrongly prefer description 2 ( Figure 10 - 11 ) . We   ascertain attention by two repeat questions where we asking them to restate their preference for an entity   in previous example without context . We exclude responses that fail the attention checks , however , all the   annotators are compensated at $ 15 / hour.83378338ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations Section   /squareA2 . Did you discuss any potential risks of your work ?   Limitations Section and Ethics Statement   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Introduction Section ( Section 1 )   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix B   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   The artifacts used in the paper were available under creative common license and our use does not   violate the license conditions .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   The data is public access news articles and wiki data and is fully non - anonymized . We expect the   dataset to not contain any harmful or offensive content as is ensured by the publication standards of   CNN , DailyMail and content moderation of Wikidata org .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Table 18339C / squareDid you run computational experiments ?   Section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4 and Appendix D   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   We do not run hyper parameter tuning and use default same hyper parameters across all models and   experiments   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 6 and Appendix F   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix F   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Section 4 , 6 and Appendix F   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   We include instructions to participants regarding the purpose of the data annotation in Appendix F .   However , we do not use the collected data for training our models . We only collect annotations to   validate assumptions of our automatic data creation and evaluation .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   The task and domain used in our setting does not include any potentially harmful content . The text   in our article is sourced from open license news articles from CNN and Daily Mail . We only show   participants content from the news articles , wherever applicable . We do not require participants to   look at any external sources to complete the annotations .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Appendix F8340