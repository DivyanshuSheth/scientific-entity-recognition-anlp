  Alexander R. FabbriXiaojian WuSrini IyerHaoran LiMona DiabYale UniversityFacebook AI   afabbri@salesforce.com   { xiaojianwu,sviyer,aimeeli,mdiab}@fb.com   Abstract   Community Question Answering ( CQA ) fora   such as Stack Overﬂow and Yahoo ! Answers   contain a rich resource of answers to a wide   range of community - based questions . Each   question thread can receive a large number   of answers with different perspectives . One   goal of answer summarization is to produce   a summary that reﬂects the range of answer   perspectives . A major obstacle for this task   is the absence of a dataset to provide super-   vision for producing such summaries . Recent   works propose heuristics to create such data ,   but these are often noisy and do not cover all   answer perspectives present . This work intro-   duces a novel dataset of 4,631 CQA threads   for answer summarization curated by profes-   sional linguists . Our pipeline gathers annota-   tions for all subtasks of answer summarization ,   including relevant answer sentence selection ,   grouping these sentences based on perspec-   tives , summarizing each perspective , and pro-   ducing an overall summary . We analyze and   benchmark state - of - the - art models on these   subtasks and introduce a novel unsupervised   approach for multi - perspective data augmen-   tation that boosts summarization performance   according to automatic evaluation . Finally , we   propose reinforcement learning rewards to im-   prove factual consistency and answer coverage   and analyze areas for improvement .   1 Introduction   In a world of information overload and the ubiquity   of discussion fora , there is a need for text summa-   rization as a means of distilling relevant informa-   tion into a concise form . The problem is even more   pertinent for question answering within the context   of Community Question Answering ( CQA ) fora ,   where a person poses a question and can get an   abundance of answers to sift through . Ideally , an   answer summary should cover the multiple perspec-   tives found in the answers , where available . Table 1   Table 1 : An example summary from our AnswerSumm   dataset , illustrating the multiple viewpoints present   manually - written summaries , and a subset of the 8 user   answers to which the summary can be aligned .   illustrates such an example where a person poses a   question about relocating to the US and obtaining a   credit score and a credit card . We present a sample   of the 8 answers to that question on StackExchange   and a manually - curated summary covering the an-   swers ’ main perspectives . Answer summarization   is a form of query - based , multi - document summa-   rization ( Ernst et al . , 2020 ) , and creating answer   summaries that reﬂect the underlying varying per-   spectives entails several subtasks : selection of an-   swer sentences relevant to the question ( query sen-   tence relevance ) , grouping these sentences based   on perspectives ( clustering ) , summarizing each per-   spective ( cluster summarization ) , and producing an   overall fused summary ( fusion ) .   To date , most CQA fora have a notion of a ’ best2508answer , ’ which is either manually chosen by the   person who asked the question or by a moderator ,   or obtained via community ratings . Work in this   ﬁeld typically makes use of this best answer as a   proxy for summaries , i.e. the focus is on extractive-   like summaries ( Tomasoni and Huang , 2010 ; Chan   et al . , 2012 ; Pande et al . , 2013 ; Wang et al . , 2014 ;   Song et al . , 2017 ) . Datasets such as WikiHowQA   ( Deng et al . , 2020a ) , which consists of a question ,   a long answer , and an answer summary , focus on   answer selection and the summarization of a sin-   gle answer . While CQASumm ( Chowdhury and   Chakraborty , 2019 ) uses the chosen best answer as   the answer summary , they also apply heuristics to   ensure token overlap with the remaining answers .   However , the best answer only presents one per-   son ’s perspective and rarely captures the variety of   perspectives discussed in a thread . Furthermore , we   ﬁnd that the heuristics applied in CQASumm gener-   ally promote only long answers instead of multiple   perspectives . To validate our hypothesis , we exam-   ine a set of 30 summaries from CQASumm and   found that only 37 % of the examples contained   multi - perspective answers . In contrast , 75 % of our   dataset requires multi - perspective summaries .   As alluded to above , although answer summa-   rization is an important research topic with prac-   tical applications , there are no relevant datasets   or techniques to address it effectively , i.e. no   manually - curated dataset exists for the answer sum-   marization problem , and no dataset decomposes the   task into its constituent subtasks . This work tries   to close the research gap in answer summariza-   tion ; we develop an annotation pipeline for multi-   perspective abstractive answer summarization . We   introduce the largest human - annotated dataset for   answer summarization , containing components for   sentence relevance , clustering , cluster summariza-   tion , and global answer summarization . We enlist   ten professional linguists to contribute to our an-   notation efforts . We iterate over instructions and   devise pre - pilot , pilot , and ﬁnal annotation stages   as well as re - annotation for quality assurance . We   collect over 4,631 high - quality data points . For val-   idation of our curated data set , we benchmark state-   of - the - art models on the subtasks of this dataset   and perform qualitative analysis to provide a clear   baseline and directions for future work . We then   propose a data augmentation pipeline to further   boost summarization performance . To generate   a silver multi - perspective summarization dataset , we introduce a pipeline for automatically creating   multi - perspective bullet - point answer summaries   for data augmentation , which boosts performance .   We ﬁnd that a strong baseline model trained on   our human - annotated data inherently outputs factu-   ally consistent summaries , and model performance   is improved by adding data from our automated   pipeline . Finally , we introduce entailment - based   and semantic area RL rewards namely to analyze   its effect on factual consistency and semantic cover-   age , ensuring we are capturing allfactually relevant   perspectives .   2 Related Work   Extractive Answer Summarization : Much work   has focused on the extractive summarization setting   as an answer - ranking problem ( Chan et al . , 2012 ;   Pande et al . , 2013 ; Wang et al . , 2014 ) . Liu et al .   ( 2008 ) ﬁnd that only 48 % of the best answers on   Yahoo ! Answers are unique best answers ; there   are multiple correct ways to answer a question .   Other recent work has focused on sentence extrac-   tion using metadata ( Tomasoni and Huang , 2010 ) ,   sparse - coding frameworks ( Song et al . , 2017 ) , or   answer - aware sequential extraction ( Deng et al . ,   2020b ) . Our focus is on an answer summariza-   tion pipeline which ultimately results in abstractive   answer summaries .   Abstractive Answer Summarization : Another   line of work has attempted abstractive answer sum-   marization by treating the tagged best answer as   the gold summary of all the other answers ( Chowd-   hury and Chakraborty , 2019 ; Chowdhury et al . ,   2020 ) . Recent work summarizes answers to med-   ical questions via a medical concept graph Zhang   et al . ( 2020 ) and incorporates multi - hop reasoning   ( Zhang et al . , 2020 ) and answer relevance from a   QA model into the summarization model ( Su et al . ,   2021 ) . Most related to our dataset creation , Chowd-   hury and Chakraborty ( 2019 ) present CQASumm ,   a dataset of about 100k automatically - created ex-   amples consisting of the best answer as the gold   summary , which , however , contains noise due to   automatic creation .   Multi - document Summarization : Answer sum-   marization can be viewed as a query - based multi-   document summarization ( MDS ) problem . Ap-   proaches to query - focused multi - document summa-2509rization have dealt with data sparsity via data aug-   mentation ( Pasunuru et al . , 2021 ) by restructuring   the title and paragraphs of news articles to match   the target task , coarse to ﬁne - grained modeling Xu   and Lapata ( 2020 ) , and by converting generic sum-   marization data into proxy queries ( Xu and Lapata ,   2021 ) Several large - scale MDS datasets have been   introduced in the news domain ( Fabbri et al . , 2019 ;   Gu et al . , 2020 ; Gholipour Ghalandari et al . , 2020 ) ,   for creating Wikipedia lead - paragraphs ( Liu et al . ,   2018 ) and for long - form question answering ( Fan   et al . , 2019 ) . However , Wikipedia summarization   is topic - based and less granular than our setting ,   and the ELI5 dataset ( Fan et al . , 2019 ) summarizes   web documents rather than direct query answers .   3 AnswerSumm   We introduce our annotation protocol and the char-   acteristics of our manually - curated answer summa-   rization dataset . Our annotation pipeline is illus-   trated in Figure 1 .   Annotation Protocol Our annotation pipeline   consists of four steps 1 ) Answer Sentence Selection   ( SentSelect ) , 2 ) Clustering ( SentCluster ) , 3 ) Clus-   ter Summarization ( ClusterSumm ) , and 4 ) Cluster   Summary Fusion ( ClusterSummFusion ) . We refer   to the task of taking forum answers and producing   ﬁnal overall summaries E2ESumm . We believe   that this pipeline mirrors the process by which hu-   mans create summaries of multiple answers by nar-   rowing and organizing information , followed by   paraphrasing . Furthermore , dividing the summa-   rization task in such a way paves the way for future   work in understanding the steps by which a model   creates a ﬁnal summary , and recent work has simi-   larly divided multi - document summarization into   these subtasks ( Ernst et al . , 2020 ) . For consistency ,   the same annotator completes all four steps for a   given example . However , we surmise that if each   subtask is performed well , then multiple annotators   can be involved for a given example .   For a given question thread , we present the an-   notator with the question , the forum from which   the question came , the title of the post , and the tags   that the original poster associated with the question .   The user answers are then presented , where each   answer has been automatically segmented into in-   dividual sentences using SpaCy ( Honnibal et al . ,   2020 ) . It is worth noting that sentence - level gran-   ularity is chosen as a simplifying assumption as   an appropriate level of segmentation . We are cog - nizant that clause level might be more accurate ,   however , given state - of - the - art clause detection as   well as the precedence for sentence - level model-   ing in previous work ( Tomasoni and Huang , 2010 ;   Song et al . , 2017 ) , we opted for sentence - level seg-   mentation .   Answer Sentence Selection ( SentSelect ): We   ask the annotators to mark each sentence as rele-   vant or not depending on whether it provides in-   formation useful in answering the user ’s question .   Annotators are instructed to mark as irrelevant sen-   tences that do not function as independent units ,   such as those which need additional context to be   understood as an answer to the question . As a re-   sult , noise from sentence segmentation may cause   certain sentences to be marked as not relevant , but   upon manual inspection , we found this to not be an   issue .   Clustering ( ClusterSumm ): Annotators then   cluster found relevant sentences into groups of the   same topic . Sentences that are on the same topic   but have different polarities are grouped together .   We do not pre - deﬁne a desired number of clusters .   Furthermore , clusters consisting of a single item   are allowed , and a sentence can belong to multiple   clusters . A sentence in multiple clusters may occur   in the case of complex sentences which present   multiple viewpoints .   Cluster Summarization ( ClustSumm ): The   annotators summarize each individual cluster of   relevant sentences from the previous step . Each   cluster summary should typically consist of 1 - 4   complete sentences . To allow for abstract sum-   maries , we instruct the annotators to try to use their   own words ( paraphrase ) instead of copying large   segments of the sentence clusters verbatim . Us-   ing the sentences ’ exact words is allowed , but they   should not copy more than ﬁve consecutive words   from a sentence . Additionally , the summary should   function as an answer rather than as an analysis   of the summary sentences . So , rather than stating ,   “ Most of the answers indicate that it is highly sub-   jective , ” the annotator writes directly “ It is highly   subjective . ” To ensure that the summary informa-   tion can be found in the input answers , we also in-   struct the annotators to focus solely on the answer   threads and not their external knowledge of the sub-   ject . The summary should solely ( 1 ) summarize the   viewpoint present in the sentence cluster ; and , ( 2 )   try to include some speciﬁc details from the asser-   tions and anecdotes made by the answer sentences.2510   We leave it to the annotator ’s judgment to leave out   details from clusters that are too minute .   Cluster Summary Fusion ( ClusterSummFu-   sion ): The annotator combines the cluster sum-   maries from the previous step into a single , co-   herent summary . The annotators can apply addi-   tional paraphrasing and need not simply insert each   cluster summary ; they may combine some cluster   summaries into a single sentence . The annotator is   asked to order and insert discourse connectives as   necessary to increase inter - sentential coherence in   the ﬁnal summary .   Data Filtering We selected question threads for   annotation from the StackExchange data release ,   as it is publicly available and has been shared us-   ing a Creative Commons ShareAlike license . We   created a whitelist of non - technical fora which do   not require domain knowledge to summarize , simi-   lar to work on non - technical email summarization   ( Ulrich et al . , 2008 ) . We sampled from 38 fora . Ta-   ble 3 illustrates the top 20 fora and their frequency .   In addition to this preliminary ﬁltering , we further   prompted annotators to discard any examples for   which they felt unable to adequately assess the rele-   vance of answer sentences to a question due to lack   of required domain knowledge or context .   The ﬁltering of question threads was motivated   by heuristics detailed in Tomasoni and Huang   ( 2010 ) , which aims to ﬁnd threads suitable for sum-   marization . We only include answers with a non-   negative community score which is determined by   the number of upvotes by community members   minus the number of downvotes . Moreover , they   do not include comments to answers for simplic-   ity , although future work may incorporate this into   modeling . Threads were removed if 1 ) there wereless than four answers , 2 ) the sum of the length   of all answers was outside of ( 100 , 1500 ) words ,   and 3 ) the average length of answers was outside   of the ( 50 , 300 ) words interval . Questions include   the subject of the post and the content of the post   when available . Out of about 870k question threads ,   about 8k met these criteria . While this ﬁltering   may be strict , it avoids threads that contain short   or single answers for which summarization may be   superﬂuous , thus creating a higher - quality , diverse ,   dataset as conﬁrmed by our analysis that 75 % of   our examples require multi - perspective summaries   .   Quality Controls Our annotators are 10 profes-   sional linguists recruited through a professional   vendor . We provide the linguists with an example   of an annotated question thread for clarity and dis-   cussed the instructions in - depth with the vendors to   avoid ambiguities . To ensure that the linguists are   well - trained and that the annotations meet our re-   quirements , we completed our annotations in three   stages . We began with a pre - pilot of 50 example   question threads , followed by a pilot of 500 exam-   ples and then a ﬁnal set of 5000 examples . We   divide annotation ﬁles into groups of 50 examples ,   which are split among the annotators . We make use   of the pilot and ﬁnal annotation sets for our dataset   release . To determine inter - annotator agreement   ( IAA ) , 250 examples were repeated across three an-   notation ﬁles . A Fleiss Kappa of 0.25 was achieved   for sentence relevance selection , the ﬁrst task . The   IAA score indicates fair agreement .   Dataset Statistics and Comparison We pro-   vide statistics about the subtasks from our dataset   pipeline in Table 2 . There does not exist a   manually - curated dataset for abstractive answer   summarization . CQASumm is the closest dataset   with our desired answer summarization qualities,2511   although it is created automatically based on heuris-   tics which simply promote answers as summaries   rather than truly summarizing answers . We also   present a comparison of dataset statistics between   our dataset AnswerSumm , and the standard XSum   and CNN - Daily Mail ( Nallapati et al . , 2016 ) sum-   marization datasets in Table 4 . In general , we   ﬁnd our dataset to be more abstractive than CNN-   DailyMail and less so than XSum . Furthermore , the   average number of input tokens for the E2ESumm   task , is larger than those two datasets , conﬁrm-   ing that the input to our tasks provides reasonable   grounds for requiring summarization .   4 Pipeline for Data Augmentation   Manually annotating data at the scale of other   existing summarization datasets such as CNN-   DailyMail is impractical . Taking advantage of the   abundance of unlabeled StackExchange fora avail-   able , we develop a pipeline to automatically create   data similar to that which is manually annotated   above . This process provides augmented data for   training summarization models .   Data Filtering Similar to ﬁltering for manual   annotation , we obtained question threads from   StackExchange and applied heuristics motivated   by Tomasoni and Huang ( 2010 ) to ﬁnd threads suit-   able for summarization . Threads are removed if : 1 )   there are less than three answers ; 2 ) the longest an-   swer is at least 400 words ; 3 ) the input token length   of all answers is not between 100 and 1000 words ;   and , 4 ) the average length of answers is between 50   and 300 words . Heuristics were chosen to provide   enough examples for data augmentation , leaving   about 130k question threads in total .   Pipeline Overview The input to our pipeline is   a user question and its answers . We select question   threads from StackExchange and operate on the   sentence - level of these answers , as in our manually-   created data . Our automatic dataset pipeline con-   sists of the following components which aim to   mirror the manual pipeline : 1 ) a relevance model   to select relevant sentences and remove irrelevant   ones ; 2 ) a clustering model to cluster similar con-   tent – reﬂecting various perspectives ; and , 3 ) in-   put and abstractive summary creation from cluster   centroids , resulting in bullet points for the various   perspectives reﬂected in the answers . Figure 2 il-   lustrates the pipeline .   Relevance model : A sentence - level relevance   model trained on CQA fora is leveraged to elimi-   nate irrelevant sentences from the input ( collection   of answers to a question ) . The output from this   stage serves as input to the clustering stage . Model   details are found in Section 6 .   Clustering : Typical K - Means clustering for   short text ( Xu et al . , 2017 ; Hadifar et al . , 2019 ;   Rakib et al . , 2020 ) does not work for our setting   as the value of K is not known a priori . In fact , it   varies from question to question . Accordingly , we   use the sentence - transformers library ( Reimers and   Gurevych , 2019a ) to perform clustering . Speciﬁ-   cally , we start with a RoBERTa - based model ﬁne-   tuned for sentence embeddings on an entailment   dataset , which is further ﬁne - tuned for semantic   similarity . Clustering parameters are chosen based   on a StackOverﬂow clustering dataset containing   labeled clusters , as provided in Rakib et al . ( 2020 ) .   We apply Agglomerative clustering with average2512   linkage , cosine distance , and a maximum distance   of0.65 . Parameters are empirically chosen .   To create the ﬁnal summaries , we locate the cen-   troid of clusters with at least two sentences and   select these centroids as bullet - point summaries .   Further , we remove the centroid sentences from   the sentence - segmented input answers to create a   challenging abstractive summarization dataset anal-   ogous to the XSum dataset ( Narayan et al . , 2018 ) .   Since each cluster contains at least two sentences ,   we assume that given a perfect clustering algorithm ,   a related sentence can help generate the removed   centroid sentence . While removing sentences nat-   urally decreases coherence , we believe that this   introduces a tolerable level of noise . We also ex-   perimented with cluster centroid paraphrasing and   not removing from the input , but this did not im-   prove downstream performance , which we use to   measure the value of this dataset and the level of   noise .   5 RL - Based Training   Cross - entropy loss in standard sequence - to-   sequence model training suffers from exposure bias   and also does not directly optimize evaluation met-   rics ( Ranzato et al . , 2016 ) . The REINFORCE algo-   rithm ( Williams , 1992 ) , on the other hand , allows   for optimizing the evaluation metrics using non-   differentiable rewards . We use an RL multi - reward   objective to promote summaries with both high   coverage of the input answers and faithfulness .   5.1 Multi - Reward Optimization   We follow the settings of Pasunuru and Bansal   ( 2018 ) for optimizing multiple rewards . In the   equations which follow , x={x , x , ... , x }   refers to the input source tokens ( e.g. a question   and its answers ) , and y={y , y , ... , y }   refers to the gold target summary which consists of{y , y , ... , y}sentences . Standard training   minimizes the negative log - likelihood ( NLL ) loss   using teacher forcing ( Williams and Zipser , 1989 ):   L=−/summationdisplaylogp(y|y, ... ,y , x)(1 )   For our RL optimization , we use self - critical policy   gradient training as in Paulus et al . ( 2018 ) ; Ren-   nie et al . ( 2017 ) . At each time - step , we produce   an outputyby sampling from the current decod-   ing probability , p(y|y, ... ,y , x ) , as well as an   output ˆyobtained by greedily decoding from the   current probability distribution . We deﬁne a re-   ward function r(y , x , y)∈[0,1 ] , i.e. , the reward   function compares ywithxandy . The RL loss   functionL(x , y ) = :   ( 2 )   As in Paulus et al . ( 2018 ) and Pasunuru and Bansal   ( 2018 ) , we use a mixture of the two losses above :   L = γL+γL , ( 3 )   whereγandγare tunable hyperparameters   used as scaling factors . Rather than applying   weights to each reward , we follow Pasunuru and   Bansal ( 2018 ) and optimize L by alternating   rewards in each minibatch .   5.2 Rewards   We use the following RL reward functions : ( 1 )   textual entailment ( NLI ) for faithfulness , and ( 2 )   semantic area to measure the coverage of a sum-   mary in a semantic space .   NLI for Faithful Summarization : We use the   degree of entailment of summaries given input an-   swers as a reward to promote faithfulness of answer2513summarization . Falke et al . ( 2019 ) deﬁne NLI as   a measure of faithfulness for ranking summaries   as follows : LetNbe an NLI model which , given   a claimcand a premise p , computesN(p , c ) , the   probability that the claim is entailed by the premise .   We use this to calculate the NLI score for a sum-   maryyconsisting of Nsentences :   NLI(y , x ) = 1   N / summationdisplaymaxN(s , y ) ( 4 )   Semantic Area for Multi - Perspective Summa-   rization : We aim to reward summaries that in-   clude more of the perspectives found in the input   answers . To achieve diverse extractive summariza-   tion , Yogatama et al . ( 2015 ) embed sentences in   the semantic space and then select those sentences   whose convex hull maximizes the volume in that   space . This idea of semantic volume is also used to   measure the semantic overlap between summaries   and references in Jung et al . ( 2019 ) . We use se-   mantic volume as a proxy for covering multiple   perspectives ; the summary with the larger semantic   volume covers a wider range of views discussed in   the input . We make use of sentence - transformers   ( Reimers and Gurevych , 2019b ) to obtain sentence   embeddings for each sentence . We project each   embedding onto two dimensions using Principal   Component Analysis ( PCA ) as in Jung et al . ( 2019 ) ,   and thus , our volume calculation reduces to an area   calculation , which we refer to as Semantic Area .   We use min - max normalization to keep the reward   between 0 and 1 . We split the dataset into training ,   validation , and testing sets of size 3131 , 500 , and   1000 examples . For relevance labeling , we train   RoBERTa ( Liu et al . , 2019 ) for binary relevance   classiﬁcation with the user question and sentence as   inputs . We train with a polynomial decay learning   rate scheduler with learning rate 2e−5 , using the   Adam optimizer ( Kingma and Ba , 2015 ) for three   epochs . We compare this model to one trained on   the ANTIQUE ( Hashemi et al . , 2020 ) relevance   data for query - sentence relevance . The data con-   sists of Yahoo ! answers and relevance labels on   a scale from 1 - 4 , with 1 - 2 not relevant and 3 - 4   relevant .   For experiments in ClusterSumm and E2ESumm ,   our baseline abstractive text summarization model   is BART ( Lewis et al . , 2020 ) , a pretrained denois-   ing autoencoder that builds off of the sequence - to-   sequence transformer of Vaswani et al . ( 2017 ) . For   E2ESumm results , our primary focus , we also ap-   ply several state - of - the - art abstractive summariza-   tion models such as T5 - base ( Raffel et al . , 2019 ) .   For the cluster summarization task , the input is   the individual sentences clustered by the annota-   tors , while for the cluster fusion step , the input is   the concatenation of the cluster summaries . For   E2ESumm , input to the models is the question con-   catenated with input answers . For both summa-   rization tasks , we ﬁne - tune BART using a polyno-   mial decay learning rate scheduler with learning   rate3×10 , using the Adam optimizer ( Kingma   and Ba , 2015 ) . We train with 500 warm - up steps   and 20,000 total steps and pick the model with the   best label - smoothed cross - entropy ( Szegedy et al . ,   2016 ) validation loss . T5 is trained for 3 epochs   with a linear learning rate scheduler . In RL ex-   periments , we train using BART from scratch , as   opposed to using a model already ﬁne - tuned on   answer summarization , as we found that this model   better learned to follow the given rewards . Fol-   lowing similar ratios in Lu et al . ( 2019 ) , we set   ( γ , γ ) = ( 0.9 , 0.1 ) . Hyperparameters are tuned   on the validation set .   6 Results & Discussion   We provide strong baseline results for the SentS-   elect , ClusterSumm , ClusterSummFusion , and   E2ESumm subtasks of AnswerSumm as a basis   for future work .   The best results for SentSelect are yielded by   RoBERTa relevance classiﬁcation as illustrated in   Table 5 . RoBERTa yields an F1 score of 0.49 . De-   spite being the highest , the relatively low result   points to the difﬁculty and subjectivity of selecting   relevant sentences for community question answer-   ing fora . This is further supported by the observed   low IAA of fair agreement ( Fleiss Kappa of 0.25 ) .   Moreover , concatenating the sentences labeled as   relevant on the test set as a ﬁnal summary results in   long summaries with high recall ( 82.81 ROUGE-1   Recall ) . This suggests that much of the impor-   tant information to be summarized can be captured   by this relevance model . The ANTIQUE - trained   model obtains an F1 score of 0.41 and notably   predicts many false positives ( 71 % ) . While this2514   model performs worse on this relevance classiﬁ-   cation task , we ﬁnd that using this trained model   for automatically generated data allows for an im-   proved downstream summarization model , when   compared to the better classiﬁer trained solely on   our manually - annotated data . Accordingly , we opt   for using the ANTIQUE - trained model in our over-   all summarization task . The improved performance   is likely due to more sentences being labeled as rel-   evant ( implicitly encoding a recall bias ) , allowing   for more sentences to be sent to the clustering al-   gorithm , a noisy step itself , ensuring better quality   clusters .   Results for ClusterSumm and ClusterSummFu-   sion are shown in Table 6 . These results point to   the difﬁculty of ClusterSumm as one of the sources   of difﬁculty for E2ESumm performance , as Cluster-   SummFusion can be done fairly easily . We believe   that some difﬁculties found in ClusterSumm are   also found in the E2ESumm task .   The results for E2ESumm are presented in Table   7 . BART - large outperforms T5 model , but scores   are rather low when compared to the extractive or-   acle above . To investigate this further , we train   a BART - only model using the question concate-   nated with the oracle relevant sentences chosen by   the annotators , BART - rel - oracle . BART - rel - oracle   signiﬁcantly outperforms the vanilla model . This   suggests that improved content selection would   boost performance . However , we believe that the   primary cause of the low performance is the difﬁ-   culty in learning the compression rate and abstrac-   tiveness of the gold summaries . The percentage   of novel uni - grams in BART is only 4 % , as op-   posed to the 21 % present in the gold summaries .   This suggests that despite being trained on more   abstractive data , BART is not learning ( not gen-   eralizing ) how to abstract well enough . We also   note the model trained on additional augmented   data through our automatic pipeline , BART - aug ,   achieves a large performance boost compared to   vanilla BART , thereby validating the efﬁcacy of our   automatic pipeline for potential applications to new   domains . It should be noted that we experimented   with augmenting our manually - curated data with   data from CQASumm , but performance did not im-   prove over vanilla BART . Hence , the task is indeed   sensitive to the quality and type of data used for   augmentation .   The results of adding RL rewards to BART   trained with data augmentation are shown in Ta-   ble 8 . Both BART with augmented data and RL   rewards achieve higher NLI scores than the base-   line , while only the model with RL rewards ob-   tains a higher Semantic Area score . The improved   ROUGE score for BART - aug likely results from   training on additional data that resembles the target   domain , as in Fabbri et al . ( 2021 ) , while noise in   the unsupervised data may reduce the Semantic   Area scores . The addition of RL rewards improves   the semantic area score over the augmented model ,   although the slight decrease in ROUGE-1/2 show   that semantic area does not completely align with   ROUGE score . We analyzed 25 model outputs   for factual consistency and found that the models   are largely factually consistent , and very extractive   ( BART - aug + RL having the fewest novel unigrams   at 3.9 $ ) . This suggests these differences in NLI   score do not exhibit a large qualitative difference   in faithfulness , and the lower NLI score of the RL   model may be from the introduction of the semantic   area reward . Also , we note that the gold summaries   themselves have low NLI and Semantic Area scores   of 0.46 and 0.03 . As the gold summaries are more   abstractive , the entailment relationship between   them and the input may not be as straightforward   as the primarily extractive model outputs . This   phenomenon suggests the need for improved met-   rics and rewards for abstractive factual consistency   and semantic coverage . We provide example sum-   maries in the supplementary materials.25157 Conclusion and Future Work   We develop an annotation pipeline for multi-   perspective answer summarization , introducing the   largest human - annotated dataset for this task . We   benchmark state - of - the - art models on the content   selection , cluster summarization , and end - to - end   summarization subtasks of this dataset . We also   introduce a pipeline for answer summarization   data augmentation that boosts summarization per-   formance . Through an analysis of the effects of   reinforcement - learning rewards and qualitative ex-   amination of model outputs , we point to difﬁculties   in these tasks and areas for future improvement in   content selection , abstraction levels , and metrics   for model comparison .   8 Ethical Considerations   As we propose a novel conversation summarization   dataset creation pipeline and modeling components ,   this section is divided into the following two parts .   8.1 New Dataset   Intellectual Properties and Privacy Rights We   make use of publicly - available StackExchange data   for all our annotations . We manually reviewed our   dataset output for quality and potential problems .   Compensation for Annotators Compensation   was determined by standard in - house rates , amount-   ing to about $ 6 per data point collected .   8.2 NLP Application   Bias Biases may exist in the datasets , such as   political bias and gender bias in Yahoo ! Answers .   Thus , models trained on these datasets may propa-   gate these biases .   Misuse Potential and Failure Mode When   used as intended , applying the summarization mod-   els described in this paper can save people much   time . However , the current models are still prone   to producing hallucinated summaries , and in such a   case , they may contribute to misinformation on the   internet . We move the needle in faithful summa-   rization in this paper , but further research is needed   to ensure the faithfulness of abstractive summaries   to address this issue , as this issue is present among   all current abstractive summarization models .   Environmental Cost The experiments described   in the paper make use of V100 GPUs . We used   up to 8 GPUs per experiment . The experimentsmay take several hours . Several dozen experiments   were run due to parameter search , and future work   should experiment with distilled models for more   light - weight training . We note that while our work   required extensive experiments to draw sound con-   clusions , future work will be able to draw on these   insights and need not run as many large - scale com-   parisons . Models in production may be trained   once for use using the most promising settings .   References251625172518   A Appendix   We provide sample model outputs in Tables 9 and   10 which characterize the factual consistency and   multi - perspective nature of the models.25192520