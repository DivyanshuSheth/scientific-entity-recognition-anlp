  Katherine Stasaski and Marti A. Hearst   UC Berkeley   { katie_stasaski , hearst}@berkeley.edu   Abstract   Generating diverse , interesting responses to   chitchat conversations is a problem for neu-   ral conversational agents . This paper makes   two substantial contributions to improving di-   versity in dialogue generation . First , we pro-   pose a novel metric which uses Natural Lan-   guage Inference ( NLI ) to measure the seman-   tic diversity of a setof model responses for   a conversation . We evaluate this metric us-   ing an established framework ( Tevet and Be-   rant , 2021 ) and ﬁnd strong evidence indicat-   ing NLI Diversity is correlated with semantic   diversity . Speciﬁcally , we show that the con-   tradiction relation is more useful than the neu-   tral relation for measuring this diversity and   that incorporating the NLI model ’s conﬁdence   achieves state - of - the - art results . Second , we   demonstrate how to iteratively improve the se-   mantic diversity of a sampled set of responses   via a new generation procedure called Diver-   sity Threshold Generation , which results in an   average 137 % increase in NLI Diversity com-   pared to standard generation procedures .   1 Introduction   Dialogue models often struggle to produce engag-   ing utterances in conversations , tending to generate   responses which are common in the training data ,   such as “ OK , ” “ Yeah , ” or “ I do n’t know ” ( Li et al . ,   2016 ) . While these responses are appropriate for a   wide variety of contexts , their over - production can   result in a dull conversation ( See et al . , 2019 ) .   An evaluation task has emerged that consists   of measuring the diversity of chitchat model re-   sponses over a test set . While some past work uses   human evaluation to measure model response di-   versity according to engagingness , speciﬁcity , or   interestingness ( Li et al . , 2016 ; See et al . , 2019 ;   Ghandeharioun et al . , 2019 ) , several automated   metrics have also been proposed to measure diver-   sity of model responses . Some metrics measure   lexical diversity , typically via n - gram overlap ( LiFigure 1 : Illustration of NLI Diversity using human   responses from DailyDialog++ . Contradictions are   weighted by 1 , entailments by -1 , and neutrals by 0 ,   so the score is ( 2×1 ) + ( 3×0 ) + ( 1×−1 ) = 1 .   et al . , 2016 ) or computing the BLEU score ( Zhu   et al . , 2018 ) among model responses generated   from the test set . Other past work attempts to mea-   sure semantic diversity via repurposing sentence   similarity metrics ( Tevet and Berant , 2021 ; Zhang   et al . , 2020a ; Cer et al . , 2017 ) .   We propose a new metric aimed at measuring se-   mantic diversity by leveraging a Natural Language   Inference ( NLI ) model to score a set of multiple   dialogue model responses for a single conversation ,   as illustrated in Figure 1 . NLI is a three - way clas-   siﬁcation task to determine whether one sentence   entails , contradicts , or is neutral toward a second   sentence . We hypothesize that a diverse set of re-   sponses for a conversation captures contradictory   ways one could respond , which can be measured   by the NLI model . We aggregate the contradiction ,   neutral , and entailment predictions among pairs of85responses from the set and combine the predictions   into a new diversity metric , called NLI Diversity .   We additionally explore two modiﬁcations of   NLI Diversity . First , because the neutral predic-   tion may be indicative of diversity , we propose   Neutral NLI Diversity , where neutral predictions   are weighted the same as contradiction predictions .   Second , since our Baseline NLI Diversity method   does not take into account the conﬁdence of the   model ’s prediction , we propose Conﬁdence NLI   Diversity , which aggregates the probability mass of   the model ’s predicted class instead of aggregating   the number of predictions for each class .   We assess NLI Diversity using Tevet and Be-   rant ( 2021 ) ’s diversity metric evaluation frame-   work , ﬁnding that NLI Diversity is highly corre-   lated both with human judgments of diversity and   with the diversity parameter , a gold standard di-   versity value used to generate the set of responses .   Conﬁdence NLI Diversity achieves state - of - the - art   performance in terms of correlation with semantic   diversity . Also , through an ablation study , we ﬁnd   positive , neutral , and negative correlations between   human judgments and the number of contradiction ,   neutral , and entailment predictions , respectively .   We next explore the use of a dialogue model to   generate a set of candidate responses with a mini-   mum target level of semantic diversity , such as 10   Contradictions . Our new generation procedure , Di-   versity Threshold Generation , iteratively improves   a set of model responses until this intended thresh-   old is reached . If a set of sampled responses does   not meet the intended threshold , the lowest - scoring   response is thrown out and a new response is sam-   pled until the diversity threshold is reached . We   show this procedure results in a more diverse set   of responses than the original sampled set , often   with only a few resampled responses . Results of   automated analysis shows relevancy is maintained   from initial to ﬁnal sets of responses .   In summary , our contributions are :   •A novel diversity metric , NLI Diversity , eval-   uated using Tevet and Berant ( 2021 ) ’s frame-   work , that measures semantic diversity and   interrogates the relationship between Contra-   diction and Neutral predictions and diversity ,   •Conﬁdence NLI Diversity , a diversity metric   which obtains state - of - the - art performance on   semantic diversity ,   •A new dialogue generation procedure , Diver-   sity Threshold Generation , which continuessampling responses until an intended diver-   sity threshold , deﬁned using NLI Diversity , is   reached ,   •Experimental results indicating dialogue mod-   els are able to generate diverse responses us-   ing Diversity Threshold Generation with min-   imal loss in relevancy .   2 Related Work   Past work has explored lexical and semantic diver-   sity metrics as well as ways of evaluating these   metrics . We also draw from work in NLI and gen-   erating diverse sets of hypotheses .   2.1 Measuring Model Response Diversity   Traditionally , a model ’s diversity has been mea-   sured in terms of its predictions over the test set ( Li   et al . , 2016 ) , which we call Test Set Diversity . In   this setup , the model predicts one response for each   conversation in the test set ( containing nconver-   sations ) , resulting in npredictions . The diversity   measure is computed over these npredictions , re-   sulting in a score over the entire test set .   The notion of diversity we investigate , however ,   measures the model ’s ability to generate a setof   responses for a single conversation ( Zhang et al . ,   2019 ; Tevet and Berant , 2021 ) , which we call Multi-   Response Diversity . Instead of generating one re-   sponse for each of the conversations in the test   set , we evaluate a model ’s ability to generate m   responses for each of the nconversations .   As shown by Tevet and Berant ( 2021 ) , metrics   which have been proposed in the Test Set Diversity   setting can still be applied in the Multi - Response   Diversity setting , however , by treating each set of   mresponses as its own “ test set ” and averaging   over thentotal sets .   2.2 Diversity Metrics   Lexical diversity metrics measure differences in   word choice , as opposed to diversity of content . Li   et al . ( 2016 ) propose distinct - n , which measures   the number of unique n - grams generated divided by   the total number of n - grams generated in the Test   Set Diversity setting . Some past work has applied   this metric to the Multi - Response Diversity setting   ( Tevet and Berant , 2021 ) . Cao and Clark ( 2017 )   propose examining the percent of unique responses   over the test set . Other past work has proposed   using BLEU score over a set of model responses in   theTest Set Diversity setting ( Zhu et al . , 2018).86Semantic diversity metrics , on the other hand ,   compare diversity of the content present in each   response . Many of these measures are adapted   from semantic similarity scores , since lower simi-   larity can indicate higher diversity ( Tevet and Be-   rant , 2021 ) . BERTScore measures the similarity of   BERT embeddings for each token in two sentences   ( Zhang et al . , 2020a ) . Bert - STS assigns a score   based on the semantic similarity of two sentences   ( Tevet and Berant , 2021 ) . The Sent - BERT met-   ric computes cosine similarity between BERT sen-   tence embeddings ( Reimers and Gurevych , 2019 ) .   Larson et al . ( 2019 ) propose identifying diverse   paraphrases by identifying embedding outliers .   Other past work has used human evaluation to   measure a model ’s diversity . Li et al . ( 2016 ) ask   humans to choose the better of two responses based   on speciﬁcity to the past conversation . See et al .   ( 2019 ) ask humans to rank dialogue responses on   a variety of factors , including interestingness and   inquisitiveness . Tevet and Berant ( 2021 ) compare   participants ’ ability to judge diversity of a set of   responses in two ways : ( i ) by ranking one response   as more diverse than a second response and ( ii )   by judging the diversity of a single response on a   Likert scale , ﬁnding that participants were equally   able to judge diversity in both conditions . They also   ﬁnd that human judges are better at distinguishing   semantic diversity than lexical diversity .   Other past work has incorporated diversity met-   rics into the dialogue dataset creation pipeline .   Stasaski et al . ( 2020 ) propose a method which mea-   sures the diversity of a crowdworker ’s contribu-   tions compared to a corpus , using that information   to determine when to stop collecting data from the   worker . This results in a more diverse dataset .   2.3 Evaluation of Diversity Metrics   Tevet and Berant ( 2021 ) propose a framework to   examine the reliability of diversity metrics . They   propose the notion of a diversity parameter , which   is used to generate a set of model responses , e.g. ,   thep - value in nucleus sampling , which speciﬁes   the vocabulary probability distribution cutoff used   to restrict sampling to the most - likely words whose   combined likelihood ≥p . Ifpis higher , the set of   responses should have higher diversity , and vice-   versa . This diversity parameter is treated as a gold   standard for a set of responses ’ diversity . Diver-   sity metrics assign scores in the Multi - Response   Diversity condition and are evaluated in terms ofcorrelation to the diversity parameter . They further   propose two datasets to evaluate diversity metrics :   one which includes model responses and contains   varying levels of lexical diversity and one which is   human - created and maintains high lexical diversity   to allow focused evaluation of semantic diversity .   2.4 Natural Language Inference   Natural Language Inference is a task aimed at pre-   dicting whether one sentence contradicts , entails ,   or is neutral towards a second sentence . Models for   NLI are typically trained using one of two datasets :   Stanford Natural Language Inference ( SNLI ) ( Bow-   man et al . , 2015 ) or Multi - Genre NLI ( MNLI )   ( Williams et al . , 2018 ) . More recent datasets in-   clude FEVER ( Thorne et al . , 2018 ; Nie et al . ,   2019 ) , adapted from a fact - checking dataset , and   ANLI ( Nie et al . , 2020 ) , collected in an adversar-   ial human - in - the - loop procedure . With the rise of   transformer architectures , models have achieved   high performance on NLI tasks ( Liu et al . , 2019 ) .   In a dialogue setting , NLI has been used to im-   prove consistency between a persona and model   responses over the course of a conversation by inte-   grating an NLI - based reward into a reinforcement   learning training procedure ( Song et al . , 2020 ) .   To our knowledge , however , NLI has not been   used to measure the diversity of model responses in   either the Test Set Diversity or the Multi - Response   Diversity setting .   2.5 Generating Diverse Sets of Hypotheses   While work has only recently begun to explore the   task of generating multiple dialogue responses to a   conversation ( Zhang et al . , 2019 ; Tevet and Berant ,   2021 ) , past work has explored generating diverse   sets of hypotheses in some other application ar-   eas . Carbonell and Goldstein ( 1998 ) explored using   Maximal Mutual Relevance to reduce redundancy   without sacriﬁcing relevancy in document selection   for summarization . Batra et al . ( 2012 ) proposed a   greedy iterative algorithm to generate diverse , prob-   able hypotheses for multiple vision tasks . Most   related to our work is Gimpel et al . ( 2013 ) , which   applied Batra et al . ( 2012 ) ’s approach to machine   translation , generating a setof translations instead   of a single translation . In contrast to Gimpel et al .   ( 2013 ) , by holding the sampling procedure constant   throughout the iterative process , our method can ex-   plore the extent to which diversity can be increased   without altering standard decoding practices.873 NLI Diversity Metric   We propose three diversity metrics in the Multi-   Response Diversity setting which leverage the pre-   dictions of an NLI model . Two metrics ( Baseline   and Neutral ) aggregate the NLI model ’s class pre-   dictions and one metric ( Conﬁdence ) aggregates   the weight of these predictions .   3.1 Baseline NLI Diversity   We propose a new metric , called Baseline NLI   Diversity , which uses an NLI model ’s predic-   tions to measure diversity . More formally , for a   given conversation , c , and a dialogue generation   modelM , a set of utterances u, ... ,uis pro-   duced by the model . Each pair of utterances is   compared in both directions using an NLI model ,   NLI ( u , u),NLI ( u , u), ... ,NLI ( u , u ) .   The NLI model predicts a distribution over   the three potential classes : contradiction , neu-   tral , and entailment . We take the argmax over   these classes , resulting in a list of NLI predictions ,   NLI ( NLI ( u , u), ... ,NLI ( u , u))of   sizen(n−1 ) . To produce an overall diversity   score forNLI ( u, ... ,u ) , we assign each of   these classes a value representing their diversity ,   denotedNLI ( NLI ( u, ... ,u ) ) .   We hypothesize that larger numbers of entail-   ment predictions found in a set of model - generated   utterances is indicative of a lack of diversity ; simi-   larly , larger number of contradiction predictions is   indicative of a larger amount of diversity . Because   we want a higher value of NLI to indicate   higher diversity , we assign values as :   NLI =       1 if contradiction   0 if neutral   -1 if entailment   The sum of the NLI values for the set of ut-   terances results in the ﬁnal NLI Diversity score ,   formally deﬁned as :   BaselineNLIDiversity =   /summationdisplayNLI ( NLI(NLI ( u , u ) )   While the Baseline NLI Diversity metric aggre-   gates all classes , we also investigate the separate   number of entailment , contradiction , and neutral   predictions in NLI , denoted # Entailment , #   Contradiction , and # Neutral , respectively.3.2 Neutral NLI Diversity   Our primary hypothesis is that contradictions indi-   cate diversity and entailments indicate lack of diver-   sity . Because it is unclear what the role of neutrals   might be , we explore a version of NLI Diversity   which weights neutral and contradiction predic-   tions as equally diverse . This metric is the same as   Baseline NLI Diversity except the NLI used   to assign values is :   NLI =       1 if contradiction   1 if neutral   -1 if entailment   3.3 Conﬁdence NLI Diversity   Because the prior two NLI Diversity metrics do not   incorporate the conﬁdence of the NLI model ’s class   predictions , we explore an additional metric which   incorporates this value . Letting conf(u , u )   represent the model ’s probability mass assigned to   the predicted NLI class aftersoftmax , the func-   tion is deﬁned as : NLI =         1×conf(u , u)if contradiction   0 if neutral   -1×conf(u , u)if entailment   Intuitively , instead of assigning a 1 value for a   contradiction prediction , this metric assigns the   probability of the contradiction class . Likewise ,   instead of a -1 for an entailment prediction , this   metric assigns the negative probability mass of the   entailment class .   4 Evaluation of NLI Diversity   We evaluate NLI Diversity by computing the cor-   relation between the metric and both human labels   anddiversity parameter labels . Below we ﬁrst de-   scribe the models and data and then present the   results of the evaluation .   4.1 Models   We explore two NLI models : a Roberta - large   model ( Liu et al . , 2019 ) ﬁne - tuned on the Multi-   Genre NLI ( MNLI ) Corpus ( Williams et al . , 2018 )   and a Roberta - large model ﬁne - tuned on a combi-   nation of MNLI , SNLI , FEVER , and ANLI , both88decTest Mixed Lexical Diversity ;   Mixed Semantic Diversity ;   Model Generated   Examples :   temp 0.28 “ I think he is the most awe-   some guy ever ”   “ He is the most awesome guy   ever ”   temp 0.55 “ The unemployment rate is   lower than what it is ”   “ No but it does make it more   likely to be higher than what   it is ”   conTest High Lexical Diversity ;   Mixed Semantic Diversity ;   Human Generated   Examples :   high lexical   and“Sorry , but I do n’t agree . ”   low semantic “ I think you are wrong about   that . ”   “ Do nt be so judgemental , try   to see   high lexical   andthings her way . ”   high seman-   tic“You are right that is insane . ”   containing 300 M parameters . We refer to these   models as NLI Diversity – MNLI andNLI Diver-   sity – Combined , respectively . We do not employ   additional ﬁne - tuning of these models .   4.2 Data   There are two different English datasets released   to evaluate diversity metrics in Tevet and Berant   ( 2021 ): conTest anddecTest , described in Table   1 . The conTest dataset is human - created and cap-   tures content , orsemantic , diversity independent   oflexical diversity . Low - diversity examples in this   dataset have high lexical diversity but low seman-   tic diversity . This dataset was created by asking   crowdworkers to generate sets of utterances with   either low or high semantic diversity using variedlanguage , in order to keep a high level of lexical   diversity constant across both conditions .   The decTest dataset includes model - generated   responses , with diversity controlled by a decoding   parameter , such as a temperature parameter . The   dataset can include duplicate responses , and does   not attempt to mediate lexical diversity ; therefore ,   low - diversity examples in this dataset may reﬂect   low lexical as well as low semantic diversity .   While the original dataset includes multiple gen-   eration tasks , we evaluate on the dialogue task ,   respGen , which is drawn from Reddit conversa-   tions ( Hashimoto et al . , 2019 ) . There are 200   conversations for each of conTest anddecTest for   therespGen task , with multiple responses for each   conversation ( 5 for conTest , 10 for decTest ) .   4.3 Diversity Parameter Correlation   The diversity parameter from Tevet and Berant   ( 2021 ) represents either a parameter directly used   to generate responses via a dialogue model , such   aspin nucleus sampling , or a binary value indi-   cating whether crowdworkers were instructed to   generate a high- or low - diversity set of responses .   A measure which is able to capture diversity will be   positively correlated with this diversity parameter .   Table 2 shows Spearman ’s correlations between   NLI Diversity and the diversity parameter . On the   conTest semantic diversity dataset , Conﬁdence NLI   Diversity achieves the highest correlation of all   metrics ( 0.62 ) and approaches human performance .   Baseline NLI Diversity performs comparably to   the top - performing automatic metric from Tevet   and Berant ( 2021 ) , at 0.59 correlation . We note the   95 % conﬁdence intervals overlaps between Base-   line NLI Diversity , Conﬁdence NLI Diversity , Sent-   BERT , and human judgements , indicating a lack of   signiﬁcant differences ( see Appendix A ) . Although   Neutral NLI Diversity does relatively poorly on   conTest ( 0.24 ) , it is the highest - performing NLI   metric on decTest ( 0.72 ) , suggesting that incor-   porating neutral predictions may capture lexical   instead of semantic diversity .   A histogram of Conﬁdence NLI Diversity val-   ues for low and high semantic diversity sets of   responses is shown in Figure 2 . We note the lack   of large overlap between the distributions of low   and high semantic diversity data . In addition to89decTest conTest   Metric ρρ   Human Performance ( ab-   sHDS)0.81 0.63   distinct - n 0.89 0.34   cos - sim 0.89 0.33   BERT - STS 0.81 0.46   Sent - BERT 0.80 0.59   BERTScore 0.87 0.49   Baseline NLI Diversity –   MNLI0.58 0.59   Baseline NLI Diversity –   Combined0.39 0.59   Neutral NLI Diversity 0.72 0.24   Conﬁdence NLI Diversity 0.44 0.62   the correlation results in Sections 4.3 and 4.4 , this   result indicates the Conﬁdence NLI Diversity met-   ric distinguishes between low and high semantic   diversity .   The higher correlation to the diversity parameter   leads us to choose NLI Diversity - MNLI instead   of Combined for all further experimentation .   4.4 Human Correlation   In this subsection , we examine the NLI Diversity   metric ’s correlation to the human annotations col-   lected by Tevet and Berant ( 2021 ) . Each set of   responses in conTest anddecTest is scored by 10   annotators from 1 ( not diverse at all ) to 5 ( very   diverse ) with half - point increments . We compute   correlation with respect to the averaged rating .   In addition to NLI Diversity , we explore the pre-   diction counts for each category . We expect that a   higher # Entailment value will be negatively cor-   related with diversity because the more pairs of   responses that entail each other , the more similar   the set of responses is . Similarly , we expect that   a higher # Contradiction value will be positively   correlated with diversity . Since the NLI Diversity   metric incorporates both # Entailment and # Con-   tradiction , we would expect this metric to be highly   correlated with human judgments as well .   MetricdecTest   ρconTest   ρ   Baseline NLI Diver-   sity0.48 0.63   Neutral NLI Diver-   sity0.69 0.40   Conﬁdence NLI Di-   versity0.41 0.64   # Contradiction 0.26 0.46   # Neutral 0.05 −0.08   # Entailment −0.48 −0.65   Spearmean ’s ρrank correlation results between   our metrics and the human diversity scores are   shown in Table 3 . The highest - performing cor-   relation for lexical diversity is the Neutral NLI   Diversity ( 0.69 ) . The highest - performing semantic   diversity correlation is Conﬁdence NLI Diversity   ( 0.64 ) . Additionally , Baseline and Conﬁdence NLI   Diversity correlations are stronger when evaluating   with the conTest dataset than the decTest dataset   ( an increase of 0.48 to 0.63 for Baseline MNLI and   0.41 to 0.64 for Conﬁdence NLI ) , indicating these   metrics are more correlated with human ratings of   semantic diversity than lexical diversity .   Across both datasets , # Entailment is negatively   correlated with diversity , # Neutral does not have   a strong correlation , and # Contradiction is posi-   tively correlated , as hypothesized . This supports90our motivation to use NLI as a diversity metric .   5 Diversity Threshold Generation   We have veriﬁed that NLI Diversity is both able to   capture semantic diversity and aligns with human   judgements . We can additionally use NLI Diver-   sity to deﬁne a straightforward desired diversity   threshold , div for a set of model - generated re-   sponses , u, ... ,u . For example , we might intend   there to be 10 Contradictions within the set . We   propose a generation procedure , Diversity Thresh-   old Generation , designed to iteratively increase the   diversity of a set of responses for a conversation .   For a conversation , Diversity Threshold Genera-   tion begins by sampling nresponses . We score the   diversity of these responses using a diversity met-   ric , div_metric ( u, ... ,u ) . If the diversity score   falls abovediv , the process is ﬁnished .   If , however , the score falls below div ,   we identify the model response which con-   tributes least to the diversity score by calculat-   ingdiv_metric ( u, ... ,u)for each sub - group   of model responses of size n−1 . We discard   the model response not present in the highest-   scoring subgroup and resample a new response .   We re - calculate div_metric ( u, ... ,u)and if   div_metric ( u, ... ,u ) > div , the process   ﬁnishes . We continue resampling until the maxi-   mum cutoff of Sis reached .   6 Evaluation of Diversity Threshold   Generation Method   6.1 Models and Datasets   We experiment with two neural dialogue mod-   els , DialoGPT ( 700 M parameters ) ( Zhang et al . ,   2020b)and BlenderBot 1.0 ( 300 M parameters )   ( Roller et al . , 2021 ) . We use the default Trans-   formers implementation for each model ( Wolf et al . ,   2020 ) and do not ﬁne - tune them . Runtime was be-   tween 3 and 36 hours on one Titan - X GPU .   All experiments involve the dialogue model M   generating 5 responses for each conversation . The   maximum number of samples , S , is set to 20 . All   experiments are averaged over 10 trials for stability .   We evaluate each model on the development set   of two public English conversational datasets : Dai-   lyDialog++ ( 1,028 conversations ) ( Sai et al . , 2020;Li et al . , 2017 ) and EmpatheticDialogues ( 2,763   conversations ) ( Rashkin et al . , 2019 ) . DailyDia-   log++ includes 5 human - written responses per con-   versation , allowing for multi - reference comparison .   We split each EmpatheticDialogues conversation   at a random turn ( consistent for all experiments )   for generation . Since BlenderBot supports up to   128 positional embeddings , we pass in the last 128   tokens of the conversation for this condition .   6.2 Metrics   We evaluate three diversity metrics : two semantic   diversity metrics , Baseline NLI Diversity ( Section   3 ) and Sent - BERT ( Reimers and Gurevych , 2019 ;   Tevet and Berant , 2021 ) , and one lexical diversity   metric , distinct - n ( Li et al . , 2016 ; Tevet and Berant ,   2021 ) . For Sent - BERT , we compute the average   negative cosine similarity between BERT sentence   embeddings for each pair of responses . Like Tevet   and Berant ( 2021 ) , for distinct - n , we compute the   average distinct n - grams from n∈1,2,3,4,5 .   Because Baseline NLI Diversity is more human-   interpretable than Conﬁdence NLI Diversity , we   use this version for experimentation . For all NLI   Diversity experiments , div is achieved when   # Contradictions is greater than 10 out of a total   of 20 pair - wise comparisons . For both Sent - BERT   and distinct - n , however , we do not have a human-   speciﬁable threshold . We use empirical thresholds   measured from the sets of 5 human responses for   each conversation in DailyDialog++ . We choose   the 90th percentile for div ( 0.98 and -0.179   for distinct - n and Sent - BERT , respectively ) .   We decode using nucleus sampling ( p= 0.9 ) ,   as it has been shown to increase response diversity   ( Holtzman et al . , 2020 ) . However our method could   be applied with other decoding procedures .   In order to robustly evaluate Diversity Threshold   Generation , we measure both ( i ) whether Diversity   Threshold Generation is able to generate more di-   verse sets of responses than was originally sampled   and ( ii ) whether the increased diversity comes at the   expense of decreased relevancy of the responses .   6.3 Diversity Results   We aim to measure whether the diversity of the 5 re-   sponses from Mincreases using Diversity Thresh-   old Generation , compared to the initial 5 sampled   responses . Diversity of the starting and ending sets   of utterances is measured by Baseline NLI Diver-   sity , distinct - n , or Sent - BERT . We also report the91Met-   ricMo-   delData-   setStart-   ing   Div . End-   ing   Div . Num .   Sam-   pledDGDaily 4.11 10.24 6.3   Emp 3.68 10.11 7.1   BBDaily −5.55 2.51 14.4   Emp −8.90 −1.72 16.5DGDaily 0.95 0.98 5.4   Emp 0.43 0.52 20.0   BBDaily 0.61 0.80 20.0   Emp 0.52 0.71 20.0DGDaily −0.26 −0.16 5.2   Emp −0.28 −0.16 5.8   BBDaily −0.62 −0.40 19.0   Emp −0.71 −0.52 19.7   number of sampled utterances required to reach   div .   Results for Diversity Threshold Generation are   shown in Table 4 . For every condition , we see an   increase from starting to ending diversity ; for NLI   Diversity , this results in an average 137 % increase .   For most conditions , distinct - n requires more sam-   ples than Sent - BERT and Baseline NLI Diversity .   We can use the results of Diversity Threshold   Generation to probe differences in the models .   In our experimental setup , DialoGPT generates   more diverse utterances across all conditions than   BlenderBot . The models change by similar pro-   portions from starting to ending diversity using the   NLI metric . However , the starting diversity for   BlenderBot is far lower than DialoGPT ; the neg-   ative value for BlenderBot indicates that a large   number of entailment predictions were present in   the starting response set .   We can also examine differences between the   datasets . For instance , we observe lower starting   diversities for the Empathetic Dialogues dataset   than for DailyDialog++ for both models . Addi-   tionally , the number of samples required for Em-   patheticDialogues is consistently higher than for   DailyDialog++ . This is likely because divfor both datasets was calculated using human re-   sponses from DailyDialog++ , since EmpatheticDia-   logues does not include multiple human responses .   Sampled responses can be seen in Appendix B   and results reporting the average overlap from start-   ing to ending sets of responses is in Appendix C.   Appendix D includes results using beam search in-   stead of nucleus sampling , and Appendix E reports   the stability of Diversity Threshold Generation .   6.4 Relevance Results   Since past work has documented a tradeoff between   diversity and relevancy ( Zhang et al . , 2018 ) , we   also report results for the relevancy of the start-   ing and ending sets of responses for Diversity   Threshold Generation . We use two established   relevancy metrics : BLEU Score ( Papineni et al . ,   2002)and BERTScore ( Zhang et al . , 2020a ) . We   show results on DailyDialog++ , which has mul-   tiple human - generated responses for comparison ,   which is more correlated to human judgements than   single - reference evaluation ( Gupta et al . , 2019 ) .   Results are shown in Table 5 . The key takeaway   is that the relevancy values remain virtually un-   changed when using the Diversity Threshold Gen-   eration procedure , according to both BLEU score   and BERTScore . The average percent difference is   0.08 % for BertScore and 1.1 % for BLEU .   7 Discussion   Limitations . While NLI Diversity is highly-   correlated with human judgements of diversity , it   is limited by the NLI model chosen . Compared to   Sent - BERT , the dataset used to train the NLI model   is limited in scope . While our experiments showed   that an NLI model trained on more datasets ( Com-   bined ) did not perform better than MNLI , future   work can more explicitly explore the effect of more   generalized data on NLI Diversity .   This work is limited by automatic evaluation   metrics for diversity and relevance . Future work   should conduct additional human validation of   model responses . More work could also be done   to examine cases where the model was not able   to generate diverse set , such as when humans also   ﬁnd creating a diverse set of responses difﬁcult .   Future Work . Our results showed Conﬁdence   NLI Diversity was highly correlated with both92Metric Model Starting   BERT   ScoreEnding   BERT   ScoreStart-   ing   BLEUEnd-   ing   BLEU   NLIDG 0.862 0.862 0.317 0.318   BB 0.868 0.867 0.367 0.368   Distinct-   nDG 0.862 0.861 0.319 0.306   BB 0.867 0.867 0.366 0.367   Sent-   BERTDG 0.863 0.862 0.318 0.313   BB 0.868 0.867 0.366 0.366   human judgements and the diversity parameter ,   achieving state - of - the - art performance on a seman-   tic diversity dataset . The ablation study deepened   this ﬁnding , showing that NLI contradiction predic-   tions are especially correlated with diversity . Fu-   ture work can leverage this ﬁnding , e.g. , by word-   ing crowdworker instructions to ask for generation   contradictory , rather than diverse , responses .   Our results also show that dialogue generation   models are able to improve the diversity of a sam-   pled sets of responses using Diversity Threshold   Generation . Diversity Threshold Generation can be   used to evaluate future models ’ capacity to generate   multiple diverse responses .   Future work should compare the resulting di-   verse responses in a conversational context . Studies   could be conducted where chatbot users or dialogue   writers can choose the way they want the model to   respond , similar to Clark and Smith ( 2021 ) .   8 Conclusion   We propose a novel semantic diversity metric , NLI   Diversity , which is highly correlated to human judg-   ments . Conﬁdence NLI Diversity achieves state - of-   the - art results on measuring semantic diversity . We   propose Diversity Threshold Generation to incen-   tivize production of diverse sets of responses for   a conversation . This results in more diverse sets   of responses than originally sampled for multiple   models , datasets , and metrics while maintaining   relevancy , and can also be used to investigate a   model ’s ability to produce diverse responses .   Acknowledgements   This work was supported by an AWS Machine   Learning Research Award , an NVIDIA Corpora-   tion GPU grant , an AI2 Key Scientiﬁc ChallengeProposal grant , and a National Science Founda-   tion ( NSF ) Graduate Research Fellowship ( DGE   1752814 ) . We thank the anonymous ARR review-   ers as well as Philippe Laban , Dongyeop Kang ,   Nate Weinman , and the Hearst Lab Research Group   for their helpful comments .   References939495   Metric Model Dataset Utterance   OverlapDGDaily 2.63   Emp 2.42   BBDaily 1.78   Emp 1.73DGDaily 2.89   Emp 0.87   BBDaily 1.51   Emp 1.65DGDaily 3.11   Emp 3.0   BBDaily 1.56   Emp 1.64   A Conﬁdence Interval Analysis   We perform experimentation using bootstrapping   to determine conﬁdence intervals for conTest cor-   relations to the diversity parameter . We sample a   dataset of 110 elements ( 50 % of the original con-   Testdataset ’s size ) from conTest with replacement   and compute corresponding Spearman ’s correlation   values using the sampled dataset for Sent - BERT ,   Baseline NLI Diversity , Conﬁdence NLI Diversity ,   and human judgements . We repeat this process   1,000 times for stability and calculate 95 % Conﬁ-   dence Intervals . The full conTest correlation value   plotted with these intervals can be seen in Figure   3 . While the Conﬁdence Interval values overlapbetween all 4 conditions , the Conﬁdence NLI Di-   versity distribution closely matches the human dis-   tribution .   B Sampled Responses   Table 7 shows randomly - sampled examples from   the DailyDialog++ dataset , created using Diversity   Threshold Generation with the DialoGPT model   and NLI Diversity as the intended div_metric .   C Average Utterance Overlap   We measure the number of utterances which occur   in both the starting and ending sets of responses ,   called utterance overlap . A high utterance overlap   represents a set of responses which did not need   to be signiﬁcantly changed to reach div . For   example , an utterance overlap of 4 indicates that   only 1 response needed to be resampled ( poten-   tially multiple times ) from the starting set to reach   div . Results are seen in Table 6 . Keeping in   mind that higher Average Overlap indicates less   resampling was needed , we note higher overlap for   DialoGPT than BlenderBot 1.0 ( with the exception   of distinct - n and EmpatheticDialogues ) .   D Beam Search   We evaluate beam search ’s ability to generate di-   verse utterances using Diversity Threshold Gen-   eration for DailyDialog++ and NLI Diversity . To   compare nucleus sampling to beam search , we gen-   erate 25 beams and consider these responses from   most to least probable , i.e. if the 5 most likely   beams do not satisfy the diversity threshold , we re-   move the lowest - scoring beam and replace it with   the 6th most likely beam . We ﬁnd the starting NLI   Diversity for beam search is -5.05 , the ending di-   versity is 5.35 , and an average of 10.97 sampled   utterances is required . While the NLI Diversity   does improve from the starting to ending set of   responses , beam search has a much lower ending   diversity than nucleus sampling . While past work   has conﬁrmed that nucleus sampling is more lexi-   cally diverse than beam search using Self - BLEU   ( Holtzman et al . , 2020 ) , our results conﬁrm that   nucleus sampling is also able to generate more se-   mantically diverse utterances .   E Stability of Procedure   We investigate the stability of Diversity Threshold   Generation by measuring the number of samples9697   required before reaching div across multiple   runs of the experiment . We present results for NLI   Diversity , DailyDialog++ , and DialoGPT and ob-   serve similar trends across all other conditions .   Figure 4 reports the number of resampled utter-   ances required before reaching the intended num-   ber of contradictions . Each bar color represents a   different run of the experiment . We do not observe   a large difference in number of resamples required   between runs of the same condition , indicating that   the method is stable . The last bucket contains sets   of responses which reached the maximum number   of samples , S= 20 , indicatingdiv could not   be reached.98