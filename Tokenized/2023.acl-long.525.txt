  Jun Zhao , Xin Zhao , Wenyu Zhan , Qi Zhang , Tao Gui ,   Zhongyu Wei , Yunwen Chen , Xiang Gao , Xuanjing HuangSchool of Computer Science , Fudan UniversityInstitute of Modern Languages and Linguistics , Fudan UniversitySchool of Data Science , Fudan UniversityDataGrand Information Technology ( Shanghai ) Co. , Ltd. International Human Phenome Institutes ( Shanghai )   { zhaoj19,qz,tgui}@fudan.edu.cn , zhaoxin21@m.fudan.edu.cn   Abstract   The existing supervised relation extraction   methods have achieved impressive perfor-   mance in a closed - set setting , where the rela-   tions during both training and testing remain   the same . In a more realistic open - set setting ,   unknown relations may appear in the test set .   Due to the lack of supervision signals from   unknown relations , a well - performing closed-   set relation extractor can still confidently   misclassify them into known relations . In this   paper , we propose an unknown - aware training   method , regularizing the model by dynamically   synthesizing negative instances . To facilitate a   compact decision boundary , “ difficult ” negative   instances are necessary . Inspired by text   adversarial attacks , we adaptively apply small   but critical perturbations to original training   instances and thus synthesizing negative in-   stances that are more likely to be mistaken by   the model as known relations . Experimental   results show that this method achieves SOTA   unknown relation detection without compro-   mising the classification of known relations .   1 Introduction   Relation extraction ( RE ) is an important basic task   in the field of natural language processing , aiming   to extract the relation between entity pairs from   unstructured text . The extracted relation facts have   a great practical interest to various downstream   applications , such as dialog system ( Madotto et al . ,   2018 ) , knowledge graph ( Lin et al . , 2015 ) , web   search ( Xiong et al . , 2017 ) , among others .   Many efforts have been devoted to improving   the quality of extracted relation facts ( Han et al . ,   2020 ) . Conventional supervised relation extraction   is oriented to known relations with pre - specified   schema . Hence , the paradigm follows a closed-   set setting , meaning that during both training and   testing the relations remain the same . Nowadays , Figure 1 : The decision boundary optimized only on the   known relations can not cope with an open set setting , in   which the input may come from the relations unobserved   in training . We target at regularizing the decision   boundary by synthesizing difficult negative instances .   neural RE methods have achieved remarkable   success within this setting ( Wang et al . , 2016 ;   Wu and He , 2019 ) ; and in contrast , open relation   extraction ( OpenRE ) is focused on discovering   constantly emerging unknown relations . Common   practices include directly tagging the relational   phrases that link entity pairs ( Zhan and Zhao ,   2020 ) , and clustering instances with the same   relation ( Hu et al . , 2020 ; Zhao et al . , 2021 ) .   However , relation extraction in real applications   follows an open - set setting , meaning that both   known and unknown relations are mixed within   testing data . This requires that a model can   not only distinguish among the known relations ,   but also filter the instances that express unknown   relations . The ability to filter these instances is also   called none - of - the - above ( NOTA ) detection ( Gao   et al . , 2019 ) .   Unfortunately , a well - performing closed - set   model can still confidently make arbitrarily wrong   predictions when exposed to unknown test data   ( Nguyen et al . , 2015 ; Recht et al . , 2019 ) . As9453shown in fig . 1 ( a ) , the decision boundary is   optimized only on the known relational data ( white   points ) , leading to a three - way partition of the   whole space . Consequently , the unknown relational   data ( black points ) , especially those far from the   decision boundary , will be confidently classified   into one of the known relations . By contrast , a   more compact decision boundary ( as shown in fig .   1 ( b ) ) is desirable for NOTA detection . However ,   the compact decision boundary requires “ difficult ”   negative data ( red points in fig . 1 ( b ) ) to be used ,   so strong supervision signals can be provided . It is   important to note that synthesizing such negative   data is a non - trivial task .   In this work , we propose an unknown - aware   training method , which simultaneously optimizes   known relation classification and NOTA detection .   To effectively regularize the classification , we   iteratively generate negative instances and optimize   a NOTA detection score . During the testing phase ,   instances with low scores are considered as NOTA   and filtered out . The key of the method is to   synthesize “ difficult ” negative instances . Inspired   by text adversarial attacks , we achieve the goal   by substituting a small number of critical tokens   in original training instances . This would erase   the original relational semantics and the model   is not aware of it . By using gradient - based   token attribution and linguistic rules , key tokens   that express the target relation are found . Then ,   the tokens are substituted by misleading normal   tokens that would cause the greatest increase of   NOTA detection score , thus misleading negative   instances , which are more likely to be mistaken   by the model as known relations , are synthesized .   Human evaluation shows that almost all the   synthesized negative instances do not express   any known relations . Experimental results show   that the proposed method learns more compact   decision boundary and achieve state - of - the - art   NOTA detection performance . Our codes are   publicly available at Github .   The contributions are threefold : ( 1 ) we propose   a new unknown - aware training method for more   realistic open - set relation extraction . The method   achieves state - of - the - art NOTA detection , without   compromising the classification of known relations ;   ( 2 ) the negative instances are more challenging   to the model , when compared to the mainstreamsynthesis method(e.g . , generative adversarial   network ( GAN)-based method ) ; ( 3 ) the compre-   hensive evaluation and analysis facilitate future   research on the pressing but underexplored task .   2 Related Works   Open - set Classification : The open - set setting   considers knowledge acquired during training   phase to be incomplete , thereby new unknown   classes can be encountered during testing . The   pioneering explorations in ( Scheirer et al . , 2013 )   formalize the open - set classification task , and have   inspired a number of subsequent works , which   roughly fall into one of the following two groups .   The first group explores model regularization   using unknown data . Larson et al . ( 2019 ) manually   collect unknown data to train a ( n+ 1 ) -way   classifier with one additional class , where ( n+1 )   class represents the unknown class . Instead of   manually collecting unknown data , Zheng et al .   ( 2020 ) generate feature vectors of unknown data   using a generative adversarial network ( Goodfellow   et al . , 2014 ) . Zhan et al . ( 2021 ) use MixUp   technique ( Thulasidasan et al . , 2019a ) to synthesize   known data into unknown data .   The second group approaches this problem   by discriminative representation learning , which   facilitates open - set classification by widening the   margin between known and unknown classes .   MSP ( Hendrycks et al . , 2017 ) is a maximum   posterior probability - based baseline and ODIN   ( Liang et al . , 2018 ) enlarges the difference between   known and unknown classes by adding temperature   scaling and perturbations to MSP . More recently ,   different optimization objectives such as large   margin loss ( Lin and Xu , 2019 ) and gaussian   mixture loss ( Yan et al . , 2020 ) are adopted to learn   more discriminative representations . Shu et al .   ( 2017 ) ; Xu et al . ( 2020 ) ; Zhang et al . ( 2021 ) also   impose gaussian assumption to data distribution to   facilitate distinct unknown data .   Open - set Relation Extraction : Open - set RE is   a pressing but underexplored task . Most of the   existing RE methods manually collect NOTA data   and adopt a ( n+ 1 ) way classifier to deal with   NOTA relations ( Zhang et al . , 2018 ; Zhu et al . ,   2019 ; Ma et al . , 2021 ) . However , the collected   NOTA data with manual bias can not cover all   NOTA relations and thus these methods can not   effectively deal with open - set RE ( Gao et al . , 2019).9454   Our method avoids the bias and the expensive cost   of manually collecting NOTA data by automatically   synthesizing negative data . Compared with general   open - set classification methods , our method takes   relational linguistic rules into consideration and   outperforms them by a large margin .   3 Approach   We start by formulating the open - set relation   extraction task . Let K={r , ... , r}denote   the set of known relations and NOTA indicates   that the instance does not express any relation in   K. Given a training set D={(x , y ) }   with Npositive samples , consisting of relation   instance xwith a pre - specified entity pairand   relation y∈ K , we aim to learn a open - set   relation extractor M={p(y|x ) , s(x ) } , where   θdenote the model parameters . p(y|x)is the   classification probability on the known relations   ( The NOTA label is excluded from p(y|x ) ) .   NOTA detection score s(x)is used to distinguish   between known relations and NOTA .xis classified   asNOTA ifs(x)is less than the threshold α .   Conversely , xis classified into a known relation   ˆy= arg maxp(y|x ) .   3.1 Method Overview   We approach the problem by an unknown - aware   training method , which dynamically synthesizes   “ difficult ” negative instances and optimizes the dual   objectives of both known relation classification and   NOTA detection . As shown in fig . 2 , the trainingloop consists of two iteration steps :   ①Synthesis Step : This step aims to synthesize   “ difficult ” negative instances for model regulariza-   tion . We draw inspiration from text adversarial   attacks to achieve the goal . Specifically , B=   { ( x , y)}represents a training batch sampled   fromD. For each ( x , y)∈ B , we synthesize a   negative instance by substituting the key relational   tokens of xwith misleading tokens . First , both the   attribution method and relational linguistic rules   are used to find key tokens expressing the target   relation y. Second , the misleading token wis   searched for each key token w , along the direction   of the gradient ∇s(x ) . By substituting wwith   w , it is expected for s(x)to experience its   greatest increase , so it is difficult for the model   to correctly detect the derived negative instance x   asNOTA .   ②Learning Step : This step aims to optimize the   open - set relation extractor M={p(y|x ) , s(x ) } .   Based on the training batch BfromD , we opti-   mizep(y|x)to accurately classify known relations .   To effectively detect NOTA instances , we further   synthesize negative batch B={(x , NOTA ) }   and optimize the model to widen the gap of s(x )   between x∈ B andx∈ B. Consequently ,   instances with low s(x)scores are filtered out   before being fed into p(y|x ) .   Next , we elaborate on the model structure of M   ( sec . 3.2 ) and the technical details of the synthesis   step ( sec . 3.3 ) and the learning step ( sec . 3.4 ) .   3.2 Open - set Relation Extractor   Instance Encoder and Classifier : Given an input   instance x={w , .. , w}with four reserved9455special tokens [ E],[\E],[E],[\E]marking the   beginning and end of the head and tail entities ,   the instance encoder aims to encode the relational   semantics into a fixed - length representation h=   enc(x)∈R. We adopt BERT ( Devlin et al . ,   2018 ) , a common practice , as the implementation   of the encoder . We follow Baldini Soares et al .   ( 2019 ) to concatenate the hidden states of special   tokens [ E]and[E]as the representation of the   input instance .   w , .. , w= BERT ( w , .. , w ) ( 1 )   h = w⊕w , ( 2 )   where w , w , wdenotes the hidden states   of token w,[E],[E ] , respectively . ⊕denotes   the concatenation operator . The classification   probability on known relations p(·|x)can be   derived through a linear head η ( · ):   η(h ) = Wh+b ( 3 )   p(·|x ) = Softmax ( η(h ) ) , ( 4 )   where W∈Ris the weight matrix transform-   ing the relation representation to the logits on n   known relations and bis the bias .   NOTA Detection Score : The goal of distinguish-   ing between known and NOTA relations requires   the modeling of the data density . However ,   directly estimating logp(x)can be computationally   intractable because it requires sampling from the   entire input space . Inspired by Liu et al . ( 2020 )   in the image understanding task , the free energy   function E(h)is theoretically proportional to the   probability density of training data . Considering   that it can be easily derived from the linear head   η(·)without additional calculation , the negative   free energy function is used to compute the NOTA   detection score as follows :   s(x ) = −E(h ) = log / summationdisplaye , ( 5 )   where η(h)denotes the jlogit value of η(h ) .   The detection score has shown to be effective   in out - of - distribution detection ( Liu et al . , 2020 ) .   Based on the classification probability p(·|x)and   NOTA detection score s(x ) , the open - set relation   extractor Mworks in the following way :   ˆy=/braceleftbiggarg maxp(y|x)S(x ) > α   NOTA S(x)≤α,(6 )   where αis the detection threshold.3.3 Iterative Negative Instances Synthesis   “ Difficult ” negative instances are the key to effective   model regularization . x={w , .. , w}is a   training instance with a label y. To synthesize   negative instance x , we perturb each key token w ,   which expresses the relation y , with a misleading   token w. The substitutions are expected   to erase original relational semantics without   the model being aware of it . Based on the   attribution technique and relational linguistic rules ,   a score I(w , x , y)is developed to measure the   contribution of a token w∈xto relation yas   follows :   I(w , x , y ) = a(w , x)·t(w , y)·dp(w , x),(7 )   where a(w , x)denotes an attribution score   reweighted by two linguistic scores t(w , y ) ,   dp(w , x ) . We rank all tokens according to   I(w , x , y)in descending order and take the first   ϵpercent of tokens as key tokens to perform   substitutions . Next , we elaborate on ( 1 ) how   to calculate the attribution score a(w , x)and   linguistic scores t(w , y),dp(w , x ) ; ( 2 ) how to   select misleading tokens for substitution .   Gradient - based Token Attribution : Ideally ,   when the key tokens are removed , instance xwill   no longer express the original known relation y ,   and the NOTA detection score s(x)would drop   accordingly . Therefore , the contribution of a token   wto relational semantics can be measured by a   counterfactual :   c(w , x ) = s(x)−s(x ) , ( 8)   where xis the instance after removing w.   However , to calculate the contribution of each   token in instance x , nforward passes are needed ,   which is highly inefficient . Fortunately , a first-   order approximation of contribution c(w , x)can   be obtained by calculating the dot product of word   embedding wand the gradient of s(x)with   respect to w , that is ∇s(x)·w(Feng et al . ,   2018 ) . The contribution of ntokens can thus be   computed with a single forward - backward pass .   Finally , a normalized attribution score is used , in   order to represent the contribution of each token :   a(w , x ) = |∇s(x)·w|/summationtext|∇s(x)·w| . ( 9 )   Linguistic Rule - based Token Reweighting : As   a supplement to the attribution method , linguistic9456rules that describe the pattern of relational phrases   can provide valuable prior knowledge for the   measure of tokens ’ contribution . Specifically , the   following two rules are used . Rule 1 : If a token   wsignificantly contributes to relation y , it should   appear more frequently in the instances of y , and   rarely in the instances of other relations . By   following this rule , tf - idf statistic ( Salton and   Buckley , 1987 ) t(w , y)is used to reflect the   contribution of token wto relation y(Appendix   A.1 contains additional details about the statistic ) .   Rule2 : Tokens that are part of the dependency path   between the entity pair usually express the relation   between the entity pair , while shorter dependency   paths are more likely to represent the relation   ( ElSahar et al . , 2018 ) . Following the rule , stanza   is used to parse the instance and the dependency   score as calculated as follows :   dp(w , x ) = /braceleftbigg|x|/|T | w∈ T   1 , otherwise,(10 )   where Tdenotes the set of tokens in the depen-   dency path between the entity pair . |x|,|T |denote   the number of tokens in instance xand set T ,   respectively . Eq . 10 indicates that the tokens in T   are given a higher weight , and the shorter the path ,   the higher the weight .   Misleading Token Selection : Negative instances   are synthesized by substituting key tokens with   misleading tokens . Note that we have obtained the   gradient of s(x)with respect to each token win   the attribution step . Based on the gradient vectors ,   a misleading token is selected from vocabulary V   for each key token was follows :   w= arg max∇s(x)·w . ( 11 )   Substituting wwith wis expected to cause   the greatest increase in s(x ) , so the synthesized   negative instance is misleading to the model . To   avoid that wis also a key token of a known   relation , the top 100 tokens with the highest   tf - idf statistic of each relation are removed   from the vocabulary V , when performing the   substitution . Human evaluation results show that   almost all the synthesized negative instances do   not express any known relation . In addition , we   provide two real substitution cases in tab . 6.3.4 Unknown - Aware Training Objective   In this section , we introduce the unknown - aware   training objective for open - set relation extraction .   Based on the synthesized negative samples , an   optimization of the dual objectives of both known   relation classification and NOTA relation detection   is performed . Specifically , at the mtraining step ,   A batch of training data B={(x , y)}is   sampled from D. Cross entropy loss is used for   the optimization of known relation classification :   L=1   B / summationdisplay(−logp(y|x ) ) , ( 12 )   where p(·|x)is the classification probability on   the known relations ( eq . 4 ) . For each instance   xinB , we synthesize a negative sample xas   described in sec . 3.3 , and finally obtain a batch   of negative samples B={(x , NOTA ) } . To   learn a compact decision boundary for NOTA   detection , we use the binary sigmoid loss to enlarge   the gap of detection scores s(·)between known   and synthesized instances as follows :   L = −1   B / summationdisplaylogσ(s(x ) )   −1   B / summationdisplaylog(1−σ(s(x)))(13 )   where σ(x ) = is the sigmoid function .   The overall optimization objective is as follows :   L = L+β · L , ( 14 )   where βis a hyper - parameter to balance the two   loss term .   4 Experimental Setup   4.1 Datasets   FewRel ( Han et al . , 2018 ) . FewRel is a human-   annotated dataset , which contains 80 types of   relations , each with 700 instances . We take the   top 40 relations as known relations . The middle   20 relations are taken as unknown relations for   validation . And the remaining 20 relations are   unknown relations for testing . Our training set   contains 22,400 instances from the 40 known   relations . Both the validation and test set consist of   5,600 instances , of which 50 % are from unknown   relations . Note that the unknown relations in the   test set and the validation set do not overlap.9457   TACRED ( Zhang et al . , 2017 ) . TACRED is a large-   scale relation extraction dataset , which contains 41   relations and a no_relation label indicating   no defined relation exists . Similar to FewRel ,   we take the top 21 relations as known relations .   The middle 10 relations are taken as unknown   relations for validation . The remaining 10 relations   andno_relation are unknown relations for   testing . We randomly sample 9,784 instances of   known relations to form the training set . Both the   validation and test set consist of 2,000 instances , of   which 50 % are from unknown relations . Unknown   relations in the validation set and the test set still   do not overlap .   For the specific composition of relations in each   dataset , please refer to Appendix A.4 .   4.2 Compared Methods   To evaluate the effectiveness of the proposed   method , we compare our method with mainstream   open - set classification methods , which can be   roughly grouped into the following categories :   MSP ( Hendrycks et al . , 2017 ) , DOC ( Shu et al . ,   2017 ) , ODIN ( Liang et al . , 2018 ) , Energy ( Liu   et al . , 2020 ) , and SCL ( Zeng et al . , 2021 )   detect unknown data through a carefully designed   score function or learning a more discriminative   representation . No synthesized negative instances   are used in these methods . MixUp ( Thulasidasan   et al . , 2019b ) , and Convex ( Zhan et al . , 2021 ) use   synthesized negative instances to regularize the   model . Please refer to the appendix A.3 for a brief   introduction to these methods .   We do not compare BERT - PAIR ( Gao et al . ,   2019 ) because it is only applicable to the few - shot   setting . We use DOC ( Shu et al . , 2017 ) with a   BERT encoder as an alternative method for it.4.3 Metrics   Following previous works ( Liu et al . , 2020 ; Zeng   et al . , 2021 ) , we treat all unknown instances as one   NOTA class and adopt three widely used metrics   for evaluation . ( 1 ) FPR95 : The false positive rate   ofNOTA instances when the true positive rate of   known instances is at 95 % . The smaller the value ,   the better . ( 2 ) AUROC : the area under the receiver   operating characteristic curve . It is a threshold - free   metric that measures how well the detection score   ranks the instances of known and NOTA relations .   ( 3)ACC : The classification accuracy on nknown   relations and one NOTA relation , measuring the   overall performance of open - set RE .   4.4 Implementation Details   We use the AdamW as the optimizer , with a   learning rate of 2e−5and batch size of 16for both   datasets . Major hyperparameters are selected with   grid search according to the model performance on   a validation set . The detection threshold is set to   the value at which the true positive rate of known   instances is at 95 % . The regularization weight βis   0.05 selected from { 0.01,0.05,0.1,0.15,0.5 } . See   the appendix A.2 for the processing of sub - tokens .   The dependency parsing is performed with stanza   1.4.2 . All experiments are conducted with Python   3.8.5 and PyTorch 1.7.0 , using a GeForce GTX   2080Ti with 12 GB memory .   5 Results and Analysis   5.1 Main Results   In this section , we evaluate the proposed method   by comparing it with several competitive open - set   classification methods . The results are reported in   tab . 1 , from which we can observe that our method9458   achieves state - of - the - art NOTA detection ( reflected   by FPR95 and AUROC ) without compromising   the classification of known relations ( reflected by   ACC ) . In some baseline methods ( e.g. , MSP , ODIN ,   Energy , SCL ) , only instances of known relations   are used for training . Compared with them , we   explicitly synthesize the negative instances to   complete the missing supervision signals , and   the improvement in NOTA detection shows the   effectiveness of the unknown - aware training . To   intuitively show the changes of the decision   boundary , we use the method of Yu et al . ( 2019 )   to visualize the decision boundary of the model in   the input space . As can be seen from fig . 3 , a more   compact decision boundary is learned with the help   of unknown - aware training . Although methods   such as MixUp , and Convex also synthesized   negative instances , our method is still superior tothem . This may be due to the fact that our negative   instances are more difficult and thus beneficial for   an effective model regularization ( we provide more   results in sec . 5.2 to support the claim ) .   5.2 Negative Instance Synthesis Analysis   In this section , the unknown - aware training objec-   tive is combined with the various negative instance   synthesis methods to fairly compare the perfor-   mance of these synthesis methods . The results are   shown in tab . 2 . Baseline means no negative   instances are used . Gaussian takes Gaussian   noise as negative instances and Gaussianadds   the noise to known instances . MixUp synthesizes   negative instances by convexly combining pairs of   known instances . Real means using real NOTA   instances . GAN synthesizes negative instances by   Generative Adversarial Network ( Ryu et al . , 2018 ) .   Correlation between effectiveness and difficulty .   ( 1)Gaussian with the largest ∆sperforms even   worse than Baseline in TACRED , suggesting   that overly simple negative instances are almost   ineffective for model regularization . ( 2 ) Our   method synthesizes the second difficult negative   instances ( reflected by ∆s ) and achieves the   best performance ( reflected by ACC , AUROC ,   FPR95 ) , which shows that the difficult negative   instances are very beneficial for effective model   regularization . ( 3 ) The difficulty of negative   instances of competitive methods ( e.g. , MixUp ,   Real , GAN ) is lower than that of Ours , which   indicates that it is non - trivial to achieve our diffi-   culty level . ( 4 ) Although Gaussiansynthesizes9459   the most difficult negative instances , our method   still significantly outperforms Gaussian . One   possible reason is that overly difficult instances   may express the semantics of known relations . This   leads to the following research question .   Do our synthetic negative instances really not   express any known relations ? We conduct human   evaluation to answer this question . Specifically , we   randomly select 100 synthesized negative instances   on each dataset and asked human judges whether   these instances express known or NOTA relations .   The evaluation is completed by three independent   human judges . We recruit 3 graduates in computer   science and English majors from top universities .   All of them passed a test batch . Each graduate   is paid $ 8 per hour . The results are shown in   tab . 3 , from which we can observe that : ( 1 )   More than 90 % of the negative instances do not   express any known relations ( NOTA ) . ( 2 ) Very few   instances remain in the original known relations   ( Known - Original ) or are transferred to another   known relation ( Known - Other ) . ( 3 ) There are   also some instances that are Controversial .   Some volunteers believe that the instances express   known relations , while others believe that the   instances are NOTA . In general , our synthesis   method achieves satisfactory results , but there is   still potential for further improvement .   5.3 Ablation Study   To study the contribution of each component in   our method , we conduct ablation experiments   on the two datasets and show the results in   tab . 4 . First , the attribution score measures   the impact of a token on NOTA detection of   the model . The dependency score and tf - idf   statistic reflect the matching degree between a   token and the relational linguistic rules . When   the three scores are removed , there may be some   key relational phrases that can not be correctly   identified and the performance decline accordingly .   It is worth mentioning that the model parameters   change dynamically with the training process , thus   iteratively synthesizing negative instances is crucial   for effective regularization . When the practice   is removed , the static negative instances can not   reflect the latest state of the model , and thus the   performance degrades significantly . Finally , we   remove misleading token selection by substituting   the identified key tokens with a special token   [ MASK ] and the performance is seriously hurt ,   which indicates that misleading tokens play an   important role in synthesizing difficult instances .   5.4 Hyper - parameter Analysis   We synthesize negative instances by substituting ϵ   percent of key tokens with misleading tokens . In   this section , we conduct experiments to study the   influence of substitution ratio ϵon NOTA detection .   From fig . 4 we obtain the following observations .   When the substitution ratio gradually increases   from 0 , the performance of NOTA detection is   also improved ( Note that the smaller the value of   FPR95 , the better ) . This means that an overly small   substitution ratio is not sufficient to remove all   relational phrases . The residual relational tokens   are detrimental to model regularization . When the   substitution ratio exceeds a certain threshold ( i.e. ,94600.2 ) , a continued increase in the substitution ratio   will lead to a decline in detection performance . One   possible reason is that too high a substitution ratio   can severely damage the original sentence structure ,   resulting in negative instances that differ too much   from the real NOTA instances .   6 Conclusions   In this work , we propose an unknown - aware   training method for open - set relation extraction ,   which is a pressing but underexplored task . We   dynamically synthesize negative instances by the   attribution technique and relational linguistic rules   to complete the missing supervision signals . The   negative instances are more difficult than that of   other competitive methods and achieve effective   model regularization . Experimental results show   that our method achieves state - of - the - art NOTA   detection without compromising the classification   of known relations . We hope our method and   analysis can inspire future research on this task .   Limitations   We synthesize negative instances by substituting re-   lational phrases with misleading tokens . However ,   the relational semantics in some instances may   be expressed implicitly . That is , there are no key   tokens that directly correspond to the target relation .   Therefore , we can not synthesize negative instances   based on these instances . Additionally , we consider   substitution ratio ϵas a fixed hyperparameter . It   may be a better choice to dynamically determine   ϵbased on the input instance . We leave these   limitations as our future work .   Acknowledgements   The authors wish to thank the anonymous reviewers   for their helpful comments . This work was partially   funded by National Natural Science Founda-   tion of China ( No.62206057,62076069,61976056 ) ,   Shanghai Rising - Star Program ( 23QA1400200 ) ,   Program of Shanghai Academic Research Leader   under grant 22XD1401100 , and Natural Science   Foundation of Shanghai ( 23ZR1403500 ) .   References94619462   A Appendix   A.1 Tf - idf statistic   We consider a token wto contribute significantly   to a known relation y∈ K if it occurs frequently   in the instances of relation yand rarely in the   instances of other relations . Tf - idf statistic   ( Salton and Buckley , 1987 ) can well characterize   this property . Specifically , Tf - idf consists of   term frequency and inverse document frequency .   The term frequency tf(w , y)describes how often   a token wappears in the instances of relation y :   tf(w , y ) = n(w , y)/summationtextn(w , y ) , ( 15 )   where n(w , y)denotes the number of times the   token wappears in the instances of relation y.   Obviously , some tokens ( e.g. , the stop words ) have   high tfvalues in different relational instances .   However , they do not contribute to the relational   semantics . The inverse document frequency   describes whether the token wappears only in   the instances of specific relations :   i d f(w ) = log|K|   |{y : n(w , y)̸= 0}| , ( 16)9463where|K|denotes total number of known relations   and|{y : n(w , y)̸= 0}|denotes the number   of known relations that token wappears in their   instances . Finally , we calculate t(w , y)as follows :   t(w , y ) = tf(w , y)×id f(w ) . ( 17 )   Thetf - idf statistic t(w , y)measures the con-   tribution of token wto the relation semantics of   y. We calculate and store the statistics based on   the entire training set Dbefore the training loop   start . During the training , the statistic of each token   in the vocabulary is fixed .   A.2 How to Deal With Sub - tokens ?   BERT adopts BPE encoding to construct vocabu-   laries . While most tokens are still single tokens ,   rare tokens are tokenized into sub - tokens . In   this section , we introduce how to deal with sub-   tokens when performing the substitution . First , the   tf - idf statistics and the dependency scores are   calculated at the token level and require no addi-   tional process . If a token consists of nsub - tokens ,   we calculate its attribution score by summing   the scores of all its sub - tokens . In addition , the   misleading token of this token is only selected from   the tokens that also have nsub - tokens according   toarg max / summationtext∇s(x)·w . V   denotes a vocabulary , in which all tokens consist of   nsub - tokens . wdenotes the embedding of the   ksub - token of the token w.   A.3 Compared Methods   To validate the effectiveness of the proposed   method , we compare our method with mainstream   open - set classification methods .   MSP ( Hendrycks et al . , 2017 ) . MSP assumes   that correctly classified instances tend to have   greater maximum softmax probability than samples   of unknown classes . Therefore , the maximum   softmax probability is used as the detection score .   DOC ( Shu et al . , 2017 ) . DOC builds a 1 - vs - rest   layer containing mbinary sigmoid classifiers for   mknown classes . The maximum probability of m   binary classifiers is used as the detection score .   ODIN ( Liang et al . , 2018 ) . Based on MSP , ODIN   uses temperature scaling and small perturbations   to separate the softmax score distributions between   samples of known and unknown classes .   MixUp ( Thulasidasan et al . , 2019b ) . MixUp trains   the model on convexly combined pairs of instances ,   which is effective to calibrate the softmax scores . Energy ( Liu et al . , 2020 ) . Instead of maximum   softmax probability , this method uses the free   energy E(x ) = −log / summationtexteas the detection   score of the unknown data .   Convex ( Zhan et al . , 2021 ) . The method learns a   more discriminative representation by generating   synthetic outliers using inlier features .   SCL ( Zeng et al . , 2021 ) . SCL proposes a   supervised contrastive learning objective , learning   a more discriminative representation for unknown   data detection .   A.4 Relations comprising the datasets   In this subsection , we present the known relations   contained in the training set , the unknown relations   included in the validation set , and the unknown   relations present in the test set , as shown in Table   5 .   A.5 Additional Results   Classification Accuracy : One of our key claims   is that the proposed method achieves state - of - the-   art SOTA detection without compromising the   classification of known relations . In this section ,   we provide an additional ACC metric , in which   only the instances of nknown relations are used   to calculate the classification accuracy . The metric   exactly indicates whether NOTA detection impairs   the classification of known relations . From tab . 7   we can observe that our method is comparable to   the existing method , which supports the key claim   at the beginning of the paragraph .   Two Real Substitution Cases : To intuitively show   the effectiveness of the proposed synthesis method,9464   we conduct a case study based on the “ Instrument ”   relation from FewRel and the “ Spouse ” relation   from TACRED . The tokens with top-10 tf - idf   statistics and a substitution case of each relation are   shown in tab . 6 , from which we can observe that :   ( 1 ) the tokens with high tf - idf statistics have a   strong semantic association with the target relation   ( such as Instrument - bass , Spouse - wife ) . ( 2 ) By   substituting only two critical tokens in original   training instances , the target relation is completely   erased.9465ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations section .   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   abstract and introduction sections .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   section 4   /squareB1 . Did you cite the creators of artifacts you used ?   section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   section 4   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   section 4   C / squareDid you run computational experiments ?   section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   section 49466 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   section 4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   section 4   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   section 3,4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   section 5   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   section 5   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Left blank.9467