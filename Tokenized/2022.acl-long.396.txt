  Jing Zhang , Xiaokang Zhang , Jifan Yu , Jian Tang ,   Jie Tang , Cuiping Li , Hong ChenSchool of Information , Renmin University of China , Beijing , China , Department of Computer Science and Technology , Tsinghua University , Beijing , ChinaMila - Quebec AI Institute .   { zhang-jing,zhang2718,licuiping,chong}@ruc.edu.cn ,   yujf18@mails.tsinghua.edu.cn , jian.tang@hec.ca ,   jietang@tsinghua.edu.cnAbstract   Recent works on knowledge base question an-   swering ( KBQA ) retrieve subgraphs for easier   reasoning . The desired subgraph is crucial as a   small one may exclude the answer but a large   one might introduce more noises . However ,   the existing retrieval is either heuristic or inter-   woven with the reasoning , causing reasoning   on the partial subgraphs , which increases the   reasoning bias when the intermediate supervi-   sion is missing . This paper proposes a train-   able subgraph retriever ( SR ) decoupled from   the subsequent reasoning process , which en-   ables a plug - and - play framework to enhance   any subgraph - oriented KBQA model . Exten-   sive experiments demonstrate SR achieves sig-   nificantly better retrieval and QA performance   than existing retrieval methods . Via weakly su-   pervised pre - training as well as the end - to - end   fine - tuning , SR achieves new state - of - the - art   performance when combined with NSM ( He   et al . , 2021 ) , a subgraph - oriented reasoner , for   embedding - based KBQA methods . Codes and   datasets are available online .   1 Introduction   Knowledge Base Question Answering   ( KBQA ) ( Zhang et al . , 2021 ) aims to seek   answers to factoid questions from structured   KBs such as Freebase , Wikidata , and DBPedia .   KBQA has attracted a lot of attention , as the   logically organized entities and their relations   are beneficial for inferring the answer . Semantic   parsing - based ( SP - based ) methods ( Das et al . ,   2021 ; Lan and Jiang , 2020 ; Sun et al . , 2020 ) and   embedding - based methods ( He et al . , 2021 ; Sun   et al . , 2018 , 2019 ) are two mainstream methods for   addressing KBQA . The former ones heavily rely   on the expensive annotation of the intermediate   logic form such as SPARQL . Instead of parsing   the questions , the later ones directly represent   Figure 1 : The impact of subgraph size on ( a ) answer   coverage rate and ( b ) QA performance ( Hits@1 ) of   NSM ( He et al . , 2021 ) on WebQSP ( Yih et al . , 2016 )   and CWQ ( Talmor and Berant , 2018 ) .   and rank entities based on their relevance to input   questions . Among them , the models which first   retrieve a question - relevant subgraph and then   perform reasoning on it ( He et al . , 2021 ; Sun et al . ,   2018 , 2019 ) reduce the reasoning space , showing   superiority compared with reasoning on the whole   KB ( Chen et al . , 2019a ; Saxena et al . , 2020 ; Xu   et al . , 2019 ) ( Cf . Table 2 for empirical proof ) .   Subgraph retrieval is crucial to the overall QA   performance , as a small subgraph is highly likely   to exclude the answer but a large one might intro-   duce noises that affect the QA performance . Fig-   ure 1(a ) presents the answer coverage rates of the   subgraphs with different sizes on two widely - used   KBQA datasets , WebQSP ( Yih et al . , 2016 ) and   CWQ ( Talmor and Berant , 2018 ) . We extract the   full multi - hop topic - centric subgraph and control   the graph size by the personalized pagerank ( PPR )   ( Haveliwala , 2003 ) scores of entities . We also   present the QA performance ( Hits@1 ) of NSM ( He   et al . , 2021 ) , a state - of - the - art embedding - based   model , under the same sizes of the subgraphs in   Figure 1(b ) . It is observed that although larger sub-   graphs are more likely to cover the answer , the QA   performance drops dramatically when the subgraph   includes more than 5,000 nodes . Moreover , it is   inefficient to extract such a full multi - hop subgraph   for online QA . The results show that such heuris-   5773   Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics   Volume 1 : Long Papers , pages 5773 - 5784   May 22 - 27 , 2022 © 2022 Association for Computational Linguisticstic retrieval is far from optimal . To improve the   retrieval performance , PullNet ( Sun et al . , 2019 )   proposes a trainable retriever , but the retrieving and   the reasoning processes are intertwined . At each   step , a LSTM - based retriever selects new relations   relevant to the question , and a GNN - based reasoner   determines which tail entities of the new relations   should be expanded into the subgraph . As a result ,   the inference as well as the training of the reasoner   needs to be performed on the intermediate partial   subgraph . Since the intermediate supervision is   usually unobserved , reasoning on partial subgraphs   increases the bias which will eventually affect the   answer reasoning on the final entire subgraph .   This paper proposes a subgraph retrieval en-   hanced model for KBQA , which devises a trainable   subgraph retriever ( SR ) decoupled from the sub-   sequent reasoner . SR is devised as an efficient   dual - encoder that can expand paths to induce the   subgraph and can stop the expansion automatically .   After that , any subgraph - oriented reasoner such as   GRAFT - Net ( Sun et al . , 2018 ) or NSM ( He et al . ,   2021 ) can be used to delicately deduce the answers   from the subgraph . Such separable retrieval and   reasoning ensure the reasoning only on the final   entire instead of the intermediate partial subgraphs ,   which enables a plug - and - play framework to en-   hance any subgraph - oriented reasoner .   We systematically investigate the advantages of   various training strategies for SR , including weakly   supervised / unsupervised pre - training and end - to-   end fine - tuning with the reasoner . Instead of the   ground truth paths , we extract the shortest paths   from a topic entity in the question to an answer   as the weak supervision signals for pre - training .   When the QA pairs themselves are also scarce , we   construct pseudo ( question , answer , path ) labels   for unsupervised pre - training . To further teach the   retriever by the final QA performance , we enable   the end - to - end fine - tuning , which injects the like-   lihood of the answer conditioned on a subgraph   as the feedback from the reasoner into the prior   distribution of the subgraph to update the retriever .   We conduct extensive experiments on WebQSP   and CWQ . The results reveal four major advan-   tages : ( 1 ) SR , combined with existing subgraph-   oriented reasoners , achieves several gains ( +0.4-   9.7 % Hits@1 and 1.3 - 8.7 % F1 ) over the same rea-   soner that is performed with other retrieval meth-   ods . Moreover , SR together with NSM creates new   state - of - the - art results for embedding - based KBQAmodels . ( 2 ) With the same coverage rate of the an-   swers , SR can result in much smaller subgraphs   that can deduce more accurate answers . ( 3 ) The   unsupervised pre - training can improve about 20 %   Hits@1 when none of the weak supervision data   is provided . ( 4 ) The end - to - end fine - tuning can   enhance the performance of the retriever as well as   the reasoner .   Contributions . ( 1 ) We propose a trainable SR   decoupled from the subsequent reasoner to en-   able a plug - and - play framework for enhancing any   subgraph - oriented reasoner . ( 2 ) We devise SR by a   simple yet effective dual - encoder , which achieves   significantly better retrieval and QA results than the   existing retrieval methods . ( 3 ) NSM equipped with   SR , via weakly supervised pre - training and end - to-   end fine - tuning , achieves new SOTA performance   for embedding - based KBQA methods .   2 Related Work   KBQA solutions can be categorized into SP - based   and embedding - based methods . SP - based meth-   ods ( Bao et al . , 2016 ; Berant and Liang , 2014 ; Das   et al . , 2021 ; Lan and Jiang , 2020 ; Liang et al . , 2017 ;   Qiu et al . , 2020b ; Sun et al . , 2020 ) parse a ques-   tion into a logic form that can be executed against   the KB . These methods need to annotate expensive   logic forms as supervision or are limited to narrow   domains with a few logical predicates . Embedding-   based methods embed entities and rank them based   on their relevance to the question , where the enti-   ties are extracted from the whole KB ( Miller et al . ,   2016 ; Saxena et al . , 2020 ) or restricted in a sub-   graph ( Chen et al . , 2019a ; He et al . , 2021 ; Sun   et al . , 2018 ; Zhang et al . , 2018 ) . They are more   fault - tolerant but the whole KB or the ad - hoc re-   trieved subgraph includes many irrelevant entities .   Some works such as PullNet ( Sun et al . , 2019 ) ,   SRN ( Qiu et al . , 2020a ) , IRN ( Zhou et al . , 2018 ) ,   and UHop ( Chen et al . , 2019b ) enhance the re-   trieval by training the retriever , but the retrieving   and the reasoning are intertwined , causing the rea-   soning on partially retrieved subgraphs . Because   of such coupled design , the reasoner in SRN , IRN ,   and UHop is degenerated into a simple MLP . On   the contrary , thanks to the decoupled design , the   reasoner can be complicated to support more com-   plex reasoning . Other works propose more compli-   cated reasoner for supporting the numerical reason-   ing in KBQA ( Feng et al . , 2021 ) .   5774Open - domain QA ( OpenQA ) aims to answer   questions based on a large number of documents .   Most of the OpenQA models also consist of a   retriever to identify the relevant documents and   a reasoner to extract the answers from the doc-   uments . The retriever is devised as a sparse   term - based method such as BM25 ( Robertson and   Zaragoza , 2009 ) or a trainable dense passage re-   trieval method ( Karpukhin et al . , 2020 ; Sachan   et al . , 2021 ) , and the reasoner deals with each doc-   ument individually ( Guu et al . , 2020 ) or fuses all   the documents together ( Izacard and Grave , 2021 ) .   Different from the documents in openQA , the sub-   graphs in KBQA can be only obtained by multi - hop   retrieval and the reasoner should deal with the en-   tire subgraph instead of each individual relation to   find the answer . Although some openQA research   proposes multi - hop document retrieval ( Asai et al . ,   2020 ) , the focus is the matching of the documents   rather than the relations to the questions in KBQA .   Thus the concrete solution for KBQA should be   different from openQA .   3 Problem Definition   Aknowledge base ( KB ) Gorganizes the fac-   tual information as a set of triples , i.e. ,G=   { ( e , r , e)|e , e∈E , r∈R } , where EandRde-   note the entity set and the relation set respectively .   Given a factoid question q , KBQA is to figure out   the answers Ato the question qfrom the entity   setEofG. The entities mentioned in qare topic   entities denoted by E={e } , which are assumed   to be given . This paper considers the complex ques-   tions where the answer entities are multi - hops away   from the topic entities , called multi - hop KBQA .   Probabilistic Formalization of KBQA . Given a   question qand one of its answers a∈A , we for-   malize the KBQA problem as maximizing the prob-   ability distribution p(a|G , q ) . Instead of directly   reasoning on G , we retrieve a subgraph G ⊆Gand   inferaonG. Since Gis unknown , we treat it as a   latent variable and rewrite p(a|G , q)as :   p(a|G , q ) = Xp(a|q , G)p(G|q).(1 )   In the above equation , the target distribution   p(a|G , q)is jointly modeled by a subgraph retriever   p(G|q)and an answer reasoner p(a|q , G ) . The   subgraph retriever pdefines a prior distribution   over a latent subgraph Gconditioned on a questionq , while the answer reasoner ppredicts the like-   lihood of the answer agivenGandq . The goal   is to find the optimal parameters θandϕthat can   maximize the log - likelihood of training data , i.e. ,   where Dis the whole training data . Thanks to this   formulation , the retriever can be decoupled from   the reasoner by firstly training the retriever pand   then the reasoner pon the subgraphs sampled by   the retriever . Via drawing a sample G(Sachan et al . ,   2021 ) , we can approximate Eq . ( 2 ) as :   where the first and the second term can be opti-   mized for the reasoner and the retriever respec-   tively . The concrete reasoner can be instantiated   by any subgraph - oriented KBQA model such as   the GNN - based GRAT - Net ( Sun et al . , 2018 ) and   NSM ( He et al . , 2021 ) .   4 Subgraph Retriever ( SR )   The retriever needs to calculate p(G|q)for any   G , which is intractable as the latent variable Gis   combinatorial in nature . To avoid enumerating G ,   we propose to expand top- Kpaths relevant to q   from the topic entities and then induce the subgraph   following these paths .   4.1 Expanding Paths   Path expanding starts from a topic entity and fol-   lows a sequential decision process . Here a path   is defined as a sequence of relations ( r , · · · , r ) ,   since a question usually implies the intermediate   relations excluding the entities . Suppose a par-   tial path p= ( r , · · · , r)has been retrieved   at time t , a tree can be induced from pby fill-   ing in the intermediate entities along the path , i.e. ,   T= ( e , r , E , · · · , r , E ) . Each Eis an en-   tity set as a head entity and a relation can usually   derive multiple tail entities . Then we select the next   relation from the union of the neighboring relations   ofE. The relevance of each relation rto the ques-   tionqis measured by the dot product between their   embeddings , i.e. ,   s(q , r ) = f(q)h(r ) , ( 4 )   where both fand hare instantiated by   RoBERTa ( Liu et al . , 2019 ) . Specifically , we input   5775   the question or the name of rinto RoBERTa and   take its [ CLS ] token as the output embedding . Ac-   cording to the assumption ( Chen et al . , 2019b ; He   et al . , 2021 ; Qiu et al . , 2020a ; Zhou et al . , 2018 ) that   expanding relations at different time steps should   attend to specific parts of a query , we update the   embedding of the question by simply concatenating   the original question with the historical expanded   relations in pas the input of RoBERTa , i.e. ,   f(q ) = RoBERTa ( [ q;r;···;r]),(5 )   Thus s(q , r)is changed to s(q , r ) =   f(q)h(r ) . Then the probability of a relation   rbeing expanded can be formalized as :   where END is a virtual relation named as “ END ” .   The score s(q , END ) represents the threshold of   the relevance score . p(r|q)is larger than 0.5   ifs(q , r ) > s(q , END ) and is no larger than   0.5 otherwise . We select the top-1 relation with   p(r|q)>0.5 . The expansion is stopped if none   of the probabilities of the relations is larger than   0.5 . Finally , the probability of a path given the   question can be computed as the joint distribution   of all the relations in the path , i.e. ,   p(p|q ) = Yp(r|q ) . ( 7 )   where|p|denotes the number of relations in p , t=   1indicates the selection at the topic entity and t=   |p|denotes the last none - stop relation selection .   Since the top-1 relevant path can not be guaranteed   to be right , we perform a top- Kbeam search at   each time to get Kpaths . From each topic entity ,   we obtain Kpaths which result in nKpaths in   total by ntopic entities . nKpaths correspond to   nKinstantiated trees.4.2 Inducing Subgraph   We take the union of top- Ktrees from one topic   entity into a single subgraph , and then merge the   same entities from different subgraphs to induce   the final subgraph . This can reduce the subgraph   size , i.e. , the answer reasoning space , as the sub-   graphs from different topic entities can be viewed   as the constraints of each other . Specifically , from   thensubgraphs of the ntopic entities , we find the   same entities and merge them . From these merged   entities , we trace back in each subgraph to the root   ( i.e. , a topic entity ) and trace forward to the leaves .   Then we only keep the entities and relations along   the tracing paths of all the trees to form the fi-   nal subgraph . For example in Figure 2 , given a   question “ Where did Canadian citizens with Turing   Award graduate ? ” with two topic entities “ Turing   Award ” and “ Canada ” , we can explain it by the   two expanded paths ( Win , Graduate ) and ( Citizen ,   Graduate ) and merge the trees induced by them to   form a unified subgraph . Only the top-1 path is   presented in the figure for a clear illustration .   5 Training Strategies   In this section , we discuss the pre - training and the   end - to - end fine - tuning strategies to train the re-   triever . Figure 3 illustrates the whole framework   and the training procedure .   5.1 Weakly Supervised Pre - Training   Since the ground truth subgraphs are not easy to   be obtained , we resort to the weak supervision   signals constructed from the ( q , a)pairs . Specif-   ically , from each topic entity of a question , we   retrieve all the shortest paths to each answer as   the supervision signals , as paths are easier to be   obtained than graphs . Since maximizing the log-   5776   likelihood of a path equals toPlogp(r|q )   according to Eq . ( 7 ) , we can maximize the prob-   abilities of all the intermediate relations in a   path . To achieve the goal , we decompose a path   p= ( r , · · · , r)into|p|+ 1 ( question , re-   lation ) instances , including ( [ q ] , r),([q;r ] , r ) ,   ... , ( [ q;r;r;···;r ] , r ) , and an additional   END instance ( [ q;r;r;···;r],END ) , and op-   timize the probability of each instance . We replace   the observed relation at each time step with other   sampled relations as the negative instances to opti-   mize the probability of the observed ones .   5.2 Unsupervised Pre - Training   When the ( q , a)pairs are also scarce , we train the   retriever in an unsupervised manner independent   from the ( q , a)pairs . We leverage the NYT dataset ,   a distant supervision dataset for relation extraction   ( Riedel et al . , 2010 ) to construct the pseudo ( q , a , p )   labels . In this dataset , each instance is denoted as   a tuple ( s,(e , r , e ) ) , where sis a sentence that   refers to the relation rbetween two entities eand   ementioned in the sentence s. For two instances   ( s,(e , r , e))and(s,(e , r , e ) ) , we treat e   as the topic entity and eas the answer . Then we   concatenate sandsas the question , and concate-   naterandras the corresponding path to train   the retriever . The training objective is the same as   the weakly supervised pre - training .   5.3 End - to - End Fine - tuning   End - to - end training is an alternative to fine - tune   the separately trained retriever and the reasoner   jointly . The main idea is to leverage the feedback   from the reasoner to guide the path expansion of   the retriever . To enable this , we optimize the poste-   riorp(G|q , a)instead of the prior p(G|q ) , since   the former one contains the additional likelihoodp(a|q , p)which exactly reflects the feedback   from the reasoner . We do not directly optimize the   posterior p(G|q , a ) , because Gis induced from   nKpaths , making it unknown which path should   receive the feedback from the likelihood computed   on the whole G. Instead , we approximate p(G|q , a )   by the sum of the probabilities of the nKpaths   and rewrite the posterior of each path by Bayes ’   rule ( Sachan et al . , 2021 ) , i.e. ,   p(G|q , a)≈Xp(p|q , a ) , ( 8)   ∝Xp(a|q , p)p(p|q ) ,   where p(p|q)is the prior distribution of the k-   th path that can be estimated by Eq . ( 7 ) , and   p(a|q , p)is the likelihood of the answer agiven   thek - th path . Essentially , p(a|q , p)estimates the   answer aon the single tree induced by the k - th path   instead of the fused subgraph by nKpaths . As a   result , the reasoning likelihood on each tree can be   reflected to the corresponding path that induces the   tree . The reasoner for estimating p(a|q , p)is the   same as that for calculating p(a|q , G ) .   In summary , the whole objective function for   each training instance ( q , a , G)is formalized as :   L= maxlogp(a|q , G )   | { z } ( 9 )   + maxlogXSG(p(a|q , p))p(p|q )   | { z } ,   where the stop - gradient operation SGis to stop   updating the parameters ϕ. The reasoner is updated   5777the same as the two - stage training by computing the   likelihood p(a|q , G)onGsampled by the retriever   ( without using information from the answer a ) . As   a result , there is no mismatch between the training   and evaluation when computing p(a|q , G ) , asG   relies only on the prior at both .   Intuitively , we train the reasoner to extract the   correct answer given the subgraph induced from   nKhighest scoring paths . And we train the re-   triever to select nKpaths which collectively have   a high score to deduce the answer when taking   the feedback from the reasoner into account . Al-   though the two components are jointly trained , the   reasoning is still performed on the retrieved entire   subgraph at each epoch . We present the training   process in Appendix .   6 Experiments   In this section , we conduct extensive experiments   to evaluate the subgraph retrieval ( SR ) enhanced   model . We design the experiments to mainly an-   swer the four questions : ( 1 ) Does SR take effect   in improving the QA performance ? ( 2 ) Can SR   obtain smaller but higher - quality subgraphs ? ( 3 )   How does the weakly supervised and unsupervised   pre - training affect SR ’s performance ? ( 4 ) Can end-   to - end fine - tuning enhance the performance of the   retriever as well as the reasoner ?   6.1 Experimental Settings   Datasets . We adopt two benchmarks , WebQues-   tionSP ( WebQSP ) ( Yih et al . , 2016 ) and Com-   plex WebQuestion 1.1 ( CWQ ) ( Talmor and Berant ,   2018 ) , for evaluating the proposed KBQA model .   Table 1 shows the statistics .   Evaluation Metrics . We evaluate the retriever by   the answer coverage rate , which is the proportion   of questions for which the topic- nKretrieved paths   contain at least one answer . This metric reflects   the upper bound of the QA performance and is de-   noted as Hits@ K. For QA performance , We use   Hits@1 to evaluate whether the top-1 predicted   answer is correct . Since some questions have mul-   tiple answers , we also predict the answers by the   optimal threshold searched on the validation set   and evaluate their F1 score .   Baseline Models . We compare with embedding-   based KBQA models , in which EmbedKGQA ( Sax-   ena et al . , 2020 ) directly optimizes the triplet ( topic   entity , question , answer ) based on their direct em-   beddings . KV - Mem ( Miller et al . , 2016 ) BAM-   Net ( Chen et al . , 2019a ) store triplets in a key-   value structured memory for reasoning . GRAFT-   Net ( Sun et al . , 2018 ) , BAMNet ( Chen et al . ,   2019a ) , NSM ( He et al . , 2021 ) , and PullNet ( Sun   et al . , 2019 ) are subgraph - oriented embedding mod-   els . We also compare with the SP - based models ,   in which QGG ( Lan and Jiang , 2020 ) generates   the query graph for a question by adding the con-   straints and extending the relation paths simulta-   neously , SPARQA ( Sun et al . , 2020 ) proposes a   novel skeleton grammar to represent a question ,   and CBR - KBQA ( Das et al . , 2021 ) leverages Big-   Bird ( Zaheer et al . , 2020 ) , a pre - trained seq2seq   model to directly parse a question into a SPARQL   statement that can be executed on graph DBs . SR   is default trained by weakly supervised pre - training   and the default path number is set to 10 .   6.2 Overall QA Evaluation   We compare with state - of - the - art KBQA models   and present the Hits@1 and F1 scores in Table 2 .   5778   SP - based Models . The SP - based model CBR-   KBQA achieves the best performance on CWQ .   This is expected , as CBR - KBQA leverages a pre-   trained seq - to - seq model to parse the input question   into a SPARQL statement . However , the model de-   pends on the annotated SPARQL statements , which   are expensive to be annotated in practice .   Embedding - based Models . Among these models ,   KV - Mem and EmbedKGQA retrieve the answers   from the global key - value memory built on the KB   or the original whole KB , which enjoys high recall   but suffers from many noisy entities . Compared   with these global retrievals , BAMNet builds the   key - value memory on a subgraph , but it is a full   multi - hop topic - entity - centric subgraph , which is   also noisy . GRAFT - Net and NSM calculate PPR   scores to control the subgraph size , but the ad - hoc   retrieval method is still far from optimal . PullNet   reinforces the retrieval by learning a retriever , but   the retriever and the reasoner are intertwined , caus-   ing the partial reasoning on part of a subgraph ,   which increases the reasoning bias .   Our Models . Compared with the above   embedding - based models , a performance improve-   ment on both the datasets can be observed , e.g. ,   NSM injected by SR ( SR+NSM ) improves 0.4 %   Hits@1 and 1.3 % F1 on WebQSP , 3.9 % Hits@1   and 4.7 % F1 on CWQ compared with the origi-   nal NSM . We also show that SR can be adapted   to different subgraph - oriented reasoners . Beyond   NSM , when injecting SR to GRAFT - NET , it also   significantly improves 9.7 % Hits@1 and 8.7%F1   on CWQ . SR+GN underperforms GN on WebQSP   because GN filters out the relations of the knowl-   edge graph not in the training set of WebQSP . We   do not inject SR into BAMNet as the model needs   entity types in the subgraph , which is temporarily   ignored by SR .   Summary . The overall evaluation shows that   SR takes effect in improving the QA performance   when injecting it before a subgraph - oriented rea-   soner , and SR equipped with NSM creates a   new state - of - the - art model for embedding - based   KBQA .   6.3 Retriever Evaluation   Quality of Retrieved Subgraph . We evaluate   whether the proposed SR can obtain smaller but   higher - quality subgraphs , which are measured by   not only the direct subgraph size and answer cov-   erage rate but also the final QA performance . For   a fair comparison , we fix the reasoner as NSM ,   and vary the retriever as SR and the PPR - based   heuristic retrieval ( Sun et al . , 2018 ; He et al . , 2021 ) .   PPR+NSM are performed on the same knowledge   graph of the proposed SR+NSM . The result of the   trainable retriever in PullNet ( Sun et al . , 2019 ) is   ignored , because its code is not published and the   5779   value of some key parameters that seriously impact   the model ’s performance is unknown .   We report the comparison results in Figure 4 .   The top row presents the answer coverage rates of   the subgraphs with various sizes . It is shown that   when retrieving the subgraphs of the same size , the   answer coverage rate of SR is significantly higher   than PPR . The bottom row presents the QA per-   formance ( Hits@1 ) on the subgraphs with various   answer coverage rates . It is shown that by perform-   ing the same NSM on the subgraphs with the same   coverage rate , the subgraphs retrieved by SR can   result in higher QA performance than PPR .   Summary . The above results show that SR can   obtain smaller but higher - quality subgraphs .   Effect of Question Update , Path Ending , and   Subgraph Merge . We investigate the effects of the   strategies used in SR , including the question up-   dating strategy ( QU ) which concatenates the orig-   inal question with the partially expanded path at   each time step , the path ending strategy ( PE ) which   learns when to stop expanding the path , and the   subgraph merging strategy ( GM ) which induces a   subgraph from the top- nKpaths .   Table 3 indicates that based on SR , Hits@1 drops   4.3 - 15.0 % when removing QU ( SR w/o QU ) and   Hits@1 drops 2.1 - 18.5 % when changing PE to the   fixed path length T(SR w/o PE ) , where the optimal   Tis set to 3 on both WebQSP and CWQ .   Table 4 shows that based on SR+NSM , the aver-   age subgraph size increases from 174 to 204 , and   Hits@1 of QA drops 0.1 % when removing the sub-   graph merging strategy ( SR+NSM w/o GM ) but   directly taking the union of all the subgraphs from   different topic entities to induce the subgraph . We   only present the results on CWQ as most of the   questions in WebQSP only contain one topic entity ,   which does not need the merge operation .   Summary . The above results verify the effective-   ness of the devised QU , PE , and GM in SR .   6.4 Training Strategy Evaluation   Effect of Pre - training . We investigate the effects   of the weakly supervised and the unsupervised pre-   training on the SR . Table 3 shows the performance   of the supervised training ( SR w SuperT ) and the   weakly supervised pre - training ( SR ) , which indi-   cates that SR is comparable with SR w SuperT   when retrieving top-10 paths . Because a single   ground - truth path between a topic entity and an   answer is provided by WebQSP , which might omit   the situation when multiple ground truth paths can   be found . In view of this , the weakly supervised   way that retrieves multiple shortest paths as the   ground truth can provide richer supervision signals .   We ignore the supervised training in CWQ because   the ground truth paths are not explicitly given in   the dataset .   We further vary the proportion of the weakly   supervised data in { 0 % , 20 % , 50 % , 100 % } , and   present the corresponding answer coverage rate   of the subgraph induced by top-10 paths ( i.e.   Hits@10 ) in Figure 5 . Note 0 % means the   RoBERTa used in SR do n’t have any fine - tuning .   The performance shows a consistent growth with   the weakly generated data size , which demonstrates   its positive effect .   Before the weakly supervised pre - training , we   create 100,000 pseudo instances for unsupervised   pre - training ( Cf . Section 5 for details ) . The re-   sults presented by the orange bars show that un-   supervised pre - training can significantly improve   the original SR ( 0 % weakly supervised data ) by   about 20 % Hits@1 . However , with the increase of   the weakly - supervised data , adding unsupervised   pre - training does not take better effect .   Summary . The above results show the effective-   ness of the weakly supervised pre - training . Mean-   while , the unsupervised strategy can be an alterna-   tive choice when the QA pairs are scarce .   Effect of End - to - End Fine - tuning . Table 3 shows   5780both SR+NSM w E2E and SR+GN w E2E im-   prove 2 - 10.6 % Hits@1 of retrieval based on SR .   Table 2 shows SR+NSM w E2E improves 0.6 %   Hits@1 of QA based on SR+NSM on WebQSP ,   and SR+GRAFT - Net w E2E improves 1.5 - 2.5 %   Hits@1 of QA based on SR+GRAFT - Net . Al-   though SR+NSM w E2E underperforms SR+NSM   on CWQ , we suggest to reason on the top-1 re-   trieved results , which are much better than those   before fine - tuning .   Summary . The above results indicate that the an-   swer likelihood estimated by the reasoner provides   positive feedback for fine - tuning the retriever . With   the improvement of the retriever , the reasoner can   be also enhanced by the updated subgraphs .   7 Conclusion   We propose a subgraph retriever ( SR ) decoupled   from the subsequent reasoner for KBQA . SR is de-   vised as an efficient dual - encoder that can update   the question when expanding the path as well as   determining the stop of the expansion . The exper-   imental results on two well - studied benchmarks   show SR takes effect in improving the QA perfor-   mance if injecting it before a subgraph - oriented rea-   soner . SR equipped with NSM creates new SOTA   results for embedding - based KBQA methods if   learning SR by weakly supervised pre - training as   well as end - to - end fine - tuning .   Acknowledgments   This work is supported by National Natural Sci-   ence Foundation of China ( 62076245 , 62072460 ,   62172424 ) ; National Key Research & Develop   Plan(2018YFB1004401 ) ; Beijing Natural Science   Foundation ( 4212022 ) ; CCF - Tencent Open Fund .   References   5781   5782   A Appendix   A.1 Interpretability of Retrieved Paths .   We present the top- nKpaths learned by the pro-   posed SR for several questions in Table 5 on We-   bQSP and CWQ . Each path is denoted by its topic   entity before the colon . A path denoted by * means   it is the new path discovered by SR beyond the   ground - truth path provided by WebQSP and CWQ .   The paths can explain why an answer is inferred   for a question .   A.2 Training Algorithm .   We present the whole training process in Algo-   rithm 1 , where we first pre - train the retriever , then   train the reasoner based on the retrieved subgraph ,   and finally end - to - end fine - tune the retriever and   the reasoner together . Algorithm 1 : Training Algorithm   Input : G,{(q , a ) }   Output : Learned parameters θandϕ.Pre - train the retriever by weakly supervised   signals or unsupervised signals plus only   20 % weakly supervised signals;Train the reasoner on the retrieved   subgraphs ;   /*End - to - End training : * /while not converge do For each ( q , a ) pair , sample a subgraph   Gby current retriever ; Update ϕby optimizing the first term of   Eq . ( 9 ) on all the ( q , a , G ) instances ; Update θby optimizing the second term   of Eq . ( 9)on all the ( q , a , G ) instances;end   5783A.3 Experimental Implementation   We provide the training and inference details of all   the experiments as below .   General Setting . We use RoBERTa - base in our   paper ( Liu et al . , 2019 ) . The basic RoBERTa con-   tains 12 layers , 768 - d hidden size , and 12 attention   heads , resulting in 110 M parameters in total . On   WebQSP and CWQ , the batch size for training both   the retriever and the reasoner is set as 16 and 20   respectively .   Supervised Training . WebQSP provides the re-   lation chains corresponding to each ( question , an-   swer ) pair . For each question , we use each relation   chain from each topic entity to the answer as the   ground truth path . In this way , we obtain 3,098   ( question , path ) instances which can be decom-   posed into 5,394 ( question , relation ) instances in   total for supervised training . The learning rate for   supervised training is set as 5e-5 . An epoch takes   about 5 minutes and the loss function converges   within 10 epochs on WebQSP / CWQ . We ignore   supervised training on CWQ because the explicit   paths are not provided .   Weakly supervised Pre - training . For weakly su-   pervised pre - training , the ground truth paths are   unavailable . To create the pseudo paths , for each   ( question , answer ) pair , we extract all the short-   est paths between each topic entity and an answer .   We create 16,000/150,000 ( question , relation ) in-   stances in total for weakly supervised pre - training .   The learning rate for weakly supervised training is   set as 5e-5 . An epoch takes about 5 minutes and   the loss function converges within 10 epochs .   Unsupervised Pre - training . From the NYT   dataset ( Riedel et al . , 2010 ) , we create 100,000   ( sentence , path ) pseudo instances . The learning   rate for unsupervised training is set as 5e-5 . An   epoch takes about 5 minutes and the loss function   converges within 10 epochs . The unsupervised   pre - training is performed once and then SR can be   adapted to various KBQA datasets .   End - to - End Training . Before end - to - end training ,   the retriever needs to be warmed up by weakly su-   pervised pre - training or unsupervised pre - training .   The reasoner also needs to be warmed up by su-   pervised training on the ( question , answer ) pairs .   For training the NSM reasoner ( He et al . , 2021 ) ,   an epoch with batch size 20 takes 55 seconds andthe loss function converges within 80 epochs . The   learning rate for warming up the reasoner is set as   1e-4 . For end - to - end training , the learning rate is   set as 1e-5 .   A.4 Inference   We retrieve the top 10 relevant relations at each   step which results in 10 paths for each topic entity .   The number 10 is determined at the pre - training   stage by checking the inflection point of the answer   coverage rate on the validation set . The average   time of online inference including both the sub-   graph retrieving and the reasoning can be within   1 second . By comparison , GRAFT - Net and NSM   which first retrieve the whole two - hop subgraph   and then prune it by the PPR scores spend about 2   to 3 seconds or even 7 to 8 seconds for retrieving   some dense subgraphs .   5784