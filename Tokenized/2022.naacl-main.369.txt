  Peiyi Wang , Runxin Xu , Tianyu Liu , Qingyu Zhou ,   Yunbo Cao , Baobao Chang , Zhifang SuiKey Laboratory of Computational Linguistics , Peking University , MOE , ChinaTencent Cloud Xiaowei   { wangpeiyi9979 , runxinxu}@gmail.com   { rogertyliu , qingyuzhou , yunbocao}@tencent.com   { chbb , szf}@pku.edu.cn   Abstract   Few - Shot Sequence Labeling ( FSSL ) is a   canonical paradigm for the tagging models ,   e.g. , named entity recognition and slot fill-   ing , to generalize on an emerging , resource-   scarce domain . Recently , the metric - based   meta - learning framework has been recognized   as a promising approach for FSSL . However ,   most prior works assign a label to each token   based on the token - level similarities , which ig-   nores the integrality of named entities or slots .   To this end , in this paper , we propose E , an   Enhanced Span - based Decomposition method   for FSSL . Eformulates FSSL as a span - level   matching problem between test query and sup-   porting instances . Specifically , Edecom-   poses the span matching problem into a se-   ries of span - level procedures , mainly includ-   ing enhanced span representation , class pro-   totype aggregation and span conflicts resolu-   tion . Extensive experiments show that E   achieves the new state - of - the - art results on   two popular FSSL benchmarks , FewNERD   and SNIPS , and is proven to be more ro-   bust in the nested and noisy tagging scenarios .   Our code is available at https://github .   com / Wangpeiyi9979 / ESD .   1 Introduction   Many natural language understanding tasks can   be formulated as sequence labeling tasks , such as   Named Entity Recognition ( NER ) and Slot Filling   ( SF ) tasks . Most prior works on sequence labeling   follow the supervised learning paradigm , which   requires large - scale annotated data and is limited   to pre - defined classes . In order to generalize on the   emerging , resource - scare domains , Few - Shot Se-   quence Labeling ( FSSL ) has been proposed ( Hou   et al . , 2020 ; Yang and Katiyar , 2020 ) . In FSSL ,   the model ( typically trained on the source domainFigure 1 : A 2 - way 2 - shot few - shot NER task . The   model needs to learn new entities with few examples .   data ) needs to solve the N - way ( Nunseen classes )   K - shot ( only Kannotated examples for each class )   task in the target domain . Figure 1 shows a 2 - way   2 - shot target domain few - shot NER task , where   ‘ PER ’ and‘ORG ’ are2unseen entity types , and   in the training set S={S , S } , both of them   have only 2annotated entities . The tagging models   should annotate ‘ Albert Einstein ’ in the test sen-   tence qas a‘PER ’ according to S.   Recently , metric - based meta - learning ( MBML )   methods have become the mainstream and state - of-   the - art methods in FSSL ( Hou et al . , 2020 ; Ding   et al . , 2021 ) , which train the models on the tasks   sampled from the source domain sentences in order   to mimic and solve the target task . In each task   of training and testing , they make the prediction   through modeling the similarity between the train-   ing set ( support set ) and the test sentence ( query ) .   Specifically , previous MBML methods ( Ding et al . ,   2021 ; Hou et al . , 2020 ; Yang and Katiyar , 2020 )   mainly formulate FSSL as a token - level matching   problem , which assigns a label to each token based   on the token - level similarities . For example , the   token ‘ Albert ’ would be labeled as ‘ B - PER ’ due to   its resemblance to ‘ Steve ’ and‘Isaac ’ .   However , for the MBML methods , selecting the   appropriate metric granularity would be fundamen-   tal to the success . We argue that prior works that   focus on modeling the token - level similarity are   sub - optimal in FSSL : 1 ) they ignore the integrality   of named entities or dialog slots that are composed   of a text span instead of a single token . 2 ) in FSSL,5012the conditional random fields ( CRF ) is an important   component for the token - level sequence labeling   models ( Yang and Katiyar , 2020 ; Hou et al . , 2020 ) ,   but the transition probability between classes in the   target domain task can not be sufficiently learned   with very few examples ( Yu et al . , 2021 ) . The pre-   vious methods resort to estimate the values with   the abundant source domain data , which may suffer   from domain shift problem ( Hou et al . , 2020 ) . To   overcome these drawbacks of the token - level mod-   els , in this paper , we propose E , anEnhanced   Span - based Decomposition model that formulates   FSSL as a span - level matching problem between   test query and support set instances .   Specifically , Edecomposes the span match-   ing problem into three main subsequent span - level   procedures . 1 ) Span representation . We find the   span representation can be enhanced by informa-   tion from other spans in the same sentence and the   interactions between query and support set . Thus   we propose a span - enhancing module to reinforce   the span representation by intra - span and inter-   span interactions . 2 ) Span prototype aggregation .   MBML methods usually aggregate the span vectors   that belongs to the same class in the support set to   form the class prototype representation . Among all   class prototypes , the O - type serves as negative ex-   amples and covers all the miscellaneous spans that   are not entities , which poses new challenges to iden-   tify the entity boundaries . To this end , we propose   a span - prototypical module to divide O - type spans   into3sub - types to distinguish the miscellaneous   semantics by their relative position with recognized   entities , together with a dynamically aggregation   mechanism to form the specific prototype repre-   sentation for each query span . 3 ) Span conflicts   resolution . In the span - level matching paradigm ,   the predicted spans may conflict with each other .   For example , a model may annotate both “ Albert   Einstein ” and “ Albert ” as “ PER ” . Therefore , we   propose a span refining module that incorporates   the Soft - NMS ( Bodla et al . , 2017 ; Shen et al . , 2021 )   algorithm into the beam search to alleviate this con-   flict problem . With Beam Soft - NMS , Ecan   also handle nested tagging cases without any extra   training , which previous methods are incapable of .   We summarize our contribution as follows : ( 1 )   We propose E , an enhanced span - based decom-   position model , which formulates FSSL as a span-   level matching problem . ( 2 ) We decompose the   span matching problem into 3 main subsequentprocedures , which firstly produce enhanced span   representation , then distinguish miscellaneous se-   mantics of O - types , and achieve the specific pro-   totype representation for each query , and finally   resolve span conflicts . ( 3 ) Extensive experiments   show that Eachieves new state - of - the - art per-   formance on both few - shot NER and slot filling   benchmarks and that ESD is more robust than other   methods in nested and noisy scenarios .   2 Related Work   2.1 Few - Shot Learning   Few - Shot Learning ( FSL ) aims to enable the model   to solve the target domain task with a very small   training set D ( Wang et al . , 2020 ) . In FSL ,   people usually consider the N - way K - shot task ,   where the D hasNclasses , and each class has   onlyKannotated examples ( Kis very small , e.g. ,   5 ) . Training a model only based on D from   scratch will inevitably lead to over - fitting . There-   fore , researchers usually introduce the source do-   main annotated data to help train models , e.g. , Few-   Shot Relation Classification ( FSRC ) ( Han et al . ,   2018 ) , Few - Shot Text Classification ( FSTC ) ( Geng   et al . , 2019 ) and Few - Shot Event Classification   ( FSEC ) ( Wang et al . , 2021a ) . The source domain   data do not contain examples belonging to classes   inD , and thus the FSL setting can be guar-   anteed . Specifically , FSSL tasks in our paper also   have the source domain data .   2.2 Metric - Based Meta - Learning   Meta - learning ( Hochreiter , Younger , and Conwell ,   2001a ) is a popular method to deal with Few - Shot   Learning ( FSL ) . Meta - learning constructs a series   of tasks sampled from the source domain data to   mimic the target domain task , and trains models   across these sampled tasks . Each task contains a   training set ( support set ) and a test instance ( query ) .   The core idea of meta - learning is to help model   learn the ability to quickly adapt to new tasks , i.e. ,   learn to learn ( Hochreiter , Younger , and Conwell ,   2001b ) . Meta - learning can be combined with the   metric - learning ( Kulis et al . , 2013 ) ( metric - based   meta - learning ) , which makes predictions based on   the similarity of the support set and the query . For   example , Prototypical Network ( Snell , Swersky ,   and Zemel , 2017 ) first learns prototype vectors   from a few examples in the support set for classes ,   and further uses prototype vectors for query pre-   diction . Specifically , E(our model ) follows this5013   metric - based meta - learning paradigm .   2.3 Few - Shot Sequence Labeling   Recently , few shot sequence labeling ( FSSL ) has   been widely explored by researchers . For exam-   ple , ( Fritzler , Logacheva , and Kretov , 2019a ) lever-   ages the prototypical network ( Snell , Swersky , and   Zemel , 2017 ) in the few - shot NER . ( Yang and Kati-   yar , 2020 ) further proposes a cheap but effective   method to capture the label dependencies between   entity tags without expensive CRF training . ( Wang   et al . , 2021b ) utilizes a large unlabelled dataset and   proposes a distillation method . ( Cui et al . , 2021 )   introduces a prompt method to tap the potential   of BART ( Lewis et al . , 2020 ) . ( Hou et al . , 2020 )   extends the TapNet ( Yoon , Seo , and Moon , 2019 )   and proposes a collapsed CRF for few - shot slot   filling . ( Ma et al . , 2021 ) and ( Athiwaratkun et al . ,   2020 ) formulate FSSL as a machine comprehen-   sion problem and a generation problem , respec-   tively . ( Yu et al . , 2021 ) proposed to retrieve the   most similar exemplars in the support set for span-   level prediction . Although their methods also in-   volve span matching , their main focus is on the   retrieval - augmented training . Our work differs   from ( Yu et al . , 2021 ) in that we propose an ef-   fective actionable pipeline to get enhanced span   representation , handle miscellaneous semantics of   O - types and resolve the potential span conflicts in   both non - nested and nested situations , where the   last two modules are essential to align the candidate   spans with class prototypes in the support set but   missing in ( Yu et al . , 2021 ) . Besides , our enhanced   span representation greatly improves the batched   softmax objective of ( Yu et al . , 2021 ) by inter- andintra - span interactions .   3 Task Formulation   We define a sentence as xand its label as y=   { ( s , y ) } , where sis a span of x(e.g . , ‘ Steve   Jobs ’ ) , yis the label of s ( e.g. , ‘ PER ’ ) and Mis   the number of spans in the x. Following the pre-   vious FSSL setting ( Hou et al . , 2020 ; Ding et al . ,   2021 ) , we have data in source domain D and   target domain D , and models are evaluated on   tasks from D . Meta - learning based FSSL has   two stages , meta - training and meta - testing . In the   meta - training stage , the model is trained on tasks   sampled from D to mimic the test situation ,   and in the meta - testing stage , the model is evalu-   ated on test tasks . A task is defined as T={S , q } ,   consisting of a support set S={(x , y ) } , and   a query sentence q = x. Sincludes Ntypes of   entities or slots ( N - way ) , and each type has Kan-   notated examples ( K - shot ) . For spans that do not   belong to the Ntypes , e.g. , ‘ studied at ’ in Figure 1 ,   we set their label to O. The types except O in each   test task are guaranteed to not exist in the training   tasks . Given a task T={S , q } , the model needs   to assign a label to each span in the query sentence   qbased on S.   4 Methodology   OurEformulates FSSL as a span - level match-   ing problem and decomposes it into a series of   span - related procedures for a better span matching .   Figure 2 illustrates the architecture of E.50144.1 Span Initialization Module   Given a task T={S , q } , we use BERT ( Devlin   et al . , 2019 ) to encode sentences in Sandq , and uti-   lize the output of the last layer to represent tokens   in the sentence . Therefore , for a sentence with   Ntokens x= ( x , x , ... , x ) , we can achieve   representations ( h , h , ... , h ) , where h∈R   is the hidden state corresponding to the token x.   Then , for a span s= ( l , r ) , where landrare   the start index and end index of span sin the   sentence x , we obtain its initial representation   s= [ h;h]W.We enumerate spans in the   sentence with a maximum length of L.   4.2 Span Enhancing Module   4.2.1 Intra Span Attention   Intuitively , the meaning of specific spans can be   inferred from other spans in the same sentence . We   thus design an Intra Span Attention ( ISA ) mecha-   nism . Given all the span representations of a sen-   tence S∈R(Bis the number of spans ) . We   denote the i - th row of Sass , which represents the   i - th span in the sentence . For s , we first get its   ISA span representation ¯ s=/summationtextαs , where   α= softmax ( sS ) . For clarity , we denote this   attention aggregation operation as ϕ , i.e. ,   ¯ s=ϕ(s , S ) ( 1 )   then , a Feed Forward Neural Networks ( FFN )   ( Vaswani et al . , 2017 ) with residual connection ( He   et al . , 2016 ) and layer normalization ( Ba , Kiros ,   and Hinton , 2016 ) are used to get the final ISA   enhanced feature ˜ s ,   ˜ s= LayerNorm ( s+ FFN(¯ s ) ) ( 2 )   FFN(¯ s ) = GELU ( ¯ sW)W ( 3 )   4.2.2 Cross Span Attention   After ISA , to accommodate the span matching   between the test query and supporting sentences ,   and facilitate the inter - span interaction , we pro-   pose Cross Span Attention ( CSA ) to enhance query   spans ˜Q∈Rwith the support set spans { ˜S∈   R;i= 1 , ... , I } , and vice versa . We first con-   catenate all span representations in the support set   into one matrix ˜S= [ ˜S,˜S , ... , ˜S]∈R. We   denote the n - th row of ˜Sas˜ sand the m - th row   of˜Qas˜ q , and obtain their CSA span representa-   tionsˆ s=ϕ(˜ s,˜Q)andˆ q=ϕ(˜ q,˜S ) . Then , we get the final CSA enhanced representation ˇ s   andˇ qas follows ,   ˇ s= LayerNorm ( ˜ s+ FFN(ˆ s ) ) ( 4 )   ˇ q= LayerNorm ( ˜ q+ FFN(ˆ q ) ) ( 5 )   FFN(x ) = GELU ( xW)W ( 6 )   4.3 Span Prototypical Module   4.3.1 Instance Span Attention   Since different support set spans play different roles   for a query span , we propose multi - INstance Span   Attention ( INSA ) to get class representations . For   thei - th class that contains Kannotated spans with   enhanced representations ˇS= [ ˇ s , ... , ˇ s]in the   support set , given a query span ˇ q , INSA gets the   corresponding prototypical representation z=   ϕ(ˇ q,ˇS ) .   4.3.2 O Partition and Prototypical Span   Attention   The O - type spans have huge quantities and miscel-   laneous semantics , which is hard to be represented   by only one prototypical vector . In the span - based   framework , considering the boundary information   is essential for a span , we divide the O - type spans   into3sub - classes according to their boundary , to   alleviate their miscellaneous semantics problem .   Specifically , given a sentence with Iannotated   spans{(l , r ) } , where landrare the left and   right boundary of the i - th annotated span . For each   of the other spans ( l , r ) , we assign it a sub - class   Oas follows ,   O=      O,∀i , s.t . r < l∨l > r   O,∃i , s.t . l≥l∧r≤r   O , Others(7 )   where Odenotes the span that does not over-   lap with any entities or slots in the sentence , e.g. ,   “ study at ” inSof Figure 2 , and Orepresents the   span that is the sub - span of an entity or slot , e.g. ,   “ Isaac ” inSof Figure 2 . After O Partition , we   get the prototypical representation of each O   via INSA , thus for a query span ˇ q , we have 3   sub - class representations Z= [ z , z , z]for   the class O. Then , we utilize Prototypical Span At-   tention ( PSA ) to achieve the final O representation   z=ϕ(ˇ q , Z ) .   4.4 Span Matching Module   Given a task T= ( S , q ) , for the m - th span in   q , we achieve its enhanced representation ˇ q5015   and corresponding prototypical vectors Z=   ( z , z , ... , z)through previous span modules .   Then , we predict qas the type zin the support   set with probability ,   p(y = z|q ) = exp(−L(ˇ q , z))/summationtextexp(−L(ˇ q , z ) )   ( 8)   where Lis the euclidean distance . During training ,   we use cross - entropy as our loss function ,   L=−1   B / summationdisplaylog p(y|q ) ( 9 )   where yis the gold label of qandBis the   number of spans in the query q.   4.5 Span Refining Module in Inference   During inference , spans outputted by the matching   module may have conflicts , we thus propose a re-   fining module that incorporates the SoftNMS into   beam search for the conflicts resolution . For the m-   th span with the left index land the right index r   in the query , we obtain its prediction probability dis-   tribution p , label y= argmax ( p)and score   score= max ( p ) . Figure 3 illustrates a simpli-   fied post - processing process . In each step , we first   expand all beam states ( e.g. , states 1 - 3 and 1 - 2 in   step2 of Figure 3 ) , and then prune new states ac-   cording to the beam size . Specifically , given a beam   stateScontaining spans { l , r , score , y } , foreach non - contained span s= ( l , r , score , y ) ,   we first calculate its decayed score score ,   score = score∗u(10 )   η=/summationdisplayI(IoU ( s , s)≥k ) ( 11 )   where IoU(s , s ) = is the over-   lap ratio of two spans . The decay ratio uand   threshold kare hyperparameters . Then , we ex-   pand the beam state Swith the non - contained span   sifscore > δ . For example , in the step2   of Figure 3 , we expand the state 1 - 3 to 1 - 3 - 4 ,   while the state 1 - 3 - 2 fails to be expanded since   score = 0.58<=δ . After expanding all   states , we prune available beam states with lower   path scores . For example , we prune states 1 - 4 , 3 - 4   and 3 - 2 in step2 of Figure 3 . In addition , as our   needed output is order - independent , we also prune   duplicate states , e.g. , the state 3 - 1 ( equivalent to the   state 1 - 3 ) for the diversity of beam states . When all   states in the beam can not be expanded or have been   expanded before but failed , we select the beam with   the largest path score as the final output .   5 Experiments   5.1 Experiments Setup   Datasets We evaluate Eon FewNERD ( Ding   et al . , 2021 ) and SNIPS ( Coucke et al . , 2018 ) .   FewNERD designs an annotation schema of 8   coarse - grained ( e.g. , ‘ Person ’ ) entity types and 66   fine - grained ( e.g. , ‘ Person - Artist ’ ) entity types , and   constructs two tasks . One is FewNERD - INTRA ,   where all the entities in the training set ( source do-   main ) , validation set and test set ( target domain )   belong to different coarse - grained types . The other   isFewNERD - INTER , where only the fine - grained   entity types are mutually disjoint in different sets .   For the sake of sampling diversity , FewNERD   adopts the N - way K∼2K - shot sampling method   to construct tasks ( each class in the support set has   K∼2Kannotated entities ) . Both FewNERD-   INTRA and FewNERD - INTER have 4settings ,   5 - way 1∼2 - shot , 5 - way 5∼10 - shot , 10 - way   1∼2 - shot and 10 - way 5∼10 - shot . SNIPS is   a slot filling dataset , which contains 7domains   D={D , D , ... , D } . ( Hou et al . , 2020 ) con-   structs few - shot slot filling task with the leave - one-   out strategy , which means when testing on the tar-   get domain D , they randomly chose D(i̸=j)5016   as the validation domain , and train the model on   source domains D − { D , D } . In the sampling   task of SNIPS , all classes have Kannotated ex-   amples in the support set , but the number of them   ( N ) is not fixed . The few - shot slot filling task in   each domain of SNIPS has two settings , 1 - shot and   5 - shot . FSSL models are trained and evaluated on   tasks sampled from the source and target domain ,   respectively . To ensure the fairness , we use the   public sampled data provided by ( Ding et al . , 2021 )   for FewNERDand data provided by ( Hou et al . ,   2020 ) for SNIPS , to train and evaluate our model .   Parameter Settings Following previous meth-   ods ( Hou et al . , 2020 ; Ding et al . , 2021 ) , we use   uncased BERT - base as our encoder . We use Adam   ( Kingma and Ba , 2015 ) as our optimizer . We set the   dropout ratio ( Srivastava et al . , 2014 ) to 0.1 . The   maximum span length Lis set to 8 . For BSNMS ,   the beam size bis 5 . Because FewNERD andSNIPS do not have nested instances , we set the   threshold to filter false positive spans δto0.1 , the   threshold to decay span scores kto1e-5and the de-   cay ratio uto1e-5to force the refining results have   no nested spans . More details of our parameter   settings are provided in Appendix A.   Evaluation Metrics For FewNERD , following   ( Ding et al . , 2021 ) , we report the micro F1 over all   test tasks , and the average result of 5different runs .   For SNIPS , following ( Hou et al . , 2020 ) , we first   calculate micro F1 score for each test episode ( an   episode contains a number of test tasks ) , and then   report the average F1 score for all test episodes as   the final result . We report the average result of 10   different runs the same as ( Hou et al . , 2020 ) .   Baselines For systematic comparisons , we intro-   duce a variety of baselines , including ProtoBERT   ( Ding et al . , 2021 ; Hou et al . , 2020 ) , NNShot ( Ding   et al . , 2021 ) , StructShot ( Ding et al . , 2021 ) , Trans-   ferBERT ( Hou et al . , 2020 ) , MN+BERT ( Hou   et al . , 2020 ) , L - TapNet+CDT ( Hou et al . , 2020 ) ,   Retriever ( Yu et al . , 2021 ) , ConVEx ( Henderson5017   and Vuli ´ c , 2021 ) and Ma2021 ( Ma et al . , 2021 ) .   Please refer to the Appendix A for more details .   5.2 Main Results   Table 1 illustrates the results on FewNERD . As is   shown , among all task settings , Econsistently   outperforms ProtoBERT , NNShot and StructShot   by a large margin . For example , compared with   StructShot , Eachieves 11.17and10.60average   F1 improvement on INTRA and INTER , respec-   tively . Table 2 shows the results on SNIPS . In   the1 - shot setting , L - TapNet+CDT is the previous   best method . Compared with L - TapNet+CDT , E   achieves comparable results and 4.58F1 - scores   improvement in the 1 - shot and 5 - shot settings , re-   spectively . We think the reason is that the informa-   tion benefits brought by our cross span attention   mechanism in 1 - shot setting is much less than that   in5 - shot setting . In addition , compared with L-   TapNet+CDT , Eperforms more stable , and also   has a better model efficiency ( Please refer to Sec-   tion 6.3 ) . In 5 - shot setting , Ma2021 is previous best   method , and Eoutperforms it 1.14and0.42F1-   scores in 1 - shot and 5 - shot settings , respectively .   These results prove the effectiveness of Ein   few - shot sequence labeling .   6 Analysis   6.1 Ablation Study   To illustrate the effect of our proposed mechanisms ,   we conduct ablation studies by removing one com-   ponent of Eat a time . Table 3 shows the results   on the validation set of SNIPS ( 1 - shot , domain   “ We ” ) . Firstly , we remove ISA or CSA , which   means the span can not be aware of other spans   within the same sentence or spans from other sen-   tences . As shown in Table 3 , the average F1 scores   drop 0.7and2.6without ISA and CSA , respec-   tively . These results suggest that our span enhanc-   ing module with ISA and CSA is effective . In addi-   tion , CSA is more effective than ISA , since CSA   can enhance query spans with whole support set   spans , while ISA enhances spans only with other   spans in the same setence . CSA brings much more   information than ISA . Secondly , when we remove   INSA and achieve the prototypical representation   of a class through an average operation , the aver-   age F1 score would drop 3.5 . When we do not   consider the sub - classes of O - type spans ( r.m . OP ) ,   the average F1 score would drop 1.1 . These results   show that our span prototypical module is neces-   sary . At last , the result without BSNMS suggests   the importance of our post - processing algorithm in   this span - level few - shot labeling framework . More-   over , with BSNMS , Ecan also easily adapt to   the nested situation .   6.2 Robustness in the Nested Situation   Sequence labeling tasks such as NER can have   nested situations . For example , in the sentence   “ Isaac Newton studied at Cambridge University " ,   where “ Cambridge ” is a location and “ Cambridge   University ” is an organization . However , both   FewNERD and SNIPS do not annotate nested ex-   amples . To explore the robustness of Eon such   a challenging but common situation , we construct   FewNERD - nested , which has the same training   tasks as FewNERD - INTRA , but different test tasks   with a nested ratio r . In FewNERD - nested ,   we sample each test task either from FewNERD   or from GENIA ( Ohta et al . , 2002 ) with the prob-   ability 1−r andr , respectively , and   all tasks sampled from GENIA are guaranteed   to have the query with nested entities . We sam-   ple validation tasks with nested instances from5018   ACE05 ( Walker et al . , 2006 ) to tune k , uandδin   BSNMS . Please refer to the Appendix B for more   details about FewNERD - nested . Figure 4 shows   the results of Eand several typical baselines   in FewNERD - nested with different r . When   r increases , Eis more stable than previous   methods , since Ewith BSNMS can easily extend   to nested tagging cases without any extra training   while previous methods are incapable of . In ad-   dition , we also compare different post - processing   methods when r = 1 . As shown in Table 4 ,   the beam search method can not handle the nested   situation , and thus it harms the performance . Soft-   NMS ( Shen et al . , 2021 ) and BSNMS can both im-   prove the model performance . However , when in-   corporating beam search into SoftNMS , the model   can be more flexible and avoid some local optimal   post - processing results achieved by SoftNMS ( e.g. ,   spans 1,3and4 in Figure 3 ) , and thus BSNMS   outperforms SoftNMS .   6.3 Model Efficiency   Compared with the token - level models , Eneeds   to enumerate all spans within the length Lfor a   sentence . Therefore , the number of spans is approx-   imately Ltimes that of tokens , which may bring   extra computation overhead . To evaluate the effi-   ciency of E , we compare the average inference   time per task of E(including the BSNMS post-   processing process ) , L - TapNet+CDT ( the state - of-   the - art token - level baseline model with the open   codebase in the SNPIS ) and ProtoBERT ( an ex-   tremely simple token - level baseline ) . As shown in   Table 5 , with exactly the same hardware setting ,   in the domain “ We ” of SNIPS 1 - shot setting , E   ( avg . 8.53ms per task ) is nearly 3 times faster than   L - TapNet+CDT ( avg . 24.67ms per task ) . We see   a similar tendency in the 5 - shot setting . Although   E(avg . 8.53and18.47ms in 1- and 5 - shot   per task ) is slower than ProtoBERT ( avg . 3.13   and5.27ms ) , it outperforms ProtoBERT by 31.53   and16.68F1 scores in 1- and 5 - shot settings ( re-   ported in Table 2 ) with a bearable latency . Besides   the inference time , we also compare the parameter   number of these models . As is shown , the added   parameter scale ( span enhancing and prototypical   modules ) is very small ( 2 M ) compared with the   ProtoBERT and L - TapNet+CDT ( 110 M ) . These   results show that Ehas an acceptable efficiency .   6.4 Error Analysis   To further explore what types of errors the model   makes in detail , we divide error of model predic-   tion into 2categories , ‘ FP - Span ’ and ‘ FP - Type ’ .   As shown in Table 6 , Eoutperforms baselines   and has much less false positive prediction errors .   ‘ FP - Span ’ is the major prediction error for all mod-   els , showing that it is hard to locate the right span   boundary in FSSL for existing models . Therefore ,   we should pay more attention to the span recogni-   tion in the future work . However , compared with   previous methods , Ehas less ratio of the ‘ FP-   span ’ error . We think the reason is that our span-   level matching framework with a series of span-   related procedures has a better perception of the   entity and slot spans than that of our baselines.50197 Conclusion   In this paper , we propose E , an enhanced span-   based decomposition model for few - shot sequence   labeling ( FSSL ) . To overcome the drawbacks of   previous token - level methods , Eformulates   FSSL as a span - level matching problem , and de-   composes it into a series of span - related procedures ,   mainly including span representation , class proto-   type aggregation and span conflicts resolution for a   better span matching . Extensive experiments show   thatEachieves the state - of - the - art performance   on two popular few - shot sequence labeling bench-   marks and that ESD is more robust than previous   models in the noisy and nested situation .   Acknowledgements   The authors would like to thank the anonymous   reviewers for their thoughtful and constructive   comments . This paper is supported by the Na-   tional Key Research and Development Program of   China under Grant No . 2020AAA0106700 , the   National Science Foundation of China under Grant   No.61936012 and 61876004 , and NSFC project   U19A2065 .   References50205021A Experiments   A.1 Baselines   We compare Ewith a variety of baselines as   follows :   •TransferBERT ( Hou et al . , 2020 ) is a fine-   tuning based model , which is a direct appli-   cation of BERT ( Devlin et al . , 2019 ) to the   few - shot sequence labeling .   •ConVEx ( Henderson and Vuli ´ c , 2021 ) is a   fine - tuning based model , which is first pre-   trained on the Reddit corpus with the sequence   labeling objective tasks and then fine - tuned   on the source domain and target domain an-   notated data for final few shot sequence label-   ingm   •Ma2021 ( Ma et al . , 2021 ) formulates se-   quence labeling as the machine reading com-   prehension problem , and proposes some ques-   tions to extract slots in the query sentence .   •ProtoBERT ( Fritzler , Logacheva , and Kretov ,   2019b ) predicts the query labels according   to the similarity of BERT hidden states of   support set and query tokens .   •Matching Net ( MN)+BERT ( Hou et al . ,   2020 ) is similar to ProtoBERT . The only dif-   ference is that MN uses the matching network   ( Vinyals et al . , 2016 ) for token classification .   •L - TapNet - CDT ( Hou et al . , 2020 ) utilizes the   task - adaptive projection network ( Yoon , Seo ,   and Moon , 2019 ) , pair - wise embedding and   collapsed dependency transfer mechanisms to   do classification .   •NNShot ( Yang and Katiyar , 2020 ) is similar   to ProtoBERT , while it makes the prediction   based on the nearest neighbor .   •StructShot ( Yang and Katiyar , 2020 ) adopts   an additional Viterbi decoder during the infer-   ence phase on top of NNShot .   •Retriever ( Yu et al . , 2021 ) is a retrieval based   method which does classification according to   the most similar example in the support set . A.2 Parameter Setting   In our implementation , we utilize BERT - base-   uncased as our backbone encoder the same as ( Hou   et al . , 2020 ; Yu et al . , 2021 ; Ding et al . , 2021 ) . We   use Adam ( Kingma and Ba , 2015 ) as our optimizer .   In FewNERD , the learning rate is 2e−5for BERT   encoder and 5e−4for other modules . In the 1 - shot   setting of SNIPS , for the domain “ Mu ” , the learn-   ing rate is 5e−6for BERT encoder and 1e−4for   other modules . For the domain “ Bo ” , the learning   rate is 1e−5for BERT encoder and 1e−4for   other modules . For other settings of SNIPS , the   learning rate is 5e−5for BERT encoder and 5e−4   for other modules . We set the dropout ratio ( Sri-   vastava et al . , 2014 ) to 0.1 . The dimension of span   representation dand the maximum span length L   is set to 100and8 , respectively . For BSNMS , the   beam size bis 5 . Since these is no nested instances   in FewNERD and SNIPS , we set the threshold to   filter false positive spans δto0.1 , the threshold to   decay span scores kto1e−5and the decay ratio u   to1e−5to force the refining results have no nested   spans . We use the grid search to search our hyper-   parameters , and the scope of each hyperparameter   are included in Table 7 . We train our model on a   single NVIDIA A40 GPU with 48 GB memory .   B FewNERD - nested   We construct our FewNERD - nested via ACE05   ( Walker et al . , 2006 ) , GENIA ( Ohta et al . , 2002 )   and the origin FewNERD datasets . GENIA is a   biological named entity recognition dataset , which   contains 5 kinds of entities , ‘ DNA ’ , ‘ Protein ’ ,   ‘ cell_type ’ , ‘ RNA ’ and‘cell_line ’ , and all of these   entity types are not included in the FewNERD . We5022   partition the sentences in GENIA into two groups ,   GandG.Gconsists of sentences with nested   entities ( 4,744 sentences in total ) , and Gcon-   sists of sentences without nested entities ( 11,924   sentences in total ) . We utilize sentences in G   andGto construct the query and support set of   the GENIA task , respectively . Our FewNERD-   nested contains 2000 5 - way 5 ∼10 - shot test tasks   in total , where r percent tasks are from GE-   NIA , and the remained tasks are from FewNERD-   INTRA . We use ACE05 to construct the validation   set . ACE05 is a widely used named entity recog-   nition dataset , which contains 7coarse - grained en-   tity types , ‘ FAC ’ , ‘ PER ’ , ‘ LOC ’ , ‘ VEH ’ , ‘ GPE ’ ,   ‘ GPE ’ , ‘ WEA ’ and ‘ ORG ’ . The ‘ LOC ’ , ‘ PER ’ ,   ‘ ORG ’ and‘FAC ’ are not included in the training set   of FewNERD - INTRA , and therefore we use them   ( 28fine - grained entity types in total ) and sample   1000 nested tasks as the validation dataset to tune   thek , δanduof BSNMS . The search scope is   included in the Table 8 . In this nested situation ,   we finally set the k , δanduto0.1,0.1and0.4   respectively .   C Robustness in the Noisy Situation   FSSL methods tend to be seriously influenced by   the noise in the support set , since they make the   prediction based on only limited annotated exam-   ples . To explore the robustness of Ein the   noisy situation , we construct FewNERD - noise . In   FewNERD - noise , we disturb each 5way5∼10   shot FewNERD - INTRA task with a noisy ratio   r , which means there are nearly r percent   entities in the support set are mislabeled . As il-   lustrated in the right part of Figure 5 , with r   increasing , the performance of Edrops less than   baselines , which furthuer shows the superiority ofour methods .   D A Detail Case Study of BSNMS   For span conflicts resolution , we propose a post-   processing method BSNMS . A step by step demon-   stration of BSNMS is shown in Figure 6.50235024