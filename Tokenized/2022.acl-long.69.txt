  Semih Yavuz Kazuma Hashimoto Yingbo Zhou   Nitish Shirish Keskar Caiming Xiong   Salesforce Research   { syavuz,k.hashimoto,yingbo.zhou,nkeskar,cxiong}@salesforce.com   Abstract   Fusion - in - decoder ( F ) ( Izacard and Grave ,   2021 ) is a generative question answering ( QA )   model that leverages passage retrieval with a   pre - trained transformer and pushed the state of   the art on single - hop QA . However , the com-   plexity of multi - hop QA hinders the effective-   ness of the generative QA approach . In this   work , we propose a simple generative approach   ( PF ) that extends the task beyond just an-   swer generation by explicitly modeling the rea-   soning process to resolve the answer for multi-   hop questions . By linearizing the hierarchical   reasoning path of supporting passages , their   key sentences , and finally the factoid answer ,   we cast the problem as a single sequence predic-   tion task . To facilitate complex reasoning with   multiple clues , we further extend the unified   flat representation of multiple input documents   by encoding cross - passage interactions . Our ex-   tensive experiments demonstrate that PF   leads to strong performance gains on two multi-   hop QA datasets : HotpotQA and IIRC . Besides   the performance gains , PFis more inter-   pretable , which in turn yields answers that are   more faithfully grounded to the supporting pas-   sages and facts compared to the baseline F   model .   1 Introduction   Leveraging knowledge to make complex reasoning   has been a fundamental problem of artificial intel-   ligence . Open - domain question answering ( QA )   ( V oorhees , 1999 ) is an integral part of such a line of   research with impactful applications ( Esteva et al . ,   2020 ; Zhang et al . , 2020 ) , where the task is to   answer general domain questions by gathering evi-   dence from a large collection of documents . While   super - human level performance has been achieved   on single - passage reading comprehension dataset   like SQuAD ( Rajpurkar et al . , 2016 ) , open - domain   QA still has a long way to go , especially for ques-   tions requiring more complex reasoning . The mainchallenge in the task of complex QA , namely multi-   hop QA , is that it requires a QA system to combine   multiple pieces of evidence from multiple docu-   ments ( Welbl et al . , 2018 ; Talmor and Berant , 2018 ;   Yang et al . , 2018 ) . Even for single - hop QA , it has   been shown challenging for extractive QA models   to effectively aggregate evidence from the com-   bined pool of multiple passages , which has been   the focus of recent work ( Clark and Gardner , 2018 ;   Min et al . , 2019 ; Guu et al . , 2020 ) .   Recent work ( Lewis et al . , 2020b ; Min et al . ,   2020 ) has demonstrated the promise of a genera-   tive approach at combining evidences from mul-   tiple passages for answer generation . Thanks   to large pre - trained transformers like T5 ( Raffel   et al . , 2020 ) , Izacard and Grave ( 2021 ) introduced   fusion - in - decoder ( F ) that leverages passage re-   trieval with generative models for open - domain   QA , achieving state - of - the - art scores across several   single - hop QA benchmarks . However , we observe   that the success of the Fmodel does not extend   to multi - hop QA , which is corroborated by the find-   ings in ( Xiong et al . , 2021 ) . Further , the Fmodel   is a rather opaque model in terms of interpretation   of the answer generation process . This capability   becomes especially important for multi - hop QA ,   which requires sequential reasoning across multiple   evidences from the pool of retrieved passages .   In this work , we propose PF , a genera-   tive QA model that learns to generate an answer   along with a reasoning path to improve its capa-   bility of multi - hop reasoning . PFextends   multi - hop QA beyond just answer generation by   explicitly modeling the full reasoning path to re-   solve the answer with a generative sequence - to-   sequence model . To this end , we cast the problem   as a single sequence prediction task that simulta-   neously models reasoning path consisting of sup-   porting passages and facts , and eventually the fac-   toid answer . Furthermore , we extend PFto   allow for cross - passage interactions between the974   retrieved passages to obtain more expressive repre-   sentations from the encoder to facilitate modeling   a complex reasoning chain by the decoder . Fig-   ure 1 shows an example of our task formulation ,   and Figure 2 shows an overview of our approach .   We evaluate our proposed approach on two multi-   hop QA datasets : HotpotQA ( Yang et al . , 2018 )   and IIRC ( Ferguson et al . , 2020 ) . Our extensive   experiments demonstrate that ( i ) PFleads   to significant performance gains over Fon an-   swer generation , ( ii ) PFis the first generative   model unlocking the possibility of generating the   reasoning path jointly with the answer while achiev-   ing competitive performance on supporting fact ex-   traction metric as well . Besides the performance   gains , PFis able to expose the underlying   reasoning process behind the answer generation ,   which allows us to conduct a much finer - grained   qualitative and quantitative analysis on the model ’s   behavior , providing insights into further improv-   ing and better understanding generative models for   multi - hop QA .   2 Problem Setup and Background   In this section , we formally introduce the problem   setup and establish the necessary background .   2.1 Multi - hop Question Answering   We first describe the multi - hop QA task in a general   way . We assume that a collection of Kpassages   are given for a question q : D={p , p , . . . , p } ,   where Dcan be a pre - defined set , or it can also   be an output from a text retrieval system ( e.g. ,   DPR ( Karpukhin et al . , 2020 ) and MDR ( Xiong   et al . , 2021 ) ) in an open - domain QA setting . That   is , in the case of the open - domain setting , Dis   a subset of a large collection of passages , such asWikipedia . The task is to generate an answer string   agiven qandD. In addition , we aim at identify-   ing which passages provide evidence , and which   sentences in them are describing the evidence . Fig-   ure 1 shows a comprehensive example of the task   definition , where we can see that some sentences   ( called supporting facts ) in the two paragraphs are   crucial to answer the question . Moreover , there is a   reasoning flow : the question →the first paragraph   →the second paragraph , which is called a reason-   ing path in previous work ( Asai et al . , 2020 ) . The   overall task is then to predict the reasoning path   along with the supporting facts , and the answer .   2.2 Fusion - in - Decoder Model ( F )   Fusion - in - Decoder ( F ) is a generative reader   based on a sequence - to - sequence architecture , ini-   tialized from pre - trained models such as T5 ( Raf-   fel et al . , 2020 ) or BART ( Lewis et al . , 2020a ) .   It consists of an encoder ( Enc ) and a decoder   ( Dec ) . First , it constructs a single block of text   b:=question : qtitle : tcontext : p   of concatenated evidence from each passage - title   pair(p , t)together with the question ( q ) . Then ,   each of the resulting evidence block bis indepen-   dently encoded into |b| ×d - dimensional output   representations , which are then concatenated to   form a unified input representation   X= [ Enc(b);Enc(b ) ; . . . , Enc(b ) ] ( 1 )   of dimension ( P|b|)×dwhere|b|denotes the   length of the n - th block bin number of tokens .   Note that , the motivation behind this strategy is to   avoid the expensive quadratic self - attention com-   putation on the encoder - side , effectively reducing   the complexity from O((P|b|))toO(P|b| ) .   Then , the overall answer generation is modeled975as a conditional generation p(a|X)givenXcon-   suming the unified input representation X , where   θrepresents the set of all model parameters . The   model is trained to minimize the cross - entropy loss   for generating answer tokens on the decoder side .   At inference time , Ffirst computes Xbased on   the retrieved passages , and then decodes the answer   token by token following p(a|a , X)with the   learned model parameters θ .   3 PFReader for Multi - hop QA   In this section , we introduce a generative reader   ( PF ) forK - hop QA that jointly generates   an alternating sequence of passage - level andfact-   level clues on the reasoning path by more explicit   fusion of evidence from the pool of input passages   to arrive at the correct answer .   3.1 Overview of PF   As illustrated in Figure 2 , PFemploys a sin-   gle sequence - to - sequence architecture that indepen-   dently encodes the input passages after inserting   special fact markers ( < f > ) before the i - th sentence   of each passage . Conditioning on the concatenation   of token - level input representations per passage , its   decoder then generates the linearized hierarchical   reasoning path obtained by concatenating the se-   quence of passage titles and their corresponding   supporting fact pointers followed by the answer .   Each segment on the reasoning path is separated   by special markers in a way that makes it possible   to uniquely recover the individual segment predic-   tions after decoding in the inference time .   3.2 Extending Multi - hop QA beyond Answer   Generation   The opaqueness of the Fmodel , which makes   understanding of the reasoning process more diffi-   cult , motivated our approach and its emphasis on   exposing the reasoning path . Instead of only model-   ing answer generation , we propose to jointly model   it with the full reasoning path in an hierarchical   fashion to derive the answer in a unified way using   multi - task maximum likelihood training .   3.2.1 Global Input Representation   We utilize the core input encoding architecture   from Fapproach ( Section 2.2 ) by introducing a   new passage representation that will facilitate sup-   porting fact generation on the reasoning path as il-   lustrated in Figure 2 . To this end , we independently   encode each input passage - title pair ( p , t)alongwith the question qas a separate block b:=   question : qtitle : tcontext : p   where we redefine the context representation by in-   serting special tokens ( < f > ) before each sentence   of the passage as   p:=<f > s < f > s···<f > s ( 2 )   where sdenotes the i - th sentence of passage p ,   andlis the number sentences it contains . Hav-   ing redefined the input blocks ( b ) per passage ,   we then compute the global input representation   similar to Eq . 1 by   X= [ Enc(b);Enc(b ) ; . . .;Enc(b ) ]   ( 3 )   Note that sentence indicators ( < f > ) are shared   across all passages , encouraging a more hierarchi-   cal passage representation by explicitly breaking   them down into sentence - level sub - blocks using   the same indicator tokens .   3.2.2 Hierarchical Reasoning Path as a   Sequence   The hierarchical design of reasoning path is in-   spired by the human reasoning process for multi-   hop QA task . More precisely , if a question qre-   quires K - hop reasoning , then we process these   Kpassages in a sequential order alternating be-   tween their passage - level and sentence - level evi-   dence until we reach the answer . To this end , let   R={p , p , . . . , p}withr∈[1 , N]denote   the sequence of passages from the larger pool D   reflecting this reasoning process for locating the   answer afor question q. As shown in Figure 2 ,   we define the hierarchical reasoning path as a lin-   earized sequence of blocks of passage titles and   supporting facts followed by the answer block   Y:= [ T;E;T;E;···;T;E;A](4 )   where Trepresents the i - th title block obtained   by inserting a special token ( < title - i > ) before   the title tandAdenotes the answer block derived   by prepending a special token ( < answer > ) to the   answer aas illustrated in Figure 2 . On the other   hand , i - th supporting fact block is defined as the   sequence of fact indicators following < facts - i >   token by   E:=<facts - i > < f > < f>···<f>(5)976   where { j , j , . . . , j}denote the indices of key   sentences to leverage from passage pto transi-   tion to the next evidence on the reasoning process   Rfor question q , and 1≤m≤ldenotes the   number of supporting facts . Note that fact indica-   tors < f > are shared between the contexts pof   input blocks ( Eq . 2 ) and supporting fact blocks   ( Eq . 5 ) on the target reasoning path to allow the   decoder to follow along the sequential reasoning   Rby pointing to the facts Eof passage p.   3.3 Encoding Cross - Passage Interactions   ( PF+ )   PFenables more explicit evidence fusion   through the reasoning path to guide the model to   towards correct answer in a structured way . How-   ever , it still relies on the decoder to combine all   the clues together , which might still struggle due to   lack of cross - passage interactions as input blocks   are encoded independently . To address this poten-   tial limitation , we propose PF+ , where we   further extend PFin a way that enables cross-   passage interaction by redefining the input block   consisting of a pair of passages ( p , p)as   b:=question : q   < title-1 > t < context-1 > p   < title-2 > t < context-2 > p   assuming that a set of passage pairs ( p , p)are   available for model to consume . In particular , wederive a set of pairs of passages from the initial   setDbyD={(p , p),(p , p ) , . . . , ( p , p ) }   where pcorresponds to the first passage that is pos-   sible to immediately hop to from question q , which   may be determined by another model , or by execut-   ing the original PFonDin our case . Global   input representation X is obtained similarly   ( Eq . 3 ) by except encoding the new blocks b   allowing for cross - passage interactions , while the   target reasoning path Y remains the same as   Y. Note that < title - i > special markers are   shared between new input block band target   reasoning path Y to provide the model with   additional clue regarding the first passage on the   reasoning path while still relaying the complete   evidence fusion to the decoder via information re-   dundancy encoded in X.   3.4 Training and Inference   Having defined global input representation X ,   the decoder autoregressively generates the rea-   soning path Yper token at each step by fol-   lowing self - attention , cross - attention on the en-   tireX , and feed - forward modules . So , the   overall reasoning path generation is modeled as   conditional generation p(Y|X ) . The   model then is trained to minimize J(θ ) =   −Plogp(y|y , X)with teacher forc-   ing over a training set of { ( q , a , D)}.977In the inference , the decoder consumes the in-   put representation Xcomputed by encoder , and   generates the full reasoning path token by token .   We then post - process the decoded sequence using   the answer indicator ( < answer > ) to first obtain   the answer , followed by recursively parsing the   remaining sequence using the special separator to-   kens ( < title - k > , < facts - k > ) to reconstruct   the title and retrieve its relevant sentences at each   hopk . As illustrated in Figure 2 , the final result of   the inference can be summarized into a dictionary   which maps each generated passage title to the list   of sentence pointers as well as the final answer .   4 Experiments   4.1 Datasets and General Setup   We conduct experiments on two multi - hop question   answering datasets : HotpotQA andIIRC .   HotpotQA ( Yang et al . , 2018 ) is a large - scale   human - annotated dataset including 113 K multi-   hop questions . It focuses on using documents from   Wikipedia as the source of information for answer-   ing questions rather than knowledge bases as in   other multi - hop QA datasets ( Welbl et al . , 2018 ;   Talmor and Berant , 2018 ) . The questions in Hot-   potQA are not constrained by the fixed knowledge-   base schema , hence they can cover more diverse   topics . The answer for each question in HotpotQA   is extracted from 10 paragraphs in the distrac-   torsetting , while it is allowed to use the entire   Wikipedia for the full wiki setting . There are two   main question types bridge ( 80 % ) and compari-   son(20 % ) in the corpus , where each question is   designed in a way that extracting the correct an-   swer requires reasoning over multiple evidence dis-   tributed across two passages . While comparison   questions do not require the these passages to be   processed in a particular order , bridge questions   often require identifying the bridge entity in the   first passage to correctly hop to the second one   that contains the answer . Each question is also   provided with the annotation of 2 supporting pas-   sages and up to 5 corresponding relevant sentences   as their supporting facts . Since our proposed ap-   proach is a reader model that reasons over a given   set of evidence documents , we primarily focus our   experiments on the distractor setting .   IIRC ( Ferguson et al . , 2020 ) is a dataset of more   than 13 K human - written questions over paragraphsfrom English Wikipedia , where crowdworkers had   access only to initial paragraph and list of hyper-   links to other relevant Wikipedia articles , with   the missing information occurring in one or more   linked documents . This annotation design encour-   aged less lexical overlap between the questions   and the contexts that actually contain the answer .   This dataset presents unique challenges compared   to HotpotQA because ( 1 ) it additionally requires   discrete / numerical reasoning and identification of   unanswerable questions , which adds up to 4 differ-   ent possible answer types ( span , binary , numerical ,   unanswerable ) , and ( 2 ) about 30 % of questions   require reasoning over more than 2 passages in-   cluding the main passage .   Evaluation Metrics . We use standard metrics   exact - match ( EM ) and Fscores for measuring the   quality of predicted answers . For HotpotQA exper-   iments , we are also able to evaluate PFon   supporting fact predictions using the official met-   rics ( Support - EM , Support- F ) , which measures   the performance of the reader model in correctly   identifying the supporting facts from the relevant   passages . Note that this metric implicitly requires   correctly identifying relevant passages among the   distractors as well . For our experiments on IIRC   dataset , similar to the baseline model constructed   in the original work ( Ferguson et al . , 2020 ) , we   follow the evaluation methods used by DROP ( Dua   et al . , 2019 ) .   Implementation Details . We use pre - trained T5-   large encoder - decoder ( Raffel et al . , 2020 ) to ini-   tialize the models in our experiments . We train the   model with batch size of 64 with constant learn-   ing rate of 1e-4 for 10 epochs . We use maximum   length of 256 ( resp . 512 ) tokens for input blocks of   PF(resp . PF+ ) , while the maximum   target sequence length is set to be 64 . However , the   sequence truncation is performed on the reasoning   path excluding answer part for sequences of length   longer than 64 tokens . All the experiments are con-   ducted on a machine with 4 or 8 many 40 GB A100   GPUs . Our code is based on Huggingface Trans-   formers ( Wolf et al . , 2019 ) . Please see Appendix   for further details on the hyperparameter settings .   4.2 Main Experiments : HotpotQA   4.2.1 Overall Results   We present our main results on the HotpotQA dis-   tractor setting in Table 1 . We report results on the   HotpotQA development set in comparison with the978Answer Support   Methods EM F1 EM F1   Baseline ( Yang et al . , 2018 ) 44.4 58.3 22.0 66.7   DFGN ( Qiu et al . , 2019 ) 55.4 69.2 - -   QFE ( Nishida et al . , 2019 ) 53.7 68.7 58.8 84.7   SAE ( Tu et al . , 2020 ) 61.3 74.8 58.1 85.3   SAE - large ( Tu et al . , 2020 ) 67.7 80.8 63.3 87.4   Graph Recurrent Retriever ( Asai et al . , 2020 ) ( base ) 52.7 65.8 57.4 84.6   Graph Recurrent Retriever ( Asai et al . , 2020 ) ( wwm ) 68.0 81.2 58.6 85.2   Gated Memory Flow ( Shao et al . , 2021 ) 69.6 83.0 64.7 89.0   This Work   F * ( Izacard and Grave , 2021 ) 64.4 77.8 - -   PF 65.8 78.9 59.3 85.7   PF+ 72.7 84.2 64.9 88.7   previous published methods . PFreader pro-   vides 1.4 % absolute gain on answer EM score in   comparison to Fmodel . Moreover , it achieves   competitive supporting fact predictions of 59.3 %   support - EM and 85.7 % support- Fas a result of   path generation compared to strong extractive mod-   els such as ( Asai et al . , 2020 ) . In summary , P-   Festablishes the usefulness of modeling the full   reasoning path along with answer generation for   multi - hop QA . More notably , PF+achieves   a quite significant performance gain across all the   central evaluation metrics , demonstrating the im-   portance of cross - passage interactions . Overall re-   sults validate the effectiveness of the two central   modeling contributions of our proposed method .   Next , we present further analysis and discussion   on the unique advantages of PFapproach   under a few central questions which motivated our   research at the first place .   4.2.2 Analysis   How faithfully grounded are the generated an-   swers on supporting facts ? In Table 2 , we present   a detailed analysis comparing different models in   terms of the faithfulness of their generated an - swers on both gold and predicted supporting facts .   The first row focuses on the passage - level answer   grounding computed by the percentage of the an-   swers found in one of the gold supporting passages ,   while the second row reports the same analysis   on sentence - level . We can observe that PF   models significantly improves on how faithfully   the generated answers are grounded on the support-   ing facts both at passage - level and sentence - level   granularities . The next two rows provide further   insight into the quality of the generated support-   ing facts by PFmodels by measuring how   often the gold answer can be found in them . This   analysis shows that the generated supporting facts   are of quite high - quality including the gold answer   for more than 95.3 % and 96.2 % at sentence - level   and passage - level , respectively . The last two rows   measure the faithfulness of the generated answers   on the model generated supporting facts , which is   not applicable to Fmodel as it does not perform   supporting fact prediction . We observe that the   generated answers are quite faithfully grounded on   the predicted supporting facts , showing the path   generation not only improves the answer EM per-   formance but also successfully grounds them on the   evidence it generates as part of the full reasoning   path .   It is important emphasize here that extractive   reader models can be guaranteed to output perfectly   grounded answers simply by locating the answer in   their predicted supporting facts . On the other hand ,   it is difficult for generative models to ensure 100 %   answer grounding simply due to its generative na-979Answer - EM Support - EM   Comparison Bridge Comparison Bridge   # Supp Facts F PF F PF F PF F PF   2 70.4 71.8 63.3 64.6 - 86.7 - 70.0   3 66.1 68.2 62.7 63.1 - 43.4 - 30.7   4 62.2 63.8 64.3 66.5 - 5.4 - 26.2   > = 5 83.3 87.5 60.0 65.0 - 0.0 - 3.8   ture . However , we are able to provide additional   evidence validating the answers generated by P-   Fare significantly grounded in the supporting   facts it generates , which might implicitly indicate   that the generated reasoning path tightly aligns with   the model ’s underlying process for answer genera-   tion . Although this is a strong evidence , it is still   quite implicit in exposing the model ’s prediction   process , so we see our approach as a step in the   right direction rather than a complete solution .   Performance breakdown by the number of sup-   porting facts and question types . In Table 3 , we   compare the performance of models by breaking   them down based on the number of gold supporting   sentences and the question type ( e.g. , bridge and   comparison ) . Our first observation is that P-   Fprovides consistent improvement on answer-   EM score over Facross both the question types   and different number of supporting facts required   to answer the question . The high variance in the   answer - EM score on comparison questions can be   attributed to the strictness of exact - match metric as   well as the imbalanced nature of the dataset where   only 5 % of the comparison questions have more   than 3 supporting facts . Surprisingly , both FDand   PFDmodels perform considerably well on the   comparison questions even when it requires at least   5 supporting facts .   A more important motivation behind the per-   formance breakdown analysis was to understand   how the supporting fact prediction of PF   would change as the number of gold supporting   facts grows . Although it starts degrading on ex-   amples with more than 2 supporting facts , it still   achieves more than 25 % Support - EM for bridge   questions with up to 4 supporting facts . Recalling   the average performance on the whole dataset is   less than 60 % , we conclude this result might be sat-   isfactory enough , especially for a fully generative   model on a very strict evaluation metric .   Analyzing the evolution of sub - tasks during   joint training with PF.In Figure 3 , we   present the evolution of PFmodel on the Hot-   potQA development set at every 500 training steps .   We observe that while the model more quickly   picks up the patterns for title generation , it takes   much longer for it to reach to a reasonable level of   fact prediction . As one would expect , the general   trend in the evolution of different segments ( title-1 ,   facts-1 , title-2 , facts-2 , answer ) of the reasoning   path mostly follows the difficulty of the correspond-   ing sub - task although all the sub - tasks are jointly   formulated and trained in an end - to - end fashion .   On the other hand , it seems counter - intuitive for   model to reach to a better accuracy on predicting   the facts of the second passage ( F2 - EM ) on the   reasoning path earlier despite having a better accu-   racy on ( T1 - EM ) . However , one can also interpret   it as a result of stronger feedback provided by the   answer segment of the reasoning path as most of   the ground - truth answers are contained in the facts   of the second passage.980   Answer   Methods EM F1   IIRC * ( Ferguson et al . , 2020 ) 63.9 69.2   F * * ( Izacard and Grave , 2021 ) 63.4 69.1   This Work   PF 65.2 70.5   PF+ 68.1 72.9   4.3 Experiments : IIRC   In addition to our main experiments presented in   greater detail , we also conduct experiments on   IIRC dataset to verify the generalization of the pro-   posed approach . To this end , we closely follow   the authors ’ model - free retrieval setting ( referred   to as Oracle L+C in Table-3 ) because the model   checkpoints for the baseline retrieval model are not   available in the public release . We use a python   scriptprovided in the open - sourced repository to   replicate the same setting for a fair comparison .   In Table 5 , we present the results on the devel-   opment set for our proposed PFDandP-   FD+in comparison with the baseline reported   in the original paper ( Ferguson et al . , 2020 ) and   our implementation of the FiD ( Izacard and Grave ,   2021 ) baseline . FDmodel obtains a compara-   ble F1 with IIRC baseline with a slightly worse   exact - match performance . However , the proposed   PFapproach is able to provide 1.3 % and   1.4 % improvement in F1 score over the two base-   lines . Furthermore , PF+extension leads to   the best performance achieving 4.7 % and 4.2 % EM   score improvement in absolute value over the Fbaseline and IIRC baseline , respectively . Our exper-   imental results validate the benefit of the proposed   approach on the IIRC dataset , suggesting strong   evidence for the generalizability of our approach .   4.4 Analyzing the Benefit of Joint Training   In Table 4 , we present the results of a case study   where we analyze the benefit of multi - task training   on the passage chain prediction . The first row of   the table shows the results for training PF   only to predict the sequence of titles for the gold   passages ( i.e. , [ t1 - t2 ] ) , which is just a subsequence   of the full reasoning path obtained by discarding   facts and the answer . The second row is another   variant , where we add the answer back to the lin-   earized target sequence while still excluding the   segments corresponding to the facts . The last row   correspond to the full reasoning path generation ,   which is corresponding to the original formulation   ofPFas described in Section 3 and illus-   trated in Figure 2 . Comparing first two rows in   Table 4 , we can immediately observe that including   answer segment in the target reasoning path ( i.e. ,   [ t1 - t2 - answer ] ) boosts the performance across the   board although in principle it makes the task more   complicated while utilizing the same underlying   model capacity . Further including segments corre-   sponding to ( sentences within supporting   passages ) in addition to answer segment ( i.e. , [ t1-   f1 - t2 - f2 - answer ] – full reasoning path ) boosts the   title - EM even further , especially before applying   title reconstruction post - processing step . Although   the objective of the first task ( i.e. , [ t1 - t2 ] ) is per-   fectly aligned with the evaluation metric used in   Table 4 , the performance of the resulting model   remains inferior compared to jointly modeling the   same task with the answer ( and/or supporting facts )   prediction . These two observations elicit a com-   pelling evidence regarding the benefit of jointly   modeling the sub - tasks of multi - hop QA as single   sequence capturing the full reasoning path.9815 Related Work   Multi - hop question answering . Research on   multi - hop QA aims to tackle complex questions   that require reasoning across multiple pieces of ev-   idence in multiple documents ( Welbl et al . , 2018 ;   Yang et al . , 2018 ; Ferguson et al . , 2020 ) . In partic-   ular , the HotpotQA dataset ( Yang et al . , 2018 ) pro-   vides both the closed and open - domain settings to   evaluate multi - hop reading comprehension models .   Compared to single - hop QA , such complex ques-   tions pose additional challenges for both reader   and retriever models since they are required to cap-   ture relationships between documents , instead of   independently processing each document . This is   challenging because the number of document com-   binations exponentially grows due to the sequential   nature of the process . Two recent works ( Nie et al . ,   2019 ; Asai et al . , 2020 ) have tackled this challenge   by leveraging hyperlink structure in the underlying   Wikipedia corpus , while Xiong et al . ( 2021 ) has   taken a recursive approach to extend the dense re-   trieval process to handle sequential search . Most of   the reading comprehension ( RC ) models in existing   work ( Xiong et al . , 2019 ; Chen et al . , 2019 ; Nishida   et al . , 2019 ; Qi et al . , 2021 ; Li et al . , 2020 ; Xiong   et al . , 2021 ) follow an extractive architecture ( De-   vlin et al . , 2019 ) for selection of the answer spans   and their corresponding supporting evidence with   minor modifications such as initializing the back-   bone model from a stronger or larger pre - trained   models ( Clark et al . , 2020 ) . On the other hand ,   some recent works ( Inoue et al . , 2021 ) take a more   abstractive approach and generate question - focused   summaries of input paragraphs as concise explana-   tions to be fed to the RC module .   Generative question answering . Especially after   the emergence of the SQuAD dataset ( Rajpurkar   et al . , 2016 ) , neural extractive QA models have   been widely studied . An underlying assumption is   that we can extract a short text span ( or a phrase )   as an answer , but it is not always the case in reality .   Motivated by this , the generative QA approach has   also been investigated ( Hewlett et al . , 2017 ; Fan   et al . , 2019 ) . Recent advances on pre - trained trans-   formers have pushed this direction ; for example ,   Lewis et al . ( 2020a ) jointly trained a generative   QA model along with a text retrieval model , and   Roberts et al . ( 2020 ) explored an ambitious ap-   proach to directly generate an answer without any   evidence documents . We focused on the fusion-   in - decoder model ( Izacard and Grave , 2021 ) ; theyclaimed that the decoder might be good at aggregat-   ing information across multiple documents . How-   ever , we have shown that it is not trivial in the multi-   hop reasoning task , and pushed the model ’s ability   to jointly learn to predict reasoning paths . Besides   question answering , jointly learning multiple in-   trinsic capabilities required by the final objective   with a generative approach has been shown useful   in modeling other NLP tasks such as task - oriented   dialogues ( Neelakantan et al . , 2019 ; Hosseini - Asl   et al . , 2020 ; Peng et al . , 2021 ) .   Open - domain question answering . Open - domain   QA ( V oorhees , 1999 ) is practically important ,   which requires a system to retrieve relevant doc-   uments to answer a given question . The task is   recently gaining much attention , thanks to the de-   velopment of large - scale datasets like HotpotQA ,   SQuAD Open ( Chen et al . , 2017 ) , Natural Ques-   tions Open ( Kwiatkowski et al . , 2019 ; Lee et al . ,   2019 ) , etc . Pre - trained transformer models like   BERT ( Devlin et al . , 2019 ) have accelerated the   development of neural text retrievers ( Lee et al . ,   2019 ; Karpukhin et al . , 2020 ; Asai et al . , 2020 ;   Xiong et al . , 2021 ; Liu et al . , 2021 ) in the retriever-   reader framework ( Chen et al . , 2017 ) . We have   investigated the effectiveness of our method in the   multi - hop open - domain QA task ( see Appendix B )   using an existing external retriever component .   6 Conclusion   In this work , we propose a generative question an-   swering ( QA ) approach that models multi - hop QA   as a single sequence prediction task . It learns to   generate an answer along with a reasoning path to   improve its capability of multi - hop reasoning . Our   experiments on prominent multi - hop QA bench-   marks , HotpotQA and IIRC , validate the promise   and effectiveness of our proposed method P-   Fand its extension PF+ . Future work will   explore ( 1 ) our PFapproach more closely   with text retrieval models in open - domain QA sce-   narios and ( 2 ) more explicit grounding on the input   information to make our approach even more inter-   pretable and controllable .   Acknowledgments   The authors would like to thank the members of   Salesforce AI Research team for fruitful discus-   sions , as well as the anonymous reviewers for their   helpful feedback.982References983984985A Visualizing the Correlation between   Evidence and Answer   In Figure 4 and 5 , we visualize the correlation   between supporting evidence and answer predic-   tion performances for comparison andbridge ques-   tion types , respectively . To obtain these plots ,   we first split the examples into 10 buckets where   n - th bucket contains the examples with support-   F1 score in ( 10∗(n−1),10∗n]percentile for   n={1,2 , . . . , 10 } . Then , we take the average an-   swer prediction accuracy ( both EM and F1 ) over   these examples for each bucket , and report this   number on the y - axis of the plot at the correspond-   ing support - F1 bucket on the x - axis , while drop-   ping the empty buckets . Note that x= 0 corre-   sponds to examples with support - F1 score of 0 .   Also note that the size of a data point on the figure   reflects the number of examples in the correspond-   ing bucket as also indicated by the legend . From   Figures 4 and 5 , we can observe that the accuracy of   the generated answers is significantly lower , 30 %   forbridge and 10 % for comparison , for the first   bucket with zero support - F1 compared to buckets   with positive support - F1 score . This suggests that   the model has a difficult time figuring out the an - swer when the supporting evidence prediction is   poor . Another observation that holds for both cat-   egories is the general trend of increased answer   quality as the supporting fact prediction improves .   Combining these two points provide additional ev-   idence ( in addition to Table 2 in the main paper )   implicitly supporting the answer generation pro-   cess of PFbeing grounded on the generated   supporting facts , which is generated as the prefix of   the answer segment in the full decoded reasoning   path sequence during inference .   B Case Study : Full - Wiki Setting with   Multi - hop Dense Retriever   In this subsection , we evaluate PF   in open domain setting of HotpotQA lever-   aging a recently proposed multi - hop dense   retriever ( MDR ) ( Xiong et al . , 2021 ) for   passage retrieval . Unlike distractor setting ,   MDR returns a set of passage pairs D =   { ( p , p),(p , p ) , . . . , ( p , p ) } for   question q , where each passage pcomes with a   titlet , being retrieved from Wikipedia corpus .   This setting naturally fits into how we formulate   PF+ , which operates on the pairs of input   passages as introduced in Section 3.3 , where   we simply set D = D. For experiments   with FandPF , which operate on set of   single input passages , we simply split the pairs   into single passages , ending up with 2Kpassages   when using top- Kretrieved paths from MDR .   We present our results for this setting in Table 6 .   Similar to our observation in distractor setting ,   PFprovides a significant ( % 1.8 ) answer   EM score improvement over F , while also   achieving a quite competitive performance on the   supporting fact prediction compared to strong   discriminative models ( Asai et al . , 2020 ; Li et al . ,   2020 ) optimized for better retrieval performance .   Most notably , PF+provides significant   gains over PF , achieving 59.8 % answer - EM   and 52.8 % supporting fact EM score , showing the   importance of encoding cross - passage interactions .   It is important to note here that our results with   PF+is not directly comparable to the reader   results from MDR ( Xiong et al . , 2021 ) because   we are able to only use top-25 retrieved paths due   to hardware limitations . Finally , we also evaluate   the same PF+model on Devobtained by   adding the pair of gold passages in D , where   we aim to isolate the error propagation from the986Answer Support   Methods EM F1 EM F1   GoldEn Retriever ( Qi et al . , 2019 ) - 49.8 - 64.6   Semantic Retrieval ( Nie et al . , 2019 ) 46.5 58.8 39.9 71.5   Transformer - XH ( Zhao et al . , 2020 ) 50.2 62.4 42.2 71.6   Graph Recurrent Retriever ( Asai et al . , 2020 ) ( wwm ) 60.5 73.3 49.3 76.1   Graph Recurrent Retriever ( Asai et al . , 2020 ) ( base ) 52.7 65.8 47.9 75.0   HopRetriever ( Li et al . , 2020 ) 62.1 75.2 52.5 78.9   HopRetriever - plus ( Li et al . , 2020 ) 66.6 79.2 56.0 81.8   MDR - Electra ( Top-50 paths ) ( Xiong et al . , 2021 ) 61.7 74.3 - -   MDR - FiD ( Top-50 paths ) ( Xiong et al . , 2021 ) 61.7 73.1 - -   Our Models   F * ( Top-25 paths ) 54.0 66.0 - -   PF(Top-25 paths ) 55.8 67.9 49.0 74.1   PF+ ( Top-25 paths ) 59.8 72.4 52.8 76.6   On DevEvaluation   PF+ ( Top-25 paths ) 70.2 81.5 60.9 86.3   underlying retriever . Table 6 shows that both the   answer and supporting fact prediction performance   improves quite significantly , showing the potential   impact that developments on retriever side of the   problem can also make .   C The Effect of Model Size for Future   Reference   As discussed in Section D , fine - tuning PF+   with T5 - large initialization might require signif-   icant resources and non - trivial memory efficient   optimization ( e.g. , gradient checkpointing ) . To pro-   vide a baseline with a smaller model for future   research , here we include the results of PF+   with T5 - base initialization using the same setting   reported in Table 6 in the main paper . As presented   in Table 7 , although the performance difference on   the supporting fact prediction is relatively small   ( 1 % ) , answer prediction performance drops signif-   icantly ( by 3.2 % ) when we switch from T5 - large to   T5 - base . However , working with T5 - base is much   more efficient in terms of resources and iterationtime for building baselines , trying out new ideas   and thought experiments . So , we hope this baseline   will be helpful for future research .   D More on Training and Implementation   Details   Hop ordering . HotpotQA benchmark provides an-   notation only for unordered gold passages , without   explicitly specifying which passage corresponds   to the k - th hop ( e.g. , first - hop , second - hop , etc . )   on the reasoning path . In our implementation , we   combine the heuristic strategies applied by GRR   ( Asai et al . , 2020 ) and MDR ( Xiong et al . , 2021 ) .   More precisely , if only one of the gold passages   contains the answer , then we take the passage that   includes the answer span as the final passage . If   the answer span is included in both passages , we   break the tie by falling back to the hyperlink - based   ordering strategy proposed by GRR ( Asai et al . ,   2020 ) .   Post - processing for passage title reconstruction .   Note that PFgenerates the titles of the pas-   sages on the reasoning path token by token includ-   ing the separator tokens . However , the decoder   might fall into some minor errors during the gener-   ation process , which may cause the resulting titles   to end up slightly different from the original ones .   To account for such minor errors , we leverage the987set of titles coming from the input passages and   find the most similar among them to our generated   passage titles based on token - level F1 - score . We   call this process title reconstruction and apply it   while reporting the performance for supporting fact   predictions . Table 4 shows the benefit of title re-   construction for mitigating such minor generation   errors . On the other hand , the small performance   boost suggests that titles PFalready gener-   ates quite faithful title predictions .   Model selection . For all the models reported in   this work , we perform evaluation at every 500 steps   during training by decoding the whole development   set on a separate machine in a non - blocking fash-   ion . We then select the best model based on the   answer exact - match score performance . However ,   since PFvariants generate more than just   the answer , it can be leveraged to optimize for a   more holistic metric including the supporting fact   prediction performance , offering further control on   model selection . We leave further exploration of   this phenomenon to future work .   Scaling to larger evidence pools for full - wiki set-   ting . As briefly noted in Appendix B , we report   results in full - wiki setting using only top-25 paths   returned by MDR ( Xiong et al . , 2021 ) due to hard-   ware constraints . More precisely , a single training   example becomes impossible to fit into GPU mem-   ory ( 40 GB ) even for top-25 paths for PF+   model with T5 - large initialization . To make the   training feasible , we resort to gradient checkpoint-   ingwhich trades off GPU memory with speed .   However , in this case , even with 25 retrieved paths ,   training PF+for 10 K steps with batch size of   64 using gradient accumulation takes 19 hours on   8 A100 GPUs with 40 GB memory each , which is   one of the most prominent limitations hurdling the   progress for this line of research . Further research   on making generative approaches with large pre-   trained models more efficient without losing on the   performance side holds a great potential impact to   accelerate the progress of fully generative models   for question answering .   E Hyperparameter Settings   In Tables 9 , 8 and 10 , we provide the full set of   important hyperparameters used for the models re-   ported both in the main paper ( HotpotQA - distractor   and IIRC ) and in the Appendix B ( HotpotQA-   fullwiki ) , respectively .   F Qualitative Analysis   In this section , we provide examples comparing   the predictions of FandPFover bridge   andcomparison question types . Each of the exam-   ple Table 11 , 12 , 13 in the next pages follows a   similar structure , where we include gold answer ,   Fanswer prediction , PFanswer ( and full   path ) prediction , and 5 supporting passages ( out   of 10 ) for the brevity of presentation . Among the   input passages , the first two correspond to gold pas-   sages , for which we include the full content as well   as highlighting the key supporting facts / sentences   with orange color . The following three passages   are presented as a subset of the distractors , for each   of which we include a one - line content unless it   plays a crucial role in distracting at least one of   the models in making a wrong prediction . In this   case , we also add the content of this particular pas-   sage as well as highlighting the specific distractor   span / sentence causing the failure of either For   PF.988989990