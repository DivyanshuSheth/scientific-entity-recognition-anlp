  Changrong Min , Ximing Li , Liang Yang , Zhilin Wang , Bo Xu , Hongfei LinSchool of Computer Science and Technology , Dalian University of Technology , ChinaCollege of Computer Science and Technology , Jilin University , ChinaKey Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of   Education , Jilin University , China   11909060@mail.dlut.edu.cn,liximing86@gmail.com   wangzl5521@mails.jlu.edu.cn,(hflin,liang,xubo)@dlut.edu.cn   Abstract   Sarcasm , as a form of irony conveying mockery   and contempt , has been widespread in social   media such as Twitter and Weibo , where the   sarcastic text is commonly characterized as an   incongruity between the surface positive and   negative situation . Naturally , it has an urgent   demand to automatically identify sarcasm from   social media , so as to illustrate people ’s real   views toward specific targets . In this paper ,   we develop a novel sarcasm detection method ,   namely Sarcasm Detector with Augmentation   of Potential Result and Reaction ( S- ) .   Inspired by the direct access view , we treat each   sarcastic text as an incomplete version without   latent content associated with implied negative   situations , including the result and human re-   action caused by its observable content . To   fill the latent content , we estimate the potential   result and human reaction for each given train-   ing sample by [ xEffect ] and[xReact ]   relations inferred by the pre - trained common-   sense reasoning tool COMET , and integrate the   sample with them as an augmented one . We   can then employ those augmented samples to   train the sarcasm detector , whose encoder is a   graph neural network with a denoising module .   We conduct extensive empirical experiments   to evaluate the effectiveness of S- . The   results demonstrate that S- can outper-   form strong baselines on benchmark datasets .   1 Introduction   Sarcasm , as subtle figures of speech , serves   many communicative purposes in human daily life   ( Ivanko and Pexman , 2003 ) , commonly used to crit-   icize an individual . Refer to the formal description   of sarcasm from the Oxford English Dictionary :   “ A way of using words that are the opposite   of what you mean in order to be unpleasant to   somebody or to make fun of them . ” The sarcastic text is typically characterized as an   incongruity between the positive surface and nega-   tive situation ( Riloff et al . , 2013 ; Liu et al . , 2022 ) .   For example , as an obvious sarcasm “ I love work-   ing for six hours every day for free ” , its surface   meaning tends to be positive , conveyed by the sen-   timent word “ love ” , but it corresponds to a negative   situation “ work for free ” , conveying people ’s com-   plaint .   Detecting sarcasm from social media is a signifi-   ca nt task due to the universal existence of sarcasm ,   but its complicated nature makes the task challeng-   ing . To resolve this task , the community has re-   cently proposed a number of Sarcasm Detection   ( SD ) methods , whose major idea is to capture the   incongruity characteristic of sarcasm ( Joshi et al . ,   2017 ; Xiong et al . , 2019 ; Pan et al . , 2020 ; Agrawal   et al . , 2020 ; Li et al . , 2021b ; Lou et al . , 2021 ) .   For example , several early SD studies express the   incongruity by extracting positive - negative pairs   from observable text content , such as rule - based   method ( Joshi et al . , 2017 ) and neural networks   with co - attention tricks ( Xiong et al . , 2019 ; Pan   et al . , 2020 ) . Unfortunately , those methods can not   accurately capture the negative situations , which   are mostly implied and associated with contexts   and background information . To alleviate this is-   sue , the recent arts of SD express the negative situa-   tions with external knowledge bases . From the per-   spective of sentiments , some SD methods employ   auxiliary affective lexicons , e.g. , SenticNet ( Cam-   bria et al . , 2020 ) , to estimate the implied affective   correlations among words and phrases of samples   ( Agrawal et al . , 2020 ; Lou et al . , 2021 ) . Addi-   tionally , the SarDeCK method ( Li et al . , 2021b )   employs the pre - trained commonsense reasoning   tool COMET ( Hwang et al . , 2021 ) to infer the re-   lations behind samples as their implied situations .   Despite the promising performance , their expres-   sions of implied negative situations are still a bit   abstract and impalpable.10172   As complicated figures of speech , we are par-   ticularly interested in how do human beings ac-   curately identify sarcasm ? Through referring to   the prior psychological , cognitive , and linguistic   literature ( Gibbs , 1986 ; W.Gibbs , 2002 ; Ivanko and   Pexman , 2003 ) , we are agreeable with two signif-   icant viewpoints . First , the negative situations of   sarcasm are mostly associated with certain social   events ( Pickering et al . , 2018 ) , and human beings   can often easily identify the events with the back-   ground information in the brain . Second , from the   direct access view ( Giora and Fein , 1999 ; W.Gibbs ,   2002 ; Ivanko and Pexman , 2003 ) , human beings   are likely to directly understand the whole sarcastic   text with both literal meanings and implied neg-   ative situations , which can be easily captured by   them .   Based on the analysis , what we expect is to de-   velop a novel SD method by simulating the way of   human thinking . Inspired by the direct access view ,   we treat each sarcastic text as an incomplete version   without latent content associated with implied neg-   ative situations . We can use the associated social   events to express the negative situations due to their   strong connection . Further , we assume the social   events can be mainly expressed by the potential re-   sults and human reactions that the events produced   ( see examples in Table 1 ) . Accordingly , for each   given sample we can estimate its potential result   and human reaction by pre - trained commonsense   reasoning tools ( acted as background information ) ,   and then integrate the observable text content with   them as an augmented sample ( acted as the whole   text ) . Finally , we can use those augmented sam-   ples to train the sarcasm detector ( just like a human   would ) .   Upon these ideas , we propose a novel   SD method , namely Sarcasm Detector with   Augmentation of Potential Result and Reaction   ( S- ) . Specifically , we estimate the poten-   tial result and human reaction for each training   sample by [ xEffect ] and[xReact ] relations   inferred by the auxiliary commonsense reasoningtool COMET ( Hwang et al . , 2021 ) , and then in-   tegrate the sample with them to generate an aug-   mented one , dubbed as event - augmented sample .   By analogy to ( Lou et al . , 2021 ; Liang et al . , 2022 ) ,   we assume that the syntactic information of event-   augmented samples can intuitively imply the incon-   gruity of sarcasm . Accordingly , we transform each   event - augmented sample into a dependency graph   ( Nivre , 2003 ) , and suggest a graph - based encoder   to generate sample embeddings . Additionally , to   resolve the noisy results and reactions inferred by   COMET , we suggest a denoising module with the   dynamic masking trick ( Yang et al . , 2021 ) , enabling   to improve the quality of sample embeddings . With   those embeddings , a single - layer MLP is used as   the sarcastic classifier finally . To examine the ef-   fectiveness of S- , we conduct extensive ex-   periments on benchmark datasets . The empirical   results demonstrate that S- can outperform   the existing baseline methods .   The contributions of this work can be summa-   rized as follows :   •We propose a novel SD method , named S- , with event - augmented samples formed   by the auxiliary commonsense reasoning tool   COMET .   •We suggest a graph - based encoder with a de-   noising module , enabling to generate strong   sample embeddings .   •The experimental results indicate that S- can achieve competitive performance   compared with existing baselines .   2 Related Works   2.1 Sarcasm Detection   Early SD methods are mostly based on special rules   and evidence ( Maynard and Greenwood , 2014 ;   Bharti et al . , 2015 ; Riloff et al . , 2013 ) . For in-   stance , the study ( Maynard and Greenwood , 2014 )   treats the hashtag sentiment as the key indicator   of sarcasm since the hashtags are usually taken10173to highlight sarcasm in Tweets ; and other meth-   ods employ various evidence , such as parser - based   negative phrase matching , interjections ( Bharti   et al . , 2015 ) , and positive - negative word pairs   ( Riloff et al . , 2013 ) . Some other methods form   incongruity - specific embeddings for sarcastic texts ,   such as shape and pointedness of words ( Ptá ˇcek   et al . , 2014 ) , extensions of words ( Rajadesingan   et al . , 2015 ) , and unexpectedness ( Reyes et al . ,   2012 ) .   Due to the success of neural networks , the main-   stream SD methods nowadays apply them to cap-   ture the incongruity between positive surface and   negative situations within the sarcastic text . Early   methods mainly capture the incongruity from the   observable text content ( Tay et al . , 2018 ; Xiong   et al . , 2019 ; Pan et al . , 2020 ) . For instance , the   methods of ( Xiong et al . , 2019 ; Pan et al . , 2020 ) ex-   tract positive - negative word pairs and phrase pairs   with co - attention tricks . However , those methods   can not fully understand the negative situation due   to its implicit nature . To resolve this issue , the re-   cent methods employ external resources to capture   negative situations and further incongruities of sar-   castic texts ( Agrawal et al . , 2020 ; Lou et al . , 2021 ;   Li et al . , 2021b ; Liu et al . , 2022 ) . For example , the   ADGCN method ( Lou et al . , 2021 ) employs the   affective lexicon SenticNet ( Cambria et al . , 2020 )   to represent intra - sentence affective relations ; and   the DC - Net method ( Liu et al . , 2022 ) exploits senti-   ment lexicon to separate literal meanings from texts   and further estimates sentiment conflicts . Orthogo-   nal to the aforementioned methods , our S-   forms augmented samples by commonsense rea-   soning and treats the augmented ones as the whole   versions of sarcastic texts from the direct access   view ( Giora and Fein , 1999 ; W.Gibbs , 2002 ; Ivanko   and Pexman , 2003 ) .   2.2 Commonsense Knowledge Graph   Large - scale commonsense knowledge graphs ( Lin   et al . , 2019 ; Yin et al . , 2022 ) can conduct reason-   ing for texts to infer the commonsense knowledge   behind them , and they have been widely applied   to a wide range of natural language processing   tasks , such as dialogue generation ( Sabour et al . ,   2022 ) , relation classification ( Hosseini et al . , 2022 ) ,   and emotion recognition ( Li et al . , 2021a ) . To our   knowledge , some representatives include Concept-   Net ( Speer et al . , 2017 ) , ATOMIC ( Sap et al . , 2019 ) ,   and TransOMCS ( Zhang et al . , 2020 ) . The Con-   ceptNet contains 3.4 M entity - relation tuples and   about 90 % of these tuples are taxonomic and lexi-   cal knowledge , resulting in relatively smaller com-   monsense portion . The recent ATOMIC contains   880 K of tuples with 9 relations , which covers social   commonsense knowledge including effects , needs ,   intents , and attributes of the actors in an event . In   addition , the TransOMCS contains 18.5 M tuples   collected from various web sources , and the rela-   tions are similar to the ConceptNet .   3 The Proposed S- Method   In this section , we briefly describe the task def-   inition of SD and the commonsense reasoning   tool COMET . We then introduce the proposed S- method in more detail . For clarity , we sum-   marize the important notations in Table 2 .   Task definition . Given Nlabeled training sam-   ples , the goal of SD is to induce a sarcasm de-   tector enabling to distinguish whether a text sam-   ple belongs to sarcasm or not . Formally , each   training sample is represented by ( s , y ) , where   s={w , · · · , w}is the raw text and y∈ Y   is the category label . The label space is commonly   defined as Y={sarc , non - sarc } .   Brief description of COMET . The COMET   ( Hwang et al . , 2021 ) is a pre - trained commonsense   reasoning tool , which can infer various kinds of   commonsense relations associated with the related   event of a given text . It totally contains 23 com-   monsense relations defined in ATOMIC . For   examples , [ xWant ] describes post - condition de-10174   sires of speakers ; and [ xReason ] gives a post-   fact explanation of the cause of an event . Here , we   specially introduce [ xEffect ] and[xReact ] ,   where [ xEffect ] provides social results that   may occur after an event , while [ xReact ] pro-   vides the speakers ’ emotional reactions to an event .   The outputs of the two relations can be directly   used as the auxiliary augmentation in S- .   We declare that the COMET takes the large ver-   sion of BART ( Lewis et al . , 2020 ) as the backbone ,   which contains 24 layers , 1024 - dimensional hidden   embeddings , and 16 heads for self - attention . Then   it was fine - tuned over ATOMIC .   3.1 Overview of S-   As depicted in Fig.1 , our S- mainly consists   of three components . ( 1 ) Event - augmented sam-   ples generation : For each raw text s , we employ   COMET to infer its result eand human reaction   e , and then concatenate them to form the corre-   sponding event - augmented sample s. ( 2 ) Maksed   graph - based encoder : For each event - augmented   sample s , we transform it into a dependency graph   G , and encode Gas the sample embedding zby   leveraging a graph neural network encoder with dy-   namic masking . ( 3 ) Sarcastic classifier : With z ,   we predict the category label by employing a single-   layer MLP finally . In the following , we introduce   each component of S- in more detail.3.2 Event - Augmented Samples Generation   For each raw text s={w , · · · , w } , we feed it   into the pre - trained COMET with the [ xEffect ]   and[xReact ] relations , and treat the outputs   e={w,···,w}ande={/tildewidew,···,/tildewidew }   as the result and human reaction of the implied   social event behind s. We then concatenate them   to form its event - augmented version . For seman-   tic coherence , we further leverage two linkers   landl , where ldenotes “ then may ” for e   andldenotes “ and I feel ” for e. Accordingly ,   the final event - augmented sample is formed by   s = s⊕l⊕e⊕l⊕e , where ⊕denotes   the concatenation operator , and it totally contains   Mword tokens , where M = M+M+/tildewiderM+ 5 .   We have shown the example in Fig.1 .   3.3 Masked Graph - based Encoder   Given event - augmented samples { s } , we sug-   gest a masked graph - based encoder to induce their   embeddings { z } .   3.3.1 Constructing Graphs of Samples   By analogy to ( Lou et al . , 2021 ; Liang et al . , 2022 ) ,   we assume that the syntactic information of event-   augmented samples can intuitively imply the incon-   gruity of sarcasm . Accordingly , we transform each   sinto an undirected graph G={V , E}with the   off - the - shelf dependency parsing tool , where V10175is the set of nodes , i.e. ,the tokens occurring in s ,   andEis the set of edges computed by dependency   parsing . Define A∈ { 0,1}as its corre-   sponding adjacency matrix , and 1/0 denotes the   component corresponds to an edge or not . Besides ,   each node is with self - loop .   3.3.2 Initializing Node Embeddings   For each G , we initialize its node embeddings   H= [ h,···,h]by leveraging a single-   layer Bi - LSTM ( Hochreiter and Schmidhuber ,   1997 ) . Specifically , we represent the nodes X=   [ x,···,x]by the pre - trained GloVe word   embeddings , and then feed Xinto the Bi - LSTM   as follows :   H= Bi - LSTM ( X;W ) , ( 1 )   where Wis the trainable parameter of Bi - LSTM .   3.3.3 Learning Sample Embeddings with   Dynamic Masking   Given each pair { G , H } , we optimize the node   embeddings H= [ h,···,h]by a L-   layer graph neural network encoder with dynamic   masking ( Yang et al . , 2021 ) , and then form the fi-   nal sample embedding zby leveraging the readout   operator with H.   To be specific , the learning process of node em-   beddings for each layer is formulated below :   h= ReLU / parenleftig   Wm / bracketleftig   h⊕h / bracketrightig / parenrightig   ,   j= 1 , · · · , M , l= 1 , · · · , L , ( 2 )   where W={W}are the trainable pa-   rameters ; m∈[0,1]is the mask weight of   thej - th node , used to capture the possible noisy   eandeinferred by COMET ; N(ij)denotes   the neighbor set of the j - th node ; and h=   /summationtextmhis the weighted sum of the   neighbors of the j - th node .   The update process of the mask weights for each   layer is formulated below :   m= Sigmoid / parenleftig   Wˆh⊕Wh / parenrightig   j= 1 , · · · , M , l= 1 , · · · , L , ( 3 )   where W={W}andW={W }   are the trainable parameters ; and ˆh =   mh .After obtaining the node embeddings Hof   the last layer , we can form the sample embedding   zby leveraging the readout operator as follows :   z=1   M / summationdisplayh ( 4 )   3.4 Sarcastic Classifier and Training   Objective   Given the sample embeddings { z } , we employ   a single - layer MLP as the sarcastic classifier . For   eachz , we predict its category label ˆyby the   following equation :   ˆy= Softmax ( Wz ) , ( 5 )   where Wis the trainable parameter of the sarcas-   tic classifier .   Consider Ntraining pairs { ( z , y ) } , we   can formulate the full objective of S-   with respect to all trainable parameters W=   { W , W , W , W , W } :   L(W ) = /summationdisplayL(y,ˆy ) + λ∥W∥,(6 )   where Lis the cross - entropy loss ; ∥ · ∥ denotes   theℓ-norm ; and λ∈[0,1]is the regularization   coefficient .   4 Experiment   4.1 Experimental Settings   Datasets . To thoroughly evaluate the perfor-   mance of S- , we conduct experiments on   four publicly available SD datasets with different   scales . Their statistics of are shown in Table 3 , and   they are briefly described below :   •SemEval18 is collected in SemEval 2018   Task 3 Subtask A ( Van Hee et al . , 2018 ) .   •iSarcasm ( Oprea and Magdy , 2020 ) consists   of tweets written by participants of an online   survey and thus is for intented sarcasm detec-   tion .   •Ghosh ( Ghosh and Veale , 2016 ) is collected   from Twitter and leverages hashtag to auto-   matically annotate samples .   •IAC - V2 ( Abbott et al . , 2016 ) is sourced from   online political debates forum . Compared   with other datasets , the samples of IAC - V2   are relatively longer and more normative.10176   Baselines . We select a number of recent base-   line methods for comparison . They are briefly de-   scribed below :   •NBOW : A traditional SD method that repre-   sents samples by the averages of word embed-   dings .   •Bi - LSTM : A SD method that sequentially en-   codes sarcastic texts with Bi - LSTM .   •SIARN andMIARN ( Tay et al . , 2018 ): Two   RNN - based SD methods that capture the in-   congruity by using the single - dimensional and   multi - dimensional intra - sentence attentions ,   respectively . We implement in - house codes .   •SA WS(Pan et al . , 2020 ): A CNN - based SD   method that cuts each text sample into snip-   pets and uses self - attention to re - weight them .   •ADGCN(Lou et al . , 2021 ): A GCN - based   SD method that builds affective and depen-   dency graphs with SenticNet to capture the   incongruity in a long distance .   •DC - Net(Liu et al . , 2022 ): A BERT - based   SD method that respectively encodes literalmeanings and implied meanings by the exter-   nal sentiment lexicon .   •SarDeCK(Li et al . , 2021b ) A BERT - based   SD method that uses the COMET to derive   dynamic commonsense knowledge and fuses   the knowledge to enrich the contexts with at-   tention .   Implementation details . In the experiments , ex-   cept the BERT - based methods , we apply the 300-   dimensional GloVe embeddingsto represent the   words initially . The dimension of the Bi - LSTM   output is set to 300 , and the layer number of the   masked graph - based encoder is set to 3 . For all   neural network - based methods , the batch size is   set to 32 . We take Adam as the optimizer , and the   learning rate is set to 0.001 . The regularization co-   efficient λis set to 0.01 . Besides , we use the Xavier   Uniform to initialize the parameters . For the BERT-   based methods , the number of training epochs is   set to 6 , while for other methods , the epoch number   is fixed to 100 with an early stopping mechanism   ( Lou et al . , 2021 ) . In terms of all datasets , the split-   ting of training and testing is shown in Table 3 . We   independently run all comparing methods 5 times   and report the average results.10177   Evaluation metrics . By convention , we employ   Accuracy and Macro - F1 as the evaluation metrics   in our experiments .   4.2 Results and Analysis   The main results of all comparing methods are re-   ported in Table 4 , and we draw the following ob-   servations : ( 1 ) First , it can be clearly seen that our   S- can achieve the highest scores of both   Accuracy and Macro - F1 in most settings , where it   ranks the first over SemEval18 , iSarcasm , and IAC-   V2 , and ranks the second over Ghosh . ( 2 ) Second ,   we observe that S- mostly outperforms the   recent strong baseline SarDeCK , which also em-   ploys COMET to generate auxiliary commonsense   relations . A major difference between SarDeCK   and S- is that the former integrates training   samples with their corresponding commonsense   results of COMET at the embedding level , while   the latter treats the augmentations of raw training   texts and inferred commonsense results of COMET   as the whole raw texts . So the improvements to   SarDeCK indirectly indicate that the direct access   view may be a better perspective for SD . ( 3 ) Third ,   compared with ADGCN that is also based on graph   neural networks , our S- achieves signifi-   ca nt improvements over all datasets . This indicates   that leveraging contextually inferred results and   reactions can be a more efficient way for SD thanleveraging context - free affective lexicons in a static   way . ( 4 ) Finally , S- , ADGCN , DC - Net , and   SarDeCK consistently perform better than NBOW ,   Bi - LSTM , SIARN , MIARN , and SAWS , the meth-   ods without external resources . The results support   the previous statement that understanding sarcasm   heavily relies on human background information .   4.3 Ablation Study   We conduct ablation studies to examine the effec-   tiveness of the augmentations of results , augmenta-   tions of human reactions , and the denoising module .   The results are reported in Table 5 . Overall , when   removing the results ( w/o Result ) and the reactions   ( w/o Reaction ) , the performance of S- show   a decline on all datasets . This indicates that the po-   tential results enable the S- to have extra   explainable contexts to understand the negativity   inside the negative situations . Meanwhile , human   reactions provide explicit emotional clues that can   be related to the negative situations during graph   learning . However , when removing the denoising   module ( w/o Masking ) , the performance of S- decreases across the IAC - V2 dataset . This is   because samples in the Ghosh are short texts , and   their syntactical information may not be accurately   captured , leading the masked graph - based encoder   skips nodes related to the sarcasm by mistake .   Additionally , we replace the masked graph-   based encoder with BERT ( Devlin et al . , 2019 ) ,   and further compare this BERT - based version of   S- with its ablative versions ( w/o Result   andw / o Reaction ) . Due to the space limitation ,   we report the results on two datasets , i.e. ,Ghosh   with relatively more training samples and IAC - V2   with longer text lengths . The results are shown in   Table 6 . We can observe that the full version per-   forms the best compared with the ablative versions .   These results further indicate the augmentations of   results and human reactions inferred by COMET   can improve the classification performance even10178   with a different encoder .   4.4 The Impact of Layer Numbers of the   Masked Graph - based Encoder   We now investigate the impact of the layer num-   berLof the masked graph - based encoder across   benchmark datasets . We present the results with   different values of L∈ { 1,2,3,4,5}in Fig 2 . We   can observe that S- performs the best re-   sults across the SemEval18 dataset when L= 1 ,   while achieving the best results across the other   datasets when L= 3 . The reason may be that the   positive surfaces and the negative situations in the   SemEval18 dataset are close to each other on the   dependency graph , so the two terms can be associ-   ated together through low - order message - passing .   While for the other three datasets , S- re-   quires higher - order message passing to model the   incongruity between the two terms . In practice , we   suggest L= 3as the default setting .   4.5 Visualization of Mask Weights .   To qualitatively visualize the impact of mask   weights , we randomly select several examples andshow the words with lower mask weights of the   final layer of the masked graph - based encoder . The   visualization is shown in Table 7 . We use the red   color to demonstrate the word tokens with lower   mask weights . From the table , we observe that   the encoder can effectively eliminate semantically   irrelevant tokens , such as " gets fired " and"see doc-   tor " , and wrong speakers ’ reactions , such as the   term of " happy " in the second and the third cases .   Besides , we observe that some sarcasm - irrelevant   parts in the original texts can also be captured , e.g. ,   the stop words " on","to","is " .   5 Conclusion and Limitations   In this paper , we propose a novel SD method , en-   titled S- , which expresses negative situa-   tions of sarcasm by the potential results and hu-   man reactions of the associated events . We employ   the COMET to estimate the results and human re-   actions , and form event - augmented samples with   them . We employ those augmented samples as   the whole sarcastic texts from the direct access   view . We suggest a masked graph - based encoder ,   enabling to generate discriminative sample embed-   dings . Experimental results demonstrate that our   S- can achieve competitive performance   compared with the existing baseline methods .   We demonstrate two limitations : ( 1 ) The datasets   used in this work are mostly collected from social   media . In the future , we plan to collect sarcas-   tic texts from various sources , such as the litera-   ture and films , and conduct more experiments with   them . ( 2 ) Our exploration of sarcasm theories still   has some space to improve . Though the incon-   gruity theory is the mainstream in the community ,   there are other theories worthy to investigate in the   future .   Acknowledgment   We would like to acknowledge support for this   project from the National Natural Science Foun-10179dation of China ( No.62076046 ) , and the Young   Scientists Fund of the National Natural Science   Foundation of China ( No.62006034 ) .   References1018010181ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Left blank .   /squareA2 . Did you discuss any potential risks of your work ?   Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Left blank .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Left blank.10182 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Left blank.10183