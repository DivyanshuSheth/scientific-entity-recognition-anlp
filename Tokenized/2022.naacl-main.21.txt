  Yu Zhang , Yu Meng , Xuan Wang , Sheng Wang , Jiawei HanUniversity of Illinois at Urbana - Champaign , IL , USAUniversity of Washington , Seattle , WA , USA   Abstract   Discovering latent topics from text corpora has   been studied for decades . Many existing topic   models adopt a fully unsupervised setting , and   their discovered topics may not cater to users ’   particular interests due to their inability of   leveraging user guidance . Although there exist   seed - guided topic discovery approaches that   leverage user - provided seeds to discover topic-   representative terms , they are less concerned   with two factors : ( 1 ) the existence of out - of-   vocabulary seeds and ( 2 ) the power of pre-   trained language models ( PLMs ) . In this paper ,   we generalize the task of seed - guided topic   discovery to allow out - of - vocabulary seeds .   We propose a novel framework , named S-   T , wherein the general knowledge of   PLMs and the local semantics learned from the   input corpus can mutually beneﬁt each other .   Experiments on three real datasets from differ-   ent domains demonstrate the effectiveness of   ST in terms of topic coherence , accu-   racy , and diversity .   1 Introduction   Automatically discovering informative and coher-   ent topics from massive text corpora is central to   text analysis through helping users efﬁciently di-   gest a large collection of documents ( Grifﬁths and   Steyvers , 2004 ) and advancing downstream appli-   cations such as summarization ( Wang et al . , 2009 ,   2022 ) , classiﬁcation ( Chen et al . , 2015 ; Meng et al . ,   2020b ) , and generation ( Liu et al . , 2021 ) .   Unsupervised topic models have been the main-   stream approach to topic discovery since the pro-   posal of pLSA ( Hofmann , 1999 ) and LDA ( Blei   et al . , 2003 ) . Despite their encouraging perfor-   mance in ﬁnding informative latent topics , these   topics may not reﬂect user preferences well , mainly   due to their unsupervised nature . For example ,   given a collection of product reviews , a user may   be speciﬁcally interested in product categoriesTable 1 : Three datasets ( Cohan et al . , 2020 ; McAuley   and Leskovec , 2013 ; Zhang et al . , 2017 ) from different   domains and their topic categories ( i.e. , seeds ) . Red :   Seeds never seen in the corpus ( i.e. , out - of - vocabulary ) .   In all three datasets , a large proportion of seeds are out-   of - vocabulary .   ( e.g. , “ books ” , “ electronics ” ) , but unsupervised   topic models may generate topics containing dif-   ferent sentiments ( e.g. , “ good ” , “ bad ” ) . To con-   sider users ’ interests and needs , seed - guided topic   discovery approaches ( Jagarlamudi et al . , 2012 ;   Gallagher et al . , 2017 ; Meng et al . , 2020a ) have   been proposed to ﬁnd representative terms for each   category based on user - provided seeds or category   names . However , there are still two less concerned   factors in these approaches .   The Existence of Out - of - Vocabulary Seeds . Pre-   vious studies ( Jagarlamudi et al . , 2012 ; Gallagher   et al . , 2017 ; Meng et al . , 2020a ) assume that all   user - provided seeds must be in - vocabulary ( i.e. ,   appear at least once in the input corpus ) , so that   they can utilize the occurrence statistics or Skip-   Gram embedding methods ( Mikolov et al . , 2013 )   to model seed semantics . However , user - interested   categories can have speciﬁc or composite descrip-   tions , which may never appear in the corpus . Table   1 shows three datasets from different domains : sci-291entiﬁc papers , product reviews , and social media   posts . In each dataset , documents can belong to one   or more categories , and we list the category names   provided by the dataset collectors . These seeds   should reﬂect their particular interests . In all three   datasets , we have a large proportion of seeds ( 45 %   in SciDocs , 60 % in Amazon , and 78 % in Twitter )   that never appear in the corpus . Some category   names are too speciﬁc ( e.g. , “ chronic respiratory   diseases ” , “ nightlife spot ” ) to be exactly matched ,   others are the composition of multiple entities ( e.g. ,   “ hepatitis a / b / c / e ” , “ neoplasms ( cancer ) ” , “ clothing ,   shoes and jewelry ” ) .   The Power of Pre - trained Language Models .   Techniques used in previous studies are mainly   based on LDA variants ( Jagarlamudi et al . , 2012 ) or   context - free embeddings ( Meng et al . , 2020a ) . Re-   cently , pre - trained language models ( PLMs ) such   as BERT ( Devlin et al . , 2019 ) have achieved signif-   icant improvement in a wide range of text mining   tasks . In topic discovery , the generic representation   power of PLMs learned from web - scale corpora   ( e.g. , Wikipedia or PubMed ) may complement the   information a model can obtain from the input cor-   pus . Moreover , out - of - vocabulary seeds usually   have meaningful in - vocabulary components ( e.g. ,   “ night ” and “ life ” in “ nightlife spot ” , “ health ” and   “ care ” in “ health and personal care ” ) . The opti-   mized tokenization strategy of PLMs ( Sennrich   et al . , 2016 ; Wu et al . , 2016 ) can help segment   the seeds into such meaningful components ( e.g. ,   “ nightlife ” →“night ” and “ # # life ” ) , and the contex-   tualization power of PLMs can help infer the cor-   rect meaning of each component ( e.g. , “ # # life ” and   “ care ” ) in the category name . Therefore , PLMs are   much needed in handling out - of - vocabulary seeds   and effectively learning their semantics .   Contributions . Being aware of these two factors ,   in this paper , we study seed - guided topic discovery   in the presence of out - of - vocabulary seeds . Our   proposed ST framework consists of two   modules : ( 1 ) The general representation moduleuses a PLM to derive the representation of each   term ( including out - of - vocabulary seeds ) based on   the general linguistic knowledge acquired through   pre - training . ( 2 ) The seed - guided local representa-   tion module learns in - vocabulary term embeddings   speciﬁc to the input corpus and the given seeds .   In order to optimize the learned representations   for topic coherence , which is commonly reﬂected   by pointwise mutual information ( PMI ) ( Newman   et al . , 2010 ) , our objective implicitly maximizes   the PMI between each word and its context , the   documents it appears , as well as the category it   belongs to . The learning of the two modules is   connected through an iterative ensemble ranking   process , in which the general knowledge of PLMs   and the term representations speciﬁcally learned   from the target corpus conditioned on the seeds can   complement each other .   To summarize , this study makes three contri-   butions . ( 1 ) Task : we propose to study seed-   guided topic discovery in the presence of out - of-   vocabulary seeds . ( 2 ) Framework : we design a uni-   ﬁed framework that jointly models general knowl-   edge through PLMs and local corpus statistics   through embedding learning . ( 3 ) Experiment : ex-   tensive experiments on three datasets demonstrate   the effectiveness of ST in terms of topic   coherence , accuracy , and diversity .   2 Problem Deﬁnition   As shown in Table 1 , we assume a seed can be   either a single word or a phrase . Given a corpus D ,   we useVto denote the set of terms appearing in   D. In accordance with the assumption of category   names , each term can also be a single word or a   phrase . In practice , given a raw corpus , one can   use existing phrase chunking tools ( Manning et al . ,   2014 ; Shang et al . , 2018 ) to detect phrases in it .   After phrase chunking , if a category name is still   not inV , we deﬁne it as out - of - vocabulary .   Problem Deﬁnition . Given a corpusD=   { d, ... ,d}and a set of category names C=   { c, ... ,c}where some category names are out-   of - vocabulary , the task is to ﬁnd a set of in-   vocabulary terms S={w, ... ,w}⊆Vfor   each category csuch that each term in Sis se-   mantically close to cand far from other categories   c(∀j / negationslash = i ) .   3 The ST Framework   In this section , we ﬁrst introduce how we model   general and local text semantics using a PLM mod-292ule and a seed - guided embedding learning module ,   respectively . Then , we present the iterative ensem-   ble ranking process and our overall framework .   3.1 Modeling General Text Semantics using a   PLM   PLMs such as BERT ( Devlin et al . , 2019 ) aim to   learn generic language representations from web-   scale corpora ( e.g. , Wikipedia or PubMed ) that   can be applied to a wide variety of text - related   applications . To transfer such general knowledge   to our topic discovery task , we employ a PLM to   encode each category name and each in - vocabulary   term to a vector . To be speciﬁc , given a term w∈   C∪V , we input the sequence “ [ CLS]w[SEP ] ”   into the PLM . Here , wcan be a phrase containing   multiple words , and each word can be out of the   PLM ’s vocabulary . To deal with this , most PLMs   use a pre - trained tokenizer ( Sennrich et al . , 2016 ;   Wu et al . , 2016 ) to segment each unseen word into   frequent subwords . Then , the contextualization   power of PLMs will help infer the correct meaning   of each word / subword , so as to provide a more   precise representation of the whole category .   After LM encoding , following ( Sia et al . , 2020 ;   Thompson and Mimno , 2020 ; Li et al . , 2020 ) , we   take the output of all tokens from the last layer and   average them to get the term embedding e. In   this way , even if a seed cisout - of - vocabulary ,   we can still obtain its representation e.   3.2 Modeling Local Text Semantics in the   Input Corpus   The motivation of topic discovery is to discover   latent topic structures from the input corpus . There-   fore , purely relying on general knowledge in the   PLM is insufﬁcient because topic discovery results   should adapt to the input corpus D. Now , we in-   troduce how we learn another set of embeddings   { u|w∈V}fromD.   Previous studies on embedding learning assume   that the semantic of a term is similar to its local   context ( Mikolov et al . , 2013 ) , the document it   appears ( Tang et al . , 2015 ; Xun et al . , 2017a ) , and   the category it belongs to ( Meng et al . , 2020a ) .   Inspired by these studies , we propose the following   embedding learning objective.where   In this objective , u(and v),v , vare the   embedding vectors of terms , documents , and cate-   gories , respectively . C(w , h)is the set of context   terms ofwind . Speciﬁcally , if d = ww ... w ,   thenC(w , h ) = { w|i−h≤j≤i+h , j / negationslash = i } ,   wherehis the context window size .   Note that the last term in Eq . ( 1 ) encourages   the similarity between each category cand its rep-   resentative termsS. Here , we adopt an iterative   process to gradually update category - representative   terms . Initially , Sconsists of just a few in-   vocabulary terms similar to caccording to the   PLM . At each iteration , the size of Swill increase   to contain more category - discriminative terms ( the   selection criterion of these terms will be introduced   in the next section ) , and we need to encourage their   proximity with cin the next iteration .   Directly optimizing the full softmax in Eq . ( 2 )   is costly . Therefore , we adopt the negative sam-   pling strategy ( Mikolov et al . , 2013 ) for efﬁcient   approximation .   Interpreting the Objective . In topic modeling   studies , pointwise mutual information ( PMI ) ( New-   man et al . , 2010 ) is a standard evaluation metric   for topic coherence ( Lau et al . , 2014 ; Röder et al . ,   2015 ) . Levy and Goldberg ( 2014 ) prove that the   Skip - Gram embedding model is implicitly factoriz-   ing the PMI matrix . Following their proof , we can   show that maximizing Eq . ( 1 ) is implicitly doing   the following factorization :   where the columns of U , V , V , Vareu ,   v , v , v , respectively ( w , w∈V , d∈D ,   c∈C);X , X , andXare PMI matrices .   Here , # ( w , w)denotes the number of co-   occurrences of wandwin a context window in   D;#(w)denotes the number of occurrences of w293inD;λis the total number of terms in D;#(w )   denotes the number of times woccurs ind;λis   the total number of terms in d;bis the number of   negative samples . ( For the derivation of Eq . ( 3 ) ,   please refer to Appendix A. )   To summarize , the learned local representations   uare implicitly optimized for topic coherence ,   where term co - occurrences are measured in context ,   document , and category levels .   3.3 Ensemble Ranking   We have obtained two sets of term embeddings   that model text semantics from different angles :   { e|w∈C∪V}carries the PLM ’s knowledge ,   while{u|w∈V}models the input corpus as   well as user - provided seeds . We now propose an   ensemble ranking method to leverage information   from both sides to grab more discriminative terms   for each category .   Given a category cand its current term set S ,   we ﬁrst calculate the scores of each term w∈V.   The subscript “ G ” here means “ general ” , while “ L ”   means “ local ” . Then , we sort all terms by these   two scores , respectively . Each term wwill hence   get two rank positions rank(w)andrank(w ) .   We propose the following ensemble score based on   the reciprocal rank :   Here , 0 < ρ≤1is a constant . In practice , in-   stead of ranking all terms in the vocabulary , we   only check the top- Mresults in the two ranking   lists . If a term wis not among the top- Mac-   cording to score(w)(resp . , score(w ) ) , we set   rank(w ) = + ∞(resp . , rank(w ) = + ∞ ) . In   fact , whenρ= 1 , Eq . ( 6 ) becomes the arith-   metic mean of the two reciprocal ranks   and . This is essentially the mean recip-   rocal rank ( MRR ) commonly used in ensemble   ranking , where a high position in one ranking list   can largely compensate a low position in the other .   In contrast , when ρ→0 , Eq . ( 6 ) becomes the   geometric mean of the two reciprocal ranks ( see   Appendix B ) , where two ranking lists both have   the “ veto power ” ( i.e. , a term needs to be ranked   as top - Min both ranking lists to obtain a non - zeroAlgorithm 1 : ST   ensemble score ) . In experiment , we set ρ= 0.1   and show it outperforms MRR ( i.e. , ρ= 1 ) in our   topic discovery task .   After computing the ensemble score score(w|S )   for eachw , we updateS. To guarantee that each   Sis category - discriminative , we do not allow any   term to belong to more than one category . There-   fore , we gradually expand each Sby turns . At the   beginning , we reset S= ... =S=∅. When it   isS ’s turn , we add one term Saccording to the   following criterion :   3.4 Overall Framework   We summarize the entire ST framework in   Algorithm 1 . To deal with out - of - vocabulary cat-   egory names , we ﬁrst utilize a PLM to ﬁnd their   nearest in - vocabulary terms as the initial category-   discriminative term set S(Lines 1 - 7 ) . After ini-   tialization,|S|=N(∀1≤i≤|C| ) . Note that   for an in - vocabulary category name c∈V , itself   will be added to the initial Sas the top-1 similar   in - vocabulary term .   After getting the initial S , we update it by Tit-   erations ( Lines 8 - 16 ) . At each iteration , according   to the up - to - dateS , S, ... ,S , we relearn embed-   dings u , v , v , and vusing Eq . ( 1 ) ( Line 10 ) .   The two set of embeddings , { e|w∈C∪V }   ( computed at Line 1 ) and { u|w∈ V}(up-   dated at Line 10 ) , are then leveraged to perform   ensemble ranking ( Lines 11 - 12 ) . Based on the294ensemble score score(w|S ) , we updateSusing   Eq . ( 7 ) ( Lines 13 - 16 ) . After the t - th iteration ,   |S|= ( t+ 1)N(∀1≤i≤|C| ) .   Complexity Analysis . The time complexity of   using the PLM is O((|C|+|V|)α ) , where   α is the complexity of encoding one term via   the PLM . The total complexity of local embed-   ding isO(Tλ(h+|C|)b)because in each iteration   1≤t≤T , everyw∈D interacts with every other   term in the context window of size h , its belong-   ing document , and each category c∈C , and each   update involves bnegative samples . The total com-   plexity of ensemble ranking is O(T|V||C||S|)as   in each iteration 1≤t≤T , we compute scores   between each w∈Vand eachw∈S(∀c∈C ) .   4 Experiments   4.1 Experimental Setup   Datasets . We conduct experiments on three pub-   lic datasets from different domains : ( 1 ) SciDocs   ( Cohan et al . , 2020)is a large collection of sci-   entiﬁc papers supporting diverse evaluation tasks .   For the MeSH classiﬁcation task ( Coletti and Ble-   ich , 2001 ) , about 23 K medical papers are collected ,   each of which is assigned to one of the 11 common   disease categories derived from the MeSH vocabu-   lary . We use the title and abstract of each paper as   documents and the 11 category names as seeds . ( 2 )   Amazon ( McAuley and Leskovec , 2013)contains   product reviews from May 1996 to July 2014 . Each   Amazon review belongs to one or more product cat-   egories . We use the subset sampled by Zhang et al .   ( 2020 , 2022 ) , which contains 10 categories and   100 K reviews . ( 3 ) Twitter ( Zhang et al . , 2017 )   is a crawl of geo - tagged tweets in New York City   from August 2014 to November 2014 . The dataset   collectors link these tweets with Foursquare ’s POI   database and assign them to 9 POI categories . We   take these category names as input seeds .   Seeds used in the three datasets are shown in   Table 1 . Dataset statistics are summarized in Ta-   ble 2 . For all three datasets , we use AutoPhrase   ( Shang et al . , 2018)to perform phrase chunking   in the corpus , and we remove words and phrases   occurring less than 3 times .   Previous studies ( Jagarlamudi et al . , 2012 ; Meng   et al . , 2020a ) have tried some other datasets ( e.g. ,   RCV1 , 20 Newsgroups , NYT , and Yelp ) . However ,   the category names they use in these datasets are   all picked from in - vocabulary terms . Therefore ,   we do not consider these datasets for evaluation in   our task settings .   Following ( Sia et al . , 2020 ) , we adopt a 60 - 40   train - test split for all three datasets . The training   set is used as the input corpus D , and the testing   set is used for calculating topic coherence metrics   ( see evaluation metrics for details ) .   Compared Methods . We compare our ST   framework with the following methods , includ-   ing seed - guided topic modeling methods , seed-   guided embedding learning methods , and PLMs .   ( 1)SeededLDA ( Jagarlamudi et al . , 2012)is a   seed - guided topic modeling method . It improves   LDA by biasing topics to produce input seeds   and by biasing documents to select topics relevant   to the seeds they contain . ( 2 ) Anchored CorEx   ( Gallagher et al . , 2017)is a seed - guided topic   modeling method . It incorporates user - provided   seeds by balancing between compressing the in-   put corpus and preserving seed - related informa-   tion . ( 3 ) Labeled ETM ( Dieng et al . , 2020)is   an embedding - based topic modeling method . It in-   corporates distributed representation of each term .   Following ( Meng et al . , 2020a ) , we retrieve repre-   sentative terms according to their embedding sim-   ilarity with the category name . ( 4 ) CatE ( Meng   et al . , 2020a)is a seed - guided embedding learn-   ing method for discriminative topic discovery . It   takes category names as input and jointly learns   term embedding and speciﬁcity from the input cor-   pus . Category - discriminative terms are then se-   lected based on both embedding similarity with   the category and speciﬁcity . ( 5 ) BERT ( Devlin   et al . , 2019)is a PLM . Following Lines 1 - 7 in   Algorithm 1 , we use BERT to encode each input   category name and each term to a vector , and then   perform similarity search to directly ﬁnd all repre-295   sentative terms . ( 6 ) BioBERT ( Lee et al . , 2020 )   is a PLM . It is used in the same way as BERT .   Since BioBERT is speciﬁcally trained for biomedi-   cal text mining tasks , we report its performance on   the SciDocs dataset only . ( 7 ) ST -NoIter   is a variant of our ST framework . In Algo-   rithm 1 , after initialization ( Lines 1 - 7 ) , it executes   Lines 9 - 16 only once ( i.e. , T= 1 ) to ﬁnd all repre-   sentative terms .   Here , all seed - guided topic modeling and em-   bedding baselines ( i.e. , SeededLDA , Anchored   CorEx , CatE , and Labeled ETM ) can only take   in - vocabulary seeds as input . For a fair compar-   ison , we run Lines 1 - 7 in Algorithm 1 to get the   initial representative in - vocabulary terms for each   category , and input these terms as seeds into the   baselines . In other words , all compared methods   use BERT / BioBERT to initialize their term sets .   Evaluation Metrics . We evaluate topic discovery   results from three different angles : topic coherence ,   term accuracy , and topic diversity .   ( 1)NPMI ( Lau et al . , 2014 ) is a standard metric in   topic modeling to measure topic coherence . Within   each topic , it calculates the normalized pointwise   mutual information for each pair of terms in S.   whereP(w , w)is the probability that wandw   co - occur in a document ; P(w)is the marginal   probability of w.   ( 2)LCP ( Mimno et al . , 2011 ) is another standard   metric to measure topic coherence . It calculates the   pairwise log conditional probability of top - rankedterms .   Note that PMI ( Newman et al . , 2010 ) is also a stan-   dard metric for topic coherence . We do observe   thatST outperforms baselines in terms of   PMI in most cases . However , since our local em-   bedding step is implicitly optimizing a PMI - like   objective , we no longer use it as our evaluation   metric .   ( 3)MACC ( Meng et al . , 2020a ) measures term ac-   curacy . It is deﬁned as the proportion of retrieved   terms that actually belong to the corresponding   category according to the category name .   where 1(w∈c)is the indicator function of   whetherwis relevant to category c. MACC re-   quires human evaluation , so we invite ﬁve anno-   tators to perform independent annotation . The re-   ported MACC score is the average MACC of the   ﬁve annotators . A high inter - annotator agreement   is observed , with Fleiss ’ kappa ( Fleiss , 1971 ) being   0.856 , 0.844 , and 0.771 on SciDocs , Amazon , and   Twitter , respectively .   ( 4)Diversity ( Dieng et al . , 2020 ) measures the   mutual exclusivity of discovered topics . It is the   percentage of unique terms in all topics , which cor-   responds to our task requirement that each retrieved   term is discriminatively close to one category and   far from the others .   Experiment Settings . We use BioBERT as the296PLM on SciDocs , and BERT - base - uncased as the   PLM on Amazon and Twitter . The embedding   dimension of uis 768 ( the same as e ) ; the   number of negative samples b= 5 . In ensem-   ble ranking , the length of the general / local ranking   listM= 100 ; the hyperparameter ρin Eq . ( 6 ) is   set as 0.1 ; the number of iterations T= 4 ; after   each iteration , we increase the size of SbyN= 3 .   We use the top-10 ranked terms in each topic for   ﬁnal evaluation ( i.e. , |S|= 10 in Eqs . ( 8)-(11 ) ) .   Experiments are run on Intel Xeon E5 - 2680 v2 @   2.80GHz and one NVIDIA GeForce GTX 1080 .   4.2 Performance Comparison   Table 3 shows the performance of all methods . We   run each experiment 3 times with the average score   reported . To show statistical signiﬁcance , we con-   duct a two - tailed unpaired t - test to compare S-   T and each baseline . ( The performance of   BERT and BioBERT is deterministic according to   our usage . When comparing ST with them ,   we conduct a two - tailed Z - test instead . ) The signif-   icance level is also marked in Table 3 .   We have the following observations from Table   3 . ( 1 ) Our ST model performs consistently   well . In fact , it achieves the highest score in 8   columns and the second highest in the remaining 4   columns . ( 2 ) Classical seed - guided topic modeling   baselines ( i.e. , SeededLDA and Anchored CorEx )   perform not well in respect of NPMI ( topic coher-   ence ) and MACC ( term accuracy ) . Embedding-   based topic discovery approaches ( i.e. , Labeled   ETM and CatE ) make some progress , but they still   signiﬁcantly underperform the PLM - empowered   ST model on SciDocs and Amazon . ( 3 )   ST consistently performs better than S-   T -NoIter on all three datasets , indicating the   positive contribution of the proposed iterative pro-   cess . ( 4 ) ST guarantees the mutual exclu-   sivity ofS, ... ,S. In comparison , SeededLDA ,   Labeled ETM , and BERT can not guarantee such   mutual exclusivity .   In - vocabulary vs. Out - of - vocabulary . Figure   1 compares the MACC scores of different seed-   guided topic discovery methods on in - vocabulary   categories and out - of - vocabulary categories . We   ﬁnd that the performance improvement of S-   T upon baselines on out - of - vocabulary cat-   egories is larger than that on in - vocabulary ones .   For example , on Amazon , ST underper-   forms CatE on in - vocabulary categories but outper-   forms CatE on out - of - vocabulary ones ; on Twit-   ter , the gap between ST and baselines be-   comes much more evident on out - of - vocabulary   categories . Note that all baselines in Figure 1 do   not utilize the power of PLMs , so this observation   validates our claim that PLMs are helpful in tack-   ling out - of - vocabulary seeds .   4.3 Parameter Study   We study the effect of two important hyperparame-   ters : ρ(the hyperparameter in ensemble ranking )   andT(the number of iterations ) . We vary the   value ofρin{0.1,0.3,0.5,0.7,0.9,1}(ST   usesρ= 0.1by default ) and the value of Tin   { 1,2,3,4,5}(ST usesT= 4 by default ,   andST -NoIter is the case when T= 1 ) .   Figure 2 shows the change of model performance   measured by NPMI and LCP.297   According to Figures 2(a ) and 2(b ) , in most   cases , the performance of ST deteriorates   asρincreases from 0.1 to 0.9 . Thus , setting ρ= 0.1   always leads to competitive NPMI and LCP scores   on the three datasets . Although ρ= 1 is better   thanρ= 0.9 , its performance is still suboptimal in   comparison with ρ= 0.1 . This ﬁnding indicates   that replacing the mean reciprocal rank ( i.e. , ρ= 1 )   with our proposed Eq . ( 6 ) is reasonable . According   to Figures 2(c ) and 2(d ) , ST usually per-   forms better when there are more iterations . On   SciDocs and Twitter , the scores start to converge   afterT= 4 . Besides , more iterations will result   in longer running time . Overall , we believe setting   T= 4strikes a good balance .   4.4 Case Study   Finally , we show the terms retrieved by different   methods as a case study . From each of the three   datasets , we select an out - of - vocabulary category   and show its topic discovery results in Table 4 . We   mark a retrieved term as correct (  ) if at least 3 of   the 5 annotators judge the term as relevant to the   seed . Otherwise , we mark the term as incorrect (  ) .   For the category “ hepatitis a / b / c / e ” from Sci - Docs , SeededLDA and Anchored CorEx can only   ﬁnd very general medical terms , which are relevant   to all seeds in SciDocs and thus inaccurate ; Labeled   ETM and CatE ﬁnd terms about “ alanine amino-   transferase ” , whose elevation suggest not only hep-   atitis but also other diseases like diabetes and heart   failure , thus not discriminative either ; BioBERT   andST , with the power of a PLM , can ac-   curately pick terms relevant to “ hepatitis b ” and   “ hepatitis c ” . For the category “ sports and out-   doors ” from Amazon , SeededLDA and Anchored   CorEx again ﬁnd very general terms , most of which   are not category - discriminative ; Labeled ETM and   CatE are able to pick more speciﬁc terms such   as “ cars and tracks ” , but they still make mistakes ;   BERT , as a PLM , can accurately ﬁnd terms that   have lexical overlap with the category name ( e.g. ,   “ outdoorsmen ” , “ sporting events ” ) , meanwhile such   terms are less diverse ; ST -NoIter starts to   discover more concrete terms than BERT ( e.g. , “ in-   door soccer ” , “ bike riding ” ) by leveraging local   text semantics ; the full ST model , with an   iterative updating process , can ﬁnd more speciﬁc   and informative terms ( e.g. , “ canoeing ” , “ picnics ” ,   and “ rafting ” ) . For the category “ travel and trans-298port ” from Twitter , both BERT and CatE make   mistakes by including the term “ natural history ” ;   ST -NoIter , without an iterative update pro-   cess , also includes this error ; the full ST   model ﬁnally excludes this error and achieves the   highest accuracy in the retrieved top-5 terms among   all compared methods .   5 Related Work   Seed - Guided Topic Discovery . Seed - guided topic   models aim to leverage user - provided seeds to dis-   cover underlying topics according to users ’ inter-   ests . Early studies take LDA ( Blei et al . , 2003 )   as the backbone and incorporate seeds into model   learning . For example , Andrzejewski et al . ( 2009 )   consider must - link and cannot - link constraints   among seeds as priors . SeededLDA ( Jagarlamudi   et al . , 2012 ) encourages topics to contain more   seeds and encourages documents to select topics   relevant to the seeds they contain . Anchored CorEx   ( Gallagher et al . , 2017 ) extracts maximally informa-   tive topics by jointly compressing the corpus and   preserving seed relevant information . Recent stud-   ies start to utilize embedding techniques to learn   better word semantics . For example , CatE ( Meng   et al . , 2020a ) explicitly encourages distinction   among retrieved topics via category - name guided   embedding learning . However , all these models   require the provided seeds to be in - vocabulary ,   mainly because they focus on the input corpus only   and are not equipped with general knowledge of   PLMs .   Embedding - Based Topic Discovery . A number   of studies extend LDA to involve word embed-   ding . The common strategy is to adapt distribu-   tions in LDA to generate real - valued data ( e.g. ,   Gaussian LDA ( Das et al . , 2015 ) , LFTM ( Nguyen   et al . , 2015 ) , Spherical HDP ( Batmanghelich et al . ,   2016 ) , and CGTM ( Xun et al . , 2017b ) ) . Some   other studies think out of the LDA backbone . For   example , TWE ( Liu et al . , 2015 ) uses topic struc-   tures to jointly learn topic embeddings and improve   word embeddings . CLM ( Xun et al . , 2017a ) col-   laboratively improves topic modeling and word   embedding by coordinating global and local con-   texts . ETM ( Dieng et al . , 2020 ) models word - topic   correlations via word embeddings to improve the   expressiveness of topic models . More recently , Sia   et al . ( 2020 ) show that directly clustering word em-   beddings ( e.g. , word2vec or BERT ) also generates   good topics ; Thompson and Mimno ( 2020 ) further   ﬁnd that BERT and GPT-2 discover high - quality   topics , but RoBERTa does not . These models areunsupervised and hard to be applied to seed - guided   settings . In contrast , our ST framework   joint leverages PLMs , word embeddings , and seed   information .   6 Conclusions and Future Work   In this paper , we study seed - guided topic discov-   ery in the presence of out - of - vocabulary seeds . To   understand and make use of in - vocabulary com-   ponents in each seed , we utilize the tokenization   and contextualization power of PLMs . We pro-   pose a seed - guided embedding learning framework   inspired by the goal of maximizing PMI in topic   modeling , and an iterative ensemble ranking pro-   cess to jointly leverage general knowledge of the   PLM and local signals learned from the input cor-   pus . Experimental results show that ST   outperforms seed - guided topic discovery baselines   and PLMs in terms of topic coherence , term accu-   racy , and topic diversity . A parameter study and a   case study further validate some design choices in   ST .   In the future , it would be interesting to extend   ST to seed - guided hierarchical topic dis-   covery , where parent and child information in the   input category hierarchy may help infer the mean-   ing of out - of - vocabulary nodes .   Acknowledgments   We thank anonymous reviewers for their valu-   able and insightful feedback . Research was sup-   ported in part by US DARPA KAIROS Program   No . FA8750 - 19 - 2 - 1004 , SocialSim Program No .   W911NF-17 - C-0099 , and INCAS Program No .   HR001121C0165 , National Science Foundation   IIS-19 - 56151 , IIS-17 - 41317 , and IIS 17 - 04532 ,   and the Molecule Maker Lab Institute : An AI Re-   search Institutes program supported by NSF under   Award No . 2019897 , and the Institute for Geospa-   tial Understanding through an Integrative Discov-   ery Environment ( I - GUIDE ) by NSF under Award   No . 2118329 . Any opinions , ﬁndings , and con-   clusions or recommendations expressed herein are   those of the authors and do not necessarily rep-   resent the views , either expressed or implied , of   DARPA or the U.S. Government .   References299300A The Embedding Learning Objective   In Section 3.2 , we propose the following embed-   ding learning objective :   Now we prove that maximizing Jis implicitly   performing the factorization in Eq . ( 3 ) .   Levy and Goldberg ( 2014 ) have proved that max-   imizingJ is implicitly doing the following   factorization .   We follow their approach to consider the other two   termsJ andJ in Eq . ( 12 ) . Using   the negative sampling strategy to rewrite J ,   we get   whereσ(·)is the sigmoid function . Following   ( Levy and Goldberg , 2014 ; Qiu et al . , 2018 ) , we   assume the negative sampling distribution ∝λ .   Then , the objective becomes   For a speciﬁc term - document pair ( w , d ) , we con-   sider its effect in the objective :   Letx = uv . To maximizeJ , we should   have301That is ,   Therefore , e=−1(which is invalid ) or   e=. In other words ,   Similarly , forJ , the objective can be   rewritten as   Following the derivation of J , we get   Putting Eqs . ( 13 ) , ( 19 ) , and ( 21 ) together gives   us Eq . ( 3 ) .   B The Ensemble Ranking Function   In Section 3.3 , we propose the following ensemble   ranking function :   Now we prove this ranking function is a general-   ization of the arithmetic mean reciprocal rank ( i.e. ,   MRR ) and the geometric mean reciprocal rank :   The case ofρ→1is trivial . When ρ→0 , we aim   to show thatIn fact , letr = andr=.   The third line is obtained by applying L’Hopital ’s   rule.302