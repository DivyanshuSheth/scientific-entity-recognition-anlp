  Zhihui XieHandong ZhaoTong YuShuai LiShanghai Jiao Tong UniversityAdobe Research   { fffffarmer,shuaili8}@sjtu.edu.cn   { hazhao,tyu}@adobe.com   Abstract   Large pretrained multilingual language models   ( ML - LMs ) have shown remarkable capabili-   ties of zero - shot cross - lingual transfer , without   direct cross - lingual supervision . While these   results are promising , follow - up works found   that , within the multilingual embedding spaces ,   there exists strong language identity informa-   tion which hinders the expression of linguistic   factors shared across languages . For seman-   tic tasks like cross - lingual sentence retrieval ,   it is desired to remove such language identity   signals to fully leverage semantic information .   In this work , we provide a novel view of pro-   jecting away language - specific factors from a   multilingual embedding space . Specifically , we   discover that there exists a low - rank subspace   that primarily encodes information irrelevant   to semantics ( e.g. , syntactic information ) . To   identify this subspace , we present a simple but   effective unsupervised method based on singu-   lar value decomposition with multiple mono-   lingual corpora as input . Once the subspace is   found , we can directly project the original em-   beddings into the null space to boost language   agnosticism without finetuning . We system-   atically evaluate our method on various tasks   including the challenging language - agnostic   QA retrieval task . Empirical results show that   applying our method consistently leads to im-   provements over commonly used ML - LMs .   1 Introduction   Large language models pretrained with self-   supervised objectives ( e.g. , masked language mod-   eling ) have become the de - facto standard for var-   ious NLP tasks ( Peters et al . , 2018 ; Devlin et al . ,   2019 ; Liu et al . , 2019 ) . Follow - up extensions to   the multilingual setting inherit similar training ob-   jectives and show very promising results ( Con-   neau and Lample , 2019 ; Conneau et al . , 2020b ;   K et al . , 2020 ) . Despite these models are trainedwithout explicit cross - lingual signals ( i.e. , trans-   lation pairs ) , they surprisingly exhibit impressive   zero - shot cross - lingual transferability on natural   language inference ( Conneau et al . , 2018 ) , ques-   tion answering ( Lewis et al . , 2020 ) , sentence re-   trieval ( Artetxe and Schwenk , 2019 ) , etc .   While these ML - LMs offer practical solutions   for cross - lingual tasks , there is an enduring debate   about why the ML - LMs work . From a positive per-   spective , Pires et al . ( 2019 ) conduct an exploratory   study on mBERT ( Devlin et al . , 2019 ) , suggesting   that cross - lingual transfer is possible even to lan-   guages in different scripts . Chi et al . ( 2020 ) probe   mBERT for structural phenomena and find that its   representations can recover syntactic tree distances   in languages other than English . These findings   present shreds of evidence that the pretrained mul-   tilingual representations do capture cross - lingual   properties in various aspects . On the flip side , a line   of research shows that pretrained ML - LMs encode   strong language - specific signals . This causes their   multilingual representations to cluster by language   identities instead of semantic meaning ( Wu and   Dredze , 2019 ; Roy et al . , 2020 ; Libovický et al . ,   2020 ) . The property largely hinders the expression   of linguistic signals shared across languages . For   applications like cross - lingual sentence retrieval   that mainly consider semantic information , ML-   LMs with strong language - specific signals tend to   retrieve answers from specific languages , regard-   less of their semantic meaning ( Roy et al . , 2020 ) .   Motivated by previous findings about language   identity information , we aim to locate language-   specific factors captured by the pretrained ML-   LMs for recovering a language - agnostic embed-   ding space . Inspired by advances in domain gen-   eralization ( Muandet et al . , 2013 ; Motiian et al . ,   2017 ; Piratla et al . , 2020 ) , we explore a simple but   effective approach , LSAR , to discover a Low - rank   Subspace for language- Agnostic Representations   within an ML - LM . The subspace primarily encodes5617information irrelevant to semantics , and can be   identified without any translation pairs based on   singular value decomposition . Once the subspace is   found , we can directly factor out language - specific   factors from the multilingual embeddings by pro-   jecting them into the null space without finetuning .   To evaluate LSAR , we focus on semantic tasks   for multilingual sentence embeddings . On standard   cross - lingual zero - shot transfer tasks including clas-   sification and sentence retrieval , LSAR consistently   achieves significant improvements . Especially , ap-   plying LSAR leads to significant improvements   for pretrained ML - LMs on LAReQA ( Roy et al . ,   2020 ) , a challenging benchmark targeting strong   language agnosticism .   We further examine what information exactly   the subspace contains . By performing correlation   analysis between structural language similarities   obtained from the URIEL database ( Littell et al . ,   2017 ) and the language similarities captured on the   subspace , we observe that the subspace encodes a   great deal of syntactic information . This implies   that LSAR successfully erases linguistic signals   that are redundant to semantic tasks to facilitate   language agnosticism .   To conclude , our main contributions are :   •We present one of the pioneering efforts to   discover that there exist low - rank subspaces of   pretrained ML - LMs ’ embeddings that mainly   encode language - specific signals .   •To identify the subspace in a ML - LM , we   present a simple unsupervised approach called   LSAR based on singular value decomposition .   By projecting embeddings onto the null space ,   LSAR can exclude the unwanted factors to   facilitate language agnosticism .   •Empirical results show that LSAR is surpris-   ingly effective for a variety of semantic tasks .   We also elucidate that the subspace encodes   strong syntactic signals with careful experi-   mental analysis .   2 Related Work   Understanding Pretrained Multilingual Rep-   resentations Recently , there has been a surge   of interest in probing pretrained ML - LMs like   mBERT ( Devlin et al . , 2019 ) . Pires et al . ( 2019 )   present an exploratory study on the cross - linguality   of mBERT , showing that mBERT exhibits strongzero - shot performances for typologically similar   languages . Libovický et al . ( 2020 ) find that the   original mBERT embeddings can be decomposed   into a language - specific component and a language-   neutral component . Chi et al . ( 2020 ) probe mBERT   for universal grammatical relations and show that   mBERT does encode fine - grained syntactic distinc-   tions across languages . Muller et al . ( 2021 ) find   that mBERT operates as the stacking of two sub-   networks and mainly the lower part of the model is   crucial for cross - lingual transfer .   Language - agnostic Representations To further   facilitate semantic downstream tasks like text clas-   sification , retrieval , and question answering , it is   appealing to remove language - specific signals from   the original embeddings without destroying the in-   trinsic semantic meaning .   LASER ( Artetxe and Schwenk , 2019 ) utilizes   parallel data to train a BiLSTM - based multilin-   gual sentence encoder . Zhao et al . ( 2021 ) obtain   language - agnostic embeddings from mBERT and   XLM - R by explicitly aligning the word pairs and   further normalizing the latent spaces with zero   mean and unit variance . Yang et al . ( 2021 ) regard   the top principal components from each language ’s   embedding space as the primary source of language   bias and propose to project them away to boost lan-   guage agnosticism .   Our work bears resemblance to Yang et al .   ( 2021 ) , but with clear distinctions in that : 1 ) we   model language - specific signals jointly in the mul-   tilingual embedding space instead of locating it   separately within each language ; 2 ) we further   verify what exactly the linguistic signals are iden-   tified , and present evidences that LSAR primar-   ily removes syntactic information . A few previ-   ous works ( Gonen et al . , 2020 ; Liang et al . , 2021 ;   Chang et al . , 2022 ) also attempt to locate language-   agnostic embeddings in subspaces of ML - LMs .   Apart from the dissimilarity of methodology , we   focus on sentence - level instead of token - level tasks   and provide shreds of evidence that the identified   subspace exhibits strong correlation with syntactic   information .   Low - rank Subspaces in Other Applications   Low - rank subspaces have been employed in   various applications . In face recognition , the   most expressive features for face representations   are located via subspace analysis methods like   PCA ( Turk and Pentland , 1991 ; Wang and Tang,5618   2004 ) . For domain adaptation and domain gener-   alization , a typical idea is to uncover a shared sub-   space on which the distribution mismatch between   domains is reduced ( Muandet et al . , 2013 ; Pan et al . ,   2011 ; Motiian et al . , 2017 ) . Recent advances in   probing Generative Adversarial Networks ( GANs )   also observe meaningful latent subspaces that en-   able precise control of GAN generation ( Wang and   Ponce , 2021 ; Zhu et al . , 2021 ) . These findings to   some extent motivate this paper .   3 Methodology   In this section , we first introduce our method to   identify the low - rank language - specific subspace   in an unsupervised manner . Once the subspace is   found , we can then suppress the language iden-   tity from the original multilingual embeddings   to achieve language agnosticism rectification by   projecting them to the null space . This post-   training alignment procedure can largely benefit   downstream tasks like cross - lingual retrieval which   solely utilize semantic - related information .   3.1 Multilingual Embedding Decomposition   To locate the language - specific factors , we follow   previous works ( Pires et al . , 2019 ; Libovický et al . ,   2020 ; Yang et al . , 2021 ) to hypothesize that each   multilingual embedding e∈Rin language lcanbe decomposed in an additive form :   e:=s+a ,   where s∈Randa∈Rrepresent the language-   specific component to remove and the language-   agnostic component to keep , respectively .   Built on the above assumption , previous unsu-   pervised approaches extract the language identity   information separately for each language space .   Given an ML - LM ( e.g. , mBERT ) , the extracted   embeddings E:={e}from nsamples of   task training data or external monolingual corpora   contain mixed linguistic information of semantic-   relevant and semantic - irrelevant signals about lan-   guage l. Libovický et al . ( 2020 ) use the empir-   ical mean / summationtexteto obtain s. Yang et al .   ( 2021 ) use the top- kprincipal components C=   PCA(E)∈Rto encode language identity sig-   nals , and propose to factor them out with s=   CCeto facilitate language agnosticism .   In spite of their promising results for semantic-   related tasks , these methods fall short of compre-   hensively discovering cross - lingual relationship in   the latent space . For each language l , both of them   leverage solely Eto locate language - specific in-   formation , which fails to distinguish itself from   semantic signals as other languages ’ characteris-   tics is unknown . Without careful tuning , this can5619lead to unexpected semantic information loss ( Kho-   dak et al . , 2018 ) . Besides , it is also unclear what   exactly language - specific signals are captured by   these approaches .   3.2 Low - rank Subspace Identification   To alleviate the above issues , we attempt to glob-   ally capture language - specific information from   the multilingual latent space . Inspired by previous   works in domain adaptation and domain general-   ization ( Muandet et al . , 2013 ; Motiian et al . , 2017 ;   Piratla et al . , 2020 ) , we present a simple approach   that identifies a low - rank subspace of the original   multilingual latent space , M∈R , spanned by   rcomponents . Intuitively , the subspace encodes   language - specific signals via measuring the latent   discrepancy among languages .   To be specific , we first extract the mean embed-   dingµ=/summationtexteof each language lin the   same spirit of previous approaches . Concatenat-   ingµofLlanguages column - by - column results   in the mean embedding matrix M∈R. As   discussed in Section 3.1 , the mean embeddings can   unexpectedly mix the desired language - specific sig-   nals with semantic information . To avoid removing   the semantic information shared among languages ,   we decompose Minto two components : 1 ) a vec-   torµrepresenting what is commonly shared across   languages in the latent space ; 2 ) a matrix Mspec-   ifying a low - rank subspace on which different lan-   guages express different linguistic signals . With   the orthogonality constraint , our objective is :   min / vextenddouble / vextenddouble / vextenddoubleM−µ 1−MΓ / vextenddouble / vextenddouble / vextenddouble   s.t.µ⊥Span ( M),(1 )   where Γ∈Ris the coordinates of language-   specific signals along the subspace ’s rcomponents   and 1∈Rcontains all ones .   The optimal solution of Equation 1 can be com-   puted efficiently via Singular Value Decomposi-   tion ( SVD ) , as proved in Appendix A. Algorithm 1   presents the detailed procedure . The only hyper-   parameter r < L controls the amount of language-   specific information captured by the identified sub-   space . The larger ris , the more language - specific   signals we can identify .   3.3 Language Agnosticism Rectification   Once we find the low - rank subspace with seman-   tically irrelevant information encoded , we can im - Algorithm 1 : language - specific Subspace   Identification   In : languages ’ mean embeddings M , rank   of subspace r   Out : language - agnostic component µ ,   language - specific subspace M ,   coordinates Γ   / * 1 ) Approximate Min low rank * /µ←M 1;M,_,Γ←Top - rSVD / parenleftbig   M−µ1 / parenrightbig   ; M←µ1+MΓ ;   / * 2 ) Force orthogonality * /µ←M1;M,_,Γ←Top - rSVD / parenleftbig   M−µ 1 / parenrightbig   prove language agnosticism via projecting multi-   lingual embeddings onto the null space of M :   a=/parenleftbigg   I−M / parenleftig   MM / parenrightig   M / parenrightbigg   e   = e−MMe .   Given that usually l≪d , the information re-   moved is restricted to aspects that emerges to be   language - specific and will not lead to dimensional   collapse .   4 Experiments   We systematically evaluate our method on various   tasks followed by further analyses , with the pur-   poses of understanding : 1 ) whether the proposed   approach can benefit downstream tasks ; 2 ) what   exactly the identified low - rank subspace captures .   To begin with , we describe our evaluation proto-   col for the alignment methods , which largely fol-   lows Yang et al . ( 2021 ) but with a broader scope   to include more base models as listed in Section B.   Given one of the pretrained ML - LMs , we first ran-   domly collect 10,000 sentences for each language   from the OSCAR corpus ( Ortiz Suárez et al . , 2020 )   covering all the evaluated languages and their web   crawl texts . The sentence embeddings extracted   by the pretrained model are then used for finding5620mBERT XLM XLM - R LABSE   Cross - lingual zero - shot transfer ( w/o finetuning )   Original 37.53 28.13 57.68 95.47   Centered ( Libovický et al . , 2020 ) 39.57 27.13 61.08 95.56   LIR ( k= 1 ) ( Yang et al . , 2021 ) 39.70 28.75 61.60 95.63   LIR ( k= 15 ) ( Yang et al . , 2021 ) 41.21 31.65 62.80 95.56   LSAR 44.64 33.16 65.05 95.54   Cross - lingual zero - shot transfer ( w/ finetuning )   Full - Model - FS ( Xu et al . , 2022)- - 60.5 /66.2 -   S - Tuning ( Xu et al . , 2022)- - 66.1 /69.5 -   Full - Model ( Ruder et al . , 2021)42.8 - 76.6 -   the low - rank subspace described in Equation 1 . Un-   less otherwise indicated , we consistently report   LSAR with r = l−1 , where lis the number of the   evaluated languages . We evaluate language agnos-   ticism over pretrained ML - LMs that are commonly   used , as described in Appendix B. Detailed results   are listed in Appendix C.3 .   4.1 Baselines   Apart from Original that keeps the pretrained ML-   LM intact , we compare LSAR with the following   baselines . The baselines share the same setting as   ours in that both of them require no parallel text   and aim at removing language - specific factors in a   post - training manner .   Centered Libovický et al . ( 2020 ) extract   language - neutral embeddings from the original   pretrained multilingual sentence encoders via   subtracting the mean embedding for each language .   The mean embeddings are calculated from the   multi - monolingual OSCAR corpus .   LIR Yang et al . ( 2021 ) propose to project away   the top- kprincipal components of each language ’s   embeddings to facilitate language agnosticism ,   where kis the hyperparameter . Again , the top   principal components are extracted from the multi-   monolingual corpus .   4.2 Sentence Retrieval   Tatoeba ( Artetxe and Schwenk , 2019 ) is a com-   monly used dataset for evaluating ML - LMs . It   comprises up to 1,000 sentences for each language   along with their English translations . We follow   the evaluation procedure of XTREME ( Hu et al . ,2020 ) that covers 36 languages . For each language   pair , we go through each sentence in the source   language and find the closest sentence in the target   language using cosine similarity .   The top-1 retrieval accuracy results are shown   in Table 1 . For mBERT ( Devlin et al . , 2019 ) ,   XLM ( Conneau and Lample , 2019 ) , and XLM-   R ( Conneau et al . , 2020a ) , applying LSAR brings   significant performance gains of up to 19 % rela-   tive improvement . Compared with Centered and   LIR which separately remove information for each   language , LSAR jointly utilizes the encoded in-   formation from all the languages to better locate   language - specific factors . Furthermore , we observe   that LSAR consistently achieves the best results   with hyperparameter r(the rank of the low - rank   subspace ) equal to the number of the evaluated   languages , as shown in Appendix C.1 . As the lan-   guages are diversely distributed , it is reasonable   that each language possesses its own linguistic char-   acteristics , resulting in a larger language - specific   subspace to factor out . In contrast , we find that LIR   is vulnerable to its hyperparameter k(the number   of the removed principal components ) , which is   best shown in Figure 7 .   For LABSE ( Feng et al . , 2022 ) , all the methods   fail to provide marked enhancement . This can be   mainly attributed to the fact that LABSE already   uses parallel corpora to explicitly align multilin-   gual embeddings . Despite that the improvement   is marginal , it is still promising to combine LSAR   with existing pretraining objectives to produce bet-   ter language - agnostic embeddings .   We also include several representative baselines   that finetune either mBERT or XLM - R for bet-5621   XQuAD - R MLQA - R   En - En X - X En - En X - X   Original 28.57 23.36 35.71 26.21   Centered 35.37 44.66 35.36 42.14   LIR ( k= 1)37.70 44.25 38.03 41.96   LSAR 41.13 45.89 40.55 43.32   ter cross - lingual transfer results . Although these   methods are not directly comparable to ours , we be-   lieve it provides additional valuable findings to in-   clude them . Full - Model - FS and S - Tuning finetune   XLM - R on full English labeled examples and then   K - shot data over target languages ( K= 64/128 ) .   For Full - Model , the pretrained models are fine-   tuned on the English SQuAD data . On mBERT ,   LSAR outperforms Full - Model by a large margin .   We also observe on XLM - R that LSAR is compet-   itive with finetuning the full model on 128 - shot   data as well as finetuning a dedicated language sub-   network ( S - Tuning ) on 64 - shot data . The results   are quite promising given that we obtain better per-   formances with the original encoders intact and no   task - relevant training data .   4.3 Language - agonstic Answer Retrieval   While Tatoeba reveals the cross - lingual transfer-   ability across English - centric language pairs , it is   restricted to monolingual pools ( i.e. , the set of can-   didates is restricted to certain language ) . Therefore ,   it fails to thoroughly evaluate whether texts with a   similar semantic meaning are grouped together inthe latent space , regardless of their languages .   With that in mind , we further examine the align-   ment methods on LAReQA ( Roy et al . , 2020 ) , a   challenging cross - lingual answer retrieval task . Un-   like Tatoeba , the targets of LAReQA must be re-   trieved from a large multilingual candidate pool .   It consists of two sub - datasets , XQuAD - R and   MLQA - R , whose candidate pool covers 11 and   7 languages respectively .   We follow Yang et al . ( 2021 ) to evaluate the   alignment methods on two models , mBERT ( En-   En)andmBERT ( X - X ) . Specifically , mBERT ( En-   En ) finetunes the original mBERT model on the   English QA pairs collected from the SQuAD v1.1   dataset . mBERT ( X - X ) employs the same train-   ing procedure but with an extended dataset where   each sample is translated into the 11 XQuAD lan-   guages . Since all positive samples for finetuning   are within the same language as the question query ,   both models exhibit strong self - language bias while   preserving the weak alignment property . For eval-   uation , we use the dot product of embeddings to   score a QA pair , which accords with the finetuning   protocol . The retrieval performance is measured by   mean Average Precision ( mAP ) .   Table 2 reports our LAReQA results . We can   observe that applying LSAR again results in sig-   nification improvements , nearly doubling mAP of   mBERT ( X - X ) on XQuAD - R. Since in the candi-   date pool each language has one of the relevant   answers , better retrieval performances directly in-   dicate better language agnosticism . Centered and   LIR ( k= 1 ) also show impressive performances ,   suggesting that in weakly aligned multilingual sys-5622mBERT XLM XLM - R   Original 74.73 75.31 80.32   LIR ( k= 1 ) 75.39 75.73 81.14   LSAR ( r= 1 ) 75.58 74.93 81.47   LSAR ( r= 2 ) 75.49 75.85 82.37   LSAR 75.24 75.27 81.25   tems , the mean embeddings and principal compo-   nents do encode language - specific signals . But for   LIR , it is shown that removing the first principal   component consistently leads to the best perfor-   mance . This is opposite to what we observe on   Tatoeba , where the optimal kis around 15 .   To further illustrate the degree of language ag-   nosticism , we project an English question ( What   theory best explains gravity ? ) as well as all candi-   dates and the ground - truth answers in English , Thai ,   and Mandarin via PCA . As plotted in Figure 2 ,   candidates in English are retrieved from mBERT   ( X - X ) with higher priority than those in Thai and   Mandarin . Applying LSAR can effectively elimi-   nate strong language identity information residing   in the original embedding space and draw closer   the question and answers from different languages .   LIR with k= 1 , however , falls short of rectify-   ing language - specific signals as illustrated by the   embedding spectrum in Figure 2b .   4.4 Zero - shot Classification   We also include the Amazon Reviews classification   task ( Prettenhofer and Stein , 2010 ) to assess zero-   shot cross - lingual transfer . The dataset consists of   product reviews in English , French , German , and   Japanese . Each review is labeled as positive or neg-   ative , making it a binary classification task . We use   mBERT XLM XLM - R   Original 0.2815 0.5422 0.2457   Centered 0.0975 0.2483 0.2004   LIR ( k= 1)0.0900 0.1875 0.2203   LSAR 0.0801 0.1320 0.0856   the same procedure to extract sentence embeddings   as in Section 4.2 , and normalize them to make reg-   ularization hyperparameters more consistent across   languages . Appendix C.1 specifies how we select   hyperparameters . Following ( Yang et al . , 2021 ) ,   the performance is evaluated via training a logistic   regression classifieron the English training data   and then evaluating it on the test sets of all four   languages .   From Table 3 , we observe that the classifier   trained on English data benefits from LSAR for   classifying reviews based on semantics as the   language - specific factors are effectively erased . An-   other interesting observation is that unlike sentence   retrieval , removing more directions does not result   in better performance . This indicates that classi-   fication tasks can be more sensitive to semantic   information .   4.5 Analysis   In this section , we present analysis on a variety   of aspects towards what exactly language - specific   information LSAR captures .   4.5.1 Language - specific Signals are Rectified   From previous findings , we conjecture that our   method achieves impressive cross - lingual perfor-   mance by effectively removing language identity   signals . To quantitatively verify this , we measure   the strength of language identity information from   the perspective of clustering quality . If the em-   beddings are clustered by language types , we can   generally state that language - specific signals still   play a prominent role in the multilingual latent   space.5623   We perform K - Means clustering on sentence rep-   resentations of Tatoeba with the number of clusters   equal to the number of languages , and then evaluate   the resulting clusters using the Normalized Mutual   Information ( NMI ) metric ( Jawahar et al . , 2019 ) .   As shown in Table 4 , the original pretrained embed-   dings have relatively high NMI scores , suggesting   the existence of strong language identity informa-   tion . Our method consistently achieves smaller   NMI scores . This indicates that the embeddings   have a lower tendency to group by language types   since LSAR successfully winnows down language-   specific information .   The same conclusion can be drawn from the   limit - to - one - target setting of LAReQA ( Roy et al . ,   2020 ) . Specifically , we remove 10 targets from   the multilingual pool of XQuAD - R to evaluate on   each target separately . We choose the most biased   X - X variant as the base model . The heatmaps in   Figure 3 show for each question language ( row ) ,   the retrieval mAP on the pool containing just   one target in different answer languages ( column ) .   Since X - X has strong self - language bias , Origi-   nal shows better performance on the diagonal than   off - diagonal . After applying LSAR , we observe   a significant increase in average off - diagonal per-   formance ( 23.76 % vs. 5.89 % ) , without sacrific-   ing much on - diagonal performance ( 81.57 % vs.   84.57 % ) . This again verifies that applying LSAR   effectively removes language - specific information .   4.5.2 Removed Components Form Groups of   Language Families   We next examine whether the removed components   found by the low - rank subspace are truly language-   specific . This is demonstrated via plotting the re-   moved components for different languages along   top basis vectors of the subspace . For the ease of   visualization , we group them by language family .   Figure 4 shows the histograms of removed com-   ponents along the top two basis vectors extracted   from mBERT on 36 languages of Tatoeba , accord-   ing to Equation 1 . We can observe that the removed   components disperse in groups of language fami-   lies along these directions . This implies that the   identified subspace do capture language - specific   signals and hence removing them along the basis   vectors can narrow down latent discrepancy .   4.5.3 The Identified Subspace Primarily   Encodes Syntactic Information   Finally , given that the removed components are   language - specific , we investigate to what extent the   low - rank subspace encodes typological relations   among languages . Specifically , we use the URIEL   database ( Littell et al . , 2017 ) to collect distances   between English and other languages set out by   experts based on certain typological information   ( e.g. , syntax and phonology ) . We then compare   the typological distances with languages similari-   ties obtained from the removed language - specific   embeddings sas well as the resulting language-   agnostic embeddings aby calculating the cosine   similarity between languages ’ mean embeddings .   Among all types of typological signals listed   in URIEL , we find that the removed language-   specific factors are mostly correlated with syntac-   tic information . Table 5 shows the Pearson cor-   relations on English and other 36 languages from   Tatoeba . The removed language - specific compo-   nentsis highly correlated with syntactic infor-5624mBERT XLM XLM - R LABSE   s 0.6910 0.6378 0.7526 0.6894   a-0.2711 0.2239 0.1338 -0.2362   mation , whereas the correlation is much smaller   in the language - agnostic embedding space with   sremoved . This finding is in line with previous   works ( Chi et al . , 2020 ; Zhao et al . , 2021 ) that ob-   serve the pretrained multilingual models encode   rich syntactic information .   We find no prominent correlation between the   removed components along certain basis vectors   of the subspace and typological information . As   we do not presuppose any correspondence between   basis vectors and linguistic signals , a specific basis   vector falls short of individually encoding language-   specific information .   5 Conclusion   We present a simple yet effective approach called   LSAR to boost language agnosticism for pretrained   multilingual encoders . LSAR identifies a low - rank   subspace residing in a pretrained model that primar-   ily encodes language - specific signals in an unsu-   pervised manner via singular value decomposition .   Once the subspace is discovered , it can be used to   efficiently project away the language identity in-   formation . Empirical results demonstrate the great   effectiveness of LSAR on semantic tasks and shed   light on its ability to locate syntactic relations be-   tween languages .   Limitations   Our method LSAR is designed and evaluated for se-   mantic tasks . For future work , we are interested in   continuing our study for locating more fine - grained   linguistic information , which can potentially boost   a larger variety of downstream tasks . While the   simplicity of the proposed LSAR is appealing , it   also opens up directions for future work by general-   izing the first - moment mean embeddings to higher-   moment statistics and combining with pretraining   objectives in more sophisticated ways . References56255626   A Theoretical Justification   In this section , we present Theorem 1 and the cor-   responding proof . We follow the same proving   procedure in Piratla et al . ( 2020).5627Theorem 1 . For any matrix M∈R , Algo-   rithm 1 returns µ∈R , M∈R , Γ∈R   that minimize Equation 1 where µ⊥Span ( M ) .   Proof . Algorithm 1 first obtains the best approx-   imation of Mwith rank r+ 1 and 1 in its row   space ( Line 1 - 3 ) . The orthogonal constraint µ⊥   Span ( M)is then forced without obeying the low-   rank property ( Line 4 - 5 ) .   To begin with , note that the optimization prob-   lem in Equation 1 is equivalent to the following :   min / vextenddouble / vextenddouble / vextenddoubleM−/hatwiderM / vextenddouble / vextenddouble / vextenddouble   s.t . rank / parenleftig   /hatwiderM / parenrightig   ≤r+ 1and   1∈Span / parenleftig   /hatwiderM / parenrightig   .(2 )   LetU , Σ , V = SVD / parenleftbig   M−µ1 / parenrightbig   . We have   that 1⊥Span / parenleftbig   V / parenrightbig   given / parenleftbig   M−µ1 / parenrightbig   1=   0 . Denote by UΣVthe top- rcomponent of   UΣV , byσ(A)thei - th largest singular value   ofAand by Athe best rank- iapproximation of   A.   The first step is to show that µ1+UΣV   minimizes the objective in Equation 2 . Following   the proof of Eckart - Young - Mirsky theorem for low-   rank approximation ( Schmidt , 1907 ; Eckart and   Young , 1936 ) , let /tildewiderM:=M−/hatwiderMwith any feasible   /hatwiderMfixed . We have   σ / parenleftig   /tildewiderM / parenrightig   = /vextenddouble / vextenddouble / vextenddouble / tildewiderM−/tildewiderM / vextenddouble / vextenddouble / vextenddouble   = /vextenddouble / vextenddouble / vextenddouble / tildewiderM−/tildewiderM / vextenddouble / vextenddouble / vextenddouble+/vextenddouble / vextenddouble / vextenddouble / hatwiderM−/hatwiderM / vextenddouble / vextenddouble / vextenddouble   ≥/vextenddouble / vextenddouble / vextenddouble / tildewiderM+/hatwiderM−/tildewiderM−/hatwiderM / vextenddouble / vextenddouble / vextenddouble   = /vextenddouble / vextenddouble / vextenddoubleM−/tildewiderM−/hatwiderM / vextenddouble / vextenddouble / vextenddouble   ≥min / vextenddouble / vextenddoubleM−¯M / vextenddouble / vextenddouble ,   where the minimum is taken over all ¯Mwith   rank / parenleftbig¯M / parenrightbig   = i+rand 1∈Span / parenleftig   ¯M / parenrightig   . By   taking ¯M=µ1+UΣV , we   have σ / parenleftig   /tildewiderM / parenrightig   ≥σ / parenleftbig   UΣV / parenrightbig   and therefore   /vextenddouble / vextenddouble / vextenddoubleM−/hatwiderM / vextenddouble / vextenddouble / vextenddouble≥/vextenddouble / vextenddoubleM−µ1−UΣV / vextenddouble / vextenddouble .   Next , we find µandMthat meet the orthog-   onality constraint while preserving the low - rank   structure . Suppose µ 1+MΓ=µ1 +   MΓwithµ⊥Span ( M ) , we have that   µ/parenleftbig   µ 1+MΓ / parenrightbig   = ∥µ∥1which yields   µ=∥µ∥/parenleftig   µ1+MΓ / parenrightig   1.B Base Models   We evaluate the alignment methods based on a num-   ber of established pretrained multilingual models .   We mainly build on the Transformers library ( Wolf   et al . , 2020 ) for our experiments .   mBERTMultilingual BERT ( Devlin et al . ,   2019 ) is a transformer model ( Vaswani et al . , 2017 )   pretrained on Wikipedia , with the objective of   Masked Language Modeling ( MLM ) and a shared   vocabulary across all languages .   XLMXLM ( Conneau and Lample , 2019 ) also   uses the MLM objective and the monolingual   Wikipedia corpus for pretraining , with a larger   model and a larger vocabulary .   XLM - RXLM - R ( Conneau et al . , 2020a ) fol-   lows a similar training procedure as XLM but col-   lects the larger - scale CommonCrawl corpus .   LABSELABSE ( Feng et al . , 2022 ) is the state-   of - the - art multilingual sentence encoder that lever-   ages bilingual sentence pairs for pretraining .   Following previous works ( Jawahar et al . , 2019 ;   Ruder et al . , 2021 ) that observe certain intermediate   layers of Transformer consistently outperform the   last layer for cross - lingual tasks , we use the 8th   layer for mBERT and XLM , and the 12th layer for   XLM - R. We apply mean - pooling to obtain sentence   embeddings as it is widely used ( Conneau et al . ,   2020b ; Muller et al . , 2021 ) . For LABSE as well   as mBERT ( X - X ) and mBERT ( En - En ) used in   LAReQA , we evaluate the alignment methods on   the original sentence embeddings .   C Supplementary Results   In this section , we provide supplementary experi-   mental results .   C.1 Hyperparameter Selection   For the considered baselines , we do not conduct   sophisticated hyperparameter search given that it is   non - trivial for LIR . To provide fair comparison , for   LIR and LSAR that both have one single hyperpa-   rameter ( the number of top principal components   kand the number of basis vectors to span the low-   rank subspace r ) , we exhaustively enumerate all5628   XQuAD - R MLQA - R   En - En X - X En - En X - X   Original 28.57 23.36 35.71 26.21   Centered 35.38 45.47 35.87 43.27   LIR ( k= 1 ) 36.71 45.24 37.56 43.24   LIR ( k= 2 ) 36.70 44.74 37.11 42.42   LIR ( k= 3 ) 36.82 44.54 36.87 42.28   LSAR ( r= 1)30.51 26.38 36.79 28.79   LSAR ( r= 2)32.31 29.22 38.05 31.70   LSAR ( r= 3)34.05 31.99 39.00 35.28   LSAR 40.95 46.39 40.70 44.02   values within a scope and report the best perfor-   mances on the test data . Figure 7 shows the trend   of accuracy on Tatoeba as the hyparameters change .   C.2 Wiki40 - B Results   In this section we list the results of LAReQA ( Ta-   ble 6 ) and Amazon Reviews ( Table 8 - 11 ) with   Wiki-40B ( Guo et al . , 2020)as the text resource .   For Amazon Reviews , we also report the perfor-   mances obtained in the last layers to reproduce   those in Yang et al . ( 2021 ) .   For Amazon Reviews , we determine the L2 reg-   ularization strength using a hyperparameter sweep   on the 5 - fold cross - validation routine , over the   range between 1e-4 and 1e4 with 10 logarithmi-   cally spaced steps . This training procedure is im-   plemented using the Scikit - Learn library ( Buitinck   et al . , 2013).XQuAD - R MLQA - R   En - En X - X En - En X - X   Original 28.57 23.36 35.71 26.21   Centered 35.37 44.66 35.36 42.14   LIR ( k= 1 ) 37.70 44.25 38.03 41.96   LIR ( k= 2 ) 36.83 43.58 37.60 41.63   LIR ( k= 3 ) 36.21 43.15 36.89 41.03   LSAR ( r= 1)30.50 26.27 36.68 28.59   LSAR ( r= 2)32.36 28.69 37.94 31.15   LSAR ( r= 3)34.20 31.49 38.82 34.46   LSAR 41.13 45.89 40.55 43.32   C.3 OSCAR Results   The detailed results with OSCAR is provided in   this section .   Tatoeba We report the results for all languages   on Tatoeba in Table 17 - 20 . Additionally , the com-   plete set of results for clustering performance is   shown in Table 12 .   LAReQA We report the detailed results on   LAReQA in Table 7 . We omit listing all languages   due to limited space .   Amazon Reviews We provide the results for all   languages on Amazon Reviews in Table 13 - 16.5629Layer 8 Layer 12   en de fr jp avg . em de fr jp avg .   Original 81.13 72.82 76.02 68.98 74.74 80.07 70.05 73.75 64.86 72.18   LIR ( k= 1)81.12 72.33 76.80 72.25 75.62 80.03 70.00 71.73 67.51 72.32   LIR ( k= 2)81.05 71.90 76.80 72.35 75.52 79.98 71.15 72.50 69.04 73.17   LIR ( k= 3)81.10 72.23 76.22 71.06 75.15 80.03 70.85 73.67 69.36 73.48   LSAR ( r= 1)81.12 72.77 75.87 72.30 75.51 79.98 71.17 73.68 71.15 73.99   LSAR ( r= 2)81.13 72.50 76.85 72.33 75.70 80.08 71.23 73.45 70.91 73.92   LSAR 81.12 72.43 76.67 72.36 75.64 79.87 70.10 71.95 68.69 72.65   Layer 8 Layer 12   en de fr jp avg . em de fr jp avg .   Original 85.45 69.07 81.50 65.21 75.31 84.43 55.42 72.87 58.23 67.74   LIR ( k= 1)85.52 73.68 81.52 65.66 76.59 84.50 75.77 79.88 60.98 75.28   LIR ( k= 2)85.52 73.32 81.45 64.31 76.15 84.65 75.58 79.73 60.79 75.19   LIR ( k= 3)85.60 72.10 81.62 62.46 75.44 84.52 75.52 79.40 63.03 75.62   LSAR ( r= 1)85.53 70.98 81.52 66.44 76.12 84.45 56.75 75.20 66.64 70.76   LSAR ( r= 2)85.48 73.77 81.65 65.43 76.58 84.48 60.35 71.25 66.54 70.66   LSAR 85.50 73.78 81.63 65.41 76.58 84.48 75.78 79.57 64.99 76.21   Layer 11 Layer 24   en de fr jp avg . em de fr jp avg .   Original 84.33 78.32 82.30 76.35 80.32 90.55 78.08 83.57 67.14 79.84   LIR ( k= 1)84.33 82.47 81.68 80.18 82.17 90.53 88.67 89.88 86.16 88.81   LIR ( k= 2)84.45 82.18 82.10 80.08 82.20 90.62 88.48 88.27 85.61 88.25   LIR ( k= 3)84.33 81.40 83.08 78.48 81.82 90.67 88.55 88.40 85.61 88.31   LSAR ( r= 1)84.35 77.95 81.93 79.78 81.00 90.62 69.20 90.00 83.98 83.45   LSAR ( r= 2)84.33 82.52 81.17 80.53 82.14 90.60 88.73 79.18 79.30 84.45   LSAR 84.30 82.67 81.80 80.56 82.33 90.58 88.42 89.33 85.95 88.57   en de fr jp avg .   Original 83.32 81.37 84.27 79.26 82.05   LIR ( k= 1)83.18 81.70 84.32 79.51 82.18   LIR ( k= 2)83.20 81.92 84.18 79.33 82.16   LIR ( k= 3)83.18 81.83 84.32 79.45 82.19   LSAR ( r= 1)83.32 81.30 84.28 79.21 82.03   LSAR ( r= 2)83.10 81.63 83.90 79.61 82.06   LSAR 83.27 81.77 83.95 79.75 82.18   mBERT XLM XLM - R LABSE   Original 0.2815 0.5422 0.2457 0.0344   Centered 0.0975 0.2483 0.2004 0.0388   LIR ( k= 1)0.0900 0.1875 0.2203 0.0352   LSAR 0.0801 0.1320 0.0856 0.03065630   Layer 8 Layer 12   en de fr jp avg . em de fr jp avg .   Original 81.13 72.82 76.02 68.98 74.74 80.07 70.05 73.75 64.86 72.18   LIR ( k= 1)81.12 72.90 75.08 72.43 75.38 80.07 71.08 71.40 67.11 72.42   LIR ( k= 2)81.03 71.47 70.58 66.09 72.29 79.97 69.35 72.07 66.29 71.92   LIR ( k= 3)80.85 68.67 74.38 67.53 72.86 79.88 66.10 69.80 66.59 70.59   LSAR ( r= 1)81.25 72.78 75.80 72.48 75.58 79.98 71.03 73.62 70.45 73.77   LSAR ( r= 2)81.27 72.57 75.85 72.30 75.49 80.07 71.12 73.48 70.11 73.69   LSAR 81.15 72.90 75.22 71.68 75.24 79.88 70.80 71.70 67.79 72.54   Layer 8 Layer 12   en de fr jp avg . em de fr jp avg .   Original 85.45 69.07 81.50 65.21 75.31 84.43 55.42 72.87 58.23 67.74   LIR ( k= 1)85.58 77.57 80.05 59.74 75.74 84.52 75.75 80.20 55.26 73.93   LIR ( k= 2)85.40 76.72 79.82 60.86 75.70 84.48 75.57 77.95 55.46 73.36   LIR ( k= 3)85.15 77.42 81.07 51.51 73.79 84.48 74.55 76.13 51.26 71.61   LSAR ( r= 1)85.47 69.08 81.42 63.78 74.94 84.50 56.33 74.63 66.84 70.58   LSAR ( r= 2)85.37 74.53 81.60 61.88 75.84 84.50 57.75 72.80 66.86 70.48   LSAR 85.45 77.15 80.25 58.24 75.27 84.62 75.87 80.65 57.14 74.57   Layer 11 Layer 24   en de fr jp avg . em de fr jp avg .   Original 84.33 78.32 82.30 76.35 80.32 90.55 78.08 83.57 67.14 79.84   LIR ( k= 1)84.32 82.55 77.82 79.93 81.15 90.53 88.85 87.67 86.11 88.29   LIR ( k= 2)84.42 82.27 78.15 79.45 81.07 90.63 89.12 85.93 85.86 87.89   LIR ( k= 3)84.33 81.05 77.57 79.16 80.53 90.68 89.85 84.68 86.30 87.88   LSAR ( r= 1)84.32 78.80 82.12 80.66 81.47 90.55 83.47 77.67 80.86 83.14   LSAR ( r= 2)84.32 82.55 82.08 80.53 82.37 90.55 87.63 76.57 77.66 83.10   LSAR 84.27 82.60 77.85 80.28 81.25 90.57 89.37 88.03 86.01 88.505631en de fr jp avg .   Original 83.32 81.37 84.27 79.26 82.05   LIR ( k= 1)83.40 81.85 82.62 79.81 81.92   LIR ( k= 2)83.28 80.92 78.37 78.73 80.32   LIR ( k= 3)82.88 78.92 78.82 78.85 79.87   LSAR ( r= 1)83.07 81.52 83.88 79.20 81.92   LSAR ( r= 2)83.02 82.10 83.55 79.66 82.08   LSAR 83.13 81.92 83.18 79.48 81.9356325633