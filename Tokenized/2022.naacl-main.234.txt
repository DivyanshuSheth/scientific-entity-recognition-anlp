  Zeming Chen Qiyue Gao   Rose - Hulman Institute of Technology   { chenz16 , gaoq}@rose-hulman.edu   Abstract   In the age of large transformer language mod-   els , linguistic evaluation play an important role   in diagnosing models ’ abilities and limitations   on natural language understanding . However ,   current evaluation methods show some signif-   icant shortcomings . In particular , they do not   provide insight into how well a language model   captures distinct linguistic skills essential for   language understanding and reasoning . Thus   they fail to effectively map out the aspects of   language understanding that remain challeng-   ing to existing models , which makes it hard to   discover potential limitations in models and   datasets . In this paper , we introduce C- as a new format of NLI benchmark   for evaluation of broad - coverage linguistic phe-   nomena . C contains a collection   of datasets that covers 36 types of major linguis-   tic phenomena and an evaluation procedure for   diagnosing how well a language model captures   reasoning skills for distinct types of linguis-   tic phenomena . We show that this linguistic-   phenomena - driven benchmark can serve as an   effective tool for diagnosing model behavior   and verifying model learning quality . In addi-   tion , our experiments provide insight into the   limitation of existing benchmark datasets and   state - of - the - art models that may encourage fu-   ture research on re - designing datasets , model   architectures , and learning objectives ..   1 Introduction   With the rising power of pre - trained language mod-   els , large - scale benchmarks serve as an important   factor driving the future progress of NLP . These   benchmarks can provide a tool for analyzing the   strengths and weaknesses of pre - trained language   models . In recent years , many benchmarks ( Wang   et al . , 2019 , 2020 ; Rajpurkar et al . , 2018 ) have   been proposed that offer a diverse set of evaluation   objectives . However , recent criticisms have beenFigure 1 : We propose a broad - coverage diagnostic   benchmark for linguistic - phenomena - driven evaluation .   Our benchmark includes both a dataset collection and an   evaluation procedure for evaluating model performance   and diagnosing linguistic skills captured by a model .   We evaluate models ﬁne - tuned on large NLI datasets   through four types of diagnostic tests : zero - shot , inocu-   lation , hypothesis - only , and cross - distribution .   made that these benchmarks fail to serve as effec-   tive measures of progress in machine learning ( Raji   et al . ,2021 ) . In particular , the task design does not   formulate speciﬁc linguistic skills required for un-   derstanding . They lack the effectiveness in helping   researchers understand how certain systems or mod-   els work and how they fail . Although many state-   of - the - art language models have shown impressive   performance on these common benchmarks , their   performance degrades considerably on adversarial   or out - of - distribution samples ( Bras et al . , 2020 ) .   The performance drop shows that models may not   be learning the required linguistic skills for solving   the tasks of these benchmarks but exploit spurious   dataset biases ( Poliak et al . , 2018b ) . Overall , the   current benchmark format seems to be more like   a contest than a tool that can explain how well a   language model captures distinct linguistic skills   essential to language understanding and reasoning .   In this paper , we propose a new form of bench-   mark that serves as a diagnostic evaluation tool for   analyzing model linguistic skills . We present C- benchmark : a framework for diagnosing3204neural language models through broad - coverage   linguistic phenomena . Our benchmark includes ( 1 )   a large - scale collection of natural language infer-   ence ( NLI ) datasets covering 36 linguistic phenom-   ena and ( 2 ) an evaluation procedure for probing   and evaluating how well a language model captures   reasoning skills for distinct types of linguistic phe-   nomena . Targeted linguistic phenomena in C- range from fundamental properties like   named entity and coreference to complex ones like   commonsense and deductive reasoning . With the   C benchmark , we aim to investigate   the following research questions :   •Q1 : Do language models trained on benchmark   datasets have the ability to reason over a wide   range of linguistic phenomena ?   •Q2 : Are linguistic phenomena missing from   the training data recoverable through inoculation   ( i.e. , continuing to train models on a small sam-   ple of examples ) ( Liu et al . , 2019a ) ?   •Q3 : Do language models learn a general reason-   ing skill of a phenomenon through inoculation ?   To address the above questions , we empirically an-   alyze NLI models trained on popular benchmark   datasets through a pipeline of evaluations that in-   cludes : a zero - shot diagnostic test , inoculation re-   training , hypothesis - only sanity check , and cross   cross - distribution generalization tests .   ForQ1 , we observe that models trained on   benchmark datasets , including adversarial data , do   not have the reasoning ability for a large set of lin-   guistic phenomena . Our results show that training   on more datasets can help the model learn more   types of reasoning but does not help the model ac-   quire complex reasoning skills such as deductive   and commonsense reasoning . Our benchmark ex-   poses multiple knowledge gaps in large NLI mod-   els regarding diverse linguistic phenomena , particu-   larly in the categories of commonsense and compre-   hension . For Q2 , our analysis provides empirical   evidence that exposes the lack of recoverable lin-   guistic phenomena in benchmark datasets and mod-   els ’ inability to learn certain linguistic phenomena .   We also show that , on some phenomena , models   may rely heavily on spurious dataset bias existing   in the hypothesis to reach high accuracy . For Q3 ,   Our experiments show that models can adapt be-   tween distributions with different difﬁculties only   on 22.2 % of the phenomena such as Boolean , con-   ditional , and comparative logic . In the majority   ( 58.3 % ) of the phenomena , models fail to gen - eralize when the difﬁculties of the train and test   distributions are different , for example , relational   knowledge , puns , and contextual commonsense   reasoning . A model ’s learning performance may   not align with its generalization ability , suggesting   the lack of a general reasoning skill .   Overall , our proposed benchmark systematically   maps out a wide range of speciﬁc linguistic skills   required for language understanding and inference .   We envision linguistic - phenomena - based evalua-   tion to be an integral component of general linguis-   tic intelligence . We hope C can serve   as a useful evaluation tool that can map out which   aspects of the problem space remain challenging   for existing systems and models .   2 Related Work   NLU Benchmarks In recent years , multiple   large - scale benchmarks for evaluating models ’ gen-   eral language understanding performance have   been proposed . Similar to our benchmark ’s task   format , SNLI ( Bowman et al . , 2015 ) and MultiNLI   ( Williams et al . , 2018 ) are the two common   benchmarks for Natural Language Inference ( NLI ) .   GLUE and SuperGLUE are the two most popular   benchmarks that aim to provide a straightforward   comparison between task - agnostic transfer learn-   ing techniques . They cover various task formats ,   task domains , and training volumes , with datasets   all collected from publicly available sources . The   construction of our benchmark is similar in that   we also collect publicly available datasets from   peer - reviewed papers . Adversarial NLI ( ANLI ) is   a new benchmark collected " via an iterative , adver-   sarial human - and - model - in - the - loop procedure . "   ( Nie et al . , 2020 ) . ANLI is shown to be a more   difﬁcult challenge than previous benchmarks . Dif-   ferent from these benchmarks , our work aims to   map out and evaluate speciﬁc linguistic skills a   model needs for language understanding .   Fine - grained NLU Evaluation On top of large-   scale benchmarks , there are several works ( Joshi   et al . , 2020 ; Tarunesh et al . , 2021 ) contributing   to the ﬁne - grained analysis of model performance .   They collect data examples from existing bench-   marks by attaching taxonomic category labels to   each data . Or , they build semi - synthetic data allow-   ing analysis on 17 reasoning dimensions . Our data   collection and categorization concepts are similar   to them . However , our work covers more linguistic   phenomena that are difﬁcult but important such as3205   commonsense and non - monotonic reasoning .   Challenge Datasets for NLU Many challenge   datasets have been developed to evaluate models on   speciﬁc linguistic skills for understanding . These   datasets are in different formats such as NLI , Ques-   tion Answering ( QA ) , and Reading Comprehen-   sion ( RC ) . They target a large set of skills includ-   ing monotonicity ( Yanaka et al . , 2019a ) , deduc-   tive logic ( Liu et al . , 2020 ) , event semantics ( Han   et al . , 2021 ) , physical and social commonsense   ( Sap et al . , 2019 ; Bisk et al . , 2019 ) , defeasible rea-   soning ( Rudinger et al . , 2020 ) , and more . Our work   brings together a set of challenge datasets to build   a benchmark covering a large set of speciﬁc lin-   guistic skills . We also merge different evaluation   methods proposed by these works into a complete   evaluation pipeline for our benchmark .   Probing Linguistic Knowledge Several works   have found evidence that pre - trained models ’ repre-   sentations encode knowledge about linguistic phe-   nomena . Tenney et al . ( 2019 ) probe contextual rep-   resentations from four pre - trained language mod-   els through the edge - probing method across tasks   ranging from syntactic and semantic phenomena .   They ﬁnd that pre - trained models encode rich in - formation on syntactic phenomena but only weakly   encode information on semantic tasks compared to   non - contextual baselines . Chen and Gao ( 2021 ) ’s   linguistic - information - probing framework extends   the edge - probing study by focusing on different se-   mantic phenomena that are important for logical in-   ference in natural language . Their results show that   pre - trained contextual embeddings encode more   linguistic information on simple semantic phenom-   ena than complex phenomena . Our work is partly   motivated by this line of work in which our evalu-   ation is based on the fact that pre - trained models   can capture speciﬁc linguistic skills from learning .   Other work investigates if models use speciﬁc   linguistic skills to solve a downstream task . The   DNC benchmark ( Poliak et al . , 2018a ) provides a   collection of datasets for analyzing if models use   distinct linguistic phenomena to conduct natural   language inference . Several tasks in our bench-   mark come directly from this collection . However ,   our benchmark covers a wider range of linguis-   tic phenomena from more categories than DNC .   In particular , our benchmark contains semantic   phenomena and includes phenomena from funda-   mental linguistic properties to complex reasoning   types . In addition , our benchmark includes a sys-3206tematic evaluation methodology that allows a more   in - depth analysis of model behavior .   3 The C Benchmark   3.1 A New Form of Benchmark   Recently , Raji et al . ( 2021 ) suggested that good   benchmark construction should focus on mapping   out a speciﬁc set of linguistic skills required for   language understanding . They recommend a fu-   ture benchmark should provide interpretation on   how systems work and how they fail on particular   aspects of a problem space . Following this sug-   gestion , we propose a new form of benchmark :   linguistic - phenomena - driven evaluation . Our main   objective is to reformulate the benchmark not sim-   ply to be a scoreboard for SOTA model contest   but rather as a real measurement and standardiza-   tion tool for ( 1 ) analyzing model performance , ( 2 )   exposing model and dataset weakness and ( 3 ) pro-   viding insights for future research directions .   The curriculum benchmark aims to map out a   speciﬁc set of linguistic skills required for language   understanding . Our benchmark will serve as a diag-   nostic framework for linguistic - phenomena - driven   probing and evaluation . The targeted linguistic   skills should range from fundamental linguistic   properties to complex reasoning types . Our lin-   guistic phenomena selection is motivated by three   benchmarks : GLUE Diagnostic , Rainbow , and   DNC . In addition , we include many more phenom-   ena focusing on complex reasoning types such as   deductive logic and analytical thinking . Our ﬁ-   nalized benchmark covers eight categories of lin-   guistic phenomena . Each linguistic phenomenon is   considered one task , and one should train , evaluate ,   and analyze models on each phenomenon individ-   ually . We brieﬂy describe the types of reasoning   skill each category focus on in Table 1 . Appendix   AandBshows a list of references and dataset de-   tails for the train and test datasets used for each   linguistic phenomenon .   3.2 Dataset   We collect many challenge NLI or NLU datasets   and ﬁlter them individually with the following crite-   ria : ( 1 ) We focus on datasets that evaluate a speciﬁc   or a set of speciﬁc linguistic phenomena . ( 2 ) We   focus on English monolingual datasets that are in-   stitutional and publicly available . ( 3 ) We exclude   tasks that require domain - speciﬁc knowledge that   we would not expect a model to learn through pre - training , such as medical knowledge . We ﬁnalize   our selection with 36 datasets . Figure 1shows   a detailed ontology of our selected linguistic phe-   nomena and their abbreviations . Our motivation for   dataset selection is mainly based on the linguistic   phenomena categories that we aim to cover which   will range from a simple to complex setting .   3.3 Uniﬁed Task Format   We uniﬁed the task formats into a single linguistic   task , Natural Language Inference ( NLI ) . NLI is a   task for Natural Language Understanding . The task   requires a model to classify the logical relationship   between premise and a hypothesis . This logical   relationship can either be Entailment ( premise is   true implies the hypothesis is absolutely true ) , Con-   tradiction ( premise is true implies the hypothesis   is absolutely false ) , and Neutral ( one can not deter-   mine if the hypothesis is true or false based on the   premise ) ( Dagan et al . , 2013 ) . We select NLI as the   universal task format because NLI often serves as a   general evaluation method for models on different   downstream tasks . A model would need to han-   dle nearly the full complexity of natural language   understanding in order to solve the NLI task ( Po-   liak et al . , 2018b ) . Our benchmark contains two   types of NLI problems : ( 1 ) the 3 - way NLI with   Entailment , Contradiction , and Neutral ; ( 2 )   the 2 - way NLI with Entailed andNot - Entailed .   Each example has a premise and a hypothesis with   2 - way or 3 - way labels .   3.4 Automatic Recast   To convert non - NLI datasets into the NLI task for-   mat , we follow the dataset recast procedure ( Poliak   et al . , 2018b ): automatically convert from non-   NLI datasets with minimum human intervention .   We design algorithmic ways to generate sentence   pairs from the input text and convert the original   labels into the NLI labels . Question Answering   ( QA ) and Reading Comprehension ( RC ) are the   two major tasks we need to convert . To convert   datasets into NLI format , we follow the standard   procedure ( Khot et al . , 2018 ) . In QA datasets , if   choices are given as declarative statements , we con-   sider them as hypotheses and the question context   as the premise . If choices are given as phrases an-   swering the question , we concatenate the context   and question to form a premise and consider the   answers as hypotheses . Several datasets are tasks   with free - response problems , and an answer can   only be converted to an entailed hypothesis . To3207   generate non - entailed hypotheses , we use several   techniques during recasting . We show more details   on our conversion techniques in Appendix C. As   a sanity check on our resulting datasets , we em-   pirically ﬁnd low performance on standard partial-   input baselines ( Poliak et al . , 2018b ) , suggesting   that our conversion yields data of high quality .   3.5 Dataset Difﬁculty   To enhance our benchmark to provide more infor-   mation on each dataset for in - depth evaluation and   analysis , we provide each phenomenon a difﬁculty   level . We use the predictive V - information ( Etha-   yarajh et al . , 2021 ) as a measurement for dataset   difﬁculty . The V - information can measure how   much information an input variable X can provide   about Y when constrained to functions V. Intu-   itively , more usable infromation X can provide , the   easier a dataset is for the functions V. Formally ,   let∅denote a null input that provides no informa-   tion about Y and Vas a predictive family , we can   compute the V - information I(X→Y)as follows :   H(Y ) = infE[−logf[∅](Y ) ]   H(Y|X ) = infE[−logf[X](Y ) ]   I(X→Y ) = H(Y)−H(Y|X )   where X , Ydenote random variables with sam-   ple spaces X , Y. According to Ethayarajh et al .   ( 2021 ) , ∅can be an empty string here as f[∅ ]   models the label entropy . This framework can nat-   urally adapt to the calculation of the point - wise   V - information ( PVI ) where we measure the difﬁ-   culty of each data example . Given a training dataset   D = { ( x , y ) } , and the predictive family   V , the PVI of a data instance ( x , y)∈ D is   computed as :   PVI ( x→y ) = −logf[∅](y ) + logf[x](y ) ,   where ∅is an empty string ( null input ) and   { f , f } ⊆ V .fandfare models ﬁne - tuned   fromD and{(∅ , y)|(x , y)∈ D}re-   spectively . The V - information framework can also   serve as a difﬁculty measurement for datasets and   can be computed explicitly by averaging over PVI :   I(X→Y ) = 1   n / summationdisplayPVI ( x→y )   As Table 2shows , the difﬁculty level ranges from   negative to positive . The higher the V - information   is , the easier a dataset is for the model .   Dataset Controlled Split For our model evalua-   tion pipeline , we are interested in verifying model ’s   ability to learn a generalizable reasoning skill on   linguistic phenomena . In particular , we want to   check if a model can generalize when its training   and testing data distributions have different mea-   surement of difﬁculty . Thus , we need to conduct   controlled split on datasets based on the point - wise   difﬁculty , i.e. the point - wise V - information of their   data examples . We ﬁrst calculate the PVI ( x→y )   for each phenomenon dataset , then we split each   dataset into two portions : simple and hard , based   on the calculation of each example ’s PVI .   4 Evaluation Methodology   We deﬁne an evaluation process for the C - benchmark that aims to bring different types   of evaluation and diagnosing methods used by pre-   vious challenge NLI datasets . Following Raji et al.3208(2021 ) ’s suggestion , we want our evaluation pro-   cess to both to analyze the model output in detail   and explore which aspects of the inference problem   space remain challenging to current models .   Zero - shot Diagnostic Test This test is motivated   by the diagnostic test in GLUE . We focus on provid-   ing ﬁne - grained analysis of zero - shot system per-   formance on a broad range of linguistic phenomena .   We follow the GLUE diagnostic dataset and use the   Matthews Correlation Coefﬁcient ( MCC ) ( Jurman   et al . , 2012 ) as the evaluation metric . MCC com-   putes the correlation coefﬁcient of the predicted   labels and the true labels . The correlation coefﬁ-   cient value is between -1 and +1 . A coefﬁcient of   +1 indicates a perfect prediction . A 0 indicates aver-   age random prediction A -1 indicates the classiﬁer   always miss - classiﬁes . MCC is perfectly symmet-   ric , so it can be used even if the dataset has classes   with different sizes .   Inoculation by Fine - tuning We use inoculation   ( Liu et al . , 2019a ) to further analyze model fail-   ures on target linguistic phenomena . This method   ﬁne - tunes the model on a down - sampled training   section of a phenomenon dataset ( inoculation ) . One   can interpret inoculation performance in two ways :   1.Good performance : the original training set of   the model , prior to inoculation , did not sufﬁ-   ciently cover the target phenomenon , but it is   recoverable through through additional training   on a small sample of data .   2.Poor performance : there exists a model weak-   ness to handle the target phenomenon .   Hypothesis - only Bias Analysis We conduct   analysis on hypothesis - only bias as ( 1 ) a sanity   check for our converted datasets and also and ( 2 )   a veriﬁcation on whether model ’s good perfor-   mance is from leveraging artifacts in the hypothe-   ses . We train a hypothesis - only baseline ( Poliak   et al . ,2018b ) for each phenomenon and compare   their performance against the best models from the   inoculation experiment . We want to ensure that   models ’ improved performance after inoculation is   due to their ability to reason about a hypothesis and   the given context together . If the hypothesis - only   baseline shows good performance , we interpret this   as a sign that the datasets contain artifact . If the   baseline shows poor performance , it gives evidence   that the model is not taking short - cuts . Cross - Distribution Generalization We conduct   the cross - distribution generalization test ( Rozen   et al . ,2019 ) to verify if the model learns a general   reasoning skill from inoculation . The good inocula-   tion performance does not ensure that the model ’s   learned skill is generalizable . The model can likely   over-ﬁt the dataset distribution by adopting superﬁ-   cial cues . We evaluate the model ’s generalization   ability by training and testing the model on distri-   butions yielding different difﬁculty levels within   the same dataset . For example , we train the model   on the simple part of the dataset ( data with high   V - information ) and test it on the hard part ( data   with low V - information ) .   4.1 Experiment Setup   For the zero - shot test , we test a model on each   test set without additional ﬁne - tuning . We select   NLI models with top performance on NLI bench-   marks MNLI and ANLI . We list these models in   Table 3 . We are interested in evaluating models   with both the single - encoder and the text2text ar-   chitecture . All models are publicly available from   Huggingface ( Wolf et al . , 2019 ) . For inoculation ,   we ﬁne - tune models on training examples with a   size ranging from 10 to 1000 examples per label .   For the cross - distribution generalization test , we   ﬁrst create variant data distributions for train and   test sets using the V - information - based dataset split   method from Section 3.5 . We split each dataset   into two portions ( simple and hard ) according to   the point - wise Vinformation . Next , we either train   and test the model on the same difﬁculty distribu-   tion or train it on one portion and test it on a dif-   ferent portion . In the inoculation , hypothesis - only ,   and generalization experiments , we all use roberta-   anli - mix as our NLI model because its training set   covers all the major NLI training datasets : SNLI ,   MNLI , FEVER ( Thorne et al . , 2018 ) , and ANLI .   We use accuracy as our evaluation metric for all   these three experiments . For all the experiments   excluding zero - shot test , we run several turns and   select the best performance for analysis .   5 Empirical Analysis   5.1 Zero - shot Linguistic Phenomena Diagnose   First , we report the results on zero - shot diagnos-   tic evaluation for each baseline model . From Fig-   ure2a , we observe that both single - encoder and   text2text models trained on MultiNLI show a neg-   ative correlation in the majority of linguistic phe-3209   nomena . Meanwhile , anli - mix models ( roberta-   anli - mix , xlnet - anli - mix ) are positively correlated   on most ( 77.8 % ) of the phenomena and they show   high correlation ( > 0.50 ) on 27.8 % of the phe-   nomena . On average , models trained on the large   dataset mixture show better performance than mod-   els trained on MultiNLI alone , suggesting that train-   ing on more datasets help models capture more   types of linguistic phenomena . However , most of   the phenomena captured by the anli - mix models are   easier to learn ( higher Vinformation ) . On harder   phenomena , models did not beneﬁt from the train-   ing dataset mixture . For instance , both the anli - mix   models have a low correlation on deductive and   analytical reasoning . Overall , we ﬁnd that NLIdatasets from common benchmarks lack examples   of a diverse set of reasoning skills .   5.2 Inoculation   Based on Figure 2b , the model can reach high ac-   curacy on about 64 % of the phenomena as the   training examples accumulate . Most of these phe-   nomena have higher Vinformation ( > 0.0 ) that   should relatively be easier to learn . We are sur-   prised that for some hard phenomena ( ≤0.0 ) such   as commonsense contextual reasoning ( cosmo , -   0.67 ) , the model ’s performance improved after in-   oculation . The improvement shows an gap in the   original training data mixture . On 25 % of the phe-   nomena , the model ’s performance did not improve3210   signiﬁcantly after inoculation , meaning that it fails   to learn the reasoning skills for these phenomena .   Most of these phenomena are difﬁcult , with a low   Vinformation , such as monotonicity(mono ) and   deductive ( logi ) reasoning . The accuracy is consis-   tently low when training examples accumulate .   We also observe that model struggles to learn   phenomena that require complex reasoning , such   as phenomena from the comprehension category .   This trends show inherent weaknesses in the model   or its training strategy that cause its failure to learn   complex and hard phenomena . Overall , results   from this experiment , combined with the zero - shot   evaluation , suggest that many linguistic phenomena   are missing from different large - scale NLI datasets   but are recoverable through additional training ex-   amples . However , the model fails to learn the skills   for hard and complex phenomena . In summary ,   our diagnostic study through inoculation exposes a   diverse set of dataset and model weaknesses .   5.3 Hypothesis - only Bias   To determine if models can leverage spurious arti-   facts in the hypotheses of each phenomenon , we   compare full models to hypothesis - only baselines .   From Figure 2b , we observe that hypothesis - only   baseline performs poorly on a majority of the phe-   nomena . This indicates that our benchmark gener-   ally requires the model to learn an inference pro-   cess between contexts and hypotheses for good   performance . We observe that on 30.6 % of the phe-   nomena , the full - model can reach a high accuracywhile the baseline has low accuracy , suggesting the   model can learn the phenomenon without relying   on hypothesis artifacts . On 36 % of the phenomena ,   the model does not show a signiﬁcant performance   gain compared to the baseline . Most of these are   complex reasoning phenomena like deductive and   analytical reasoning . The result validates that the   model struggles more with complex linguistic phe-   nomena . On 33.3 % of the phenomena , both the   full - model and the baseline achieve high accuracy   showing the possibility that the model exploits arti-   facts from the hypothesis to reach high accuracy .   Also , note that the hypothesis - only baseline per-   forms better for some tasks than the ﬁne - tuned   model , which can be interpreted in two ways .   When both the baseline and ﬁne - tuned model   achieve high accuracy ( vbc , syn - alt ) , higher accu-   racy on baseline indicates that the hypothesis - only   bias is pretty strong in the dataset . When the inter-   vention from the premise is removed ( hypothesis-   only input ) , the models can easily exploit the bias   to achieve higher accuracy . In contrast , when both   the baseline and ﬁne - tuned model achieve low ac-   curacy ( hypo , analytic , social , ester ) , higher ac-   curacy on baseline indicates that the task is very   difﬁcult for a model to master successfully . Low   baseline accuracy means that the dataset does not   contain much bias , so a model must learn the cor-   rect reasoning to perform well . However , the ﬁne-   tuned model has even worse performance than the   baseline , meaning that it fails to learn the skill re-   quired for these tasks . Our main ﬁnding here is3211that good performance on a linguistic phenomenon   dataset does not mean the model captured the asso-   ciated phenomena . The model can learn short - cuts   through hypothesis - only bias and artifacts .   5.4 Generalization   As Figure 3show , the model can adapt between   different distributions only on 22.2 % of the phe-   nomena . The model achieves high accuracy consis-   tently for all four categories in the generalization   matrix suggesting the learned skills are general-   izable . On 58.3 % phenomena , models can not   generalize between different difﬁculty distributions .   They show higher accuracy when trained and tested   on the same distribution but low accuracy when the   test distribution shifted . For example , on relational   knowledge reasoning ( kg - rel ) , the model achieves   83 % for simple →simple and 98 % for hard →   hard . Nevertheless , the performance drops to 53 %   for hard →simple and 38 % for simple →hard .   Notice that model ’s good performance on inoc-   ulation does not align with its generalization abil-   ity . For example , the model reaches 90.9 % accu-   racy on kg - rel , but its generalization performance is   poor . This behavior highlights a model weakness :   can over-ﬁt to a particular distribution but fail to   learn a general reasoning skill for the target phe-   nomenon . We observe an interesting behavior that   models struggle to generalize from hard to simple   distribution on about 14 % of the phenomena while   showing good generalization from simple to hard   distribution . We think the possible reason is that   the hard distribution contains data with relatively   lowVinformation . A low amount of usable in-   formation makes it hard for the model to learn the   phenomena sufﬁciently for generalization .   6 Conclusion and Future Work   In this paper , we introduce a new form of bench-   mark that can serve as an effective tool for evaluat-   ing and analyzing model outcomes . We propose a   linguistic - phenomena - driven benchmark that aims   to diagnose neural language models to discover   types of linguistic skills that remain challenging   to models . We compiled a dataset collection cov-   ering 36 types of linguistic phenomena ranging   from fundamental linguistic properties to complex   reasoning skills . In addition , we deﬁne an evalu-   ation procedure that can provide an in - depth anal-   ysis of model and dataset weaknesses . Using our   benchmark , we comprehensively study how welllanguage models capture speciﬁc linguistic skills   essential for understanding . Our major ﬁndings   include :   •Models trained on benchmark NLI datasets fail to   reason over a diverse set of linguistic phenomena .   •Good inoculation performance on some phenom-   ena results from the model leveraging superﬁcial   artifacts in the hypothesis .   •The model tends to over-ﬁt the dataset distribu-   tion without learning a general reasoning skill on   a majority of phenomena .   Overall , our benchmark effectively evaluates a   model on speciﬁc linguistic skills and exposes a   list of model and training data weaknesses . We   hope that our benchmark and empirical ﬁndings   can encourage the communicate to rethink dataset   construction and model architecture design . In   particular , we hope to encourage the the develop-   ment of new datasets that cover richer types of   linguistic phenomena and language models that   can learn essential linguistic skills that are gener-   alizable . For future work , we plan to add more   datasets to cover more phenomena such as psycho-   linguistics ( Laverghetta Jr. et al . , 2021 ) . We envi-   sion our benchmark to be dynamic , meaning that   a higher - quality and more difﬁcult dataset for a   phenomenon should replace the current ones in the   future . For example , the StepGame benchmark ( Shi   et al . ,2022 ) provides better data for spatial reason-   ing , which can replace the current spatial reason-   ing dataset . We also plan to explore new learning   methods to help models overcome the weakness of   learning non - generalizable skills , such as calibra-   tion through symbolic loss functions .   Acknowledgment   We thank the anonymous reviewers for their   thoughtful and constructive comments . We thank   Kyle Richardson from AI2 for his insights and sug-   gestions on improving our camera - ready version .   Thanks also to our advisors Laurence S. Moss and   Michael Wollowski for their feedback on earlier   drafts of this work . Special thanks to the Machine   Learning for Language Group at NYU for their   wonderful NLP toolkit , JIANT ( Phang et al . , 2020 ) .   References3212321332143215A Linguistic Phenomena in C3216B C Dataset Details in C3217C Data Recasting Details   Here we provide more details on the major techniques we used to convert Question Answering ( QA ) and   Reading Comprehension ( RC ) datasets into recast NLI datasets .   C.1 Entity Swapping   C.2 Question / Answer Concatenation   D Reproducibility   Implementation . Our model training and testing pipeline is modiﬁed from the JIANT toolkit . We mainly   adapted several components on classes and functions involving task , dataset , reprocessing , tokenization ,   model version control , and evaluation metrics . All our experiments are implemented with models publicly   available from Huggingface Transformers ( Wolf et al . , 2020 ) .   Hyper - parameters We mainly follow the practice in ( Nie et al . , 2020 ) . For all the experiments excluding   the zero - shot test in Section 5.1 , we use a learning rate of 1e−5with a batch size of 8 . We set the number   of warmup updates to be 1000 . We set the epoch number to be 3 and 5 . We evaluate the model on D   every 200 steps for the inoculation and generalization experiments , and 500 steps for the hypothesis - only   experiment . For the low - data generalization on ANLI , we evaluate on the full - test set according to the   number of training examples listed in Figure ? ? . We use the AdamW ( Loshchilov and Hutter , 2019 ) as   our optimizer.3218Infrastructure All experiments are done with one single Geforce RTX 3090 ( 24 GB ) . A single inocu-   lation or generalization job ﬁnishes within 0.5 hours on average . A single hypothesis - only job ﬁnishes   within 1 - 2 hours on average . A single job on sequential training and low - data ﬁne - tuning ﬁnishes within   approximately 1.5 hours on average .   Number of Parameters . RoBERTa - large model contains 355 million parameters . BART - large model   contains 139 million parameters . BART - Large model contains 406 million parameters . XLNet - large   model contains 340 million parameters.3219