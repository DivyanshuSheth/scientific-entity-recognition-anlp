  Zhekun LuoShalini GhoshDevin Guillory   Keizo KatoTrevor DarrellHuijuan Xu   Abstract   Action in video usually involves the interac-   tion of human with objects . Action labels are   typically composed of various combinations of   verbs and nouns , but we may not have training   data for all possible combinations . In this paper ,   we aim to improve the generalization ability   of the compositional action recognition model   to novel verbs or novel nouns that are unseen   during training time , by leveraging the power   of knowledge graphs . Previous work utilizes   verb - noun compositional action nodes in the   knowledge graph , making it inefficient to scale   since the number of compositional action nodes   grows quadratically with respect to the number   of verbs and nouns . To address this issue , we   propose our approach : Disentangled Action   Recognition with Knowledge - bases ( DARK ) ,   which leverages the inherent compositionality   of actions . DARK trains a factorized model by   first extracting disentangled feature representa-   tions for verbs and nouns , and then predicting   classification weights using relations in exter-   nal knowledge graphs . The type constraint be-   tween verb and noun is extracted from external   knowledge bases and finally applied when com-   posing actions . DARK has better scalability in   the number of objects and verbs , and achieves   state - of - the - art performance on the Charades   dataset . We further propose a new benchmark   split based on the Epic - kitchen dataset which   is an order of magnitude bigger in the numbers   of classes and samples , and benchmark various   models on this benchmark .   1 Introduction   Understanding human - object interaction is crucial   for modeling human behavior , and plays a key role   in developing robotic agents that interact with hu-   mans . In videos , many of these interactions can   be described using the combination of verbs andFigure 1 : DARK extracts disentangled features of   verbs / nouns and leverages knowledge graphs ( KG ) to   generate classifiers for unseen verbs and nouns . Pre-   dictions are then composed under constraints of object   affordance priors ( Affd ) from knowledge bases ( e.g.   banana can be cut ) .   nouns , e.g , move chair , peel apple . Recently re-   searchers have been focusing on the task of compo-   sitional action recognition with the goal of recog-   nizing actions represented by such verb - noun pairs .   The key challenge comes from the extremely large   label space of various combinations . The number   of possible verb - noun pairs grows quadratically   with respect to the number of verbs and nouns . It   is infeasible to collect training data for all possible   actions . This motivates us to study the problem of   zero - shot compositional action recognition , which   aims to predict action with components beyond the   vocabularies in train data .   To conduct zero - shot learning , we propose our   Disentangled Action Recognition with Knowledge-   bases ( DARK ) , which leverages knowledge graphs .   A knowledge graph ( KG ) encodes semantic rela-   tionships between verb or noun nodes . We apply   graph convolutional network ( GCN ) on KG to pre-   dict classifier weights for unseen nodes in graphs.573Previous work ( Kato et al . , 2018 ) has explored   using knowledge graphs for zero - shot composi-   tional action learning . However , their model builds   a graph containing verb , noun and compositional   action nodes ( which link verbs and nouns ) . It learns   features of novel action nodes by propagating in-   formation from connected verb / noun nodes . The   number of compositional action nodes during train-   ing is in the order of O(n ) , and the memory con-   sumption may become prohibitively expensive as   we scale this approach to large vocabularies . To   overcome this issue , we propose to learn separate   classifiers for verbs and nouns , which scales lin-   early with respect to the vocabulary size .   Specifically , DARK extracts verb and noun fea-   tures separately , and relies on separate verb and   noun knowledge graphs to predict unseen concepts   before composing the action label . Activity recog-   nition is particularly well - suited for such a factor-   ized approach , because nouns may be better cap-   tured using object detection - based approaches and   verbs may be represented by motion . Compared to   prior work ( Kato et al . , 2018 ; Zhukov et al . , 2019 )   that use the same feature representation for both   verb and noun , our separate features model noun   and verb more precisely . In addition , we adopt dis-   entanglement between the learned verb and noun   features , so they compose more readily and im-   prove generalization on unseen actions .   Though scalability is achieved using our fac-   torized approach , verbs and nouns are actually   not fully independent . For instance , the process   ofsanding an object and scrubbing an object   are visually similar , however you are more likely   to be scrubbing acarthansanding it . Prior   work ( Kato et al . , 2018 ) models the verb - noun rela-   tionships by constructing quadratic compositional   action nodes . In our model , when composing ac-   tion labels , we take object affordances ( Gibson ,   1978 ) , namely the commonsense relationship of   verbs and nouns , into consideration . We extract   affordance knowledge from a caption corpus and   build a scoring component to consider the relation-   ship between verbs and nouns , to further improve   the generalization ability . The basic idea of our   proposed model is illustrated in Figure 1 .   Furthermore , we investigate the evaluation of   zero - shot compositional action recognition task and   identify the drawback of existing metrics . With N   verbs , Nnouns , it constructs a N×Nlabel   space for possible actions . Among these actions , some are invalid ( e.g.peel a car ) and some are   valid but not presented in the dataset . In real-   world applications , the model would need to make   predictions in the whole N×Nlabel space . But   current evaluation protocols , implicitly or explic-   itly , only evaluate on compositional classes that are   valid and presented in the dataset , which does   not reflect the real difficulty of this task . We pro-   pose a new setting , where predictions are made and   evaluated in the full N×Nlabel space .   The Charades ( Sigurdsson et al . , 2016 ) dataset is   relatively small scale for testing zero - shot compo-   sitional action recognition ( Kato et al . , 2018 ) . To   promote further research , we propose a new bench-   mark based on the Epic - kitchen ( Damen et al . ,   2018 , 2020 ) dataset , which is an order of mag-   nitude bigger both in number of classes and sample   size . The key contributions of our paper are :   1.We propose a novel factorized model that   learns disentangled representation separately   for verbs and nouns , facilitating scalability .   2.We further improve the model ’s generalization   performance by learning the interaction con-   straints between verbs and nouns ( affordance   priors ) from an external corpus .   3.We propose a new evaluation protocol for   zero - shot compositional learnings , which bet-   ter reflects the real - world application setting .   4.We propose a new large - scale benchmark   based on the Epic - Kitchen dataset and achieve   state - of - the - art results .   2 Related Work   Zero - shot learning with knowledge graphs :   Zero - shot learning has been widely studied in com-   puter vision ( Akata et al . , 2015 ; Lampert et al . ,   2013 ; Lee et al . , 2018 ; Sahu et al . , 2020 ; Wang   et al . , 2019 ; Xian et al . , 2018 ) . We will focus on   related work relevant to our approach . ( Wang et al . ,   2018 ) proposes to distill both the implicit knowl-   edge representations ( e.g. , word embedding ) and   explicit relationships ( e.g. , knowledge graph ) to   learn a visual classifier for new classes through   GCN ( Kipf and Welling , 2016 ) . ( Kampffmeyer   et al . , 2019 ) later proposes to augment the knowl-   edge graph ( KG ) with dense connections which   directly connects multi - hop relationship and distin-   guishes between parent and children nodes . The   graph learning of our model mostly follows their574work . Recently there have been improvements on   GCN models . ( Nayak and Bach , 2020 ) designs   a novel transformer GCN to learn representations   based on common - sense KGs . ( Geng et al . , 2020b )   uses an attentive GCN together with an explanation   generator to conduct explainable zero - shot learning .   Instead of generating classifier for unseen classes   directly , ( Geng et al . , 2020a ) uses a generative ad-   versarial network to synthesize features for unseen   classes to conduct classification . These directions   could be potentially explored in our problem set-   ting to further improve performance . ( Gao et al . ,   2019 ) conducts zero - shot action recognition based   on KGs , but unlike our problem setting , their verb-   noun relationship is not compositional and objects   are used as attributes to infer action .   Compositional action recognition : Many prior   works aim to understand actions through interac-   tion with objects . ( Wang et al . , 2020 ; Xu et al . ,   2019 ) tackle zero - shot human - object interaction   in images . ( Zhukov et al . , 2019 ) conduct weakly-   supervised action recognition , leveraging composi-   tionality of verb - noun pairs to decompose tasks into   a set of verb / noun classifiers . This shares certain   similarities with our factorized model , but it is not   a zero - shot setting , nor does it enforce feature dis-   entanglement . ( Materzynska et al . , 2020 ) conduct   zero - shot compositional action recognition , where   individual verb / noun concepts have been seen dur-   ing training but not in the same interaction with   each other . Although it can not deal with unseen   verbs or nouns , using object detector to explicitly   model object features inspires our approach . One   of the closest works to our proposed approach is   ( Kato et al . , 2018 ) . It constructs a KG that contains   verb nodes , noun nodes and compositional action   nodes , and learns the feature representation for   each action node to match visual features . Novel   actions ’ features are inferred jointly during training   through GCN . The number of action nodes grows   quadratically with respect to the number of nouns   or verbs , which makes this approach difficult to   scale , especially considering that GCN ’s forward   pass needs to learn all features simultaneously .   3 Method   We propose DARK – Disentangled Action Recog-   nition with Knowledge - bases ( Figure 2 ) . It extracts   disentangled feature representations for verbs and   nouns , then predicts classifier weights for unseen   components using knowledge graphs , and com - poses them under object affordance priors .   3.1 Factorized verb - noun classifier   Given a video X , we first use a verb feature extrac-   torFto extract verb feature , and a noun feature   extractor Ffor noun feature . Subsequently , we   learn one - layer predictors W andW for   predicting the final verb / noun class . F , Fand   W , W are trained via cross entropy ( CE )   lossesLandLwith verb / noun labels y , y.   L = CE(F;W;y ) ( 1 )   L = CE(F;W;y ) ( 2 )   We extract disentangled features for verbs and   nouns , so that verbs and nouns can be treated as   separate entities . If verb features contain much   information about nouns , it would overfit to seen   actions and would not generalize to unseen compo-   sitions . Standard networks like Inception3D ( I3D )   ( Carreira and Zisserman , 2017 ) can rely on scene   or object information to predict verbs ( Battaglia   et al . , 2018 ; Materzynska et al . , 2020 ) . To decou-   ple verb ’s representation from noun ’s , we add ex-   plicit regularization to the model input . We first   used an off - the - shelf class - agnostic object detector   to detect the bounding box of interacting objects .   Then , we crop the object from videos and use I3D   backbone to extract verb features from the cropped   videos . ( Yun et al . , 2019 ; DeVries and Taylor ,   2017 ; Choi et al . , 2019 ; Singh and Lee , 2017 ) use   similar cropping technique to remove bias in other   tasks . We also detect hand masks and add hand   regions separately to the verb input because the   class - agnostic detector tends to crop out the hands   as well . Adding hand gesture information back   provides hints for verbs . Disentanglement method   on Charades dataset ( Sigurdsson et al . , 2016 ) is   different as it contains third - person view videos ,   and relevant details are discussed in Section 4.5 .   3.2 GCN for learning novel concept   After training on seen concepts , we can infer the   classifier for unseen ones . In this subsection , we   drop the subscript v / n as the same process applies   for both verb and noun . After learning feature   extractor Fand classifier of seen concept W ,   learning classifier for unseen concepts is equivalent   to learning the weight W. This step lever-   ages graph convolution network ( GCN ) following   previous work ( Kampffmeyer et al . , 2019 ; Kato575   et al . , 2018 ; Xian et al . , 2017 ) . It takes the word   embedding of unseen / seen concepts s , s ,   and conducts graph convolution on the KG , with   previously learned classifier weight Was su-   pervision . In each layer , it calculates :   Z=ˆAZW ( 3 )   ZandZare input and output of the layer i ,   ˆAis the adjacency matrix of the graph . Following   ( Kampffmeyer et al . , 2019 ; Kato et al . , 2018 ; Wang   et al . , 2018 ) , we normalize the adjacency matrix .   Wis a learnable parameter . GCN first transforms   features linearly , then aggregates information be-   tween nodes via graph edges . The 0layer ’s   input Zis the word embedding [ s , s ] .   The last layer ’s output Zis the classifier weight   [ ˆW , ˆW ] . We use the Wlearned   previously as supervision , and calculate the mean   square error loss between Wand ˆW. The   training process is illustrated in Figure 2 . Only the   GCN learning of nouns is shown for brevity .   L = L(ˆW , W ) ( 4 )   3.3 Incorporating affordance prior   Not all verb - noun pairs are equally important —   some objects can only admit certain actions but   not others . ( Gibson , 1978 ) proposed the notion   of “ affordance " — the shape of an object may pro-   vide hints on how humans should use it , whichinduces the set of suitable actions . Affordance can   be extracted from the language source , e.g. we will   often say peel the apple but rarely peel the chair .   Prior works ( Zhuang et al . , 2017 ; Lu et al . , 2016 )   used language information as prior to improve their   performance . In this paper , we use captions of   HowTO100 M dataset ( Miech et al . , 2019 ) which   records human - object interaction . We run the   Standford NLP parser ( Chen and Manning , 2014 )   to extract nouns / verbs from captions automatically .   After extracting verb - noun pairs , we train a scor-   ing function Ato calculate the verb - noun affor-   dance matching score . We project verb embed-   dingsto the noun embedding space and calculate   cosine distance with s , followed by sigmoid to   output a scalar value indicating whether this verb-   noun compositional action is plausible . For train-   ing , we generate positive / negative pairs and use   binary cross - entropy loss L. Note that there   underlies an open - world assumption ( Nickel et al . ,   2015 ): the verb - noun pairs missing are not entirely   infeasible , but could be unobserved . Further re-   search can be explored to develop a more precise   way of modeling the affordance constraint .   A scoring function based on only word-   embedding is similar to a static look - up table for   verb - noun pairs , and may fail to encode diverse   action visual features . Thus we train a mapping   function Mto transform verb ’s visual input to its   word embedding susing mean square error loss.576   Algorithm 1 Training process of our DARK model   1 . Train the feature extractor and corresponding   classification weight ( F , W ) , ( F , W ) via   classification loss L , Lseparately .   2 . Use the word embeddings s , sas input   and the learned classification weight W , Was   supervision , to train the GCN model ( G , G ) with   mean square error ( MSE ) loss ( Equation 4 ) .   3 . Use extracted affordance pairs to train scoring   function A(s , s)(Figure 3 ) , via binary cross   entropy ( BCE ) loss L.   4 . Train mapping function Mto map visual verb   inputs to semantic embedding space ( Equation 5 ) .   L(M(X ) , s ) ( 5 )   The separation of A , Malso adds interpretabil-   ity and allows learning from different data . Acan   be trained on a language corpus without video data .   Also , Adeals with textual affordance relationship   directly and adds interpretability . In test time we   map verb ’s visual input to verb embedding space   and calculate affordance score with target noun ’s   embedding s(Figure 3 ) . The model is asymmet-   ric , since we use object proposals with false detec-   tion and verb visual input is more reliable .   3.4 Overall algorithm and inference   The training of our DARK model is shown in Al-   gorithm 1 . During inference , we calculate the prob-   ability of a video containing the compositional ac-   tion ( v , n ) using following equations :   P(v , n ) = P(v)∗ P(n)∗ A(M(X ) , s)(6 )   P(v ) = σ(W∗ F(X ) ) ( 7 )   σis sigmoid function . For the classification   weights Wused in verb prediction P(v ) , we use   the learned classification weight W for seen   classes , and ˆW predicted by GCN for un-   seen classes . Similar equation applies for noun   prediction P(n ) , which is omitted .   4 Experiments   In this section , we discuss experiment evaluation ,   setup and results . Some implementation details are   in appendix .   4.1 Evaluation of zero - shot compositional task   Following previous work ( Kato et al . , 2018 ) ,   we partition verbs into two disjoint sets for   seen / unseen classes , V / V , and same for nouns ,   N / N. Thus , “ seen compositional actions " corre-   spond to VN , while “ unseen ( zero - shot ) actions "   correspond to VN , VNandVN .   Prior to ( Chao et al . , 2016 ) , most works   ( e.g. ( Norouzi et al . , 2013 ) ) on zero - shot learning   adopt an evaluation protocol where predictions are   made and evaluated only among unseen classes.577This is later denoted as close world setting . ( Chao   et al . , 2016 ) points out that it does not reflect the   difficulty of recognition in practice , and there ex-   ists a trade - off between classification accuracies of   seen / unseen classes . They propose generalized   zero - shot learning ( GZL ) setting — test set con-   tains samples of both seen / unseen classes . Predic-   tions are made and evaluated on both categories . By   adding different biases to unseen classes ’ predic-   tion , one can draw a curve depicting the trade - off   between accuracies of seen / unseen samples . They   use area under the curve ( AUC ) to better reflect   model ’s overall performance on both seen and un-   seen classes . Our evaluation follows this setup .   Currently , relatively few prior works tackle zero-   shot compositional action recognition ( Kato et al . ,   2018 ; Materzynska et al . , 2020 ) . Taking other zero-   shot compositional learning tasks such as zero - shot   attribute - object classification ( Misra et al . , 2017 ;   Nagarajan and Grauman , 2018 ; Purushwalkam   et al . , 2019 ; Yang et al . , 2020 ) and image - based   zero - shot human - object interaction ( Wang et al . ,   2020 ; Xu et al . , 2019 ) into consideration , we find   that zero - shot compositional learning task poses ex-   tra challenges due to its combinatorial label space :   not all compositional labels are valid , and for the   valid ones , there may be no samples in the dataset .   For brevity , we continue to use the term “ verb " and   “ noun " , but the following discussion could be also   applied to other zero - shot compositional learning   tasks ( e.g. , attribute - object ) . As in Figure 4 , with   Nverbs , Nnouns , we can construct a N×N   action label space . Among these actions , some are   invalid , because the verb - noun pair contradicts   our common sense ( e.g. peel a car ) . And some are   valid but not presented in the dataset . Most pre-   vious works only consider labels that contain sam-   ples in the dataset , namely compositional classes   that are valid and presented in the dataset .   Prior works ( Materzynska et al . , 2020 ; Misra   et al . , 2017 ; Nagarajan and Grauman , 2018 ; Yang   et al . , 2020 ) use disjoint label spaces in training   and test sets , which corresponds to the close world   setting . ( Purushwalkam et al . , 2019 ) ’s test set con-   tains both seen and unseen classes ( GZL setting )   and uses the AUC metric like ours , but their predic-   tion is made and evaluated only among composi-   tional classes with samples in dataset . ( Kato et al . ,   2018 ; Wang et al . , 2020 ) also follow GZL setting ,   but they use mean average precision ( mAP ) over   compositional classes presented in dataset , whichimplicitly only considers valid and presented   classes . We denote this as open world setting :   test set contains samples from seen / unseen classes ,   but prediction is made and/or evaluated among   valid and presented classes .   Neither close world setting nor open word set-   ting reflects the difficulty of zero - shot composi-   tional action recognition task . When deploying   recognition models in the real world , it would need   to make predictions in the whole N×Nlabel   space . Thus , compositional constraints between   verbs and nouns ( affordance ) should be properly   modeled to exclude invalid classes . In addition ,   the evaluation protocol should not distinguish be-   tween classes that are valid but not presented   andvalid and presented in the dataset , because   models would not have access to that information   beforehand . We propose the marco open world   setting . In test time , sample can be from all   seen / unseen classes , including VN , VN , VN   andVN , and model receives no information   about where the sample comes from . Predictions   are made and evaluated in the whole N×Nlabel   space , and the AUC metric ( Chao et al . , 2016 ) con-   sidering the trade - off between seen / unseen classes   is reported . Figure 4 compares these three settings .   4.2 Experimental setup   Dataset and split : We conduct experiments on   two datasets , Epic - kitchen v-2 ( Damen et al . , 2020 ,   2018 ) and Charades ( Sigurdsson et al . , 2016 ) . On   Epic - kitchen benchmark , we create the composi-   tional split for compositional action recognition .   To avoid inductive bias brought by pretrained back-   bones ( e.g. Faster R - CNN ( Girshick , 2015 ) pre-   trained on ImageNet ( Deng et al . , 2009 ) , or Incep-   tion3D ( I3D ) ( Carreira and Zisserman , 2017 ) pre-   trained on Kinetics ( Kay et al . , 2017 ) ) as discussed   in ( Wang et al . , 2020 ) , we ensure all nouns / verbs   seen during pre - training stay in VNwhen creat-   ing compositional split on Epic - kitchen benchmark .   For Charades , we follow the same splits in ( Kato   et al . , 2018 ) for fair comparison .   Charades dataset ( Sigurdsson et al . , 2016 ) con-   tains 9848 videos , and many involve compositional   human - object interaction . We use the composi-   tional benchmark proposed by ( Kato et al . , 2018 ):   they remove “ no interaction " action categories ,   leaving 9625 videos with 34 verbs and 37 nouns .   Those verbs and nouns are further partitioned into   two verb splits V , V(number of classes being578   20 / 14 ) , and two noun splits N , N(18 / 19 ) . The   total number of compositional actions is 149 .   Epic - kitchen version 2 dataset ( Damen et al . ,   2020 , 2018 ) contains videos recorded in kitchens ,   where people demonstrate their interaction with ob-   jects like pan , etc . The diversity of actions in this   dataset makes it especially challenging . We follow   the steps in similar previous works ( Rahman et al . ,   2018 ; Wang et al . , 2020 ) to create our composi-   tional split . We first make sure that classes seen in   pre - training stay in the seen split . Then for the re-   maining classes we sort them based on the number   of instances in descending order , and pick the last   20 % to be unseen classes , because ( Rahman et al . ,   2018 ) pointed out that zero - shot learning targets   the classes not easy to collect ( especially those in   the tail part of the long tail class distribution ) . We   show dataset statistics in Table 2 . We get a total   number of 76605 videos , including 90 verbs , 249   nouns , and 3629 compositional actions . Compared   to Charades , our proposed benchmark is at a larger   scale in terms of classes involved and sample size .   Baselines : We establish our baselines following   previous work ( Kato et al . , 2018 ) . Here we briefly   summarize their architectures , and readers can refer   to ( Kato et al . , 2018 ) or original papers for details .   These baselines are based on Inception3D features .   Triplet Siamese Network ( Triplet ) by ( Kato   et al . , 2018 ): verb / noun embeddings are concate-   nated , and transformed by fully connected ( FC )   layers . The output is concatenated with visual fea-   tures to predict scores through one FC layer with   the training of BCE loss . Semantic Embedding Space ( SES ) ( Xu et al . ,   2015 ): The model projects visual features into em-   bedding space through FC layers and then matches   output with corresponding action embeddings ( av-   erage of verb / noun embeddings ) using L2 loss .   Deep Embedding Model ( DEM ) ( Zhang et al . ,   2017 ): Verb / noun embeddings are transformed sep-   arately via FC layers and summed together . Then   output is matched with visual features via L2 loss .   4.3 Results on Epic - kitchen dataset   The results of the proposed DARK model , as well   as the aforementioned baselines ( Triplet , SES and   DEM ) and previous model GCNCL ( Kato et al . ,   2018 ) on the Epic - kitchen dataset are listed in Ta-   ble 1 . We report the results in the proposed AUC   metric ( Chao et al . , 2016 ) with precision calculated   at top 1/2/3 prediction for both open world and   macro open world settings , which evaluates the   overall trade - off between seen / unseen class . We   also report the mean average precision ( mAP ) used   in ( Kato et al . , 2018 ) on all and zero - shot composi-   tional action classes for reference .   Our best performing DARK model outperforms   all baselines and GCNCL by a large margin un-   der all metrics , illustrating the benefit of disentan-   gled action representation for compositional action   recognition . DARK is also more scalable , and re-   duces the number of graph nodes from 22749 ( GC-   NCL with no external knowledge ) to 339 ( ours ) .   DARK considers the type constraint of verbs and   nouns when composing verb and noun into com-   positional action label by training an affordance   scoring module , while GCNCL considers the con-   straint when building compositional action nodes   by collecting the existing verb - noun pairs from   NEIL ( Chen et al . , 2013 ) . For fair comparison ,   we re - implement three versions of KG in GCNCL   model . In “ GT " , we use the ground - truth verb-   noun relationships that are presented in the dataset579   ( open world setting ) . In “ Affd " , we only consider   relationships in the same corpus with DARK . We   use the relationships as a hard look - up since GC-   NCL only contains unweighted “ hard " edges in   its knowledge graph . In “ Both " , we use the union   of the constraints in “ GT " and “ Affd " . Under all   the three circumstances , our DARK model outper-   forms other models by a large margin . For all   experiments , we report the best results .   4.4 Ablations on different components   Zero - shot learning in verb / noun classifier : In   DARK model , we do separate verb and noun clas-   sification in two branches . We investigate different   implementations of zero - shot learning in verb / noun   classifier . Specifically , we consider three options   for both verb and noun , namely “ KG " , “ SES "   and “ ConSE " . “ KG " stands for zero - shot learn-   ing by using knowledge graph to predict classifi-   cation weights for unseen component with GloVe   embedding ( Pennington et al . , 2014 ) as in ( Kato   et al . , 2018 ) . “ SES " ( Xu et al . , 2015 ) is the best   common embedding baseline in Table 1 using bet-   ter BERT word embedding ( Devlin et al . , 2018 )   ( based on observation in Table 5 , BERT tends to   have better performance ) . “ ConSE " ( Norouzi et al . ,   2013 ) learns a semantic structure aware embed-   ding space compared to original word embeddings ,   which is modeled with graph . “ ConSE " ( Norouzi   et al . , 2013 ) is used as the zero - shot learning com-   ponent in previous image - based action recognition   task ( Xu et al . , 2019 ) . It learns a semantic structure   aware embedding space and we also use GloVe   embedding . For better comparison of zero - shot   learning component , we report the “ open world ”   AUC on the Epic - kitchen dataset without using   affordance ( same as “ ground - truth " affordance in   macro open setting ) , thus excluding the influence   of affordance prior . Different zero - shot learning   combinations for verbs and nouns are reported in   Table 3 . Using “ KG " for both verb / noun outper-   forms others by a large margin , and we take this   approach in the rest experiments .   WN dis VN group VN tree   one - way 1.86 0.83 1.93   two - way × × 1.79   Construction of verb knowledge graph : Com-   pared to nouns , the concept of verb is relatively   abstract and the relationship between verbs is hard   to capture . We explore different ways of construct-   ing the verb KG , namely , “ WN dis " , “ VN group "   and “ VN tree " . ( The details of noun KG are dis-   cussed in the appendix . ) In “ WN dis " , we use   WordNet ( Miller , 1995 ) structure and add edges be-   tween nodes if their LCH ( Leacock and Chodorow ,   1998 ) distance is bigger than a threshold . We also   explore VerbNet ( Kipper et al . , 2008 ) which is de-   signed to capture the semantic similarity of verbs .   VerbNet categorizes verbs into different classes ,   and each class contains multiple verb members . To   resolve the duplication in each class , we add edges   between verbs in the same class , and denote this as   “ VN group " . We also try adding a meta node for   each class and connecting all its members to the   meta node , denoted as “ VN tree " . Graphs of “ WN   dis " and “ VN group " are naturally undirected . For   “ VN tree " , we consider an additional “ two - way "   setting as in ( Kampffmeyer et al . , 2019 ) , where a   GCN model separates the parent - to - children and   children - to - parent knowledge propagation into two   stages to better model hierarchical graph structure .   However , we do not observe performance improve-   ment in this setting . In Table 4 , we report top1 AUC   for verbs using different KGs under the GZL(Chao   et al . , 2016 ) setting . “ VN tree " in “ one - way " gives   the best prediction for verb , and we keep this con-   figuration in rest experiments .   Affordance learning : We consider the compo-   sitional constraints between verbs and nouns ( af-   fordance ) when composing the compositional ac-   tion . We explore various ways of learning affor-   dance in Table 5 . In “ Word - only " we train a word-   embedding only model . And “ Visual " represents   the approach in method section where an additional   projection module maps visual features to embed-   ding space . For each , we explore two word embed-   dings , GLoVe ( Pennington et al . , 2014 ) and BERT   ( Devlin et al . , 2018 ) . In terms of score calculation ,   we try three different methods each . In “ Concat-   Scoring " , we concatenate verb / noun features , and580   Model All Zero - shot   Chance 1.37 1.00   Baseline Triplet 10.41 7.82   SES 10.14 7.81   DEM 9.57 7.74   GCNCL GCNCL - I+A 10.48 7.95   GCNCL+A 10.53 8.09   Ours DARK 11.21 8.38   train a scoring model . In “ Context - Scoring " , in-   stead of concatenating , for BERT the scoring model   embeds verb - noun phrase together and averages   their embeddings , and for GloVe we simply aver-   age their embeddings . In “ Proj - Cosine " , we project   verb embedding to noun embedding space and cal-   culate the cosine distance . We also try a lookup   table , where affordance is one if the compositional   pair exists in train set or knowledge bases , and zero   otherwise . “ Uniform " sets all affordance to be one ,   which means no weighting is applied . “ Ground   Truth " sets one for pairs existing in the dataset   ( train / test ) , equivalent to “ open world " . In all ex-   periments , we use best configuration from Table   3 , and label compositional pairs seen in training to   one . BERT constantly improve affordance relation-   ships in different methods . Lookup table performs   worse than “ Uniform " ( no affordance ) since some   valid pairings are missing in knowledge bases .   4.5 Results on Charades dataset   We report results on Charades in Table 6 . Follow-   ing ( Kato et al . , 2018 ) , we report mean average   precision ( mAP ) and compare our model to theirs   and baselines . We also report zero - shot classes   ( VN+VN+VN ) separately but all predic-   tions are made under GZL setting . Unlike Epic-   Kitchen which contains ego - centric actions , Cha-   rades contains third - person view videos and can not   detect the mask of person ’s hand . Thus we directly   learn the verb and noun feature disentanglement   leveraging a discriminator and a disentanglement   loss . Following ( Peng et al . , 2018 ) , discriminator   tries to adversarially classify noun label yfrom its   verb feature , and feature extractor Fgoes against   it . To better capture the multi - label property in   Charades , we use an un - factorized classification   model for actions in VNso they can be treated   separately . Since we report the mAP results for   fair comparison with GCNCL , we do not use affor-   dance in our model . As indicated in ( Kato et al . ,   2018 ) , we also notice that the amount of improve-   ment over baselines is not large , possibly because   Charades is relatively small and easy to overfit .   And this motivated us to propose a large - scale zero-   shot compositional action recognition benchmark .   4.6 Qualitative error analysis   We also visualize some examples in Figure 5 . The   model misclassifies coriander asleaf , and foilas   plastic wrap due to visual similarity .   5 Conclusion   In this paper , we propose DARK , a novel com-   positional action recognition model that reduces   complexity from quadratic to linear , making the   training more scalable . DARK generalizes to un-   seen verb - noun pairs , and can be combined with   knowledge bases to produce state - of - the - art com-   positional action recognition results .   Acknowledgement   Prof. Darrell ’s group was supported in part by DoD ,   BAIR and BDD.581References582583584A Implementation Detail   To build the object feature extractor F , we first   use an off - the - shelf class - agnostic object detec-   tor to detect the bounding box of interacting ob-   jects . For the Charades ( Sigurdsson et al . , 2016 )   dataset , we use the detected object boxes generated   by HOID ( Wang et al . , 2020 ) . HOID model first   detects the box of human , then based on the hu-   man ’s bounding box it detects interacting objects .   We use the code publicly released by ( Wang et al . ,   2020 ) , with the default parameters provided by the   author . We use the weights of the class - agnostic ob-   ject detector provided on the project page . For the   Epic - kitchen dataset ( Damen et al . , 2020 , 2018 ) ,   we use the pre - computed class - agnostic boxes pro-   vided by the authors . They use the model from   ( Shan et al . , 2020 ) , which detects human hands and   locates the interacting objects . We use different   models for object detection because Epic - kitchen   ( Damen et al . , 2018 ) contains mostly ego - centric   action videos while Charades ( Sigurdsson et al . ,   2016 ) contains third - person view videos . For the   Epic - kitchen dataset , we additionally use its pre-   computed detected masks of human hands .   After the object boxes are detected , we run the   Faster - RCNN ( Girshick , 2015 ) model with the   ResNet-101 backbone to extract the object features .   We use the implementation provided by the Detec-   tron2 ( Wu et al . , 2019 ) library , with the weights   pre - trained on ImageNet ( Deng et al . , 2009 ) . We   obtain the features before the classifier layer in the   Faster - RCNN model , which results in 2048 dimen-   sion object features . If we detect multiple boxes   in one frame , we conduct max - pooling over their   extracted features to obtain one feature represen-   tation for each frame . If no box is detected for a   particular frame , we then extract the feature of the   whole image as its object feature .   We sample several frames along the temporal   axis of the video to conduct object detection . Since   videos in the Charades ( Sigurdsson et al . , 2016 )   dataset may contain more than one action , we treat   each frame in Charades dataset as one sample for   training . For the Epic - kitchen ( Damen et al . , 2020 ,   2018 ) dataset whose videos contain only one action ,   we instead simply apply mean pooling over object   features of sampled frames to obtain one feature   representation for the whole video . We add fully-   connected ( FC ) layers upon the fixed Faster - RCNN   backbone to conduct feature extraction .   For verb features , we build our feature extrac - torFbased on a standard two - stream Incep-   tion3D ( Carreira and Zisserman , 2017 ) backbone   pre - trained on the Kinetics dataset ( Kay et al . ,   2017 ) . We use both the RGB branch and the optical-   flow branch , each producing a 1024 dimension fea-   ture in the layer Mixed_5c . We then concatenate   them , resulting in a feature representation of 2048   dimension . For the Epic - kitchen dataset , we gen-   erate features first using the video input with the   object cropped out . Then we do the same using   the video input with everything cropped except for   the detected hands in order to obtain hand gesture   movement information . We further concatenate   these two features to get a 4096 dimension fea-   ture . Similar to the object feature extractor , we   add FC layers to features generated by the fixed   Inception3D backbone . For the Charades dataset ,   since another disentanglement approach is used ,   we simply use the 2048 dimension feature .   Following common practice , we split the whole   video into video clips with a small duration , and   generate features for each clip during training and   inference . For the Charades dataset , we sample   10 clips per video to conduct training and we treat   each clip as a sample . Whereas for the Epic - kitchen   dataset , we apply max pooling to the features of all   the clips generated from one video to obtain one   feature representation for the whole video .   Our model is implemented in PyTorch with   Adam optimizer . We used in total around 20 GPUs   through out the experiments . But a single run only   needs 5 GPUs . ( we launch parallel experiments )   BThe Proposed Epic - kitchen Benchmark   We build our compositional action recognition   benchmark based on the Epic - kitchen(Damen et al . ,   2020 , 2018 ) dataset version two . We take the class   that overlaps with pre - trained backbones into con-   sideration when creating seen / unseen class splits .   We find that there are 95 noun classes overlapping   with ImageNet classes , and 23 verb classes over-   lapping with Kinetics classes , where the backbones   that we use have been pre - trained . We make sure   these overlapping classes stay in the seen split .   We then remove the tail verb and noun classes   with less than 10 instances . The remaining dataset   contains a total number of 76605 videos , including   90 verbs , 249 nouns , and 3629 compositional ac-   tions . We have 29 verbs in the seen category , and   61 verbs in the unseen category . On the other hand ,   102 nouns are seen and 147 nouns are unseen.585TheVNsplit contains 840 compositional ac-   tions , and 51228 samples . The VNsplit has 1073   compositional actions , and 10105 samples . For the   VNsplit , there are 896 compositional actions   and 11073 samples . And for the VNsplit , there   are 820 compositional actions and 4199 samples .   Epic - Kitchen dataset is realized under the Cre-   ative Commons Attribution - NonCommercial 4.0   International License . The licence for non-   Commercial use of Charades dataset can be found   at http://vuchallenge.org/charades.html . We follow   the intended usage of these two dataset .   C Noun Knowledge Graph Construction   We discuss the construction of the verb knowl-   edge graph in the paper , due to the space limit ,   we present the details of the noun knowledge graph   in this section . We construct the noun knowledge   graph following ( Wang et al . , 2018 ) ’s approach . We   begin from the nouns presented in the dataset , and   recursively search their hyper - norms using Word-   Net ( Miller , 1995 ) ’s lexical relationship to add to   the graph . In addition , we augment the knowledge   graph by adding nouns from ImageNet ’s ( ( Deng   et al . , 2009 ) ) class labels .   When building noun knowledge graphs , we add   an edge if two entities are direct synonyms or hyper-   norms . Our model is built upon the graph con-   volution model implemented by ( ? ) . We use its   plain GCN version without attention . And we use   the “ two - way " approach , which separates parent-   to - children and children - to - parent knowledge prop-   agation into two stages to better model the hierar-   chical graph structure . For noun knowledge graph   learning , we use 300d GloVe ( Pennington et al . ,   2014 ) embeddings as input .   D Disentanglement in Charades Dataset   LetXdenote the input to the verb feature extrac-   torF , andXdenote the extracted verb features .   Similarly , Xis the input to FandXis the   extracted noun features .   X = F(X ) ( 8)   X = F(X ) ( 9 )   To obtain disentangled verb / noun features , we   take the idea from the previous paper ( Peng et al . ,   2018 ) . We use a discriminator to limit the infor-   mation verb and noun features contain . The dis-   criminator Dtries to adversarially classify nounlabel yfrom its verb feature X , and the feature   extractor Fgoes against it via a minimax process .   The discriminator helps to limit the information   which verb feature Xcontains about the nouns   in the video . The same procedure happens for D.   We use one layer linear classifier for discriminator   DandDand they output class predictions for the   opposite branch . This leads to the disentanglement   loss :   L=−CE(D(X);y ) ( 10 )   L=−CE(D(X);y ) ( 11 )   TheCE refers to the cross - entropy loss . The   overall loss for training the feature extractor F , F   and the classifier for seen classes W , W is :   L = L+L ( 12 )   L = L+L ( 13 )   The definitions of L , Lare the same as   discussed in the main paper.586