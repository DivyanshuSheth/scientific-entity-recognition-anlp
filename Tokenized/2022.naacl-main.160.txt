  Nidhi Vakil   Department of Computer Science   University of Massachusetts Lowell   nvakil@cs.uml.eduHadi Amiri   Department of Computer Science   University of Massachusetts Lowell   hadi@cs.uml.edu   Abstract   We present a generic and trend - aware curricu-   lum learning approach for graph neural net-   works . It extends existing approaches by in-   corporating sample - level loss trends to better   discriminate easier from harder samples and   schedule them for training . The model effec-   tively integrates textual and structural infor-   mation for relation extraction in text graphs .   Experimental results show that the model pro-   vides robust estimations of sample difficulty   and shows sizable improvement over the state-   of - the - art approaches across several datasets .   1 Introduction   Relation extraction is the task of detecting ( of-   ten pre - defined ) relations between entity pairs . It   has been investigated in both natural language pro-   cessing ( Mintz et al . , 2009 ; Lin et al . , 2016 ; Peng   et al . , 2017 ; Zhang et al . , 2018 ) and network sci-   ence ( Zhang and Chen , 2018 ; Fout et al . , 2017 ) .   Relation extraction is a challenging task , especially   when data is scarce . Nonetheless , the ability to   automatically link entity pairs is a crucial task as it   can reveal relations that have not been previously   identified , e.g. , informing clinicians about a causal   relation between a gene and a phenotype or dis-   ease . Figure 1 shows an example sentence from   a PubMed article in the Gene Phenotype Relation   ( PGR ) dataset ( Sousa et al . , 2019 ) , which describes   the application domain of the present work as well .   Previous research has extensively investigated   relation extraction at both sentence ( Zeng et al . ,   2015 ; dos Santos et al . , 2015 ; Sousa et al . , 2019 )   and document ( Yao et al . , 2019b ; Quirk and Poon ,   2017 ) levels . Furthermore , effective graph - based   neural network approaches have been developed   for various prediction tasks on graphs , including   link prediction between given node pairs ( Kipf and   Welling , 2017 ; Hamilton et al . , 2017 ; Xu et al . ,   2018 ; Veli ˇckovi ´ c et al . , 2018 ) . Several recent ap-   proaches ( Li et al . , 2020 ; Zhang and Chen , 2018;Figure 1 : An example showing the report of a causal   relation between a gene and a phenotype ( symptom )   from the PGR dataset ( Sousa et al . , 2019 ) .   Alsentzer et al . , 2020 ) illustrated the importance of   enhancing graph neural networks using structurally-   informed features such as shortest paths , random   walks and node position features .   In this work , we develop a graph neural network   titled Graph TextNeural Network ( GTNN ) that   employs structurally - informed node embeddings   as well as textual descriptions of nodes at predic-   tion layer to avoid information loss for relation   extraction . GTNN can be trained using a standard   approach where data samples are fed to the network   in a random order ( Hamilton et al . , 2017 ) . How-   ever , nodes , edges or sub - graphs can significantly   vary in their difficulty to learn , owing to frequent   substructures , complicated topology and indistinct   patterns in graph data . We tackle these challenges   by presenting a generic and trend - aware curricu-   lum learning approach that incorporates sample-   level loss trajectories ( trends ) to better discriminate   easier from harder samples and schedule them for   training graph neural networks .   The contributions of this paper are : ( a ): a graph   neural network that effectively integrates textual   data and graph structure for relation extraction , il-   lustrating the importance of direct use of text em-   beddings at prediction layer to avoid information   loss in the iterative process of learning node em-   beddings for graph data ; and ( b ): a novel curricu-   lum learning approach that incorporates loss trends   at sample - level to discover effective curricula for   training graph neural networks.2202We conduct extensive experiments on real world   datasets in both general and specific domains , and   compare our model against a range of existing ap-   proaches including the state - of - the - art models for   relation extraction . Experimental results demon-   strate the effectiveness of the proposed approach ;   the model achieves an average of 8.6 points im-   provement in F1 score against the best - performing   graph neural network baseline that does not di-   rectly use text embeddings at its prediction layer .   The proposed curriculum learning approach further   improves this performance by 0.7 points , result-   ing in an average F1 score of 89.9 on our three   datasets . We conduct extensive experiments to   shed light on the improved performance of the   model . Code and data are available at https :   //clu.cs.uml.edu / tools.html .   2 Method   Consider an undirected graph G= ( V , E ) where V   andEare nodes and edges respectively , and nodes   carry text summaries as their descriptions . Edges   in the graph indicate “ relations ” between their end   points , e.g. , causal relations between genes and dis-   eases , or links between concepts in an encyclopedia .   Our goal is to predict relations / links between given   node pairs in G.   2.1 Graph Text Neural Network   We present the Graph Text Neural Network   ( GTNN ) model which directly operates on Gand   textual descriptions of its nodes . Figure 2 shows the   architecture of GTNN , which we describe below .   2.1.1 Graph Encoder   Given Gand its initial text embeddings , xfor each   node i , we apply a graph encoder ( Hamilton et al . ,   2017 ) to generate a d - dimensional embedding for   each node by iteratively aggregating the current   embeddings of the node and its t - hop neighbours   through the sigmod function denoted by g :   h = g / parenleftig   Wh+W(1   |N|/summationdisplayh)/parenrightig   , ( 1 )   where his the embedding of node iat the t   layer of the encoder and is initialized by x , i.e. ,   h = x,∀i , andNis the set of neighbors of   node iaggregated through a mean operation . W   andWare parameter matrices to learn during   training . Equation ( 1 ) , applied iteratively , generates   node embeddings z = h∈R.2.1.2 Additional Text Features   In addition to the representations obtained from   the graph encoder , we use additional features from   text data to better learn the relations between enti-   ties . Here , we consider three types of features : ( a )   relevance score between the descriptions of node   pairs obtained from information retrieval ( IR ) algo-   rithms ; we use BM-25 ( Robertson et al . , 1995 ) ,   classic TF / IDF ( Jones , 1972 ) , as well as DFR-   H and DFR - Z ( Amati and Van Rijsbergen , 2002 )   models . These IR models capture lexical similari-   ties and relevance between node pairs through dif-   ferent approaches ; ( b ): we also use the initial text   embeddings of nodes ( x,∀i ) as additional features   because the direct uses of these embeddings at pre-   diction layer can avoid information loss in the itera-   tive process of learning node embeddings for graph   data ; and ( c ): if there exist other text information   for a given node pair , e.g. , a sentence mentioning   the node pair as in Figure 1 , we use the embeddings   of such information as additional features .   2.1.3 Graph Text Decoder   For a given node pair ( u , v ) , we combined repre-   sentation of their additional features using a single   hidden layer neural network as follows :   h = ReLU / parenleftbig   Wa+b / parenrightbig   , ( 2 )   where ais obtained by concatenating the additional   feature vectors of uandv . We combine hwith   node representations , zandz , and pass them to   a two layer decoder to predict their relations :   h = ReLU / parenleftig   Wf(h , z , z ) + b / parenrightig   , ( 3 )   p(u , v ) = g / parenleftbig   Wh+b / parenrightbig   ,   where fis a fusion operator , gis the sigmod   function , and p(u , v)indicates the probability of   an edge between nodes uandv . Flattened outer   product , inner product , concatenation and 1 - D con-   volution can be used as the fusion operator ( Amiri   et al . , 2021 ) . In our experiments , we obtained bet-   ter performance using outer product , perhaps due   to its better encoding of feature interactions :   f(h , z , z ) = h⊗[z;z ] . ( 4 )   2.2 Generic Trend - aware Curricula   Graph neural networks are often trained using the   standard or “ rote ” approach where samples are   fed to the network in a random order for train-   ing ( Hamilton et al . , 2017 ) . However , edges ( and2203   other entities in graphs such as nodes and sub-   graphs ) can vary significantly in their classifica-   tion difficulty , and therefore we argue that graph   neural networks can benefit from a curriculum for   training . Recent work by Castells et al . ( 2020 ) de-   scribed a generic loss function called SuperLoss   ( SL ) which can be added on top of any target - task   loss function to dynamically weight training sam-   ples according to their difficulty for the model .   Specifically , it uses a global difficulty threshold   ( τ ) , determined by the exponential moving average   of all sample losses , and considers samples with an   instantaneous loss smaller than τas easy and the   rest as hard . Similar to the commonly - used easy - to-   hard transition curricula , such as those in ( Bengio   et al . , 2009 ) and ( Kumar et al . , 2010 ) , the model   initially assigns higher weights to easier samples ,   thereby allowing back - propagation to initially fo-   cus more on easier samples than harder ones .   However , SL does not take into account the trend   of instantaneous losses at sample - level , which can   ( a ): improve the difficulty estimations of the model   by making them local , sample dependent and po-   tentially more precise , and ( b ): enable the model   to distinguish samples with similar losses based on   their known loss trajectories . For example , con-   sider an easy sample with a rising loss trend whichis about to become a hard sample versus another   easy sample with the same instantaneous loss but   a falling loss trend which is about to become fur-   ther easier for the model . Trend information allows   distinguishing such examples .   The above observations inspire our work to uti-   lize trend information in our curriculum learning   framework , called Trend - SL . The model uses loss   information from the local time window before   each iteration to capture a form of momentum of   loss in terms of rising or falling trends and deter-   mine individual sample weights as follows :   where σis the latent weight for the training   sample ( u , v),lis the target - task loss ( binary   cross - entropy in our experiments ) for ( u , v)at cur-   rent iteration , τis the batch - level global difficulty   threshold determined by the exponential moving   average of sample losses ( Castells et al . , 2020 ) , and   ∆∈[−1,1]is the trend indicator quantified by   the normalized sample - level loss trend weighted   byα∈[0,1 ] ; our approach reduces to SL with   α= 0.∆captures the trend in the instantaneous   losses of samples over recent kiterations , effec-2204   tively utilizing local sample - level information to   determine difficulty . There are various techniques   for fitting trends to time series data ( Bianchi et al . ,   1999 ) . We use differences between consecutive   losses to determine the trend for each sample :   ∆=/summationdisplay(l−l)//summationdisplay|l−l| ,   ( 6 )   where iis the current iteration , lindicates loss at   iteration jandkcontrols the number of previous   losses to consider . As Figure 3 illustrates , Trend-   SL increases the difficulty threshold for samples   with falling loss trends ( negative ∆s ) , becoming   more flexible in increasing the weights of such   samples by allowing greater instantaneous losses .   On the other hand , it becomes more conservative   in weighting samples with rising trends ( positive   ∆s ) by reducing the difficulty threshold .   Finally , we note that the weight σin ( 5 ) can   be computed as follows , where Wis the Lambert   W function ( Euler , 1783 ) ; see details in the supple-   mentary materials in ( Castells et al . , 2020 ):   σ= exp / parenleftig   −W / parenleftbig1   2max(−2   e , β)/parenrightbig / parenrightig   , ( 7 )   β = l−(τ−α∆ )   λ . ( 8)3 Experiments   3.1 Datasets   Gene , Disease , Phenotype Relation ( )   dataset contains textual descriptions for genes , dis-   eases and phenotypes ( symptoms ) as well as their   relations , and is obtained by combining two freely   available datasets : Online Mendelian Inheritance in   Man ( OMIM ) ( Amberger et al . , 2019 ) and Human   Phenotype Ontology ( HPO ) ( Köhler et al . , 2021 ) .   OMIM is the primary repository of curated infor-   mation on the causal relations between genes and   rare diseases , and HPO provides mappings of phe-   notypes to genes / diseases in the OMIM.We intro-   duce a challenging experimental setup based on the   task of differential diagnosis ( Raftery et al . , 2014 )   using , where competing models should dis-   tinguish relevant diseases to a gene from irrelevant   ones that present similar clinical features , making   the task more difficult because of high textual and   structural similarity between relevant and irrelevant   diseases . For example , diseases 3 - methylglutaconic   type I , Barth syndrome and3 - methylglutaconic type   IIIare of the same disease type and have high lex-   ical similarity in their descriptions , but they are   not related to the same genes . We include such   harder negative gene - disease pairs by sampling   genes from those that are linked to diseases that   share the same disease type with a target disease ,   but are not linked to the target disease . We also   include an equal number of randomly sampled neg-   ative pairs to this set .   Gene Phenotype Relation ( ) ( Sousa et al . ,   2019 ) is created from PubMed articles and con-   tains sentences describing relations between given   genes and phenotypes ( Figure 1 ) . We only include   data points with available text descriptions for their   genes and phenotypes . For fair comparison , we   apply the best model from ( Sousa et al . , 2019 ) to   this dataset .   Wikipedia ( Rozemberczki et al . , 2021 ) is on the   topic of the old world lizards Chameleons with 202   species . In this dataset , nodes represent pages and   edges indicate mutual links between them . Each   page has an informative set of nouns , which we use   as additional features . We note that this dataset con-   tains only these noun features but not the original2205   text , which is required by our text only models .   Table 1 shows statistics of these datasets . In case   of and , we create five negative   examples for every positive pair . We divide these   pairs into 80%,10 % and 10 % as training , valida-   tion and test splits respectively . The data splits for is the same as the original dataset , except that   we discard data points ( node pairs ) that do not have   text descriptions .   3.2 Baselines   We use the following baselines :   •Co - occurrence labels a test pair as positive if   both entities occur together in the input text .   •Relevance Score uses scores from IR models   ( Section 2.1.2 ) as features of a logistic classifier .   •Doc2Vec ( Le and Mikolov , 2014 ) uses domain-   specific text embeddings obtained from Doc2Vec   as features of a logistic classifier .   •BioBERT ( Lee et al . , 2020 ; Devlin et al . , 2019 )   is a BERT model pre - trained on PubMed arti-   cles . BioBERT is most appropriate for relation   extraction on both and datasets as   they are also developed based on PubMed arti-   cles . It is the current state - of - the - art model on ( Sousa et al . , 2019 ) . We also include a ver-   sion of BioBERT that uses graph information by   concatenating the representation of each given   pair with the average embedding of its neighbors .   •Graph Convolutional Network ( GCN ) ( Kipf   and Welling , 2017 ) is an efficient and scalable   approach based on convolution neural networks   which directly operates on graphs .   •Graph Attention Network ( GAT ) ( Veli ˇckovi ´ c   et al . , 2018 ) extends GCN by employing self-   attention layers to identify informative neighbors   while aggregating their information , effectively   prioritizing important neighbors for target tasks .   •GraphSAGE ( Hamilton et al . , 2017 ) is an induc-   tive framework which aggregates node featuresand network structure to generate node embed-   dings , see ( 1 ) . It uses both text and graph in-   formation . We use Doc2Vec ( Le and Mikolov ,   2014 ) embeddings to initialize node features of   GraphSAGE , as they led to better performance   than other embeddings in our experiments .   •Graph Isomorphism Network ( GIN ) ( Xu et al . ,   2018 ) identifies the graph structures that are not   distinguishable by the variants of graph neural   networks like GCN and GraphSAGE . Compared   to GraphSAGE and GCN , GIN uses extra learn-   able parameters during sum aggregation and uses   MLP encoding .   •CurGraph ( Wang et al . , 2021 ) is a curriculum   learning framework for graphs that computes   difficulty scores based on the intra- and inter-   class distributions of embeddings and develops a   smooth - step function to gradually include harder   samples in training . We report the results of our   implementation of this approach .   •SuperLoss ( SL ) ( Castells et al . , 2020 ) is a   generic curriculum learning approach that dy-   namically learns a curriculum from model behav-   ior . It uses a fixed difficulty threshold at batch   level , determined by the exponential moving aver-   age of all sample losses , to assign higher weights   to easier samples than harder ones .   We compare these baselines against GTNN and   Trend - SL , described in Section 2 .   3.3 Settings   We reproduce the results reported in ( Sousa   et al . , 2019 ) using BioBERT and therefore fol-   low the same settings on the dataset . Ini-   tial domain - specific node embeddings are obtained   using Doc2Vec ( Le and Mikolov , 2014 ) or Bio-   BERT ( Lee et al . , 2020 ) . In case of Bio - BERT ,   since nodes carry long descriptions , we first gen-   erate sentence level embeddings and use their av-   erage to represent each node , following ( Zhang   et al . , 2020a ) . More recent techniques can be used   as well ( Beltagy et al . , 2020 ) . We consider 1-   hop neighbors and set t= 1 in ( 1 ) . To optimize   our model , we use the Adam optimizer ( Kingma   and Ba , 2015 ) and apply hyper - parameter search   and tuning for all competing models based on   performance on validation data . In ( 5 ) , we set   αfrom [ 0,1]with a step size of 0.1 , λfrom   { 0.1,0.5,1.0,5,10,100 } , and loss window kfrom   [ 1,10]with a step size of 1 . We consider a maxi-2206   mum number of 100training iterations with early   stopping based on validation data for all models . In   addition , we evaluate models based on the standard   Recall , Precision and F1 score for classification   tasks ( Buitinck et al . , 2013 ) . We experiment with   five random seeds and report the average results .   For all experiments , we use Ubuntu 18.04 with one   40 GB A100 Nvidia GPU , 1 TB RAM and 16 TB   hard disk space . GPU hours to train our model have   been linear to the size of the datasets ranging from   30 min to 5 hours . We use Precision ( P ) , Recall ( R )   and F1 score ( F1 ) as evaluation metrics .   3.4 Results   Table 2 shows the results . We start with text only   and graph only baselines followed by baselines that   incorporate both data modalities .   Text models ( T ): Comparing all text based   model , Relevance Score and Doc2Vec outperform   other models . In case of , high performance   of Relevance Score indicates the ability of unsu-   pervised IR models in finding relevant informa-   tion in long text descriptions . However , Relevance   Score shows poor performance on compared   to Doc2Vec , which is better at semantic represen-   tation of input data . BioBERT ( node pair ) obtainshigher precision on both datasets and good perfor-   mance on . In addition , the F1 score of the   BioBERT model developed in ( Sousa et al . , 2019 )   for is 76.6 . We note that Doc2Vec obtains   better performance than BioBERT , perhaps due to   its in - domain pre - training .   Graph models ( G ): The results show that GCN   and GAT perform better than other competing   graph models . We attribute their performance to the   use of convolution and attention networks , which   effectively prioritize important neighboring nodes   with respect to the target tasks .   Graph models with additional information :   Comparing GraphSAGE ( Doc2Vec ) and Graph-   SAGE ( random ) illustrates the significant effect   of initialization with in - domain embeddings . In   addition , GTNN outperforms GraphSAGE , result-   ing in an average of 8.6 points improvement in F1   score . This improvement is because GTNN directly   uses text descriptions at its prediction layer . This   information , although available to GraphSAGE as   well , can be lost in the iterative process of learning   node embeddings through neighbors , see ( 1 ) .   Training with curricula : The results in Table 3   show that training GTNN with effective curricula   can further improve its performance . We attribute   the better performance of Trend - SL compared to   SL to the use of trend information , which leads to   better curricula . We conduct further analysis on the   effect of trend information below . The lower per-   formance of CurGraph could be due to close proba-   bility densities that we obtained for samples in our   datasets , which do not allow easy and hard samples   to be effectively discriminated by CurGraph.2207   4 Trend Model Introspection   We conduct several ablation studies to shed light   on the improved performance of Trend - SL .   4.1 Inversion Analysis   Trend - SL results in robust estimation of diffi-   culty : In curriculum learning , instantaneous sam-   ple losses can fluctuate as model trains ( Zhou et al . ,   2020 ) . These changes result in samples being   moved across easy and hard data groups . Let ’s   define an inversion as an event where the difficulty   group of a sample is inverted in two consecutive   epochs ( determined by curricula ) , i.e. , an easy sam-   ple becomes hard in the next iteration or vice versa .   Figure 4 shows the number of inversions in SL   and Trend - SL during training . Both models con-   verge on their estimated difficulty classes of sam-   ples as training progresses . However , we observe   that Trend - SL results in fewer inversions compared   to SL , as the area under the curve for Trend - SL is   2.12 compared to 2.15 of SL . Given these results   and the performance of Trend - SL on our target   tasks , we conjecture that trend information leads to   more robust estimation of sample difficulty .   Transition patterns at inversion time : Let   epoch ebe the epoch at which an inversion oc-   curs . Considering SL as the curriculum , Figure 5   reports the average normalized loss of samples at   their inversion epochs ( e ) and kepochs before and   after that . There are some insightful patterns : ( a ):   easy - to - easy ( E2E ) and hard - to - hard ( H2H ) transi-   tions are almost flat lines , indicating the lack of any   significant trend when no inversion occurs ; and ( b ):   easy - to - hard ( E2H ) and hard - to - easy ( H2E ) tran-   sitions show that , on average , there is a sharp and   significant increase and decrease in loss patterns   as samples are inverted to hard and easy difficulty   groups respectively . Since SL does not directly   take into account trend information , these results   show that trend dynamics can inform our technical   objective of developing better curricula .   Inversions occur early during training : Fig-   ure 6(a ) shows the fraction of samples that were   easy at epoch ibut became hard with a rising trend   at epoch j > i. The corresponding heatmap for   Hard - to - Easy with falling trend is shown in Fig-   ure 6(b ) . In both case , the initial epochs ( see the   y - axis ) are brighter then later epochs , indicating   that most inversions occur early in training and the   effect of trend is more prominent in the initial part   of training.2208   Inversions occur with falling or rising loss   trends : SL does not use trend information . How-   ever , its estimated difficulty for a considerable frac-   tion of samples ( with falling or rising loss trends )   is inverted during training . In fact , we observe that   21.2 % to 50.0 % of hard samples that have a falling   loss trend will become easy in their next training   iteration ; similarly 1.3 % to 11.1 % of easy samples   that have a rising loss trend will become hard in   their next training iteration . Figure 7 shows the   inversion heatmap for such Easy - to - Hard and Hard-   to - Easy transitions in consecutive epochs . The area   under the curve for Easy - to - Hard with rising trend   and Hard - to - Easy with falling trend are 24.87 and   4.51 respectively . Trend - SL employs such trend   dynamics to create better curricula .   4.2 Domain and Feature Analysis   In - domain embeddings improve the perfor-   mance : In these experiments , we re - train our   model with different embedding initialization . As   shown in Figures 8 , Doc2Vec embeddings result   in an overall better performance than BioBERT   and random initialization approaches across the   datasets . We attribute this result to in - domain train-   ing using text summaries of genes , diseases and   phenotypes associated to rare diseases . In addition ,   the performance using BioBERT embeddings is   either comparable or considerably lower than that   of other embeddings including Random . This is   perhaps due to pre - training of BioBERT using a   large scale PubMED dataset , which has a signif-   icantly lower prevalence of publications on rare   versus common diseases . On the other hand , we di-   rectly optimize Doc2Vec on in - domain rare - disease   datasets , which leads to higher performance of the   model . We tried to fine tune BioBERT on our cor-   pus but as the text summaries are long , only a small   fraction of texts ( 512 tokens ) can be considered .   Additional Features improve the performance :   We re - train our models and exclude additional fea-   ture ( i.e. , relevance scores for and sentence   embeddings for ) , with different node embed-   ding initialization . Figure 9 shows that excluding   these features considerably reduces the F1 - scores   of our model across datasets and embedding initial-   ization . These results show that both text features   and information obtained from graph structure con-   tribute to predicting relations between nodes .   5 Related Work   Previous research on relation extraction can be cat-   egorized into text- and graph - based approaches . In   addition , to our knowledge , there is limited work   on curriculum learning with graph datasets.2209Text - based models : Text - based methods extract   entities and the relations between them from given   texts . Although , previous works typically focus on   extracting intra - sentence relations for entity pairs in   supervised and distant supervised settings ( Sousa   et al . , 2019 ; Mintz et al . , 2009 ; Dai et al . , 2019 ;   Lin et al . , 2016 ; Peng et al . , 2017 ; Zhang et al . ,   2018 ; Fout et al . , 2017 ; Zhang and Chen , 2018 ;   Quirk and Poon , 2017 ) , there are relation extrac-   tion approaches that focus on inter - sentence rela-   tions ( Kilicoglu , 2016 ; Yao et al . , 2019b ) . Kil-   icoglu ( 2016 ) investigated multi - sentence relation   extraction between chemical - disease entity pairs   mentioned at multi - sentence level . They consid-   ered lexical features , and features obtained from   intervening sentences as input to a classifier . A   close related work to our study has been conducted   by Sousa et al . ( 2019 ) , who developed an effective   model to detect relations between genes and pheno-   types at sentence - level using sentential context and   medical named entities in text . We compared our   approach with Sousa et al . ( 2019 ) on the dataset   that they developed ( PGR ) , see Section 3.2 .   Graph based models : Previous research show   that adding informative additional features with   graph helps models learn better node representa-   tions for extracting relation between entity pairs .   For example , Zhang and Chen ( 2018 ) used distance   metric information , and Li et al . ( 2020 ) used dis-   tance features like shortest path and landing proba-   bilities between pair of nodes in subgraphs as addi-   tional features . We note that some graph properties ,   although informative and effective , can be expen-   sive to calculate on large graphs during training   and should be computed offline .   Curriculum learning with graph data : Curricu-   lum learning approaches design curricula for model   training and generalizability ( Bengio et al . , 2009 ;   Kumar et al . , 2010 ; Jiang et al . , 2015 ; Amiri et al . ,   2017 ; Jiang et al . , 2018 ; Castells et al . , 2020 ; Zhou   et al . , 2020 ) . The common approach is to detect   and use easy examples to train the model and grad-   ually add harder examples as training progresses .   Curricula can be static and pre - built by humans or   can be automatically and dynamically learned by   the model . There are very few curriculum learning   methods designed to work on the graph structure .   Wang et al . ( 2021 ) developed CurGraph , which is   a curriculum learning method for sub - graph classi-   fication . The model estimates the difficulty of sam - ples using intra and inter - class distributions of sub-   graph embeddings and orders training instances to   initially expose easy sub - graphs to the underlying   graph neural network followed by harder ones . As   opposed to static curriculum , Saxena et al . ( 2019 )   introduced a dynamic curriculum approach which   automatically assigns a confidence score to sam-   ples based on their estimated difficulty . However ,   the model requires a large number of extra train-   able parameters especially when data set is large .   To overcome this limitation , Castells et al . ( 2020 )   introduced a framework with similar idea but cal-   culates the optimal confidence score for each in-   stances using a closed - form solution , thereby avoid-   ing learning extra parameters . We extended this   approach to include trend information at sample-   level for learning effective curriculum .   Graph neural networks for NLP : There are sev-   eral distantly related work that develop graph neu-   ral network algorithm for downstream tasks such   as semantic role labeling ( Marcheggiani and Titov ,   2017 ) , machine translation ( Bastings et al . , 2017 ;   Marcheggiani et al . , 2018 ) , multimedia event ex-   traction ( Liu et al . , 2020 ) , text classification ( Yao   et al . , 2019a ; Zhang et al . , 2020b ) and abstract   meaning representation ( Song et al . , 2018 ) . Graph   neural networks are used to model word - word or   word - document relations , or applied to dependency   trees . Yao et al . ( 2019a ) generated a single text   graph using word occurrences and document word   relations from text data , and used the GCN method   to learn embeddings of words and documents . Sim-   ilarly , Peng et al . ( 2018 ) used GCN to capture   the semantics between non - consecutive and long-   distance entities .   6 Conclusion and Future Work   We propose a novel graph neural network ap-   proach that effectively integrates textual and struc-   tural information and uses loss trajectories of sam-   ples during training to learn effective curricula   for predicting relations between given entity pairs .   Our approach can be used for both sentence- and   document - level relation extraction , and shows a   sizable improvement over the state - of - the - art mod-   els across several datasets . In future , we will in-   vestigate curriculum learning approaches for other   sub - tasks of relation extraction , develop more ef-   fective techniques to better fit trends to time series   data , and investigate the effect of curricula on other   graph neural networks for relation extraction.2210References22112212   Ethical statement   This investigation partially uses data from the field   of medicine . Specifically , it includes genes , dis-   eases and phenotypes that contribute to rare dis-   eases . Although the present work does not include   any patient information , it is translational in nature   and its broader impacts are first and foremost the   potential to improve the well - being of individual   patients in the society , and support clinicians in   their diagnostic efforts , especially for rare diseases .   Our work can also help Wikipedia curators and   content generators in finding relevant concepts.2213