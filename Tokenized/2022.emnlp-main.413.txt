  Roshanak Mirzaee   Michigan State University   mirzaee@msu.eduParisa Kordjamshidi   Michigan State University   kordjams@msu.edu   Abstract   Recent research shows synthetic data as a   source of supervision helps pretrained language   models ( PLM ) transfer learning to new tar-   get tasks / domains . However , this idea is less   explored for spatial language . We provide   two new data resources on multiple spatial   language processing tasks . The first dataset   is synthesized for transfer learning on spatial   question answering ( SQA ) and spatial role la-   beling ( SpRL ) . Compared to previous SQA   datasets , we include a larger variety of spatial   relation types and spatial expressions . Our data   generation process is easily extendable with   new spatial expression lexicons . The second   one is a real - world SQA dataset with human-   generated questions built on an existing corpus   with SRLannotations . This dataset can be   used to evaluate spatial language processing   models in realistic situations . We show pre-   training with automatically generated data sig-   nificantly improves the SOTA results on several   SQA and SRLbenchmarks , particularly when   the training data in the target domain is small .   1 Introduction   Understanding spatial language is important in   many applications such as navigation ( Zhang and   Kordjamshidi , 2022 ; Zhang et al . , 2021 ; Chen   et al . , 2019 ) , medical domain ( Datta et al . , 2020 ;   Kamel Boulos et al . , 2019 ; Massa et al . , 2015 ) , and   robotics ( Venkatesh et al . , 2021 ; Kennedy et al . ,   2007 ) . However , few benchmarks have directly   focused on comprehending the spatial semantics of   the text . Moreover , the existing datasets are either   synthetic ( Mirzaee et al . , 2021 ; Weston et al . , 2015 ;   Shi et al . , 2022 ) or at small scale ( Mirzaee et al . ,   2021 ; Kordjamshidi et al . , 2017 ) .   The synthetic datasets often focus on specific   types of relations with a small coverage of spa-   tial semantics needed for spatial language under-   standing in various domains . Figure 2 indicates   the coverage of sixteen spatial relation types ( in   Figure 1 : Two new datasets on SQA   Table 1 ) collected from existing resources ( Ran-   dell et al . , 1992 ; Wolter , 2009 ; Renz and Nebel ,   2007 ) . The human - generated datasets , despite   helping study the problem as evaluation bench-   marks , are less helpful for training models that can   reliably understand spatial language due to their   small size ( Mirzaee et al . , 2021 ) .   In this work , we build a new synthetic dataset   on SQA , called SRTUN(Fig 1a ) to provide   a source of supervision with broad coverage of   spatial relation types and expressions.6148   To generate SRTUN , we follow the idea of   S QA(Mirzaee et al . , 2021 ) benchmark and   generate scene graphs from a set of images . The   edges in this graph yield a set of triplets such   asABOVE(blue circle , red triangle ) , which are   used to generate a scene description ( i.e. , a story ) .   InSRTUN , we map the spatial relation types   in triplets ( e.g. , ABOVE ) to a variety of spatial   language expressions ( e.g. , over , north , above ) to   enable the transfer learning for various data do-   mains . We also build a logical spatial reasoner to   compute all possible direct and indirect spatial rela-   tions between graph nodes . Then , the questions of   this dataset are selected from the indirect relations .   To evaluate the effectiveness of SRTUN   in transfer learning , we created another dataset   named RSQ(Fig 1b ) . This dataset is built onSRL(Kordjamshidi et al . , 2017 ) corpus while   we added human - generated spatial questions andanswers to its real image descriptions . This dataset   comparatively reflects more realistic challenges   and complexities of the SQA problem .   We analyze the impact of SRTUN as source   of extra supervision on several SQA and SRL   benchmarks . To the best of our knowledge , we are   the first to use synthetic supervision for the SRL   task . Our results show that the auto - generated   data successfully improves the SOTA results onSRLandS QA - H , which are an-   notated for SRLtask . Moreover , further pre-   training models with SRTUN for SQA task im-   proves the result of previous models on RSQ ,   StepGame , and S QA - H benchmarks .   Furthermore , studying the broad coverage of spa-   tial relation expressions of SRTUN in realistic   domains demonstrates that this feature is a key fac-   tor for transfer learning .   The contributions of this paper can be summa-   rized as : ( 1)We build a new synthetic dataset to   serve as a source of supervision and transfer learn-   ing for spatial language understanding tasks with   broad coverage of spatial relation types and expres-   sions ( which is easily extendable ) ; ( 2)We provide   a human - generated dataset to evaluate the perfor-   mance of transfer learning on real - world spatial   question answering ; ( 3)We evaluate the transfer-   ability of the models pretrained with SRTUN   on multiple SQA and SRLbenchmarks and show   significant improvements in SOTA results .   2 Related Research   Requiring large amounts of annotated data is a well-   known issue in training complex deep neural mod-6149els ( Zhu et al . , 2016 ) that is extended to spatial   language processing tasks . In our study , we no-   ticed that all available large datasets on SQA task   including bAbI ( Weston et al . , 2015 ) , S QA-   A ( Mirzaee et al . , 2021 ) , and StepGame ( Shi   et al . , 2022 ) are , all , synthetic .   bAbI is a simple dataset that covers a limited   set of relation types , spatial rules , and vocabu-   lary . StepGame focuses on a few relation types but   with more relation expressions for each and con-   siders multiple reasoning steps . S QA - A ,   comparatively , contains more relation types and   needs complex multi - hop spatial reasoning . How-   ever , it contains a single linguistic spatial expres-   sion for each relation type . All of these datasets   are created based on controlled toy settings and   are not comparable with real - world spatial prob-   lems in the sense of realistic language complex-   ity and coverage of all possible relation types .   S QA - H ( Mirzaee et al . , 2021 ) is a   human - generated version of S QA - A with   more spatial expressions . However , this dataset is   provided for probing purposes and has a small train-   ing set that is not sufficient for effectively training   deep models .   For the SRL task , SRL and SpaceE-   val ( SemEval-2015 task 8) ( Pustejovsky et al . ,   2015 ) are two available datasets with spatial roles   and relation annotations . These are small - scale   datasets for studying the SRLproblem . From   the previous works which tried transfer learning on   SRLtask , ( Moussa et al . , 2021 ) only used it on   word embedding of their SRLmodel , and ( Shin   et al . , 2020 ) used PLM without any specifically de-   signed dataset for further pretraining . These issues   motivated us to create SRTUN for further pre-   training and transfer learning for SQA and SRL .   Transfer learning has been used effectively   in different NLP tasks to further fine - tune the   PLMs ( Razeghi et al . , 2022 ; Alrashdi and O’Keefe ,   2020 ; Magge et al . , 2018 ) . Besides transfer learn-   ing , several other approaches are used to tackle   the lack of training data in various NLP areas ,   such as providing techniques to label the unlabeled   data ( Enayati et al . , 2021 ) , using semi - supervised   models ( Van Krieken et al . , 2019 ; Li et al . , 2021 )   or data augmentation with synthetic data ( Li et al . ,   2019 ; Min et al . , 2020 ) . However , transfer learn-   ing is a simple way of using synthetic data as an   extra source of supervision at no annotation cost .   Compared to the augmentation methods , the datain the transfer learning only needs to be close to the   target task / domain ( Ma et al . , 2021 ) and not neces-   sarily the same . Mirzaee et al . is the first work that   considers transfer learning for SQA . It shows that   training models on synthetic data and finetuning   with small human - generated data results in a better   performance of PLMs . However , their coverage of   spatial relations and expressions is insufficient for   effective transfer learning to realistic domains .   Using logical reasoning for building datasets that   need complex reasoning for question answering is   used before in building QA datasets ( Clark et al . ,   2020 ; Saeed et al . , 2021 ) . More recent efforts even   use the path of reasoning and train models to follow   that ( Tafjord et al . , 2021 ) . However , there are no   previous works to model spatial reasoning as we   do here with the broad coverage of spatial logic .   3Transfer Learning for Spatial Language   Understanding   To evaluate transfer learning on spatial language   understanding , we select two main tasks , spatial   question answering ( SQA ) and spatial role labeling   ( SRL ) . Given the popularity of PLMs in transfer   learning ( Khashabi et al . , 2020 ; Ma et al . , 2021 ;   Clark et al . , 2020 ) , we design PLM - based models   for this evaluation . In the rest of this section , we   describe each task and model in detail .   3.1 Spatial Question Answering   In spatial question answering , given a scene de-   scription , the task is to answer questions about the   spatial relations between entities ( e.g. , Figure 1 ) .   Here , we focus on challenging questions that need   multi - hop spatial reasoning over explicit relations .   We consider two question types , YN ( Yes / No ) and   FR(Find relations ) . The answer to YN is chosen   from " Yes " or " No , " and the answer to FR is chosen   from a set of relation types .   We use a PLM with classification layers as a   baseline for the SQA task . We use a binary classifi-   cation layer for each label for questions with more   than one valid answer and a multi - class classifica-   tion layer for questions with a single valid answer .   To predict the answer , we pass the concatenation   of the question and story to the PLM ( more detail   in ( Devlin et al . , 2019 ) . ) The final output of [ CLS ]   token is passed to the classification layer and de-   pending on the question type , a label or multiple   labels with the highest probability are chosen as   the final answer.6150We train the models based on the summation of   the cross - entropy losses of all binary classifiers   in multi - label classification or the single cross-   entropy for a single classifier in multi - classification .   In the multi - label setting , we remove inconsistent   answers by post - processing during the inference   phase . For instance , LEFT and RIGHT relations   can not be valid answers simultaneously .   3.2 Spatial Role Labeling   Spatial role labeling ( Kordjamshidi et al . , 2010 ,   2011 ) is the task of identifying and classi-   fying the spatial roles ( Trajector , Landmark ,   and spatial indicator ) and their relations . A   relation is selected from the relation types   in Table 1 and assigned to each triplet of   ( Trajector , Spatial indicator , Landmark ) ex-   tracted from the sentence . We call the former spa-   tial role extraction and the latter spatial relation   extraction ( Figure 3 ) .   Several neural models have been proposed to   solve spatial role ( Mazalov et al . , 2015 ; Ludwig   et al . , 2016 ; Datta and Roberts , 2020 ) . We take   a similar approach to prior research ( Shin et al . ,   2020 ) for the extraction of spatial roles ( enti-   ties ( Trajector / Landmark ) and spatial indicators ) .   First , we separately tokenize each sentence in   the context and use a PLM ( which is BERT here )   to compute the tokens representation . Next , we   apply a BIO tagging layer on tokens representations   using ( O , B - entity , I - entity , B - indicator , I - indicator )   tags . A Softmax layer on BIO tagger output is used   to select the spatial entities and spatial indicators   with the highest probability . For training , we use   CrossEntropy loss given the spatial annotation .   For the spatial relation extraction model , sim-   ilar to ( Yao et al . , 2019 ; Shin et al . , 2020 ) , we   useBERT and a classification layer to extract   correct triplets . Given the output of the spa-   tial role extraction model , for each combination   of(spatial entity ( tr),spatial _ indicator ( sp ) ,   spatial entity ( lm))in each sentence we create an   inputand pass it to the BERT model . To indicate   the position of each spatial role in the sentence , we   use segment embeddings and add 1if it is a role   position and 0otherwise .   The[CLS ] output of BERT will be passed to a   one - layer MLP that provides the probability for the   triplet ( see Fig 3 ) . Compared to the prior research ,   we predict the spatial type for each triplet as an   auxiliary task for spatial relation extraction . To   this aim , we apply another multi - class classifica-   tion layeron the same [ CLS ] token . To train the   model , we use a joint loss function for both relation   and type modules ( more detail in Appendix B ) .   4 SRTUN : Dataset Construction   To provide a source of supervision for spatial lan-   guage understanding tasks , we generate a synthetic   dataset with SQA format that contains SRLan-   notation of sentences . We build this dataset by   expanding S QAin multiple aspects . The fol-   lowing additional features are considered in creat-   ing SRTUN :   F1 ) A broad coverage of various types of spatial   relations and including rules of reasoning over their   combinations ( e.g. NTPP ( a , b),LEFT ( b , c)→   LEFT ( a , c ) ) in various domains .   F2 ) A broad coverage of spatial language expres-   sions and utterances used in various domains .   F3 ) Including extra annotations such as the support-   ing facts and number of reasoning steps for SQA   to be used in complex modeling.6151   In the rest of this section , we describe the details   of creating SRTUN and the way we support   the above mentioned features . Figure 4 depicts   SRTUN data construction flow .   Spatial Relation Computation . Following   S QA - A , we use the NLVR scene   graphs ( Suhr et al . , 2017 ) and compute relations   between objects in each block based on their   given coordinates . NLVR is limited to 2D relation   types , therefore to add more dimensions ( FRONT   and BEHIND ) , we randomly change the LEFT   and RIGHT to BEHIND and FRONT in a subset   of examples . Moreover , there are no relations   between blocks in NLVR descriptions .   To expand the types of relations , we extend this   limitation and randomly assign relationsto the   blocks while ensuring the spatial constraints are   not violated . Then , we create a new scene graph   with computed spatial relations . The nodes in this   graph represent the entities ( objects or blocks ) , and   the directed edges are the spatial relations .   Question Selection . There are several paths be-   tween each pair of entities in the generated scene   graph . We call a path valid if at least one re-   lation can be inferred between its start and end   nodes can be inferred . For example , in Figure 4 ,   NTPP ( A , X ) , FRONT ( X , Y ) , TPPI ( Y , B)is   valid since it results in FRONT ( A , B)while   NTPP ( A , X ) , NTPPI ( X , C ) is not a valid path   – there is no rules of reasoning that can be applied   to infer new relations .   To verify the validity of each path , we pass   its edges , represented as triplets in the predicate-   arguments form to a logical spatial reasoner ( imple - mented in Prolog ) and query all possible relations   between the pair . The number of triplets in each   path represents the number of reasoning steps for   inferring the relation .   We generate the question triplets from the paths   with the most steps of reasoning ( edges ) . This ques-   tion will ask about the spatial relationship between   the head and tail entity of the selected path . The   triplets in this path are used to generate the story   and are annotated as supporting facts . Additionally ,   the story will include additional information ( extra   triplets ) unnecessary for answering the question to   increase the complexity of the task .   Spatial Reasoner . We implement several   rules ( in the form of Horn clauses shown in   Table 2 ) in Prolog , which express the logic   between the relation types ( described in Table 1 )   in various formalisms and model the logical   spatial reasoning computation ( see Appendix B.1 ) .   Compared to previous tools ( Wolter , 2009 ) ,   we are the first to include the spatial , logical   computation between multiple formalisms . This   reasoner validates the question / queries based   on the given facts . For instance , by using the   Combination rule in Table 2 over the set of facts   { NTPP ( A , X ) , FRONT ( X , Y ) , TPPI ( Y , B ) } ,   the reasoner returns True for the query   FRONT ( A , B)andFalse forFRONT ( B , A)or   BEHIND ( A , B ) .   Text generation . The scene description is gener-   ated from the selected story triplets in question se-   lection phase and using a publicly available context-   free grammar ( CFG ) provided in S QA - A.   However , we increase the variety of spatial expres-   sions by using a vocabulary of various entity prop-   erties and relation expressions ( e.g. , above , over , or   north for ABOVE relation type ) taken from exist-6152   ing resources ( Freeman , 1975 ; Mark et al . , 1989 ;   Lockwood et al . , 2006 ; Stock et al . , 2022 ; Her-   skovits , 1986 ) We map the relation types and the   entity properties to the lexical forms in our col-   lected vocabulary .   For the question text , we generate the entity de-   scription and relation expression for each question   triplet . The entity description is generated based on   a subset of its properties in the story . For instance ,   an expression such as “ a black object ” can be gen-   erated to refer to both “ a big black circle ” and “ a   black rectangle ” . We generate two question types ,   YN ( Yes / No ) questions that ask whether a specific   relation exists between two entities , and FR ( Find   Relations ) questions that ask about all possible re-   lations between them . To make YN questions more   complex , we add quantifiers ( “ all ” and “ any ” ) to   the entities ’ descriptions .   Our text generation method can flexibly use an   extended vocabulary to provide a richer corpus to   supervise new target tasks when required .   Finding Answers . We search all entities in the   story based on the entity descriptions ( e.g. , all cir-   cles , a black object ) in each question and use the   spatial reasoner to find the final answer .   SpRL Annotations . Along with generating the   sentences for the story and questions , we automati-   cally annotate the described spatial configurations   with spatial roles and relations ( trajector , landmark ,   spatial indicator , spatial type , triplet , entity ids ) .   These annotations are based on a previously pro-   posed annotation scheme of SRLand provide free   annotations for the SRL task .   To generate SRTUN , we use 6.6k NLVR   scene graphs for training and 1k for each dev and   test set . We collect 20k training , 3k dev , and 3k   test examples for each FR and YN question ( see   Table 3 ) . On average , each story of SRTUN   contains eight sentences and 91 tokens that describeon average 10 relations between different mentions   of entities . More details about the dataset statistics   can be seen in Appendix A.1 .   5 Experimental Results   The focus of this paper is to provide a generic   source of supervision for spatial language under-   standing tasks rather than proposing new tech-   niques or architectures . Therefore , in the exper-   iments , we analyze the impact of SRTUN on   SQA and SRLusing the PLM - based models de-   scribed in Section 3 .   In all experiments , we compare the perfor-   mance of models fine - tuned with the target datasets   with and without further pretraining on synthetic   supervision ( SynSup ) . All codes are publicly avail-   able . The details of experimental settings and   hyperparameters of datasets are provided in the   Appendix .   5.1 Spatial Question Answering   Here , we evaluate the impact of SRTUN and   compare it with the supervision received from other   existing synthetic datasets . Since the datasets that   we use contain different question types , we super-   vise the models based on the same question type as   the target task .   The baselines for all experiments include a ma-   jority baseline ( MB ) which predicts the most re-   peated label as the answer to all questions , and a   pretrained language model , that is , BERT here . We   also report the human accuracy in answering the   questions for the human - generated datasets . For   all experiments , to evaluate the models , we mea-   sure the accuracy which is the percentage of correct   predictions in the test sets.61535.1.1 SQA Evaluation Datasets   bAbI We use tasks 17 and 19 of bAbI . Task 17   is on spatial reasoning and contains binary Yes / No   questions . Task 19 is on path finding and con-   tains FR questions with answers in { LEFT , RIGHT ,   ABOVE , BELOW } set . The original dataset con-   tains west , east , north , and south , which we mapped   to their corresponding relative relation type .   S QA - H is a small human - generated   dataset containing YN and FR questions that need   multi - hop spatial reasoning . The answer of YN   questions is in { Yes , No , DK } where DK denotes   Do not Know is used when the answer can not be   inferred from the context . The answer to FR ques-   tions is in { left , right , above , below , near to , far   from , touching , DK } .   StepGame is a synthetic SQA dataset contain-   ing FR questions which need kreasoning steps to   be answered ( k= 1 to10 ) . The answer to each   question is one relation in { left , right , below , above ,   lower - left , upper - right , lower - right , upper - left } set .   RSQ We created this dataset to reflect the nat-   ural complexity of real - world spatial descriptions   and questions . We asked three volunteers ( English-   speaking undergrad students ) to generate Yes / No   questions forSRLdataset that contains com-   plex human - generated sentences . The questions   require at least one step of reasoning . The advan-   tage of RSQis that the human - generated spatial   descriptions and their spatial annotations already   exist in the original dataset . The statistics of this   dataset are provided in Appendix A.2 .   One of the challenges of the RSQ , which is not   addressed here , is that the questions require spatial   commonsense knowledge in addition to capturing   the spatial semantics . For example , by using com-   monsense knowledge from the sentence , “ a lamp   hanging on the ceiling ” , we can infer that the lamp   is above all the objects in the room . To compute   the human accuracy , we asked two volunteers to   answer 100 questions from the test set of RSQ   and compute the accuracy .   5.1.2 Transfer Learning in SQA   The following experiments demonstrate the impact   of transfer learning for SQA benchmarks consider-   ing different supervisions .   Due to the simplicity of bAbI dataset , PLM can   solve this benchmark with 100 % accuracy ( Mirzaee   et al . , 2021 ) . Hence we run our experiment on   only 1k and 500 training examples of task 17 and   task 19 , respectively . Table 4 demonstrates the im-   pact of synthetic supervision on both tasks of bAbI .   The results with various synthetic data are fairly   similar for these two tasks . However , pretraining   the model with the simple version of SRTUN ,   named SRTUN -S , performs better than other   synthetic datasets on task 17 . This can be due   to the fewer relation expressions in SRTUN -S ,   which follows the same structure as task 17 .   In the next experiment , we investigate the im-   pact of SRTUN onS QA - H result .   Comparing the results in Table 5 , we find that   even though the classification layer for S QA-6154   A andS QA - H are the same , the   model trained on SRTUN has a better transfer-   ability . It achieves 2.6 % better accuracy on FR and   9 % better accuracy on YN questions compared to   S QA - A. YN is , yet , the most challenging   question type in S QA - H and none of   the PLM - based models can reach even the simple   majority baseline .   Table 6 demonstrates our experiments on   StepGame .BERT without any extra supervision ,   significantly , outperforms the best reported model   in Shi et al . , TP - MANN , which is based on a neural   memory network . As expected , all the PLM - based   models almost solve the questions with one step of   reasoning ( i.e. where the answer directly exists in   the text ) . However , with increasing the steps of rea-   soning , the performance of the models decreases .   Comparing the impact of different synthetic su-   pervision , SRTUN achieves the best result on   k > 3 . For questions with k < = 3,SRTUN -   S achieves competitive similar results compared   toSRTUN . Overall , the performance gap in   SRTUN -S , S QA - A andSRTUN   shows that more coverage of relation expressions   in SRTUN is effective .   In the next experiment , we show the influence   ofSRTUN on real - world examples , which con-   tain more types of spatial relations and need more   rules of reasoning to be solved . Table 7 shows theresult of transfer learning on RSQ . This result   shows that the limited coverage of spatial relations   and expression in S QA - A impacts the   performance of BERT negatively . However , fur-   ther pretraining BERT onSRTUN -S improves   the result on RSQ . This can be due to the higher   coverage of relation types in SRTUN -S than   S QA - A. Using SRTUN for further   pretraining BERT has the best performance and   improves the result by 5.5 % , indicating its advan-   tage for transferring knowledge to solve real - world   spatial challenges .   5.2 Spatial Role Labeling   Here , we analyze the influence of the extra syn-   thetic supervision on SRLtask when evaluated   on human - generated datasets . Table 8 shows the   number of sentences in each SRL benchmarks .   The pipeline model provided in Section 3 , con-   tains two main parts , a model for spatial role ex-   traction ( SRol ) and a model for spatial relation   extraction ( SRel ) , which we analyze separately .   We further pretrain the BERT module in these   models and then fine - tune it on the target domain .   We use Macro F1 - score ( mean of F1 for all classes )   to evaluate the performance of the SRol and SRel   models .   5.2.1 SRL Evaluation Datasets6155SRL is a human - curated dataset provided on   SRLtask . This dataset contains spatial descrip-   tion of real - world images and corresponding SRL   annotations ( see Appendix A.6 ) .   S QA - H did not contain SRLanno-   tations . Hence , we asked two expert volunteers to   annotate the story / questions of this dataset . Then   another expert annotator checked the annotation   and discarded the erroneous ones . As a result , half   of this training data is annotated with SRL tags .   5.2.2 Transfer learning in SRL   Table 9 demonstrates the influence of synthetic   supervision in spatial role extraction evaluated onSRL and S QA - H .   We compare the result of SRol model with   the previous SOTA , “ R - Inf ” ( Manzoor and Kord-   jamshidi , 2018 ) , onSRLdataset . R - Inf uses ex-   ternal multi - modal resources and global inference .   All of the BERT -based SRol models outperform   the R - Inf , which shows the power of PLMs for this   task . However , since the accuracy of the SRol is al-   ready very high , using synthetic supervision shows   no improvements compared to the model that only   trained withSRLtraining set for the SRol . In   contrast , on S QA - H , using synthetic   supervision helps the model perform better . Espe-   cially , using SRTUN increases the performance   of the SRol model dramatically , by 15 % .   In table 10 , we show the result of SRel model   ( containing spatial relation extraction and spatial   relation type classification ) for spatial relation ex-   traction , with and without extra supervision from   synthetic data . Same as SRol model , extra supervi-   sion from SRTUN achieves the best result when   tested on S QA - H .   ForSRL , we compared the SRel model with   R - Inf on spatial relation extraction . As table 10   demonstrates we improve the SOTA by 2.6 % on   F1 measure using SRTUN as synthetic supervi-   sion . Also , model further pretrained on S QA-   A gets lower result than model with no extra   supervision due to the limited relation expressions   used in this data .   In conclusion , our experiments show the effi-   ciency of SRTUN in improving the performance   of models on different benchmarks due to the flexi-   ble coverage of relation types and expressions .   6 Conclusion and Future Work   We created a new synthetic dataset as a source of su-   pervision for transfer learning for spatial question   answering ( SQA ) and spatial role labeling ( SRL )   tasks . We show that expanding the coverage of re-   lation types and combinations and spatial language   expressions can provide a more robust source of su-   pervision for pretraining and transfer learning . As a   result , this data improves the models ’ performance   in many experimental scenarios on both tasks when   tested on various evaluation benchmarks . This data   includes rules of spatial reasoning and the chain of   logical reasoning for answering the questions that   can be used for further research in the future .   Moreover , we provide a human - generated   dataset on a realistic SQA task that can be used   to evaluate the models and methods for spatial lan-   guage understanding related tasks in real - world   problems . This data is an extension of a previous   benchmark on SRLtask with spatial semantic   annotations . As a result , this dataset contains anno-   tations for both SRL and SQA tasks .   In future work , we plan to investigate explicit   spatial reasoning over text by neuro - symbolic mod-   els . Moreover , using our methodology to generate   synthetic spatial corpus in other languages or for   other types of reasoning , such as temporal reason-   ing , is an exciting direction for future research.6156Limitations   Though we aim for a broad coverage of relation   types and relations , we collected this from our avail-   able resources and spatial lexicons but this is not   by any means complete . There can be relations and   expressions that are not covered . In particular , the   relation expressions are limited to verbs and prepo-   sitions . The performance and reasoning ability of   our models is improved with transfer learning but   this is , certainly , far from the natural language un-   derstanding desiderata . Our models are based on   large language models and need GPU resources to   execute .   Acknowledgements   This project is supported by National Science Foun-   dation ( NSF ) CAREER award 2028626 and par-   tially supported by the Office of Naval Research   ( ONR ) grant N00014 - 20 - 1 - 2005 . Any opinions ,   findings , and conclusions or recommendations ex-   pressed in this material are those of the authors   and do not necessarily reflect the views of the Na-   tional Science Foundation nor the Office of Naval   Research . We thank all reviewers for their helpful   comments and suggestions . We also thank Sania   Sinha and Timothy Moran for their help in the hu-   man data generation and annotations .   References615761586159A Datasets   A.1 SRTUN   As we described in Section 4 to cover more spatial   expressions and spatial relation types , we provide   an extendable vocabulary of these spatial phenom-   ena . The entire vocabulary of supported relation   expressions and entity properties are provided in   Figure 10 .   Statistic information : Each example in SR-   TUN contains a story that describe the spatial re-   lation between entities and some questions which   ask about indirect relations between entities . On   average , each story contains eight sentences and 91   tokens , which describe ten relations on average .   We follow S QAfor dataset split . The num-   ber of questions in each train , dev , and test sets is   provided in Table 3 . YN questions can have two   answers " Yes , " which is the answer to 54 % of ques-   tions , and " No , " which is the answer to 46 % of   questions .   FR is a question type with multiple answers . In   below , you can see the percentage of existence   of each relation in the whole data : { left : 10 % ,   right:10 % , above : 27 % , below : 26 % , behind : 19 % ,   front : 10 % , near : 2 % , far : 15 % , dc : 26 % , ec : 7 % ,   po : 0.2 % , tpp : 2 % , ntpp : 10 % , tppi : 3 % , and ntppi :   8 % }   A.2 RSQ   The RSQdataset generated over the context ofSRLdataset . For each group of sentences   ( describing an image ) , we ask three volunteers   ( English - speaking undergraduate students ) to gen-   erate at least four Yes / No questions . On average ,   they spent 20 minutes generating questions for each   group of sentences which , in total , they spent 210   hours generating the whole data . After gathering   the data , another undergrad student check the ques-   tions and remove the incorrect ones and keep the   rest . The train set is provided on the train set ofSRL , and since it does not have a dev set , we   split the 32 % of test data ( equal to 20 % of the train-   ing set ) and keep it as the dev set . 50 % of questions   in this data are " Yes " and 50 % are " No " . The static   information of this dataset comes in Table 3 .   To compute the human accuracy we ask two   undergraduate students , one from those who create   the questions and one new volunteer to answer 100   questions from the test set of RSQ . In the end a   third students grade their answers . A.3 bAbI   This dataset is automatically generated data includ-   ing samples with two sentences describing relation-   ships between three objects and Yes / No questions   asking about the existence of a relation between   two objects ( Fig 5 ) focuses on multi - hop spatial   reasoning question answering .   bAbI task 19 , contain questions asking about   the directed path from one room to another . More   statistic information of this dataset comes in table 3 .   A.4 S QA   S QA - A contains more complex tex-   tual context ( story ) and questions requiring com-   plex multi - hop spatial reasoning ( e.g. Fig 6 ) . This   datasets contains one large synthesized ( S QA-   A ) and a small human - generated ( S QA-   H ) subsets .   One of the advantages of S QAis the SRL   annotation of whole data ( Contexts and Questions )   provided with the main dataset . In this work , we   also recruited two experts annotator which spent   270 hours annotating 2k sentences in S QA-   H using WebAnno framework . Then an-   other expert annotator checks their annotation and   discards the wrong ones . The statistic information   of S QA comes in Table 3 .   A.5 StepGame   StepGame is another synthesized datasets de-   scribed in this paper . You can check a sample of   this dataset in Figure 7 .   A.6SRL   SRLis the task of identifying and classifying the   spatial arguments of the spatial expressions men-   tioned in a sentence ( Kordjamshidi et al . , 2010 ) .   TheSRL(Kordjamshidi et al . , 2017 ) is a dataset   provided on SRLtask .. The statistic data of this   dataset comes in Table 11 . A SRLcan have fol-   lowing spatial semantic component ( Zlatev , 2008 )   on the static environment , trajector ( the main6160   Train Test All   Sentences 600 613 1213   Trajectors 716 874 1590   Landmarks 612 573 1185   Spatial Indicators 666 795 1461   Spatial Triplets 761 939 1700   entity ) , landmark ( the reference entity ) , and spa-   tial_indicator ( the spatial term describing the re-   lationship between trajector and landmark . ) . The   dynamic environment can also have path , region ,   direction , and motion . To understandSRLbet-   ter you can take a look at Figure 8 . In this figure   the spatial value assigned to each spatial triplet can   be chosen from Table 1 .   B Models and modules   We use the huggingFaceimplementation of pre-   trained BERT base which has 768 hidden dimen-   sions . All models are trained on the training set ,   evaluated on the dev set , and reported the result   on the test set . For training , we train the model   until no changes happen on the dev set and then   store and use the best model on the dev set . We use   AdamW ( ( Loshchilov and Hutter , 2017 ) ) optimizer   on all models and modules .   For SQA tasks we use Focal Loss ( Lin et al . ,   2017 ) with γ= 2 . For spatial argument extraction,6161we use cross - entropy loss for BIO - tagging , and for   spatial relation extraction , we use the summation   of loss for each spatial relation and relation type   classification part .   Loss = /summationdisplay   CrossEntropyLoss(p , y )   + BCELoss(p , y)(1 )   The rest of experimental setting such as number   of epochs , batch size , and learning rate are provided   in Table 13 . This settings are chosen after trial and   test on the dev set of the target task .   Dataset YN FR   SRTUN 92.83 93.66   SRTUN - Simple 90.30 93.66   SRTUN - Clock - 87.13   S QA 82.05 94.17   Besides , The result of BERT model trained on   SRTUN andSRTUN and tested on the same   dataset are provided in Table 12 . SRTUN -   Simple only contains one spatial expresion for each   relation types , and SRTUN -Clock contains all   relation expression plus clock expressions ( Column   5 in Table 10a ) for relation types .   B.1 Logic - based spatial reasoner   We consider the logic rules mentioned in Figure 2   and in the form of the Horn clauses . we collect   the different combinations of spatial relations men-   tioned in Table 1 and implement the logic - based   spatial reasoner . Figure 9a shows an example of   some parts of our code on LEFT relation . In Fig-   ure 9b , on the left , some facts are given , and the   query “ ntppi ( room , X ) ” ask about all objects that   existed in the room . Below each query , there are   all possible predictions for them.6162616361646165