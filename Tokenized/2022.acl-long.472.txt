  Florian Lux and Ngoc Thang Vu   Institute for Natural Language Processing   University of Stuttgart   florian.lux@ims.uni-stuttgart.de   Abstract   While neural text - to - speech systems perform   remarkably well in high - resource scenarios ,   they can not be applied to the majority of the   over 6,000 spoken languages in the world due   to a lack of appropriate training data . In this   work , we use embeddings derived from articu-   latory vectors rather than embeddings derived   from phoneme identities to learn phoneme rep-   resentations that hold across languages . In con-   junction with language agnostic meta learning ,   this enables us to fine - tune a high - quality text-   to - speech model on just 30 minutes of data in a   previously unseen language spoken by a previ-   ously unseen speaker .   1 Introduction   The advance of deep learning ( Vaswani et al . , 2017 ;   Goodfellow et al . , 2014 ) has enabled great im-   provements in the field of Text - to - Speech ( TTS ) .   ( Towards-)end - to - end models , such as Tacotron   2 ( Wang et al . , 2017 ; Shen et al . , 2018 ) , Trans-   formerTTS ( Li et al . , 2019b ) , FastSpeech 2 ( Ren   et al . , 2019 , 2020 ) , FastPitch ( Ła ´ ncucki , 2021 )   and many more famous instances ( e.g. Arık et al .   ( 2017 ) and Prenger et al . ( 2019 ) ) allow for speech   synthesis with unprecedented quality and controlla-   bility . The models mentioned here rely on vocoders ,   such as WaveNet ( van den Oord et al . , 2016 ) ,   MelGAN ( Kumar et al . , 2019 ) , Parallel Wave-   GAN ( Yamamoto et al . , 2020 ) or HiFi - GAN ( Kong   et al . , 2020 ) to turn the parametric representations   that they produce into waveforms . Recently pro-   posed models even include some with the ability   to go directly to the waveform from a grapheme or   phoneme input sequence , such as EATS ( Donahue   et al . , 2020 ) or VITS ( Kim et al . , 2021 ) .   While these methods all perform remarkably   well if given enough data , cross - lingual use of   data remains a key challenge in TTS . Most mod-   ern methods are limited to languages and domains   that are rich in resources , which over 6,000 lan - guages are not . Attempts at reducing the required   resources in a target language by making use of   transfer learning from multilingual data have been   made by Azizah et al . ( 2020 ) ; Xu et al . ( 2020 ) ;   Chen et al . ( 2019 ) . The mismatch of input spaces   however requires complex architectural changes ,   which limits their ability to be used in conjunction   with other modern TTS architectures . Attempts   at fixing the issue of having to transfer knowledge   from a source to a target by just jointly training on a   mixed set of more and less resource rich languages   have been made by He et al . ( 2021 ) ; de Korte et al .   ( 2020 ) ; Yang and He ( 2020 ) , which requires com-   plex training procedures . In this work , we will also   attempt to transfer knowledge from a set of high   resource languages to a low resource language . We   fix previous shortcomings by 1 ) using a linguisti-   cally motivated representation of the inputs to such   a system ( articulatory and phonological features of   phonemes ) that enables cross - lingual knowledge   sharing and 2 ) applying the model agnostic meta   learning ( MAML ) framework ( Finn et al . , 2017 ) to   the field of low - resource TTS for the first time .   Using articulatory features as inputs for neu-   ral TTS has been attempted recently by Staib   et al . ( 2020 ) and Wells et al . ( 2021 ) , following   the classical approach of Jakobson et al . ( 1961 ) .   Both achieved good results when applying this   idea to the codeswitching problem , since unseen   phonemes in the input space no longer map to non-   sensical positions , as it would be the case for the   standard embedding - lookup . It has to be noted how-   ever , that this only works across languages with   similar types of phonemes . Also Gutkin ( 2017 )   have applied phonological features to low - resource   TTS with fair success . They did however rely   on supplementary features , such as dependency   parsers and morphological analyzers . Furthermore   all of their data and models are proprietary and can   therefore not be used to compare results to . In this   work , we extend the use of articulatory inputs with6858the MAML framework to enable very simple yet   well working low - resource TTS that can be applied   to almost all modern TTS architectures .   We encounter severe instabilities when using   MAML on TTS , which make the standard formula-   tion of MAML infeasible to use . Thus we also pro-   pose a modification to MAML , which reduces the   procedure ’s complexity . This allows us to create   a set of parameters of a model that can be used to   fine - tune to a well working single - language single-   speaker TTS model with as little as 30 minutes of   paired training data available and even enables zero-   shot adaptation to unseen languages . We evaluate   the success of our approach with both automatic   measures and human evaluation .   Our contributions are as follows : 1 ) We show   that it is beneficial to train a TTS model on articu-   latory features rather than on phoneme - identities ,   even in the standard single - language high - resource   case ; 2 ) We introduce a training procedure that   is closely related to MAML which allows train-   ing a set of parameters for a TTS model that can   be fine - tuned in a low resource scenario ; 3 ) We   provide insights on how much data and training   time are required to fine - tune a model across differ-   ent languages and speakers simultaneously using   said meta - parameters ; 4 ) We show that the meta-   parameters can generalize to unseen phonemes and   rapidly improve their ability to properly pronounce   them when fine - tuning .   2 Background and Related Work   2.1 Input Representations   Character Embeddings The simplest approach   to representing text as input to a TTS is using in-   dexes of graphemes to look up embeddings . This is   however prone to mistakes . Taylor and Richmond   ( 2020 ) bring up the example of coathanger . If the   TTS is not aware of the morpheme boundary be-   tween the coat and the hang , it will be inclined   to produce something like [ k2T@InÃ@]rather than   the correct [ koUthæN@ ] . Such a representation of   the input will be highly language dependent , since   special pronunciation rules rarely hold for more   than a single language .   The textual input can be augmented by adding   information , such as morpheme boundaries , intona - tion phrase boundaries derived from e.g. syntactic   parsing as is done in many TTS frontends ( Schröder   and Trouvain , 2003 ; Clark et al . , 2007 ; Ebden and   Sproat , 2015 ) , or even the semantic identity of the   word a character belongs to , using e.g. BERT em-   beddings ( Hayashi et al . , 2019 ) .   Phoneme Embeddings Rather than looking up   embeddings for graphemes , it is often beneficial   to use embeddings of phonemes . Phonemizers   ( Bisani and Ney , 2008 ; Taylor , 2005 ; Rao et al . ,   2015 ) produce a sequence of phonetic units , which   correlate with the segments in the audio much more   than raw text . One such standard of phonetic repre-   sentation which we make use of is the International   Phonetic Alphabet ( IPA ) . Using this set of phonetic   units alleviates the problems of TTS fine - tuning   and transfer - learning to low - resource domains , be-   cause the phonetic units should be mostly language   independent . Deri and Knight ( 2016 ) provide a data   driven approach for the grapheme to phoneme con-   version task , which performs well on over 500 lan-   guages and can be adapted fairly easily to any new   low - resource language . There remains however   one major challenge : The use of different phoneme   sets for each language , leading to completely un-   seen units in inference or fine - tuning data .   Latent Representations Li et al . ( 2019a ) claim   that multilinguality in speech recognition and TTS   can be achieved by changing the input to a la-   tent representation that is trained across languages .   While their results seem very promising , their tech-   nique needs training data in all languages it should   be applied to , which rules out zero - shot settings .   Articulatory Features We fix the shortcoming   of not being able to handle unseen phonemes by   specifying phonemes in terms of articulatory fea-   tures such as position ( e.g. frontness of the tongue )   and category ( e.g. voicedness ) . We show that sys-   tems trained on this input can produce a phoneme   given nothing but an articulatory description and   thus generalize to unseen phonemes . This makes   the transfer of knowledge across languages much   simpler . A similar approach for the purpose of han-   dling codeswitching has been done in Staib et al .   ( 2020 ) . Our work builds on top of theirs by extend-   ing the idea to transfer learning an entire TTS in   a new language with minimal data , making use of   meta learning on top of articulatory features.68592.2 Model Agnostic Meta Learning ( MAML )   The goal of MAML ( Finn et al . , 2017 ) is to find a   set of parameters , that work well as initialization   point for multiple tasks , including unseen ones .   The procedure consists of an outer loop and an   inner loop . The outer loop starts with a set of   parameters , which we will call the Meta Model .   The inner loop trains task specific copies of the   Meta Model for a low amount of steps . Once the   inner loop is complete , the loss for each of the   models is calculated , summed , and backpropagated   to the original Meta Model by unrolling the inner   loop . This includes the very costly calculation of   second order derivatives . The Meta Model is then   updated and the inner loop starts again .   This procedure moves the initialization point   closer to the optimal configuration for each of the   trained tasks , which generalizes to even unseen   tasks . Multiple variants of MAML have been sug-   gested that try to fix the high computational cost   of the second order derivatives . The simplest one   is called first - order MAML and simply applies the   gradient of the task specific model at the end of   the inner loop directly to the Meta Model . Other   variants are described in Antoniou et al . ( 2019 ) ;   Rajeswaran et al . ( 2019 ) .   3 Approach   3.1 System Description   For the implementation of our method , we use the   open source IMS Toucan speech synthesis toolkit ,   first introduced in ( Lux et al . , 2021 ) , which is in   turn based on the ESPnet end - to - end speech pro-   cessing toolkit ( Watanabe et al . , 2018 ; Hayashi   et al . , 2020 , 2021 ) . Neekhara et al . ( 2021 ) show ,   that it is beneficial to fine - tune a single - speaker   model to a new speaker rather than to train a multi-   speaker model . Inspired by this , we decided to also   use a model that is not conditioned on speakers   or on languages rather than a conditioned multi-   speaker multi - lingual model and fine - tune it on the   data from a new speaker in a new language . In pre-   liminary experimentation we got similar results to   them within one language , but found their method   to not work across languages . In comparison to the   fine - tuning of a simple single speaker model , we   found training and fine - tuning a model conditioned   on language embeddings and speaker embeddings   much more sensitive to the choice of hyperparam-   eters . Figure 1 shows an overview of our system ,   underlining how it is not specific to a certain archi-   tecture , but could instead be used in conjunction   with almost all modern TTS methods .   Tacotron 2 For our implementation of Tacotron   2 ( Shen et al . , 2018 ) , we make use of the forward   attention with transition agent introduced in Zhang   et al . ( 2018 ) , which uses a CTC - like forward vari-   able ( Graves et al . , 2006 ) to promote the quick   learning of monotonic alignment between text and   speech . To further help with this , we make use of   the guided attention loss introduced in Tachibana   et al . ( 2018 ) .   FastSpeech 2 To train the parallel FastSpeech 2   model ( Ren et al . , 2020 ) , annotations of durations   for each phoneme are needed . These also have   to be generated for the low - resource fine - tuning   data . To that end , we generate alignments using   the encoder - decoder attention map of a Tacotron   2 model . Following Kim et al . ( 2020 ) ; Shih et al .   ( 2021 ) ; Badlani et al . ( 2021 ) , we apply the Viterbi   algorithm to find the most probable monotonic path   through the attention map , which significantly im-   proves the quality of the alignments .   This is especially important , because we train   our FastSpeech 2 model with pitch and energy la-   bels that are averaged over the duration of each   individual phoneme to allow for great controllabil-   ity during inference , as is introduced by Ła ´ ncucki   ( 2021 ) . Incorrect alignments would lead to follow-   up errors such as an unnaturally flat prosody .   Furthermore , we make use of the conformer   block ( Gulati et al . , 2020 ) as the encoder and   decoder , rather than the standard transformer   ( Vaswani et al . , 2017 ) .   3.2 Articulatory Vectors   PanPhon The PanPhon resource ( Mortensen   et al . , 2016 ) can be used to get linguistic specifica-   tions of phonemes . It comes with an open - source6860toolwhich we use to convert phonemes into nu-   meric vectors . Each vector encodes one feature   per dimension and takes the value of either -1 , 0 or   1 , putting the features on a scale wherever mean-   ingful . This featureset also includes phonological   features which go beyond simple phonetics , such   as whether a phoneme is syllabic .   Papercup Additionally we make use of the   purely articulatory description system of phonemes   introduced in Staib et al . ( 2020 ) , which we will   call Papercup features in the following . For the   encoding we use one - hot vectors , similar to their   implementation . Some of the features , like open-   ness or frontness , should be on a scale rather than   one - hot encoded . However since the articulatory   vector is fed into a fully connected layer , we leave   the reconstruction of this dependency between fea-   tures for the network to learn .   3.3 Language Agnostic Meta Learning   We find that the standard implementation of   MAML does not work well for the TTS task . The   inner loop needs hundreds of updates in order to   make a significant change to the performance of   the task specific model . This is probably due to   the TTS task being a one - to - many mapping task ,   where the loss function of measuring the distance   to a spectrogram is not an accurate objective for   the TTS . For every text , there are infinitely many   spectrograms , which could be considered gold data .   Those spectrograms could differ in e.g. the speaker   who reads the text and how they read the text . Since   there are no conditioning signals , the TTS has to   update its parameters towards a certain speaker ’s   characteristics in general . However because in our   case each task is a different language and a differ-   ent speaker , the training becomes highly unstable .   So ideally we would either need to run MAML ’s   inner loop until convergence , which is generally in-   feasible , or stabilize the procedure by not allowing   the model to adapt further to one task than to the   others .   To fix this issue , we calculate the Meta Model ’s   loss on one batch per language . We then sum up the   losses , backpropagate and update the Meta Model   directly using Adam ( Kingma and Ba , 2015 ) . This   stabilizes the learning procedure , but still allows   the model to update its parameters towards a more   universal configuration . Since we have to make   this simplification to MAML in order to deal withthe different languages as tasks , we call this pro-   cedure language agnostic meta learning ( LAML ) .   Ultimately , the model should not care about the   language it is fine - tuned in , since it should be close   to a universal representation of an acoustic model .   To give an exact notion of our modifications : We   simplified equation 1 to equation 2 , where optis   a gradient descent update , Bis a batch sampled   from task i , Lis an objective function , Θis the set   of parameters from the Meta Model and θis the set   of parameters specific to task i. To the best of our   knowledge , we are the first to successfully apply   MAML to TTS with languages being the tasks .   fortsteps do :   Θ = opt   Θ,∇XL(θ , B ) !   where θ= Θand for dsteps do :   θ = opt(θ,∇L(θ , B))(1 )   fortsteps do :   Θ = opt   Θ,∇XL(Θ , B ) !   ( 2 )   4 Experiments   In this section we will go over the experiments we   conducted . First we will evaluate the articulatory   features on their own in a single language setting   using automatic measures . Then we will evaluate   the combination of LAML and articulatory features   in a cross - lingual setting using both automatic mea-   sures and human evaluation .   In our experiments we make use of the follow-   ing datasets : The English Nancy Krebs dataset   ( 16h ) from the Blizzard challenge 2011 ( Wilhelms-   Tricarico et al . , 2011 ; King and Karaiskos , 2011 ) ;   The German dataset of the speaker Karlsson ( 29h )   from the HUI - Audio - Corpus - German ( Puchtler   et al . , 2021 ) ; The Greek ( 4h ) , Spanish ( 24h ) ,   Finnish ( 11h ) , Russian ( 21h ) , Hungarian ( 10h ) ,   Dutch ( 14h ) and French ( 19h ) subsets of the CSS10   dataset ( Park and Mulc , 2019 ) .   4.1 Mono - Lingual Experiments   4.1.1 Embedding Function Design   To explore our first hypothesis , we investigate the   capabilities of the articulatory phoneme represen-   tations to be used in a single - speaker and single-   language TTS system . To compare different ways6861of embedding the features , we train only the embed-   ding function . As gold data we use the embeddings   from a well trained lookup - table based Tacotron 2   model . In table 1 we show the average distances   of all articulatory vectors as projected by the em-   bedding function to their identity based embedding   counterpart . The distance dbetween two embed-   ding vectors AandBis defined in equation 3 .   d= X|A−B| !   −PA·BqPA·qPB   ( 3 )   This distance function is also used as the objec-   tive function . The embedding functions are each   trained for 3000 epochs using Adam ( Kingma and   Ba , 2015 ) with a batchsize of 32 . The first column   shows the results of the articulatory features being   fed into a linear layer that projects them into a 512   dimensional space . The second column shows the   results of the articulatory features being fed into   a linear layer that projects them into a 100 dimen-   sional space , applies the tanh activation function   and then further projects them into a 512 dimen-   sional space . As can be seen from the results , it is   beneficial to both concatenate the PanPhon features   with the Papercup features despite their overlap and   to add a nonlinearity into the embedding function   to match the embeddingspace of a well trained   Tacotron 2 model . Hence we use this setup in all   following experiments .   d Linear Non - Linear   PanPhon 0.47 0.1   Papercup 0.44 0.05   Combined 0.4 0.001   4.1.2 Convergence Time   To investigate the impact that the articulatory fea-   tures have on their own , we train a Tacotron 2 with   and without them on the Nancy dataset and com-   pare their training time and final quality . While   the model trained on embedding tables shows a   clear diagonal alignment of text and spectrogram   frames on an unseen test sentence after 2,000 steps ,   the one trained on articulatory features does so al-   ready at 500 steps . This is visualized in figure 2 .   The decoder of the Tacotron 2 model can only startto learn to decode after the alignment of inputs   to outputs is learned . So learning the alignment   earlier gives the articulatory model a clear benefit .   After training for 80,000 steps however , our own   subjective assessment finds no difference in qual-   ity between the two . The earlier convergence of   the alignment however shows a possible advantage   of using the articulatory features on low - resource   tasks , as quicker training progress means that train-   ing can be stopped earlier , before overfitting on   little data becomes too problematic .   4.2 Cross - Lingual Experiments   In order to investigate the effectiveness of our pro-   posed LAML procedure , we train a Tacotron 2   model and a FastSpeech 2 model on the full Karls-   son dataset as a strong baseline . We also train   another Tacotron 2 model and another FastSpeech   2 model on speech in 8 languages with one speaker   per language ( Nancy dataset and CSS10 dataset )   and fine - tune those models on a randomly chosen   30 minute subset from the Karlsson dataset . To our   surprise , we did not only match , but even outper-   form the model trained on 29 hours with the model   fine - tuned on just 30 minutes in multiple metrics .   As a second baseline we tried to train another   meta - checkpoint using the embedding lookup - table   approach to also further investigate the effective-   ness of the articulatory features . We did however   not manage to get such a model to converge to a   usable state . This already shows the superiority of6862the articulatory feature representations for such a   multilingual use - case .   Furthermore we tried to fine - tune the well   trained English single speaker models from the   first experiment on the 30 minutes of German to   have another baseline that can be used to measure   the impact of the LAML procedure . This setup   however also did not yield any usable results . Dur-   ing the fine - tuning process , the model was capable   of speaking German with a strong English accent ,   yet it did not properly learn to speak in the voice of   the target speaker . By the time the model learned   to speak in the new speaker ’s voice , it had overfit-   ted the 30 minutes of training data and collapsed ,   producing no more intelligible speech . We con-   clude that the method proposed in this paper not   only improves on the ability to use cross - lingual   data easily , but actually enables it in the first place .   Both the articulatory features , as well as the LAML   pretraining seem necessary to achieve cross - lingual   fine - tuning on low - resource data .   The texts we use for the following experiments   are disjunct from any training data used . Human   speech as gold standard is not used , since we are   interested in the difference in performance between   the systems , not their absolute performance . The   close to state - of - the - art performance of the base-   lines is considered as given , considering their ideal   training conditions and use of proven methods . Fur-   thermore , we chose to use German as our bench-   mark language over an actual low - resource lan-   guage , since it is much easier to acquire reliable   ratings on intelligibility and naturalness for Ger-   man , than it would be for an actual low - resource   language .   4.2.1 Intelligibility   To compare intelligibility between our baseline   models and our low - resource models , we use the   word error rate ( WER ) of an automatic speech   recognition system ( ASR ) as a proxy . We syn-   thesize 100 sentences of German radio news texts   taken from the DIRNDL corpus ( Eckart et al . ,   2012 ) with each of our baselines and corresponding   low - resource systems . Table 2 shows WERs that   the German IMS - Speech ASR ( Denisov and Vu ,   2019 ) achieves on the synthesized data . For both   Tacotron 2 and the FastSpeech 2 based system , the   WER of the low - resource model is slightly lower   than that of the baseline , thus the low - resource   models performed slightly better .   Looking into the cases where the low - resourceWER Baseline Low - Resource   Tacotron 2 13.1 % 12.7 %   FastSpeech 2 9.9 % 9.7 %   system outperformed the baseline , we find code-   switched segments , where the texts contain names   of Russian cities . Since the pretraining data of   the low - resource model includes Russian speech , it   seems to have not forgotten entirely about what it   has seen in the pretraining phase , which in our   interpretation confirms the effectiveness of the   LAML against the catastrophic - forgetting problem   ( French , 1999 ) of regular pretraining .   4.2.2 Naturalness   In order to assess the naturalness of the fine - tuned   models , we conduct a preference study with 34   native speakers of German . Each participant is   shown 12 phonetically balanced samples produced   by the Tacotron 2 and FastSpeech 2 models . For   every sentence , there is one sample produced by   the baseline and one by the low - resource model .   The participants are then asked to indicate their   subjective overall preference between the two sam-   ples . The results for Tacotron 2 are shown in figure   3 ( a ) . The low - resource system was the preferred   system in more than half of the cases , with an equal   rating taking up more than another third , showing   a clear preference for the low - resource model over   the baseline . The results for FastSpeech 2 , as seen   in figure 3 ( b ) , are a lot more balanced . While   the baseline is preferred more often than the low-   resource variant , it is not the case in the majority   of the ratings . In 56 % of the cases , the model fine-   tuned on 30 minutes of data was perceived to be as   good or better than the model trained on 29 hours .   Computational Resources All models were   trained on a single NVIDIA A6000 GPU . Training   the Tacotron Baseline took 2 days . Training time of   the FastSpeech Baseline was 1 day . Training time   of the meta - checkpoint was 4 days , finetuning to a   new model from the meta - checkpoint however only   takes 2 hours . The HiFi - GAN vocoder used to gen-   erate all samples took 4 days to train and was not   fine - tuned on the unseen data . We did not perform   hyperparameter searches and used the suggested   default settings for all methods , which worked suf-   ficiently well , but could surely be improved.6863   5 Further Analysis and Future Work   What is the ideal amount of training steps for   fine - tuning ? To investigate the amount of update   steps needed to fully adapt to the new speaker with   the added difficulty of learning a new language ,   we show the cosine similarity of a speaker embed-   ding of the fine - tuned model to that of the ground   truth throughout the fine - tuning process in figure   4 . The speaker embedding is built according to the   ECAPA - TDNN architecture ( Desplanques et al . ,   2020 ) and provided open source by SpeechBrain   ( Ravanelli et al . , 2021 ) . It is trained on V oxCeleb   1 and 2 ( Nagrani et al . , 2017 , 2019 ; Chung et al . ,   2018 ) which to the best of our knowledge does not   overlap with any of the other training and evalua-   tion data we used . We tried to decrease adaptation   time further by incorporating said speaker embed-   ding similarity as an additional objective function ,   similar to Nachmani et al . ( 2018 ) , we did however   see only marginal improvements in the amount of   steps needed at the expense of greatly increased   training time .   Can this setup handle zero - shot phonemes ?   We show the model ’s zero shot capabilities in figure   5 . We removed Dutch and Finnish from the train-   ing data of the meta - checkpoint and trained another   version of it , to be able to see how it handles all of   the now completely unseen phonemes specific to   German . While their correct position in plot ( a ) can   be considered given , since it shows the articulatory   featurespace , their meaningful positions in plot ( b )   and ( c ) show that the meta - checkpoint does not just   collapse the vector of the unseen phoneme to the   one it is most similar to , but actually generalizes .   While their pronunciation when produced does not   match the correct pronunciation perfectly , it can   be understood in the context of a longer sequence .   This is congruent with the results of Staib et al .   ( 2020 ) . During the adaptation phase , the pronunci-   ation of the unseen phonemes rapidly matches the   correct pronunciation after less than 100 steps .   Does this setup learn the difference between lan-   guage and speaker ? When analyzing the fine-   tuned meta - checkpoint , we observed that it seems   to link the language of the input to the voice of the   speaker . For example when synthesizing an unseen   Hungarian text using Tacotron 2 , the voice of the   synthesis resembles that of the Hungarian female   speaker , even though the model has been fine - tuned   on the male German speaker and there are no ad-   ditional conditioning signals . We hypothesize that   the LAML procedure induces certain subsets of pa-   rameters in the model to be speaker dependent and   the encoder of the model priming those parameters   purely based on the phoneme sequence . This leads   us to believe , that the fine - tuning of all parameters   in the model may neither be necessary , nor even   the best way of adapting to new data . This also   fits the observations of the speaker embedding over   time , since the Tacotron model adapts to the new   speaker very rapidly . Further investigations into6864   the interactions between parameter groups could   allow cutting down the amount of parameters that   need to be trained significantly , further reducing   the need for training data .   How can we bring down FastSpeech 2 ’s data   need further ? A similar observation regarding   language and speaker can be made with FastSpeech   2 , however as could be seen from the experiment on   naturalness and the training time , the FastSpeech   2 model can benefit more from additional data and   training time . This may come down to its nearly   twice as high parameter count . So a more effec-   tive fine - tuning strategy , that considers some pa-   rameters as constants , could benefit the fine - tuning   capabilities of the FastSpeech 2 model greatly .   Does this work across language families ? One   limitation to our findings is that we investigated   only the transfer of languages that share similar   phoneme inventories . It is possible that fine - tuning   to a language that uses e.g. the lexical tone rather   than pitch accents or word accents would require   pretraining in more closely related high - resource   languages , such as Chinese . However , as Vu and   Schultz ( 2013 ) find in their analysis of multilin-   gual ASR , the fast adaptation of an acoustic model   trained on multiple languages to unseen languages   works well , even across different language families .   We thus believe that the technique and analysis   presented in this paper also holds across language   families and types .   6 Conclusion   In this paper , we show an approach for training a   model in a language for which only 30 minutesof data are available by making use of articulatory   features and language agnostic meta learning . The   main takeaways from our work are as follows :   Articulatory Features for TTS Using articula-   tory features as the input representation to a TTS   system enables the use of multilingual data with-   out the need for increased architectural complex-   ity , such as language specific projection spaces .   It is furthermore beneficial to use even in single-   language scenarios , since the knowledge sharing   between phonemes makes the TTS system con-   verge much earlier to an usable state during train-   ing .   MAML on TTS Applying MAML to TTS does   not work well . If we however remove the inner   loop , we are able to pretrain a low - resource capable   checkpoint for TTS . This modification not only   makes it work , it also simplifies the formulation .   Zero - shot capabilities The use of articulatory   features enables zero - shot inference on unseen   phonemes . This is further enhanced by the LAML   training procedure . The implications of this are   particularly interesting for codeswitching , as Staib   et al . ( 2020 ) ; Wells et al . ( 2021 ) have pointed out   previously . Using these two techniques in con-   junction could be used to reduce the problem of   codeswitching to a problem of token - wise language   identification .   Acknowledgements   We would like to thank the anonymous reviewers   for their insightful feedback and suggestions . This   work was funded by the Carl Zeiss Foundation.6865References686668676868