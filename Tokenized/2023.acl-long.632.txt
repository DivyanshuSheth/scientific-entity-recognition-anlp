  Chenglei SiDan Friedman   Nitish JoshiShi FengDanqi ChenHe HeUniversity of MarylandPrinceton UniversityNew York UniversityUniversity of Chicago   clsi@umd.edu , dfriedman@cs.princeton.edu   Abstract   In - context learning ( ICL ) is an important   paradigm for adapting large language models   ( LLMs ) to new tasks , but the generalization be-   havior of ICL remains poorly understood . We   investigate the inductive biases of ICL from   the perspective of feature bias : which feature   ICL is more likely to use given a set of under-   speciﬁed demonstrations in which two features   are equally predictive of the labels . First , we   characterize the feature biases of GPT-3 models   by constructing underspeciﬁed demonstrations   from a range of NLP datasets and feature com-   binations . We ﬁnd that LLMs exhibit clear   feature biases — for example , demonstrating a   strong bias to predict labels according to senti-   ment rather than shallow lexical features , like   punctuation . Second , we evaluate the effect   of different interventions that are designed to   impose an inductive bias in favor of a particu-   lar feature , such as adding a natural language   instruction or using semantically relevant la-   bel words . We ﬁnd that , while many interven-   tions can inﬂuence the learner to prefer a par-   ticular feature , it can be difﬁcult to overcome   strong prior biases . Overall , our results provide   a broader picture of the types of features that   ICL may be more likely to exploit and how to   impose inductive biases that are better aligned   with the intended task .   1 Introduction   In - context learning ( ICL ) is an increasingly popu-   lar paradigm for adapting large language models   ( LLMs ) to downstream tasks ( Brown et al . , 2020 ) .   It works by prompting LLMs with a small set of   examples that demonstrate the input and output of   a task , without requiring any parameter updates .   However , a key limitation of ICL is that it can only   incorporate a small number of demonstration ex-   amples ( e.g. , 16 ) due to the context length limit ofFigure 1 : We prompt language models with underspeci-   ﬁed demonstrations in which two features are equally   predictive of the label . In this case , the decision rule   could be based on either sentiment ( positive vs. neg-   ative ) or topic ( movie vs. food ) . We measure feature   biases by testing the model on disambiguation exam-   ples where the two hypotheses disagree , such as positive   restaurant reviews . Here , GPT-3 strongly favors the sen-   timent hypothesis , but we can encourage it to prefer the   topic feature using various interventions , e.g. , adding a   natural - language instruction or using verbalized labels .   Transformer models , meaning that most tasks will   be highly underspeciﬁed . For example , as shown   in Figure 1 , we present the model with an under-   speciﬁed text classiﬁcation problem . The sentences   with label ‘ 1 ’ are positive reviews of movies , and   the sentences with label ‘ 0 ’ are negative reviews of   restaurants . From the demonstrations , it is unclear   whether the labels are determined by the sentiment   feature ( positive vs. negative ) or the topic feature   ( movie vs. food ) . Moreover , due to the limited   context window , it is difﬁcult to specify the task11289by supplying a large number of additional training   examples . Instead , ICL can succeed only if ( a ) the   LLM has an inductive bias that happens to align   well with the given task or ( b ) there is a mecha-   nism for imposing an inductive bias on the system ,   which can specify the task ( e.g. , whether it is senti-   ment classiﬁcation or topic classiﬁcation ) without   incorporating more training examples .   In this paper , we study the inductive biases of   ICL with LLMs and measure the effectiveness of   different interventions to steer ICL towards a par-   ticular hypothesis . We focus on feature biases : a   tendency to use one feature rather than another   given a prompt in which the two features predict   the label equally well . As illustrated in Figure 1 ,   by evaluating the model on sentences where the   two features disagree — such as positive restaurant   reviews — we can measure the feature bias of the   learning algorithm , and we can attempt to mod-   ify the feature biases through various interventions   applied to the prompt .   In the ﬁrst part of the paper , we characterize the   feature biases of ICL by constructing underspeci-   ﬁed demonstrations from a variety of NLP datasets   and feature pairs . We ﬁnd that ICL models exhibit   some clear feature biases . For example , in a senti-   ment analysis setting , the LLMs we study exhibit a   strong tendency to generalize on the basis of senti-   ment rather than other equally predictive features ,   including topics of sentences . On sentence - pair   tasks , such as question answering and natural lan-   guage inference , the GPT-3 model ( Brown et al . ,   2020 ) generally prefers shallow lexical features ,   while the instruction - tuned models ( Ouyang et al . ,   2022 ) generalize more consistently with the labels   associated with those datasets . Such feature biases   could potentially be problematic — users could have   intended either of the two predictive features as the   actual task . When the model ’s feature bias does not   align with the intended task , we want the model to   be steerable with appropriate interventions .   In the second part of the paper , we measure   whether simple modiﬁcations of the prompt can   supply an inductive bias to the ICL learning algo-   rithm , steering the model to generalize according   to one feature rather than another . These interven-   tions include using natural - language instructions   or explanations and using label words that are se-   mantically related to the intended feature . As a   baseline , we compare these methods with unam-   biguous prompts , in which some demonstrationexamples are consistent with one hypothesis but   not the other . We ﬁnd that these interventions are   most effective when the model does not have a   strong feature bias , or already has a bias in favor of   the intended task feature . They are less effective at   steering the model to use one feature when it has a   strong bias to use another feature . For example , the   instruction - tuned model generalizes on the basis   of sentiment despite adding instructions and even   disambiguating evidence in favor of lexical fea-   tures like punctuation . Interestingly , we ﬁnd that   data - independent methods , like using semantically   relevant label words , sometimes have a stronger   effect than providing unambiguous data .   These results underscore some of the challenges   involved with using ICL as a general - purpose ma-   chine learning method , complementing a growing   body of work that has attempted to explain how   ICL works from an empirical ( e.g. , Min et al . , 2022 ;   Webson and Pavlick , 2022 ; Chan et al . , 2022 ) and   theoretical ( Xie et al . , 2022 ; Akyürek et al . , 2023 ;   von Oswald et al . , 2022 ) point of view . On one   hand , the strong inductive biases of LLMs are help-   ful when they happen to be well aligned with the   given task , enabling ICL to generalize successfully   from very few training examples . Moreover , sim-   ple modiﬁcations to the prompt are often successful   at steering the model towards a particular feature in   underspeciﬁed settings . On the other hand , simple   prompting methods can not systematically align the   model with user intention : they have limited effec-   tiveness when the model ’s feature biases conﬂict   with the intended task .   2 Setup   2.1 Measuring Feature Biases   We consider text classiﬁcation problems , where   x∈X is a text input and h , h : X→ { 0,1}are   two binary feature functions . For example , hmay   be a sentiment classiﬁer ( returning 0 if xis negative   and 1 if it is positive ) , and his a domain classiﬁer   indicating whether xis a review of a movie or a   restaurant . We consider a learning algorithm /lscript ,   deﬁned as a mapping from a dataset D⊆X×   { 0,1}to a classiﬁer f :X → { 0,1 } . Given a   learning algorithm /lscriptand a pair of feature functions   h , h , our aim is to understand whether /lscripttends to   return classiﬁers more similar to horh , given   thathandhhave the same accuracy on D.   In the context of ICL , we measure this property   behaviorally by prompting language models with a11290set of underspeciﬁed demonstrations D and   evaluating the resulting function f=/lscript(D )   on a disambiguating dataset D. The under-   speciﬁed demonstrations are examples D∈   X× { 0,1}such that , for all ( x , y)∈D , y=   h(x ) = h(x ) ; and we ensure that the labels are   balanced onD. The disambiguating dataset Dis   constructed so that , for all x , h(x)/negationslash = h(x ) , and   the dataset is balanced with respect to h(x)and   h(x ) . A simple example is illustrated in Figure 1 .   We measure whether fis more consistent with   horhby comparing the predictions of f , h ,   andhonD. For a given feature function h ,   we deﬁne the accuracy on has the proportion of   examples for which f(x ) = h(x ):   h - accuracy = 1   |D|/summationdisplay1[f(x ) = h(x ) ] .   The difference between h - accuracy and h-   accuracy onDcan be interpreted as a feature   bias : for example , a high value of h - accuracy in-   dicates that the model is more likely to predict the   same label as hin situations where handhdis-   agree . h - accuracy and h - accuracy always add up   to 1 and , becauseDis balanced , an h - accuracy   of 0.5 indicates that the model does not exhibit a   strong bias towards either feature .   2.2 In - Context Learning   The learning algorithms we consider in this pa-   per are based on in - context learning ( ICL ) of   LLMs ( Brown et al . , 2020 ) . A language model   p(w)assigns scores to strings w∈ V , where   Vis a discrete vocabulary . The input to ICL is   a language model p(w)and a function that con-   verts a datasetDand a single test example x   into a prompt c(D , x)∈V. We consider the   basic form of ICL , which consists of : ( 1 ) an in-   stance template t :X →Vthat encodes each   data instance xas a string ; ( 2 ) a label verbalizer   v:{0,1 } → Vthat encodes each label as a   string . For the ﬁrst part of our analysis on mea-   suring feature biases ( Section 4 ) , we adopt the sim-   plest format and deﬁne the instance template as   t(x ) = “ Input : $ xLabel : ” , and the label verbal-   izer as v(0 ) = “ 0”andv(1 ) = “ 1 ” . The prompt c   is then the concatenation of t(x)andv(y)for all   demonstration examples ( x , y)∈D ; and lastlythe test instance t(x ) . The resulting classiﬁer is :   f(x;D ) = arg maxp(v(y)|c(D , x ) ) .   ICL is known to be sensitive to the order of the   demonstrations ( Lu et al . , 2022 ) and to demonstrate   other biases that are orthogonal to this study , in-   cluding majority label bias and recency bias ( Zhao   et al . , 2021 ) . We control for these by ordering the   demonstration examples randomly and performing   label calibration , following ( Zhao et al . , 2021 ) .   3 Data Construction   We choose datasets to cover four different NLP   tasks , including both single - sentence and sentence-   pair classiﬁcation . For sentiment analysis , we   use IMDb ( Maas et al . , 2011 ) and Yelp ( Asghar ,   2016 ) datasets ; for toxicity classiﬁcation , we use   the CivilComments dataset ( Borkan et al . , 2019 ) ;   for natural language inference , we use the MNLI   dataset ( Williams et al . , 2018 ) ; and for question   answering , we use BoolQ ( Clark et al . , 2019 ) .   For each dataset , we select the original classi-   ﬁcation label as the default feature and denote it   ash . We select alternative comparison features   ( h ) using existing metadata or simple , determin-   istic functions , chosen to reﬂect realistic sources   of ambiguity or spurious correlation that have been   studied in prior work ( McCoy et al . , 2019 ; Guru-   rangan et al . , 2018 ; Poliak et al . , 2018 ; Joshi et al . ,   2022 ) , as well as common shallow features such   as length , capitalization , and the presence of par-   ticular words or punctuation marks . All datasets   and features we use are listed in Table 1 , which we   elaborate below :   ( 1 ) For sentiment analysis , the default feature is   the sentiment , and the alternative features include :   domain or source of the review ( based on whether it   is from IMDb or Yelp ) , length of the review ( longer   or shorter than a threshold ) , the ﬁnal punctuation   mark of the review ( exclamation mark or period ) ,   whether it contains certain keywords ( “ food ” and   “ nice ” ) , and whether it contains uppercase words   ( e.g. , “ THIS ” ) .   ( 2 ) For toxicity classiﬁcation , the default feature   is whether the comment is toxic . The alternative   features are : gender , sexuality , religion , and race   mentioned in the comment ( all based on human-   annotated meta - data ) , and its length and capitaliza-   tion ( whether containing uppercase words ) .   ( 3 ) For NLI , the default feature is the entailment   relationship between the sentence pair , and we con-11291   dense the neutral and contradiction classes as non-   entailment to cast the task as binary classiﬁcation .   For alternative features , we consider : domain of   the text ( from the genre meta - data ) ; lexical overlap ,   following the deﬁnition in HANS ( McCoy et al . ,   2019 ) ; whether the hypothesis is shorter or longer   than the premise ; and whether the hypothesis con-   tains negation words ( “ no ” , “ not ” , “ n’t ” ) .   ( 4 ) For question answering , the default feature   is whether the answer is yes or no , and the alter-   native features are : the question word , whether all   words from the question also appear in the passage   ( lexical overlap ) , question structure ( whether it is a   comparison question ) , and passage length . These   features are potential spurious features in QA that   have been documented in prior work ( Pezeshkpour   et al . , 2022 ; Shinoda et al . , 2022 ) .   4 Measuring Feature Biases   4.1 Experiment Details   Evaluation protocol . For all experiments , we   randomly sample a balanced set of 16 demon-   stration examples ( randomly shufﬂed ) to form   the prompt . For eight of the examples , y=   h(x ) = h(x ) = 0 ; for the other eight , y = h(x ) = h(x ) = 1 . For each experiment , we   randomly sample three sets of prompts and re-   port the average performance on a set of 1,200   test examples , balanced between examples with   h(x ) = 0 /h(x ) = 1 andh(x ) = 1 /h(x ) = 0 .   In this baseline setting , the instance template is   t(x ) = “ Input : $ xLabel : ” , and the label verbal-   izer provides no additional information about the   task : v(0 ) = “ 0 ” and v(1 ) = “ 1 ” .   Models . We focus on the T - D -002   andD checkpoints of GPT-3 mainly be-   cause smaller - scale models often perform no better   than random on tasks like NLI . The main differ-   ences between the two checkpoints are the pretrain-   ing data and the additional instruction tuning for   T - D -002 ( Ouyang et al . , 2022 ) . For all   experiments , we use a temperature value of 0 in   GPT-3 decoding , and all experiments involving the   OpenAI API were conducted in January 2023 .   Metric . We report the h - accuracy for each fea-   ture . Higher h - accuracy indicates a higher prefer-   ence for that feature . We denote the default feature   h(sentiment , toxicity , entailment , answer ) and the   alternative features h.11292   4.2 Results   We present ICL results on all the datasets and pairs   of features in Figure 2 , and note several interesting   trends as follows :   LLMs often have clear feature biases . For ex-   ample , in the sentiment analysis setting ( Figure 2a ) ,   both models generally show a strong preference to   predict labels according to the sentiment of the sen-   tence rather than other features , such as sentence   length or the presence of individual lexical items   like the word “ food ” . Such biases can be helpful   when they are aligned with the intended task .   On the other hand , we do not observe clear fea-   ture preferences in the CivilComments dataset ( Fig-   ure 2c ) , suggesting that these models may not have   a strong feature biases in this setting .   The instruction - tuned model is generally more   aligned with standard dataset labels . While   both D andT - D -002 show sim-   ilar feature biases in the sentiment analysis setting ,   they show very different biases on the the sentence   pair datasets MultiNLI ( Figure 2b ) and BoolQ ( Fig-   ure 2d ): the D model tends to prefer the   shallow distractor features , such as lexical overlap , while T - D -002 tends to prefer the se-   mantic feature associated with the dataset — either   the entailment relationship or the answer to the   question . This may be due to some aspect of in-   struction tuning , which might have exposed the   model to similar tasks .   5 Comparing Interventions   Our ﬁndings that LLMs can have strong feature bi-   ases have important implications : when the LLMs ’   biases do not align with users ’ intended task , such   biases would hurt performance . To resolve such   misalignment , we explore a set of intervention   methods designed to encourage the model to prefer   one feature over another , examining whether the h-   accuracy for the intended feature indeed increases .   5.1 Experiment Details   We now consider more variants of ICL that can be   decomposed into four components that are com-   monly used in various prompting methods . In ad-   dition to the instance template tand label verbal-   izervdescribed in Section 2.2 , we also introduce :   ( 1 ) An instruction s∈V , which is prepended   to the prompt ; and ( 2 ) a free - form explanation11293e : X→Vafter each input text t(x)and before   the label v(x ) . The prompt cis then the concate-   nation of s , followed by t(x);e(x);v(y)for all   demonstration examples ( x , y)∈D ; and lastly   the test instance t(x ) .   Each intervention operates on a combination   of the above components . We apply these inter-   ventions separately and compare with the baseline   rather than applying all interventions on top of each   other in order to analyze the impact of each of the   methods . We compare interventions designed to   steer the model towards handhas the intended   feature respectively , to understand the effectiveness   of interventions towards different features .   •Recall that in the baseline setting , there is no   instruction or explanation ( sandeare empty   strings ) . We simply concatenate demonstra-   tion examples as the prompt , and use “ 1 ” and   “ 0 ” as the verbalizer .   •In the semantic verbalizer setting , the verbal-   izer selects label words that are semantically   related to a chosen feature in order to hint at   the intended task . For example , if the intended   feature is sentiment , then v(0 ) = “ negative ”   andv(1 ) = “ positive ” ; and if the intended   feature is length , then v(0 ) = “ short ” and   v(1 ) = “ long ” . Our choice of verbalizers is   inspired by prior work ( Gao et al . , 2021 ; Shi   et al . , 2022 ) and we list all of them in Table 6 .   •In the instruction setting , we add a preﬁx   string describing the intended feature and in-   structing the model to use this feature . We   format our instructions mostly following prior   work such as Natural Instructions ( Mishra   et al . , 2021 ; Wang et al . , 2022 ) , and we list all   our instructions in Tables 7 and 8 .   •In the explanation setting , we append a tem-   plate explanation after each demo example to   explain why the prediction is made based on   the intended feature , formatted in a similar   manner as Chain - of - Thought prompting ( Wei   et al . , 2022 ) and “ explain - then - predict ” ( Ye   and Durrett , 2022 ) . Since hand - written expla-   nations are hard to obtain , we create ﬁxed   human - written templates for each feature   value . For example , for the punctuation fea-   ture , all positive examples have the explana-   tion “ The review ends with an exclamation   mark . Therefore , the answer is 1 ” . We list all   our template explanations in Tables 9 and 10 .   •Finally , we include a disambiguation setting ,   in which we change half of the demonstration   examples to those that disambiguate the task   in favor of the intended feature . For example ,   to steer the model towards h , the demon-   stration includes examples such that h(x)/negationslash=   h(x)andy = h(x ) . Intuitively , this pro-   vides additional evidence for the model to dif-   ferentiate the intended feature .   We measure the effectiveness of the intervention in   terms of the increase in h - accuracy relative to the   baseline model , where his the intended feature .   5.2 Results   Which interventions are effective ? Table 2 sum-   marizes the results of these experiments , comparing   the effect of the different interventions on D   andT - D -002 , averaged over features   and datasets , and comparing between interventions   designed to steer towards the default feature ( h )   and the alternative features . Nearly all interven-   tions increase the average h - accuracy , in many   cases by a substantial amount . However , the ef-   fectiveness of the intervention varies depending   on the model . For the D model , the best-   performing interventions include semantic verbal-   izers and template - based explanations , while the   worst - performing intervention is natural - language   instructions . For the T - D -002 model ,   instructions are much more effective.11294   In some cases , prompt - based interventions are   more effective at steering the model than provid-   ing unambiguous demonstration examples . On one   hand , this suggests that ICL can be effective even   given highly underspeciﬁed data , but it also indi-   cates that ICL models may fail to exploit the infor-   mation provided in the demonstrations . This ﬁnd-   ing illustrates that ICL works very differently from   standard supervised learning , and calls to mind ex-   isting empirical ( Min et al . , 2022 ) and theoretical   results ( Xie et al . , 2022 ) suggesting that ICL might   work in part by recognizing existing tasks rather   than directly learning the input - output relation from   the demonstration .   When are interventions effective ? Figure 3   compares the results of different interventions on   theT - D -002 model , aggregated over   features . ( Detailed results for each feature and   D results are in Appendix A.1 . ) The ef-   fectiveness of the intervention varies depending   on whether the prior feature bias and the intended   feature are aligned . The interventions are most ef-   fective in two scenarios . First , interventions are   effective when the model already has a feature bias   for the intended features . This is evident in the   interventions that steer the model towards hinNLI and QA , settings in which the model already   has a feature bias in favor of h. Second , inter-   ventions are effective when the model has a low   feature bias . This is the case in the Toxicity Classi-   ﬁcation dataset , where the model does not exhibit a   strong feature bias . In this setting , all interventions   are moderately successful at steering the model   towards h , and more successful at steering the   model towards h.   On the other hand , interventions are less effec-   tive at overriding feature biases . This trend is illus-   trated in the second row of Figure 3 , in which the   intervention is designed to steer the model towards   hrather than the standard dataset label . While   some interventions increase h - accuracy , no inter-   vention consistently gets the model to generalize   according to the intended feature .   Which features are most susceptible to interven-   tions ? In Table 3 , we compare the effect of inter-   ventions on different features in MultiNLI . Using   meaningful label words works better on the genre   features , where the label words are semantically   similar to the input example , but it is more difﬁcult   to steer the model toward the use of features like   length and lexical overlap , which are not related to   the semantics of the sentences . More work may be11295   needed to develop interventions that work well for   higher - order language features .   Lastly , we compile a summary of practical take-   aways for steering LLMs :   •When using non - instruction - tuned LLMs   ( e.g. , D ) , specifying feature prefer-   ences as instructions is not effective , instead   adding explanations or disambiguating exam-   ples can be more effective .   •When using instruction - tuned LLMs ( e.g. ,   T - D -002 ) , all interventions can   be effective .   •Features not related to semantics , such as sen-   tence lengths or overlap , are difﬁcult to inter-   vene across all conditions .   6 Related Work   Measuring inductive biases . Our work builds   on existing research on measuring the inductive bi-   ases of learning algorithms in machine learning and   NLP . Dasgupta et al . ( 2022 ) introduce a method-   ology for measuring feature bias as well as rule-   vs. exemplar - based generalization , and Chan et al .   ( 2022 ) apply this approach to compare rule- vs.   exemplar - based learning in ICL . We use a simi-   lar framing as Dasgupta et al . ( 2022 ) , but focus   on feature bias . In NLP , another line of work has   studied the inductive biases of neural networks in   the context of the poverty of the stimulus argu-   ment ( Chomsky , 1980 ) . These studies evaluate   whether neural networks generalize in a manner   consistent with structural or superﬁcial linguistic   rules ( McCoy et al . , 2018 , 2020 ) . Several studieshave found that models such as BERT acquire a   preference for structural generalizations from large-   scale masked language model pretraining ( Warstadt   and Bowman , 2020 ; Warstadt et al . , 2020 ; Zhang   and Hashimoto , 2021 ; Mueller et al . , 2022 ) . We fol-   low a similar poverty - of - the - stimulus experimental   setup but focus on in - context learning and on fea-   tures arising in common NLP tasks . Lovering   et al . ( 2021 ) explore whether it is possible to pre-   dict the inductive biases of pre - trained models and   show that models tend to generalize on the basis   of features that are more “ extractable ” , measured   using probing techniques ( V oita and Titov , 2020 ) ,   but it is not straightforward to extend the notion   of extrability to in - context learning . Tamkin et al .   ( 2023 ) also study how LLMs generalize to ambigu-   ous classiﬁcation tasks , but focus on ambiguous   instructions and use template - generated datasets .   Spurious correlations . A related line of work   has explored the inductive biases of pretrained LMs   in relation to spurious correlations , or shortcuts   ( e.g. Gururangan et al . , 2018 ; Poliak et al . , 2018 ;   McCoy et al . , 2019 ; Geirhos et al . , 2020 ; Sagawa   et al . , 2020)—shallow features that are correlated   with the classiﬁcation targets . Models can general-   ize successfully if they have an inductive bias that   tends to favor the intended feature over the short-   cut . Hendrycks et al . ( 2019 , 2020 ) ; Tu et al . ( 2020 )   found that pre - trained models can generalize more   successfully to such distribution shifts .   Explaining in - context learning . A number of   empirical studies have attempted to characterize the   behavior of ICL and explain why it works . These   studies have found that ICL can be overly sensitive11296to certain aspects of the prompt , such as the order   of the demonstration examples ( Lu et al . , 2022 ) and   choice of label words ( Zhao et al . , 2021 ) , but also   surprisingly insensitive to others . In particular , Min   et al . ( 2022 ) show that LLMs can largely ignore   the relationship between inputs and labels in the   demonstration example and Webson and Pavlick   ( 2022 ) show that the performance of ICL can per-   form well on NLI even if the prompt is irrelevant   or misleading . Relatedly , Wei et al . ( 2023 ) ; Pan   et al . ( 2023 ) show that LLMs can perform ICL well   with ﬂipped labels or semantically - unrelated la-   bels , but such ability of overriding semantic priors   emerges with scale . Some theoretical work has also   attempted to explain why prompt - based methods   work , by drawing connections between the prompt-   ing setting and properties of the pretraining distri-   bution ( Saunshi et al . , 2021 ; Wei et al . , 2021 ; Xie   et al . , 2022 ) or by arguing that Transformers can   act as meta - learners , implicitly performing gradient   descent on the in - context examples ( von Oswald   et al . , 2022 ; Akyürek et al . , 2023 ) . Our results pro-   vide empirical evidence that there is a strong task   bias from pretraining when the LLMs must infer   the task by input - output relations .   Improving in - context learning . Recent work   studied the effect of including explanations in the   prompt to produce better quality answers ( Wei   et al . , 2022 ; Lampinen et al . , 2022 ) . While they   show the beneﬁt of high - quality human - annotated   explanations for improving task performance , we   demonstrated the effectiveness of simple template   explanations in steering feature biases . Another   line of work collects large pools of instructions   from diverse tasks and uses them to tune and con-   trol language models ( Wang et al . , 2022 ; Chung   et al . , 2022 ) . We also adopt instructions as an inter-   vention method and show that it works particularly   well on instruction - tuned models . In a similar man-   ner , Si et al . ( 2023 ) studied prompting methods to   make GPT-3 more reliable , such as instructing it to   not rely on demographic biases .   7 Conclusion   In this work , we constructed underspeciﬁed   prompts from real - world datasets to study feature   biases of large language models . We found that the   instruction - tuned InstructGPT model prefers the   “ default ” task features over distractor features more   often than the base GPT-3 model , and we demon-   strated the effectiveness of various interventionmethods in steering models to use the speciﬁed fea-   ture . These results not only shed new insights into   the working mechanisms of ICL , but also have prac-   tical takeaways for discouraging models from ex-   ploiting unintended features such as demographic   biases or shallow statistical cues .   Limitations   Model coverage . Our study is targeted specif-   ically at GPT-3 and it would be interesting to   study feature bias patterns on other large language   models such as OPT ( Zhang et al . , 2022 ) and   BLOOM ( Scao et al . , 2022 ) ; and it is possible that   our intervention methods may have different effects   on these language models trained with different   data sources and scales .   Task coverage . Apart from model coverage , our   analysis is focused on only four common binary   classiﬁcation tasks . Our main metric , h - accuracy ,   compares the predictions between a learned func-   tionfand a feature function h. For simplicity , we   only study binary functions ( consistent with prior   work ) to illustrate the main ideas , but the frame-   work applies equally well if fandhare multi - class   classiﬁers . For example , in the case of the three-   way NLI task , we might set hto predict on the   basis of entailment / contradiction / neutral , and h   to predict on the basis of the genres – e.g. ﬁction /   government / telephone . Future work could extend   our framework to more tasks , and consider how to   apply it to more complex tasks such as generation .   Feature coverage . Our current experiments are   limited to a set of hand - crafted features . One po-   tential way to systematically scale our approach is   to develop novel techniques for automatic feature   discovery , for example , to cluster the data and treat   each cluster as having a distinct feature .   Explaining feature biases . While our empirical   ﬁndings shed light on the feature bias patterns of   GPT-3 , we do not yet have a conclusion on how   these biases arise from pretraining . Future work   could attempt to draw connections to the pretrain-   ing data or to theoretical accounts of in - context   learning .   Risks and ethics . While we do not foresee any   ethical risks resulting from our work , we caution   against making extrapolations about the extent to   which LLMs exhibit feature biases towards pro-   tected social attributes . Although we do not ﬁnd11297evidence of strong feature biases in a particular tox-   icity classiﬁcation setting , care should be taken to   evaluate the fairness and reliability of these models   directly before they are deployed in downstream   applications .   Acknowledgement   We thank Alex Tamkin , Xi Ye , Sewon Min , and Jor-   dan Boyd - Graber for their helpful feedback . This   research is partially funded by the National Science   Foundation ( IIS-2211779 ) , a Sloan research fellow-   ship , Samsung Advanced Institute of Tech- nology   ( under the project Next Generation Deep Learn-   ing : From Pattern Recognition to AI ) and AWS   AI . NJ is supported by an NSF Graduate Research   Fellowship under grant number 1839302 .   References112981129911300A Appendix   A.1 Intervention Results Across Features   We present the complete set of intervention results   broken down by features in Table 4 for D   and Table 5 for T - D -002 .   It is worth noting that the intervention methods ’   effectiveness often varies across features even on   the same dataset . For example , all intervention   methods can effectively steer models towards using   the genre feature over the entailment feature on   MNLI , but the success is limited for the lexical   overlap feature on MNLI . We hypothesize this is   because features like lexical overlap are harder for   models to recognize .   A.2 List of Semantic Verbalizers   We present the full list of semantically meaning-   ful verbalizers for the intervention experiments in   Table 6 .   A.3 List of Task Instructions   We present the full list of task instructions for the   intervention experiments in Table 7 and Table 8 .   A.4 List of Template Explanations   We present the full list of template explanations   for the intervention experiments in Table 9 and   Table 10.1130111302113031130411305113061130711308ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Last section   /squareA2 . Did you discuss any potential risks of your work ?   Last section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Left blank.11309 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.11310