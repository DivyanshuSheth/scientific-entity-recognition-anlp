  Clara Meister Tiago Pimentel Thomas Hikaru ClarkRyan Cotterell Roger LevyETH Zürich University of Cambridge Massachusetts Institute of Technology   clara.meister@inf.ethz.ch tp472@cam.ac.uk thclark@mit.edu   ryan.cotterell@inf.ethz.ch rplevy@mit.edu   Abstract   Numerous analyses of reading time ( RT ) data   have been implemented — all in an effort to   better understand the cognitive processes   driving reading comprehension . However , data   measured on words at the end of a sentence — or   even at the end of a clause — is often omitted   due to the confounding factors introduced by   so - called “ wrap - up effects , ” which manifests   as a skewed distribution of RTs for these words .   Consequently , the understanding of the cog-   nitive processes that might be involved in these   wrap - up effects is limited . In this work , we   attempt to learn more about these processes by   examining the relationship between wrap - up ef-   fects and information - theoretic quantities , such   as word and context surprisals . We find that   the distribution of information in prior contexts   is often predictive of sentence- and clause - final   RTs ( while not of sentence - medial RTs ) . This   lends support to several prior hypotheses about   the processes involved in wrap - up effects .   1 Introduction   Reading puts the unfolding of linguistic input in   the hands — or , really , the eyes — of the reader . Con-   sequently , it presents a unique opportunity to gain   a better understanding of how humans comprehend   written language . The rate at which humans choose   to read text ( and process its information ) should   be determined by their goal of understanding   it . Ergo , examining where a reader spends their   time should help us to understand the nature of   language comprehension processes themselves .   Indeed , studies analyzing reading times have been   employed to explore a number of psycholinguistic   theories ( e.g. , Smith and Levy , 2013 ; Futrell et al . ,   2020 ; Van Schijndel and Linzen , 2021 ) .   One behavior revealed by such studies is the   tendency for humans to spend more timeon   the last word of a sentence or clause . While theexistence of such wrap - up effects is well - known   ( Just et al . , 1982 ; Hill and Murray , 2000 ; Rayner   et al . , 2000 ; Camblin et al . , 2007 ) , the cognitive   processes giving rise to them are still not fully   understood . This is likely ( at least in part ) due   to the dearth of analyses targeting naturalistic   sentence - final reading behavior . First , most studies   of online processing omit data from these words   to explicitly control for the confounding factors   wrap - up effects introduce ( e.g. , Smith and Levy ,   2013 ; Goodkind and Bicknell , 2018 ) . Second ,   the few studies on wrap - up effects rely on small   datasets , none of which analyze naturalistic text   ( Just and Carpenter , 1980 ; Rayner et al . , 2000 ;   Kuperberg et al . , 2011 ) . This work addresses this   gap , using several large corpora of reading time   data . Specifically , we study whether information-   theoretic concepts ( such as surprisal ) provide   insights into the cognitive processes that occur   at a sentence ’s boundary . Notedly , information-   theoretic approaches have been proven effective for   analyzing sentence - medial reading time behavior .   We follow the long line of work that has   connected information - theoretic measures and   psychometric data ( Frank et al . , 2015 ; Goodkind   and Bicknell , 2018 ; Wilcox et al . , 2020 ; Meister   et al . , 2021 , inter alia ) , employing similar methods   to build models of sentence- and clause - final RTs .   Using surprisal estimates from state - of - the - art lan-   guage models , we search for a link between wrap-   up effects and the information content within a   sentence . We find that the distribution of surprisals   of prior context is often predictive of sentence- and   clause - final reading times ( RTs ) , while not adding   significant predictive power to models of sentence-   medial RTs . This result suggests that the nature   of cognitive processes involved during the reading   of these boundary words may indeed be different   than those at other positions . Such findings lend   support to several prior hypotheses regarding   which processes may underlie wrap - up effects20(e.g . , the resolution of prior ambiguities ) , while   providing evidence against other speculations ( e.g. ,   that the time spent at sentence boundaries can be   quantified with a constant factor , independent of   the processing difficulty of the text itself ) .   2 The Process of Reading   Decades of research on reading behavior have   improved our understanding of the cognitive   processes involved in reading comprehension ( Just   and Carpenter , 1980 ; Rayner and Clifton , 2009 ,   inter alia ) . Here , we will briefly describe overar-   ching themes that are relevant for understanding   wrap - up effects .   2.1 Incrementality and its Implications   It is widely accepted that language processing is   incremental in nature , i.e. , readers process text   one word at a time ( Hale , 2001 , 2006 ; Rayner and   Clifton , 2009 ; Boston et al . , 2011 , inter alia ) . Con-   sequently , much can be uncovered about reading   comprehension via studies that analyze cognitive   processing at the word - level . Many pyscholin-   guistic studies make use of this notion , taking   per - word RTs in self - paced reading ( SPR ) or eye-   tracking studies to be a direct reflection of the pro-   cessing load of that word ( e.g. , Smith and Levy ,   2013 ; Van Schijndel and Linzen , 2021 ) . This   RT – processing effort relationship then allows us   to identify relationships between a word ’s pro-   cessing load and its attributes ( e.g. , surprisal or   length)—which in turn hints at the underlying cog-   nitive processes involved in comprehension . One   prominently studied attribute is word predictabil-   ity ; a notion naturally quantified by surprisal ( also   known as Shannon ’s ( 1948 ) information content ) .   Formally , the surprisal of a word wis defined as   s(w)=−logp(w|w ) , i.e. , a unit ’s negative   log - probability given the prior sentential context   w. Notedly , this operationalization provides a   way of quantifying how our prior expectations can   affect our ability to process a linguistic signal .   There are several hypothesis about the math-   ematical nature of the relationship between per-   word surprisal and processing load . While there   has been much empirical proof that surprisal es-   timates serve as a good predictor of word - level   RTs ( Smith and Levy , 2013 ; Goodkind and Bick-   nell , 2018 ; Wilcox et al . , 2020 ) , the data observedfrom sentence - final words appears not to follow the   same relationship . Specifically , in comparison to   sentence - medial words , sentence- or clause - final   words are associated with increased RTs in self-   paced studies ( Just et al . , 1982 ; Hill and Murray ,   2000 ) and both increased fixation and regression   times in eye - tracking studies ( Rayner et al . , 2000 ;   Camblin et al . , 2007 ) . Such behavior has also   been observed in controlled settings — for exam-   ple , Rayner et al . ( 1989 ) found that readers fixated   longer on a word when it ended a clause than when   the same word did not end a clause .   Such wide - spread experimental evidence sug-   gests sentence - final and sentence - medial reading   behaviors differ from each other , and that other   cognitive processes ( besides standard word - level   processing ) effort may be at play . Yet unfortunately ,   these wrap - up effects have received relatively little   attention in the psycholinguistic community : Most   reading time studies simply exclude sentence - final   ( or even clause - final ) words from their analyses ,   claiming that the ( poorly - understood ) effects are   confounding factors in understanding the reading   process ( e.g. , Frank et al . , 2013 , 2015 ; Wilcox   et al . , 2020 ) . Rather , we believe this data can   potentially provide new insights in their own right .   2.2 Wrap - up Effects   It remains unclear what exactly occurs in the mind   of the reader at the end of a sentence or clause .   Which cognitive processes are encompassed by the   term wrap - up effects ? Several theories have been   posited . First , Just and Carpenter ( 1980 ) hypoth-   esize that wrap - up effects include actions such as   “ the constructions of inter - clause relations . ” Second ,   Rayner et al . ( 2000 ) suggest they might involve   attempts to resolve previously postponed compre-   hension problems , which could have been deferred   in the hope that upcoming words would resolve   the problem . Third , Hirotani et al . ( 2006 ) posit the   hesitation when crossing clause boundaries is out   of efficiency ( Jarvella , 1971 ) ; readers do not want   to have to return to the clause later , so they take the   extra time to make sure there are no inconsistencies   in the prior text .   While some prior hypotheses have been largely   dismissed ( see Stowe et al . , 2018 for a more   detailed summary ) due to , e.g. , the wide - spread   support of theories of incremental processing ,   most others lack formal testing in naturalistic   reading studies . We attempt to address this gap.21Concretely , we posit the relationship between   text ’s information - theoretic attributes and its   observed wrap - up times can provide an indication   of the presence ( or lack ) of several cognitive   processes that are potentially a part of sentence   wrap - up . For example , high - surprisal words in the   preceding context may correlate with the presence   of ambiguities in the text ; they may also correlate   with complex linguistic relationships of the current   text with prior sentences — which are two driving   forces in the theories given above . Consequently ,   in this work , we ask whether the reading behavior   observed at the end of a sentence or clause can be   described ( at least partially ) by the distribution of   information content in the preceding context , as   this may give insights for several prior hypotheses   about wrap - up effects .   3 Language Models as Predictors of   Psychometric Data   Formally , a language model bpis a probability dis-   tribution over natural language sentences . In the   case when bpis locally normalized , which is the pre-   dominant case for today ’s neural language models ,   bpis defined as the product of conditional probabil-   ity distributions : bp(y ) = Qbp(y|y ) , where   eachbp(·|y)is a distribution with support over   linguistic units y(typically words ) from a set vocab-   ularyV , which includes a special end - of - sequence   token . Consequently , we can use bpto estimate in-   dividual word probabilities . Model parameters are   typically estimated by minimizing the negative log-   likelihood of a corpus of natural language strings   C , i.e. , minimizing L(bp ) = −Plogbp(y ) .   One widely embraced technique in information-   theoretic psycholinguistics is the use of these lan-   guage models to estimate the probabilities required   for computing surprisal ( Hale , 2001 ; Demberg and   Keller , 2008 ; Mitchell et al . , 2010 ; Fernandez Mon-   salve et al . , 2012 ) . It has even been observed that a   language model ’s perplexitycorrelates negatively   with the psychometric predictive power provided   by its surprisal estimates ( Frank and Bod , 2011 ;   Goodkind and Bicknell , 2018 ; Wilcox et al . , 2020 ) .   If these language models keep improving at their   current fast pace ( Radford et al . , 2019 ; Brown et al . ,   2020 ) , exciting new results in computational psy-   cholinguistics may follow , connecting reading be-   havior to the statistics of natural language .   Predicting Reading Times . In the computa-   tional psycholinguistics literature , the RT – surprisal   relationship is typically studied using predictive   models : RTs are predicted using surprisal estimates   ( along with other attributes such as number of char-   acters ) for the current word . The predictive power   of these models , together with the structure of the   model itself ( which defines a specific relationship   between RTs and surprisal ) , is then used as   evidence of the studied effect . While this paradigm   is successful in modeling sentence - medial RTs   ( Smith and Levy , 2013 ; Goodkind and Bicknell ,   2018 ; Wilcox et al . , 2020 ) , its effectiveness for   modeling sentence- and clause - final times is   largely unknown due to the omission of this data   from the majority of RT analyses .   A priori , we might expect per - word surprisal to   be a similarly powerful predictor of sentence and   clause - final RTs . Yet in Fig . 1 , we see that when   our baseline linear model ( described more precisely   in § 4 ) is fit to sentence - medial RTs , the residuals   for predictions of clause - final RTs appear to be   neither normally distributed nor centered around 0 .   Further , these trends appear to be different for eye-   tracking and SPR data , where the latter are skewed   towards lower values for all datasets . These re-22sults provide further confirmation that clause - final   data does not adhere to the same relationship with   RT as sentence - medial data , a phenomenon that   may perhaps be accounted for by additional fac-   tors at play in the comprehension of clause - final   words . Thus , we ask whether taking into account   information from the entire prior context can give   us a better model of these clause - final RTs .   To this end , we operationalize the information   contentin text w(of length T ) as:(w)=Ps(w)(k≥0 ) ( 1 )   where wmay be an entire sentence , or only its first   Twords . Notably , the case of k= 0 returns T ;   under k= 1 , we get the total information content   ofw . For k > 1 , moments of high - surprisal will   disproportionately drive up the value of(w ) .   Such words may indicate , e.g. , moments of   ambiguity or uneven distributions of information   in text . Thus , how well(w)(as a function of   k ) predicts model sentence- and clause - final RTs   may indicate which attributes of prior text ( if any )   can be linked to the additional cognitive processes   involved in wrap - up effects .   4 Experiments   Data . We use reading time data from 5 corpora   over 2 modalities : the Natural Stories ( Futrell et al . ,   2018 ) , Brown ( Smith and Levy , 2013 ) , and UCL   ( SP ) ( Frank et al . , 2013 ) Corpora , which contain   SPR data , as well as the Provo ( Luke and Chris-   tianson , 2018 ) , Dundee ( Kennedy et al . , 2003 ) and   UCL ( ET ) ( Frank et al . , 2013 ) Corpora , which con-   tain eye movements during reading . All corpora are   in English . For eye - tracking data , we take reading   time to be the sum over all fixation times on that   word . We provide an analysis of regression ( a.k.a .   go - past ) time in App . B. We provide further details   regarding pre - processing in App . A.   Estimating Surprisal . We obtain surprisal esti-   mates from three language models : GPT-2 ( Rad-   ford et al . , 2019 ) , TransformerXL ( Dai et al . , 2019 )   and a 5 - gram model , estimated using Modified   Kneser – Essen – Ney Smoothing ( Ney et al . , 1994 ) .   We compute per - word surprisal as the sum of sub-   word surprisals , when applicable . Additionally ,   punctuation is included in these estimates , although   see App . B for results omitting punctuation , whichare qualitatively the same . More details are given   in App . A.   Evaluation . Following Wilcox et al . ( 2020 ) and   Meister et al . ( 2021 ) , we quantify the predictive   power of a variable of interest ( ( w)here ) as   the mean difference in log - likelihood ∆LogLik of   a ( held - out ) data point when using a model with and   without that predictor . In other words , we train two   models to predict RTs — one with and one without   access to(w)—the difference in their pre-   dictive power is ∆LogLik . A positive ∆LogLik   value indicates the model with this predictor fits the   observed data more closely than a model without   this predictor . We use 10 - fold cross - validation to   compute ∆LogLik values so as to avoid overfitting ,   taking the mean across the held - out folds as our   final metric . Our baseline model for predicting per-   word RTs contains predictors for surprisal , unigram   log - frequency , character length , and the interaction   of the latter two . These values , albeit computed on   the previous word , are also included to account for   spill - over effects ( Smith and Levy , 2013 ) . Surprisal   from two words back is included for SPR datasets .   Unless otherwise stated , GPT-2 estimates are used   for baseline surprisal estimates in all models .   Results . Here we explore the additional predic-   tive power thatgives us when modeling   clause - final RTs . In Fig . 2 , we observe that often   the additional information provided by(w )   indeed leads to better models of clause - final RTs .   In most cases , at some value of k > 0leads   to larger gains in predictive power than k= 0 .   Ergo , the information content of the preceding   text is more indicative of wrap - up behavior than   length alone . Further , while often within standard   error,(w)atk > 1provides more predictive   power than at k= 1across the majority of datasets .   This indicates that unevenness in the distribution   of surprisal is stronger than the total surprisal con-   tent alone as a predictor of clause - final RTs . The   same experiments for sentence - medial words show   these quantities are less helpful when modeling   their RTs . Note that these effects hold above and   beyond the spill - over effects from the window im-   mediately preceding the sentence boundary . The   effect of the distribution of surprisal throughout the   sentence is stronger for eye - tracking data than for   SPR ; further , the trends are even more pronounced   when measuring regression times for eye - tracking   data ( see App . B).23   Notably , we see some variation in trends across   datasets . Due to the nature of psycholinguistic   studies , it is natural to expect some variation due   to , e.g. , data collection procedures or inaccuracies   from measurement devices . Another ( perhaps more   influential ) factor in the difference in trends comes   from the variation in dataset sizes . We see that   with the smaller datasets ( e.g. , UCL and Provo ) ,   there may not be enough data to learn accurate   model parameters . This artifact may manifest as   the noisiness or a lack of a significant increase   in log - likelihood ( on a held - out test set ) over the   baseline that we observe in some cases .   When considering prior theories of wrap - up   processes , these results have several implications .   For example , they can be interpreted as supporting   and extending Rayner et al . ’s ( 2000 ) hypothesis ,   which suggests the extra time at sentence bound-   aries is spent resolving prior ambiguities . In this   case , the observed correlation between wrap - up   times and(w)may potentially be linked to   two factors : ( 1 ) contextual ambiguities increasing   variation in per - word information content ; and ( 2 )   contextual ambiguities being resolved at clause   ends . On the other hand , these results provide   evidence against the hypothesis that the cognitive   processes occurring during the comprehension   of sentence - medial and clause - final words are the   same . Further , it also goes against Hirotani et al . ’s   ( 2006 ) hypothesis ( discussed in § 2.2 ) , as the dif-   ferences in sentence - medial and clause - final times   can not be purely quantified by a constant factor.5 Conclusion   We attempt to shed light on the nature of wrap - up   effects by exploring the relationship between   clause - final RTs and information - theoretic at-   tributes of text . We find that operationalizations of   the information contained in preceding context lead   to better predictions of these RTs , while not adding   significant predictive power for sentence - medial   RTs . This suggests that information - theoretic   attributes of text can shed light on the cognitive   processes happening during the comprehension of   clause - final words . Further , these processes may   indeed be different in nature than those required for   sentence - medial words . In short , our results pro-   vide evidence ( either in support or against ) about   several theories of the nature of wrap - up processes .   Ethics Statement   All studies involving human evaluations were con-   ducted outside of the scope of this paper . The   authors foresee no ethical concerns with the work   presented in this paper .   Acknowledgments   RC acknowledges support from the Swiss National   Science Foundation ( SNSF ) as part of the “ The   Forgotten Role of Inductive Bias in Interpretability ”   project . TP is supported by a Facebook Fellowship   Award . RPL acknowledges support from NSF grant   2121074.24References2526A Experimental Setup   A.1 Data Pre - processing   We use the Moses decodertokenizer and punctua-   tion normalizer to pre - process all text data . Some   of the Hugging Face tokenizers for respective neu-   ral models performed additional tokenization ; we   refer the reader to the library documentation for   more details . We determine clause - final words as   all those ending in punctuation . Capitalization was   kept intact albeit the lowercase version of words   were used in unigram probability estimates . We es-   timate unigram log - probabilities on WikiText-103   using the KenLM ( Heafield , 2011 ) library with de-   fault hyperparameters . We removed outlier word-   level reading times ( specifically those with a z-   score > 3when the distribution was modeled as   log - linear ) .   A.2 Surprisal Estimates   We use pre - trained neural language models to com-   pute most surprisal estimates . For reproducibil-   ity , we employ the model checkpoints provided   by Hugging Face ( Wolf et al . , 2020 ) . Specifi-   cally , for GPT-2 , we use the default OpenAI ver-   sion ( gpt2 ) ; for TransformerXL , we use a ver-   sion of the model ( architecture described in Dai   et al . ( 2019 ) ) that has been fine - tuned on WikiText-   103 ( transfo - xl - wt103 ) ; for BERT , we use the   bert - base - cased version . Notably , BERT mod-   els the probability of a word given both prior   andlater context , which means it can only give   us pseudo estimates of surprisal . Both GPT-2   and BERT use sub - word tokenization . We ad-   ditionally use surprisal estimates from a 5 - gram   model trained on WikiText-103 using the KenLM   ( Heafield , 2011 ) library with default hyperparame-   ters for Kneser – Essen – Ney smoothing . B Additional Results   B.1 Regression Times Analysis2728