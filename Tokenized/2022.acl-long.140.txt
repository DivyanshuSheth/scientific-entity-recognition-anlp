  Yanzeng Li , Jiangxia Cao , Xin Cong , Zhenyu Zhang   Bowen Yu , Hongsong Zhu , Tingwen LiuWangxuan Institute of Computer Technology , Peking University . Beijing , ChinaInstitute of Information Engineering , Chinese Academy of Sciences . Beijing , ChinaSchool of Cyber Security , University of Chinese Academy of Sciences . Beijing , China   liyanzeng@stu.pku.edu.cn   { caojiangxia , congxin , zhangzhenyu1996}@iie.ac.cn   { yubowen , zhuhongsong , liutingwen}@iie.ac.cn   Abstract   Chinese pre - trained language models usually   exploit contextual character information to   learn representations , while ignoring the lin-   guistics knowledge , e.g. , word and sentence   information . Hence , we propose a task - free en-   hancement module termed as Heterogeneous   Linguistics Graph ( HLG ) to enhance Chinese   pre - trained language models by integrating   linguistics knowledge . Speciﬁcally , we con-   struct a hierarchical heterogeneous graph to   model the characteristics linguistics structure   of Chinese language , and conduct a graph-   based method to summarize and concretize   information on different granularities of Chi-   nese linguistics hierarchies . Experimental re-   sults demonstrate our model has the ability   to improve the performance of vanilla BERT ,   BERTwwm and ERNIE 1.0 on 6 natural lan-   guage processing tasks with 10 benchmark   datasets . Further , the detailed experimental   analyses have proven that this kind of mod-   elization achieves more improvements com-   pared with previous strong baseline MWA .   Meanwhile , our model introduces far fewer   parameters ( about half of MWA ) and the   training / inference speed is about 7x faster   than MWA . Our code and processed datasets   are available at https://github.com/   lsvih / HLG .   1 Introduction   Pre - trained Language Models ( PLM ) ( Peters et al . ,   2018 ; Devlin et al . , 2019 ; Radford et al . , 2018 ;   Yang et al . , 2019 ) have recently demonstrated the   effectiveness on a variety of natural language pro-   cessing ( NLP ) tasks , such as machine translation   and text summarization . For a speciﬁc downstream   task , the parameters of PLMs can be ﬁne - tunedwith accurately labeled instances or weakly labeled   instances of the task to achieve better performance .   In recent , there are a series of studies on adapting   PLMs for Chinese ( Meng et al . , 2019 ; Sun et al . ,   2019 ; Cui et al . , 2019a ; Sun et al . , 2020 ; Wei et al . ,   2019 ; Diao et al . , 2020 ; Lai et al . , 2021 ) . Many re-   searchers introduce the Chinese - speciﬁc linguistics   knowledge such as word information into PLMs by   conducting elaborate self - supervised tasks to pre-   train Chinese PLMs from scratch . Nevertheless ,   pre - training a PLM is computationally expensive   and time - consuming since it needs large - scale Chi-   nese corpus and heavy computational resources .   The high cost makes it difﬁcult for researchers to   pre - train a PLM from scratch .   An alternative way is to integrate the Chinese-   speciﬁc linguistics knowledge into pre - trained   PLMs in the ﬁne - tuning stage in downstream tasks   directly . Following this idea , the task - free enhance-   ment module is widely used in the ﬁne - tuning stage   by adding an additional adapter in PLMs to in-   tegrate external knowledge ( Li et al . , 2020 ) . As   shown in Figure 1 , the enhancement module is in-   serted between PLMs and task - speciﬁc module ,   and its inputs are the hidden representations of   PLMs and embeddings of external knowledge . To   achieve the goal of integrating external knowledge   into PLMs in the ﬁne - tuning stage , the enhance-   ment module should have the following character-   istics . First , as a plug - in adapter module in ﬁne-   tuning stage , it should maintain consistent output   formulation with PLM . Second , it should not in-   troduce unacceptable time or space complexity for   training and inference . Third , it should improve the   performance of downstream tasks universally .   With the core idea of the enhancement mod-   ule , Li et al . ( 2020 ) proposed a multi - source word-   aligned model ( MWA ) to enhance PLMs by inte-   grating Chinese Word Segmentation ( CWS ) bound-1986   aries information implicitly . It ﬁrst exploits var-   ious CWS tools to generate multiple word se-   quences and then utilizes word - aligned attention   with a mixed pooling to integrate the word infor-   mation into characters . Experimental results show   that MWA has the ability to utilize CWS segmen-   tation information to enhance Chinese PLMs to   achieve SOTA performance in many downstream   NLP tasks . However , MWA has two weaknesses :   1)Efﬁciency Degradation : The model structure of   MWA is naturally non - parallel and can not beneﬁt   from GPU acceleration ( detailed in § 4.3.3 ) , which   results in time inefﬁciencies in both training and   inference processes . 2 ) Linguistic Information   Loss : MWA utilizes a pooling - based mechanism to   perform interaction between characters and words .   Such a heuristic method could not make full use of   information , resulting in sub - optimal results .   To tackle the aforementioned limitations , we pro-   pose Heterogeneous Linguistics Graph ( HLG ) ,   which is Graph Neural Network ( GNN ) based   method to integrate CWS information to enhance   PLMs . Speciﬁcally , the hierarchical CWS informa-   tion is ﬁrst conducted by a heterogeneous graph ,   which contains character nodes , word nodes and   sentence nodes . The edge between nodes indicates   the inclusion relationship of the grammatical struc-   ture between the linguistic hierarchies . Then , a   simple but effective multi - step information propa-   gation ( MSIP ) is proposed to incorporate the lin-   guistics knowledge of heterogeneous graph to en-   hance Chinese PLMs inductively . In this way , we   can obtain adequate information interaction among   characters , words and sentences . Furthermore , the   internal implementation of HLG is highly paral-   lelized , which is conducive to GPU accelerate and   raises the operating efﬁciency . In summary , we abstract out an adapter com-   ponent named enhancement module for PLMs to   integrate external knowledge during the ﬁne - tuning   stage . In this paradigm , we further introduce HLG   to integrate CWS information delicately and model   it via an effective MSIP . Extensive experiments   conducted on 10 benchmark datasets of 6 NLP   tasks demonstrate that our model outperforms the   BERT , BERTwwm and ERNIE 1.0 signiﬁcantly   and steadily . Comparing with MWA , a strong base-   line that also incorporates CWS information to en-   hance PLMs , our model achieves a steady improve-   ment with the same information . Meanwhile , com-   pared with previous work , MWA , our proposed   HLG introduces only half additional parameters   and the training / inference speed is about 7x faster .   2 Preliminaries   2.1 Pre - trained Language Model ( PLM )   As mentioned in § 1 , the pre - trained language mod-   els ( PLMs ) have achieved great success in many   NLP applications with the 2 - stage paradigm of pre-   training and ﬁne - tuning . The PLMs usually per-   form pre - training on large - scale unlabeled corpus   in virtue of self - supervised reconstruction tasks .   For example , BERT ( Bidirectional Encoder Repre-   sentations from Transformers ) ( Devlin et al . , 2019 )   is a typical well - known PLM , which conducts   masked language modeling and next sentence pre-   diction as pre - training tasks . After completing the   pre - training , the PLMs learn substantial contextual-   ized text representations , and then adapt ﬁne - tuning   on speciﬁc downstream tasks .   In Chinese NLP , PLMs are generally character-   based models ( Li et al . , 2019 ; Cui et al . , 2019a ) .   Speciﬁcally , given a character sequence :   S= [ c;c;:::;c ] ( 1 )   the outputs of Chinese PLMs can be treated as the   character - level representations H2R , where   thedis the dimension of representation .   2.2 Chinese Word Segmentation   As the same as most East - Asian languages , Chinese   language is written without explicit word delimiters   and the character is the smallest morpheme unit in   Chinese linguistic ( Cai and Zhao , 2016 ) . Although   character - based models could achieve good per-   formance ( Li et al . , 2019 ) , Li et al . ( 2020 ) point   out that introducing Chinese Word Segmentation1987   ( CWS ) information to character - based models can   effectively improve the model performance .   We give a formality deﬁnition of segmenter and   its partition strategy . Given a sentence consisting   of a sequence of characters as Eq . 1 , a segmenter   is deﬁned as :   S:S!S   whereis a partition strategy of sentence . Speciﬁ-   cally,partition and group the character sequence   Sinto the word sequence S :   (S ) = S= [ w;w;:::;w ] ( 2 )   wheremnandw= [ c;c;:::;c ]   is thei - th segmented word with a length of l   andsis the index of w ’s ﬁrst character in S.   Namely , the word wis a sequence of characters   fc;c;:::;cg , and the sentence Sis a se-   quence of wordsfw;w;:::;wg .   2.3 MWA for Enhancing Chinese PLM   Li et al . ( 2020 ) carried out researches on integrat-   ing CWS information into Chinese PLMs . The au-   thors brought an architecture named Multi - source   Word - aligned Attention ( MWA ) to incorporate   multi - granularity segmentation via pooling atten-   tion weights among characters within the word .   Formally , given a character sequence Sas Eq . 1   and its partition strategy as Eq . 2 . The character-   based representation Hcould be gained via PLM ,   MWA conducted self - attention between characters :   A = softmax(KW)(QW )   p   d   where QandKare both H , dis deﬁned in § 2.1 ,   andArepresents the attention score matrix . We   decompose Aover columns as [ a;a;:::;a ] ,   and then perform partition on it:(A ) =[ fa;ag;fag;:::fa;:::;ag:::;fa;ag ]   wheresandlare deﬁned in § 2.2 . Pooling each   group of partitioned columns :   a = MixPooling ( fa;:::;ag )   in which MixPooling is deﬁned in Yu et al .   ( 2014 ) . The gained A= [ a;a;:::;a]2   Ris the character - to - word attention weight   matrix . After performing alignment - wise multi-   ply ( Li et al . , 2020 ) between character - to - word at-   tention weight matrix Aand the character - based   representation H , the enhanced character - based rep-   resentation which integrates CWS information can   be obtained .   In essence , the MWA conducts interaction be-   tween characters and words via character - to - word   attention weight matrix A , implicitly summary   the information from characters , and performs Mix-   Pooling to aggregate the word - based segmentation   information and concrete the character - level repre-   sentation .   3 Heterogeneous Linguistics Graph   This section introduces the components of our   model HLG which instantiates the enhancement   module by exploiting the CWS information . We   ﬁrst brieﬂy explain the graph convolutional net-   work as our base encoder , and then describe the   graph construction of HLG . Finally , we give the   details of the multi - step information propagation   ( MSIP ) to integrate the CWS information into   PLMs .   3.1 Graph Convolutional Network   Graph Convolutional Network ( GCN ) ( Bruna et al . ,   2014 ; Kipf and Welling , 2017 ; Defferrard et al . ,   2016 ) is a powerful tool to extend the convolution   operation from the grid data to irregular graph data.1988The basic idea of GCN is to aggregate the represen-   tations of neighbors to obtain better representation   expression of nodes in the graph . For instance , con-   sider a homogeneous graph G= ( V;E)constructed   by nodes setVand edges setE.A2Ris a   binary adjacency matrix where each element A   denotes whether node ihas an edge with node j   in the edge setE. Formally , a standard GCN layer   can be abstracted as :   H=(^AHW);^A = Norm ( A)(3 )   where Hdenotes the input representation matrix ,   His the updated representation matrix , Norm (  )   means row normalizing function , ^Ais the normal-   ized adjacency matrix , ()is the ReLU function   andWis a parameter matrix . After such convolu-   tion operation , the representation Hwere aggre-   gated rely on edge connections deﬁned by A , and   transformed into Hby linear multiplication and   active function .   3.2 Graph Construction   We build a heterogeneous graph G= ( C;W;S;E )   to model the structure of Chinese linguistic , where   C , W , S , Edenote the character nodeset , word   nodeset , sentence nodeset and edge set , respec-   tively . Besides , different from homogeneous graph ,   HLG models relationship between three granulari-   ties of linguistic in a hierarchical way .   As presented in Figure 2 , Gis composed of three   hierarchies including characters , words and sen-   tences . In this case , we employed three different   CWS tools , and got three different segmentation re-   sults , which resulted in three sentences with slightly   different semantics . Note that the same word seg-   mentation results in the same position obtained   by different CWS tools will be regarded as the   same word node to enhance the interaction ( e.g. ,   Beijing and park in Figure 2 ) . This purpose is to   denoise the mistake word nodes brought by seg-   menter error . If a word is segmented by multiple   segmenters at the same time , the corresponding   word node will have a higher vertex degree . Such   nodes with higher betweenness centrality will lead   to a stronger inﬂuence on the followed information   propagation and achieve the effect of denoising   intuitively , like the vote - based multi - model ensem-   ble .   In HLG , only one adjacency matrix Ais not   enough to describe the hierarchical relationships be-   tween characters , words and sentences . Hence , we   conduct two interaction matrices A2R   andA2Rto indicate aforementioned   relationships . To be speciﬁc , we take the Aas   an example ( the one for Ais analogous ) , the   element Adenotes whether wordihas an edge   with characterjin the edge setE. Similar to Eq .   3 , we also denote normalized interaction matrices   as^Aand^A.   3.3 Multi - Step Information Propagation   To model the granularities hierarchical relation-   ships inG , we devise a multi - step information prop-   agation to learn the linguistics knowledge . In CWS ,   the partition and group processes could be consid-   ered as the partition of semantic representation and   the aggregation of separated semantic respectively   ( detailed in § 2.2 ) . Inspired by CWS processes , we   introduce two operations into MSIP to simulate   such processes and named as summarization and   concretization . Figure 3 shows the information   propagation procedure of MSIP .   Summarization . The summarization operation   focuses on generalizing hierarchical word and sen-   tence representations ( e.g. , from character - level to   word - level ) . Speciﬁcally , given a heterogeneous   graphGand corresponding character representa-   tions Hfrom PLM , the summarization operation   can be formulated as follows :   H=(^AHW ) ;   H=(^AHW);(4 )   where the W , Ware parameter matrices ,   H , Hare the interim representations of words   and sentences.1989Concretization . Concretization is the inverse op-   eration of summarization , it is used to repartition   the semantics from high - level to low - level ( e.g.   from sentence - level to word - level ) . To do so , we   ﬁrst calculate the normalized interaction matrices   ^Aand^A , which can be simply obtained   by ﬁrst transposed then normalized the predeﬁned   interaction matrices AandA , respectively .   Thus , we have :   ^A = Norm    ( A)   ;   ^A = Norm    ( A)   ; ( 5 )   where ( )is the transpose function . Afterward ,   the concretization operation is deﬁned as follows :   H   = (^AHW ) ;   H   = (^AHW);(6 )   where Wand Ware parameter matrices ,   H   andH   are also interim word and character   representations , Hdenote the ﬁnal word repre-   sentations deﬁned in Eq . 7 .   Skip Connection . Intuitively , it is difﬁcult to gen-   erate satisﬁed low - level representations from the   high - level representations directly . For example ,   it is easy to learn a few sentence representations   from dozens of word representations , but hard to   generate dozens of word representations from a few   sentence representations .   To mitigate this problem , in this paper , we in-   troduce the skip connection to enhance the MSIP ,   which is to simulate the self - loop in vanilla GCN .   As shown in Figure 3 , we add skip connections   between the summarization representations and the   concretization representations directly . Formally ,   the skip connection can be simply expressed as :   H = H   + (HW ) ;   H = H   + (HW);(7 )   where WandWare parameter matrices . Fur-   thermore , Hdenote the ﬁnal representations for   characters , which incorporates the ﬁne - grained lin-   guistics knowledge in G.   4 Experiments   4.1 Experimental Setting   For a fair comparison with MWA , which also gives   an enhancement module by incorporating CWS in-   formation . We conduct the same experiments onﬁve NLP tasks with various benchmark datasets .   Three frequently - used Chinese PLMs : BERT ( De-   vlin et al . , 2019 ) , ERNIE 1.0 ( Sun et al . , 2019 ) and   BERTwwm ( Cui et al . , 2019a ) are employed as   the basic PLM to enhance . Three CWS tools : thu-   lac ( Sun et al . , 2016a ) , ictclas ( Zhang et al . , 2003 )   and hanlp ( He , 2014 ) are employed to gain the seg-   mentation information . The time of pre - processing   including applying CWS tools is ignored in the ex-   perimental report . In the production , preprocessing   and inference can be asynchronously executed in   parallel ( while inference a batch of data , the subse-   quence data can be preprocessed with multiprocess )   ( Cheng et al . , 2019 ) , all three of the CWS tools   we ’ve introduced are fast enough to achieve this   effect . According to rough estimates and technical   reports , the processing speed of these tools are thu-   lac 1221KB / s , ictclas 769KB / s , hanlp 1375KB / s ,   respectively .   Speciﬁcally , we instantiate the enhancement   module as HLG and incorporate with downstream   task - speciﬁc model . To verify the effectiveness of   HLG , we execute 5 times ﬁne - tuning on 10 bench-   mark datasets of 6 NLP tasks and report the aver-   age score . The tasks include Sentiment Classiﬁca-   tion ( SC ) , Document Classiﬁcation ( DC ) , Named   Entity Recognition ( NER ) , Sentence Pair Match-   ing ( SPM ) , Natural Language Inference ( NLI ) and   Machine Reading Comprehension ( MRC ) . Speciﬁ-   cally , the following benchmark datasets are chosen   to evaluate the performance : 1 ) SC : ChnSentiand   weibo100ksentiment datasets are used for eval-   uating the capacity of short text classiﬁcation . 2 )   DC : THUCNews ( Sun et al . , 2016b ) dataset con-   tains 10 types of news for performing long text   classiﬁcation . 3 ) NER : Ontonotes 4.0 ( Weischedel   et al . , 2011 ) and MSRA - NER ( Levow , 2006a ) are   used for testing model in sequence tagging task . 4 )   SPM : LCQMC ( Liu et al . , 2018 ) and BQ ( Chen   et al . , 2018 ) are used to evaluate the text match-   ing ability of model . 5 ) NLI : We conduct experi-   ments on the Chinese part of XNLI ( Conneau et al . ,   2018 ) dataset , and adopt the same pre - processing   strategy as ERNIE ( Sun et al . , 2019 ) . 6 ) MRC :   Commonly used datasets DRCD ( Shao et al . , 2018 )   and CMRC2018 ( Cui et al . , 2019b ) are tested .   CMRC2018 is only evaluated on dev set as same   as ( Wei et al . , 2019 ; Sun et al . , 2020).1990   We implement the presented approach in Py-   Torch and ﬁne - tune the downstream tasks on multi-   ple Nvidia Tesla V100 GPUs . The basic architec-   ture of PLMs and pre - trained parameters are pro-   vided by Huggingface ( Wolf et al . , 2020 ) . The ini-   tial learning rate and other hyper - parameters refer   to the previous works reported ( Cui et al . , 2019a ;   Li et al . , 2020 ; Sun et al . , 2020 ) . Since the pa-   rameters of PLMs have been optimized , while the   parameters of HLG and the downstream tasks are   untrained . Hence , the learning rate of HLG part   is larger than PLM part , we manually tuned the   learning rates of PLM and HLG separately .   4.2 Experimental Results   The experimental results are shown in Table 1 .   Overall , we can observe that both HLG and MWA   outperform baseline models ( BERT , BERTwwm   and ERNIE 1.0 ) . Comparing with WMA , HLG   achieves further improvement and signiﬁcantly out-   performs baseline models on 10 tasks . In detail ,   HLG outperforms MWA on ChnSent , weibo100k ,   MSRA - NER , ontonotes , LCQMC , BQ , THUC-   News and XNLI tasks , and obtains comparable   results on DRCD and CMRC2018 datasets . For the text classiﬁcation tasks , namely SC   andDC , HLG respectively achieves 0.88 % and   0.84 % average improvement on ChnSenti and   weibo100k dataset , while MWA gains 0.53 % and   0.82 % . Meanwhile , HLG obtains 0.35 % improve-   ment on the long text multi - classiﬁcation bench-   mark THUCNews , and MWA gets 0.31 % points .   Comparing with text classiﬁcation tasks , the   improvements over NER tasks are more obvious .   The main reason may be that CWS explicitly pro-   vides the word boundaries , which are important   to recognize entities accurately . On the ontonotes   dataset , the promotion of HLG ( 1.28 % averagely )   is distinctly higher than that of MWA ( 0.92 % av-   eragely ) . Compared to the strong baseline models ,   the F1 scores of MSRA - NER have improved aver-   age 0.13 % and 0.10 % by HLG and MWA , respec-   tively .   HLG achieves the best results on the text match-   ing tasks ( SPM ) and its variant NLI , which brings   1.28 % average improvement to LCQMC , 0.26 %   average improvement to BQ , and 0.78 % average   improvement to XNLI . The improvements of HLG   are much higher than that of MWA ( 0.3 % , 0.23 %   and 0.55 % ) . As described in Chen et al . ( 2020 ) and1991   Lyu et al . ( 2021 ) , text matching tasks can beneﬁt   from the interaction between the paired sentences .   HLG follows them to construct graphs over sen-   tence pairs collectively , which naturally obtains   advantages in text matching tasks .   ForMRC task , HLG and MWA achieve com-   parable results on those datasets . HLG gets an   average improvement of 1.41 in EM and 0.85 in F1   score , while MWA gets 1.4 EM and 0.86 F1 score .   However , HLG has dominant advantage in training   speed and inference speed . Detail analysis of time   efﬁciency is in § 4.3.3 .   4.3 Analyses   4.3.1 Ablation Study   We conduct ablation experiments to explore the   effectiveness of the number of CWS tools . The   ablation experiments are organized on sentiment   classiﬁcation task , ChnSenti and weibo100k dev   set . As shown in Table 2 , 5 popular CWS tools   are added into our model successively according   to the order , and we also show the total number   of word nodes in our HLG . Meanwhile , the infor-   mation from multiple word segmentation tools can   be integrated at the same time without increasing   parameter size in HLG ( only the Ais changed ) .   Figure 4 shows the performance of BERT+HLG   with different numbers of CWS tools on ChnSenti   and weibo100k dev sets . Experimental results   demonstrate the effectiveness of introducing word   segmentation information . We can observe that   when the number of CWS tools is larger , the num-   ber of generated word nodes gradually increasing   to converge , and the performance of the model   slightly is not always increasing as the word count .   The more CWS tools introduced will bring more   diversity but also bring noise caused by segmenter   error . In practice , we ﬁnd using 4 or more CWS   tools can slightly increase the performance but take   much longer preprocessing time , hence we select   the elbow of the curve as the number of CWS tools .   That is , using 3 as the number of CWS tools might   be a balance between the performance of model   and the cost of preprocessing . This number also   coincides with the conﬁguration in MWA .   4.3.2 Parameter - Efﬁcient Analysis   In general , the enhancement module should be able   to bring performance improvements without unac-   ceptable space complexity . Therefore , we conduct   a comparative experiment on XNLI dev set to ex-   plore the performance improvement and the space   overhead between MWA and HLG .   To be speciﬁc , the number of parameters in   MWA depends on the dimension of PLM ’s rep-   resentation and the number of CWS tools K. Con-   cretely , MWA contains Ktransformer layers and   1 aggregation layer . Nevertheless , our HLG only   depends on the dimension of PLM ’s representation   and simply contains 4 basic GCN layers and 2 skip   connections . Thus , the number of parameters of1992   them can be calculated as :   size(MWA ) = K(4d ) + d   size(HLG ) = ( 4d)+ 2d   As discussed before , we employ 3 CWS tools   in both MWA and HLG . Table 3 reports the per-   formance of BERT , BERTwwm , and ERNIE 1.0   on the XNLI dev set . Obviously , HLG can get a   greater performance improvement with only half   additional parameters . It shows that as an enhance-   ment module , HLG is superior to MWA in terms   of parameter utilization efﬁciency .   In addition , to verify the impact of the addi-   tional parameters , we also conduct an ablation   experiment on XNLI dev set that utilizes the ran-   dom tokenizer , the single - character tokenizer , and   sole segmenter to obtain the different word seg-   mentation results , and send those results to HLG to   eliminate the additional beneﬁt from the change of   neural network structure and the increase of param-   eters . The results are shown in Table 4 , which indi-   cates that the increment of parameters can slightly   affect character - based model performance , and the   CWS information is signiﬁcantly useful to promote   the performance of character - based PLM .   4.3.3 Time Efﬁciency Analysis   Time efﬁciency is an important indicator in the   real - world production . Less training time and infer-   ence time means lower costs . In order to analyze   the additional time cost of different enhancement   modules , we conduct comparative experiments   among BERT , BERT+MWA , and BERT+HLG on   ChnSenti , LCQMC and XNLI datasets . For the   fair comparison , we remain other hyper - parameters   consistent for the three models .   As shown in Figure 5 , we compare time cost   during training and inference between vanilla   BERT , BERT+MWA and BERT+HLG . We can   observe that the training time and inference time of   BERT+HLG are basically consistent with vanilla   BERT . However , when MWA is introduced , the   average training time increases by 7 times , and   the average inference time increases by 7.6 times .   This is because MWA must calculate aligned atten-   tion weights token by token , and it can not beneﬁt   from CUDNN parallelization , resulting in terrible   operating efﬁciency . On the contrary , HLG is com-   posed of GCNs , and its internal implementation   is basically the simplest non - linear transformation .   Therefore , HLG could be maximally accelerated   through the optimized matrix operation of CUDNN   primitive , which only produces a negligible impact   on time efﬁciency .   5 Related Works   Pre - training language models , such as ELMo ( Pe-   ters et al . , 2018 ) , BERT ( Devlin et al . , 2019 ) , XL-   NET ( Yang et al . , 2019 ) and GPT ( Radford et al . ,   2018 ) , have shown their powerful performances on   various natural language processing tasks and have   been applied in many applications .   In recent past , there are studies adapting PLMs   for Chinese with Chinese - speciﬁc features such as   word information . Glyce ( Meng et al . , 2019 ) pro-   posed to use the glyph information of Chinese char-   acters to enhance PLMs . ERNIE 1.0/2.0 ( Sun et al . ,   2019 , 2020 ) and BERTwwm ( Cui et al . , 2019a )   used the whole word mask to learn the structure   of words or entities in the pre - training stage and   conducted more and better pre - training tasks to per-   ceive large - scale data . NEZHA ( Wei et al . , 2019 )   used a series of methods such as functional relative1993positional encoding and whole word masking to   improve the pre - training tasks , which had brought   improvement . ZEN ( Diao et al . , 2020 ) adopted   n - gram masking to enhance pre - trained encoder   and obtained outstanding performance . Lattice-   BERT ( Lai et al . , 2021 ) introduced word lattice in-   formation ( Zhang and Yang , 2018 ) into pre - training   framework via lattice position attention .   As a fundamental feature of Chinese , word seg-   mentation information is ﬂexibility , granularity ,   and easy - to - get ( Sproat and Emerson , 2003 ; Levow ,   2006b ) . Further , Zhang et al . ( 2018 ) ; Li et al . ( 2019 ,   2020 ) conducted detailed research and experiments   on the application of CWS in deep learning , and   found that CWS information can effectively im-   prove the performance of Chinese character - based   PLMs .   Recently , a lot of works have been proposed to   prompt NLP applications by constructing graph on   text and modeling with graph neural networks . Yao   et al . ( 2019 ) ﬁrst constructed word co - occurrence   graph between documents and introduced GCN to   modeling and aggregating document representa-   tion for text classiﬁcation . Chen et al . ( 2020 ) ; Lyu   et al . ( 2021 ) constructed lattice graph to maintain   multi - granularity information and external knowl-   edge in Chinese short text matching task . Nguyen   and Grishman ( 2018 ) proposed performing GCN   over dependency trees to extract event trigger . Sui   et al . ( 2019 ) conducted a character - word interaction   graph and performed graph attention network on   it to recognize Chinese named entities . Shu et al .   ( 2020 ) introduced a bipartite - graph based trans-   former PLM for integrating hierarchical semantic   information .   6 Conclusion   In this paper , we propose HLG which acts as the en-   hancement module to enhance Chinese PLMs with   CWS information . The HLG ﬁrstly constructs het-   erogeneous graph based on multiple word segmen-   tations to model the hierarchy of Chinese . Then ,   the MSIP is proposed to model the ﬁne - grained   linguistics knowledge of the heterogeneous graph .   Experimental results on 6 NLP tasks with 10 bench-   mark datasets demonstrate that the performance of   our model outperforms previous work , MWA . Be-   sides the performance improvements , HLG intro-   duces only half the additional parameters of MWA   and its training / inference speed is 7x faster than   MWA . At present , the experimental results of HLGare lagging behind SOTA , and we will try to mi-   grate it to some of the latest PLMs . Besides , HLG   has the expansibility to introduce the representation   layer of the CWS model directly , or introduce some   other information sources such as the knowledge   graph , etc . We leave these further improvements to   the future .   Acknowledgement   This work is supported by the National Key Re-   search and Development Program of China ( grant   No.2021YFB3100600 and 2020YFB2103803 ) , the   Strategic Priority Research Program of Chinese   Academy of Sciences ( grant No . XDC02040400 ) ,   the Youth Innovation Promotion Association of   CAS ( Grant No . 2021153 ) and National Natural   Science Foundation of China ( Grant No.61902394 ) .   This work performed while the ﬁrst author was at   IIE , CAS .   References199419951996