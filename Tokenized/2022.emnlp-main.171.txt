  Song Xu , Haoran Li , Peng Yuan , Youzheng Wu , Xiaodong He   JD AI Research   { xusong28 , lihaoran24}@jd.com   Abstract   Pre - trained Language Models ( PLMs ) have   shown effectiveness in various Natural Lan-   guage Processing ( NLP ) tasks . Denoising au-   toencoder is one of the most successful pre-   training frameworks , learning to recompose   the original text given a noise - corrupted one .   The existing studies mainly focus on injecting   noises into the input . This paper introduces   a simple yet effective pre - training paradigm ,   equipped with a knowledge - enhanced decoder   that predicts the next entity token with noises   in the prefix , explicitly strengthening the rep-   resentation learning of entities that span over   multiple input tokens . Specifically , when pre-   dicting the next token within an entity , we feed   masks into the prefix in place of some of the   previous ground - truth tokens that constitute the   entity . Our model achieves new state - of - the - art   results on two knowledge - driven data - to - text   generation tasks with up to 2 % BLEU gains .   1 Introduction   Pre - trained language models ( PLMs ) , such as   BERT ( Devlin et al . , 2019 ) , MASS ( Song et al . ,   2019 ) , BART ( Lewis et al . , 2020 ) , and T5 ( Raffel   et al . , 2020 ) , have had remarkable performances in   various Natural Language Processing ( NLP ) tasks   thanks to the use of denoising autoencoder pre-   training schema that is optimized to reconstruct the   original text given a noise - corrupted one . Despite   its common usage , to the best of our knowledge ,   existing work mainly focuses on injecting noises   into the encoding sequence , while the feasibility   of injecting noises into the decoding sequence for   PLMs remains an open question . This is the pri-   mary interest of this work .   On the other hand , recent researches ( Zhang   et al . , 2019 ; Peters et al . , 2019 ; Xiong et al . , 2020 ;   Wang et al . , 2019 , 2020 ; Ke et al . , 2020 ; Tian et al . ,   2020 ; Xu et al . , 2021 ) demonstrate that enhanc-   ing PLMs with real - world knowledge is crucialfor knowledge - driven downstream tasks , includ-   ing entity typing , relation classification , sentiment   analysis , and entity - related question answering .   However , the pre - training objectives for the above-   mentioned studies are usually designed for Natu-   ral Language Understanding ( NLU ) tasks . In this   work , we focus on knowledge - oriented sequence-   to - sequence ( Seq2Seq ) pre - training objectives for   PLMs , so that they can be applied to Natural Lan-   guage Generation ( NLG ) tasks , such as the data - to-   text task .   Given a noise - corrupted sentence , such as “ Tom   Cruise was born in [ MASK ] [ MASK ] [ MASK ] in   the year 1962 ” , the standard Seq2Seq pre - training   would predict the masked text fragment ( i.e. , “ New   York City ” ) token by token , while our proposed pre-   training would generate the correct output by learn-   ing facts regarding the concerned entity . We argue   that given merely “ Tom Cruise was born in ” , if a   PLM can immediately predict the masked fragment   to be “ New York City ” , it suggests that the PLM has   learned the fact ( “ Tom Cruise ” , “ born in ” , “ New   York City ” ) . In contrast , if the PLM could only pre-   dict “ York ” after being provided with the entity ’s   partial ground - truth token “ New ” , which is what   most existing PLMs are capable of , although it in-   deed correctly predicts the masked tokens , it does   not truly learn the fact or the entity of Tom Cruise ’s   birthplace . Thus , we propose PRefIx - masked   decoding for k Nowledge enhan CEd sequence - to-   sequence pre - training ( PRINCE ) , which decodes   entity tokens with noisy prefixes rather than ground-   truth tokens . For example , when predicting “ York ” ,   a mask symbol is fed into the decoder as the prefix ,   in place of “ New ” .   Different from the work of Chen et al . ( 2020 )   that generates knowledge - enriched text based on   a knowledge subgraph from WikiData , PRINCE   can directly incorporate entity knowledge into   PLMs based on the raw text with extracted en-   tities , alleviating error propagations throughout2675   the additional data processing , such as entity   alignment and knowledge triple retrieval . Af-   ter pre - training PRINCE , we evaluate it on two   data - to - text datasets that require entity knowledge :   WebNLG ( Shimorina and Gardent , 2018 ) and Wik-   iBio ( Liu et al . , 2018 ) . On both datasets , our model   achieves new state - of - the - art results .   Our main contributions are as follows :   •We present PRINCE that predicts entity to-   kens with masked prefixes , aiming to improve   the representation learning of entities that   span over multiple tokens .   •PRINCE exhibits new state - of - the - art perfor-   mances on two data - to - text tasks .   2 Background   We first introduce the existing denoising au-   toencoder schema for the Seq2Seq pre - training .   Given a sentence with a masked text fragment ,   MASS ( Song et al . , 2019 ) proposes a masked   Seq2seq pre - training objective that is optimized   to predict the masked tokens auto - regressively . Re-   cent work , such as BART ( Lewis et al . , 2020 ) and   T5 ( Raffel et al . , 2020 ) , has exhibited gains by ap-   plying a wide range of approaches to inject noises   into the input , such as sentence permutation , docu-   ment rotation , and text infilling . In this work , we   adopt the framework of MASS . PRINCE is simple   and flexible that we anticipate it would be able to   integrate to other denoising autoencoder schemas   with more sophisticated noising approaches .   Next , we will describe the basic framework   of the masked Seq2seq pre - training . Given asentence x={x , x , · · · , x},x =   { x , x , · · · , x}is a text span from xwhere   1≤p < q≤s.x is a sequence in which the   tokens ranging from xtoxare replaced by the   mask symbol M. The masked Seq2seq pre - training   model maximizes the conditional probability of   x : P(x |x ) .   The loss function Lfor each time tis the nega-   tive log likelihood of the token x∈x :   L=−logP(x|{x , · · · , x},x)(1 )   3 Our Model   PRINCE is built upon the masked Seq2seq pre-   training . The difference is that PRINCE pre-   dicts entity tokens with noises , in place of the   previous ground - truth entity token , as the decod-   ing prefix . The framework of PRINCE is shown   in Figure 1 . Formally , for a fragment of entity   x = { x , x , · · · , x } ∈x where   p≤m < n ≤q , PRINCE predicts x∈x   ( m < t ≤n ) by replacing the previous ground-   truth entity tokens { x , · · · , x}withM. The   loss is :   L=−logP(x|{x , · · · , x , M},x )   ( 2 )   where we denote the replaced sequence of entity   tokens as Mfor simplicity .   For the example shown in Figure 1 , the masked   entity fragment is { New , York , City } . When pre-   dicting “ York ” , for MASS , the previous ground-   truth sequence “ New ” is fed into the decoder . While2676for PRINCE , “ New ” is replaced by M. In this way ,   each token in the entity fragment is generated with-   out any indication from other entity tokens .   4 Experiments   4.1 Model Architecture   PRINCE applies the sequence - to - sequence Trans-   former architecture ( Vaswani et al . , 2017a ) , consist-   ing of a 12 - layer encoder and a 12 - layer decoder .   The size of hidden vectors is set to 1024 . We adopt   GELU activation ( Hendrycks and Gimpel , 2016 ) .   We use Adam optimizer ( Kingma and Ba , 2015 )   with a learning rate of 3e-5 , β= 0.9 , β= 0.98 ,   weight decay of 0.01 . The dropout probability is   0.1 . The maximum sequence length is set to 512 .   Pre - training takes 5 days with 8 Telsa V100 GPUs .   We use the beam search with a beam size of 4 for   the inference . Other hyper - parameters can be found   in our code .   4.2 Pre - training Dataset   We use English Wikipedia as the source of our pre-   training data , which is aligned to Wikidata . Entity   tokens can be extracted through the alignment .   We set the max length of the sentence to 256 and   abandon sentences containing less than three enti-   ties . To obviate data leakage during pre - training ,   we discard the pre - training data overlapped with   the samples in downstream datasets . In the end , we   obtain 14 GB of pre - training data .   4.3 Pre - training Details   PRINCE prioritizes masked fragments covering en-   tity tokens , and the number of the masked tokens   is set to 30 % of the length of the input sentence .   When injecting noises into the decoder , following   Devlin et al . ( 2019 ) , the noise will be a mask sym-   bol 80 % of the time , a random token 10 % of the   time , and an unchanged token in the rest of the time .   We adopt the architecture of BART large model .   4.4 Fine - Tuning on Data - to - Text Tasks   WebNLG ( 2.0 ) Dataset ( Shimorina and Gardent ,   2018 ) This dataset takes RDF triples as input and   outputs a textual description .   WikiBio Dataset ( Lebret et al . , 2016 ) This dataset   takes a Wikipedia infoboxes table as input and out-   puts a biography description.4.5 Experimental Results   4.5.1 Results on WebNLG   The results on the WebNLG dataset are shown   in Table 1 . Shimorina and Gardent ( 2018 ) ap-   plySeq2Seq model with attention ( Luong et al . ,   2015 ) as the baseline and address rare words by   delexicalization ( Delex ) and copying ( Copy ) . The   GCN model ( Marcheggiani and Perez - Beltrachini ,   2018 ) adopts a graph convolutional networks   based generator . The KGPT model ( Chen   et al . , 2020 ) is a knowledge - grounded pre - training   model with graph ( Graph ) or sequential ( Seq ) en-   coders . Our implementations are based on Trans-   former ( Vaswani et al . , 2017b ) . We first evaluate   theBART ( Lewis et al . , 2020 ) pre - trained with En-   glish Wikipedia ( 14 GB ) and pre - trained with data   used in BART ( 160 GB ) , respectively , For our pro-   posed pre - training method , we pre - train PRINCE   from scratch and warm - start PRINCE with BART   ( pre - trained with 160 GB data ) .   As we can see , pre - training PRINCE from   scratch with 14 GB data already achieves compa-   rable performance with BART pre - trained with   160 G data . Pre - training PRINCE based on the   BART initialization leads to the best performance ,   which significantly improves the results over the   original BART model ( + 2.25%/0.70%/1.58 % for   BLEU / METEOR / ROUGE - L scores , paired t - test ,   p - value<0.01).26774.5.2 Results on WikiBio   The results on the WikiBio dataset are shown in   Table 2 . Table NLM ( Lebret et al . , 2016 ) is a   table - conditioned neural language model . Order-   Planning ( Sha et al . , 2018 ) is an order - planning   text generation model . Field - Gating ( Liu et al . ,   2018 ) is a structure - aware seq2seq model with   the field - gating encoder . KBAtt ( Chen et al . ,   2019 ) enhances data - to - text model with external   background knowledge . Hierarchical+Auxiliary   Loss ( Liu et al . , 2019 ) is a data - to - text model   trained with multiple auxiliary objectives .   First , we can find that the models with pre-   training outperform the models without pre-   training , while the improvement is smaller than   that on the WebNLG dataset , which can be as-   cribed to the larger size of the WikiBio dataset .   Second , similar to the results on the WebNLG   dataset , pre - training PRINCE with the BART ini-   tialization brings significant improvements com-   pared to other models ( + 1.23 % for BLEU score   over BART , paired t - test , p - value<0.01 ) .   4.6 Further Analysis   4.6.1 Can PRINCE Generate Entities Better ?   PRINCE aims to enhance the representation learn-   ing of entities that span over multiple tokens . Look-   ing into the WebNLG datasets , we observe that   97.61 % entities are composed of multiple tokens af-   ter BPE ( Sennrich et al . , 2016 ) preprocessing . Can   PRINCE generate these entities better ? To answer   this question , we perform manual evaluations with   100 examples from the test set of WebNLG . Threeannotators are involved in deciding whether the   generated entities are faithful and readable . The re-   sults are shown in Table 3 , where faithfulness refers   to that the generated text accurately expresses the   true meaning of the input , and readability refers to   that the generated text is easy to understand . The   results depict that PRINCE outperforms other mod-   els .   4.6.2 Benefit of Entity - oriented Noises   We evaluate PRINCE ( BART initialized ) with dis-   tinct noising strategies , including injecting noises   for entities ( our main model ) , injecting noises for   common tokens , and no noising . The results in Ta-   ble 4 demonstrate the superiority of entity - oriented   noising against other strategies .   4.6.3 Case Study   Table 5 illustrates an example from the WebNLG   dataset . PRINCE with common token noising strat-   egy generates a text with wrong team name of “ los   angeles ” and missing information of “ draft round   2 ” . By contrast , PRINCE with entity token noising   successfully expresses exactly the same meaning   as the references .   5 Discussions   5.1 Motivation of PRINCE   Denoising autoencoder pre - training can train the   model to learn language representations by trans-   ferring a noise - corrupted input back to the orig-   inal state . Injecting noises into the decoding se-   quence makes this process more challenging and   strengthens the robustness of the decoder . More-   over , knowledge - oriented noises force the model to2678   predict the knowledge with noisy context . Specif-   ically , the noises are injected when the decoder   predicts the entity tokens , and the previously gen-   erated partial entity tokens are unseen for the latter .   In that case , the decoder needs to predict the com-   plete entity tokens without of any clues from the   entity itself , which can motivate the model to learn   better to predict the entity relying solely on the   context . In this way , we argue that our model can   enhance the representation learning of knowledge   and the ability of knowledge reasoning .   5.2 Type of Knowledge to be Injected Noises   While in this work , we regard entities stored in   Wikidata as the knowledge to be masked , PRINCE   is actually not designed for any specific type of   knowledge . Other knowledge , such as lexical re-   lation ( Lauscher et al . , 2019 ; Wang et al . , 2020 ) ,   sentiment words ( Ke et al . , 2020 ; Tian et al . , 2020 ) ,   keywords ( Li et al . , 2020b ; Xu et al . , 2020 ) , entail-   ment ( Eichler et al . , 2017 ; Li et al . , 2018 ) , and do-   main attribute schema ( Li et al . , 2020a ; Zhu et al . ,   2020 ) , would be compatible with our model as   well .   5.3 Comparison with ProphetNet   The motivation of ProphetNet ( Qi et al . , 2020 )   partially resembles that of PRINCE . ProphetNet   predicts future n - gram based on an n - stream self-   attention mechanism that contains a main stream   andnpredicting stream . Only the main stream   is maintained for fine - tuning on the downstream   tasks . In contrast , the PRINCE decoder is con-   sistent throughout , bridging the gap between pre - training and fine - tuning . Besides , ProphetNet pre-   dicts a fixed n - step ahead , while PRINCE is more   flexible designed for knowledge fragments with   variable lengths .   6 Conclusion and Future Work   We propose PRINCE , a Seq2Seq pre - training   model equipped with a knowledge - enhanced de-   coder that predicts entity tokens with masked pre-   fixes . PRINCE achieves new state - of - the - art results   on two data - to - text datasets . In the future , we will   adopt PRINCE to other pre - training schemas and   more knowledge - driven tasks . Our code is publicly   available .   Limitations   A limitation of our work is that it is designed for   entity - oriented text generation tasks ( i.e. , data - to-   text tasks ) , where the text is generated from struc-   tural data , such as RDF triples and infoboxes table .   Based on the observation of our work , we can con-   clude that the performance of pre - training models   can be improved for data - to - text tasks , while the   improvements for other general text - to - text tasks   are sometimes not significant . Thus , it asks for fur-   ther explorations on whether universal pre - training   with denoising common token noises is helpful for   general text - to - text tasks .   Second , we consider entities as the knowledge in   our work , and the pre - training aims to learn better   representations for the entities . We argue that a   broader definition of the knowledge may lead to a   much wider set of application scenarios .   In addition , various types of noises injected into   the input text have been proved effective ( Lewis   et al . , 2020 ; Raffel et al . , 2020 ) , while we only   test with noises in a simple form . We believe that   more complex noises are potentially extended to   the decoder .   Acknowledgements   This work was supported by the National   Key R&D Program of China under Grant No .   2020AAA0108600 . We thank the anonymous re-   viewers for their helpful comments and sugges-   tions.2679References26802681