  Patrick FernandesKayo YinEmmy Liu   André F. T. MartinsGraham NeubigLanguage Technologies Institute , Carnegie Mellon University , Pittsburgh , PAInstituto Superior Técnico & LUMLIS ( Lisbon ELLIS Unit ) , Lisbon , PortugalInstituto de Telecomunicações , Lisbon , PortugalUniversity of California , BerkeleyUnbabel , Lisbon , Portugal   Abstract   Although proper handling of discourse signif-   icantly contributes to the quality of machine   translation ( MT ) , these improvements are not   adequately measured in common translation   quality metrics . Recent works in context - aware   MT attempt to target a small set of discourse   phenomena during evaluation , however not in a   fully systematic way . In this paper , we develop   theMultilingual Discourse- Aware ( MDA )   benchmark , a series of taggers that identify and   evaluate model performance on discourse phe-   nomena in any given dataset . The choice of   phenomena is inspired by a novel methodology   to systematically identify translations requiring   context . We confirm the difficulty of previously   studied phenomena while uncovering others   that were previously unaddressed . We find that   common context - aware MT models make only   marginal improvements over context - agnostic   models , which suggests these models do not   handle these ambiguities effectively . We re-   lease code and data for 14 language pairs to   encourage the MT community to focus on ac-   curately capturing discourse phenomena .   1 Introduction   In order to properly translate discourse phenom-   ena including anaphoric pronouns , lexical cohe-   sion , and discourse markers , a machine translation   ( MT ) model must use information from previous   utterances ( Guillou et al . , 2018 ; Läubli et al . , 2018 ;   Toral et al . , 2018 ) .   However , while generating proper translations   of these phenomena is important for comprehen-   sion , they represent a small portion of words in   natural language . Therefore , common metrics such   as BLEU ( Papineni et al . , 2002 ) can not be used to   judge the quality of discourse translation .   Table 1 : Some representative works on contextual ma-   chine translation that perform evaluation on discourse   phenomena , contrasted to our work . For a more com-   plete review see Maruf et al . ( 2021 ) .   Recent work on neural machine translation   ( NMT ) models that attempt to incorporate extra-   sentential context ( Tiedemann and Scherrer , 2017 ;   Miculicich et al . , 2018 ; Maruf and Haffari , 2018 ,   inter alia ) often perform targeted evaluation of cer-   tain discourse phenomena , mostly focusing on el-   lipsis , formality ( V oita et al . , 2019b , a ) , and pro-   noun translation ( Müller et al . , 2018 ; Bawden et al . ,   2018 ; Lopes et al . , 2020 ) . However , only a lim-   ited set of discourse phenomena for a few language   pairs have been studied ( see summary in Table 1 ) .   The difficulty of broadening these studies stems   from the reliance of previous work on introspec-   tion and domain knowledge to identify the relevant   discourse phenomena , frequently involving expert   speakers , which then requires engineering complex   language - specific methods to create test suites or   manually designing data for evaluation .   In this paper , we identify sentences that contain   discourse phenomena through a data - driven , semi-   automatic methodology . We apply this method to   create a multilingual benchmark testing discourse   phenomena in the domain of MT . First , we develop   P - CXMI ( § 2 ) as a metric to identify when context   is helpful in MT , or more broadly text generation in   general . Then , we perform a systematic analysis of   words with high P - CXMI to find categories of trans-606lations where context is useful ( § 3 ) . We identify   novel discourse phenomena that to our knowledge   have not been addressed previously ( e.g. consis-   tency of verb forms ) , without requiring a - priori   language - specific knowledge . Finally , we design   a series of methods to automatically tag words be-   longing to the identified classes of ambiguities ( § 4 )   and we evaluate existing translation models for dif-   ferent categories of ambiguous translations ( § 5 ) .   We examine a parallel corpus spanning 14 lan-   guage pairs , measuring translation ambiguity and   model performance . We find that the context-   aware methods , while improving on standard evalu-   ation metrics , only perform significantly better than   context - agnostic baselines for certain discourse   phenomena in our benchmark . Our benchmark pro-   vides a more fine - grained evaluation of translation   models and reveals weaknesses of context - aware   models , such as verb form cohesion . We also find   that DeepL , a commercial document - level transla-   tion system , does better in our benchmark than its   sentence - level ablation and Google Translate . We   hope that the released benchmark and code , as well   as our findings , will spur targeted evaluation of dis-   course phenomena in MT to cover more languages   and more phenomena in the future .   2 Measuring Context Usage   2.1 Cross - Mutual Information   Past work on contrastive evaluation has examined   correct and incorrect translations of specific dis-   course phenomena ( Bawden et al . , 2018 ; Müller   et al . , 2018 ) , but this provides only a limited mea-   sure of context usage on phenomena defined by   the creators of the dataset . We are therefore inter-   ested in devising a metric that is able to capture all   context usage by a model , beyond a predefined set .   Conditional Cross - Mutual Information ( CXMI )   ( Bugliarello et al . , 2020 ; Fernandes et al . , 2021 )   measures the influence of context on model predic-   tions at the corpus level . CXMI is defined as :   CXMI ( C→Y|X ) =   H(Y|X)−H(Y|X , C ) ,   where XandYare a source and target sentence ,   respectively , Cis the context , His the entropy   of acontext - agnostic MT model , and Hrefers   to acontext - aware MT model . This quantity can   be estimated over a held - out set with Nsentencepairs and their respective context as :   CXMI ( C→Y|X)≈   −1   N / summationdisplaylogq(y|x )   q(y|x , C )   Importantly , the authors find that training a sin-   glemodel qas both the context - agnostic and   context - aware model ensures that non - zero CXMI   values are due to context and not other factors ( see   Fernandes et al . ( 2021 ) and § 3.1 for details ) .   Although this approach is promising , it is de-   fined only at a corpus level : as the previous equa-   tion shows , CXMI is estimated by over a full set   of sentences . Since we are interested in measuring   how important context is for single sentences or   words within a sentence , we extend this definition   to capture lower - level context dependency in the   next section .   2.2 Context Usage Per Sentence and Word   Pointwise Mutual Information ( P - MI ) ( Church and   Hanks , 1990 ) measures the association between   two random variables for specific outcomes . Mu-   tual information can be seen as the expected value   of P - MI over all possible outcomes of the variables .   Taking inspiration from this , we define the Point-   wise Cross - Mutual Information ( P - CXMI ) for a   source , target , context triplet ( x , y , C ) as :   P - CXMI ( y , x , C ) = −logq(y|x )   q(y|x , C )   Intuitively , P - CXMI measures how much more   ( or less ) likely a target sentence yis when it is   given context C , compared to not being given that   context . Note that this is estimated according to   the models qandqsince , just like CXMI ,   this measure depends on their learned distributions .   We can also apply P - CXMI at the word level to   measure how much more likely a particular word   in a sentence is when it is given the context , by   leveraging the auto - regressive property of the neu-   ral decoder . Given the triplet ( x , y , C ) and the   word index i , we can measure the P - CXMI for that   particular word as :   P - CXMI ( i , y , x , C ) = −logq(y|y , x )   q(y|y , x , C )   Note that nothing constrains the form of Cor even   xand P - CXMI can , in principle , be applied to any   conditional language modelling problem.607   We use this metric to find words that are strongly   context - dependent , which is to say that their like-   lihood increases greatly with context relative to   other words . These words are the ones that likely   correspond to discourse phenomena .   3 Which Translation Phenomena Benefit   from Context ?   To identify salient translation phenomena that re-   quire context , we perform a thematic analysis   ( Braun and Clarke , 2006 ) , examining words with   high P - CXMI across different language pairs and   manually identifying patterns and categorizing   them into phenomena where context is useful for   translation .   To do so , we systematically examined ( 1 ) the   mean P - CXMI per part - of - speech ( POS ) tag , ( 2 )   the words with the highest mean P - CXMI across   the corpus , and ( 3 ) the individual words with the   highest P - CXMI in a particular sentence .   3.1 Data & Model   To compare linguistic phenomena that arise during   document - level translation across language pairs ,   we use a dataset consisting of TED talks ’ tran-   scripts and translations ( Qi et al . , 2018 ) . We use   this dataset due to its abundance of discourse phe-   nomena , as well as its availability across many   parallel languages . We study translation between   English and Arabic , German , Spanish , French , He-   brew , Italian , Japanese , Korean , Dutch , Portuguese ,   Romanian , Russian , Turkish and Mandarin Chi-   nese . These 14 target languages are chosen for their   high availability of TED talks and linguistic tools ,   as well as for the diversity of language types in our   comparative study ( Table 4 in Appendix B ) . For   each language pair , our dataset contains 113,711parallel training sentences from 1,368 talks , 2,678   development sentences from 41 talks , and 3,385   testing sentences from 43 talks .   To obtain the P - CXMI for words in the data ,   we train a small Transformer ( Vaswani et al . , 2017 )   model for every target language and incorporate the   target context by concatenating it with the current   target sentence ( Tiedemann and Scherrer , 2017 ) .   We train the model with dynamic context size ( Fer-   nandes et al . , 2021 ) , by sampling 0 - 3 target context   sentences and estimating P - CXMI by using this   model for qandq(details in Appendix G ) .   3.2 Analysis Procedure   We start our analysis by studying POS tags with   high mean P - CXMI . In Appendix C , we report   the mean P - CXMI for selected POS tags on test   data . Some types of ambiguity , such as dual form   pronouns ( § 3.3 ) , can be linked to a single POS tag   and be identified at this step , whereas others require   finer inspection .   Next , we inspect the vocabulary items with high   mean P - CXMI . At this step , we can detect phenom-   ena that are reflected by certain lexical items that   consistently benefit from context for translation .   Finally , we examine individual tokens that ob-   tain the highest P - CXMI . In doing so , we iden-   tify patterns that do not depend on lexical features ,   but rather on syntactic constructions for example .   In Table 2 , we provide selected examples of to-   kens that have high P - CXMI and the discourse   phenomenon we have identified from them .   3.3 Identified Phenomena   Through our thematic analysis of items with high   P - CXMI , we identified various types of translation   ambiguity . Unlike previous work , our method re-   quires no prior knowledge of languages and easily608scales to new languages ( § 4.4 ) .   Although this procedure may find phenomena   that are intuitive to the annotators , the data - driven   approach makes confirmation bias less severe than   works relying on introspection . Hence , our proce-   dure can allow us to discover relevant phenomena   that have not been previously addressed , such as   verb forms . Examples of each phenomenon are   given in Table 2 .   3.3.1 Lexical Cohesion   Entities may have multiple possible translations   in the target language , but the same entity should   be referred to by the same word in a translated   document . This is called lexical cohesion .   3.3.2 Formality   We identify two phenomena which fall under the   general category of formality . First , several lan-   guages we examined have a T - V distinction ( Ap-   pendix B , “ Pronouns Politeness ” ) in which the   second - person pronouns a speaker uses to refer   to someone depend on the relationship between the   speaker and the addressee .   Second , languages such as Japanese and Korean   use honorifics to indicate formality , which are spe-   cial titles or words expressing courtesy or respect   for position .   3.3.3 Pronoun Choice   Unlike in English , many languages use gendered   pronouns for pronouns other than the third - person   singular , or assign gender based on formal rules   rather than semantic ones . In order to assign the   correct pronoun , it is therefore necessary to use   the previous context to distinguish the grammatical   gender of the antecedent .   3.3.4 Verb Form   While English verbs may have five forms , other   languages may have a more fine - grained verb mor-   phology . For example , English has only a single   form for the past tense , while the Spanish past tense   consists of six verb forms . Verbs must be translated   using the verb form that reflects the tone , mood and   cohesion of the document .   3.3.5 Ellipsis   Ellipsis refers to the omission of superfluous words   that are able to be inferred from the context . For   instance , in the last row of Table 2 , the Englishtext does not repeat the verb know in the second   sentence as it can be understood from the previous   sentence . However , in Turkish , there is no natural   way to translate the verb - phrase ellipsis , so context   is important for translating the verb correctly .   4 Cross - phenomenon MT Evaluation   Next , we we develop a series of methods   to automatically tag tokens belonging to these   classes of ambiguous translations and propose   theMultilingual Discourse- Aware ( MuDA ) bench-   mark for context - aware MT models .   4.1 MT Evaluation Framework   Given a pair of parallel source and target docu-   ments ( X , Y ) , our MuDA tagger assigns one or   more tags from a set of discourse phenomena   { t , · · · , t}to each target token y∈Y. Using   thecompare - mt toolkit ( Neubig et al . , 2019 ) , we   compute the mean word f - measure of system out-   puts compared to the reference for each tag . This   allows us to identify which discourse phenomena   models can translate more or less accurately .   4.2 Automatic Tagging   We now describe our taggers for each identified dis-   course phenomenon . Note that these do notrequire   C - XMI to be calculated , and are based on reliable   methods for identifying each phenomenon men-   tioned in subsection 3.3 . For formality , pronoun   choice and verb form , we created language - specific   word lists that were verified by native speakers .   Not all phenomena are present in each language .   Phenomena that are absent are indicated in Ap-   pendix D , as a zero count for that language .   Lexical Cohesion To tag words that re-   quire lexical cohesion , we first extract609word alignments from a parallel corpus   D = { ( X , Y),···,(X , Y ) } , where   ( X , Y)denote the source and target reference   document pair . We use the AWESOME aligner   ( Dou and Neubig , 2021 ) to obtain :   A={⟨x , y⟩ |x↔y , x∈X , y∈Y } ,   where each xandyare the lemmatized content   source and target words and ↔denotes a bidirec-   tional word alignment . For each target word y   that is aligned to source word x , if the alignment   pair⟨x , y⟩occurred at least 3times already in the   current document , excluding the current sentence ,   we tag yfor lexical cohesion .   Formality For languages with T - V distinction , we   tag the target pronouns containing formality distinc-   tion if there has previously been a word pertaining   to the same formality level in the same document .   Some languages such as Spanish often drop the   subject pronoun , and T - V distinction is instead   reflected in the verb form . For these languages ,   we use spaCy ( Honnibal and Montani , 2017 ) and   Stanza ( Qi et al . , 2020 ) to find POS tags and detect   verbs with a second - person subject in the source ,   and conjugated in the second ( T ) or third ( V ) per-   son in the target .   For languages with more complex honorifics sys-   tems , such as Japanese , we construct a word list of   common honorifics - related words to tag ( details in   Appendix F.3 ) .   Pronoun Choice To find pronouns in English   that have multiple translations , we manually con-   struct a list P={⟨p , p⟩}for each language   ( Appendix F.2 ) , where each pis an English pro-   noun and pthe list of possible translations of   pin the language ℓ. Then , for each aligned to-   ken pair ⟨x , y⟩ , ifx , yare both pronouns with   ⟨x , p|y∈p⟩ ∈P , and the antecedent of x   isnotin current sentence , we tag yas an am-   biguous pronoun . To obtain antencedents , we use   AllenNLP ( Gardner et al . , 2017 ) ’s coreference res-   olution module . This procedure is similar to Müller   et al . ( 2018 ) .   Verb Form For each target language , we define   a list V={v , · · · , v}of verb forms ( Appendix   F.3 ) where v∈Vif there exists a verb form in   English uand an alternate verb form v̸=vin   the target language such that an English verb with   formumay be translated to a target verb with formvorvdepending on the context . Then , for each   target token y , ifyis a verb of form v∈V , and   another verb with form vhas appeared previously   in the same document , we tag yas ambiguous .   Ellipsis To detect translation ambiguity due to VP   and NP ellipsis , we look for instances where the el-   lipsis occurs on the source side , but not on the target   side , which means that the ellipsis must be resolved   during translation . Since existing ellipsis models   are limited to specific types of ellipsis , we first train   an English ( source - side ) ellipsis detection model .   To do so , we extract an ellipsis dataset from the   English data in the Penn Treebank ( Marcus et al . ,   1993 ) and train a BERT text classification model   ( Devlin et al . , 2019 ) , which achieves 0.77 preci-   sion and 0.73 recall ( see Appendix F.4 for training   details ) . Then , for each sentence pair where the   source sentence is predicted to contain an ellipsis ,   we tag the word yin the target sentence Yif : ( 1 )   yis a verb , noun , proper noun or pronoun ; ( 2 ) y   has occurred in the previous target sentences of the   same document ; ( 3 ) yis not aligned to any source   words , that is , ̸ ∃x∈Xs.t.⟨x , y⟩ ∈A.   4.3 Evaluation of Automatic Tags   We apply the MuDA tagger to the reference trans-   lations of our TED talk data . We thus obtain an   evaluation set of 3,385 parallel sentences for each   of the 14 language pairs . In Appendix C we report   the mean P - CXMI for each language and MuDA   tag . Overall , we find higher P - CXMI on tokens   with a tag compared to those without , which pro-   vides empirical evidence that models indeed rely   on context to predict words with MuDA tags .   Appendix D shows that the frequency of tags   varies significantly across languages . Overall , only   4.5 % of the English sentences have been marked   for ellipsis , giving an upper bound for the number   of ellipsis tags in other languages . We find that   languages from a different family than English have   a relatively high number of ellipsis tags . We also   find that Korean and especially Japanese have more   formality tags than languages with T - V distinction ,   which reflects that register is more often important   when translating to languages with honorifics .   Manual Evaluation To evaluate our tagger , we   asked native speakers with computational linguis-   tics backgrounds to manually verify MuDA tags for   8 languages on 50 randomly selected utterances as   well as all words tagged with ellipsis in our corpus .   This allows us to measure how many automatic610   tags violate the given definition of the linguistic   tag . Table 3 reports the tags ’ precision .   For all languages , we obtain high precision for   all tags except ellipsis , confirming that the method-   ology can scale to languages where no native speak-   ers were involved in developing the tags . For ellip-   sis , false positives often come from one - to - many or   non - literal translations , where the aligner does not   align all target words to the corresponding source   word . We believe that the ellipsis tagger is still   useful in selecting difficult examples that require   context for translation ; despite the low precision ,   we find a significantly higher P - CXMI on ellipsis   words for many languages ( Appendix C ) .   4.4 Extension to New Languages   While MuDA currently supports 14 language pairs ,   our methodology can be easily extended to new lan-   guages . The lexical andellipsis tags can be directly   applied to other languages provided a word aligner   between English and the new target language . The   formality tag can be extended by adding a list of   pronouns or verb forms related to formality in the   new language . Similarly , the pronouns andverb   forms tag can also be extended by providing a list   of ambiguous pronouns and verb forms .   Exhaustively listing all relevant phenomena in   document - level MT is extremely complex and be-   yond the scope of our paper . To identify new dis-   course phenomena on other languages , our the-   matic analysis can be reused as follows : ( 1 ) Train a   model with dynamic context size on translation be-   tween the new language pair ; ( 2 ) Use the model to   compute P - CXMI for words in a parallel document-   level corpus of the language pair ; ( 3 ) Manually   analyze the POS tags , vocabulary items and indi-   vidual tokens with high P - CXMI ; ( 4 ) Link patterns   of tokens with high P - CXMI to particular discourse   phenomena by consulting linguistic resources.5 Exploring Context - aware MT   Our MuDA tagger can be applied to documents   in the supported languages to create benchmark-   ing datasets for discourse phenomena during trans-   lation . We use our benchmark of the TED talk   dataset enhanced with MuDA tags to perform an   exploration of context usage across languages with   4 models , including commercial systems .   5.1 Trained Models   We train a sentence - level and document - level   concatenation - based small transformer ( base ) for   every target language . While conceptually sim-   ple , concatenation approaches have been shown to   outperform more complex models when properly   trained . For the context - aware model , the major   difference from § 3.1 is that we use a static context   size of 3 , since we are not using these models to   measure P - CXMI . ( Lopes et al . , 2020 ) .   To evaluate stronger models , we additionally   train a large transformer model ( large ) that was   pretrained on a large , sentence - level corpora , for   German , French , Japanese and Chinese . Further   details can be found in Appendix G.   5.2 Commercial Models   To assess if commercially available machine trans-   lation engines are able to leverage context and   therefore do well in MuDA , we consider two en-   gines:(1 ) the Google Cloud Translation v2 API .   In early experiments , we assessed that this model   only does sentence - level translation , but included it   due to its widespread usage ; ( 2 ) the DeepL v2 API .   This model advertises its usage of context as part   of translations and our experiments confirm this .   Early experimentation with other providers ( Ama-   zon and Azure ) indicated that these are not context-   aware so we refrained from evaluating them .   To obtain provider translations , we feed the docu-   ments into an API request . To re - segment the trans-   lation into sentences , we include special marker   tokens in the source that are preserved during trans-   lation and split the translation on those tokens . We   also evaluate a sentence - level version of DeepL   where we feed each sentence separately to compare   with its document - level counterpart.611   5.3 Results and Discussion   Figure 2 shows results for base models , trained ei-   ther without ( no - context ) or with context , and   for the latter with either predicted ( context ) or   reference context ( context - gold ) during de - coding . Results are reported with respect to stan-   dard MT metrics BLEU ( Papineni et al . , 2002 ) and   COMET ( Rei et al . , 2020 ) , as well as the MuDA   benchmark . The corpus - level metrics BLEU and   COMET are calculated over the entire corpus ,   rather than just the sentences tagged by MuDA .   First , we find that BLEU scores are highest for   context - gold models for most language pairs ,   but context - agnostic models have higher COMET   scores . Moreover , in terms of mean word f - measure   overall , we do not find significant differences be-   tween the three systems . It is therefore difficult to   see which system performs the best on document-   level ambiguities using only corpus - level metrics .   For words tagged by MuDA as requiring context   for translation , context - aware models often achieve   significantly higher word f - measure than context-   agnostic models on certain tags such as ellipsis   andformality , but not on other tags such as lexi-   calandverb form . This demonstrates how MuDA   allows us to clarify which inter - sentential ambigui-   ties context - aware models are able to resolve .   For the pretrained large models ( Figure 3 ) ,   context - aware models perform better than the   context - agnostic on corpus - level metrics , espe-   cially COMET . On words tagged with MuDA ,   context - aware models generally obtain the high-612   est f - measure as well , particularly when given ref-   erence context , especially on phenomena such as   lexical andpronouns , but improvements are less   pronounced than on corpus - level evaluation .   Among commercial engines ( Figure 4 ) , DeepL   outperforms Google on most metrics and language   pairs . The sentence - level ablation of DeepL per-   forms worse than its document - level system for   most MuDA tags .   Current context - aware MT systems translate   some inter - sentential discourse phenomena well ,   but are unable to consistently obtain significant im-   provements over context - agnostic counterparts on   challenging MuDA data . Tables with all results can   be found in Appendix H.   6 Related Work   Several works have worked on measuring the per-   formance of MT models on contextual discourse   phenomena . The first example of this was done   by Hardmeier et al . ( 2010 ) , which evaluated au-   tomatically the precision and recall of pronoun   translation in statistical MT systems . Jwalapuram   et al . ( 2019 ) proposed evaluating models on pro-   noun translation based on a pairwise comparison   between translations that were generated with and   without context , and later Jwalapuram et al . ( 2020)extended this work to include more languages and   phenomena in their automatic evaluation / test set   creation . These works rely on prior domain knowl-   edge and intuition to identify context - aware phe-   nomena , whereas we take a systematic , data - driven   approach .   Most works have focused on evaluating perfor-   mance in discourse phenomena through the use   ofcontrastive datasets . Müller et al . ( 2018 ) auto-   matically create a dataset for anaphoric pronoun   resolution to evaluate MT models in EN→DE .   Bawden et al . ( 2018 ) manually creates a dataset   for both pronoun resolution and lexical choice in   EN→FR . V oita et al . ( 2018 , 2019b ) creates a   dataset for anaphora resolution , deixis , ellipsis and   lexical cohesion in EN→RU . However , Yin et al .   ( 2021 ) suggest that translating anddisambiguating   between two contrastive choices are inherently dif-   ferent , motivating our approach in measuring direct   translation performance .   7 Conclusions and Future Work   We investigate types of ambiguous translations   where MT models benefit from context using our   proposed P - CXMI metric . We perform a data-   driven thematic analysis across 14 languages to   identify context - sensitive discourse phenomena,613some of which ( such as verb forms ) have not been   previously addressed in work on MT . In compari-   son to previous work , our approach is systematic ,   extensible , and does not require prior knowledge   of the language . Additionally , the P - CXMI metric   can be used to identify other context - dependent   words in generation . We construct the MuDA   benchmark that tags words in parallel corpora and   evaluates models on 5 context - dependent phenom-   ena . Our evaluation reveals that context - aware and   commercial translation systems achieve small im-   provements over context - agnostic models on our   benchmark , and we encourage further development   of models that improve on context - aware transla-   tion .   Limitations   While MuDA relies on set of hand - crafted rules for   tagging specific phenomena , these rules might in-   volve the use of other error - prone systems ( such as   coreference resolution and alignment models ) and   these errors might be susceptible to problems ( such   as lack of out - of - domain generalization ) that could   limit the applicability of our tagger . However , this   could be fixed by extending MuDA to use newer   and better versions of these systems .   The use of F-1 per tag with surface - form match-   ing between reference / translation can also lead to   penalizing translations that use context correctly   but choose other equivalent words . Nevertheless ,   this should also be mitigable by extending the scor-   ing method to , for example , match synonyms .   Finally , the benchmarking of context - aware mod-   els might not apply to newer , state - of - the - art trans-   lation models , especially if these leverage large   language models that were trained on long - context   data .   Acknowledgements   We would like to thank Uri Alon , Ipek Baris ,   George Bejinariu , Hiba Belkadi , Chloé Billiotte ,   Giovanni Campagna , Remi Castera , V olkan Cirik ,   Taisiya Glushkova , Junxian He , Mert Inan , Alina   Karakanta , Benno Krojer , Emma Landry , Chany-   oung Park , Artidoro Pagnoni , Maria Ryskina ,   Odette Scharenborg , Melanie Sclar , Jenny Seok ,   Emma Schippers , Bogdan Vasilescu for advice on   various languages and help with manual annota-   tions .   We would also like to thank all the members of   DeepSPIN and NeuLab who provided feedback onearlier versions of this work . This work was sup-   ported by the European Research Council ( ERC   StG DeepSPIN 758969 ) , by EU ’s Horizon Europe   Research and Innovation Actions ( UTTER , con-   tract 101070631 ) , by the P2020 program MAIA   ( LISBOA-01 - 0247 - FEDER-045909 ) , by the Por-   tuguese Recovery and Resilience Plan through   project C645008882 - 00000055 ( NextGenAI , Cen-   ter for Responsible AI ) , and by the Fundação   para a Ciência e Tecnologia through contracts   SFRH / BD/150706/2020 and UIDB/50008/2020 .   References614615616   A MuDA Toolkit Usage   To tag an existing dataset and extract the tags for later use , run the following command : python muda / main.py \ --src /path / to / src \ --tgt /path / to / tgt \ --docids /path / to / docids \ --dump - tags /tmp / maia_ende.tags \ --tgt - lang lang   To evaluate models on a particular dataset ( reporting per - tag metrics dicussed in this paper ) , run the   following command : python muda / main.py \ --src /path / to / src \ --tgt /path / to / tgt \ --docids /path / to / docids \ --hyps /path / to / hyps.m1 /path / to / hyps.m2 \ --tgt - lang lang   B Language Properties   Table 4 summarizes the properties of the languages analyzed in this work .   C P - CXMI Results   Table 5 presents the average P - CXMI value per POS tag and per MuDA tag .   D Tag Numbers   Table 6 lists the counts of each tag per language .   E Tagging other Document - level Datasets   We report the number of tags found for two other document - level datasets commonly used in the literature :   ( 1 ) IWSLT-17 ( Cettolo et al . , 2012 ) test sets for EN→DEandEN→FRand ( 2 ) A randomly subsampled   portion of the news - commentary dataset for EN→ { AR , DE , ES , FR , NL , PT , RU , ZH}(Barrault et al . ,   2019 ) . These results can be found respectively in Figure 5 and Figure 6.617618   F Tagger Details   F.1 Formality Words   Table 7 gives the list of words related to formality for each target language .   F.2 Ambiguous Pronouns   Table 8 provides English pronouns and the list of possible target pronouns .   F.3 Ambiguous Verbs   Table 9 lists verb forms that may require disambiguation during translation .   F.4 Ellipsis Classifier   We train a BERT text classification model ( Devlin et al . , 2019 ) on data from the Penn Treebank , where we   labeled each sentence containing the tag ‘ * ? * ’ as containing ellipsis ( Bies et al . , 1995 ) . We obtain 248,596   sentences total , with 2,863 tagged as ellipsis . Then , our model using HuggingFace Transformers ( Wolf619pronouns formality verb form lexical ellipsis   ar 90 0 0 116 982   de 398 1000 0 19 1356   es 245 86 409 15 1496   fr 1591 839 1938 48 1586   he 0 0 468 122 1210   it 182 118 484 31 1320   ja 245 3328 0 94 990   ko 0 221 0 71 373   nl 0 783 1060 27 1590   pt_br 372 515 0 27 1677   ro 60 407 792 53 1002   ru 0 466 2091 41 668   tr 0 30 47 137 704   zh_cn 0 526 0 49 1092   et al . , 2020 ) . To address the imbalance in labels , we up - weight the loss for samples tagged as ellipsis by a   factor of 100 .   G Training details   Thetransformer - small model has hidden size of 512 , feedforward size of 1024 , 6 layersa and 8 attention   heads . The transformer - large model has hidden size of 1024 , feedforward size of 4096 , 6 layers , 16   attention heads .   As in Vaswani et al . ( 2017 ) , we train using the Adam optimizer with β= 0.9andβ= 0.98and   use an inverse square root learning rate scheduler , with an initial value of 10forlarge model and   5×10for the base andmulti models , with a linear warm - up in the first 4000 steps .   For the pretrained models we used Paracrawl ( Esplà et al . , 2019 ) for German and French , JParacrawl   ( Morishita et al . , 2020 ) for Japanese and the Backtranslated News from WMT2021 for Chinese .   Due to the sheer number of experiments , we use a single seed per experiment .   We base our experiments on the framework Fairseq ( Ott et al . , 2019 ) .   H Results Tables620621622623624ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Last section in page 9 ( unnumber )   /squareA2 . Did you discuss any potential risks of your work ?   Work does n’t have immediate ethical risk   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1 and Abstract   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4 and 5   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4 and 5   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Datasets used are commonly used by the community , and the ( permissive ) license for our tagger is in   the ofﬁcial code repository   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Datasets used are commonly used by the community   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4   C / squareDid you run computational experiments ?   Section 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 5.1 and Appendix F625 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 5.1 and Appendix F   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4 and 5   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 4.3   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   We provide a brief description ( but not full text instructions ) in section 4.3   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Section 4.3   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Section 4.3626