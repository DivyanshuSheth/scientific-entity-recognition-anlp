  Henrik Voigt , Özge Alacam , Monique Meuschke , Kai Lawonnand Sina ZarrießUniversity of JenaUniversity of BielefeldUniversity of Magdeburgfirst.last@uni-jena.defirst.last@uni-bielefeld.delast@isg.cs.uni-magdeburg.de   Abstract   Natural language as a modality of interaction   is becoming increasingly popular in the field of   visualization . In addition to the popular query   interfaces , other language - based interactions   such as annotations , recommendations , expla-   nations , or documentation experience growing   interest . In this survey , we provide an overview   of natural language - based interaction in the   research area of visualization . We discuss a   renowned taxonomy of visualization tasks and   classify 119 related works to illustrate the state-   of - the - art of how current natural language in-   terfaces support their performance . We exam-   ine applied NLP methods and discuss human-   machine dialogue structures with a focus on   initiative , duration , and communicative func-   tions in recent visualization - oriented dialogue   interfaces . Based on this overview , we point   out interesting areas for the future application   of NLP methods in the field of visualization .   1 Introduction   Natural language as a modality for interacting   with visual models enjoys increasing popularity   in human - computer interface research in the fields   of Human - Computer - Interaction ( HCI ) and Visu-   alization ( VIS ) ( Yu and Silva , 2020 ; Srinivasan   et al . , 2020a ; Liu et al . , 2021 ; Narechania et al . ,   2021 ; Kim et al . , 2021c ) . At the same time , interest   in tasks involving the visual modality has grown   strongly in NLP research in recent years ( Suhr et al . ,   2017 ; Hudson and Manning , 2019 ; Acharya et al . ,   2019 ) . While there are common interests and paral-   lel trends in VIS and NLP , research in these fields   often adopts different perspectives on what inter-   action is and how it should be modeled . Broadly   speaking , in the VIS community , a lot of research   aims to understand why users need to interact with   a visualization and what users ’ intents are when   they interact with a visual model ( Dimara and Perin ,   2020 ) . Therefore , Brehmer and Munzner ( 2013)categorize users ’ data - related intents into visual-   ization tasks and introduce a taxonomy to describe   them in general terms and compare them among   applications . Recent contributions show that dif-   ferent forms of natural language - based interaction   prove suitable to support users in accomplishing   various visualization tasks . This concerns not only   the popular query interfaces , but also , on a broader   scale , the provision of recommendations , annota-   tions , explanations , documentations , or for support   in analytical conversation . However , the variety of   existing visualization tasks benefiting from natural   language interaction beyond simple query inter-   faces has not yet received much attention in the   NLP community . On the other hand , modern NLP   methods offer enormous potential for modeling   multi - modal dialogues in the visualization tasks .   In this survey , we aim to complement the why-   oriented perspective of classifying visualization   tasks by intent in VIS with the how - oriented di-   alogue modeling perspective in NLP for works   involving natural language interaction . To sub-   stantiate the classification of the papers , we first   delimit the scope of the survey and explain the   methodology employed to derive the selected pa-   pers in Section 2 . We discuss the taxonomy of   abstract visualization tasks by Brehmer and Mun-   zner ( 2013 ) as the basis for the classification in   Section 3 by explaining why interaction with a vi-   sualization is performed . Section 4 focuses on how   interaction is implemented in the works at hand in   terms of applied NLP methods as well as character-   istic structures in human - machine dialogue ( Bunt   et al . , 2010 ) . Finally , challenges arising from cur-   rent approaches are pointed out . As such , the need   to compile reliable data sets to support the adoption   of deep learning - based NLP methods in the field   of VIS yields promising space for future creative   work.348   Related Surveys . The survey of Shen et al .   ( 2021 ) considers 55 visualization - oriented natural   language interfaces ( V - NLI ) at the intersection of   NLP and VIS . The work focuses on the applica-   bility of natural language interfaces at the differ-   ent steps of the information visualization pipeline   by Kard et al . ( 1999 ) . The authors discuss query-   based language interfaces in detail . Similarly , Öz-   can et al . ( 2020 ) put their focus on querying data   visualizations using natural language . However ,   querying a visualization interface is only one pos-   sible language - based interaction among many such   asannotation , description generation , documenta-   tion , and others . Klopfenstein et al . ( 2017 ) conduct   a more general study on the application of conver-   sational interfaces and derive usage patterns and   paradigms for their implementation . Further re-   lated work is done at the intersection of Machine   Learning ( ML ) and VIS as by Wu et al . ( 2021 )   or Wang et al . ( 2020 ) , who illuminate where and   how ML gains ground in VIS and discuss future   directions for applying ML in VIS research . Fac-   ing that , we identify substantial ground for a sur-   vey that examines current natural language inter-   action techniques supporting the accomplishment   of visualization tasks . Related work in VIS yields   comprehensive and well - conducted state - of - the - art   surveys focusing on where dialogue systems can   be integrated into the information visualizationpipeline proposed by Kard et al . ( 1999 ) . This work   is complementary in that it illuminates from an   NLP perspective how visualization - oriented dia-   logue is structured in terms of initiative , duration ,   and present communicative functions within the re-   spective visualization task at hand ( see Figure 1 ) .   By shedding a light on this we hope to arouse in-   terest in the NLP community for the interesting   multi - modal dialogue modeling tasks emerging at   the intersection of NLP and VIS .   2 Methodology   For a paper to be included in the survey paper se-   lection , it must meet the following criteria :   •Language - based interaction must be a desig-   nated input / output modality and some kind   of language interface must be provided for it ,   e.g. , a text box or a microphone / speaker .   •Language - based interaction must serve to ful-   fill or support a main visualization task . For   example , using natural language for logging   into an application is neither a visualization   tasknordoes it support the accomplishment   of that task and is , therefore , not valid . In   contrast , using natural language to annotate   certain aspects of a visualization isconsid-   ered supportive of achieving the goal of the   visualization task and therefore is valid .   In addition to contributions that include concrete   implementations of interaction scenarios , theoreti-   cal papers that discuss design spaces or consider-   ations of language - based interaction possibilities   are included . The aim is to explicitly show not   only what has already been implemented , but also   which interaction possibilities are conceivable and   useful in multi - modal visualization - oriented dia-   logue . The paper selection is made in a two - stage   process . First , a set of seed papers is derived from   conference proceedings of the main conferences in   HCI , namely SIGCHI , VIS , namely IEEE VIS , Paci-   ficVIS , andEuroVIS , and NLP , namely ACL and   EMNLP , starting from the year 2010 until 2021 .   The papers are filtered using the keywords lan-   guage , visualization , interface in combination with   a semantic embedding map of the abstracts based   on Reimers and Gurevych ( 2019 ) . The exploratory   process results in a set of 76 papers . In the second   stage , the references of the seed papers are exam-   ined and relevant papers that meet the specified349criteria are included in the set . This results in a   final set of 119 papers . For a detailed insight into   the scope of the survey , we refer to Appendix A.   3 Why Users interact with Visualizations   Brehmer and Munzner ( 2013 ) introduce a multi-   level typology for abstract description and compari-   son of visualization tasks between applications . An   abstract visualization task represents a high - level   description of whyinteraction with a visualization   application is performed , how it is performed , and   what the input and output of the task are . The why-   branch of Brehmer and Munzner ’s typology was   chosen primarily for three reasons : First , the high   level of abstraction allows to cover a high number   of visualization tasks and therefore ensures high   representativeness . On the other hand , the modular   character of the typology is beneficial for break-   ing down complex tasks into smaller subtasks in   which commonalities can then be identified . In ad-   dition , the combination with the what and the how   branch offers the possibility to describe task chains ,   which can serve as a blueprint for the design of a   dialog with the system . The papers are classified   on the basis of the why - branch of the taxonomy   because it distinguishes the tasks taking into ac-   count the goal to be achieved and thus corresponds   to the goal definition as also used in goal - oriented   dialogue modeling ( Bordes et al . , 2016 ; Li et al . ,   2017 ; Liu et al . , 2018 ) . The why branch spawns the   abstract visualization tasks present , discover , en-   joy , and produce illustrated in Figure 2 . Following   Munzner ( 2009 ) , we consider language - based in-   teraction as domain- and interface - independent   operations performed by users and/or systems   by applying natural language in any kind of   representation , e.g , written- or spoken text . Ta-   ble 1 shows an overview of the contributions and   the respective visualization task to which they are   assigned . In the following subsections , concrete   tasks involving natural language interaction are pre-   sented for each abstract visualization task of the   taxonomy . Each section includes a brief definition   of the targeted visualization task and a detailed dis-   cussion of current related work that addresses it .   For a detailed inspection , we refer to Appendix B.   3.1 Present Task   Brehmer and Munzner ( 2013 ) define presentation   as’the use of visualization for the succinct com-   munication of information , for telling a story with   data , guiding an audience through a series of cogni-   tive operations ’ . During this task , natural language   is used to complement the presentation of visual   findings and results , for data - driven storytelling , or   to explain , evaluate and discuss them .   Visual Storytelling . Visual storytelling consid-   ers the communication of knowledge to a broad   audience using visual and textual elements that fol-   low a coherent narrative . The main idea is to match   linguistic and visual elements and arrange them   consistently within a story . V on Landesberger et   al . ( 2021 ) study how text and visualization inter-   act with each other in a visual storytelling scenario   pointing out that visualizations complement the   narrative by providing overview , details , and com-   parison . Automatic story generation is done by Shi   et al . ( 2021 ) who leverage the generation of a vi-   sual story from spreadsheet input . Natural language   text drives the story of a visualization presentation   in Kwon et al . ( 2014 ) ; Bryan et al . ( 2017 ) ; Metoyer   et al . ( 2018 ) . Users are guided through the story   by interacting with the text segments and the sys-   tem creates visual animations correspondingly in   response .   Explanation Generation . Visualizations offer   great potential to create understanding for com-   plex issues among different user groups . In con-   trast to storytelling , explanation generation is not   about assigning a sequence of visual elements to a   text - based story , but about automatically explaining   given visual facts through natural language texts .   Combining text and visualization is used to ex-   plain complex processes , e.g. , in verbalizing the   functionality of ML models ( Hohman et al . , 2019).350Visualization Task Subtask # Representative Paper(s )   Present Visual Storytelling 6 ( Kwon et al . , 2014 ; Metoyer et al . ,   2018 )   Explanation Generation 3 ( Sevastjanova et al . , 2018 ; Hohman   et al . , 2019 )   Discover Keyword Search 15 ( Isaacs et al . , 2014 ; Feng et al . , 2018 ;   Schleußinger and Henkel , 2018 )   Querying 45 ( Setlur et al . , 2016 ; Yu and Silva ,   2020 ; Narechania et al . , 2021 )   VQA 3 ( Mathew et al . , 2021 ; Chaudhry et al . ,   2020 )   Browsing 7 ( Setlur et al . , 2020 ; Luo et al . , 2020 ;   Srinivasan and Setlur , 2021 )   Enjoy Augmentation 12 ( Srinivasan et al . , 2019b ; Hullman   et al . , 2013 )   Description Generation 14 ( Obeid and Hoque , 2020 ; Hsu et al . ,   2021 )   Produce Annotation 7 ( Chen et al . , 2010b ; Ren et al . , 2017 )   Documentation 1 ( Nafari and Weaver , 2015 )   Visualization Creation 6 ( Cui et al . , 2020 ; Fulda et al . , 2016 )   Sevastjanova et al . ( 2018 ) discuss strategies on how   to present language explanations during the model   inference process and the interaction techniques to   be required , such as details - on - demand , guidance ,   dialogue , and exploration .   3.2 Discover Task   Using natural language to discover information is   one of the most common visualization tasks tar-   geted by V - NLI . Brehmer and Munzner ( 2013 ) dif-   ferentiate between different levels of task granular-   ity such as discover - search - query ( see Figure 2 ) .   The discovery of concepts , objects , and relation-   ships in a visualization depends on the role that   the user and the interface take in the visualization-   oriented dialogue , as well as on the concreteness   of the user ’s intent . Intents are formulated in oral   or written form . Less concrete user intents lead to   a more exploratory character of the search . Con-   crete intents formalized in a query lead to a spe-   cific system response . Vague and fuzzy intents are   much more difficult to formalize in a single query   and must be inferred by the V - NLI through the ap-   plication of intelligent recommendations or user   guidance . Keyword Search . Discovering information   about a visualization by supplementing it with a   keyword search interface is examined by Feng   et al . ( 2018 ) . The authors enable the search of   visual concepts in a 2D visualization via text input .   Further visualization - oriented keyword search   interfaces are applied in Chowdhury et al . ( 2021 ) ;   Siddiqui and Hoque ( 2020 ) ; Chung et al . ( 2010 ) .   In contrast to that , visual search interfaces take   in keywords but focus on displaying results in a   way that facilitates visual exploration , as targeted   in Wilson et al . ( 2010 ) ; Schleußinger and Henkel   ( 2018 ) ; Peltonen et al . ( 2017 ) . Search history and   coverage are tracked and visualized by Isaacs et al .   ( 2014 ) .   Natural Language Querying . Natural language   querying is a scenario in which a user formulates   a query to a visual model and the system is tasked   with outputting a visual response to that query – re-   ferred to as query2viz . Most of the existing V - NLIs   focus on this task . Theoretical work on utterance   structures in natural language querying has been   done by Srinivasan et al . ( 2019a , 2021b ) finding   that utterances mainly target attribute , chart type ,   encoding , aggregation , and design aspects of a vi-   sual model . Liu et al . ( 2021 ) ; Sun et al . ( 2014);351Narechania et al . ( 2021 ) generate a visualization   based on a data table and a natural language query .   Yu and Silva ( 2020 ) allow query sequences to be   specified in a visual exploration workflow .   Ambiguities . Resolving ambiguities and underspec-   ified utterances poses a difficult problem in this   visualization task , especially for single - turn query   interfaces . Hearst et al . ( 2019 ) ; Tory and Setlur   ( 2019 ) develop design guidelines for how systems   should respond to queries that contain vague mod-   ifiers or -user intents by exploring contextual in-   ference strategies . Gao et al . ( 2015 ) manage am-   biguities in input utterances using visual ambigu-   ity widgets . Setlur et al . ( 2019 ) apply inferencing   rules based on known syntactic and semantic input   structures . Setlur and Kumar ( 2020 ) use word co-   occurrence in combination with sentiment analysis   to determine data attributes and filter ranges associ-   ated with the articulated vague property .   Hypothesis Verification . Discovering novel insights   from data is usually done by ( dis-)validating hy-   potheses . Choi et al . ( 2019a , b ) study the use   of visualizations to prove or disprove natural lan-   guage hypotheses visually . The user initiates a   hypothesis test by formulating it in natural lan-   guage , and the system indicates the match with the   underlying data set by creating a graph that high-   lights matches / discrepancies in striking green / red   colours .   Query Dialogue . Setlur et al . ( 2016 ) ; Aurisano   et al . ( 2016 ) ; Bacci et al . ( 2020 ) extend the single-   turn query2viz interaction to a multi - turn interac-   tive visual exploration also referred to as analytical   conversation . Analytical conversation is the sup-   port of visual analysis processes by V - NLI with   the aim of inspecting visual features through a   visualization - oriented human - machine dialogue , as   studied by Turkay and Henkin ( 2018 ) ; Aurisano   et al . ( 2015 ) . In contrast to visualization creation   ( see section 3.4 ) , where visualizations are gener-   ated based on natural language text , the manipula-   tion or composition of a visualization in the query   dialog is used in the sense of a speech act . The   produced or manipulated visualization can be seen   here as a dynamically generated visual response to   a user query with the goal of providing information   in the dialog . Setlur and Tory ( 2017 ) ; Hoque et al .   ( 2018 ) apply pragmatics to visualization - oriented   dialogue modeling by taking the dialogue history   into account for computing more adequate future re-   sponses . Visualization - oriented dialogue assistantshave been developed in various forms . General-   purpose assistants for driving a visual analytics con-   versation are proposed by Fast et al . ( 2018 ) ; Kassel   and Rohs ( 2018 ) . Assistants implementing instruc-   tion following as in plot manipulation or navigation   scenarios process and execute commands in a visu-   alization environment ( Shao and Nakashole , 2020 ;   Wang et al . , 2021 ) . Multi - modal dialogue assistants   combine natural language input in oral or written   form with touch gestures ( Srinivasan and Stasko ,   2018 ; Kim et al . , 2021c ; Srinivasan et al . , 2020a ,   2021a ) . Sperrle et al . ( 2020 , 2021 ) study adap-   tive guidance to support a visual analytics process .   Collaborative approaches using mixed - initiative in-   teractions for visual analytics are explored by Hu   et al . ( 2018 ) ; Langevin et al . ( 2018 ) . The potential   of competitive visualization - oriented dialogue in-   terfaces for educational purposes is theoretically   investigated by Reicherts and Rogers ( 2020 ) by ex-   amining the role of questions in these dialogues .   Kumar et al . ( 2020b ) provide a data set of contextu-   alized dialogue acts in a visual exploration scenario   as a basis for training dialogue assistants .   Visual Question Answering . VQA is a well-   studied task in Language & Vision ( Antol et al . ,   2015 ; Yang et al . , 2016 ; Anderson et al . , 2018 )   with the goal to answer questions related to the   visual content of images . In VIS , the aim is to an-   swer complex questions related to visual models   such as charts or scientific illustrations as in Singh   and Shekhar ( 2020 ) ; Chaudhry et al . ( 2020 ) . Info-   graphics as sophisticated arrangements of visual   elements and text are supported by VQA in Mathew   et al . ( 2021 ) . Meeting the high informative stan-   dards of response generation required to harness   the explanatory purposes of visualizations presents   itself as a challenging task .   Browsing . Browsing supports users with a vague   or fuzzy data - related intent in discovering visu-   alizations . The idea is to narrow down the user   intent through language interaction using text in-   put , multi - step questions , or dialogue and suggest   appropriate next steps in the interaction with the   visualization . Luo et al . ( 2018 ) use keyword input   to execute personalized visualization recommenda-   tions . Other approaches leverage auto - completion   in text input ( Setlur et al . , 2020 ; Dhamdhere et al . ,   2017 ) or use multi - step question procedures to re-   strict the user ’s target area ( de Araújo Lima and   Barbosa , 2020 ; Luo et al . , 2020 ) . Srinivasan and352Setlur ( 2021 ) recommend data - related utterances   users can use to start a visual analysis or shimmy   along . Lee et al . ( 2021 ) guide users through a   visualization - oriented analytical conversation us-   ing insights found in the data similar to Cui et al .   ( 2019 ) .   3.3 Enjoy Task   Brehmer and Munzner ( 2013 ) consider enjoying as   the’casual encounter ’ with a visualization without   having a concrete hypothesis to verify . Natural lan-   guage enhances the perception of a visualization by   displaying additional information such as captions   that contextualize the visual experience , as applied   in immersive experiences , exhibitions , or museums .   Visually impaired people experience visualizations   through translation into auditory language .   Augmentation . In augmentation , visualizations   are complemented by automatically generated tex-   tual elements , such as labels orlinks . Srinivasan   et al . ( 2019b ) ; Hullman et al . ( 2013 ) augment vi-   sualizations with additional facts to substantiate   the message to be transmitted . Kandogan ( 2012 )   propose the concept of just - in - time augmentation   of visual structures during visual analytics to help   users understand the structure of the data . Lai et al .   ( 2020 ) automatically annotate visualizations based   on their textual description . Lallé et al . ( 2021 )   highlight corresponding elements of a visualization   based on tracked gazes of users as they read a text   description associated with the visualization . In   contrast to that Xia et al . ( 2020 ) augment audio   podcasts with visual elements . Gao et al . ( 2014 ) ;   Latif and Beck ( 2019 ) augment map visualizations   by automatically mining and linking site - related   facts out of articles to their location on the map .   Augmentation is also used to textually describe   GUI components automatically as a preliminary   step for auditory scene description helping visually   impaired people interact with visualizations ( Chen   et al . , 2020a ) .   Visualization Description Generation . Textual   descriptions for visualizations are created to com-   plement visual elements during the encounter with   a visualization . Spreafico and Carenini ( 2020 ) ;   Qian et al . ( 2021b ) ; Liu et al . ( 2020 ) complement   visualizations with text analogously to image cap-   tioning ( Vinyals et al . , 2015 ; Xu et al . , 2015 ) .   Murillo - Morales and Miesenberger ( 2020 ) gener-   ate auditory descriptions to make statistical charts   accessible to visually impaired people . Hsu et al.(2021 ) captions scientific illustrations with highly   informative labels that meet scientific quality stan-   dards . Summarization of visualization content into   textual form is researched by Demir et al . ( 2012 ) ;   Moraes et al . ( 2014 ) . Bylinskii et al . ( 2017 ) ;   Madan et al . ( 2018 ) extend this to aggregating info-   graphics into a single descriptive hashtag . Theoret-   ical work on how charts and their descriptions are   linked and verbalized by users is carried out in Kim   et al . ( 2021a ) , where it is found that users tend to   retain different amounts of information depending   on how prominent the visual feature presented in   the caption is .   3.4 Produce Task   Brehmer and Munzner ( 2013 ) refer to produce as a   ’ reference to tasks in which the intent is to generate   new artifacts ’ . Artifacts generated through natural   language interaction are , e.g. , annotations of ob-   jects in a visualization , scene descriptions , ortask   reports as used , e.g. , in medical visual analysis .   Annotation . Annotating areas of interest , com-   paring them among each other , and sharing them   with colleagues is a common language interac-   tion while working with visualizations ( Ren et al . ,   2017 ) . Chen et al . ( 2010a , b ) leverage touch and   click interactions for situated visualization annota-   tion . Latif et al . ( 2018 , 2021 ) explore the possibili-   ties of linking text and visualization . Sperrle et al .   ( 2019 ) study the visual annotation of argumenta-   tion and how this facilitates analysis . Theoretical   work on the sustainable extraction of knowledge   from visualization annotations is provided by Van-   hulst et al . ( 2021 ) , who propose a classification   framework that enables a structured capture and   ordering of annotations .   Documentation . Visualization systems are used   by experts , e.g. , in the medical domain ( Meuschke   et al . , 2021 ) to plan and discuss a surgery . Report-   ing , summarizing , and sharing this visualization-   related work is an important task that is an addi-   tional burden to the surgeon and therefore should be   executed by a machine . Nafari and Weaver ( 2013 ,   2015 ) generate natural language questions from   queries executed on a visualization resulting in a   natural language translation of the interaction . This   leaves a step - by - step report of the interaction find-   ing usage as a report of done work .   Visualization Creation . Visualization creation   considers the production of a visual model from a353natural language description – also referred to as   text2viz . Rashid et al . ( 2021 ) generate chart visu-   alizations from natural language text input . Col-   laborative authoring tools assisting users in visu-   alization creation use natural language as an input   modality . Cui et al . ( 2020 ) provide a tool that gener-   ates infographics using natural language statements   as input , similar to Qian et al . ( 2021a ) . Fulda et al .   ( 2016 ) design an interactive production process for   generating timelines from unstructured text input .   Language - based 3D scene generation , also referred   to as text2scene , which allows users to describe 3D   scenes using text without having to learn software   tools , is investigated in Coyne and Sproat ( 2001 ) ;   Coyne et al . ( 2012 ) ; Ulinski et al . ( 2018 ) .   4 How Users interact with Visualizations   After discussing why users interact with visualiza-   tions using natural language , Section 4 provides   a complementary discussion of how these interac-   tions are modeled . First , in Section 4.1 it is ex-   plained which NLP methods are used in these sys-   tems . Subsequently , Section 4.2 summarizes the   structure of the visualization - oriented dialogues in   the analyzed paper set in terms of initiative , dura-   tion , and present communication functions within   the respective visualization task .   4.1 NLP Methods   For each paper in the collection , both the NLP   methods used , if any , and if named the specific NLP   toolkits used for implementation are elaborated .   For the sake of clarity , the methods are roughly   divided into two areas : Natural Language Under-   standing ( NLU ) andNatural Language Generation   ( NLG ) . The majority of the systems apply standard   NLP methods like tokenization , stemming orstop-   word removal to pre - process text inputs , which is   why these are not recorded separately . For a de-   tailed inspection , we refer to Appendix C. Figure 3   shows the distribution of applied NLU methods   over all papers . Semantic Parsing , which relies on   rule - based mapping procedures from recognized in-   put tokens to semantic predicates , is predominantly   used . Often , POS - Tagging , Word Embeddings , and   Named Entity Recognition ( NER ) are additionally   applied to increase the accuracy of the mapping .   ForWord Sense Disambiguation WordNet , VerbNet   or ConceptNet are leveraged . Speech - to - Text APIs   are a common method used in many systems to en-   able auditory input . A small number of pioneering   systems integrate more sophisticated NLP meth-   ods such as Sentiment Analysis , Vector Search or   Co - Reference Resolution . One main reason for the   hesitant use of deep learning methods is the high de-   mands on performance and robustness of visualiza-   tion systems as Dhamdhere et al . ( 2017 ) points out .   Fluid interaction between user and system in real   time is a crucial factor for the success of a visual-   ization application . Adopting state - of - the - art deep   learning models to real - time interactions in visual-   ization , e.g. , by using Knowledge Distillation ( Hin-   ton et al . , 2015 ) or Quantization ( Jacob et al . , 2018 )   leaves space for future work . Figure 4 shows the   distribution of applied NLG methods over all pa-   pers . Template - based language generation is used   by the majority of the systems followed by a sig-   nificantly smaller number of deep learning - based   Seq2Seq Modeling approaches . Multi - turn systems   are predominantly based on rule - based or proba-   bilistic Dialogue Management . Only a few systems   use the Text - to - Speech functionality , as most of the   generated responses consist of visual elements . In   order to advance the adoption of deep learning-   based methods in visualization - related text gener-   ation , extensive training data sets are required , as   pointed out by Kumar et al . ( 2020a ) . There is a354limited number of data sets for Visualization De-   scription Generation ( Obeid and Hoque , 2020 ) ,   Visual Question Answering ( Mathew et al . , 2021 ;   Kim et al . , 2020 ) and Natural Language Querying   ( Fu et al . , 2020 ; Srinivasan et al . , 2021b ; Luo et al . ,   2021 ) . In particular , the compilation of data sets   for emerging dialogue scenarios in Analytical Con-   versation , Hypothesis Verification or collaborative   authoring in Visualization Creation would moti-   vate the use of deep learning based NLP methods   in these tasks . Therefore , generating high - quality   data sets for the aforementioned visualization tasks   leaves room for future work .   4.2 Dialogue Structures   The study of structures in visualization - oriented   dialogue is done with the idea of identifying task-   specific patterns , as shown in Figure 5 . The struc-   tural analysis is based on the work of Bunt ( 2009 )   and highlights , in particular , the initiative , duration ,   andcommunicative functions present in the mod-   eled dialogues . For each contribution that provides   access to sample data illustrating human - machine   dialogue within the paper or supplementary mate-   rial , the presence of the communicative functions   information providing , information - seeking , com-   missive , or directive for the user and system is   detected . A comprehensive list of allocations is   presented in Appendix C. Bunt ’s DIT++ taxon-   omy was chosen due to the fact that it focuses   on the function of the individual speech act . In   the context of a visualization task , it is important   which function a dialog act fulfills in the success-   ful execution of this task . This manifests itself   particularly in the design of dialog agents , where   speech acts that are intended to help solve the task   must be specified . Other taxonomies focus on the   rhetorical relations of speech acts to each other ,   as in Prasad et al . ( 2008 ) , or the emotional infor-   mation a speech act conveys in the dialogue , as in   EmotionML ( Schröder et al . , 2011 ) . In contrast to   the aforementioned taxonomies , Bunt ’s taxonomy   proves suitable in two respects : It allows to under-   stand how current dialogue situations are function-   ally structured in the visualization context . In this   way , patterns can be identified that are common for   the respective visualization tasks . From the genera-   tion perspective , it allows to specify dialog actions   that need to be prepared in certain visualization   task contexts in order to support the solution of the   visualization task .   Present Task . InVisual Storytelling , users initi-   ate the interactions which are performed as a se-   quence of multiple turns triggered through the se-   lection of text phrases . The human - machine dia-   logue is characterized by the actors complementing   each other through text and visual animations as   speech acts . Similar to storytelling , the human-   machine dialogue in Explanation Generation is   characterized by a complementing of user input   and visualization system output by matching vi-   sual and textual elements . Communicating insights   found by investigating a visual model is little ac-   companied by NLP techniques so far compared   to other visualization tasks . Template - based story   generation systems leave room for innovation in   grounding story segments in visualization elements .   Discover Task . The human - machine dialogue   inKeyword Search is a short , single - turn dialogue   characterized by user - initiated text input that   is reciprocated by a visual response from the   system , similar to Natural Language Querying .   Keyword - based visualization search relies on text   label - only search without including the on - screen   representation . This leaves space for grounding   abstract visualization concepts like outliers or   clusters in natural language as a promising   step towards a generalized visualization search .   Keyword search beyond 2D visualizations is an   open issue . The modeled dialogue interaction in   analytical conversation is a user - initiated dialogue   with multiple turns . Visual elements function   in the communication as information providers   carrying the response . This scenario offers the   possibility to apply modern NLP methods to   multi - modal dialogues by checking the user’s355intents and dynamically adapting the user ’s   experience by using the feedback of multiple turns .   The implemented systems in Visual Question   Answering follow a user - initiated , single - turn   dialogue approach in which a user question is   answered based on textual or visual information   the visualization holds . In Browsing the initiative   in the systems varies between user- , mixed- and   system - initiated ( see Figure 5 ) with variable   duration . Most approaches focus on high - quality   auto - completion to lead users , leaving space for   innovation in guidance - based dialogue approaches .   Enjoy Task . The predominant features of human-   machine dialogue in Augmentation are system-   initiated single - turn interactions where written or   spoken text is used to augment the visual represen-   tation . The augmentation of visualizations leaves   room for a stronger inclusion of multi - modal in-   teraction triggers such as gazes and gestures in   the dialogue conception . Visualization Description   Generation is characterized by system - initiated sin-   gle turn systems . Summarizing visualizations is a   challenge because it requires a high - quality scene   description due to the high explanatory potential   of visualizations . Particularly visually impaired   people benefit from well - designed auditory visu-   alization descriptions , which are a motivation for   further improvements .   Produce Task . During Annotation , users initiate   interactions , which can be continued by system sug-   gestions or completions . Producing artifacts based   on a visualization so far relies on template - based   authoring tools . Guidance andcompetition ined-   ucational contexts , as well as the collaboration of   user and system during artifact production , seem to   be promising directions for production - supporting   human - machine dialogue conception in the future .   Authoring tools take the initiative in Visualization   Creation by suggesting answers or partial task com-   pletions . The cooperation with the user appears   often in form of a multi - turn production process .   Documentation is done as a complement of the   user ’s actions , in that the system provides the user   with a report of the work performed after or during   the user - initiated interaction with a visualization .   5 Conclusion   In this survey , for a renowned taxonomy of abstract   visualization tasks , we classified 119 approachesof language - based interaction that support users   in pursuing data - related intents . In particular , we   shed a light on how the human - machine dialogue   is constructed in these works and which NLP meth-   ods current V - NLI use . Considering the progress   of NLP methods in the field of visualization , we   can summarize our work on two main outcomes :   A compilation of data sets for the individual vi-   sualization tasks seems promising to advance the   use of deep learning - based NLP methods ; When   introducing them , special attention must be paid to   performance and robustness aspects due to the high   requirements in the VIS area . Finally , the support   of visualization tasks through natural language in-   teraction offers a large number of interesting areas   for the application of state - of - the - art NLP methods ,   inviting the NLP and VIS communities to work cre-   atively in the emerging intersection of both fields .   Acknowledgments   We thank the Michael Stifel Center Jena for fund-   ing this work , which is part of the Carl Zeiss   Foundation - funded project ’ A Virtual Workshop   for Digitization in the Sciences ’ ( 062017 - 02 ) .   References356357358359360361362   A Scope   Research on visualization - oriented natural   language - based interaction is conducted in the VIS ,   HCI , and NLP communities . For providing an   overview of the number of selected contributions   per community , the selection set is grouped based   on publication venues related to their respective   community . Important related work with high   subject relevance being derived from other sources   is subsumed in the category Miscellaneous . The   time span of surveyed works is restricted to be   between 2010 and 2021 . Next to application   papers implementing human - machine interaction   theoretical works related to language - based   interaction modeling are explicitly included .   Table 2 shows the distribution of contributions   over community - related venues .   Venues Papers   Visualization ( VIS ) 49   Human - Computer - Interaction ( HCI ) 27   Natural Language Processing ( NLP ) 9   Miscellaneous 34   Total 119   The survey is targeted to touch the intersection of   the domains of NLP , HCI and VIS . The scope is de-   fined to reflect on how interaction is modeled and   implemented in the different communities as well   as to point out how researchers combine different   ideas originating from the three fields into work   that can be deposited in the intersection of them .   For the VIS domain , the most common venues are   IEEE Transactions on Visualization and Computer   Graphics ( 19 ) and Computer Graphics Forum ( 5 )   containing work that is mostly specialized on query-   based natural language interfaces . The area of HCI   is ostensibly represented by the venue of the Con-   ference on Human Factors in Computing Systems   ( SIGCHI ) ( 14 ) originating works that consider the   interaction aspect and focus on language as a tool   that transmits information for reaching a goal . In   the NLP domain , the most frequent venue is the   Annual Meeting of the Association for Computa-   tional Linguistics ( ACL ) ( 3 ) including works less363visualization - related focusing to a large extent on   the dialogue modeling . B Classification   The contributions are classified based on the Multi-   Level Typology of Abstract Visualization Tasks   by Brehmer and Munzner ( 2013 ) . Figure 6 illus-   trates a distribution of papers over the abstract vi-   sualization tasks . The visualization task accommo-   dating the highest number of works considering   natural language - based interaction is the task dis-   cover ( 70 ) , followed by enjoy ( 26 ) . Contributions   supporting present ( 9 ) and produce ( 14 ) tasks are   less frequent .   Figure 7 shows the distribution of papers over the   inner class sub - tasks . For sake of simplicity , papers   are categorized into the single most suitable cate-   gory only , although some works touch on several   categories . Natural Language Querying ( 45 ) is by   far the sub - task with the highest amount of con-   tributions followed by Keyword Search ( 15 ) and   Visualization Description Generation ( 14 ) . Less   frequently studied tasks are Explanation Genera-   tion(3),Visual Question Answering ( VQA ) ( 3 ) , and   Documentation ( 1 ) .   Table 3 contains a comprehensive listing of the   classification of the single contributions into the   taxonomy by Brehmer and Munzner ( 2013).364   Visualization Task Subtask References   Discover Keyword Search ( Feng et al . , 2018 ; Chowdhury et al . , 2021 ; Chung   et al . , 2010 ; Fraser et al . , 2020 ; Wilson et al . , 2010 ;   Schleußinger and Henkel , 2018 ; Isaacs et al . , 2014 ;   Peltonen et al . , 2017 ; Siddiqui and Hoque , 2020 )   VQA ( Singh and Shekhar , 2020 ; Mathew et al . , 2021 ;   Chaudhry et al . , 2020 )   Querying ( Srinivasan et al . , 2020b ; Srinivasan and Stasko ,   2017 ; Kassel and Rohs , 2019 ; Crovari et al . , 2020 ;   Tory and Setlur , 2019 ; Hearst and Tory , 2019 ; Liu   et al . , 2021 ; Hoque et al . , 2018 ; Setlur and Tory ,   2017 ; Siddiqui , 2021 ; Bacci et al . , 2020 ; Narecha-   nia et al . , 2021 ; Yu and Silva , 2020 ; Setlur et al . ,   2016 ; Sun et al . , 2014 ; Aurisano et al . , 2016 ; Srini-   vasan et al . , 2021b , 2019a ; Gao et al . , 2015 ; Setlur   et al . , 2019 ; Hearst et al . , 2019 ; Setlur and Ku-   mar , 2020 ; Choi et al . , 2019b , a ; Sperrle et al . ,   2020 , 2021 ; El - Assady et al . , 2020 ; Kumar et al . ,   2020a ; Aurisano et al . , 2015 ; Turkay and Henkin ,   2018 ; Mazumder and Riva , 2021 ; Lawrence and   Riezler , 2016 ; Shao and Nakashole , 2020 ; Wang   et al . , 2021 ; Manuvinakurike et al . , 2018 ; Fast   et al . , 2018 ; Kassel and Rohs , 2018 ; Bieliauskas   and Schreiber , 2017 ; Seipel et al . , 2019 ; Lee and   Parameswaran , 2018 ; Kumar et al . , 2020b ; Srini-   vasan and Stasko , 2018 ; Kim et al . , 2021c ; Srini-   vasan et al . , 2020a ; Kumar et al . , 2017 ; Srinivasan   et al . , 2021a ; Hu et al . , 2018 ; Langevin et al . , 2018 ;   Reicherts and Rogers , 2020 ; John et al . , 2017 )   Browsing ( Setlur et al . , 2020 ; Lee et al . , 2021 ; Luo et al . ,   2018 ; de Araújo Lima and Barbosa , 2020 ; Luo   et al . , 2020 ; Srinivasan and Setlur , 2021 ; Cui et al . ,   2019 ; Dhamdhere et al . , 2017 )   Enjoy Augmentation ( Srinivasan et al . , 2019b ; Hullman et al . , 2013 ;   Xia et al . , 2020 ; Gao et al . , 2014 ; Kandogan , 2012 ;   Chen et al . , 2020c , a ; Lai et al . , 2020 ; Bylinskii   et al . , 2017 ; Madan et al . , 2018 ; Lallé et al . , 2021 ;   Latif and Beck , 2019 )   Description Generation ( Demir et al . , 2012 ; Moraes et al . , 2014 ; Spreafico   and Carenini , 2020 ; Qian et al . , 2021b ; Murillo-   Morales and Miesenberger , 2020 ; Kim et al . ,   2021a ; Lundgard and Satyanarayan , 2021 ; Kim   et al . , 2021b ; Jung et al . , 2021 ; Choi et al . , 2019c ;   Obeid and Hoque , 2020 ; Hsu et al . , 2021 ; Henkin   and Turkay , 2020 ; Liu et al . , 2020 )   Continued on next page365Visualization Task Subtask References   Present Storytelling ( Bryan et al . , 2017 ; Kwon et al . , 2014 ; Metoyer   et al . , 2018 ; Choudhry et al . , 2021 ; Shi et al . , 2021 ;   Chen et al . , 2020b )   Explanation Generation ( Sevastjanova et al . , 2018 ; Hohman et al . , 2019 ;   von Landesberger et al . , 2021 )   Produce Annotation ( Chen et al . , 2010b , a ; Vanhulst et al . , 2021 ; Sper-   rle et al . , 2019 ; Latif et al . , 2018 ; Ren et al . , 2017 ;   Latif et al . , 2021 )   Documentation ( Nafari and Weaver , 2013 , 2015 )   Visualization Creation ( Rashid et al . , 2021 ; Cui et al . , 2020 ; Qian et al . ,   2021a ; Fulda et al . , 2016 ; Xia , 2020)366C Analysis Details   The language - based interaction implemented in the   visualization applications is analyzed considering   their initiative , duration andcommunicative func-   tions present in the human - machine dialogue based   on the DIT++ taxonomy of dialogue acts by Bunt   ( 2009 ) . The idea is to create an overview of how   the modeled interactions in the respective tasks and   sub - tasks are structured . The variables considered   in the study and the definitions used for them are   explained below . Only contributions that present   systems that implement human - machine interac-   tion are part of this examination , theoretical works   are excluded . Table 6 contains a comprehensive   listing of all contributions evaluated as well as their   respective investigation results .   C.1 NLP Methods   For all papers in the selection , the NLP methods   used , if any , and if named the NLP toolkits used   for implementation are elaborated . For the sake of   clarity , the methods are roughly divided into two ar-   eas : Natural Language Understanding ( NLU ) and   Natural Language Generation ( NLG ) . Due to the   fact , that the majority of the systems use standard   NLP methods such as tokenization , stemming , or   stopword removal in text pre - processing , these are   not recorded separately .   Figure 8 shows the distribution of applied NLU   methods over the four visualization tasks . It can   be seen , that in the discover task the largest variety   of methods is applied . Predominantly used are   Semantic Parsing , POS - Tagging , and Speech - to-   Textmethods followed by Language Modeling and   Word Sense Disambiguation . Interfaces in produce   to a greater extend rely on Word Embedding and   Named Entity Recognition ( NER ) . The enjoy task   similar to discover employs a variety of methods . Atpresent , NLU methods are only used to a minor   extent .   Figure 9 illustrates the distribution of applied NLG   methods over the four abstract visualization tasks .   Template - based NLG methods are predominantly   used to generate text in all tasks at hand . In   theenjoy task Seq2Seq Modeling based on deep   learning technologies is primarily used presumably   due to the proximity to the common NLP task of   Image Captioning . The same probably applies   to the task of text summarization , which is also   carried out . Dialogue Management is only applied   indiscover , mostly relying on rule - based or   probabilistic modeling methods , e.g. , by leverag-   ingFinite - State - Machine ( FSM ) approaches to   manage the sequence of dialogue acts . A small   number of systems in discover andenjoy rely on   Text - to - Speech technologies in the interaction .   NLP Toolkits . An overview of the applied toolkits   in NLU is shown in Table 4 . Especially Stanford   Core NLP , ANTLR , SpaCy , and NLTK are found   to accomplish several tasks . Word2Vec is the most   popular embedding method , followed by FastText .   The Web Speech API is used primarily because   many visualization applications use web technolo-   gies . It is striking that many systems rely on N-   gram language models . In terms of Word Sense   Disambiguation , WordNet experiences great popu-   larity .   An overview of the applied toolkits in NLG is illus-   trated in Table 4 . The markup language for chatbots   AIML as well as the Rasa toolkit are adopted for   Template - based text generation , as well as hand-   crafted Context - Free - Grammars .LL * Parsers are   predominantly applied in the generation of auto-   completions . Seq2Seq Modeling is experiencing   increasing interest expressed through the adoption367NLU Method Toolkits and Technologies   Semantic Pars-   ingANTLR ( 4 ) , Context - Free-   Grammars(CFG ) ( 3 ) , NLTK   ( 3 ) , Stanford Core NLP   ( 2 ) , AIML ( 2 ) , IBM Wat-   son , NL4DV Toolkit , SpaCy ,   Google Cloud Natural Lan-   guage API , OpenCalais   API , Conditional Random   Fields ( CRF ) , Wit.ai , Stanford   SEMPRE   POS - Tagging Stanford Core NLP ( 6 ) , SpaCy   ( 3 ) , ClearNLP , Rasa , Compro-   mise JS   Word Embed-   dingWord2Vec ( 8) , FastText   ( 3 ) , GloVe ( 2 ) , TF - IDF(2 ) ,   Sent2Vec , BERT Embedding   Speech - to-   TextWeb Speech API ( 8) , Mi-   crosoft Speech API ( 3 ) , Google   Speech API ( 2 ) , Apple Speech   Framework   NER Stanford Core NLP ( 3 ) ,   Chrono JS , Google NLP   Toolkit , Wikifier , OpenCalais   API , TimeML , TERNIP   Language   ModelingN - Gram Language Model ( 6 ) ,   BERT ( 4 ) , Bidirectional LSTM   ( 2 )   Word Sense   Disambigua-   tionWordNet ( 6 ) , VerbNet , Con-   ceptNet , FrameNet   Dependency   ParsingStanford Core NLP ( 3 ) ,   Apache OpenNLP , SpaCy   Knowledge   RepresentationRDF ( 2 ) , Wolfram Alpha Unit   Taxonomy , SIMON   Constituency   ParsingANTLR ( 2 ) , Stanford Core   NLP   Keyword Ex-   tractionTF - IDF ( 3 )   Co - Reference   ResolutionCogCompNLP   Sentiment   AnalysisStanford Core NLP , LSTM   Vector Search Word2Vec ( 2 ) , TF - IDF   of deep learning models such as different variants   ofLSTM andTransformers . It is striking that , inNLG Method Toolkits and Technologies   Template-   based NLGAIML , Rasa , LL * Parser ,   Context - Free - Grammars   ( CFG ) , IBM Watson   Seq2Seq Mod-   elingLSTM ( 3 ) , LSTM+Attention   ( 2 ) , CNN+Conditional Ran-   dom Fields ( CRF ) ( 2 ) , Trans-   former , M4C , LayoutLM , Im-   age Transformer , Bidirectional   LSTM   Dialogue Man-   agementFinite - State - Machines ( FSM )   ( 2 ) , AIML , Rasa , IBM Watson   Text - to-   SpeechMicrosoft TTS   Text-   SummarizationPageRank Algorithm   addition to ready - made toolkits such as Rasa and   mark - up languages such as AIML forDialogue   Management , Finite - State - Machines are also pre-   dominantly used .   C.2 Initiative   In an interaction , the initiative is taken by the actor   that leads or controls the dialogue , e.g. , via ques-   tions . McTear ( 2002 ) classifies initiative into user   initiative , system initiative , and mixed- initiative .   Litman and Pan ( 2004 ) mark , that the initiative   within a dialogue determines the set of possible   questions and responses of user and system and   therefore the outline of the dialogue . Considering   the initiator , the classification of McTear ( 2002 ) is   used as a basis for the classification including the   three general categories :   User Initiative . Language - based interactions are   classified as user - initiated when the direction of   the dialog is determined by the user ’s actions , in   this case , written- or spoken utterances or other   text input . The conversation is usually conducted   through commands or questions . In addition to   prior works ( McTear , 2002 ; Litman and Pan , 2004 )   considering visualization - oriented dialogue the   data - related intent depicts an important factor   for a user to take the initiative . Exemplary , this   happens when users initiate a conversation by   formulating a query to discover new insights about368a visualization .   System Initiative . System - initiated language - based   interactions are determined by natural language   utterances generated by the system . The system   creates the outline of the interaction towards   a previously determined goal . The user is led   towards the goal and if the goal is achieved the   visualization task is completed . An exemplary case   is a system guiding a user in a step - by - step tutorial   through the execution of a task , e.g. , the identifi-   cation and elimination of outliers in a visualization .   Mixed Initiative . In mixed - initiated language - based   interaction users and systems at different times   and to different proportions contribute to the   determination of the interaction . In a visualization   context both follow a data - related intent but   the way there is characterized by negotiation ,   proposals , and agreement and disagreement .   Exemplary this is the case during interactive   clustering where user and system propose different   divisions of the data space to each other nego-   tiating a good classification for the underlying data .   To carry out the classification an interaction is con-   sidered to be single - initiated (= user initiative or   system initiative only ) if during the whole com-   pletion of the visualization task the same actor   is initiating . If the initiative changes at least once   the interaction is classified as mixed - initiative inter-   action . The scaling of the visualizations is normal-   ized to 100 percent .   Figure 10 shows the distribution of initiative over   all included papers in the study . In the set of contri-   butions , user - initiated interactions are predominant ,   followed by mixed - initiative interactions . Only   about ten percent of the interactions are system - initiated .   The distribution of the initiative within the individ-   ual visualization tasks is illustrated in Figure 11 . In   discover andproduce user - initiated interactions are   predominant . The present task contains a balanced   ratio of user- and mixed - initiated interactions . The   enjoy task is the only task , where system - initiated   interactions represent the majority .   Figure 12 shows the distribution of initiative within   the single sub - task categories . Users initiate the   interactions in Keyword Search , Explanation Gen-   eration , VQA , Visualization Creation , and Docu-   mentation . System initiative is present in Augmen-   tation , Visualization Description Generation , and   Browsing . Mixed initiative interaction is modeled   inAnnotation , Storytelling and less frequently in   Natural Language Querying , Browsing , and Aug-   mentation .   C.3 Duration   Within dialogue modeling , natural language - based   interactions are modeled as a sequence of dialogue   turns . In their survey Deriu et al . ( 2020 ) propose   a characterization of dialogue system types as   task - oriented dialogue systems , conversational369agents , and interactive QA systems . The basis for   this classification is differences in the dialogue   structures supported by the different systems ,   especially in their duration and task - orientedness .   Interactive QA systems are considered task - related   single- or multi - turn systems . Task - oriented   dialogue systems are considered multi - turn   systems with short interaction lengths due to the   optimization goal . Conversational agents are   classified as non - task - oriented multi - turn systems   with long interaction lengths . The decision   for single- or multi - turn dialogue systems in a   visualization - oriented dialogue is a conceptual one   that V - NLI designers have to make concerning the   quality measure that is set on the system . Single   turn systems , e.g. , hold higher risks in failing to   resolve ambiguities or vague expressions from a   single query than multi - turn systems that can pose   requests , but also deliver the result in the quickest   possible way .   Depending on the visualization task and sub - task   at hand , the interaction structures differ . To carry   out a uniform duration classification we consider   the length of the human - machine dialogue , that   is modeled by the application as the decisive cri-   terion . Therefore , interactions that include more   than a single utterance in the calculation of the   next response ( e.g. by including the dialogue his-   tory in context management ) and interactions that   support more than one dialogue turn for users   and system respectively are considered multi - turn .   An interaction is considered to be single - turn if   user and system utter at maximum one utter-   ance respectively in a coherent dialogue .   Figure 13 shows that the modeled interactions in   almost two - thirds are single turn and one - third are   multi - turn .   On the task level , in present andenjoy interactions   are predominantly single - turned ( see Figure 14 ) . In   thediscover andproduce task the ratio of multi- to   single turn interactions is rather balanced .   Figure 15 illuminates the distribution of duration   on a sub - task level . Explanation Generation and   VQA are modeled in single turn interactions similar   toKeyword Search , Augmentation , and Browsing .   Multi - turn interactions are found predominantly   inNatural Language Querying , Visualization Cre-   ation , and Documentation .   C.4 Communicative Functions   Bunt ( 2009 ) introduces a taxonomy for dialogue   act classification . Following that , each individual   speech act in a conversation is classified due to   the communicative function it carries . On a high   level , these general - purpose functions distinguish   speech acts as that every individual turn carries   either information providing , information seeking ,   commissive ordirective functionality . Especially   since visualization - oriented interactions are multi-   modal designers of V - NLI have to decide which   communicative functions are adopted by visual el-   ements as a complementary modality to language370 in multi - modal dialogue . The examination of com-   municative functions in the applications at hand   is carried out with the idea in mind of gaining an   overview of who holds which share of which com-   municative function in the modeled dialogues . The   aim is to help to better characterize and compare   the dialogues in the individual tasks and sub - tasks .   For contributions that provide access to exemplary   human - machine dialogues either within the paper   or the supplemental material the presence of each   of the communicative functions information provid-   ing , information seeking , commissive , ordirective   is detected for user and system respectively ( see   Table 6 ) . The representation of the identified com-   municative functions is in absolute quantities for   the respective sub - task under consideration .   Figure 16 shows the characteristic distribution of   communicative functions for the task present and   its respective sub - tasks . It turns out that in these   interactions systems predominantly provide users   with information . In Visual Storytelling the user   and the system complement each other in different   modalities , textual and visual , to jointly present   visual insights in the form of a multi - modal story .   Explanation Generation is characterized by users   who are looking for an explanation for a certain   behaviour , which can be understood more easily   with the help of a visualization and a generated text   description acting as a guide .   Figure 17 illuminates the shares of communicative   functions in the visualization task discover and the   respective sub - tasks . It shows that interactions in   Keyword Search andVisual Question Answering   are predominantly characterized by users seeking   information and systems providing those to the user .   InKeyword Search commissives are occasionally   uttered by the system to respond to user - induced   directives in dialogue . Visual Question Answer-   ingfollows the classic question - answer scheme in   which users bundle their search for information in a   question and systems provide textual answers that   can be substantiated by the visualization . Natural   Language Querying andBrowsing contain a more   variable profile of communicative functions which   also accommodate higher numbers of directives   such as , e.g. , system - generated suggestions used to   pose recommendations to the user in Browsing or   commands inNatural Language Querying applied371by users to make the system execute an action . In-   terestingly , commissive utterances occur especially   in longer analytical conversations , for example , to   confirm the loading of a data set or to acknowledge   the perception of a command given by the user .   When looking at the distribution of communicative   functions in Browsing , it becomes clear that the   system tries to facilitate the user ’s entry into visual   exploration by providing additional information or   directives .   The enjoy task is characterized through systems   providing the user with additional information as   well as occasional directives in the Augmentation   task ( see Figure 18 ) . Users occasionally ask for   information , but the bulk of the interaction consists   of the system presenting information to the user or   suggesting directives for future interaction . In Visu-   alization Description Generation , the special focus   of systems is on providing information to visually   impaired people . Describing scenes from a visu-   alization in detail so that visually impaired people   can perceive them in their full detail requires high-   quality text generation that goes beyond standard   image captioning .   Interactions in the context of artifact production de-   liver diverse profiles of communicative functions ,   as shown in Figure 19 . Annotation is characterized   by users providing information , e.g. , in form of text   labels and systems that direct the user , e.g. , by mak-   ingsuggestions where to put those . Interactions in   Visualization Creation face user and system con-   tributing information as well as systems delivering   additional suggestions for the next step in the cre-   ation process . In Documentation , systems provide   textual information in form of a report during or   after a user interaction with a visual model.372   References Task Sub Dur Init CF - User CF - System   IS IP CM DI IS IP CM DI   Bryan et al . ( 2017 ) Pre Sto MT MI ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗   Kwon et al . ( 2014 ) Pre Sto ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Metoyer et al . ( 2018 ) Pre Sto ST MI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Hohman et al . ( 2019 ) Pre Exp ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Feng et al . ( 2018 ) Dis Key ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Chowdhury et al . ( 2021 ) Dis Key MT UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Chung et al . ( 2010 ) Dis Key ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Fraser et al . ( 2020 ) Dis Key ST UI ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✗   Schleußinger ( 2018 ) Dis Key ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Isaacs et al . ( 2014 ) Dis Key MT UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Peltonen et al . ( 2017 ) Dis Key ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Siddiqui and Hoque ( 2020 ) Dis Key ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Singh and Shekhar ( 2020 ) Dis VQA ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Mathew et al . ( 2021 ) Dis VQA ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Chaudhry et al . ( 2020 ) Dis VQA ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Choi et al . ( 2019b ) Dis Que ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Choi et al . ( 2019a ) Dis Que ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Liu et al . ( 2021 ) Dis Que ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Hoque et al . ( 2018 ) Dis Que MT UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Siddiqui ( 2021 ) Dis Que ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Bacci et al . ( 2020 ) Dis Que ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Narechania et al . ( 2021 ) Dis Que ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Yu and Silva ( 2020 ) Dis Que MT UI ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Setlur et al . ( 2016 ) Dis Que MT UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Sun et al . ( 2014 ) Dis Que ST UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Aurisano et al . ( 2016 ) Dis Que MT UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Srinivasan et al . ( 2019a ) Dis Que ST MI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Setlur and Kumar ( 2020 ) Dis Que ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Setlur et al . ( 2019 ) Dis Que ST UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Gao et al . ( 2015 ) Dis Que ST MI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   John et al . ( 2017 ) Dis Que MT MI ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✓   Srinivasan et al . ( 2021a ) Dis Que ST UI ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Wang et al . ( 2021 ) Dis Que MT MI ✗ ✗ ✗ ✓ ✓ ✓ ✗ ✓   Shao and Nakashole ( 2020 ) Dis Que MT UI ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Srinivasan et al . ( 2020a ) Dis Que MT UI ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Mazumder and Riva ( 2021 ) Dis Que ST UI ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Lawrence and Riezler ( 2016 ) Dis Que ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Fast et al . ( 2018 ) Dis Que MT MI ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓   Kassel and Rohs ( 2018 ) Dis Que MT UI ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✗   Kumar et al . ( 2020a ) Dis Que MT UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Dhamdhere et al . ( 2017 ) Dis Que MT UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Continued on next page373References Task Sub Dur Init CF - User CF - System   IS IP CM DI IS IP CM DI   Manuvinakurike et al . ( 2018 ) Dis Que MT UI ✗ ✓ ✗ ✓ ✗ ✓ ✓ ✗   Seipel et al . ( 2019 ) Dis Que ST UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Srinivasan and Stasko ( 2018 ) Dis Que MT UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Kim et al . ( 2021c ) Dis Que ST UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✗   Bieliauskas ( 2017 ) Dis Que MT MI ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓   Sperrle et al . ( 2021 ) Dis Que MT MI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✓   El - Assady et al . ( 2020 ) Dis Que MT MI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✓   Langevin et al . ( 2018 ) Dis Que MT MI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Hu et al . ( 2018 ) Dis Que ST MI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✓   Setlur et al . ( 2020 ) Dis Bro ST UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Lee et al . ( 2021 ) Dis Bro MT UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Luo et al . ( 2018 ) Dis Bro ST UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Lima ( 2020 ) Dis Bro ST UI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Srinivasan and Setlur ( 2021 ) Dis Bro ST MI ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓   Luo et al . ( 2020 ) Dis Bro MT SI ✗ ✓ ✗ ✗ ✓ ✓ ✗ ✗   Cui et al . ( 2019 ) Dis Bro ST MI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✓   Lallé et al . ( 2021 ) Enj Aug ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Lai et al . ( 2020 ) Enj Aug ST SI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Kandogan ( 2012 ) Enj Aug ST SI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Srinivasan et al . ( 2019b ) Enj Aug MT MI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✓   Xia et al . ( 2020 ) Enj Aug ST SI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Hullman et al . ( 2013 ) Enj Aug ST SI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Moraes et al . ( 2014 ) Enj VDG ST SI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Murillo ( 2020 ) Enj VDG ST UI ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Choi et al . ( 2019c ) Enj VDG MT SI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗   Sperrle et al . ( 2019 ) Pro Ann ST MI ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓   Ren et al . ( 2017 ) Pro Ann ST UI ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓   Latif et al . ( 2021 ) Pro Ann MT MI ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓   Cui et al . ( 2020 ) Pro VC ST UI ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗   Xia ( 2020 ) Pro VC MT UI ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓   Nafari and Weaver ( 2015 ) Pro Doc MT UI ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ 374