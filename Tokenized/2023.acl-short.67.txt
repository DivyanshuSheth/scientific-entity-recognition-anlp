  David PonceandThierry EtchegoyhenandVictor RuizVicomtech Foundation , Basque Research and Technology Alliance ( BRTA)University of the Basque Country UPV / EHU   { adponce,tetchegoyhen,vruiz}@vicomtech.org   Abstract   We describe a novel unsupervised approach   to subtitle segmentation , based on pretrained   masked language models , where line endings   and subtitle breaks are predicted according to   the likelihood of punctuation to occur at can-   didate segmentation points . Our approach ob-   tained competitive results in terms of segmen-   tation accuracy across metrics , while also fully   preserving the original text and complying with   length constraints . Although supervised mod-   els trained on in - domain data and with access   to source audio information can provide better   segmentation accuracy , our approach is highly   portable across languages and domains and   may constitute a robust off - the - shelf solution   for subtitle segmentation .   1 Introduction   Subtitling is one of the principal means of pro-   viding accessible audiovisual content . With the   ever increasing production of audiovisual content   in multiple domains and languages , in the current   digital era , subtitle provision can benefit from au-   tomation support , via Automatic Speech Recogni-   tion and/or Machine Translation ( V olk et al . , 2010 ;   Aliprandi et al . , 2014 ; Etchegoyhen et al . , 2014 ;   Tardel , 2020 ; Bojar et al . , 2021 ) .   Subtitles are subject to specific constraints in   order to achieve adequate readability , including   layout , on - screen duration and text editing . Among   these constraints , segmentation addresses the maxi-   mum number of characters per line , the number of   lines per subtitle , and breaks at natural linguistic   frontiers . Segmentation has been shown to be an   important readability factor ( Perego et al . , 2010 ;   Rajendran et al . , 2013 ) , with improperly segmented   subtitles resulting in increased cognitive effort and   reading times for users . Thus , automated subti-   tling systems need to generate properly segmented   subtitles to achieve readability . A typical baseline for subtitle segmentation , still   used in some production systems , is simple char-   acter counting , whereby line breaks are inserted   before reaching the maximum allowed number of   characters per line . Although simple and fast , this   approach does not address the need for linguisti-   cally correct segments and , therefore , falls short in   terms of readability . Several approaches have been   proposed to improve segmentation by automated   means . Álvarez et al . ( 2014 ) proposed a machine   learning method where subtitle breaks are predicted   by Support Vector Machine and Linear Regression   models trained on professionally - created subtitles .   A similar method based on Conditional Random   Fields was then shown to improve over these results   ( Alvarez et al . , 2017 ) . Approaches that directly   generate subtitle breaks within Neural Machine   Translation have also been proposed in recent years   ( Matusov et al . , 2019 ; Karakanta et al . , 2020a ) . Re-   cently , Papi et al . ( 2022 ) developed a multilingual   segmenter which generates both text and breaks   and may be trained on textual input only , or on   joint text and audio data .   Although quality subtitle segmentation may be   achieved with the aforementioned approaches , they   require supervised training on segmented subti-   tle corpora . At present , the largest subtitle cor-   pus is Open Subtitles ( Lison et al . , 2018 ) , which   mainly covers entertainment material , contains sub-   titles mostly created by non - professionals or au-   tomatically translated , and does not include line   breaks . The MuST - Cinema corpus ( Karakanta   et al . , 2020b ) , on the other hand , is a multilingual   speech translation corpus that includes subtitles   breaks , but is only available for 8 languages at the   moment . Considering the vast amount of languages   and domains in audiovisual content , the lack of seg-   mented training data hinders the development of   robust automated subtitling systems .   In this work , we describe a novel unsupervised   method based on pretrained masked language mod-771els ( MLM ) , where line and subtitle breaks are in-   serted according to the likelihood of a segment act-   ing as an isolated unit , as approximated by the prob-   ability of a punctuation mark occurring at a given   segmentation point . In our experiments , this novel   approach obtained competitive results on most met-   rics , while also fully preserving the original text   and complying with length constraints . Our system   may thus be used as a simple yet efficient subtitle   segmenter with any pretrained masked language   model , for any language covered by the model .   2 Approach   Our approach is based on the standard view that   the more appropriate subtitle segments are those   that may function as isolated grammatical chunks .   We further hypothesise that a relevant approxima-   tion for the identification of this type of unit is the   likelihood of a punctuation mark being inserted at   the end of a candidate segment , as punctuation may   mark the closure of a syntactic unit and is often   associated with discursive pauses . To test this hy-   pothesis , we compute the likelihood of punctuation   marks at different segmentation points , as predicted   by a pretrained MLM , and select the insertion point   with the highest likelihood .   The segmentation candidates are determined un-   der a sliding - window approach over the entire input   text . We first generate the list of all pairs < α , β >   over the unprocessed portion of the text , where   αis a segmentation candidate of length under a   specified limit K , corresponding to the maximum   number of characters per line , and βis the remain-   ing portion of the text to be segmented .   We then score all segmentation candidates α   with one of the LM scoring variants described be-   low . A segmentation marker , either end - of - line   ( < eol > ) , or end - of - block indicating the end of a   subtitle ( < eob > ) , is then appended to the best scor-   ing candidate , and βbecomes the input text to be   segmented in a recursive iteration of the process .   Since our method does not rely on any additional   information , such as an audio source , to determine   the segmentation type , an < eob > tag is inserted   every even segment or when βis empty ; otherwise ,   an < eol > tag is inserted . We thus generate subtitles   with a maximum of two lines , following a standard   recommendation in subtitling . We also define a   minimal number of characters ( min ) inαfor thesegmentation process to apply , and do not segment   lines that are under the specified character limit .   We evaluated three approaches to compute seg-   mentation scores over each candidate pair < α , β > :   •Substitution : The last token of αis masked   and the score is the highest MLM probability   among punctuation marks on this mask .   •Insertion : A mask is appended to αand the   score is the highest MLM probability among   punctuation marks on this mask .   •LM - Score : The score is the average of the   perplexity of αandβ , as derived from the   MLM probabilities for each token in the cor-   responding sequence .   The first two methods are variants of our core ap-   proach . The third method , while also based on the   same pretrained MLM , relies instead on the pseudo-   perplexity of the sequences according to the MLM ,   computed following Salazar et al . ( 2020 ) . We in-   cluded this latter variant to measure the potential   of using LM scoring directly , without resorting to   the likelihood of punctuation marks .   3 Experimental Setup   Corpora . For all experiments , we used the   MustST - Cinema corpus ( Karakanta et al . , 2020b ) ,   which is derived from TED talks and contains both   line and subtitle break markers . In addition to be-   ing publicly available , it also allows for a direct   comparison with the supervised models of Papi   et al . ( 2022 ) . We report results of our approach   on the 6 MuST - Cinema datasets for which com-   parative results were available , directly predicting   segmentation on the test sets without any training .   Methods . For our approach , we tested the three   variants described in Section 2 . We used BERT   ( Devlin et al . , 2019 ) as our MLM for all lan-   guages .. Additionally , we included a variant called   overt clueing ( OC ) , where an overt punctuation   mark at the end of a candidate segment increments   the mask score by 1 . We then compared the results   of the best LM - based variant with those obtained   by alternative approaches . In all cases , our results   were computed with min = 15 , as this value ob-   tained the best results overall over the development772English Spanish German   Method Sigma EOL EOB Sigma EOL EOB Sigma EOL EOB   Substitution 71.65 +19.86 -10.96 69.34 +12.36 -5.74 69.31 +19.05 -7.05   Insertion 76.77 +19.18 -9.91 73.47 +12.98 -4.91 70.85 +18.53 -7.96   LM - Score 69.97 +21.40 -8.66 67.70 +13.29 -5.37 64.07 +16.45 -6.51   sets , although the differences were minor with the   other values we tested ( 1 , 10 and 20 ) .   We used the simple character counting approach   ( hereafter , CountChars ) as baseline , and , as rep-   resentative supervised methods on the selected   datasets , the models described by ( Papi et al . , 2022 ) .   Their core supervised approach is based on a Trans-   former ( Vaswani et al . , 2017 ) architecture with 3   encoder layers and 3 decoder layers , trained on   textual MuST - Cinema input only ( MC.Text ) , or   on complementary audio data as well via an ad-   ditional speech encoder with 12 encoder layers   ( MC.Multi ) . They trained each variant on either   monolingual data alone ( mono ) , or in a multilin-   gual setting ( multi ) . Finally , they also report results   for a variant ( OS.Text ) trained on the Open Subti-   tles corpus ( Lison et al . , 2018 ) for their zero - shot   experiments .   Evaluation . We use the subtitle - oriented metric   Sigma ( Karakanta et al . , 2022 ) , which computes the   ratio of achieved BLEU ( Papineni et al . , 2002 ) over   an approximated upper - bound BLEU score , on text   that includes line and subtitle breaks . Sigma is   meant to support the evaluation of imperfect texts ,   i.e. text that differs from the reference when breaks   are omitted . Although our approach does not pro-   duce imperfect text , achieving perfect BLEU scores   when breaks are ignored , we used this metric for   comparison purposes . We also report break cover-   age results ( Papi et al . , 2022 ) , defined as the ratio   of predicted breaks over reference breaks , which   we computed separately for the EOL and EOB   breaks . Finally , we include length conformity re-   sults ( CPL ) , measured as the percentage of subtitle   lines whose length is under the maximum number   of characters defined by the subtitle guidelines ( 42   in the TED guidelines).4 LM - based Segmentation Variants   We first compared the three methods described in   Section 2 on the English , Spanish and German   datasets , with the results described in Table 1 . In   terms of Sigma , the Insertion method obtained the   best results in all cases . It also obtained the best   scores in terms of coverage for the EOL marker , ex-   cept in Spanish , although all three variants tend to   overgenerate end - of - line markers to similar extents .   The LM - Score variant obtained the worst results in   terms of Sigma , but outperformed the alternatives   in terms of EOB coverage , a metric on which the   three variants performed markedly better than on   EOL coverage . Considering the overall results , we   selected the Insertion variant as the most balanced   one for all remaining experiments reported below .   5 Comparative Results   In Table 2 , we present the results obtained by the   selected approaches on the languages for which re-   sults were available with supervised models trained   on in - domain data . Overall , our approach outper-   formed the CountChars baseline across the board ,   and was in turn outperformed by the supervised   variants in terms of Sigma scores . Although it is   clear from these results that training segmentation   models on in - domain data , with or without audio   data , provides clear advantages in terms of sub-   title segmentation , it is worth noting that Sigma   does not , by design , reflect the actual BLEU score   without breaks , i.e. the generation of imperfect   text , which is a by - product of the above supervised   approaches and non - existent in ours . In terms   of CPL , all supervised models generate subtitle   lines that overflow the limit , to a significant degree ,   whereas the selected unsupervised models trivially   respect the length constraint.773English French German Italian   Method Training Sigma CPL Sigma CPL Sigma CPL Sigma CPL   CountChars N / A 63.71 100 % 62.87 100 % 62.34 100 % 61.49 100 %   MC.Textmono 84.87 96.6 % 83.68 96.7 % 83.62 90.9 % 82.22 90.0 %   multi 85.98 88.5 % 84.56 94.3 % 84.02 90.9 % 83.04 91.2 %   MC.Multimono 85.76 94.8 % 84.25 93.9 % 84.22 91.4 % 82.62 89.9 %   multi 87.44 95.0 % 86.49 94.1 % 86.40 89.9 % 85.33 90.0 %   MLM N / A 76.77 100 % 73.78 100 % 70.85 100 % 71.38 100 %   MLM+OC N / A 77.89 100 % 76.07 100 % 75.63 100 % 74.20 100 %   Dutch   Method BLEU Sigma CPL EOL EOB   CountChars 100 63.2 100 % -21.2 -7.1   OS.Text 89.5 64.4 71.2 % -31.4 -51.3   MC.Text 61.3 74.4 77.8 % -23.4 -9.9   MC.Multi 99.9 80.3 91.4 % -27.2 0.4   MLM 100 68.7 100 % +20.4 -10.0   MLM+OC 100 73.9 100 % +21.2 -10.0   Spanish   Method BLEU Sigma CPL EOL EOB   CountChars 100 63.2 100 % -24.6 -4.4   OS.Text 92.6 64.1 71.2 % -32.3 -45.4   MC.Text 69.6 75.8 70.1 % -47.6 -19.3   MC.Multi 99.6 78.7 91.8 % -22.4 4.7   MLM 100 73.5 100 % +13.0 -4.9   MLM+OC 100 75.6 100 % +13.4 -4.6   In Table 3 , we show the comparative results be-   tween the selected unsupervised methods and the   supervised variants , in languages where zero - shot   results were available for the latter approaches . In   this scenario , in terms of Sigma our approach ob-   tained results on a par with the supervised MC.Text   models trained on in - domain MuST - Cinema data ,   outperformed the OS.Text models trained on Open   Subtitles data , and was surpassed by the MC.Multi   model , which exploits additional audio information , by 3.1 and 6.4 points . In terms of break coverage , in   most cases our unsupervised method outperformed   the supervised variants , to a significant degree com-   pared to the text - based OS.Text andMC.Text mod-   els . Regarding BLEU scores without breaks , only   theMC.Multi model reaches a score close to the   perfect one achieved by the unsupervised models ,   whereas the MC.Text model is outperformed by   38.7 and 31.4 points in Dutch and Spanish , respec-   tively . In all cases , the CPL scores indicate that   none of the supervised approaches fully meet the   length constraint , leading to overflowing lines in   8.2 % of the cases at best and 29.9 % at worst . In   this scenario as well , the unsupervised approaches   fully meet the length constraint , by design .   Overall , overt clueing improved over our core   method by an average of 3.12 Sigma points , indi-   cating that some likely punctuation configurations   were not properly captured by our MLM approxi-   mation . In general , our approach tends to overgen-   erate EOL markers , whereas the opposite is true   for the selected supervised models . Determining   which of these tendencies leads to better subtitle   readability would require a specific human evalua-   tion which we leave for future research .   Although the zero - shot Sigma results obtained   by the supervised MC.Multi method show the po-   tential of this approach to provide pretrained mod-   els applicable to other languages , two important   aspects are worth considering . First , the available   zero - shot results were obtained on datasets in the   same domain as the data seen to train the super-   vised models . A more complete assessment of the   capabilities of these models in zero - shot settings ,   which would be the most frequent scenario consid-774ering the lack of training data across domains and   languages , would require specific evaluations in   other domains . Secondly , although segmentation is   a key aspect for subtitle readability , length confor-   mity is an equally important constraint , if not more   so considering that subtitles with lines over the   CPL limit are considered invalid in subtitling . Our   proposed unsupervised method can thus be seen   as a pragmatic approach which guarantees valid   subtitles while also providing quality segmentation   across the board .   6 Conclusions   We described an unsupervised approach to subti-   tle segmentation , based on pretrained masked lan-   guage models , where line or subtitle breaks are   inserted according to the likelihood of punctuation   occurring at candidate segmentation points .   Although supervised models , trained on in-   domain data with audio support , were shown to   perform better that this simple textual approach in   terms of the Sigma metric , they tend to generate   imperfect text to varying degrees , while also failing   to fully meet length constraints that are essential   for subtitling .   In contrast , our LM - based textual approach out-   performed supervised models in most cases in   terms of break generation coverage , while also fully   preserving the original text , complying with length   constraints , and obtaining competitive results in   terms of Sigma . This simple approach may thus   provide a highly portable complementary solution   for subtitle segmentation across languages and do-   mains .   7 Limitations   The first clear limitation of our approach is its text-   based nature . This prevents important audio infor-   mation , typically silences in speech patterns , from   being exploited to generate subtitle breaks . A more   complete system could be devised though , for in-   stance by associating our text - based approach with   the information provided by a forced alignment   toolkit , whenever audio information is available .   A simple method along these lines could be the   following : 1 . Apply our MLM - based segmenta-   tion but only generating a unique segmentation   tag SEG ; 2 . Insert EOB markers wherever thesilence between two aligned words is above a spec-   ified threshold ; 3 . Traverse the text sequentially   and replace SEG with EOL if there exists a previ-   ous marker of type EOB , otherwise replace with   EOB . We left this use of our method in combi-   nation with audio information for future research ,   as audio alignment for subtitles typically involves   additional factors such as non - literal transcriptions .   Additionally , our method is limited in its adapt-   ability to specific segmentation guidelines , which   may be company - specific . The main adaptable   parameters of our methods are the minimum and   maximum parameters of the segmentation window ,   and the set of predefined punctuation marks over   which masking is computed , neither of which could   fully model idiosyncratic segmentation guidelines .   However , in our experience at least , segmentation   in real professional data tends to display varying de-   grees of consistency with respect to guidelines , and   natural linguistic breaks seem to be the dominant   factor for subtitle segmentation . A specific evalua-   tion would be needed on data from varied profes-   sional datasets to determine the extent to which our   method might deviate from specific guidelines .   Finally , other aspects of subtitling , such as the   recommendation in some guidelines for subtitles to   appear in a pyramidal view , i.e. with the first line   shorter than the second line , have not been taken   into consideration in this work . Our aim was to   evaluate our core LM - based approach without addi-   tional variables that can vary across guidelines and   may also have led to results that are more difficult   to interpret overall . Our approach could nonethe-   less be easily augmented with constraints on rela-   tive line lengths within subtitles , by incrementing   the scores of segmentation candidates that respect   this surface - level constraint .   8 Ethical Considerations   Our approach involves the use of large pretrained   language models , whose computational perfor-   mance is typically higher when deployed in more   powerful environments with GPUs . Under such   usage , electric consumption and associated carbon   footprint are likely to increase and users of our   method under these conditions should be aware   of this type of impact . However , subtitle segmen-   tation is often performed offline , where efficient   processing is less of a concern , and lower - cost CPU   deployments are an entirely viable option . All our   results were obtained with a single large LM de-775ployed on CPU , with the aim of reducing energy   consumption at inference time .   Additionally , our method requires no training   for the task at hand and thus removes the cost of   model training associated with the supervised meth-   ods with which we compare our results . For in-   stance , Papi et al . ( 2022 ) indicate that they use four   K80 GPUs to train their models , which we took as   comparison points , with 1 day of training for their   text - only models and 1 week for their multimodal   segmenters . Therefore , given the large number of   potential language pairs and domains in need of   segmented subtitle content , our approach can pro-   vide competitive results with a comparatively lesser   impact on energy resource consumption .   Acknowledgements   We thank the anonymous reviewers for their help-   ful comments . This work was partially supported   by the Department of Economic Development and   Competitiveness of the Basque Government ( Spri   Group ) through funding for the StreAmS project   ( ZL-2021/00700 ) .   References776   A Segmentation Examples   Table 4 provides examples of subtitles in the MuST-   Cinema test sets segmented with either the charac-   ter counting baseline or our LM - based approach ,   in its insertion variant without resorting to overt   punctuation clueing .   In these examples , the MLM approach generates   end - of - line and end - of - subtitle breaks that are over-   all in line with natural linguistic breaks , contrary   to the character counting baseline . As such , on ei-   ther short , medium or longer input , the readability   of the generated subtitles is significantly enhanced   with our approach . B Extended Results   The results presented in Section 5 were limited to   the subset of languages and metrics for which pub-   lished comparative results were available on the   MuST - Cinema datasets . In Table 5 , we present the   complete list of results obtained with our method ,   for all languages and metrics . The selected variant   of our method is the insertion masking approach ,   which was selected for the main results in our paper ,   with a segmentation window starting at 15 charac-   ters and ending at 42 . We do not include BLEU   scores computed over text that includes segmen-   tation breaks , as the results are identical to those   obtained with the Sigma metric for our approach ,   which does not generate imperfect text .   Across languages , the results are relatively uni-   form , with the best Sigma scores obtained in En-   glish and the lowest in Dutch , for a difference of   4.1 points between the two languages . In terms of   break coverage , the best results were obtained for   Spanish and the worst for Romanian , although re-   sults were also relatively uniform across languages .   In all cases , overt clueing , where overt punctuation   marks raised the LM score by 1 , improved Sigma   scores , although it had less of an impact on break   coverage results , where both variants performed   similarly overall .   CResults With Different minParameters   As noted in Section 3 , considering preliminary re-   sults over the development set we selected a default   value of 15 for the minparameter , which indicates   the number of characters after which the segmenta-   tion process applies . In Table 6 , we present com-   parative results on the test sets with different min   values . In terms of Sigma , values of 15 and 20 led   to rather similar results ; values of 1 and 10 resulted   in slightly lower results , with the lowest results   achieved with the former .   In terms of < eol > and < eob > coverage , the for-   mer increases with larger minvalues , which is ex-   pected given the more restricted space to insert   these end - of - line markers as the value increases ;   for < eob > , the restricted insertion space results in   increased under - generation , which in turn results in   better scores for lower values of the minparameter.777CountChars MLM   They ’re things you access through your < eol > They ’re things you access < eol >   computer . < eob > through your computer . < eob >   Every row of data is a life whose story < eol > Every row of data is a life < eol >   deserves to be told with dignity . < eob > whose story deserves to be told < eob >   with dignity . < eob >   During the winter , struggling to get < eol > During the winter , struggling to get warm , < eol >   warm , my neighbors would have no choice < eob > my neighbors would have no choice < eob >   but to bypass the meter after their heat < eol > but to bypass the meter < eol >   was shut off , just to keep their family < eob > after their heat was shut off , < eob >   comfortable for one more day . < eob > just to keep their family comfortable < eol >   for one more day . < eob >   Language Method BLEU Sigma EOL EOB CPL   DEMLM 100 70.85 18.53 -7.96 100 %   MLM+OC 100 75.63 19.81 -7.78 100 %   ENMLM 100 76.77 19.18 -9.91 100 %   MLM+OC 100 77.89 19.86 -9.73 100 %   ESMLM 100 73.47 12.98 -4.91 100 %   MLM+OC 100 75.59 13.45 -4.63 100 %   FRMLM 100 73.78 16.51 -6.58 100 %   MLM+OC 100 76.07 17.47 -6.12 100 %   ITMLM 100 71.38 18.49 -9.55 100 %   MLM+OC 100 74.20 20.34 -8.57 100 %   NLMLM 100 68.71 20.37 -9.96 100 %   MLM+OC 100 73.88 21.22 -9.96 100 %   PTMLM 100 71.59 20.03 -10.81 100 %   MLM+OC 100 75.50 19.87 -10.02 100 %   ROMLM 100 69.45 23.37 -10.44 100 %   MLM+OC 100 74.13 23.37 -10.09 100%778Language min BLEU Sigma EOL EOB   DE1 100 72.31 28.75 -0.18   10 100 73.96 22.68 -4.43   15 100 75.63 19.81 -7.78   20 100 75.28 14.54 -11.21   EN1 100 74.30 37.33 -0.98   10 100 77.14 24.49 -7.77   15 100 77.89 19.86 -9.73   20 100 77.16 15.24 -12.68   ES1 100 73.00 20.87 0.28   10 100 74.32 18.24 -2.04   15 100 75.59 13.45 -4.63   20 100 75.83 8.66 -7.87   FR1 100 73.89 24.68 -0.73   10 100 75.26 20.83 -3.93   15 100 76.07 17.47 -6.12   20 100 76.75 12.5 -10.05   IT1 100 72.01 29.75 -3.66   10 100 73.75 24.71 -6.61   15 100 74.20 20.34 -8.57   20 100 73.66 14.62 -11.61   NL1 100 72.16 26.83 -5.47   10 100 73.56 23.26 -8.47   15 100 73.88 21.22 -9.96   20 100 74.40 16.81 -12.43   PT1 100 72.87 26.38 -6.24   10 100 74.53 22.15 -8.08   15 100 75.50 19.87 -10.02   20 100 74.98 14.17 -13.36   RO1 100 72.05 32.3 -4.51   10 100 73.76 26.98 -7.52   15 100 74.13 23.37 -10.09   20 100 74.89 17.53 -12.83779ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   7   /squareA2 . Did you discuss any potential risks of your work ?   8   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Not applicable . Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Not applicable . Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Not applicable . Left blank .   C / squareDid you run computational experiments ?   3   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   We did n’t trained any models for this paper , and inference was performed on CPU.780 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   3   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   3   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.781