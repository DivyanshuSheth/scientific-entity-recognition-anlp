  Yichen JiangXiang ZhouMohit Bansal   UNC Chapel Hill   { yichenj , xzh , mbansal}@cs.unc.edu   Abstract   Recent datasets expose the lack of the system-   atic generalization ability in standard sequence-   to - sequence models . In this work , we analyze   this behavior of seq2seq models and identify   two contributing factors : a lack of mutual ex-   clusivity bias ( one target sequence can only   be mapped to one source sequence ) , and the   tendency to memorize whole examples rather   than separating structures from contents . We   propose two techniques to address these two is-   sues respectively : Mutual Exclusivity Training   that prevents the model from producing seen   generations when facing novel examples via   an unlikelihood - based loss , and prim2primX   data augmentation that automatically diversi-   fies the arguments of every syntactic function   to prevent memorizing and provide a compo-   sitional inductive bias without exposing test-   set data . Combining these two techniques ,   we show substantial empirical improvements   using standard sequence - to - sequence models   ( LSTMs and Transformers ) on two widely - used   compositionality datasets : SCAN and COGS .   Finally , we provide analysis characterizing the   improvements as well as the remaining chal-   lenges , and provide detailed ablations of our   method .   1 Introduction   Human intelligence demonstrates systematic com-   positionality , the algebraic capacity to understand   and produce a potentially infinite number of novel   combinations of known components ( Chomsky ,   1957 ; Montague , 1970 ) , in comprehending and   generating natural language . However , a number   of recent datasets , e.g. , SCAN ( Lake and Baroni ,   2018 ) , COGS ( Kim and Linzen , 2020 ) , etc . , pro-   vide clear evidence of the lack of systematic com-   positionality in state - of - the - art neural networks . Byanalogy with what humans do in meaningful learn-   ing ( Ausubel , 1963 ; Shi et al . , 2022 ) , methods to   solve this problem can be categorized into deduc-   tive learning andinductive learning methods . In de-   ductive learning , the learner is first introduced to a   general rule and then followed by specific examples   where the rule is applied ( Thornbury , 1999 ) , while   ininductive learning , the learner is first presented   with abundant examples so that the learner can auto-   matically infer the general rule by itself . Under the   context of compositional generalization , deductive   learning approaches directly inject useful priors ( Li   et al . , 2019 ; Russin et al . , 2020 ; Liu et al . , 2020 ,   2021 ) to the model ; while inductive learning ap-   proaches utilize data augmentation(Andreas , 2020 ;   Akyürek et al . , 2021 ) to provide more accurate   examples that facilitate the learning of composi-   tionality . In this work , we focus on designing meth-   ods that do not require any specific architectural   changes , so it is applicable to standard sequence-   to - sequence ( seq2seq ) models , and propose both   deductive and inductive learning paradigms .   Our deductive method is derived from an ob-   servation that when facing compositionally novel   examples , models have a tendency to generate old   patterns seen during training , hence leading to nu-   merous mistakes . Despite being a major challenge   for models , humans avoid these mistakes by ex-   ploiting the mutual exclusivity ( ME ) bias ( Mark-   man and Wachtel , 1988 ) , i.e. , if a concept is already   associated with one expression , humans are less   likely to associate a new expression to that concept .   Consequently , when given an unseen test input ( ex-   pression ) , humans have the prior that this new input   should not be mapped to a seen output ( concept )   that is already mapped to another input expres-   sion . Inspired by the ME bias , we propose a novel   training framework Mutual Exclusivity Training   ( MET ) . Given an input - output pair , MET encour-   ages the model to assign a high probability to the   output given the input , and to notgenerate this out-11778put given any other inputs . We achieve this by first   creating a randomly perturbed version of the input ,   and then adopting the unlikelihood loss ( Welleck   et al . , 2019 ) on the perturbed input to penalize   the generation of the original output . Finally , we   train the model using a joint loss of MLE on the   original batch and the unlikelihood loss . Addi-   tionally , we explore another variant MET - Meta ,   where we embed the unlikelihood loss as the meta   generalization objective of the MAML framework   ( visualized in Fig . 1 ) . This meta - learning objec-   tive will provide more regularization on the gra-   dient updates instead of the final optimization tar-   get . On COGS and SCAN , we see substantial im-   provements from both variants of MET as long as   the baseline seq2seq model has acceptable perfor-   mance . MET improves a Transformer model from   76.1 % to 80.6 % on COGS , and improves an LSTM   model from 13.5 % to 38.7 % on SCAN MCD2 . The   two MET variants also show interesting behavior   differences . For Transformers , MET shows advan-   tages on COGS , and MET - Meta show advantages   on most of the SCAN tasks .   Despite the above improvements , a Transformer   model equipped with MET may still struggle with   very poor baselines . We hypothesize that this phe-   nomenon may partially be because that the syn-   tactic function to be generalized has not been ap-   plied to a sufficient number of distinct arguments   in the training set . For instance , in the SCAN   Jump , the function “ around left ” is only paired   with four different lexical arguments ( “ walk , run ,   look , turn ” ) . This lack of argument diversity makes   it easier for the model to memorize all these pat-   terns , but harder to infer the syntactic rules with   deductive learning . To alleviate this memorization   problem , we propose prim2primX , a data aug-   mentation method to promote generalization by   automatically generating a large set of new lexi-   cal arguments for each syntactic function . As the   number of distinct arguments for each function in-   creases , memorizing each single example indepen-   dently becomes more challenging , while the diffi-   culty of understanding the compositional structure   remains the same , which makes it a more encourag-   ing behavior . Specifically , the prim2primX proce-   dure first builds a lexicon using a dataset - agnostic ,   rule - based word alignment algorithm . This lexicon   contains the mapping between the input and the   output form of each argument ( e.g. , “ run∝ ⇕ ⊣√∫⊔≀→RUN ” ) ,   while ignoring functional words ( e.g. , “ around ” ) that only decide the syntactic structure of the out-   puts . Then , to enrich the set of lexicons , we mutate   each primitive ( “ run∝ ⇕ ⊣√∫⊔≀→run0 ; RUN∝ ⇕ ⊣√∫⊔≀→RUN0 ” ) , and   create new examples by swapping the primitive ar-   gument ( prim ) with its mutated form ( primX ) on   both sides . Our experiments ( see Sec . 4 ) provide ev-   idence that supports our claim above : with 15 new   primitives ( 5 per original primitive ) added to the   SCAN lexicon , the accuracy on the SCAN Jump   test set reaches 63.65 % from the original perfor-   mance of 3.49 % . On COGS , with 2 new primitives   per original one , prim2primX also improves the   Transformer performance from 76.14 % to 80.07 %   accuracy . Furthermore , we show that combining   prim2primX and MET can achieve further improve-   ment . Our prim2primX+MET achieves 81.1 % ac-   curacy on COGS , and prim2primX+MET - Meta   achieves 74.0 % on SCAN Jump . Finally , we also   provide detailed ablations about each component   in our proposed methods , and an analysis character-   izing our improvements and remaining challenges .   Overall , our contribution is two - fold : ( 1 ) we   propose MET , a novel , deductive training frame-   work to inject mutual exclusivity bias into models ;   and ( 2 ) we propose prim2primX , a data augmen-   tation method that automatically creates new argu-   ments to facilitate inductive compositionality learn-   ing . Both methods and their combination show   substantial empirical improvements on SCAN and   COGS . Moreover , our methods are data and model-   agnostic , and do not leak any test examples , so the   improvements reveal the compositional generaliza-   tion potential of general seq2seq models .   2 Background   2.1 Compositional Generalization Challenge   Compositional generalization challenges test mod-   els with unseen combinations of seen structures   and seen contents . While neural models perform   well on in - distribution examples , they fail on most   compositionality challenges with a distributional   shift between training and test examples even when   some surface statistics ( e.g. , word frequency ) are   similar . One example dataset used in this work is   SCAN ( Lake and Baroni , 2018 ) , which includes   paired natural language command and action se-   quence outputs . We can represent this task as ap-   plying syntactic functions to the arguments .   Definition 1 . Asyntactic function fis a symbolic   function which maps certain input patterns to cor-   responding output structures.11779   Table 1 lists some example syntactic functions   and corresponding input - output pairs in SCAN .   The main challenge in SCAN Jump is to apply   functions like Around ( x , left)to a lexical argu-   ment “ jump ” , while such combination does not   appear in the training set . Another dataset we use   in this work is COGS ( Kim and Linzen , 2020 ) ,   which requires parsing a diverse set of natural lan-   guage sentences into their corresponding logical   forms . COGS raises five different generalization   challenges : syntactic functions with novel ( 1 ) prim-   itives or ( 2 ) modified phrases ; ( 3 ) deeper recursion ;   ( 4 ) alternative verb argument ; and ( 5 ) novel iden-   tification of a verb . Challenges 1 , 4 , and 5 require   generalizing to unseen lexicons , while challenges   2 and 3 require generalizing to unseen structures .   2.2 Limitations of Standard Seq2Seq Models   Standard seq2seq models often struggle facing   compositional challenges and fall into a common   mistake pattern . For example , given a test com-   mand “ jump left twice ” that requires generaliz-   ing a syntactic function to “ jump ” , Transformer   sometimes generates the correct output structure   but mistakenly picks another familiar primitive   ( e.g. , “ walk ” ) over “ jump ” , yielding “ TLWALK   TLWALK ” . This suggests that during the training ,   Transformer is not separately mapping the func-   tion “ xleft twice ” to the output structure “ TLxTL   x ” ; and the argument “ walk ” to the action “ WALK ” .   As a result , when facing a novel command ( e.g. ,   “ jump left twice ” ) , in 79.8 % of test cases , the model   does not generate “ JUMP ” but instead repeats a   seen training - set output . Additionally , in more than   63 % of the test examples , the model can not even   correctly interpret the functional structure , e.g. , it   generates the structure for “ jump around left and   run ” when it is required to “ jump around left and   run thrice ” . In contrast , humans ( 1 ) have a natural   prior to not map two different sentences to the same   point in the output space ( Markman and Wachtel ,   1988 ) ; and ( 2 ) are able to correctly separate syntac-   tic structures from the actual contents ( Lake et al . ,   2019 ) . In the next section , we will show how we   relieve this problem .   3 Methods   3.1 Injecting Mutual Exclusivity Bias   One crucial factor for humans to prevent the   mistakes in Sec . 2.2 is the mutual exclusivity   bias ( Markman and Wachtel , 1988 ): if a concept is   already associated with one expression , a human   is less likely to associate a new expression to that   concept . Under the context of seq2seq tasks , this   bias should lead to the following behavior : in a bi-   jective function task ( i.e. , the input and the output   have one - to - one correspondence ) , when facing an   unseen input , the model should not produce a seen   output that is already associated with a seen input .   Formally , we can represent it as :   Property 1 . Given a bijective training set Dcon-   taining ninput - output pairs ( x , y ) . For a given   test example ˆx , if∀i,ˆx̸=x , then the label for the   test example ˆywill have the property ∀i,ˆy̸=y .   Following this formalization , we propose a novel   framework Mutual Exclusivity Training ( MET ) to   inject this property into standard seq2seq models .   MET with Unlikelihood Loss . We notice that   Property 1 can be achieved in a model - agnostic11780way by penalizing the predicted likelihood on ˆy ,   hence becoming a variant of unlikelihood train-   ing(Welleck et al . , 2019 ) . Intuitively , during train-   ing , for a given mini - batch ( x , y ) , we sample some   negative inputs ˜x̸=x , which are similar to the   original input xbutdo not map to the correspond-   ing target y ; and then we encourage the model to   notgenerate ygiven ˜x . Specifically , we train our   model on the sum of maximum likelihood estima-   tion ( MLE ) and unlikelihood loss :   L=−log(p(y|x)),L=−log(1−p(y|˜x ) )   Notably , this unlikelihood loss Lis different   from the loss used in Welleck et al . ( 2019 ) , as the   original unlikelihood is an average over all possible   words , while the unlikelihood loss in Eq . 3.1 is op-   erated at the sentence level . We adopt this change   since sentence - level unlikelihood provides a suit-   able amount of regularization in our setting , while   the regularization from the original word - level loss   is too strong . See Appendix C for more discussions ,   proof - of - concept experiments and ablation results .   MET - Meta . MET encourages Property 1 in the   final trained model . Alternatively , Property 1 can   also be viewed as a regularization on the learn-   ing behavior , i.e. , once a model is trained on a   pair(x , y ) , it will give a lower probability to the   pair with the same output but a different input   ( ˜x , y ) . We propose a variant : MET - Meta that   achieves this effect by embedding the unlikelihood   loss in a MAML ( Finn et al . , 2017 ; Conklin et al . ,   2021 ) framework . In MAML , each batch consists   of two sets of examples , ‘ meta - train ’ and ‘ meta-   test ’ . MAML encourages the model optimized   on the meta - train examples to also optimize the   loss on the meta - test , thus encouraging more gen-   eralizable behaviors . In MET - Meta , we use un-   likelihood loss as the loss on meta - test . Specif-   ically , with a sampled mini - batch ( x , y)(meta-   train ) , we first calculate the MLE loss and per-   form an SGD update θ = θ−α∇L(θ ) ,   where αis the step size . Next , we use the unlike-   lihood loss on meta - test ( ˜x , y)as the loss for the   meta optimization w.r.t . the updated parameters θ :   L(θ ) = −log(1−p(y|˜x ) ) . Finally , we opti-   mize the sum of the MLE loss L(θ)and the   meta unlikelihood loss L(θ)w.r.t . the original   parameter θto get the updated parameter θ :   L(θ ) = L(θ ) + L(θ )   = L(θ ) + L(θ−α∇L(θ ) )   θ = θ−β∇L(θ)αandβare the step sizes for the two updates . A   visualization of MET and MET - Meta is in Fig . 1 .   Selecting the Best Negative Examples ˜x . Simi-   lar to other training schema using multiple exam-   ples ( e.g. , contrastive learning ( Gao et al . , 2021 ) ,   meta learning ( Conklin et al . , 2021 ) , etc . ) , MET   also depends on the quality of the negative exam-   ples˜x . One notable difference is that MET only   uses input side ˜x , and does not need the corre-   sponding output ˜y . This allows us to remove the   constraint of only using seen training examples ,   and to use any good negative examples as long as   it is similar to the positive example but semanti-   cally different . To achieve a similar goal , Conklin   et al . ( 2021 ) mine examples from the training set   via similarity metrics . However , the input distribu-   tion of these mined examples is constrained to the   training set , and inherits the highly skewed train-   ing distribution . To overcome this limitation , we   directly perturb the original training examples x   to create ˜x . Specifically , we randomly select one   token in the training example , and replace the se-   lected word with another word . In our preliminary   experiments , we notice that it is important ( 1 ) to   maintain the grammaticality of the perturbed sen-   tence , as training on ungrammatical sentences can   make the model easily overfit to the grammar er-   rors ; and ( 2 ) to have access to the whole vocabulary   during training so that we can minimize the influ-   ence of a highly skewed training set distribution ( a   related discussion is at Sec . 7 ) . Hence , we replace   the selected words with other words sharing the   same grammar properties . In this work , we do not   delve in the direction of clustering the words with   the same grammar properties and use simple ap-   proaches to test our ideas . We use an off - the - shelf   tagger ( Bird et al . , 2009 ) to obtain the tags of words   in COGS , and we cluster the words in SCAN to-   gether if they share the same immediate contexts . A   similar effect can be obtained on more complicated   datasets by several different approaches ( Stratos ,   2019 ; Akyürek and Andreas , 2022 ) .   3.2 Diversifying Lexical Arguments with   Primitive Augmentation   While our empirical results demonstrate the effec-   tiveness of MET , we also notice that MET does not   work on extremely weak baselines . In the training   set of SCAN Jump , where a Transformer scores   a poor 3.5 % accuracy and does not improve with   MET , each syntactic function ( e.g. , “ around left ” ) 11781is only paired with four different lexical arguments   ( “ walk , run , look , turn ” ) . Such lack of argument   variability in the dataset makes it very easy for the   model to memorize all input - output mappings in-   dependently . We hypothesize that current datasets   do not provide enough incentives for the model   to infer the functional structures with inductive   learning .Specifically , in order for a neural model   to correctly learn a syntactic function fthat can   generalize to all the suitable arguments , the dataset   must meet two hypotheses .   Hypothesis 1 ( Sufficient stimuli for function gen-   eralizability ) .fmust be applied to a sufficient   number of different arguments in the training set .   Satisfying Hypothesis 1 ensures that models rec-   ognize fas a generalizable function instead of a   special phrase ( e.g. , idioms ) to memorize indepen-   dently . To further generalize fto an unseen argu-   ment a , the dataset must suffice another hypothesis :   Hypothesis 2 ( Sufficient stimuli for argument   equivalence ) .Suppose during training , function f   is applied to a set of arguments B. Then in order   to apply fto an unseen argument a , one of the   following two conditions must hold :   2.Aaand another argument b∈Bboth appear   in a sufficient number of distinct functions ;   2.Bamust appear in the same function with a   sufficiently large subset ¯B⊆B.   This hypothesis makes sure that there is enough   stimuli to infer the syntactic equivalence between   the new argument aand arguments in B , so that f   can generalize to a. For example , in SCAN Around   Right , primitives “ left ” and “ right ” both appear   in a number of functions like “ walk left / right ” ,   “ jump opposite left / right ” , which meet Hypothe-   sis 2.A above and help demonstrate the equiva-   lence of “ left ” and “ right ” . Therefore , to verify   the effectiveness of these hypotheses , we propose   theprim2primX data augmentation procedure to   automatically enlarge the set of distinct lexical ar-   guments that are applied to a syntactic function .   and lead to empirical improvements in Sec . 4 . We   explain this procedure in the next few paragraphs   ( also see Alg . 1 in Appendix A ) .   Building a Lexicon . We first use a dataset-   agnostic , rule - based word alignment algorithm to   build a lexicon that maps a primitive ’s input form   to its logical form ( e.g. , “ run∝ ⇕ ⊣√∫⊔≀→RUN ” ) , while ig-   noring functional words ( e.g. , “ around ” ) that onlydecide the syntactic structure of the outputs . We   denote the source vocabulary as Vand the target   vocabulary as W. The general intuition is that we   want to find pairs ( v , w ) , v∈V , w∈Wsuch that   the presence of vin the input is both necessary and   sufficient for the presence of win the output .   suff(v , w ) = ∀(x , y),(v∈x)− →(w∈y )   ness(v , w ) = ∀(x , y),(w∈y)− →(v∈x)(1 )   Enforcing these two conditions on the SCAN Jump   task returns 6 primitive pairs , e.g. , ( run , RUN ) ,   ( walk , WALK ) , etc . Successfully identifying them   allows us to later swap them with new primitives   to enlarge the set of distinct arguments applied to   a function . However , for tasks with a larger vo-   cabulary , a word ’s different forms are often parsed   to the same output token . Hence we use the “ no-   winner ” condition ( Akyurek and Andreas , 2021 )   that only enforces suff(v , w)and allows many - to-   one mappings . This relaxation makes it possible   to build a comprehensive lexicon on more compli-   cated datasets ( e.g. , COGS ) as it allows mappings   like “ walk ∝ ⇕ ⊣√∫⊔≀→WALK ; walked ∝ ⇕ ⊣√∫⊔≀→WALK ” that share   the same target token . We refer to Appendix A.1   for details .   Mutating Primitives . With a primitive lexicon ,   we go over each example in the training set and   randomly mutate some primitives ( prim ) by adding   a suffix to their source and target forms ( primX ) .   Given an original example “ walk left twice ” , we   select a primitive “ walk ” and mutate it to get new   examples like “ walk0 left twice ∝ ⇕ ⊣√∫⊔≀→TLWALK0 TL   WALK0 ” . This mutation operation can enhance Hy-   pothesis 1 by creating more primitive arguments   for each function . It also improves Hypothesis 2.B   by creating a larger argument set Bthat is ap-   plied to the identity function . We argue that as   the number of distinct arguments for each syntactic   function increases , it becomes more challenging   to memorize the corresponding output of each in-   put , while the difficulty of compositionally sepa-   rating function structures from argument symbols   remains the same . Therefore , compositional gen-   eralization would become a more favorable solu-   tion to the model . Compared to previous data aug-   mentations ( Andreas , 2020 ; Akyürek et al . , 2021 )   prim2primX has two advantages : ( 1 ) it avoids the   difficult task of mining semantically equivalent   input - output pairs that can be swapped as it brings   in new primitives ; ( 2 ) hence the augmented data   will never reveal any test compositions , which leads   to the model ’s generalization to future primitives.11782   4 Experiments   4.1 Experimental Setup and Baselines   We report results for both LSTMs and Transformers   in our SCAN experiments . We split the original   test set into a new development set that contains   10 % of the examples and a new test set containing   the rest 90%.We select the checkpoint with the   best dev - set accuracy . We refer to Appendix B.3   for more details of our experimental setup .   Additionally , in our experiment tables , we pro-   vide results for previous representative methods   for a more complete picture of the comparison .   We show results of the meta - learning method used   in Conklin et al . ( 2021 ) as “ + MAML ” in the ta-   bles . To the best of our knowledge , this is one of   the few baselines that do not require task - specific   architecture changes to the model , similar to our   method . Note that while both our method and Con-   klin et al . ( 2021 ) use the MAML framework ( Finn   et al . , 2017 ) , the training objectives are different :   the meta - loss in Conklin et al . ( 2021 ) is the stan-   dard MLE loss , while ours is the unlikelihood loss .   We discuss the combination of both ideas in Sec 4.4 .   Similar to the finding in Csordás et al . ( 2021 ) , we   notice different baseline configurations can have   a substantial influence on results , hence we report   the reimplemented MAML on our baselines us-   ing the Levenshtein distance for a fair compari-   son . Additionally , for experiments on SCAN , we   provide results from previous work with top task - specialized model architecture ( Li et al . , 2019 ; Liu   et al . , 2020 ) , large pretrained model ( Furrer et al . ,   2020 ) , and state - of - the - art data augmentation ( An-   dreas , 2020 ) . On COGS , we provide results for   Lex Learn ( Akyurek and Andreas , 2021 ) , which   enhances a normal LSTM model with a special   copying mechanism . Many of these baselines ( e.g. ,   CGPS , T5 - 11B , LANE , Lex Learn ) require special   architecture changes or pretraining models , so they   are not directly comparable to our method .   4.2 COGS Experiment   Our results on COGS are shown in Table 2 , and   they also demonstrate the effectiveness of both   MET and prim2primX. MET improves the Trans-   former performance from 76.14 % to 80.64 % , out-   performing MAML and MET - Meta ; prim2primX   can bring additional improvements by creating ex-   tra lexical arguments , and pushes the best perfor-   mance to 81.12 % . More result discussions and   an example showing how prim2primX works on   COGS are shown in Appendix D.   Next , to have a better understanding of the be-   havior of current models , we break down the model   accuracy on COGS to five different challenges . As   explained in Sec . 2.1 , challenges 1 , 4 , and 5 focus   on generalizing a learned syntactic function to an   unseen lexical argument , while challenges 2 and 3   focus on generalizing to unseen phrases ( structural   arguments ) . We notice both MET and prim2primX   improve the generalization accuracy on challenges   1 , 4 , 5 , reaching over 90 % on all three challenges ,   showing promising progress in lexical generaliza-   tion . However , challenges 2 and 3 remain to be   two formidable tasks , as all our models constantly   fall below 1 % accuracy in test examples with novel11783   structural arguments or deeper function recursion .   We discuss this limitation and future remedial ap-   proaches in Sec . 7 .   4.3 SCAN Results   Generalizing functional compositions to new   lexical arguments . We report results on SCAN   Jump andAround Right in Table 3 . Both the LSTM   and Transformer baselines perform poorly on gen-   eralizing learned functions to unseen lexical argu-   ments , reaching < 20 % accuracy on Around Right   and < 4 % accuracy on Jump . We first focus on   the 2nd and 3rd row - groups in Table 3 . On the   Around Right task , with moderate baselines , MET   can improve both Transformer ( from 19.86 % to   32.41 % ) and LSTM baselines ( from 10.33 % to   26.77 % ) . Compared to the other deductive MAML   methods ( Conklin et al . , 2021 ) , the MET is bet-   ter for LSTMs , but worse on Transformers . How-   ever , with an extremely weak baseline ( < 4 % ac-   curacy ) , none of the deductive methods success-   fully improves the model on SCAN Jump . Next ,   the bottom two row - groups present the results us-   ing prim2primX data augmentation . By adding   in total 15 additional new lexical arguments ( not   from the test dataset ) , prim2primX substantially im-   proves the results on both LSTM and Transformer   baselines . Most notably , it improves the Trans-   former performance from 3.49 % to 63.65 % on   SCAN Jump , and from 19.86 to 84.50 on Around   Right . With stronger baselines on Jump , our de-   ductive methods can bring additional improvement .   MET - Meta improves the performance from 3.34 %   to 7.51 % for LSTM , and from 63.65 % to 73.94 %   for Transformers . On these two tasks , we also see   MET - Meta often outperforms the standard MET .   Generalizing to future unseen primitives . Com-   pared to previous data augmentation methods ( An-   dreas , 2020 ; Akyürek et al . , 2021 ) , our prim2primX   has one crucial advantage : as it replaces original   words with new primitives , it only encourages the   model to induce compositionality from data and   does not leak any test compositions . Oppositely ,   previous methods learn rules to uncover possible   test examples , while models still struggle to gen-   eralize beyond the augmented data . As a result ,   models trained with prim2primX will generalize   to future unseen primitives better . To demonstrate   this , we design a new task Dax where we add the   same number of “ dax∝ ⇕ ⊣√∫⊔≀→DAX ” into a few augmented   SCAN Jump training sets and evaluate on test ex-   amples like “ dax around left twice ” . Table 4 show   that prim2primX maintains similar improvements   onDax asJump , while models trained with GECA   and Recombine perform poorly on Dax.11784   Generalizing to new structural arguments   ( MCD Tasks ) . The results on three SCAN MCD   tasks are presented in Table 5 . As opposed to the   two tasks above that focus on generalizing func-   tions over lexical arguments , the MCD challenges   additionally require the model to generalize syntac-   tic functions to structural arguments , which contain   their own hierarchical structures . We use LSTM   as our baseline because previous work ( Conklin   et al . , 2021 ) has shown that it outperforms Trans-   former significantly on SCAN MCD tasks , hence is   more suitable for our deductive learning improve-   ments . On all three tasks , MET brings substantial   improvements to the baseline , with over 12 % of av-   erage improvement , and both MET variants show   similar performances and outperform the MAML   method ( Conklin et al . , 2021).These results suggest   that MET can improve the generalization to not   only new lexical arguments , but also new structural   arguments . We do not observe any improvement   by training the LSTM on our augmented data that   only create new lexical arguments . We discuss this   limitation and possible future direction in Sec . 7 .   4.4 Ablation Studies   We investigate the combination of deductive meth-   ods ( MET and MAML ) , and the effect of different   numbers of new arguments . More ablations about   the unlikelihood loss are in Appendix C.2 .   Combination of MET and MAML . Both MET   and the MAML loss used in Conklin et al . ( 2021 )   aim to improve the deduction learning ability of   models . Since the two methods come from two   different angles , in theory they should also be com-   plementary to each other . However , in Table 6 , we   see a mixed trend by combining MET and MAML .   For example , on COGS , using both unlikelihood   and MAML provide no additional gain , while there   does appear to be sizable improvements on SCAN   Around Right , with around 10 % improvement on   MET and 24 % over MAML . These mixed trends   indicate that while two deductive methods can have   complementary improvements in some situations ,   but there also exist common limitation that is un-   solvable to both methods and can lead to a dimin-   ishing gain from combining them .   Data Augmentation Ablation . We study how   the different number of new arguments in   prim2primX improves inductive learning of a   Transformer . In Table 7 , there is an overall trend   that the model ’s performance improves as the num-   ber of new primitives and total training data in-   creases . However , the gain from using additional   data also saturates after a certain amount . The   Transformer baseline requires 2 new primitives per   original one to converge to its best performance   on SCAN Jump and COGS , while LSTM needs   5 new primitives to achieve a significant improve-   ment on Jump . Hence , in our experiments , we use 2   new primitives for COGS , and 5 new primitives for   SCAN , where LSTM reaches its best performance ,   and Transformers still have decent accuracy .   4.5 Qualitative Analysis   Finally , we present a qualitative analysis into how   effective are MET and prim2primX in inducing   compositionality in Transformer and reduce the   errors discussed in Sec . 2.2 . Specifically , we in-   spect the generated outputs of the multiple best-   seed models on SCAN Jump test set in Table 8 . We   categorize model errors into two categories : primi-   tive errors , and structural errors . In primitive errors ,   the models generate the correct structure , but only   choose the wrong primitive ( e.g. , the model gen-   erates the output of “ walks left twice ” when the   input is “ jump left twice ” ) . In structural errors , the   models make more significant errors in the gener-   ated structure of the output . In Table 8 , 22.99 % of   Transformer baseline ’s errors are primitive error.11785However , Training the model with prim2primX can   reduce the error rate to 0.52 % , and further to 0.16 %   by also using MET - Meta . The improvements sug-   gest that prim2primX and MET can effectively pre-   vent the model from associating a familiar concept   to a new expression . Our methods reduce the struc-   tural error rate to 25.9 % but can not completely   eliminate them . How to further reduce structural   errors remains a future direction .   5 Related Work   Compositional Generalization . Early work has   studied neural networks ’ ability in systematic be-   havior ( Wong and Wang , 2007 ; Brakel and Frank ,   2009 ) in language learning , compositional count-   ing ability ( Wiles , 1998 ; Weiss et al . , 2018 ) and   syntax learning ability ( Linzen et al . , 2016 ) . Many   recent compositional generalization datasets ( Lake   and Baroni , 2018 ; Kim and Linzen , 2020 ; Loula   et al . , 2018 ; Liška et al . , 2018 ; Bastings et al . ,   2018 ; Keysers et al . , 2020 ; Tsarkov et al . , 2021 )   greatly facilitate research in this direction . Pre-   vious research explored different ways to tackle   the compositionality challenge , including grammar-   based approaches ( Nye et al . , 2020 ) , separating   syntax and semantics ( Li et al . , 2019 ; Russin et al . ,   2020 ) , architecture improvements ( Dessì and Ba-   roni , 2019 ; Gordon et al . , 2020 ; Oren et al . , 2020 ;   Zheng and Lapata , 2021 ; Herzig et al . , 2021 ; Shaw   et al . , 2021 ) , task decomposition ( Liu et al . , 2020 ,   2021 ) , semi - supervised learning ( Guo et al . , 2021 ) ,   multi - task learning ( Jiang and Bansal , 2021 ) , meta-   learning ( Lake , 2019 ; Conklin et al . , 2021 ) , etc .   Data Augmentation . Most data augmentation ( An-   dreas , 2020 ; Akyürek et al . , 2021 ) methods pro-   mote generalization by creating extra data that re-   semble the test - set distribution , hence simplifying   the problem . Qiu et al . ( 2021 ) sample new combi-   nations of structures from a context - free grammar   induced from the training data . Contemporarily ,   Akyürek and Andreas ( 2022 ) adopt a similar pro-   cedure to ours . They detect a primitive lexicon and   then swap an original primitive with another one   of the same types . In comparison , prim2primX   focuses on introducing new primitives , and hence   not exposing any novel test - set compositions . Also   concurrently , Patel et al . ( 2022 ) create new primi-   tives ( e.g. , “ swim ” , “ clap ” ) and replace the original   primitives ( e.g. , “ walk ” ) with these new ones toaugment the training data . Similar to our findings ,   they reveal that the number of different lexical ar-   guments greatly affects the model ’s generalization .   They further show the limit of models under MLE   training on SCAN Jump using larger models and   much more augmented primitives . We instead fo-   cus on proving the effectiveness of inductive and   deductive methods on widely - used baseline config-   urations and multiple datasets .   Mutual Exclusivity Bias . Mutual exclusivity ( ME )   bias helps children acquire the meaning of new   words ( Markman and Wachtel , 1988 ) . However ,   vanilla neural networks do not possess this prop-   erty ( Gandhi and Lake , 2020 ) . Recently , a num-   ber of works explore to inject ME bias into the   models , mostly focusing on cross - situation word   learning ( Kádár et al . , 2015 ; Lazaridou et al . , 2016 ;   Gulordava et al . , 2020 ) . On a high level , our work   is most similar to Gulordava et al . ( 2020 ) , where   they use a margin - based loss for feed - forward net-   works in a word learning experiment . In compari-   son , our work uses an unlikelihood - based method   for seq2seq generation problems . Some composi-   tional generalization methods are also connected   to ME biases ( Lake , 2019 ; Akyurek and Andreas ,   2021 ) , but their specific architecture may not be   generalizable to standard seq2seq models .   6 Conclusion   We propose two methods to improve compositional   generalization : ( 1 ) MET , a mutual exclusivity bias-   inspired method implemented with unlikelihood   training ; and ( 2 ) prim2primX , a data augmenta-   tion procedure automatically diversifying the ar-   guments of syntactic functions . Empirical results   demonstrate the effectiveness of both methods .   7 Limitations   In this section , we discuss the limitations of our   methods and point out promising future directions   and challenges . In our experiments , we observe   MET does provide substantial improvements on   SCAN and COGS by injecting a certain amount of   ME bias into the models . Nonetheless , the effect   of ME bias achieved by MET is not equal to the   ME bias of humans . MET regularizes the model to   not generate seen outputs ( or expressions ) facing   unseen inputs ( or concepts ) . Human ME biases ,   however , will additionally let humans assign two   different unseen expressions ( or outputs ) to two   different unseen concepts ( or inputs ) . To summa-11786rize , MET achieves ME bias on the trained domain ,   which already leads to substantial improvement on   SCAN and COGS . Future work should explore di-   rections to achieve generalizable ME bias , which   may lead to a further breakthrough in this task .   Additionally , as discussed in Sec . 4.2 , one re-   maining challenge of our models is to improve   generalization to novel structural arguments , where   all the models achieve close to zero accuracy . Sim-   ilar to the trends on other datasets , the extremely   low performance of the baselines makes it hard   for our deductive method MET to improve . On   the other hand , our prim2primx data augmentation   has the potential to improve similar problems , as   reflected in the SCAN Jump results . However , the   success depends on automatically identifying lex-   icon - level arguments . In contrast , automatically   identifying any phrase -level structures is still an   open challenge , and we leave this for future work .   8 Ethical Considerations   The methods proposed in this paper aim to im-   prove the compositional generalization ability of   the models . Progress in this direction can lead to   more systematic and predictable behavior of neu-   ral models in downstream applications . In this   work , improvements are evaluated on two highly-   controlled datasets , SCAN ( Lake and Baroni , 2018 )   and COGS ( Kim and Linzen , 2020 ) as proof - of-   concept experiments . We also point out potential   challenges and directions to apply our methods in   practice in Sec . 7 .   Acknowledgements   We thank the reviewers for their helpful comments .   This work was supported by ONR Grant N00014-   18 - 1 - 2871 , NSF - CAREER Award 1846185 , and   DARPA MCS Grant N66001 - 19 - 2 - 4031 . The   views are those of the authors and not of the fund-   ing agency .   References117871178811789Appendix   A Methods   A.1 Diversify Lexical Arguments with   Primitive Augmentation   Building a Lexicon . Enforcing the two condi-   tions in Eqn . 1 on the SCAN Jump task would   return 6 pairs of primitives , e.g. , ( run , RUN ) , ( walk ,   WK ) , etc . However , for other tasks with a larger vo-   cabulary , a word ’s different surface forms are often   parsed to the same output meaning . For example ,   in COGS , both “ paint ” and “ painted ” are mapped   to “ paint ” in the output form . Therefore , we use   the “ no - winner ” condition ( Akyurek and Andreas ,   2021 ) that allows an M - to - one mapping if vis not   necessary for was long as M < threshold value ψ .   NoWinner ( w ) = ∄v , suff(v , w)∧ness(v , w )   IsPrim ( v , w ) = suff ( v , w)∧   ( ness ( v , w)∨NoWinner ( w ) )   Finally , we delete words above a certain frequency   and words below another frequency from the lexi-   con . This frequency thresholds is decided based on   dev - set tuning results .   Compared to the previous compositional data   augmentation methods ( Andreas , 2020 ; Akyürek   et al . , 2021 ) that substitute words with other se-   mantically equivalent words from the same lexicon ,   our method has two main advantages : ( 1 ) it cir-   cumvents the difficult task of finding semantically   equivalent words that share some common envi-   ronment and creating templates that can be filled   with these words . ( 2 ) because we are not replacing   the primitive with another word from the exist-   ing lexicon ( e.g. , jump ) , the augmented data will   not expose / reveal any novel compositions ( “ jump   around left ” ) in the test set , and thus still preserve   the compositional generalization challenge in the   original task . Another side advantage of creating   new primitives instead of swapping with one from   the original lexicon is that we can train the model   to generalize to future new primitives ( e.g. , dax )   simply by adding “ dax∝ ⇕ ⊣√∫⊔≀→DAX ” to the training set   without the need to rerun any data - augmentation   procedures .   B Experimental Details   B.1 SCAN Dataset   The SCAN dataset ( Lake and Baroni , 2018 ) con-   sists of natural language commands paired withAlgorithm 1 prim2primX Data Augmentation   Require : Training data D   Require : All primitives P   Require : Number of new primitives n   Ensure : Augmented data D   for(x , y)∈Ddo   D = D∪(x , y )   fortoken xinxdo   x←x , y←y   y←P[x ]   ift∈Pthen   Sample sfrom{0,1 , ... , n }   ifs= 0then break   end if   Replace all xinxwithxs   Replace all yinywithys   end if   end for   D = D∪(x , y )   end for   action sequences . Each sub - command is made of   three types of words : action primitive ( “ walk , jump ,   look , run , turn ” ) , direction primitive ( “ left , right ” ) ,   and functional words ( “ opposite , around , twice ,   thrice ” ) , and can be connected with another sub-   command via a function ( “ and , after ” ) . Table 9   describes all syntactic functions of SCAN .   Jump evaluates the model ’s ability to generalize   compositions of syntactic functions across different   lexical ( primitive ) arguments . The training set con-   sists of the primitive command “ jump ” on its own ,   all other primitives , and compound commands with-   out“jump ” ( e.g. , “ walk around left ” ) ; the test set   contains compound commands with “ jump ” ( e.g. ,   “ jump around left ” ) .   Around Right ( Loula et al . , 2018 ) puts all com-   mands containing templates of the form “ [ primi-   tive ] around right ” in the test set , with the remain-   ing examples in the training set .   MCD splits are created to maximize the output   compound divergence while guaranteeing a small   atom divergence between train and test sets ( Key-   sers et al . , 2020 ) . For example , the training and dev   sets of MCD1 have a similar distribution of indi-   vidual words to ensure minimal atom divergence .   However , the training set does not contain com-   pounds “ [ primitive ] around left twice ” , which only   appear in the dev and test sets . These splits require11790   a higher level of compositionality than recognizing   the syntactic equivalence of primitives : the mod-   els must be able to ( 1 ) understand the underlying   symbolic functions “ xtwice∝ ⇕ ⊣√∫⊔≀→x x ” from training   examples like “ jump left twice ” and master the se-   mantics of “ jump around left ” from examples like   “ jump around left thrice ” ; ( 2 ) compositionally ap-   ply the function “ twice ” to a novel argument “ jump   around left ” in the dev and test sets .   B.2 COGS Dataset   The COGS dataset ( Kim and Linzen , 2020 ) re-   quires parsing a diverse set of natural language   sentences into their corresponding logical forms   based on lambda calculus to accurately reflect the   semantic representation of the natural sentence .   COGS raises five different systematic generaliza-   tion challenges in its test set : ( 1 ) novel combination   of familiar primitives and syntactic functions ; ( 2 )   novel combination of modified phrases and syntac-   tic functions ; ( 3 ) sentences with deeper recursion ;   ( 4 ) sentences with alternative verb argument struc-   tures ( e.g. , active - passive ) , and ( 5 ) novel identity   of a verb ( e.g. , unaccusative and unergative ) . Chal-   lenges 1 , 4 , and 5 require generalizing a syntactic   function to a lexical argument which was never as-   sociated with this function during training , while   challenges 2 and 3 require generalizing to unseen   structural arguments .   B.3 Experimental Setup   Baselines We report results for both LSTMs and   Transformers in our experiments . Our LSTM   hyper - parameters are adopted from Akyurek and   Andreas ( 2021 ) , with 2 layers in both encoder and   decoder , a hidden dimension size of 512 , and a   dropout rate of 0.4 . Our Transformer hyperparame - ters and configurations are set following the find-   ings in Csordás et al . ( 2021 ) . We use relative po-   sitional embedding ( Shaw et al . , 2018 ) and use 3   layers in both encoder and decoder . The hidden   dimension size and the embedding size are set to   256 , and the dimension of the feed - forward layer is   set to 512 . We use a dropout rate of 0.1 and 4 self-   attention heads . The Transformer model is imple-   mented using OpenNMT ( Klein et al . , 2017 ) . Addi-   tionally , we also report the performance of the meta-   learning method used in Conklin et al . ( 2021 ) as our   baseline , and will be shown as “ + MAML ” in the   result tables . Since in our preliminary experiments ,   we notice different baselines have a substantial in-   fluence on the performance ( Csordás et al . , 2021 ) ,   we report the reimplemented performance of their   method using the Levenshtein distance on the previ-   ously described LSTM and Transformer baselines   for a fair comparison with other approaches .   Training Details Recent works ( Csordás et al . ,   2021 ; Conklin et al . , 2021 ) observed that the in-   domain development set can not provide enough   signal for doing model selection for compositional   generalization . Hence , similar to Conklin et al .   ( 2021 ) , we split the original test set into a new   development set that contains 10 % of the examples   and a new test set containing the rest 90 % . We   select the checkpoint with the best accuracy on   the development set . In all our experiments , we   report the mean performance in 5 runs and the   corresponding standard deviation .   For all our LSTM models , we train our model   using a dropout rate of 0.4 , a peak learning rate of   1.0 with the Noam learning rate scheduling . The   baseline model is trained using a batch size of 512 ,   and for 8000 steps with 4000 warm - up steps on11791   the SCAN datasets . For the primitive augmentation   datasets , we notice the model needs a longer time to   converge since the dataset is multiple times larger ,   so we train those models for 40000 steps with 4000   warm - up steps . For all our Transformer models , we   train our model using a dropout rate of 0.1 , a peak   learning rate of 2.0 with the Noam learning rate   scheduling . The model is trained using a batch size   of 128 , and for 50000 steps with 5000 warm - up   steps in all the experiments . All our models can be   trained on a single NVIDIA Titan Xp GPU .   C Additional Results   C.1 Encouraging Mutual Exclusivity in a   Synthetic Experiment   In this experiment , we design a highly - controlled   synthetic experiment to demonstrate how MET en-   courages mutual exclusivity in the model . Our ex-   periment design is adapted from the toy experiment   design in Zheng and Lapata ( 2022 ) .   We construct the dataset using two bijections :   fbetween a source vocabulary Sand a target   vocabulary T , and fbetween vocabulary Sand   T. The set Scan be further split to two disjoint   setsS andS , each mapping to T and   Trespectively .   Following Zheng and Lapata ( 2022 ) , we con-   struct our training set so the model can exploit   some spurious correlations between the input and   the output . Specifically , the training set will con-   tain two types of examples . For the first type of   examples , the input only contain one token s∈S   the output is f(s ) . For the second type of ex-   amples , the input sequence contains two tokens‘s , s ’ , where s∈S ands∈S.   The corresponding output is ‘ f(s ) , f(s ) ’ .   All the testing examples will look similar to the   second type and contain two tokens , sands ,   where s∈Sands∈S. Crucially , the main   difficulty of this testing set lies in predicting the   correct outputs for inputs that never appears in   the training set , i.e. inputs containing sand   s , while snever appears in the second type of   examples in the training set .   The performance of the Transformer baseline is   shown in Table 10 . We can see that despite the   simplicity of this task , the Transformer baseline   struggles to perform well on the novel examples ,   only reaching 62.80 accuracy . A common failure   case for the baselines is to only predict f(s )   for the input sequence ‘ s , s ’ and completely   ignores the second input token . This is because dur-   ing training , sonly appears in sentences with   total length 1 , and the model can exploit this spuri-   ous pattern . By applying the MET framework , we   can prevent this incorrect behavior , since the out-   putf(s)should only be matched with the input   sand should not be paired with any other input .   The empirical improvements on this task confirm   the effectiveness of MET . From Table 10 , we can   see using both MET or MET - Meta can make the   model reach 100 % accuracy and completely solves   this task .   C.2 Ablation Experiments on the   Implementation of the Unlikelihood Loss   Notably , the unlikelihood loss Limplementa-   tion used in MET is different from the loss used in   Welleck et al . ( 2019 ) , as the original unlikelihood is   an average over all possible locations , while the un-   likelihood loss in Eq . 3.1 is operated at the sentence   level . We make this design choice since our unlike-   lihood is not used to penalize the generation of each   token ( which is the target in Welleck et al . ( 2019 ) ) ,   but to penalize the generation of the whole sen-   tence , in which only several key tokens are wrong   and should be penalized . The sentence - level unlike-   lihood loss can achieve this by only maximizing   the unlikelihood on a few words , while keeping the   likelihood on most words unchanged . Our ablation   experiments below also verify the advantage of this   design choice .   Empirical Comparison of Different Loss Func-   tions In Table 11 , we compare the effect of dif-   ferent unlikelihood loss variants . On both SCAN11792Jump and COGS , our sentence - level variant ( Sent   Unlike ) is substantially better than the original vari-   ant used in Welleck et al . ( 2019 ) that averages un-   likelihood loss at the word level ( Avg - Word Un-   like ) , as the penalization effect of the latter method   is too strong and lead to a very bad final perfor-   mance . In our preliminary experiments , we have   also tried to use a smaller weight coefficient for the   Avg - Word Unlike variant . While using a smaller   coefficient is beneficial , the Sent Unlike variant still   shows an advantage . We also experimented with   another variant where we still compute the unlikeli-   hood loss at the word level , but use a min - pooling   operation to get the final loss , which achieves a   similar effect as our Sent Unlike loss since the pe-   nalization will only apply to a certain token . From   Table 11 , we can see this variant shows similar per-   formance to the Sent Unlike , further verifying our   hypothesis about why Sent Unlike is preferable to   the original variant in Welleck et al . ( 2019 ) in our   experiments .   DExamples for the effect of prim2primX   on COGS   Given a training example “ A rose was helped by   a dog ” , our prim2primX data augmentation will   first identifies “ rose ” , “ helped ” , and “ dog ” as prim-   itives , and then randomly swap some of these prim-   itives with their mutated forms to create new train-   ing examples ( e.g. , “ Arose1 washelped0 by a   dog . ” ) . The quantitative results on COGS are   in Table 2 in the main paper . Just by using the   prim2primX data augmentation , we can improve   the Transformer baseline from 76.14 % accuracy   to 80.07 % accuracy , showing a 3.93 % absolute im-   provement from the model trained with original   data . MET further boosts the accuracy to 81.12 % ,   again demonstrating that deductive MET method   can provide complementary performance gain on   top of inductive data augmentation.11793