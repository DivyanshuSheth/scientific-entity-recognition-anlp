  Nikita Sorokin   Moscow Institute   of Physics and TechnologyDmitry Abulkhanov   sorokin.na@phystech.edu,abulkhanov.dmitry@huawei.com ,   piontkovskaya.irina@huawei.com , valentin.malykh@huawei.comIrina Piontkovskaya   Huawei Noah ’s Ark labValentin Malykh   Abstract   Cross - lingual question answering is a thriving   field in the modern world , helping people to   search information on the web more efficiently .   One of the important scenarios is to give an an-   swer even there is no answer in the language a   person asks a question with . We present a novel   approach based on single encoder for query   and passage for retrieval from multi - lingual   collection , together with cross - lingual gener-   ative reader . It achieves a new state of the art   in both retrieval and end - to - end tasks on the   XOR TyDi dataset outperforming the previous   results up to 10 % on several languages . We   find that our approach can be generalized to   more than 20 languages in zero - shot approach   and outperform all previous models by 12 % .   1 Introduction   Question answering ( QA ) is an important tool for   information search on the Internet . Since it is natu-   ral for a person to ask a question to get some infor-   mation , the QA systems are designed to meet this   requirement . QA provides a wide range of tasks ,   from engineering to cornerstone scientific tasks .   Open - domain QA , in this direction , is an interest-   ing example of a problem that connects both : mul-   tilingual knowledge sources form differing knowl-   edge and supplement gaps in each specific lan-   guage . The requirement for modern language mod-   els to be cross - lingual is gradually becoming more   and more important and is being incorporated into   popular benchmarks such as XGLUE ( Ruder et al . ,   2021 ) or XTREME ( Liang et al . , 2020 ) , where   the systems are evaluated not only by their per-   formance metrics on single language tasks but the   ones on many languages . Figure 1 : Overview of Sentri and a case example of   answering a Finnish question using non - Finnish sources .   In short , at the first step system retrieves information   from factoid knowledge sources ( Wikipedia ’s ) on di-   verse languages . In the second step it fuses the retrieved   information regardless of the language of each part , even   in the absence of texts on query language . Finally , it   produces an answer on the query language that aggre-   gates all ( as it can ) diverse pieces from other languages .   For this particular example , there is no answerin   Finnish Wikipedia . One of the reasons is that there   are n’t many articles in Finnish Wikipedia because   Finnish is a low - resource language in general . More-   over , the answer can be found in rich - resources lan-   guages such as English or Russian for example.395Generally , passage retriever is based on the so   called dual - encoder , i.e. two independent modules   of the same architecture for the question and con-   text encoding . However , in previous works ( Qu   et al . , 2021 ; Gao and Callan , 2021 ) authors found   out that on the one hand dual - encoder is not noise-   resistant and on the other hand large batch training   can improve stability of resulting embedding space .   For instance , in ( Qu et al . , 2021 ; Gao and Callan ,   2021 ) authors used 512×8and512×4respec-   tively .   Ouguz et al . ( 2021 ) present DPR - PAQ model   combining large semi - supervised corpus for   pre - training with the better and larger LM   RoBERTa , DPR - PAQ achieves the state of   the art results on Natural Questions dataset . Nev-   ertheless , large models and large batch training re-   quire abundance of GPU memory . The mentioned   models are represent dual - encoder scheme , where   two independent encoders are used – one for the   questions and one for the passages . To reduce mem-   ory usage , we present a system including a single   encoder used for both tasks , moreover , we show   that the system using single encoder can learn an   embedding space better suited for transfer learning   and thus improve result in cross - lingual question   answering in zero - shot scenario .   The system , we call Sentri , achieves a new   state of the art on XOR TyDi QA cross - lingual   dataset ( Asai et al . , 2021a ) outperforming the pre-   vious approaches by 10 % on retrieval task and 7 %   on end - to - end question answering task . In addition ,   on MKQA multi - lingual dataset ( Longpre et al . ,   2020 ) , which contains translations of Natural Ques-   tions to different languages , in zero - shot scenario   our system outperforms a strong baseline by 8 % .   The overall contribution of this paper is two - fold :   ( i ) we present a system , including single encoder   for questions and contexts , that achieves state - of-   the - art results in the retrieval and end - to - end tasks   of the XOR TyDi dataset , ( ii ) we provide an anal-   ysis of the system behaviour in zero - shot scenario   on unseen languages proving the its transferability   and lower resource consumption .   The rest of the paper is organized as follows .   Section 6 presents an overview of the recent stud-   ies on multilingual QA models . Section 2 which   details the dataset design choices , outlines the data   preparation pipeline and data used for evaluation .   Section 3 presents the engineering choices and   describes the resulting model and its training pro-   cess . We describe experimental setup and describe   the achieved results in Section 4 . We provide addi-   tional results analysis in Section 5 , and Section 7   concludes the paper .   2 Datasets   In this work , we use XOR TyDi and MKQA to   evaluate our system onto and several datasets to   ( pre-)train it .   XOR TyDi ( Asai et al . , 2021a ) is a multilin-   gual open - retrieval QA dataset that enables cross-   lingual answer retrieval . The dataset , based on   questions from TyDi QA ( Clark et al . , 2020 ) , ar-   ticulates three new tasks that involve finding doc-   uments in different languages using multilingual   and English resources . It consists of questions writ-   ten by information - seeking native speakers in 7   typologically diverse languages : Arabian , Bengali ,   Finnish , Japanese , Korean , Russian and Telugu .   Answer annotations are retrieved from multilingual   document collections . XOR - Retrieve is a cross-   lingual retrieval task where a question is written   in the target language ( e.g. , Japanese ) , and a sys-   tem is required to retrieve an English document   that answers the question . XOR - English Span is   a cross - lingual retrieval task where a question is   written in the target language ( e.g. , Japanese ) , and   a system is required to output a short answer in   English . XOR - Full is a cross - lingual retrieval task   where a question is written in the target language   ( e.g. , Japanese ) , and a system is required to output   a short answer in the target language . In our work ,   we concentrate on XOR - Retrieve and XOR - Full396tasks and use XOR TyDi to train and evaluate our   system .   2.1 Training Dataset Pre - processing   Since there are low - resource languages in the XOR   TyDi dataset , the task of preparing data is of pri-   ority importance . To generate data in different   languages , we used NQ and Trivia QA datasets de-   scribed above . Both NQ and Trivia QA are English   language datasets . To use them for training pur-   poses in our setup we had to translate them to the   languages of XOR TyDi . The quality of machine   translation still does not match the human transla-   tion quality in most of the languages and domains .   However , questions and answers in NQ and Trivia   QA datasets are short and easy to translate .   For each pair of question qand answer afrom   the datasets , we made translations from English to   each language Lusing the M2M100 model de-   scribed in ( Fan et al . , 2020 ) , it is a state - of - the - art   model for translation for many languages , includ-   ing the ones we are interested in . Thus we trans-   lated question - answer pairs and we got an aligned   dataset . The pairs are not enough , since the open-   domain QA task is based on so - called support pas-   sages retrieved from some document collection .   To overcome this issue we mined the positive and   hard negative sample passages for each language   L. We consider a paragraph from a document   to be a positive sample if it is ranked high by a   retriever model and includes the answer . We use   complicated morphology - aware answer detection   technique which we describe in Appendix . If a   paragraph is highly ranked but contains no answer ,   we consider it as a hard negative sample . The re-   sulting statistics and analysis of the training dataset   we also present in Appendix .   Information Retrieval We took into account that   most XOR TyDi languages have complex morphol-   ogy and other linguistic features , which makes in-   formation retrieval less effective for the models   using token comparison . Thus we decided to nor-   malize the morphology for at least the languages ,   which has publicly available stemmers , namely ,   Arabic , Bengali , Korean , and Russian . The   Telugu language has no publicly available stemmer , but there is a lemmatizer , which we used .   For Korean we apply token splitting by the part-   of - speech tag , i.e. modifier POS as a Josa andEomi   are treated as separate tokens . Unfortunately , we   have not found accessible stemmers and/or lem-   matizers for Japanese and Finnish languages . We   use this normalisation to improve positive passage   mining for self - training procedure . More details on   normalisation could be found in Appendix .   Natural Questions ( NQ ) ( Kwiatkowski et al . ,   2019 ) dataset is designed for end - to - end question   answering . The questions are mined from real   Google search queries and the answers are spans in   Wikipedia articles identified by annotators . We   use this dataset in two ways . One way is for   training and another one is for zero - shot evalua-   tion . The latter option is provided to us by MKQA   dataset ( Longpre et al . , 2020 ) . It is a translation   of 10 thousand question - answer pairs from NQ to   26 different languages , thus giving us an aligned   dataset of 260 thousand question - answer pairs total .   The former option is described below .   We also use Trivia QA for pre - training of our   model . Joshi et al . ( 2017 ) presented Trivia QA ,   a large - scale question - answering dataset that in-   cludes so - called evidence documents , allowing one   to state a task of information retrieval . Trivia QA in-   cludes 95 thousand question - answer pairs authored   by trivia enthusiasts and independently gathered   evidence documents , six per question on average   ending with 650 thousand total triples .   3 Method   The open domain question answering task heavily   relies on retrieval from some ( possibly more than   one ) document collections . In the case of the cross-   lingual variant of this task , the usage of several ( at   least two - in English and in a target language ) doc-   ument collections is almost inevitable . We evaluate   our model in two cross - lingual setups : using En-   glish Wikipedia ( W ) to search for a relevant pas-   sage containing the answer to the question or using   collection of multilingual reference passages from   Arabic , Russian , English , Finnish , Telugu , Bengali ,   Japanese , Korean Wikipedia ( W ) . More for-   mally , given a question qin language L , a system   retrieves the documents from WorW , and   formulates an answer a. Thus the system could be   virtually split to retriever , which creates a list of397relevant documents , and reader , which generates   an answer using the most relevant documents . The   sample of the system output is presented on Fig . 1 .   3.1 Single Encoder Retriever   We follow common ( Qu et al . , 2021 ; Asai et al . ,   2021b ; Ouguz et al . , 2021 ) dual - encoder approach   in data representation . The system consists of   question encoder E(·)and passage encoder E ( · )   which maps text to d - dimensional real - valued vec-   tors . Before run - time , E(·)applied to all passages   in knowledge source to create search index . To find   out relevant passages to certain question system   operates a similarity function :   sim(q , p ) = E(q)·E(p ) . ( 1 )   i.e. similarity between the question and the passage   defined by the dot product of their vectors .   In this work we investigate case when   E ( · ) = E(·)and call this approach as Single En-   coder . In addition to it , a model with E(·)̸=E ( · )   we call Bi - Encoder to avoid confusion .   The architecture that utilizes Single encoder ap-   proach for re trieval ( Sentri ) shares one encoder   forE(·)andE(·)contrary to bi - encoder which   based on two separate models .   Since our model is used in a multi - lingual setting ,   the choice of multilingual models is natural for base   model . We use XLM - RoBERTa ( Conneau et al . ,   2020 ) ( large ) in our experiments .   Training Sentri is trained to give positive pas-   sages higher scores than negative passages . More   specifically , given a question qin a language from   Ltogether with its positive passage pandmneg-   ative passages { p}sampled from W , we   minimize the loss function : L(q , p,{p } )   = −loge   /summationtexte+e,(2 )   where we aim to optimize the negative log-   likelihood of the positive passage against a set of   mnegative passages .   For each question , we treat other passages in the   training batch that do not answer this particular   question as negative passages ( in - batch negative   trick , Henderson et al . 2017 ; Karpukhin et al . 2020 )   In particular , for batch size neach question can   be further paired with m = n−1 + nnegatives   ( i.e. , positive and hard negative passages of the rest   questions ) without sampling additional negatives .   Furthermore , in the case of multilingual data , it   helps enforce the cross - lingual ability of the model   because of an increasing number of cross - language   pairs .   3.2 In - batch False Negatives Filtering   Although the above strategy can increase the num-   ber of negatives , some of them may turn out to   be false negatives . We analyze the batches gener-   ated for the training and found out that different   questions in the same batch could have the same   positive passages . Since these positive passages are   used for in - batch negative training that produces   false negative pairs . For English Wikipedia this   overlap is significant but not so crucial like for   lower resource Wikipedias . For instance , for Nat-   ural Questions passages in 44 % training triplets   ( questions , answer , passage ) are used more than   once in the dataset . The sample of positive pas-   sage overlap for NQ is presented on Fig . 3 . We   use in - batch filtering allowing us to eliminate this   overlap from generated batches and thus improve   the overall system quality .   3.3 Self - Training   Several works ( Qu et al . , 2021 ; Izacard and Grave ,   2020a ) refer to iterative learning as a source of   model quality improvement . We use this idea in the   form described below , which we call self - training .   Fig . 2 presents the framework which we use in   this work . Sentrimodel retrieves top- kpassages   from Wikipedia for each question from the initial   QA training set . Then we select the positive ( p )   passages for each question ( we know the ones for   the initial training set ) and treat the rest as hard398   negative ( p ) examples . Afterwards , we train   new iteration Sentrimodel using the passages   marked up previously . In contrast to ( Asai et al . ,   2021b ) we train iteratively only a retrieval part of   the whole system .   At stage 0when there is no trained model avail-   able , we use well - known BM25 model ( Sanderson ,   2010 ) as a retriever in our experiments . The impor-   tant feature of this model is that it does not need any   kind of training , thus it could be used to retrieve   documents from collections in languages with little   or no training data .   For Sentri system we report the results for   thesecond stage of self - training below . For Bi-   Encoder we report results for stages 1and2adding   the specifying index .   3.4 Answer Generation   We have experimented with both extractive and   abstractive answer generation and found out that   abstractive is more profitable . Here we describe   the abstractive reader approach we use as primary   one . We decided to use the FiD model ( Izacard and   Grave , 2020a ) as a reader model in Sentri for end   to end question answering task XOR - Full since it   allows us to exclude the translator from a pipeline   and to aggregate information in a cross - lingual   setup . Since the original FiD model is monolingual ,   we present extension of this work , multilingual ver-   sion which we call MFiD . To train MFiD , we use   several QA datasets , listed in Sec . 2 , namely XOR-   Full , XOR TyDi , Natural Questions , and Trivia   QA datasets , the same ones used for training the   retriever part of the Sentri model . For each ques-   tion from the QA datasets we retrieve top-50 pas-   sages from multi - language knowledge source using   our retriever model . And then use it for trainingMFiD as cross - lingual fusion reader . We also ex-   perimented with standard extractive reader . The   details on extractive approach could be found in   Sec . 5 .   4 Experiments   We have conducted a series of experiments with   number of models , namely these are Sentri model   combined with different reader parts and Bi-   Encoder model with one or two stages of self-   training . Bi - Encoder model is using standard ex-   tractive reader ( plus machine translation where ap-   plicable ) . The main difference between Sentri and   Bi - Encoder , that the latter is based on classic dual-   encoder architecture , while the former is using sin-   gle encoder for questions and paragraphs .   4.1 Results on XOR TyDi   Tables 1 and 2 contain results for our system in   retrieval and end - to - end setups , XOR - Retrieve and   XOR - Full respectively . These two tables contain   the results of the models ’ evaluation on the devel-   opment and test parts of the XOR TyDi dataset . It   is important to mention that we use name Sentri for   our model in both tasks , while in XOR - Retrieve   task the reader part is not used , since it is essentially   a passage ranking evaluation . Also , you can find   results for our models titled as Bi - Encoderand   Bi - Encoder(for first and stages of self - training   respectively ) . As one can see Sentri significantly   outperforms these baseline models and existing   state - of - the - art models . We provide more detailed   analysis in section Ablation Study .   Tab . 1 displays recall scores for 2000 and 5000   first tokens ( R@2kt andR@5kt respectively ) .   That means that we expect to find an answer span   in the first ltokens . This metric was proposed399   in ( Asai et al . , 2021a ) as alternative to more com-   monRecall @Nin purpose to make more fair com-   parison across various models with different pas-   sage size used .   Our system outperforms the previous state - of-   the - art system in both R@2kt and R@5kt metrics   by a wide margin on four languages , namely Ara-   bic , Japanese , Russian , and Telugu . More impor-   tantly , our system outperforms the previous system   on average for all the languages . Interestingly , self-   training improves the results in all the languages ,   with the intriguing exception of the Russian lan-   guage . This fact requires an additional investiga-   tion , we leave it as future work for now .   Table 2 displays F1 , Exact Match ( EM ) , and   BLEU scores for the end - to - end setup where given   a question in target language Land Wikipedia in   both English and L , a system is required to gener-   ate an answer in the target language . F1 measure   is computed per token for an answer span . Ex-   act Match compares the golden answer span with   the system output for exact equality . BLEU met-   ric , defined as in ( Papineni et al . , 2002 ) , computesthe number of overlapping n - gram between the   golden answer and the system output . In this ex-   periment , we see a somewhat different behaviour   of the model . Our model outperform previous state   of the art system for all languages , with exception   for Telugu where CORA model ( Asai et al . , 2021b )   shows insignificantly higher score .   4.2 Zero - Shot Cross - Lingual Transfer   We investigated the transferability across the lan-   guages for the trained system . We used the MKQA   dataset in similar to the XOR - Retrieve setup , i.e.   we retrieved the passages from English Wikipedia ,   extracted the answer from the top - ranked passage ,   and translated it with a machine translation model .   Here we again use M2M100 model for machine   translation task . As a baseline for this task , we   utilized BM25 with extractive reader and the ma-   chine translation model at the end of pipeline . We   selected from MKQA such unseen languages that   were not presented to the system during the training   process . The achieved results presented in Tab . 3   show that even in such a zero - shot setting our sys-400tem significantly outperforms both the strong base-   line and previous approaches in all languages . Ad-   ditional details on zero - shot transfer could be found   in Appendix .   5 Ablation Study   Sentri model has five important features which dif-   ferentiate it from the previous work : self - training ,   a single encoder model for passage and question   processing as a retriever , a generative model as   a reader , in - batch negative filtering , usage of the   machine translated data during the training process .   ( I ) The effect of the first of mentioned features   could be analysed basing on Tab . 1 ( upper part   of the table , showing results on development set )   and 2 . Self - training for Bi - Encoder model im-   proves results by 3 % on average for the XOR - Full   task and about 5 % for the XOR - Retrieve task ( Bi-   Encodervs Bi - Encoder ) .   ( II ) The usage of a single encoder could be es-   timated as again 3 % for the XOR - Full task and   about 5 - 6 % for the XOR - Retrieve task ( Sentri + ext .   reader vs. Bi - Encoder ) . In Tab . 4 we demonstrate   key motivation of using shared encoder model . We   observe that the single encoder approach superior   to the Bi - Encoder in terms of memory efficiency   and overall performance . With the same size ( less   than 2 % difference ) it achieves more than 12 % rel-   ative improvement or ∼6 - 7 difference in absolute   points in retrieval task . Note that Sentri and Bi-   Encodertrained in the same setting and same QA   datasets except used base model variants ( base and   large respectively ) . We do it for the sake of the   matching number of parameters , matched mem-   ory consumption and similar training time of bothmodels .   ( III ) The replacement of standard extractive   reader , i.e. span - tagging model , with a generative   one , MFiD model in our case , turned out to add up   to 34 % of F1 measure ( for Japanese ) and 25 % on   average . We observe that with the number of con-   texts more than 5 performance of extractive reader   degrades . Contrary that using more contexts for   answer generation can significantly improve model   quality . We further evaluate generative reader us-   age by adding the one described in ( Asai et al . ,   2021b ) , it uses only 15 top - ranked contexts due to   memory constraints . Unlike that , MFiD can use up   to 100 top - ranked contexts thanks to the indepen-   dent processing of passages in the reader ’s encoder .   The results are presented in Tab . 5 .   ( IV & V ) The importance of the last two features   could be estimated basing on Tab . 6 . While the   former one adds up to 2 per cent , the latter is of   crucial importance adding up to 20 % depending on   task and measure .   We could conclude that all the features are im-   portant for our approach to present state - of - the - art   results in retrieval and end - to - end tasks .   5.1 Languages Used   It is also interesting to know if the system is ac-   tually using the data in other languages for an-   swer generation . In other words , if our model is   truly cross - lingual . The analysis of the top-100   paragraphs for all the questions in validation set   of XOR TyDi , namely the breakdown on the lan-   guages used , is shown on Tab . 7 . As one can see ,   there are several interesting features could be spot-   ted in the table . The highest self - usage ( i.e. when   a question and a paragraph are in the same lan-   guage ) percentage is shown by Korean language   ( almost entirely , 97.6 % ) , with negligible usage of   other languages , except English with 1.4 % . On   the contrary , Finnish language is shown the lowest   self - usage of 52.9 % with 34.6 % usage of English .   These two facts could be explained as Korean hav-   ing unique writing system , thus it has almost non-   existent intersection with other languages in terms .   In the contrary Finnish is Latin - based and is in   close contact with Swedish language since the Mid-   dle Ages , while Swedish is close German language   for English . But this speculation has its down-   side : the second most - used language for Finnish is   Japanese ( 7.4 % ) , which is both unrelated and uses   other script . Interestingly , Japanese language is the401R@2kt R@5kt   MKQA XOR MKQA XOR   Sentri 53.3 51.0 60.3 60.8   Sentri w/o false negative filtering 52.1 49.3 60.1 60.3   Sentri w/o weight - sharing ( Bi - Encoder approach ) 45.3 45.4 52.9 53.9   Sentri w/o multi - language translations of training set 41.5 30.8 45.8 42.3   second language by usage for almost all the lan-   guages , including Korean but with only 0.7 % . For   Telugu the second most used language is Bengali ,   which is related to it . But surprisingly , the other   way around it is almost unused . Another peculiar   feature is that Russian language is significant in   usage for almost all the languages with exception   for Korean . We hypothesise that it is due to the   proportion of Russian data in the training set , this   language being the second by size in the dataset .   It is important to mention that the largest present   language is Arabic , but its influence is lower than   Russian . The influence of Arabic , Russian , and   Japanese need more in - depth future investigation .   6 Related Work   Datasets The cross - lingual question answering   datasets were scarce before recent years . Fortu-   nately , these years left us with several publicly   available datasets . Lewis et al . ( 2020 ) introduced   MLQA dataset . It consists of parallel QA pairs in   several languages . Liu et al . ( 2019 ) have presented   XQA dataset , with training set in English and vali-   dation and test sets in the other languages . Cross-   lingual Question Answering Dataset ( XQuAD )   benchmark presented in Artetxe et al . ( 2020 ) .   It consists of a subset of 240 paragraphs and   1190 question - answer pairs from SQuAD v1.1 ( Ra-   jpurkar et al . , 2016 ) together with their translations   into ten languages .   Systems Open - domain question answering task   assumes answering factoid questions without a pre-   defined domain ( Kwiatkowski et al . , 2019 ) . Re-   cent research was focused on creating non - English   question answering datasets and applying cross-   lingual transfer learning techniques , from English   to other languages . Until recently , the availability   of appropriate train and test datasets has been a   key factor in the development of the field : how-   ever , in recent years , many works have focused   on the collection of loosely aligned data obtainedthrough automatic translation or by parsing similar   multilingual sources . Artetxe et al . ( 2020 ) studied   cross - lingual transferability of monolingual repre-   sentations of a transformer - based masked language   model .   In most previous approaches the authors use   extractive models to generate the actual answer .   This could be explained by the mental inertia from   SQuAD - like datasets . By SQuAD - like we mean   a dataset where labelled data includes an explic-   itly stated question , a passage , containing an an-   swer , and a span markup for the answer . Such   markup was presented for the question answering   task called SQuAD in ( Rajpurkar et al . , 2016 ) . But   recently there were presented cross - lingual gen-   eration of answers from raw texts . Kumar et al .   ( 2019 ) ; Chi et al . ( 2019 ) studied cross - lingual ques-   tion generation . Shakeri et al . ( 2020 ) proposed   a method to generate multilingual question and   answer pairs by a generative model ( namely , a   fine - tuned multilingual T5 model ) , it is based on   automatically translated samples from English to   the target domain . Generative question answering   was mostly considered in previous work for long   answers datasets . However , FiD model ( Izacard   and Grave , 2020b ) archives competitive results on   SQuAD - like datasets , where an answer is supposed   to be short text span . For open domain question   answering , one of the first approaches named RAG   used generative models was presented in ( Lewis   et al . , 2021 ) . A key idea of this RAG model is   to process several ( top k ) passages from the re-   triever in the encoder simultaneously . The pro-   duced dense representations of the passages are   used in the decoder for the answer generation , this   process is called fusion . Processing the passages   independently in the encoder allows a model to   scale to many contexts , as it only runs self - attention   over one context at a time . FiD model follows this   paradigm further improving the results in question   generation.402Ar Bn Fi Ja Ko Ru Te En   Ar 80.6 0.2 0.1 7.1 0.2 1.2 0.0 10.5   Bn 0.3 89.8 0.1 4.8 0.2 0.8 0.1 3.7   Fi 0.8 1.4 52.9 7.4 0.3 2.1 0.1 34.6   Ja 0.8 3.7 0.2 77.5 1.1 1.7 0.2 14.5   Ko 0.0 0.0 0.0 0.7 97.6 0.1 0.0 1.4   Ru 0.9 0.8 0.3 8.7 0.3 74.5 0.0 14.2   Te 0.2 8.3 0.0 3.8 0.4 0.4 75.5 11.2   7 Conclusion   Nowadays multi - lingual and cross - lingual prob-   lems are coming to the stage once the natural lan-   guage models become more and more powerful .   One of these problems is that where the systems an-   swer the questions using various mutually disjoint   language data , as it stated in XOR TyDi task . This   task is based on a specific XOR TyDi dataset ( Asai   et al . , 2021a ) , which ensured such information   asymmetry in the different language data . We intro-   duced the cross - lingual system to solve the XOR   task . While the XOR TyDi is a challenging test   that stimulates cross - linguality in NLP systems , we   have outperformed the existing models in two sub-   tasks : XOR - Retrieve and XOR - Full without using   external APIs . The first task is a classical passage   retrieval task , while the second one is an end - to - end   question - answering task . Besides showing the state   of the art results on these two subtasks , our system   is demonstrated the ability the transfer to the un-   seen languages in retrieval task , including the lan-   guages which were not presented in the pre - trained   language model we use as an encoder for the re-   triever part of our Sentri model . And last we found   that the previous works ignored the existence of   the morphology in XOR TyDi presented languages ,   thus missing many results in information retrieval .   We propose to solve this issue by using stemming   or lemmatization for such languages .   Our system has five differentiating features ,   which are self - training ( using the output of the pre-   viously trained models ) , single encoder ( allowing   us to reduce the number of parameters about twice   in retriever ) , usage of a generative model to get the   question from retrieved passages , in - batch negative   filtering , and usage of the machine translated data   during the training process . All of these features   are proved to make a share in the achieved sig-   nificant quality improvement demonstrated by ourmodel . Although , our system has several flaws , e.g.   passage selection strategy and stemming for the   languages , we consider these flaws as our future   work . But we hope that current study will foster   research in cross - lingual question answering tasks .   References403404   A Implementation Details   For our system , we have adapted models from   the Huggingface Transformers library ( Wolf et al . ,   2019 ) . We trained the question and passage en-   coders using the in - batch negative sampling with   a batch size of 16 , one hard negative per question .   We trained the system for 40 epochs with a learning   rate of 10using Adam , linear scheduling with   warm - up and dropout rate of 0.1 . For training and   validation , we used NQ , Trivia , XOR and TyDi QA   datasets . The number of hard negative passages   was 32 and 50 for the first and second stages re-   spectively . All experiments were carried out on   four nVIDIA V100 GPUs ( with 32Gb RAM each ) .   B Data Distribution   The overall statistics on mined data from the men-   tioned datasets are available in Tab . 8 . As one can   see , the data acquired is following the rough pat-   tern : most of the samples come from TriviaQA , the   second source is NQ , while the rest comes from   XOR TyDi QA . In the perspective of languages the   pattern follows the XOR TyDi distribution , with   Arabic being the largest non - English language , and   Korean being the smallest one .   NQ TriviaQA XOR QA Total   Ar 16420 38652 10496 65568   Bn 10165 25178 1973 37316   Fi 10787 23145 4688 38620   Ja 15357 26877 2869 45103   Ko 1327 2247 721 4295   Ru 18499 35081 3981 57561   Te 4964 12880 1481 19325C Effects of Normalisation   Since we are using normalisation for the retrieval ,   we decided to look into the evaluation process .   The metrics used for the evaluation , namely per-   token F1 , Exact Match , and BLEU , are based on   simple token comparison . Such comparison is   inefficient for the languages with rich morphol-   ogy , like Russian or Japanese . So we applied the   same normalisation as in retrieval for the generated   and gold answers . Table 9 shows the achieved   results . As one can see normalisation helps to   achieve less strict and thus more informative com-   parison for the morphology - rich languages . Since   Japanese and Finnish are both synthetic aggluti-   native languages , we suppose that the results on   them could also be improved with usage of the   stemming / lemmatization , thus improving the cross-   lingual average further . Given that we think the   usage of some kind of normalisation should be rec-   ommended for any cross - lingual QA task including   morphology - rich languages .   D Zero - Shot Cross - Lingual Transfer   In addition to the MKQA results on which were pre-   sented in main contents , we used M2M100 model   to translate the MKQA English subset to all known   to it languages , thus extending MKQA to 98 lan-   guages . We provide the results on the this extended   dataset in Tab . 10 .   E Unseen Languages   We thoroughly analysed the ( partially ) unseen lan-   guages and found out that our system performs rea-   sonably well even for those languages , which are   not present in the pre - trained XLM Roberta used as   an encoder in our model . The results are presented405 in Tab . 11 . We also provide aggregated results for   the unseen on training stage languages and seen   in the training stage ones . As one could see , our   model performs surprisingly well on the languages   which were not presented on any training stage ,   although the training improves Recall by 12 and 13   per cent on average , while pre - training adds 13.8 %   of Recall . We think this is another evidence of the   great generalizability of the pre - trained language   models.406