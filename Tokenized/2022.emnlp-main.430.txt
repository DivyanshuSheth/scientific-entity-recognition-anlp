  Zhuo ZhangXiangjing HuLizhen Qu   Qifan WangZenglin XuHarbin Institute of Technology ( Shenzhen ) , ChinaPeng Cheng Lab , Shenzhen , ChinaMonash University , Melbourne , AustraliaMeta AI , CA , USA   { iezhuo17,starry.hxj}@gmail.com Lizhen.Qu@monash.edu   wqfcr@fb.com xuzenglin@hit.edu.cn   Abstract   With the necessity of privacy protection , it be-   comes increasingly vital to train deep neural   models in a federated learning manner for nat-   ural language processing ( NLP ) tasks . How-   ever , recent studies show eavesdroppers ( i.e. ,   dishonest servers ) can still reconstruct the pri-   vate input in federated learning ( FL ) . Such a   data reconstruction attack relies on the map-   pings between vocabulary and associated word   embedding in NLP tasks , which are unfor-   tunately less studied in current FL methods .   In this paper , we propose a fedrated model   decomposition method that protects the privacy   ofvocab ularies , shorted as FV . In   FV , each participant keeps the local   embedding layer in the local device and de-   taches the local embedding parameters from   federated aggregation . However , it is chal-   lenging to train an accurate NLP model when   the private mappings are unknown and vary   across participants in a cross - device FL set-   ting . To address this problem , we further pro-   pose an adaptive updating technique to improve   the performance of local models . Experimen-   tal results show that FV maintains   competitive performance and provides better   privacy - preserving capacity compared to status   quo methods .   1 Introduction   Privacy - sensitive Natural Language Processing   ( NLP ) applications , such as personal virtual as-   sistants ( Chen et al . , 2017 ) , online medical diagno-   sis ( Hakak et al . , 2020 ) and mobile keyboards ( Ji   et al . , 2019 ) , often have sensitive user data stored   in local devices to protect user privacy . To ensure   service quality , they also need a large amount of   such data for training the corresponding machine   learning models . However , it would breach user   privacy or data protection law , e.g. , GDPR , to apply   conventional training methods by putting sensitive   data in a centralized place . Figure 1 : User - device Online Medical Conversation .   The doctor records the user ’s health conditions , demo-   graphic information and contact details . The sensitive   information of the user is made bold . The sensitive in-   formation can either be constructed from model updates   and private mappings by e.g. data reconstruction attack ,   or be inferred from the local vocabulary when the local   data is small .   To resolve the dilemma , federated learning ( FL )   methods are proposed to collaboratively learn a   global model in a distributed manner without re-   quiring sensitive data to leave local devices ( McMa-   han et al . , 2017a ) . In the FL framework , each local   device downloads the current model , updates the   model using local data , and subsequently sends   model updates to a central server for improving   the current model . The process is repeated until   certain criteria are met . Due to improved privacy   protection , there is a fast - growing interest in apply-   ing FL to NLP applications ( Ge et al . , 2020 ; Sui   et al . , 2020 ; Liu et al . , 2021 ) .   However , recent studies show that it is still pos-   sible to reconstruct user data from model updates   sent to servers ( Boenisch et al . , 2021 ) . Such at-   tack methods , e.g. Deep Leakage from Gradient   ( DLG ) ( Zhu and Han , 2020 ) , need to know the   mappings between words and their embeddings .   Eavesdroppers may be able to infer sensitive infor-   mation by knowing which words are used in local   devices , as shown in Figure 1 . Hence , privacy-6413preserving models should protect the mappings be-   tween sensitive words and their embeddings . How-   ever , it is challenging to train NLP models using   current FL methods if such mappings are unknown   and vary across devices . Despite potential privacy   risks , prior works largely neglect the importance of   protecting private vocabularies and the associated   word embedding layers in local devices in the FL   framework .   To address the above problems , we propose a   fedrated model decomposition algorithm that pro-   tects private vocab ularies , coined FV ,   which keeps the word embedding layer ( i.e. , local   parameters ) of a deep neural network in local de-   vices and only collaboratively updates the remain-   ing components of the model ( i.e. , global parame-   ters ) . Because the word embedding layer does not   participate in federated aggregations and a user de-   vice may not be always - available in each federated   training round in cross - device FL settings ( Kairouz   et al . , 2021 ) , the local parameters may be poorly   coupled with those global parameters , resulting in   performance degeneration ( Reddi et al . , 2020 ) . To   alleviate this issue , we introduce an adaptive up-   dating procedure into FV , which learns   effective local parameters for resolving the perfor-   mance drops .   To sum up , our contributions are two - fold :   •We propose a novel federated learning algo-   rithm ( FV for short ) to strengthen   the protection of user privacy by detaching   word embedding layers from federated aggre-   gations and adaptively updating local parame-   ters in order to further improve performance   in the challenging yet practical cross - device   FL setting .   •We conduct extensive experiments on three   corpora to evaluate FL methods with regard to   privacy protection , model utility and commu-   nication efficiency . Our method significantly   outperforms the state - of - the - art FL methods   in terms of privacy protection and communi-   cation efficiency , while still achieving com-   parable test accuracies on text classification   regardless if BiLSTM ( Graves et al . , 2005 ) or   DistilBERT ( Sanh et al . , 2019 ) serves as the   backbone model . Compared with the state-   of - the - art FL method , our method can reduce   token - wise recovery by 80.1 % on average , im-   prove local model performance by about 1.3%on average , and reduce communication cost   by 83 times at most .   2 Background and related work   Federated learning . Federated learning ( McMa-   han et al . , 2017a ) ( FL ) is a privacy - enhancing dis-   tributed machine learning paradigm with individual   user ’s data preserved locally . In FL , there has a cen-   tral server and numerous user devices where the   server is responsible for aggregating model param-   eters or gradients from users ’ local training . At the   beginning , the service randomly selects multiple   users from large - scale devices in each communi-   cation round . The selected device computes and   uploads the model ’s parameters or gradients to the   service provider that aggregates them to update the   global model . The above process is repeated for   multiple times until model converges . With the   user data remaining on local device , FL has widely   been applied to privacy - sensitive NLP tasks ( Liu   et al . , 2021 ; Ge et al . , 2020 ; Sui et al . , 2020 ) .   However , recent studies have contested the   privacy - preserving ability of FL ( Zhu and Han ,   2020 ; Geiping et al . , 2020 ; Boenisch et al . , 2021 ;   Wei et al . , 2020 ) . Zhu and Han ( 2020 ) first show   how to recover the user ’s input text from the up-   loaded gradients . Boenisch et al . ( 2021 ) propose   a perfect attack to recover input text based on sent   gradients with near - zero costs . In these works , the   mappings between words and their embeddings are   critical to recovering text data due to the discrete   nature of text . Unfortunately , previous studies have   focused on how to protect shared gradients while   largely ignoring this vital mapping challenge .   A line of work has introduced differential pri-   vacy ( DP ) ( Dwork et al . , 2014 ) or homomorphic   encryption ( Gentry , 2009 ) into the federated train-   ing pipeline to ensure user privacy . FL with DP is   a general technique to protect user data by inject-   ing controlled noise to shared gradients ( McMahan   et al . , 2017b ; Zhu et al . , 2020 ) . Nevertheless , it falls   into the dilemma of low utility ( Basu et al . , 2021 ;   Zhu and Han , 2020 ) , and is still uncertain how   much privacy can be preserved in real - world appli-   cations ( Huang et al . , 2020 ) . FL with encryption   method is another way to secure federated learn-   ing ( Bonawitz et al . , 2017 ) . It is several orders of   magnitude slower than the unencrypted equivalent ,   which is impractical for deep learning . Recently ,   Huang et al . ( 2020 ) proposed TextHide based on   instance - encoding to preserve the model ’s utility,6414which shows promising results against gradients   matching attack . However , Xie and Hong ( 2021 )   have experimentally shown that TextHide can not   provide rigorous privacy guarantee . Orthogonal to   previous work , our work proposes a novel method   to ensure user data by protecting word embedding   layers and the associated private vocabularies in   local devices .   Partially local federated learning . Our work   is also related to recent partially local federated   learning . Liang et al . ( 2020 ) propose LG - FedAvg ,   which jointly learns compact local representations   on each device and a global model across devices .   Although LG - FedAvg is very similar to our method ,   the differences are as follows : 1 ) LG - FedAvg is   tested on a few users and small data sets , and does   not use pre - trained models ( such as GloVe ( Pen-   nington et al . , 2014 ) or DistilBERT ( Devlin et al . ,   2018 ) ) ; 2 ) LG - FedAvg assumes users are state-   ful or always - available , which is not undesirable   at scale in cross - device settings ( Singhal et al . ,   2021 ) ; 3 ) LG - FedAvg partitions local and global   parameters unstructuredly , which may lead to per-   formance degradation . Singhal et al . ( 2021 ) present   the state - of - the - art partially local FL method called   FedRecon , which trains sensitive user - specific pa-   rameters locally and other parameters globally . Fe-   dRecon only protects mappings between sparsely   sensitive words and embeddings , yet it performs   poorly against data reconstruction attacks and in-   curs large communication overhead compared with   FV . Distinguished from previous studies ,   our work conceals all mappings between vocabu-   lary and word embedding in the stateless federated   training process , and handles the trade - off between   the model ’s utility and privacy protection .   3 Method   3.1 Overview   Figure 2 depicts an overview of the proposed F-   V . Our work mainly considers the classifi-   cation model because text classification is a fun-   damental task in NLP and one of the fields widely   used in FL ( Zhu et al . , 2020 ) . As shown in Figure 2   ( a ) , the federated NLP model consists of a word   embedding layer , an encoder , and a classification   layer .   To protect models from the data reconstruction   attack while preserving the model ’s utility , we   propose an efficient method named FV ,   which protects the private mappings between words   and their embeddings . To simulate realistic applica-   tions , we consider a more challenging yet practical   cross - device FL setting where participant users are   unstably , e.g. , user may drop out , and the data on   each device are not independent and identically   distributed ( Non - IID ) .   In the following , we describe the two key ideas   ofFV . The first one is private mappings   protection ( Sec . 3.2 ) , which is critical to defend   data reconstruction attack . The second is the adap-   tive updating ( Sec . 3.3 ) to minimize the perfor-   mance drops in cross - device FL setting . The pseu-   docode of overall training processing of FV- is illustrated in Algorithm 1 .   3.2 Private mappings protection   In this section , we elaborate on the core algorithm   ofFV . When the eavesdroppers perform   the data reconstruction attack , it is necessary to   know the mappings between words and their em-   beddings due to the discrete nature of the text data .   Based on this , it is more privacy - preserving to lo-   cally store word embedding layers and associated   vocabularies than sharing them . To this end , we   decouple vocabulary and word embedding from the   global NLP model . As shown in Figure 2 ( a ) , users   can construct their personalized vocabulary and em-   bedding layer and keep them on the local device . In6415   this way , FV can protect the important   mappings between vocabularies and embedding   layers against data reconstruction attacks . Such   vocabularies are personalized w.r.t . to the local   texts , e.g. , reflecting users ’ preference of wording ,   in contrast , global models typically have a fixed   vocabulary shared among all users .   We then show how to inject private mappings   protection into the federated learning pipeline . Fig-   ure 2 ( b ) describes the federated training process   using FV . Our method considers a stan-   dard setting in the FL framework that there is a   central server responsible for coordinating users   of interest for local training and distributing the   global parameters . In each federated communica-   tion round , there are Kusers to be selected from   allNusers for training the FL model . We do   not assume user devices are always online , which   is realistic in many real - world scenarios . For in-   stance , mobile devices may drop out ( shut down )   at anytime . For each selected k - th user , FV- decompose federated NLP model ’s parame-   tersW={W , W}into local embedding pa-   rameters Wand global parameters W. Unlike   the canonical FL method knowing all Wdetails ,   the server only accesses the global parameters and   distributes Wto selected users . The optimization   for the server with the global parameters is :   minL(W ) = min / summationdisplaypL / parenleftig   W , W / parenrightig   ( 1 )   where Lis the local training objective of k - th   user , pis the weight of the k - th user such that   p = andnis amount of k - th user’sdataset . Suppose that the k - th user has a super-   vised data set D={(x , y ) } , and the model   G(W , x ) : R→ Y maps inputs x∈Rto pre-   dicted label . The k - th user ’s local training objective   Lis defined by :   L(W , W ) = 1   n / summationdisplayl / parenleftig   G / parenleftbig   ( W , W ) , x / parenrightbig   , y / parenrightig   ( 2 )   where lis the local loss function . Next , we present   how FV trains such decomposed NLP   model in each federated communication round ,   including the central server update and the user-   device update .   Central server update . The server distributes   global parameters to every user of interest at the   beginning of each federated communication round .   Then it monitors the collection of updated param-   eters sent by each user . After receiving the global   parameters of all selected users , the server performs   the federated aggregation and updates the global   parameters by Eq . 1 .   User - device update . When the selected users   download the global parameters , they assemble a   whole model with distributed global parameters   and local embedding parameters . Then , selected   users train the assembled model with local private   data by W = W−ηwhere ηis the learning   rate . After local training , the k - th user sends its   updated global parameters Wto the central server   for federated aggregation .   The training process described above is repeated   until specific criteria are met.64163.3 Adaptive Updating   It is realistic to assume that a user device does   not participate in every round of training . Hence ,   the parameters of local word embeddings may   well be outdated and are incompatible with the   newly received global parameters . The prior stud-   ies show that it leads to deteriorated model per-   formance ( Reddi et al . , 2020 ; Singhal et al . , 2021 ) .   The challenge is thus to learn local parameters com-   patible with the new global parameters efficiently .   Motivated by gradient - based alternating mini-   mization , we introduce a simple but effective adap-   tive updating strategy into the local training process .   Specifically , once receiving new global parameters ,   FV performs one local training epoch   to adapt the local embedding parameters to the   global parameters and freezes the global module   during the process . Considering the limited com-   puting resources of user devices , it is sufficient to   reuse the same optimization method , which is used   for updating all model parameters after this step .   FV can also adopt other gradient - based   alternating minimization techniques ( Singhal et al . ,   2021 ; Zhu and Sun , 2021 ) , which we will explore   in the future .   4 Experiments   In this section , we demonstrate how our method   1 ) effectively defends against data reconstruction   attacks by protecting private mappings between vo-   cabularies and word embeddings , 2 ) preserves the   model ’s utility with achieving competitive perfor-   mance , 3 ) efficiently reduces communicated param-   eters by keeping embedding layer in local devices .   In addition , we also show adaptive updating plays   a critical role in challenging yet practical cross-   device FL setting .   4.1 Experimental Setup   Datasets and Non - IID Partitions . Following   Lin et al . ( 2021 ) , we conduct experiments on there   classification datasets : 20News ( Lang , 1995 ) , AG   News ( Zhang et al . , 2015 ) , and SST-2 ( Socher   et al . , 2013 ) . These public datasets serve as bench-   marks in Lin et al . ( 2021 ) to verify the proposed   FL method . In order to evaluate our method in a   realistic and challenging setting , we consider the   Non - IID data partitioning throughout the experi-   ments . In particular , instead of uniformly sampling   the datasets , we partition the datasets by using the   Dirichlet distribution as the class priors . We sample   D ∼Dir(α)and allocate data Dtok - th user . α   determines the degree of Non - IID , and a smaller α   generates a high label distribution shift . Following   Lin et al . ( 2021 ) configuration , we set α= 1.0as   default and set the number of cross - device users as   100 for all datasets . The statistics of these datasets   are in Table 1 .   Models . We primarily evaluate two prevalent   NLP models in our experiments : BiLSTM ( Graves   et al . , 2005 ) and DistilBERT ( Sanh et al . , 2019 ) .   These models are also widely used to mimic real-   istic federated NLP applications ( Sui et al . , 2020 ;   Lin et al . , 2021 ; Huang et al . , 2020 ) when the user ’s   computational capabilities and bandwidth are re-   stricted . We have also evaluated our methods with   bigger models , such as BERT - Base , and the results   are shown in B.1 . More details of model hyperpa-   rameter tuning are given in Appendix A.1 .   Baselines . To comprehensively evaluate the per-   formance of FV , we compare FV- against five baselines with different models   on various datasets . Local - only refers to training   model only using local data on each user device   without collaborations between other users . In the   FL family , we compare two classic and global FL   methods : FedAvg ( McMahan et al . , 2017a ) is the   benchmark FL method that collaboratively trains   a global FL model across users , and FedProx ( Li   et al . , 2020 ) excels at handling heterogeneity in fed-   erated learning by using Lregularization to limit   local model updates to be closer to the global model   for more stable and accurate convergence . We pro-   vide two partially local FL methods which also   decompose the model parameters into global pa-   rameters and local parameters : LG - FedAvg ( Liang   et al . , 2020 ) jointly learns compact local represen-   tations on each device and a global model across   all devices . FedRecon ( Singhal et al . , 2021 ) is the   state - of - the - art FL method that trains sensitive user-   specific parameters locally and other parameters   federated . See Appendix A.2 for details on each   baseline method implementations.6417   4.2 Privacy Experiments   We first evaluate FV against the gradient-   based data reconstruction attack , which imposes a   severe challenge to FL . Gradient - based data recon-   struction attack is first proposed by Zhu and Han   ( 2020 ) and can effectively recover users ’ private   data . They assume eavesdroppers get access to the   complete model details and are able to intercept   gradients in the FL process . With this assumption ,   the eavesdroppers can obtain both the training in-   puts through the DLG optimization algorithm . See   Appendix A.3 for details .   Baselines and Metrics . We consider the FL   methods performing well in the utility experiments   as baselines ( see Sec . 4.3 ) . In particular , we choose   the classic global FL method ( FedAvg ) and state-   of - the - art partially local FL method ( FedRecon )   and evaluate them for privacy protection . As the   aim of attackers is to recover user text in auxil-   iary private datasets , we evaluate FV and   baselines in terms of precision ( the average percent-   age of recovered words in the target texts ) , recall   ( the average percentage of words in the target texts   are predicted ) and F1 score which is the harmonic   mean between precision and recall . Given that   only private token embeddings are locally trained   in FedRecon , we also exploit privacy tokens leak-   age ratio ( PTLR ) to evaluate each method ’s ability   to protect privacy - sensitive tokens . More details   of this attack and the auxiliary private dataset are   provided in Appendix A.3 .   Results . Table 2 and Figure 3 show the pri-   vacy protection results . These results demonstrate   that FV can consistently outperform   previous methods indefending against thedata   reconstruction attack .   Compared with FedRecon , eavesdroppers can-   not recover users ’ local data with our method by   achieving almost zero recall and precision . F-   V effectively protects privacy through mak-   ing the mappings between the local device ’s vo-   cabulary and their word embeddings private . In   contrast , FedRecon shares most of these essential   mappings , which leads to high recall and precision .   As shown in Table 2 , FedRecon has achieved sim-   ilar performance results regarding privacy tokens   protection . These results demonstrate that privacy-   preserving user mappings can protect against data   reconstruction attacks efficiently . More case stud-   ies can be found in Appendix B.3 .   The canonical FL method FedAvg shows frustrat-   ing results of privacy attacks , especially in sensitive   tokens . It is not a surprise that FV is sub-   stantially better than FedAvg in attack metrics and   PTLR because FedAvg discloses the private vocab-   ulary and associated word embedding in training   processing .   In Figure 3 , we show the privacy - preserving ca-   pacity of each FL method with different batch sizes .   We can see that increasing batch size makes the re-   covery more difficult . The plausible reason behind   this is that there are more variables to solve dur-   ing DLG optimization in large batch size . This   result is also in accordance with the experimental   results in Zhu and Han ( 2020 ) , which suggests that   increasing the training batch size is a good defense   strategy . However , there is a trade - off with lim-   ited user - device computing resources ( i.e. , mobile )   and large batches . Notably , our method ’s privacy-   preserving capability is not affected by the batch   size , providing a practical defense strategy for users   with restricted computing resources.6418   4.3 Utility Experiments   We measure the utility of our method in terms of   test accuracies and compare it with various base-   lines with both DistilBERT and BiLSTM on three   text classification corpora . Our experiments evalu-   ate all methods using two metrics :   1 ) Global Test ( Global ): FL methods are evalu-   ated on each test set having the same distribution   as the global data distribution . For each method ,   we report the geometric mean of the test accura-   cies collected from each local device . The global   test results can measure the capability of models to   learn global knowledge .   2 ) Local Test ( Local ): Each test set on each local   device follows the local training data distributions ,   which vary across devices . For each method , we   report the averaged test accuracy from all users .   Compared with the global test , the local test is   more realistic for real - world NLP applications and   it can show performance improvement without cen-   tralizing user - sensitive data .   Results . The utility results for FV and   baselines are listed in Table 3 and Table 4 . Overall ,   FV achieves competitive performance   results ontheglobal testandlocal testwhere the   performance gapwith thebest baseline iswithin   1 % . This competitive result demonstrates the us-   ability of detaching the embedding layer from fed-   erated aggregations . From Table 3 and Table 4 , wehave two key findings :   First , the partially local FL method with adap-   tive updating outperforms the global FL method .   Compared with the global FL , FV and   FedRecon significantly improve test accuracy on   the global test and local test . In particular , the   largest performance gains for FV and Fe-   dRecon are 8.5 % and 4.7 % ( local test on 20News   with DistilBERT in Table 3 ) , respectively . These re-   sults show partially local FL method is able to help   the local model adapt to the local task and learn   global knowledge better in Non - IID data distribu-   tion . However , another partially local FL method   LG - FedAvg performs poorly on most datasets and   models . We conjecture that this is largely due to   the fact that 1 ) LG - FedAvg does not use adaptive   updating , resulting in a performance decrease in the   cross - device FL scenario ; 2 ) Decoupling local and   global parameters in an unstructured way makes it   harder to learn , especially on data with the Non - IID   distributions .   Second , FL methods significantly outperform   the method with only local training . As shown in   Table 3 and Table 4 , FL methods have better per-   formance compared to the local - only method , espe-   cially for users with a small amount of local data   and learning global knowledge ( the maximum per-   formance gap is 67.3 % in the global test of 20News   with DistilBERT ) . Unsurprisingly , the local - only   method produces extremely poor results . The rea-6419   son is the local - only method exploits only limited   training samples on the local device to train the   model . Conversely , FL methods significantly im-   prove local model performance despite not having   direct access to the user data where centralized   training is impossible .   Compared with FedRecon , we find that F-   V can obtain better results with BiLSTM and   comparable results on DistilBERT . We conjecture   that the performance gap is resulted by different   ways of local model updating : 1 ) our method up-   dates the word embedding layer by using all the lo-   cal data during adaptive updating . FedRecon based   on meta - learning ( Finn et al . , 2017 ) only uses part   of the data to update a part of local word embed-   dings ( i.e. , privacy - sensitive word embedding ) . It   is particularly challenging for this method to train   on a small volume of data in a local device ; 2 ) Col-   laboratively training shared word embeddings may   make models suffer from instability due to Non-   IID data distributions . Table 3 and Table 4 also   demonstrate that FedAvg and FedProx get subopti-   mal performance where they cooperatively train the   whole embedding layer using data from Non - IID   data distributions .   4.4 Communication efficiency results   Current large - scale NLP models often contain bil-   lions of parameters , it is challenging to deploy   large models in realistic FL applications due to the   insufficient communication speed and low band-   width ( Sui et al . , 2020 ) . Therefore , we evaluate the   communication efficiency of FV and the   baselines .   From Figure 4 we can tell that , compared with all   baselines , FV cantransmit fewer model   parameters , hence itispractical forusers with   limited bandwidth . The reduced communication   cost of FV comes from detaching the em-   bedding layers from the federated aggregation . In   Figure 4 , FV is more effective in the clas-   sic NLP models ( e.g. , only 1.41 MB in BiLSTM ) .   The global FL methods have almost no communica-   tion cost decrease compared with the partially local   FL methods . The decreasing communication cost   of FedRecon comes from the size of the privacy   word embeddings . However , the privacy - sensitive   words in the real world are sparse . Although LG-   FedAvg can achieve more flexible traffic reduction ,   it needs to communicate more parameters consid-   ering its performance ( see Sec.4.3 ) .   4.5 Contribution of adaptive updating   We investigate the contribution of adaptive updat-   ing used in FV . We conduct ablation   studies and show the results by removing adaptive   updating ( - adap ) on all datasets . As illustrated   in Table 5 , FV with removing adaptive   updating obtains worse performance . Therefore ,   adaptive updating is essential in enhancing F-   V , especially for the cross - device FL setting .   5 Conclusion   We have presented FV , a practical train-   ing method for privacy - preserving NLP models .   FV protects the private mappings be-   tween local vocabulary and the associated embed-   ding layer by detaching the embedding layer from   federated aggregation . In this manner , FV- allows users to personalize their vocabularies   and word embedding layers . To tackle the dilemma   in cross - device FL , we propose an adaptive updat-   ing to minimize performance drops . The privacy   and utility experiments show FV pro-6420vides significantly better privacy protection than the   baselines while maintaining the utility of models .   Moreover , FV also significantly reduces   communication costs than the SOTA FL methods .   Limitations   We show the limitations of FV in terms   of various privacy attacks in FL and additional com-   puting costs in local training .   Privacy attacks . FV mainly considers   the defense against gradient - based data reconstruc-   tion attacks and shows significant defense results .   However , the attacker is still able to recover the   sentence embedding before the encoder because of   shared gradients of the global module . Although   it is difficult for an attacker to recover the input   text without knowing the mappings between words   and their embeddings , it may perform membership   information attack ( Melis et al . , 2019 ) and sensitive   attribute information attack ( Alnasser et al . , 2021 ) ,   which will also lead to the user privacy disclosure   to a certain extent .   From the realistic scenario , the purpose of users ’   participation in FL is not only to improve the perfor-   mance of local models , but also to protect their sen-   sitive data from obtaining or detecting . Compared   with other mentioned attacks , data reconstruction   attack is the primary privacy attacks to be defensed   in federated learning . Recently , some work has   proposed effective defense against these mentioned   attacks , such as adversarial training ( Louppe et al . ,   2017 ) . We think FV is easy to combine   these methods , which we will explore in the future .   In addition , there is no unified privacy protection   metric to evaluate the existing privacy protection   technologies . The evaluation metrics of differen-   tial privacy and homomorphic encryption are not   suitable for measuring the privacy protection abil-   ity of FV . This is also the reason why   our method is not directly compared with existing   privacy protection technologies . With the growing   privacy concerns in deep learning , it is an urgent   need to explore a general privacy protection evalu-   ation metric .   Additional computing costs . InFV ,   we introduced adaptive updating to reduce the per-   formance degradation in the cross device FL sce-   nario . During local training , we use all local data   to update the embedded layer and freeze the global   module . Although our method is uncomplicated , it also brings additional computing overhead , es-   pecially for devices with limited computing power   and a large amount of data .   Acknowledgements   We ’d like to thank all the anonymous review-   ers for their careful readings and valuable com-   ments . This paper was partially supported by Na-   tional Key Research and Development Program   of China ( No . 2018AAA0100204 ) , a key pro-   gram of fundamental research from Shenzhen Sci-   ence and Technology Innovation Commission ( No .   JCYJ20200109113403826 ) , and the Major Key   Project of PCL ( No . PCL2021A06 ) .   References642164226423A Experimental Details   A.1 Models Implementations   Our model implementations and each user ’s local   training procedure are based on FedNLP ’s code .   For fair comparison , we adopt the model hyperpa-   rameters straight from the default set in here for   all baselines and our method . To be more con-   crete , the one - layer BiLSTM has 300 hidden states   and the dropout rate is set to 0.5 . Adam is cho-   sen as the optimizer with an initial learning rate   of 5e-3 . For the transformer - based model , we   exploit AdamW as the optimizer and set an ini-   tial learning rate of 5e-5 with linear decay . To   make a fair comparison , we keep the same embed-   ding layer initialization for all methods where the   BiLSTM utilizes pre - trained GloVe ( Pennington   et al . , 2014 ) while DistilBERT uses the primary pre-   trained embedding layer . Our code are available at   https://github.com/SMILELab-FL/FedV ocab .   A.2 Baseline Setup   All baseline methods are based on FedLab ’s   code , which is a lightweight open - source frame-   work ( Zeng et al . , 2021 ) for FL simulations . For   LG - FedAvg , we tune the interpolation between the   local and global model and report the best results .   For FedRecon , we follow Singhal et al . ( 2021 ) next   words prediction experiment where they configure   that out - of - vocabulary ( OOV ) embeddings are lo-   cal and the rest of the model ( including the core   vocabulary embeddings ) is global . In our experi-   ment , we set sensitive tokens ( i.e. , digital tokens )   in the model ’s vocabulary as OOV tokens . The   communication rounds of each FL method is 100   and one training local epoch for all models . Note   that there is no collaborative training for the local-   only method . To make fair comparisons , the total   number of local training epochs in the local - only   method will be greater than that of FL methods .   We set local training epochs as 10 . We train all   methods on an NVIDIA Tesla V100 and report the   best results .   A.3 Details of attacks   Auxiliary Dataset . For the auxiliary datatset , we   sample 128 sentences from AG News dataset as the   target data Dand perform data reconstruction at-   tack to get recovered data D. To demonstrate the   protection of private tokens ( such as digital tokens),each sentence in the auxiliary dataset contains at   least three digital tokens .   Deep Leakage from Gradients . TheDeep Leak-   age from Gradients ( DLG ) optimization algo-   rithm ( Zhu and Han , 2020 ) shows that sharing   the gradients can leak private training data . It   starts by randomly initializing a pair of dummy   data and labels and performs the usual forward and   backward . When getting the user - uploaded real   gradients , the attacker computes the l - distance   between the dummy gradients deriving from the   dummy data and the real gradients . And then it   back - propagates this loss to update the dummy data .   After multiple iterative updates , the attacker can   recover the original input data .   However , NLP models need to preprocess dis-   crete words into embeddings which is different   from vision tasks where image inputs are contin-   uous values . The mappings between vocabulary   and its word embedding is able to inversely map   the continuous embedding into the original token .   Knowing the mappings is the critical point to re-   cover input text in data reconstruction attacks . Fol-   lowing Zhu and Han ( 2020 ) , we apply DLG on   embedding space and minimize the gradients dis-   tance between dummy embeddings and real ones .   After optimization finishes , we can get the recov-   ered embeddings and derive original words by find-   ing the closest token in different mappings . For   FedAvg , we use the known mappings . The map-   pings in FedRecon are different from the known   mappings only in privacy tokens ’ embeddings . Be-   cause FV allows users to customize their   mappings , we use a mapping that is completely   different from the known mapping .   B Extra Results   B.1 The results of bigger model   To verify the effectiveness in the large model , we   evaluated our methods with bigger models , such as   BERT - Base . The results are shown in Table 6 and   Table 7 . We find that FV can outperform   the baseline methods in terms of privacy protec-   tion and achieve competitive performance results   in terms of utility .   B.2 The effect of different embedding   initializations   We present results with different initialization   ( Different init ) and the default same initializa-   tion ( Same init ) on three benchmark datasets . In6424   theDifferent init setting , each user can choose   its embedding initialization from pre - trained em-   bedding initialization set ( i.e. , GloVe embedding   initialization from { 6B , 42B , 840B } or DistilBERT   embedder initialization from { DistilBERT , BERT-   Base , BERT - Large , GPT2 } ) . As a result , users   can not only customize the vocabularies but also   choose different embedding initialization . The per-   formance results are shown in Figure 5 . Compared   with Same init , FV can even outper-   form the models trained with the same initial em-   beddings in terms of privacy protection ( see Fig-   ure 3 ) while achieving comparable performance in   terms of utility .   We can observe that a slight performance degra-   dation of Different init is consistent for SST-2   and AG News , compared with Same init . But we   notice that Different init can outperform Same   init on the 20 News dataset . We speculate that the   reason is due to the characteristics of the dataset .   Compared with SST-2 and AG News , 20 News has   fewer data per user and more category labels . As a   result , 20New is more challenging in the federated   learning setting . Different init can help users   more personalize their local models and perform   better in more challenging settings .   B.3 Case Study   Table 8 and Table 9 show examples of the data   reconstruction attack from AG News dataset . We   perform the DLG described in A.3 and set batch   size as 1 . Compared with baselines , FV   can provide the firmly privacy - preserving ability   for the user - device text and privacy - sensitive to-   kens . FedAvg divulges user privacy tokens and al-   most all user text data since it discloses the shared   model ’s details and its gradients in the communi-   cation process . Although FedRecon can protect   private tokens , other input tokens can be recon-   structed , which may increase the risk of legal and   ethical issues in the real world.6425