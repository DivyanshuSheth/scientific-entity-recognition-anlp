  Patrick Lewis , Barlas O ˘guz , Wenhan Xiong ,   Fabio Petroni , Wen - tau Yih , Sebastian Riedel   Meta AI   { plewis , barlaso , xwhan , fabiopetroni , scottyih , sriedel } @fb.com   Abstract   We propose DrBoost , a dense retrieval ensem-   ble inspired by boosting . DrBoost is trained   in stages : each component model is learned   sequentially and specialized by focusing only   on retrieval mistakes made by the current en-   semble . The final representation is the concate-   nation of the output vectors of all the compo-   nent models , making it a drop - in replacement   for standard dense retrievers at test time . Dr-   Boost enjoys several advantages compared to   standard dense retrieval models . It produces   representations which are 4x more compact ,   while delivering comparable retrieval results . It   also performs surprisingly well under approxi-   mate search with coarse quantization , reducing   latency and bandwidth needs by another 4x .   In practice , this can make the difference be-   tween serving indices from disk versus from   memory , paving the way for much cheaper de-   ployments .   1 Introduction   Identifying a small number of relevant documents   from a large corpus to a given query , information   retrieval is not only an important task in - and - of   itself , but also plays a vital role in supporting a   variety of knowledge - intensive NLP tasks ( Lewis   et al . , 2020 ; Petroni et al . , 2021 ) , such as open-   domain Question Answering ( ODQA , V oorhees   and Tice , 2000 ; Chen et al . , 2017 ) and Fact Check-   ing ( Thorne et al . , 2018 ) . While traditional retrieval   methods , such as TF - IDF and BM25 ( Robertson ,   2008 ) , are built on sparse representations of queries   and documents , dense retrieval approaches have   shown superior performance recently on a range   of retrieval and ranking tasks ( Guu et al . , 2020 ;   Karpukhin et al . , 2020 ; Reimers and Gurevych ,   2019 ; Hofst ¨atter et al . , 2021b ) . Dense retrievalinvolves embedding queries and documents as low-   dimensional , continuous vectors , such that query   and document embeddings are similar when the   document is relevant to the query . The embedding   function leverages the representational power of   pretrained language models and is further finetuned   using any available training query - document pairs .   Document representations are computed offline in   anindex allowing dense retrieval to scale to mil-   lions of documents , with query embeddings being   computed on the fly .   When deploying dense retrievers in real - world   settings , however , there are two practical concerns :   thesizeof the index and the retrieval time latency .   The index size is largely determined by the num-   ber of documents in the collection , as well as the   embedding dimension . Whilst we can not gener-   ally control the former , reducing the embedding   size is an attractive way to reduce index size . On   lowering latency , Approximate Nearest - Neighbor   ( ANN ) or Maximum Inner Product Search ( MIPS )   techniques are required in practice . This implies   that it is far more important for retrieval models to   perform well under approximate search rather than   in the exact search setting . Developing a dense   retrieval model that produces more compact em-   beddings and are more amenable to approximate   search is thus the focus of this research .   In this paper , we propose DrBoost , an ensem-   ble method for learning a dense retriever , inspired   byboosting ( Schapire , 1990 ; Freund and Schapire ,   1997 ) . DrBoost attempts to incrementally build   compact representations at training time . It con-   sists of multiple component dense retrieval models   ( “ weak learners ” in boosting terminology ) , where   each component is a BERT - based bi - encoder , pro-   ducing vector embeddings of the query and docu-   ment . These component embeddings are in lower   dimensions ( e.g. , 32 vs. 768 ) compared to those   of regular BERT encoders . The final relevance   function is implemented as a linear combination3102of inner products of embeddings produced by each   weak learner . This can be efficiently calculated   by concatenating vectors from each component   and then performing a single MIPS search , which   makes DrBoost a drop - in replacement for standard   dense retrievers at test time . Component models   are trained and added to the ensemble sequentially .   Each model is trained as a reranker over negative   examples sampled by the current ensemble and can   be seen as specializing on retrieval mistakes made   previously . For example , early components focus   on high - level topical information , whereas later   components can capture finer - grained tail phenom-   ena . Through this mechanism , individual compo-   nents are disentangled and redundancy minimized ,   leading to more compact representations .   There are a couple of noticeable differences in   training DrBoost when compared to existing dense   retrieval models . Although iterative training us-   ing negatives sampled by models learned in the   previous rounds has been proposed ( Xiong et al . ,   2020 ; Qu et al . , 2021 ; O ˘guz et al . , 2021 ; Sachan   et al . , 2021 , inter alia . ) , existing methods keep only   the final model . In contrast , each weak learner in   DrBoost is preserved and added to the ensemble .   The construction of the embedding also differs . Dr-   Boost can be viewed as a method of slowly “ grow-   ing ” overall dense vector representations , lend-   ing some structure to otherwise de - localized repre-   sentations , while existing retrieval models encode   queries and documents in one step .   More importantly , DrBoost enjoys several ad-   vantages in real - world settings . Because each weak   learner in DrBoost produces very low - dimensional   embeddings to avoid overfitting ( 32 - dim in our   experiments ) , many components can be added   whilst the index stays small . Our experiments   demonstrate that DrBoost produces very compact   embeddings overall , achieving accuracy on par   with a comparable non - boosting baseline with 4 –   5x smaller vectors , and strongly outperforming   a dimensionally - matched variant . Probing Dr-   Boost ’s embeddings using a novel technique , we   also show that the embeddings can be used to re-   cover more topical information from Wikipedia   than a dimensionally - matched baseline .   Empirically , DrBoost performs superbly when   using approximate fast MIPS . With a K - mean in-   verted file index ( IVF ) , the simple and widely used   approach , especially in hierarchical indices and   Web - scale settings ( J ´ egou et al . , 2011 ; Johnsonet al . , 2019 ; Matsui et al . , 2018 ) , DrBoost greatly   outperforms the baseline DPR model ( Karpukhin   et al . , 2020 ) by 3–10 points . Alternatively , it can   reduce bandwidth and latency requirements by 4 –   64x while retaining accuracy . In principle , this   allows for the approximate index to be served on-   disk rather than in expensive and limited RAM   ( which is typically 25x faster ) , making it feasible   to deploy dense retrieval systems more cheaply   and at much larger scale . We also show that Dr-   Boost ’s index is amenable to compression , and can   be compressed to 800 MB , 2.5x smaller than a re-   cent state - of - the - art efficient retriever , whilst being   more accurate ( Yamada et al . , 2021 ) .   2 Dense Retrieval   We give here the background of dense retrieval and   boosting , as well as our proposed method . More   extensive related work can be found in § 5.5 .   Dense Retrieval involves learning a scalable rel-   evance function h(q , c)which takes high values for   passages cthat are relevant for question q , and low   otherwise . In the popular dense bi - encoder frame-   work , h(q , c)is implemented as the dot product   between qandc , dense vector representations of   passages and questions respectively , produced by a   pair of neural network encoders , EandE ,   h(q , c ) = E(q)E(c ) = qc ( 1 )   where q = E(q)andc = E(c ) . At in-   ference time , retrieval from a large corpus C=   { c , . . . , c}is accomplished by solving the fol-   lowing MIPS problem : c= arg maxqc .   In standard settings , we assume access to a set of   mgold question - passage pairs D={(q , c ) } .   It is most common to learn models by training   to score gold pairs higher than sampled nega-   tives . Negatives can be obtained in a variety   of ways , such as by sampling at random from   corpus C , or by using some kind of importance   sampling function on retrieval results ( see § 2.1 ) .   When augmented by nnegatives per gold passage-   document pair , we have training data of the form   /tildewideD={(q , c , c , . . . c ) } , which we use to   train a model , e.g. , using a ranking or margin ob-   jective , or in our case , by optimizing negative log-   likelihood ( NLL ) of positive pairs   L=−loge   e+/summationtexte3103Algorithm 1 Dense Retrieval with Iteratively-   sampled Negatives v.s. Boosted Dense Retrieval   2.1 Iterated Negatives for Dense Retrieval   The choice of negatives is an important factor for   what behaviour dense retrievers will learn . Simply   using randomly - sampled negatives has been shown   to perform poorly , because they are too easy for   the model to discriminate . Thus it is common to   mix in some hard negatives along with random   negatives , which are designed to be more challeng-   ing to distinguish from gold passages ( Karpukhin   et al . , 2020 ) . Hard negatives are typically collected   by retrieving passages using an untrained retriever ,   such as BM25 , and filtering out any unintentional   golds . This ensures the hard negatives are at least   topically - relevant .   Recently , it has become common practice to run   a number of rounds of dense retrieval training to   bootstrap hard negatives ( Xiong et al . , 2020 ; Qu   et al . , 2021 ; O ˘guz et al . , 2021 ; Sachan et al . , 2021 ,   inter alia . ) . Here , we first train a dense retriever   following the method we describe above , and then   use this retriever to produce a new set of hard nega-   tives . This retriever is discarded , and a new one is   trained from scratch , using the new , “ harder ” nega-   tives . This process can then be repeated until per-   formance ceases to improve . This approach , which   we refer to dense retrieval with iteratively - sampled   negatives is described in Algorithm 1 .   2.2 Boosting   Boosting is a loose family of training algorithms   for machine learning problems , based on the princi-   ple of gradually ensembling “ weak learners ” into astrong learner . Boosting can be described by the fol-   lowing high - level formalism ( Schapire , 2007 ) . For   a task with a training set { ( x , y),···,(x , y ) } ,   where ( x , y)∈X×Ywe want to learn a function   h : X→Y , such that h(x ) = ˆy≈y . This is   achieved using an iterative procedure over Rsteps :   •For round r , we construct an importance dis-   tribution Dover the training data , based on   where error ϵof our current model his high   •Learn a “ weak learner ” hto minimize er-   rorϵ=/summationtextD(i)L(h(x ) , y)for some   loss function Lmeasuring the discrepancy be-   tween predictions and real values .   •Combine handhto form a new , stronger   overall model , e.g. , by a linear combination   h = αh+βh . The iteration can now be   repeated .   The initial importance distribution Dis usually as-   sumed to be a uniform distribution , and hmodels   a constant function . Note that how each additional   model added to his specifically designed to solve   instances that hcurrently struggles with .   2.3 Boosted Dense Retrieval : DrBoost   We note similarities between the boosting formu-   lation , and the dense retrieval with iteratively-   sampled negatives . We can adapt a boosting-   inspired approach to dense retrieval with minimal   changes , as shown in Algorithm 1 . Algorithmically ,   the only difference ( lines 10–13 ) is that in the case   of iterative negatives , the model hafterrrounds   isreplaced by the new model h , whereas in the   boosting case , we combine handh .   In this paper , we view the boosted “ weak learner ”   models hasrerankers over the retrieval distri-   bution from the current model h. That is , when   training dense boosted retrievers , we only train us-   ing hard negatives , and do not use any random or   in - batch negatives . Using the construction of neg-   atives as a mechanism to define the importance   distribution , each new model is directly trained to   solve the retrieval mistakes that the current ensem-   ble makes . Each model his implemented as a   bi - encoder , as in Eq . ( 1 ) . We combine models as   linear combinations : ( h , h ) = h = αh+βh   The coefficients could be learnt from development   data , or , simply by setting all coefficients to 1,3104which we find to be empirically effective . The   overall model after Rrounds can be written as :   where [ . . .]indicates vector concatenation . Thus   hcan be computed as a single inner product , mak-   ing it a drop - in replacement for standard MIPS   dense retrievers at test time .   One downside of the boosting approach is that   we must maintain Rencoders for both passages   and questions . Since passages are embedded of-   fline , this does not create additional computational   burden on the passage side at test time . However ,   on the query side , for a question q , boosted dense   retrieval requires Rforward passes to compute   the full representation , one for each subvector q.   While this step is fully parallelizable , it is still un-   desirable . We can remedy this for low - latency , low-   resource settings by distilling the question encoders   ofhinto a single encoder , which can produce the   overall question representations ¯qdirectly . Here ,   given the training dataset Dof gold question-   passage pairs , and a model hwe want to distill , we   first compute overall representations ¯qand¯cfor   all pairs using has distillation targets , then train a   new question encoder Ewith parameters ϕ , by   minimizing the objective :   L=/summationdisplay∥E(q)−¯q∥+∥E(q)−¯c∥   3 Experiments   3.1 Datasets   Natural Questions ( NQ ) We evaluate retrieval   for downstream ODQA using the widely - used NQ-   open retrieval task ( Kwiatkowski et al . , 2019 ) .   This requires retrieving Wikipedia passages which   contain answers to questions mined from Google   search logs . We use the preprocessed and gold   pairs prepared by Karpukhin et al . ( 2020 ) , and re-   port recall - at - K ( R@K ) for K ∈ { 20,100 } .   MSMARCO We evaluate in a Web - text setting   using the widely - used passage retrieval task from   MSMARCO ( Bajaj et al . , 2016 ) . Queries consist   of user search queries from Bing , with human-   annotated gold relevant documents . We use thepreprocessed corpus , training and dev data ( gold   pairs and data splits ) from O ˘guz et al . ( 2021 ) . We   follow the common practice of reporting the Mean-   Reciprocal - Rank - at-10 ( MRR@10 ) metric for the   public development set .   3.2 Tasks   In this section , we will describe the experiments   we perform , and the motivations behind them .   Exact Retrieval We are interested in understand-   ing whether the boosting approach results in supe-   rior performance for exhaustive ( exact ) retrieval .   Here , no quantization or approximations are made   to MIPS , which results in large indices , and slow   retrieval , but represents the upper bound of accu-   racy . This setting is the most commonly - reported   in the literature .   Approximate MIPS : IVF Exact Retrieval does   not evaluate how a model performs in practically-   relevant settings . As a result , we also evaluate in   two approximate MIPS settings . First , we con-   sider approximate MIPS with an Inverted File In-   dex ( IVF , Sivic and Zisserman , 2003 ) . IVF works   by first clustering the document embeddings of-   fline using K - means ( Lloyd , 1982 ) resulting K   cluster centroids . At test time , for a given query   vector , rather than compute an inner product for   each document in the index , we instead compute   inner products to the Kcentroids . We then visit   thenprobes highest scoring clusters , and com-   pute inner products for only the documents in these   clusters . This technique increases the speed of   search significantly , at the expense of some accu-   racy . Increasing K , the number of centroids , in-   creases speed , at the expense of accuracy , as does   decreasing the value of nprobes . A model is   preferable if retrieval accuracy remains high with   very fast search , i.e. , low nprobes and high K.   In our experiments we fit K= 65536 clusters and   sweep over a range of values of nprobes from2   to2 . Other methods such as HNSW ( Malkov and   Yashunin , 2020 ) are also available for fast search ,   but are generally more complex and can increase   index sizes significantly . IVF is a particularly pop-   ular approach due it its simplicity , and as a first   coarse quantizer in hierarchical indexing ( Johnson   et al . , 2019 ) , since it is straightforward to apply3105sharding to the clusters , and further search indices   can be built for each cluster .   Approximate MIPS : PQ Whilst IVF will in-   crease search speeds , it does not reduce the size of   the index , which may be important for scalability ,   latency and memory bandwidth considerations . To   investigate whether embeddings are amenable to   compression , we experiment with applying Product   Quantization ( PQ , J ´ egou et al . , 2011 ) . PQ is a lossy   quantization method that works by 1 ) splitting vec-   tors into subvectors 2 ) clustering each subvector   space and 3 ) representing vectors as a collection   cluster assignment codes . We apply PQ using 4-   dimensional sub - vectors and 256 clusters per sub-   space , leading to a compression factor of 16x over   uncompressed float32 .   All MIPS retrieval is implemented using   FAISS ( Johnson et al . , 2019 ) .   Generalization Tests In addition to in - domain   evaluation , we also perform two generalization   tests . These will determine whether the boosting   approach is superior to iteratively - sampling nega-   tives in out - of - distribution settings . We evaluate   MSMARCO - trained models for zero - shot general-   ization using selected BEIR ( Thakur et al . , 2021 )   datasets that have binary relevance labels . Namely ,   we test on the SciFact , FiQA , Quora and ArguAna   subsets . This will test how well models generalize   to new textual domains and different query surface   forms . We also evaluate NQ - trained models on   EntityQuestions ( Sciavolino et al . , 2021 ) , a dataset   of simple entity - centric questions which has been   recently shown to challenge dense retrievers . This   dataset uses the same Wikipedia index as NQ , and   tests primarily for robustness and generalization to   new entities at test time .   3.3 Models   We compare a model trained with iteratively-   sampled negatives to an analogous model trained   with boosting , which we call DrBoost . There   are many dense retrieval training algorithms avail-   able which would be suitable for training with   iteratively - sampled negatives and boosting with   DrBoost . Broadly - speaking , any dense retriever   could be used if utilizes negative sampling , and   could be trained in Step 9 of Algorithm 1 . We   choose Dense Passage Retriever ( DPR , Karpukhin   et al . , 2020 ) with iteratively - sampled negatives due   to its comparative simplicity and popularity.3.3.1 Iteratively - sampled negatives baseline :   DPR   DPR follows the dense retrieval paradigm outlined   in§2 It is trained with a combination of in - batch   negatives , where gold passages for one question   are treated as negatives for other questions in the   batch ( which efficiently simulates random nega-   tives ) , and with hard negatives , sampled initially   from BM25 , and then from the previous round ,   as in Algorithm 1 . We broadly follow the DPR   training set - up of O ˘guz et al . ( 2021 ) . We train   BERT - base DPR models using the standard 768   dimensions , as well as models which match the   final dimension size of DrBoost . We use parameter-   sharing for the bi - encoders , and layer - norm after   linear projection . Models are trained to minimize   the negative log - likelihood of positives , and the   number of training rounds is decided using devel-   opment data , as in Algorithm 1 , using an initial h   retriever BM25 .   3.3.2 DrBoost Implementation   For our DrBoost version of DPR , we keep as many   experimental settings the same as possible . There   are two exceptions , which are required for adapting   dense retrieval to boosting . The first is that each   component “ weak learner ” model has a low embed-   ding dimension . This is to avoid overfitting , and to   make sure the final index size is manageable . We   report using models of 32 dims ( cf . the standard   768 dims ) , but note that training with dimension as   low as 8 is stable . The second is that , as motivated   in§2.3 , we train each weak learner using only hard   negatives , and no in - batch negatives . In effect , this   choice of negatives means that each model is essen-   tially trained as a reranker . DrBoost models are fit   following Algorithm 1 , and we stop adding mod-   els when the development set performance stops   improving . The initial retriever hfor DrBoost is   a constant function , and thus the initial negatives   for DrBoost are sampled at random from the cor-   pus , unlike DPR , which uses initial hard negatives   collected from BM25 .   DrBoost αCoefficients DrBoost combines weak   learners as a linear combination . We experiment   with learning the αcoefficients using development   data , however this does not significantly improve   results over simply setting them all to 1 . There-3106fore , for the sake of simplicity and efficiency , we   report DrBoost numbers with all α= 1.0 . Em-   pirically , we find the magnitudes of embeddings   for DrBoost ’s component models to be similar . In   other words , one component does not dominate   over others .   DrBoost Distillation We experiment with dis-   tilling DrBoost ensembles into a single model for   latency - sensitive applications using the L2 loss at   the end of § 2.3 . We distill a single BERT - base   query encoder , and perform early stopping and   model selection using development L2 loss .   4 Results   4.1 Exact Retrieval   Exact Retrieval results for MSMARCO and Nat-   ural Questions are shown in Table 1 in the “ Ex-   act Search ” Column . We find that our DrBoost   version of DPR reaches peak accuracy after 5 or   6 rounds when using 32 - dim weak learners ( see   § A.1 ) , leading to overall test - time index of 160/192-   dim . In terms of Exact Search , DrBoost outper-   forms the iteratively - sampled negatives DPR base-   line on MSMARCO by 2.2 % , and trails it by only   0.3 % on NQ R@100 , despite having a total dimen-   sion 4–5x smaller . It also strongly outperforms   a dimensionally - matched DPR , by 3 % on MS-   MARCO , and 1 % on NQ in R@100 , demonstrating   DrBoost ’s ability to learn high - quality , compact em-   beddings . We also quote recent state - of - the - art re-   sults , which generally achieve stronger exact search   results ( AR2 , Zhang et al . , 2021 ) . Our empha-   sis , however , is on comparing iteratively - sampled   negatives to boosting , and we note that state - of-   the - art approaches generally use larger models and   more complex training strategies than the “ inner   loop ” BERT - base DPR we report here . Such strate-   gies could also be incorporated into DrBoost if   higher accuracy was desired , as DrBoost is largely-   agnostic to the training algorithm used .   4.2 Approximate MIPS   Table 1 also shows how DPR and DrBoost behave   under IVF MIPS search , which is shown graphi-   cally in Figure 1 as well . We find that DrBoost   dramatically outperforms DPR in IVF search , in-   dicating that much faster search is possible with   DrBoost . High - dimensional embeddings suffer un-   der IVF due to the the curse of dimensionality , and   thus compact embeddings are important . Using 8   search probes , DrBoost outperforms DPR by 10.5 %   on MSMARCO and 6.3 % on NQ in R@100 . The   dimensionally - matched DPR is stronger , but still   trails DrBoost by about 4 % using 8 probes . The   strongest exact search model is thus not necessarily   the best in practical approximate MIPS settings .   For example , if we can tolerate a 10 % relative   drop in accuracy from the best performing sys-   tem ’s exact search , DrBoost requires 16 ( 4 ) probes   for MSMARCO ( NQ ) to reach the required accu-   racy , whereas DPR will require 1024 ( 16 ) , meaning   DrBoost can be operated approximately 64x ( 4x )   faster .   The distilled DrBoost is also shown for NQ in   Table 1 . The precision ( low R@K values ) is essen-   tially unaffected , ( exact search drops by 0.1 % for   R@20 ) , but recall drops slightly ( -0.7 % R@100 ) .   Interestingly , the distilled DrBoost performs even   better under IVF search , improving over DrBoost   by∼1 % at low numbers of probes . Crucially ,   whilst the distilled DrBoost is only slightly better   than the 192 - dim DPR under exact search , it is 4 –   5 % stronger under IVF with 8 probes ( alternatively ,   8x faster for equivalent accuracy ) .   Aside from fast retrieval , small indices are also   important for edge devices , or for scalability rea-   sons . While DrBoost can already produce high   quality compact embeddings , Product Quantization3107   can reduce this even further . Table 2 shows that   DrBoost ’s NQ index can be compressed from 13.5   GB to 840 MB with less than 1 % drop in perfor-   mance . We compare to BPR ( Yamada et al . , 2021 ) ,   a method specifically designed to learn small in-   dices through binarization . DrBoost ’s PQ index   is 2.4x smaller than the BPR index reported by   Yamada et al . ( 2021 ) , whilst being 2.4 % more ac-   curate ( R@20 ) . A more aggressive quantization   leads to a 420 MB index — 4.8x smaller than BPR   — whilst only being 1.2 % less accurate .   5 Analysis   5.1 Qualitative Analysis   Since each round ’s model is learned on the errors   of the previous round , we expect each learner to   “ specialize ” and learn complementary representa-   tions . To see if this is qualitatively true , we look at   the retrieved passages from each round ’s retriever   in isolation ( Table 10 in § A.3 ) . Indeed , we find   that each 32 - dim sub - vector tackles the query from   different angles . For instance , for the query “ who   got the first nobel prize in physics ? ” , the first sub-   vector captures general topical similarity based on   keywords , retrieving passages related to the “ Nobel   Prize ” . The second focuses mostly on the first para-   graphs of articles of prominent historical person-   alities , presumably because these are highly likely   to contain answers in general ; and the third one   retrieves from the pages of famous scientists and   inventors . The combined DrBoost model would   favor passages in the intersection of these sets .   5.2 In - distribution generalization   Boosting algorithms are remarkably resistant to   over - fitting , even when the classifier has sufficient   capacity to achieve zero training error . In their   landmark paper , Bartlett et al . ( 1998 ) show that   this generalization property is a result of the fol-   lowing : the training margins increase with each   iteration of boosting . We empirically show the   same to be true for DrBoost . For a fixed query   embedding , dense retrieval acts as a linear binary   classifier , where the gold passage is positive and all   other passages are negatives ( Eq . ( 1 ) ) . We adopt   the classical definition of margin for linear classi-   fiers to dense retrieval by defining a top- kmargin :   Top - k margin = h(q , c)−maxh(q , c )   ||q||µ   ( 2)3108   where µis the average norm of passage embed-   dings and the operator maxreturns the k - th   maximum element in the set . For a fixed qand   k= 1 , this definition is identical to the classical   margin definition . Figure 2 plots the 50 , 75and   90percentiles of the top-20 margin for DrBoost   on the NQ training set . We clearly see that margins   indeed increase at each step , especially for cases   that the model is confident in ( high margin ) . We   hypothesize this property to be the main reason for   the strong in - distribution generalization of DrBoost   that we observed , and potentially also for the sur-   prisingly strong IVF results , since wide margins   should intuitively make clustering easier as well .   5.3 Cross - domain generalization   It has been observed in previous work ( Thakur   et al . , 2021 ) that dense retrievers still largely lag   behind sparse retrievers in terms of generalization   capabilities . We are interested to test whether our   method could be beneficial for out - of - domain trans-   fer as well . We show the results for zero - shot trans-   fer on a subset of the BEIR benchmark in Table 3   and the EntityQuestions dataset in Table 4 . While   DrBoost improves slightly over the dimension-   matched baseline on EntityQuestions , where the   passage corpora stays the same , it produces worse   results on the BEIR datasets . We conclude that   boosting is not especially useful for cross - domain   transfer , and should be combined with other meth-   ods if this is a concern .   5.4 Representation Probing   One of the hypothesis we formulate for the stronger   performance of DrBoost over DPR is that the for-   mer might better capture topical information of pas-   sages and questions . To test this , we collected top-   ics for all Wikipedia articles in Natural Questions   using the strategy of Johnson et al . ( 2021 ) and as-   sociate them with both passages and questions . We   then probed both DPR and DrBoost representations   with an SVM ( Steinwart and Christmann , 2008 )   classifier considering a 5 - fold cross - validation over   500 instances and 8 different seeds . Results ( in Fig-   ure 3 ) confirms our hypothesis : the topic classifier   accuracy is higher with DrBoost representations   with respect to DPR ones of the same dimension   ( i.e. , 192 ) , for both questions and passages .   5.5 Related Work   Boosting for retrieval Boosting has been studied   in machine learning for over three decades ( Kearns   and Valiant , 1989 ; Schapire , 1990 ) . Models   such as AdaBoost ( Freund and Schapire , 1997 )   and GBMs ( Friedman , 2001 ) became popular   approaches to classification problems , with im-   plementations such as XGBoost still popular to-   day ( Chen and Guestrin , 2016 ) . Many boosting   approaches have been proposed for retrieval and   learning - to - rank ( LTR ) problems , typically employ-   ing decision trees , such as AdaRank ( Xu and Li ,   2007 ) , RankBoost ( Freund et al . , 2003 ) and lam-   daMART ( Wu et al . , 2009 ) . Apart from speed   and accuracy , boosting is attractive due to promis-   ing theoretical properties such as convergence and   generalization . ( Bartlett et al . , 1998 ; Freund et al . ,   2003 ; Mohri et al . , 2012 ) . Boosted decision trees   have recently been demonstrated to be competitive   on LTR tasks ( Qin et al . , 2021 ) , but , in recent years ,   boosting approaches have generally received less   attention , as ( pretrained ) neural models began to   dominate much of the literature . However , modern   neural models and boosting techniques need not be   exclusive , and a small amount of work exploring3109boosting in the context of modern pre - trained neu-   ral models has been carried out ( Huang et al . , 2020 ;   Qin et al . , 2021 ) . Our work follows this line of   thinking , identifying dimensionally - constrained bi-   encoders as good candidates as neural weak learn-   ers , adopting a simple boosting approach which   allows for simple and efficient MIPS at test time .   Dense Retrieval Sparse , term - based Retrievers   such as BM25 ( Robertson and Zaragoza , 2009 )   have dominated retrieval until recently . Dense ,   MIPS - based Retrieval using bi - encoder architec-   tures leveraging contrastive training with gold   pairs ( Yih et al . , 2011 ) has recently shown to   be effective in several settings ( Lee et al . , 2019 ;   Karpukhin et al . , 2020 ; Reimers and Gurevych ,   2019 ; Hofst ¨atter et al . , 2021b ) . See Yates et al .   ( 2021 ) for a survey . The success of Dense Re-   trieval has led to many recent papers proposing   schemes to improve dense retriever training by in-   novating on how negatives are sampled ( Xiong   et al . , 2020 ; Qu et al . , 2021 ; Zhan et al . , 2021c ;   Lin et al . , 2021 , inter alia . ) , and/or proposing pre-   training objectives ( O ˘guz et al . , 2021 ; Guu et al . ,   2020 ; Chang et al . , 2020 ; Sachan et al . , 2021 ; Gao   and Callan , 2021 ) . Our work also innovates on   how dense retrievers are trained , but is arguably   orthogonal to most of these training innovations ,   since these could still be employed when training   each component weak learner .   Distillation We leverage a simple distillation   technique to make DrBoost more efficient at test   time . Distillation for dense retrievers is an active   area , and more complex schemes exist which could   improve results further ( Izacard and Grave , 2021 ;   Qu et al . , 2021 ; Yang and Seo , 2020 ; Lin et al . ,   2021 ; Hofst ¨atter et al . , 2021a ; Barkan et al . , 2020 ;   Gao et al . , 2020 ) .   Multi - vector Retrievers Several approaches rep-   resent passages with multiple vectors . Humeau   et al . ( 2020 ) represent queries with multiple vec-   tors , but retrieval is comparatively slow as rele-   vance can not be calculated with a single MIPS call .   ME - BERT ( Luan et al . , 2021 ) index a fixed number   of vectors for each passage and ColBERT ( Khattab   and Zaharia , 2020 ) index a vector for every word .   Both can perform retrieval with a single MIPS   call ( although ColBERT requires reranking ) but   produce very large indices , which , in turn , slows   down search . DrBoost can also be seen as a multi-   vector approach , with each weak learner produc - ing a vector . However , each vector is small , and   we index concatenated vectors , rather than index-   ing each vector independently , leading to small in-   dices and fast search . This said , adapting DrBoost-   style training to these settings would be feasible .   SPAR ( Chen et al . , 2021 ) is a two - vector method :   one from a standard dense retriever , and the other   from a more lexically - oriented model . SPAR uses   a similar test - time MIPS retrieval strategy to ours ,   and SPAR ’s lexical embeddings could be trivially   added to DrBoost as an additional subvector .   Efficient retrievers There have been several re-   cent efforts to build more efficient retrieval and   question answering systems ( Min et al . , 2021 ) .   Izacard et al . ( 2020 ) and Yang and Seo ( 2021 )   experiment with post - hoc compression and lower-   dimensional embeddings , Lewis et al . ( 2021 ) in-   dex and retrieve question - answer pairs and Yamada   et al . ( 2021 ) propose BPR , which approximates   MIPS using binary vectors . There is also a line of   work learning embeddings specifically suited for   approximate search ( Yu et al . , 2018 ; Zhan et al . ,   2021a , b ) Generative retrievers ( De Cao et al . , 2021 )   can also be very efficient . DrBoost also employs   lower - dimensional embeddings and off - the - shelf   post - hoc compression for its smallest index .   6 Discussion   In this work we have explored boosting in the con-   text of dense retrieval , inspired by the similarity   of iteratively - sampling negatives to boosting . We   find that our simple boosting approach , DrBoost ,   performs largely on par with a 768 - dimensional   DPR baseline , but produces more compact vectors ,   and is more amenable to approximate search . We   note that DrBoost requires maintaining more neu-   ral models at test time , which may put a greater   demand on GPU resources . However , the mod-   els can be run in parallel if latency is a concern ,   and if needed , these models can be distilled into   a single model with little drop in accuracy . We   hope that future work will build on boosting ap-   proaches for dense retrieval , including adding adap-   tive weights , and investigating alternative losses   and sampling techniques . We also suggest that em-   phasis in dense retrieval should be placed on more   holistic evaluation than just exact retrieval accu-   racy , demonstrating that models with quite similar   exact retrieval can perform very differently under   practically - important approximate search settings.3110References311131123113   A Appendix   A.1 Number of Rounds   The performance of DPR and DrBoost on MS-   MARCO for different numbers of rounds are shown   in Table 5 . We find that all models saturate at about   4 or 5 rounds . Note DrBoost does not need more it-   erations to train , even though it does not use BM25   negatives for the first round . On NQ , adding a 6   model slightly improves DrBoost ’s precision , at the   expense of recall ( see Table 1 ) .   While iterative training is expensive , we find that   subsequent rounds are much cheaper than the first   round , with the first round taking ∼20 K steps in our   experiments to converge , with additional DrBoost   rounds converging after about 3 K steps .   Bagging Dense Retrieval We also trained a sim-   ple ensemble of six 32 - dim DPR models for NQ ,   which we compare to our 6 ×32 - dim component   DrBoost . This experiment investigates whether the   improvement over DPR is just a simple ensembling   effect , or whether it is due to boosting effects and   specialization of concerns . This DPR ensemble   performs poorly , scoring 74.5 R@20 ( not shown in   tables ) , 6.8 % below the equivalent DrBoost , con-   firming that the boosting formulation is important ,   not simply having several ensembled dense retriev-   ers .   A.2 Implementation Details   We implement our models architectures based on   HuggingFace ’s Transformers ( Wolf et al . , 2020)3114and run our experiments on 16 V100 GPUs . For all   training rounds , we used the same set of training   hyperparameters — we set learning rate as 3e-5 ,   dropout as 0.1 , weight decay as 0.01 , batch size as 2   ( per GPU ) and max training steps as 30k . The max-   imum question and passage lengths are set as 40   and 200 respectively and we accompany each ques-   tion with 50 passages during training . Using our   training infrastructure , the first round of the train-   ing takes about 8 hours and each additional training   round takes about 1.5 hours until convergence . We   always use the dev loss for model selection .   A.3 Detailed results311531163117