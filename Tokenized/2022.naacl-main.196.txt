  Rahul Sharma   rahul.sharma@usc.eduAnil Ramakrishna   aniramak@amazon.comAnsel MacLaughlin   ammaclau@amazon.com   Anna Rumshisky   arum@cs.uml.eduJimit Majmudar   mjimit@amazon.comClement Chung   chungcle@amazon.com   Salman Avestimehr   avestime@usc.eduRahul Gupta   gupra@amazon.com   Abstract   Machine Learning ( ML ) systems are getting   increasingly popular , and drive more and more   applications and services in our daily life . This   has led to growing concerns over user privacy ,   since human interaction data typically needs   to be transmitted to the cloud in order to train   and improve such systems . Federated learn-   ing ( FL ) has recently emerged as a method for   training ML models on edge devices using sen-   sitive user data and is seen as a way to mitigate   concerns over data privacy . However , since ML   models are most commonly trained with label   supervision , we need a way to extract labels   on edge to make FL viable . In this work , we   propose a strategy for training FL models us-   ing positive and negative user feedback . We   also design a novel framework to study differ-   ent noise patterns in user feedback , and explore   how well standard noise - robust objectives can   help mitigate this noise when training models in   a federated setting . We evaluate our proposed   training setup through detailed experiments on   two text classification datasets and analyze the   effects of varying levels of user reliability and   feedback noise on model performance . We   show that our method improves substantially   over a self - training baseline , achieving perfor-   mance closer to models trained with full super-   vision .   1 Introduction   Artificial Intelligence ( AI ) and Machine Learning   ( ML ) have become increasingly common in mod-   ern society with applications ranging from simple   standalone products to complex modules Kaplan   and Haenlein ( 2019 ) . However , this rise has also   created growing privacy concerns Papernot et al .   ( 2016 ) ; Yeom et al . ( 2018 ) . Such concerns may   affect user willingness the adapt new technologies   Guhr et al . ( 2020 ) . In response , many government   agencies across the world have introduced regu-   lations to protect the data - handling rights of theircitizens , such as the European Union ’s GDPR Gen-   eral Data Protection Regulation and California ’s   CCPA California Consumer Privacy Act .   Federated Learning ( FL ) is a step in this direc-   tion to improve consumer trust , where models are   trained without moving data out of client devices .   The typical FL approach is to iteratively train local   models on edge devices and then propagate them   back to a central node in order to update the global   model . Most commonly , this is done using Feder-   ated Averaging ( FedAvg ) McMahan et al . ( 2017 ) ,   where we take a simple average over the client   model parameters . However , in order to update   local models on the edge , this setup assumes the   presence of labeled user data on each device , which   is often not possible . Most prior works do not ad-   dress this problem , but simulate fully - supervised   federated learning by distributing existing labeled   datasets across edge devices . In this work , we con-   sider a more realistic scenario , where labels are   not available on device . Rather than turning to un-   supervised learning as seen in Hard et al . ( 2018 ) ,   we instead propose a novel setup to leverage user   feedback in order to train the FL model .   In many real world AI applications with direct or   indirect human interaction , it is possible to collect   either explicit user feedback ( e.g. , using a voice   or a screen - based prompt ) or implicit feedback in   the form of behavioral cues . For an example of   implicit feedback , consider a user interacting with   a virtual AI assistant ( such as Alexa ) , who asks to   play the song ‘ Bohemian Rhapsody ’ from the band   Queen . The virtual assistant would interpret the   prompt and select a song from its library to play .   If the user lets the music play without interruption ,   this can be viewed as positive feedback , suggesting   that the underlying model interpreted the request   correctly . On the other hand , if the user interrupts   the music and makes a repeat ( or different ) request ,   this can be viewed as negative feedback , suggesting   that the underlying model prediction was incorrect.2726In this work , we propose to leverage such feedback   in federated model training .   Leveraging Positive and Negative Feedback In   our proposed setup , we first train a seed model on   a small amount of labeled data on a central node .   This mimics the real - world scenario where a small   amount of data can be collected and annotated to   bootstrap an initial model . We then propagate this   weak seed model to each of the clients . On the   edge , we use this seed model to make predictions   for each user ’s request . Since the model is trained   with limited data , these predictions may be incor-   rect . To further improve this model performance ,   we leverage user feedback as an indirect indicator   of the predicted label ’s quality . Since positive user   feedback likely indicates that a model prediction is   correct , we label examples with positive feedback   with the seed model ’s prediction and add them to   the training data . This mirrors the standard self-   training setup Rosenberg et al . ( 2005 ) , where weak   models are further trained on a set of their own   predictions . When a user gives negative feedback ,   however , we can not assign a label to the example ,   since we only know that the seed model ’s predic-   tion is wrong . We instead treat these prediction as   complementary labels Ishida et al . ( 2017 ) ; Yu et al .   ( 2018 ) and extend federated model training to use   such labels .   Modeling Feedback Noise In realistic human   interactions , however , the user may not always pro-   vide consistent feedback , making user feedback   signal noisy . In the virtual AI assistant example   above , if the model predicts a different song from   the same band , the user may choose to continue   listening without interruption . Similarly , even if   the model predicts the correct song , the user may   change their mind once the song starts playing and   interrupt with a new request . Such user behav-   ior will introduce noise into the feedback signal .   In order to assess typical levels of such noise in   user feedback , we conduct a pilot study on Ama-   zon Mechanical Turk ( Mturk ) , and evaluate the   accuracy of feedback from Mturk users on two dif-   ferent text classification problems . Based on this   study , we define a model of user noise defined by   two parameters that specify how likely they are   to give accurate feedback on both correct and in-   correct predictions by the seed model . With this   model of user behaviour , we then study the effects   of user noise on model performance . We conductextensive experiments on two text classification   datasets , training FL models on feedback data with   varying amounts of user noise simulated using our   model . We further experiment with various noise-   robustness strategies to mitigate the effect of such   noisy labels and present promising results .   The key contributions in this paper are as fol-   lows :   1.We propose a new framework for model-   ing and leveraging user feedback in FL and   present a strategy to train supervised FL mod-   els directly on positive and negative user feed-   back . We show that , under mild to mod-   erate noise conditions , incorporating feed-   back improves model performance over self-   supervised baselines .   2.We propose a novel model of user feedback   noise and study the effects of varying levels   of this noise on model performance .   3.We study the effectiveness of existing noise-   robustness techniques to mitigate the effects   of user - feedback noise and identify promising   directions for future exploration .   2 Related Work   2.1 Federated Learning   Federated Learning McMahan et al . ( 2017 ) has   recently seen a rise in popularity in a number   of domains , including natural language process-   ing ( NLP ) Yang et al . ( 2018 ) ; Ramaswamy et al .   ( 2019 ) ; Hard et al . ( 2018 ) . This is due to growing   privacy concerns Papernot et al . ( 2016 ) ; Geyer et al .   ( 2017 ) ; Truex et al . ( 2019 ) , abundance of unlabeled   data , and an increase in the computational capacity   on edge devices . However , availability of labels on   edge ( or rather , lack thereof ) limits the practical   application of FL in most real - world use cases . In   this work , we present an extension to FL and show   improvements in federated model performance by   leveraging user feedback . Recent works have also   revealed risks of information leakage from gradi-   ents in federated learning , and several techniques   have been developed to mitigate this risk ( see Zhu   et al . ( 2019 ) , Lyu et al . ( 2020 ) and the references   there in ) .   2.2 Learning With User Feedback   User feedback on model behavior provides learning   signals which can be leveraged to continuously im-   prove model performance . Using feedback signals2727for model training provides robustness to concept   and data drifts , as new data is always accompa-   nied with new feedback labels from which to learn .   Learning methods that leverage user feedback have   been applied to a variety of tasks in NLP , such   as semantic parsing Iyer et al . ( 2017 ) , machine   translation Kreutzer et al . ( 2018 ) and question an-   swering Kratzwald and Feuerriegel ( 2019 ) . To our   knowledge , however , our work is the first to build   a parametric model of user feedback noise and to   study how to train federated learning algorithms   with noisy user feedback .   2.3 Negative Label Learning   Standard supervised learning operates on data la-   beled with their true classes . Feedback data from   users , however , can also be negative , indicating   that the predicted class is wrong . Since the cor-   rect class of examples with such negative - feedback   is unknown , our proposed method must be able   to handle such ambiguity during training . In our   work , we label examples with negative feedback   with a complementary label corresponding to the   predicted class ( Ishida et al . , 2017 ) . Complemen-   tary labels simply specify the category that a given   example does not belong to . In our work , we fol-   low the setup of Yu et al . ( 2018 ) , who propose loss   functions to train neural models on biased comple-   mentary labels .   2.4 Noise - Robust Learning   When training models on labels derived from noisy   user feedback signals , it is important to use a strat-   egy to mitigate the effects of label noise on model   performance . One straightforward approach is to   use a noise - robust loss function , such as Reverse   Cross Entropy Wang et al . ( 2019 ) or Mean Abso-   lute Error Ghosh et al . ( 2017 ) . In our work we   follow the noise - robust learning setup of Ma et al .   ( 2020 ) , who present a training strategy that com-   bines two robust loss functions ( one active , one   passive ) to better handle label noise .   3 Modeling User Feedback   We propose a general framework for leveraging   user feedback in federated learning . We use text   classification as an exemplar task to evaluate this   framework , but the proposed setup can be easily   applied to other tasks . We use two benchmark text   classification datasets : the Stanford Sentiment Tree-   bank dataset ( sst2 ) and the 20 newsgroups dataset(20news ) . The sst2 dataset comprises of 11,855   phrases from movie reviews and the corresponding   binary sentiment labels . The 20 newsgroup dataset   20news consists of 18knews articles and headers ,   organized into 20 classes .   3.1 Pilot Study : Real World User Feedback   To understand the dynamics of user feedback noise ,   we conduct a pilot study using Amazon Mechani-   cal Turk ( Mturk ) . We use text classification on the   above two datasets , sst_2 and20news , as the task   of interest . For each dataset , we train a seed model   on1%of the training data , then run inference with   this model to generate pseudo - labels on the remain-   ing99 % of the training examples . We sample 2000   pseudo - labeled examples from this set , split them   into disjoint groups of 40 examples each , and show   them to 50and40different MTurk workers for   sst_2 and20news , respectively . For each example ,   the worker is shown the text of the example and the   corresponding predicted pseudo label . The work-   ers are then asked to specify whether the pseudo   label is accurate ( positive feedback ) or not ( neg-   ative feedback ) along with an option to mark ‘ I   Do n’t Know ’ in case they find it difficult to decide .   Further details about the specific instructions used   in our Mturk study can be found in Appendix C.   We use the ground truth gold labels in sst_2 and   20news to evaluate the quality of user feedback by   computing the proportions of times users gave pos-   itive feedback to correct pseudo labels and negative   feedback to incorrect ones . The observed average   error in feedback is 33.9%for 20news and 27.13 %   for sst2 . The higher observed error for 20news is   likely due to the fact that 20news is a 20 - way topic   classification problem with overlapping categories   such as ‘ autos ’ and ‘ motorcycles ’ . We further an-   alyze the collected data using the noisy feedback   model described next . Full data will be released   with the final draft of the paper .   3.2 Feedback Noise Modeling   Motivated by the observed noisy user behavior   above , we propose a parametric noise model us-   ing two user - specific Bernoulli random variables   parameterized by γandδ , as shown below .   P(positive feedback |correct prediction ) = γ   P(negative feedback |incorrect prediction ) = δ   γandδcapture the probability of the user accu-   rately providing positive and negative feedback,2728respectively . This scheme provides a powerful tool   to model user noises of various types - by varying   the values of these two parameters , we can simu-   late various user feedback behaviors . For instance ,   highly reliable users can be simulated by choosing   γ∼1andδ∼1while adversarial users can be   simulated by choosing γ∼0andδ∼0 . Similarly ,   users that provide consistently positive feedback   can be simulated by selecting γ∼1andδ∼0 ,   and vice versa .   For each worker in our MTurk study , we esti-   mate the noise parameters γandδusing the MLE   estimators described in Appendix B. We show the   distributions over the estimated noise parameters   for each worker in Figure 1 , which highlights sev-   eral characteristics of the user noise . We find that   parameters vary across users and across datasets .   Many workers have high values for both γandδ   ( upper right quadrant in the plot ) , especially for the   sst2dataset . In such cases , user noise is relatively   low . Some workers have γ∼1andδ∼0 , sug-   gesting that they provide positive feedback with   very high probability , regardless of the correctness   of the pseudo label . Similarly , we also observe   some points with higher δbutγclose to 0 , suggest-   ing that these workers provide negative feedback   with high probability . Since we only recruited reli-   able worker from Mturk ( 95%+ approval rate and   5000 + approved HITs , see Appendix C ) , we donot see any workers in the adversarial or extremely-   high noise scenarios ( lower - left quadrant in the   plot ) . Finally , we also observe that the workers in   thesst2dataset are more concentrated towards the   right top corner while , for the 20news dataset , they   are relatively spread out . This can be attributed to   the inherent difficulty of the two datasets – sst2is   an easier 2 - class sentiment classification dataset ,   while 20news is a more difficult news - classification   dataset with 20 ( sometimes overlapping ) classes .   4 Approach   4.1 Federated Self - Training   Given a training dataset D={x , y } , we divide   it into 3 parts : a training split D⊂D:|D|=   k|D| , k≪1 , used to train the seed model ; a vali-   dation split D⊂D:|D|=v|D| , v < 1and an   unlabeled split D = D−(D / uniontextD ) . We assume   that the examples in DandDhave gold labels   available for training , which mimics the real - world   situation where a small amount of data can be an-   notated to bootstrap the model training . We treat   Das the unlabeled dataset which is available on   edge . We initialize the seed model f(x)by train-   ing on Dusing standard cross entropy loss . After   convergence , this model , f(x ) , is deployed to the   edge devices to start federated training . In order to   simulate a real - world federated learning setup , we   distribute the examples from Damong Nedge   clients following a skewed distribution , described   in detail in § 5 . The dataset on each client nis la-   beled Dwhere n∈[1 , N ] . The seed model on de-   vicejthen makes predictions on its corresponding   client - specific dataset D. Since the edge model   does not have access to gold labels for training ,   there are only two potential sources of information   it can learn from . First , its own predictions , ρ ,   which we call pseudo labels :   ρ= arg max ( f(x ) ) : i∈D ( 1 )   Labeling an example x∈Dwithρis typically   referred to as self - training , a commonly - used semi-   supervised training setup . However , in our setup ,   there is also a second source of information : user   feedback to each ρ . We assume that users give bi-   nary ( positive or negative ) feedback to each ρ . We   can thus use this feedback to validate or reject each   ρ , generating label ρwhen the feedback is posi-   tive and ¯ρwhen the feedback is negative . Then ,   with these new user - feedback - labeled datasets on   each edge device , we can follow the standard FL2729training , further training a copy of the initial global   model on each edge device , then propagating each   local model back to the global server for aggrega-   tion . Our overall setup used for federated learning   with user feedback is shown in Figure 2 .   4.2 User Feedback Simulation   As discussed in § 1 , in the real world , users provide   feedback on predictions made by deployed mod-   els . However , large - scale collection of user feed-   back in a deployed FL application is an expensive   endeavor with no publicly - available datasets . In   this work , we instead devise a novel framework to   simulate realistic noisy user feedback on publicly-   available , human - annotated data , and defer the task   of real world deployment to future work . Specifi-   cally , when we distribute the unlabeled dataset D   across the Nclient devices , we also send along   the true gold label for each example x. For each   x∈D , we then simulate feedback by compar-   ing the model prediction ρto its underlying gold   label . These gold labels are only used to simu-   late user feedback – they are not used for edge   model training . Specifically , we create two new   pseudo - labeled datasets corresponding to positive   ( D ) and negative feedback ( D ) from each   device ’s dataset D. For a given sample x∈D ,   we assign it to the positive feedback set Dwith   probability γif the corresponding pseudo label ρis correct and 1−δifρis incorrect . Similarly , we   assign a sample to the negative feedback set D   with probability 1−γ , ifρis correct and δifρis   incorrect . A pseudo - code detailing our strategy to   simulate user feedback is shown in Algorithm 1 .   4.3 Federated Learning with Feedback   For examples with positive user feedback , since   we have user confirmation that the pseudo - label ρ   is correct , we directly incorporate ρinto model   training as if they were ground - truth . We use the   standard categorical cross entropy ( CCE ) loss func-   tion similar to the seed model :   loss=−/summationdisplayρlog(f(x ) ) ( 2 )   where f(x)represents the posterior probability   distribution for sample iandρis overloaded to   depict the one - hot representation of the pseudo   label for sample i. On the other hand , negative   feedback signifies that the sample does not belong   to the class ρ . Although the user feedback does   not indicate which class these samples ought to   be , we do acquire information for what the model   should not do . Thus we can assume that these are   complementary labels , denoted as ¯y = ρ . To   incorporate these in our model training , we adapt   the complementary learning methods introduced   by Yu et al . ( 2018 ) , in which the authors model the   complementary posterior probability distribution ,   P(¯Y = d|X)as a function of true class posterior   distribution , P(Y = c|X)and the transition proba-   bility matrix Q , where qis an entry in the matrix   Qwithcanddare class labels , following :   q=/braceleftigg   P(¯Y = d|Y = c)c̸=d   0 c = d   ( 3 )   P(¯Y = d|X ) = /summationdisplayP(¯Y = d|Y = c )   P(Y = c|X ) ( 4 )   We estimate the transition probability matrix Q   using the validation set Dand the initial seed   model f(x ) . To compute Q , the transition prob-   ability distribution for the class c , we average the   posterior probability of those samples with gold   label c , but are incorrectly predicted by the model .   Given this , we set q= 0 and renormalize the2730vector as shown in Equation 5 .   Q=1   K / summationdisplayf(x )   1−f(x):K=|D|(5 )   D⊂D : argmax ( f(x))̸=c;y = c   Using the estimated transition matrix , and the pos-   terior distribution for the true class , we estimate the   posterior distribution for the complementary class ,   following Equation 3 . We then use the pseudo la-   bels as complementary labels and train the model   with the objective function :   loss=−/summationdisplayρlog(Q.f(x ) ) ( 6 )   here , we overload ρto depict the one - hot represen-   tation of the pseudo label for sample i.   Our overall model is trained to jointly opti-   mize the loss functions from positive feedback   and negative feedback . Inspired by Kim and Kim   ( 2020 ) where the contribution of negative learning   is slowly increased during training , we use a sched-   uler to weigh the two loss functions , ensuring that   the positive label learning component has higher   weight in the early epochs , gradually increasing   the weight for negative label learning as training   proceeds . Specifically , at each client we optimize   the following objective :   loss= ( 1−α)∗loss+α∗loss ( 7 )   where , α= 1−p , tdenotes the current epoch   in the training process and p∈(0,1 ) , which was   selected using a held out validation set .   4.4 Noise - Robust Loss Functions   Though user feedback provides a valuable learning   signal to train our models on edge , it can be noisy .   As noted in § 3.2 , we expect two behaviors from   noisy users : if the user provides incorrect positive   feedback , we have incorrect true labels in D.   Similarly , if the user provides incorrect negative   feedback , we have samples in the Dwith in-   correct complementary labels . Since we use cross   entropy loss functions for training on both positive   and negatively labeled data points , our model is   prone to overfitting to noisy samples Zhang and   Sabuncu ( 2018 ) since they would have lower poste-   riors ( forcing the algorithm to put more emphasis   on them during training ) . This necessitates some   form of noise mitigation in our model training . To mitigate label noise , we use the approach in-   troduced by Ma et al . ( 2020 ) , where they propose   to add noise robustness to any given loss function   by normalizing it across all labels . Ma et al . ( 2020 )   further improve convergence by presenting a com-   bination loss function with active and passive loss   components , to maximize the posterior for the true   class and to minimize the posterior for complemen-   tary classes , respectively . In our experiments , we   use a combination of Normalized Cross Entropy   ( NCE ) Ma et al . ( 2020 ) and Reverse Cross Entropy   ( RCE ) Wang et al . ( 2019 ) as the active and passive   components , weighted in a ratio 1:2 selected using   our validation set .   loss = NCE + 2∗RCE ( 8)   The NCE and RCE functions are listed below .   NCE = −/summationtextq(k|x ) logp(k|x )   −/summationtext / summationtextq(y = j|x ) logp(k|x )   ( 9 )   RCE = −/summationdisplayp(k|x ) logq(k|x ) ) ( 10 )   where Kis number of label classes , q(k|x)denotes   the gold label distribution and p(k|x)denotes the   posterior probability distribution .   5 Experiments   5.1 Implementation Details   We use the publicly - provided train and test splits   for the sst2and20news datasets and further derive   a validation split consisting of 20 % ( v= 0.2 ) of   the train split ( D ) , with uniform class distribution .   We use another 1 % ( k= 0.01 ) ofDas seed model   set , D. We choose a small value for kto mimic a   real world scenario where a larger volume of data   may be un - annotated in an FL setup . The remain-   ing unlabeled dataset D(79 % of the ( D ) is fur-   ther divided among 15 mutually exclusive subsets   ( D , n∈[1,15 ] ) , which simulates the data for 15   edge clients . While creating the clients ’ partitions   we ensure that all clients have data with a uniform   class distribution which enables us to focus on our   model performance in an idealized case . We use   the DistilBERT Sanh et al . ( 2019 ) base model as   the classifier for our tasks and follow the standard   fine - tuning setup for text classification . To imple-   ment the federated learning pipeline we use the   publicly - available FedNLP Lin et al . ( 2021 ) bench-   mark and apply the FedAvg McMahan et al . ( 2017)2731Experimental settings 20 news sst2   Initial model ( D ) 59.14 77.37   Self training ( no feedback ) 60.79 77.26   Positive feedback ( noisy ) 62.10 79.79   All feedback ( noisy ) 65.01 85.17   Positive feedback ( noise robust ) 62.33 79.85   All feedback ( noise robust ) 65.13 85.39   Positive feedback ( noise free ) 70.44 83.80   All feedback ( noise free ) 75.13 88.58   Full supervision 82.12 89.12   algorithm to aggregate the client model updates at   the server end . We train the model on an 8 - GPU   ( Nvidia V100s ) machine for up to 50 rounds with   early stopping enabled . Within each round , we use   a batch size of 8 to train the client models for 5   epochs each .   5.2 Model Evaluation   We compare our model performance against three   baselines :   Initial model This is the seed model f(x )   trained on just D(1%of the training data ) .   Self - training We train this model using feder-   ated learning with pseudo labels , but do not utilize   the user feedback . Hence , at each client , we only   have the raw pseudo labels ρfor each x∈Dto   train on . We use this setup as the primary baseline   against which to compare the performance of our   models trained with user feedback .   Full supervision We train a model in a federated   setting using Dand the true gold labels at each   client . Although in a real - world setting , the clients   will not have access to the gold labels , we establish   this benchmark to set an upper bound .   We evaluate performance of our proposed strat-   egy of leveraging user feedback in two settings :   Positive feedback At each client , we train the   local version of the model using only the samples   inDand corresponding pseudo labels ρ , i.e.   only the samples which receive positive feedback .   Since this baseline is trained using regular cross   entropy , it provides a powerful yet computationally   less - intensive alternative to training with both types   of feedback , which is especially important in edgedevices with low compute power .   All feedback We utilize all the data samples in   DandDat each client and train using the   loss function described in Section 4.3 .   In both the positive andall feedback setups , we   evaluate models with and without user feedback   noise . For the noise - free scenario , we set γ= 1   andδ= 1while simulating the user feedback . This   leads to perfectly accurate feedback , as discussed   in § 4.2 . For the noisy feedback scenario , we use   the noise parameters derived from the Mturk study .   We obtain the following dataset - specific values of γ   andδby averaging the estimates of γandδacross   all annotators : ( γ= 0.79,δ= 0.55 ) for 20news   and ( γ= 0.76,δ= 0.70 ) for sst2 .   Table 1 reports the % accuracy for each of the   experimental setups described above across both   datasets . We observe that in both the noisy and   noise - free settings , the introduction of positive user   feedback shows a marked improvement in perfor-   mance when compared to the self - training base-   line . There is an additional performance gain when   we add negative feedback ( all feedback baseline ) ,   which signifies the importance of learning from   complementary labels . As expected , the improve-   ment is substantially larger in the noise free setting ,   suggesting the need for model robustness to miti-   gate the effect of noise . Note that for sst2 , perfor-   mance of the noise free model with all feedback is   very close to that of full supervision , thanks to the   fact that complementary labels in the case of binary   classification provide same information as true la-   bels . On the other hand , using perfect positive and   negative feedback in 20news is still sub - optimal   compared to full supervision , since a negative label   in this dataset is less informative compared to sst2 .   5.3 Noise Robustness   To mitigate the effects of noise , we replace the tra-   ditional cross - entropy loss function with the active-   passive loss described in § 4.4 , using the same ex-   perimental setups presented earlier ( positive only   and all - feedback ) , with γandδvalues from the   Mturk study . However , as evident in Table 1 , the   robust loss functions only seem to confer marginal   performance improvements in both datasets . This   is likely due to the fact that the noise parameters   extracted from Mturk belong to a moderate to low   noise regime ( Section 3.2 ) , providing limited room   for gains with noise robustness .   To further investigate this , we explore two ex-2732Noise level Loss Accuracy   Lowloss 73.29   loss 74.30   Adversarialloss 42.26 *   loss 25.19   treme cases of user feedback noise for the 20news   dataset : i ) low noise , where we simulate user feed-   back with γ→1,δ→1for all the clients , which   imitates clients providing correct feedback with   very high probability , and ii ) adversarial noise , with   γ→0,δ→0for all the clients , which captures   the possible risk of users deliberately providing in-   correct feedback with high probability . In Table 2 ,   we compare the performances of the all feedback   model trained with and without noise robustness   in these two scenarios . As seen in the table , when   user noise is high , the noise - robust loss functions   show a statistically significant improvement against   the noisy model , highlighting the value of adding   noise robustness . In the low noise regime , adding   robustness seems to cause negligible degradation in   accuracy , but within the bounds of statistical error .   Given this , we recommend using noise robustness   in all applications of this framework unless it is   well known before hand that the feedback has very   low noise . We defer the task of developing a noise   robustness regime that works for all noise levels to   future work .   5.4 Ablation Studies   5.4.1 Varying Degrees of Noise   As discussed in § 4.4 , the level of feedback noise   has a substantial impact on model performance .   In this section , we further investigate this effect ,   simulating user feedback across various noise pa-   rameters values , spanning γ , δ∈ { 0.3,0.5,0.7 } , to   capture different points in the γ−δspace . Table 3   shows our results for each dataset with the noise   robust loss function 8 . As expected , as γ→0   and/or δ→0 , model performance decreases on   both datasets . At very low values of δandγ , e.g.   both≤0.5 , training on the extremely noisy user   feedback actually decreases model performance   below the original seed model . This is not unex-   pected , since at δ= 0.5andγ= 0.5 , user feedback   is essentially random noise , and at lower values theγ / δ 0.7 0.5 0.3   0.7 66.69 63.18 60.66   0.5 65.56 59.15 59.73   0.3 60.01 58.94 58.21   γ / δ 0.7 0.5 0.3   0.7 83.86 80.89 76.17   0.5 81.99 77.38 75.07   0.3 78.03 74.41 71.99   feedback is adversarial . These results highlight the   importance of evaluating the reliability of user feed-   back before using it to further train an ML system .   5.4.2 Non - identical User Feedback Behavior   In previous sections , we used identical values of   the noise parameters γandδfor all clients in the FL   training setup . However , as observed in our Mturk   study , real users have different feedback behaviors ,   with scores spread out over the γ−δspace . In this   section , we simulate non - identical user feedback   for four potential user behaviors :   1 . Low noise users ( γ→1 , δ→1 )   2 . Adversarial / high noise users ( γ→0 , δ→0 )   3.Positive users ( γ→1 , δ→0 ) who provide   consistently positive feedback , regardless of   the correctness of the model prediction ; and   4.Negative users ( γ→0 , δ→1 ) who provide   consistently negative feedback .   To simulate non - identical user feedback for the   clients , we sample the noise parameters from an   appropriately skewed β(a , b)distribution . For ex-   ample , in order to generate δandγscores for setup   four ( negative users ) , each user needs γ≈0,δ≈1 .   To generate these parameters , we sample γfrom   β(1,10)andδfromβ(10,1 ) . Proceeding this way ,   we can simulate all four user behaviors listed above .   Finally , we also conduct an experiment closer to the   real - world scenario , where we randomly sample   15 workers each for both datasets from our Mturk   study and use their estimated values of γandδto   simulate user feedback.2733User Behavior 20news sst2   Low noise 73.67 88.35   Adversarial 55.86 64.85   Always positive 60.99 77.16   Always negative 58.92 74.13   Real world ( mturk study ) 65.37 85.61   Table 4 shows our results across all five simula-   tions for both datasets when trained with the noise   robust loss function 8 . As expected , the best model   performance is achieved with the low - noise users ,   followed by the real - world users sampled from our   MTurk study . In the three other simulations ( adver-   sarial , consistently positive , consistently negative ) ,   user feedback is highly noisy and unreliable , and   the models show limited improvement over the ini-   tial seed model . Note that the performance in the   positive feedback scenario is higher than negative   feedback , which can be accredited to the fact that   the initial seed model ’s accuracy is greater 50 % for   both datasets ( Table 1 ) . With > 50 % accuracy , a   majority of the pseudo - labels generated using the   seed model will match the gold label . Hence , con-   sistently positive feedback introduces less noise   and in turn better performance compared to the all   negative feedback model .   6 Conclusion   In this work , we propose a novel framework for   federated learning which leverages noisy user feed-   back on the edge . Our framework eliminates the   need for labeling edge data , in turn improving cus-   tomer privacy since we no longer need to move   data off of edge devices for annotation . In order   to evaluate our framework , we propose a method   to simulate user feedback on publicly - available   datasets and a parametric model to simulate user   noise in that feedback . Using that method , we con-   duct experiments on two benchmark text classifica-   tion datasets and show that models trained with our   framework significantly improve over self - training   baselines .   Future work includes deploying our framework   in a real world FL application and incorporating   live user feedback in model training . We can also   explore other noise - robustness strategies for low   and medium label - noise scenarios . One such strat-   egy would be to incorporate a measure of user relia-   bility into the calculation of each new global modelin FedA VG – e.g. the updated global model pa-   rameters could be computed as a weighted average   of client models , with the weight capturing some   measure of each client ’s reliability .   7 Ethics Statement   Our Mturk data collection recruited annotators   from across the globe without any constraints on   user demographics . The annotators were compen-   sated with above minimum wages and no personal   information was collected from the annotators .   References2734   A Pseudocode   Algorithm 1 lists our training loop .   B Estimators for γand δ   LetXbe the data set and nbe the total number   of data points . For any data point i∈[n ] , letp ,   tandfdenote the model predicted label , ground   truth label , and user feedback respectively . ( Note   thatpandttake values from the set of labels   andftakes values from the set { pos , neg , idk }   representing feedbacks positive , negative , and ’ I   do n’t know ’ . ) By definition , we have   γ:= Pr ( f = pos|p = t )   δ:= Pr ( f = neg|p̸=t )   Let us also define   α:= Pr ( f = neg|p = t )   β:= Pr ( f = pos|p̸=t)2735Algorithm 1 Algorithm for simulating user feedback   INPUT : Client data : D={x , y } ; Pseudo labels : ρ   OUTPUT : DandDD← { } , D← { } forsample i in Ddo ify==ρthen ▷correct model prediction D← { D∪i}with probability γ   or D← { D∪i}with probability 1−γ ▷ noise else if y ! = ρthen ▷incorrect model prediction D← { D∪i}with probability δ   or D← { D∪i}with probability 1−δ ▷ noise end ifend forreturn DandD   Note that the above definitions imply that   1−α−γ= Pr ( f = idk|p = t )   1−β−δ= Pr ( f = idk|p̸=t )   Moreover , let adenote the accuracy of the labels   predicted by the model defined as   a:= Pr ( p = t )   Define sets { S}such that   S:={i∈[n ] : f = posandp = t }   S:={i∈[n ] : f = posandp̸=t }   S:={i∈[n ] : f = negandp = t }   S:={i∈[n ] : f = negandp̸=t }   S:={i∈[n ] : f = idkandp = t }   S:={i∈[n ] : f = idkandp̸=t }   define n:=|S| . Note that / summationtextn = n.   Theorem 1 . The maximum likelihood estimators   forγandδaren/(n+n+n)andn/(n+   n+n)respectively .   Proof . Now for any data point i∈[n ] , we have   Pr(i∈S ) = Pr ( f = posandp = t )   = Pr ( f = pos|p = t)·Pr(p = t )   = γa . By a similar reasoning , we have   Pr(i∈S ) = β(1−a )   Pr(i∈S ) = αa   Pr(i∈S ) = δ(1−a )   Pr(i∈S ) = ( 1 −α−γ)a   Pr(i∈S ) = ( 1 −β−δ)(1−a )   Therefore the likelihood function of the model   is   L(α , β , γ , δ |X ) = n !   n ! . . . n!(γa )   ( β(1−a))(αa )   ( δ(1−a))((1−α−γ)a )   ( ( 1−β−δ)(1−a ) )   and consequently the log - likelihood function is   logL(α , β , γ , δ |X ) = log / parenleftbiggn !   n ! . . . n!/parenrightbigg   +   nlog(γa ) + nlog(β(1−a))+   nlog(αa ) + nlog(δ(1−a))+   nlog((1 −α−γ)a)+   nlog((1 −β−δ)(1−a ) )   ( 11 )   To obtain MLE estimates of parameters   α , β , γ , δ , we wish to solve the following optimiza-   tion problem   maxlogL(α , β , γ , δ |X ) ( 12)2736By Fermat ’s theorem , the optimal solution to the   above optimization problem lies at either a bound-   ary point or a stationary point .   The boundary points of the set [ 0,1]are given   by the set   B:={(α , β , γ , δ ) ∈[0,1]:α= 0orα= 1or   β= 0orβ= 1or   γ= 0orγ= 1or   δ= 0orδ= 1 }   The value of the function logL(α , β , γ , δ ) is nega-   tively unbounded on the set B.   On the other hand , the stationary points can be   determined by setting the gradient to be zero , i.e. ,   by solving the equation   ∇logL(α , β , γ , δ |X ) = 0 .   Solving the above equation yields the stationary   point ( α , β , γ , δ)given as   α = n/(n+n+n )   β = n/(n+n+n )   γ = n/(n+n+n )   δ = n/(n+n+n )   The value of the log - likelihood function at the   above critical point , i.e. , logL(α , β , γ , δ|X )   is positive which suggests that it is the optimizer of   the optimization problem in ( 12 ) .   C Details on MTurk study   Figure 3 shows the instruction page used to guide   the workers on Mturk . Since our goal here was   to simulate real user feedback for AI systems , we   designed the prompt to mimic a situation where   users provide their judgements on the accuracy of   machine predictions on a given task . Each assign-   ment page had 40 questions for the 20news task   ( 50 for sst2 ) , with an example question shown in   Figure 4 . For a real world application of this set-   ting , we can imagine an email categorization model   deployed on end - user email clients which automat-   ically classifies incoming emails to a predefined   class . The user would approve ( select Accurate to   above question ) if the categorization was correct ,   reject or make correction ( select Inaccurate to the   question ) or take no action . This closely follows   the federated user feedback scenario described in   our experiments with users explicitly providing   positive or negative feedback . We recruited highly reliable annotators on Mturk   by selecting past approval rate as 95%+ and num-   ber of past approved tasks as 5000 + . The average   time for each task was 30 minutes , and the annota-   tors were paid $ 7for completing the task which is   above the US federal minimum hourly wage , given   the average time for task completion . Note that we   did not place any geographic restrictions on the an-   notators , nor reject any partial submissions , despite   stating as such in the instruction sheet , as they were   few in number .   In Figure 5 , we show the error in user feedback   computed against gold labels for all the users . We   also show the distribution of positive and negative   responses for all the users . As evident from the   figure , users provide positive feedback in majority   cases . This behavior is expected since the initial   model ’s accuracy for 20news is59.14 % and for   sst2is77.37 % ; since a the majority of the pseudo   labels are correct predictions , we expect mostly   positive feedback from the users.273727382739