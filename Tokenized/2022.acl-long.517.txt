  Katherine Thai Yapei Chang Kalpesh Krishna Mohit IyyerUniversity of Massachusetts Amherst , Smith College   { kbthai,kalpesh,miyyer}@cs.umass.edu   echang33@smith.edu   Project Page : https://relic.cs.umass.edu   Abstract   Humanities scholars commonly provide evi-   dence for claims that they make about a work   of literature ( e.g. , a novel ) in the form of quo-   tations from the work . We collect a large - scale   dataset ( RELiC ) of 78 K literary quotations and   surrounding critical analysis and use it to for-   mulate the novel task of literary evidence re-   trieval , in which models are given an excerpt   of literary analysis surrounding a masked quo-   tation and asked to retrieve the quoted passage   from the set of all passages in the work . Solv-   ing this retrieval task requires a deep under-   standing of complex literary and linguistic phe-   nomena , which proves challenging to meth-   ods that overwhelmingly rely on lexical and   semantic similarity matching . We implement   a RoBERTa - based dense passage retriever for   this task that outperforms existing pretrained   information retrieval baselines ; however , ex-   periments and analysis by human domain ex-   perts indicate that there is substantial room for   improvement over our dense retriever .   1 Introduction   When analyzing a literary work ( e.g. , a novel or   short story ) , scholars make claims about the text   and provide supporting evidence in the form of quo-   tations from the work ( Thompson , 2002 ; Finnegan ,   2011 ; Graff et al . , 2014 ) . For example , Monaghan   ( 1980 ) claims that Elizabeth , the main character in   Jane Austen ’s Pride and Prejudice , does n’t just   refuse an offer to join the standofﬁsh bachelor   Darcy and the wealthy Bingleys on their morning   walk , “ but does so in such a way as to group Darcy   with the snobbish Bingley sisters , ” and then di-   rectly quotes Elizabeth ’s tongue - in - cheek rejection :   “ No , no ; stay where you are . You are charmingly   grouped , and appear to uncommon advantage . The   picturesque would be spoilt by admitting a fourth . ”   Literary scholars construct arguments like these   by making complex connective inferences between   their interpretations , framed as claims , and quota - tions ( e.g. , recognizing that Elizabeth says “ charm-   ingly grouped ” and “ picturesque ” ironically in or-   der to group Darcy with the snobbish Bingley sis-   ters ) . This process requires a deep understand-   ing of both literary phenomena , such as irony and   metaphor , and linguistic phenomena ( coreference ,   paraphrasing , and stylistics ) . In this paper , we com-   putationally study the relationship between liter-   ary claims and quotations by collecting a large-   scale dataset for Retrieving Evidence for Literary   Claims ( RELiC ) , which contains 78 K scholarly ex-   cerpts of literary analysis that each directly quote a   passage from one of 79 widely - read English texts .   The complexity of the claims and quotations in   RELiC makes it a challenging testbed for modern   neural retrievers : given just the text of the claim   and analysis that surrounds a masked quotation , can   a model retrieve the quoted passage from the set of   all possible passages in the literary work ? This lit-   erary evidence retrieval task ( see Figure 1 ) differs   considerably from retrieval problems commonly   studied in NLP , such as those used for fact check-   ing ( Thorne et al . , 2018 ) , open - domain QA ( Chen   et al . , 2017 ; Chen and Yih , 2020 ) , and text gener-   ation ( Krishna et al . , 2021 ) , in the relative lack of   lexical or even semantic similarity between claims   and queries . Instead of latching onto surface - level   cues , our task requires models to understand com-   plex devices in literary writing and apply general   theories of interpretation . RELiC is also challeng-   ing because of the large number of retrieval candi-   dates : for War and Peace , the longest literary work   in the dataset , models must choose from one of   32 K candidate passages .   How well do state - of - the - art retrievers perform   on RELiC ? Inspired by recent research on dense   passage retrieval ( Guu et al . , 2020 ; Karpukhin et al . ,   2020 ) , we build a neural model ( dense - RELiC ) by   embedding both scholarly claims and candidate   literary quotations with pretrained RoBERTa net-   works ( Liu et al . , 2019 ) , which are then ﬁne - tuned7500   using a contrastive objective that encourages the   representation for the ground - truth quotation to lie   nearby to that of the claim . Both sparse retrieval   methods such as BM25 and pretrained dense re-   trievers such as DPR and REALM perform poorly   on RELiC , which underscores the difference be-   tween our dataset and existing information retrieval   benchmarks ( Thakur et al . , 2021 ) on which these   baselines are much more competitive . Our dense-   RELiC model fares better than these baselines but   still lags far behind human performance , and an   analysis of its errors suggests that it struggles to   understand complex literary phenomena .   Finally , we qualitatively explore whether our   dense - RELiC model can be used to support   evidence - gathering efforts by researchers in the   humanities . Inspired by prompt - based query-   ing ( Jiang et al . , 2020 ) , we issue our own out - of-   distribution queries to the model by formulating   simple descriptions of events or devices of inter-   est ( e.g. , symbols of Gatsby ’s lavish lifestyle ) and   discover that it often returns relevant quotations .   To facilitate future research in this direction , we   publicly release our dataset and models .   2 Collecting a Dataset for Literary   Evidence Retrieval   We collect a dataset for the task of Retrieving   Evidence for Literary Claims , or RELiC , the ﬁrst   large - scale retrieval dataset that focuses on the chal-   lenging literary domain . Each example in RELiC   consists of two parts : ( 1 ) the context surround - ing the quoted material , which consists of literary   claims and analysis , and ( 2 ) a quotation from a   widely - read English work of literature . This section   describes our data collection and preprocessing , as   well as a ﬁne - grained analysis of 200 examples   from RELiC to shed light on the types of quota-   tions it contains . See Table 1 for corpus statistics .   2.1 Collecting and Preprocessing RELiC   Selecting works of literature : We collect 79 pri-   mary source works written or translated into En-   glishfrom Project Gutenberg and Project Guten-   berg Australia . These public domain sources were   selected because of their popularity and status as   members of the Western literary canon , which also   yield more scholarship ( Porter , 2018 ) . All primary   sources were published in America or Europe be-   tween 1811 and 1949 . 77 of the 79 are ﬁctional nov-   els or novellas , one is a collection of short stories   ( The Garden Party and Other Stories by Katherine   Mansﬁeld ) , and one is a collection of essays ( The   Souls of Black Folk by W. E. B. Du Bois ) .   Collecting quotations from literary analysis :   We queried all documents in the HathiTrust Digi-   tal Library , a collaborative repository of volumes   from academic and research libraries , for exact   matches of all sentences of ten or more tokens from   each of the 79 works . The overwhelming majority7501   of HathiTrust documents are scholarly in nature ,   so most of these matches yielded critical analy-   sis of the 79 primary source works . We received   permission from the HathiTrust to publicly release   short windows of text surrounding each matching   quotation .   Filtering and preprocessing : The scholarly ar-   ticles we collected from our HathiTrust queries   were ﬁltered to exclude duplicates and non - English   sources . We then preprocessed the resulting text   to remove pervasive artifacts such as in - line cita-   tions , headers , footers , page numbers , and word   breaks using a pattern - matching approach ( details   in Appendix A ) . Finally , we applied sentence tok-   enization using spaCy ’s dependency parser - based   sentence segmenterto standardize the size of the   windows in our dataset . Each window in RELiC   contains the identiﬁed quotation and four sentences   of claims and analysison each side of the quota-   tion ( see Table 2 for examples ) . To avoid asking   models to retrieve a quote they have already seen   during training , we create training , validation , and   test splits such that primary sources in each fold   are mutually exclusive . Statistics of our dataset   sources are provided in Appendix A.3 .   2.2 Comparison to other retrieval datasets   Table 1 contains detailed statistics of RELiC. To   the best of our knowledge , RELiC is the ﬁrst re-   trieval dataset in the literary domain , and the onlyone that requires understanding complex phenom-   ena like irony and metaphor . We provide a detailed   comparison of RELiC to other retrieval datasets   in the recently - proposed BEIR retrieval bench-   mark ( Thakur et al . , 2021 ) in Appendix Table A6 .   RELiC has a much longer query length ( 157.7 to-   kens on average ) than all BEIR datasets except Ar-   guAna ( Wachsmuth et al . , 2018 ) . Furthermore , our   results in Section 3.3 show that while these longer   queries confuse pretrained retriever models ( which   heavily rely on token overlap ) , a model trained on   RELiC is able to leverage the longer queries for   better retrieval .   2.3 Analyzing different types of quotation   What are the different ways in which literary schol-   ars use direct quotation in RELiC ? We perform a   manual analysis of 200 held - out examples to gain a   better understanding of quotation usage , categoriz-   ing each quotation into the following three types :   Claim - supporting evidence : In 151 of the 200   annotated examples , literary scholars used direct   quotation to provide evidence for a more general   claim about the primary source work . In the ﬁrst   row of Table 2 , Hartstein ( 1985 ) claims that “ this   whale ... brings into focus such fundamental ques-   tions as the knowability of space : ” and then quotes   the following metaphorical description from Moby   Dick as evidence : “ And as for this whale spout , you   might almost stand in it , and yet be undecided as to   what it is precisely . ” When quoted material is used   asclaim - supporting evidence , the context before   and after usually refers directly to the quoted ma-   terial;for example , the paradoxes of reality and   uncertainties of this world are exempliﬁed by the   vague nature of the whale spout .   Paraphrase - supporting evidence : In 31 of the   examples , we observe that scholars used the pri-   mary source work to support their own paraphras-   ing of the plot in order to contextualize later anal-   ysis . In the second row of Table 2 , Blackstone   ( 1972 ) uses the quoted material to enhance a sum-   mary of a speciﬁc scene in which Jacob ’s mind is   wandering during a chapel service . Jacob ’s day-   dreaming is later used in an analysis of Cambridge   as a location in Virginia Woolf ’s works , but no   literary argument is made in the immediate con-   text . When quoted material is being employed as7502   paraphrase - supporting evidence , the surround-   ing context does not refer directly to the quotation .   Miscellaneous : 18 of the 200 samples were not   literary analysis , though some were still related   to literature ( for example , analysis of the the ﬁlm   adaptation of The Age of Innocence ) . Others were   excerpts from the primary sources that suffered   from severe OCR artifacts and were not detected   or extracted by the methods in Appendix A.2 .   3 Literary Evidence Retrieval   Having established that the examples in RELiC   contain complex interplay between literary quota-   tion and scholarly analysis , we now shift to measur-   ing how well neural models can understand these   interactions . In this section , we ﬁrst formalize our   evidence retrieval task , which provides the schol-   arly context without the quotation as input to a   model , along with a set of candidate passages that   come from the same book , and asks the model to re-   trieve the ground - truth missing quotation from the   candidates . Then , we describe standard informa-   tion retrieval baselines as well as a RoBERTa - based   ranking model that we implement to solve our task.3.1 Task formulation   Formally , we represent a single window in RELiC   from book bas (: : : ; l ; l ; q ; r ; r ; : : :) where   qis the quoted n - sentence long passage , and land   rcorrespond to individual sentences before and   after the quotation in the scholarly article , respec-   tively . The window size on each side is bounded   by hyperparameters landr , each of which   can be up to 4 sentences . Given the land   rsentences surrounding the missing quota-   tion , we ask models to identify the quoted passage   qfrom the candidate set C , which consists of   alln - sentence long passages in book b(see Fig-   ure 1 ) . This is a particularly challenging retrieval   task because the candidates are part of the same   overall narrative and thus mention the same overall   set of entities ( e.g. , characters , locations ) and other   plot elements , which is a disadvantage for methods   based on string overlap .   Evaluation : Models built for our task must pro-   duce a ranked list of candidates Cfor each ex-   ample . We evaluate these rankings using both   recall@ kfork= 1;3;5;10;50;100 andmean   rank ofqin the ranked list . Both types of metrics   focus on the position of the ground - truth quotation7503   qin the ranked list , and neither gives special treat-   ment to candidates that overlap with q. As such ,   recall@1 alone is overly strict when the quotation   length l > 1 , which is why we show recall at mul-   tiple values of k. An additional motivation is that   there may be multiple different candidates that ﬁt   a single context equally well . We also report ac-   curacy on a proxy task with only three candidates ,   which allows us to compare with human perfor-   mance as described in Section 4 .   3.2 Models   Baselines : Our baselines include both standard   term matching methods as well as pretrained dense   retrievers . BM25 ( Robertson et al . , 1995 ) is a bag-   of - words method that is very effective for informa-   tion retrieval . We form queries by concatenating   the left and right context and use the implementa-   tion from the rank_bm25 libraryto build a BM25   model for each unique candidate set C , tuningthe free parameters as per Kamphuis et al . ( 2020 ) .   Meanwhile , our dense retrieval baselines are   pretrained neural encoders that map queries and   candidates to vectors . We compute vector similar-   ity scores ( e.g. , cosine similarity ) between every   query / candidate pair , which are used to rank can-   didates for every query and perform retrieval . We   consider the following four pretrained dense re-   triever baselines in our work , which we deploy in a   zero - shot manner ( i.e. , not ﬁne - tuned on RELiC ):   •DPR ( Dense Passage Retrieval ) is a dense re-   trieval model from Karpukhin et al . ( 2020 )   trained to retrieve relevant context paragraphs   in open - domain question answering . We use   the DPR context encoderpretrained on Nat-   ural Questions ( Kwiatkowski et al . , 2019 )   with dot product as a similarity function .   •SIM is a semantic similarity model from Wi-   eting et al . ( 2019 ) that is effective on semantic   textual similarity benchmarks ( Agirre et al . ,   2016 ) . SIM is trained on ParaNMT ( Wiet-   ing and Gimpel , 2018 ) , a dataset containing750416.8 M paraphrases ; we follow the original im-   plementation , and use cosine similarity as   the similarity function .   •c - REALM ( contrastive Retrieval Augmented   Language Model ) is a dense retrieval model   from Krishna et al . ( 2021 ) trained to retrieve   relevant contexts in open - domain long - form   question answering , and shown to be a better   retriever than REALM ( Guu et al . , 2020 ) on   the ELI5 KILT benchmark ( Fan et al . , 2019 ;   Petroni et al . , 2021 ) .   •ColBERT is a ranking model from Khattab   and Zaharia ( 2020 ) that estimates the rele-   vance between a query and a document using   contextualized late interaction . It is trained   on MS MARCO ranking data ( Nguyen et al . ,   2016 ) .   Training retrievers on RELiC ( dense - RELiC ):   Both BM25 and the pretrained dense retriever base-   lines perform similarly poorly on RELiC ( Table   3 ) . These methods are unable to capture more com-   plex interactions within RELiC that do not exhibit   extensive string overlap between quotation and con-   text . As such , we also implement a strong neural   retrieval model that is actually trained on RELiC ,   using a similar setup to DPR and REALM . We   ﬁrst form a context string cby concatenating a win-   dow of sentences on either side of the quotation q   ( replaced by a MASK token ) ,   c= ( l ; : : : ; l;[MASK ] ; r ; : : : ; r )   We train two encoder neural networks to project   the literary context and quote to ﬁxed 768- dvec-   tors . Speciﬁcally , we project candqusing sepa-   rate encoder networks initialized with a pretrained   RoBERTa - base model ( Liu et al . , 2019 ) . We use   the < s > token of RoBERTa to obtain 768- dvectors   for the context and quotation , which we denote as   candq . To train this model , we use a contrastive   objective ( Chen et al . , 2020 ) that pushes the context   vector cclose to its quotation vector q , but away   from all other quotation vectors qin the same   minibatch ( “ in - batch negative sampling ” ):   loss= XlogexpcqPexpcqwhere Bis a minibatch . Note that the size of the   minibatchjBjis an important hyperparameter since   it determines the number of negative samples .   All elements of the minibatch are context / quotation   pairs sampled from the same book . During infer-   ence , we rank all quotation candidate vectors by   their dot product with the context vector .   3.3 Results   We report results from the baselines and our dense-   RELiC model in Table 3 with varying context sizes   where L = R refers to Lpreceding context sentences   andRsubsequent context sentences . While all   models substantially outperform random candidate   selection , all pretrained neural dense retrievers per-   form similarly to BM25 , with ColBERT being the   best pretrained neural retriever ( 2.9 recall@1 ) . This   result indicates that matching based on string over-   lap or semantic similarity is not enough to solve   RELiC , and even powerful neural retrievers strug-   gle on this benchmark . Training on RELiC is cru-   cial : our best - performing dense - RELiC model per-   forms 7x better than BM25 ( 9.4 vs 1.3 recall@1 ) .   Context size and location matters for model per-   formance : Table 3 shows that dense - RELiC ef-   fectively utilizes longer context — feeding only   one sentence on each side of the quotation ( 1/1 ) is   not as effective as a longer context ( 4/4 ) of four sen-   tences on each side ( 7.8 vs 9.4 recall@1 ) . However ,   the longer contexts hurt performance for pretrained   dense retrievers in the zero - shot setting ( 1.6 vs 0.9   recall@1 for c - REALM ) , perhaps because context   further away from the quotation is less likely to   be helpful . Finally , we observe that dense - RELiC   performance is strictly better ( 5.2 vs 6.8 recall@1 )   when the model is given only preceding context   ( 4/0 or 1/0 ) compared to when the model is given   only subsequent context ( 0/4 or 0/1 ) .   Dense vs. sparse retrievers : As expected ,   BM25 retrieves the correct quotation when there   is signiﬁcant string overlap between the quotation   and context , as in the following example from The   Great Gatsby , in which the terms sky , bloom , Mrs.   McKee , voice , call , and back appear in both places:7505   However , this behavior is undesirable for most   examples in RELiC , since string overlap is gen-   erally not predictive of the relationship between   quotations and claims . The top row of Table 5 con-   tains one such example , where dense - RELiC cor-   rectly chooses the missing quotation while BM25   is misled by string overlap .   4 Human performance and analysis   How well do humans actually perform on RELiC ?   To compare the performance of our dense retriever   to that of humans , we hired six domain experts with   at least undergraduate - level degrees in English lit-   erature from the Upworkfreelancing platform .   Because providing thousands of candidates to a   human evaluator is infeasible , we instead measure   human performance on a simpliﬁed proxy task : we   provide our evaluators with four sentences on either   side of a missing quotation from Pride and Prej-   udiceand ask them to select one of only three   candidates to ﬁll in the blank . We obtain human   judgments both to measure a human upper bound   on this proxy task as well as to evaluate whether hu-   mans struggle with examples that fool our model .   Human upper bound : First , to measure a hu-   man upper bound on this proxy task , we chose   200 test set examples from Pride and Prejudice   and formed a candidate pool for each by includ-   ing BM25 ’s top two ranked answers along with   the ground - truth quotation for the single sentence   case . As the task is trivial to solve with random   candidates , we decided to use a model to select   harder negatives , and we chose BM25 to see if hu-   mans would be distracted by high string overlap in   the negatives . Each of the 200 examples was sep-   arately annotated by three experts , and they werepaid $ 100 for annotating 100 examples . The last   column of Table 3 compares all of our baselines   along with dense - RELiC against human domain   experts on this proxy task . Humans substantially   outperform all models on the task , with at least two   of the three domain experts selecting the correct   quote 93.5 % of the time ; meanwhile , the highest   score for dense - RELiC is 67.5 % , which indicates   huge room for improvement . Interestingly , all of   the zero - shot dense retrievers except ColBERT 1/1   underperform random selection on this task ; we   theorize that this is because all of these retrievers   are misled by the high string overlap of the neg-   ative BM25 - selected examples . Table 4 conﬁrms   substantial agreement among our annotators .   Human error analysis of dense - RELiC : To   evaluate the shortcomings of our dense - RELiC   retriever , we also administered a version of the   proxy task where the candidate pool included the   ground - truth quotation along with dense - RELiC ’s   two top - ranked candidates , where for all examples   the model ranked the ground - truth outside of the   top 1000 candidates . Three domain experts at-   tempted 100 of these examples and achieved an   accuracy of 94 % , demonstrating that humans can   easily disambiguate cases on which our model fails ,   though we note our model ’s poorer performance   when retrieving a single sentence ( as in the proxy   task ) versus multiple sentences ( A5 ) . The bottom   two rows of Table 5 contain instances in which all   human annotators agreed on the correct candidate   but dense - RELiC failed to rank it in the top 1000 .   In one , all human annotators immediately recog-   nized the opening line of Pride and Prejudice , one7506   of the most famous in English literature . In the   other , the claim mentions that the interpretation   hinges on a single word ’s ( “ got ” ) connotation of “ a   market , ” which humans understood .   Issuing out - of - distribution queries to the re-   triever : Does our dense - RELiC model have po-   tential to support humanities scholars in their   evidence - gathering process ? Inspired by prompt-   based learning , we manually craft simple yet out - of-   distribution prompts and queried our dense - RELiC   retriever trained with 1 sentence of left context and   no right context . A qualitative inspection of the   top - ranked quotations in response to these prompts   ( Table 6 ) reveals that the retriever is able to obtain   evidence for distinct character traits , such as the   ignorance of the titular character in Frankenstein   or Gatsby ’s wealthy lifestyle in The Great Gatsby .   More impressively , when queried for an example   from Pride and Prejudice of the main character ,   Elizabeth , demonstrating frustration towards her   mother , the retriever returns relevant excerpts in   the ﬁrst - person that do not mention Elizabeth , and   the top - ranked quotations have little to no string   overlap with the prompts . Limitations : While these results show dense-   RELiC ’s potential to assist research in the humani-   ties , the model suffers from the limited expressivity   of its candidate quotation embeddings q , and ad-   dressing this problem is an important direction for   future work . The quotation embeddings do not in-   corporate any broader context from the narrative ,   which prevents resolving coreferences to pronomi-   nal character mentions and understanding other im-   portant discourse phenomena . For example , Table   A5 shows that dense - RELiC ’s top two 1 - sentence   candidates for the above Pride and Prejudice ex-   ample are not appropriate evidence for the literary   claim ; the increased relevancy of the 2 - sentence   candidates ( Table 6 , third row ) over the 1 - sentence   candidates suggests that dense - RELiC may ben-   eﬁt from more contextualized quotation embed-   dings . Furthermore , dense - RELiC struggles with   retrieving concepts unique to a text , such as the   “ hypnopaedic phrases ” strewn throughout Brave   New World ( Table 6 , bottom ) .   5 Related Work   Datasets for literary analysis : Our work relates   to previous efforts to apply NLP to literary datasets7507   such as LitBank ( Bamman et al . , 2019 ; Sims et al . ,   2019 ) , an annotated dataset of 100 works of ﬁc-   tion with annotations of entities , events , corefer-   ences , and quotations . Papay and Padó ( 2020 ) in-   troduced RiQuA , an annotated dataset of quota-   tions in English literary text for studying dialogue   structure , while Chaturvedi et al . ( 2016 ) and Iyyer   et al . ( 2016 ) characterize character relationships in   novels . Our work also relates to quotability iden-   tiﬁcation ( MacLaughlin and Smith , 2021 ) , which   focuses on ranking passages in a literary work by   how often they are quoted in a larger collection .   Unlike RELiC , however , these datasets do not con-   tain literary analysis about the works .   Retrieving cited material : Citation retrieval   closely relates to RELiC and has a long history   of research , mostly on scientiﬁc papers : O’Connor   ( 1982 ) formulated the task of document retrieval   using “ citing statements ” , which Liu et al . ( 2014 )   revisit to create a reference retrieval tool that recom-   mends references given context . Bertin et al . ( 2016 )   examine the rhetorical structure of citation con-   texts . Perhaps closest to RELiC is the work of Grav(2019 ) , which concentrates on the quotation of sec-   ondary sources in other secondary sources , unlike   our focus on quotation from primary sources . Fi-   nally , as described in more detail in Section 2.2 and   Appendix A6 , RELiC differs signiﬁcantly from   existing NLP and IR retrieval datasets in domain ,   linguistic complexity , and query length .   6 Conclusion   In this work , we introduce the task of literary   evidence retrieval and an accompanying dataset ,   RELiC. We ﬁnd that direct quotation of primary   sources in literary analysis is most commonly used   as evidence for literary claims or arguments . We   train a dense retriever model for our task ; while it   signiﬁcantly outperforms baselines , human perfor-   mance indicates a large room for improvement . Im-   portant future directions include ( 1 ) building better   models of primary sources that integrate narrative   and discourse structure into the candidate represen-   tations instead of computing them out - of - context ,   and ( 2 ) integrating RELiC models into real tools   that can beneﬁt humanities researchers.7508Acknowledgements   First and foremost , we would like to thank the   HathiTrust Research Center staff ( especially Ryan   Dubnicek ) for their extensive feedback throughout   our project . We are also grateful to Naveen Jafer   Nizar for his help in cleaning the dataset , Vishal   Kalakonnavar for his help with the project web-   page , Marzena Karpinska for her guidance on com-   puting inter - annotator agreement , and the UMass   NLP community for their insights and discussions   during this project . KT and MI are supported by   awards IIS-1955567 and IIS-2046248 from the Na-   tional Science Foundation ( NSF ) . KK is supported   by the Google PhD Fellowship awarded in 2021 .   Ethical Considerations   We acknowledge that the group of authors from   whom we selected primary sources lacks diversity   because we selected from among digitized , pub-   lic domain sources in the Western literary canon ,   which is heavily biased towards white , male writers .   We made this choice because there are relatively   few primary sources in the public domain that are   written by minority authors and also have substan-   tial amounts of literary analysis written about them .   We hope that our data collection approach will be   followed by those with access to copyrighted texts   in an effort to collect a more diverse dataset . The   experiments involving humans were reviewed by   the UMass Amherst IRB with a status of Exempt .   References7509751075117512Appendices for “ RELiC : Retrieving   Evidence from Literature in Context ”   A Dataset Collection & Statistics   Filtering secondary sources : The HathiTrust is   not exclusively a repository of literary analysis ,   and we observe that many matching quotes come   from different editions of a primary source , writing   manuals , and even advertisements . Because we are   seeking only scholarly work that directly analyzes   the quoted sentences , we performed a combination   of manual and automatic ﬁltering to remove such   extraneous matches . For each primary source , we   ﬁrst aggregate all secondary sources matches by the   their unique HathiTrust - assigned identiﬁer . From   manual inspection of the secondary source titles ,   most sources that quote a particular literary work   only once or twice are not likely to be literary schol-   arship , while sources with hundreds of matches are   almost always a different edition of the primary   source itself . For each primary source , we create   upper and lower thresholds for number of matches ,   discarding sources that fall outside of these bounds .   Additionally , we discard secondary sources whose   titles contain the words “ dictionary ” , “ anthology ” ,   “ encyclopedia , ” and others that indicate that a sec-   ondary source is not literary scholarship .   Preprocessing : After the above ﬁltering , we   identiﬁed and removed all non - English secondary   sources using langid , a Python tool for language   identiﬁcation . Next , because the secondary source   texts in the HathiTrust are digitized via OCR , vari-   ous artifacts appear throughout the pages we down-   load . Some of these , such as citations that in-   clude the page number of primary source quotes ,   allow models trained on our task to “ cheat ” to iden-   tify the proper quote ( see Table A1 ) , necessitating   their removal . Using a pattern - matching approach ,   we eliminate the most pervasive : in - line citations ,   headers , footers , and word breaks . Finally , we ap-   ply sentence tokenization in order to standardize   the length of preceding and subsequent context win-   dows for the ﬁnal dataset . Speciﬁcally , we feed the   preprocessed text through spaCy’sdependency   parser - based sentence segmenter on the cleaned   text . The default segmenter in spaCy is modiﬁed to   use ellipses , colons , and semicolons as custom sen-   tence boundaries , based on the observation that lit-   erary scholars often only quote part of what wouldtypically be deﬁned as a sentence ( Table A2 ) .   Identifying quoted sentences : As previously   mentioned , HathiTrust does not provide the exact   indices corresponding to the primary source quote .   As such , we identify which secondary source sen-   tences ( from the output of the sentence tokenizer )   include quotes from primary source works using   RapidFuzz , a fuzzy string match library , with the   QRatio metric and a score threshold of 80.0 . Fuzzy   match is essential for detecting quotes with OCR   mistakes or with author modiﬁcations ; in Appendix   Table A3 , for instance , the author adds clariﬁcation   [ the natives ] and omits “ he would say ” when cit-   ing two sentences from Joseph Conrad ’s Heart of   Darkness . Once a fuzzy match is identiﬁed in a   secondary source document , we replace it with its   corresponding primary source sentence.7513   Identifying block quotes : While we query   HathiTrust at a sentence level , many of the returned   results are actually block quotes in which multi-   ple contiguous sentences from the primary source   are quoted . Correct identiﬁcation of these block   quotes is integral to the quality of our dataset and   formulated task : if the preceding or subsequent   context contains part of the quoted span , our evi-   dence retrieval task becomes trivial because part   of the answer exists in the input . In our approach ,   if the fuzzy match yields consecutive matches in   secondary source documents for sentences that also   appear consecutively in the primary source , we con-   catenate them together and consider them a single   block quote .   Handling ellipses : One prevalent technique for   direct quotation in literary analysis is the use of   ellipses to condense primary source material . As   our fuzzy match method still falls short in detecting   block quotes that contain ellipses , we implement   an additional method for insuring that block quotes   are properly delineated . Once the fuzzy match   approach fails to identify any more consecutively   quoted sentences in a secondary source , we con-   tinue to search for matches adjacent to the block   quote using the Longest Common Substring ( LCS )   metric . If a block - quote - adjacent sentence in the   secondary source shares an LCS of 15 or more char-   acters with the block - quote - adjacent sentence in theprimary source , this is considered a match and con-   catenated with the block quote ( see Appendix A.1   for an example ) .   A.1 LCS example   For example , in Parker ( 1985 ) , Kenneth Parker   cites a passage from Joseph Conrad ’s Heart of   Darkness : “ The narrator , Marlow , informs us , ap-   provingly : ... I met a white man , in such an un-   expected elegance of get - up that in the ﬁrst mo-   ment I took him for a sort of vision . I saw a   high starched collar , white cuffs , a light alpaca   jacket , snowy trousers , a clean necktie , and var-   nished boots . ” Fuzzy match alone is insufﬁcient   for detecting the ﬁrst sentence in this block quote   that contains an ellipse in place of primary source   text . With our LCS approach , we are able to re-   place the ﬁrst sentence of block quote above with   “ When near the buildings I met a white man , in   such an unexpected elegance of get - up that in   the ﬁrst moment I took him for a sort of vision . ”   A.2 Noise when standardizing quotes :   In a small number of cases , our quote standardiza-   tion process removes important context . For ex-   ample , the analysis of Maes - Jelinek ( 1970 ) quotes   a sentence from D.H. Lawrence ’s The Rainbow   as “ As to Will , his intimate life was so violently   active , that it set another man free in him . ” . Af-   ter standardization , the example in our dataset be-   comes “ His intimate life was so violently active ,   that it set another man free in him . ” , dropping   the critical “ As to Will ” necessary for the integra-   tion of the quote in the surrounding analysis .   Model - predicted quotes are sometimes as valid   as the gold quote : Human raters also identify   cases in which multiple quotes appear to be appro-   priate evidence for a literary claim , which illus-   trate the model ’s potential in helping humanities   scholars ﬁnd evidence . In Table A4 , both model   and experts failed to identify the correct quote that   both depicts Elizabeth ’s “ discomﬁture ” and has   a “ Greek ring to it : ” “ Till this moment I never   knew myself . ” However , the experts all selected   the model ’s second ranked choice which mentions   Elizabeth ’s “ anger ” at “ herself . ” This quote also   shows Elizabeth ’s displeasure while referring to   the Greek idea of self.7514   A.3 More dataset statistics   Each primary source has relevant windows from an   average of 112 unique secondary sources , and an   average of 16.35 % of the sentences in each primary   source are quoted in secondary sources . On aver-   age , each primary source has 995 corresponding   windows in our dataset , and each secondary source   produced an average of 9 windows . Figure 2 shows   the distribution of quote lengths in RELiC , sug-   gesting that successful models will have to learn to   understand both single - sentence and block quotes   in context .   B Best Model Detailed Results   Candidate length does not signiﬁcantly affect   model performance : We observe in Table A9   that the length of the ground - truth quote and the   candidates does not signiﬁcantly impact model per - formance — for a ﬁxed k , model performance is   within 10 % for any candidate length . Model per-   formance is slightly worse for longer candidates of   length 4 or 5 , and for the shortest single sentence   contexts ( possibly due to under - speciﬁcation).751575167517   # of sents # instances recall@k mean rank avg . # candidates   in quote 1 3 5 10 50 100   1 3279 8.8 16.2 21.0 29.0 46.2 55.8 454.7 4913.0   2 2028 11.0 21.5 27.4 35.6 55.5 65.2 337.6 4991.0   3 1189 9.3 20.1 26.8 35.5 55.9 64.4 298.2 4873.7   4 796 9.0 17.8 24.0 33.0 53.9 64.1 312.9 4753.5   5 493 6.9 15.8 22.3 33.7 52.9 62.7 377.3 4549.97518