  Artem Vazhentsev , Gleb Kuzmin , Akim Tsvigun ,   Alexander Panchenko , Maxim Panov , Mikhail Burtsev , and Artem ShelmanovAIRI , Skoltech , MBZUAI , FRC CSC RAS , AI Center NUST MISiS , TII , London Institute for Mathematical Sciences , Semrush   { vazhentsev , kuzmin , panchenko}@airi.net   maxim.panov@tii.ae artem.shelmanov@mbzuai.ac.ae   Abstract   Many text classification tasks are inherently   ambiguous , which results in automatic sys-   tems having a high risk of making mistakes ,   in spite of using advanced machine learning   models . For example , toxicity detection in user-   generated content is a subjective task , and no-   tions of toxicity can be annotated according to   a variety of definitions that can be in conflict   with one another . Instead of relying solely on   automatic solutions , moderation of the most   difficult and ambiguous cases can be delegated   to human workers . Potential mistakes in auto-   mated classification can be identified by using   uncertainty estimation ( UE ) techniques . Al-   though UE is a rapidly growing field within   natural language processing , we find that state-   of - the - art UE methods estimate only epistemic   uncertainty and show poor performance , or   under - perform trivial methods for ambiguous   tasks such as toxicity detection . We argue that   in order to create robust uncertainty estimation   methods for ambiguous tasks it is necessary to   account also for aleatoric uncertainty . In this   paper , we propose a new uncertainty estimation   method that combines epistemic and aleatoric   UE methods . We show that by using our hybrid   method , we can outperform state - of - the - art UE   methods for toxicity detection and other am-   biguous text classification tasks .   1 Introduction   Many natural language processing ( NLP ) tasks are   subjective and contain inherent ambiguity . For ex-   ample , the notion of toxicity is inherently subjec-   tive ( Waseem , 2016 ) and can be defined in a num-   ber of ways that may conflict with one another and   differ according to the demographic that the meth-   ods are applied to ( Thylstrup and Waseem , 2020 ) .   For many datasets , implicit or ambiguous toxicity   can comprise more than 90 % of the labeled toxiccontent ( Hartvigsen et al . , 2022 ) . Such ambiguity   introduces a high risk of classification mistakes for   machine learning ( ML ) models . Classification mis-   takes for toxicity detection can result in the removal   of legitimate non - toxic content on one hand , and   the lack of sanction for toxic content , on the other .   A common method for addressing this concern for   content moderation is to abstain from predictions   on ambiguous instances and process them with the   help of human workers ( Roberts , 2019 ) .   A classification task where some model predic-   tions can be “ rejected ” is called selective classifica-   tion(Geifman and El - Yaniv , 2017 ) . The common   approach to solving it is applying uncertainty es-   timation ( UE ) techniques . UE is a field of ML   that seeks to model the degree to which model   predictions can be trusted by correlating model   mistakes and performance . Better UE methods im-   prove the performance of selective classification   and the trade - off between the amount of labor and   the reliability of downstream applications . In tox-   icity detection , better UE methods minimize the   amount of content that is reviewed by human mod-   erators to predominately be classification errors .   Recent works have suggested deterministic ap-   proaches to UE of neural network predictions based   on fitting the density of latent instance representa-   tions ( Lee et al . , 2018 ; van Amersfoort et al . , 2020 ;   Mukhoti et al . , 2023 ; Yoo et al . , 2022 ; Kotelevskii   et al . , 2022 ) . They have shown good performance   in NLP for the detection of out - of - distribution   ( OOD ) instances , adversarial attacks , and misclas-   sified objects in non - ambiguous tasks . However ,   they primarily capture epistemic uncertainty , i.e.   uncertainty related to the lack of knowledge about   model parameters and training data , overlooking   aleatoric uncertainty , i.e. uncertainty that arises   from ambiguity and noise in data .   This work aims to create a UE method for more   reliable selective classification in ambiguous tasks   such as toxicity detection by combining different11659   types of uncertainty . Instances that carry a high risk   of classification mistakes come from two sources :   a ) OOD areas , which can be detected with epis-   temic UE methods ; and b ) in - distribution ambigu-   ous areas , for detection of which , aleatoric UE   methods are appropriate ( for illustration see Fig-   ure 1 ) . Therefore , we propose a Hybrid Uncer-   tainty Quantification ( HUQ ) method that switches   between epistemic and aleatoric uncertainties or lin-   early combines them . It produces better scores of   total uncertainty , which subsequently leads to better   selective classification . The experiments on various   ambiguous tasks show that HUQ in a majority of   cases significantly outperforms other state - of - the-   art UE techniques . To summarize , the contributions   of this work are the following .   •In Section 4 , we propose a new uncertainty   estimation method HUQ that combines epis-   temic and aleatoric UE techniques in a special   way that allows to improve the quality of se-   lective classification in ambiguous tasks .   •To the best of our knowledge , this work is   the first to conduct an empirical investigation   of state - of - the - art UE methods for ambiguous   text classification tasks such as toxicity detec-   tion . Our analysis shows that the proposed   HUQ approach outperforms state - of - the - art   methods in selective text classification on am-   biguous tasks ; see Sections 5 and 6 .   •We analyze the limitations of the proposed   method and suggest conditions to be met forachieving the improvements ; see Section 7 .   2 Related Work   Quantifying uncertainty of deep neural network   predictions can be successfully accomplished us-   ing deep ensembles ( DE ; Lakshminarayanan et al . ,   2017 ) , Bayesian models ( Blundell et al . , 2015 ) ,   or their approximations . However , most of these   methods have various drawbacks , including large   computational overhead . For example , for DE , we   need to multiply training time , the occupied mem-   ory , and inference time , since this network requires   training , storing , and running inference for multi-   ple versions of the same model . This makes DE   hardly applicable in real - world scenarios .   Recent work has investigated computationally   efficient deterministic approaches ( e.g. , Lee et al . ,   2018 ; van Amersfoort et al . , 2020 ; Liu et al . , 2020 ) .   However , most work is based on feature space   density and focuses only on the OOD detection   task and epistemic uncertainty estimation . An-   other computationally efficient approach is Selec-   tiveNet ( Geifman and El - Yaniv , 2019 ) , which was   designed for computer vision tasks . It introduces   two separate heads for prediction and selection   within the model architecture and adds a special   loss component to minimize selective risk with a   specified coverage .   Most similar to our work is Mukhoti et al . ( 2023 ) ,   which also considers both aleatoric and epistemic   uncertainty . DDU uses a combination of feature-11660space density for epistemic uncertainty and the soft-   max predictive distribution for aleatoric uncertainty .   They advocate for the usage of different methods   for quantifying uncertainty , depending on whether   a considered instance is ID or OOD . However , they   overlook using a linear combination of uncertainty   scores , relying solely on feature - space density for   instances considered OOD . We note that these in-   stances can also be borderline ( instances from mid-   dle to low - density areas ) , for which using aleatoric   uncertainty measures may also be appropriate . Be-   sides , Mukhoti et al . ( 2023 ) do not provide results   for selective classification and mostly experiment   with image classification tasks .   Recently , selective classification ( or misclas-   sification detection ) has been studied for NLP   tasks . One line of such work has proposed adding   a regularization term to the training loss . Xin et al .   ( 2021 ) introduces a penalty term for confident in-   stances with a high loss value . Another approach   proposed by Zhang et al . ( 2019 ) uses a metric reg-   ularization that minimizes the inter - class distance   in the latent feature space while maximizing the   margin between classes . He et al . ( 2020 ) propose a   regularization technique based on self - ensembling   that aims to minimize the difference between pre-   dictions of the two versions of the model . They   also combine this approach with mix - up ( Thulasi-   dasan et al . , 2019 ) and a distinctiveness score based   on the MD . Some work has also considered approx-   imations of deep ensembles based on Monte - Carlo   dropout ( e.g. , Shelmanov et al . , 2021 ; Vazhentsev   et al . , 2022 ) . Vazhentsev et al . ( 2022 ) conduct   a vast empirical investigation and suggest several   promising combinations of regularizers and feature-   density - based methods . They also highlight the   importance of spectral normalization for obtaining   good results . Kotelevskii et al . ( 2022 ) propose a   new UE method NUQ and test it for text classifica-   tion models trained in the low - resource regime .   Despite the aforementioned efforts , highly am-   biguous text classification tasks such as toxicity de-   tection have been overlooked in the previous work .   Moreover , to the best of our knowledge , no prior   work in NLP takes into account aleatoric uncer-   tainty and combines multiple types of uncertainty   for a holistic view of uncertainty .   3 Background   Two types of uncertainty have been documented   in the literature : aleatoric and epistemic ( Der Ki - ureghian and Ditlevsen , 2009 ) . Aleatoric , or data   uncertainty , arises from ambiguity and noise in   data . It should be high , for example , for groups of   instances prone to annotation discrepancy . Epis-   temic , or model uncertainty , pertains to a lack of   knowledge about model parameters and can of-   ten be mitigated through additional training data   collection . Epistemic uncertainty is particularly im-   portant for OOD detection ( Hendrycks and Gimpel ,   2017 ) and active learning ( Settles , 2009 ) .   According to the Bayesian approach to measur-   ing uncertainty in deep learning networks ( Blundell   et al . , 2015 ; Gal , 2016 ; Depeweg et al . , 2018 ) , the   total uncertainty of a model prediction xis a sum of   aleatoric U(x)and epistemic uncertainty U(x ):   U(x ) = U(x ) + U(x ) . ( 1 )   High total uncertainty should correlate with clas-   sification mistakes and can be used to flag model   predictions for human review .   3.1 Out - of - Distribution and ( Ambiguous )   In - Distribution Instances   We define out - of - distribution ( OOD ) instances   Xas those located either outside a training data   distribution or in its low - density regions . They can   be identified by high epistemic uncertainty .   In - distribution ( ID ) instances we define to be-   long to the domain of the dataset Dlocated “ in-   side ” the training data distribution . ID instances   are those , for which model predictions have very   small epistemic uncertainty , i.e. below some thresh-   oldδ :   X={x : U(x)≤δ } . ( 2 )   Note that for the in - distribution data , on the basis   of(1)and taking into account ( 2 ) , we can empiri-   cally approximate U(x)≃U(x ) .   We also define ambiguous in - distribution ( AID )   instances as those , predictions on which having   the highest values of aleatoric uncertainty with a   lower bound δ . AID instances lie around the   class - decision boundaries virtually established by   the discriminative model :   X={x∈ X : U(x ) > δ } . ( 3 )   3.2 Quantifying Epistemic Uncertainty   Recent works have proposed a variety of compu-   tationally efficient methods for quantifying epis-   temic uncertainty on the basis of fitting the proba-   bility density of latent instance representations . In11661this work , we experiment with Mahalanobis Dis-   tance ( MD , Lee et al . , 2018 ) , Robust Density Esti-   mation ( RDE , Yoo et al . , 2022 ) , and Deep Deter-   ministic Uncertainty ( DDU , Mukhoti et al . , 2023 ) .   LetDbe a training dataset , h(x)be a latent rep-   resentation of an instance x(it is usually taken from   the penultimate layer of the network ) , and c∈C   be a class . The UE method based on MD ( Lee   et al . , 2018 ) , for each class , fits a Gaussian cen-   tered in a class centroid { µ}with a covariance   matrix Σshared across classes . The highest class-   conditional probability density p(h(x)|y = c)de-   termines the confidence of the prediction , and the   uncertainty score is computed as the Mahalanobis   distance between h(x)and the closest centroid :   U(x ) = min(h(x)−µ)Σ(h(x)−µ ) .   RDE ( Yoo et al . , 2022 ) improves on MD by com-   puting the covariance matrix Σfor each individual   class using the Minimum Covariance Determinant   estimation ( Rousseeuw , 1984 ) and by reducing the   dimensionality of the hidden representations via   PCA decomposition with an RBF kernel . These   modifications aim to minimize the determinant of   the covariance matrix and reduce the influence of   outliers in the training data .   DDU ( Mukhoti et al . , 2023 ) fits a Gaussian Mix-   ture Model ( GMM ) p(h(x ) , y)with a single mix-   ture component per class . The uncertainty score is   the probability density of h(x)under the GMM :   U(x ) = /summationdisplayp(h(x)|y = c)p(y = c ) ,   where p(h(x)|y = c)∼ N(h(x)|µ,Σ)and   p(y = c ) = /summationtext1[y = c ] .   Methods based on the fitting density of latent rep-   resentations are suitable for finding OOD instances   but are not capable of identifying AID instances .   More generally , they are not good estimators of   uncertainty in X. Therefore , for ambiguous tasks   where AID instances comprise a large portion of   the data , these epistemic UE methods can not fully   cover all potential misclassifications .   3.3 Quantifying Aleatoric Uncertainty   As measures of aleatoric uncertainty , we use two   well - known methods based on probabilities from   the output softmax layer of a neural network : en-   tropy ( Gal , 2016 ) and Softmax Response ( SR , Geif - man and El - Yaniv , 2017 ):   U(x ) = −/summationdisplayp(y = c|x ) logp(y = c|x ) ,   U(x ) = 1−maxp(y = c|x ) .   Entropy and SR have been proposed also as mea-   sures of total uncertainty ( Malinin and Gales , 2018 ) .   However , this assumption holds only when one has   access to the full posterior distribution under the   Bayesian paradigm , i.e. all possible uncertainties   are quantified within the model . In practice , train-   ing datasets are limited , and we can only approx-   imate considered probability distributions . Thus ,   these methods could not capture all the epistemic   uncertainty and mostly reflect the aleatoric one ( van   Amersfoort et al . , 2020 ; Mukhoti et al . , 2023 ) .   4 Hybrid Uncertainty Quantification   There are two major sources of mistakes in model   predictions : OOD instances and instances that lie in   proximity to the decision boundary ( AID instances ) .   Aleatoric uncertainty can help to detect AID in-   stances , while epistemic uncertainty can help to   detect OOD instances . In many tasks , we have   to deal with both types of mistakes arising from   task ambiguity or from a marked covariate shift   between training and test data . To address this is-   sue , we propose a hybrid method that combines the   strengths of aleatoric and epistemic uncertainty .   Our hybrid uncertainty quantification ( HUQ )   method first uses Eq . ( 2 ) to determine whether an   instance xis ID or OOD . If x∈ X , HUQ applies   Eq . ( 3 ) to determine if xis near a class - decision   boundary , i.e. , x∈ X. Once the type of instance   has been identified , we can apply an appropriate   uncertainty estimation method for it or combine   multiple uncertainty scores into a single estimate .   Uncertainty scores from different methods may   however not be comparable with one another due   to different magnitudes . Therefore , instead of us-   ing absolute values , we propose to rank instances   in some dataset Dby their uncertainty scores and   as a final score use these ranks or their combina-   tions . Ranking can be considered as a form of   normalization . Moreover , such an approach is de-   sirable for the selective classification task , as we   are only interested in the ability to rank predictions   by their uncertainty . We define a ranking function   R(u , D)as the rank of uover a sorted dataset D ,   sou > uimplies R(u , D ) > R(u , D).11662Having the ranks according to epistemic and   aleatoric scores and the type of x , we can define   the final total uncertainty score . We consider pre-   dictions for ID instances as the most trustworthy ,   therefore , we define their total uncertainty score   as the rank of their aleatoric score R(U(x),D )   only among known ID instances D={x : x∈   D ∩x∈ X } . Predictions on AID instances are   considered the most error - prone . Their total score   is the rank of the aleatoric score among all known   instances R(U(x),D ) . Lastly , for x/∈ X , we   calculate a linear combination of ranks of aleatoric   and epistemic scores among all known instances :   ( 1−α)R(U(x),D)+αR(U(x),D ) , where α∈   [ 0,1]is a task - specific hyperparameter that depends   on the quality of the softmax classifier , and a num-   ber of training instances . The usage of a mixture   rather than only the epistemic score is justified by   the fact that the generalization capabilities of mod-   els allow them to make meaningful predictions also   in OOD regions , so aleatoric scores to some extent   remain meaningful in these areas .   Thus , the total uncertainty score for xaccording   to HUQ is   U(x ) =       R(U(x),D),∀x∈ X\ X ,   R(U(x),D),∀x∈ X ,   ( 1−α)R(U(x),D)+   αR(U(x),D),∀x/∈ X.   Note that HUQ can plug - in various “ base ” meth-   ods for the estimation of epistemic and aleatoric un-   certainty . Algorithm 1 summarizes the uncertainty   score calculation procedure according to HUQ .   The threshold hyperparameters ( δ , δ ) that   determine x∈ { X| X| X}can be set   using the validation dataset . We set δto be the   epistemic uncertainty score of the instances xwith   the lowest β%epistemic uncertainty on the training   set . Similarly , the hyperparameter δis selected   as the uncertainty score of the most confident in-   stances xfrom topof instances in the training   set with the highest aleatoric uncertainty :   δ = U(β%);δ = U(γ% ) .   5 Experimental Setup   5.1 Models   We experiment with two pre - trained Transformers :   ELECTRA ( “ electra - base - discriminator ” ) ( ClarkAlgorithm 1 : The HUQ algorithm with   MD for epistemic UE and SR for aleatoric   UE .   Input : Target text x ,   Some dataset D={x } ,   Hyperparameters : δ , δ , α   Output : Uncertainty score U(x)U(x)←MD(x);U(x)←SR(x)X← { x : U(x)≤δ};D={x : x∈ D,∩x∈ X}ifx∈ XthenX← { x : U(x ) > δ } ifx∈ Xthen U(x)←R(U(x ) , D ) else U(x)←R(U(x ) , D ) endelse U(x)←   ( 1−α)R(U(x ) , D ) + αR(U(x ) , D)end   et al . , 2020 ) and BERT ( “ bert - base - uncased ” ) ( De-   vlin et al . , 2019 ) with 110 million parameters . We   use a spectral normalization of the weight matrix   in the penultimate linear layer of the classification   heads of the models ( Liu et al . , 2020 ) as it can   be helpful for density - based methods ( Vazhentsev   et al . , 2022 ) . The details on the model hyperparam-   eter optimization procedure and optimal values are   presented in Appendix A. To report the deviation   of results , for each experiment , we train 5 models   with optimal hyperparameters , but different ran-   dom seeds .   5.2 Datasets   There are several tasks that contain highly sub-   jective data , e.g. , toxicity detection , particularly   detecting implicit hate and sentiment analysis .   We conduct experiments on five datasets for tox-   icity detection : P ( Logacheva et al . ,   2022 ) , J with binary labels , a collec-   tion of tweets with annotation of hate and offen-   sive language ( T ; Davidson et al . , 2017 ) ,   TG(Hartvigsen et al . , 2022 ) , and I- H ( ElSherief et al . , 2021 ) ; and three   multi - class classification tasks with high ambiguity :   20 N G ( Lang , 1995 ) , Stanford Senti-   ment Treebank with 5 classes ( SST-5 ; Socher et al . ,11663   2013 ) , and A R ( McAuley and   Leskovec , 2013 ) ( sports and outdoors categories ) .   Note that for TGandI H , im-   plicit hate speech accounts for more than 95 % of   the positive class . The T dataset does not   contain a predefined test set , so we create it by our-   selves . It is constructed from the documents with   high annotator disagreement . In all other cases , we   use original test sets . See Appendix B for dataset   statistics and the analysis of their ambiguity .   To reduce the computational burden of the ex-   periments , the datasets are randomly subsampled .   For training , we sample 10 % from A , I- H , and J ; and 20 % from P- . For evaluation , we sample 10 % from   P , I H , and J .   5.3 Metrics   Selective classification differs from the standard   classification task as low certainty predictions are   rejected and deferred to alternate procedures , e.g. ,   human review . Therefore , for performance evalua-   tion in this task , a special metric is used : area under   the risk coverage curve ( AUC - RC ; El - Yaniv and   Wiener , 2010 ) . Consider all predictions in a dataset   are sorted in ascending order by uncertainty , so we   can discard some % of the most uncertain predic-   tions . The % of predictions remaining after that   is called a coverage rate , and the total loss of the   remaining predictions is called the selective risk .   The RC curve plots a dependence of the selectiverisk from the coverage rate . Finally , the AUC - RC   is a cumulative sum of the selective losses for each   coverage rate . Lower values of AUC - RC indicate   better performance .   5.4 Hyperparameter Selection for HUQ   To find optimal hyperparameters for HUQ , we   select 20 % of the training set as a validation   set and optimize AUC - RC on it , using a grid   search . For each variant of models trained with   different random seeds , we select its specific set   of hyperparameters . The hyperparameter grid is   the following : α∈[0 ; 1 ] with a step size 0.1 ;   δ∈ { 0%,0.05%,0.1%,0.15%,0.2%};δ∈   { 0.9%,0.95%,1.0 % } . The values of δ and   δin % are converted into absolute values , when   we apply them to the test data .   6 Results   In our illustrative example of the two moons dataset   in Figure 1 , the state - of - the - art epistemic UE meth-   ods , MD and RDE , separate the ID area from the   remaining feature space well . However , the middle   area between the two classes is marked with high   confidence , yet for SR and Entropy , this area is   marked as highly uncertain due to the presence of   instances with high aleatoric uncertainty . HUQ ,   which combines aleatoric and epistemic uncer-   tainty , accurately detects both areas of uncertainty ,   thereby overcoming the weaknesses of aleatoric   and epistemic uncertainty individually applied.11664   6.1 HUQ Against its Base Methods   When presenting results , we denote HUQ with a   specific base epistemic UE method as HUQ ( < UE   method > ) . Note that in the main part of the pa-   per , we present the results only with SR as a base   aleatoric UE method . The results for entropy are   very similar to SR and are presented in Appendix E.   HUQ - MD yields significant improvements over   its base methods ( MD and SR ) on 6/8datasets   for both ELECTRA and BERT ( see Table 1 ) . The   largest improvements are achieved on 20 N   G andP , where HUQ reduces   AUC - RC by 13.0 % and 4.9 % ( ELECTRA ) , and   6.6 % and 7.0 % ( BERT ) .   HUQ - DDU produces improvements over DDU   and SR on all 8 datasets for ELECTRA and on 5   datasets for BERT ( see Table 2 ) . For ELECTRA ,   HUQ produces large effects on P and   TGwith 4.6 % and 11 % AUC - RC reduction ,   and with BERT on P with a 10.6 % re-   duction . Interestingly , vanilla DDU is significantly   outperformed by SR for ELECTRA on J .   Applying HUQ addresses this issue , and improves   on the results using SR by 1.8 % .   The results for HUQ - RDE are more ambiguous   than for DDU and SR ( see Table 3 ) . RDE is a   good method for selective classification and is a   hard - to - beat baseline for HUQ . This is because , in   addition to OOD detection RDE computes a co-   variance matrix for each class , thereby making it   suitable for identifying decision boundaries . ForRDE as the base epistemic UE method , HUQ im-   proves results on 4 datasets for ELECTRA and on   6 datasets for BERT . On some datasets , HUQ does   not improve on SR and RDE , e.g. , for T   ( ELECTRA ) and P ( BERT ) . However ,   on others , HUQ shows big improvements in RC-   AUC , e.g. , 18.0 % for 20 N G ( ELEC-   TRA ) and 5.0 % for TG(BERT ) .   Overall , we see that HUQ usually improves upon   its base methods , but in some cases , retains the   same performance . We suspect that the configura-   tions where HUQ does not outperform the baselines   are due to the presence of large covariate shifts be-   tween the training and test data . We discuss this in   detail in Section 7 .   6.2 Overall Comparison   Here , we compare HUQ in selective classification   tasks with various other UE techniques , includ-   ing strong , yet computationally intensive deep en-   sembles ( DE Lakshminarayanan et al . , 2017 ) and   SelectiveNet ( Geifman and El - Yaniv , 2019 ) specif-   ically designed for selective classification , but pre-   viously tested only in computer vision . Figure 2   presents results for the ELECTRA model and Fig-   ure 9 in Appendix D presents results for BERT .   The base epistemic UE methods sometimes can-   not outperform even the weak SR baseline or even   fall behind it by a large margin . It is especially   noticeable for RDE on T andA   reviews and for DDU on J andA re-11665views . This effect might appear because the major-   ity of model mistakes arise from ambiguity rather   than OOD instances , while these methods are better   suitable for OOD detection . On some datasets , it is   very hard to overcome the weak SR baseline . For   example , on I HandA , only   DE confidently outperforms SR .   The results for our implementation of Selec-   tiveNet for text classification models and the de-   tailed experimental setup for this method are pre-   sented in Appendix F. On all considered datasets ,   SelectiveNet never outperforms the SR baseline   and significantly falls behind it .   Variants of HUQ are usually the best or the sec-   ond best after DE . For example , HUQ outperforms   this strong baseline on P , 20 N   G , and SST-5 . However , while DE intro-   duces computational overhead of 400 % , HUQ re-   quires additionally less than 5 % of standard model   inference time ( see Table 15 in Appendix G ) .   6.3 Analyses   Hyperparameter for mixing aleatoric and epis-   temic uncertainty scores in HUQ . When vary-   ing the hyperparameter α , we change the impact   of aleatoric and epistemic uncertainty for the fi-   nal score . Figure 3 reports the impact of αon   theTGdataset . When αis close to 0 , the   performance of the total score approximates the   epistemic uncertainty represented by MD , which is   even worse in terms of AUC - RC than the SR base-   line . When αis close to 1 , we use solely the SR   score in the mixture of uncertainties , while treat-   ing AID , ID , and other instances differently , which   results in better performance compared to vanilla   SR . The best results are obtained when we select   αon the validation set . We can see that obtained   α= 0.5is very close to its optimum on the test set .   HUQ - MD in this case outperforms MD by 10.6 %   and SR by 9.6 % in terms of AUC - RC . This again   illustrates the importance of mixing different types   of uncertainties for selective classification . Similar   charts for other considered datasets are presented   in Figure 6 in Appendix C and for other hyperpa-   rameters in Figures . 7 and 8 in Appendix C.   Qualitative analysis . Table 16 in Appendix H   presents several instances from various datasets ,   as well as model predictions and their normalized   uncertainty scores . The qualitative analysis reveals   that baseline uncertainty scores MD and SR may   be high regardless of whether a classification of   an instance is correct . For example , we see that   four correctly classified instances in P   are marked with high uncertainty by at least one   of the methods . Moreover , MD and SR disagree   with each other : MD yields high uncertainty scores   for the first two instances , whereas SR produces   low uncertainty . For the last two instances , the   pattern is reversed . In all of these cases , the MD   score is not low enough to consider instances as ID .   Therefore , HUQ - MD linearly mixes the SR and   MD scores , producing more balanced results with   moderately low uncertainty , which is consistent   with the fact that classifications are correct .   For the last example from Jigsaw , MD falls be-   low a threshold αobtained for this dataset . Conse-   quently , the example is classified as an ID instance ,   leading to the HUQ - MD score being equal to the   SR score for this particular case . Contrary to MD ,   which yields low uncertainty , high uncertainty of   SR and HUQ correctly indicates a prediction error .   For two examples , HUQ - MD contradict the re-   sults . Specifically , in the third example of ToxiGen   and the second example of Jigsaw , the predictions   are accurate , but uncertainty is moderately high .   This discrepancy arises from both SR and MD be-   ing erroneously high . In such cases , the hybrid   method is unable to correct the uncertainty score .   7 Limitations   While HUQ outperforms individual aleatoric and   epistemic UE methods for most datasets consid-   ered , for some , the effects are negligible . To un-   derstand this pattern , we analyze the difference   between the training and test sets . We generate   latent representations of instances in the datasets11666   using a fine - tuned ELECTRA model and fit a lo-   gistic regression model to discriminate between   train and test sets using these representations as   features . Good performance of the discriminator   indicates a covariance shift between the training   and test data , while bad performance indicates that   instances come from the same distribution .   Table 4 presents F1 scores for this task aligned   with the performance gains of HUQ - DDU in per-   centages over the best method from the pair < SR ,   DDU > . As we can see , high F1 scores often cor-   respond to low values of performance gains ( the   Spearman rank correlation = 0.8 ) . This means that   HUQ is unlikely to provide improvements to the   base methods for the tasks with big covariate shifts .   In our analysis , this is due to prediction mistakes   primarily arising from OOD instances , which are   well - handled by epistemic UE methods .   Visualizing the differences between the datasets   using a t - SNE decomposition of the latent repre-   sentations ( see Figure 4 ) , we can see that for I- HandT , where HUQ does not   provide improvements , some regions of the test   data are not covered by the training set . For P- andTG , on the other hand , the   training dataset completely overlays all regions ofthe test data , and using HUQ improves AUC - RC   on the base methods .   8 Conclusion   In this work , we proposed a hybrid uncertainty   quantification method for selective text classifica-   tion . It combines pre - existing methods for aleatoric   and epistemic uncertainty , providing scores of to-   tal uncertainty . Experimentally , we find that HUQ   usually outperforms in terms of RC - AUC other UE   methods that aim at quantifying only one type of   uncertainty . In real terms , the improved uncertainty   estimation offered by our method affords improved   identification of erroneous predictions for ambigu-   ous text classification tasks .   Although the HUQ method often provides better   results , there are some cases where it is unable to   surpass its base methods and performs at a com-   parable level to them . In our analysis of these   examples , we find that this issue arises when there   is a substantial covariate shift between the training   and test data . In future work , we are planning to   analyze other factors that affect the performance of   UE methods in selective classification tasks . Our   goal is to achieve more consistent and stable im-   provements over baselines across diverse datasets.11667Acknowledgements   We are very grateful to Zeerak Talat for generously   sharing their expertise in toxicity detection , offer-   ing valuable suggestions for text edits , and the help   with the work in general . We thank anonymous   reviewers for their insightful feedback towards im-   proving this paper . The financial support was pro-   vided by the Russian Science Foundation , grant   20 - 71 - 10135 .   Ethical Considerations   The task of uncertainty estimation is one that is   closely tied to the construction of ethical machine   learning methods , as it pertains to the identifica-   tion of potential misclassified instances . For the   task of toxic content classification , uncertainty esti-   mation is particularly important due to the speech   concerns surrounding toxicity detection . Moreover ,   toxicity detection has shown disparate performance   along gendered and racialized lines , uncertainty es-   timation provides an avenue for identifying when a   model may no longer be applied without further im-   provement . However , while uncertainty estimation   may have potential benefits to the tasks under the   umbrella of abusive language detection , approach-   ing misclassifications and uncertainty without an   intersectional ( Crenshaw , 1991 ) lens , and with-   out appropriate measures for deep engagements   with affected communities may propagate issues   of social control , and particularly of enforcing re-   spectability politics of language use . It is therefore   important to understand that uncertainty estimation   can only provide a partial perspective to the chal-   lenges that are faced in abusive language detection .   For instance , data that is mislabeled , or labeled   such that it propagates stereotypes can exhibit low   levels of uncertainty while being undesirable in   relation to the goal of equitable machine learning   methods for content moderation .   References116681166911670A Hyperparameter Values and Hardware Configuration   The optimal hyperparameters are obtained using Bayesian optimization with early stopping . We train a   model on 80 % of the training dataset and validate on the remaining 20 % . The optimal hyperparameters   are selected according to the best accuracy score on the validation set . After the hyperparameters are   selected , we use them to fine - tune the model on the full training set . The hyperparameter grid is the   following :   Learning rate : [ 5e-6 , 6e-6 , 7e-6 , 9e-6 , 1e-5 , 2e-5 , 3e-5 , 5e-5 , 7e-5 , 1e-4 ] ;   Num . of epochs : { n∈N|2≤n≤15 } ;   Batch size : [ 4 , 8 , 16 , 32 , 64 ] ;   Weight decay : [ 0 , 1e-2 , 1e-1 ] .   Table 6 presents the hardware configuration used in experiments . In addition , we provide the ap-   proximate number of GPU hours that are needed for training and evaluating all models for all datasets.11671B Dataset Statistics and Analysis of Ambiguity in Datasets   Figure 5 presents the t - SNE decomposition of latent representations from ELECTRA for various test   sets , where classes are marked with different colors . For TG , J , and I H , we can   see that there is no clear boundary between the “ neutral ” and “ toxic ” classes . For SST-5 andA ,   we can see a smooth transition from the “ very negative ” to the “ very positive ” classes . This illustration   reveals the presence of noisy and ambiguous instances in these datasets .   Table 7 presents the dataset statistics with the number of instances in the test and training sets and the   number of labels.11672C Contribution of Different Components of HUQ   Figures . 6 to 8 present the dependence of the AUC - RC score when varying one of the hyperparameters in   HUQ , while others are fixed to optimal values . According to the results , the most valuable hyperparameter   of HUQ is α . In Figure 6 , for all datasets , except I H , we see that there exists an optimal   value of αdifferent from 0 or 1 that gives the smallest AUC - RC . This means that for these datasets , the   contributions of both types of uncertainties are important . In addition , we can see that our validation   strategy finds ˆαclose to its optimal value .   Hyperparameters δandδcontribute to the final score , but their effect is less significant . Never-   theless , it is crucial to take into account all components of HUQ to achieve the best results.1167311674D Overall Comparison of UE Methods for BERT11675E Additional Experiments with Different Aleatoric Uncertainty Estimation Methods   Tables 8 to 10 present the comparison of AUC - RC when using SR or Entropy as measures of aleatoric   uncertainty in various versions of HUQ . Only for SST-5 , we see a small significant difference between   them : SR is better than Entropy in terms of AUC - RC by 2.5 % in HUQ - MD , by 2.7 % in HUQ - DDU , and   by 1.7 % in HUQ - RDE.11676F Additional Experiments with SelectiveNet   Tables 11 to 13 present the comparison of the SelectiveNet performance ( Geifman and El - Yaniv , 2019 )   with the performance of the SR baseline . The experiments are conducted with the ELECTRA model   on the P , TG , and 20 N G datasets . SelectiveNet is designed only for a   specific coverage , which is fixed during training . Therefore , we select multiple coverage values and for   each value , we fine - tune a separate model , following the standard approach for training SelectiveNet .   Since the coverage for each model is fixed , the AUC - RC metric is not appropriate for evaluation of this   method . Therefore , instead , we use the selective risk for the specified coverage as an evaluation metric .   The results show that for the considered text classification datasets , SelectiveNet significantly falls behind   the standard SR baseline , which is different from the results obtained by Geifman and El - Yaniv ( 2019 ) on   computer vision tasks . The optimal hyperparameters for SelectiveNet are presented in Table 14.1167711678 G Computation Overhead for Uncertainty Estimation   Table 15 presents the computation time for various UE methods . The HUQ - MD during the inference   stage introduces only 0.02 % of overhead in comparison with the MD and less than 5 % of overhead in   comparison with the SR baseline . On the contrary , a deep ensemble of 5 models introduces 400 % of   overhead in comparison with the MD and the SR , which makes it impractical .   H Qualitative Analysis11679ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   8   /squareA2 . Did you discuss any potential risks of your work ?   8   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4 and Appendix A11680 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4 and Appendix A   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   No , we used already preprocessed datasets   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.11681