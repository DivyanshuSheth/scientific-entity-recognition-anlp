  Wenhao Zhu , Jingjing Xu , Shujian Huang , Lingpeng Kong , Jiajun ChenNational Key Laboratory for Novel Software Technology , Nanjing UniversityShanghai AI LaboratoryThe University of Hong Kong   zhuwh@smail.nju.edu.cn , jingjingxu@pku.edu.cn   huangsj@nju.edu.cn , lpk@cs.hku.hk , chenjj@nju.edu.cn   Abstract   Neural machine translation has achieved   promising results on many translation tasks .   However , previous studies have shown that neu-   ral models induce a non - smooth representation   space , which harms its generalization results .   Recently , kNN - MT has provided an effective   paradigm to smooth the prediction based on   neighbor representations during inference . De-   spite promising results , kNN - MT usually re-   quires large inference overhead . We propose   an effective training framework INK to directly   smooth the representation space via adjusting   representations of kNN neighbors with a small   number of new parameters . The new parame-   ters are then used to refresh the whole repre-   sentation datastore to get new kNN knowledge   asynchronously . This loop keeps running un-   til convergence . Experiments on four bench-   mark datasets show that INK achieves average   gains of 1.99 COMET and 1.0 BLEU , outper-   forming the state - of - the - art kNN - MT system   with 0.02×memory space and 1.9 ×inference   speedup .   1 Introduction   Neural machine translation ( NMT ) have achieved   promising results in recent years ( Vaswani et al . ,   2017 ; Ng et al . , 2019 ; Qian et al . , 2021b ) . The   target of NMT is to learn a generalized representa-   tion space to adapt to diverse scenarios . However ,   recent studies have shown that neural networks ,   such as BERT and GPT , induce non - smooth rep-   resentation space , limiting the generalization abil-   ities ( Gao et al . , 2018 ; Ethayarajh , 2019 ; Li et al . ,   2020 ) . In NMT , we also observe a similar phe-   nomenon in the learned representation space where   low - frequency tokens disperse sparsely , even for a   strong NMT model ( More details are described in   Section Experiments ) . Due to the sparsity , manyFigure 1 : The overview of our training loop . We refine   the representation space of an NMT model according   to the extracted kNN knowledge . The new parameters   are then used to refresh the datastore to update kNN   knowledge asynchronously .   “ holes ” could be formed . When it is used to trans-   late examples from an unseen domain , the perfor-   mance drops sharply ( Wang et al . , 2022a , b )   Recently , k - Nearest - Neighbor Machine Trans-   lation ( kNN - MT ) ( Khandelwal et al . , 2021 ) pro-   vides an effective solution to smooth predictions   by equipping an NMT model with a key - value data-   store . For each entry , the value is the target token   andkeyis the contextualized representation at the   target position . It requires a training set to record   tokens and representations . By aggregating near-   est neighbors during inference , the NMT model   can achieve decent translation results ( Khandel-   wal et al . , 2021 ; Zheng et al . , 2021 ; Jiang et al . ,   2022 ) . Despite the success , kNN - MT also brings   new issues with the increasing scale of training   data . Retrieving neighbors from a large datas-   tore ( Wang et al . , 2022a ) at each decoding step   is time - consuming ( Martins et al . , 2022a ; Meng   et al . , 2022 ) . Furthermore , once the datastore is   constructed , representations can not be easily up-   dated , limiting the performance ceiling of kNN-   MT .   Given above strengths and weaknesses of kNN-   MT , we propose to directly smooth the representa-   tion space with a small number of parameters . In   this paper , we propose a training framework INK , 15948to iteratively refine the representation space with   the help of extracted kNN knowledge ( Fig . 1 ) .   Specifically , we adjust the representation distribu-   tion by aligning three kinds of representations with   Kullback - Leibler ( KL ) divergence to train a small   number of adaptation parameters . First , we align   the contextualized representation and its target em-   bedding to keep semantic meanings . Second , we   align the contextualized representations of a target   token and align the extracted kNN contextualized   representations to address the sparsely dispersing   problem . After a training epoch , we refresh the   datastore asynchronously with refined models to   update kNN representations . During inference , we   only load the off - the - shelf NMT model and tune   adaptation parameters .   We conduct experiments on four benchmark   datasets . Experiment results show that our frame-   work brings average gains of 1.99 COMET and 1.0   BLEU . Compared with the state - of - the - art kNN-   MT method ( i.e. Robust kNN - MT ; Jiang et al .   2022 ) , INK achieves better translation performance   with 0.02 ×memory space and 1.9 ×inference   speed . Our contributions can be summarized be-   low :   •We propose a training framework to smooth   the representation space according to kNN   knowledge .   •We devise an inject - and - refine training loop   in our framework . Experiments show that re-   freshing the datastore asynchronously matters .   •Our INK system achieves promising improve-   ments and beats the state - of - the - art kNN - MT   system .   2 Background   This section briefly introduces the working process   ofkNN - MT and the architecture of adapter ( Bapna   and Firat , 2019 ) . For the latter , we will use it to   improve the representation space in our framework .   2.1 kNN - MT   Given an off - the - shelf NMT model Mand train-   ing set C , kNN - MT memorizes training examples   explicitly with a key - value datastore Dand use D   to assist the NMT model during inference .   Memorize representations into datastore   Specifically , we feed training example ( X , Y)in   CintoMin a teacher - forcing manner ( Williamsand Zipser , 1989 ) . At time step t , we record the   contextualized representationhaskeyand the   corresponding target token yasvalue . We then   put the key - value pair into the datastore . In this   way , the full datastore Dcan be created through a   single forward pass over the training dataset C :   where each datastore entry explicitly memorizes   the mapping relationship between the representa-   tionhand its target token y.   Translate with memorized representations   During inference , the contextualized representa-   tion of the test translation context ( X , Y)will be   used to query the datastore for nearest neighbor   representations and their corresponding target to-   kensN={(ˆh,ˆy ) } . Then , the retrieved entries   are converted to a distribution over the vocabulary :   where hdenotes h(X , Y)for short , dmeasures   Euclidean distance and Tis the temperature .   2.2 Adapter   Previous research shows that adapter can be an ef-   ficient plug - and - play module for adapting an NMT   model ( Bapna and Firat , 2019 ) . In common , the   adapter layer is inserted after each encoder and de-   coder layer of M. The architecture of the adapter   layer is simple , which includes a feed - forward layer   and a normalization layer . Given the output vec-   torz∈ Rof a specific encoder / decoder layer ,   the computation result of the adapter layer can be   written as :   where fdenotes layer - normalization , W∈ R ,   W∈ Rare two projection matrices . dis the   inner dimension of these two projections . Bias term   and activation function is omitted in the equation   for clarity . /tildewidezis the output of the adapter layer .   3 Approach : INK   This section introduces our training framework   INK . The key idea of the proposed approach is15949   to use kNN knowledge to smooth the representa-   tion space . The training process is built on a cycled   loop : extracting kNN knowledge to adjust represen-   tations via a small adapter . The updated parameters   are then used to refresh and refine the datastore to   get new kNN knowledge . We define three kinds of   alignment loss to adjust representations , which are   described in Section 3.1 , Section 3.2 , and Section   3.3 . An illustration of the proposed framework is   shown in Figure 2 .   3.1 Align Contextualized Representations and   Token Embeddings   The basic way to optimize the adapter to minimize   the KL divergence between the NMT system ’s pre-   diction probability pand the one - hot golden dis-   tribution p :   where Eis the embedding matrix . wandvdenote   the token embedding and its corresponding token   respectively . hdenotes the contextualized repre-   sentation h(X , Y).ydenotes the target token .   κ(h , w ) = e. Following the widely - accepted   alignment - and - uniformity theory ( Wang and Isola ,   2020 ) , this learning objective aligns the contextual-   ized representation hwith the tokens embedding   of its corresponding target token.3.2 Align Contextualized Representations and   kNN Token Embeddings   Previous research in kNN - MT has shown that the   nearest neighbors in the representation space can   produce better estimation via aggregating kNN   neighbors ( Khandelwal et al . , 2021 ; Zheng et al . ,   2021 ; Yang et al . , 2022 ) . Apart from the refer-   ence target token , the retrieval results provide some   other reasonable translation candidates . Taking the   translation case in Figure 2 as an example , retrieval   results provide three candidate words , where both   “ happens ” and “ occurs ” are possible translations .   Compared with the basic one - hot supervision sig-   nal , the diverse kNN knowledge in the datastore   can be beneficial for building a representation space   with more expressive abilities .   Therefore , we extract kNN knowledge by using   the contextualized representation hto query the   datastore for nearest neighbors N={(ˆh,ˆy ) }   ( illustrated in Fig . 2 ) . For more stable training , we   reformulate the computation process of kNN distri-   bution as kernel density estimation ( KDE ) ( Parzen ,   1962 ) .   Formulation The general idea of KDE is to esti-   mate the probability density of a point by referring   to its neighborhood , which shares the same spirit   withkNN - MT . The computation of kNN distribu-   tion can be written as:15950where κcan be set as any kernel function . Thus ,   Equation 2 can be seen as a special case of Equation   4 by setting κ ( · , · ) = e.   After extracting kNN knowledge , we use it to   smooth the representation space by by minimizing   the KL divergence between the kNN distribution   pand NMT distribution p :   where Ydenotes identical tokens in nearest   neighbors Nandp(¯y)denotes p(y=   ¯y|X , Y)for short . Eis the embedding matrix .   wandvdenote the token embedding and its corre-   sponding token respectively . hdenotes h(X , Y )   for short . κis the kernel function . Following   the widely - accepted alignment - and - uniformity the-   ory ( Wang and Isola , 2020 ) , this learning objec-   tive encourages hto align with the embeddings of   retrieved reasonable tokens , e.g. , “ occurs ” , “ hap-   pens ” .   3.3 Align Contextualized Representations of   the Same Target Token   Although kNN knowledge could provide fruitful   translation knowledge , it is also sometimes noisy   ( Zheng et al . , 2021 ; Jiang et al . , 2022 ) . For exam-   ple , in Figure 2 , the retrieved word “ works ” is a   wrong translation here .   To address this problem , we propose to adjust lo-   cal representation distribution . Specifically , our so-   lution is to optimize the kNN distribution towards   the reference distribution by minimizing the KL   divergence between the gold distribution pand   kNN distribution p. Thanks to the new formu-   lation ( Eq . 4 ) , we can choose kernel function here   to achieve better stability for gradient optimization .   In the end , we find that exponential - cosine kernel   works stably in our framework :   κ(h , h ) = e(5 )   Therefore , the loss function can be written as :   where Nis the retrieved k nearest neigh-   bors . ˆhandˆydenotes the neighbor representa-   tions and the corresponding target token . hde-   notes h(X , Y)for short . Following the widely-   accepted alignment - and - uniformity theory ( Wangand Isola , 2020 ) , this learning objective aligns the   contextualized representation of the same target to-   ken . With this goal , we can make the kNN knowl-   edge less noisy in the next training loop by refresh-   ing the datastore with the updated representations .   3.4 Overall Training Procedure   The combined learning objective To summa-   rize , we adjust representation space via a small   adapter with the combination of three alignment   lossL , L , L. Given one batch of training ex-   amples B={(X , Y ) } , the learning objective is   minimizing the following loss :   where α , βis the interpolation weight . We notice   that , in general , all three learning objective pull   together closely related vectors and push apart less   related vectors in the representation space , which   has an interesting connection to contrastive learn-   ing ( Lee et al . , 2021 ; An et al . , 2022 ) by sharing   the similar goal .   Refresh datastore asynchronously In our train-   ing loop , once the parameters are updated , we re-   fresh the datastore with the refined representation .   In practice , due to the computation cost , we re-   fresh the datastore asynchronously at the end of   each training epoch to strike a balance between   efficiency and effectiveness As the training reaches   convergence , we drop the datastore and only use the   optimized adapter to help the off - the - shelf NMT   model for the target domain translation .   4 Experiments   4.1 Setting   We introduce the general experiment setting in this   section . For fair comparison , we adopt the same set-   ting as previous research of kNN - MT ( Khandelwal   et al . , 2021 ; Zheng et al . , 2021 ; Jiang et al . , 2022 ) ,   e.g. , using the same benchmark datasets and NMT   model . For training INK , we tune the weight α   andβamong { 0.1 , 0.2 , 0.3 } . More implementation   details are reported in the appendix .   Target Domain Data We use four benchmark   German - English dataset ( Medical , Law , IT , Ko-   ran ) ( Tiedemann , 2012 ) and directly use the pre-   processed datareleased by Zheng et al . ( 2021 ) .   Statistics of four datasets are listed in Table 1.15951   NMT Model We choose the winner model(Ng   et al . , 2019 ) of WMT’19 German - English news   translation task as the off - the - shelf NMT model   for translation and datastore construction , which is   based on the big Transformer architecture ( Vaswani   et al . , 2017 ) .   Baselines For comparison , we consider three   kNN - MT systems , which use datastore in different   fashions . We report the translation performance   of the adapter baseline to show the effectiveness   of our training framework . Besides , we report the   translation performance of kNN - KD , which is an-   other work using kNN knowledge to help NMT .   •V - kNN(Khandelwal et al . , 2021 ) , the vanilla   version of k - nearest - neighbor machine trans-   lation .   •A - kNN(Zheng et al . , 2021 ) , an advanced vari-   ants of kNN - MT , which dynamically decides   the usage of retrieval results and achieve more   stable performance .   •R - kNN(Jiang et al . , 2022 ) , the state - of - the-   artkNN - MT variant , which dynamically cali-   brates kNN distribution and control more hy-   perparameters , e.g. temperature , interpolation   weight .   •Adapter ( Bapna and Firat , 2019 ) , adjusting   representation by simply align contextualized   representation and token embeddings .   •kNN - KD ( Yang et al . , 2022 ) , aiming at from-   scratch train a NMT model by distilling kNN   knowledge into it .   Metric To evaluate translation performance , we   use the following two metrics :   •BLEU ( Papineni et al . , 2002 ) , the standard   evaluation metric for machine translation . We   report case - sensitive detokenized sacrebleu.•COMET ( Rei et al . , 2020 ) , a recently pro-   posed metric , which has stronger correlation   with human judgement . We report COMET   score computed by publicly available wmt20-   comet - damodel .   Approximate Nearest Neighbor Search We fol-   low previous kNN - MT studies and use Faissin-   dex ( Johnson et al . , 2019 ) to represent the datastore   and accelerate nearest neighbors search . Basically ,   thekeyfile can be removed to save memory space   once the index is built . But , it is an exception   that R- kNN relies on the keyfile to re - compute ac-   curate distance between query representation and   retrieved representations .   4.2 Main Results   We conduct experiments to explore the following   questions to better understand the effectiveness of   our proposed framework and relationship between   two ways of smoothing predictions :   •RQ1 : Can we smooth the representation   space via small adapter and drop datastore   aside during inference ?   •RQ2 : How much improvement can be brought   by using kNN knowledge to adjust the repre-   sentation distribution ?   •RQ3 : Will together using adapter and datas-   tore bring further improvement ?   INK system achieves the best performance by   smoothing the representation space Table 2   presents the comparison results of different sys-   tems . Due to the poor quality of representation   space , the off - the - shelf NMT model does not per-   form well . The performance of kNN - KD is unsta-   ble , e.g. , it performs poorly on IT dataset . kNN-   MT systems generate more accurate translation .   Among them , R- kNN achieves the best perfor-   mance , which is consistent with previous observa-   tion ( Jiang et al . , 2022 ) . Our INK system achieves   the best translation performance with the least   memory space . Compared with the strongest kNN-   MT system , i.e. R- kNN , INK achieves better per-   formance on three out of four domains ( Medical ,   IT , Koran ) . In average , INK outperforms R- kNN   with an improvement of 4.84 COMET and 0.31   BLEU while occupying 0.02 ×memory space.15952   Representation refinement according to kNN   knowledge brings large performance improve-   ment In Table 2 , compared with the adapter base-   line that simply align the contextualized represen-   tations and word embeddings , INK outperforms   it by 1.99 COMET and 1.00 BLEU in average ,   which demonstrates the effectiveness of adjusting   representation distribution with kNN knowledge .   To better show the effect of INK framework , we   use adapters of different sizes to refine the repre-   sentation space . Figure 3 shows the BLEU scores   and added memory of different systems on four   datasets . We can see that representation - refined sys-   tem occupies much less memory than the datastore-   enhanced system . In general , INK systems locateson the top - right of each figure , which means that   INK achieves higher BLEU scores with less mem-   ory space . In most cases , INK outperforms adapter   with a large margin , which demonstrates the supe-   riority of our training framework .   Jointly applying adapter and datastore can fur-   ther smooth predictions Given the fact that both   INK and datastore can smooth predictions , we take   a step further and explore to use them together   as a hybrid approach . Specifically , on top of our   INK system , we follow the fashion of R- kNN to   use an additional datastore to assist it during infer-   ence . Experiment results are shown in Figure 4 .   On three out of four datasets , we can observe fur-   ther improvements over INK . On the Law dataset,15953   the performance improvement even reaches 4.19   BLEU . On the Medical and IT dataset , the perfor-   mance improvement is 0.71 BLEU and 0.79 BLEU   respectively . Such phenomenon indicates that the   representation space of the NMT model is not fully   refined by the adapter . If a more effective frame-   work can be designed , the benefit of smoothing   representation space will be further revealed . The   results on the Koran dataset is an exception here .   We suggest that it is because of the sparse training   data , which makes it difficult to accurately estimate   kNN distribution during inference .   5 Analysis and Discussion   We conduce more analysis in this section to better   understand our INK system .   INK greatly refines the representation space of   the NMT model Inspired by Li et al . ( 2022 ) , we   evaluate the quality of the representation space bycomputing mean kNN accuracy , which measures   the ratio of k - nearest representations sharing the   same target token with the query representation .   Ideally , all of the representations in a neighbor-   hood should share the same target token . Here ,   we use the contextualized representations from the   unseen development set as the query . For each   query , the nearest representations from the training   set will be checked . Table 3 shows the evaluation   results on medical dataset . INK achieves higher   accuracy than the NMT model consistently . For   low frequency tokens , the representation quality   gap is especially large .   Ablation study To show the necessity of differ-   ent proposed techniques in our INK framework , we   conduct ablation study in this section . In Table 4 ,   we can see that keeping the datastore frozen de-   generates the translation performance most , which   demonstrates the necessity of refreshing datastore   asynchronously during training . Removing either   of the two alignment loss ( LandL ) would cause   the translation performance to decline , which vali-   dates their importance for adjusting the representa-   tion distribution.15954INK enjoys faster inference speed After refin-   ing the representation space , our adapted system no   longer need to querying datastore during inference .   We compare the inference speedof INK and R-   kNN . Considering that decoding with large batch   size is a more practical setting ( Helcl et al . , 2022 ) ,   we evaluate their inference speed with increasing   batch sizes . To make our evaluation results more   reliable , we repeat each experiment three times and   report averaged inference speed . Table 5 shows   the results . As the decoding batch size grows , the   speed gap between the two adapted system be-   comes larger . Our INK can achieve up to 1.9 ×   speedup . Besides , due to the fact that neural param-   eters allows highly parallelizable computation , the   inference speed of INK may be further accelerated   in the future with the support of non - autoregressive   decoding ( Qian et al . , 2021a ; Bao et al . , 2022 ) .   6 Related Work   Nearest Neighbor Machine Translation kNN-   MT presents a novel paradigm for enhancing the   NMT system with a symbolic datastore . However ,   kNN - MT has two major flaws : ( 1 ) querying the   datastore at each decoding step is time consuming   and the datastore occupies large space . ( 2 ) the   noise representation in the datastore can not be   easily updated , which causes the retrieval results to   include noise .   Recently , a line of work focuses on optimizing   system efficiency . Martins et al . ( 2022a ) and Wang   et al . ( 2022a ) propose to prune datastore entries   and conduct dimension reduction to compress the   datastore . Meng et al . ( 2022 ) propose to in - advance   narrow down the search space with word - alignment   to accelerate retrieval speed . Martins et al . ( 2022b )   propose to retrieve a chunk of tokens at a time and   conduct retrieval only at a few decoding steps with   a heuristic rule . However , according to their em - pirical results , the translation performance always   declines after efficiency optimization .   To exclude noise in the retrieval results , Zheng   et al . ( 2021 ) propose to dynamically decide the us-   age of retrieved nearest neighbors with a meta- k   network . Jiang et al . ( 2022 ) propose to dynamically   calibrate the kNN distribution and control more hy-   perparameters in kNN - MT . Li et al . ( 2022 ) propose   to build datastore with more powerful pre - trained   models , e.g. XLM - R ( Conneau et al . , 2020 ) . How-   ever , all of this methods rely on a full datastore   during inference . When the training data becomes   larger , the inference efficiency of these approaches   will becomes worse . Overall , it remains an open   challenge to deploy a high - quality and efficient   kNN - MT system .   Using kNN knowledge to build better NMT mod-   els As datastore stores a pile of helpful transla-   tion knowledge , recent research starts exploring   to use kNN knowledge in the datastore to build a   better NMT model . As an initial attempt , Yang   et al . ( 2022 ) try to from scratch train a better NMT   model by distilling kNN knowledge into it . Dif-   ferent from their work , we focus on smoothing   the representation space of an off - the - shelf NMT   model and enhancing its generalization ability via   a small adapter . Besides , in our devised inject-   and - refine training loop we keep datastore being   asynchronously updated , while they use a fixed   datastore .   7 Conclusion   In this paper , we propose a novel training frame-   work INK , to iteratively refine the representation   space of the NMT model according to kNN knowl-   edge . In our framework , we devise a inject - and-   refine training loop , where we adjust the represen-   tation distribution by aligning three kinds of repre-   sentation and refresh the datastore asynchronously   with the refined representations to update kNN   knowledge . Experiment results on four benchmark   dataset shows that INK system achieves an aver-   age gain of 1.99 COMET and 1.0 BLEU . Com-   pared with the state - of - the - art kNN system ( Robust   kNN - MT ) , our INK also achieves better translation   performance with 0.02 ×memory space and 1.9 ×   inference speed up .   8 Limitation   Despite promising results , we also observe that re-   freshing and querying the datastore during training15955is time - consuming . Our proposed training frame-   work usually takes 3 × ∼ 4×training time . In   future work , we will explore methods to improve   training efficiency . We include a training loop to   dynamically use the latest datastore to inject knowl-   edge into neural networks . However , we still find   that the kNN knowledge still helps the inference   even after our training loops , demonstrating that   there still remains space to improve the effective-   ness of knowledge injection .   Acknowledgement   We would like to thank the anonymous review-   ers for their insightful comments . Shujian Huang   is the corresponding author . This work is sup-   ported by National Science Foundation of China   ( No . 62176120 ) , the Liaoning Provincial Research   Foundation for Basic Research ( No . 2022 - KF-26-   02 ) .   References15956   A Used Scientific Artifacts   Below lists scientific artifacts that are used in our   work . For the sake of ethic , our use of these arti-   facts is consistent with their intended use .   •Fairseq ( MIT - license ) , a sequence modeling   toolkit that allows researchers and developers   to train custom models for translation , sum-   marization and other text generation tasks .   •Faiss ( MIT - license ) , a library for approximate   nearest neighbor search .   B Implementation Details   We reproduce baseline systems with their released   code . We implement our system with fairseq ( Ott   et al . , 2019 ) . Adam is used as the optimizer and   inverse sqrt is used as the learning rate scheduler .   We set 4k warm - up steps and a maximum learning   rate as 5e-4 . We set batch size as 4096 tokens . All   INK systems are trained on a single Tesla A100 .   During inference , we set beam size as 4 and length   penalty as 0.6.15957ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   section 8   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   section 4   /squareB1 . Did you cite the creators of artifacts you used ?   section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   appendix a   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   appendix a   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   section 4   C / squareDid you run computational experiments ?   section 4 , 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   section 4 , 515958 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   section 4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   section 4   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Not applicable . Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.15959