  S. K. Hong and Tae Young Jang   Samsung SDS , ML Research Center   { s.k.hong , tae10.jang}@samsung.com   Abstract   In recent years , NLP has advanced greatly   along with the proliferation of pre - trained lan-   guage models . The pre - trained language mod-   els are also properly adapted to downstream   tasks when there is sufficient labeled data .   However , in real - world applications , we often   encounter the deficiency of labeled data . When   only given a few instances for a new task , ex-   tracting task - aware features from a pre - trained   language model regardless of the adaptation is   a promising alternative . In the study , we pro-   pose a novel embedding transfer method , called   LEA , for leveraging pre - trained language mod-   els with even only few - shot instances . LEA   derives meta - level attention aspects using our   new meta - learning framework . We evaluate   our method on five text classification bench-   mark datasets . The results show that the novel   method robustly provides the competitive per-   formance compared to recent few - shot learning   methods .   1 Introduction   A deficiency of supervised data is often experi-   enced in real - world NLP applications . Few - shot   learning aims to yield an AI - driven NLP model   capable of recognizing unseen tasks using a few   labeled data . Meanwhile , fine - tuning pre - trained   models ( PTMs ) ( Howard and Ruder , 2018 ; Devlin   et al . , 2019 ; Lan et al . , 2019 ; Liu et al . , 2019 ) has   been the most successful approach in recent years   of NLP . Unfortunately , it is still challenging to uti-   lize PTMs ( Lee et al . , 2019 ) in few - shot learning .   To address this subtle problem ( Sun et al . , 2019 ) ,   we propose a meta - knowledge driven self - attentive   embedding transfer method , called LEA ( LEarning-   to - Attend ) , based on a novel meta - learning frame-   work , through which meta - level attention aspects   are derived by encoding how to attend for given   tasks . LEA is an efficient and practical method that   facilitates the utilization of large - sized PTMs in   few - shot learning . There are the two common transfer learning   paradigms in NLP : feature - based transfer ( Cer   et al . , 2018 ) and fine - tuning ( Houlsby et al . , 2019 ) .   Our approach belongs to the feature - based transfer .   LEA includes two key ideas : ( 1 ) construction of   a meta - level attention aspects dictionary and ( 2 ) in-   ference of the task - specific attention aspects upon   the arrival of a new task . The former is a process   by which useful meta - level attention aspects across   tasks are derived based on a particular PTM via   our meta - learning framework . The latter refers to   as a task - adaption process , where a subset of task-   specific attention aspects is inferred by determining   the top- kmost relevant attention aspects from the   meta - level attention aspects dictionary . While LEA   can be applied to a wide variety of downstream   tasks , we demonstrate LEA on few - shot text classi-   fication problems in the paper .   2 Related Work   Few - shot text classification : In ( Geng et al . ,   2019 ) , INDUCTION is proposed to build class-   wise embedding to represent each class using a   particular dynamic routing algorithm coalesced   with meta - learning . In ( Bao et al . , 2019 ) , DS is   introduced to keep track of underlying word distri-   butions across all available classes and to specify   important lexical features for new classes .   Meta - learning : As a metric learning - based   method , ( Snell et al . , 2017 ) suggested a deep neu-   ral network , called a prototype network ( PROTO ) ,   through which class representations are composed   using a learning similarity metric for members   of the same class . In ( Sung et al . , 2018 ) , sim-   ilar to PROTO , a deep neural network , called a   relation network , is proposed to learn a non - linear   distance metric rather than the Euclidean distance .   In addition , LEO ( Rusu et al . , 2018 ) learns a low-   dimensional latent embedding of the model param-   eters such that the classifiers are generated from the   latent space into which the tasks are mapped . Frog-99GNN ( Xu and Xiang , 2021 ) focuses on all query-   support pairs and proposes a multi - perspective ag-   gregation based graph neural network to explicitly   reflect intra - class similarity and inter - class dissimi-   larity .   3 Background   3.1 Problem Setup   Few - shot text classification is a task in which a   classifier must be adapted to accommodate new   classes using only a few labeled examples . In the   literature , this is called a C - way K - shot problem   in which K - labeled examples are given for each   of the Cnumber of classes . In a meta - learning   setting , tasks are divided into a meta - training set   ( S ) , meta - validation set ( S ) , and meta - test set   ( S ) as disjoint sets of classes .   3.2 Model - Agnostic Meta - Learning   Our proposed meta training strategy follows the   overall procedure of optimization - based meta-   learning ( Finn et al . , 2017 ) . For a parametric model   f , MAML seeks to find task - specific parameters   θfor any new task τsampled from a particular dis-   tribution of tasks . For a particular task τ∼p(τ ) ,   the task data Dconsist of DandDduring the   meta - training phase . MAML alternates between   two update processes during meta - training : ( 1 )   task - adaptation and ( 2 ) meta - optimization .   Task adaptation ( or inner update ): Each task   learner updates its own parameters through a gra-   dient descent using the loss evaluated based on its   own training data Dwith the initial parameter θ   given by the outer meta - optimization process . The   task - adaptation process is formulated as in Equa-   tion 1 .   θ←θ−α ▽ L / parenleftbig   f , D / parenrightbig   , ( 1 )   Meta - optimization ( or outer update ): The meta-   learner updates its parameters through a gradient   descent using the loss evaluated by Dwith re-   spect to the task - specific parameters θ . The meta-   optimization process is formulated as in Equation   2 :   θ←θ−β ▽ /summationdisplayL / parenleftig   f , D / parenrightig   , ( 2 )   where Ldenotes a loss function for a task   τ , and the inner and outer updates are applied   through their own standard gradient descent with   fixed learning rates αandβ , respectively , which   are given as hyperparameters .   In the meta - testing phase , the meta - learner pro-   vides the initial parameters for task - specific model   learners . Subsequently , each task learner is indi-   vidually tailored to find the optimal parameters θ   by applying the above task adaptation process . In   this meta - testing , the dataset of task τis given as   D=/parenleftbig   D , D / parenrightbig   .   3.3 Pre - Trained Models   We conducted all experiments with BERT ( Devlin   et al . , 2019 ) and RoBERTa ( Liu et al . , 2019 ) as the   underlying PTMs in the study . Given a text input ,   a dummy token ( CLS ) is added to the beginning of   the input , and another token ( SEP ) is added to the   end of a sentence . The PTMs end up with providing   the corresponding embedding vectors ( i.e. , denoted   as [ CLS ] and [ SEP ] ) for the artificial tokens as well   as embeddings for original tokens for the input text .   For downstream classification tasks , the special   embedding vector [ CLS ] is typically used to make   a prediction as the representative of an text instance .   In this study , the [ CLS ] vector plays an important   role in probing the distinctive properties for an   incoming task . In manufacturing a task - specific   embedding , we especially utilize the token - level   output embeddings of the individual tokens of the   jth text instance under a particular task τ , which   we denote as H= [ h , . . . , h ] . Likewise , the   corresponding [ CLS ] embedding is denoted as c.   4 Proposed Method   The overall architecture of LEA is shown in Figure   1 . It represents our meta learning framework for   the task - specific feature extraction . It is trained in   an end - to - end manner using our proposed meta-   learning strategy . The meta training alternates two100Algorithm 1 Our Proposed Meta - Training   processes : ( 1 ) deriving all valid meta - attention as-   pects across tasks ( namely , meta - optimization ) , and   ( 2 ) choosing a task - specific subset from all the   meta - attention aspects for each task ( called , task   adaptation ) . The high - level operation is described   in Algorithm 1 .   4.1 Meta Attention Aspects Dictionary   In this study , the meta - level knowledge dictionary   maintains all attention aspects derived across tasks   τ∼p(τ ) . The concept was inspired by ( Lin et al . ,   2017 ) . The meta - attention aspects in the dictionary   are established throughout the meta - optimization   process during which it seeks to learn how to attend   according to the distribution of tasks . Herein , we   define a matrix W∈Ras the meta - level   attention aspect dictionary . In addition , Aand   uare the total number of attention aspects and   dimension of the attention aspect , respectively .   4.2 Top - kAttention Aspects Selection through   Gate Network   When a novel task τis given , its related attention   aspects , denoted by W , are selectively obtained   by assigning the corresponding weights to mem-   bers of the meta - level attention aspects Win the   task - adaptation process . Here , W∈Rindi-   cates the selected kattention aspects of the task τ . Note that kandKare different in that the former   is the number of topmost relevant attention aspects ,   whereas the latter , indicates as K - shot , refers to   the number of samples in few - shot learning . To do   so , we assess the relevance of the task among the   meta - level attention aspects W. First , each task is   fed into an encoding process , which is formulated   as follows :   where eis the representative embedding for the   particular class nunder a given task τ , findi-   cates the relation network ( Sung et al . , 2018 ) , and   fis an encoder network that transforms the dele-   gate embedding [ CLS ] ( denoted as cfor the case   of the jth text instance of a specific task τ ) of a   text instance in PTMs ( Devlin et al . , 2019 ; Lan   et al . , 2019 ; Liu et al . , 2019 ) . As a result , the class   embedding eis enforced to encode the pairwise   relationship with other classes .   Using the aforementioned class embedding , we   attempt to selectively ( i.e. , top- k ) collect task-   specific attention aspects for a given task by em-   ploying a gating mechanism ( Shazeer et al . , 2017 ) .   The gating output vector is calculated through the   following formulation :   g = softmax ( G(e;W , W , k)),(4 )   where gis the gating output vector whose num-   ber of dimensions must be the same as the size of   the meta - attention - aspects dictionary . The gating   process Gproduces a sparse output vector by be-   ing parameterized with { W∈R , W∈   R , k } , where the remaining values except   for the kelements are forced to become zeros , and   the top- kweights are finally generated through a   softmax function .   As a result , we can extract the top- ktask - specific   attention aspects for the task τ . This is formulated   as follows :   W= ( ( W)g ) . ( 5 )   4.3 Task - Specific Self - Attentive Document   Embedding   Here , we perform the self - attentive feature extrac-   tion using the aforementioned top- ktask - specific   attention aspects for a task . We then apply it into   the generation of document embeddings for text101   classification . For a text input , we utilize the corre-   sponding embedding vectors for the individual to-   kens , which are denoted as H= [ h , . . . , h ]   for the jth text example of the task τ . This is   formulated as follows :   E = WH , ( 6 )   where E∈Ris the self - attentive document   embedding of the jth input of the task τ , and   H∈Ris a set of token embedding vectors   for the jth instance with Ltokens in the task τ .   For the text classification , we sum Ecolumn-   wise and then feed it into a fully connected neural   network ( denoted as FC ) with the parameters   θ , which are optimized in the task - adaptation step   to make the final predictions .   4.4 Meta - Training Objectives   As noted in Algorithm 1 , LEA alternates the fol-   lowing two update steps : ( 1 ) task adaptation ( or   inner - update ) and ( 2 ) meta - optimization ( or outer-   update ) . The former proceeds as follows :   θ←θ−α ▽ L / parenleftig   f , E / parenrightig   , ( 7 )   where θindicates the task model parameters , and   Lis the classification loss by relying on Ede-   rived from D.   During the meta - optimization step , the groups   of parameters { W , W , W , θ , θ , θ , θ}are   trained in the outer loop with D. This is for-   mulated as follows :   ϕ←ϕ−β ▽ /summationdisplayL / parenleftig   f , E,/parenrightig   ) + λ·Ω(8 )   where , Ω , as a regularization , includes the term   that encourages all attention aspects to have equal   importance ( Shazeer et al . , 2017 ) , and λis its asso-   ciated coefficient as usual .   5 Experimental Results   We evaluated LEA on five text datasets — 20 News-   group(Lang , 1995 ) , Huffpost headline(Misra and   Grover , 2021 ) , Reuters-21578(Lewis . , 1997 ) , RCV-   1(Lewis et al . , 2004 ) , and Amazon product reviews   ( He and McAuley , 2016 ) — and compared it with   current state - of - art methods . We conducted two   different experiments : 5 - way 1 - shot and 5 - way 5-   shot all over datasets . The details of the datasets   are introduced in Appendix A.1 .   5.1 Baselines   In this experiment , we evaluate and compare LEA   with six state - of - art methods as follows : Here ,   MAML ( Finn et al . , 2017 ) denotes the represen-   tative optimization - based meta - learning algorithm ,   PROTO ( Snell et al . , 2017 ) indicates the proto-   type network , LEO ( Rusu et al . , 2018 ) denotes the   meta - learning algorithm using latent embedding op-   timization , INDUCTION indicates the induction   network ( Geng et al . , 2019 ) , DS(Bao et al . , 2019 )   denotes few - shot text classification algorithm using   the underlying word distributions , and Frog - GNN   ( Xu and Xiang , 2021 ) denotes the multi - perspective   aggregation based graph neural network .   5.2 Overall Performance   We performed all experiments on a frozen   BERT ( Devlin et al . , 2019 ) as a represen-102   tative PTM for LEA and all baselines . As in LEA ,   DSis given the [ CLS ] embedding and the token em-   beddings from BERT ’s last layer , whereas the other   algorithms used the [ CLS ] embedding of BERT .   For the comparison with Frog - GNN , we referred   to the reported results from ( Xu and Xiang , 2021 ) .   We additionally applied LEA on RoBERTa   ( Liu et al . , 2019 ) and fastText ( Bojanowski et al . ,   2017 ) to verify the applicability of LEA . All perfor-   mance scores are reported as the average for three   repetitions .   As shown in Table 1 and 2 , LEA exhibits the   competitive performance in both the 5 - way 1 - shot   and 5 - way 5 - shot , compared to the state - of - the - arts   for all the datasets . Namely , the results demonstrate   that LEA quickly recognizes how to attend for new   tasks using the established meta - attention aspects   and provides a robust performance in few - shot text   classification problems .   5.3 Hyperparameter Study : Effect of the   Number of Top- kAttention Aspects   We also investigate the impact of the number ( i.e. ,   k ) of task - specific attention aspects . This spe-   cific study was conducted on the same frozen   BERT as the underlying PTM with the 5 - way   5 - shot experiment for the all datasets . We fixed   the size of the meta attention aspects dictionary to   150 and measured the performances by gradually   scaling the kup to 1 , 10 , 20 , 30 , 50 , 75 , 150 . As   shown in Figure 2 , all the datasets exhibit their   best performance when setting the top- kattention   aspects to 20 . This empirical result indicates that   each task derives its optimal document embeddingby referring only to the most relevant subset rather   than exploiting all meta - level attention aspects .   5.4 Task - Specific Document Embedding   Visualization   In addition , we plots the task - specific document   embeddings and observe the relationships among   classes on 20 Newsgroups dataset . To qualitatively   characterize the task - specific document embedding   space , we split 20 Newsgroup into seven top - level   domains , that is , ‘ atheism ’ , ‘ computer ’ , ‘ for - sale ’ ,   ‘ recreation ’ , ‘ science ’ , ‘ religion ’ , and ‘ talk ’ and pro-   jected them via t - SNE as shown in Figure 3a . Fig-   ure 3b shows the relationships between the ‘ recre-   ation ’ domain composed of four classes and the   rest on the space . Figure 3c shows the relationships   between the four classes of the ‘ science ’ domain   and the others on the space . These plots demon-   strate that LEA produces a structured task - specific   embedding space after our task - adaptation step .   6 Conclusion   We hypothesized that a type of task - specific self-   attentive mechanism might improve few - shot learn-   ing performance , especially when it is prohibitive   to fine - tune a large - sized PTM . We have attempted   to design a novel embedding transfer method for   deriving a meta - level attention aspects dictionary   to enable a new task to simply borrow the most   relevant attention aspects from the dictionary . As a   result , we proposed a novel meta - learning frame-   work for the learning - to - attend and showed that   LEA is an effective method that facilitates the uti-   lization of large - sized PTMs in few - shot learning.103References104   A Appendix   A.1 Datasets   We introduce the datasets and the split ( i.e. ,   train / val / test ) which had been maintained in our   experiments .   20 Newsgroups is a collection of discourses in   newsgroup posts for 20 topics ( Lang , 1995 ) .   Huffpost Headlines is a collection of news head-   lines published in the Huffington Post from 2012   to 2018 ( Misra and Grover , 2021 ) . It is composed   of 41 topics .   Reuters-21578 is composed of documents that   appeared on the Reuters newswire in 1987 ( Lewis . ,   1997 ) . In addition , we use the ApteMod version   and discard documents with more than one label to   avoid ambiguity , and thus 31 classes remain .   RCV-1 is a set of newswire stories published by   Reuters journalists from 1996 to 1997 ( Lewis et al . ,   2004 ) and comprises 71 topic classes .   Amazon data is a real - world dataset collected   from Amazon.com as a set of customer reviews   from 24 types of product categories(He and   McAuley , 2016 ) . Our goal is to match reviews   to their own corresponding product categories .   To train and evaluate the models , we divided   each of the aforementioned datasets into a meta-   training set ( S ) , meta - validation set ( S ) , and   meta - test set ( S ) as disjoint sets of classes   within the experimental setting . In this work , we   used the same split of classes as in ( Bao et al . , 2019 )   for the Huffpost headline(Misra and Grover , 2021 ) ,   Reuters-21578(Lewis . , 1997 ) , and RCV-1(Lewis   et al . , 2004 ) datasets . Hence , the Huffpost headline   is divided into 20 , 5 , and 16 disjoint classes for   meta - training , validation , and test sets .   In terms of Reuters-21678 , 15 , 5 , and 11 disjoint   classes are used for meta - training / validation / test   sets and 37 , 10 , and 24 disjoint classes for RCV-1 .   In Amazon product data , we split the data using   rules in ( Bao et al . , 2019 ) , and its training and   validation sets are used for meta - training set . As   a result , Amazon product data is divided into 15 ,   and 9 disjoint classes for meta - training and test   sets and meta - validation set is not used in Amazon   product data . For the 20 Newsgroup dataset , we   randomly selected 20 topic classes , and the meta-   training set , meta - validation set , and meta - test set   contained 10 , 5 , and 5 disjoint classes , respectively .   We summarize the above information in Table 4 .   A.2 Implementation Details   We share the breakdown of LEA ’s implementa-   tion . In the encoding process of our experiments ,   the768 - dimensional [ CLS ] vector , which is of the   same size of the output of the pre - trained BERT-   base - uncased , is linearly transformed through f   into a 300 - dimensional vector . The relation net-   work , fis composed of two - layers neural net-   work with ReLU activation and input size is two   times of encoder outputs and the size of output   is the number of meta - attention - aspects , i.e. , 150.105   The gating network , Wis linear transformation   and its size is the number of meta - attention - aspects .   For each task classifier , that is , f , it is designed   as single - layer fully connected neural network . We   set the size to 150 for the meta - attention - aspects   dictionary , and importantly fixed the number of   top - kattention aspects to 20 . Table 3 summarizes   the above model parameters .   A.3 Training Details and Hyperparameter   Tuning   In our work , we train all experiments on a single   NVIDIA A100 32 G GPU . During the meta - training   process , we sampled four tasks with 15 queries   fromS , and it leads to performing task adaptation   four times per each meta - optimization update and   early stop when the validation loss fails to improve   for 20 steps . In validation and test process , we   sampled 30 tasks with 15 queries from Sand   S , and only performed task adaptation using   K - shots . After that , the performance of the adapted   task model is obtained using queries . We used   the Adam optimizer with learning rates of 0.1and   0.001 in the inner and outer updates , that is , αand   βin , respectively . In addition , the coefficient λ   of the regularization term was set as 0.0001 . We   summarize the hyperparameters in Table 5 .   A.4 Case Study : Visualization of Attention   Weights on Text   Herein , we visualize the heatmaps in some cases to   investigate how to assign attention weights to text .   Figure 4b demonstrates a termination of stock salepact , and Figure 4a shows a company growth in   terms of consumer products . These were extracted   under the Corporate and Industrial topic in the   RCV-1 dataset and some seminal words such as   “ agreement ” , “ contractual ” and “ receivership ” are   highlighted to appear in the topic . Figure 4c shows   that the Turkish market was closed related to the   Market topic , and its relevant words such as “ Turk-   ish , ” “ markets , ” and “ closed ” are highly attended   as expected . Figure 4d talks about the authority of   platinum and gold coins under the Economics topic ,   and the words “ US , ” “ Mint , ” “ authority , ” “ gold , ”   “ platinum , ” and “ coin ” are hence highlighted . As   shown in these cases , LEA properly captures im-   portant words under a certain topic and assigns   attention weights to a given text.106