  Jungsoo ParkSewon MinJaewoo Kang   Luke ZettlemoyerHannaneh HajishirziKorea UniversityUniversity of WashingtonAllen Institute of AI   { jungsoo_park,kangj}@korea.ac.kr   { sewon,lsz,hannaneh}@cs.washington.edu   Abstract   Despite signiﬁcant interest in developing gen-   eral purpose fact checking models , it is chal-   lenging to construct a large - scale fact veriﬁ-   cation dataset with realistic real - world claims .   Existing claims are either authored by crowd-   workers , thereby introducing subtle biases that   are difﬁcult to control for , or manually veriﬁed   by professional fact checkers , causing them to   be expensive and limited in scale . In this paper ,   we construct a large - scale challenging fact ver-   iﬁcation dataset called FVIQ , consisting of   188k claims derived from an existing corpus   of ambiguous information - seeking questions .   The ambiguities in the questions enable auto-   matically constructing true and false claims   that reﬂect user confusions ( e.g. , the year of   the movie being ﬁlmed vs. being released ) .   Claims in FVIQ are veriﬁed to be natural ,   contain little lexical bias , and require a com-   plete understanding of the evidence for veriﬁ-   cation . Our experiments show that the state-   of - the - art models are far from solving our new   task . Moreover , training on our data helps   in professional fact - checking , outperforming   models trained on the widely used dataset   FEVER or in - domain data by up to 17 % abso-   lute . Altogether , our data will serve as a chal-   lenging benchmark for natural language under-   standing and support future progress in profes-   sional fact checking .   1 Introduction   Fact veriﬁcation , the task of verifying the factuality   of the natural language claim , is an important NLP   application ( Cohen et al . , 2011 ) and has also been   used to evaluate the amount of external knowledge   a model has learned ( Petroni et al . , 2021 ) . How-   ever , it is challenging to construct fact veriﬁcation   data with claims that contain realistic and implicit   misinformation . Crowdsourced claims from prior   work such as FEVER ( Thorne et al . , 2018a ) areFigure 1 : An example of a refute claim on FVIQ ,   constructed using ambiguity in the information - seeking   question , e.g. , through a crossover of the year of the   ﬁlmbeing released andbeing ﬁlmed .   written with minimal edits to reference sentences ,   leading to strong lexical biases such as the overuse   of explicit negation and unrealistic misinformation   that is less likely to occur in real life ( Schuster   et al . , 2019 ) . On the other hand , data constructed   by professional fact - checkers are expensive and are   typically small - scale ( Hanselowski et al . , 2019 ) .   In this paper , we show it is possible to use   information - seeking questions ( Kwiatkowski et al . ,   2019 ) and their ambiguities ( Min et al . , 2020 ) to   construct a large - scale , challenging , and realistic   fact veriﬁcation dataset . Information - seeking ques-   tions are inherently ambiguous because users do   not know the answers to the questions they are   posing . For example , in Figure 1 , the question is   ambiguous because the ﬁlming of the movie and   the release of the movie can both be seen as the   creation time .   We introduce a new dataset FVIQ — FAct   Veriﬁcation derived from Information - seeking   Questions , which uses such ambiguities to gen-   erate challenging fact veriﬁcation problems . For   instance , the claim in Figure 1 requires the model   to identify that the movie released in 2001 is in   fact ﬁlmed in 2000 and to return refute . Like   this , claims generated through the crossover of the   disambiguation of information - seeking questions5154are likely to contain misinformation that real users   are easily confused with . We automatically gen-   erate such claims by composing valid and invalid   question - answer pairs and transforming them into   textual claims using a neural model . The data is   further augmented by claims from regular question-   answer annotations .   In total , FVIQ consists of 188k claims . We   manually veriﬁed a subset of claims to ensure   that they are as natural as human - written claims .   Our analysis shows that the claims have signif-   icantly lower lexical bias than existing crowd-   sourced claims ; claims involve diverse types of   distinct entities , events , or properties that are se-   mantically close , being more realistic and harder   to verify without a complete understanding of the   evidence text .   Our experiments show that a model with no   background knowledge performs only slightly bet-   ter than random guessing , and the state - of - the - art   model achieves an accuracy of 65 % , leaving signif-   icant room for improvement . Furthermore , training   onFVIQ improves the accuracy of veriﬁcation of   claims written by professional fact checkers , out-   performing models trained on the target data only   or pretrained on FEVER by up to 17 % absolute .   Together , our experiments demonstrate that FVIQ   is a challenging benchmark as well as a useful re-   source for professional fact checking .   To summarize , our contributions are three - fold :   1.We introduce FVIQ , a fact veriﬁcation   dataset consisting of 188k claims . By lever-   aging information - seeking questions and their   natural ambiguities , our claims require the   identiﬁcation of entities , events , or properties   that are semantically close but distinct , mak-   ing the fact veriﬁcation problem very chal-   lenging and realistic .   2.Our experiments show that the state - of - the - art   fact veriﬁcation models are far from solving   FVIQ , indicating signiﬁcant room for im-   provement .   3.Training on FVIQ signiﬁcantly improves the   veriﬁcation of claims written by professional   fact checkers , indicating that FVIQ can sup-   port progress in professional fact checking .   2 Related Work   Fact veriﬁcation Fact veriﬁcation is crucial for   real - world applications ( Cohen et al . , 2011 ) andas a benchmark to evaluate the knowledge in a   model ( Petroni et al . , 2021 ) .   One line of work has studied professional fact   checking , dealing with claims collected by profes-   sional fact checkers in speciﬁc domains ( Vlachos   and Riedel , 2014 ; Ferreira and Vlachos , 2016 ; Au-   genstein et al . , 2019 ; Hanselowski et al . , 2019 ) .   While such data contains realistic claims that have   occurred in the real world , it is expensive to con-   struct as it requires labor from professional fact   checkers . Moreover , it is less suitable as a bench-   mark due to lack of a standard evidence corpus   such as Wikipediaand ambiguities in labels .   Other fact veriﬁcation datasets are collected   through crowdsourcing ( e.g. , FEVER ( Thorne   et al . , 2018a ) and its variants ( Thorne et al . , 2018b ;   Thorne and Vlachos , 2019 ) ) by altering a word or   negating the reference text to intentionally make   true or false claims . This process leads to large-   scale datasets but with strong artifacts and unre-   alistic claims ( Schuster et al . , 2019 ; Thorne and   Vlachos , 2019 ; Eisenschlos et al . , 2021 ) . Conse-   quently , a trivial claim - only baseline with no ev-   idence achieves near 80 % ( Petroni et al . ( 2021 ) ,   veriﬁed in Section 4.1 ) . While more recent work   proposes new crowdsourcing methods that alleviate   artifacts ( Schuster et al . , 2021 ; Eisenschlos et al . ,   2021 ) , their claims are still written given particular   evidence text , being vulnerable to subtle lexical   biases that can be hard to explicitly measure .   We construct a fact veriﬁcation dataset from   highly ambiguous information - seeking questions .   Our claims have signiﬁcantly less lexical bias than   other crowdsourced ones ( Figure 3 ) , contain real-   istic misinformation that people are likely to be   confused about ( Table 4 ) , and are challenging to   current state - of - the - art models ( Section 4.1 ) . More-   over , training a model on our data improves profes-   sional fact checking ( Section 4.2 ) .   QA to Veriﬁcation Task Prior work has also   used QA data to create entailment or fact veriﬁ-   cation benchmarks . Most make use of synthetic or   annotated questions ( Demszky et al . , 2018 ; Jiang   et al . , 2020 ; Pan et al . , 2021 ; Chen et al . , 2021)5155   while we use questions posed by real users to re-   ﬂect confusions that naturally occur while seeking   information . Thorne et al . ( 2021 ) use information-   seeking questions , by converting yes / no questions   tosupport /refute claims , but at a small scale   and with unambiguous questions . Instead , our   work uses large - scale information - seeking ques-   tions ( with no restriction in answers ) to claims .   We are also unique in using highly ambiguous QA   pairs to obtain claims that are more challenging   to verify and have signiﬁcantly fewer lexical cues   ( quantitative comparisons in Section 3.3 ) .   3 Data   3.1 Data Construction   We construct FVIQ — FActVeriﬁcation derived   from Information - seeking Questions , where the   model is given a natural language claim and pre-   dicts support orrefute with respect to the   English Wikipedia . The key idea to construct the   data is to gather a set of valid and invalid question-   answer pairs ( Section 3.1.2 ) from annotations of   information - seeking questions and their ambigui-   ties ( Section 3.1.1 ) , and then convert each question-   answer pair ( q;a)to a claim ( Section 3.1.3 ) . Fig-   ure 2 presents an overview of this process .   3.1.1 Data Sources   We use QA data from Natural Questions ( NQ ,   Kwiatkowski et al . ( 2019 ) ) and AmbigQA ( Min   et al . , 2020 ) . NQ is a large - scale dataset consist-   ing of the English information - seeking questions   mined from Google search queries . AmbigQA pro-   vides disambiguated question - answer pairs for NQ   questions , thereby highlighting the ambiguity that   is inherent in information - seeking questions . Givenan ambiguous question , it provides a set of multi-   ple distinct answers , each paired with a new disam-   biguated question that uniquely has that answer .   3.1.2 Composing Valid and Invalid QA Pairs   FVIQ is constructed from ambiguous questions   and their disambiguation ( Aset ) and is further aug-   mented by using unambiguous question - answer   pairs ( Rset ) .   From ambiguous questions ( Aset ) We use the   data consisting of a set of ( q;fq;ag;fq;ag ) ,   whereqis an information seeking question that   hasa;aas multiple distinct answers.qand   qare disambiguated questions for the answers a   anda , i.e. ,qhasaas a valid answer and a   as an invalid answer . We use ( q;a)and(q;a )   as valid question - answer pairs , and ( q;a)and   ( q;a)as invalid question - answer pairs .   This data is particularly well suited to fact check-   ing because individual examples require identiﬁca-   tion of entities , events , or properties that are seman-   tically close but distinct : the fact that a user asked   an ambiguous question qwithout realizing the dif-   ference between ( q;a)and(q;a)indicates that   the distinction is non - trivial and is hard to notice   without sufﬁcient background knowledge about the   topic of the question .   From regular questions ( Rset ) We use the QA   data consisting of a set of ( q;a ): an information-   seeking question qand its answer a. We then ob-   tain an invalid answer to q , denoted as a , from   an off - the - shelf QA model for which we use the   model from Karpukhin et al . ( 2020)—DPR fol-   lowed by a span extraction model . We choose a5156   with heuristics to obtain hard negatives but not the   false negative ; details provided in Appendix A. We   use(q;a)and(q;a)as a valid and an invalid   question - answer pair , respectively .   We can think of ( q;a)as ahard negative pair   chosen adversarially from the QA model . This   data can be obtained on a much larger scale than   the A set because annotating a single valid answer   is easier than annotating disambiguations .   3.1.3 Transforming QA pairs to Claims   We transform question - answer pairs to claims by   training a neural model which maps ( q;a)to a   claim that is support if and only if ais a valid   answer toq , otherwise refute . We ﬁrst manually   convert 250 valid and invalid question - answer pairs   obtained through Section 3.1.2 to claims . We then   train a T5 - 3B model ( Raffel et al . , 2020 ) , using   150 claims for training and 100 claims for valida-   tion . The model is additionally pretrained on data   from Demszky et al . ( 2018 ) , see Appendix A.   3.1.4 Obtaining silver evidence passages   We obtain silver evidence passages for FVIQ by   ( 1 ) taking the question that was the source of the   claim during the data creation ( either a user ques-   tion from NQ or a disambiguated question from   AmbigQA ) , ( 2 ) using it as a query for TF - IDF over   the English Wikipedia , and ( 3 ) taking the top pas-   sage that contains the answer . Based on our manual   veriﬁcation on 100 random samples , the precision   of the silver evidence passages is 70 % . We provide   silver evidence passages primarily for supporting   training of the model , and do not explicitly evaluate   passage prediction ; more discussion in Appendix A.   Future work may use human annotations on top of   our silver evidence passages in order to further im-   prove the quality , or evaluate passage prediction .   3.2 Data Validation   In order to evaluate the quality of claims and la-   bels , three native English speakers were given 300   random samples from FVIQ , and were asked   to : ( 1 ) verify whether the claim is as natural as   a human - written claim , with three possible ratings   ( perfect , minor issues but comprehensible , incom-   prehensible ) , and ( 2 ) predict the label of the claim   ( support orrefute ) . Validators were allowed   to use search engines , and were encouraged to use   the English Wikipedia as a primary source .   Validators found 80.7 % of the A set and 89.3 %   of the R set to be natural , and 0 % to be incompre-   hensible . The rest have minor grammatical errors   or typos , e.g. , missing “ the ” . In most cases the   errors actually come from the original NQ ques-   tions which were human - authored , indicating that   these grammatical errors and typos occur in real   life . Lastly , validators achieved an accuracy of   95.0 % ( 92.7 % of A and 97.3 % of R ) when evalu-   ated against gold labels in the data — this indicates   high - quality of the data and high human perfor-   mance . This accuracy level is slightly higher than   that of FEVER ( 91.2 % ) .   3.3 Data Analysis   Data statistics for FVIQ are listed in Table 1 . It   has 188k claims in total , with balanced support   andrefute labels . We present quantitative and   qualitative analyses showing that claims on FVIQ   contain much less lexical bias than other crowd-   sourced datasets and include misinformation that   is realistic and harder to identify.5157   Comparison of size and claim length Table 2   compares statistics of a variety of fact veriﬁcation   datasets : S ( Hanselowski et al . , 2019 ) , S-   FACT ( Wadden et al . , 2020 ) , FEVER ( Thorne   et al . , 2018a ) , FM2 ( Eisenschlos et al . , 2021 ) ,   BQ - FV ( Thorne et al . , 2021 ) and FVIQ .   FVIQ is as large - scale as FEVER , while its dis-   tributions of claim length is much closer to claims   authored by professional fact checkers ( S   andSFACT ) .FM2 is smaller scale , due to difﬁ-   culty in scaling multi - player games used for data   construction , and has claims that are slightly longer   than professional claims , likely because they are   intentionally written to be difﬁcult . BQ - FV is   smaller , likely due to relative difﬁculties in collect-   ing naturally - occurring yes / no questions .   Lexical cues in claims We further analyze lexi-   cal cues in the claims on FEVER , FM2 , BQ-   FVandFVIQ by measuring local mutual infor-   mation ( LMI ; Schuster et al . ( 2019 ) ; Eisenschlos   et al . ( 2021 ) ) . LMI measures whether the given   bigram correlates with a particular label . More   speciﬁcally , LMI is deﬁned as :   LMI ( w;c ) = P(w;c)logP(w;c )   P(w)P(c ) ;   wherewis a bigram , cis a label , and P()are   estimated by counting ( Schuster et al . , 2019 ) .   The distributions of the LMI scores for the top-   100 bigrams are shown in Figure 3 . The LMI scores   ofFVIQ are signiﬁcantly lower than those of   FEVER , FM2 , and BQ - FV , indicating that   FVIQ contains signiﬁcantly less lexical bias .   Tables 3 shows the top six bigrams with the high-   est LMI scores for FEVER andFVIQ . As high-   lighted , all of the top bigrams in refute claims   ofFEVER contain negative expressions , e.g. , “ is   only ” , “ incapable of ” , “ did not ” . In contrast , the   top bigrams from FVIQ do not include obvious   negations and mostly overlap across different la-   bels , strongly suggesting the task has fewer lexical   cues . Although there are still top bigrams from   FVIQ causing bias ( e.g. , related to time , such as   ‘ on October ’ ) , their LMI values are signiﬁcantly   lower compared those from other datasets .   Qualitative analysis of the refute claims We   also analyzed 30 randomly sampled refute   claims from FVIQ andFEVER respectively . We   categorized the cause of misinformation as detailed   in Appendix B , and show three most common cate-   gories for each dataset as a summary in Table 4 .   OnFVIQ , 60 % of the claims involve entities ,   events or properties that are semantically close ,   but still distinct . For example , they are speciﬁed   with conjunctions ( e.g. , “ was foreign minister ” and   “ signed the treaty of versailles from germany ” ) , or   share key attributes ( e.g. , ﬁlms with the same ti-   tle ) . This means that relying on lexical overlap   or partially understanding the evidence text would   lead to incorrect predictions ; one must read the   full evidence text to realize that the claim is false .   Furthermore , 16.7 % involve events , e.g. , from ﬁl-   ing for bankruptcy for the ﬁrst time to completely   ceasing operations ( Table 4 ) . This requires full un-   derstanding of the underlying event and tracking of   state changes ( Das et al . , 2019 ; Amini et al . , 2020 ) .   The same analysis on FEVER conﬁrms the ﬁnd-   ings from Schuster et al . ( 2019 ) ; Eisenschlos et al.5158   ( 2021 ) ; many of claims contain explicit negations   ( 30 % ) and antonyms ( 13 % ) , with misinformation   that is less likely to occur in the real world ( 20 % ) .   4 Experiments   We ﬁrst evaluate state - of - the - art fact veriﬁcation   models on FVIQ in order to establish baseline   performance levels ( Section 4.1 ) . We then conduct   experiments on professional fact - checking datasets   to measure the improvements from training on   FVIQ ( Section 4.2 ) .   4.1 Baseline Experiments on FVIQ   4.1.1 Models   We experiment with two settings : a zero - shot setup   where models are trained on FEVER , and a stan-   dard setup where models are trained on FVIQ .   ForFEVER , we use the KILT ( Petroni et al . , 2021 )   version following prior work ; we randomly split the   ofﬁcial validation set into equally sized validation   and test sets , as the ofﬁcial test set is hidden . All models are based on BART ( Lewis et al . ,   2020 ) , a pretrained sequence - to - sequence model   which we train to generate either support or   refute . We describe three different variants   which differ in their input , along with their accu-   racy on FEVER by our own experiments .   Claim only BART takes a claim as the only input .   Although this is a trivial baseline , it achieves an   accuracy of 79 % on FEVER .   TF - IDF + BART takes a concatenation of a claim   andkpassages retrieved by TF - IDF from Chen   et al . ( 2017 ) . It achieves 87 % on FEVER . We   choose TF - IDF over other sparse retrieval meth-   ods like BM25 ( Robertson and Zaragoza , 2009 )   because Petroni et al . ( 2021 ) report that TF - IDF   outperforms BM25 on FEVER .   DPR + BART takes a concatenation of a claim   andkpassages retrieved by DPR ( Karpukhin et al . ,   2020 ) , a dual encoder based model . It is the state-   of - the - art on FEVER based on Petroni et al . ( 2021 )   and Maillard et al . ( 2021 ) , achieving an accuracy   of 90 % .   Implementation details We use the En-   glish Wikipedia from 08/01/2019 following   KILT ( Petroni et al . , 2021 ) . We take the plain text   and lists provided by KILT and create a collection   of passages where each passage has up to 100   tokens . This results in 26 M passages . We set   the number of input passages kto 3 , following   previous work ( Petroni et al . , 2021 ; Maillard et al . ,   2021 ) . Baselines on FVIQ are jointly trained on   the A set and the R set .   Training DPR requires a positive and a nega-   tive passage — a passage that supports and does not   support the verdict , respectively . We use the sil-   ver evidence passage associated with FVIQ as a   positive , and the top TF - IDF passage that is not   the silver evidence passages as a negative . More   training details are in Appendix C. Experiments   are reproducible from https://github.com/   faviq / faviq / tree / main / codes .   4.1.2 Results   Table 5 reports results on FVIQ . The overall accu-   racy of the baselines is low , despite their high per-   formance on FEVER . The zero - shot performance   is barely better than random guessing , indicating   that the model trained on FEVER is not able to   generalize to our more challenging data . When the   baselines are trained on FVIQ , the best model5159   achieves an accuracy of 65 % on the A set , indi-   cating that existing state - of - the - art models do not   solve our benchmark .   Impact of retrieval The performance of the   claim only baseline that does not use retrieval is   almost random on FVIQ , while achieving nearly   80 % accuracy on FEVER . This result suggests   signiﬁcantly less bias in the claims , and the rela-   tive importance of using background knowledge   to solve the task . When retrieval is used , DPR   outperforms TF - IDF , consistent with the ﬁnding   from Petroni et al . ( 2021 ) .   A set vs. R set The performance of the models   on the R set is consistently higher than that on the A   set by a large margin , implying that claims based on   ambiguity arisen from real users are more challeng-   ing to verify than claims generated from regular   question - answer pairs . This indicates clearer con-   trast to prior work that converts regular QA data to   declarative sentences ( Demszky et al . , 2018 ; Pan   et al . , 2021 ) .   Error Analysis We randomly sample 50 error   cases from DPR + BART on the A set of FVIQ   and categorize them , as shown in Table 6 .   •Retrieval error is the most frequent type of er-   rors . DPR typically retrieves a passage with   the correct topic ( e.g. , about “ Lie to Me ” ) but   that is missing more speciﬁc information ( e.g. ,   the end date ) . We think the claim having less   lexical overlap with the evidence text leads to   low recall@kof the retrieval model ( k= 3).•28 % of error cases involve events . In particular ,   14 % involve procedural events , and 6 % involve   distinct events that share similar properties but   differ in location or time frame .   •In 18 % of error cases , retrieved evidence is   valid but not notably explicit , which is natu-   rally the case for the claims occurring in real   life . FVIQ has this property likely because   it is derived from questions that are gathered   independently from the evidence text , unlike   prior work ( Thorne et al . , 2018a ; Schuster et al . ,   2021 ; Eisenschlos et al . , 2021 ) with claims writ-   ten given the evidence text .   •16 % of the failure cases require multi - hop infer-   ence over the evidence . Claims in this category   usually involve procedural events or composi-   tions ( e.g. “ is Seth Curry ’s brother ” and “ played   for Davidson in college ” ) . This indicates that   we can construct a substantial portion of claims   requiring multi - hop inference without having   to make data that artiﬁcially encourages such   reasoning ( Yang et al . , 2018 ; Jiang et al . , 2020 ) .   •Finally , 10 % of the errors were made due to a   subtle mismatch in properties , e.g. , in the ex-   ample in Figure 6 , the model makes a decision   based on “ required minimum number ” rather   than “ exact number ” of a particular brand .   4.2 Professional Fact Checking Experiments   We use two professional fact - checking datasets .   S ( Hanselowski et al . , 2019 ) consists of   6,422 claims , authored and labeled by professional   fact - checkers , gathered from the Snopes website .   We use the ofﬁcial data split .   SFACT ( Wadden et al . , 2020 ) consists of 1,109   claims based on scientiﬁc papers , annotated by do-   main experts . As the ofﬁcial test set is hidden , we   use the ofﬁcial validation set as the test set , and sep-   arate the subset of the training data as the validation   set to be an equal size as the test set .   For both datasets , we merge not enough   info ( NEI ) torefute , following prior work that   converts the 3 - way classiﬁcation to the 2 - way clas-   siﬁcation ( Wang et al . , 2019 ; Sathe et al . , 2020 ;   Petroni et al . , 2021 ) .   4.2.1 Models   As in Section 4 , all models are based on BART   which is given a concatenation of the claim and5160   the evidence text and is trained to generate either   support orrefute . For S , the evidence   text is given in the original data . For SFACT ,   the evidence text is retrieved by TF - IDF over the   corpus of abstracts from scientiﬁc papers , provided   in the original data . We use TF - IDF over DPR   because we found DPR works poorly when the   training data is very small .   We consider two settings . In the ﬁrst setting , we   assume the target training data is unavailable and   compare the model trained on FEVER andFVIQ   in a zero - shot setup . In the second setting , we allow   training on the target data and compare the model   trained on the target data only and the model with   the transfer learning — pretrained on either FEVER   or FVIQ and ﬁnetuned on the target data .   To explore models pretrained on NEI labels , we   add a baseline that is trained on a union of the KILT   version of FEVER andNEI data from the original   FEVER from Thorne et al . ( 2018a ) . For FVIQ ,   we also conduct an ablation that includes the R set   only or the A set only .   Implementation details When using TF - IDF   forSFACT , we use a sentence as a retrieval   unit , and retrieve the top 10 sentences , which av-   erage length approximates that of 3 passages from   Wikipedia . When using the model trained on ei-   therFEVER orFVIQ , we use DPR + BART by   default , which gives the best result in Section 4.1 .   As an exception , we use TF - IDF + BART on S-   FACT for a more direct comparison with the model   trained on the target data only that uses TF - IDF.When the models trained on FEVER orFVIQ   are used for professional fact checking , we ﬁnd   models are poorly calibrated , likely due to a do-   main shift , as also observed by Kamath et al . ( 2020 )   and Desai and Durrett ( 2020 ) . We therefore use a   simpliﬁed version of Platt scaling , a post - hoc cali-   bration method ( Platt et al . , 1999 ; Guo et al . , 2017 ;   Zhao et al . , 2021 ) . Given normalized probabilities   ofsupport andrefute , denoted as pandp ,   modiﬁed probabilities pandpare obtained via :   p   p   = Softmaxp+   p   ;   where 1 <    < 1is a hyperparameter tuned on   the validation set .   4.2.2 Results   Table 7 reports accuracy on professional fact-   checking datasets , S and SFACT .   Impact of transfer learning We ﬁnd that trans-   fer learning is effective — pretraining on large ,   crowdsourced datasets ( either FEVER orFVIQ )   and ﬁnetuning on the target datasets always helps .   Improvements are especially signiﬁcant on S-   FACT , likely because its data size is smaller .   Using the target data is still important — models   ﬁnetuned on the target data outperform zero - shot   models by up to 20 % . This indicates that crowd-   sourced data can not completely replace profes-   sional fact checking data , but transfer learning from   crowdsourced data leads to signiﬁcantly better pro-   fessional fact checking performance.5161   FVIQ vs. FEVER Models that are trained on   FVIQ consistently outperform models trained on   FEVER , both with and without the target data ,   by up to 4.8 % absolute . This demonstrates that   FVIQ is a more effective resource than FEVER   for professional fact - checking .   The model on FEVER is more competitive   when NEI data is included , by up to 3 % absolute .   While the models on FVIQ outperform models   onFEVER even without NEI data , future work   can possibly create NEI data in FVIQ for further   improvement .   Impact of the A set in FVIQ The performance   of the models that use FVIQ substantially de-   grades when the A set is excluded . Moreover , mod-   els trained on the A set ( without R set ) perform   moderately well despite its small scale , e.g. , on   S , achieving the second best performance   following the model trained on the full FVIQ .   This demonstrates the importance of the A set cre-   ated based on ambiguity in questions .   S beneﬁts more from the A set than the   R set , while SFACT beneﬁts more from the R   set than the A set . This is likely because SFACT   is much smaller - scale ( 1k claims ) and thus bene-   ﬁts more from the larger data like the R set . This   suggests that having both the R set and the A set is   important for performance .   5 Conclusion & Future Work   We introduced FVIQ , a new fact veriﬁcation   dataset derived from ambiguous information - seeking questions . We incorporate facts that real   users were unaware of when posing the question ,   leading to false claims that are more realistic and   challenging to identify without fully understanding   the context . Our extensive analysis shows that our   data contains signiﬁcantly less lexical bias than pre-   vious fact checking datasets , and include refute   claims that are challenging and realistic . Our ex-   periments showed that the state - of - the - art models   are far from solving FVIQ , and models trained   onFVIQ lead to improvements in professional   fact checking . Altogether , we believe FVIQ will   serve as a challenging benchmark as well as sup-   port future progress in professional fact - checking .   We suggest future work to improve the FVIQ   model with respect to our analysis of the model   prediction in Section 4.1.2 , such as improving re-   trieval , modeling multi - hop inference , and better   distinctions between entities , events and proper-   ties . Moreover , future work may investigate using   other aspects of information - seeking questions that   reﬂect facts that users are unaware of or easily   confused with . For example , one can incorporate   false presuppositions in questions that arise when   users have limited background knowledge ( Kim   et al . , 2021 ) . As another example , one can explore   generating NEI claims by leveraging unanswer-   able information - seeking questions . Furthermore ,   FVIQ can potentially be a challenging bench-   mark for the claim correction , a task recently stud-   ied by Thorne and Vlachos ( 2021 ) that requires a   model to correct the refute claims .   Acknowledgements   We thank Dave Wadden , James Thorn and Jinhyuk   Lee for discussion and feedback on the paper . We   thank James Lee , Skyler Hallinan and Sourojit   Ghosh for their help in data validation . This work   was supported by NSF IIS-2044660 , ONR N00014-   18 - 1 - 2826 , an Allen Distinguished Investigator   Award , a Sloan Fellowship , National Research   Foundation of Korea ( NRF-2020R1A2C3010638 )   and the Ministry of Science and ICT , Korea , under   the ICT Creative Consilience program ( IITP-2022-   2020 - 0 - 01819 ) .   References516251635164A Details in Data Construction   Details of obtaining a We obtain an invalid   answer to the question , denoted as a , using an   off - the - shelf QA model , for which we use DPR fol-   lowed by a span extractor ( Karpukhin et al . , 2020 ) .   The most naive way to obtain ais to take   the highest scored prediction that is not equal to a.   We however found such prediction is likely to be a   valid answer to q , either because it is semantically   the same as a , or because the ambiguity in the   question leads to multiple distinct valid answers .   We therefore use two heuristics that we ﬁnd greatly   reduce such false negatives . First , instead of taking   the top incorrect prediction , we obtain the top k   predictionsp:::pfrom the model and randomly   sample one fromfp:::pgnfag . We usek= 50 .   Although this is not a fundamental solution to   remove false negatives , it signiﬁcantly alleviates   the problem , drastically dropping the portion of   false negatives from 14 % to 2 % based on our man-   ual veriﬁcation on 50 random samples . Second ,   we train a neural model that is given a pair of the   text and classiﬁes whether they are semantically   equivalent or not . This model is based on T5 - large ,   trained and validated respectively on 150 and 100   pairs of ( a;p)(i= 1:::k ) which we manually la-   bel . We then exclude the predictions in fp:::pg   which are classiﬁed as semantically equivalent to a   by the classiﬁer .   QA - to - claim converter We use a pretrained   sequence - to - sequence model trained on a small   number of our own annotations . We ﬁrst manually   write 250 claims given valid or invalid question-   answer pairs . We then train a T5 - 3B model ( Raffel   et al . , 2020 ) , using 150 claims for training and 100   claims for validation . Each question - answer pair is   fed into T5 with special tokens question : and   answer : , respectively .   When training , we evaluate on the validation   data every epoch and stop training when the valida-   tion accuracy does not increase for ten epochs . The   accuracy is measured by the exact match score of   the generated and the reference text after normal-   ization , which we found to correlate well with the   quality of the generated claims . The ﬁnal model   we train achieves 83 % on the validation data . At   inference time , we ﬁlter claims that do not contain   the answer string , which may happen when the   question is overly speciﬁc . Why do n’t we evaluate evidence prediction ?   Unlike FEVER ( Thorne et al . , 2018a ) , which in-   cludes evidence prediction as part of the task , our   paper does not report the evidence prediction per-   formance and mainly reports the classiﬁcation ac-   curacy . There are three reasons for this change :   1.As claims on FVIQ were written independent   from any reference text , gold evidence text must   be gathered through a separate process , which   greatly increases the cost . This is different from   other annotated fact checking datasets where a   crowdworker wrote a claim based on the refer-   ence text and therefore the same reference text   can be considered as gold evidence .   2.Finding gold evidence text is an inherently in-   complete process ; no human can get close to , or   even measure the upperbound . Therefore , even   after exhaustive human annotation , evaluation   against annotated evidence leads to signiﬁcant   amount of false negatives . For example , when   manually evaluating the top negatives of TF - IDF   on 50 random samples from FEVER , 42 % are   false negatives .   3.Including evidence prediction as part of evalua-   tion signiﬁcantly restricts the approach models   can take . For instance , one may choose not to   use the text corpus provided in the dataset ( e.g. ,   Wikipedia ) , and decide to use other sources such   as structured data ( e.g. knowledge bases ) or im-   plicit knowledge stored in large neural models .   Nonetheless , as described in Section 3.1.4 , we still   provide the silver evidence passages which is useful   to train a model , e.g. , DPR , and supports future   work to evaluate the evidence prediction accuracy .   B Analysis of refute Claims   We randomly sample 30 refute claims from   FVIQ andFEVER , respectively , and categorize   the cause of the misinformation , as shown in Ta-   ble 8 . See Section 3.3 for discussion .   C Details of Experiments   DPR training for FEVER AsFEVER pro-   vides the annotated evidence passage , we use it   as a positive training example . We obtain a nega-   tive by querying the claim to TF - IDF and taking the   passage that is not the positive passage and has the   second highest score . We initially considered using   the negative with the highest score , but found that5165   many of them ( 37 % ) are false negatives based on   our manual evaluation of 30 random samples . This   is likely due to incomprehensive evidence annota-   tion as discussed in Appendix A. We ﬁnd using the   negative with the second highest instead decreases   the portion of false negatives from 37 % to 13 % .   Other details Our implementations are based on   PyTorch(Paszke et al . , 2019 ) and Huggingface   Transformers(Wolf et al . , 2020 ) .   When training a BART - based model , we map   support andrefute labels to the words ‘ true ’   and ‘ false ’ respectively so that each label is   mapped to a single token . This choice was made   against mapping to ‘ support ’ and ‘ refute’because   the BART tokenizer maps ‘ refute ’ into two to-   kens , making it difﬁcult to compare probabilities   ofsupport andrefute .   By default , we use a batch size of 32 , a maximum   sequence length of 1024 , and 500 warmup stepsusing eight 32 GB GPUs . For SFACT , we use   a batch size of 8 and no warmup steps using four   32 G GPUs . We tune the learning rate in between   { 7e-6 , 8e-6 , 9e-6 , 1e-5 } on the validation data .   D Additional Experiments   Table 9 reports the model performance when   trained on FVIQ and tested on FEVER . The   best - performing model achieves non - trivial perfor-   mance ( 67 % ) . However , their overall performance   is not as good as model performance when trained   onFEVER , likely because the models do not ex-   ploit the bias in the FEVER dataset . Nonetheless ,   we underweight the test performance on FEVER   due to known bias in the data.5166