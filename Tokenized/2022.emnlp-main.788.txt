  Tiago PimentelJosef ValvodaNiklas Stoehr Ryan CotterellUniversity of Cambridge ETH Zürich   { tp472 , jv406}@cam.ac.uk   { niklas.stoehr , ryan.cotterell}@inf.ethz.ch   Abstract   In this paper , we seek to measure how much   information a component in a neural network   could extract from the representations fed   into it . Our work stands in contrast to prior   probing work , most of which investigates how   much information a model ’s representations   contain . This shift in perspective leads us   to propose a new principle for probing , the   architectural bottleneck principle : In order   to estimate how much information a given   component could extract , a probe should look   exactly like the component . Relying on this   principle , we estimate how much syntactic   information is available to transformers   through our attentional probe , a probe that   exactly resembles a transformer ’s self - attention   head . Experimentally , we find that , in three   models ( BERT , ALBERT , and RoBERTa ) , a   sentence ’s syntax tree is mostly extractable by   our probe , suggesting these models have access   to syntactic information while composing   their contextual representations . Whether this   information is actually used by these models ,   however , remains an open question.https://github.com/rycolab/   attentional - probe   1 Introduction   The surprising performance of pretrained language   models on diverse natural language processing   tasks has sparked interest in their analysis . Probing   is one of the most prevalent methods employed to   engage in such an analysis . In a typical probing   study ( Alain and Bengio , 2016 ; Belinkov et al . ,   2017 ; Adi et al . , 2017 , inter alia ) , the weights   of the model under consideration are first frozen .   A probe is then trained on top of the model ’s   contextual representations in an attempt to predict   one of the input sentence ’s properties , e.g. , its   syntactic parse . Unfortunately , best practices on   how to design such probes remain contested .   On one side of the debate , some argue for   simplicity , suggesting that simple probes are to bepreferred so that we can distinguish probing from   simply learning an NLP task ( Hewitt and Liang ,   2019 ) . On the other side of the debate , some   argue we need complex probes in order to extract   all relevant information from the representations   ( Saphra and Lopez , 2019 ; Pimentel et al . , 2020b ) .   Bridging the gap , some have also called for a   compromise , advocating that all probes on the   complexity – accuracy Pareto curve should be   considered ( Pimentel et al . , 2020a ) .   In this paper , we propose the architectural   bottleneck principle ( ABP ) as a guideline for   constructing useful probes . Under the ABP , a   probe ’s architecture should mirror a component   of the model being probed . Previous work has   mostly focused on how much information is   contained in a set of representations . However , if   we care about whether the information is in fact   used by the model , we should instead ask how   much information the model in question could   use . Under this perspective , the probed model ’s   architecture acts as a natural bottleneck to how   much information the model could use — and   should thus also act as a constraint when probing .   As a concrete example , we posit that a trans-   former ’s attention head serves as a bottleneck to   its use of syntactic information , as these are the   only components in a transformer with access   to multiple tokens at once . Following the ABP ,   we thus propose the attentional probe , which   looks exactly like an attention head . This probe   allows us to answer one specific question : How   much syntactic information could a transformer   use while computing its attention weights ?   Our results reveal that most — albeit not all —   syntactic information is extractable with this   simple attention head architecture : While we   estimate English sentences to contain on average1145931.2bits of information about their syntactic tree   structure , the attentional probe can extract up to   28.0bits . Furthermore , while these results hold for   three popular transformer - based language models   ( BERT , ALBERT and RoBERTa ) , they do not   for a similar but untrained model . This suggests   that training a model shapes its representations to   encode syntactic information . We find this trend   holds across four typologically diverse languages   ( Basque , English , Tamil , and Turkish ) . In contrast ,   when we keep BERT ’s pretrained parameters   frozen and analyze the weights of its pretrained   attention heads , we observe that they do not seem   to encode syntax under our operationalisation .   Ergo , while we know these models could use   syntactic information to compute attention weights ,   whether they actually do remains an open question .   2 A Taxonomy of Probing Principles   There are many competing approaches for how   to design an effective probe ( Belinkov and Glass ,   2019 ) . We taxonomise them into principles here .   1.The Linearity Principle ( Alain and Bengio ,   2016 ) . A neural network ’s purpose is to make   information linearly separable for its final layer .   Thus , probes should be linear models .   Focusing on how much information a model could   use in its final layer , Alain and Bengio ( 2016 )   propose what we term the linearity principle ; many   subsequent studies then adopted it in designing   their probes ( Shi et al . , 2016 ; Ettinger et al . , 2016 ;   Bisazza and Tump , 2018 ; Liu et al . , 2019a ) . Other   researchers , however , argued that a model ’s non-   final layers do not necessarily encode information   linearly ( Conneau et al . , 2018 ; Pimentel et al . ,   2020b ) . They then suggested that a probe should   measure the total amount of information present   in a model ’s representations — independent of   whether it is actually used by the model . This led   to a second principle , which we outline below .   2.The Maximum Information Principle ( Pi-   mentel et al . , 2020b ) . A probe ’s goal is to estimate   how much information is encoded in a set of rep-   resentations . Thus , probes should be as complex   as necessary to extract all relevant information .   Following this principle , some authors have found ,   unsurprisingly , that non - linear probes estimate   larger amounts of information to be encoded ina representation than linear ones ( Qian et al . , 2016 ;   Belinkov et al . , 2017 ; White et al . , 2021 ) . Pimentel   et al . ( 2020b ) , however , argued that all contextual   representations , e.g. , the ones produced by BERT ,   encode as much information about a target attribute   as the original sentence . It follows that probing   only makes sense with some constraint on probe   complexity . Taking complexity into account sug-   gests another natural principle for probe design .   3.The Easy - extraction Principle ( Hewitt and   Liang , 2019 ) . The goal of probing is to reveal how   easy it is to extract the information encoded in the   representations . Thus , probes should be as simple   as possible without sacrificing performance .   The idea of preferring simple probes goes by many   names in the literature . Some authors discuss the   complexity of probing architectures ( Hewitt and   Liang , 2019 ; V oita and Titov , 2020 ; Pimentel et al . ,   2020a ; Cao et al . , 2021 ) , while others discuss the   amount of data required to train the probe ( Pimentel   and Cotterell , 2021 ) . None of the work above ,   however , discusses how the model actually uses the   information about the target attribute ( Elazar et al . ,   2021 ; Lasri et al . , 2022 ) . If we are interested in   whether information can be used by the model , we   need a new principle . In this work , we argue that a   model ’s architecture should factor into the probe ’s   design , because the model ’s architecture constrains   the amount of information the model can use . This   leads us to propose the following principle .   4.The Architectural Bottleneck Principle ( ABP ) .   A probe should measure how much information a   component of a model could use . Thus , a probe ’s   architecture should mirror that component .   We believe the ABP naturally connects the first   three principles . Importantly , the ABP generalises   the linearity principle : If a model employs a linear   projection coupled with a softmax in its final layer ,   and our probe mirrors that layer , as linear probes   do , then the ABP will be equivalent to the linearity   principle . Furthermore , the ABP also relates to   the maximum information principle : If we probe a   component that is expressive enough , it should be   able to extract all relevant information from a set   of representations . Finally , the ABP also implicitly   controls for ease of extraction by restricting the   capacity of probes .   3 Probing with Information Theory   In this paper , we take the position that the goal of   probing is to determine how much information one11460can extract from the representations being probed .   Following Pimentel et al . ( 2020b ) , we now oper-   ationalise this value formally using information   theory , which offers us a clean framework to quan-   tify information . Specifically , the measure we are   interested in is a V - information ( Xu et al . , 2020 ) .   3.1 Mutual Information   Shannon ( 1948 ) famously quantified the amount   of information that a random variable ( R ) contains   about another ( A ) as their mutual information   I(R;A)= H(A)−H(A|R ) ( 1 )   where H(A)andH(A|R)are , respectively , the   entropy of Aand the conditional entropy of A   given R. Given that Ris a continuous - valued   representation with values r∈ R , and Ais a   discrete - valued attribute with values a∈ A , these   quantities are defined formally as   H(A)=/summationdisplayp(a ) log1   p(a)(2 )   H(A|R)=/integraldisplay / summationdisplayp(r , a ) log1   p(a|r)dr(3 )   The maximum information principle seeks to   estimate Eq . ( 1 ) . Notably , the relationship between   RandA , represented by distribution p(a|r ) ,   may be arbitrarily complex , and this distributions ’   computational complexity has no direct effect on   the conditional entropy ’s value H(A|R ) .   3.2V - information   Under the architectural bottleneck principle , we   are interested in how much information we can   extract from RaboutA , when constrained to only   using extraction functions in a set V , the set of   functions a model ’s component can represent . The   V - information ( Xu et al . , 2020 ) , a generalisation   of Shannon ’s ( 1948 ) mutual information , naturally   operationalises this value as   I(R→A)= H(A)−H(A|R)(4 )   where the conditional V - entropy is defined as   H(A|R)= inf / integraldisplay / summationdisplayp(r , a ) log1   q(a|r)dr   ( 5)The unconditional V - entropy is defined similarly .   In words , the V - information computes the   maximum information that can be extracted by   a model with an architecture V. Notably , if Vis   sufficiently expressive , i.e. , if p(a|r)∈ V , the   V - information will be equivalent to the traditional   mutual information . Further , V - information is   bounded above by the mutual information , which   leads to a new value termed here the V - coefficient   C(A|R)=I(R→A )   I(R;A)(6 )   In short , the V - coefficient computes the percentage   of information we can extract from a random   variable when restricted to variational family V.   4 An Attentional Probe   In our experiments , we will focus on a trans-   former ’s attention mechanism . Concretely , many   researchers ( e.g. , Vig and Belinkov , 2019 ; Htut   et al . , 2019 ; Manning et al . , 2020 ) have asserted   that syntactic information is used by transform-   ers when computing their attention weights ( al-   beit not uncontroversially ; for a review , see Rogers   et al . , 2021 ) . Further , attention heads are the only   components in a transformer which have access   to multiple words at the same time . Thus , explor-   ing the ABD in the context of attention heads is a   natural starting point . Specifically , following the   ABP , we will investigate how much information a   transformer ’s attention head could extract from the   representations fed into it .   Given an input sentence s , a transformer   ( Vaswani et al . , 2017 ) will generate a set of repre-   sentations r∈ R = Rat layer ℓ. An attention   head then uses these representations to compute   the attention weights   α= ( Kr)Qr , w = e   /summationtexte(7 )   where iandjindex word positions in a sentence   s , K , Q∈Rare the key and query matri-   ces , and α , w∈Rare , respectively , the   self - attention logits and attention weights .   We now consider an attentional probe parame-   terised using the head in Eq . ( 7 ) , but with randomly   initialised KandQmatrices . Our goal is to   train this probe , as we explain towards the end   of this section . We use the attention weights ,   defined in Eq . ( 7 ) , to compute the probability of11461   a specific directed spanning tree a , which encodes   the syntactic dependencies   q(a|r ) = /producttextw / summationtext / producttextw(8 )   where tree ais represented as a set of pairs ( i , j )   which index an edge in it , Arepresents the   set of all directed spanning trees with a specific   number of nodes |s| , and θ= [ K;Q]∈R   represents the probe ’s parameters . We can easily   compute the numerator in this equation for a   specific tree . The normalising factor in the   denominator is more complex , as it requires   looping through a prohibitively large sum . Luckily ,   we can efficiently compute it with Koo et al . ’s   ( 2007 ) variation of the matrix – tree theorem ( MTT )   for root - constrained directed spanning trees ( Tutte ,   1984 ; Zmigrod et al . , 2020 ) .   The Variational Family . The attentional probe   architecture is defined by Eqs . ( 7 ) and ( 8) . We now   define the equivalent variational family   V=/braceleftig   q(a|r)|K , Q∈R / bracerightig   ( 9 )   This variational family includes the set of all   distributions computable by an attention head   architecture . In practice , however , we can not   compute the infimum over the set Vas required   in Eq . ( 5 ) . As an approximation , we train our   attentional probe to minimise a cross - entropy loss ,   which gives us an estimate of the V - entropy . We   expand on this point in App . A.1.5 Experiments   Data . We use the universal dependencies ’ ( UD )   treebanks ( Zeman et al . , 2020 ) . Specifically , we   analyse results in four typologically diverse lan-   guages : Basque , English , Tamil , and Turkish . Fur-   thermore , we focus our analysis on unlabelled de-   pendency trees . We note that UD uses a particular   syntax formalism , which could impact our results   ( Kuznetsov and Gurevych , 2020 ) .   Models . Empirically , we study multilingual   BERT in all four languages under consideration   ( Devlin et al . , 2019 ) as well as RoBERTa and   ALBERT ( Liu et al . , 2019b ; Lan et al . , 2020 ) ,   which are only available in English . In line with   the ABD , we keep our probe ’s hidden size the   same as in the probed architectures . Finally , we   also probe an untrained transformer model with   the same architecture as BERT as a baseline .   Training . We train our probes with AdamW   ( Loshchilov and Hutter , 2019 ) using its default   hyper - parameters in PyTorch ( Paszke et al . , 2019 ) .   Baselines and Skylines . We contrast our atten-   tional probe ’s V - information against two other val-   ues . First , as a baseline , we investigate a special   case of our model where K = Q , inspired by re-   cent work on structural probing ( Hewitt and Liang ,   2019 ; Maudslay et al . , 2020 ; White et al . , 2021 ) .   Notably , this equality leads to symmetric attention   weight matrices ; by modelling the root explicitly ,   however , we still get a distribution over directed   trees . This baseline evaluates whether previous   work , by over - constraining their probes , has under-   estimated the amount of information available to a   transformer ’s attention mechanism . We report this   value as structural V - information . Second , as   a skyline , we compare our attentional probe to an   estimate of the true mutual information I(R;A),11462Model Layer I I C   BERT 7 28.0 31.2 90 %   RoBERTa 9 25.5 31.2 82 %   ALBERT 7 27.7 31.2 89 %   for which we follow Pimentel et al . ( 2020b ) in us-   ing a deep neural network ( DNN ) to estimate .   6 Results   We present our main results in Fig . 1 . First , our   probes estimate that most syntactic information   is extractable in the middle layers , as previously   reported by Tenney et al . ( 2019 ) . Second , Fig . 1   shows that a large amount of syntactic information   is encoded in the representations fed to the atten-   tion heads . Further , while we estimate close to 31   bits of information to be encoded in English , Tamil ,   and Basque sentences , we only estimate around 15   bits to be encoded in Turkish sentences ; we suspect   this is due to Turkish having the shortest sentences   in the corpus ( see App . I for these lengths ) .   Third , we find that , out of the total syntactic   information present in the sentences , nearly all is   available to the transformer - based models under   consideration . In English , for instance , we find   theV - coefficient of the most informative layer   to be 90%,82 % , and 89 % in BERT , RoBERTa   and ALBERT , respectively ; see Tab . 1 . This   means they have access to roughly 85 % of all   syntactic information in a sentence . These trends   are consistent across the four languages we have   considered . Notably , this is not the case for the   untrained BERT representations , which suggests   this structure is a byproduct of the language   models ’ pretraining procedures .   Additionally , we find that our structural baseline   considerably underestimates the models ’ potential   ability to reconstruct a syntax tree ; the best English   structural baseline recovers only 23bits of infor-   mation ( versus 28bits by the attentional probe ) .   One can see this effect in Fig . 1 , where all of the   structural baseline results fall beneath their corre-   sponding attentional probe counterparts .   In a final experiment , we plug BERT ’s attention   weights , as computed with its pretrained attention   heads , directly into Eq . ( 8) and analyse its resulting   unlabelled attachment scores . These results are   presented in Fig . 2 for English ( as well as in App . H   for the other analysed languages ) . In short , they   reveal that , while attention heads could use a large   amount of syntactic information , none of the actual   heads compute weights that strongly resemble   syntax trees ; see Htut et al . 2019 for similar results .   As BERT has 8 attention heads , however , it might   be the case that the syntactic information is used in   a distributed manner , with each head relying on a   subset of this information ( see Tab . 3 in Clark et al .   2019 for results partly supporting this hypothesis ) .   7 Conclusions   In this paper , we have approached probing from   a new perspective . Rather than asking how much   information is encoded by the model , we ask how   much information its components could extract .   We then quantify this amount using V - information .   Evaluating the attention mechanism of popular   transformer language models , we find that the   majority of the information about the syntax tree of   a sentence is in fact extractable by the model . This ,   however , is not true for randomly initialised trans-   former models . Our results , thus , lead us to con-   clude that a transformer ’s training leads its attention   heads to have the potential to decode syntax trees.11463Acknowledgements   We thank the reviewers and action editor for their   helpful comments . We also thank Kevin Du , Clara   Meister , Lucas Torroba Hennigen , and Afra Amini   for their feedback on this manuscript .   Limitations   In this paper , we propose a new principled way to   choose a probe ’s architecture , operationalising the   question “ how much information could a model   extract from a set of representations ? ” in terms   of aV - information . We note , however , that this   probe design principle is only applicable to answer   the specific question above . Explicitly , we do not   answer what we believe to be the more interesting   question : “ how much information does a model   actually extract from a set of representations ? ” In   practice , while our proposed probing method does   not answer this second question , it does offer an   upperbound for it ; the amount of information a   model could extract from a set of representations   is strictly larger than the amount actually extracted .   Quantifying how tight ( or loose ) this upperbound   is remains future work .   Ethical Concerns   We foresee no ethical concerns with this work .   References1146411465114661146711468A More on the V - information   A.1 Probing as approximating V - information   In this section , we make a similar argument to   Hewitt et al . ’s ( 2021 ) , who first pointed out the   equivalence between the goals of probing and esti-   mating a V - information . When probing for some   information , we typically train a probabilistic clas-   sifier q(a|r)(with parameters θ ) to approximate   a target probability distribution p(a|r ) . We do this   by using an empirical cross - entropy loss function   L(D;θ)=/summationdisplaylog1   q(a|r)(10 )   where D is a training set composed of ( r , a )   pairs , which are assumed to be sampled from the   true distribution p(r , a ) . Further , we usually have   access to a development set D , on which we   estimate this same loss L(D;θ)and which we   use to avoid overfitting . Together , these steps aim   at making q(a|r)approximate the distribution   which minimises the true cross - entropy   H(A|R)=/integraldisplay / summationdisplayp(r , a ) log1   q(a|r)dr   ( 11 )   Minimising this cross - entropy is equivalent to find-   ing the q(a|r)∈ V which minimises the condi-   tionalV - entropy in Eq . ( 5 ) . Furthermore , since   H(A)is constant with respect to the represen-   tations R , this is also equivalent ( up to an addi-   tive constant ) to estimating the V - information in   Eq . ( 4)—where Vis defined by our choice of archi-   tecture for the probing classifier .   A.2 On V , Expressivity and Learnability   Ideally , a trained probe q(r|a)would converge   to the infimum q∈ V from Eq . ( 5 ) . In practice ,   however , limitations on the dataset size and optimi-   sation algorithms may lead to poor approximations .   Moreover , even with a trained probe , we still can-   not compute Eq . ( 11 ) , but must instead empirically   approximate it with a test set and the loss function   in Eq . ( 10 ) . We can thus decompose our actually   measured value into four terms   L(D;θ ) = H ( A|R ) + ϵ+ϵ+ϵ(12)where   ϵ= H(A|R)−H(A|R)/bracehtipupleft /bracehtipdownright / bracehtipdownleft /bracehtipupright(13 )   ϵ= H(A|R)−H(A|R)/bracehtipupleft /bracehtipdownright / bracehtipdownleft /bracehtipupright(14 )   ϵ=L(D;θ)−H(A|R)/bracehtipupleft /bracehtipdownright / bracehtipdownleft /bracehtipupright(15 )   Given a large enough testset , ϵshould be roughly   zero , as the empirical loss in Eq . ( 10 ) is an un-   biased estimator of the cross - entropy in Eq . ( 11 ) .   This leaves ϵandϵ. While ϵis intentionally   imposed by the choice of V , which defines the ex-   pressivity constraints on the structure of the learned   information extractors , ϵis a byproduct of multi-   ple factors : Vitself , the optimisation algorithm and   both the train and devset sizes .   Analysing the V - information of a very expres-   sive variational family may thus be vacuous , as we   may expect ϵto be relatively small compared to ϵ ;   this would likely be the case for a Vresembling the   entire BERT architecture . For smaller variational   families , however , such as the ones we explore here ,   we can expect our learning procedures to be well   behaved and for ϵto be relatively small .   B Inverse Ablation Perspective   One could view our work as a reversed ablation   study . In a typical ablation experiment a compo-   nent of a model is removed to observe its effect   on the functioning of the entire model . The idea is   that the observed difference in performance of the   model will indicate the relative importance of the   component . However , with ablation it is impossi-   ble to tell what role the component plays in solving   the target task . In comparison , we freeze the en-   tire model up to a particular component we are   interested in . Instead of asking how important the   component is to the overall goal of the model , we   ask how good it is at a task we believe is important   towards achieving such goal .   C Baseline and Skyline   C.1 Structural Baseline   Hewitt and Manning ( 2019 ) propose the structural   probe to investigate the encoding of syntactic struc-   ture in contextual representations . Intuitively , they11469probe to which extent they can reconstruct a sen-   tence ’s syntactic tree purely from the distance be-   tween contextual representations r. Instead of   learning separate query Qand key Kmatrices as   we do , however , they limit themselves to a single   projection matrix B∈R. Their probe can   thus be written as   α= ( Br)Br ( 16 )   Since we want to train the structural probe with the   same cross - entropy parsing loss as our attentional   probe , we softmax its distances   w = e   /summationtexte(17 )   making it similar to White et al . ’s ( 2021 ) non - linear   structural probe . This is necessary because the   MTT we use to compute the denominator in Eq . ( 8)   assumes non - negative inputs . We then train it with   the same loss function as our proposed attentional   probe , also making it similar to Maudslay et al . ’s   ( 2020 ) structural parser . In practice , thus , our struc-   tural baseline ’s implementation can be seen as a   non - linear structural parser .   C.2 DNN Parser   To approximate the true mutual information   I(R;A ) , we follow Pimentel et al . ( 2020b ) in using   more powerful feed forward neural network probes .   Specifically , we rely on a variant of Dozat and Man-   ning ’s ( 2017 ) parser . We first use two multi - layer   perceptrons ( MLP ) , one for the dependent and one   for the head token in a dependency arc   r= MLP ( r),r= MLP ( r ) ( 18 )   These MLP ’s are composed of a number of lin-   ear transformations , interweaved with ReLU non-   linearities and dropout layers . We then feed both   these transformed representations into a biaffine   transformation to get the dependency logits   α = rW r ( 19 )   Finally , we again make these values non - negative   by softmaxing them   w = e   /summationtexte(20 )   We train this model with the same cross - entropy   loss function as our proposed attentional probe . To choose the hyper - parameters of this model ’s   MLP we use random search , training 50 indepen-   dent models . We random search for the number   of layers in { 0,1,2 } , dropout in [ 0.0,0.5 ] , and the   hidden size in [ 32 ; 512 ] . Furthermore , we note that ,   as demonstrated by Pimentel et al . ( 2020b ) , the   mutual information I(R;A)is constant across con-   textual representations and equivalent to I(S;A ) ,   where Sis a random variable representing the orig-   inal input sentence . We thus use our single best ap-   proximation of it in each language as our estimate .   D Unconditional Entropy Parser   We still need to estimate the unconditional en-   tropies H(A ) . As these unconditional entropies   are not conditioned on anything , however , we can-   not estimate them using the previous parsers di-   rectly . Specifically , the representations randrin   Eqs . ( 7 ) , ( 16 ) and ( 18 ) can not be used . We sidestep   this issue by dropping our contextual representa-   tions from these equations and using position em-   beddings in their place . Importantly , these position   embeddings do not depend on the input sentences .   In short , we compute these equations as   α= ( Kp)Qp ( attentional ) ( 21 )   α= ( Bp)Bp ( structural ) ( 22 )   r= MLP ( p),r= MLP ( p ) ( DNN)(23 )   where p∈Ris a randomly initialised position   embedding and is trained with the rest of the probe .   E Extra Information about Training   We use the base version of all our analysed pre-   trained models ( taken from the transformers library   Wolf et al . , 2020 ) . We train the model with a batch   size of 2048 , evaluate the model every 100 batches ,   and stop training when the model does not im-   prove over 10 consecutive evaluations . Both the   attentional and structural probes are trained with   a dropout of 0.2(applied both on the raw input   representations and on the key and query represen-   tations before being multiplied together ) and with   a hidden size ( i.e. d ) of 64 — this is the size of   the query , and key representations in both BERT ,   RoBERTa and ALBERT . As our data , we used the   treebanks : English EWT ( Silveira et al . , 2014 ) ;   Basque BDT ( Aduriz et al . , 2003 ) ; Turkish IMST   ( Sulubacak et al . , 2016 ) ; Tamil TTB ( Ramasamy   and Žabokrtský , 2012).11470F UAS Results   GV - information by Language11471H Attention Head Weights Results   We additionally compute the parsing accuracy of the attention heads with their actual weights as taken   from BERT ( with its parameters as pretrained).In Fig . 5 ( as well as Fig . 2 in the main text ) , we label   the heads in order of their performance ( 1 is always the least accurate per layer , 8 the most ) . These results   show that an attention head ’s potential to extract syntax trees is far above what each individual head   actually extracts .   I Average Sentence Lengths   Language Average Sentence Length   Basque 13   English 15   Tamil 17   Turkish 1011472