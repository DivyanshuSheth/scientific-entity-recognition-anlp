  Guangtao ZengPeiyuan ZhangWei Lu   StatNLP Research Group   Singapore University of Technology and Design   guangtao_zeng@mymail.sutd.edu.sg , { peiyuan_zhang , luwei}@sutd.edu.sg   Abstract   Fine - tuning pre - trained language models for   multiple tasks tends to be expensive in terms   of storage . To mitigate this , parameter - efficient   transfer learning ( PETL ) methods have been   proposed to address this issue , but they still re-   quire a significant number of parameters and   storage when being applied to broader ranges   of tasks . To achieve even greater storage re-   duction , we propose PP , a novel method   that enables efficient sharing of a single PETL   module which we call prototype network ( e.g. ,   adapter , LoRA , and prefix - tuning ) across layers   and tasks . We then learn binary masks to select   different sub - networks from the shared proto-   type network and apply them as PETL modules   into different layers . We find that the binary   masks can determine crucial information from   the network , which is often ignored in previous   studies . Our work can also be seen as a type of   pruning method , where we find that overparam-   eterization also exists in the seemingly small   PETL modules . We evaluate PPon var-   ious downstream tasks and show that it can   outperform other PETL methods with approxi-   mately 10 % of the parameter storage required   by the latter .   1 Introduction   With the release and wide application of numerous   pre - trained language models ( PLMs ) ( Devlin et al . ,   2019 ; Liu et al . , 2019 ) , pre - training and subse-   quently fine - tuning them becomes prevalent in nat-   ural language processing ( NLP ) . This yields good   performance in many downstream tasks . However ,   such a paradigm requires the entire model to be up-   dated and saved after fine - tuning . As PLMs grow in   size , traditional fine - tuning becomes costly in stor-   age , limiting the application to multi - task scenarios .   In order to ameliorate this issue , many Parameter-   Efficient Transfer Learning ( PETL ) methods haveFigure 1 : An illustration of our PP model .   Note that PPis orthogonal to the specific PETL   architectures . LoRA and prefix - tuning are also imple-   mented in our framework .   been proposed ( Houlsby et al . , 2019 ; Li and Liang ,   2021 ; Hu et al . , 2022 ) . Rather than fine - tuning   the entire model , they introduce new parameters   and only fine - tune those additional parameters on   downstream tasks , which drastically reduces the   storage of the parameters required by each task .   However , they still require a significant number of   parameters when more tasks are considered .   In this paper , we continue this line of research   and target using even less storage per task . We   observe that recent advancements in the field of   PETL focus on finding better ways to apply the   additional parameters , such as the adapter module   after each feed - forward layer ( Houlsby et al . , 2019 ;   Pfeiffer et al . , 2021 ) , or the low - rank matrices in   the query and value projection of the self - attention7564networks ( Hu et al . , 2022 ) . However , limited works   examine the impact of sub - network structure and   integrate pruning methods with PETL methods . In   fact , studies in network pruning have shown that   the modeling ability of neural networks relies not   only on the parameters but also on the sub - network   structures that are decided by the pruning masks .   For instance , Zhou et al . ( 2019 ) discovered that   the sub - network of an untrained model can yield   great performance without any parameter update .   In light of this , we seek to incorporate the struc-   tural information of sub - networks into PETL . We   believe that when enough structural information   is injected into the network , we no longer need   that many parameters in PELT modules and further   improve the parameter efficiency .   To this end , we propose a novel PETL method   dubbed PP that enables efficient sharing of   a single prototype adapter , prefix , or LoRA across   layers and tasks . When sharing the prototype net-   work , PPlearns binary masks to prune dif-   ferent sub - networks in different layers and tasks   ( Figure 1 ) . The connections of the prototype net-   work pruned in one layer can be used in another   with a different pruning mask . In this way , a pa-   rameter can be used multiple times across differ-   ent modules , achieving higher parameter efficiency .   Previous methods ( He et al . , 2022b ; Ma et al . , 2022 )   only consider simply discarding ( pruning ) the use-   less parameters , while we focus on the structural   information in masks by strategically dispatching   the parameters in the single prototype network to   different modules . We evaluate PPon var-   ious downstream tasks , including GLUE ( Wang   et al . , 2018 ) , XSum ( Narayan et al . , 2018 ) , and   WMT16 Ro - En ( Bojar et al . , 2016 ) . Experiments   show that PPachieves better performance   than other PETL methods while using significantly   fewer parameters .   Our contributions are summarized as follows :   •We propose PP , a highly parameter-   efficient transfer learning method that injects   structural information into PELT and allows   for efficient sharing of a single prototype net-   work across layers and tasks .   •Experiments show PP is able to dra-   matically reduce the storage of the parameters   while achieving better performance than con-   ventional PETL methods .   •PP offers an alternative view for net - work pruning and sharing , where we use bi-   nary masks to decide when to discard or share   the parameters . We hope to inspire more in-   triguing explorations in this direction .   2 Related Work   In this section , we briefly survey ideas that are   related to our work from three fields : parameter-   efficient transfer learning , pruning methods , and   multi - task learning .   2.1 Parameter - Efficient Transfer Learning   Recently , as the pre - trained language models get   larger and larger , some parameter - efficient trans-   fer learning methods that only update a few ex-   tra parameters while freezing the PLM backbone   have been proposed . Adapter - tuning ( Houlsby   et al . , 2019 ) fine - tuned adapter modules inserted   after each attention and feed - forward layer . Prefix-   tuning ( Li and Liang , 2021 ) placed an addi-   tional trainable prefix to the keys and values ma-   trix in the attention module . LoRA ( Hu et al . ,   2022 ) injected tunable rank decomposition matri-   ces into each Transformer layer . Based on these   parameter - efficient transfer learning methods , He   et al . ( 2022a ) gave a unified framework that al-   lows for the transfer of design elements across   various PETL approaches . However , when ap-   plied to larger PLMs and a broader range of tasks ,   these methods still require a large storage space   because the number of extra parameters is directly   proportional to the number of layers and tasks . In-   spired by the parameter - sharing techniques of AL-   BERT ( Lan et al . , 2020a ) , we propose sharing the   additional parameters in PETL modules across lay-   ers and tasks . Our method can thus obtain higher   parameter - efficient efficiency with a significantly   smaller portion of additional storage than existing   PETL methods .   2.2 Pruning Methods   Pruning is one of the most popular methods to re-   duce unnecessary weights from over - parameterized   neural networks while maintaining comparable per-   formance . Recently , Frankle and Carbin ( 2019 )   proposed Lottery Ticket Hypothesis and stated that   in a randomly initialized dense model , a sparse sub-   network exists that , when trained in isolation , can   achieve performance comparable to dense models .   Accompanied by this hypothesis , many pruning-   before - training methods have emerged ( Lee et al . ,7565   2019 ; Bai et al . , 2022 ; Sreenivasan et al . , 2022 ) . Xu   et al . ( 2021 ) further propose a method that prunes   the backward gradient of the neural network , as   opposed to pruning the network parameters them-   selves . Based on these methods , some works ( He   et al . , 2022b ; Ma et al . , 2022 ) also proposed to com-   bine pruning algorithms with parameter - efficient   methods to further decrease the additional mod-   ule size . However , those methods only focus on   discarding redundant parameters . A parameter is   either discarded or retained without any sharing .   They fail to make full use of the additional parame-   ters and can not achieve highly sparse sub - networks   without significantly compromising accuracy .   2.3 Multi - Task Learning   Multi - task learning ( Zhang and Yang , 2022 ) , which   involves training a single model to perform well   on multiple tasks , has gained popularity as a re-   search direction in machine learning . However , this   approach can be hindered by catastrophic forget-   ting ( Kirkpatrick et al . , 2016 ) and data imbalance   among tasks , which will result in overfitting on low-   resource tasks and underfitting on high - resource   tasks ( Arivazhagan et al . , 2019 ) . Houlsby et al .   ( 2019 ) propose adapter tuning that only introduces   and updates small additional parameters for eachtask while freezing the pre - trained model . Based   on such a parameter - efficient method , Mahabadi   et al . ( 2021 ) train a hyper - network named Hyper-   former , which generates task - specific weights for   the adapter modules when fed with different task   embeddings .   3 Methods   In this section , we first give an introduction to   parameter - efficient transfer learning ( PETL ) . We   then present our method PP , depicted in Fig-   ure 2 , which combines the techniques of parameter-   sharing and pruning to further improve the parame-   ter efficiency compared to existing PETL methods .   3.1 Preliminaries   In parameter - efficient transfer learning , we freeze   the parameters θof the pre - trained language   model and then introduce additional fine - tunable   parameters denoted as θ . Given dataset   { X , Y } , the goal of parameter - efficient fine-   tuning is to maximize the following likelihood of   the label Yby only updating the additional param-   etersθ :   max / summationdisplaylogP(Y|X;θ , θ ) ( 1)7566Such parameter - efficient methods suggest a   more effective way to adapt pre - trained language   models over downstream tasks than fully fine-   tuning . We give a brief introduction of the   three most widely used PETL modules , namely   adapter ( Houlsby et al . , 2019 ) , prefix - tuning ( Li   and Liang , 2021 ) , and LoRA ( Hu et al . , 2022 )   in Appendix A. However , there is still a storage   limitation when we handle a large range of tasks   using these methods . In this paper , we investi-   gate the potential for further enhancing parame-   ter efficiency in neural network models by reduc-   ing storage requirements . While previous PETL   methods have primarily focused on decreasing the   number of parameters to improve efficiency , our   approach posits that employing varying bit lengths   ( e.g. , 1 - bit , 8 - bit , 32 - bit ) during storage can lead to   significant improvements in parameter efficiency   by reducing the overall number of bits used by   the parameters . To this end , we use bits to mea-   sure the storage , which we call Bit - Level Storage   ( BLS ) , to take into account the fact that different   parameters may have different bit lengths . Con-   sider a neural model , where each parameter has a   specific bit length . Then we divide these parame-   ters into Ndistinct groups based on their respective   bit lengths . Let { ρ}denote the number of pa-   rameters within the group i , with corresponding bit   lengths { b } . The BLS for these parameters can   subsequently be determined as follows :   Bit - Level Storage = /summationdisplayρb ( 2 )   3.2 Shared Prototype Network   Parameter - efficient methods like adapter and prefix-   tuning tend to introduce an additional module   in each Transformer layer . Assuming the Trans-   former has Llayers , we can split the parameters θ   into[θ , ... , θ]according to their layer indexes .   Therefore , we can rewrite Equation 2 as :   max / summationdisplaylogP(Y|X;θ,[θ , ... , θ ] )   ( 3 )   Inspired by ALBERT ( Lan et al . , 2020b ) , in our   methods , we first introduce additional parameters   for a single PETL module as our prototype network ,   denoted as θ . Then , we share the prototype net-   work across different layers . Assuming that the   number of parameters in a single PETL module isAlgorithm 1 : PPtraining algorithm   n , we can decrease total parameters from nLto   onlyL , which significantly improves the parame-   ter efficiency . Therefore , we convert the objective   function to a more concise one :   max / summationdisplaylogP(Y|X;θ , θ ) ( 4 )   3.3 Masked Sub - Networks   Sharing the parameters alone will reduce the   model ’s capacity to capture meaningful represen-   tation in different layers , leading to suboptimal   results . Inspired by Zhou et al . ( 2019 ) and Ra-   manujan et al . ( 2020 ) , we believe that parameters   and network structures are both crucial contribut-   ing factors to the model ’s representative capacity .   To this end , we introduce different binary masks   m={0,1}in each Transformer layer l(left part   of Figure 2 ) , where ndenotes the number of pa-   rameters in a single PETL module . Each mask rep-   resents a corresponding subnetwork of the shared   prototype network . Even though the prototype is   shared among all layers , we can use different masks   to create different sub - networks for each layer l ,   whose parameter will be θ = θ⊙m , where   ⊙indicates the element - wise product . With this ,   we can get our final objective function as :   max / summationdisplaylogP(Y|X;θ , θ)(5 )   where θ= [ θ⊙m , θ⊙m , ... , θ⊙   m].7567To learn such masks , we develop our training   algorithm based on the edge - popup approach ( Ra-   manujan et al . , 2020 ) . Specifically , for each bi-   nary mask m , we introduce floating - point scores   s∈R. In the forward pass , we generate the   binary mask mby setting the top- k% with the   largest absolute value in sas 1 and the rest as 0 .   We denote such top- k% thresholding function as h ,   andm = h(s ) . We refer to the hyperparameter   k%as the sparsity ratio in subsequent sections of   this paper . During the backpropagation , we use the   straight - through gradient estimator ( Bengio et al . ,   2013 ) to approximate the gradient of the scores ,   where function h(·)is treated as the identity func-   tion . In addition to training the masks , we also   jointly optimize the prototype network .   Our approach employs learnable floating - point   scores for each binary mask during fine - tuning ,   leading to nLparameters . When integrated with   the prototype network , PP updates a total   ofnL+nparameters during the training phase ,   which is marginally more than the conventional   PEFT methods which have nLextra parameters .   A comprehensive overview of the PPalgo-   rithm can be found in Algorithm 1 . After train-   ing , we discard the floating - point scores and retain   only the binary masks ( 1 - bit ) together with the   shared prototype network ( 32 - bit ) . Assuming that   the 32 - bit prototype network requires pbit - level   storage and the binary mask of the same dimen-   sion demands p/32 , our PP can achieve a   substantial decrease in storage from around pLto   p(1 + L/32 ) . To reconstruct the network structure   during inference , we adhere to the following steps :   ( 1 ) first load the unpruned , shared 32 - bit prototype   network , ( 2 ) then load the binary masks ( 1 - bit ) for   each layer / task , and ( 3 ) extract and use the pruned   subnets from the shared prototype network based   on specific binary masks during the inference step .   3.4 Hybrid Masks for Multi - Task Learning   Rather than just sharing a PETL module across   layers under single - task learning , we can also al-   low for efficient sharing of the prototype network   across multiple tasks . In our approach , we lever-   age layer masks , as introduced in the previous sec-   tion , to support parameter sharing within the model .   Additionally , we introduce task masks to support   parameter sharing across multiple tasks . By per - forming a logical OR operationon these masks ,   we can obtain a hybrid mask for a specific layer in a   specific task , as shown in the right side of Figure 2 .   m = m∨m ( 6 )   With the design of the hybrid mask , given T   tasks and Nlayers in the pre - trained language mod-   els , we only require one PPmodule , Nlayer   masks , and Ttask masks , further reducing the BLS   from the single task scenario ( e.g. , 0.011 % BLS   as in Table 2 ) . In addition , layer masks and task   masks , which can be combined as hybrid masks ,   will potentially help infuse the knowledge into the   shared prototype network from layers and tasks .   4 Experimental Setup   We briefly summarize the experimental setup in this   section . More details can be found in Appendix C.   Datasets We evaluate PPon a wide range   of benchmarks , including language understanding   ( GLUE ( Wang et al . , 2018 ) ) , text summarization   ( XSum ( Narayan et al . , 2018 ) ) , and machine trans-   lation ( WMT16 Ro - En ( Bojar et al . , 2016 ) ) .   Backbones We use RoBERTa ( Liu et al . ,   2019 ) for single - task learning on GLUE . During   fine - tuning , we only tune our PP module   and the text classification head . For generation and   multi - task learning benchmark , we use T5(Raf-   fel et al . , 2020 ) and only tune the PPmodule .   Note that some previous works also tune the layer   norms ( Houlsby et al . , 2019 ; Mahabadi et al . , 2021 )   while we keep them frozen during fine - tuning .   PETL Modules and PP We use the Pfeif-   fer adapter ( Pfeiffer et al . , 2021 ) as our adapter   module and set the bottleneck dimension as 64 by   default . For prefix - tuning , we follow Li and Liang   ( 2021 ) and choose the prefix length to be 64 . As   for LoRA tuning ( Hu et al . , 2022 ) , the bottleneck   dimension and the scaling factor αare both set to   32 . In PP , we increase the value of αto 48   to scale the representation , as the sparse network   will decrease the norm of the output representation .   We give a brief summary of these PETL modules   and how PPis implemented on top of them   in Appendix A. Following Ramanujan et al . ( 2020 ) ,   we choose the sparsity ratio k% of PP as   0.5 , which we will also further discuss in Section 6.7568   In multi - task learning , we aim to maintain an ex-   pected k% around 0.5 for the hybrid mask , so we   set the k% to 0.3 for both the layer and task masks .   Evaluation For text generation , we report   ROUGE-2 ( Lin , 2004 ) on the XSUM test set and   BLEU ( Papineni et al . , 2002 ) score on the Ro - En   test set . Since the test sets of GLUE are not re-   leased publicly , following Zhang et al . ( 2021 ) and   Mao et al . ( 2022 ) , when the sample number of the   datasets is fewer than 10k ( RTE , MRPC , STS - B ,   CoLA ) , we divide the original validation set into   halves – the first half for validation and the sec-   ond for testing . As for the other datasets in GLUE ,   we randomly choose 1k samples from the training   set as our validation data and test on the original   validation set . we report both accuracy and F1 for   MRPC and QQP in GLUE . For STS - B , we report   both Pearson and Spearman correlation coefficients .   For CoLA , we report Matthews correlation . For   all remaining sub - tasks in GLUE , we report accu-   racy . Due to high training overhead for generation   tasks , we report experimental results with one run   for XSum and Ro - En . For GLUE , we report the   mean of three different random runs .   5 Results   5.1 Single - Task Learning   Results in Language Understanding In Table   1 , we report the performance of PP and   various baselines on the GLUE benchmark . Both   PP andPPdemonstrate su-   perior performance compared to their respective   counterparts ( adapter and LoRA ) . Despite havingslightly more parameters during the fine - tuning   process , PP requires only 1/9 ( 0.11 % v.s.   0.95 % ) of bit - level storage during inference , mak-   ing them more efficient options . Specifically ,   PPincreases the average score of the adapter   by 0.95 and improves the score of LoRA by 0.41 .   Besides , PP remarkably outperforms   the fully fine - tuned model ( 86.60 v.s. 86.16 ) while   using 0.11 % storage of the fully fine - tuned model .   These results indicate that although with reduced   parameters , PPinjected with the structure   information from masks can make more use of the   single prototype network and achieve better perfor-   mance compared to their counterparts . However ,   we also found that PPdid not outper-   form prefix - tuning , which we believe is caused   by the reparameterization in prefix - tuning that has   a harmful effect on the mask learning . Overall ,   PPincreases the performance of the adapter   to the greatest extent . PP also achieves   the highest performance among the three PP   variants . We will stick to PP for the rest   of the experiments .   Results in Language Generation To verify   PPcan also be applied to harder tasks , we   evaluate our method on two language generation   datasets , XSum and WMT16 Ro - En . The results   are presented in Figure 3 ( a ) and ( b ) . We find that   PP can perform just as well as the regu-   lar adapter method while using significantly less bit-   level storage . Additionally , when consuming more   than 1.6 % of the storage , PP is able   to achieve competitive performance on the XSum   dataset compared with the fully fine - tuned T5 .   However , both adapter tuning and PP   do not reach the level of the fully fine - tuned model   on Ro - En . One potential explanation is Ro - En is7569   harder because translation knowledge may not be   covered a lot during the pre - training process of T5 .   To perform well on such tasks , the model needs to   learn additional knowledge and requires more tun-   able parameters during fine - tuning . We note that   such sub - optimal performance on hard generation   tasks is not only a nature of PPbut generally   exists in all PETL methods . Similar findings are   also presented in Raffel et al . ( 2020 ) and He et al .   ( 2022a ) . Overall , these experiments show that our   PPis also more parameter - efficient on text   generation benchmarks compared to existing PETL   methods .   5.2 Multi - Task Learning   We present the results of multi - task learning and   also provide baseline results from single - task   learning using the T5 in Table 2 . Our best-   performing model , PP with bottleneck   dimension of 64 , surpasses the fully fine - tuned T5 .   We also compare PPwith Hyperformer++   ( Mahabadi et al . , 2021 ) , a hyper - network that   is specifically designed to transfer knowledge   across tasks . The latter uses significantly more   task - specific bit - level storage ( 26 ×:0.011 % v.s.   0.29 % per task ) , while only increasing the average   score by 0.51 . Compared to the vanilla adapter ,   PP can be marginally better under a   similar fine - tuning parameter budget but with 1/9   of the original storage . Besides , we experiment   with an extreme case , where we set the bottleneck   dimension to 6 . Our results show that the accuracy   of adapter tuning decreases from 85.48 to 83.94 ,   while PP still maintains a comparable   performance to the fully fine - tuned model ( 85.32   v.s. 85.47 ) with a remarkably small percentage   ( 0.001 % ) of bit - level storage per task . This demon-   strates that normal adapter tuning can not make   full use of the parameters and may fail to perform   well with a relatively small bottleneck dimension .   However , in the case of PP , even with   a bottleneck dimension that is only 6 , it can still   achieve a reasonable result . To further validate that   PPis effective for larger - sized models , we   also carry out experiments on the T5 3B variant   and present the findings in Table 3 . The outcomes   align with our conclusions drawn from the T5   base model . We believe that , in PP ,   the structure information learned by the mask can   make up for the performance drop due to the shrink   of the bottleneck dimension to a certain extent . We   further compare adapter tuning with PP   with different percentages of bit - level storage by   varying the bottleneck dimension . The results are   presented in Figure 3 ( c ) . It shows PP   is able to reach the fine - tuned performance with as   few as 0.002 % task - specific bit - level storage . We   can also see that the curve shares a similar trend   with those in Figure 3 ( a ) and ( b ) , which we will   further discuss in the next section .   6 Discussion   How does PP Scale Differently to   Adapter ? Figure 3 presents the results when we7570   adjust the size of the adapter and PP on   three different datasets . Despite the difference in   tasks and methods , we discover that adapter and   PP show similar scaling trends on all   three datasets when we increase the proportion of   the task - specific bit - level storage . Their perfor-   mance increases linearly with respect to the log   scale of the extra storage in the beginning . When   the adapter and PP reach close to the   performance of the fully fine - tuned model , their   performance gradually converges . Even though   PP and adapter tuning can slightly ex-   ceed the fully fine - tuned performance on some   datasets , their performance is still bounded to the   same level and can not outperform the fully fine-   tuned model by a large margin . For instance ,   the performance of both adapter and PP   starts to drop when the task - specific storage ex-   ceed 0.4 % per task on GLUE ( Figure 3 ( c ) ) . How-   ever , PP is able to reach the fully fine-   tuned level much earlier in the scaling curves .   Given a fixed amount of task - specific storage ,   PP is also able to achieve better results   than the adapter . These results indicate that our   share - and - mask method is overall more efficient   than the adapter across almost all scales .   Which is More Important , Sharing or Masking ?   In this section , we discuss the effect of masking   and sharing in the prototype network by compar-   ingPP with a random mask baseline and   two alternative settings ( only mask andonly share ) .   For the random mask setting , we randomly select   the sub - network of the prototype module during   the forward process rather than relying on the up-   dated mask score . Only mask involves not sharingthe module across layers but only learning masks   to prune different PETL modules into sparse sub-   networks . Only share shares a single PETL mod-   ule among layers without using masks for pruning .   Masking without sharing will drastically increase   the number of fine - tunable parameters if we keep   the same bottleneck dimension and sparsity ratio .   To keep the number of fine - tunable parameters on   the same level , we either keep the bottleneck di-   mension the same and reduce the sparsity ratio k%   or use the same sparsity ratio with reduced bottle-   neck dimension for the only mask setting . We also   slightly increase the bottleneck dimension of the   only share set up to compensate for the parame-   ters of masks . Our results , as presented in Table   4 , indicate that the random mask setting yields the   poorest performance . Moreover , neither only mask   noronly share reaches the same performance with   PP , highlighting the necessity to use both   masking and sharing to achieve higher parameter   efficiency . We believe masking injects crucial   structural information into the sub - networks , while   sharing is necessary to expand the size of each   sub - network when the number of fine - tunable pa-   rameters is fixed . Therefore , our method can use   the parameters more efficiently .   How do Sub - Networks ’ Size and Structure Af-   fect Each Other ? The sparsity ratio k%is an im-   portant hyperparameter in PP . We study the   impact of such sub - network sparsity and present   the results in Figure 4 . The performance of our   PP improves as k%increases from 0.1 to   0.5 but then declines as k%continues to grow from7571   0.5 to 1.0 . Additionally , it can be seen that all these   PETL methods can achieve the best accuracy when   k%is set to 0.5 . This is likely due to the fact that   as the network becomes denser from 0.1 to 0.5 ,   the sub - networks get to use more parameters and   thus obtain better modeling ability . However , after   0.5 , the network in different layers becomes more   homogeneous as the sub - networks overlap more ,   leading to less distinctive structural information in   each layer . The absence of enough structural infor-   mation starts to harm the performance , which even   outweighs the benefits of potential knowledge shar-   ing across the layers . We also discover that sharing   the PETL module without any pruning ( k%= 1.0 )   results in the worst performance among all spar-   sity levels . These results suggest that given the   fixed percentage of tunable parameters , it is cru-   cial to find a good balance between the distinctive   structural information of each sub - network and the   number of parameters used in each sub - network .   How is PP Conceptually Related to   PETL ? Other than the explanation of proto-   type network sharing and masking , our proposed   PP can also be considered as a PETL - on-   PETL approach , which we refer to as PETL . In   other words , the mask is to the prototype network   ( in our approach ) as the PETL module is to the   PLM ( in conventional PETL approaches ) . Vanilla   PETL methods , such as adapter and prefix - tuning ,   update specific additional parameters for each layer   and downstream task while only sharing the param-   eters of the PLM . In contrast , PP extends   this approach by sharing not only the PLM but   also the prototype PETL module among layers and   tasks , resulting in a higher degree of parameter   sharing . Our method uses binary masks that func-   tion like PETL modules on top of the prototype   PETL module to prune different structures in dif-   ferent sub - networks . These task - specific tunable   parameters are thus an order of magnitude smaller   than conventional PETL modules .   7 Conclusion and Future Work   In this paper , we introduce PP , a method   for sharing prototype PETL modules across dif-   ferent layers and tasks . Our method significantly   improves the parameter efficiency by utilizing the   prototype network and maintaining a binary mask   for each layer and task . Extensive experiments   show that our method achieves comparable perfor-   mance to fully fine - tuned models and conventional   PETL methods with a much smaller fraction of   storage . For future works , we aim to study the in-   terpretability of the masks in different layers and   explore their potential relationships . We also intend   to apply our method to the pre - training process of   large language models to reduce the overall number   of parameters .   Limitations   Although our masks in different layers are binary   and require significantly less storage compared to   other PETL networks , we still need the underly-   ing 32 - bit scores for each mask during the training   process . Therefore , PP consumes slightly   more memory in the training time than existing   PETL methods . To fine - tune PP , it takes a   similar training time to conventional PETL mod-   ules , which means our method will normally take   a longer time to converge compared to the fully   fine - tuned model.7572Acknowledgements   We would like to thank the anonymous review-   ers , our meta - reviewer , and senior area chairs for   their constructive comments and support on this   work . This research / project is supported by the   National Research Foundation Singapore and DSO   National Laboratories under the AI Singapore Pro-   gram ( AISG Award No : AISG2 - RP-2020 - 016 ) ,   and AI Singapore Programme ( AISG Award No :   AISG2 - PhD-2021 - 08 - 007 ) .   References7573   A Implementation Details of the Three   Variants of PP   A.1 PP   An adapter module modifies a model ’s hidden rep-   resentation through a down - sampling projection   and an up - sampling projection with a non - linear   layer in between :   h←h+f(hW+b)W+b ( 7 )   where Wrepresents the weight matrix , fdenotes   the non - linear layer and bis the bias term . In   PP , we apply our binary pruning masks   on the up - sampling and down - sampling weights ,   respectively :   h←h+f(h / tildewiderW+b)/tildewiderW+b ( 8)   where / tildewiderW = W⊙m and / tildewiderW = W⊙   m.   The original Houlsby adapter ( Houlsby et al . ,   2019 ) introduces the adapter module after each7574multi - head attention and feed - forward layer . Pfeif-   fer et al . ( 2021 ) later propose a more efficient vari-   ant of the adapter that is only inserted after the   feed - forward layer .   A.2 PP   LoRA ( Hu et al . , 2022 ) , like adapter tuning , also   includes a down - sampling projection and an up-   sampling projection . The difference is that LoRA   does not have any non - linear layers , but it does   have an additional scaling factor α≥1 :   h←h+α·xWW ( 9 )   To modify PLMs ’ hidden representation , LoRA   is applied to the query and value representations   of the attention modules in Transformers . In   PP , the network is pruned with binary   masks :   h←h+α·x(W⊙m)(W⊙m )   ( 10 )   The scaling factor αis an important parame-   ter for LoRA and Hu et al . ( 2022 ) use different   αfor different datasets in their released code . In   PP , applying the pruning masks mwill   reduce the norm of the output of the LoRA module .   We find that applying a larger αforPP   than that used in LoRA will remedy the issue of   reduced feature norm and result in better perfor-   mance .   A.3 PP   Li and Liang ( 2021 ) propose to insert tunable ma-   trices , which they call prefixes , into the key - value   pair of the Transformers ’ attention modules :   H= Attention ( Q,[P;K],[P;V ] )   = softmax(Q[P;K ]   √   d)[P;V](11 )   where [ · ; · ] stands for matric concatenation . They   also find that directly updating the Pparameters   will lead to unstable training and a bit drop in per-   formance . In order to ameliorate the problem , they   propose to reparametrize the matrix :   P = PW ( 12 )   where Pis a smaller learnable matrix and Wis   the weight of an up - projecting feedforward neural   network . In PP , we apply our binary   pruning masks on the reparametrized prefixes :   H= Attention ( Q,[P⊙m;K],[P⊙m;V ] )   ( 13 )   Note that different from adapter and LoRA tuning ,   the pruning masks in PPdo not directly   operate on the parameters of the network . Instead ,   the masks are applied on PandPthat are out-   put activation depending collectively on WandP.   Thus , it might be hard for the mask training pro-   cess to identify good structures from PandP ,   which potentially explains the sub - optimal results   ofPPin Table 1 . To verify this claim ,   we further compare 3 PETL modules against their   counterparts pruned with a sparsity of 0.5 in Ta-   ble 5 . We find that both adapter and LoRA tuning   improve their performance when 50 % of parame-   ters are pruned , which substantiates the idea that   structural information of sub - networks is important .   However , for prefix tuning that uses reparameteriza-   tion , its performance drops when we learn pruning   masks on top of it . This shows that the masks fail   to locate suitable structures for prefix tuning .   B Parameter Efficiency in PP   B.1 Overview   We present an approximate calculation of the   bits ( space ) required by PPduring inference   in Table 6 , in which we assume that a single shared   PETL network contains pBLS and the model has   Llayers . In addition , the storage space required for   the binary mask makes up 1/32(≈0.031 ) of the   32 - bit PETL module . Depending on the specific   PETL module used , the calculations may vary a bit ,   and we detail them in the following sections.7575   B.2 PP   Given the number of layers ( L ) , hidden dimension   ( d ) of the pre - trained language model , and bottle-   neck dimension ( bn ) of the adapter module , the   BLS consumed by PP is :   32·(2·bn·d+bn+d)/bracehtipupleft /bracehtipdownright / bracehtipdownleft /bracehtipupright+ 2·bn·d·L / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright(14 )   Note that our method does not apply any masks   on the bias terms in the prototype adapter . Since   ourPP will reuse the prototype network ,   we do not consider the pruning ratio k%when   calculating the storage . Here we also discuss how   to calculate the storage cost of parameters used   under the only mask setting in Table 4 , in which   we do not share the PETL module across layers but   only learn masks to prune them . In that case , we   do not count the pruned parameters because they   will never be reused . Therefore , the formula for the   BLS required by only mask is :   32·(2·k%·bn·d+bn+d)·L ( 15 )   B.3 PP   Similarly , given the number of layers ( L ) , hid-   den dimension ( d ) of the pre - trained language   model , and bottleneck dimension ( bn ) of the LoRA   module , the formula for the storage needed by   PPis :   32·4·bn·d / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright+ 4·bn·d·L / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright(16 )   Note that there is no bias term in LoRA . Under the   only mask setup , given the sparsity ratio k% , the   storage of parameters is :   32·4·k%·bn·d·L ( 17 )   B.4 PP   Given the number of layers ( L ) , hidden dimension   ( d ) of the pre - trained language model , and the pre-   fix length ( l ) of the prefix module , the formula to   calculate the BLS of PPis :   32·2·l·d / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright+ 2·l·d·L / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright(18)Under only mask settings , given the sparsity ratio   k% , the approximate required storage is :   2·k%·l·d·L ( 19 )   C Experimental Details   We briefly introduce the benchmark datasets used   in this work . Their statistics can be found in Ta-   ble 7 .   C.1 Datasets   GLUE The General Language Understanding   Evaluation ( GLUE ) benchmark ( Wang et al . , 2018 )   is widely used to benchmark models ’ language un-   derstanding ability . It consists of a broad range   of sentence - level tasks , including natural language   inference ( MNLI , QNLI , RTE ) , sentence similarity   ( MRPC ) , paraphrase detection ( QQP , STS - B ) , and   single sentence classification ( SST-2 , CoLA ) .   XSum The Extreme Summarization dataset   ( XSum ) ( Narayan et al . , 2018 ) is designed to evalu-   ate systems ’ abstractive summarization ability of a   single document . It is collected from the online arti-   cles of the British Broadcasting Corporation ( BBC ) .   The input is a single document with an average to-   ken count of 431.07 , and the model is expected to   generate a short , single - sentence summarization of   this document .   WMT16 Ro - En WMT16 is the 2016 edition   of the Workshop on Machine Translation ( WMT )   dataset ( Bojar et al . , 2016 ) . This dataset is widely   used to evaluate machine translation systems . To   benchmark on WMT16 Ro - En , the model is sup-   posed to take in a Romanian sentence and output   translation in English.7576   C.2 Implementation Details   Single - Task Learning on RoBERTa Our   models are implemented based on the Adapter-   Hub package ( Pfeiffer et al . , 2020 ) . We use the   datasets ( Lhoest et al . , 2021 ) library to calculate   each sub - task ’s scores in GLUE .   The learning rate of the PETL module is set to   1e-4 , and we detail the learning rate of pruning   masks in Table 8 . We find that it is important to   set a higher learning rate for the masks than the   learning rate of the PELT network . The batch size   is set to 128 and the weight decay is 0.1 in all   experiments with Roberta . When conducting ex-   periments on GLUE datasets , we train 10 epochs   when the dataset is large ( MNLI , QNLI , QQP , SST-   2 ) , while training 20 epochs on the small dataset   ( RTE , MRPC , STS - B , CoLA ) . Table 9 shows the   training time taken on a single A100 GPU .   Single and Multi - Task Learning on T5   We implement T5 based on the transformers li-   brary ( Wolf et al . , 2020 ) . We use the ROUGE   package ( Lin , 2004 ) for ROUGE-2 calculation and   sacrebleu ( Post , 2018 ) for the BLEU score . Ta-   ble 10 shows the training time on a single A100   GPU and the detailed hyperparameters used . We   mainly follow He et al . ( 2022a ) and Mahabadi et al .   ( 2021 ) to select the hyperparameters and do not   perform an exhaustive search . The same set of hy-   perparameters is used across the fully - finetuning ,   adapter tuning , and PPmodels . For GLUE ,   we follow Mahabadi et al . ( 2021 ) to sample data   from each GLUE sub - task with a temperature of 10 .   Specifically , a sub - task is sampled with probability   pwhere p = andT= 10 .   D Additional Ablation Studies   D.1 Choice of Mask Combining Methods   As shown in Table 11 , using the OR logical opera-   tion to combine the layer mask and the task mask   achieves the best performance . This is intuitive   because given a specific task and a Transformer   layer , parameters that contain the layer information   and parameters that contain the task information   should be both used in the forward pass .   D.2 Choice of Sharing and Masking   We provide additional details of the experiments in   Table 4 in this section . With the equations shown   in Appendix B , we calculate the bottleneck dimen-7577   sion ( bn ) and sparsity ratio ( k% ) to ensure all the   settings have a similar BLS to PP . We also   add one more setup , only mask with the same k%   andbnasPP . Note that this new setup will   result in more storage than the other settings . The   hyperparameters are listed in Table 12 and the re-   sults are detailed in Table 13 .   We find that on the GLUE dataset , PP   achieves matched performance to this new setup ,   even though the latter has used more than 4 times   the bit - level storage ( 0.11 % v.s. 0.49 % ) . We be-   lieve that , with masks , models with only 0.11 % of   storage can be enough to have a good performance   while more storage cost will not have a significantimprovement or lead to overfitting under simple   tasks like GLUE . However , on the more challeng-   ing dataset Ro - En , when we keep the k%andbn   unchanged and only mask the adapter modules , the   model indeed improves its performance by 0.65 ,   but this comes at a cost of around 7 ×more storage   of parameters .   E Additional Results   We additionally present the experimental results   from Figure 3 in table format in Table 14 , 15 ,   and 16.7578ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section Limitations .   /squareA2 . Did you discuss any potential risks of your work ?   Section Ethics Statement .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4 and Appendix C   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4 and Appendix C   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Apache License 2.0 gives permission on Commercial use , Modiﬁcation , Distribution ,   Patent use and Private use .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . The dataset we applied is a commonly used open - source benchmarks datasets in   NLP .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Appendix C   C / squareDid you run computational experiments ?   Section 5 and 6   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix C7579 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix C   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4 , 5 and 6   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section C   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.7580