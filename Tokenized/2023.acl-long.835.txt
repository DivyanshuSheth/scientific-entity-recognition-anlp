  Lining Zhang , * Simon Mille , Yufang Hou , Daniel Deutsch , Elizabeth Clark , Yixin Liu ,   Saad Mahamood , Sebastian Gehrmann , Miruna Clinciu , Khyathi Chandu , João Sedoc   Abstract   To prevent the costly and inefficient use of re-   sources on low - quality annotations , we want a   method for creating a pool of dependable an-   notators who can effectively complete difficult   tasks , such as evaluating automatic summariza-   tion . Thus , we investigate the recruitment of   high - quality Amazon Mechanical Turk workers   via a two - step pipeline . We show that we can   successfully filter out subpar workers before   they carry out the evaluations and obtain high-   agreement annotations with similar constraints   on resources . Although our workers demon-   strate a strong consensus among themselves   and CloudResearch workers , their alignment   with expert judgments on a subset of the data   is not as expected and needs further training   in correctness . This paper still serves as a best   practice for the recruitment of qualified annota-   tors in other challenging annotation tasks .   1 Introduction   Natural language generation ( NLG ) tasks like text   summarization are challenging to evaluate both in   terms of automatic metrics and human evaluations   ( Gehrmann et al . , 2022 ) . Although automatic met-   rics are inexpensive proxies for human annotations   for tasks like dialog evaluation ( Mehri et al . , 2022 ) ,   they may have problems dealing with paraphrases ,   capturing distant dependencies , or identifying nu-   ances in human languages ( Banerjee and Lavie ,   2005 ; Isozaki et al . , 2010 ; Manning et al . , 2020 ) .   Thus , it is still crucial to obtain high - quality human   annotations as gold labels for evaluation . Amazon   Mechanical Turk ( MTurk)is a commonly used   crowdsourcing platform for collecting human an-   notations on designed tasks , known as Human Intel-   ligence Tasks ( HITs ) . However , finding qualified   workers for high - quality annotations with a bet-   ter inter - annotator agreement ( IAA ) is challenging , Figure 1 : Two - step pipeline for finding high - agreement   MTurk workers : participants who satisfy basic qualifi-   cation settings and answer designed questions correctly   ( Qualification ) are subsequently filtered in a longer task   ( Endurance ) . The maintained worker list is tested for   the true annotation task later ( Reference - based ) .   especially for difficult tasks such as text summa-   rization . Best practices for recruiting high - quality   workers are also poorly understood , and the rela-   tionship between high quality and high agreement   needs further investigation .   To tackle the above issues , we design a recruit-   ment pipeline to identify workers who are able to   produce high - agreement annotations for the eval-   uation of text summarization on MTurk . It com-   prises a qualification task and an endurance task ,   followed by a reference - based task ( see Figure 1 ) .   In the qualification task , workers who meet pre-   defined qualification settings receive instructions   and qualification questions , including an attention   check ( Oppenheimer et al . , 2009 ) . The qualifica-   tion questions are designed to assess the annota-   tor ’s ability to evaluate multiple dimensions of a14944summary correctly . Performance on this task de-   termines whether they are categorized into , , , or . Only the best work-   ers ( and ) move on to the endurance   task , which consists of 10 HITs with 4 summaries   in each to evaluate . This task only tests the sum-   mary ’s saliency , which is the most subjective di-   mension ( Howcroft et al . , 2020 ) , but it challenges   the annotator ’s capacity for handling a heavy an-   notation workload . and workers   who complete all HITs are added to a maintained   worker list as high - agreement annotators for future   tasks . To ensure their general performance for the   true annotation task , a reference - based task to eval-   uate information coverage between summaries is   conducted with these workers later .   While serving as a best practice beyond its scope ,   our study has the following contributions :   •establish a cost - effective recruitment pipeline   on MTurk to consistently build a pool of an-   notators for high - agreement annotations .   •successfully recruit 12 out of 200 ( 6 % ) supe-   rior annotators for text summarization evalu-   ation , while reducing costs and guaranteeing   high agreement .   •rigorously demonstrate that the annotators   identified through our pipeline can match or   surpass the IAA of expert annotators and stan-   dard statistical techniques , though further cal-   ibration may be required for correctness .   2 Related Work   Challenges of Human Evaluation Compared   to automatic evaluation metrics for NLG tasks like   BLEU ( Papineni et al . , 2002 ) and ROUGE ( Lin ,   2004 ) , human annotations from non - expert annota-   tors on MTurk can reach an agreement with gold   standards or expert judgments ( Callison - Burch ,   2009 ) . Although recent works leverage language   models like BERT ( Devlin et al . , 2019 ) to get better   automatic evaluations ( Zhang et al . , 2020 ) , human   judgments are still indispensable in identifying nu-   ances in specific language tasks ( Manning et al . ,   2020 ) . Finding qualified workers to carry out the   evaluations is crucial . This is especially true for   tasks like text summarization , which lacks consen-   sus on evaluation protocols ( Fabbri et al . , 2021 )   and is often inconsistent with previous human eval-   uations ( Hardy et al . , 2019 ) . However , human eval-   uation from non - expert crowdsourcing platforms   have low quality ( Gillick and Liu , 2010 ) and a sim - ple qualification filter is not sufficient to identify   qualified workers ( Berinsky et al . , 2012 ; Robinson   et al . , 2019 ) . Some studies applied quality control   mechanisms to filter out poor quality annotations ,   resulting in a relatively low pass rate for a variety of   tasks ( Graham et al . , 2017 , 2018 ; Mille et al . , 2019 ) .   The fact that up to 70 % of the HITs are eventually   discarded indicates a huge resource waste .   Even with qualified workers , human annotations   might still be adversely affected by factors like   incomplete instructions or unfair wages paid to an-   notators ( Huynh et al . , 2021 ) , and workers need   clear references , schemes , or standards to follow   ( Howcroft et al . , 2020 ; Karpinska et al . , 2021 ) .   Thus , our study serves as a detailed reference for   finding qualified MTurk workers for a summariza-   tion evaluation task and further identifying those   who can assist in a large number of annotations .   Inter - Annotator Agreement For annotations   without true labels or those evaluated with a qual-   itative scale such as Likert scale ( Likert , 1932 ) ,   the inter - annotator agreement ( IAA ) among MTurk   workers measures the reliability of the annotations .   For example , Cohen ’s Kappa ( Cohen , 1960 ) mea-   sures IAA between a pair of results of the same   length from two annotators , while Krippendorff ’s   Alpha ( Hayes and Krippendorff , 2007 ) measures   the agreement of a set of results from any num-   ber of annotators , even with unequal sample sizes .   Both range from −1to1 , with 1indicating com-   plete agreement . Further studies also continue to   mitigate annotator bias through complementary   methods to IAA ( Amidei et al . , 2020 ) , aimed at   high - quality annotations . In our study , we utilize   both Cohen ’s Kappa and Krippendorff ’s Alpha as   the measurement of annotation reliability .   3 Methods   In this section , we detail how the workers were   recruited and which tasks were carried out .   3.1 MTurk Qualification Settings   To narrow down the pool of our target workers ,   we set a few pre - defined qualifications for work-   ers on MTurk before publishing the qualification   task : ( i ) the Location is set to “ UNITED STATES   ( US ) ” ; ( ii ) the Number of HITs Approved is set   to be “ greater than 1000 ” to target workers who   are already experienced on MTurk ; ( iii ) the HIT   Approval Rate ( % ) is set to be “ greater than or14945equal to 99 ” to target workers who are able to fin-   ish tasks with high quality and have stable perfor-   mance . We also set the task visibility as “ Private ” ,   which means our tasks are visible to any worker ,   but only workers who meet all qualification require-   ments can preview and accept .   Paolacci et al . ( 2010 ) show that the annotations   collected with the “ Location ” setting on MTurk   are representative of the population of our target   country in terms of demographic data . This helps   mitigate biases introduced by samples from tradi-   tional recruitment methods like college undergrad-   uate samples ( Buhrmester et al . , 2011 ) . We set   qualification settings ( ii ) and ( iii ) based on previ-   ous work ( Whiting et al . , 2019 ; Oppenlaender et al . ,   2020 ; Kummerfeld , 2021 ) and our own experience   on MTurk . Workers who meet all qualification   requirements are eligible to participate in the quali-   fication task .   3.2 Qualification Task   Summarization task In summarization , the in-   put is the text of a document and the output is a   short summary . We evaluate a summary Saccord-   ing to 6 dimensions based on the criteria taxonomy   presented in Howcroft et al . ( 2020 ) , and workers   are asked for a binary answer as to whether a di-   mension is satisfied in a summary or not :   •Understandability : can the worker under-   stand Sand is Sworth being annotated .   •Compactness : Sdoes not contain duplicated   information .   •Grammaticality : Sis free from grammatical   & spelling errors .   •Coherence : Sis presented in a clear , well-   structured , logical , and meaningful way .   •Faithfulness : all of the information in Scan   be found in the article ; Saccurately reflects   the contents of the article .   •Saliency : Scaptures the most important in-   formation of the article and does not include   parts of the article that are less important .   Training and qualification There are two main   parts of the qualification task . The training part   guides the workers through the above evaluation   dimensions and instructs them on how to annotate .   The definition of each dimension is illustrated with   positive and negative examples , and full annotation   examples are shown ( summary and binary rating   for each dimension ) . Then , workers are required to   write an instruction summary in their own wordsto make sure they have understood the task and are   ready to annotate . The qualification part tests the   worker ’s understanding of the task . Three docu-   ments are provided , each with one summary . The   worker reads the document and annotates the cor-   responding summary according to each dimension .   The ratings are then compared to expert ratings   provided by the authors of this paper . The last   document comes with an attention check to test   whether a worker is just randomly assigning scores   without reading : a highlighted instruction asks the   worker to ignore the task and select specific an-   swers . Finally , an optional field is provided to   collect feedback .   Worker categorization Upon finishing their   task , workers are categorized into four types :   •GOLD . The workers pass the attention   check and annotate every dimension of every   document in the qualification part correctly .   •SILVER . The workers pass the atten-   tion check and make only one mistake when   annotating each dimension of the documents   in the qualification part .   •BRONZE . The workers pass the at-   tention check and make more than one mis-   take when annotating each dimension of the   documents in the qualification part .   •BLOCK . The workers fail to pass the   attention check .   The and workers are assigned a   qualification score and proceed with the endurance   task . Besides , we conducted multiple rounds of the   qualification task to avoid influence from the time   or day when the task was conducted and randomly   sampled workers ( Arechar et al . , 2017 ; Berinsky   et al . , 2012 ) .   3.3 Endurance Task   The endurance task is designed to test whether a   worker can reliably perform a large number of an-   notations . The workers who finish all HITs of this   task are assigned the highest qualification score and   are added to a maintained worker list .   The endurance task comprises 10 HITs . For   each HIT , a document and 4 corresponding sum-   maries generated by different models are provided ;   each HIT takes around 5 minutes to finish ( approx-   imately an hour for all HITs ) . To keep the task   simple we only evaluate each summary on one di-   mension , but to ensure that the task is challenging   enough we ( i ) use the most subjective of the 6 di-14946   mensions , Saliency , and ( ii ) use a more fine - grained   10 - point Likert scale ( from 1 to 10 ) .   Rationale for choosing 10 HITs Our motiva-   tion is two - fold : to find workers who were able to   complete many tasks and whose annotations are   better than random . As the number of HITs in-   creases , the number of remaining workers drops   from 26 to 12 . The survival rate defined by the   Kaplan – Meier estimator ( Kaplan and Meier , 1958 )   is 38.59 % when the number of HITs is set to 10   which is an estimate of a worker ’s capacity to be   able to complete many tasks . We empirically found   that we need a minimum of 8 HITs completed by   a worker in order to validate that their annotations   are statistically significantly different from random   noise ( see Table 2 ) .   3.4 Reference - based Task   Finally , to test whether the selected MTurk workers   actually perform better at annotating summaries   in general , we conduct a reference - based task that   comprises 30 HITs . In each HIT , a reference sum-   mary and 4 candidate summaries are provided . The   worker is asked to assign each candidate summary   two scores ( “ can2ref ” score and “ ref2can ” score)on a scale from 1 to 5 . The “ can2ref ” score indi-   cates whether all of the information in the candi-   date summary can also be found in the reference   summary , while the “ ref2can ” score checks the   converse coverage direction . A score of 1 means   that almost no information in one summary can   be found in the other , while a score of 5 indicates   complete information coverage . The worker is pro-   vided with instructions and examples of the rating   at the beginning of the task .   4 Results   4.1 Annotation Data and Cost   The collected experimental data not only contained   annotation results but also metadata reflecting anno-   tator behaviors . The cost of annotation on MTurk   included both the wages paid to MTurk Workers   and the fees paid to MTurk ( which may vary ac-   cording to the task ) . A worker who participated   in the qualification and the endurance tasks earned   $ 8.5 ( $ 1 for the qualification task plus $ 7.5 for the   endurance task ) on average , while a worker who   participated only in the qualification task ( i.e. who   did not qualify ) earned $ 1 on average . Given the   total cost of $ 514 for the entire pipeline which   yielded 12 workers , the cost of identifying a quali-   fied worker is $ 42.8 . For details , the breakdown of   the cost is shown in Table 3 .   4.2 Qualification Task Results   We conducted four rounds of the qualification task ,   each round included 50 MTurk workers ( see Table   1 ) . This choice of multiple rounds aimed to guaran-   tee the stability of the annotation results ( Berinsky   et al . , 2012 ; Arechar et al . , 2017 ) . The overall pass   rate of the attention check was 0.69 ; thus , 62 work-   ers in total did not pass the attention check and14947   were categorized as . Out of 200 MTurk   workers , there were only 8 workers and 18 after the qualification task . Thus , only 26   MTurk workers ( 13 % of all participants ) qualified   for the endurance task .   For each round , we calculated Krippendorff ’s   Alphato measure the agreement among annota-   tors . The highest Krippendorff ’s Alpha was 0.33   reached by the first round , and the average Krip-   pendorff ’s Alpha of all four rounds was 0.25 . In   addition , the exclusion of workers led to an   increase in Krippendorff ’s Alpha , compared to the   value calculated on all workers . The highest Krip-   pendorff ’s Alpha without workers was 0.44   ( second round ) , and the average Krippendorff ’s Al-   pha of all four rounds increased to 0.41 . These   results showed that , as expected , workers   seemed to lack good - faith effort in the task and   likely yielded low quality annotations .   4.3 Endurance Task Results   We published the same endurance task for   and workers separately , and reported IAA   using Cohen ’s Kappa and Krippendorff ’s Alpha   among each type of worker ; we also reported simi-   lar IAA results from combined and   workers . We additionally collected endurance task   results from volunteer researchers unrelated to this   paper for a comparison between MTurk workers   and NLG “ experts ” .   SILVER Workers There were 18 work-   ers after the qualification task , 13 of whom ac-   cepted the endurance task . However , only 8   workers finished all 10 HITs – a yield rate of around   44 % given the number of workers enter-   ing this task . To calculate the IAA , we considered   the annotation scores of all summaries ( 40 ratings )   for each of the 8 workers and calculated Cohen ’s   Kappa for each worker pair ; the highest Cohen’sKappa was 0.451 between workers SandS.   To avoid influence from a possible unstable perfor-   mance at the beginning of the task , we also tried   to omit the first two HITs , that is , we only used 32   ratings when calculating Cohen ’s Kappa ; the result-   ing improvement for Cohen ’s Kappa was very low .   In addition , we calculated Krippendorff ’s Alpha on   the entire annotation results for all summaries and   workers , and it reached 0.358 .   GOLD Workers There were 8 workers   after the qualification task and 6 of them accepted   the endurance task . However , only 4 workers   finished all 10 HITs , for a yield rate of around 67 %   given the number of workers entering this   task . This rate was higher than that of   workers . We calculated pairwise Cohen ’s Kappa   using all the scores , and the highest IAA score   increased to 0.48 , compared to 0.45 for   workers . There was no significant improvement   after omitting the first two HITs . Krippendorff ’s   Alpha for the workers reached 0.443 , which   is higher than with workers ( 0.358 ) .   GOLD and SILVER Workers To investigate   IAA of worker pairs across and   workers , we combined the results of these two cate-   gories of workers and calculated pairwise Cohen ’s   Kappa . The highest pairwise Cohen ’s Kappa on   the 40 ratings per worker was 0.55 ; see the matrix   in Figure 2 . Again , omitting the first two HITs   also did not change the scores much . For Krippen-   dorff ’s Alpha , the value was 0.396 , which fell in   the range between the worker ’s ( 0.358 ) and worker ’s ( 0.443 ) values .   In Appendix A.2 , we show a breakdown of the   results per text position in each HIT ( correlations   for all first texts , for all second texts , etc . ) for each   of the three subgroups ( , , ) ; the possibly sightly darker heat maps14948could indicate higher correlations for the second   text of each HIT .   Comparison to Expert Ratings To get an idea   of the quality of qualified MTurk workers accord-   ing to our approach , we compared their IAA with   the IAA obtained by conducting the same en-   durance task with three researchers as NLG “ ex-   perts ” . The pairwise Cohen ’s Kappa for all 40   ratings only reached 0.268 ( see Table 10 in Ap-   pendix A.3 ) . The IAA among the experts was com-   paratively lower than the and work-   ers , indicating that qualified workers identified by   our tasks reached a better agreement at least for the   endurance task . Thus , it seems possible to recruit   high - quality workers using our pipeline .   Detection of Abnormal Workers From Cohen ’s   Kappa scores shown in Figure 2 , the worker S   had much lower agreement scores ( heatmap in the   yellow colors on the row and column corresponding   to the worker ) . Recent studies have uncovered the   presence of bots on MTurk ( Webb and Tangney ,   2022 ) . To understand the reason for this worker ’s   lower agreement with other workers , we analyzed   their online behavior using the metadata extracted   from their annotation results .   Figure 3 shows the timeline of each of the 10   HITs as a horizontal gray line . The timelines are   plotted from top to bottom , corresponding to the   first to the last HIT in the endurance task . The   X - axis represents the duration between the time of   acceptance and submission , which is normalized   by the duration for each HIT ( ranging from 0 to 1 ) .   Different marks present each annotator behavior , as   shown in the legend . Among these behaviors , blue   points represent the time when the MTurk workerassigned a score for one of the four summaries ,   and the corresponding number on top represents   the summary index ( valued from 0 to 3 ) . Orange   crosses denote the suggested reading time of the ar-   ticle in each HIT , given the average human reading   speed of 130 words per minute . If the suggested   reading time after normalization was longer than   the duration , we marked the orange cross as 1 at   the time of submission which is at the end of the   gray line .   Most of the orange crosses were marked at the   end of the timelines in Figure 3 ( right ) , indicat-   ing this worker assigned scores and submitted the   HIT in less time than it usually takes for a hu-   man to even finish reading the article . This result   demonstrates that this worker may not have put in   good faith in the endurance task , which possibly   explains the low IAA with other workers . By re-   moving this worker and calculating Krippendorff ’s   Alpha again within and workers , the   IAA increased to 0.454 ( compared to 0.396 when   including the worker ) .   4.4 Reference - based Task Results   To test the reliability of our qualified workers and   compare them to workers who do not undergo our   selection process , we launched the reference - based   task ( see Section 3.4 ) , which is open to our quali-   fied workers as well as to any other workers satis-   fying basic qualification settings .   Qualified Workers after Pipeline We published   the reference - based task to the 12 MTurk workers   from four rounds who have passed both the qual-   ification and the endurance task . All 12 workers   accepted this task but only 8 workers finished 30   HITs within a week .   There are two scores to evaluate the informa-   tion coverage between each candidate summary   and the reference summary . We use the “ can2ref ”   score to represent whether all information in the   candidate summary can be found in the reference   summary , and the “ ref2can ” score to represent the   converse coverage . For both types of scores , we   calculated Cohen ’s Kappa for every worker pair   ( given 4 candidate summaries per HIT , 30 HITS   per worker ) . Cohen ’s Kappa for “ can2ref ” score   ranges from 0.15 to 0.71 , with a relatively high   IAA between the first workers from the first   two rounds ( GandG ) . Similarly , Cohen ’s   Kappa for “ ref2can ” score ranges from 0.14 to 0.66.14949   Finally , Cohen ’s Kappa for the combined scores   ranges from 0.15 to 0.68 ( see Figure 4 ) , demonstrat-   ing that the agreement numbers are stable across   multiple measures . Krippendorff ’s Alpha for the   above scenarios ( “ can2ref ” score , “ ref2can ” score ,   and combined ) are 0.558 , 0.508 , and 0.534 .   Baseline MTurk Workers For comparison , we   published the same reference - based task to MTurk   workers who did not participate in our previous   experiments . 276 MTurk workers participated and   each worker finished on average 2 HITs ( In total   30 HITs ×20 Assignments / HIT ) . Krippendorff ’s   Alpha for “ can2ref ” , “ ref2can ” , and the two com-   bined were extremely low , at 0.087 , 0.077 , and   0.080 respectively , demonstrating the necessity of   a high - quality recruitment pipeline . We experi-   mented with the following approaches to inves-   tigate whether we could increase the agreement   between random MTurk workers to a level compa-   rable to qualified workers from our pipeline .   IAA with Median Among the 20 assignments of   each HIT , we randomly divided the work-   ers into 4 groups of 5 workers and took the   median of each group representing a “ newworker ” ( Lau et al . , 2014 ) . Then , we concate-   nated the results of 20 HITs for the 4 “ new   workers ” to calculate IAA . Krippendorff ’s   Alpha scores increased to 0.191 , 0.185 , and   0.188 respectively .   Filter on Timing and Number of Finished HITs   To exclude unqualified workers whose an-   notations may decrease IAA , only workers   who ( i ) spent more than the suggested reading   timeand ( ii ) finished 3 or more HITs were   selected for calculation of IAA . This resulted   in 25 workers remaining , but Krippendorff ’s   Alpha remained almost the same as calculated   without the filter .   Statistical Filter ( MACE ) We applied the Multi-   Annotator Competence Estimation ( MACE )   ( Hovy et al . , 2013 ; Paun et al . , 2018 ) to iden-   tify reliable workers based on competence   scores calculated on annotations . The work-   ers with competence scores above a threshold   were kept . We additionally calculated Spear-   man ’s coefficient ( Spearman , 1904 ) within the   groups of our pipeline and MACE ( see Ta-   ble 4 ) . We report the results of additional   failed attempts to improve Spearman ’s coeffi-   cient across these two groups , in Table 12 in   the Appendix .   In summary , the most effective methods to im-   prove agreement numbers among random workers   were median grouping and MACE . IAA on median   scores can raise Krippendorff ’s Alpha to almost   0.2 . MACE increases Krippendorff ’s Alpha as the   threshold increases , but at the cost of an incom-   plete HIT coverage ( 27/30 and 18/30 respectively   for the threshold of 0.6 and 0.7 in Table 4 ) and   fewer workers per HIT ( 1.9 and 1.2 , respectively ,   for the threshold of 0.6 and 0.7 in Table 4 ) . Sim-   ilarly , Spearman ’s coefficient of MACE workers14950   can be increased above our pipeline workers ’ only   at the same expense as above .   CloudResearch MTurk Workers To fur-   ther test our pipeline , we conducted the same   reference - based task on the CloudResearch plat-   form ( cloudresearch.com ) , which helps researchers   recruit high - quality annotators . We recruited the   same number ( eight ) of CloudResearch workers as   our pipeline . The Krippendorff ’s Alpha and Co-   hen ’s Kappafor CloudResearch workers is slightly   lower than our pipeline workers ( see Table 5 and   Figure 9 ) . Additionally , we found that our pipeline   workers have a higher task acceptance rate . This   results in a shorter experimental period compared   to the task conducted on CloudResearch .   Analysis of Correctness Across Annotation   Sources We randomly sampled 50 annotation   questions from the reference - based task to test cor-   rectness , which is defined as the alignment with   expert judgments . In addition , we also compared   the expert judgment with scores generated by GPT   models : GPT-3.5 ( “ text - davinci-003 ” ) and Chat-   GPT which are built on InstructGPT ( Ouyang et al . ,   2022 ) , and GPT-4 ( OpenAI , 2023 ) . Scores are ag-   gregated by taking the median within groups of   pipeline , MACE , and CloudResearch workers , as   well as experts . For ChatGPT we ran inference   5 times with default parameters ( temperature=1 ,   top_p=1 ) and took the median . To obtain GPT-3.5   and GPT-4 scores temperature was set to 0 with a   single run .   We did not find that pipeline workers were su-   perior to MACE workers in terms of correctness .   Pipeline and CloudResearch workers had a signif-   icant Spearman ’s correlation with each other ( see   Figure 5 ) , which indicates a reproduction of the re-   cruitment procedure on CloudResearch at a lower   cost . However , the confidence intervals are too   wide to draw any conclusion about the correlation   between crowd annotators and expert judgments   ( see Table 6 ) . This indicates that the pipeline may   not guarantee the training of the correctness of an-   notations . However , we found that GPT models   correlated well with expert judgments . Further de-   tails can be found in Appendix A.7 and A.8 .   4.5 Discussion   In Section 4.4 , we published the same reference-   based task as a test to different crowd annotators14951(pipeline , MACE , and CloudResearch ) . It showed   that filtering workers before the actual evaluation   task ( pipeline ) can avoid the waste of time and re-   sources and achieve high agreement at a lower cost   and a full coverage of HITs , compared to discard-   ing annotations after the task ( MACE ) ( see Table   7 ) . Our pipeline also recruited workers of similar   quality to CloudResearch at a lower cost ; however ,   based on further analysis , the correctness of annota-   tions was not guaranteed ( see Section 7 for details ) .   Besides , details about the estimated cost of GPT   models for the reference - based task can be found   in Table 15 in Appendix A.8.2 .   5 Statistical Test for Stability of Pipeline   We next examined whether there was a difference   in the probability of passing the qualification and   endurance task among MTurk workers . Thus , we   started by assuming the probability of passing each   task for each round came from the same distribu-   tion , and we performed a statistical test as follows .   LetXdenote the random variable representing   the MTurk worker . For the qualification task , let   q(x)denote the binary random variable which   has the value of 1 if the worker can pass the task ,   and 0 otherwise . Similarly , let e(x)denote   the binary random variable indicating whether the   worker can pass the endurance task . Given 50   MTurk workers in each round , we use Qto de-   note the binary random variables in a round as ( 1 ) .   It can also be regarded as examples sampled from   q(x ) . Among the samples , the probability of aworker who can pass the qualification task is equal   to the expectation of q(x ) = 1 as ( 2 ) . Since   only workers who passed the qualification task are   eligible for the endurance task , the probability of a   worker passing the endurance task is equal to the   expectation of e(x ) = 1 as ( 3 ) , which is   a joint distribution of q(x)ande(x ) .   Q={q(x ) , ... , q(x ) } ( 1 )   P(q(x ) = 1 ) = E(q(x ) = 1 ) ( 2 )   P(e(x ) = 1 )   = E(e(x ) = 1 ) ( 3 )   = P(e(x ) = 1|q(x ) = 1 ) P(q(x ) = 1 )   Thus , we used the Bootstrap method ( Efron ,   1992 ) with 10,000 iterations to estimate the mean   and standard deviation of the probability of pass-   ing the qualification and endurance task . Table 8   shows the results of all rounds with breakdowns of   each round . We can see some variance that might   come from MTurk workers given each round . To   test whether there is a difference in the probability   of passing each task among different rounds , we   conducted the permutation test ( Fisher , 1936 ; Pit-   man , 1937 ) for every two rounds . The results show   that we can not reject the null hypothesis that the   underlying distributions of every two rounds are   the same ( see Appendix A.4 ) .   6 Conclusion   In this paper , we present a two - step recruitment   pipeline that yields 12 qualified workers ( 4   and 8 workers ) out of 200 MTurk work-   ers with basic qualification settings in our exper-   iments . We show that workers identified by our   pipeline can ( i ) achieve a higher inter - annotator   agreement than expert annotators in the endurance   task , ( ii ) outperform the statistical filter ( MACE )   that discards annotation after the reference - based   task , and ( iii ) replicate a proxy of CloudResearch   annotations in the correctness analysis . Though the   6 % yield rate is not as expected , our pipeline serves   as the best practice to deliver high - agreement an-   notations and addresses the widespread waste of   resources on low - quality annotations through fil-   tering out subpar workers before they embark on   large - scale tasks . In the future , we plan to build   up a pool of reliable annotators who can deliver   high - quality ( both high agreement and correctness )   evaluations on a large scale and in multiple tasks ,   languages , and platforms.149527 Limitations   This research creates a relatively complete pipeline   to identify qualified MTurk workers for high-   quality human evaluations based on existing tech-   niques , and thoroughly tests the effectiveness of   this pipeline both qualitatively and quantitatively   compared to other methods . However , there are   several limitations of this work :   •The experiments are only conducted for   summarization tasks in English on MTurk   platform . Thus , this pipeline can also be   tested on other NLG tasks , in other languages ,   and on other platforms to see whether our   three - step concept generalizes broadly to all   human evaluations .   •The specific questions designed for each   task are not “ panacea ” solutions . A better   HIT design may exist for different experimen-   tal purposes , as long as it follows the ideas   behind each task . For example , the endurance   task aims to ensure the worker ’s reliable per-   formance on a large number of annotations , so   modifications based on this idea might work   better in case - by - case scenarios .   •There is no guarantee for the training of   correctness in the pipeline though a high   agreement is achieved . An additional correct-   ness check might need to be included along   with the endurance task to achieve both high   agreement and correctness through the filter-   ing of the pipeline .   8 Ethical Considerations   Considering that crowd workers are often under-   paid , experiments in this work all followed fair   working wage standardswhen using MTurk for   recruitment purposes ( details for each task are in   Table 3 ) . In addition , we have not rejected the   work from any unqualified workers so far , though   we reserve the right to do so when conducting the   experiments .   In our experiments , personal data ( any informa-   tion relating to an identifiable natural person ) was   collected , processed , and stored based on certain   data protection regulations , given relevant pri-   vacy concerns . Special category information ( i.e.personal data revealing racial or ethnic origin , etc . )   was not included in this work . More information   about the details of human evaluation experiments   in this work can be found in the Human Evaluation   Datasheet ( HEDS ) ( Shimorina and Belz , 2022 ) in   the Appendix .   Acknowledgements   We would like to thank the anonymous reviewers   for their helpful feedback on our paper . We would   like to thank Claire Daniele for her editorial sup-   port . Mille ’s contribution was funded by the Eu-   ropean Union under the Marie Skłodowska - Curie   grant agreement No 101062572 ( M - FleNS ) .   References149531495414955A Appendix   A.1 Proportion of Worker Categories in Qualification Task for Each Round   Annotation TaskTotal Number   of Workers   Workers   Workers   Workers   Workers   Qualification   TaskRound 1 50 1 ( 2 % ) 4 ( 8 % ) 32 ( 64 % ) 13 ( 26 % )   Round 2 50 3 ( 6 % ) 5 ( 10 % ) 29 ( 58 % ) 13 ( 26 % )   Round 3 50 2 ( 4 % ) 3 ( 6 % ) 24 ( 48 % ) 21 ( 42 % )   Round 4 50 2 ( 4 % ) 6 ( 12 % ) 27 ( 54 % ) 15 ( 30 % )   A.2 Cohen ’s Kappa for Each Summary in Endurance Task   For the figures below , “ Answer.score_0 ” to “ Answer.score_3 ” correspond to the scores aggregated from   the 1st to 4th summary separately for each HIT . The dark color indicates a high IAA in terms of Cohen ’s   Kappa score . S42stands for the second worker from Round 4.14956A.3 Endurance Task Result of Lab Members   Worker Combination A and B B and C C and A   Cohen ’s Kappa ( Each Summary)Answer.score_0 -0.261 -0.083 0.246   Answer.score_1 0.285 0.13 0.285   Answer.score_2 0.206 -0.006 -0.049   Answer.score_3 0.066 0.006 0.387   Cohen ’s Kappa ( Concatenation ) 0.1 0.055 0.268   Cohen ’s Kappa ( Omit first 2 HITs ) 0.2 0.091 0.196   Krippendorff ’s Alpha 0.201   A.4 Statistical Test Results of Qualification and Endurance Tasks for Each Round   Annotation Task Pass RateMean of   Pass Rate ( Bootstrap)Standard Dev . of   Pass Rate ( Bootstrap )   Round 1Qua . Task 0.1 0.0997 0.0424   End . Task 0.02 0.0199 0.0198   Round 2Qua . Task 0.16 0.1611 0.0521   End . Task 0.08 0.0805 0.0384   Round 3Qua . Task 0.1 0.1000 0.0482   End . Task 0.06 0.0599 0.0339   Round 4Qua . Task 0.16 0.1595 0.0511   End . Task 0.08 0.0800 0.0380   All RoundsQua . Task 0.13 0.1302 0.0236   End . Task 0.06 0.0602 0.0168   A.5 Cohen ’s Kappa ( combined scores ) for CloudResearch Workers in Reference - based Task14957A.6 Spearman ’s Coefficient for Inter - groups ( Pipeline & MACE ) in Reference - based Task   For the reference - based task , we used 4 methods to calculate Spearman ’s coefficient :   •Method 1 : Given the different numbers of remaining MACE workers for each HIT , we calculate   Spearman ’s coefficient between our pipeline and MACE workers in each HIT . Then we take the   average of these coefficients as the inter - group Spearman ’s coefficient shown in Table 12 .   •Method 2 : The only difference between this method and Method 1 is that we take the absolute value   when calculating Spearman ’s coefficient for each HIT .   •Method 3 : We take the average of each annotation question in each HIT within the group of our   pipeline and MACE workers separately , then concatenate these average scores of all HITs together   for each group and calculate Spearman ’s coefficient .   •Method 4 : The only difference between this method and Method 3 is that we calculate Spearman ’s   coefficient for each HIT and then take the average of all coefficients instead of concatenating first   and then calculating the coefficient.14958A.7 Qualitative Analysis of Correctness Across Annotation Sources in Reference - based Task   For the reference - based task , we first randomly select 50 HITs out of 30 HITs ( HIT index ranges from 0   to 29 ) , and then 1 annotation question out of 8 questions ( annotation index ranges from 0 to 7 ) for each of   these HITs selected in the above step .   For each randomly selected annotation question , we calculate the median within the groups of our   pipeline , MACE , and CloudResearch workers separately , as well as the scores generated by GPT models   ( GPT-3.5 ( “ text - davinci-003 ” ) , ChatGPT , and GPT-4 ) . The expert judgment ( aggregated by the median )   and details for 50 randomly selected annotation questions can be found in Table 13 and Table 14 .   Figure 10 shows Spearman ’s coefficient among different groups aggregated by the median before ( left )   and after ( right ) the removal of controversial HITs ( HIT with index 15 , 16 , and 28 ) . We also perform a   similar analysis aggregated by the mean shown in Figure 11.149591496014961A.8 Interaction with GPT models in Reference - based Task   A.8.1 Prompt Design   In Figure 12 , we show an example of the interaction with ChatGPT and the exact prompt design we   use to acquire scores generated by GPT models through APIfor the analysis of correctness in the   reference - based task .   This prompt design follows the instructions we provide to the crowd annotators in the reference - based   task ( see Figure 16 for details ) with minor modifications for the score generation from GPT models .   Details about running experiments through API can be found in Section 4.4 .   A.8.2 Estimated Cost of GPT Models   We estimate the cost of running GPT models for the score generation in the reference - based task ( 240   annotation questions in total ) based on the cost of 50 random annotation questions . Details of pricing can   be found on OpenAI ’s website . We assume the GPT model only returns the score without explanations.14962A.9 Instruction and Annotation Question Examples of HIT   Here we provide some examples of instructions and annotation questions for all three tasks as screenshots .   A.9.1 Qualification Task   •Figure 13 shows the definition of an evaluation dimension illustrated with examples in the training   part .   • Figure 14 shows the example of the qualification question in the qualification part.14963A.9.2 Endurance Task   Figure 15 shows the example of the annotation question on a Likert scale of 1 to 10 in the endurance task .   A.9.3 Reference - based Task   • Figure 16 shows the instructions for the reference - based task .   • Figure 17 shows the example of the annotation question in the reference - based task.14964   ( described in Shimorina and Belz ( 2022 ) )   1 Questions about Paper and   Supplementary Resources ( Questions   1.1–1.3 )   Questions 1.1–1.3 record bibliographic and related   information . These are straightforward and do n’t   warrant much in - depth explanation . Question 1.1 : Link to paper reporting the   evaluation experiment . If the paper reports   more than one experiment , state which ex-   periment you ’re completing this sheet for .   Or , if applicable , enter ‘ for preregistration . ’   Paper Link : This sheet is completed for   three experiments in the paper : Qualifica-   tion Task , Endurance Task , and Reference-   based Task .   What to enter in the text box : a link to an online   copy of the main reference for the human evalu-   ation experiment , identifying which of the exper-   iments the form is being completed for if there   are several . If the experiment has n’t been run yet ,   and the form is being completed for the purpose of   submitting it for preregistration , simply enter ‘ for   preregistration’ . Question 1.2 : Link to website providing re-   sources used in the evaluation experiment   ( e.g. system outputs , evaluation tools , etc . ) .   If there is n’t one , enter ‘ N / A ’ .   The data and code are available at .What to enter in the text box : link(s ) to any re-   sources used in the evaluation experiment , such as   system outputs , evaluation tools , etc . If there are n’t   any publicly shared resources ( yet ) , enter ‘ N / A’ . Question 1.3 : Name , affiliation and email   address of person completing this sheet , and   of contact author if different .   Lining Zhang , New York University   lz2332@nyu.edu   What to enter in the text box : names , affiliations   and email addresses as appropriate .   2 System Questions 2.1–2.5   Questions 2.1–2.5 record information about the   system(s ) ( or human - authored stand - ins ) whose   outputs are evaluated in the Evaluation experiment   that this sheet is being completed for .   The input , output , and task questions in this sec-   tion are closely interrelated : the value for one par-   tially determines the others , as indicated for some   combinations in Question 2.3.Question 2.1 : What type of input do the eval-   uated system(s ) take ? Select all that apply .   If none match , select ‘ Other ’ and describe .   Describe the type of input , where input refers to   the representations and/or data structures shared by   all evaluated systems .   This question is about input type , regardless of   number . E.g. if the input is a set of documents , you   would still select text : document below .   Check - box options ( select all that apply ) : 14965raw / structured data : numerical , symbolic , and   other data , possibly structured into trees , graphs ,   graphical models , etc . May be the input e.g. to   Referring Expression Generation ( REG ) , end-   to - end text generation , etc . NB : excludes lin-   guistic structures.deep linguistic representation ( DLR ) : any of a   variety of deep , underspecified , semantic repre-   sentations , such as abstract meaning represen-   tations ( AMRs ; Banarescu et al . , 2013 ) or dis-   course representation structures ( DRSs ; Kamp   and Reyle , 2013).shallow linguistic representation ( SLR ) : any of   a variety of shallow , syntactic representations ,   e.g. Universal Dependency ( UD ) structures ; typ-   ically the input to surface realisation.text : subsentential unit of text : a unit of text   shorter than a sentence , e.g. Referring Expres-   sions ( REs ) , verb phrase , text fragment of any   length ; includes titles / headlines .   ✓ text : sentence : a single sentence ( or set of sen-   tences ) .   ✓ text : multiple sentences : a sequence of multiple   sentences , without any document structure ( or a   set of such sequences ) .   ✓ text : document : a text with document structure ,   such as a title , paragraph breaks or sections , e.g.   a set of news reports for summarisation.text : dialogue : a dialogue of any length , exclud-   ing a single turn which would come under one   of the other text types.text : other : input is text but does n’t match any   of the above text :* categories.speech : a recording of speech.visual : an image or video.multi - modal : catch - all value for any combi-   nation of data and/or linguistic representation   and/or visual data etc.control feature : a feature or parameter specifi-   cally present to control a property of the output   text , e.g. positive stance , formality , author style.no input ( human generation ) : human genera-   tion , therefore no system inputs.other ( please specify ) : if input is none of the   above , choose this option and describe it . Question 2.2 : What type of output do the   evaluated system(s ) generate ? Select all that   apply . If none match , select ‘ Other ’ and   describe .   Describe the type of output , where output refers to   the representations and/or data structures shared by   all evaluated systems .   This question is about output type , regardless of   number . E.g. if the output is a set of documents ,   you would still select text : document below .   Note that the options for outputs are the same   as for inputs except that the no input ( human gen-   eration ) option is replaced with human - generated   ‘ outputs ’ , and the control feature option is removed .   Check - box options ( select all that apply ) : raw / structured data : numerical , symbolic , and   other data , possibly structured into trees , graphs ,   graphical models , etc . May be the input e.g. to   Referring Expression Generation ( REG ) , end-   to - end text generation , etc . NB : excludes lin-   guistic structures.deep linguistic representation ( DLR ) : any of a   variety of deep , underspecified , semantic repre-   sentations , such as abstract meaning represen-   tations ( AMRs ; Banarescu et al . , 2013 ) or dis-   course representation structures ( DRSs ; Kamp   and Reyle , 2013).shallow linguistic representation ( SLR ) : any of   a variety of shallow , syntactic representations ,   e.g. Universal Dependency ( UD ) structures ; typ-   ically the input to surface realisation.text : subsentential unit of text : a unit of text   shorter than a sentence , e.g. Referring Expres-   sions ( REs ) , verb phrase , text fragment of any   length ; includes titles / headlines.text : sentence : a single sentence ( or set of sen-   tences).text : multiple sentences : a sequence of multiple   sentences , without any document structure ( or a   set of such sequences).text : document : a text with document structure ,   such as a title , paragraph breaks or sections , e.g.   a set of news reports for summarisation.text : dialogue : a dialogue of any length , exclud-   ing a single turn which would come under one   of the other text types.14966text : other : select if output is text but does n’t   match any of the above text :* categories.speech : a recording of speech.visual : an image or video.multi - modal : catch - all value for any combi-   nation of data and/or linguistic representation   and/or visual data etc .   ✓ human - generated ‘ outputs ’ : manually created   stand - ins exemplifying outputs.other ( please specify ) : if output is none of the   above , choose this option and describe it . Question 2.3 : How would you describe the   task that the evaluated system(s ) perform in   mapping the inputs in Q2.1 to the outputs   in Q2.2 ? Occasionally , more than one of the   options below may apply . If none match ,   select ‘ Other ’ and describe .   This field records the task performed by the sys-   tem(s ) being evaluated . This is independent of the   application domain ( financial reporting , weather   forecasting , etc . ) , or the specific method ( rule-   based , neural , etc . ) implemented in the system . We   indicate mutual constraints between inputs , outputs   and task for some of the options below .   Check - box options ( select all that apply ) : content selection / determination : selecting the   specific content that will be expressed in the   generated text from a representation of possible   content . This could be attribute selection for   REG ( without the surface realisation step ) . Note   that the output here is not text.content ordering / structuring : assigning an or-   der and/or structure to content to be included in   generated text . Note that the output here is not   text.aggregation : converting inputs ( typically deep   linguistic representations orshallow linguistic   representations ) in some way in order to reduce   redundancy ( e.g. representations for ‘ they like   swimming ’ , ‘ they like running ’ →representa-   tion for ‘ they like swimming and running’).referring expression generation : generating   textto refer to a given referent , typically rep-   resented in the input as a set of attributes or a   linguistic representation.lexicalisation : associating ( parts of ) an input   representation with specific lexical items to be   used in their realisation.deep generation : one - step text generation from   raw / structured data ordeep linguistic represen-   tations . One - step means that no intermediate   representations are passed from one indepen-   dently run module to another.surface realisation ( SLR to text ) : one - step text   generation from shallow linguistic representa-   tions . One - step means that no intermediate rep-   resentations are passed from one independently   run module to another.feature - controlled text generation : generation   of text that varies along specific dimensions   where the variation is controlled via control   feature s specified as part of the input . In-   put is a non - textual representation ( for feature-   controlled text - to - text generation select the   matching text - to - text task).data - to - text generation : generation from   raw / structured data which may or may not in-   clude some amount of content selection as part   of the generation process . Output is likely to be   text :* ormulti - modal .dialogue turn generation : generating a dia-   logue turn ( can be a greeting or closing ) from   a representation of dialogue state and/or last   turn(s ) , etc.question generation : generation of questions   from given input text and/or knowledge base   such that the question can be answered from the   input.question answering : input is a question plus   optionally a set of reference texts and/or knowl-   edge base , and the output is the answer to the   question.paraphrasing/lossless simplification : text - to-   text generation where the aim is to preserve   the meaning of the input while changing its   wording . This can include the aim of chang-   ing the text on a given dimension , e.g. mak-   ing it simpler , changing its stance or sentiment ,   etc . , which may be controllable via input fea-   tures . Note that this task type includes meaning-   preserving text simplification ( non - meaning pre-   serving simplification comes under compres-   sion / lossy simplification below).14967compression / lossy simplification : text - to - text   generation that has the aim to generate a shorter ,   or shorter and simpler , version of the input text .   This will normally affect meaning to some ex-   tent , but as a side effect , rather than the primary   aim , as is the case in summarisation .machine translation : translating text in a   source language to text in a target language   while maximally preserving the meaning .   ✓ summarisation ( text - to - text ) : output is an ex-   tractive or abstractive summary of the impor-   tant / relevant / salient content of the input docu-   ment(s).end - to - end text generation : use this option if   the single system task corresponds to more than   one of tasks above , implemented either as sepa-   rate modules pipelined together , or as one - step   generation , other than deep generation andsur-   face realisation .image / video description : input includes visual ,   and the output describes it in some way.post - editing / correction : system edits and/or   corrects the input text ( typically itself the tex-   tual output from another system ) to yield an   improved version of the text.other ( please specify ) : if task is none of the   above , choose this option and describe it . Question 2.4 : Input Language(s ) , or ‘ N / A ’ .   This field records the language(s ) of the inputs ac-   cepted by the system(s ) being evaluated .   English .   What to enter in the text box : any language name(s )   that apply , mapped to standardised full language   names in ISO 639 - 1 . E.g. English , Herero , Hindi .   If no language is accepted as ( part of ) the input ,   enter ‘ N / A’ . Question 2.5 : Output Language(s ) , or ‘ N / A’ . This field records the language(s ) of the outputs   generated by the system(s ) being evaluated .   English .   What to enter in the text box : any language name(s )   that apply , mapped to standardised full language   names in ISO 639 - 1 ( 2019 ) . E.g. English , Herero ,   Hindi . If no language is generated , enter ‘ N / A ’ .   3 Questions about Output Sample ,   Evaluators , Experimental Design   3.1 Sample of system outputs ( or   human - authored stand - ins ) evaluated   ( Questions 3.1.1–3.1.3 )   Questions 3.1.1–3.1.3 record information about the   size of the sample of outputs ( or human - authored   stand - ins ) evaluated per system , how the sample   was selected , and what its statistical power is . Question 3.1.1 : How many system outputs   ( or other evaluation items ) are evaluated per   system in the evaluation experiment ? An-   swer should be an integer .   Qualification Task : 3 * 6=18   Endurance Task : 10 * 4=40   Reference - based Task : 30 * 8=240   What to enter in the text box : The number of system   outputs ( or other evaluation items ) that are evalu-   ated per system by at least one evaluator in the   experiment , as an integer . Question 3.1.2 : How are system outputs ( or   other evaluation items ) selected for inclusion   in the evaluation experiment ? If none match ,   select ‘ Other ’ and describe .   Multiple - choice options ( select one ) : by an automatic random process from a larger   set : outputs were selected for inclusion in the   experiment by a script using a pseudo - random   number generator ; do n’t use this option if the   script selects every nth output ( which is not   random).14968 ✓ by an automatic random process but using   stratified sampling over given properties : use   this option if selection was by a random script as   above , but with added constraints ensuring that   the sample is representative of the set of outputs   it was selected from , in terms of given proper-   ties , such as sentence length , positive / negative   stance , etc.by manual , arbitrary selection : output sample   was selected by hand , or automatically from a   manually compiled list , without a specific selec-   tion criterion.by manual selection aimed at achieving bal-   ance or variety relative to given properties : se-   lection by hand as above , but with specific selec-   tion criteria , e.g. same number of outputs from   each time period . Other ( please specify ) : if selection method is   none of the above , choose this option and de-   scribe it . Question 3.1.3 : What is the statistical power   of the sample size ?   See Section 5 and Appendix A.4 for details .   What to enter in the text box : The results of a statis-   tical power calculation on the output sample : pro-   vide numerical results and a link to the script used   ( or another way of identifying the script ) . See , e.g. ,   Card et al . ( 2020 ) ; Howcroft and Rieser ( 2021 ) .   3.2 Evaluators ( Questions 3.2.1–3.2.5 )   Questions 3.2.1–3.2.5 record information about the   evaluators participating in the experiment . Question 3.2.1 : How many evaluators are   there in this experiment ? Answer should be   an integer .   Qualification Task : 200   Endurance Task : 26   Reference - based Task : 12What to enter in the text box : the total number of   evaluators participating in the experiment , as an   integer . Question 3.2.2 : What kind of evaluators are   in this experiment ? Select all that apply .   If none match , select ‘ Other ’ and describe .   In all cases , provide details in the text box   under ‘ Other ’ .   Check - box options ( select all that apply ) : experts : participants are considered domain ex-   perts , e.g. meteorologists evaluating a weather   forecast generator , or nurses evaluating an ICU   report generator .   ✓ non - experts : participants are not domain ex-   perts .   ✓ paid ( including non - monetary compensation   such as course credits ) : participants were given   some form of compensation for their participa-   tion , including vouchers , course credits , and   reimbursement for travel unless based on re-   ceipts.not paid : participants were not given compen-   sation of any kind.previously known to authors : ( one of the ) re-   searchers running the experiment knew some or   all of the participants before recruiting them for   the experiment .   ✓ not previously known to authors : none of the   researchers running the experiment knew any of   the participants before recruiting them for the   experiment.evaluators include one or more of the authors :   one or more researchers running the experiment   was among the participants .   ✓ evaluators do not include any of the authors :   none of the researchers running the experiment   were among the participants . Other ( fewer than 4 of the above apply ): we   believe you should be able to tick 4 options of   the above . If that ’s not the case , use this box to   explain . Question 3.2.3 : How are evaluators re-   cruited?14969Qualification Task : On Amazon Me-   chanical Turk ( MTurk ) with pre - defined   qualification settings ( i.e. Location , etc . ) .   Endurance Task : On MTurk with evalua-   tors who passed Qualification Task .   Reference - based Task : On MTurk with   evaluators who passed Endurance Task .   What to enter in the text box : Please explain how   your evaluators are recruited . Do you send emails   to a given list ? Do you post invitations on social   media ? Posters on university walls ? Were there   any gatekeepers involved ? What are the exclu-   sion / inclusion criteria?Question 3.2.4 : What training and/or prac-   tice are evaluators given before starting on   the evaluation itself ?   Qualification Task : We include a training   part to illustrate evaluation dimensions   along with examples , and require evaluators   to write an instruction summary in their   own words .   Endurance Task : Evaluators are provided   with task instructions .   Reference - based Task : Evaluators are   provided with instructions and examples of   the rating at the beginning of the task .   What to enter in the text box : Use this space to   describe any training evaluators were given as part   of the experiment to prepare them for the evaluation   task , including any practice evaluations they did .   This includes any introductory explanations they ’re   given , e.g. on the start page of an online evaluation   tool . Question 3.2.5 : What other characteristics   do the evaluators have , known either be-   cause these were qualifying criteria , or from   information gathered as part of the evalua-   tion?Qualification Task : Evaluators are satisfied   with : ( i ) the Location as “ UNITED STATES   ( US ) ” ; ( ii ) the Number of HITs Approved is   “ greater than 1000 ” ; ( iii ) the HIT Approval   Rate ( % ) is “ greater than or equal to 99 ” .   Endurance Task : Evaluators pass the   attention check and make no ( ) or   only one mistake ( ) when annotating   each dimension of the documents in the   qualification part .   Reference - based Task : Evaluators (   and ) finish all 10 HITs in En-   durance Task .   What to enter in the text box : Use this space to list   any characteristics not covered in previous ques-   tions that the evaluators are known to have , either   because evaluators were selected on the basis of   a characteristic , or because information about a   characteristic was collected as part of the evalua-   tion . This might include geographic location of IP   address , educational level , or demographic informa-   tion such as gender , age , etc . Where characteristics   differ among evaluators ( e.g. gender , age , location   etc . ) , also give numbers for each subgroup .   3.3 Experimental Design Questions   3.3.1–3.3.8   Questions 3.3.1–3.3.8 record information about the   experimental design of the evaluation experiment . Question 3.3.1 : Has the experimental design   been preregistered ? If yes , on which reg-   istry ?   No .   What to enter in the text box : State ‘ Yes ’ or ‘ No ’ ; if   ‘ Yes ’ also give the name of the registry and a link   to the registration page for the experiment . Question 3.3.2 : How are responses col-   lected ? E.g. paper forms , online survey tool ,   etc .   Amazon Mechanical Turk ( MTurk).14970What to enter in the text box : Use this space to   describe how you collected responses , e.g. paper   forms , Google forms , SurveyMonkey , Mechanical   Turk , CrowdFlower , audio / video recording , etc . Question 3.3.3 : What quality assurance   methods are used ? Select all that apply . If   none match , select ‘ Other ’ and describe . In   all cases , provide details in the text box un-   der ‘ Other ’ .   Check - box options ( select all that apply ) : evaluators are required to be native speakers   of the language they evaluate : mechanisms   are in place to ensure all participants are native   speakers of the language they evaluate .   ✓ automatic quality checking methods are   used during / post evaluation : evaluations are   checked for quality by automatic scripts during   or after evaluations , e.g. evaluators are given   known bad / good outputs to check they ’re given   bad / good scores on MTurk .   ✓ manual quality checking methods are used   during / post evaluation : evaluations are   checked for quality by a manual process during   or after evaluations , e.g. scores assigned by eval-   uators are monitored by researchers conducting   the experiment .   ✓ evaluators are excluded if they fail quality   checks ( often or badly enough ) : there are con-   ditions under which evaluations produced by   participants are not included in the final results   due to quality issues .   ✓ some evaluations are excluded because of   failed quality checks : there are conditions un-   der which some ( but not all ) of the evaluations   produced by some participants are not included   in the final results due to quality issues.none of the above : tick this box if none of the   above apply . Other ( please specify ) : use this box to describe   any other quality assurance methods used dur-   ing or after evaluations , and to provide addi-   tional details for any of the options selected   above . Question 3.3.4 : What do evaluators see   when carrying out evaluations ? Link to   screenshot(s ) and/or describe the evaluation   interface(s ) .   See details in Appendix A.9 .   What to enter in the text box : Use this space to   describe the interface , paper form , etc . that evalua-   tors see when they carry out the evaluation . Link   to a screenshot / copy if possible . If there is a sep-   arate introductory interface / page , include it under   Question 3.2.4.Question 3.3.5 : How free are evaluators re-   garding when and how quickly to carry out   evaluations ? Select all that apply . In all   cases , provide details in the text box under   ‘ Other ’ .   Check - box options ( select all that apply ) :   ✓ evaluators have to complete each individual   assessment within a set time : evaluators are   timed while carrying out each assessment and   can not complete the assessment once time has   run out.evaluators have to complete the whole evalu-   ation in one sitting : partial progress can not be   saved and the evaluation returned to on a later   occasion.neither of the above : Choose this option if nei-   ther of the above are the case in the experiment . Other ( please specify ) : Use this space to de-   scribe any other way in which time taken or   number of sessions used by evaluators is con-   trolled in the experiment , and to provide ad-   ditional details for any of the options selected   above . Question 3.3.6 : Are evaluators told they can   ask questions about the evaluation and/or   provide feedback ? Select all that apply . In   all cases , provide details in the text box un-   der ‘ Other’.14971Check - box options ( select all that apply ) : evaluators are told they can ask any ques-   tions during / after receiving initial train-   ing / instructions , and before the start of the   evaluation : evaluators are told explicitly that   they can ask questions about the evaluation ex-   periment before starting on their assessments ,   either during or after training.evaluators are told they can ask any questions   during the evaluation : evaluators are told ex-   plicitly that they can ask questions about the   evaluation experiment during their assessments .   ✓ evaluators are asked for feedback and/or com-   ments after the evaluation , e.g. via an exit   questionnaire or a comment box : evaluators   are explicitly asked to provide feedback and/or   comments about the experiment after their as-   sessments , either verbally or in written form . None of the above : Choose this option if none   of the above are the case in the experiment . Other ( please specify ) : use this space to de-   scribe any other ways you provide for evaluators   to ask questions or provide feedback . Question 3.3.7 : What are the experimental   conditions in which evaluators carry out the   evaluations ? If none match , select ‘ Other ’   and describe .   Multiple - choice options ( select one ) :   ✓ evaluation carried out by evaluators at a place   of their own choosing , e.g. online , using a   paper form , etc . : evaluators are given access to   the tool or form specified in Question 3.3.2 , and   subsequently choose where to carry out their   evaluations.evaluation carried out in a lab , and conditions   are the same for each evaluator : evaluations   are carried out in a lab , and conditions in which   evaluations are carried out arecontrolled to be   the same , i.e. the different evaluators all carry   out the evaluations in identical conditions of   quietness , same type of computer , same room ,   etc . Note we ’re not after very fine - grained dif-   ferences here , such as time of day or tempera-   ture , but the line is difficult to draw , so some   judgment is involved here.evaluation carried out in a lab , and conditions   vary for different evaluators : choose this op-   tion if evaluations are carried out in a lab , but   the preceding option does not apply , i.e. condi-   tions in which evaluations are carried out are   notcontrolled to be the same.evaluation carried out in a real - life situation ,   and conditions are the same for each evalu-   ator : evaluations are carried out in a real - life   situation , i.e. one that would occur whether or   not the evaluation was carried out ( e.g. evalu-   ating a dialogue system deployed in a live chat   function on a website ) , and conditions in which   evaluations are carried out arecontrolled to be   the same.evaluation carried out in a real - life situation ,   and conditions vary for different evaluators :   choose this option if evaluations are carried out   in a real - life situation , but the preceding option   does not apply , i.e. conditions in which evalua-   tions are carried out are notcontrolled to be the   same.evaluation carried out outside of the lab , in a   situation designed to resemble a real - life sit-   uation , and conditions are the same for each   evaluator : evaluations are carried out outside   of the lab , in a situation intentionally similar to   a real - life situation ( but not actually a real - life   situation ) , e.g. user - testing a navigation system   where the destination is part of the evaluation   design , rather than chosen by the user . Condi-   tions in which evaluations are carried out are   controlled to be the same.evaluation carried out outside of the lab , in a   situation designed to resemble a real - life situ-   ation , and conditions vary for different eval-   uators : choose this option if evaluations are   carried out outside of the lab , in a situation in-   tentionally similar to a real - life situation , but   the preceding option does not apply , i.e. condi-   tions in which evaluations are carried out are   notcontrolled to be the same . Other ( please specify ) : Use this space to   provide additional , or alternative , information   about the conditions in which evaluators carry   out assessments , not covered by the options   above.14972Question 3.3.8 : Unless the evaluation is car-   ried out at a place of the evaluators ’ own   choosing , briefly describe the ( range of dif-   ferent ) conditions in which evaluators carry   out the evaluations .   The evaluation is carried out at a place of   the evaluators ’ own choosing .   What to enter in the text box : use this space to   describe the variations in the conditions in which   evaluators carry out the evaluation , for both situ-   ations where those variations are controlled , and   situations where they are not controlled .   4 Quality Criterion n – Definition and   Operationalisation   Questions in this section collect information about   thenth quality criterion assessed in the single hu-   man evaluation experiment that this sheet is being   completed for .   4.1 Quality criterion properties ( Questions   4.1.1–4.1.3 )   Questions 4.1.1–4.1.3 capture the aspect of qual-   ity that is assessed by a given quality criterion in   terms of three orthogonal properties . They help de-   termine whether or not the same aspect of quality   is being evaluated in different evaluation experi-   ments . The three properties characterise quality   criteria in terms of ( i ) what type of quality is being   assessed ; ( ii ) what aspect of the system output is   being assessed ; and ( iii ) whether system outputs   are assessed in their own right or with reference   to some system - internal or system - external frame   of reference . For full explanations see Belz et al .   ( 2020).Question 4.1.1 : What type of quality is as-   sessed by the quality criterion ?   Multiple - choice options ( select one ) : Correctness : Select this option if it is possi-   ble to state , generally for all outputs , the condi-   tions under which outputs are maximally correct   ( hence of maximal quality ) . E.g. for Grammati - cality , outputs are ( maximally ) correct if they   contain no grammatical errors ; for Semantic   Completeness , outputs are correct if they ex-   press all the content in the input . Goodness : Select this option if , in contrast to   correctness criteria , there is no single , general   mechanism for deciding when outputs are max-   imally good , only for deciding for any two out-   puts which is better and which is worse . E.g. for   Fluency , even if outputs contain no disfluencies ,   there may be other ways in which any given   output could be more fluent .   ✓ Feature : Choose this option if , in terms of prop-   ertyXcaptured by the criterion , outputs are not   generally better if they are more X , but instead ,   depending on evaluation context , more Xmay   be either better or worse . E.g. for Specificity ,   outputs can be more specific or less specific , but   it ’s not the case that outputs are , in the general   case , better when they are more specific . Question 4.1.2 : Which aspect of system out-   puts is assessed by the quality criterion ?   Multiple - choice options ( select one ) : Form of output : Choose this option if the cri-   terion assesses the form of outputs alone , e.g.   Grammaticality is only about the form , a sen-   tence can be grammatical yet be wrong or non-   sensical in terms of content . Content of output : Select this option if the crite-   rion assesses the content / meaning of the output   alone , e.g. Meaning Preservation only assesses   content ; two sentences can be considered to   have the same meaning , but differ in form .   ✓ Both form and content of output : Choose this   option if the criterion assesses outputs as a   whole , not just form or just content . E.g. Co-   herence is a property of outputs as a whole ,   either form or meaning can detract from it . In-   herently extrinsic criteria such as Usefulness or   Task Completion also fall in this category.14973Question 4.1.3 : Is each output assessed for   quality in its own right , or with reference   to a system - internal or external frame of   reference ?   Multiple - choice options ( select one ) : Quality of output in its own right : Select this   option if output quality is assessed without re-   ferring to anything other than the output itself ,   i.e. no system - internal or external frame of refer-   ence . E.g. Poeticness is assessed by considering   ( just ) the output and how poetic it is .   ✓ Quality of output relative to the input : Choose   this option if output quality is assessed relative   to the input . E.g. Answerability is the degree   to which the output question can be answered   from information in the input . Quality of output relative to a system - external   frame of reference : Choose this option if out-   put quality is assessed with reference to system-   external information , such as a knowledge base ,   a person ’s individual writing style , or the perfor-   mance of an embedding system . E.g. Factual   Accuracy assesses outputs relative to a source   of real - world knowledge .   4.2 Evaluation mode properties ( Questions   4.2.1–4.2.3 )   Questions 4.2.1–4.2.3 record properties that are   orthogonal to quality criteria ( covered by questions   in the preceding section ) , i.e. any given quality   criterion can in principle be combined with any of   the modes ( although some combinations are more   common than others).Question 4.2.1 : Does an individual assess-   ment involve an objective or a subjective   judgment ?   Multiple - choice options ( select one ) : Objective : Choose this option if the evaluation   uses objective assessment , e.g. any automati-   cally counted or otherwise quantified measure-   ments such as mouse - clicks , occurrences in text ,   etc . Repeated assessments of the same output   with an objective - mode evaluation method al-   ways yield the same score / result. ✓ Subjective : Choose this option in all other cases .   Subjective assessments involve ratings , opin-   ions and preferences by evaluators . Some crite-   ria lend themselves more readily to subjective   assessments , e.g. Friendliness of a conversa-   tional agent , but an objective measure e.g. based   on lexical markers is also conceivable . Question 4.2.2 : Are outputs assessed in ab-   solute or relative terms ?   Multiple - choice options ( select one ) :   ✓ Absolute : Select this option if evaluators are   shown outputs from a single system during each   individual assessment . Relative : Choose this option if evaluators are   shown outputs from multiple systems at the   same time during assessments , typically ranking   or preference - judging them . Question 4.2.3 : Is the evaluation intrinsic or   extrinsic ?   Multiple - choice options ( select one ) :   ✓ Intrinsic : Choose this option if quality of out-   puts is assessed without considering their effect   on something external to the system , e.g. the   performance of an embedding system or of a   user at a task . Extrinsic : Choose this option if quality of out-   puts is assessed in terms of their effect on some-   thing external to the system such as the perfor-   mance of an embedding system or of a user at a   task .   4.3 Response elicitation ( Questions   4.3.1–4.3.11 )   The questions in this section concern response elic-   itation , by which we mean how the ratings or other   measurements that represent assessments for the   quality criterion in question are obtained , cover-   ing what is presented to evaluators , how they se-   lect response and via what type of tool , etc . The   eleven questions ( 4.3.1–4.3.11 ) are based on the   information annotated in the large scale survey of   human evaluation methods in NLG by Howcroft   et al . ( 2020).14974Question 4.3.1 : What do you call the quality   criterion in explanations / interfaces to evalu-   ators ? Enter ‘ N / A ’ if criterion not named .   We evaluate a summary according to 6 di-   mensions : Understandability , Compactness ,   Grammaticality , Coherence , Faithfulness ,   and Saliency .   What to enter in the text box : the name you use to   refer to the quality criterion in explanations and/or   interfaces created for evaluators . Examples of qual-   ity criterion names include Fluency , Clarity , Mean-   ing Preservation . If no name is used , state ‘ N / A’ . Question 4.3.2 : What definition do you   give for the quality criterion in explana-   tions / interfaces to evaluators ? Enter ‘ N / A ’   if no definition given .   For a summary S ,   •Understandability : can the worker un-   derstand Sand is Sworth being anno-   tated .   •Compactness : Sdoes not contain du-   plicated information .   •Grammaticality : Sis free from gram-   matical spelling errors .   •Coherence : Sis presented in a clear ,   wellstructured , logical , and meaningful   way .   •Faithfulness : all of the information in   Scan also be found in the article ; S   accurately reflects the contents of the   article .   •Saliency : Scaptures the most impor-   tant information of the article and does   not include parts of the article that are   less important .   What to enter in the text box : Copy and past the ver-   batim definition you give to evaluators to explainthe quality criterion they ’re assessing . If you do n’t   explicitly call it a definition , enter the nearest thing   to a definition you give them . If you do n’t give any   definition , state ‘ N / A’ . Question 4.3.3 : Size of scale or other rating   instrument ( i.e. how many different possi-   ble values there are ) . Answer should be an   integer or ‘ continuous ’ ( if it ’s not possible   to state how many possible responses there   are ) . Enter ‘ N / A ’ if there is no rating instru-   ment .   Qualification Task : 2 ( binary classifica-   tion )   Endurance Task : 10 ( 10 - point EASL   scale )   Reference - based Task : 5 ( 5 - point Likert   scale )   What to enter in the text box : The number of differ-   ent response values for this quality criterion . E.g.   for a 5 - point Likert scale , the size to enter is 5 . For   two - way forced - choice preference judgments , it is   2 ; if there ’s also a no - preference option , enter 3 .   For a slider that is mapped to 100 different values   for the purpose of recording assessments , the size   to enter is 100 . If no rating instrument is used ( e.g.   when evaluation gathers post - edits or qualitative   feedback only ) , enter ‘ N / A’ . Question 4.3.4 : List or range of possible val-   ues of the scale or other rating instrument .   Enter ‘ N / A ’ , if there is no rating instrument .   Qualification Task : Yes , No   Endurance Task : 1 - 10   Reference - based Task : 1 - 5   What to enter in the text box : list , or give the range   of , the possible values of the rating instrument .   The list or range should be of the size specified in   Question 4.3.3 . If there are too many to list , use a   range . E.g. for two - way forced - choice preference   judgments , the list entered might be A better , B14975better ; if there ’s also a no - preference option , the list   might be A better , B better , neither . For a slider that   is mapped to 100 different values for the purpose   of recording assessments , the range 1–100 might   be entered . If no rating instrument is used ( e.g.   when evaluation gathers post - edits or qualitative   feedback only ) , enter ’ N / A’ . Question 4.3.5 : How is the scale or other   rating instrument presented to evaluators ?   If none match , select ‘ Other ’ and describe .   Multiple - choice options ( select one ) : Multiple - choice options : choose this option if   evaluators select exactly one of multiple op-   tions . Check - boxes : choose this option if evaluators   select any number of options from multiple   given options . Slider : choose this option if evaluators move a   pointer on a slider scale to the position corre-   sponding to their assessment . N / A ( there is no rating instrument ) : choose   this option if there is no rating instrument .   ✓ Other ( please specify ) : choose this option if   there is a rating instrument , but none of the   above adequately describe the way you present   it to evaluators . Use the text box to describe the   rating instrument and link to a screenshot .   Qualification Task : Multiple - choice   options   Endurance Task : Slider   Reference - based Task : Multiple - choice   optionsQuestion 4.3.6 : If there is no rating instru-   ment , describe briefly what task the evalua-   tors perform ( e.g. ranking multiple outputs ,   finding information , playing a game , etc . ) ,   and what information is recorded . Enter   ‘ N / A ’ if there is a rating instrument .   N / A.What to enter in the text box : If ( and only if ) there   is no rating instrument , i.e. you entered ‘ N / A ’ for   Questions 4.3.3–4.3.5 , describe the task evaluators   perform in this space . Otherwise , here enter ‘ N / A ’   if there isa rating instrument . Question 4.3.7 : What is the verbatim ques-   tion , prompt or instruction given to evalua-   tors ( visible to them during each individual   assessment ) ?   Qualification Task :   •In each of the following sections , we   explain the different dimensions you   will evaluate and provide example sum-   maries with ratings . You must answer   each question , but these training exam-   ples are not part of the qualification .   •This section contains examples of sum-   maries and ratings for each dimension .   These examples show how a summary   can be good on one dimension and bad   on another . Please read these examples   and move on .   •To make sure you understand the   instructions , please summarize them   briefly in your own words ( 2 - 3 sen-   tences ) . This is required as part of the   qualification ( min . 100 characters ) .   •This section contains the actual qualifi-   cation questions . Read the documents   and the corresponding summaries care-   fully , then annotate the summaries   across the various dimensions . You   will be graded using these questions ,   so the answers will not be shown to   you.14976Endurance Task : In this task , you will eval-   uate the salience of different summaries of   an article . First , read the article , then assign   each summary a salience score from 1 to 10 .   A salient summary is one which captures   the most important information of the article   and does not include parts of the article that   are less important .   Please use the sliders to rate the salience of   the summary from 1 to 10 ( see the instruc-   tions above for the definition of salience ) .   Reference - based Task : In this task , you   will be shown a reference summary and sev-   eral candidate summaries and asked to as-   sign each candidate summary two scores   from 1 to 5 based on how much you agree   with the following statements :   •All of the information in the candidate   summary can also be found in the ref-   erence summary .   •All of the information in the reference   summary can also be found in the can-   didate summary .   What is important is if the candidate sum-   mary and reference summary convey the   same information , not if they use exactly   the same words . Usually the reference sum-   mary and candidate summary are not exactly   the same nor totally different .   If the score is 1 , it means that almost no   information in one summary can be found   in the other . If the score is 5 , it means   that almost all of the information in one   summary can be found in the other .   What to enter in the text box : Copy and paste the   verbatim text that evaluators see during each assess-   ment , that is intended to convey the evaluation task   to them . E.g. Which of these texts do you prefer ?   OrMake any corrections to this text that you think   are necessary in order to improve it to the point   where you would be happy to provide it to a client . Question 4.3.8 : Form of response elicitation .   If none match , select ‘ Other ’ and describe .   Multiple - choice options ( select one ) : ( dis)agreement with quality statement : Partici-   pants specify the degree to which they agree   with a given quality statement by indicating   their agreement on a rating instrument . The   rating instrument is labelled with degrees of   agreement and can additionally have numerical   labels . E.g. This text is fluent — 1 = strongly   disagree ... 5 = strongly agree .   ✓ direct quality estimation : Participants are asked   to provide a rating using a rating instrument ,   which typically ( but not always ) mentions the   quality criterion explicitly . E.g. How fluent is   this text ? — 1 = not at all fluent ... 5 = very fluent .relative quality estimation ( including ranking ) :   Participants evaluate two or more items in terms   of which is better . E.g. Rank these texts in terms   of fluency ; Which of these texts is more fluent ? ;   Which of these items do you prefer ? .counting occurrences in text : Evaluators are   asked to count how many times some type of   phenomenon occurs , e.g. the number of facts   contained in the output that are inconsistent with   the input.qualitative feedback ( e.g. via comments en-   tered in a text box ) : Typically , these are re-   sponses to open - ended questions in a survey or   interview.evaluation through post - editing / annotation :   Choose this option if the evaluators ’ task con-   sists of editing or inserting annotations in text .   E.g. evaluators may perform error correction   and edits are then automatically measured to   yield a numerical score.output classification or labelling : Choose this   option if evaluators assign outputs to categories .   E.g. What is the overall sentiment of this piece   of text ? — Positive / neutral / negative.user - text interaction measurements : choose   this option if participants in the evaluation ex-   periment interact with a text in some way , and   measurements are taken of their interaction . E.g.   reading speed , eye movement tracking , com-   prehension questions , etc . Excludes situations14977where participants are given a task to solve and   their performance is measured which comes un-   der the next option.task performance measurements : choose this   option if participants in the evaluation experi-   ment are given a task to perform , and measure-   ments are taken of their performance at the task .   E.g. task is finding information , and task perfor-   mance measurement is task completion speed   and success rate.user - system interaction measurements : choose   this option if participants in the evaluation ex-   periment interact with a system in some way ,   while measurements are taken of their interac-   tion . E.g. duration of interaction , hyperlinks   followed , number of likes , or completed sales . Other ( please specify ) : Use the text box to de-   scribe the form of response elicitation used in   assessing the quality criterion if it does n’t fall   in any of the above categories . Question 4.3.9 : How are raw responses from   participants aggregated or otherwise pro-   cessed to obtain reported scores for this qual-   ity criterion ? State if no scores reported .   We use raw responses to calculate Inter-   Annotator Agreement ( IAA ) , but sometimes   the median of scores is taken to increase   IAA .   What to enter in the text box : normally a set of   separate assessments is collected from evaluators   and is converted to the results as reported . Describe   here the method(s ) used in the conversion(s ) . E.g.   macro - averages or micro - averages are computed   from numerical scores to provide summary , per-   system results . Question 4.3.10 : Method(s ) used for deter-   mining effect size and significance of find-   ings for this quality criterion .   See Section 5 and Appendix A.4 for details . What to enter in the text box : A list of methods   used for calculating the effect size and significance   of any results , both as reported in the paper given   in Question 1.1 , for this quality criterion . If none   calculated , state ‘ None’ . Question 4.3.11 : Has the inter - annotator   and intra - annotator agreement between   evaluators for this quality criterion been   measured ? If yes , what method was used ,   and what are the agreement scores ?   We use Cohen ’s Kappa and Krippendorff ’s   Alpha for the inter - annotator agreement be-   tween evaluators . For the agreement scores ,   see Section 4 for details .   What to enter in the text box : the methods used to   compute , and results obtained from , any measures   of inter - annotator and intra - annotator agreement   obtained for the quality criterion .   5 Ethics Questions ( Questions 5.1 - 5.4 )   The questions in this section relate to ethical as-   pects of the evaluation . Information can be entered   in the text box provided , and/or by linking to a   source where complete information can be found . Question 5.1 : Has the evaluation experi-   ment this sheet is being completed for , or the   larger study it is part of , been approved by   a research ethics committee ? If yes , which   research ethics committee ?   This research is conducted by following the   equivalent hourly rate listed here :   What to enter in the text box : Typically , re-   search organisations , universities and other higher-   education institutions require some form ethical   approval before experiments involving human par-   ticipants , however innocuous , are permitted to pro-   ceed . Please provide here the name of the body that   approved the experiment , or state ‘ No ’ if approval   has not ( yet ) been obtained.14978Question 5.2 : Do any of the system outputs   ( or human - authored stand - ins ) evaluated , or   do any of the responses collected , in the ex-   periment contain personal data ( as defined   in GDPR Art . 4 , § 1 : https://gdpr.eu/article-   4 - definitions/ ) ? If yes , describe data and   state how addressed .   In our experiments , personal data ( any in-   formation relating to an identifiable natural   person ) was collected , processed , and stored   based on certain data protection regulations ,   given relevant privacy concerns .   What to enter in the text box : State ‘ No ’ if no   personal data as defined by GDPR was recorded or   collected , otherwise explain how conformity with   GDPR requirements such as privacy and security   was ensured , e.g. by linking to the ( successful )   application for ethics approval from Question 5.1.Question 5.3 : Do any of the system outputs   ( or human - authored stand - ins ) evaluated ,   or do any of the responses collected , in   the experiment contain special category   information ( as defined in GDPR Art . 9 , § 1 :   https://gdpr.eu/article-9-processing-special-   categories - of - personal - data - prohibited/ ) ? If   yes , describe data and state how addressed .   No .   What to enter in the text box : State ‘ No ’ if   no special - category data as defined by GDPR   was recorded or collected , otherwise explain how   conformity with GDPR requirements relating to   special - category data was ensured , e.g. by linking   to the ( successful ) application for ethics approval   from Question 5.1.Question 5.4 : Have any impact assessments   been carried out for the evaluation experi-   ment , and/or any data collected / evaluated   in connection with it ? If yes , summarise ap-   proach(es ) and outcomes .   No .   What to enter in the text box : Use this box to de-   scribe any ex ante orex post impact assessments   that have been carried out in relation to the evalua-   tion experiment , such that the assessment plan and   process , as well as the outcomes , were captured   in written form . Link to documents if possible .   Types of impact assessment include data protection   impact assessments , e.g. under GDPR.Environ-   mental and social impact assessment frameworks   are also available .   Credits   Questions 2.1–2.5 relating to evaluated system ,   and 4.3.1–4.3.8 relating to response elicitation , are   based on Howcroft et al . ( 2020 ) , with some signif-   icant changes . Questions 4.1.1–4.2.3 relating to   quality criteria , and some of the questions about   system outputs , evaluators , and experimental de-   sign ( 3.1.1–3.2.3 , 4.3.5 , 4.3.6 , 4.3.9–4.3.11 ) are   based on Belz et al . ( 2020 ) . HEDS was also in-   formed by van der Lee et al . ( 2019 , 2021 ) and by   Gehrmann et al . ( 2021)’sdata card guide .   More generally , the original inspiration for creat-   ing a ‘ datasheet ’ for describing human evaluation   experiments of course comes from seminal papers   by Bender and Friedman ( 2018 ) , Mitchell et al .   ( 2019 ) and Gebru et al . ( 2020 ) .   References1497914980ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Please see Section 7 .   /squareA2 . Did you discuss any potential risks of your work ?   Please see Section 8 .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Please see the abstract and Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Please see Section 3 .   /squareB1 . Did you cite the creators of artifacts you used ?   Please see Section 3 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Please see Section 3 .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Please see Section 3 .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Please see Section 8 .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Please see Section 3 .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Please see Section 4 .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   No response.14981 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   No response .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   No response .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   No response .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Please see Section 3&4 for details of the implementation and results involving human annotators .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Please see Section 3 and Appendix A.7 .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Please see Section 3 .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Please see Section 8 .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Please see Section 3.1.14982