  Qi Zhu , Bing Li , Fei Mi , Xiaoyan Zhu , Minlie HuangCoAI Group , DCST , IAI , BNRIST , Tsinghua UniversityHuawei Noah ’s Ark Lab   Abstract   A desirable dialog system should be able to   continually learn new skills without forgetting   old ones , and thereby adapt to new domains   or tasks in its life cycle . However , continually   training a model often leads to a well - known   catastrophic forgetting issue . In this paper , we   present Continual Prompt Tuning , a parameter-   efﬁcient framework that not only avoids for-   getting but also enables knowledge transfer be-   tween tasks . To avoid forgetting , we only learn   and store a few prompt tokens ’ embeddings   for each task while freezing the backbone   pre - trained model . To achieve bi - directional   knowledge transfer among tasks , we propose   several techniques ( continual prompt initial-   ization , query fusion , and memory replay ) to   transfer knowledge from preceding tasks and   a memory - guided technique to transfer knowl-   edge from subsequent tasks . Extensive exper-   iments demonstrate the effectiveness and efﬁ-   ciency of our proposed method on continual   learning for dialog state tracking , compared   with state - of - the - art baselines .   1 Introduction   Recently , most studies have focused on developing   dialog systems for speciﬁc domains in an ofﬂine   manner , assuming the data distribution stays the   same . However , this is far from realistic because a   deployed dialog system is often required to support   new domains and provide more services constantly   over time . Therefore , it is crucial for a dialog sys-   tem to continually learn new tasks without forget-   ting old ones with high efﬁciency .   Previous studies on continual learning ( Kirk-   patrick et al . , 2017 ; Li and Hoiem , 2018 ) mainly   focused on solving the catastrophic forgetting ( CF )   problem ( McCloskey and Cohen , 1989 ): when a   neural model is trained on a sequence of tasks , new   tasks may interfere catastrophically with old tasks .   Simply storing a model version for each task toFigure 1 : An illustration of Continual Prompt Tuning .   We train a soft prompt for each task and freeze the   pre - trained model . Several techniques are proposed to   transfer knowledge from preceding tasks ( green solid   arrows ) and subsequent tasks ( red dashed arrows ) .   mitigate forgetting is prohibitive as the number   of tasks grows , especially when the model size is   large . To mitigate catastrophic forgetting with low   computation and storage overhead , recent methods   freeze the backbone model and propose to train   a weight / feature mask ( Mallya et al . , 2018 ; Geng   et al . , 2021 ) or an adapter ( Madotto et al . , 2021 ) for   each task independently . However , the techniques   above are still not efﬁcient enough , and they largely   ignore knowledge transfer among tasks .   In this paper , we develop prompt tuning ( Lester   et al . , 2021 ) for continual learning . We freeze the   backbone pre - trained model and train a few prompt   tokens ’ embeddings for each task , which is highly   parameter - efﬁcient to avoid forgetting . As illus-   trated by yellow components in Figure 1 , we con-   catenate the input with a few tunable task - speciﬁc   prompt tokens before feeding it to a frozen pre-   trained model . Since these prompt tokens have   only a small number of parameters ( 0.1 % of the pre-   trained model ’s parameters in our experiments ) , we   can efﬁciently train and store the prompt for each   task . During inference , the same pre - trained model   can handle different tasks by inputting different   prompts , which is friendly for deployment .   Unlike the vanilla approach of training each   task ’s prompt from scratch and ﬁxing it afterward ,   we propose Continual Prompt Tuning , a frameworkthat enables knowledge transfer between tasks .   We consider transferring knowledge from both pre-   ceding tasks ( forward ) and subsequent tasks ( back-   ward ) . To realize forward transfer , we propose   several techniques , including continual prompt ini-   tialization , query fusion , and memory replay ( green   solid arrows in Figure 1 ) . To achieve positive back-   ward transfer , we propose a memory - guided tech-   nique that uses subsequent tasks ’ data to update   the previous tasks ’ prompts selectively ( red dashed   arrows in Figure 1 ) .   We conduct experiments on Dialog State Track-   ing ( DST ) , a core component of a dialog system ,   using the Schema - Guided Dialog dataset ( Rastogi   et al . , 2020 ) . The model continually learns new   services that have multiple slots to ﬁll . We con-   catenate all slots ’ descriptions with the input and   insert a sentinel token after each description , for-   mulating DST as a masked spans recovering task ,   which is similar to the pre - training objective of T5   ( Raffel et al . , 2020 ) . We empirically show that our   proposed framework effectively outperforms state-   of - the - art baselines on continual learning for DST ,   and is extremely efﬁcient in terms of computation   and storage .   To summarize , our main contributions are :   1.For the ﬁrst time , we develop prompt tuning   for continual learning , which avoids forgetting   efﬁciently and is friendly for deployment .   2.We investigate several techniques for forward   and backward knowledge transfer based on   prompt tuning , further boosting the continual   learning performance .   3.Our experiments on continual DST demonstrate   the superior performance and efﬁciency of our   proposed method .   2 Related Work   2.1 Continual Learning   Continual Learning ( CL ) studies the problem   of continually acquiring knowledge from a data   stream and reusing it for future learning while   avoiding forgetting . Three kinds of CL methods   have been developed . Rehearsal methods store   and replay some training samples from previous   tasks ( Rebufﬁ et al . , 2017 ; Lopez - Paz and Ranzato ,   2017 ) . Regularization methods apply additional   loss to aid knowledge consolidation ( Kirkpatricket al . , 2017 ; Li and Hoiem , 2018 ) . Architectural   methods introduce task - speciﬁc parameters for new   tasks and ﬁx parameters for old tasks to prevent   forgetting , to which our method belongs . Previous   architectural methods include dynamic expanding   network structure ( Rusu et al . , 2016 ) , iterative net-   work pruning and re - training ( Mallya and Lazeb-   nik , 2018 ) , learning a parameter mask for each task   individually ( Mallya et al . , 2018 ) , etc .   For continual learning in dialog system , variants   of general CL methods have been applied ( Lee ,   2017 ; Shen et al . , 2019 ; Wu et al . , 2019 ; Mi et al . ,   2020 ; Geng et al . , 2021 ) . AdapterCL ( Madotto   et al . , 2021 ) is the most related to our work , which   freezes the pre - trained model and learns an adapter   ( Houlsby et al . , 2019 ) for each task independently .   Compared with AdapterCL , our method is more   parameter - efﬁcient , and we explore the effect of   both forward and backward transfer .   2.2 Prompt - based Tuning   Recent studies have found that using a textual   prompt to convert downstream tasks to the lan-   guage modeling task is a more effective way to   use pre - trained language models than typical ﬁne-   tuning ( Brown et al . , 2020 ; Schick and Schütze ,   2021 ) . Prompts can be manual designed ( Petroni   et al . , 2019 ) or generated automatically ( Shin et al . ,   2020 ; Jiang et al . , 2020 ; Gao et al . , 2021 ) . Since   searching prompts in discrete spaces is sub - optimal ,   some works ( Qin and Eisner , 2021 ; Liu et al . ,   2021 ; Han et al . , 2021 ) combine hard text prompts   and soft prompts whose embeddings are learned   through back - propagation . Lester et al . ( 2021 )   show that freezing the pre - trained model and only   tuning soft prompts , known as prompt tuning , is   parameter - efﬁcient and becomes more competitive   with ﬁne - tuning as the model size grows .   Prompt tuning differs from embedding adapter   ( Zhu et al . , 2021 ) that aims to address the multilin-   gual embedding deﬁciency . An embedding adapter   transforms all tokens embeddings but do not affect   transformer layers ’ computation , while prompt tun-   ing does not change tokens embeddings but adds   new tunable prompt tokens to the input , serving   as context and affecting all following transformer   layers . Gu et al . ( 2021 ) and Vu et al . ( 2021 ) further   explore the transferability of soft prompts across   tasks . While they investigate one - step adaptation ,   we are interested in prompt transfer in the continual   learning setting.2.3 Dialog State Tracking   Dialog State Tracking ( DST ) aims to capture user   goals in the form of ( slot , value ) pairs . Traditional   ontology - based classiﬁcation methods ( Mrkši ´ c   et al . , 2017 ; Lee et al . , 2019 ) require access to   all candidate values . To alleviate the reliance on   the ontology and improve generalization to unseen   values , some work extract values from a dialog   context ( Xu and Hu , 2018 ; Gao et al . , 2019 ) while   others generate values directly to handle situations   where values are missing from the context ( Wu   et al . , 2019 ; Hosseini - Asl et al . , 2020 ) .   Generation - based models either generate all   ( slot , value ) pairs in one pass ( Hosseini - Asl et al . ,   2020 ; Madotto et al . , 2021 ) or generate value for   each given slot separately ( Wu et al . , 2019 ) . The   former are more efﬁcient but can only predict in-   domain slots and lack transferability while the latter   can incorporate more information about a slot as   a query , such as a brief natural language descrip-   tion ( Rastogi et al . , 2020 ) , slot type information   ( Lin et al . , 2021 ) , possible values ( Lee et al . , 2021 ) ,   and the task deﬁnition and constraint ( Mi et al . ,   2022 ) . Our proposed method integrates multiple   slot descriptions into a single query and generates   all values in one pass , which improves performance   without losing efﬁciency .   3 Method   3.1 Overview   The goal of continual learning is to sequentially   learn a model f : XT ! Y from a stream of   tasksT:::Tthat can predict the target ygiven the   inputxand taskT2T. We denote the data for   each taskTasD. Our method is based on pre-   trained language models . Instead of ﬁne - tuning a   pre - trained model in a traditional manner ( Figure   2(a ) ) , we freeze the model but " reprogram " it to   solve taskTby addingmnew soft prompt tokens   P = PP:::Pto the textual input and tuning   the embeddings of Ponly . Since the prompt ’s   parameters are much less than the model ’s , we save   Pfor each task to avoid forgetting .   We treat each service / API as a task in continual   DST ( service and task are used interchangeably ) .   To incorporate informative slot descriptions and   ease the decoding process , we convert the descrip-   tions into a query with masked spans and formulate   DST as a masked spans recovering task ( Sec . 3.2 ) .   To enhance knowledge transfer between tasks , we   propose continual prompt initialization , query fu - sion , and memory replay for forward transfer ( Sec .   3.3 ) and explore a memory - guided technique for   backward transfer ( Sec . 3.4 ) .   3.2 DST as Masked Spans Recovering   In DST , each service Thas a set of pre - deﬁned   slotsS = fs;:::;sgto be tracked . The input x   is a dialog and the output yconsists of slot - value   pairs : f(s;v);(s;v);:::;(s;v)g . Similar   to many NLP tasks , DST can be formulated as a   text - to - text generation task . Formally , we deﬁne a   functiong : XY!VVfor each service   Tto transform the original data ( x;y)to :   ~x;~y = g(x;y ) ( 1 )   whereVis the vocabulary and ~x;~yare texts that   serve as the model input and output , respectively .   For example , ~xcan be the concatenation of xand   service name , while ~yis a sequence of slot - value   pairs ( Madotto et al . , 2021 ) ( Figure 2(a ) ) .   Previous research has shown that incorporating   a natural language description dfor each slot sis   beneﬁcial ( Lin et al . , 2021 ; Lee et al . , 2021 ) . They   concatenate the dialog xwith each slot description   dand decode the value vindependently . However ,   separately decoding is inefﬁcient , especially when   there are many slots . To solve this , we concatenate   all slot descriptions and insert a sentinel token after   each description to form a query added to the input ,   formulating DST as a masked spans recovering task   that generates all slot values in one pass :   ~x= [ x;Q;P ]   Q= \d : hMi::::d : hMi : "   ~y= \hMiv:::hMiv"(2 )   where [ ;]is the concatenation operation and hMi   are distinct sentinel tokens representing masked   spans . The queryQcontains all nslot descrip-   tions for taskTwithnmasked spans and ~ycon-   tains corresponding slot values leaded by the sen-   tinel tokens . If the value of a slot can not be inferred   from the input , we set it to " None " . We freeze the   pre - trained model ’s parameters and only optimize   the prompt ’s parameters for each serviceT.   The loss function is :   L(D ) =  Xlogp(~yj[x;Q;P])(3 )   3.3 Forward Transfer   Reusing the knowledge acquired from preceding   tasks often improves and accelerates the learning   on future tasks . Therefore , we propose three types   of techniques for forward transfer that can be em-   ployed in combination .   3.3.1 Continual Prompt Initialization   An intuitive way to transfer knowledge is parame-   ter initialization . We explore two continual prompt   initialization strategies . CLInit uses last task ’s   promptPto initialize current task ’s prompt P.   SelectInit evaluates allfPgon the validation   set ofTwithout training and selects the one with   the lowest loss to initialize P. The initial prompt   of CLInit has been continually trained on all previ-   ous tasks , while SelectInit only considers the most   relevant task without interference from its subse-   quent tasks . We empirically compare these two   strategies in Sec . 5.3 .   3.3.2 Query Fusion   We hope the model can learn to generate values   according to any slot descriptions , which is a gen-   eral skill that may improve performance on future   tasks . However , when training on the current task ,   there is only one query that consists of the slot   descriptions of that task in a ﬁxed order , whichmay hinder the model from learning the general   skill . Therefore , we propose to augment the query   by mixing slot descriptions from the current and   previous tasks to help the prompt better understand   the correspondence between slot descriptions and   values . We fuse the query Qwith previous tasks ’   queriesfQgfor each sample , including three   steps : 1 ) sample nslots fromSrandomly , where   nis sampled from [ 1;jSj]uniformly . 2 ) sample   nslots from previous tasks ’ slotsSSran-   domly , where nis sampled from [ 1;n]uniformly .   3 ) combine the above nandnslots ’ descriptions   in a random order as new Q , and modify ~yaccord-   ingly . Note that some original slots are dropped ,   and values for added slots are set to " None " .   3.3.3 Memory Replay   Previous studies ( Rebufﬁ et al . , 2017 ; Lopez - Paz   and Ranzato , 2017 ) store a few samples for each   task and replay them when training on new tasks to   mitigate forgetting . Since our prompt tuning frame-   work has already resolved forgetting , we focus on   how these samples beneﬁt the current task . We as-   sume we can storejMjsamples for each task ( jMj   should be small ) and denote Mas the memory for   taskT. When a new task Tcomes , we optimize   PonDandM = SMjointly , changing   the loss function to L(D+M).When combined with query fusion , query Q   for samples in the memory Mare also fused with   queriesfQgfrom other seen tasks , includ-   ing the current task . Note that in this way , samples   from other tasks can be viewed as " positive " sam-   ples to those added slots in Qsince these samples   may have not " None " values for those added slots .   3.4 Memory - Guided Backward Transfer   Although ﬁxing Pimmediately after training on   taskTcan avoid forgetting , it also blocks the   backward knowledge transfer from future tasks .   Motivated by Chaudhry et al . ( 2019 ) , we explore   whether it is possible to improve the performance   on previous tasks with the help of memory when   a new task comes . Speciﬁcally , for each previous   taskT;i < k , we initialize a new prompt Pto   Pand trained it on current task ’s data Dwith   memoryMas regularization . During training , we   sample a batch from Dand a batch from Msyn-   chronously and denote the gradient from each batch   asgandg , respectively . We decide the gradi-   ent for update according to the angle between g   andg :   g= (   g;ifgg>0   0 ; otherwise(4 )   which means we abort the update that will increase   the loss on memory batch . We empirically ﬁnd that   this simple abortion is better than projecting g   onto the normal plane of g(Chaudhry et al . ,   2019 ) . After training , we update PtoPif   Pobtains lower loss and better ( or equal ) per-   formance on MthanP.   4 Experimental Setup   Recently , Madotto et al . ( 2021 ) proposed a con-   tinual learning benchmark for task - oriented dialog   systems and compared several classic CL methods .   We adapt their data processing steps and baselines   in our experiments .   4.1 Dataset   We conduct experiments on Schema - Guided Dia-   log dataset ( SGD ) ( Rastogi et al . , 2020 ) that has   44 services over 19 domains . It also provides a   one - sentence description for each slot . We treat   each service as a task and only consider dialogs   involving a single service . We randomly split a   service ’s dialogs into train / val / test sets at the ratio   of 7:1:2 . The number of training samples of eachservice ranges from 112 to 4.7 K , and there are 2   to 10 slots for one service . More details about data   statistics can be found in the Appendix ( Table 8) .   4.2 Evaluation Protocol   We evaluate DST performance using the widely   adopted Joint Goal Accuracy ( JGA ) ( Wu et al . ,   2019 ) , which requires all slots ’ values are correctly   predicted . We assign the target service during test-   ing to avoid ambiguity since the same dialog can   be parsed differently under different services . We   denoteaas the JGA on the test set of task T   right after training on task T. We evaluate the CL   performance as the average JGA on all tasks after   training on the ﬁnal task T :   Avg : JGA = 1   TXa ( 5 )   Following Lopez - Paz and Ranzato ( 2017 ) , we   deﬁne two metrics to measure the effect of forward   transfer and backward transfer , respectively :   FWT = 1   T 1Xa   BWT = 1   T 1Xa a(6 )   FWT is the averaged zero - shot performance on new   tasks , evaluating a model ’s generalization ability .   BWT assesses the impact that learning on subse-   quent tasks has on a previous task . Negative BWT   indicates that the model has forgotten some previ-   ously acquired knowledge .   4.3 Baselines and Training Details   We adopt the following models from Madotto et al .   ( 2021 ) as baselines :   •Fine - tuning : Fine - tune the model on new task   data continually .   •Replay : SavejMjsamples randomly sampled   from the training set of each task Tto memory   Mand jointly train the model on new task data   Dand memory M.   •EWC : Maintain the memory in the same way as   Replay but use it to compute the Fisher informa-   tion matrix for regularization ( Kirkpatrick et al . ,   2017 ) .   •AdapterCL : Freeze the pre - trained model and   train a residual Adapter ( Houlsby et al . , 2019 ) for   each task independently ( Madotto et al . , 2021 ) .   Above methods use the same input and output for-   mat as in Figure 2(a).Prompt tuning based methods including our pro-   posed Continual Prompt Tuning are list below :   •Prompt Tuning : Formulate DST as a masked   spans recovering task ( Sec . 3.2 ) and only tune   the prompt for each task independently .   •Multi - task Prompt Tuning : Prompt Tuning in a   multi - task manner instead of CL . Train a single   prompt using all tasks ’ data concurrently .   •Continual Prompt Tuning : Prompt Tuning with   CLInit ( Sec . 3.3.1 ) and query fusion ( Sec . 3.3.2 ) .   – w/ memory with memory replay ( Sec . 3.3.3 ) .   – w/ memory & backward with memory replay   and memory - guided backward transfer ( Sec .   3.4 ) .   We use the following setting in the experiments   unless otherwise speciﬁed .   Training task sequences Since a sequence of all   ( 44 ) tasks is too long for the evaluation purpose , we   conduct most of the experiments on 15 tasks chosen   at random to save computing resources . We run   AdapterCL , Prompt Tuning , and Multi - task Prompt   Tuning 5 times with different random seeds because   they are agnostic to task order . The FWT and BWT   metrics for these models are left blank . We run   other methods in the same 5 task orders created   by random permutation . The selected tasks and   ordering are listed in the Appendix ( Table 9 ) .   Hyper - parameters We use T5 - small as the back-   bone model and reuse its sentinel tokens ( Raffel   et al . , 2020 ) . For each task , Continual Prompt Tun-   ingﬁrst trains 10 epochs with fused query ( and   using memory if available ) for forward transfer .   Afterward , it concentrates on the current task and   continues training 10 epochs on the original data of   the current task . When using backward transfer , we   train 5 epochs for each previous task . Other meth-   ods train 20 epochs for each task . We use AdamW   and set the learning rate to 3e-5 for Fine - tuning ,   Replay , and EWC , 3e-3 for AdapterCL , and 0.5   for all prompt tuning based methods . We set the   batch size to 16 for prompt tuning based methods   and 8 for other methods . To avoid overﬁtting , we   perform early stopping if validation performance   does not improve for 5 consecutive epochs . The   weight for EWC regularization loss is 0.01 . We set   the memory sizejMjto 50 for each task and save   the same samples for all methods that require mem-   ory . We initialize prompt tokens with the tokens   randomly drawn from the vocabulary . For prompt   tuning based methods , we tune 100 soft prompttokens with the embedding size 512 for each task ,   resulting in 51.2 K parameters . To compare param-   eter efﬁciency , we adjust AdapterCL ’s parameters   for each task to be nearly 1x or 20x as ours .   5 Experiments and Analysis   The experiments are organized as follows . We com-   pare our method with baselines in Sec . 5.1 , and   present a comprehensive ablation study in Sec . 5.2 .   We investigate the effect of prompt initialization in   Sec . 5.3 , and the effect of model size and prompt   length in Sec . 5.4 .   5.1 Main Experiment   Computation Resource Analysis . In CL , there   is a trade - off between performance and computa-   tion resources . Ideally , we hope to utilize the least   amount of computation resources to achieve the   best performance . We take three vital resources   into our consideration . Memory saves previous   tasks ’ samples , which may involve privacy issue   and requires extra storage . Additional parame-   tersare the extra parameters we add to our model   to cope with different tasks along the CL process ,   which should be kept to a minimum in order to   scale to long task sequences . Tunable parame-   ters are the trainable parameters when we learn   a task , which is important for GPU memory and   computation . We show the usage of these resources   in Table 1 ( right ) . Replay storesjMjsamples for   each task and does not need extra parameters . EWC   saves the Fisher information matrix and original   parameters , requiring two times additional param-   eters . AdapterCL , Prompt Tuning , and Continual   Prompt Tuning require no memory and only add a   small number ( 2 % or 0.1 % ) of additional param-   eters for each task , largely reducing the computa-   tional and storage overhead . Apart from the vanilla   form , Continual Prompt Tuning can also utilize the   memory if available .   CL Performance Analysis . Overall CL results   of different methods are summarized in Table 1   ( left ) . We have the following ﬁndings :   •Consistent with Madotto et al . ( 2021 ) , both Fine-   tuning andEWC suffer from catastrophic forget-   ting while replaying memory can alleviate the   problem to a large extend . Fine - tuning andEWC   have a low Avg . JGA because of the large neg-   ative BWT , while Replay improves BWT a lot   thus has a high Avg . JGA.Method Avg . JGA FWT BWT Memory + Params Tune Params   Fine - tuning 14.3 8.3 - 49.9 - 0 1   EWC 13.9 8.4 - 50.8jMj*T 2 1   Replay 58.6 10.9 -3.2jMj*T 0 1   AdapterCL ( 20x ) 49.8 - - - 2%*T 2 %   AdapterCL ( 1x ) 30.6 - - - 0.1%*T 0.1 %   Prompt Tuning 48.1 - - -   0.1%*T 0.1%Continual Prompt Tuning 59.5 9.9 0 -   w/ memory 60.7 13.7 0jMj*T   w/ memory & backward 61.2 13.7 0.5jMj*T   Multi - task Prompt Tuning 64.0 - - - 0.1 % 0.1 %   •Our proposed Prompt Tuning with masked spans   recovering is more parameter efﬁcient than   AdapterCL . In terms of Avg . JGA , Prompt Tun-   ingis much better than AdapterCL with the same   size and comparable to AdapterCL with 20x pa-   rameters .   •Forward transfer through CLInit and query fu-   sion is effective for Prompt Tuning .Continual   Prompt Tuning improves over Prompt Tuning sig-   niﬁcantly and outperforms baselines .   •When memory is available , our method achieves   the best results w.r.t . all metrics , closing the gap   between CL and multi - task learning . Memory   improves zero - shot performance ( FWT ) on new   tasks as Replay is better than Fine - tuning and   Continual Prompt Tuning w/ memory is better   than without memory .   •Our memory - guided backward transfer effec-   tively utilizes subsequent tasks to help previous   tasks . Although minor , Continual Prompt Tuning   w/ memory & backward is the only method that   exhibits positive BWT .   5.2 Ablation Study   To understand the effect of different proposed tech-   niques , we conduct an in - depth ablation study and   show the result in Table 2 . Row 1 and 2 do not for-   mulate DST as a masked spans recovering ( MSR )   task : the input is the concatenate of the dialog , ser-   vice name , and soft prompt , while the output is a   sequence of slot - value pairs as in Fine - tuning ( Fig-   ure 2(a ) ) . Several interesting observations can be   noted : First , formulating DST as MSR is beneﬁ-MSR CLInit QF MR Avg . JGA FWT   1 29.6 -   2 X 41.8 6.7   3X 48.1 -   4X X 57.6 9.6   5X X X 59.5 9.9   6X X X 60.411.9   7X X X X 60.713.7   cial . Using MSR achieves better CL performance   regardless of learning each task independently ( row   3 v.s. row 1 ) or continually using CLInit ( row 4   v.s. row 2 ) . Besides , MSR formulation improves   zero - shot generalization on new tasks ( row 4 v.s.   row 2 ) . Second , forward transfer through CLInit   brings large improvement for CL . CLInit outper-   forms random initialization greatly for both using   MSR formulation ( row 4 v.s. 3 ) and not ( row 2 v.s.   1).Third , both query fusion and memory replay   are effective . When they are used separately , mem-   ory replay ( row 6 ) boosts the performance more   than query fusion ( row 5 ) , while applying them   altogether achieves the best performance ( row 7 ) .   5.3 Continual Prompt Initialization   In this experiment ( Table 3 ) , we compare CLInit   with other prompt initialization strategies for   Prompt Tuning in CL . SelectInit ( see Sec . 3.3.1)Initialization Avg . JGA FWT   Random 48.1 -   SelectInit 54.5 8.2   CLInit 57.6 9.6   Training Testing tasks   task sequenceTTT   T 45.1 - -   T 54.2 59.7 -   T 59.0 64.4 64.3   T 60.7 67.8 69.3   selects the prompt that has the best zero - shot perfor-   mance on the current task from all previous tasks ’   prompts for initialization . We could see that both   SelectInit and CLInit outperform random initial-   ization signiﬁcantly , demonstrating the effective-   ness of transferring knowledge from previous tasks   through prompt initialization . CLInit is slightly bet-   ter than SelectInit in both Avg . JGA and zero - shot   generalization ( FWT ) , which reveals the beneﬁt of   accumulating knowledge from all seen tasks . In   contrast , the prompt initialized by SelectInit has   seen fewer tasks and thus contains less knowledge ,   which might explain the slightly worse result .   Based on the observation above , we further study   that whether seeing more preceding tasks further   helps CLInit . To this end , we choose a task order   of all 44 tasks at random ( see Table 8 in the Ap-   pendix ) and perform Prompt Tuning with CLInit on   the last 5 , last 15 , last 30 , and all 44 tasks separately .   Formally , we train on four CL curriculums T ,   T , T , andT , which have the same end-   ing . We calculate the Avg . JGA on the T ,   T , andT if possible . As illustrated in Ta-   ble 4 , performance on the same tasks ( in the same   column ) increases monotonously as the number of   preceding tasks grows . This pattern validates that   the beneﬁt of CLInit becomes more evident as the   number of tasks increases . This ﬁnding suggests   that our method is suitable for long task sequences .   Prompt Length   1 5 20 100 150   T5 - small ( 60 M ) 6.1 6.7 8.9 9.8 9.8   T5 - base ( 220 M ) 5.7 9.9 12.9 18.3 15.0   T5 - large ( 770 M ) 10.6 17.0 18.5 28.0 31.2   5.4 Model Size and Prompt Length   In this experiment , we analyze the inﬂuence of pre-   trained model size and prompt length . We vary the   pre - trained model in { T5 - small , T5 - base , T5 - large }   and prompt length in { 1 , 5 , 20 , 100 , 150 } for Con-   tinual Prompt Tuning on the 15 tasks ( the task order   is in Table 9 in the Appendix ) . Figure 3 shows Avg .   JGA and Table 5 shows FWT . We can observe that :   First , when ﬁxing the prompt length , increasing   the model size improves the Avg . JGA as well   as the generalization ability measured by FWT in   most cases . Second , when the backbone model   size is ﬁxed , increasing the prompt length improves   the overall performance in general . Furthermore ,   we found that increasing prompt token length from   20 to 100 improves Avg . JGA and FWT more than   increasing it from 100 to 150 , which is consistent   with the ﬁnding in Lester et al . ( 2021 ) . Third , our   method becomes more parameter - efﬁcient as the   backbone model size grows . With the same num-   ber of tunable parameters ( x - axis ) , using a larger   pre - trained model achieves better Avg . JGA .   5.5 The Effect of Memory Size   In this section , we compare the role of memory in   Replay and our method . We vary the memory size   per taskjMjin { 10 , 50 , 100 } and show the per-   formance of Replay andContinual Prompt Tuning   with memory replay ( and memory - guided back-   ward transfer ) in Table 6 . We can ﬁnd that increas-   ing the memory size beneﬁts Replay signiﬁcantly .   This is not surprising because Replay and other   rehearsal methods rely on memory to solve the   challenging forgetting problem . When the memory   size is unlimited , Replay degenerates to multi - task   learning , which is powerful but costly in storage   and computation .   ForContinual Prompt Tuning , however , the   memory is not used for retaining the performance   on previous tasks since parameters for previous   tasks are saved .   •In forward transfer , the memory helps recall pre-   vious tasks ’ knowledge and serves as a comple-   ment to CLInit and query fusion . The inﬂuence   on Avg . JGA depends on the effect of transfer   learning on the current task via multi - task train-   ing ( L(D+M ) ) . As shown in the row 2   in Table 6 , increasing the memory size does not   improve Avg . JGA signiﬁcantly and may even   distract the model from learning the current do-   main . This result suggests that our method does   not need a large memory for forward transfer .   •In backward transfer , the memory gives refer-   ence gradients to guide the updates and serves as   a ﬁlter to decide whether to accept the updates .   Thus larger memory gives more accurate guid-   ance . From the bottom row in Table 6 , we can   ﬁnd that increasing memory size can improve the   effect of backward transfer .   We also conduct experiments using a percentage   memory budget , setting the memory size for each   task proportional to task data size : jMj / jDj .   This means low - resource tasks have fewer samples   stored in the memory than in the original setting .   We set the total memory size to 50 * T , where   T is the number of tasks . As shown in Table 7 ,   Replay performs much worse ( 58.6 ! 55.8 ) in the   unbalanced task memory setting while the effect   onContinual Prompt Tuning w/ mem . is slight   ( 60.7!60.3 ) . Besides , our proposed backward   transfer technique is still effective .   Overall , these results indicate that compared   with Replay , our method uses the memory differ-   ently and beneﬁts less from enlarging the memory .   6 Conclusion   In this paper , we develop prompt tuning for con-   tinual learning for the ﬁrst time . We propose Con-   tinual Prompt Tuning , a highly parameter - efﬁcient   framework that avoids forgetting and enables for-   ward / backward knowledge transfer among tasks .   For forward transfer , we explore continual prompt   initialization , query fusion , and memory replay   techniques . For backward transfer , we devise   a memory - guided technique . Extensive experi-   ments on continual learning for DST demonstrate   the effectiveness and efﬁciency of our proposed   method compared with state - of - the - art baselines .   Our method and ﬁndings will foster more future   studies towards building more scalable , adaptable   task - oriented dialog systems .   Acknowledgements   This work was supported by the National Sci-   ence Foundation for Distinguished Young Scholars   ( with No . 62125604 ) and the NSFC projects ( Key   project with No . 61936010 and regular project with   No . 61876096 ) . This work was also supported   by the Guoqiang Institute of Tsinghua University ,   with Grant No . 2019GQG1 and 2020GQG0005 ,   and sponsored by Tsinghua - Toyota Joint Research   Fund . ReferencesTask ID Service # Slots # Dialogs # Samples Avg . tokens   Train Dev Test Train Dev Test Context Query   1 events_3 5 53 7 16 312 40 105 121 47   2 banks_2 4 29 4 9 220 31 72 111 49   3 banks_1 4 144 21 42 1138 169 335 114 57   4 calendar_1 4 118 17 34 773 110 234 112 33   5 movies_3 3 33 5 10 112 18 37 72 26   6 music_2 5 231 33 67 1593 221 469 117 54   7 services_2 5 129 19 37 917 148 253 131 52   8 payment_1 4 25 3 8 233 33 89 171 52   9 media_1 4 196 28 57 1207 182 360 99 48   10 weather_1 2 58 8 17 259 39 66 77 16   11 events_1 6 202 29 58 1424 195 400 132 64   12 ﬂights_4 7 60 9 18 290 41 87 90 77   13 travel_1 4 48 7 14 231 28 63 87 59   14 buses_2 6 111 16 32 857 120 234 137 54   15 events_2 6 400 57 115 3537 521 1067 159 59   16 alarm_1 2 58 9 17 367 49 107 101 22   17 buses_3 7 61 9 18 405 66 114 123 69   18 services_1 5 185 27 53 1241 180 352 129 58   19 buses_1 5 136 20 39 1054 143 313 138 49   20 restaurants_2 9 87 13 28 807 113 240 154 97   21 hotels_2 6 212 31 61 1569 234 460 152 73   22 ridesharing_2 3 64 9 19 380 49 108 106 34   23 rentalcars_1 6 100 14 29 840 120 242 161 59   24 movies_1 8 263 37 76 1873 250 556 122 70   25 ridesharing_1 3 74 10 22 412 57 125 103 36   26 media_3 4 56 8 16 327 42 89 95 36   27 music_3 6 17 3 5 112 19 32 114 60   28 movies_2 3 32 5 10 118 20 38 70 30   29 ﬂights_2 7 129 19 37 822 115 251 127 75   30 services_4 5 86 13 25 680 97 208 154 49   31 ﬂights_1 10 560 80 160 4680 667 1379 168 10   32 services_3 5 131 19 38 959 143 290 143 54   33 ﬂights_3 8 65 10 19 420 75 116 133 79   34 trains_1 7 58 9 17 415 67 117 131 76   35 homes_2 8 62 9 18 424 56 139 140 89   36 rentalcars_2 6 77 11 23 631 91 185 157 61   37 restaurants_1 9 256 37 74 2098 297 581 153 10   38 music_1 6 68 10 20 468 73 142 118 61   39 hotels_4 7 80 12 23 559 99 141 134 72   40 media_2 5 32 4 10 215 29 71 112 59   41 hotels_3 6 90 13 26 737 100 193 157 64   42 rentalcars_3 7 44 7 13 332 55 99 148 72   43 hotels_1 7 99 14 29 868 105 250 161 71   44 homes_1 7 244 35 70 1829 282 540 159 81Task order Tasks ’ IDs in order   Order1 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44   Order2 39 33 36 42 40 37 38 34 32 35 41 31 30 44 43   Order3 30 41 38 31 43 39 40 33 34 44 37 36 32 35 42   Order4 43 40 44 38 30 37 31 39 32 35 41 34 33 36 42   Order5 30 33 44 31 38 32 42 40 37 43 36 39 41 35 34