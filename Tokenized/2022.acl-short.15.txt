  Matthew A. Byrd Shashank Srivastava   University of North Carolina at Chapel Hill   matthew_a_byrd@outlook.com , ssrivastava@cs.unc.edu   Abstract   Item Response Theory ( IRT ) has been exten-   sively used to numerically characterize ques-   tion difﬁculty and discrimination for human   subjects in domains including cognitive psy-   chology and education ( Primi et al . , 2014 ;   Downing , 2003 ) . More recently , IRT has been   used to similarly characterize item difﬁculty   and discrimination for natural language mod-   els across various datasets ( Lalor et al . , 2019 ;   Vania et al . , 2021 ; Rodriguez et al . , 2021 ) . In   this work , we explore predictive models for   directly estimating and explaining these traits   for natural language questions in a question-   answering context . We use HotpotQA for il-   lustration . Our experiments show that it is pos-   sible to predict both difﬁculty and discrimina-   tion parameters for new questions , and these   traits are correlated with features of questions ,   answers , and associated contexts . Our ﬁndings   can have signiﬁcant implications for the cre-   ation of new datasets and tests on the one hand   and strategies such as active learning and cur-   riculum learning on the other .   1 Introduction   The use of question answering for testing learning   often relies on characterizing questions on aspects   such as difﬁculty anddiscrimination . For exam-   ple , ordering questions by difﬁculty can enable   curriculum learning ( Bengio et al . , 2009 ) . Simi-   larly , discrimination is used in standardized exams   such as the SAT to ensure that questions are varied   enough to discriminate between high - ability and   low - ability respondents . Item Response Theory   ( IRT ) ( Wright and Stone , 1979 ; Lord , 1980 ) has   been a widely applied framework to jointly esti-   mate such parameters for questions ( or items ) andthe abilities of respondents . While IRT has its in-   ception in psychometrics and has traditionally been   used with human respondents , recently , it has been   explored for analyzing predictions from an ‘ artiﬁ-   cial crowd ’ of ML models ( Prudêncio et al . , 2015 ;   Plumed et al . , 2016 ; Martínez - Plumed et al . , 2019 ;   Lalor et al . , 2019 ; Vania et al . , 2021 ; Rodriguez   et al . , 2021 ) .   While it can be helpful to know which ques-   tions are difﬁcult / discriminatory , it can be equally   important to be able to determine a question ’s dif-   ﬁculty / discrimination parameters without having   to use it in a testing environment ( as is required to   estimate IRT parameters ) . Some recent work , such   as Ha et al . ( 2019 ) , has explored using features   derived from the text of a question to predict the   difﬁculty in the context of multiple - choice medi-   cal exams . While others ( Benedetto et al . , 2020 )   have used tf - idf features to predict the difﬁculty   of questions as measured by IRT . We differ from   these works in two ways : Firstly , while Ha et al .   ( 2019 ) ; Benedetto et al . ( 2020 ) both predict the dif-   ﬁculty of items for humans , we are interested in pre-   dicting the difﬁculty ( and discrimination ) of items   for QA models . Secondly , we choose a question-   answering dataset , HotpotQA ( Yang et al . , 2018 ) ,   as our testbed . We utilize this dataset to generate a   rich and varied feature set across each item ’s ques-   tion , answer , and associated contexts . We can then   employ these features to analyze our difﬁculty and   discrimination predictions , giving us insights into   both our underlying QA model and factors that can   increase the difﬁculty / discrimination of a question .   Our analysis shows signiﬁcant variations among   questions and reveals some surprising patterns . We   show that it is possible to predict both difﬁculty   and discrimination of natural language questions ,   which can have multiple applications in education   and pedagogy . Additionally , we see that different   surface - level features are associated with high dis-   crimination and high difﬁculty , which can inform119new evaluation methods and the creation of new   datasets . Further , we identify attributes for predict-   ing difﬁculty and discrimination that are general   enough to be adapted to various QA datasets .   2 IRT Analysis of HotpotQA   IRT background : We begin by summarizing the   1PL and 2PL models from IRT , which form the   basis of our later analysis . The 1PL ( 1 Parameter   Logistic ) model describes the probability of respon-   denticorrectly answering the j’th item ( question )   in terms of scalar - valued parameters for question   difﬁculty ( d ) and respondent ability (  ) . These   parameters are estimated from data y2f0;1 g   for a set of i , jpairs . Here , y= 1 indicates a   correct answer . The 1PL model is described by :   p(y= 1j;d ) = 1   1 + e   The 2PL model extends the 1PL by adding a scalar-   valued parameter  , which represents the discrim-   ination of the j’th item . Intuitively , this parameter   denotes how sharply the probability of answering   a question correctly changes as the ability of the   respondent increases . The 2PL model is described   by :   p(y= 1j;d ;  ) = 1   1 + e   Dataset description : We chose HotpotQA for our   analysis since it is signiﬁcantly more complex than   other datasets such as SQuAD ( Rajpurkar et al . ,   2016 ) due to the questions requiring multi - hop rea-   soning and having more complex language . In   HotpotQA , each question is paired with two para-   graphs considered ‘ gold ’ contexts and several other   paragraphs considered ‘ distractor ’ contexts . The   answer to each question is a span in one of the   gold contexts , but correctly answering the question   requires combining information from both ‘ gold ’   contexts .   2.1 Estimating IRT Parameters   We estimate the IRT parameters for the questions   in HotpotQA ’s dev set ( 7;405questions ) . How-   ever , collecting human responses for each question ,   which is necessary to estimate IRT parameters , is   infeasible . Motivated by Lalor et al . ( 2019 ) , we   create an artiﬁcial crowd of QA models in placeof a crowd of human respondents . For this , we   train 148instances of DFGN ( Qiu et al . , 2019 )   models on HotpotQA ’s train set . To ensure diver-   sity , we uniformly sample the number of training   epochs from 1to15and sample the fraction of the   training data used for model training from U(0;1 ) .   Otherwise , each model was trained with the hyper-   parameters described in Qiu et al . ( 2019 ) . Next , we   generate an item - response matrix indicating which   questions from the HotpotQA dev set each model   answered correctly ( i.e. , the model ’s answer ex-   actly matched the correct answer ) . We remove any   questions that received no correct answers or no   incorrect answers . This is done as during the esti-   mation process , these questions tend towards ( + /- )   inﬁnity in their difﬁculty parameters , as well , their   discrimination parameter estimate tends towards   zero ( unable to distinguish between high and low   performing models ) . Our ﬁnal dataset is a subset   of4;000questions ( 2;000train , 1;000dev , and   1;000test ) . Finally , we ﬁt the 1PL and 2PL mod-   els on the foresaid item - response matrix using the   variational IRT training procedure from Natesan   et al . ( 2016 ) .   2.2 Analysis of Estimated Parameters   Figure 1 shows a scatter - plot of estimated dif-   ﬁculty and discrimination values for individual   questions . We note that some discrimination val-   ues asymptotically approach 0 . This occurs when   some questions receive very few or many correct   answers ; these questions can not discriminate high-   performing from low - performing models . We also   note that some questions have negative discrimina-   tion , i.e. , as a model ’s ability increases , its probabil-   ity of answering the question correctly decreases .   This is primarily a result of some of the highest per-120   forming models giving an answer which is either a   subspan of or contains the ground - truth answer of   questions that were otherwise answered correctly   by lower - performing models . Overall , there is a   weak positive correlation between discrimination   and difﬁculty ( =0:04 ) .   To visualize any correlation between the seman-   tic and syntactic information of questions and their   respective difﬁculty levels , we clustered questions   based on their BERT embeddings using KMeans   ( K=20 ) clustering ( 2D UMAP reduction shown in   Figure 2 ) . Through manually examining and label-   ing the clusters , we found that many clusters could   be described with a speciﬁc style ( e.g. , yes / no ques-   tions ) or general topic . Some clusters , such as C.3 ,   have a large variety in the phrasing of questions   being asked and the potential answers in both syn-   tactic and semantic features . For example , both Q :   Khushi Ek Roag is broadcast by a company based   out of where ? A : Dubai andQ : To Catch a Preda-   tor was devoted to impersonating people below the   age of consent for which in North America varies   by what ? A : jurisdiction are in C.3 .   Other clusters , such as C.1 and C.2 , ( yes / no clus-   ters ) , only vary in topic rather than the type of   question . In particular , for these clusters , the es-   timated difﬁculty has signiﬁcantly lower variance   than the other clusters ( = 0:02,= 0:04respec-   tively ) , indicating that these yes / no questions tend   to be consistent in their difﬁculty . The standard   deviation values for C.1 and C.2 are 1:08and1:19   respectively , the average standard deviation valueis2:27 . We further explore how these factors affect   predicting the difﬁculty values in section 4 .   3 Predicting IRT Parameters   We next discuss predictive models for discrimina-   tion and difﬁculty using features from the question ,   answer , and associated context . First , we describe   our feature set , then provide an ablation study , a   feature importance study , and ﬁnally qualitatively   analyze the predictions of our best model .   3.1 Feature Design   We experiment with two categories of fea-   tures : human - centric and machine - centric features .   For human - centric features , we considered ( 1 )   counting - based Lexical & Syntactic features ex-   tracted for both questions and answers like Con-   tentWords , Type - token ratio , Avg . Word Length ,   Complex Words ( > 3syllables ) ; ( 2 ) Semantic-   Ambiguity features measuring a question ’s or an-   swer ’s ambiguity ( Ha et al . , 2019 ) ; and ( 3 ) Read-   ability features based on measures like Fleisch   Kincaid index . More feature details can be found   in Appendix C. For machine - centric features , we   considered ( 1 ) Contextual Embeddings for ques-   tions and answers from BERT ( Devlin et al . , 2019 ) ;   ( 2 ) n - gram Overlap Counts between the question   and answer , and between question / answer and the   gold / distractor paragraphs ; and ( 3 ) POS Counts   from the Stanford Tagset ( Toutanova et al . , 2003 )   for the question and answer .   3.2 Quantitative Analysis and Ablation   Table 1 and Table 2 show the regression perfor-   mance of our models for predicting the IRT difﬁ-   culty / discrimination parameters of the questions in   our dev / test sets using the feature sets described   before . The reported results are averaged over a 10-   fold cross - validation . We note that the best models   for both difﬁculty and discrimination show signif-   icant (  < 0:10 ) predictive performance ( Rof   0:17and0:13 ) against our baseline ( Mean ) .   The best performance is achieved in both tasks   by considering all features . In both cases , there is   a signiﬁcant difference (  < 0:1 ) in performance   between using any single set and using all features ,   except the best - performing BERT feature set . We   also note that features derived from the answer   are typically better at capturing difﬁculty , while   features derived from the question better predict   the discrimination parameters . However , the per-121   formance of All ( Q ) and All ( A ) for both the dis-   crimination and difﬁculty is weaker than using all   features . Since the difference is not statistically sig-   niﬁcant , it is unclear how much predictive power is   added when considering both answer and question   features in these predictions .   The features that focus on human difﬁculty are   among the less effective feature sets , indicating that   the human difﬁculty features of a question do not   fully capture difﬁculty for QA models . We provide   details of models and their training and the exper-   iment setup in Appendix A ; as well , signiﬁcance   tests can be found in Appendix D.   3.3 Feature Importance Study   We estimated feature importance by permuting   each feature individually and measuring the change   in MSE on the dev set . We list features that caused   a change in MSE of at least .01 in tables 3 and 4 .   We point out that for predicting the discrimina-   tion , the number of cardinal digits in the answer   was the most important indicator of high discrimi-   nation . The positive correlation between the num-   ber of digits in the answer and the discrimination   of a question is expected . Qiu et al . ( 2019 ) showed   that the DFGN model has a signiﬁcant weakness   in numeric operations . This gives questions with   numeric answers a high discrimination value as   DFGN models are naturally inhibited in this regard ,   and thus only a few models with the most training   data will be capable of answering these questions .   We ﬁnd a similar positive Pearson score ( = 0:14 )   between the difﬁculty and the number of cardinal   digits in the answer . While this weakness of the   DFGN model can not be applied to an arbitrary QA   model , the methodology used to determine this   weakness can be applied arbitrarily , which can give   solid grounding to claims about model weaknesses .   4 Qualitative Analysis   We qualitatively analyze the difﬁculty predic-   tions to understand the predictions of our best-   performing model . Similar to Figure 2 , Figure 3122shows a UMAP scatterplotfor questions on our   test split of the estimated IRT parameters . In this   case , instead of color - coding by difﬁculty as in   Figure 2 , we instead color - code by the absolute   error between our predictions and the measured dif-   ﬁculty of each question . We again apply KMeans   ( k= 10 ) to our data with a smaller number of   clusters due to the smaller size of the test set . We   highlight CT.1 , like C.1 and C.2 of Figure 2 , this   cluster consists primarily of yes / no questions . The   difﬁculty in CT.1 has signiﬁcantly smaller vari-   ance in the estimated difﬁculties than the rest of   the clusters ( = 0:02 ) . As well , the prediction   error for CT.1 has signiﬁcantly smaller variance   ( =0:04)and had the smallest average prediction   error compared to the other clusters ( 0:68 ) . This   indicates that the model is able to recognize when   question groupings , such as yes / no questions , have   consistent difﬁculties ( as discussed in 2.1 ) and has   consistently lower error when predicting difﬁculty   for these questions . However , the prediction error   tends to vary more when the surface - level ques-   tion types are not sufﬁcient to characterize their   difﬁculty .   We explore this further through a small counter-   factual experiment . We are interested in taking an   item with high prediction error and slightly tweak-   ing it to understand how the model ’s predictions   can change with changes in the question and an-   swer . We selected an item with > 2absolute error   to perform this experiment . The question we use   in this study is : Which university is this American   philosopher , theologian , and Christian apologist   who supports theistic science , professor at ? with   an answer of Biola University . The predicted dif-   ﬁculty was 0:51 . We found that simple changes   to the question , such as using synonyms and re-   moving unnecessary information , can increase the   predicted difﬁculty up to  0:21 . However , by mod-   ifying the answer ( and by necessity the question )   to be either a date or yes , we achieve a higher difﬁ-   culty prediction ( 0:53and1:02 , respectively ) . This   further indicates the model ’s bias towards yes / no   questions being of a higher difﬁculty regardless of   the style or topic of question being asked . Some   of our changes and their corresponding predictions   are listed in Appendix E.   5 Conclusion   In this paper , we explored QA datasets through the   lens of Item Response Theory . We have demon-   strated a way to build regression models that can   describe the difﬁculty and discrimination of a ques-   tion . We note that our work is limited in two im-   portant ways : ﬁrstly , we only use the DFGN model   in our artiﬁcial crowd , which may have introduced   a bias in which some factors that make questions   difﬁcult / discriminatory are only applicable to this   model . Secondly , we only explore the HotPotQA   dataset , which may further limit our analysis to   only be applicable to HotPotQA or similar datasets .   Future work could incorporate multiple models and   datasets to explore a more easily generalizable dif-   ﬁculty / discrimination prediction pipeline . We also   note that our analysis here focused on QA . How-   ever , there are many NLP tasks in which the difﬁ-   culty or discrimination of an item may be important .   Our work here could naturally extend to these do-   mains . Finally , automatically predicting these traits   without relying on user responses can engender a   host of creative educational applications . Future   work can also leverage such predictive models to   explore more efﬁcient strategies for learning and   evaluation .   References123124125A Models & Training   For the 1PL and 2PL prediction , we considered   linear models with L1 & L2 regularization , random   forests , gradient boosted regressors , and bayesian   ridge models . All hyperparameters were kept con-   stant as the default in the sklearn package ( Pe-   dregosa et al . , 2011 ) . We performed 10 - fold cross-   validation using PyCaret ( Ali , 2020 ) . All models   were trained on a consumer grade processor .   B Feature Deﬁnitions   •Human - Centric Features   – Lexical & Syntactic features : These   consist primarily of counting features :   ContentWords , Type - token ratio , Avg .   Word Length , Complex Words ( > 3syl-   lables ) . These are calculated for both the   answer and question . A full list of these   features can be found in Appendix F   – Semantic - Ambiguity features : We use   WordNet ( Miller , 1995 ) to calculate the   ambiguity of sentences , similar to Ha   et al . ( 2019 ) . These are calculated for   both answer and question .   – Readability features : We use previous   work ( Kincaid et al . , 1975 ; Gunning ,   1952 ; Laughlin , 1969 ) to model the read-   ability of a question / answer ( e.g. Fleisch   Kincaid index ) . These are further ex-   panded on in Appendix C.   •Machine - Centric Features   – Contextual Embeddings : We use the   BERT - base model ( Devlin et al . , 2019 )   to obtain sentence embeddings for ques-   tions and answers .   – Overlap Counts : We count overlaps   between the question and answer of n-   grams up to n= 3 . We also com-   pute overlap counts between the ques-   tion / answer and the gold and distractor   paragraphs .   – Part of Speech Counts : We count POS   tags for tags from the Stanford NLP   tagset ( Toutanova et al . , 2003 ) for both   the question and answer .   C Reading Difﬁculty Features   We list the reading difﬁculty features we used in our   experiments and an overview of their calculations .   Each calculation has its own coefﬁcients that can   be found in their respective citations.•Flesch Reading Ease - linear combination of   words / sentence and syllables / word ( Flesch )   •Flesch Kincaid Grade Level - linear combi-   nation of word / sentence and syllables / word   ( Kincaid et al . , 1975 )   •Automated Readability Index ( ARI ) - lin-   ear combination of characters / word and   words / sentence ( Smith and Senter , 1967 )   •Gunning Fog index - linear combination of   words / sentence and complex words / words .   Complex words are words with 3syllabus   ( Gunning , 1952 )   •Coleman - Liau - linear combination of   letters/100 words and sentences/100   words.(Entin and Klare , 1978 )   •SMOG index - calculates the grade level   by considering the number of complex   words / sentence ( Laughlin , 1969 )   D Signiﬁcance Tests   We provide signiﬁcance tests for the difﬁculty and   discrimination predictions in tables 5 and 6 . We see   that the BERT features and using all features are   able to beat the baseline with statistical signiﬁcance   ( :1 ) . Note that we compare using MSE rather   thanRas the baseline always has an Rscore of   0 . We also provide in table 7 the signiﬁcance tests   for using all features against BERT features . We   ﬁnd that the best performing BERT feature set does   not have a statistically signiﬁcant improvement in   performance when compared to the all feature set .   In this case , we use Ras the performance metric .   E Counterfactual Results   •–Question ( original ): Which university is   this American philosopher , theologian ,   and Christian apologist , who supports   theistic science , professor at?’126   – Answer : " Biola University "   – Pred . Diff: 0:51   •–Question : Which school is this philoso-   pher and theologian who supports sci-   ence , professor at ?   – Answer : " Biola University "   – Pred . Diff: 0:21   •–Question : What was the birth date of a   professor at Biola University who is an   American philosopher , theologian , and   Christian apologist , who supports theis-   tic science ?   – Answer : March 9 , 1948   – Pred . Diff : 0:53   •–Question : Does Biola University have   a professor who is an American philoso-   pher , theologian , and Christian apologist ,   who supports theistic science ?   – Answer : yes   – Pred . Diff : 1:02   F Lexical Features   We list our full list of lexical features , these features   are a subset of the lexical features used in Ha et al .   ( 2019 ) .   • Word Count   • Content Word Count   • Content Word Incidence• Content Word Count No Stopwords   • Noun Count   • Noun Incidence   • Verb Count   • Verb Incidence   • Adjective Count   • Adjective Incidence   • Adverb Count   • Adverb Incidence   • Number Count   • Number Incidence   • Type Count   • Type Token Ratio   • Comma Count   • Comma Incidence   • Average Word Length In Syllables   • Complex Word Count   • Complex Word Incidence ,   • Average Sentence Length   • Negation Count   • Negation Incidence   • Negation In Stem   • NP Count   • NP Incidence   • Average NP Length   • NP Count With Embedding   • NP Incidence With Embedding   • Average All NP Length ,   • PP Count   • PP Incidence   • PPs Per Sentence Ratio   • VP Count127• VP Incidence   • Passive Active Ratio   • Proportion Active VPs   • Proportion Passive VPs   • Agentless Passive Count   • Relative Clauses Count   • Relative Clauses Incidence   • Proportion Relative Clauses   • Polysemic Word Count   • Polysemic Word Incidence   • Average Sense No Content Words   • Average Sense No Nouns   • Average Sense No Verbs   • Average Sense No Non Auxiliary Verbs   • Average Sense No Adjectives   • Average Sense No Adverbs   • Average Noun Distance To WNRoot   • Average Verb Distance To WNRoot ,   •Average Noun And Verb Distance To WN-   Root   • Answer Words In Word Net Ratio   • Average Word Frequency Abs   • Average Word Frequency Rel   • Average Word Frequency Rank   • Average Content Frequency Abs   • Average Content Frequency Rel   • Average Content Frequency Rank   • Not In First 2000 Count   • Not In First 2000 Incidence   • Not In First 3000 Count   • Not In First 3000 Incidence   • Not In First 4000 Count• Not In First 4000 Incidence   • Not In First 5000 Count   • Not In First 5000 Incidence   • Imagability   • Imagability Found Only   • Imagability Ratio   • Familiarity   • Familiarity Found Only   • Familiarity Ratio   • Concreteness   • Concreteness Found Only   • Concreteness Ratio   • Age Of Acquisition   • Age Of Acquisition Found Only   • Age Of Acquisition Ratio   • Meaningfulness Colorado Found Only   • Meaningfulness Pavio Found Only   • No Imagability Rating   • No Familiarity Rating   • No Concreteness Rating   • No Age of Acquisition Rating   • Connectives Count   • Connectives Incidence   • Additive Connectives Count   • Additive Connectives Incidence   • Temporal Connectives Count   • Temporal Connectives Incidence   • Causal Connectives Count   • Causal Connectives Incidence   • Referential Pronoun Count ,   • Referential Pronoun Incidence128 G Discrimination UMAP plots   In the following section , we provide the UMAP   reduction plots for the discrimination parameters   ( darker being more discriminatory ) , as well as the   prediction error UMAP plot for our best model   ( darker meaning higher error).129130