  Enrique Amigó   UNED   Madrid , SpainAgustín D. Delgado   UNED   Madrid , Spain   Abstract   Several natural language processing ( NLP )   tasks are deﬁned as a classiﬁcation problem in   its most complex form : Multi - label Hierarchi-   cal Extreme classiﬁcation , in which items may   be associated with multiple classes from a set   of thousands of possible classes organized in   a hierarchy and with a highly unbalanced dis-   tribution both in terms of class frequency and   the number of labels per item . We analyze the   state of the art of evaluation metrics based on   a set of formal properties and we deﬁne an in-   formation theoretic based metric inspired by   the Information Contrast Model ( ICM ) . Exper-   iments on synthetic data and a case study on   real data show the suitability of the ICM for   such scenarios .   1 Introduction   Many natural language processing ( NLP ) problems   involve classiﬁcation , such as sentiment analysis ,   entity linking , etc . However , the adequacy of eval-   uation metrics is still an open problem . Different   metrics such as Accuracy , F - measure or Macro Av-   erage Accuracy ( MAAC ) may differ substantially ,   seriously affecting the system optimization process .   For example , assigning all elements to the majority   class may be very effective according to Accuracy   and score low according to MAAC .   In addition , in many scenarios such as tagging   in social networks ( Coope et al . , 2018 ) or topic   identiﬁcation ( Yu et al . , 2019 ) , the classiﬁer must   assign several labels to each item ( multi - label clas-   siﬁcation ) . This greatly complicates the evaluation   problem since , in addition to the class speciﬁcity   ( frequency ) , other variables appears such as the dis-   tribution of labels per item in the gold standard , the   excess or absence of labels in the system output , etc .   The evaluation problem becomes even more   complicated if we consider hierarchical category   structures , which are very common in NLP . For   example , toxic messages are divided into different   types of toxicity ( Fortuna et al . , 2019 ) , named enti-   ties could be organized in nested categories ( Sekine   and Nobata , 2004 ) , etc . In these scenarios , the cat-   egory proximity in the hierarchical structure is an   additional variable .   Even , the problem can be further complicated .   Extreme Classiﬁcation scenarios address with thou-   sands of highly unbalanced categories ( Gupta et al . ,   2019 ) , where a few categories are very frequent   and others completely infrequent ( Almagro et al . ,   2020 ) . In addition , some items have no category   at all and some have many . An example scenario   that we will use as a case study in this article is the   labelling of adverse events in medical documents .   In this paper , we analyse the state of the art on   metrics for multi - label , hierarchical and extreme   classiﬁcation problems . We characterize existing   metrics by means of a set of formal properties . The   analysis shows that different metric families satisfy   different properties , and that satisfying all of them   at the same time is not straightforward .   Then , propose an information - theoretic based   metric inspired by the Information Contrast Model   similarity measure ( ICM ) , which can be particular-   ized to simpler scenarios ( e.g. ﬂat , single labeled )   while keeping its formal properties . Later , we de-   ﬁne a set of ﬁve tests on synthetic data to compare   empirically ICM against existing metrics . Finally ,   we explore a case study with real data which shows   the suitability of ICM for such extreme scenarios .   The paper ends with some conclusions and future   work.58092 Background   In this section , we analyze the literature on the   two main evaluation problems tackled in this paper :   multi - labeling and class hierarchies , keeping the   focus on extreme scenarios ( numerous and unbal-   anced classes ) .   2.1 Multi - Label Classiﬁcation   There are three main ways of generalizing effec-   tiveness metrics to the multi - label scenario ( Zhang   and Zhou , 2014 ) . The ﬁrst one consists in model-   ing the problem as a ranking task , i.e. the system   returns an ordered label list for each item according   to their suitability . Some speciﬁc ranking metrics   applied in multi - label classiﬁcation displayed in   ( Wu and Zhou , 2017 ) are : Ranking Loss , which   is a ordinal correlation measure , one - error which   is based on Precision at 1 , or Average Precision .   Although these metrics are very common , they do   not take into account the speciﬁcity of ( unbalanced )   classes . Jain et al . proposed the propensity versions   of ranking metrics ( Precision@k , nDCG ) in order   to weight classes according to their frequency in   the data set ( Jain et al . , 2016 ) .   Reducing the classiﬁcation to a ranking problem   is specially appropriate in extreme classiﬁcation   scenarios and simpliﬁes the deﬁnition of metrics .   However , it also has several disadvantages . First , it   requires the output of the classiﬁer to be in ranking   format , and that does not ﬁt many scenarios . For   example , annotating posts in social networks re-   quires predicting the amount of tags to be assigned   to the post . For this reason , we focus on classiﬁ-   cation outputs , so ranking based metrics are out of   our scope .   Apart from ranking metrics , multi - label effec-   tiveness metrics have been categorized into label-   and example - based metrics ( Tsoumakas et al . ,   2010 ; Zhang and Zhou , 2014 ) . Label - based eval-   uation measures assess and average the predictive   performance for each category as a binary classi-   ﬁcation problem , where the negative category cor-   responds with the other categories . The most pop-   ular are the label - based Accuracy ( LB - ACC ) and   F - measure ( LB - F ) . The label - based metrics have   some drawbacks . First , they do not consider the   distribution of labels per item . Hits are rewarded   independently of how many labels are associatedto the item . Second , while items are supposed to   be random samples , classes are not , so the idea   of averaging results across classes is not always   consistent . That is , the metric scores can vary sub-   stantially depending on how the category space is   conﬁgured . Finally , if there are a large number   of possible categories ( extreme classiﬁcation ) , the   score contribution of any label has an upper limit of , beingCthe set of categories . This limit can be   problematic , specially when labels are unbalanced   and numerous .   On the other hand , the example - based metrics   compute for each object , the proximity between   predicted and true label sets ( s(d ) = fc;::;cg   andg(d ) = fc;::;cg ) . Some popular ways to   match category sets in multi - label classiﬁcation   evaluation are the Jaccard similarity ( EB - JACC )   which is computed as(Godbole and   Sarawagi , 2004 ) , or the precision   , re-   call   and their F combination ( EB - F ) .   Another example - based metric is the Hamming   Loss ( EB - HAMM ) ( Zhang et al . , 2006 ) which   matching function is deﬁned as :   whereCrepresents the set of categories anno-   tated in the gold standard . Subset Accuracy ( EB-   SUBACC ) ( Ghamrawi and McCallum , 2005 ) is a   more strict measure due to it requires exact match-   ing between both category sets . Notice that all   example - based multi - label metrics converge to Ac-   curacy in the single - label scenario . On the other   hand , there are some situations in which these met-   rics are undeﬁned . If both the gold standard and the   system output label sets are empty , the maximum   score is usually assigned to the item .   The main drawback of these approaches is that   they do not take into account the speciﬁcity of   classes ( i.e. unbalanced classes in extreme clas-   siﬁcation ) . The label propensity applied over preci-   sion and recall for single items can solve this lack .   Each accurate class in the intersection is weighted   according to the class propensityp(Jain et al . ,   2016 ):   Prop(i ) = P   js(i)j   Prop(i ) = P   jg(i)j   The propensity factor pfor each class is com-   puted as :p = whereNis5810the number of data points annotated with label c   in the observed ground truth data set of size N   andA , Bare application speciﬁc parameters and   C= ( logN 1)(B+ 1 ) . In our experiments , we   set the recommended parameter values A= 0:55   andB= 1:5 .   However , propensity precision and recall values   are not upper bounded astends to inﬁnite when   ptends to zero . In order to solve this issue , in our   experiments we replace the normalization factors   js(i)jandjg(i)jwith the accumulation of inverse   propensities in the system output or the gold stan-   dard . We also add the empty class cin both the   system output and the gold standard in order to   capture the speciﬁcity of classes in the mono - label   scenario :   Prop(i ) = PP   Prop(i ) = PP   wheres(i ) = s(i)[fcgandg(i ) = g(i)[fcg .   Propensity F - measure ( PROP - F ) is computed as   the harmonic mean of these values .   2.2 Hierarchical Classiﬁcation   There are different taxonomies of hierarchical clas-   siﬁcation metrics ( Costa et al . , 2007 ; Kosmopou-   los et al . , 2013 ) . Kosmopoulos et al . distinguish   between pair and set - based metrics . Pair - based   metrics weight hits or misses according to the dis-   tance between categories in the hierarchy . This dis-   tance depends on the number of intermediate nodes   ( Wang et al . , 1999 ; Sun and Lim , 2001 ) , with the   disadvantage that the speciﬁcity of the categories is   not taken into account . Depth - based distance met-   rics include the class depth in the metric ( Blockeel   et al . , 2002 ) . However , the depth of the node is not   sufﬁcient to model its speciﬁcity since depending   on their frequency , leaf nodes at the ﬁrst levels may   be more speciﬁc than leaf nodes at deeper levels .   It is possible to compare the predicted and true   single labels by means of standard ontological sim-   ilarity measures such as Leackock and Chodorow   ( path - based ) ( Leacock and Chodorow , 1998 ) , Wu   and Palmer ( Wu and Palmer , 1994 ) , Resnik ( depth-   based ) ( Resnik , 1999 ) , Jiang and Conrath ( Jiang   and Conrath , 1997 ) or Lin ( Lin , 1998 ) similarities .   The last two are based on the notion of Informa-   tion Content ( IC ) or category speciﬁcity , i.e. , theamount of items belonging to the category or any   of its descendants .   However , extending pair - based hierarchical met-   rics to the multi - label scenario is not straightfor-   ward . Sun and Lim extended Accuracy , Precision   and Recall measures for ontological distance based   metrics ( Sun and Lim , 2001 ) . This method has   two drawbacks . First , it requires deﬁning a neutral   hierarchical distance , i.e. , an acceptable distance   threshold for range normalization purposes . The   second drawback is that it inherits the weaknesses   of label - based metrics ( see previous section ) . Bloc-   keel et al . proposed computing a kernel and thus   deﬁne a Euclidean distance metric between sums   of class values ( Blockeel et al . , 2002 ) . The draw-   back is that they assume a previously deﬁned dis-   tance metric between categories and the origin and   between different categories . Information based   ontological similarity measures such as Jiang and   Conrath or Lin ’s similarity do not have an upper   bound which is necessary for the calculation of   accuracy and coverage .   On the other hand , set - based metrics ( also   called hierarchical - based ) consider the ancestor   overlap ( Kiritchenko et al . , 2004 ; Costa et al . ,   2007 ) . More concretely , hierarchical precision and   recall are computed as the intersection of ancestor   divided by the amount of ancestors of the system   output category and of the gold standard respec-   tively . Their combination is the Hierarchical F-   measure ( HF ) . Since these metrics are based on cat-   egory set overlap , they can be applied as example   based multi - label classiﬁcation by joining ances-   tors and computing the F measure . Their drawback   is that the speciﬁcity of categories is not strictly   captured since they assume a correspondence be-   tween speciﬁcity and hierarchical deepness . How-   ever , this correspondence is not necessarily true .   Categories in ﬁrst levels can be infrequent whereas   leaf categories can be very common in the data set .   In this paper , we propose an information theo-   retic similarity measure called Information Con-   trast Model ( ICM ) . ICM is an example - based met-   ric as it is computed per item . Just like HF , ICM   is a set - based multi - label metric as it computes   the similarity between category sets . Unlike HF ,   ICM takes into account the statistical speciﬁcity of   categories.58113 Formal Properties   In order to deﬁne the set of desirable properties ,   we formalize both the gold standard gand the sys-   tem outputsas sets of item / category assignments   ( i;c)2IC , whereIandCrepresent the set of   items and categories respectively . We will denote   asP(c)the probability of items to be classiﬁed   ascin the gold standard ( P((i;c)2gji2I ) ) .   We also assume that the categories in the hier-   archical structure are subsumed . For instance ,   items in a PERSON_NAMED_ENTITY category   are implicitly labeled with the parent category   NAMED_ENTITY . The common ancestor with   maximum depth is denoted as lso(c;c)and the   descendant categories are denoted as Desc ( c)in-   cluding itself .   Note that we do not claim that all properties are   necessary in any scenario . The purpose of this   article is to provide at least one metric that is capa-   ble of capturing all aspects simultaneously when   necessary .   The ﬁrst property is related to hits . In order to   make this aspect independent from the ability of   the metrics to capture hierarchical relationships or   multi - labeling , we deﬁne monotonicity over hits in   the simplest case ( ﬂat single label scenario ):   Property 1 [ Strict Monotonicity ] A hit increases   effectiveness . Given a ﬂat single label category   structure , if ( i;c)2gns , thenEff(s[f(i;c)g ) >   Eff(s )   The next two properties state that the speciﬁcity   of both the predicted and the true category affects   the metric score . That is , an error or a hit in an   infrequent category should have more effect than   in the majority category . For instance , identifying   a rare symptom in a medical report should be re-   warded more than identifying a common malady   present in the vast majority of patients . In addition ,   both the speciﬁcity of the actual category and the   speciﬁcity of the category predicted by the system   must be taken into account . Again , we make this   aspect independent of hierarchical structures and   multi - labeling .   Property 2 [ True Category Speciﬁcity ] Given a   ﬂat single label category distribution , if P(c ) <   P(c)and(i;c);(i;c)2gns , then Eff(s [   f(i;c)g)>Eff(s[f(i;c)g ) .   Property 3 [ Wrong Category Speciﬁcity ] Given a   ﬂat single label category distribution , if P(c)<P(c)and(i;c);(i;c)=2g[s , then Eff(s [   f(i;c)g)<Eff(s[f(i;c)g ) .   The following property captures the effect of the hi-   erarchical category structure . A common element   of any hierarchical proximity measure is that it is   monotonic with respect to the common ancestor .   That is , our brother is always closer to us than our   cousin , regardless of which family proximity crite-   rion is applied . In this property we do not consider   multi - labelling .   Property 4 [ Hierarchical Proximity ] Under   equiprobable categories ( P(c ) = P(c ) =   P(c ) ) , the deepness of the common ancestor   affects similarity . Given a single label hierarchical   category structure , if s(i ) = ; , g(i ) = c   and lso(c;c)2Desc ( lso(c;c))then   Eff(s[f(i;c)g)>Eff(s[f(i;c)g ) .   The last two properties are related with the multi-   labeling problem . Property 5 rewards the amount   of predicted categories per item .   Property 5 [ Multi - label Monotonicity ] The   amount of predicted categories increases effective-   ness . Given a ﬂat multi - label category structure , if   ( i;c)2gns , then Eff(s[f(i;c)g)>Eff(s )   Property 6 rewards hits on multiple items regarding   a single item with multiple categories . To under-   stand the motivation for this property , we can con-   sider an extreme case . Identifying 1000 symptoms   in one patient report is of less health beneﬁt than   identifying one symptom in 1000 patients .   Property 6 [ Label vs. Item Quantity ] n hits on   different items are more beneﬁcial than n labels   assigned to one item . Given a ﬂat multi - label   category distribution , if 8j= 1::n((j;c)2   gns)and8j= 1::n;i > n ( ( i;c)2gns )   then Eff(s[f(1;c);::;(n;c)g)>Eff(s [   f(i;c);::;(i;c)g ) .   4 Metric Analysis   In this section , we analyze existing metrics on the   basis of the proposed formal properties ( Table 1 ) .   Most of metrics satisfy Strict Monotonicity in sin-   gle label scenarios . The label - based metric LB - F   captures the true and wrong category speciﬁcity   via the recall component . The example - based met-   ric PROP - F ( modiﬁed as described in Section 2 )   captures these properties via the propensity factor .   Notice that the original propensity F - measure does   not capture the wrong category speciﬁcity ( Prop-   erty 3 ) given that the pfactor is applied only to5812   hits . In addition , both kind of metrics do not cap-   ture hierarchical structures . The contribution of   example regarding label - based metrics is that , as   label - based metrics are computed item by item , the   property Label vs. Item Quantity is satisﬁed ( Prop-   erty 6 ) . The exception is EB - HAMM which does   not normalize the results with respect to the amount   of labels assigned to the item .   Unlike previous metrics , the set based F - measure   ( HF ) captures the hierarchical structure ( Property   4 ) . However , it does not capture the category speci-   ﬁcity ( properties 2 and 3 ) . Some information - based   ontological similarity measures , ( Lin and Jiang &   Conrath ) capture both the category speciﬁcity and   the hierarchical structure . However , they are not de-   ﬁned for multi - label classiﬁcation ( properties 5 and   6 ) . In sum , different metric families satisfy differ-   ent properties , and that satisfying all of them at the   same time is not straightforward . The properties of   ICM are described in the next section .   5 Information Contrast Model   TheInformation Contrast Model ( ICM ) is a simi-   larity measure that uniﬁes measures based on both   object feature sets and Information Theory ( Amigó   et al . , 2020 ) . Given two feature sets AandB , ICM   is computed as :   ICM(A;B ) =  IC(A)+  IC(B)   IC(A[B )   WhereIC(A)represents the information content   (  log(P(A))of the feature set A. In our scenario , objects are items to be classiﬁed and features are   categories . The intuition is that the more the cat-   egory sets are unlikely to occur simultaneously   ( largeIC(A[B ) ) , the less they are similar . Given   a ﬁxed joint IC , the more the category sets are   speciﬁc ( IC(A)andIC(B ) ) , the more they are   similar . ICM is grounded on similarity axioms sup-   ported by the literature in both information access   and cognitive sciences . In addition , it generalizes   the Pointwise Mutual Information and the Tver-   sky ’s linear contrast model ( Amigó et al . , 2020 ) .   5.1 Computing Information Content   The IC of a single category corresponds with the   probability of items to appear in the category or any   of its descendant . It can be estimated as follows :   whereIrepresent the set of items assigned to the   categorycandDesc(c)represents the set of de-   scendant categories . In order to estimate the IC of   category set , we state the following considerations .   The ﬁrst one is that , given two categories AandB   the common ancestor represents their intersection   in terms of feature sets :   fcg\fcg = lso(c;c ) ( 1 )   The second consideration is that we assume Infor-   mation Additivity , i.e. the IC of the union of two5813sets is the sum of their IC ’s minus the IC of its   intersection :   Equations 1 and 2 are enough to compute ICM in   the single label scenario . Generalizing for category   sets :   where , according to the transitivity property ;   fcg\fc;::;cg=[(fcg\fcg )   and according to Equation 1 , it is equivalent toSflso(c;c)g . Then , we ﬁnally obtain a   recursive function to compute the IC of a category   set :   In the case of ICM , it is possible the need for   estimating the IC of classes that do not appear in   the gold standard . Therefore , we have not evidence   about its frequency or probability . We apply a   smoothing approach by considering the minimum   probability .   5.2 Parameterization and Formal Properties   On the basis of ﬁve general similarity axioms , in   ( Amigó et al . , 2020 ) it is stated that the ICM pa-   rameters should satisfy  ;  <   <   1 +  2 .   We propose the parameter values  =  = 2an   = 3 . This parameterization leads to the follow-   ing instantiations for each particular classiﬁcation   scenario . In the hierarchical mono - label scenario ,   it becomes into ( equations 1 and 2 ):   which is similar to the Jiang and Conrath onto-   logical similarity measure . In the ﬂat multi - label   scenario , it becomes into :   which is an information additive example - based   metric . That is , the information content of the com-   mon categories minus the differences . Finally , in   the traditional ﬂat mono - label scenario , it becomes   into :   which corresponds with Accuracy weighted ac-   cording to the information content of categories .   According to the ﬂat mono - label instantiation   ( Equation 5 ) ICMsatisﬁes the prop-   erties 1 2 and 3 . According to the single label   hierarchical instantiation ( Equation 3 ) Property 4   is satisﬁed . According to the ﬂat multi - label instan-   tiation ( Equation 4 ) , Property 5 is satisﬁed . Un-   fortunately , the label vs item quantity property is   not strictly satisﬁed given that the gain per hit is   additive in non hierarchical scenarios ( Property 6 ) .   However , in the experiments we will see that the   hit gain on items with many categories is smoothed   out if the categories are related to each other by a   hierarchical structure .   6 Experiments on Synthetic Data   Different evaluation aspects such as error rate , cat-   egory speciﬁcity , hierarchical structures , etc . , may   have more or less weight depending on the scenario .   These aspects correspond to the formal properties   deﬁned in the previous section . We perform a set of   tests in order to quantify the suitability of metrics   with respect to each property or evaluation aspect.5814   First , we generate the following synthetic data   set . First , we deﬁnea hierarchical structure struc-   ture of 700 categories exposed in Figure 1 . Note   that categoriesf1::10gare parent categories spread   throughout the hierarchy , and categories f11::700 g   are leaf categories . Secondly , We distributed 100   items across all categories . We generate assign-   ments for each pair item / category ( i;c)with a prob-   ability ofppwherep= max ;   withi= 1::1000 andp=()where   c= 1::700 . We repeat this 1000 times . The result   is a distribution ( 300;150;40;::;0:6;0:6)items   per category and ( 22:5;22;21:6;21:1;:::;0:5;0:5 )   labels per item . The purpose is to ensure unbal-   anced assignments across items and classes . We   generate 1000 gold standards by reordering the cat-   egory identiﬁers ceach time in the pcomputation   in order to alter the distribution of items in the   hierarchical structure .   We consider in this experiment the metrics label-   based Accuracy and F - measure ( LB - ACC and   LB - F ) , the example - based metrics Hamming ( EB-   HAMM ) , Jaccard ( EB - JACC ) , Subset Accuracy   ( EB - SUBACC ) , F - measure ( EB - F ) and Propensity   F - measure ( PROP - F ) , the Hierarchical F - measure   ( HF ) and ICM . The ontological similarity metrics   are discarded given that they are not deﬁned for   the multi - label case . Ranking based metrics are   discarded as the synthetic data set does not include   graded assignments .   After this , we perform the following tests by   comparing two noisy versions of the gold standard .   The test result is the percentage of cases in which   the hypothetically worse noised output is outscored   by the best noised output ( Table 2 ) . Ties count 0.5.In the ﬁrst experiment referred in Table 2 as Sen-   sitivity to Error Rate , We ran an error insertion   procedure 1000 times on the goldstandard , with a   probability of 0.09 and 0.1 for the best and worst   output respectively . On average we will have 9 and   10 errors respectively . Each error consists of ran-   domly choosing one of the 1000 assignments ( i;c )   of the goldstandard and removing it . For all met-   rics the best output outperforms the worst output in   more that 50 % of cases . LB - ACC and EB - HAMM   seems to be specially sensitive to the error rate .   This is due to the fact that they do not consider   other aspects such as the category speciﬁcity or the   hierarchical proximity . Surprisingly , ICM achieves   a relatively high error rate sensitivity although it   also consider other aspects . We do not have a clear   explanation for this .   The second experiment is the True Category   Speciﬁcity test . The intuition is that a gap in a fre-   quent category should have less effect than a gap in   an infrequent category . With an error rate of 0.05 ,   for the best output , we remove a single label assign-   ment randomly selected from all the goldstandard .   For the worst output , we ﬁrst select randomly a cat-   egory and then we remove an assignment from this   category . The result is that the best output tends   to concentrate the gaps in frequent categories to a   greater extent than the worst output . At the table   shows , the metrics that satisfy the corresponding   property achieve high scores ( LB - F , PROP - F and   ICM ) .   The third experiment is the Wrong Category   Speciﬁcity test . The intuition is that a wrong as-   signment in a frequent category should have less   effect than a wrong assignment in an infrequent   category . With an error rate of 0.05 , we select an5815   assignment ( i;c)randomly from items with a sin-   gle label . For the best output we replace cwith the   most frequent class different than c. For the worst   output , we replace cwith a randomly selected cat-   egory different than c. We obtain the same result   than in the previous experiment .   The fourth experiment is the Hierarchical Sim-   ilarity test . The intuition is that the more a wrong   assignment is far away from the correct category ,   the more it has effect in the effectiveness score .   Again , with an error rate of 0.05 , we select an as-   signment ( i;c)randomly from single labeled items   with leaf categories . For the best output we replace   cwith a sister wrong category . For the worst out-   put , we replace cwith a randomly selected wrong   category . Again , the metrics that satisfy the corre-   sponding property achieve high scores .   The last test is Item Speciﬁcity . The intuition   is that a wrong assignment in an item with many   labels should have more effect than an error in an   item with one or a few labels . For the best out-   put , for each error insertion iteration , we randomly   select an assignment ( i;c)(with the same error   rate 0.05 ) . For the worst output , we randomly se-   lect an item i , and we take one of its assignments   ( i;c ) . In both cases , the category is replaced with   a randomly selected wrong label . In other words ,   we distribute errors uniformly across item / category   assignments in the best output and we distribute   errors uniformly across items in the worst output .   The effect is that the best output concentrates errors   in items with many labels . Again , those metrics   that satisfy the corresponding metric achieve high   performance . The label - based F - measure tends to   reward the worst output . The reason is that items   with many labels tend to concentrate diverse labels .   Therefore , the label - based F measure penalizes thebest output . As discussed in the previous section ,   although ICM does not satisfy the property , the hit   gain on items with many categories is smoothed   out if the categories are related to each other by a   hierarchical structure .   7 A Case Study   The problem addressed is the automatic encod-   ing of discharge reports ( Dermouche et al . , 2016 ;   Bampa and Dalianis , 2020 ) from a Spanish hospital   to detect adverse events ( AEs ) from CIE-10 - ES ,   the Spanish version of the tenth revision of the   International Classiﬁcation of Diseases ( ICD-10 ) .   AEs detection ﬁts to the scenario tackled in this   article due to the following reasons : ( i ) Extreme :   CIE-10 - ES contains 4816 codes related to AEs ,   which probability follows a power - law distribution   since most of them rarely appear in health records   or even they do not appear ; ( ii ) Hierarchical : CIE-   10 - ES is a hierarchy with six levels : an empty root   ( csuch thatIC(c ) = 0 ) , and then a level com-   posed by three - character - codes categories which   can be divided into successive nested subcategories   adding characters until seven - character - codes at   most ; and ( iii ) Multi - label classiﬁcation : Each   discharge report could have associated with several   AEs codes .   We have used a corpus composed of 36264   real anonymized discharge reports ( Almagro et al . ,   2020 ) annotated with AEs codes by experts . The   corpus has been divided into three data sets , train-   ing , development and test , following the proportion   50%-30%-20 % respectively . The corpus includes   only 671 AEs codes of 4816 and 84 % of the dis-   charge reports have no AEs , so the data is highly   biased and unbalanced.5816We have applied ﬁve simple baselines in order   to analyze the behaviour of the metrics : ( i ) ALL   NONE does not assign any code to each item ;   ( ii)MOST FREQ . assigns the most frequent AE   code in the training data set ( T45.1X5A ) to each   item , which just appears in 68 items of 7253 ; ( iii )   MATCH 75 % divides each item into sentences   and assigns a code if a sentence contains 75 % of the   words of the code description avoiding stop - words ;   ( iv)SVM DESCR . creates a binary classiﬁer for   each AE code in the training set using the pres-   ence of words of the AEs codes descriptions in the   items as features , excepting stop - words ; ( v ) SVM   CODES : similar to the previous one but using as   features the annotated non - AEs codes in order to   check if AEs codes are related to non - AEs codes .   Note that MATCH 75 % is able to assign any AE ,   but the SVM baselines are only able to assign AEs   appearing in the training data set .   Table 3 shows the metrics results obtained by   each baseline . Unfortunately , with only ﬁve sys-   tems it is difﬁcult to ﬁnd differences in terms of   system ranking . Therefore , we have normalised the   values for each metric between the maximum and   the minimum obtained across the 5 systems in order   to study the relative differences of scores ( values in   brackets ) . LB - ACC , LB - F and EB - HAMM reward   the absence of most of the labels in the corpus , so   they are not suitable in this scenario . The rest of   the metrics sort systems in the same way . The par-   ticularity of ICM is that , as shows the normalized   results , the baseline MATCH 75 % is penalized with   respect to ALL NONE to a greater extent than in   other metrics , since MATCH 75 % assigns many   codes incorrectly , whereas ALL NONE does not   provide any information . Another slight particu-   larity of ICM is that the system SVM CODES is   rewarded against the rest of baselines to a greater   extent . Notice that SVM CODES achieves 269 hits   while SVM DESCR achieves 77 hits .   8 Conclusions and Future Work   The deﬁnition of evaluation metrics is an open   problem for extreme hierarchical multi - label clas-   siﬁcation scenarios due to the role of several vari-   ables , for instance , a huge number of labels , un-   balanced and biased label and item distributions ,   proximity between classes into the hierarchy , etc .   Our formal analysis shows that metrics from differ-   ent families ( label , example , set - based , ontological   similarity measures etc . ) satisfy different proper - ties and capture different evaluation aspects . The   information - theoretic metric ICM proposed in this   paper , combines strengths from different families .   Just like example - based multi - label metrics , it com-   putes scores by items . Just like set - based metrics , it   compares hierarchical category sets . Just like some   ontological similarity measures ( Lin or Jiang and   Conrath ) , it considers the speciﬁcity of categories   in terms of Information Content . Our experiments   using synthetic and real data show the suitability   of ICM with respect to existing metrics .   ICM does not strictly hold the label vs. item   quantity property . We propose to adapt ICM in or-   der to guarantee all the formal properties as future   work .   Acknowledgments   Research cooperation between UNED and the   Spanish Ministry of Economy and Competitive-   ness , ref . C039/21 - OT and in the framework of   DOTT - HEALTH project ( MCI / AEI / FEDER , UE )   under Grant PID2019 - 106942RB - C32 .   References581758185819