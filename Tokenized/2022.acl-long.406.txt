  Zhibin ChenYansong FengDongyan ZhaoWangxuan Institute of Computer Technology , Peking University , ChinaCenter for Data Science , Peking University , ChinaThe MOE Key Laboratory of Computational Linguistics , Peking University , China   { czb - peking , fengyansong , zhaody}@pku.edu.cn   Abstract   Typed entailment graphs try to learn the en-   tailment relations between predicates from text   and model them as edges between predicate   nodes . The construction of entailment graphs   usually suffers from severe sparsity and unrelia-   bility of distributional similarity . We propose a   two - stage method , Entailment Graph with Tex-   tual Entailment and Transitivity ( EGT2 ) . EGT2   learns local entailment relations by recogniz-   ing possible textual entailment between tem-   plate sentences formed by typed CCG - parsed   predicates . Based on the generated local graph ,   EGT2 then uses three novel soft transitivity con-   straints to consider the logical transitivity in en-   tailment structures . Experiments on benchmark   datasets show that EGT2 can well model the   transitivity in entailment graph to alleviate the   sparsity issue , and lead to significant improve-   ment over current state - of - the - art methods .   1 Introduction   Entailment , as an important relation in natural   language processing ( NLP ) , is critical to seman-   tic understanding and natural language inference   ( NLI ) . Entailment relation has been widely applied   in different NLP tasks such as Question Answer-   ing ( Pathak et al . , 2021 ; Khot et al . , 2018 ) , Machine   Translation ( Padó et al . , 2009 ) and Knowledge   Graph Completion ( Yoshikawa et al . , 2019 ) . When   coming across a question that " Which medicine   cures the infection ? " , one can recognize the infor-   mation " Griseofulvin is preferred for the infection , "   in the corpus and appropriately write down the an-   swer with the knowledge that " is preferred for "   entails " cures " when their arguments are medicines   anddiseases , although the surface form of predi-   cate"cures " does not exactly appear in the corpus .   There are many ways to present one question , and it   is impossible to handle them without understandingFigure 1 : A simple example of entailment graph with   types medicine anddisease . The dashed line repre-   sents a missing entailment recovered by considering the   transitivity constraint ( red ) based on the two premise   entailment between three boldfaced predicates .   the entailment relations behind the predicates . Pre-   vious works on analyzing entailment mainly focus   on Recognizing Textual Entailment ( RTE ) between   pairs of sentences , and many recent attempts have   achieved quite promising performance in detecting   entailment relations using transformer - based lan-   guage models ( He et al . , 2020 ; Raffel et al . , 2020 ;   Schmitt and Schütze , 2021b ) .   By modeling typed predicates as nodes and   entailment relations as directed edges , the   Entailment Graph ( EG ) is a powerful and   well - established form to represent the context-   independent entailment relations between predi-   cates and reflect the global features of entailment   inference , such as paraphrasing and transitivity . As   EGs are able to help reasoning without additional   context or resource , they can be seen as a special   type of structural knowledge in natural language .   Figure 1 shows an excerpt entailment graph about   two types of arguments , Medicine andDisease .   Generally speaking , an entailment graphs can be   built based on a three - step process : extracting   predicate pairs from a corpus , building local graphs   with locally computed entailment scores , and mod-   ifying the graphs with global methods .   However , existing EG construction methods still   face challenges in both local and global stages . The   Distributional Inclusion Hypothesis ( DIH ) about   entailment assumes that given a predicate ( rela-5899tion ) p , it can be replaced in any context by an-   other predicate ( relation ) qif and only if pentails   q(Geffet and Dagan , 2005 ) . Most local methods   in previous works are guided by DIH , thus rely   on the distributional co - occurrences from corpora ,   including named entities , entity pairs and context ,   as features to compute the local entailment scores .   Since different predicate pairs are processed in-   dependently , the locally built graphs suffer from   severe data sparsity . That is , there are many en-   tailment relations missing ( as edges ) in the graphs   if the predicate pairs do not co - occur in the corpus .   Furthermore , predictions from local models may   not be coherent with each other , for example , a   local model may output three predictions like , aen-   tailsb , bentails candcentails aat the same time ,   which actually indicate possible errors among the   local predictions .   To overcome the challenges faced by local mod-   els , different global approaches are used to take the   interactions and dependencies between entailment   relations into consideration . The first discussed   global dependency is the logical transitivity among   different predicates , that is , predicate aentails pred-   icatecif there is another predicate bmaking both " a   entails b " and " bentails c " hold simultaneously . Be-   rant et al . ( 2011 ) uses the Integer Linear Program-   ming ( ILP ) to ensure the transitivity constraints on   the entailment graphs , which is , unfortunately , not   scalable on large graphs with thousands of nodes .   Hosseini et al . ( 2018 ) models the structural similar-   ity across graphs and paraphrasing relations within   graphs to learn the global consistency , but does   not gain further improvement due to the lack of   high - quality local graphs and proper transitivity   modeling .   In order to deal with the problems in the lo-   cal and global stages , we propose a novel en-   tailment graph learning approach , Entailment   Graph with Textual Entailment and Transitivity   ( EGT2 ) . EGT2 builds high - quality local entail-   ment graphs by inputting predicates as sentences   into a transformer - based language model fine - tuned   on an RTE task to avoid the unreliability of distri-   butional scores , and models the global transitivity   on these scores through carefully designed soft con-   straint losses , which alleviate the data sparsity and   are feasible on large - scale local graphs . Our key   insight is that the entailment relation a→ccor-   rectly implied by the transitivity constraint is based   on two conditions : ( 1 ) the appropriate constraintscalable on large graphs containing rich informa-   tion , and ( 2 ) the reliability of local graphs offering   the premise a→bandb→c , which is imprac-   tical for previous distributional approaches , but   may be available for the models well - behaved on   RTE tasks . Specifically , the input sentences fed   to transformer - based language models are formed   without context , which makes our method accessi-   ble to those predicates not appearing in the corpus .   The transitivity implication is confined to entail-   ment relations with high confidence , which im-   proves the quality of implied edges and cuts down   the computational overheads . In a word , this paper   makes the following contributions :   •we present a novel approach based on textual   entailment to scoring predicate pairs on local   entailment graphs , which is reliable without   distributional features and valid for arbitrary   predicate pairs .   •we present three carefully designed global soft   constraint loss functions to model the transitiv-   ity among entailment relations on large entail-   ment graphs , thus alleviate the data sparsity   issue of previous local approaches .   •we evaluate our method on benchmark   datasets , and show that our EGT2 significantly   outperforms previous entailment graphs con-   struction approaches . The further analysis   proves that our local and global approaches   are both useful for learning entailment graphs .   2 Related Work   Based on DIH , previous works extract feature vec-   tors for typed predicates to compute the local dis-   tributional similarity . The set of entity argument   pair strings , like " Griseofulvin - infection " in the   example of Section 1 , are used as the features   weighted by Pointwise Mutual Information ( Be-   rant et al . , 2015 ; Hosseini et al . , 2018 ) . Given the   feature vectors for a predicate pair , different simi-   larity scores , like cosine similarity , Lin ( Lin , 1998 ) ,   DIRT ( Lin and Pantel , 2001 ) , Weeds ( Weeds and   Weir , 2003 ) and Balanced Inclusion ( Szpektor and   Dagan , 2008 ) , are calculated as the local similar-   ities . Hosseini et al . ( 2019 ) and Hosseini et al .   ( 2021 ) use Markov Chain on an entity - predicate   bipartite graph weighted by link prediction scores   to calculate the transition probability between two   predicates as the local score . They rely on the link5900predication model to generate the features in fact .   Guillou et al . ( 2020 ) adds temporal information   into entailment graphs by extracting entity pairs   within a limited temporal window as predicate fea-   tures . McKenna et al . ( 2021 ) extends the graphs   to include entailment relations between predicates   with different numbers of arguments by splitting   the features from argument pairs into independent   entity slots , which impairs the representation abil-   ity of features when unary predicates are involved .   As mentioned in Section 1 , entailment graphs are   generally learned by imposing global constraints   on the local entailment relations about extracted   predicates . The transitivity in entailment graphs is   modeled by the Integer Linear Programming ( ILP )   in Berant et al . ( 2011 ) , which selects a transitive   sub - graph of a local weighted graph to maximize   the summation over the weights of its edges . Their   work is limited to a few hundreds of predicates   due to the computational complexity of ILP . For   better scalability , Berant et al . ( 2012 ) and Berant   et al . ( 2015 ) make a strong FRG - assumption that if   predicate aentails predicates bandc , bandcentail   each other , and an approximation method , called   Tree - Node - Fix ( TNF ) . Obviously , the assumption   is too strong to be satisfied by real cases .   Since the hard constraints are difficult to work   well on large - scale entailment graphs , Hosseini   et al . ( 2018 ) propose two global soft constraints   that maintain the similarity between paraphrasing   predicates within typed graphs and between predi-   cates with the same names in graphs with different   argument types . Their soft constraints are also   used in Hosseini et al . ( 2019 ) and Hosseini et al .   ( 2021 ) . The similarity between paraphrasing pred-   icates , which ensures ( a→c)⊙(b→c)and   ( c→a)⊙(c→b)when a↔b , implicitly takes   the transitivity between paraphrasing predicates   and third predicate into consideration . But it ig-   nores the transitivity in more common cases , and   leads to a limited improvement on performance .   Meanwhile , the transformer - based Language   Model ( LM ) , although proved to be effective in   RTE tasks ( He et al . , 2020 ; Raffel et al . , 2020 ;   Schmitt and Schütze , 2021b ) , has received less at-   tention in entailment graph learning . Schmitt and   Schütze ( 2021a ) uses pretrained LM on the Lexical   Inference in Context ( LIiC ) task , which is closely   related to entailment graph learning . Hosseini et al .   ( 2021 ) uses pretrained BERT to initialize the con-   textualized embeddings in their contextualized linkprediction and entailment score calculation . Higher   scores are assigned to the entailed predicates in the   context of their premises , which is one implicit ex-   pression form of DIH and different from our direct   utilization of LM on textual entailment .   3 Our Method : EGT2   3.1 Definition and Notations   The goal of entailment graph learning is to ex-   tract predicates , learn the entailment relations   and build entailment graphs from raw text cor-   pora . Following previous works ( Hosseini et al . ,   2018 , 2019 ) , we use the binary relations from   neo - Davisonian semantics as predicates , which   is a type of first - order logic with event identi-   fiers . For instance , with the semantic parser ( here ,   GraphParser ( Reddy et al . , 2014 ) ) , the sentence :   " Griseofulvin is preferred for the infection . "   can be transformed into the logical form   ∃e.prefer(e , Griseofulvin )   ∩prefer(e , infection )   where edenotes an event . By considering a   relation for each pair of extracted arguments ,   this sentence refers to one predicate , p=(pre-   fer.2,prefer.for.2,medicine , disease ) . Likely , the   sentence " Griseofulvin cures the infection . " con-   tains q=(cure.1,cure.2,medicine , disease ) . For-   mally , a predicate with argument types tandt   is represented as p= ( w.i , w.i , t , t ) .   The event - based predicate form is strong enough to   describe most of the relations in real cases ( Parsons ,   1990 ) .   With Tas the set of types and Pas the set of   all typed predicates , V(t , t)contains typed pred-   icates pwith unordered argument types tandt ,   where p∈Pandt , t∈T. For predicate p=   ( w.i , w.i , t , t ) , we denote that τ(p ) =   t , τ(p ) = tandπ(p ) = ( w.i , w.i ) . In   other words , V(t , t ) = { p|(τ(p ) = t∧τ(p ) =   t)∨(τ(p ) = t∧τ(p ) = t ) } .   A typed entailment graph G(t , t ) = <   V(t , t ) , E(t , t)>is composed of the nodes   of typed predicates V(t , t)and the weighted   edges E(t , t ) . The edges can be also rep-   resented as sparse score matrix W(t , t)∈   [ 0,1 ] , containing the entailment   scores between predicates with type tandt . As5901   Predicates Sentences   ( be.1,be.capital.of.2,location , location ) Location A is capital of Location B.   ( contain.1,contain.2,location , location ) Location B contains Location A.   ( prefer.2,prefer.for.2,medicine , disease ) Medicine A is preferred for Disease B.   ( give.2,give.3,person , thing ) Person A is given Thing B.   ( aggrieved.by.2,aggrieved.felt.1,thing , person ) Person B feels aggrieved by Thing A.   the different argument types can naturally deter-   mine whether two predicates have the same order of   arguments , the order of argument type is not impor-   tant while t̸=t , and therefore we can ensure that   G(t , t ) = G(t , t ) . For those predicates pwith   τ(p ) = τ(p ) , the two argument types are labeled   with orders , which allows the graph to contain the   entailment relations with different argument or-   ders , like ( be.1,be.capital.of.2,location , location )   →(contain.1,contain.2,location , location ) .   3.2 Local Entailment based on Textual   Entailment   Inspired by the outstanding performance of pre-   trained and fine - tuned LMs on RTE task , which is   closely related to the entailment graphs , EGT2 uses   fine - tuned transformer - based LM to calculate the   local entailment scores of typed predicated pairs .   In order to utilize the knowledge about entail-   ment relations in pretrained and fine - tuned LM ,   EGT2 firstly transfers the predicate pair ( p , q)into   corresponding sentence pair ( S(p ) , S(q))by sen-   tence generator S , as the complicated predicates   can not be directly input into the LM . For typed   predicate p= ( w.i , w.i , t , t ) , the gen-   erator deduces the positions of arguments about   the predicate based on iandi , generates the   surface form of pbased on wandw , and   finally concatenates the surface form with capi-   talized types as its arguments . Some generated   examples are shown in Table 1 , and the detailed   algorithm of Sis described in Appendix A.   After generating sentence pair ( S(p ) , S(q))for   predicate pair ( p , q ) , EGT2 inputs ( S(p ) , S(q ) )   into a transformer - based LM to calculate the prob-   ability of the entailment relation p→qas the local   entailment score in G(t , t ) . In our experiments ,   the LM is implemented as DeBERTa ( He et al . ,   2020 ) . Generally , an entailment - oriented LM will   output three scores for a sentence pair , represent-   ing the probability of relationship entail , contra-   dictandneutral respectively . Formally , we denotethe weighted matrix of local entailment graph with   typetandtasW , and the weight of the edge   between pandqinWis calculated as :   W = P(p→q)∈[0,1 ] ,   P(p→q ) = e   Pe ,   ( 1 )   where LM(r|p , q)is the output score of corre-   sponding relationship by the LM . As the local en-   tailment is based on the LM fine - tuned to perform   textual entailment , the local graph can be built for   any predicates in the parsed semantic form , or in   any other forms by changing sentence generator S.   3.3 Global Entailment with Soft Transitivity   Constraint   Existing approaches use global learning to find   correct entailment relations which are missing or   underestimated in local entailment graphs to over-   come the data sparsity . Following Hosseini et al .   ( 2018 ) , the evidence from existing local edges with   high confidence is used by EGT2 to predict missing   edges in the entailment graphs .   Thetransitivity in entailment relation inference   implies a→cwhile both a→bandb→chold .   For instance , in the example of Figure 1 , the en-   tailment " is preferred for " →"is effective for " is   discovered because " is preferred for " →"cures "   and"cures " →"is effective for " have been learned .   The key challenge to incorporate the transitivity   constraint into weighted graphs is discreteness of   logical rules . Discreteness makes the rules impos-   sible to be directly used in gradient - based learning   methods without NP - hard complexity , as differ-   ent predicate pairs are jointly involved in the cal-   culation . To unify the discrete logical rules with   gradient - based learning , inspired by Li et al . ( 2019 ) ,   EGT2 uses the logical constraints in the form of dif-   ferentiable triangular norms ( Gupta and Qi , 1991 ;   Klement et al . , 2013 ) , or called t - norms , as the5902L=−logYmin(1 , W   WW )   = XI(W)I(W)ReLU ( logW+logW−logW )   L = X−I(W)I(W)I(WW−W)logW   L = X−I(W)I(W)I(WW−W)WWlogW(2 )   soft constraints so that the gradient - based learning   methods can be applied .   Different t - norm methods transfer the discrete   rules into different continuous loss functions . Tra-   ditional product t - norm maps P(A∧B)into   P(A)P(B),P(A∨B)intoP(A ) + P(B)−   P(A)P(B ) , and P(A→B)intomin(1 , ) .   For the entailment relations , the probability of tran-   sitivity to be satisfied is :   P[(a→b∧b→c)→(a→c ) ]   = min(1 , W   WW),(3 )   where the probability of the entailment relation   a→bis represented by the local entailment scores   W. To alleviate the noise from those edges as-   signed low confidence by local LM , EGT2 only   takes the local edges whose scores are higher than   1−ϵinto account ( as a→bandb→c ) , where ϵ   is a small hyper - parameter because the local proba-   bility scores tend to be close to 0or1 in practice .   Therefore , to maximize the probability of transi-   tivity constraint satisfied over all predicates in the   entailment graph G(t , t ) , EGT2 tries to minimize   the following minus - log - likelihood loss function   Lin Eq . 2 , where I(x ) = 1 ifx > y , or0   otherwise .   Another important t - norm , called the Gödel t-   norm , maps P(A→B)into1ifP(B)≥P(A)or   P(B)otherwise . Therefore , the Gödel probability   of transitivity to be satisfied is :   P[(a→b∧b→c)→(a→c ) ]   = WWW > W   1 otherwise,(4 )   and EGT2 similarly tries to minimize the loss func-   tionLin Eq . 2 . It should be noted that transitivity   constraints will be disobeyed not only by the miss-   ing edges , but also by the spurious edges in thelocal graphs . Therefore , we expect the soft con-   straints to take reducing the weights of premise   edges into consideration . Lachieves this by the   loss item WandW , and we modify LtoL   in Eq . 2 so that the low confidence of Wwill   help to detect whether WandWare spurious .   Our t - norm soft constraints , although do not guar-   antee the obedience of transitivity , are effective   approximations for the transitivity property .   Given the local entailment graph G(t , t)with   weighted edges W , in order to ensure that the   global entailment graph Wis not too far from   W , EGT2 finally minimizes the following loss   function Lto trade off the distance from local   graphs and the soft transitivity constraint :   L = X(W−W)+λL , i= 1,2,3   ( 5 )   where Lis the specified implementation of soft   transitivity constraint in Eq . 2 , and λis a non-   negative hyper - parameter that controls the influ-   ence of two loss terms .   4 Experimental Setup   4.1 Predicate Extraction   Following Hosseini et al . ( 2018 ) and Hosseini et al .   ( 2019 ) , we use the multiple - source NewsSpike   corpus ( Zhang and Weld , 2013 ) , which contains   550 K news articles , to extract binary relations as   generated predicates in EGT2 . We make use of   the triples released and filtered in Hosseini et al .   ( 2019 ) , which applies GraphParser ( Reddy et al . ,   2014 ) based on Combinatorial Categorial Grammar   ( CCG ) syntactic derivations to extracting binary re-   lations between predicates and arguments . The   argument entities are linked to Freebase ( Bollacker   et al . , 2008 ) and mapped to the first level of FIGER   types ( Ling and Weld , 2012 ) hierarchy . The type5903of a predicate is determined by its two correspond-   ing argument entities . The triples are filtered by   two rules to remove the noisy binary relations and   arguments : ( 1 ) we only keep those argument - pairs   appearing in at least 3 relations ; ( 2 ) we only keep   those relations with at least 3 different argument-   pairs . The number of relations in the corpus is   reduced from 26 M to 3.9 M , covering 304 K typed   predicates in 355 typed entailment graphs . Only   those predicate pairs co - occurring with at least one   same entity - pair ( e.g. , Griseofulvin - infection ) will   be linked to calculate the local scores , and as a   result , our local predicate pairs are identical with   Hosseini et al . ( 2019 ) . As we focus on using global   models to alleviate the sparsity of local edges , more   potential methods to extracting denser local edges   will be studied in our future research .   4.2 Evaluation Datasets and Metrics   We use Levy / Holt Dataset ( Levy and Dagan , 2016 ;   Holt , 2018 ) and Berant Dataset ( Berant et al . , 2011 )   to evaluate the performance of entailment graph   models .   In Levy ’s dataset , each example contains a pair   of triples with the same entities but different pred-   icates . Some questions with one predicate were   shown to the annotating workers , like " Which   medicine cures the infection ? " . The label for   each example are either True orFalse , indicating   whether the first typed predicate entails the second   one , by asking the workers whether the first predi-   cates can answer the question with the second one .   For example , if " Griseofulvin is preferred for the   infection " is a correct answer of the above ques-   tion , the dataset labels " is preferred for " →"cures " .   Holt ( 2018 ) re - annotates Levy ’s dataset and forms   a new dataset with 18,407 examples ( 3,916 posi-   tive and 14,491 negative ) , referred as Levy / Holt   Dataset . The dataset is split into validation set   ( 30 % ) and test set ( 70 % ) as Hosseini et al . ( 2018 )   in our experiments .   Berant et al . ( 2011 ) annotates all the entailment   relations in their corpus , which generates 3,427   positive and 35,585 negative examples , referred as   Berant Dataset . Their entity types do not exactly   match with the first level of FIGER types hierarchy ,   and therefore a simple hand - mapping by Hosseini   et al . ( 2018 ) is used to unify the predicate types .   To be comparable with previous works , we eval-   uate our methods on the test set of Levy / Holt   Dataset and the whole Berant Dataset by calcu - lating the area under the curves ( AUC ) with chang-   ing the classification threshold of global entailment   scores . Hosseini et al . ( 2018 ) argues that the AUC   of Precision - Recall Curve ( PRC ) for precisions in   the range [ 0.5,1 ] , as predictions with higher preci-   sion than random are more important for the down-   stream applications . Therefore , we report both the   AUC of PRC for precisions in the range [ 0.5,1]and   the traditional AUC of ROC , which is more widely   used in evaluation of other tasks .   4.3 Comparison Methods   We compare our model with existing entailment   graph construction methods ( Berant et al . , 2011 ;   Hosseini et al . , 2018 , 2019 , 2021 ) and the best local   distributional method , Balanced Inclusion ( Szpek-   tor and Dagan , 2008 ) , referred as BInc . We also   include ablation variants of our EGT2 , including   local models with or without fine - tuning .   4.4 Implementation Details   For local transformer - based LM , EGT2 uses De-   BERTa ( He et al . , 2020 ) implemented by the Hug-   ging Face transformers library ( Wolf et al . , 2019 ) ,   which has been fine - tuned on MNLI ( Williams   et al . , 2018 ) dataset . In order to adapt it to the spe-   cial type - oriented sentence pattern generated by S ,   we expand the validation set by extracting all of the   predicates , generating sentence pairs by generator   Sfor every two predicates , and checking whether   they are labeled as paraphrase or entailment in the   Paraphrase Database collection ( PPDB ) ( Pavlick   et al . , 2015 ) . We split 80 % of the generated corpus   to fine - tune the DeBERTa with Cross - Entropy Loss ,   and the rest as the validation set of fine - tuning pro-   cess . The fine - tuning learning rate α= 10 ,   and the process is terminated while the Fscore   ofentail on validation set does not increase in 10   epochs or training after 100 epochs .   For global soft transitivity constrains , we use   SGD ( Cun et al . , 1998 ) to optimize the scores Win   entailment graphs with loss function Lin Eq . 5 for   e= 5 epochs . The SGD learning rate α= 0.05 ,   the coefficient λ= 1 , and the confidence threshold   ϵ= 0.02 . The hyper - parameters are selected based   on Levy / Holt validation dataset . More implemen-   tation details are given in Appendix B.   For testing , if one or both predicates of the ex-   ample do not appear in the corresponding typed   entailment graph , we handle the example as un-5904   Methods Levy / Holt Berant   Metrics PRC ROC PRC ROC   BInc .155 .632 .147 .677   Local - Sup .161 .632 .129 .651   Hosseini18 .163 .637 .174 .682   Hosseini19.187 - - -   - Local .167 .639 .118 .378   Hosseini21.195 - - -   EGT2 - Local .313 .712 .360 .857   - w/o Fine - tuning .234 .673 .147 .732   EGT2- L .345 .761 .437 .880   EGT2- L .319 .755 .361 .879   EGT2- L .356 .755 .443 .871   typed one by resorting to its average score among   all typed entailment graphs . This setting is also   used for all local and global methods in the experi-   ments for fair comparison .   5 Experiment Results and Discussion   5.1 Main Results   We summarize the model performances on both   Levy / Holt and Berant datasets in Table 2 . All   global methods , including Hosseini et al . ( 2018 ) ,   Hosseini et al . ( 2019 ) and EGT2 , perform bet-   ter than their corresponding local methods , which   demonstrates the effect of global constraints in   alleviating the data sparsity . Although using the   same extracted entailment relations with Hosseini   et al . ( 2019 ) , our EGT2 - Local significantly outper-   forms previous local methods because of the high-   quality entailment scores generated by reliable fine-   tuned textual entailment LM . On the whole , EGT2   with transitivity constraint Loutperforms all the   other models on both Levy / Holt Dataset and Be-   rant Dataset with AUC of PRC , while EGT2- L   performs best with AUC of ROC . All of three soft   transitivity constraints boost the performance of   local model on all evaluation metrics , which shows   that making use of transitivity rule between entail-   ment relations improves the local entailment graph .   EGT2- Lor EGT2- Lperforms better than EGT2-   L , which indicates that involving the premises   a→bandb→cinto loss function is also impor-   tant for using transitivity constraints .   The Precision - Recall Curves of different meth - ods and the Precision - Recall Point of Berant et al .   ( 2011 ) on the two evaluation datasets are shown in   Figure 2(a ) and 2(b ) respectively . The local and   global models of EGT2 consistently outperform   previous state - of - the - art methods on all levels of   precision and recall , which indicates the effect of   our local model based on textual entailment and   global soft constraints based on transitivity . The   EGT2 - Local achieves slightly higher precision than   global models in the range recall < 0.5 , but its   precision drops quickly if we require higher re-   call and therefore leads to worse performance than   global models . The result indicates that global   models with transitivity constraints gain significant   improvement on recall with far less expense on   precision than EGT2 - Local .   5.2 How the local model fine - tuning works ?   As described in Section 4.4 , a new corpus is gener-   ated for fine - tuning the local model . We claim that   the fine - tuning corpus helps to improve the perfor-   mance of EGT2 - Local by adapting it to the special   sentence pattern by S , rather than offering addi-   tional data to fit the distribution of target datasets   as traditional training datasets do . To prove this , we   also test a simple supervised method , labelled as   Local - Sup , which fits a 2 - layers feedforward neural   network on the fine - tuning corpus with cosine sim-   ilarity , Weed , Lin and BInc scores as features . If   the corpus acts as training dataset , the performance   of Local - Sup should be obviously better than its   unsupervised features .   As shown in Table 2 , Local - Sup does not per-   form significantly better on Levy / Holt Dataset , and   even worse on Berant Dataset than BInc , which is   one of the inputting features of Local - Sup . The   result illustrates the difference between the fine-   tuning corpus and the evaluation datasets , and   shows that the corpus plays a role as pattern adapt-   ing corpus rather than training dataset .   5.3 Why are global constraints helpful ?   In Section 1 , we expect that the improvement of   soft transitivity constraints is attributed to the alle-   viation of data sparsity in corpus . To examine the   sparsity before and after the applying of transitivity   constraints , we count how many the positive and   negative entailment relations in the Levy / Holt test   set exactly appear in the local and global entail-   ment graph respectively , and show the counting   results in Table 3 . All three soft transitivity con-   straints help to find more entailment relations than5905   Methods Positive # Negative #   EGT2 - Local 378 75   EGT2- L 642 174   EGT2- L 783 277   EGT2- L 685 190   local entailment graph and therefore achieve better   performance on the evaluation datasets . Although   EGT2- Lfinds the most entailment relations in   the dataset in global stage , it finds more negative   examples concurrently and thus performs worse   thanLandLas shown in Table 2 . On the other   hand , EGT2- Land EGT2- Lobtain more propor-   tions of positive examples by considering premise   relations during the gradient calculation . The low   confidence of hypothesis relationship Wshould   be helpful to detect spurious premises Wand   W. Therefore , EGT2- Lslightly outperforms   EGT2- Las the gradients of WandWinL   are related to the hypothesis relationship W.   We have also applied the soft transitivity con-   straints on the local graph with BInc and Hos-   seini et al . ( 2019 ) , but observed only slightly im-   provement of performance , as .155→.157and   .167→.170for EGT2- Lon PRC of Levy / Holt   Dataset respectively . Comparing it with the signifi-   ca nt improvement based on EGT2 - Local , we claim   that the high - quality local entailment graphs are   the basis of effective soft transitivity constraints .   The previous cross - graph soft constraint and   paraphrase resolution soft constraint proposed in   Methods PRC ROC   BInc .038 .567   Hosseini18 .038 .564   Hosseini19 - Local .040 .579   EGT2 - Local .176 .654   EGT2- L .171 .696   Hosseini et al . ( 2018 ) have shown improvement   of performance based on their local graphs . How-   ever , due to the distinct distribution and scales of   local scores , their constraints are computationally   unavailable on our local graphs , partially due to   the high overhead for cross - graph calculation .   5.4 Does EGT2 learn directional entailment ?   Generally , the logical entailment should be direc-   tional which makes it different from paraphrase .   Although EGT2 significantly improves the per-   formance on two datasets , it is unclear whether   the improvement comes from the directional en-   tailment cases , or only paraphrasing ones , as the   local LM might be strong in recognizing para-   phrases but weak in recognizing directional entail-   ment ( Cabezudo et al . , 2020 ) . To examine how   EGT2 works under directional cases , we eliminate   those paraphrase predicate pairs a→bwith label   l∈ { True , False } from Levy / Holt test dataset ,   if the corresponding b→ais also appearing and   labelled as lin the test dataset . The rest direc-   tional section of Levy / Holt Dataset contains 8,140   examples ( 753 positive and 7,387 negative ) . We5906expect that this section should be more challenging   as undirectional paraphrase becomes unavailable .   We report the model performance on the direc-   tional section of Levy / Hold Dataset in Table 4 . We   can see that previous baselines do not perform well   on AUC of PRC , which indicate that it is diffi-   cult for them to reach precision > 0.5 . Mean-   while , EGT2 - Local and EGT2- Loutperform all   baselines on the directional section of Levy / Holt   Dataset . Unsurprisingly , all models ’ AUC scores   on the directional section become lower compared   on the original Levy / Holt Dataset , showing the   challenges of directional entailment inference . Two   EGT2 variants maintain high performance , which   proves that our local model can learn to capture   directional predicate entailment better than distri-   butional baselines , and the global soft constraint   also helps to make directional entailment inference .   5.5 Error Analysis   We randomly sample and analyze 100 false pos-   itive ( FP ) examples and 100 false negative ( FN )   examples from Levy / Holt test set according to pre-   dictions by EGT2- L. We manually setup the deci-   sion threshold as 0.574 to make the precision level   close to 0.76 , which is the same as Berant et al .   ( 2011 ) . The major error types are shown in Table 5 .   Although the global constraint is used , about half   of FN errors are due to the data sparsity where the   entailment relations are not found in the entailment   graph . When compared with the results in Hosseini   et al . ( 2018 ) , EGT2- Lreduces the ratio of Spar-   sityin FN errors from 93 % to 46 % with stronger   alleviation ability of data sparsity . About a quarter   of FN are caused by the Under - weighted Relations   in the graph , where EGT2 finds the entailment rela-   tions but gives them scores lower than the threshold .   The rest of FN are related to Dataset Wrong La-   belswhich happens when the predicates are indeed   entailed by others but labelled as negative , or the   predicate pairs are incomplete .   Most of FP errors are caused by the Spurious   Correlation as these relations are too fraudulent for   EGT2 to see through their spurious relationships   and consequently given high scores . A few FP   errors are caused by Lemma - based Processing in   LM inevitably , but the ratio still reduces from 12 %   in Hosseini et al . ( 2018 ) to 5 % . The result indicates   that our fine - tuned LM can handle the predicates   even with similar surface forms and contexts better   than parsing - based distributional local features .   Error Types Examples   False Negative   Sparsity ( 46%)Pain relieves by application   of Chloroform . →Chloro-   form reduces pain . ( 0.0 )   Under - weighted   Relations ( 23%)The Druids build the   Stonehenge . →The Druids   construct the Stonehenge .   ( 0.558 )   Dataset Wrong   Labels ( 31%)Salicylates reduces pain .   →Salicylates is given for   pain . ( 0.034 )   False Positive   Spurious Cor-   relation ( 68%)The cat sleeps on a fur . →   The cat has a fur . ( 0.683 )   Lemma - based   Process ( 5%)Lincoln comes to New   York.→Lincoln comes   from New York . ( 0.867 )   Dataset Wrong   Labels ( 27%)The lamps are made of   metal . →The lamps are   made of metal . ( 1.0 )   6 Conclusions   In this paper , we propose a novel typed entail-   ment graph learning framework , EGT2 , which uses   language models fine - tuned on textual entailment   tasks to calculate local entailment scores and ap-   plies soft transitivity constraints to learn global   entailment graphs in gradient - based method . The   transitivity constraints are achieved by carefully   designed loss functions , and effectively boost the   quality of local entailment graphs . By using the   fine - tuned local LM and global soft constraints ,   EGT2 does not rely on distributional features , and   can be easily applied to large - scale graphs . Ex-   periments on standard benchmark datasets show   that EGT2 achieves significantly better perfor-   mance than existing state - of - the - art entailment   graph methods .   Acknowledgements   This work is supported in part by National Key   R&D Program of China ( No . 2020AAA0106600 )   and NSFC ( 62161160339 ) . We would like to thank   the anonymous reviewers and action editors for   their helpful comments and suggestions.5907References59085909A Algorithm for Sentence Generator   Algorithm 1 The sentence generator S.B Additional Implementation Details   We select the SGD learning rate αfrom   { 0.02,0.05,0.1 } , the number of training   epochs from { 2,3,5,7 } , the coefficient λ   from{0.5,1,2 } , and the confidence threshold   ϵfrom{0.005,0.01,0.02 } . We manually tune   the hyper - parameters based on the AUC of PRC   on Levy / Holt validation dataset , which is .327   corresponding to our settings .   Under our experiment settings , one training   epoch costs about 4 hours on an NVIDIA A40   GPU.5910