  Dongyu Ru , Lin Qiu , Xipeng Qiu , Yue Zhang , Zheng ZhangAmazon AWS AISchool of Computer Science , Fudan UniversitySchool of Engineering , Westlake University   { rudongyu,quln,zhaz}@amazon.com   xpqiu@fudan.edu.cn   zhangyue@westlake.edu.cn   Abstract   Discourse analysis is an important task because   it models intrinsic semantic structures between   sentences in a document . Discourse markers   are natural representations of discourse in our   daily language . One challenge is that the mark-   ers as well as pre - defined and human - labeled   discourse relations can be ambiguous when   describing the semantics between sentences .   We believe that a better approach is to use   a contextual - dependent distribution over the   markers to express discourse information . In   this work , we propose to learn a Distributed   Marker Representation ( DMR ) by utilizing the   ( potentially ) unlimited discourse marker data   with a latent discourse sense , thereby bridg-   ing markers with sentence pairs . Such repre-   sentations can be learned automatically from   data without supervision , and in turn provide   insights into the data itself . Experiments show   the SOTA performance of our DMR on the im-   plicit discourse relation recognition task and   strong interpretability . Our method also offers   a valuable tool to understand complex ambigu-   ity and entanglement among discourse markers   and manually defined discourse relations .   1 Introduction   Discourse analysis is a fundamental problem in   natural language processing . It studies the linguis-   tic structures beyond the sentence boundary and   is a component of chains of thinking . Such struc-   tural information has been widely applied in many   downstream applications , including information   extraction ( Peng et al . , 2017 ) , long documents sum-   marization ( Cohan et al . , 2018 ) , document - level   machine translation ( Chen et al . , 2020 ) , conversa-   tional machine reading ( Gao et al . , 2020 ) .   Discourse relation recognition ( DRR ) focuses   on semantic relations , namely , discourse senses   between sentences or clauses . Such inter - sentence   structures are sometimes explicitly expressed in nat-   ural language by discourse connectives , or markersFigure 1 : Entangled discourse relations and correspond-   ing markers between clauses . As shown in the figure ,   there exist diverse discourse relations ( marked in blue )   and corresponding markers ( marked in red ) for the same   pair of clauses . It suggests that the semantic meaning   of different discourse relations can be entangled to each   other .   ( e.g. , and , but , or ) . The availability of these mark-   ers makes it easier to identify corresponding rela-   tions ( Pitler et al . , 2008 ) , as is in the task of explicit   discourse relation recognition ( EDRR ) , since there   is strong correlation between discourse markers   and relations . On the contrary , implicit discourse   relation recognition ( IDRR ) , where markers are   missing , remains a more challenging problem .   Prior work aims to address such challenges by   making use of discourse marker information over   explicit data in learning implicit discourse rela-   tions , either by injecting marker prediction knowl-   edge into a representation model ( Zhou et al . , 2010 ;   Braud and Denis , 2016 ) , or transferring the marker   prediction task into implicit discourse relation pre-   diction by manually defining a marker - relation   mapping ( Xiang et al . , 2022 ; Zhou et al . , 2022 ) . It   has been shown that discourse marker information   can effectively improve relation prediction results .   Nevertheless , relatively little work has investigated   various subtleties concerning the correlation be-   tween discourse markers and discourse relations ,   and their effect to IDRR in further detail .   To properly model discourse relations and mark-   ers , we need to consider that manually - defined dis-   course relations can be semantically entangled and5334markers are ambiguous . As shown in Fig . 1 , for   a pair of clauses , based on different emphasis on   semantics , we have different choices on discourse   relations and their corresponding markers . The   existence of multiple plausible discourse relations   indicates the entanglement between their semantic   meaning . Besides , discourse markers and relations   do not exclusively map to each other . As an ex-   ample , “ Ann went to the movies , and Bill went   home ” ( Temporal . Synchrony ) and “ Ann went to the   movies , andBill got upset ” ( Contingency . Cause )   both use the marker andbut express different mean-   ings . Identifying relations based on single markers   are difficult in certain scenarios because of such am-   biguity . Thus , a discrete and deterministic mapping   between discourse relations and markers can not   precisely express the correlations between them .   Based on the study of above issues , we propose   to use Distributed Marker Representation to en-   hance the informativeness of discourse expression .   Specifically , We use a probabilistic distribution on   markers or corresponding latent senses instead of   a single marker or relation to express discourse se-   mantics . We introduce a bottleneck in the latent   space , namely a discrete latent variable indicat-   ing discourse senses , to capture semantics between   clauses . The latent sense then produces a distribu-   tionof plausible markers to reflect its surface form .   This probabilistic model , which we call DMR , nat-   urally deals with ambiguities between markers and   entanglement among the relations . We show that   the latent space reveals a hierarchical marker - sense   clustering , and that entanglement among relations   are currently under - reported . Empirical results on   the IDRR benchmark Penn Discourse Tree Bank   2 ( PDTB2 ) ( Prasad et al . , 2008 ) shows the effec-   tiveness of our framework . We summarize our   contributions as follows :   •We propose a latent - space learning framework   for discourse relations and effectively optimize it   with cheap marker data .   •With the latent bottleneck and corresponding   probabilistic modeling , our framework achieves   the SOTA performance on implicit discourse rela-   tion recognition without a complicated architecture   design .   •We investigate the ambiguity of discourse mark-   ers and entanglement among discourse relations toexplain the plausibility of probabilistic modeling   of discourse relations and markers .   2 Related Work   Discourse analysis ( Brown et al . , 1983 ; Joty et al . ,   2019 ; McCarthy et al . , 2019 ) , targets the discourse   relation between adjacent sentences . It has at-   tracted attention beyond intra - sentence semantics .   It is formulated into two main tasks : explicit dis-   course relation recognition and implicit discourse   relation recognition , referring to the relation iden-   tification between a pair of sentences with mark-   ers explicitly included or not . While EDRR has   achieved satisfactory performance ( Pitler et al . ,   2008 ) with wide applications , IDRR remains to be   challenging ( Pitler et al . , 2009 ; Zhang et al . , 2015 ;   Rutherford et al . , 2017 ; Shi and Demberg , 2019 ) .   Our work builds upon the correlation between the   two critical elements in discourse analysis : dis-   course relations and markers .   Discourse markers have been used for not   only marker prediction training ( Malmi et al . ,   2018 ) , but also for improving the performance of   IDRR ( Marcu and Echihabi , 2002 ; Rutherford and   Xue , 2015 ) and representation learning ( Jernite   et al . , 2017 ) . Prior efforts on exploring markers   have found that training with discourse markers   can alleviate the difficulty on IDRC ( Sporleder and   Lascarides , 2008 ; Zhou et al . , 2010 ; Braud and   Denis , 2016 ) . Compared to their work , we focus   on a unified representation using distributed mark-   ers instead of relying on transferring from explicit   markers to implicit relations . Jernite et al . ( 2017 )   first extended the usage of markers to sentence rep-   resentation learning , followed by Nie et al . ( 2019 ) ;   Sileo et al . ( 2019 ) which introduced principled pre-   training frameworks and large - scale marker data .   Xiang et al . ( 2022 ) ; Zhou et al . ( 2022 ) explored   the possibility of connecting markers and relations   with prompts . In this work , we continue the line of   improving the expression of discourse information   as distributed markers .   3 Distributed Marker Representation   Learning   We elaborate on the probabilistic model in Sec . 3.1   and its implementation with neural networks in   Sec . 3.2 . We then describe the way we optimize   the model ( Sec . 3.3).5335   3.1 Probabilistic Formulation   We learn the distributed marker representation by   predicting markers given pairs of sentences . We   model the distribution of markers by introducing   an extra latent variable 𝒛which indicates the latent   senses between two sentences . We assume the   distribution of markers depends only on the latent   senses , and is independent of the original sentence   pairs when 𝒛is given , namely 𝒎⊥(𝑠,𝑠)|𝒛.   𝑝(𝒎|𝑠,𝑠)=∑︁𝑝(𝒛|𝑠,𝑠)·𝑝(𝒎|𝒛),(1 )   where the latent semantic senses 𝒛describes the un-   ambiguous semantic meaning of 𝑚in the specific   context , and our target is to model the probabilistic   distribution 𝑝(𝒎|𝑠,𝑠)with 𝒛. The probabilistic   model is depicted in Fig . 2 with an example .   The key inductive bias here is that we assume   the distribution of discourse markers is indepen-   dent of the original sentence pairs given the latent   semantic senses ( Eq . 1 ) . This formulation is based   on the intuition that humans decide the relationship   between two sentences in their cognitive worlds   first , then pick one proper expression with a map-   ping from latent senses to expressions ( which we   callz2mmapping in this paper ) without reconsid-   ering the semantic of sentences . Decoupling the   z2 m mapping from the distribution of discourse   marker prediction makes the model exhibit more   interpretability and transparency .   Therefore , the probabilistic distribution of   𝑝(𝒎|𝑠,𝑠)can be decomposed into 𝑝(𝑚|𝒛 )   and𝑝(𝒛|𝑠,𝑠)based on the independence as-   sumption above . 𝜓and𝜙denote parameters for   each part . The training objective with latent   senses included is to maximize the likelihood onlarge - scale corpus under this assumption :   L(𝜓,𝜙)=Elog𝑝(𝑚|𝑠,𝑠).(2 )   3.2 Neural Architecture   Our model begins by processing each sentence with   an encoder SentEnc :   ℎ=SentEnc([𝑠,[SEP],𝑠 ] ) , ( 3 )   whereℎ∈Rdenote the sentence pair represen-   tation in𝑑dimensions for 𝑠and𝑠.𝜓are pa-   rameters of the sentence encoder . The encoder   is instantiated as a pre - trained language model in   practice .   Then we use two linear layers to map the pair   representation ℎto the distribution of 𝒛as below :   ℎ=𝜓·ℎ+𝜓 , ( 4 )   𝑝(𝑧|𝑠,𝑠)=softmax(𝜓·ℎ+𝜓),(5 )   where𝜓∈R,𝜓∈R,𝜓∈   R,𝜓∈Rare trainable parameters . 𝐾is   the dimension of latent discourse senses .   The parameter 𝜓not only acts as the mapping   from representation ℎto𝒛 ’s distribution , but can   also be seen as an embedding lookup table for the   𝐾values of 𝒛. Each row in 𝜓is a representation   vector for the corresponding value , as an anchor in   the companion continuous space of 𝒛.   To parameterize the z2mmapping , the parameter   𝜙∈Ris defined as a probabilistic transition   matrix from latent semantic senses 𝑧to markers𝑚   ( in log space ) , where 𝑁is the number of candidate   markers :   log𝑝(𝑚|𝑧)=logsoftmax(𝜙 ) , ( 6 )   where𝜓=(𝜓,𝜓,𝜓,𝜓,𝜓),𝜙are the   learnable parameters for parameterize the distri-   bution𝑝(𝒎|𝑠,𝑠 ) .   3.3 Optimization   We optimize the parameters 𝜓and𝜙with the clas-   sic EM algorithm due to the existence of the latent   variable 𝒛. The latent variable 𝒛serves as a reg-   ularizer during model training . In the E - step of   each iteration , we obtain the posterior distribution   𝑝(𝑧|𝑠,𝑠,𝑚)according to the parameters in the   current iteration 𝜓,𝜙as shown in Eq . 7.5336Algorithm 1 EM Optimization for Discourse Marker Training with Latent SensesInitialize model parameters as 𝜓,𝜙.while not converge do ⊲𝑡-th iteration Sample a batch of examples for EM optimization . foreach example(𝑠,𝑠,𝑚)in the EM batch do Calculate and save the posterior 𝑝(𝒛|𝑠,𝑠,𝑚)according to 𝜓,𝜙. end for foreach example(𝑠,𝑠,𝑚)in the EM batch do Estimate E[log𝑝(𝑚,𝑧|𝑠,𝑠)]according to 𝜓,𝜙. ⊲E - step end for Update parameters 𝜓to𝜓in mini - batch with the gradient calculated as ∇L(𝜓,𝜙 ) . Update parameters 𝜙to𝜙according to the updated 𝜓and the gradient∇L(𝜓,𝜙 ) .   ⊲M - stepend while   Based on our assumption that 𝒎⊥(𝑠,𝑠)|𝒛 ,   we can get the posterior distribution :   𝑝(𝑧|𝑠,𝑠,𝑚)=𝑝(𝑚|𝑠,𝑠,𝑧)·𝑝(𝑧|𝑠,𝑠 )   𝑝(𝑚|𝑠,𝑠 )   = 𝑝(𝑚|𝑧)·𝑝(𝑧|𝑠,𝑠 )   𝑝(𝑚|𝑠,𝑠 )   ∝𝑝(𝑧|𝑠,𝑠)·𝑝(𝑚|𝑧).(7 )   In M - step , we optimize the parameters 𝜓,𝜙 by   maximizing the expectation of joint log likelihood   on estimated posterior 𝑝(𝑧|𝑠,𝑠,𝑚 ) . The updated   parameters𝜓,𝜙for the next iteration can   be obtained as in Eq . 8 .   𝜓,𝜙= ( 8)   arg maxE[log𝑝(𝑚,𝑧|𝑠,𝑠 ) ] .   In practice , the alternative EM optimization can   be costly and unstable due to the expensive ex-   pectation computation and the subtlety on hyper-   parameters when optimizing 𝜓and𝜙jointly . We   alleviate the training difficulty by empirically esti-   mating the expectation on mini - batch and separate   the optimization of 𝜓and𝜙. We formulate the loss   functions as below , for separate gradient descent   optimization of 𝜓and𝜙 :   where𝜙means the value of 𝜙before the𝑡-th   iteration and 𝜓means the value of 𝜓after the   𝑡-th iteration of optimization . KLDiv denotes the   Kullback - Leibler divergence . The overall optimiza-   tion algorithm is summarized in Algorithm 1.4 Experiments   DMR adopts a latent bottleneck for the space of   latent discourse senses . We first prove the effec-   tiveness of the latent variable and compare against   current SOTA solutions on the IDRR task . We then   examine what the latent bottleneck learned during   training and how it addresses the ambiguity and   entanglement of discourse markers and relations .   4.1 Dataset   We use two datasets for learning our DMR model   and evaluating its strength on downstream implicit   discourse relation recognition , respectively . See   Appendix A for statistics of the datasets .   Discovery Dataset ( Sileo et al . , 2019 ) is a large-   scale discourse marker dataset extracted from com-   moncrawl web data , the Depcc corpus ( Panchenko   et al . , 2018 ) . It contains 1.74 million sentence   pairs with a total of 174 types of explicit discourse   markers between them . Markers are automatically   extracted based on part - of - speech tagging . We use   top - k accuracy ACC@k to evaluate the marker pre-   diction performance on this dataaset . Note that we   use explicit markers to train DMR but evaluate it   on IDRR thanks to different degrees of verbosity   when using markers in everyday language .   Penn Discourse Tree Bank 2.0 ( PDTB2 )   ( Prasad et al . , 2008 ) is a popular discourse analysis   benchmark with manually - annotated discourse re-   lations and markers on Wall Street Journal articles .   We perform the evaluation on its implicit part with   11 major second - level relations included . We fol-   low ( Ji and Eisenstein , 2015 ) for data split , which   is widely used in recent studies for IDRR . Macro-5337   FandACC are metrics for IDRR performance .   We note that although annotators are allowed to   annotate multiple senses ( relations ) , only 2.3 %   of the data have more than one relation . There-   fore whether DMR can capture more entanglement   among relations is of interest as well ( Sec . 4.5 ) .   4.2 Baselines   We compare our DMR model with competitive   baseline approaches to validate the effectiveness   of DMR . For the IDRR task , we compare DMR-   based classifier with current SOTA methods , in-   cluding BMGF ( Liu et al . , 2021 ) , which combines   representation , matching , and fusion ; LDSGM ( Wu   et al . , 2022 ) , which considers the hierarchical de-   pendency among labels ; the prompt - based connec-   tive prediction method , PCP ( Zhou et al . , 2022 ) and   so on . For further analysis on DMR , we also in-   clude a vanilla sentence encoder without the latent   bottleneck as an extra baseline , denoted as BASE .   4.3 Implementation Details   Our DMR model is trained on 1.57 million ex-   amples with 174 types of markers in Discovery   dataset . We use pretrained RoBERTa model ( Liu   et al . , 2019 ) as SentEnc in DMR . We set the   default latent dimension 𝐾to 30 . More details re-   garding the implementation of DMR can be found   in Appendix A.   For the IDRR task , we strip the marker genera-   tion part from the DMR model and use the hidden   stateℎas the pair representation . BASE uses   the[CLS ] token representation as the representa-   tion of input pairs . A linear classification layer is   stacked on top of models to predict relations .   4.4 Implicit Discourse Relation Recognition   We first validate the effectiveness of modeling la-   tent senses on the challenging IDRR task .   Main Results DMR demonstrates comparable   performance with current SOTAs on IDRR , but   with a simpler architecture . As shown in Table 1 ,   DMR leads in terms of accuracy by 2.7pt and is a   close second in macro - F.   The results exhibit the strength of DMR by more   straightforwardly modeling the correlation between   discourse markers and relations . Despite the ab-   sence of supervision on discourse relations during   DMR learning , the semantics of latent senses dis-   tilled by EM optimization successfully transferred   to manually - defined relations in IDRR.5338   Based on the comparison to DMR without la-   tentz , we observe a significant performance drop   resulted from the missing latent bottleneck . It indi-   cates that the latent bottleneck in DMR serves as a   regularizer to avoid overfitting on similar markers .   Fine - grained Performance We list the fine-   granined performance of DMR and compare it   with SOTA approaches on second - level senses of   PDTB2 . As shown in Table 2 , DMR achieves sig-   nificant improvements on relations with little super-   vision , like Expa . List andTemp . Async . The perfor-   mance of majority classes , e.g. Expa . Conjunction ,   are slightly worse . It may be caused by the entan-   glement between Expa . Conjunction andExpa . List   to be discussed in Sec . 4.5 . In summary , DMR   achieves better overall performance by maintain-   ing equilibrium among entangled relations with   different strength of supervision .   Few - shot Analysis Fig . 3 shows DMR achieves   significant gains against BASE in few - shot learning   experiments . The results are averaged on 3 inde-   pendent runs for each setting . In fact , with only   ∼60 % of annotated data , DMR achieves the same   performance as BASE with full data by utilizing   the cheap marker data more effectively .   To understand the ceiling of the family of   such BERT - based pretrained model with mark-   ers as an extra input , we augment the data in   two ways : BASEinserts the groundtruth marker ,   andBASEwhere the markers are predicted by   a modelofficially released by Discovery ( Sileo   et al . , 2019 ) . Table 3 presents the results where the   informative markers are inserted to improve the per-   formance of BASE , following the observations and   ideas from ( Zhou et al . , 2010 ; Pitler et al . , 2008 ) .   DMR continues to enjoy the lead , even when the   markers are groundtruth ( i.e. BASE ) , suggest-   ing DMR ’s hidden state contains more information   than single markers .   4.5 Analysis & Discussion   Marker Prediction The performance of DMR   on marker prediction is sensitive to the capacity of   the bottleneck . When setting 𝐾to be the number   of markers ( 174 ) , it matches and even outperforms   the Discovery model which directly predicts the   markers on the same data ( Table 4 ) . A smaller 𝐾   sacrifices marker prediction performance but it can   cluster related senses , resulting in more informative   and interpretable representation .   Multiple markers may share similar meanings   when connecting sentences . Thus , evaluating the   performance of marker prediction simply on top1   accuracy is inappropriate . In Table 4 , we demon-   strated the results on ACC@k and observed that5339   DMR(K=174 ) gets better performance against the   model optimized by an MLE objective when k gets   larger . We assume that it comes from the marker   ambiguity . Our DMR models the ambiguity better ,   thus with any of the plausible markers easier to be   observed in a larger range of predictions but more   difficult as top1 . To prove the marker ambiguity   more directly , we randomly sample 50 examples to   analyze their top5 predictions . The statistics show   that over 80 % of those predictions have plausible   explanations . To conclude , considerable examples   have multiple plausible markers thus ACC@k with   larger k can better reflect the true performance on   marker prediction , where DMR can beat the MLE-   optimized model .   z2 m Mapping The latent space is not inter-   pretable , but DMR has a transition matrix that out-   puts a distribution of markers , which reveals what   a particular dimension may encode .   To analyze the latent space , we use 𝜓(Eq . 5 )   as the corresponding embedding vectors and per-   form T - SNE visualization of the latent 𝒛 , similar to   what Discover ( Sileo et al . , 2019 ) does using the   softmax weight at the final prediction layer . The   complete T - SNE result can be found in Appendix B.   What we observe is an emerging hierarchical pat-   tern , in addition to proximity . That is , while syn-   onymous markers are clustered as expected , seman-   tically related clusters are often closer . Fig . 4b   shows the top left corner of the T - SNE result . We   can see that the temporal connectives and senses   are located in the top left corner . According to   their coupled markers , we can recover the semantic   of these latent 𝒛 : preceding ( 𝑧 ) , succeeding ( 𝑧 ,   𝑧,𝑧 ) and synchronous ( 𝑧 ) form nearby but   separated clusters .   For a comparison with connective - based prompt-   ing approaches , we also demonstrate the T - SNE   visualization of marker representations from BASE   in Fig . 4a . Unlike semantically aligned vector   space of DMR , locality of markers in the space   of BASE representation is determined by surface   form of markers and shifted from their exact mean-   ing . Marker representations of the model w/o latent   𝑧are closer because of similar lexical formats in-   stead of underlying discourse .   From z2 m mapping , we can take a step further   to analyze the correlation between markers learned   by DMR . Table 5 shows the top 2 corresponding   clusters of three randomly sampled markers . We   can observe correlations between markers like pol-   ysemy and synonym .   Understanding Entanglement Labeling dis-   course relations is challenging since some of them   can correlate , and discern the subtleties can be chal-   lenging . For example , Liststrongly correlates with   Conjunction and the two are hardly distinguishable .   DMR is trained to predict a distribution of mark-   ers , thus we expect its hidden state to capture the   distribution of relations as well even when the   multi - sense labels are scarce . We drew 100 random   samples and ask two researchers to check whether   each of the corresponding top-3 predictions is valid5340   and give a binary justification . Fig . 5a shows that a   considerable amount of 64 % examples have two or   more relations evaluated as reasonable in top-3 pre-   dictions , much higher than 2.3 % multi - sense labels   in PDTB2 . This suggests that one way to improve   upon the lack of multi - sense annotation is to use   DMR to provide candidates for the annotators . For   these samples , we also inspect annotator agreement   in PDTB2 ( Fig . 5b ) . While the trend is consistent   with what DMR reports , it also validates again that   the PTDB2 annotators under - labeled multi - senses .   To gain a deeper understanding of relation cor-   relation , we rank the sentence pairs according to   the entropy of relation prediction , a higher entropy   suggests more model uncertainty , namely moreconfusion .   We use the top-3 predictions of the 20 highest   entropy examples to demonstrate highly confus-   ing discourse relations as shown in Fig . 6 . The   accumulated joint probability of paired relations   on these examples is computed as weights in the   confusion matrix . The statistics meet our expecta-   tion that there exist specific patterns of confusion .   For example , asynchronous relations are correlated   with causal relations , while another type of tempo-   ral relations , synchronous ones are correlated with   conjunction . A complete list of these high entropy   examples is listed in Appendix C.   To further prove DMR can learn diverse distri-   bution even when multi - sense labels are scarce , we   also evaluate our model on the DiscoGeM ( Schol-   man et al . , 2022 ) , where each instance is annotated   by 10 crowd workers . The distribution discrep-   ancy is evaluated with cross entropy . Our model ,   trained solely on majority labels , achieved a cross   entropy score of 1.81 against all labels . Notably ,   our model outperforms the BMGF model ( 1.86 )   under the same conditions and comes close to the   performance of the BMGF model trained on multi-   ple labels ( 1.79 ) ( Yung et al . , 2022 ) . These results   highlight the strength of our model in capturing   multiple senses within the data .   To conclude , while we believe explicit relation   labeling is still useful , it is incomplete without also   specifying a distribution . As such , DMR ’s ℎor the   distribution of markers are legitimate alternatives   to model inter - sentence discourse.5341Case Study on Specific Examples As a comple-   tion of the previous discussion on understanding   entanglement in a macro perspective , we present a   few examples in PDTB2 with markers and relations   predicted by the DMR - based model . As demon-   strated in Table 6 , the identification of discourse   relations relies on different emphasis of seman-   tic pairs . Taking the first case as an example , the   connection between “ two or three favorities ” and   “ Ragu spaghetti sauce ” indicates the Instantiation   relation while the connection between complete   semantics of these two sentences results in Cause .   Thanks to the probabilistic modeling of discourse   information in DMR , the cases demonstrate entan-   glement among relations and ambiguity of markers   well .   5 Conclusion   In this paper , we propose the distributed marker   representation for modeling discourse based on the   strong correlation between discourse markers and   relations . We design the probabilistic model by in-   troducing a latent variable for discourse senses . We   use the EM algorithm to effectively optimize the   framework . The study on our well - trained DMR   model shows that the latent - included model can   offer a meaningful semantic view of markers . Such   semantic view significantly improves the perfor-   mance of implicit discourse relation recognition .   Further analysis of our model provides a better   understanding of discourse relations and markers ,   especially the ambiguity and entanglement issues .   Limitation & Risks   In this paper , we bridge the gap between discourse   markers and the underlying relations . We use dis-   tributed discourse markers to express discourse   more informatively . However , learning DMR re-   quires large - scale data on markers . Although it ’s   potentially unlimited in corpus , the distribution   and types of markers may affect the performance   of DMR . Besides , the current solution proposed in   this paper is limited to relations between adjacent   sentences .   Our model can be potentially used for natural   language commonsense inference and has the po-   tential to be a component for large - scale common-   sense acquisition in a new form . Potential risks   include a possible bias on collected commonsense   due to the data it relies on , which may be alleviated   by introducing a voting - based selection mechanismon large - scale data .   References534253435344   A Implementation Details   We use Huggingface transformers ( 4.2.1 ) for the   use of PLM backbones in our experiments . For   optimization , we optimize the overall framework   according to Algorithm 1 . We train the model on   Discovery for 3 epochs with the learning rate for   𝜓set to 3e-5 and the learning rate for 𝜙set to 1e-   2 . The EM batchsize is set to 500 according to   the trade - off between optimization efficiency and   performance . The optimization requires around   40 hrs to converge in a Tesla - V100 GPU . For the   experiments on PDTB2 , we use them according to   the LDC license for research purposes on discourse   relation classification . The corresponding statistics   of the two datasets are listed in Table 7 and Table 8 .   B Visualization of the latent 𝒛   To obtain an intrinsic view of how well the con-   nections between markers 𝒎and𝒛can be learned   in our DMR model . We draw a T - SNE 2 - d visual-   ization of 𝒛 ’s representations in Fig . 7 with top-3   connectives of each 𝑧attached nearby . The repre-   sentation vector for each 𝑧is extracted from 𝜓.   The results are interesting that we can observe not   only the clustering of similar connectives as 𝑧 , but   also semantically related 𝑧closely located in the   representation space . C High Entropy Examples from Human   Evaluation   For analysis of the entanglement among relations ,   we did a human evaluation on randomly extracted   examples from PDTB2 . To better understand the   entanglement among relations , we further filter the   20 most confusing examples with entropy as a met-   ric . The entanglement is shown as Fig.6 in Sec . 4.5 .   We list these examples in Table 9 for clarity.53455346534753485349ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   After Section 5   /squareA2 . Did you discuss any potential risks of your work ?   After Section 5   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix A   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 4 , Appendix A.   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4 , Appendix A.   C / squareDid you run computational experiments ?   Section 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A.5350 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4 , Appendix A   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4 , Appendix A.   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix A.   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.5351