  Dongyu Ru , Lin Qiu , Xipeng Qiu , Yue Zhang , Zheng ZhangAmazon AWS AISchool of Computer Science , Fudan UniversitySchool of Engineering , Westlake University   { rudongyu,quln,zhaz}@amazon.com   xpqiu@fudan.edu.cn   zhangyue@westlake.edu.cn   Abstract   Discourse analysis is an important task because   it models intrinsic semantic structures between   sentences in a document . Discourse markers   are natural representations of discourse in our   daily language . One challenge is that the mark-   ers as well as pre - defined and human - labeled   discourse relations can be ambiguous when   describing the semantics between sentences .   We believe that a better approach is to use   a contextual - dependent distribution over the   markers to express discourse information . In   this work , we propose to learn a Distributed   Marker Representation ( DMR ) by utilizing the   ( potentially ) unlimited discourse marker data   with a latent discourse sense , thereby bridg-   ing markers with sentence pairs . Such repre-   sentations can be learned automatically from   data without supervision , and in turn provide   insights into the data itself . Experiments show   the SOTA performance of our DMR on the im-   plicit discourse relation recognition task and   strong interpretability . Our method also offers   a valuable tool to understand complex ambigu-   ity and entanglement among discourse markers   and manually defined discourse relations .   1 Introduction   Discourse analysis is a fundamental problem in   natural language processing . It studies the linguis-   tic structures beyond the sentence boundary and   is a component of chains of thinking . Such struc-   tural information has been widely applied in many   downstream applications , including information   extraction ( Peng et al . , 2017 ) , long documents sum-   marization ( Cohan et al . , 2018 ) , document - level   machine translation ( Chen et al . , 2020 ) , conversa-   tional machine reading ( Gao et al . , 2020 ) .   Discourse relation recognition ( DRR ) focuses   on semantic relations , namely , discourse senses   between sentences or clauses . Such inter - sentence   structures are sometimes explicitly expressed in nat-   ural language by discourse connectives , or markersFigure 1 : Entangled discourse relations and correspond-   ing markers between clauses . As shown in the figure ,   there exist diverse discourse relations ( marked in blue )   and corresponding markers ( marked in red ) for the same   pair of clauses . It suggests that the semantic meaning   of different discourse relations can be entangled to each   other .   ( e.g. , and , but , or ) . The availability of these mark-   ers makes it easier to identify corresponding rela-   tions ( Pitler et al . , 2008 ) , as is in the task of explicit   discourse relation recognition ( EDRR ) , since there   is strong correlation between discourse markers   and relations . On the contrary , implicit discourse   relation recognition ( IDRR ) , where markers are   missing , remains a more challenging problem .   Prior work aims to address such challenges by   making use of discourse marker information over   explicit data in learning implicit discourse rela-   tions , either by injecting marker prediction knowl-   edge into a representation model ( Zhou et al . , 2010 ;   Braud and Denis , 2016 ) , or transferring the marker   prediction task into implicit discourse relation pre-   diction by manually defining a marker - relation   mapping ( Xiang et al . , 2022 ; Zhou et al . , 2022 ) . It   has been shown that discourse marker information   can effectively improve relation prediction results .   Nevertheless , relatively little work has investigated   various subtleties concerning the correlation be-   tween discourse markers and discourse relations ,   and their effect to IDRR in further detail .   To properly model discourse relations and mark-   ers , we need to consider that manually - defined dis-   course relations can be semantically entangled and5334markers are ambiguous . As shown in Fig . 1 , for   a pair of clauses , based on different emphasis on   semantics , we have different choices on discourse   relations and their corresponding markers . The   existence of multiple plausible discourse relations   indicates the entanglement between their semantic   meaning . Besides , discourse markers and relations   do not exclusively map to each other . As an ex-   ample , â€œ Ann went to the movies , and Bill went   home â€ ( Temporal . Synchrony ) and â€œ Ann went to the   movies , andBill got upset â€ ( Contingency . Cause )   both use the marker andbut express different mean-   ings . Identifying relations based on single markers   are difficult in certain scenarios because of such am-   biguity . Thus , a discrete and deterministic mapping   between discourse relations and markers can not   precisely express the correlations between them .   Based on the study of above issues , we propose   to use Distributed Marker Representation to en-   hance the informativeness of discourse expression .   Specifically , We use a probabilistic distribution on   markers or corresponding latent senses instead of   a single marker or relation to express discourse se-   mantics . We introduce a bottleneck in the latent   space , namely a discrete latent variable indicat-   ing discourse senses , to capture semantics between   clauses . The latent sense then produces a distribu-   tionof plausible markers to reflect its surface form .   This probabilistic model , which we call DMR , nat-   urally deals with ambiguities between markers and   entanglement among the relations . We show that   the latent space reveals a hierarchical marker - sense   clustering , and that entanglement among relations   are currently under - reported . Empirical results on   the IDRR benchmark Penn Discourse Tree Bank   2 ( PDTB2 ) ( Prasad et al . , 2008 ) shows the effec-   tiveness of our framework . We summarize our   contributions as follows :   â€¢We propose a latent - space learning framework   for discourse relations and effectively optimize it   with cheap marker data .   â€¢With the latent bottleneck and corresponding   probabilistic modeling , our framework achieves   the SOTA performance on implicit discourse rela-   tion recognition without a complicated architecture   design .   â€¢We investigate the ambiguity of discourse mark-   ers and entanglement among discourse relations toexplain the plausibility of probabilistic modeling   of discourse relations and markers .   2 Related Work   Discourse analysis ( Brown et al . , 1983 ; Joty et al . ,   2019 ; McCarthy et al . , 2019 ) , targets the discourse   relation between adjacent sentences . It has at-   tracted attention beyond intra - sentence semantics .   It is formulated into two main tasks : explicit dis-   course relation recognition and implicit discourse   relation recognition , referring to the relation iden-   tification between a pair of sentences with mark-   ers explicitly included or not . While EDRR has   achieved satisfactory performance ( Pitler et al . ,   2008 ) with wide applications , IDRR remains to be   challenging ( Pitler et al . , 2009 ; Zhang et al . , 2015 ;   Rutherford et al . , 2017 ; Shi and Demberg , 2019 ) .   Our work builds upon the correlation between the   two critical elements in discourse analysis : dis-   course relations and markers .   Discourse markers have been used for not   only marker prediction training ( Malmi et al . ,   2018 ) , but also for improving the performance of   IDRR ( Marcu and Echihabi , 2002 ; Rutherford and   Xue , 2015 ) and representation learning ( Jernite   et al . , 2017 ) . Prior efforts on exploring markers   have found that training with discourse markers   can alleviate the difficulty on IDRC ( Sporleder and   Lascarides , 2008 ; Zhou et al . , 2010 ; Braud and   Denis , 2016 ) . Compared to their work , we focus   on a unified representation using distributed mark-   ers instead of relying on transferring from explicit   markers to implicit relations . Jernite et al . ( 2017 )   first extended the usage of markers to sentence rep-   resentation learning , followed by Nie et al . ( 2019 ) ;   Sileo et al . ( 2019 ) which introduced principled pre-   training frameworks and large - scale marker data .   Xiang et al . ( 2022 ) ; Zhou et al . ( 2022 ) explored   the possibility of connecting markers and relations   with prompts . In this work , we continue the line of   improving the expression of discourse information   as distributed markers .   3 Distributed Marker Representation   Learning   We elaborate on the probabilistic model in Sec . 3.1   and its implementation with neural networks in   Sec . 3.2 . We then describe the way we optimize   the model ( Sec . 3.3).5335   3.1 Probabilistic Formulation   We learn the distributed marker representation by   predicting markers given pairs of sentences . We   model the distribution of markers by introducing   an extra latent variable ğ’›which indicates the latent   senses between two sentences . We assume the   distribution of markers depends only on the latent   senses , and is independent of the original sentence   pairs when ğ’›is given , namely ğ’âŠ¥(ğ‘ ,ğ‘ )|ğ’›.   ğ‘(ğ’|ğ‘ ,ğ‘ )=âˆ‘ï¸ğ‘(ğ’›|ğ‘ ,ğ‘ )Â·ğ‘(ğ’|ğ’›),(1 )   where the latent semantic senses ğ’›describes the un-   ambiguous semantic meaning of ğ‘šin the specific   context , and our target is to model the probabilistic   distribution ğ‘(ğ’|ğ‘ ,ğ‘ )with ğ’›. The probabilistic   model is depicted in Fig . 2 with an example .   The key inductive bias here is that we assume   the distribution of discourse markers is indepen-   dent of the original sentence pairs given the latent   semantic senses ( Eq . 1 ) . This formulation is based   on the intuition that humans decide the relationship   between two sentences in their cognitive worlds   first , then pick one proper expression with a map-   ping from latent senses to expressions ( which we   callz2mmapping in this paper ) without reconsid-   ering the semantic of sentences . Decoupling the   z2 m mapping from the distribution of discourse   marker prediction makes the model exhibit more   interpretability and transparency .   Therefore , the probabilistic distribution of   ğ‘(ğ’|ğ‘ ,ğ‘ )can be decomposed into ğ‘(ğ‘š|ğ’› )   andğ‘(ğ’›|ğ‘ ,ğ‘ )based on the independence as-   sumption above . ğœ“andğœ™denote parameters for   each part . The training objective with latent   senses included is to maximize the likelihood onlarge - scale corpus under this assumption :   L(ğœ“,ğœ™)=Elogğ‘(ğ‘š|ğ‘ ,ğ‘ ).(2 )   3.2 Neural Architecture   Our model begins by processing each sentence with   an encoder SentEnc :   â„=SentEnc([ğ‘ ,[SEP],ğ‘  ] ) , ( 3 )   whereâ„âˆˆRdenote the sentence pair represen-   tation inğ‘‘dimensions for ğ‘ andğ‘ .ğœ“are pa-   rameters of the sentence encoder . The encoder   is instantiated as a pre - trained language model in   practice .   Then we use two linear layers to map the pair   representation â„to the distribution of ğ’›as below :   â„=ğœ“Â·â„+ğœ“ , ( 4 )   ğ‘(ğ‘§|ğ‘ ,ğ‘ )=softmax(ğœ“Â·â„+ğœ“),(5 )   whereğœ“âˆˆR,ğœ“âˆˆR,ğœ“âˆˆ   R,ğœ“âˆˆRare trainable parameters . ğ¾is   the dimension of latent discourse senses .   The parameter ğœ“not only acts as the mapping   from representation â„toğ’› â€™s distribution , but can   also be seen as an embedding lookup table for the   ğ¾values of ğ’›. Each row in ğœ“is a representation   vector for the corresponding value , as an anchor in   the companion continuous space of ğ’›.   To parameterize the z2mmapping , the parameter   ğœ™âˆˆRis defined as a probabilistic transition   matrix from latent semantic senses ğ‘§to markersğ‘š   ( in log space ) , where ğ‘is the number of candidate   markers :   logğ‘(ğ‘š|ğ‘§)=logsoftmax(ğœ™ ) , ( 6 )   whereğœ“=(ğœ“,ğœ“,ğœ“,ğœ“,ğœ“),ğœ™are the   learnable parameters for parameterize the distri-   butionğ‘(ğ’|ğ‘ ,ğ‘  ) .   3.3 Optimization   We optimize the parameters ğœ“andğœ™with the clas-   sic EM algorithm due to the existence of the latent   variable ğ’›. The latent variable ğ’›serves as a reg-   ularizer during model training . In the E - step of   each iteration , we obtain the posterior distribution   ğ‘(ğ‘§|ğ‘ ,ğ‘ ,ğ‘š)according to the parameters in the   current iteration ğœ“,ğœ™as shown in Eq . 7.5336Algorithm 1 EM Optimization for Discourse Marker Training with Latent SensesInitialize model parameters as ğœ“,ğœ™.while not converge do âŠ²ğ‘¡-th iteration Sample a batch of examples for EM optimization . foreach example(ğ‘ ,ğ‘ ,ğ‘š)in the EM batch do Calculate and save the posterior ğ‘(ğ’›|ğ‘ ,ğ‘ ,ğ‘š)according to ğœ“,ğœ™. end for foreach example(ğ‘ ,ğ‘ ,ğ‘š)in the EM batch do Estimate E[logğ‘(ğ‘š,ğ‘§|ğ‘ ,ğ‘ )]according to ğœ“,ğœ™. âŠ²E - step end for Update parameters ğœ“toğœ“in mini - batch with the gradient calculated as âˆ‡L(ğœ“,ğœ™ ) . Update parameters ğœ™toğœ™according to the updated ğœ“and the gradientâˆ‡L(ğœ“,ğœ™ ) .   âŠ²M - stepend while   Based on our assumption that ğ’âŠ¥(ğ‘ ,ğ‘ )|ğ’› ,   we can get the posterior distribution :   ğ‘(ğ‘§|ğ‘ ,ğ‘ ,ğ‘š)=ğ‘(ğ‘š|ğ‘ ,ğ‘ ,ğ‘§)Â·ğ‘(ğ‘§|ğ‘ ,ğ‘  )   ğ‘(ğ‘š|ğ‘ ,ğ‘  )   = ğ‘(ğ‘š|ğ‘§)Â·ğ‘(ğ‘§|ğ‘ ,ğ‘  )   ğ‘(ğ‘š|ğ‘ ,ğ‘  )   âˆğ‘(ğ‘§|ğ‘ ,ğ‘ )Â·ğ‘(ğ‘š|ğ‘§).(7 )   In M - step , we optimize the parameters ğœ“,ğœ™ by   maximizing the expectation of joint log likelihood   on estimated posterior ğ‘(ğ‘§|ğ‘ ,ğ‘ ,ğ‘š ) . The updated   parametersğœ“,ğœ™for the next iteration can   be obtained as in Eq . 8 .   ğœ“,ğœ™= ( 8)   arg maxE[logğ‘(ğ‘š,ğ‘§|ğ‘ ,ğ‘  ) ] .   In practice , the alternative EM optimization can   be costly and unstable due to the expensive ex-   pectation computation and the subtlety on hyper-   parameters when optimizing ğœ“andğœ™jointly . We   alleviate the training difficulty by empirically esti-   mating the expectation on mini - batch and separate   the optimization of ğœ“andğœ™. We formulate the loss   functions as below , for separate gradient descent   optimization of ğœ“andğœ™ :   whereğœ™means the value of ğœ™before theğ‘¡-th   iteration and ğœ“means the value of ğœ“after the   ğ‘¡-th iteration of optimization . KLDiv denotes the   Kullback - Leibler divergence . The overall optimiza-   tion algorithm is summarized in Algorithm 1.4 Experiments   DMR adopts a latent bottleneck for the space of   latent discourse senses . We first prove the effec-   tiveness of the latent variable and compare against   current SOTA solutions on the IDRR task . We then   examine what the latent bottleneck learned during   training and how it addresses the ambiguity and   entanglement of discourse markers and relations .   4.1 Dataset   We use two datasets for learning our DMR model   and evaluating its strength on downstream implicit   discourse relation recognition , respectively . See   Appendix A for statistics of the datasets .   Discovery Dataset ( Sileo et al . , 2019 ) is a large-   scale discourse marker dataset extracted from com-   moncrawl web data , the Depcc corpus ( Panchenko   et al . , 2018 ) . It contains 1.74 million sentence   pairs with a total of 174 types of explicit discourse   markers between them . Markers are automatically   extracted based on part - of - speech tagging . We use   top - k accuracy ACC@k to evaluate the marker pre-   diction performance on this dataaset . Note that we   use explicit markers to train DMR but evaluate it   on IDRR thanks to different degrees of verbosity   when using markers in everyday language .   Penn Discourse Tree Bank 2.0 ( PDTB2 )   ( Prasad et al . , 2008 ) is a popular discourse analysis   benchmark with manually - annotated discourse re-   lations and markers on Wall Street Journal articles .   We perform the evaluation on its implicit part with   11 major second - level relations included . We fol-   low ( Ji and Eisenstein , 2015 ) for data split , which   is widely used in recent studies for IDRR . Macro-5337   FandACC are metrics for IDRR performance .   We note that although annotators are allowed to   annotate multiple senses ( relations ) , only 2.3 %   of the data have more than one relation . There-   fore whether DMR can capture more entanglement   among relations is of interest as well ( Sec . 4.5 ) .   4.2 Baselines   We compare our DMR model with competitive   baseline approaches to validate the effectiveness   of DMR . For the IDRR task , we compare DMR-   based classifier with current SOTA methods , in-   cluding BMGF ( Liu et al . , 2021 ) , which combines   representation , matching , and fusion ; LDSGM ( Wu   et al . , 2022 ) , which considers the hierarchical de-   pendency among labels ; the prompt - based connec-   tive prediction method , PCP ( Zhou et al . , 2022 ) and   so on . For further analysis on DMR , we also in-   clude a vanilla sentence encoder without the latent   bottleneck as an extra baseline , denoted as BASE .   4.3 Implementation Details   Our DMR model is trained on 1.57 million ex-   amples with 174 types of markers in Discovery   dataset . We use pretrained RoBERTa model ( Liu   et al . , 2019 ) as SentEnc in DMR . We set the   default latent dimension ğ¾to 30 . More details re-   garding the implementation of DMR can be found   in Appendix A.   For the IDRR task , we strip the marker genera-   tion part from the DMR model and use the hidden   stateâ„as the pair representation . BASE uses   the[CLS ] token representation as the representa-   tion of input pairs . A linear classification layer is   stacked on top of models to predict relations .   4.4 Implicit Discourse Relation Recognition   We first validate the effectiveness of modeling la-   tent senses on the challenging IDRR task .   Main Results DMR demonstrates comparable   performance with current SOTAs on IDRR , but   with a simpler architecture . As shown in Table 1 ,   DMR leads in terms of accuracy by 2.7pt and is a   close second in macro - F.   The results exhibit the strength of DMR by more   straightforwardly modeling the correlation between   discourse markers and relations . Despite the ab-   sence of supervision on discourse relations during   DMR learning , the semantics of latent senses dis-   tilled by EM optimization successfully transferred   to manually - defined relations in IDRR.5338   Based on the comparison to DMR without la-   tentz , we observe a significant performance drop   resulted from the missing latent bottleneck . It indi-   cates that the latent bottleneck in DMR serves as a   regularizer to avoid overfitting on similar markers .   Fine - grained Performance We list the fine-   granined performance of DMR and compare it   with SOTA approaches on second - level senses of   PDTB2 . As shown in Table 2 , DMR achieves sig-   nificant improvements on relations with little super-   vision , like Expa . List andTemp . Async . The perfor-   mance of majority classes , e.g. Expa . Conjunction ,   are slightly worse . It may be caused by the entan-   glement between Expa . Conjunction andExpa . List   to be discussed in Sec . 4.5 . In summary , DMR   achieves better overall performance by maintain-   ing equilibrium among entangled relations with   different strength of supervision .   Few - shot Analysis Fig . 3 shows DMR achieves   significant gains against BASE in few - shot learning   experiments . The results are averaged on 3 inde-   pendent runs for each setting . In fact , with only   âˆ¼60 % of annotated data , DMR achieves the same   performance as BASE with full data by utilizing   the cheap marker data more effectively .   To understand the ceiling of the family of   such BERT - based pretrained model with mark-   ers as an extra input , we augment the data in   two ways : BASEinserts the groundtruth marker ,   andBASEwhere the markers are predicted by   a modelofficially released by Discovery ( Sileo   et al . , 2019 ) . Table 3 presents the results where the   informative markers are inserted to improve the per-   formance of BASE , following the observations and   ideas from ( Zhou et al . , 2010 ; Pitler et al . , 2008 ) .   DMR continues to enjoy the lead , even when the   markers are groundtruth ( i.e. BASE ) , suggest-   ing DMR â€™s hidden state contains more information   than single markers .   4.5 Analysis & Discussion   Marker Prediction The performance of DMR   on marker prediction is sensitive to the capacity of   the bottleneck . When setting ğ¾to be the number   of markers ( 174 ) , it matches and even outperforms   the Discovery model which directly predicts the   markers on the same data ( Table 4 ) . A smaller ğ¾   sacrifices marker prediction performance but it can   cluster related senses , resulting in more informative   and interpretable representation .   Multiple markers may share similar meanings   when connecting sentences . Thus , evaluating the   performance of marker prediction simply on top1   accuracy is inappropriate . In Table 4 , we demon-   strated the results on ACC@k and observed that5339   DMR(K=174 ) gets better performance against the   model optimized by an MLE objective when k gets   larger . We assume that it comes from the marker   ambiguity . Our DMR models the ambiguity better ,   thus with any of the plausible markers easier to be   observed in a larger range of predictions but more   difficult as top1 . To prove the marker ambiguity   more directly , we randomly sample 50 examples to   analyze their top5 predictions . The statistics show   that over 80 % of those predictions have plausible   explanations . To conclude , considerable examples   have multiple plausible markers thus ACC@k with   larger k can better reflect the true performance on   marker prediction , where DMR can beat the MLE-   optimized model .   z2 m Mapping The latent space is not inter-   pretable , but DMR has a transition matrix that out-   puts a distribution of markers , which reveals what   a particular dimension may encode .   To analyze the latent space , we use ğœ“(Eq . 5 )   as the corresponding embedding vectors and per-   form T - SNE visualization of the latent ğ’› , similar to   what Discover ( Sileo et al . , 2019 ) does using the   softmax weight at the final prediction layer . The   complete T - SNE result can be found in Appendix B.   What we observe is an emerging hierarchical pat-   tern , in addition to proximity . That is , while syn-   onymous markers are clustered as expected , seman-   tically related clusters are often closer . Fig . 4b   shows the top left corner of the T - SNE result . We   can see that the temporal connectives and senses   are located in the top left corner . According to   their coupled markers , we can recover the semantic   of these latent ğ’› : preceding ( ğ‘§ ) , succeeding ( ğ‘§ ,   ğ‘§,ğ‘§ ) and synchronous ( ğ‘§ ) form nearby but   separated clusters .   For a comparison with connective - based prompt-   ing approaches , we also demonstrate the T - SNE   visualization of marker representations from BASE   in Fig . 4a . Unlike semantically aligned vector   space of DMR , locality of markers in the space   of BASE representation is determined by surface   form of markers and shifted from their exact mean-   ing . Marker representations of the model w/o latent   ğ‘§are closer because of similar lexical formats in-   stead of underlying discourse .   From z2 m mapping , we can take a step further   to analyze the correlation between markers learned   by DMR . Table 5 shows the top 2 corresponding   clusters of three randomly sampled markers . We   can observe correlations between markers like pol-   ysemy and synonym .   Understanding Entanglement Labeling dis-   course relations is challenging since some of them   can correlate , and discern the subtleties can be chal-   lenging . For example , Liststrongly correlates with   Conjunction and the two are hardly distinguishable .   DMR is trained to predict a distribution of mark-   ers , thus we expect its hidden state to capture the   distribution of relations as well even when the   multi - sense labels are scarce . We drew 100 random   samples and ask two researchers to check whether   each of the corresponding top-3 predictions is valid5340   and give a binary justification . Fig . 5a shows that a   considerable amount of 64 % examples have two or   more relations evaluated as reasonable in top-3 pre-   dictions , much higher than 2.3 % multi - sense labels   in PDTB2 . This suggests that one way to improve   upon the lack of multi - sense annotation is to use   DMR to provide candidates for the annotators . For   these samples , we also inspect annotator agreement   in PDTB2 ( Fig . 5b ) . While the trend is consistent   with what DMR reports , it also validates again that   the PTDB2 annotators under - labeled multi - senses .   To gain a deeper understanding of relation cor-   relation , we rank the sentence pairs according to   the entropy of relation prediction , a higher entropy   suggests more model uncertainty , namely moreconfusion .   We use the top-3 predictions of the 20 highest   entropy examples to demonstrate highly confus-   ing discourse relations as shown in Fig . 6 . The   accumulated joint probability of paired relations   on these examples is computed as weights in the   confusion matrix . The statistics meet our expecta-   tion that there exist specific patterns of confusion .   For example , asynchronous relations are correlated   with causal relations , while another type of tempo-   ral relations , synchronous ones are correlated with   conjunction . A complete list of these high entropy   examples is listed in Appendix C.   To further prove DMR can learn diverse distri-   bution even when multi - sense labels are scarce , we   also evaluate our model on the DiscoGeM ( Schol-   man et al . , 2022 ) , where each instance is annotated   by 10 crowd workers . The distribution discrep-   ancy is evaluated with cross entropy . Our model ,   trained solely on majority labels , achieved a cross   entropy score of 1.81 against all labels . Notably ,   our model outperforms the BMGF model ( 1.86 )   under the same conditions and comes close to the   performance of the BMGF model trained on multi-   ple labels ( 1.79 ) ( Yung et al . , 2022 ) . These results   highlight the strength of our model in capturing   multiple senses within the data .   To conclude , while we believe explicit relation   labeling is still useful , it is incomplete without also   specifying a distribution . As such , DMR â€™s â„or the   distribution of markers are legitimate alternatives   to model inter - sentence discourse.5341Case Study on Specific Examples As a comple-   tion of the previous discussion on understanding   entanglement in a macro perspective , we present a   few examples in PDTB2 with markers and relations   predicted by the DMR - based model . As demon-   strated in Table 6 , the identification of discourse   relations relies on different emphasis of seman-   tic pairs . Taking the first case as an example , the   connection between â€œ two or three favorities â€ and   â€œ Ragu spaghetti sauce â€ indicates the Instantiation   relation while the connection between complete   semantics of these two sentences results in Cause .   Thanks to the probabilistic modeling of discourse   information in DMR , the cases demonstrate entan-   glement among relations and ambiguity of markers   well .   5 Conclusion   In this paper , we propose the distributed marker   representation for modeling discourse based on the   strong correlation between discourse markers and   relations . We design the probabilistic model by in-   troducing a latent variable for discourse senses . We   use the EM algorithm to effectively optimize the   framework . The study on our well - trained DMR   model shows that the latent - included model can   offer a meaningful semantic view of markers . Such   semantic view significantly improves the perfor-   mance of implicit discourse relation recognition .   Further analysis of our model provides a better   understanding of discourse relations and markers ,   especially the ambiguity and entanglement issues .   Limitation & Risks   In this paper , we bridge the gap between discourse   markers and the underlying relations . We use dis-   tributed discourse markers to express discourse   more informatively . However , learning DMR re-   quires large - scale data on markers . Although it â€™s   potentially unlimited in corpus , the distribution   and types of markers may affect the performance   of DMR . Besides , the current solution proposed in   this paper is limited to relations between adjacent   sentences .   Our model can be potentially used for natural   language commonsense inference and has the po-   tential to be a component for large - scale common-   sense acquisition in a new form . Potential risks   include a possible bias on collected commonsense   due to the data it relies on , which may be alleviated   by introducing a voting - based selection mechanismon large - scale data .   References534253435344   A Implementation Details   We use Huggingface transformers ( 4.2.1 ) for the   use of PLM backbones in our experiments . For   optimization , we optimize the overall framework   according to Algorithm 1 . We train the model on   Discovery for 3 epochs with the learning rate for   ğœ“set to 3e-5 and the learning rate for ğœ™set to 1e-   2 . The EM batchsize is set to 500 according to   the trade - off between optimization efficiency and   performance . The optimization requires around   40 hrs to converge in a Tesla - V100 GPU . For the   experiments on PDTB2 , we use them according to   the LDC license for research purposes on discourse   relation classification . The corresponding statistics   of the two datasets are listed in Table 7 and Table 8 .   B Visualization of the latent ğ’›   To obtain an intrinsic view of how well the con-   nections between markers ğ’andğ’›can be learned   in our DMR model . We draw a T - SNE 2 - d visual-   ization of ğ’› â€™s representations in Fig . 7 with top-3   connectives of each ğ‘§attached nearby . The repre-   sentation vector for each ğ‘§is extracted from ğœ“.   The results are interesting that we can observe not   only the clustering of similar connectives as ğ‘§ , but   also semantically related ğ‘§closely located in the   representation space . C High Entropy Examples from Human   Evaluation   For analysis of the entanglement among relations ,   we did a human evaluation on randomly extracted   examples from PDTB2 . To better understand the   entanglement among relations , we further filter the   20 most confusing examples with entropy as a met-   ric . The entanglement is shown as Fig.6 in Sec . 4.5 .   We list these examples in Table 9 for clarity.53455346534753485349ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   After Section 5   /squareA2 . Did you discuss any potential risks of your work ?   After Section 5   /squareA3 . Do the abstract and introduction summarize the paper â€™s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiï¬c artifacts ?   Section 4   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix A   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciï¬ed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 4 , Appendix A.   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiï¬es individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiï¬cant , while on small test sets they may not be .   Section 4 , Appendix A.   C / squareDid you run computational experiments ?   Section 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A.5350 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4 , Appendix A   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4 , Appendix A.   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix A.   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants â€™ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you â€™re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.5351