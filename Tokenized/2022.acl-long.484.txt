  Matthias HagenMaik FröbeArtur JurkMartin PotthastMartin - Luther - Universität Halle - WittenbergLeipzig University   Abstract   We introduce and study the task of clickbait   spoiling : generating a short text that satisﬁes   the curiosity induced by a clickbait post . Click-   bait links to a web page and advertises its con-   tents by arousing curiosity instead of provid-   ing an informative summary . Our contribu-   tions are approaches to classify the type of   spoiler needed ( i.e. , a phrase or a passage ) , and   to generate appropriate spoilers . A large - scale   evaluation and error analysis on a new corpus   of 5,000 manually spoiled clickbait posts —   the Webis Clickbait Spoiling Corpus 2022 —   shows that our spoiler type classiﬁer achieves   an accuracy of 80 % , while the question an-   swering model DeBERTa - large outperforms   all others in generating spoilers for both types .   1 Introduction   Clickbait is the term used to describe posts in social   media that are intended to inappropriately entice   their readers to visit a web page . This is achieved   through formulations such as sensationalism or cat-   aphors that are believed to create a so - called cu-   riosity gap : “ a form of cognitively induced depri-   vation that arises from the perception of a gap in   knowledge or understanding ” ( Loewenstein , 1994 ) .   Clickbait is perceived as inappropriate since its res-   olution is usually ordinary or trivial , comprising   little more than a phrase , short passage , or a list of   things that could just as easily have been included   in the post . This observation motivates us to intro-   duce the task of clickbait spoiling : identifying or   generating a spoiler for a clickbait post .   Figure 1 shows four examples of clickbait on   Twitter , along with spoilers . The ﬁrst two tweets   explicitly or implicitly promise a surprising resolu-   tion to spark curiosity , but their spoilers are brief   and trivial . The linked page of the ﬁrst tweet adds   almost nothing , and the spoiler of the second is   common sense . The third spoiler is a passage from   the linked page , and the fourth is a list of things . Figure 1 : Examples of clickbait tweets and spoilers for   them extracted from the respective linked web page .   Even though there are length limits to the infor-   mativeness of tweets , the spoilers in all examples   could easily have been part of the original tweets .   This paper reports about our investigation into   clickbait spoiling and the following contributions :   ( 1 ) The Webis Clickbait Spoiling Corpus 2022   ( Webis - Clickbait-22 ) , consisting of 5,000 clickbait   posts , their linked pages and a spoiling piece of   text therein.(2 ) A two - step approach to clickbait   spoiling that ﬁrst classiﬁes a clickbait post accord-   ing to its spoiler type ( phrase or passage ) , and then   treats spoiling either as a question answering or as a   passage retrieval task . ( 3 ) A systematic evaluation   of state - of - the - art methods for spoiler type classiﬁ-   cation , question answering , and passage retrieval .   Although the ﬁrst step of spoiler type classiﬁcation   is not necessary , our results suggest that it can be   helpful . Even more so , as we have not yet tack-   led multipart spoilers ( bottom example in Figure 1 ;   876 cases also part of our corpus ) that probably   require a different spoiling approach.70252 Related Work   Following an overview of research on clickbait and   its operationalization so far , models of question   answering and passage retrieval are examined .   2.1 Clickbait and its Operationalization   The underlying assumption of most research on   clickbait is that it is a form of data - driven opti-   mization of social media posts to exploit the cu-   riosity gap described by Loewenstein ( 1994 ) . At   least that ’s what Peter Koechley ( 2012 ) , the CEO   of Upworthy , claimed . Upworthy became one   of the ﬁrst major spreaders of clickbait on Face-   book , and their success has prompted Facebook   to change its news recommendation algorithms to   curb the amount of clickbait , twice ( El - Arini and   Tang , 2014 ; Peysakhovich and Hendrix , 2016 ) .   Exploratory and theoretical studies of clickbait   and its impact on journalism analyzed its preva-   lence for more than 150 publishers ( Rony et al . ,   2017 ) ; its economics for the news market ( Munger ,   2020 ) ; its impact on perceptions of credibility and   quality ( overall negative ) ( Molyneux and Codding-   ton , 2020 ) ; and noted a slow decline over the past   decade ( Lischka and Garz , 2021 ) .   Journalistic studies of this kind rely on click-   bait detection technologies . Originally proposed   by Rubin et al . ( 2015 ) but not followed up , Potthast   et al . ( 2016 ) and Chakraborty et al . ( 2016 ) indepen-   dently developed the ﬁrst detectors . Starting from   a shared task organized by Potthast et al . ( 2018 )   shortly after , more than 50 approaches have been   contributed to date . An overview is beyond the   scope of our work , but transformer models domi-   nate this task as well . For the clickbait generation   task , preceded by a rule - based generator ( Eidnes ,   2015 ) , only Shu et al . ( 2018 ) and Xu et al . ( 2019 )   have presented more advanced models , while Karn   et al . ( 2019 ) generate teaser headlines that are ex-   plicitly not meant to be clickbait . So far , no attempt   has been made to generate spoilers for clickbait .   2.2 Question Answering   If one considers clickbait spoiling as a question   answering problem , there are numerous possible   solutions . Among the available question - answering   benchmarks ( Dzendzik et al . , 2021 ) , we select two   to choose appropriate state - of - the - art models for   our evaluation : ( 1 ) SQuAD ( Rajpurkar et al . , 2016 )   compiles 107,785 questions and answers based on   536 Wikipedia articles . Although a wide range ofquestions and answers are included , the vast major-   ity of 93.6 % are factual ( 32 % names , 31.8 % noun   phrases , 19.8 % numbers , 5.5 % verb phrases , and   3.9 % adjective phrases ) , while the remainder are   descriptive ( 3.7 % clauses and 2.7 % other ) . We   use SQuAD v1.1 , not the v2.0 superset ( Rajpurkar   et al . , 2018 ) , which contains unanswerable ques-   tions , since we do not expect clickbait to be “ un-   spoilable ” . ( 2 ) TriviaQA ( Joshi et al . , 2017 ) con-   tains 95,000 question – answer pairs , mostly dealing   with trivia questions that are supposed to be partic-   ularly difﬁcult to answer . These are comparable to   clickbait in that many of them address rather trivial   things ( see Figure 1 ) .   The question answering models used in our ex-   periments are ALBERT ( Lan et al . , 2020 ) , AllenAI-   Document - QA ( Clark and Gardner , 2018 ) , BERT   ( cased / uncased ) ( Devlin et al . , 2019 ) , Big Bird ( Za-   heer et al . , 2020 ) , DeBERTa ( large ) ( He et al . ,   2021 ) , ELECTRA ( Clark et al . , 2020 ) , Funnel-   Transformer ( Dai et al . , 2020 ) , MPNet ( Song et al . ,   2020 ) , and RoBERTa ( base / large ) ( Liu et al . , 2019 ) .   Many of them are or were state of the art on the   above benchmarks and implement various different   architectural paradigms .   2.3 Passage Retrieval   Passage retrieval relaxes the question answering   task a bit in the sense of allowing longer passages   of text as answers ( e.g. , one or more sentences ) ,   rather than exact phrases or statements . Neural re-   trieval models , as surveyed by Guo et al . ( 2020 ) and   Lin et al . ( 2021 ) , have been successfully applied to   passage retrieval . One of the most important pas-   sage retrieval benchmarks is part of MS MARCO , a   series of challenges whose ﬁrst edition was a large   question answering task ( Nguyen et al . , 2016 ) . A   passage retrieval dataset of 8.8 million passages   was derived for the underlying set of 100,000 ques-   tions originally submitted to Bing . This dataset   formed the basis for two consecutive shared tasks   at the TREC 2019 and 2020 Deep Learning tracks   ( Craswell et al . , 2019 , 2020 ) .   The passage retrieval models used in our experi-   ments are MonoBERT ( Nogueira and Cho , 2019 ;   Nogueira et al . , 2019 ) and MonoT5 ( Nogueira et al . ,   2020 ) ( both topped the MS MARCO passage re-   trieval leaderboard once ) , and the classic baseline   models BM25 ( Robertson and Zaragoza , 2009 ) and   Query Likelihood ( Ponte and Croft , 1998 ) , imple-   mented in Anserini ( Yang et al . , 2017).70263 Webis Clickbait Spoiling Corpus 2022   To tackle clickbait spoiling for the ﬁrst time , we   created the Webis Clickbait Spoiling Corpus 2022   ( Webis - Clickbait-22 ) , a collection of 5,000 click-   bait posts and their associated spoilers .   3.1 Corpus Construction   Our corpus is primarily based on ﬁve social me-   dia accounts on Twitter , Reddit , and Facebook   that manually spoil clickbait : r / savedyouaclick ,   @HuffPoSpoilers , @SavedYouAClick , @Upwor-   thySpoiler , and @StopClickBaitOfﬁcial . With the   goal of collecting 5,000 “ spoilable ” clickbait posts   at an expected rejection rate of around 10 % of un-   usable posts , 5,555 were initially collected from   the accounts . Each of them was manually reviewed ,   and those that turned out not to be spoiled clickbait   were removed ( e.g. , funny posts not intended to   be spoilers , or posts with unavailable linked doc-   uments ) . The rejection rate was higher than ex-   pected , and only 4,204 posts remained .   To reach our goal of 5,000 posts , we then sam-   pled from the Webis - Clickbait-17 corpus used in   the Clickbait Challenge 2017 ( Potthast et al . , 2018 ) .   The corpus contains 38,517 tweets , each of which   was rated by 5 annotators on a 4 - point Likert scale   for clickbaitiness : “ no clickbait , ” “ slight clickbait , ”   “ considerable clickbait , ” and “ heavy clickbait . ” Of   the tweets , 1,845 scored an average of 0.8 or higher   and can safely be considered clickbait . We selected   tweets from this subset and manually spoiled them   based on the linked document until our target size   of 5,000 posts was reached .   Thus , our ﬁnal corpus consists of 4,204 posts   from Twitter , Reddit , and Facebook that were   spoiled by a third party specializing in this task ,   and 796 tweets from the Webis - Clickbait-17 corpus   with an average clickbaitiness of at least 0.8 that we   spoiled ourselves . For each of the 5,000 clickbait   posts , we also reviewed and corrected erroneous   spoilers and labeled their exact positions in the   linked documents . Our internal guidelines dictated   that a spoiler should be as short as possible ( i.e. , if   one word is enough , not a whole sentence should   be chosen ) . Since the underlying annotation task   is simple , one main annotator was sufﬁcient . Nev-   ertheless , randomly selected as well as ambiguous   cases were discussed with two additional experts   among the co - authors . No systematic errors or un-   foreseen difﬁculties in solving the annotation task   were identiﬁed during these discussions . During our annotation , we found that none of   the common approaches to main content extraction   worked reliably for all the documents linked in the   clickbait posts . Yet , clean content is a prerequi-   site for research on clickbait spoiling to eliminate   as many confounding variables as possible . To   ensure a clean corpus , one annotator manually ex-   tracted the main content of the linked documents ,   removing ( inline ) advertisements , links to related   articles ( e.g. , “ READ ALSO : [ . . . ] ” or “ Also from   CNBC [ . . . ] ” ) , credits ( e.g. , “ Image credit : [ . . . ] ”   or “ Photo by [ . . . ] ” ) , and social media links ( e.g. ,   “ Subscribe to [ . . . ] ” or “ Follow us on [ . . . ] ” ) . A ran-   dom selection was reviewed to ensure high quality .   Moreover , during spoiler annotation , it turned   out that there are basically three types of spoilers :   ( 1)phrase spoilers consisting of a single word or   phrase from the linked document ( e.g. , the ﬁrst two   spoilers in Figure 1 , but often named entity spoilers   as well ) , ( 2 ) passage spoilers consisting of one or   a few sentences of the linked document ( e.g. , the   third spoiler in Figure 1 ) , and ( 3 ) multipart spoil-   ersconsisting of more than one non - consecutive   phrases or passages of the linked document ( e.g. ,   the fourth spoiler in Figure 1 ) . Spoiler types were   also annotated by the main annotator , and randomly   checked by the other two .   In sum , each of the 5,000 posts in our corpus   consists of a unique ID , the platform from which   it was taken , the respective platform ’s post ID , the   post ’s text ( i.e. , the “ clickbait ” ) , the URL to the   linked document , the manually extracted title and   paragraph - divided main content of the linked docu-   ment , the manually optimized spoiler , the spoiler ’s   character position in the main content , and the type   of spoiler ( phrase , passage , or multipart ) . In total ,   the annotation took about 560 hours , which marked   the limit of our budget dedicated for this step .   3.2 Corpus Statistics   Table 1 summarizes the main statistics of our   corpus . Most spoiled clickbait posts come from   Twitter ( 47.5 % ) and Reddit ( 36 % ) , whereas the   Facebook account contributes less ( 16.5 % ) . Most   spoilers are phrases ( 42.5 % ) and passages ( 40 % ) .   That there are fewer multi - part spoilers could   be due to the fact that spoiler account oper-   ators prefer to spoil “ simpler ” clickbait posts .   For the corpus , we also provide a ﬁxed random   80/20/20 train / validation / test split to ensure future   reproducibility and comparability with our results.7027   4 Type - dependent Clickbait Spoiling   Our approach to clickbait spoiling is based on   the observation that there are three types of spoil-   ers : ( 1 ) phrase spoilers , ( 2 ) passage spoilers , and   ( 3 ) multipart spoilers . We assume that different   tailored approaches will work best for each spoiler   type . However , an important prerequisite for this is   the corresponding classiﬁcation of clickbait . There-   fore , we ﬁrst investigate how well the spoiler type   of a clickbait post can be predicted ( Section 4.1 ) .   The generation of phrase and passage spoilers   for a given clickbait post is similar in that the so-   lution to the problem in both cases amounts to   extracting a coherent piece of text from the linked   document . To this end , there are a variety of exist-   ing approaches in related disciplines whose output   is either a phrase or a passage , and which may be   adapted to clickbait spoiling . We therefore inves-   tigate whether phrase spoilers can be identiﬁed by   conventional question answering methods ( i.e. , we   treat a clickbait post as a “ question ” to which a   phrase of the linked document should be returned   as the “ answer ” ; Section 4.2 ) , and whether passage   spoilers can be identiﬁed by conventional passage   retrieval methods ( i.e. , we treat a clickbait post as a   “ query ” and the paragraphs of the linked document   as the collection from which to retrieve the best   “ passage ” ; Section 4.3 ) . In our evaluation , we focus   on phrase and passage spoilers and also examine   the abilities of the above question answering and   passage retrieval methods to serve as one - size-ﬁts-   all solutions for phrases and passages . For mul-   tipart spoilers , a novel approach will be needed ,   which is beyond the scope of our current work but   an interesting direction for the future.4.1 Spoiler Type Classiﬁcation   For the spoiler classiﬁcation subtask , we experi-   mented with classic feature - based models ( Naïve   Bayes , Logistic Regression , SVM ) and the neural   models BERT- , DeBERTa- , and RoBERTa .   As feature types for the classic models , we use   tf- and tfidf - weighted word and POS tag uni-   and bigrams from the clickbait post and tfidf-   weighted word and POS tag uni- and bigrams from   the linked document . We include features from the   linked document , since it has to be analyzed for   the spoiler generation anyway . The idfvalues are   calculated on the OpenWebText corpus ( Gokaslan   and Cohen , 2019 ) to prevent any bias from the   comparatively small size of our corpus .   The input for the neural models is a post concate-   nated with the main content of the linked document .   4.2 Phrase Spoiler Generation   Viewing a clickbait post for which a phrase spoiler   should be derived as a “ question ” and the linked   document as potentially containing an “ answer ” ,   phrase spoiler generation can be tackled by ques-   tion answering methods . We therefore employ   ten state - of - the - art question answering methods   trained on the SQuAD data and ﬁne - tune them   on our new clickbait spoiling training set : AL-   BERT , BERT ( cased / uncased ) , BigBird , DeBERTa   ( large ) , ELECTRA , FunnelTransformer , MPNet ,   and RoBERTa ( base / large ) .   4.3 Passage Spoiler Generation   Treating the clickbait post whose spoiler type is   a passage as a “ query ” for which the “ most rele-   vant ” passage from the linked document is to be7028   retrieved , passage spoiler generation can be tackled   by passage retrieval methods . We therefore use ten   state - of - the - art passage retrieval approaches trained   on the MS MARCO data : BM25 and QLD in four   variants each ( alone or with RM3 / Ax / PRF query   expansion ) , MonoBERT , and MonoT5 . In addition ,   we also adapt all of the above question answering   models to retrieve passages by simply considering   the passage as the returned result from which the   question answering model extracts its answer .   5   In our evaluation , we assume a setup in which a   previous clickbait detection would have ( perfectly )   identiﬁed posts as clickbait . To then evaluate the   effectiveness of spoiler type classiﬁcation on such   detected clickbait posts , we conduct three experi-   ments : ( 1 ) multi - class , ( 2 ) one - vs - rest , and ( 3 ) one-   vs - one for the types of phrase and passage spoilers .   In all cases , the hyperparameters of the six stud-   ied classiﬁers were optimized based on the vali-   dation set of our corpus . For the three feature-   based approaches , a chi - square feature selection   step selected all post - based features and 70 % of the   document - based features . The post - based features   are weighted 4 - times higher than the document-   based features . Most hyperparameters of the trans-   former models were left at their default values , but   a grid search was used to ﬁnd the most effective   combination of learning rate ( 1e-5 , 4e-5 , 1e-4 ) ,   warm - up ratio ( 0.02 , 0.06 , and 0.1 ) , stack size ( 8 ,   16 , and 32 ) , number of epochs ( 1 to 10 ) , and maxi-   mum sequence length ( 256 , 384 , 512 ) .   Table 2 shows the balanced accuracy of the six   classiﬁers . All are less effective in the multi - class   setting than in the one - vs - rest settings and the   transformer - based classiﬁers are clearly more effec-   tive than the feature - based ones ; DeBERTa is best   in the multi - class setting ( accuracy of 73.63 ) and   RoBERTa in the one - vs - rest ones ( 79.12 to 80.39 ) .   Table 3 shows the accuracy of the six classiﬁers   on the 826 test posts with phrase and passage spoil-   ers ( almost balanced setup , since there is hardly   any class imbalance ) . Again , the transformer-   based classiﬁers clearly are more effective than   the feature - based ones ; with RoBERTa achieving   the best accuracy of 80.39 .   The substantial improvements of DeBERTa and   RoBERTa over the feature - based classiﬁers in all   settings ( about 9–10 accuracy points ) indicates that   classifying the clickbait spoiler type requires more   advanced language “ understanding ” than what is   encoded in the basic features that the Naïve Bayes ,   SVM , or logistic regression classiﬁers used .   6 Evaluation of Spoiler Generation   To assess the effectiveness of the question answer-   ing and passage retrieval methods for clickbait   spoiling , we evaluate both for their respective in-   tended spoiler types , but each also for the respective   other spoiler type . Multipart spoilers are deferred   to future work . We continue to assume that prior   clickbait detection ( perfectly ) identiﬁes clickbait   posts as such . Our evaluation of the generated   spoilers includes quantitative and qualitative as-   sessments ( Section 6.1 ) . In a pilot study with ten   question answering and ten passage retrieval mod-   els at their default settings , two models in each cat-   egory dominate the respective others ( Section 6.2 ) .   The computationally expensive step of hyperparam-   eter optimization is restricted to these four models   plus two baselines ( Section 6.3 ) . Then , the effec-   tiveness of spoiling clickbait posts dependent on   spoiler type is evaluated ( Sections 6.4 and 6.5 ) , and   compared to an end - to - end clickbait spoiling setup   independent of spoiler type ( section 6.6).70296.1 Quantitative and Qualitative Assessment   We introduce the measures used to evaluate gener-   ated spoilers and how we manually determined   thresholds for them above which a generated   spoiler is considered as “ correct ” .   Evaluation measures . To assess the quantitative   correspondence between a derived spoiler and the   ground truth , we use three question answering-   oriented and one passage retrieval - oriented mea-   sure : BLEU-4 ( Papineni et al . , 2002 ) , METEOR   ( Banerjee and Lavie , 2005 ) in its extended ver-   sion of Denkowski and Lavie ( 2014 ) , BERTScore   ( Zhang et al . , 2020 ) , and Precision@1 .   The three question answering - oriented measures   each calculate a ( penalized ) harmonic mean of   measure - speciﬁc deﬁnitions of precision and re-   call when comparing a generated spoiler to the   ground truth . In case of BLEU-4 , the overlap of   word 1- to 4 - grams is determined ( if the length n   of a generated spoiler is less than 4 words , we com-   pute BLEU- n ) , in case of METEOR the overlap of   word 1 - grams , and in case of BERTScore the best   matching embeddings of word pairs . Note that in   their original formulation , BLEU-4 and METEOR   penalize the score , the more the n - gram order dif-   fers . To arrange the measures on a spectrum from   calculating predominantly syntactic ( BLEU-4 ) to   predominantly semantic similarity ( BERTScore ) ,   we omit METEOR ’s penalization term .   The question answering - oriented measures are   not really suited to assess the effectiveness of pas-   sage retrieval models since a retrieved passage is   often longer than the ground truth spoiler . There-   fore , we also use Precision@1 to measure whether   the top - ranked passage contains the ground truth   spoiler ( all phrase spoilers and 98 % of the passage   spoilers come from a single passage ; for the other   passage spoilers , we consider all containing pas-   sages as relevant ) . To calculate the Precision@1   of question answering models , we use the ﬁrst pas-   sage that contains the returned spoiler .   High - conﬁdence thresholds . Candidates with   higher scores on the question answering - oriented   measures BLEU-4 , METEOR , and BERTScore are   closer to the ground truth . However , it is unclear   what score threshold a particular spoiler candidate   has to exceed so that it would be considered a true   positive in a manual analysis . Determining such   thresholds enables “ high conﬁdence ” estimations   of how many correct spoilers an approach gener-   ates without having to manually check its outputs   each time with each new variant .   In a pilot study , we thus determined such thresh-   olds by running all question answering models   ( cf . Section 4.2 and 4.3 ) on a random sample   of 500 clickbait posts with phrase spoilers and   500 with passage spoilers . For each post , a ran-   dom spoiler generated by a question answering   model and a random spoiler generated by a passage   retrieval model were manually checked for whether   they could be viewed as correct . Table 4 shows the   number of manually determined false positives and   false negatives for different thresholds of BLEU-4 ,   METEOR , and BERTScore . The manually selected   subjective thresholds ( FP / FN in bold ) for each com-   bination of measure , spoiler type , and model type   ( question answering or passage retrieval ) minimize   the false positives at a rate where being more strict   would incur too many false negatives . For instance ,   for phrase spoilers and BLEU-4 , we set the ques-   tion answering model threshold at 50 % since a   more strict threshold of 60 % does not reduce the   false positives but increases the false negatives .   In addition to reporting quantitative mean effec-   tiveness scores , applying the determined thresholds   helps to estimate how many of the spoilers of a7030   model would be perceived as “ good ” by human   readers . This corresponds to a conservative assess-   ment , since we believe that a model should only be   deployed to production if it has been tuned to not   return a spoiler if in doubt about its correctness ;   also probably somewhat minimizing the otherwise   possible spread of auto - generated misinformation .   6.2 Pilot Study for Model Selection   In a pilot study on 1,000 clickbait posts ( 800 train-   ing , 200 validation ) , we compare ten question an-   swering and ten passage retrieval models ( cf . Ta-   ble 5 ) at their default settings to select models for   subsequent experiments with more extensive ( and   expensive ) hyperparameter tuning . The question   answering models were or are among the most   effective in the SQuAD and TriviaQA question an-   swering benchmarks . In our setup , they return a   piece of text from the linked document as an “ an-   swer ” to the clickbait post as the “ query ” . As pas-   sage retrieval models , we empoly MonoBERT and   MonoT5 using their PyGaggleimplementations ,   and eight variants of the popular baseline retrieval   models BM25 and QLD using their Anserini imple-   mentations ( Yang et al . , 2017 ) . These models re-   turn the most “ relevant ” paragraph from the linked   document for the clickbait post as the “ query” . Using Nvidia A100 GPUs , the question answer-   ing models were ﬁrst ﬁne - tuned on SQuAD v1.1   and then on the pilot training data . This was   the most effective setup from an ablation study   with other ﬁne - tuning regimes ( e.g. , the phrase   spoiler BERTScore for RoBERTa - large dropped   from 84.04 to 69.91 when only ﬁne - tuned on our   pilot study data , to 64.61 when only ﬁne - tuned on   SQuAD , and to 46.60 without ﬁne - tuning ) . Interest-   ingly , the models ’ SQuAD effectiveness does not   predict their spoiling effectiveness ( e.g. , RoBERTa-   base and FunnelTransformer were tied on SQuAD ,   but RoBERTa - base is more effective at spoiling ) .   This indicates the importance of the pilot study .   Table 5 shows the pilot study effectiveness of   all models on the 200 validation posts . RoBERTa-   large ( for phrasal spoilers ) and DeBERTa - large ( for   passage spoilers ) are the most effective . Among the   passage retrieval models , MonoBERT and MonoT5   achieve the best scores . Contrary to our original   assumption that passage retrieval models might be   particularly well - suited to identify passage spoil-   ers , MonoBERT and MonoT5 have similar Preci-   sion@1 scores on both phrase and passage spoilers   and are substantially less effective than the best   question answering models ( e.g. , DeBERTa - large   has a Precision@1 of 48.39 for passage spoilers   compared to 31.18 for MonoBERT).7031   6.3 Tuning the Selected Models   Given the pilot study results , six models are se-   lected for a more extensive hyperparameter tuning :   the best two question answering models ( DeBERTa-   large was best for phrase spoilers , RoBERTa - large   for passage spoilers ) plus BERT as baseline , as   well as the best two passage retrieval models   ( MonoBERT and MonoT5 ) plus BM25 as baseline .   As the ablation study in our pilot study showed   that ﬁne - tuning the question answering models   on SQuAD ﬁrst and then on our corpus works   best , we apply this ﬁne - tuning regime to DeBERTa-   large , RoBERTa - large , and BERT using the click-   bait spoiling training data ( depending on the ex-   periment , either only the phrase spoilers , only the   passage spoilers , or both combined ) . Most hy-   perparameters of DeBERTa - large , RoBERTa - large ,   BERT , MonoBERT , and MonoT5 are left at their   defaults , but a grid search is run to ﬁnd the most   effective combination of learning rate ( 1e-5 , 4e-5 ,   1e-4 ) , warmup ratio ( 0.02 , 0.06 , 0.1 ) , batch size ( 8 ,   16 , 32 ) , number of epochs ( 1 to 10 ) , and maximum   sequence length ( 256 , 384 , 512 ) . For BM25 , we try   combinations of kfrom 0.1 to 0.4 and bfrom 0.1   to 1.0 with a step size of 0.1 .   6.4 Effectiveness on Phrase Spoilers   The ‘ Phrase Spoilers ’ column group in Table 6   shows the effectiveness of the selected question   answering and passage retrieval models on the   423 test clickbait posts with phrase spoilers . Given   the ground - truth spoiler , we report the predicted   spoilers ’ average BLEU-4 , METEOR , BERTScore ,   and Precision@1 ( using 1,367 posts with phrase   spoilers for training and 335 posts for validation to   tune the hyperparameters ; cf . Table 1 ) .   Overall , DeBERTa - large is the most effective   model for phrase spoilers . Based on our high-   conﬁdence score thresholds , it generates the cor - rect spoiler for 250–300 of the 423 test posts ( i.e. ,   for about 60–70 % of the cases ) according to a   BERTScore or BLEU-4 evaluation . Similar to our   pilot study , the passage retrieval models are com-   parably ineffective in identifying phrase spoilers .   Among them , MonoT5 achieves the highest scores   but is even substantially less effective than the ques-   tion answering baseline BERT . For instance , with a   BLEU-4 of 58.89 and probably 257 correct spoilers   ( 61 % of the 423 test posts ) , BERT is way ahead of   MonoT5 with a BLEU-4 of 4.95 and only 82 prob-   ably correct spoilers ( 19 % of the 423 posts ) .   6.5 Effectiveness on Passage Spoilers   The ‘ Passage Spoilers ’ column group in Table 6   shows the effectiveness of the selected passage re-   trieval models on the 403 test clickbait posts with   passage spoilers ( using 1,274 and 322 posts for   training and validation ) . The numbers of prob-   ably correct spoilers are lower for all models   compared to the phrase spoilers ( even the higher   amount of probably correct passage spoilers of   the passage retrieval models according to their   BERTScore threshold are still worse than the es-   timated probably correct phrase spoilers accord-   ing to BLEU-4 or METEOR ) . Similar to the pilot   study , all question answering models are also sub-   stantially more effective on passage spoilers than   the passage retrieval models . Overall , DeBERTa-   large and RoBERTa - large achieve the highest Preci-   sion@1 scores and the highest amount of probably   correct passage spoilers ( about 35–41 % of the pas-   sage spoilers are correctly identiﬁed according to   our high - conﬁdence thresholds ) .   6.6 Effectiveness of the End - to - End System   We evaluate the entire spoiling pipeline using all   826 phrase and passage test posts by comparing   two - step pipelines that ﬁrst classify the spoiler type7032   to then select an appropriately trained spoiler model   ( trained on the respective type ) and single - step ap-   proaches that skip the spoiler type classiﬁcation   and simply run the same spoiler model on all posts   ( trained on the complete training data ) . For the   two - step pipelines , we experiment with two vari-   ants : ( 1 ) using an artiﬁcial classiﬁer that returns   perfect oracle - style answers about a post ’s type ,   and ( 2 ) using the best RoBERTa - based phrase - vs-   passage classiﬁer from Section 5 .   Since the passage retrieval models were less ef-   fective in our spoiler experiments ( cf . Table 6 ) ,   we report results only for pipelines with question   answering models . In the two - step pipelines the re-   spective question answering models are ﬁne - tuned   on the respective spoiler types , in the single - step   approach on the combined training data .   Table 7 shows the achieved end - to - end effective-   ness values . The individual two - step pipelines with   oracle type classiﬁcation ( row group ‘ Oracle ’ ) are   substantially more effective than their single - step   counterparts without type classiﬁcation ( row group   ‘ None ’ ) that again are more effective than the re-   spective two - step pipelines with “ real ” RoBERTa-   based type classiﬁcation ( row group ‘ Classif . ’ ) .   Overall , the DeBERTa pipeline with oracle classi-   ﬁer achieves an estimated amount of about 50–55 %   correctly spoiled posts ( i.e. , 411 to 457 of 826 ) .   This result conﬁrms that classifying the required   spoiler type can be beneﬁcial for clickbait spoiling .   Still , among the currently realistically applicable   end - to - end spoiling approaches ( with RoBERTa   type classiﬁcation or without spoiler type classi-ﬁcation ) , the one - step DeBERTa approach with-   out spoiler type classiﬁcation is the most effec-   tive according to the number of probably correctly   spoiled posts ( 382 to 409 of the 826 posts , i.e. ,   46–50 % ) . This indicates that the currently best   RoBERTa - based spoiler type classiﬁer with its ac-   curacy of 80.39 % is still not good enough to result   in an end - to - end system that actually beneﬁts from   spoiler type classiﬁcation .   Our results show that effectively spoiling click-   bait with question answering models is possible   in practice but also that there is still room for im-   provements ( e.g. , improved spoiler type classiﬁca-   tion , improved spoiler generation for the individual   types , and taking multipart spoilers into account ) .   7 Conclusion   Clickbait spoiling is a new task to help social me-   dia users who do not want to be manipulated into   falling for clickbait links . Unlike clickbait detec-   tion , which often involves ﬁltering out clickbait   posts from users ’ timelines , clickbait spoiling sub-   verts the curiosity triggered by clickbait , presenting   users with the withheld “ punchline ” in advance .   We compile the ﬁrst large resource for clickbait   with associated spoilers . By interpreting clickbait   spoiling as either a question answering task or a   passage retrieval task , many possible approaches   are available to extract from the linked document   of a clickbait post the phrase or passage that spoils   it . We have explored the effectiveness of a number   of state - of - the - art solutions for both tasks in a large-   scale experiment , including ﬁne - tuning the respec-   tive models on our resource to determine their ef-   fectiveness for type - speciﬁc clickbait spoiling . Our   experimental setup considers type - speciﬁc spoiling   on the one hand , but on the other hand it also in-   cludes an end - to - end conﬁguration for comparison .   Overall , our results show that type - agnostic ques-   tion answering - based spoiling is the most effective   yet , but also that spoiler type - speciﬁc solutions   have the potential to outperform them .   In addition to the possibilities explored , there   might also be other approaches to clickbait spoil-   ing : for example , paraphrasing technology could be   used to directly transform a clickbait post into a ver-   sion that contains its own spoiler . With respect to   multipart spoilers , the use of summarization mod-   els could be an interesting direction to select the   different parts of the linked document of a clickbait   post that make up its multipart spoiler.7033Acknowledgement   We thank Tim Gollub , and our students Jana   Puschmann and Bagrat Ter - Akopyan , who helped   to create earlier versions of the dataset .   Ethics Statement   The spread of clickbait on social media by news   publishers to promote click - through to their web-   sites has been empirically found to decrease their   perceived credibility in readers ( Molyneux and   Coddington , 2020 ) . There is , of course , nothing   wrong with monitoring and optimizing the effec-   tiveness of marketing a newly published news arti-   cle , especially in cases where the editors make an   honest effort to reach and inform their target audi-   ence . But the clickbait in our corpus mostly spreads   trivial facts that could have been easily ﬁtted into   the length limits of a social media post , which is   why we consider these posts to fall short of the   journalistic ideal . However , it is as of yet unclear ,   in terms of journalism ethics , whether clickbait is   an acceptable means to an end for publishers ( i.e. ,   whether it is “ necessary in driving audiences to   the journalism they need by giving them the jour-   nalism they seem to want . ” ) , or whether it serves   to “ crowding out « real » journalism by reducing   quality in favor of the need for a click - through at   whatever cost ” ( Harte , 2021 ) .   Facebook intervened twice with algorithmic ﬁl-   ters to reduce the amount of clickbait that people   are exposed to in their timelines — even though this   probably also lowered Facebook ’s user engagement   metrics . Our technology demonstrates another ,   complementary way of relatively simply circum-   venting the purported exploitation of the curiosity   gap by giving the audience a choice on whether   or not they wish their cognitive “ loopholes ” to be   exploited . If a sufﬁciently large portion of peo-   ple decide to adopt spoiling tools , that would send   a clear message to publishers and social media   platforms alike . Spoiling clickbait , as opposed to   removing it , however , still gives publishers the ben-   eﬁt of the doubt , since , as the publishers claim ,   there are people who enjoy these kinds of trivia .   References703470357036