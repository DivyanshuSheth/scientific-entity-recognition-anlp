  Hillary Dawkins   University of Guelph , Canada   Vector Institute , Toronto , CanadaIsar Nejadgholi   National Research Council Canada   Ottawa , Canada   Abstract   Certainty calibration is an important goal on   the path to interpretability and trustworthy AI .   Particularly in the context of human - in - the-   loop systems , high - quality low to mid - range   certainty estimates are essential . In the pres-   ence of a dominant high - certainty class , for   instance the non - entity class in NER problems ,   existing calibration error measures are com-   pletely insensitive to potentially large errors in   this certainty region of interest . We introduce   a region - balanced calibration error metric that   weights all certainty regions equally . When low   and mid certainty estimates are taken into ac-   count , calibration error is typically larger than   previously reported . We introduce a simple   extension of temperature scaling , requiring no   additional computation , that can reduce both   traditional and region - balanced notions of cali-   bration error over existing baselines .   1 Introduction   Calibrating the certainty estimates of neural net-   works is of the utmost importance for interpretabil-   ity of results and building trust in AI systems . Ide-   ally , if a model outputs some prediction with an as-   sociated probability , we would like to interpret that   quantity as the probability of a correct prediction   ( i.e. as a meaningful certainty estimate ) ( Zadrozny   and Elkan , 2001 ; Niculescu - Mizil and Caruana ,   2005 ) . However , contemporary models are consis-   tently over - confident in their output probabilities   ( Guo et al . , 2017 ) .   Guo et al . ( 2017 ) demonstrates that over-   confident models can arise by overfitting to the   Negative Log - Likelihood ( NLL ) loss , without over-   fitting to the classification accuracy . Many cal-   ibration methods involve modulating the output   logits somehow , according to a prescribed func-   tional form . The parameters of the modulation   function are learned on the associated validation   set by minimizing the NLL loss ( thereby correcting   the overfit ) . Guo et al . ( 2017 ) , as well as manysubsequent studies ( e.g. Müller et al . , 2019 ; Gupta   et al . , 2021 ) , showcase the surprising effectiveness   of temperature scaling , a single - parameter modula-   tion function .   The calibration error is reported as a single quan-   tity computed on the associated test set . Typically ,   the error is composed of a sum of observed errors   across the certainty landscape , visualized using a   reliability diagram ( DeGroot and Fienberg , 1983 ;   Niculescu - Mizil and Caruana , 2005 ) . However , not   all regions contribute equally , especially in the case   of class - imbalanced datasets . Consider an output   with a predicted certainty of 99.9 % vs. an expected   actual certainty of 99.8 % . In terms of human in-   terpretability and intervention , this difference is   negligible . Now consider 79 % predicted certainty   vs. 71 % expected certainty . Clearly the second case   is one we should care more about correcting . How-   ever , as we will discuss in the following section ,   the presence of a dominant high - certainty class can   cause the first discrepancy to contribute more to   the reported calibration error than the second . High   quality mid - certainty estimates are most impact-   ful for human - in - the - loop applications , yet current   error measures are not sensitive to this region .   Here we take NER ( Grishman and Sundheim ,   1996 ; Yadav and Bethard , 2018 ; Li et al . , 2020 ) as   a case study for class - imbalanced token classifica-   tion . Naturally , the “ outside ” or non - entity class   dominates the dataset . In the following section , we   introduce a region - balanced calibration error . We   then introduce region - dependent temperature scal-   ing , a calibration method that further reduces error   over traditional temperature scaling , across various   NER scenarios , without additional computation .   2 Region - balanced expected calibration   error   The most popular calibration error metric is the ex-   pected calibration error ( ECE ) ( Naeini et al . , 2015 ) .   A test set is partitioned into certainty bins , each538   containing samples with a certainty score hwithin   the bin boundaries . The uncalibrated certainty h   for a given sample is simply the output probability   associated with the predicted class for that sam-   ple . Within each bin , we compare the actual and   predicted certainty :   ECE = Xn   N|acc(B)−conf(B)|(1 )   where conf(B)is the predicted confidence score   ( the mean hof samples in bin B ) , and acc(B)is   the actual accuracy ( proportion of correct predic-   tions in bin B ) . Each bin error is weighted by the   bin support , where nis the number of samples in   B. If a very high proportion of all samples have a   high certainty estimate , only the final bin error has   a non - negligible contribution to the overall ECE .   Refer to Figure 1 for an illustrated example .   One extension of ECE is to find bin partitions   adaptively ( Nixon et al . , 2020 ) , such that each bin   contains an equal number of samples , and each bin   contributes equally to the overall error . The result   is that many more bins exist in the high certainty   region , each of which are narrower in width . Essen-   tially , adaptive - ECE reports the exact same error   quantity as ECE in theory , but estimates the quan-   tity using a finer - toothed comb . Neither metric is   informative on lower or mid - certainty regions ifsupport is dominated by a high - certainty class .   Maximum expected calibration error ( MECE )   ( Naeini et al . , 2015 ) partially tells the story of low-   certainty regions by reporting the maximum bin   error . However , MECE is overly sensitive to outlier   bins . For example , if a single sample happens to   fall in the 0 - 5 % certainty bin , and it has the correct   predicted class , we have MECE > .95 , which is   clearly an unusable characterization of the calibra-   tion error as a whole .   Here we consider Region - balanced ECE ( RB-   ECE ) as a way to characterize calibration error   weighted evenly across certainty regions . Simply ,   RBECE = 1   |Θ|X|acc(B)−conf(B)| .   ( 2 )   The error in each bin Bcontributes to the error   equally , subject to some threshold support require-   ment n > θ ( to ensure acc(B)is well - defined ) .   The set of bins that meet this requirement is de-   noted by Θ.   Alternative threshold requirements such as vari-   ance in conf(B)vs . bin size could be explored in   the future . Another possible extension is custom   bin - weighting according to a certainty region of   interest for your application ( e.g. for human - in - the-   loop systems with an intervention criterion).5393 Region - dependent temperature scaling   The idea underlying all calibration methods is gen-   erally to modulate overconfident predictions . In   traditional temperature scaling ( TS ) , a higher tem-   perature means stronger modulation . Temperature   is taken to be a constant , meaning all samples are   treated with the same modulation strength .   The idea underlying region - dependent tempera-   ture scaling ( RD - TS ) is simply that the most con-   fident predictions likely need greater modulation   than less confident predictions , and therefore tem-   perature should depend on the uncalibrated cer-   tainty . If we consider the hypothetical limit of a 0 %   confidence score , it is intuitive that this does not   need any modulation . To investigate this idea em-   pirically , we apply TS to subsets of the OntoNotes   dataset , partitioned according to uncalibrated confi-   dence scores . For each confidence region , the ideal   temperature is shown in Figure 2 . As expected ,   temperature increases as a function of confidence .   A linear fit sufficiently describes the dependence .   Within uncertainty , the intercept is equal to the ex-   pected value of 1 ( T(h= 0 ) = 1 , corresponding   to no modulation ) .   To apply RD - TS , uncalibrated logits ⃗ aare scaled   as⃗ q=⃗ a / T(h)to obtain calibrated logits ⃗ q. Tem-   perature is now a function of confidence T(h ) =   mh+ 1 , where h = max(softmax ( ⃗ a))is the proba-   bility estimate for the predicted class on each sam-   ple . The slope mis the single parameter controlling   modulation strength .   To estimate m , one could repeat temperature   scaling on multiple data subsets , collect data points ,   and fit the slope as in Figure 2 . However , this   method increases computational overhead . Instead ,   let us estimate mfrom the original TS constant   Tand some knowledge of the validation dataset   which was used to compute T. Each sample in   the validation set has an ideal temperature , here   taken to be in the form T = mh+ 1 . Assuming   each sample contributed to the found Tequally ,   T = P(mh+ 1 ) . Given access to the vali-   dation set , this sum can be computed exactly to   findm . However , we can further approximate   the sum by loosely assuming that the data has a   high proportion of samples ( say ≈90 % ) with very   high certainty estimates ( say ≈.99on average ) .   Then the sum is dominated by the first leading   term , T≈.9(.99m+ 1 ) . This quick sketch is   sufficient to achieve good error reduction over the   baseline TS method . The numerical exactness is   not too important , but rather the general signature   of a high proportion of high - certainty samples is   sufficient . We take this further approximation to   gain the advantage that nothing specifically needs   to be known about the calibration dataset . I.e. If   a large pre - trained model has been calibrated on   a large or private dataset , and the corresponding   temperature Tis known , RD - TS can be applied to   your model outputs without access to the calibra-   tion data or further computation .   In summary , the RD - TS method is performed as   follows :   1.Perform regular temperature scaling to obtain   T , or obtain a previously published Tfor   your model .   2.Find the linear dependence parameter m=   ( T−.9)/.89 .   3.Apply calibration to logits ⃗ aas⃗ q=⃗ a / T(h ) ,   T = mh+ 1 .   RD - TS is a simple extension of temperature scal-   ing which requires no additional training . Like   temperature scaling , RD - TS can not change the pre-   dicted class or model accuracy ( unlike some other   generalizations , vector and matrix scaling).540Scenario Uncal . TS VS MS WTS RD - TS   Classic .09328 .02543 ( T= 1.28 ) .07040 .06940 .05236 .02151 ( m=.426 )   Rare & emerging .09878 .05777 ( T= 1.39 ) .07490 .04932 .11559 .03549 ( m=.550 )   Fine - grained .05333 .02179 ( T= 1.12 ) .03440 .04628 .03278 .01263 ( m=.243 )   Specialized .07088 .04147 ( T= 1.29 ) .03844 .03590 .03820 .02781 ( m=.439 )   Sparse training .09683 .07820 ( T= 1.10 ) .11653 .09528 .06279 .04110 ( m=.229 )   Differing sources .05730 .05960 ( T= 1.09 ) .10824 .08470 .05551 .04019 ( m=.214 )   Scenario Uncal . TS VS MS WTS RD - TS   Classic .02001 .00862 ( T= 1.28 ) .01359 .01083 .00962 .00155 ( m=.426 )   Rare & emerging .04278 .02323 ( T= 1.39 ) .02585 .01580 .04712 .00949 ( m=.550 )   Fine - grained .02287 .00783 ( T= 1.12 ) .01587 .01786 .01462 .00839 ( m=.243 )   Specialized .01555 .00617 ( T= 1.29 ) .00608 .00573 .00631 .00651 ( m=.439 )   Sparse training .03267 .02190 ( T= 1.10 ) .03113 .02599 .01645 .01798 ( m=.229 )   Differing sources .00950 .00723 ( T= 1.09 ) .01211 .01344 .01020 .00383 ( m=.214 )   Dataset h|(P=.9)P|(h=.99 )   OntoNotes .998 .964   W - NUT 17 .997 .953   Few - nerd .972 .801   BC2GM .997 .968   OntoNotes ( tc ) .999 .978   4 Experimental results   4.1 Baseline methods   As RD - TS is a simple extension of regular tem-   perature scaling , we focus comparison on similar   post - training parametric calibration methods :   Temperature scaling ( TS ) : Uncalibrated logits ⃗ a   are scaled by a single constant T(as⃗ q=⃗ a / T )   before softmax is applied to obtain calibrated prob-   ability estimates over all classes ( Guo et al . , 2017 ) .   Vector ( generalized Platt ) scaling ( VS ) : A gen-   eralization of TS such that logits are scaled by 2k   learned parameters , ⃗ q=⃗ v ◦ ⃗ a+⃗b , where kis the   number of classes ( Platt , 1999 ; Niculescu - Mizil   and Caruana , 2005 ; Guo et al . , 2017 ) .   Matrix scaling ( MS ) : A further generalized lineartransformation such that logits are scaled by k+   klearned parameters , ⃗ q = M⃗ a+⃗b(Guo et al . ,   2017 ) .   Weighted temperature scaling ( WTS ) : TS us-   ing a class - weighted NLL loss during convergence   ( Obadinma et al . , 2021 ) .   4.2 Datasets   We take the NER task as a case study . Datasets   represent several important scenarios in token clas-   sification settings more broadly :   Classic : The OntoNotes 5.0 NER dataset   ( Weischedel et al . , 2013 ) represents a baseline   “ classic ” scenario involving plentiful training and   calibration data from robust sources .   Rare and emerging named entities : The W - NUT   NER dataset(Derczynski et al . , 2017 ) is gathered   from noisy social media data which contains dif-   ficult entities ( e.g. “ kktny " ) due to informal and   evolving language .   Fine - grained and few - shot : Few - nerd(Ding   et al . , 2021 ) is a challenging few - shot NER dataset   with 66 fine - grained entity types ( e.g. “ art - film " ) .   Specialized language : The BioCreative II Gene   Mention Recognition ( BC2GM ) dataset(Smith   et al . , 2008 ) is composed of scientific text where   named entities are gene mentions.541Sparse training data : OntoNotes telephone call   data is used for training while the full OntoNotes   dataset is used for calibration and evaluation . The   telephone call data subset is a sparse representa-   tion since it is very heavily skewed to the non-   entity outside class , and entity mentions are con-   centrated on “ person " and “ location " , compared to   the full OntoNotes dataset ( generally containing   much richer entity mentions from news sources ) .   Differing language sources : OntoNotes broadcast   news data is used for training , and telephone call   data is used for calibration and evaluation . Broad-   cast news language is professional and grammat-   ically correct . Telephone call language is casual ,   fragmented and incoherent at times .   4.3 Implementation notes   All NER models use DistilBERT(Sanh et al . ,   2019 ) as the base pre - trained model , fine - tuned   for NER using the train dataset for each scenario as   described above . Further details and performance   on the NER task are provided in Appendix A.   Calibration is performed using the uncalibrated   logits of the associated validation set as model in-   puts . Calibration parameters are learned by min-   imizing the NLL ( or weighted NLL ) loss for 50   epochs ( using SGD with 0.01 learning rate , and   0.9 momentum ) . Calibration error is computed   on the associated test set . To compute both ECE   ( eq . 1 ) and RBECE ( eq . 2 ) , the number of bins   is set to 20 . To compute RBECE , the threshold   for support per bin is set to θ= 40 . The code   needed to reproduce these results is made publicly   available . All datasets are publicly available with   preset train / validation / test data splits .   4.4 Results   Experimental results are summarized in Tables 1   and 2 . When low and mid - certainty regions are   taken into account by the RBECE , calibration error   is larger than previously thought ( as reported by   ECE ) . In all scenarios , RD - TS produces the small-   est RBECE ( in many cases quite substantially ) . Ad-   ditionally , RD - TS improves the traditional ECE in   the majority of scenarios . The results show that   RD - TS is an effective extension of TS across a   range of temperature ( T ) values .   Recall in Section 3 , we sketch a way to estimate   the modulation parameter m , and this approxima - tion follows from assuming that a high proportion   of all samples in the calibration set ( say ≈.9 ) have   a high certainty estimate ( say ≈.99on average ) .   We claim that the numerical exactness of these   values is not too important ( and therefore RD - TS   outperforms TS across a range of datasets ) . This   claim is supported empirically ( Table 3 ) .   5 Discussion and Conclusion   Good quality mid - range certainty estimates are es-   sential for productive human - model interactions .   Despite this , existing calibration error measures   can be insensitive to all but the highest certainty   regions . We propose a region - balanced error metric   to probe this unreported information . When low   and mid - certainty regions are taken into account ,   greater calibration errors are revealed .   Further , we explore the idea of a certainty-   dependent temperature . While previous general-   izations of TS , such as vector and matrix scaling ,   allow certainty dependence by increasing the num-   ber of learned parameters , these methods are gener-   ally outperformed by TS ( Guo et al . , 2017 ) . Rather   than allowing a complicated certainty dependence ,   we enforce a simple linear dependence ( motivated   by intuition and an empirical example ) without   introducing any learnable parameters . Unlike vec-   tor and matrix scaling , RD - TS can not change the   relative ranking of logits , and therefore model ac-   curacy is retained ( in single - label settings ) . One   line of future work could be to apply RD - TS on top   of weighted temperature scaling , a method known   to decrease variance in calibration error among   classes ( Obadinma et al . , 2021 ) . Another line of   work would be to investigate whether improved   certainty estimates can increase model accuracy   ( in multi - label settings where predictions are ap-   plied by meeting a certainty threshold ) , especially   in out - of - domain problems .   Finally , it is important to note that our discus-   sion of a region - balanced error measure , as well as   our sketch derivation of the RD - TS method , have   been generally applicable to any problem with a   dominant proportion of high - certainty predic-   tions . This situation does arise in any token clas-   sification problem with a dominant “ easy ” class ,   as is the case in NER , however this situation can   equally occur in class - balanced situations . There-   fore , region - dependent temperature scaling can find   utility beyond NER , token classification , or class-   imbalanced situations.542Ethical Considerations   We proposed a novel method to calibrate class-   imbalanced token classifiers , and demonstrated the   method for NER models . This calibration method   is a step toward responsible use of AI by offer-   ing a measure of reliability , but also has risks that   should be considered from an ethical point of view .   Calibrated scores are a measure of transparency ,   and users can interpret a well - calibrated model bet-   ter . However , all transparency methods expose AI   systems to malicious attacks by providing more   information about the internal workings of the sys-   tem . This risk should be taken into account in   sensitive tasks , e.g. when an NER model is used to   extract personally identifiable information for pri-   vacy reasons . Also , users should be warned that a   low calibration error does not guarantee robustness   in out - of - domain settings . Therefore , in the case of   safety - critical tasks such as medical applications of   NER , a low calibration error should be interpreted   with caution .   Further , low calibration errors should not be used   to justify inherently unethical tasks or those out of   the scope of the capabilities of NLP technologies .   Every task should be evaluated in terms of feasi-   bility and ethical use regardless of reliability and   transparency of trained models . It is also impor-   tant to keep in mind that a well - calibrated model   can become miscalibrated as the data changes , and   continuous calibration is needed to deal with the   ever - changing nature of language .   References543   A NER performance   NER models were obtained by fine - tuning Distil-   BERT , using the default configuration , for 3 epochs   ( with learning rate of 2e-5 , and weight decay of   0.01 ) . The performance of all NER models is pro-   vided in Table A.1 for reference .   Dataset P R F A   OntoNotes .778 .621 .691 .976   W - NUT 17 .543 .234 .327 .938   Few - nerd .639 .679 .659 .906   BC2GM .802 .844 .822 .965   OntoNotes ( bc ) .711 .753 .732 .973544