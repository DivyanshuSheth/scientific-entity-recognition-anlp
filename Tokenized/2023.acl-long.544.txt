  Luyao Zhu , Wei Li , Rui Mao , Vlad Pandelea and Erik Cambria   Nanyang Technological University , Singapore   { luyao001 , wei008}@e.ntu.edu.sg ,   { rui.mao , vlad.pandelea , cambria}@ntu.edu.sg   Abstract   Persona attribute extraction is critical for per-   sonalized human - computer interaction . Dia-   logue is an important medium that communi-   cates and delivers persona information . Al-   though there is a public dataset for triplet - based   persona attribute extraction from conversations ,   its automatically generated labels present many   issues , including unspecific relations and incon-   sistent annotations . We fix such issues by lever-   aging more reliable text - label matching crite-   ria to generate high - quality data for persona   attribute extraction . We also propose a con-   trastive learning- and generation - based model   with a novel hard negative sampling strategy   for generalized zero - shot persona attribute ex-   traction . We benchmark our model with state-   of - the - art baselines on our dataset and a public   dataset , showing outstanding accuracy gains .   Our sampling strategy also exceeds others by a   large margin in persona attribute extraction .   1 Introduction   Persona attribute extraction in dialogues ( PAED )   is a crucial task for persona - based dialogue sys-   tems ( Zheng et al . , 2020 ; Cao et al . , 2022 ) . It can   extract persona attribute information from conver-   sations . Then , a dialogue system can use extracted   persona attributes to generate personalized , user-   preference - aware responses to user queries . Previ-   ous works define the task as a sentence - level ( Da-   niulaityte et al . , 2016 ) or utterance - level ( Gu et al . ,   2021 ) classification task . A model learns to classify   whether a text contains persona information or not .   However , the identified persona - informed texts are   still unstructured , resulting in the lower utility of   the extracted information in downstream dialogue   systems , e.g. , irrelevant contexts and different rep-   resentations towards the same persona attribute .   Thus , we define persona attribute extraction as a   triplet extraction task . A model should extract a   subject , an object , and the persona - relevant relation   linking the subject and object from utterances . The extracted attributes should be in the form   of triplets ( s , r , o ) , where the relation ( r ) indicates   the persona attribute type of the subject ( s ) towards   the object ( o ) . Although the existing relation triplet   extraction ( RTE ) task aims to extract triplets from   documents ( Li and Ji , 2014 ; Chia et al . , 2022 ) ,   its framework can not be directly transferred to   PAED task because the sentences in documents de-   scribe the facts or knowledge in the real world and   each pair of entities in triplets can be connected   by very limited relations . For example , entities   ‘ Eiffel Tower ’ and ‘ France ’ are very likely con-   nected by relation located_in in traditional RTE   datasets ( Chen and Li , 2021 ) . But the subject and   object in dialogues can be linked by many rela-   tions , causing hard sample problems , e.g. , the rela-   tion between ‘ I ’ and ‘ my father ’ may be live_with ,   raised_by , get_along , etc . Hence it is essential to   formulate a framework for PAED , which is capable   of processing hard samples .   To the best of our knowledge , Wu et al . ( 2020 )   proposed the largest persona attribute triplet ex-   traction dataset in dialogues based on the triplet   annotation of Dialogue Natural Language Infer-   ence ( NLI ) dataset ( Welleck et al . , 2019 ) . However ,   we observe that the relation labels were not well-   defined in the Dialogue NLI dataset and the dataset   of Wu et al . ( 2020 ) that inherits the same label   set , containing unspecific relation types . For ex-   ample , negative expressions such as never , and   do n’t have , are collectively categorized as the   relation type of other in both datasets . Dialogue   NLI missed many triplet labels , resulting in incon-   sistent annotations , while the dataset of Wu et al .   ( 2020 ) introduced considerably less reliable labels   because utterances and triplet labels were automat-   ically paired by greedy rules . Motivated by ad-   dressing the unspecific and inconsistent annotation   issues of Dialogue NLI and avoiding the unreliable   label - utterance pairing of Wu et al . ( 2020 ) , we aim   to deliver a rigorous dataset for PAED.9771We source data from Dialogue NLI and Per-   sonaChat ( Zhang et al . , 2018 ) datasets , forming   a new dataset , termed PersonaExt . We manually   correct 1896 triplet labels of the original Dialogue   NLI dataset to improve specificity . We use a more   conservative strategy to assign triplet labels to utter-   ances to improve label reliability and consistency :   Only the triplet selected by both trained classifiers   BERT ( Kenton and Toutanova , 2019 ) and term fre-   quency – inverse document frequency ( TF - IDF ) is   assigned to the utterances . We conduct a human   evaluation on PersonaExt and the dataset of Wu   et al . ( 2020 ) . It shows improvements of Person-   aExt in label specificity and annotation accuracy .   We formulate PAED as a generalized zero - shot   learning ( GZSL ) task because it is common that   the training utterances for a model can not cover all   the relation types . The hard sample issue becomes   more severe in GZSL setting . Thus , we propose a   generation - based framework with a novel hard neg-   ative sampling ( HNS ) strategy for zero - shot PAED .   Our HNS strategy consists of a Meta - V AE sampler   and a contrastive structured constraint ( CSC ) .   Meta - V AE sampler uses |R|latent variables   of variational autoencoder ( V AE ) ( Kingma and   Welling , 2014 ) to represent |R|kinds of utterance   distributions under |R|different relations . It pairs   an utterance under a certain relation with one under   another relation as positive and hard negative sam-   ples , if the distance between their distributions is   the shortest . CSC is designed to disperse the paired   samples in semantic vector space . On average , our   framework surpasses the strongest baseline on our   PersonaExt by 1.06 % and on the public FewRel   dataset ( Han et al . , 2018 ) by 0.8 % ( single triplet ) &   3.18 % ( multiple triplets ) ; Our Meta - V AE sampler   exceeds others ( Eberts and Ulges , 2020 ; Yuan et al . ,   2021b ; Zeng et al . , 2021 ) in PAED by 2.66 % .   The main contributions of this work are : ( 1 ) We   develop a PAED dataset , PersonaExt , with 1,896   re - annotated triplets and 6,357 corrected utterance-   triplet pairs . ( 2 ) We present a generation - based   framework for zero - shot PAED . A novel HNS strat-   egy , Meta - V AE sampler with CSC , is presented to   enhance the performance of our model . ( 3 ) Our   model achieves better results than strong baselines   in zero - shot PAED and negative sampling . Our   code and data are publicly available.2 Related Work   2.1 Persona Extraction   Persona extraction was initially formalized as a   classification task inferring user attributes such   as gender ( Ciot et al . , 2013 ) , age ( Alekseev and   Nikolenko , 2016 ) , opinion ( Li et al . , 2023 ) , oc-   cupation ( Preo¸ tiuc - Pietro et al . , 2015 ) and pref-   erence ( Cambria et al . , 2022 ) from social media .   Welleck et al . ( 2020 ) formulated persona extraction   as a natural language inference task by learning the   relation between an utterance and a persona descrip-   tion . Recently , Wu et al . ( 2020 ) formalized persona   extraction as a generation task extracting structured   and easy - to - use user attributes from human - agent   dialogues through a two - stage extractor . However ,   the extractor is not designed for a zero - shot setting .   2.2 Relation Triplet Extraction   RTE was defined as jointly extracting relations ( He   et al . , 2023 ) and entities ( Li and Ji , 2014 ) . Many   existing models ( Gupta et al . , 2016 ; Zhang et al . ,   2017 ; Geng et al . , 2021 ) can not generalize to un-   seen relations , which is inevitable in PAED . Chia   et al . ( 2022 ) proposed a framework RelationPrompt   for RTE in a zero - shot setting . However , the above   models are tailored for documents and can not be   directly used for PAED as there are more hard sam-   ples in dialogues , e.g. , the subject and the object   may have multiple possible relations . We need to   handle the hard samples for zero - shot PAED .   2.3 Hard Negative Sampling   Negative sampling has been proven a key ingredi-   ent for contrastive learning ( Robinson et al . , 2020 ;   Du et al . , 2021 ) and deep metric learning ( Suh et al . ,   2019 ) . Many RTE methods ( Qin et al . , 2018 ; Yuan   et al . , 2021b ; Eberts and Ulges , 2020 ; Guo et al . ,   2022 ; Chen et al . , 2022 ) also benefit from robust   negative sampling strategies . Hard negative sam-   ples that are close to the positive ones in feature   space have a crucial influence on extraction per-   formance . As opposed to computer vision - related   works ( Shrivastava et al . , 2016 ; Liao and Shao ,   2022 ) , HNS is rarely studied in RTE and zero - shot   settings . Existing joint RTE samplers ( Eberts and   Ulges , 2020 ; Yuan et al . , 2021a ; Zeng et al . , 2021 )   were not designed for hard samples . Therefore , we   develop an HNS strategy employing V AE to select   hard negative samples to improve the representa-   tion ability of the extractor.97723 PersonaExt Construction   Our PersonaExt dataset is developed for PAED ,   constructed from multi - turn chitchat datasets , e.g. ,   PersonaChat ( Zhang et al . , 2018 ) and Dialogue   NLI ( Welleck et al . , 2019 ) . We use PersonaChat as   the data source because it is a dialogue corpus with   many personal profiles . PersonaChat was built by   two crowd - workers to chat with each other , con-   ditioned on predefined personas , e.g. , food pref-   erence , job status , and education . Each persona   includes 4 to 6 persona sentences . The dataset con-   tains 1,155 personas with over 10,907 dialogues .   Dialogue NLI annotated triplet ( s , r , o ) for dia-   logue utterances uand persona sentences pin Per-   sonaChat . They generated entailment , neutral , or   contradiction labels for pairs ( p , u ) and ( p , p ) based   on annotated triplets . For instance , I adopted a cat   andI have a cat named Alfred are both labeled with   a triplet ( I , have_pet , cat ) . Then , they are consid-   ered as entailment ; Sentences with different triplets   are regarded as neutral orcontradiction .   However , many utterances in Dialogue NLI do   not have triplet labels , although the utterances con-   tain persona information . Wu et al . ( 2020 ) em-   ployed a greedy method to improve the label cov-   erage of Dialogue NLI . A triplet label of persona   sentence por utterance uis assigned to another   utterance uif an entailment relationship of ( p , u )   or ( u , u ) is predicted by either BERT ( Kenton   and Toutanova , 2019 ) or TF - IDF classifiers .   The triplets of Dialogue NLI and the dataset   of Wu et al . ( 2020 ) are somewhat unreliable , hav-   ing issues in the unspecific label set definition and   inconsistent utterance - triplet pairing . Thus , in our   PersonaExt construction , an automatic intersection   assignment strategy and manual attribute triplet   label correction are used to improve label qualities .   3.1 Automatic Intersection Assignment   Instead of accepting all the triplet labels given by   the greedy selection method ( Wu et al . , 2020 ) , we   conservatively assign the triplet label of poruto   an utterance uonly if both BERT and TF - IDF in-   dicate that ( p , u ) or ( u , u ) are in an entailment re-   lationship . This change largely improves the label   reliability , although there are chances our method   may miss a few labels . We believe that extracting   reliable persona information is more practical in   real - world applications because a dialogue system   should avoid using misinformation , although some   persona information is conservatively ignored.3.2 Attribute Triplet Label Correction   We re - annotate the relation types and entities of   the attribute triplets in Dialogue NLI , because Dia-   logue NLI has issues in consistency and specificity .   Consistency . Some details in persona sentences   do not appear in utterances , even if they are in   an entailment relationship . For instance , persona   sentence I have 1 cat and I dislike dogs has the   triplets ( I , have_pet , 1 cat ) and ( I , dislike , dogs ) ;   Given an utterance I usually play with my cat and   the persona sentence are predicted as entailment ,   the utterance is assigned with the two triplets in   Dialogue NLI . However , ( I , dislike , dogs ) did not   really appear in the utterance . Thus , the utterance   is over - annotated in Dialogue NLI .   Specificity . A relation type should be specific to   distinguish it from others . Most negations , such as   never , and do n’t have are categorized into the re-   lation other in Dialogue NLI . However , we expect   them to be never_do andhave_no , so that we can   categorize them into different personas , e.g. , the   negation of action and the negation of possession .   Besides , an object should be quantity - specific , thus   dialogue systems can precisely present the nuances   in responses . For example , ( I , have_pet , 1 cat ) and   ( I , dislike_animal , dogs ) have more details that can   be used to generate responses than ( I , have_pet ,   cat ) and ( I , dislike , dogs ) .   Therefore , we design a semi - automatic annota-   tion method for triplet label correction . Step 1 .   We retrieve persona sentences with negations or   any relation type in [ other , have , like , like_general ,   < blank > ] , and manually re - annotate them . In to-   tal , 1,896 sentences are corrected . The detailed   rules and all the relation types in our dataset are   in Appendix D. Step 2 . We assign the triplets of   persona sentences to each utterance according to   the method in § 3.1 . Step 3 . We use SnowballStem-   merto eliminate over - annotations , e.g , redundant   numbers , groundless adverbs , adjectives or nouns ,   and incorrect form of verbs , to make the subject and   object consistent with the utterance . The number   of processed sentences adds up to 6,357 .   In Step 1 , we first invited an expert ( a main anno-   tator ) to manually annotate triplet labels for 1,896   persona sentences . The expert is a native English   speaker with rich persona - based dialogue system   research experience . We invited a single main an-   notator to annotate data to secure annotation con-   sistency.9773   Next , we invited two side annotators to vote   triplet labels given by the main annotator and Di-   alogue NLI dataset , respectively . The main and   side annotators follow the criteria that the triplet   information should be completely presented in a   persona sentence or an utterance ; the relationship   and object of a triplet should be specific for repre-   senting the persona information .   Cohen ( 1960 ) ’s kappa of the side annotators was   0.72 . 82.8 % annotations , generated by the main   annotator were supported by both side annotators .   89.4 % of newly generated annotations were sup-   ported by at least a side annotator . We use the   newly generated triplet labels that were supported   by at least a side annotator . We use the triplet labels   of Dialogue NLI if no side annotator supports the   re - annotated triplets .   To evaluate the quality of the semi - automatically   generated triplet labels in our PersonaExt dataset ,   we invited two English - speaking graduate students   to score 150 randomly selected utterances with   triplets . Both the dataset of Wu et al . ( 2020 ) and   our re - annotated triplets were scored in terms of   ‘ consistency ’ ( cns . ) and ‘ specificity ’ ( spec . ) for   relations and objects , respectively . The scale of the   score was { 0,1 } . The average scores in Table 1   show that PersonaExt largely advances the dataset   of Wu et al . ( 2020 ) in the two evaluation indices .   4 Generalized Zero - Shot PAED   We propose a framework for PAED in general-   ized zero - shot learning ( GZSL ) setting ( Xian et al . ,   2018 ) . The framework consists of two parts : a   persona attribute generator ( PAG ) and a persona   attribute extractor ( PAE ) . PAG is trained to gen-   erate synthetic dialogue utterances containing per-   sona descriptions . PAE is trained on the synthetic   data and extracts attribute triplets of unseen target   data . PAE is a pretrained language model ( PLM )   based extractor enhanced by our proposed Meta-   V AE sampler with CSC loss.4.1 Task Definition   A PAED dataset is denoted as D= ( U , Y ) , where   Uis the input dialogue utterance set and Yis the   persona attribute set . y= ( s , r , o ) ∈Yis an at-   tribute triplet , where s , o , and rare a subject , an   object , and a relation type . The goal of general-   ized zero - shot PAED is to train the model on seen   dataDand generalize to the unseen test data D.   During training , Dand test relation Rare avail-   able ( Verma et al . , 2018 ) . At test time , the relation   search space of the trained model contains both   training and test relations ( R∪R ) , and is even   much larger as PAE is generation - based instead of   classification - based . R∩R=∅. A test utterance   can be assigned to either a training ror test rela-   tionr , where r∈R , r∈R(Xian et al . , 2018 ) .   4.2 Persona Attribute Generator   Prompt tuning is proven to improve the generaliza-   tion of PLMs in zero - shot learning ( Lester et al . ,   2021 ) , as it bridges the gap between the pretrain-   ing tasks and the downstream tasks ( Mao et al . ,   2023 ) . Thus , we prompt - tune the PLM to syn-   thesize samples Dbased on relations rin the   unseen test set D , following the research of Verma   et al . ( 2018 ) . First , PAG is trained on training   dataD , then prompt - tuned with rto generate   synthetic data D. In the testing phase , given a   prompt “ : r ” , PAG is trained to generate   a structured output in the form of “ : u , : s , :o ” . During training , PAG is   trained with the causal language modeling objec-   tive , next word prediction ( Bengio et al . , 2000 ) .   p(x|x;tp ) = PAG ( x ) , ( 1 )   where xis the i - th token in input tokens   “ : r , : u , : s , :   o ” . We maximize the probability of current to-   kenxconditioned on previous tokens x : L=/summationtextlogp(x|x;tp ) . Temperature tp(Hinton   et al . , 2015 ) adjusts the diversity of generation .   4.3 Persona Attribute Extractor   Similarly , we first finetune the PLM - based PAE on   training data D , then tune the extractor on syn-   thetic samples Dgenerated by PAG . PAE is   trained with the seq - to - seq objective ( Lewis et al . ,   2020 ) . Given the prompt “ : u ” , the   extractor learns to predict a structured output as   “ : s , :o , : r”.9774   However , during testing , it becomes harder for   PAE to distinguish the relation types in unseen data   D , as a dialogue utterance may convey a com-   pletely opposite meaning by replacing only one   token , e.g. , from ‘ like ’ to ‘ hate ’ . Hence , we pro-   pose CSC to help to differentiate the relation types   and Meta - V AE sampler for hard negative sampling ,   which are introduced in § 4.5 and § 4.4 , separately .   4.4 Meta - V AE Sampler   The premise ( supported by § 4.4.1 ) of our model   is that , for each relation type , Meta - V AE captures   the distribution of all the utterances with such a   relation . In addition , the utterances uwith relation   randuwith relation rare considered hard neg-   ative samples for each other if randris close in   terms of distribution distance .   In Fig . 1 , an utterance u(I enjoy playing with   cats ) with a triplet ( s , r , o)(I , like_animal ,   cats ) is a sentence with relation class like_animal .   And a positive sample of CSC is formulated as   “ : I enjoy playing with cats . :   I : cats : like_animal ” . Then ,   the top- kclosest relations ( like_music , like_sport ,   andhave_pet ) to relation like_animal are retrieved   by Meta - V AE sampler . For each retrieved relation   r , e.g. , like_sport , an utterance , e.g. , I enjoy play-   ing basketball , is randomly selected . Then , the   selected kutterances are assigned with the same   triplet ( s , r , o)ofuto construct hard negative   samples . For example , one of the hard negative   samples is “ : I enjoy playing basket-   ball . : I : cats :   like_animal ” . Then the extractor is trained to dis-   perse the positive and negative samples in vector   space with CSC and seq2seq loss . Meta - V AE is   trained with KL divergence and next word predic-   tion loss as Eq . 4.4.4.1 Meta - V AE   V AE ( Kingma and Welling , 2014 ) can approximate   the prior distribution p(z)of latent continuous   random variable zthrough approximate posterior   q(z|u)for a given dataset . Intuitively , for each   dataset with a certain relation type r , we want   to train a V AE to approximate a different prior   distribution of its latent continuous random vari-   able . Thus , we will obtain |R|different V AEs in   total;|R|is the number of relation classes . How-   ever , this is parameter - inefficient . Therefore , we   propose Meta - V AE to reduce the complexity : We   map each relation class into a relation embedding   Emb(r)through a fully - connected layer with pa-   rameters τ , concatenating each encoded utterance   Emb(u)with the corresponding relation embed-   ding and feeding the concatenated features into   V AE . This is because the concatenation - based con-   ditioning ( Sitzmann et al . , 2020 ) equals a special   case of hypernetwork ( Ha et al . , 2017 ) which is   an emerging branch of meta - learning ( Chang et al . ,   2019 ) , and generates the weights of the layers for   different tasks ( i.e. , relation types ) .   We use GRUs ( Chung et al . , 2014 ) as the encoder   and decoder of Meta - V AE . Considering the struc-   ture of update and reset gates of GRU , we simplify   the concatenation by feeding Emb(r)as an initial   hidden state of a GRU encoder as Eqs . 2 and 3 .   It is because additive equals concatenation atten-   tion ( Luong et al . , 2015 ; Bahdanau et al . , 2015 ) .   a = σ(WEmb(x ) + UEmb(r))(2 )   c = σ(WEmb(x ) + UEmb(r)),(3 )   where Emb(x)is the first token embedding in u.   aandcare the update gate and reset gate at time   step 1 for the j - th GRU unit ; W , U , W , Uare   learnable parameters.9775The empirical objective of Meta - V AE with Gaus-   sian latent variables zis as follows :   L(u;θ , ϕ , τ ) = −D(q(z|u)||p(z ) )   +1   L / summationdisplaylogp(u|z).(4 )   For each relation r , a set of latent variable zis ob-   tained from the prior distribution p(u|z)and the   datauis generated by the generative distribution   p(u|z)conditioned on z.z = q(z|u)∼   N(µ , σI),p(z)∼ N ( 0 , I).q(z|u)is a   probabilistic encoder . θ , ϕandτare learnable pa-   rameters . Lis the number of samples .   4.4.2 Sampling Criteria   As the latent variable model can express the dis-   tributions of variables in terms of a small amount   of latent variables ( Bishop , 1998 ) , the latent vari-   ablezcaptures the distributions of utterances   with different relations r. Thus , we use KL di-   vergence ( Kullback and Leibler , 1951 ) between the   distributions of latent variables zandzto repre-   sent the distances between different utterances with   different relation classes randr .   We assume that the latent variable zof each   relation class obeys a multivariate Gaussian Distri-   bution z∼ N(z;µ,Σ)and all components of zare   independent , i.e. , Σ= 0 , i̸=j . Then , for latent   variables zandzof relation classes randr , the   KL divergence is :   D(P||P ) = E[logP   P ] = 1   2{log|Σ|   |Σ|−n+   tr(ΣΣ ) + ( µ−µ)Σ(µ−µ ) } ,   ( 5 )   where P , Pare the probabilities of zandz . As   we assume Σis a diagonal matrix , Eq . 5 can be   simplified as :   D(P||P ) = 1   2{tr(logΣ−logΣ)−n+   tr(Σ./Σ ) + ( µ−µ)./Σ(µ−µ ) } .   ( 6 )   Here , ./is an element - wise division operation on   Σthrough which we obtain 1 / σfor each diago-   nal element in Σ.   Our sampling strategy is : For each relation class   r , we randomly select one utterance , feeding it to   the trained Meta - V AE and obtaining zto represent   the distribution of utterances under relation r ; We   compute the distance between the distributions ofzandzas the distance between utterances under   randr , i̸=j . Then , the top- kclosest relations   are selected for each relation r ; For any utterance ,   we randomly select one utterance for each top- k   closest relations and get khard negative samples .   The detailed sampling algorithm is in Appendix C.   4.5 Contrastive Structured Constraint   The existing generation - based triplet extraction   methods seldom focus on the fact that triplets are   supposed to be consistent with the input utterance   u(Ye et al . , 2021 ) . Additionally , the similar token   distribution of some dialogue utterances exacer-   bates the problem . For example , we aim to extract   the attribute triplet like ( My mom , have_pet , 1 cat )   instead of ( My mom , like_animal , 1 cat ) for a given   input utterance My mom has a cat named Kitty .   This is because we believe the former explicitly   conveys the fact that the cat belongs to my mother ,   while the latter does not convey the property of   ownership .   To this end , we transform the triplet contrastive   learning into a binary classification problem : For   the utterance uwith label ( s , r , o ) , we get   khard samples ( u , ... , u ) from Meta - V AE   sampler ; We represent the positive sample as   “ : u , : s , : o,- : r ” and the j - th negative sample as   “ : u , : s , : o,- : r ” . We use the hidden state h(h ) of   the last input token as the i - th positive ( j - th neg-   ative ) sample semantic representation from PAE   and feed it to a fully - connected layer to compute   classification logits l. Instead of constraining the   samples to converge to a fixed positive or nega-   tive polarity ( Zhu et al . , 2020 ) , we employ CSC   to relocate the positive and negative samples and   make them diverge from each other . The structural   contrastive loss based on KL divergence is :   L=−D(l||l)−D(l||l )   = −/summationdisplay / summationdisplay1   k(llogl   l+llogl   l).(7 )   Here , lis the logits for the i - th positive sample   andlis the logits for the j - th negative sample .   5 Experiments   Besides our PersonaExt ( PerExt ) , we experimented   on FewRel to explore the capability of our model   in multiple triplet extraction and the potential to9776   generalize on zero - shot RTE . Another reason is we   do not have another triplet - based PAED dataset to   test our model . The statistics are listed in Table 2 .   We evaluated the performance in multiple triplet   extraction with a standard metric Micro F(Paolini   et al . , 2020 ) , precision ( P. ) and recall ( R. ) . For   single triplet extraction , we used accuracy ( Acc . ) .   5.1 Datasets   FewRel is built through distant supervision where   a set of candidate relations and instances are auto-   matically extracted over Wikipedia and Wikidata ,   and then human annotation is employed to filter   low - quality relations ( Han et al . , 2018 ) . We follow   the same operation as Chia et al . ( 2022 ) to make   FewRel suitable for zero - shot RTE .   For the two datasets , we randomly select a fixed   number of seen and unseen labels during training .   The number of unseen label size nis set to three   incremental setups { 5,10,15 } . To obtain consol-   idated experimental results , we use five different   random seeds to repeatedly select five combina-   tions of the seen and unseen labels , yielding five   different data folds . Each data fold consists of   training , validation and test sets . The test set con-   tains sentences with unseen labels . The validation   set contains five labels which are used to select   sentences for hyper - parameter tuning . The remain-   ing sentences comprise the training set . With this   setting , we ensure training , validation and test sen-   tences come from disjoint label sets .   5.2 Baselines   TableSequence ( TS ) ( Wang and Lu , 2020 ) is pri-   marily designed for joint learning of named entity   recognition and relation extraction .   RelationPrompt ( RP ) ( Chia et al . , 2022 ) is the   first to solve zero - shot RTE by prompting PLMs to   synthesize relation samples given relation labels .   SpERT ( Eberts and Ulges , 2020 ) transfers the   strong negative sampler by concatenating the cur-   rent utterance of which the triplet is ( s , r , o )   and any other utterance of which the triplet is   ( s , r , o ) . The negative triplet is ( s , r , o )   or(s , r , o ) , where ris a random relation type .   RSAN ( Yuan et al . , 2021b ) randomly selects sev-   eral relations different from that of the current sen-   tence .   GenTaxo ( Zeng et al . , 2021 ) randomly selects a   triplet ( s , r , o ) , then the negative triplet is gen-   erated as ( s , r , o)or(s , r , o ) .   5.3 Setups   We used the PLM GPT-2 ( Radford et al . , 2019 )   with 124 M parameters as PAG and BART ( Lewis   et al . , 2020 ) with 140 M parameters as PAE . Meta-   V AE sampler has 2.4 M parameters . We first fine-   tuned the models on the training set for 5 epochs   and selected the best model parameters based on   the validation loss with AdamW ( Loshchilov and   Hutter , 2018 ) optimizer . We set batch size as 128   for PAG and 32for PAE , learning rates as 3e-5   for PAG , 6e-5for PAE and 0.005for Meta - V AE ,   and warm up ratio as 0.2 . For each relation , 250   sentences were synthesized by PAG utilizing the   relation labels of validation and test set as prompts .   Then , we finetuned the PAE again on the synthetic   sentences . We employed greedy decoding strategy   for single triplet extraction and triplet search de-   coding ( TSD ) ( Chia et al . , 2022 ) strategy for multi-   triplet extraction . More implementation details are   in Appendix B.   5.4 Experimental Results   We reported the main results for generalized zero-   shot RTE and PAED in Table 3 . For each n∈   { 5,10,15 } , we run 5 different data folds 3 times   and obtained the average with a significance level   of 0.05 . We found that surpasses RP ( 1.06 %   on average ) in all settings on PersonaExt . On   FewRel dataset , performs better than RP   in most settings.9777   We attribute the significant improvement ( 3.18 %   on average ) in multi - triplet extraction to our Meta-   V AE sampler with CSC that introduces hard sam-   ples during training . In particular , consis-   tently achieves higher precision ( 3.22 % on aver-   age ) than RP . The false positive problem is more   severe than the false negative in PAED as speakers   are more likely to tolerate negligence rather than   confusion . The results also show the generalization   capability of our framework on zero - shot RTE .   5.5 Ablation Study   We conducted an ablation study on PersonaExt   dataset to compare Meta - V AE sampler with sev-   eral benchmark samplers . All the samplers use the   same random seed and CSC loss . We run them   with three unseen label setups and reported the   average accuracy of three runs . In Table 4 , Meta-   V AE sampler outperforms the other four samplers   by 2.66 % on average and surpasses the strongest   baseline GenTaxo by 1.37 % . This indicates our   Meta - V AE sampler yields better negative samples   because of its good approximation to the distribu-   tions of different relations . We also observed a   significant performance drop of w/o HNS , yet it   still exceeds some of the baseline samplers . It sug-   gests a bad sampler may cause a decline instead   of an enhancement . Therefore , it is crucial for a   sampler to accurately identify the hard negative   samples to make the best of contrastive learning .   5.6 Revisiting Meta - V AE Sampler with CSC   The KL divergence between positive and negative   samples gets larger during finetuning on the syn-   thetic dataset ( details are in Appendix A ) . This is   explained by the fact that we utilized KL diver-   gence to formulate our CSC loss . However , to get a   concrete understanding of whether our Meta - V AE   sampler and CSC work as expected in vector space ,   we studied the distribution of positive and negative   samples before and after finetuning ( Fig . 2 ) .   In each contrastive group , one sample is paired   with three samples under different relation types   retrieved by Meta - V AE . All the scatter points are   obtained by decomposing the sample representa-   tionhfrom PAE with principal component analysis   ( PCA ) ( Wold et al . , 1987 ) . Different groups are in   distinct colors . Fig . 2 ( a ) shows negative samples in   each group are closely scattered around the positive   sample . This indicates Meta - V AE sampler can find   out the hard negative samples which are semanti-   cally closest to the positive one . Figs . 2 ( b ) , ( c ) and   ( d ) suggest fintuning with CSC loss disperses the   positive and negative samples in semantic vector   space . We conclude that Meta - V AE is capable of   retrieving the hard negative samples in terms of   semantic meaning and CSC loss enables the model   to relocate the positive and negative samples in a   sparse manner .   5.7 Case Study   We show three PAED cases in Fig . 3 to reveal the   pros and cons of the extraction methods and annota-   tions . As shown , in cases 1 and 3 , the RP - extracted   objects do not fit well with the relations . In addi-   tion , RP extracted incorrect relations which contain   the opposite meanings to the ground truth ( Person-   aExt ) in cases 1 and 2 . In contrast , the strong   performance of our extractor indicates it benefits   from dealing with hard negative samples . The ob-   ject ‘ all ’ in case 2 is not specific . Wu et al . ( 2020 ) ’s   annotations of relations and objects in cases 1 and   3 are inconsistent with utterances.9778   5.8 Exploration of Experimental Settings   To further explore the robustness of our framework ,   we analyzed the effects of PAE ’s decoding strat-   egy and the data size of the samples generated   from PAG on PersonaExt dataset . The comparison   in Table 5 was conducted with three unseen label   setups and shows the accuracy change between   a decoding strategy and our default greedy strat-   egy . We observed that top- krandom sampling ( Fan   et al . , 2018 ) weakened the extraction performance   although it was proved to be more effective than   beam search in various generation tasks .   As discussed in Lu et al . ( 2022 ) , top- krandom   sampling is commonly used in open - ended genera-   tion and , hence , it is not a suitable decoding strat-   egy for PAED . Additionally , TSD improved the   accuracy in single triplet extraction in PAED task ,   which was initially proposed to improve the perfor-   mance of RP in multi - triplet extraction . However ,   as TSD is a beam search - based decoding strategy ,   the slight increase of accuracy came at the cost   of much longer computation time . We conducted   the experiments on RelationExt dataset with 10   unseen labels and report the results in Fig . 4 . In   general , the proposed framework is robust with the   synthetic data size changing from 250 to 550 . An   obvious improvement of accuracy can be observed   by increasing the synthesized sample number from   1 to 100 . The best performance was obtained when   the synthesized samples sums up to 450 . However ,   the further increase of the synthetic data size led to   gradual reduction in accuracy .   6 Conclusion   In this work , we studied generalized zero - shot   learning for persona attribute extraction in dia-   logues ( PAED ) . We first built PersonaExt based   on PersonaChat and Dialogue NLI through a semi-   automatic annotation framework , yielding consis-   tent and specific triplet labels . Then we proposed   an effective and interpretable Meta - V AE sampler   with CSC loss to process hard negative samples   and incorporated it into PAE for generalized zero-   shot PAED task . Empirical results demonstrate   that our framework surpasses the strongest base-   line by a large margin . A visualized quantitative   analysis provides a thorough explanation for the   mechanism of our Meta - V AE sampler and CSC in   sample representations .   Limitations   Due to the lack of theoretical support , it is challeng-   ing for us to formalize an annotation scheme for   implicit persona attributes in the current stage , e.g. ,   extracting an implicit triplet ( I , like_animal , dogs )   from a sentence “ every day , I personally take my   dogs out for a walk and lend a hand to my neigh-   bors by occasionally taking their furry friends out   for a stroll as well ” , besides ( I , have_pet , dogs ) .   Therefore , our PersonaExt is not compatible with   the implicit or multiple persona attribute triplet ex-   traction tasks . Additionally , our framework did not   exploit complementary information from the con-   text of the current utterance for PAED . For an input   with multiple dialogue utterances , it is hard for our   model to match extracted persona triplets with the   exact speaker because of the existence of pronouns   and more than one speaker in dialogues .   Acknowledgements   This research is supported by the Agency for Sci-   ence , Technology and Research ( A*STAR ) under   its AME Programmatic Funding Scheme ( Project   # A18A2b0046).9779Ethics Statement   In this work , human annotation is conducted with   the utmost care to ensure the absence of offensive   content and the non - collection of personal identify-   ing information . Comprehensive explanations are   provided to the annotators regarding the purpose   and appropriate usage of their annotations , ensur-   ing their informed consent has been obtained . The   basic demographic and geographic characteristics   of the annotator population are not reported , as they   do not serve as the primary source of the data in   this work .   References978097819782A Performance of finetuning with   Meta - V AE sampler and CSC   To show the effect of Meta - V AE sampler and CSC   during the finetuning process , we report the trend   of losses in Fig . 5 . Under the same training condi-   tions , we finetuned the persona attribute extractor   with or without Meta - V AE sampler and CSC on the   synthetic dataset . The losses from the two experi-   ments are depicted in Fig . 5 ( a ) and ( b ) , separately .   The results show that the KL divergence between   positive and negative samples selected by Meta-   V AE sampler became larger when finetuning with   the CSC loss .   B Implementation details   We used one Tesla V100 32 GB GPU for training   in our experiments . It took around three minutes   to finetune on training set of PersonaExt in each   epoch . And it took around two hours for one run of   PersonaExt in each setup for each random seed . It   took around 5 hours for each run on FewRel dataset .   Hyperparameters , i.e. , the weight of CSC loss and   the learning rate of Meta - V AE are tuned manually   according to the performance on the PersonaExt   validation set with 5 unseen labels . For the weight   of CSC loss , we considered the values 0.5 , 0.1 ,   0.05 , 0.01 , 0.005 ; For the learning rate of Meta-   V AE , we tried the values 0.05 , 0.01 , 0.005 . Finally ,   the number of negative samples kis set to 3 and the   weight of CSC loss is 0.5 . Due to the computational   constraints , the other hyperparameters are fixed   values , which are listed in Table 6 .   Algorithm 1 : Meta - V AE Sampler   Input : Utterance dataset D , vae - based   distance function d ; number of   relations n ; number of negative   relation sample per relation class k   Output : Iterative sampler of dataset D   Initialization : rel2utt : dictionary   containing all utterance indices of each   relation class ;   utt2rel : inverse dictionary of rel2utt ;   dist : zero matrix of size n×n .   fori= 1;i≤ndo   index = random.choice ( rel2utt[i ] )   utt= D[index ]   forj= 1;j≤ndo   index = random.choice ( rel2utt[j ] )   utt= D[index ]   dist[i , j]=d(utt , utt )   end   end   dist[i , i]=Inf   topks = topk ( −dist , k )   indices = [ ]   foridxin range ( |D|)do   sub_index = [ ]   relation _ index = utt2rel[idx ]   sub_index.append ( idx )   foriin topks [ relation _ index ] do   select _ utt = random.choice ( rel2utt[i ] )   sub_index.append ( select _ utt )   end   indices.append(sub_index )   end   return iterator(indices )   C Meta - V AE Sampling   The pseudo - code of our Meta - V AE sampler is   shown in Algorithm 1.9783D Dataset Annotation   D.1 Statistics of Annotated Sentences   In this subsection , we display the statistical infor-   mation of corrected triplet labels . As shown in Ta-   ble 7 , we manually correct three kinds of errors , i.e. ,   like , neg.(negation ) , misc.(miscellaneous ) . Column   names ‘ Like ’ corresponds to sentences with rela-   tions likeandlike_general . ‘ Neg . ’ corresponds   to sentences with negations or zeros in object o.   ‘ Misc . ’ corresponds to sentences with relations   other , < blank > andhave . Within the scope of Au-   tomatic , ‘ No . ’ and ‘ Prn . ’ refer to the numbers and   the pronouns automatically processed by Snowball-   Stemmer , respectively .   D.2 Relation Types   We have 105 relation types in PersonaExt   Dataset : live_in_citystatecountry , like_food ,   place_origin , employed_by_general , like_goto ,   has_profession , has_age , have_pet , has_ability ,   never_do , like_music , like_animal , want_do ,   favorite_food , has_hobby , favorite , like_read , fa-   vorite_music_artist , own , employed_by_company ,   allergy_to , have_vehicle , attend_school ,   like_drink , favorite_music , have , misc_attribute ,   previous_profession , dislike_food , physi-   cal_attribute , like_sports , school_status ,   live_with , other , name , favorite_color , be-   lief , like_movie , scared_of , want , favorite_sport ,   have_children , favorite_hobby , gender , diet , teach ,   dislike_animal , live_in_general , favorite_animal ,   have_family , fall_out , dislike_music , do_not_eat ,   favorite_movie , have_no , job_status , fa-   vorite_season , dislike_drink , favorite_activity ,   worry_about , member_of , do_not_drink , fa-   vorite_drink , marital_status , has_degree ,   favorite_book , do_not_do , dislike_sport ,   have_children , weakness , international_exp ,   industry , doing , have_no_family , like_sport , dis-   like_subject , relationship , like_character , collect ,   pre_employed_by_company , nationality , sex-   ual_orientation , race , pre_employed_by_general ,   raised_by , dislike_job , dislike_color , want_no ,   work_schedule , like_subject , like_activity ,   like_watching , health_status , favorite_show ,   dislike_activity , have_no_sibling , used_to ,   get_along , like_general , have_sibling , dis-   like_general , like_color , want_job , favorite_place ,   have_no_children .   D.3 Annotation Rules for Selected Relation   Types   The relation types [ other , have , like , like_general ,   < blank > ] are subdivided into the following differ-   ent relation types based on the semantic meaning   of the persona sentence .   •other/ < blank > : { diet , allergy_to , scared_of ,   relationship , belief , health_status , job_status ,   school_status , attend_school , doing , used_to ,   raised_by , work_schedule , get_along ,   live_with , worry_about , place_origin , race ,   industry , name , collect , sexual_orientation ,   misc_attribute , has_ability , have_children ,   gender , like_music , like_activity , like_goto ,   like_drink , have_family , have_no_family ,   live_in_citystatecountry , previous_profession ,   pre_employed_by_company , physi-   cal_attribute , pre_employed_by_general ,   other }   •have : { collect , relationship , physi-   cal_attribute , live_with , live_in_general ,   like_activity , has_profession , allergy_to ,   health_status , have_vehicle , interna-   tional_exp , member_of , want_do , weakness ,   have_family , has_hobby , marital_status ,   employed_by , have }   •like/ like_general : { like_character , like_color ,   like_activity , like_movie , like_music ,   like_watching , has_hobby , favorite_season ,   favorite_music_artist , misc_attribute ,   get_along , job_status , has_profession , collect ,   have_family , want_job , marital_status ,   like_general }   •dislike : { dislike_color , dislike_food , dis-   like_subject , dislike_job , dislike_sport , dis-   like_animal , dislike_activity , dislike_drink ,   dislike_read , dislike_music , dislike_general }   Sentences with negations or zeros in oare re-   annotated by the following relation types based on   the context .   •negations/ zeros:{do_not_drink , never_do ,   have_no_family , have_no , have_no_children,9784weakness , want_no , have_no_sibling ,   fall_out , do_not_eat , do_not_do , dis-   like_job , dislike_food , job_status ,   scared_of , allergy_to , dislike_color , dis-   like_sport , dislike_activity , used_to , previ-   ous_profession , pre_employed_by_general ,   marital_status , have_no_pet , health_status ,   physical_attribute , misc_attribute , sex-   ual_orientation , dislike_general , worry_about ,   diet , belief , relationship }   E Discussion of data and code   Our PersonaExt is developed on the basis of Per-   sonaChat ( MIT license ) and Dialogue NLI ( CC - BY   4.0 ) . The pretrained language models we used , i.e. ,   GPT-2 and BART , are under MIT license . The   data of our PersonaExt is sufficiently anonymized   as all persona data are pre - defined instead of ex-   tracted information from personal profiles in the   real world .   For our human annotation and human evaluation ,   we invited 5 English - speaking participants among   which one is an expert with dialogue system re-   search experience and the other four are graduate   students . The hourly payment is around 80 % of   their hourly salary or stipend . It took totally 64   hours for each annotator in the human annotation   task and 5 hours for each in the human evaluation   task . The task is scheduled to be finished in one   month.9785ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   We did not use any AI writing assistant when working on this paper .   B / squareDid you use or create scientiﬁc artifacts ?   sections 3 and 4   /squareB1 . Did you cite the creators of artifacts you used ?   sections 3 , 4 and 5   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We discuss it in the appendix section E.   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 3   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   In Appendix section E we claimed Human annotation in our work does not show any offensive content   or collect any personal identifying information .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 5 and Appendix section D   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 5 and Appendix section D   C / squareDid you run computational experiments ?   Section 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 5 and Appendix section B9786 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 5 and Appendix section B   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Sections 3 and Sections 5 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Sections 3   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 3 .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Section 3 and Appendix section E   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Appendix section E   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Appendix section E   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   We reported the annotators ’ research experience and education background in Appendix section E to   meet the requirement of the checklist question D2 . But we do not report the basic demographic and   geographic characteristics of the annotator population and it is not the source of our data.9787