  Zepeng Zhai , Hao Chen , Fangxiang Feng , Ruifan Li , Xiaojie WangSchool of Artificial Intelligence , Beijing University of Posts and Telecommunications , ChinaEngineering Research Center of Information Networks , Ministry of Education , China   { zepeng , ccchenhao997 , fxfeng , rfli , xjwang}@bupt.edu.cn   Abstract   Aspect Sentiment Triplet Extraction ( ASTE )   aims to extract sentiment triplets from sen-   tences , which was recently formalized as an ef-   fective machine reading comprehension ( MRC )   based framework . However , when facing mul-   tiple aspect terms , the MRC - based methods   could fail due to the interference from other   aspect terms . In this paper , we propose a novel   COntext - Masked MRC ( COM - MRC ) frame-   work for ASTE . Our COM - MRC framework   comprises three closely - related components : a   context augmentation strategy , a discrimina-   tive model , and an inference method . Specif-   ically , a context augmentation strategy is de-   signed by enumerating all masked contexts for   each aspect term . The discriminative model   comprises four modules , i.e. , aspect and opin-   ion extraction modules , sentiment classification   and aspect detection modules . In addition , a   two - stage inference method first extracts all   aspects and then identifies their opinions and   sentiment through iteratively masking the as-   pects . Extensive experimental results on bench-   mark datasets show the effectiveness of our pro-   posed COM - MRC framework , which outper-   forms state - of - the - art methods consistently .   1 Introduction   Aspect Sentiment Triplet Extraction ( ASTE ) has re-   cently been proposed , which is a variant of the fine-   grained Aspect - based Sentiment Analysis ( ABSA )   task . For a given sentence , ASTE aims to extract   the sentiment triplets , including aspect term , opin-   ion term and the corresponding sentiment polarity .   As shown in Figure 1 , ASTE could produce two   triplets from the given sentence .   For ASTE task , early methods adopt a two - stage   pipeline framework that first identifies aspects with   sentiment and opinions then pairs them , producingFigure 1 : A sentence and its sentiment triplets are shown   at the top half . Moreover , the primary difference be-   tween the traditional MRC and our COM - MRC is high-   lighted at the bottom .   the sentiment triplets ( Peng et al . , 2020 ) . However ,   these pipeline - based methods ignore the interac-   tion among triplets , which could result in the error   propagation . To alleviate this problem , some re-   cent studies jointly extract the sentiment triplets   in an end - to - end framework ( Xu et al . , 2020 ; Wu   et al . , 2020a ; Zhang et al . , 2020 ; Chen et al . , 2021b ;   Yan et al . , 2021 ) , which is constructed mainly by   designing a tagging scheme .   Very recently , Mao et al . ( 2021 ) and Chen et al .   ( 2021a ) formalized ASTE by using a machine read-   ing comprehension ( MRC ) framework . The ba-   sic idea of MRC - based methods is using multi-   turn QA under an identical context with diverse   queries . Specifically , MRC - based methods involve   two stages , Aspect Inference ( AI ) and Aspect Ac-   cessory Inference ( AAI ) . The former is to extract   aspect terms by constructing a query about aspects ,   e.g. , “ What aspects ? ” . The latter is to identify the   corresponding opinions and sentiment by construct-   ing queries for each aspect term , e.g. , “ What opin-   ions and sentiment given the aspect ambience ? ” .   Despite the impressive performance , however ,   MRC - based methods may suffer the interference   problem when analyzing sentences with multiple   aspects . Intuitively , the more aspects a sentence3230   contains , the harder it is usually to obtain the cor-   rect correspondence between aspects and their ac-   cessories . For the example in Figure 1 , the model   may mistakenly match the opinion “ overrated ” to   “ ambience ” . Moreover , the trained model equipped   with attention mechanism would usually capture   the potential correlation between aspects and opin-   ions . If the model pays attention to other aspects , it   will also attend to the opinions of these aspects ,   which will interfere with the opinion inference   process of the current aspect . As shown in the   example in Figure 1 , for the aspect “ ambience ” ,   the model may pay more attention to the incorrect   opinion “ overrated ” if the aspect “ place ” can be at-   tended . Note that statistics in Table 1 on benchmark   datasets show that remark sentences with multiple   aspect terms occupy a large portion . The sentiment   triplets in these sentences account for roughly a   half . Therefore , how to identify the information   from different aspects more effectively and further   alleviate the interference of unrelated aspects is   challenging . Motivated by above observations , we   present the idea of masking aspects for alleviating   the interference problem .   In this paper , we propose a novel framework   called COntext - Masked machine reading compre-   hension ( COM - MRC ) for ASTE . Our COM - MRC   framework is in general based on the idea of mask-   ing aspect , and it comprises three closely - related   components : a context augmentation strategy , a dis-   criminative model , an inference method . Firstly ,   to alleviate the interference and to better identify   information from different aspects , we use the idea   of masking aspects for context augmentation . We   argue that a sentence with multiple aspect terms   deserves to be treated as multiple training samples   due to the difficulty of extracting triplets . Hence ,   we set a regular query with various masked con-   texts to identify each aspect and its accessories .   For a sentence with taspect terms , the number of   samples grows from 1to2 . Thus , the training cor-   pus effectively expands . Secondly , to effectively   capture the correlation among sentiment triplets ,   we design a discriminative model . An aspect de - tection module detects whether there exist aspect   terms in the masked context . This module and the   other three modules , i.e. , aspect extraction , opinion   extraction , and sentiment classification modules ,   work collaboratively for ASTE task . Thirdly , the   aspect is extracted one by one from left to right dur-   ing AI stage through iteratively masking aspects .   Then , during AAI stage , all other unrelated aspects   are masked in the context for more precise iden-   tification . The three components constitute our   COM - MRC framework which could alleviate the   interference problem . Experimental results show   that our COM - MRC framework consistently out-   performs state - of - the - art methods .   Our contributions are summarized as follows .   •We propose a novel COM - MRC framework   for ASTE task . Our framework comprises three   components : a context augmentation strategy , a   discriminative model , and an inference method .   •We use the context augmentation strategy to   obtain effective expansion of the training corpus .   We design the discriminative model with four col-   laborative modules . We implement our inference   method by iteratively masking aspects .   •We conduct extensive experiments on two   groups of benchmark datasets . The experimental   results demonstrate the effectiveness of our COM-   MRC framework . The source code and data of our   work are released for knowledge sharing .   2 Proposed COM - MRC Framework   2.1 Problem Formulation   Given a sentence S={w , w , ... , w}with n   tokens , the aim of ASTE task is to extract all senti-   ment triplets Twithin the sentence . Each sentiment   triplet is represented as a tuple ( a , o , s ) , where sym-   bolsa , o , andsrepresent the aspect term , the opin-   ion term and the sentiment polarity , respectively .   The range of sentiment polarity is given as three   types , i.e. , s∈ { POS , NEU , NEG } .   2.2 Context Augmentation Strategy   Primarily , our discriminative model takes a fixed   query and a masked context as input . We then   adopt BERT ( Devlin et al . , 2019 ) as the sentence   encoder to represent their semantics .   Specifically , we devise a fixed query to prompt   our model for adapting ASTE task . Here , we iden-   tify the leftmost aspect and its corresponding opin-3231   ion terms . The query qis given as follows :   q=“Find the first aspect term and   corresponding opinion terms in the text ” ( 1 )   Strategy . For the contexts , we design an aug-   mentation strategy . Suppose that a sentence Scon-   sists of taspect terms . For each aspect term , we   perform two types of operations , i.e. , masking or   not masking . Thus , one training sentence expands   to2instances . This augmentation strategy is illus-   trated in Figure 2 .   Specifically , we mask the k - th token by setting   its attention score to 0 . A masking matrix Mis   accordingly defined as follows ,   M=/braceleftigg   −∞,ifj = k   0,otherwise(2 )   Then , we apply the matrix to the attention module   Ain BERT given the query Q , the key Kand the   value Vas follows ,   A(Q , K , V ) = softmax / parenleftbiggQK   √   d+M / parenrightbigg   V(3 )   where dis the dimension of the key .   With the fixed query qin Eq . ( 1)and a masked   context xproduced using the aforementioned strat-   egy , we then adopt BERT ( Devlin et al . , 2019 )   to represent their semantics . The specific inputis given as “ [ CLS ] q [ SEP ] x [ SEP ] ” . Suppose   that the query qcontains mtokens . Note that the   masked context xcontains the same length of to-   kensnas that of the sentence . We obtain the rep-   resentation h∈Rfrom the last BERT   block . The representations of the masked context   and the token [ CLS ] are denoted as h∈R   andh∈R , respectively .   2.3 Discriminative Model   Our discriminative model comprises four modules .   The structure is depicted in Figure 2 .   Aspect Extraction Module . To obtain the first   unmasked aspect term , motivated by span - based   methods ( Hu et al . , 2019 ) , we obtain the proba-   bilities for starting and ending positions from the   context representation has follows :   r = Wh ( 4 )   p= softmax ( Wr ) ( 5 )   p= softmax ( Wr ) ( 6 )   where W∈R , W∈R , and W∈   Rare trainable parameters . In addition , r∈   Rstands for the representation of aspect term .   Correspondingly , we use the cross - entropy as the   loss function for starting and ending positions . The3232aspect extraction loss Lis defined as follows :   L=−/summationdisplayylogp−/summationdisplayylogp(7 )   where yandy∈Rare ground truths of   starting and ending positions for the first unmasked   aspect term . The subscript idenotes the i - th token .   Opinion Extraction Module . To obtain all opin-   ion terms for the first unmasked aspect , we build   an opinion extraction module similar to aspect ex-   traction module . Thus , we obtain the opinion rep-   resentation rand the module ’s loss function L.   Sentiment Classification Module . Intuitively ,   the sentiment polarity is highly related to the   masked context , the aspect term , and the opinion   term . In our model , we use multi - head attention   ( Vaswani et al . , 2017 ) to fuse these three semantic   information . This process is formulated as follows :   r= LN ( h+ MultiHead ( h , r , r ) ) ( 8)   g= MP ( r ) ( 9 )   p= softmax ( Wg+b ) ( 10 )   where LN , MultiHead , and MP represent three   operations , i.e. , layer norm , multi - head attention   and max pooling , respectively . In addition , r∈   Randg∈Rare intermediate variables .   W∈Randbare the trainable weight and   bias , respectively . The cross entropy loss Lfor   the sentiment classification is then given as follows :   L=−/summationdisplayylogp ( 11 )   where y∈Ris the label of sentiment polarity .   Aspect Detection Module . This module is to de-   tect whether there exist aspect terms in the masked   context . For the context with all aspect terms be-   ing masked , its label is set False , otherwise True .   The module works according to the [ CLS ] token   representation h , aspect representation r , and   opinion representation ras follows :   r = h⊕MP(r)⊕MP(r ) ( 12 )   p= softmax ( Wr+b ) ( 13 )   where r∈Ris an intermediate variable . In   addition , W∈Randbare the trainable   weight and bias , respectively . The symbol ⊕means   the concatenation operation . We use the binary   cross entropy loss Lfor aspect detection module . Algorithm 1 Inference Algorithm   Loss Function . At last , our objective is to mini-   mize the following total loss :   L = αL+βL+γL+δL ( 14 )   where α , β , γandδare four hyper - parameters used   to adjust the influence of the corresponding losses .   2.4 Inference Method   To alleviate the interference from all the other   aspects , we present our inference method . Our   method involves two successive stages , AI and AAI .   AI stage is to extract all aspects ; AAI stage is to   identify the opinions and sentiment polarities for   all the aspects . In Figure 2 , we give an example to   illustrate the two stages .   During AIstage , firstly , we obtain the aspect   detection flag eand the first aspect term ausing   our trained model for the query qand the sentence   S. If the detection flag eis True , we append the   aspect ato aspect set Aand then mask aspect a   in the sentence S. With the query and the masked   context , we again use the trained model to obtain   the next aspect detection flag and the next aspect .   We repeat the above step until the detection flag e   is False . At last , we obtain the aspect set A.   During AAI stage , to produce the context for   one aspect term a , we mask all the aspects except   a. Combined with the fixed query q , the masked   context is fed into our trained model . Thus , for that   aspect term we obtain its opinion term set Oand the   corresponding sentiment polarity s. Finally , based   on the set O , we append all the triplets to the triplet   setT. The inference method is formally summa-   rized in Algorithm 1 . Here , S.Mask ( A)in Line   5 means that the sentence Sis updated to masked   context by masking all the aspects belonging to the   current aspect set A.3233   3 Experiments   3.1 Datasets   We conduct experiments on two groups of bench-   mark datasets for ASTE . These datasets were cre-   ated from the SemEval Challenges ( Pontiki et al . ,   2014 , 2015 , 2016 ) . The first group Dincluding   four subsets ( Rest 14 , Lap 14 , Rest 15 , and Rest   16 ) is annotated by Wu et al . ( 2020a ) . The second   groupDis proposed by Xu et al . ( 2020 ) , which   is a corrected version of dataset annotated by Peng   et al . ( 2020 ) . Table 2 shows the statistics of these   two groups of datasets .   3.2 Baseline Methods   We compare our COM - MRC with state - of - the - art   baselines . These baseline models are briefly catego-   rized into the following three groups . 1 ) Pipeline .   CMLA+ , RINANTE+ , Li - unified - R , and Peng - two-   stage are proposed by Peng et al . ( 2020 ) . Peng - two-   stage+IOG and IMN+IOG are proposed by Wu   et al . ( 2020a ) . 2 ) End - to - end . This group includes   OTE - MTL ( Zhang et al . , 2020 ) , JET - BERT ( Xu   et al . , 2020 ) , GTS ( Wu et al . , 2020a ) , SE(Chen   et al . , 2021b ) , Unified ( Yan et al . , 2021 ) , SPAN-   ASTE ( Xu et al . , 2021 ) and EMC - GCN ( Chen et al . ,   2022 ) . 3 ) MRC - based . BMRC ( Chen et al . , 2021a )   devises three types of queries to build the associa-   tions among different subtasks based on MRC .   3.3 Implementation Details   We use the Bert - Base - Uncased English version   as our base encoder . Our model is trained for 100   epochs with a linear warmup for 10 % of training   steps followed by a cosine decay of learning rate   to 0 . AdamW optimizer ( Loshchilov and Hutter ,   2019 ) is used with the maximum learning rate of9×10for BERT weights and weight decay of   10 . The batch size is 15 , and the dropout rate is   set to 0.1 . Considering the prediction performance   with masked contexts will be greatly affected if the   detected aspect terms are incorrect , we set a larger   weight for aspect extraction in the loss function for   more accurate identification of aspect term . Four   hyper - parameters α , β , γandδin Eq . ( 14 ) are   set to 8.0 , 3.2 , 1.0 and 1.0 respectively . We use a   heuristic multi - span decoding algorithm ( Hu et al . ,   2019 ) to obtain the aspect and opinion spans dur-   ing inference and the threshold is manually set . We   use a GeForce RTX 3090 to train the model for   an average of 0.85h . We save the model parame-   ters according to the model ’s best performance on   the development set . The reported results are the   averages on five runs with different random seeds .   3.4 Main Results   We compare our COM - MRC with other baselines   in terms of Precision , Recall and F1 scores . The   experimental results on DandDare reported   in Tables 3 and 4 , respectively . Under F1 met-   ric , our COM - MRC consistently outperforms all   pipeline , end - to - end and MRC - based methods on   the two groups of datasets . Note that our method   outperforms the best end - to - end method SPAN-   ASTE on D. We observe that the end - to - end and   MRC - based methods are more competitive than   the pipeline methods as they alleviate the error   propagation and establish the correlations between   related subtasks . Moreover , compared with another   strong MRC - based method , i.e. , BMRC , our COM-   MRC significantly surpasses its performance by an   average of 3.59 % and 4.18 % F1 - score on Dand   D , respectively . This improvement is attributed to   that our COM - MRC can effectively alleviate the   interference problem via a context augmentation   strategy , a discriminative model , and an inference   method . In addition , in order to show the signifi-3234   cance of our experimental results , we conduct pair-   wiset - test on F1 comparing our COM - MRC with   BMRC and EMC - GCN on two datasets , Dand   D. All of the produced p - values are less than 0.05 .   4 Analysis   4.1 On Context Augmentation Strategy   Is the strategy on context augmentation effective ?   We conduct experiments compared with another   two strategies . The linear strategy is only consid-   ering contexts to be used in the inference process .   For a sentence with taspects , this method produces   2tsamples . The NOP strategy is using the original   sentences without augmentation . For the exponen-   tial strategy used in our COM - MRC , we obtain   2samples for one sentence , described in Section   2.2 . Table 5 shows the experimental results . In   addition , the numbers of training samples via three   different context augmentation strategies are also   reported . We observe that our exponential strat-   egy achieves the significant performance compared   with the linear and NOP strategies . With the in-   crease of training samples , the performance grows   consistently . Note that the number of training sam-   ples using our exponential strategy grows up to   about 3.5 times on average . This would not cause   too much computational burden .   Furthermore , as reported in Table 6 , we observe   that the increment in the multi - aspect setting is3235   much larger than that in the single - aspect setting   when compare Exp with other strategies . To sum   up , the strategy of context augmentation in our   COM - MRC is effective .   4.2 On Discriminative Model   Are the modules in our discriminative model effec-   tive ? To this end , we conduct ablation experiments   onD. For aspect and opinion extraction modules ,   we remove the aspect representation and opinion   representation in Figure 2 , respectively . For the   aspect detection module , we remove the concatena-   tion in Eq . ( 12 ) using the [ CLS ] token representa-   tionh . For the sentiment classification module ,   we remove the sentiment attention in Eq . ( 8)using   only the context representation h. The experimen-   tal results are reported in Table 7 . We observe that   the sentiment attention has the largest impact , re-   sulting in a 1.50 % decrement on the performance .   This shows our attention mechanism effectively   fuse the semantic information within aspects and   opinions . In addition , the performances of the other   three variants decrease in some degree . To sum up ,   all four modules in our model contribute to the   superior performance on ASTE task .   4.3 On Inference Method   Is our inference method effective ? First , we show   two versions of inference method . These two meth-   ods involve an identical AI stage but a different   AAI stage . In Figure 3 , we illustrate the two AAIs .   The left denoted as AAI 1 is a naïve version which   only masks necessary aspects . The right denoted as   AAI 2 is the one used in our COM - MRC . In AAI 1 ,   to obtain the opinions and the sentiment of each as-   pect , the aspects are masked one by one from left to   right . However , the current aspect processing stage   could be disturbed by the subsequent aspects . As   shown in Figure 3 , the aspect “ ambience ” process-   ing stage is disturbed by “ place ” . Thus , the term   “ overrated ” is mistakenly identified as an opinion   of “ ambience ” .   Furthermore , we conduct experiments using AAI   1 and AAI 2 based on COM - MRC framework . We   consider both the single- and multi - aspect settings   based on the discriminative model for D. The   experimental results are shown in Table 8 . These   two AAIs perform identically in the single - aspect   setting . However , in the multi - aspect setting , AAI   2 outperforms AAI 1 significantly ( a maximum in-   crement 3.77 % on Rest 15 ) on all four datasets . It   indicates that other aspects would cause much in-   terference and masking other aspects can alleviate   the interference effectively . Note that the reason   why not directly mask opinions is that an opinion   may match multiple aspects and masking one opin-   ion will blind the opinion extraction of all other   aspects corresponding to it . Furthermore , the ex-   perimental results verify that our inference method   in COM - MRC is effective especially for sentences   with multiple aspects .   4.4 On Query   Is our query qeffective ? To answer this question ,   we conduct experiments with three types of queries .   The first is the regular query adopted in our COM-   MRC . The second is an improper query by remov-   ing the keyword “ first ” . The third is null which   means no query is provided . The experimental re-3236   sults are reported in Table 9 . The performance of   the improper query decreases by a mean 1.26 % .   Compared with the improper query , a null query   drops much more with a mean decrement of 2.60 % .   This shows the effectiveness of our query .   4.5 Attention Visualization   To show the effective treatment of the interference   problem , we visualize the attention matrices , which   imply the opinion information on the first aspect   under the regular query and the masked context .   Consider the sentence with two opposite polarities ,   “ good food , bad decor , great customer service , bad   manager ” . As shown in Figure 4(a ) for identifying   the opinion term of “ food ” , both subfigures show   that major attention is paid to the golden opinion   “ good ” . However , the left indicates there exists   non - negligible attention on the incorrect opinions ,   especially on “ bad ” . In contrast , the right shows if   other aspect terms are masked , the attention on in-   correct opinions could be drastically reduced . Sim-   ilarly , Figure 4(b ) shows the attention on incorrect   opinions , especially on “ great ” can be reduced if   other aspects are all masked . In addition , we ob-   serve that the span “ corresponding opinion terms ”   in our query has high attention scores with golden   opinions . To sum up , masking other aspects can   effectively help identify current aspect information .   4.6 Case Study   In Table 10 , we show several cases with multiple   aspects to compare our COM - MRC with BMRC .   In the first example , both methods correctly ex-   tract the aspect terms “ ambience ” and “ place ” with   their opinion terms “ Nice ” and “ overrated ” , respec-   tively . However , BMRC fails to correctly identify   the sentiment polarity of “ place ” . Note that the   sentiment polarity of “ ambience ” is also positive .   In the second example , BMRC could extract a in-   correct triplet . Here , two aspect terms , “ price ” and   “ shipping ” do not share the opionion term “ great ” .   5 Related Work   ABSA generally comprises three subtasks : Aspect   Term Extraction ( ATE ) ( Hu and Liu , 2004 ; Yin   et al . , 2016 ; Li et al . , 2018b ; Xu et al . , 2018 ; Ma   et al . , 2019 ; Chen and Qian , 2020 ; Wei et al . , 2020 ) ,   Aspect Sentiment Classification ( ASC ) ( Wang et al . ,   2016b ; Tang et al . , 2016 ; Ma et al . , 2017 ; Fan et al . ,   2018 ; Li et al . , 2018a ; Zhang et al . , 2019 ; Sun   et al . , 2019 ; Wang et al . , 2020 ; Li et al . , 2021 ) and   Opinion Term Extraction ( OTE ) ( Yang and Cardie ,   2012 , 2013 ; Fan et al . , 2019 ; Wu et al . , 2020b ) .   The studies ignore the correlations between these   subtasks .   Some subsequent studies devoted to couple two   subtasks . These works mainly are grouped into   two tasks : Aspect and Opinion Term Co - Extraction   ( AOTE ) ( Wang et al . , 2016a , 2017 ; Dai and Song ,   2019 ; Wang and Pan , 2019 ; Chen et al . , 2020 ; Wu   et al . , 2020a ) and Aspect - Sentiment Pair Extrac-   tion ( ASPE ) ( Ma et al . , 2018 ; Li et al . , 2019a , b ;   He et al . , 2019 ) . Most recently , ASTE as a new   variant of ABSA has received wide attention . Peng   et al . ( 2020 ) originally proposed a pipeline method   that identifies aspects with sentiment and opinions   independently then pairs them for forming senti-   ment triplets . End - to - end approaches ( Xu et al . ,   2020 ; Wu et al . , 2020a ; Zhang et al . , 2020 ; Chen   et al . , 2021b ; Yan et al . , 2021 ) are then proposed .   Another mainstream framework is the MRC based3237   methods ( Mao et al . , 2021 ; Chen et al . , 2021a ) ,   whereas they are susceptible to interference from   the existence of multiple aspect terms .   6 Conclusion and Future Work   In this paper , we propose a novel COntext - Masked   MRC ( COM - MRC ) framework to alleviate the in-   terference problem in the ASTE task . Our COM-   MRC comprises three close - related components .   The context augmentation can effectively expand   the training corpus . The discriminative model com-   prising four modules works collaboratively . Our   inference method involving two stages can effec-   tively reduce the interference from other aspects .   Extensive experiments on two groups of bench-   mark datasets demonstrate the effectiveness of our   COM - MRC framework . In the future , we will de-   vote to devising a one - stage method with a faster   inference .   Limitations   In our COM - MRC framework , we design a con-   text augmentation strategy . This strategy produces   the masked context from 1to2for one sentence .   This could increase the training samples achieving   remarkable performance . On the other side , this   would increase the training time of the discrimi-   native model . Therefore , this would prevent our   COM - MRC from applying to the scenarios with   large - scale data .   Acknowledgements   This work was supported in part by the Na-   tional Key R&D Program of China under Grant   2019YFF0303302 and in part by the National Nat-   ural Science Foundation of China under Grant   62076032 . We appreciate constructive feedback   from the anonymous reviewers for improving the   final version of this paper . References3238323932403241