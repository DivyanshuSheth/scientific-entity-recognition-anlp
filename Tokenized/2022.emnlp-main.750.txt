  Yuning Mao , Ming Zhong , Jiawei Han   University of Illinois Urbana - Champaign   { yuningm2 , mingz5 , hanj}@illinois.edu   Abstract   Scientific extreme summarization ( TLDR ) aims   to form ultra - short summaries of scientific pa-   pers . Previous efforts on curating scientific   TLDR datasets failed to scale up due to the   heavy human annotation and domain expertise   required . In this paper , we propose a simple   yet effective approach to automatically extract-   ing TLDR summaries for scientific papers from   their citation texts . Based on the proposed ap-   proach , we create a new benchmark CiteSum   without human annotation , which is around 30   times larger than the previous human - curated   dataset SciTLDR . We conduct a comprehen-   sive analysis of CiteSum , examining its data   characteristics and establishing strong base-   lines . We further demonstrate the usefulness   of CiteSum by adapting models pre - trained on   CiteSum ( named CTS ) to new tasks and do-   mains with limited supervision . For scientific   extreme summarization , CTSoutperforms   most fully - supervised methods on SciTLDR   without any fine - tuning and obtains state - of - the-   art results with only 128 examples . For news   extreme summarization , CTSachieves sig-   nificant gains on XSum over its base model ( not   pre - trained on CiteSum ) , e.g. , +7.2 ROUGE-   1 zero - shot performance and state - of - the - art   few - shot performance . For news headline gen-   eration , CTSperforms the best among unsu-   pervised and zero - shot methods on Gigaword .   1 Introduction   Scientific summarization typically regards paper   abstract as the ground - truth summary , as it is writ-   ten by the authors themselves with relatively high   quality and readily available in most scientific docu-   ments . However , paper abstract may not always be   the ideal summary because it often involves certain   details such as task description , background infor-   mation , and experiment results ( cf . the abstract of   this paper ) . As a result , recent work ( Cachola et al . ,   Table 1 : An example showing that the citation texts of a   paper can often be used as its ultra - short summary .   2020 ) has studied the problem of scientific extreme   summarization , which aims at forming ultra - short   summaries ( usually one sentence ) of the papers ,   namely the TLDRsummaries .   However , unlike paper abstracts , ultra - short pa-   per summaries are far from being universally avail-   able . Only certain scientific venues such as Open-   Review.net support a TLDR field during paper sub-   mission , which is completely optional , and not all   submitted papers provide such information . In ad-   dition , human - annotated summaries of scientific   documents are rather costly and require domain ex-   pertise . As a consequence , the previous SciTLDR   dataset ( Cachola et al . , 2020 ) , using a combination   of author - provided TLDR and human - annotated   TLDR ( rephrased from paper reviews on OpenRe-10922view ) , only collected around 2,000 examples for   training and 600 for testing .   In this paper , we argue that citation texts can   often serve as high - quality short summaries of the   cited papers . In Table 1 , we show the abstract of   one paper and its citation sentence in a follow - up   paper . We observe that the citation sentence intro-   duces the cited method and its contributions in a   concise and accurate manner . Motivated by such   observations , we propose a simple yet effective   approach to locating , extracting , and filtering ci-   tation texts from scientific papers . We then treat   the processed citation texts as ground - truth sum-   maries of the cited papers . Based on the proposed   approach , we create a large - scale scientific extreme   summarization benchmark , CiteSum , which is au-   tomatically derived from citation texts and around   30 times larger than the previous human - annotated   dataset SciTLDR ( Cachola et al . , 2020 ) .   We conduct a comprehensive analysis of Cite-   Sum regarding its data characteristics and quality ,   meanwhile establishing strong baselines as the ref-   erence for future studies . We further verify the use-   fulness of CiteSum by demonstrating that models   pre - trained on CiteSum , which we name as CTS   ( Citation Text - guided Summarizer ) , exhibit supe-   rior generalizability during low - resource adaptation   to new tasks and domains .   On the human - annotated scientific extreme sum-   marization dataset SciTLDR ( Cachola et al . , 2020 ) ,   our zero - shot BART - based ( Lewis et al . , 2020 )   CTS , without any fine - tuning , performs bet-   ter than most fully - supervised baselines , includ-   ing the fully - supervised BART model ( without   pre - training on CiteSum ) . Our few - shot CTS   achieves state - of - the - art performance with only   128 labeled examples from SciTLDR . In addition ,   CTSoutperforms its base model ( BART ) on two   more diverse scientific tasks – discipline classi-   fication and title generation . When transferring   to news extreme summarization , despite the do-   main mismatch , CTSachieves significantly bet-   ter zero - shot performance than BART and PEGA-   SUS ( Zhang et al . , 2020 ) ( e.g. , +7.2 ROUGE-1 )   and state - of - the - art few - shot performance on the   XSum dataset ( Narayan et al . , 2018 ) . Furthermore ,   CTSperforms the best among unsupervised and   zero - shot methods on the Gigaword news headline   generation dataset ( Rush et al . , 2015 ) .   Contributions . ( 1 ) We propose a simple yet ef-   fective approach to automatically extracting ultra-   short paper summaries from citation texts . ( 2 )   Based on the proposed approach , we create a large-   scale scientific extreme summarization benchmark   CiteSum and conduct a comprehensive analysis of   it . ( 3 ) We further verify the quality and useful-   ness of CiteSum by demonstrating that models pre-   trained on CiteSum perform very well on new tasks   and domains such as news extreme summarization   and headline generation with limited training .   2 CiteSum : A Large - scale Scientific   Extreme Summarization Benchmark   2.1 Data Creation   Data Source We take the publicly available Se-   mantic Scholar Open Research Corpus ( S2ORC )   ( Lo et al . , 2020 ) as the source for data creation . In   the latest version of S2ORC , there are 136 M scien-   tific papers from different academic disciplines and   the number of papers with full - text access is 12M.   We further remove papers without citation informa-   tion , resulting in 9 M papers as the candidates .   Quality Control Not all citation texts are of high   quality and can be used as summaries of the cited   papers . In Table 2 , we show two examples ( in   our paper ) where the citation sentence simply ( 1 )   describes the data source or ( 2 ) introduces the dif-   ference of the citing paper from the cited paper . We   note that prior studies on citation text generation   ( Chen et al . , 2021 ; Ge et al . , 2021 ) often do not   filter these citation texts and simply treat all para-   graphs / sentences with citations as the ground - truth   labels , as their goals are not on paper summariza-   tion but writing assistance .   To ensure data quality , we carefully locate , ex-   tract , and filter the citation texts of papers in the fol-   lowing manner . First , we only take citation texts in   the Related Work section of a paper , which largely   ensures that they describe the content of the cited   paper instead of irrelevant information , such as task10923   background in the Introduction section or the imple-   mentation details in the Experiment section . After   filtering papers without a Related Work section ,   there are around 288 K papers left .   Second , we only keep citation sentences that cite   a single paper , since those with multiple citations   typically discuss one line of work and can not be   used as the summary of a specific paper . In total ,   we obtain about 426 K citation sentences .   Next , we measure the similarity between the ci-   tation texts and the cited papers and filter dissimilar   pairs . Intuitively , if a citation sentence can serve as   a high - quality summary , certain amount of its con-   tent should be from the cited paper . Prior work ( Lu   et al . , 2020 ) also showed that authors tend to cite   a paper using the information in the abstract of   the cited paper . We thus calculate the overlap be-   tween paper abstracts and their citation sentences ,   and filter those below a threshold T. We set Tto   50/20/40 for ROUGE-1/2 / L recall through manual   examination , resulting in a ROUGE-1/2 / L recall of   73.1/39.4/58.5 after filtering . As a reference , the   ROUGE-1/2 / L recall between paper abstracts and   reference summaries on SciTLDR ( Cachola et al . ,   2020 ) is 81.1/38.9/62.0 and 65.2/17.9/45.7 for   author - provided ( SciTLDR - Auth ) and peer review-   derived ( SciTLDR - PR ) TLDR , respectively . That   is , the abstraction level of CiteSum is between   SciTLDR - Auth and SciTLDR - PR . This filtering   step is rather strict as we prefer quality to quantity   of the data and only 93 K of the 426 K examples   ( 21.8 % ) are kept .   We further replace each citation span ( e.g. , “ Taig-   man et al . [ 8 ] ” ) with a special token “ REF ” as they   vary in different papers but essentially have the   same meaning ( i.e. , referring to a cited paper).Dataset Split After data filtering and preprocess-   ing , there are 92,946 examples in the final citation   text - guided summarization dataset , which we name   as CiteSum . We take about 5 % of the data as the   validation and test sets respectively , and the remain-   ing 90 % as the training set . As one paper may be   cited multiple times in different papers , we ensure   that there is no label leakage by excluding papers   used for evaluation from the training set .   2.2 Data Analysis   Dataset Statistics In Table 3 , we show the data   statistics of CiteSum and other relevant summa-   rization datasets . In terms of data size , CiteSum   is about half the size of other automatically con-   structed datasets like XSum ( Narayan et al . , 2018 )   and arXiv ( Cohan et al . , 2018 ) due to the avail-   ability of citation texts and our strict quality con-   trol . On the other hand , the size of CiteSum is   much larger than human - annotated datasets on pa-   per summarization ( Yasunaga et al . , 2019 ; Cachola   et al . , 2020 ) – almost 30 times larger than the Sc-   iTLDR dataset ( Cachola et al . , 2020 ) .   When compared to SciTLDR , the average length   of source documents in CiteSum is longer , while   that of the reference summaries is similar as the ma-   jority of summaries in SciTLDR also involve one   sentence . When compared to XSum , the summary   length in CiteSum is also quite similar . However ,   the inputs in XSum are news articles instead of   scientific papers and the input lengths also vary .   As for Gigaword ( Rush et al . , 2015 ) , a news head-   line generation dataset , both its source input and   target output are much shorter than CiteSum . De-   spite such differences , we observe that our models   pre - trained on CiteSum transfer very well to these   datasets in zero - shot and few - shot settings ( Sec . 4).10924   Discipline Analysis In Fig . 1 , we show the disci-   pline distribution of papers in CiteSum . The disci-   pline information is derived from the field of study   in Microsoft Academic Graph ( MAG ) ( Shen et al . ,   2018 ) . We take the top field of study for each paper   if there are multiple . We note that the discipline dis-   tribution in CiteSum is quite different from its data   source S2ORC ( Lo et al . , 2020 ) where medicine   and biology dominate . In contrast , most papers in   CiteSum are in computer science . The shift in dis-   cipline distribution is because we explicitly keep   papers with a Related Work section , where around   82.8 % are computer science papers . We then take   the citation texts in the above papers , which largely   lead to papers in similar disciplines . As a result ,   most papers in CiteSum are from computer science ,   mathematics , and engineering .   Citation Analysis In Fig . 2 , we show the average   number of citations for papers in CiteSum . Note   that the citation count shown does NOT reflect the   total number of citations due to data filtering , but   how many times a paper appears in CiteSum as   examples ( with the same input and different cita-   tion sentences as target output ) . In total , there are   59,707 unique papers in CiteSum with an average   citation of 1.56 , and 98 % of the papers have fewer   than 5 citations . Compared to prior work , we do   not only target popularly cited papers ( Yasunaga   et al . , 2019 ) and use different citation texts as differ-   ent training examples instead of multiple reference   summaries ( Cachola et al . , 2020 ) .   Human Evaluation We randomly sample 50 ex-   amples from CiteSum and ask two human anno-   tators with a background in computer science to   examine whether the citation sentences can serve   as high - quality summaries of the papers . Similar   to Cachola et al . ( 2020 ) , we use a 4 - point scale for   evaluation with 1 - false or misleading , 2 - partially   accurate , 3 - mostly accurate , and 4 - accurate . The   rating distribution is listed in Table 4 . 80 % citation   sentences are considered ( mostly ) accurate to be   used as summaries of the cited papers . On the other   hand , there are still 10 % misleading summaries ,   which we argue is quite common in automatically   created summarization datasets ( Mao et al . , 2020a ) .   We show 4 examples corresponding to each rating   in App . A. We will further verify the quality of   CiteSum by adapting models pre - trained on it to   new tasks and domains ( Sec . 4 ) .   3 Experiments on CiteSum   In this section , we experiment on CiteSum with   state - of - the - art baselines and analyze their perfor-   mance under different setups to provide references   for future studies . Implementation and training   details are provided in App . B.   3.1 Examined Methods   We use BART - large ( Lewis et al . , 2020 ) and   PEGASUS - large ( Zhang et al . , 2020 ) as the base   models as they are the state - of - the - art methods on   multiple summarization datasets . We examine the   base models with different inputs such as paper   abstract ( Abs ) , abstract+introduction+conclusion10925(AIC ) , and abstract+title . In addition to using the   TLDR ( citation text ) as the only generation target ,   we evaluate two multi - task settings with paper title   and discipline ( Disci)as the targets , where differ-   ent prefix tokens are added to the input such that   the model can generate different targets given the   same paper abstract as input ( Cachola et al . , 2020 ) .   We further evaluate the following extractive base-   lines . E - L : a method that takes the first sen-   tence of the paper abstract , which performs fairly   well in news summarization . E - H : a   heuristic method that looks for the first sentence   containing “ propose ” , “ introduce ” , or “ in this pa-   per ” , as such sentences likely reflect the contribu-   tion of the paper . It falls back to E - L if no   such sentences are found . E - O : an up-   per bound that matches each sentence in the paper   abstract with the reference summary and takes the   sentence with the highest ROUGE-2 F1 .   3.2 Results   In Table 5 , we show the results of various base-   line methods on CiteSum . When given paper ab-   stract as the source document , PEGASUS performs   worse than BART and we thus use BART as the   major model in the following experiments . Further   adding paper introduction and conclusion to the   model input slightly improves model performance ,   at the expense of longer training time and increased   memory usage . The gains brought by adding ti-   tle and discipline information to model input are   marginal , while using them for multi - task learning   does not lead to clearly better results . The fact   that methods proposed by recent studies such as   multi - task learning ( Cachola et al . , 2020 ) perform   ineffectively on CiteSum indicates that CiteSum   remains an unresolved and challenging scenario .   For the extractive baselines , E - L per-   forms significantly worse than that in the news   domain ( Mao et al . , 2020a ) . E - H im-   proves upon E - L drastically and yet lags   behind state - of - the - art methods by a large margin .   E - O performs the best , the performance   of which is generally consistent with the numbers   on the human - annotated SciTLDR dataset ( Cachola   et al . , 2020 ) . On the other hand , the fact that   abstractive methods have approached the extrac-   tive upper bound indicates that more abstraction is   needed to further improve model performance on   CiteSum .   We believe that CiteSum provides a well-   established testbed for future studies on ( scientific )   extreme summarization . The following future di-   rections may be worth exploring : 1 ) how to better   understand the structure and content of scientific pa-   pers with domain knowledge ( via relevant papers ,   terminology , taxonomies , etc ) ; 2 ) how to better   capture the differences in writing styles across var-   ious domains ; and 3 ) how to improve the saliency ,   factual correctness , and explainability of TLDR   summaries given their conciseness .   4 Transferring to New Tasks and   Domains with CTS   To further verify the quality and usefulness of Cite-   Sum , we adapt models pre - trained on CiteSum to   new tasks and domains , some of which are rather   different from CiteSum and make model transfer   with limited supervision very challenging .   Specifically , we name our pre - trained model   asCTS(Citation Text - guided Summarizer ) .   CTSuses the simplest form in Sec . 3 with pa-   per abstract as input and TLDR as target output .   We evaluate CTSon various downstream tasks   with no fine - tuning ( zero - shot ) or limited training   examples ( few - shot ) , including scientific extreme   summarization on SciTLDR ( Cachola et al . , 2020 ) ,   news extreme summarization on XSum ( Narayan   et al . , 2018 ) , and news headline generation on Giga-   word ( Rush et al . , 2015 ) . Additionally , we evaluate   CTSon two more diverse tasks in the scientific   domain , namely discipline classification and title   generation , in a fully - supervised setting.109264.1 Scientific Extreme Summarization   Setup SciTLDR ( Cachola et al . , 2020 ) , the   human - annotated scientific extreme summarization   dataset , is an ideal testbed for further verifying   the quality and usefulness of CiteSum since they   both target extreme summarization , belong to the   scientific domain ( though CiteSum involves more   disciplines ) , and share similar input / output formats   ( though CiteSum has slightly longer inputs ) . One   noticeable difference , however , is the point of view   of the summaries – in SciTLDR the reference sum-   maries typically start with “ We ” or “ This paper ” ,   while in CiteSum they often begin with “ Author-   Name et al . ” ( replaced by a special token “ REF ”   during preprocessing ) .   We propose two simple techniques to tackle such   subtle style differences when adapting CTSto   SciTLDR in a zero - shot setting without fine - tuning .   The first technique is post - processing : we replace   “ REF ” with “ This paper ” if the summary begins   with “ REF ” and remove all other “ REF ” within the   summary . The second technique is prompting : we   use “ This paper ” as a prompt in the model decoder   such that the summary always starts with “ This pa-   per ” . Similarly , in the few - shot setting , we replace   the leading “ We ” with “ This paper REF ” in the   reference summaries of SciTLDR ( on the training   set only ) to alleviate the style mismatch .   We use BART - large ( Lewis et al . , 2020 ) as the   base model of CTSsince most baselines on Sc-   iTLDR , including the state - of - the - art methods , use   the same base model .   Zero - shot Results In Table 6 , we show the   performance comparison of different methods   on SciTLDR . In the zero - shot setting , CTS   ( post - processing ) outperforms competitive fully-   supervised baselines such as BERTSum ( Liu and   Lapata , 2019 ) . CTS(prompting ) performs even   better than CTS(post - processing ) , outperform-   ing the fully - supervised BART model it is based   upon . Such results demonstrate the benefits of pre-   training on CiteSum . CTS(prompting ) , without   any fine - tuning , is also on par with the state - of - the-   art method C ( Cachola et al . , 2020 ) , while   slightly worse than C , which pre - trains   on the XSum dataset ( Narayan et al . , 2018 ) first .   We additionally test a zero - shot upper bound for   CTSby providing our prompting model with   thefirst 3 tokens in the reference summary ( the   most common ones are “ We propose a ” and “ We   present a ” ) such that it knows how to start to sum-   marize and ( hopefully ) which aspect to focus on .   CTS(prompting , gold 3 tokens ) achieves com-   petitive ROUGE-1 and significantly better ROUGE-   2 / L than the extractive upper bound E - O   that has access to the entire reference summary .   Few - shot Results In the few - shot setting , CTS   with 32 examples improves over its zero - shot coun-   terpart . Furthermore , 128 - shot CTSoutperforms   all fully - supervised methods and achieves new   state - of - the - art results on SciTLDR . In contrast ,   a 128 - shot BART model without first pre - training   on CiteSum largely lags behind , performing even   worse than our zero - shot CTS . Such results again   show the effectiveness of our pre - training strategy   and the quality of CiteSum despite being automati-   cally created thanks to our quality control .   Data Overlap To ensure that the superior gener-   alizability of CTSdoes not merely come from   data leakage , we detect the overlap between Cite-   Sum and SciTLDR . We consider two papers ( near )   identical if their TF - IDF cosine similarity is greater   than 0.9 and find that only 9.7 % papers in the test   set of SciTLDR appear in the training set of Cite-   Sum . Also , note that the training labels in CiteSum   are automatically extracted citation sentences and   different from SciTLDR .   4.2 Scientific Discipline Classification and   Title Generation   We have demonstrated the effectiveness of CTS   on the task of scientific extreme summarization.10927   Next , we explore the feasibility of transferring   CTS to more diverse tasks .   Setup We evaluate CTSwith the task of sci-   entific discipline classification and title generation .   Similar to the multi - task experiments in Sec . 3 , we   use the same dataset split and model input , while   replacing the generation target from summaries ( ci-   tation texts ) to the discipline or title of the papers .   Examples with unavailable discipline or title are   removed . We use BART - large as the base model for   this experiment and compare BART with CTS   in an apple - to - apple comparison .   Results In Table 7 , we show the performance   comparison on title generation and discipline clas-   sification . CTSconsistently outperforms BART   on both tasks , although the differences are not as   significant as in other low - resource transfer exper-   iments . The moderate gains are possibly because   there is abundant training data for the two tasks   and continuous pre - training thus does not help   much . As evidence , the ( unweighted ) Macro - F1 of   CTSis considerably better than BART , which   we found is because CTSperforms well on those   disciplines with fewer examples . Regarding the   Weighted - F1 , CTSis only slightly better as most   papers belong to a single discipline ( computer sci-   ence ) that dominates the score .   4.3 News Extreme Summarization   Setup With the success on different tasks in the   scientific domain , we next evaluate CTSon a   more difficult setting where the domain is signifi-   cantly different while the task is still extreme sum-   marization . We take the XSum dataset ( Narayan   et al . , 2018 ) in the news domain for this purpose .   We mainly use PEGASUS - large ( Zhang et al . ,   2020 ) as the base model of CTSas its fully-   supervised version holds the state - of - the - art results   on XSum . We additionally evaluate CTSin   the zero - shot setting , which is the variant used for   title generation in Sec . 4.2 .   Zero - shot Results In Table 8 , we show the re-   sults on XSum with various training data sizes . In   the zero - shot setting , CTSsignificantly improves   over its base model PEGASUS ( +7.2 ROUGE-1 ) .   In addition , CTSis on par with other pre - trained   models such as BART - LB and T5 - LB ( Zhu et al . ,   2021a ) , which are specifically designed to leverage   the lead bias in the news domain and require much   more resources ( 32 vs. 1 GPU , 21.4 M vs. 83 K   training examples ) for summarization pre - training .   CTSfurther improves over CTSand out-   performs most zero - shot baselines ( +8.9 ROUGE-1   over PEGASUS ) . CTSdoes not outperform   WikiTransfer ( Fabbri et al . , 2021 ) , which is some-   what expected as WikiTransfer carefully prepares   its pre - training data to specific downstream tasks   given , e.g. , summary length and its level of abstrac-   tion . Unlike WikiTransfer ( Fabbri et al . , 2021 ) ,   CTSdoes not involve any downstream task-   specific data selection or model tuning – we use   the same CiteSum corpus in all the experiments .   Few - shot Results When given a few examples   for fine - tuning , CTSquickly adapts to the new   task despite the domain mismatch during pre-   training . We observe that CTSconsistently out-   performs not only its base model but all other base-10928   line methods , including WikiTransfer , and achieves   state - of - the - art few - shot performance on XSum .   In particular , CTSperforms better than fully-   supervised methods such as BERTSum ( Liu and   Lapata , 2019 ) with only 100 examples .   4.4 News Headline Generation   Setup To take a step further , we study the transfer   performance of CTSto news headline genera-   tion . We use the Gigaword headline generation   dataset ( Rush et al . , 2015 ) for this evaluation . We   again consider two variants of CTS , one pre-   trained with citation texts as the generation target   and the other further pretrained with paper titles   as in Sec . 4.2 . We use BART - large ( Lewis et al . ,   2020 ) as the base model in this evaluation .   Results In Table 9 , we show the results of vari-   ous methods on news headline generation . CTS   again outperforms its base model ( BART ) signifi-   cantly and achieves competitive performance with   most unsupervised and zero - shot methods designed   for news summarization ( Zhang et al . , 2020 ; Zhu   et al . , 2021a ) . CTSfurther achieves state - of-   the - art zero - shot performance despite pre - training   on the scientific domain , demonstrating the gener-   alizability and usefulness of CiteSum .   5 Related Work   Citation Text Generation There have been prior   studies utilizing citation texts for different purposes .   One popular line of work focuses on the generation   of the citation texts for writing assistance or paper   comparison ( Xing et al . , 2020 ; Luu et al . , 2021;Chen et al . , 2021 ; Ge et al . , 2021 ) . However , they   typically do not distinguish the citation texts that   can serve as summaries of the cited paper from   those used for other purposes , e.g. , background or   result comparison ( Cohan et al . , 2019 ) . For exam-   ple , Chen et al . ( 2021 ) treat citation text generation   as a multi - document summarization task , where   the target output is a paragraph with more than two   citations and the model input is the abstracts of all   cited papers . There is no filtering regarding the   citation texts and all the paragraphs with enough   citations are included . Besides including citation   texts with various intents and the lack of quality   control , prior studies differ from CiteSum in that   they target longer outputs , e.g. , multiple sentences   ( Xing et al . , 2020 ) or the entire Related Work sec-   tion ( Lu et al . , 2020 ; Chen et al . , 2021 ) .   Citation Text for Paper Summarization An-   other line of work does not generate but extracts   the citation texts and either uses them to form a   summary directly ( Nakov et al . , 2004 ; Abu - Jbara   and Radev , 2011 ; Qazvinian et al . , 2013 ) or treats   them as a bridge to the cited paper ( Cohan and Go-   harian , 2015 ; Yasunaga et al . , 2019 ) . Specifically ,   the citation texts in the latter studies are used to   find relevant contexts in the cited paper ( called cita-   tion contexts ) . Then , a long summary is formulated   primarily using the cited paper , e.g. , by selecting   sentences from the citation contexts ( Cohan and   Goharian , 2015 ) . Unlike CTS , prior citation-   based summarization methods require ( often mul-   tiple ) citation texts of a paper as input , which are   unavailable for new papers . In addition , they do   not target ultra - short but abstract - long summaries .   Extreme Summarization Extreme summariza-   tion aims to form ultra - short summaries of the   documents . Notable benchmarks in this direction   include XSum ( Narayan et al . , 2018 ) and New-   SHead ( Gu et al . , 2020 ) in the news domain , Sc-   iTLDR ( Cachola et al . , 2020 ) in the scientific do-   main , and Webis - TLDR-17 ( Völske et al . , 2017 )   for social media summarization . Compared to Sc-   iTLDR , our CiteSum dataset is significantly larger   in scale , from more venues than OpenReview , and   composed of various disciplines .   Summarization with Limited Supervision Our   work is also related to unsupervised and zero / few-   shot summarization that constructs weakly super-   vised guidance signals using e.g. , data characteris-   tics ( Chu and Liu , 2019 ; Mao et al . , 2020b ) , domain10929knowledge ( Zhu et al . , 2021b ) , or pseudo labeled   data ( Yang et al . , 2020 ; Zhong et al . , 2022 ) . Com-   pared to prior studies , CTSshows great cross-   domain capability that has not been well explored .   6 Conclusion   In this paper , we propose a simple yet effective   approach to automatically extracting ultra - short pa-   per summaries from citation texts . Based on the   proposed approach , we create a large - scale , high-   quality benchmark for scientific extreme summa-   rization . We conduct a comprehensive analysis on   the created benchmark and further demonstrate that   models pre - trained on it exhibit superior general-   izability to new tasks and domains such as news   extreme summarization and headline generation   with limited supervision .   Limitations   Regarding data collection , while we have taken   multiple steps to improve data quality , as in all   automatically created datasets , there are still low-   quality examples . We show some examples of low   quality in App . A. Limiting citation texts to the   Related Work section improves data quality , but   also excludes the majority of available citation sen-   tences and makes CiteSum concentrated in the field   of computer science and engineering .   Regarding model performance , our transfer ex-   periments are performed in scientific and news do-   mains . While promising , there is no guarantee that   CTSworks well in other domains . Also , with   abundant in - domain training data , pre - training on   CiteSum may not lead to significant improvements .   References109301093110932A Data Examples in CiteSum   In Tables 10 and 11 , we show four data examples   in CiteSum corresponding to different ratings in   the human evaluation . While some of the examples   are still of low quality after quality control , most of   the filtered citation texts can serve as high - quality   summaries .   B Implementation Details   Official results of the baselines are taken from prior   studies when possible . Model checkpoint selection   is done on the validation set for every task . The   special token “ REF ” ( used to indicate citation span )   is removed from model output for all transfer ex-   periments . Paper abstract is used as input for all   compared methods on SciTLDR . We experimented   with other prompts such as “ We ” and “ In REF ” but   found “ This paper ” works the best . FP16 is used in   most experiments for training efficiency except for   pre - training PEGASUS on CiteSum , with which   it failed to learn . We use a batch size of 8 . Hy-   perparameters like min / max generation length are   generally set following prior work ( Zhang et al . ,   2020 ) .   All the experiments are conducted with 1 Nvidia   RTX A6000 GPU . Pre - training on CiteSum only   takes about 6.5h for BART and 10h for PEGASUS .   The transfer experiments typically take less than   1h ( time mostly spent on evaluation ) as we use   very few labeled data for training . The codebase   is based on Huggingface transformers ( Wolf et al . ,   2020).109331093410935