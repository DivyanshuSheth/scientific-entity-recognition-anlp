  Xuming Hu , Zhijiang Guo , Guanyu Wu , Aiwei Liu , Lijie Wen , Philip S. YuTsinghua UniversityUniversity of CambridgeUniversity of Illinois at Chicago{hxm19,wugy18,liuaw20}@mails.tsinghua.edu.cnzg283@cam.ac.ukwenlj@tsinghua.edu.cnpsyu@uic.edu   Abstract   The explosion of misinformation spreading in   the media ecosystem urges for automated fact-   checking . While misinformation spans both ge-   ographic and linguistic boundaries , most work   in the field has focused on English . Datasets   and tools available in other languages , such   as Chinese , are limited . In order to bridge   this gap , we construct CHEF , the first CHinese   Evidence - based Fact - checking dataset of 10 K   real - world claims . The dataset covers multiple   domains , ranging from politics to public health ,   and provides annotated evidence retrieved from   the Internet . Further , we develop established   baselines and a novel approach that is able to   model the evidence retrieval as a latent variable ,   allowing jointly training with the veracity pre-   diction model in an end - to - end fashion . Exten-   sive experiments show that CHEF will provide   a challenging testbed for the development of   fact - checking systems designed to retrieve and   reason over non - English claims . Source code   and data are available .   1 Introduction   Misinformation is being spread online at increas-   ing rates , posing a challenge to media platforms   from newswire to social media . In order to combat   the proliferation of misinformation , fact - checking   is an essential task that assesses the veracity of   a given claim based on evidence ( Vlachos and   Riedel , 2014 ) . Fact - checking is commonly con-   ducted by journalists . However , fact - checking is   a time - consuming task , which can take journalists   several hours or days ( Adair et al . , 2017 ) . Thus ,   there is a need for automating the process .   Although misinformation spans both geographic   and linguistic boundaries , most existing works fo-   cused on English ( Wang , 2017 ; Thorne et al . , 2018 ;   Augenstein et al . , 2019 ; Hanselowski et al . , 2019 ;   Table 1 : An example from CHEF ( Chinese is translated   into English ) . The claim is refuted by the evidence ,   which are sentences retrieved ( highlighted ) from the   document . For brevity , only the relevant snippet of the   document is shown .   Chen et al . , 2020 ) . There only exists a handful   of non - English datasets for verifying real - world   claims . However , these datasets are either small   in size ( Baly et al . , 2018 ) , or designed for multi-   lingual systems ( Gupta and Srikumar , 2021 ) . On   the other hand , Khouja ( 2020 ) and Nørregaard and   Derczynski ( 2021 ) created claims by paraphrasing   sentences from non - English articles , but synthetic   claims can not replace real - world claims for training   generally applicable fact - checking systems .   To bridge this gap , we introduce a CHinese   dataset for Evidence - based Fact - checking ( CHEF ) .   CHEF includes claims that are not only relevant   to the Chinese world , but also originally made in   Chinese . It consists of 10,000 real - world claims ,   collected from 6 Chinese fact - checking websites   covered multiple domains and paired with anno-   tated evidence . To ensure annotation consistency,3362   we developed suitable guidelines and performed   data validation . We shared some of the insights   obtained during the annotation process that we   hope will be beneficial to other non - English an-   notation efforts . Table 1 shows an instance from   CHEF . In order to verify the claim , one needs to   first retrieve the evidence sentences from related   documents ( e.g. government reports ) , then predict   the veracity based on the evidence . After com-   paring the statistics of the entire city and central   area , we can reach the verdict that the claim is re-   futed by evidence . To characterize the challenge   of the dataset presented , we perform a thorough   analysis and demonstrate the utility of the dataset   by developing two types of baselines , including   pipeline and joint systems . Our key contributions   are summarized as follows :   1.We provide the first sizable multi - domain Chi-   nese dataset for automated fact - checking . It   consists of 10 K real - world claims with manu-   ally annotated evidence sentences .   2.We further propose an approach that is able   to model the evidence selection as a latent   variable , which can be jointly trained with the   veracity prediction module .   3.We develop several established baselines andconduct a detailed analysis of the systems eval-   uated on the dataset , identifying challenges   that need to be addressed in future research .   2 Background : Dataset Comparisons   In this section , we reviewed the existing fact-   checking dataset as summarized in Table 2 . Fol-   lowing Guo et al . ( 2022 ) , we grouped the datasets   into two categories : natural and synthetic . Natural   datasets consist of real - world claims , while syn-   thetic datasets contain claims created artificially by   mutating sentences from Wikipedia articles .   2.1 Non - English Dataset   Existing efforts in the construction of non - English   datasets are limited , both in scope and in size .   Verify ( Baly et al . , 2018 ) contains 422 claims in   Arabic and FakeCovid ( Shahi and Nandini , 2020 )   has 3,306 non - English claims about COVID-19 .   Though XFact ( Gupta et al . , 2020 ) includes 31,189   claims in 25 languages , it mainly focuses on the   multilingual setting , where the average number   of instances per language is 1,248 . More impor-   tantly , these datasets do not include annotated ev-   idence . For example , XFact used search sum-   maries returned by Google as evidence . On the   other hand , Khouja ( 2020 ) and Nørregaard and Der-   czynski ( 2021 ) constructed synthetic datasets by   mutating sentences from Arabic news and Danish3363Wikipedia articles , respectively . Unlike previous   efforts , CHEF consists of 10 K real - world claims   paired with annotated evidence .   There exist Chinese datasets focused on rumor   detection ( Ma et al . , 2016 ; Zhang et al . , 2021 ) ,   which is classified into claim detection ( Kotonya   and Toni , 2020a ; Guo et al . , 2022 ) , as it is based   on language subjectivity and growth of reader-   ship ( Qazvinian et al . , 2011 ) . A claim can be   factual regardless of whether it is a rumour ( Zu-   biaga et al . , 2018 ) . Unlike existing rumor detection   datasets , CHEF focuses on factuality of the claim .   2.2 Evidence - Based Fact - Checking   Early efforts predicted the veracity solely based   on the claims or with metadata ( Rashkin et al . ,   2017 ; Wang , 2017 ) , but relying on surface pat-   terns of claims without considering the state of   the world fails to identify well - presented misinfor-   mation ( Schuster et al . , 2020 ) . Therefore , synthetic   datasets ( Thorne et al . , 2018 ; Jiang et al . , 2020 ; Aly   et al . , 2021 ) considered Wikipedia as the source   of evidence and annotated the sentences support-   ing or refuting each claim . However , these efforts   restricted world knowledge to a single source ( i.e.   Wikipedia ) , ignoring the challenge of retrieving ev-   idence from heterogeneous sources on the Internet .   To address this , recent natural datasets ( Augen-   stein et al . , 2019 ; Gupta and Srikumar , 2021 ) used   the summary snippets returned by Google as evi-   dence . One key limitation of this approach is that   summary snippets do not provide sufficient infor-   mation to verify the claim . Gupta and Srikumar   ( 2021 ) showed that only 45 % of snippets provide   sufficient information , while 83 % of the full text   from web pages provides sufficient evidence to   determine veracity of the claim . To construct a   better evidence - based dataset , we retrieve docu-   ments from web pages and manually select relevant   evidence sentences from documents as evidence .   Such a design makes CHEF suitable to train fact-   checking systems that can extract evidence from   web - sources and validate real - world claims based   on evidence found on the Internet .   3 Dataset Construction   CHEF is constructed in four stages : data collec-   tion , claim labeling , evidence retrieval and data   validation . Data collection selects sources , crawls   claims and associated metadata . Claim labeling   identifies claims from fact - checking articles and   assigns the veracity labels of claims based on the   article . Evidence retrieval collects documents from   the Internet and selects the most relevant sentences   as evidence . Data validation controls the annota-   tion quality . The annotation team has 25 members ,   5 of them are only involved in data validation . All   annotators are native Chinese speakers . To ensure   the annotation quality , they were trained by the   authors and went through several pilot annotations .   3.1 Data Collection   We crawled all active Chinese fact - checking web-   sites listed by Duke Reporters . However , most   claims fact - checked by the fact - checkers are non-   factual , solely relying on such claims will lead   to an imbalance dataset . Therefore , we fol-   lowed Kotonya and Toni ( 2020b ) by crawling ar-   ticles from the news review site . As shown in Ta-   ble 3 , this resulted in 5 websites in total . From each   website , we crawled the full text of the article and   corresponding metadata ( e.g. author , domain , URL   publication date ) . Totally , we crawled 14,770 fact-   checking and news articles . There exists a number   of crawling issues , such as the article could not be   retrieved , or the content is not textual . We removed   such instances . Next , we checked the dataset for   duplications . Upon manual inspections , this was   mainly due to them appearing on different websites .   All duplications would be in the training split of   the dataset , so that the model would not have an   unfair advantage . As shown in Figure 5 , claims   cover multiple domains , including politics , public   health , science , society and culture . More than   36 % of claims belong to public health domain , as   many fact - checking articles focused on countering   misinformation related to COVID-19 . The society   domain has second most claims , which involves   social events that are closely related to people ’s   daily lives.3364   3.2 Claim Labeling   The major challenge of constructing a non - English   dataset is extracting a claim and its veracity from a   fact - checking article usually requires human efforts .   Unlike fact - checking articles in English , many non-   English articles ( e.g. Chinese , Hindi , Filipino ) do   not explicitly give the claim and assign the veracity .   Therefore , extracting the claim , which can appear   in the title , or anywhere in the article , requires man-   ual efforts . Before labeling the claim , we need to   extract them from the fact - checking articles . When   performing claim extraction , annotators need to   read the fact - checking article first , then identify   the claim . They are encouraged to select sentences   directly from the article . However , resulted sen-   tences may not be complete , which means they   do not provide enough context for fact - checking .   One common case is that the sentence describing   a fact often lacks the time stamp , or the location .   For example , the claim “ Price of pork increases   dramatically due to the African swine fever . ” is   factual in 2020 but non - factual in 2021 . There-   fore , annotators are asked to complete the claim   by adding missing content to ensure the claim to   be standalone for later verification . Another issue   is that Chinese fact - checkers tend to use rhetori-   cal questions to express non - factual claims . To   alleviate the bias that the factuality of a claim can   be decided by its surface form , annotators are re-   quired to paraphrase the questions into declarative   statements .   Next , annotators are required to label the ex-   tracted claims . English fact - checking articles often   provide different truth - rating scales , such as false ,   mostly false and mixture , while many non - English   counterparts do not have such taxonomies . There-   fore , annotators need to label the extracted claim   based on the understanding of the fact - checking   article . Journalism researchers showed that fine-   grained labels are often assigned inconsistently due   to subjectivity ( Uscinski and Butler , 2013 ; Lim ,   2018 ) . Therefore , we chose to follow previous ef-   forts ( Thorne et al . , 2018 ; Hanselowski et al . , 2019 )   by adopting three types of labels : supported ( SUP ) ,   refuted ( REF ) and not enough information ( NEI ) ,   given the evidence . The distribution of labels in   CHEF is shown in Table 4 . CHEF consists of a   majority of refuted claims , as the majority of fact-   checking articles aim to debunk non - factual claims .   3.3 Evidence Retrieval   When verifying a claim , journalists first find in-   formation relating to the fact and evaluate it given   the collected evidence . As shown in Figure 1 , the   biggest challenge of verifying a claim is to collect   relevant evidence . In order to validate real - world   claims , we chose to manually extract evidence from   web - sources . We have two measures to ensure the   reliability of the evidence . Firstly , we maintained a   list of misinformation and disinformation websites ,   all search results from these websites will be fil-   tered out . Secondly , we required the annotators to   manually select evidence sentences from the search   results . In order to collect evidence from the web-   sources , we first submitted each claim as a query   to the Google Search API by following Augenstein   et al . ( 2019 ) and Gupta and Srikumar ( 2021 ) . The   ten most highly ranked search results are retrieved .   For each result , we saved the search rank , URL ,   time stamp and document . Then we filtered out   results from fact - checking websites to prevent the3365   answer from being trivially found . Next , annotators   were asked to select sentences from the resulted   documents . To maintain a balance between keeping   relevant and removing irrelevant information , we   followed Thorne et al . ( 2018 ) and Hanselowski   et al . ( 2019 ) that up to five sentences were se-   lected as evidence . Before deciding which sen-   tences should be selected , annotators were required   to answer auxiliary questions , such as “ Whether   selected sentences provide sufficient information   for factual verification ? ” They were encouraged   to select the five most relevant sentences , but they   were allowed to pick less when relevant sentences   are not available . A small fraction ( 5.6 % ) of in-   stances do not have any relevant evidence , and we   chose to discard them .   3.4 Data Validation   To ensure the annotation consistency , we conducted   an additional 5 - way inter - annotator agreement and   manual validation . For inter - annotator agreement ,   we randomly selected 3 % ( n = 310 ) of claims to   be annotated by 5 annotators . We calculated the   Fleiss Kscore ( Fleiss , 1971 ) to be 0.74 , which   is comparable with 0.68 reported in Thorne et al .   ( 2018 ) and 0.70 in Hanselowski et al . ( 2019 ) . In   order to verify if evidence sentences provide suffi-   cient information , we chose another 310 instances .   The second group of annotators were required to   assign the labels based on the evidence sentences .   We found that 88.7 % of the instances were labeled   correctly and 83.6 % of them provided sufficient   information to determine the veracity . Finally , as   shown in Table 4 , we partitioned the dataset CHEF   into training , development and test sets . Our devel-   opment and test sets have balanced class distribu-   tions . Each claim is paired with Google snippets ,   evidence sentences and source documents.4 Baseline Systems   Unlike previous natural datasets , CHEF requires   the system to first retrieve the evidence sentences   from the documents , then predict the veracity based   on the evidence . Therefore , we design two types of   baselines : pipeline and joint systems .   4.1 Pipeline System   The pipeline system treats evidence retrieval and   veracity prediction as two independent steps .   4.1.1 Evidence Retrieval   Given the claim and documents , this step aims to   select the most relevant sentences from documents   as evidence , which can be viewed as a ranking   problem . Thus , we adopt the following models :   Surface Ranker Following retrieval models de-   signed for synthetic datasets ( Thorne et al . , 2018 ;   Jiang et al . , 2020 ; Aly et al . , 2021 ) , We use TF - IDF   to sort the most similar sentences first and tune a   cut - off using validation accuracy on the dev set .   Semantic Ranker Inspired by Nie et al . ( 2019 )   and Liu et al . ( 2020 ) , we choose semantic matching   based on BERT ( Devlin et al . , 2019 ) pre - trained   on Chinese corpus ( Wolf et al . , 2020 ) . The cosine   similarity scores between the embedding of the   claim and the embeddings of other sentences in the   document are used for ranking .   Hybrid Ranker Since semantic encoding is com-   plementary to surface form matching , they can be   combined for better ranking . Following Shaar et al .   ( 2020 ) , we use the rankSVM , based on the fea-   ture sets of rankings returned by TF - IDF and the   similarity scores computed with BERT .   Google Snippets As discussed in Section 2 , exist-   ing natural datasets ( Augenstein et al . , 2019 ; Gupta   and Srikumar , 2021 ) do not require the system to   retrieve the evidence sentences from the documents .   Instead , they used summary snippets returned by   the Google Search Engine as evidence . We also   include this type of evidence for comparisons .   4.1.2 Veracity Prediction   After retrieving the evidence sentences , veracity   prediction aims to predict the label of the given   claim . We implement the following classifiers :   BERT - Based Model Following Jiang et al .   ( 2020 ) and Schuster et al . ( 2021 ) , we use a multi-   layer perceptron with embeddings from BERT as3366the classifier . The embeddings of claim and re-   trieved evidence are concatenated as the input . The   model performs the classification based on the out-   put representation of the CLS token .   Attention - Based Model Following Gupta and   Srikumar ( 2021 ) , we first extract the output embed-   ding of the CLS token of each selected evidence   and calculate relevance weights with the claim   through dot product attention . Then we feed the   concatenated claim and weighted evidence into the   BERT - based classifier .   Graph - Based Model Recent efforts ( Zhou et al . ,   2019 ; Zhong et al . , 2020 ; Liu et al . , 2020 ) showed   that graphs help to capture richer interactions   among multiple evidence for fact - checking . We   adopt the Kernel Graph Attention Network ( Liu   et al . , 2020 ) for veracity prediction . The evidence   graph is constructed based on the claim and evi-   dence sentences , then node and edge kernels are   used to conduct fine - grained evidence propagation .   The updated node representations are used to cal-   culate the claim label probability .   4.2 Joint System   Evidence retrieval in the pipeline system could not   solicit optimization feedback obtained from verac-   ity prediction . In order to optimize two steps jointly ,   we proposed to model the evidence retrieval as a   latent variable . The joint system contains two mod-   ules : a latent retriever and a classifier . For the   classifier , we used the same models described in   Section 4.1 . Latent retriever labels each sentence   in the documents with a binary mask . Sentences   labeled with 1 are selected as the evidence , while   sentences labeled with 0 will be neglected .   4.2.1 Latent Retriever   We built the latent retriever based on the Hard Ku-   maraswamy distribution ( Bastings et al . , 2019 ) ,   which gives support to binary outcomes and al-   lows for reparameterized gradient estimates . We   first stretch the Kumaraswamy distribution ( Ku-   maraswamy , 1980 ) to include 0and1by the sup-   port of open interval ( l , r)where l < 0andr > 1 ,   defined as K∼Kuma ( a , b , l , r ) with CDF :   F(k;a , b , l , r ) = F((k−l)/(r−l);a , b )   ( 1)A sigmoid function k = min(1 , max(0 , k))is used   to rectify random variables into the closed inter-   val[0,1 ] , denoted by K∼HardKuma ( a , b , l , r )   andk = s(u;a , b , l , r ) for short . Note that we map   all negative values k∈(l,0]intok=0andk   ∈[1 , r)intok=1deterministically , so the sets   whose masses under Kuma ( k|a , b , l , r ) are avail-   able in the closed form :   P(K= 0 ) = F(−l   r−l;a , b )   P(K= 1 ) = 1 −F(1−l   r−l;a , b)(2 )   Given source documents D , the latent retriever se-   lects relevant sentences as evidence that can be   used to predict the veracity for the claim c. For the   i - th sentence x∈D , we obtain the sentence - level   embedding hbased on a BERT encoder by using   theCLS token . Then we can calculate the latent   selector kby :   k = s(u;a , b , l , r ) ,   a = f(h;ϕ)b = f(h;ϕ)u∼ U(0,1 )   ( 3 )   where f(·;ϕ)andf(·;ϕ)are feed - forward net-   works with softplus outputs aandb.s(·)turns   the uniform sample uinto the latent selector k.   Next , we use the sampled kto modulate inputs to   the classifier for veracity prediction :   f / parenleftig   k·h , c;θ / parenrightig   ( 4 )   where c = f(c)denotes the embedding for the   given claim cobtained by using the CLS token   through a BERT encoder . f(·;θ)represents   the classifier ( e.g. graph - based model ) . The joint   system can be optimized by gradient estimates of   E(ϕ , θ)via Monte Carlo sampling from :   E(ϕ , θ ) = E[logP(y|X , s(u , X ) , θ ) ]   ( 5 )   where yis the label of veracity and k = s(u , X )   abbreviate the transformation from uniform sam-   ples to HardKuma samples .   4.2.2 More Baselines   Apart from the proposed system , we include fol-   lowing baselines for comprehensive comparisons :   Reinforce Instead of using gradient - based train-   ing , we follow Lei et al . ( 2016 ) by assigning a bi-   nary Bernoulli variable to each evidence sentence.3367Because gradients do not flow through discrete sam-   ples , the evidence retriever is optimized using RE-   INFORCE ( Williams , 1992 ) . A Lregularizer is   used to impose sparsity - inducing penalties .   Multi - task We also adopt the multi - task learn-   ing method proposed by Yin and Roth ( 2018 ) ,   which is the state - of - the - art joint model on FEVER   dataset ( Thorne et al . , 2018 ) . The model predicts a   binary vector that indicates the subset of sentences   as evidence , and a one - hot vector indicates the ve-   racity of the claim . The overall training loss is the   sum of these two prediction losses .   5 Experiments and Analyses   5.1 Experimental Setup   Following Augenstein et al . ( 2019 ) , we computed   the Micro F1 and Macro F1 as the evaluation met-   ric . We further reported the mean F1 score and   standard deviation by using 5 models from inde-   pendent runs . For the pipeline system , we used 6   different evidence settings , including evidence sen-   tences retrieved by surface ranker , semantic ranker   and hybrid ranker , Google snippets , gold evidence   and without using any evidence . For the joint sys-   tem , we used 2 types of evidence : Google snippets   and source documents , where the latent retriever   can select sentences from . We used three classi-   fiers for both systems , BERT - based ( Schuster et al . ,   2021 ) , attention - based ( Gupta and Srikumar , 2021 )   and graph - based models ( Liu et al . , 2020 ) .   The hyper - parameters are chosen based on the   development set . In the evidence retrieval step of   the pipeline system , we set the retrieved evidence   obtained from TF - IDF to be more than 5words   for surface ranker . We use the BERT default tok-   enizer with max - length as 256to preprocess data   for semantic ranker . We use the default parame-   ters in sklearn.svm . LinearSVC with RBF kernel   for hybrid ranker .   In the veracity predication step of the pipeline   system , we use the BERT default tokenizer with   max - length as 256 and pretrained BERT - base-   Chinese as the initial parameter to encode claim   and evidence . For BERT - based model , the fully   connected network for classification is defined with   layer dimensions of h - h/2 - verification_labels ,   where h= 768 . We use BertAdam ( Devlin et al . ,   2019 ) with 5e−6learning rate , warmup with 0.1   to optimize the cross entropy loss and set the batchsize as 16 . For attention - based model , we use   BertAdam with 2e−5learning rate , warmup with   0.1to optimize the cross entropy loss and set the   batch size as 8 . For graph - based model , we use   BertAdam with 5e−5learning rate , warmup with   0.1 , batch size with 16 , dropout with 0.6and kernel   size with 21 .   For the joint system , we use Adam ( Kingma and   Ba , 2015 ) with 5e−5learning rate , learning rate   decay with 0.5to optimize the cross entropy loss .   We set the batch size as 32 . The fully - connected   networks f(·;ϕ)andf(·;ϕ)for two parame-   tersaandbare defined with layer dimensions of   h= 768 . We set the dropout rate as 0.5 .   5.2 Main Results   Pipeline System : According to Table 5 , pipeline   systems with evidence including Google snippets ,   sentences returned by rankers and gold evidence   consistently outperform systems without using evi-   dence . These results confirm that evidence plays an   important role in verifying real - world claims . On   the other hand , systems with retrieved sentences   achieve higher scores than systems with Google   snippets . Specifically , systems with gold evidence   significantly outperform the ones with Google snip-   pets , indicating information that is necessary for   verification is missing in the snippets . Moreover ,   systems with retrieved evidence are more robust in   terms of standard deviation . We hypothesize the   reason is that irrelevant information is presented   in the snippets . When comparing with different   rankers , we observed that using contextualized rep-   resentations to measure the similarity ( Semantic   Ranker ) is generally better than exact string match   ( Surface Ranker ) . However , there still exists a large   performance gap between the pipeline system with   semantic ranker and the system with gold evidence .   One potential solution is to develop better retrieval   models based on the supervision signal of gold   evidence provided by CHEF . Given the evidence   sentences , graph - based models tend to have higher   scores than BERT - based and attention - based mod-   els , which shows the effectiveness of leveraging   graph structure to synthesize multiple evidence .   Joint System : Similar to the pipeline systems ,   joint systems that retrieve evidence sentences from   documents achieve better F1 scores than directly   use the summary snippets . In order to verify real-   world claims , it is necessary to train fact - checking   systems that learn how to effectively retrieve evi-3368   dence sentences from full documents on web pages .   In addition , joint system outperforms pipeline sys-   tem consistently with both Google snippets and   source documents as inputs . For example , latent re-   triever with Google snippets are able to achieve an   average 2.74 % and 1.77 % Micro / Macro F1 boost   compared with the pipeline systems with the same   type of evidence . We attribute the consistent im-   provement of joint system to the explicit feedback   to the evidence retrieval via gradient estimation   on veracity prediction . Another advantage of the   joint system is that the latent evidence retriever is   able to dynamically select relevant sentences from   documents for each instance , while rankers return   a fixed number of evidence .   Compared with the reinforce and multi - task   methods , the proposed latent retriever achieves   1.41 % and 1.98 % higher F1 on average with   Google snippets and source documents as inputs   across various classifiers . When considering stan-   dard deviation , reinforce is less robust . We be-   lieve the main reason is that latent retriever facili-   tate training through differentiable binary variables ,   which leads to robust and generalized model that   exhibits small variance over multiple runs .   5.3 Analysis and Discussion   In this section , we further provide fine - grained anal-   yses for baseline systems on CHEF . For brevity , we   abbreviate pipeline systems with Google Snippets ,   Surface Ranker , Semantic Ranker , Hybrid Ranker   asGS , Sur , Sem , Hyb , while joint systems with   Google snippets and source documents as inputs   asJGandJS , respectively . All results are reported   based on the BERT - based model . We further pro-   vide case study and error analysis on CHEF . Due   to limited spaces , we attach them in the appendix .   Effect of Evidence : In Table 6 , we varied the   numbers of evidence retrieved and reported the   Macro F1 on the test set . The fluctuation results   indicate that both quantity and quality of retrieved   evidence affect the performance . Using fewer ev-   idence will lead to incomplete coverage , which   may not provide sufficient information to verify   the claims . On the other hand , incorporating more   evidence may introduce irrelevant sentences thus   propagate errors to veracity prediction . In gen-   eral , systems with 5 evidence sentences achieves   the best performance except the joint system with   source documents as inputs . We believe the reason   is that the latent retriever maintains a better balance   between keeping relevant and removing irrelevant   sentences , which helps to achieve higher scores   with more evidence sentences .   Performance against Claim Length : We parti-   tioned the test set into 4 classes ( < 10 , 10 - 19 , 20 - 29 ,   ≥30 ) based on lengths of the claims and reported3369   the Macro F1 score . For clarity , we choose the best   reported pipeline system with semantic ranker to   compare with joint systems . As shown in Figure 3 ,   most claims are longer than 10 words . Performance   of the systems on short claims ( e.g. < 10 ) is lower   than other . One reason is that such claims do not   contain sufficient information to retrieve evidence   and to be verified , based on the observation that   the performance of all the systems improve as the   length of the claim increase . In general , the joint   system outperforms the pipeline system against   various claim lengths .   Performance against Classes and Domains : As   CHEF is constructed based on real - world claims , most of them are non - factual claims verified by   fact - checking websites . Such an imbalance issue   poses a challenge to the fact - checking system . Fig-   ure 4 shows the performance of models for differ-   ent veracity labels . The scores of minor classes are   much lower than the majority class . This reflects   the difficulty of judging SUP and NEI . Informative   evidence helps to alleviate this issue . For example ,   the pipeline system with gold evidence achieves   significant improvement on predicting NEI labels   when comparing with the system with semantic   ranker . Figure 5 shows the performance of differ-   ent domains . Claims from science , politics and   culture domains have fewer training instances as   most claims in the dataset focus on the society and   public health topics . Again , retrieving informative   evidence sentences ( JS and Gold ) from full docu-   ments is beneficial to this data sparsity issue .   6 Conclusion   We constructed the first Chinese dataset for   evidence - based fact - checking . Further , we have   discussed the annotation methods and shared some   of the insights obtained that will be useful to other   non - English annotation efforts . To evaluate the   challenge CHEF presents , we have developed es-   tablished baselines and conducted extensive exper-   iments . We show that the task is challenging yet   feasible . We believe that CHEF will provide a stim-   ulating challenge for automatic fact - checking .   7 Acknowledgement   We thank the reviewers for their valuable com-   ments . The work was supported by the National   Key Research and Development Program of China   ( No . 2019YFB1704003 ) , the National Nature Sci-   ence Foundation of China ( No . 62021002 and   No . 71690231 ) , NSF under grants III-1763325 , III-   1909323 , III-2106758 , SaTC-1930941 , Tsinghua   BNRist and Beijing Key Laboratory of Industrial   Bigdata System and Application .   8 Ethical Consideration   Datasets have been collected in a manner that is   consistent with the terms of use of any sources and   the intellectual property . For each annotator , we   compensate based on the number and quality of   annotated sentences . More details of our datasets   are depicted in Section 3.3370References33713372A Supplement Materials   A.1 CHEF annotation guidelines   A.1.1声明抽取和规范化的指引Guidelines   for claim extraction and normalization :   标注者首先需要认真阅读事实验证的文章，然   后使用一到两句话来概括这篇文章描述的事件   作为声明。请标注者直接使用文章中的句子作   为声明，比如文章的标题，或者首段的前几句   话都可能是这篇文章需要验证的事实。如果没   法抽取出相应的句子，可以使用自己的语言总   结文章来撰写声明。在撰写声明的时候，有以   下注意事项 ：   •每个声明必须完整 。   •每个声明不应该存在事实验证偏差 。   •每个声明不应该存在信息泄露 。   请仔细阅读以下详细指引和相应的规范化例   子 :   •声明中描述的事件缺乏必要的细节 , 比   如：时间和地点。标注者需要加上这些   细节让声明完整，才能被验证。比如：今   年共有12.08万人参加中考，但招生计划   只有4.3万。需要改写声明为：2019年 ，   共有12.08万人参加成都中考，但招生计   划只有4.3万 。   •声明中存在特殊符号，需要去除特设符号   避免声明中存在偏差。比如：“纯天然”喷   雾一喷“秒睡”。需要去除句子中的“”，因   为这些特殊符号隐晦地表达了这个声明其   实是不实的。模型可以通过特殊符号直接   判断一个声明的真实性。这个句子需要改   写为：纯天然喷雾一喷秒睡。，这可以避   免由于声明中的特殊符号“”带来事实验证   偏差 。   •声明中存在信息泄露，需要去除直接指   出声明真实性的相关词语。比如：谣言 ！   纯天然喷雾一喷秒睡。句子中使用的“谣   言！”已经直接指出该声明是不实的，造   成了信息泄露。需要改写声明为：纯天   然喷雾一喷秒睡。不能在声明中出现诸   如：“谣言”、“错误”，“骗局”等信息泄露   词 。   •声明中的反问句需要被改写为陈述句，由   于采用反问句形式的声明大部分都是不实   的，反问句的形式会造成数据集偏差。比   如：别人打了新冠疫苗，我们就可以不打   新冠疫苗吗？需要被改写为：别人打了新   冠疫苗，我们就可以不打新冠疫苗。•不陈述事实的声明需要被丢弃。有两大   类的声明是无法进行事实验证的。第一大   类为表示推测的声明，比如：明年深圳房   价会上涨。第二大类为表示个人意见的声   明，比如：我认为特朗普应该连任 。   •声明中如果包含多个声明，需要拆分为   多个声明逐一验证。比如：关于新冠疫苗   接种的两个事实：第一，别人打了新冠疫   苗，自己就可以不打新冠疫苗。其次，新   冠疫苗只需要打一针就能具备新冠病毒防   护能力。这个声明包括了两个子声明，需   要被拆分为：别人打了新冠疫苗，我就可   以不打新冠疫苗。第二个声明为：新冠疫   苗只需要打一针就能具备新冠病毒防护能   力 。   The annotator first needs to read the fact - checking   article carefully , and then use one or two sentences   to summarize the event described in this article as   a claim . The annotator is encouraged to directly   extract the sentences in the article as the claim , such   as the title of the article , or the first few sentences in   the first paragraph . If the annotator can not find the   sentence that can serve as a claim , you can use your   own language to write the claim . When extracting   the claim , there are the following considerations :   • Each claim must be complete .   •For each claim , explicit bias should be re-   moved .   •Each claim should not have information leak-   age .   Please read the following detailed guidelines and   corresponding normalized examples carefully :   •If the event described in the claim lacks im-   portant details , such as time and location , an-   notator need to add these necessary metadata   to make the claim complete before it can be   verified . For example , a total of 120,800 peo-   ple took the entrance examination this year ,   but the enrollment plan is only 43,000 . The   claim needs to be rewritten as follows : In   2019 , a total of 120,800 people participated   in the Chengdu high school entrance examina-   tion , but the enrollment plan was only 43,000 .   •If there exist special symbols in the claim ,   such symbols that may lead to bias for claim   verification should be removed . For example :   " Natural spray " helps you " sleep instantly".3373Quotation marks should be removed in the   sentence , as these special symbols implicitly   indicates such a claim is non - factual . The   model can predict the veracity simply based   on the special symbols in the claim . This   claim needs to be rewritten as : Natural spray   helps you sleep instantly .   •Claims contains words that will lead to infor-   mation leakage should be removed . For exam-   ple : Rumors ! Natural spray helps you sleep   instantly . The word " Rumor ! " in the claim   directly pointed out that the claim is nonfac-   tual , causing information leakage . The word   " Rumor ! " should be removed . Do not include   information leaking words such as " rumors " ,   " errors " , " scams " , etc . in the claim .   •Claims used rhetorical questions need to be   rewritten into declarative sentences . Since   most of the claims in the form of rhetorical   question are nonfactual , the form of the rhetor-   ical question exhibits a bias in the dataset . For   example : if someone else gets the COVID-   19 vaccine , can we not get the vaccine ? It   needs to be rewritten as : if someone else gets   a COVID-19 vaccine , we do not need to get   the vaccine ..   •Claims that do not related to factuality should   be discarded . There are two major types of   claims that can not be verified . The first cate-   gory is speculative claims , such as : Shenzhen   housing prices will rise next year . The second   category is claims expressing personal opin-   ions , such as : I think Donald Trump should   be the president .   •A claim contains multiple statements should   be split into multiple claims to be verified   one by one . For example , a claim stated that :   First , if someone else gets the COVID-19 vac-   cine , you do not need to get one . Also , the   COVID-19 vaccine only needs one shot to   protect against the virus . This claim includes   two sub - claims . It needs to be split into two   claims .   A.1.2声明标注的指引 Guidelines for claim   labeling   标注者在抽取出和规范化声明之后，需要根据   事实验证的文章给出的结论，给每个声明打上   标签。我们提供了以下三 种标签，请选择其中的一种。注意的是，对于大部分为真，部分为   真，大部分为为假，部分为假和半真半假的情   况，我们统一归类为信息不足 ：   •支持，有充分证据表明这个声明是被证据   所支持的 。   •反对，有充分证据表明这个声明是被证据   所反对的 。   •信息不足，没有足够的证据表明这个声明   是被支持还是反对 。   After extracting and normalizing the claim , annota-   tors needs to label each claim based on the conclu-   sions of the fact - checking article . We provide the   following three labels , please choose one of them .   Note that for conclusions such as mostly true , par-   tially true , mostly false , partial false and mixture ,   we consider them as not enough information :   •Supported , there is sufficient evidence to show   that this claim is supported by the evidence .   •Refuted , there is sufficient evidence to show   that this claim is refuted by the evidence .   •Not enough information , there is not enough   evidence to show whether this claim is sup-   ported or refuted .   A.1.3证据标注的指引 Guidelines for   evidence labelling   标注者需要阅读规范化过后的声明，事实验证   的文章还有搜集到的源文档。标注者首先需要   理解文章的验证思路，再从源文档当中直接选   择能够作为证据的句子。针对每个声明，标注   者最少选择1个，最多选择5个相关的句子作为   证据。在选择句子作为证据的时候有以下注意   事项 ：   •请标注者选择完整的句子，以句号为结束   标志 。   •选择句子作为证据的条件是，在仅仅基于   当前选中的句子作为证据的前提下，能够   验证给定的声明。也就是说，选中的句子   必须要提供给足够的信息来帮助判断声明   的事实性 。   •如果出现多于5个句子能够作为证据的情   况，选择你认为最相关的5个句子；或者   能够形成推理逻辑链的句子；或者和事实   验证文章推理过程最相似的句子。3374•如果出现源文档互相矛盾的情况，优先选   择支持事实验证文章结论的文档，从中选   择相关的句子作为证据 。   •如果提供的源文档并没有提供足够的证据   来验证声明，请报告这条声明 。   •如果提供的源文档只包含和事实验证文章   结论矛盾的证据，请报告这条声明 。   The annotator needs to read the normalized claim ,   the fact - checking article and the collected docu-   ments . Annotators first need to understand the ver-   ification process in the fact - checking article , and   then directly select sentences from the sources doc-   uments . These selected sentence are used as evi-   dence to verify the claim . For each claim , annota-   tors should select at least 1 and at most 5 relevant   sentences as evidence . There are the following con-   siderations when choosing sentences as evidence :   •Please select a complete sentence which ends   with a period .   •When selecting sentences as evidence , the an-   notator should consider given the selected sen-   tences if the given claim can be verified . In   other words , the selected sentences must pro-   vide sufficient information to predict the fac-   tuality of the given claim .   •If there are more than 5 sentences that can be   used as evidence , choose the 5 sentences that   you think are the most relevant ; or the sen-   tences that can form a reasoning chain for ver-   ification ; or the sentence that is most similar   to the reasoning process of the fact - checking   article .   •If there are conflicting source documents , the   documents that support the conclusion of the   fact - checking article should be considered ,   and the most relevant sentences in these docu-   ments are selected as evidence .   •If the source documents do not provide suffi-   cient evidence to verify the statement , please   report the claim .   •If the source documents only contains evi-   dence that contradicts the conclusion of the   fact - checking article , please report this claim . A.1.4数据验证的指引 Guidelines for data   validation   给定一个声明和搜集到的证据句子，标注者需   要根据证据去判断这个声明的真实性。如果标   注者认为提供的声明缺失重要信息，或者是不   可读的，请报告该条声明。我们提供了以下三   种标签，请选择其中的一种 ：   •支持，有充分证据表明这个声明是被证据   所支持的 。   •反对，有充分证据表明这个声明是被证据   所反对的 。   •信息不足，没有足够的证据表明这个声明   是被支持还是反对 。   Given a claim and the evidence sentences , the an-   notator needs to label the factuality of the claim   based on the evidence . If the annotator believes   that the given claim lacks important information or   is unreadable , please report the claim . We provide   three kinds of labels , please choose one of them :   •Supported , there is sufficient evidence to show   that this claim is supported by the evidence .   •Refuted , there is sufficient evidence to show   that this claim is refuted by the evidence .   •Not enough information , there is not enough   evidence to show whether this claim is sup-   ported or refuted .   A.1.5判断声明领域的指引 Guidelines for   determining claim domain   标注者需要阅读声明，根据给出的五个领域判   断声明属于哪个领域 ：   •政治：主要是关于国际与国内政治等方面   的声明 。   •公卫：主要是关于公共卫生方面的声明 ，   比如有关新冠病毒，人体健康，食品安全   等方面 。   •科学：主要是关于自然科学和工程技术等   方面的声明 。   •文化：主要是关于历史，人文，娱乐，体   育等方面的声明 。   •社会：主要是除了上述四类，社会生活方   面的声明。3375The annotator needs to read the claim and deter-   mine which domain the claim belongs to based on   the five domains given :   •Politics : Claims mainly focus on international   and domestic politics .   •Health : Claims mainly focus on public health ,   including topic related to COVID-19 , health   care , food safety , etc .   •Science : Claims mainly focus on natural sci-   ence and technology .   •Culture : Claims mainly focus on history , hu-   manities , entertainment , sports , etc .   •Society : Claims not related on the above four   categories , and related to daily social life .   A.1.6验证声明的挑战（多选 ） Claim   verification challenges ( Multiple choice )   对声明进行事实验证往往会遇到许多挑战，挑   战可以分为以下四类，标注者需要阅读事实验   证的文章，判断验证声明时会遇到哪些挑战 ：   •证据搜集：通过搜集证据，比如找相关的   新闻，论文，法律法规等来验证声明 。   •专家咨询：通过咨询专家或者相关人士 ，   比如外交部发言人陈述，部委回复，记者   采访等来验证声明 。   •数值推理：通过数值的比较，趋势的分析   来验证声明 。   •多模态：通过除了文本外的其他证据，比   如图片，视频，音频来验证声明 。   Factual verification of a claim often encounters   many challenges . The challenges are summarized   into the following four categories . The annotator   needs to read the fact - checking article to determine   which challenges will be encountered in verifying   the claim :   •Evidence Collection : Verify the claim by   collecting evidence , such as finding relevant   news , papers , laws and regulations , etc .   •Expert Consultation : Verify the claim by con-   sulting experts or related people , such as state-   ments by the spokesperson of the Ministry of   Foreign Affairs , replies from ministries and   commissions , interviews with reporters , etc.•Numerical Reasoning : Verify the claim by   numerical comparison , trend analysis , etc .   •Multi - Modality : Verify the claim with other   evidence besides articles , such as pictures ,   videos , and audio.3376