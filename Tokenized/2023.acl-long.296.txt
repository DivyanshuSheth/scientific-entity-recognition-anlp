  Luis F. Guzman - Nateras , Franck Dernoncourt , and Thien Huu NguyenDepartment of Computer Science , University of Oregon , Eugene , OR , USAAdobe Research , Seattle , WA , USA   { lfguzman,thien}@cs.uoregon.edu ,   franck.dernoncourt@adobe.com   Abstract   In this paper , we address the Event Detection   task under a zero - shot cross - lingual setting   where a model is trained on a source language   but evaluated on a distinct target language for   which there is no labeled data available . Most   recent efforts in this field follow a direct trans-   fer approach in which the model is trained   using language - invariant features and then di-   rectly applied to the target language . However ,   we argue that these methods fail to take advan-   tage of the benefits of the data transfer approach   where a cross - lingual model is trained on target-   language data and is able to learn task - specific   information from syntactical features or word-   label relations in the target language . As such ,   we propose a hybrid knowledge - transfer ap-   proach that leverages a teacher - student frame-   work where the teacher and student networks   are trained following the direct and data transfer   approaches , respectively . Our method is com-   plemented by a hierarchical training - sample   selection scheme designed to address the issue   of noisy labels being generated by the teacher   model . Our model achieves state - of - the - art re-   sults on 9 morphologically - diverse target lan-   guages across 3 distinct datasets , highlighting   the importance of exploiting the benefits of hy-   brid transfer .   1 Introduction   Event Detection ( ED ) is a sub - task of the encom-   passing Information Extraction ( IE ) Natural Lan-   guage Processing ( NLP ) task . The main objective   of ED is to detect and categorize the event triggers   in a sentence , i.e. , the words that most clearly in-   dicate the occurrence of an event . Event triggers   are known to be frequently related to the verb in a   sentence ( Majewska et al . , 2021 ) . However , they   can also be other parts of speech such as nouns   or adjectives . For instance , in the sentence “ The   ceremony was chaired by the former Secretary of   State ” , an ED system should recognize former asthe trigger of a Personnel : End - Position event .   Generating labeled data for IE tasks such as ED   can be a long and expensive endeavor . As such ,   most labeled ED datasets pertain to a small set of   popular languages ( e.g. , English , Chinese , Span-   ish ) . In turn , labeled data is scarce or non - existent   for a vast majority of languages . This imbalance   in annotated data availability has prompted many   research efforts into zero - shot cross - lingual trans-   fer learning which attempts to transfer knowledge   obtained from annotated data in a high - resource   source language to a low - resource target language   for which no labeled data is available . There are   two predominant knowledge - transfer paradigms   employed by such cross - lingual methods : Data   transfer andDirect transfer .   Approaches that adhere to the data transfer   paradigm generate pseudo - labeled data in the target   language and then train a model on such data . This   pseudo - training data can be constructed by map-   ping the gold source labels into parallel , or trans-   lated , versions of the source data , or by leveraging   source - trained models to annotate unlabeled target   data . Since models in this category are trained on   the target language , they can directly exploit word-   label relations and other target - language - specific   information such as word order and lexical fea-   tures ( Xie et al . , 2018 ) . However , annotated paral-   lel corpora are extremely scarce , and misaligned or   incorrect translations introduce noise that affects   the model performance .   In contrast , direct - transfer - based approaches   aim at creating cross - lingual models by training   them with delexicalized , language - independent fea-   tures obtained from the labeled , source - language   data . The resulting language - agnostic models can   then be applied directly to unlabeled data in the   target language .   In recent years , direct transfer has become the   favored transfer paradigm as such models have less5414need for cross - lingual resources and can be applied   to a broader range of languages . As such , previ-   ous research efforts on Cross - Lingual Event De-   tection ( CLED ) have mostly focused on the direct   transfer approach ( M’hamdi et al . , 2019 ; Majew-   ska et al . , 2021 ) and , in consequence , have failed to   exploit the aforementioned advantages of training   with target - language data .   More recent approaches have attempted to ad-   dress this issue by incorporating unlabeled target-   language data into the training process . For ex-   ample , Nguyen et al . ( 2021 ) propose a class-   aware , cross - lingual alignment mechanism where   they align examples from the source and target   languages based on class information . Guzman-   Nateras et al . ( 2022 ) instead propose to im-   prove standard Adversarial Language Adaptation   ( ALA ) ( Joty et al . , 2017 ; Chen et al . , 2018 ) by only   presenting the language discriminator with informa-   tivesamples . Despite their improved results , these   models only learn task - related information from   the source language and fail to make use of the   potentially useful information contained in word-   label relations in the target language . Furthermore ,   previous studies on similar tasks have shown that ,   even for direct transfer methods , lexical features   are useful if the source and target languages are   close to each other ( Tsai et al . , 2016 ) .   Given that the data transfer and direct transfer   paradigms are orthogonal , in this paper we present   ahybrid transfer approach for cross - lingual event   detection that ( 1 ) exploits the desirable features of   both and ( 2 ) minimizes their respective shortcom-   ings . For this purpose , we propose a knowledge dis-   tillation framework which has already been proven   effective on similar cross - lingual tasks ( Wu et al . ,   2020a , b ; Liang et al . , 2021 ; Chen et al . , 2021 ) . In   our proposed framework , a teacher model is trained   using a direct transfer approach ( i.e. , with language-   invariant features obtained from annotated source   data ) and applied to unlabeled target - language data .   Then , this pseudo - labeled data is utilized to train   a student model so that it benefits from the advan-   tages of the data transfer paradigm .   Nonetheless , we recognize that the pseudo - labels   obtained from the teacher model are prone to con-   taining noisy predictions which can be hurtful for   student training . To address this issue , we ar-   gue that the teacher model should produce more   dependable predictions on target - language exam-   ples that share some similarities with their source - language counterparts . As such , we propose to   improve the teacher - student learning process by   restricting student training to samples with such   desirable characteristics . We perform our training-   sample selection in a hierarchical manner : First , we   leverage Optimal Transport ( OT , Villani , 2008 ) to   compute similarity scores between batch samples   in the source and target languages . Only samples   with similarity scores above a certain threshold are   selected in this first step . OT has already been   shown to be effective at estimating cross - lingual   similarities for sample selection ( Phung et al . , 2021 ;   Guzman - Nateras et al . , 2022 ) . Then , in the second   step , we make use of Cross - domain Similarity Lo-   cal Scaling ( CSLS , Conneau et al . , 2018 ) to refine   our sample selection . CSLS provides an enhanced   measure to obtain reliable matches between sam-   ples in the source and target languages by address-   ing the hubness phenomenon that plagues nearest-   neighbor - based pair - matching methods . The stu-   dent model is then trained on the hierarchically-   selected target - language samples exclusively .   In order to validate our approach , we compare   our model ’s performance against current state - of-   the - art models for CLED . For this purpose , we   report our results on the most commonly used   CLED benchmarking datasets : ACE05 ( Walker   et al . , 2006 ) and ACE05 - ERE ( Song et al . , 2015 ) .   These datasets , in conjunction , contain ED anno-   tations for 3 distinct target languages . Our experi-   mental results show that our approach consistently   outperforms such state - of - the - art CLED models .   Additionally , we further evaluate the flexibility and   applicability of our model by leveraging the re-   cently released MINION dataset ( Pouran Ben Vey-   seh et al . , 2022 ) which contains ED annotations for   8 typologically different languages .   The remainder of this document is organized as   follows : section 2 presents the definition of the   ED task and an in - depth description of our model   and approach , section 3 includes the main results   from our experiments and related analysis , sec-   tion 4 provides a review of previous relevant work ,   and finally , section 5 presents our conclusions .   2 Model   2.1 Event Detection : Problem Definition   We follow a similar approach to previous CLED ef-   forts ( M’hamdi et al . , 2019 ; Majewska et al . , 2021 ;   Guzman - Nateras et al . , 2022 ) and model the ED   task as a sequence labeling problem.5415Given a group of sentences S =   { s , s , . . . , s}where each of such sen-   tences is considered as a sequence of tokens   s={t , t , . . . , t}accompanied by a corre-   sponding label sequence y={y , y , ... , y } ,   the main idea is to train a model to generate   token - level contextualized representations which   can then be used to predict token - level labels .   In broad terms , a sequence labeling model con-   sists of an encoder Eand a classifier C. The encoder   consumes a sequence of input tokens tand out-   puts a sequence of contextualized representations   h(Eq . 1 ) . These representations are then fed to the   classifier which produces a probability distribution   over all of the possible types . A candidate label is   selected by choosing the type with the largest prob-   ability . The model loss Lis then computed via   negative log - likelihood with the classifier - selected   labels and the expected gold labels ( Eq . 2 ) .   h , h , . . . , h = E(t , t , . . . , t)(1 )   L=−1   n∗m / summationdisplay / summationdisplaylogC(y|h)(2 )   2.1.1 Zero - shot Cross - lingual Event Detection   In a cross - lingual setting , different languages are   utilized during the training and testing phases . The   language utilized during training is referred to as   thesource language . Once training is complete , the   model is tested on the so - called target language .   A zero - shot setting further assumes that there   is no labeled data in the target language to be   leveraged during training . Nonetheless , raw , unla-   beled target - language text can usually be collected   without major difficulties . As such , in our work ,   we assume the availability of two distinct sets of   sentences during training : the labeled source sen-   tences Sand unlabeled target sentences S.   For model evaluation purposes , we leverage a set   of labeled target - language sentences .   2.2 Hybrid Knowledge Transfer   As mentioned in Section 1 , we propose to com-   bine the direct transfer and data transfer approaches   by leveraging a Knowledge Distillation framework .   Knowledge distillation was originally proposed as a   way to compress models by transferring knowledge   from a larger teacher model onto a smaller student   model ( Bucilua et al . , 2006 ) . However , knowledge   distillation has since been applied to several differ-   ent tasks such as machine translation ( Weng et al . ,2020 ) , automated machine learning ( Kang et al . ,   2020 ) , cross - modal learning ( Hu et al . , 2020 ) , and   cross - lingual named entity recognition ( Wu et al . ,   2020a , b ; Liang et al . , 2021 ; Chen et al . , 2021 ) .   To the best of our knowledge , our approach   is the first effort into leveraging a knowledge-   distillation framework for CLED . The following   sections present the details of our teacher and stu-   dent models as well as our hierarchical data - sample   selection strategy for student - model training .   2.2.1 Teacher Model   Our teacher model architecture follows that   of previous direct - transfer - based models for   CLED ( M’hamdi et al . , 2019 ; Majewska et al . ,   2021 ; Guzman - Nateras et al . , 2022 ) . We lever-   age a transformer - based pre - trained multilingual   language model as the encoder E. In particular ,   we make use of XLM - R ( Conneau et al . , 2019 ) as   it often outperforms multilingual BERT ( Devlin   et al . , 2019 ) on the CLED task ( Pouran Ben Vey-   seh et al . , 2022 ) . For the classifier C , we employ   a simple Feed - Forward Neural Network ( FFNN )   with 2 hidden layers ( Eq . 3 ) . A softmax operation   is applied to the resulting predictions to obtain a   probability distribution over the event types .   C(y ) = softmax ( WReLU ( Wh ) )   ( 3 )   where WandWare parameter matrices to   be learned and C(y)∈Ris the probability   distribution over the event type set Cfor token   t∈ S.   Some related works use a Conditional Random   Field ( CRF ) layer on top of the FFNN classifier in   an attempt to capture the interactions between the   label sequences ( M’hamdi et al . , 2019 ) . However ,   we did not find substantial performance differences   when using a CRF layer and choose not to include   it to keep our model as simple as possible .   2.2.2 Teacher Adversarial Training   Pre - trained multilingual language models such as   mBERT or XLM - R provide contextualized rep-   resentations for word sequences in multiple lan-   guages by embedding the words into a shared multi-   lingual latent space . However , several studies have   shown that , in such multilingual latent space , words   from the same language group together , creating   language clusters ( Nguyen et al . , 2021 ; Yarmoham-   madi et al . , 2021 ) . As such , the word representa-5416tions generated by these encoders are not language   invariant . For a cross - lingual model , however , it   is beneficial for similar words in the source and   target languages to have similar ( i.e. close ) rep-   resentations in the latent space . For instance , an   English - trained Spanish - tested cross - lingual model   would benefit if the representations for the words   dogandperro were similar to each other as then the   model could adequately handle the Spanish sam-   ple provided it learns how to handle its English   counterpart during training .   A technique that has been frequently used to   promote the generation of such language - invariant   representations is Adversarial Language Adapta-   tion ( ALA ) ( Joty et al . , 2017 ; Chen et al . , 2018 ) .   ALA introduces a language discriminator network   Dwhose objective is to differentiate between the   source and target languages . It learns language-   dependent features that allow it to classify word   representations as belonging to either the source   or target languages . Concurrently , the encoder   network is trained in an adversarial manner : it   attempts to fool the discriminator by generating   language - independent representations that are dif-   ficult to classify . A key feature of ALA is that it   only requires unlabeled target - language data and ,   as such , it can be applied in a zero - shot setting   using the available Ssentence set .   Other works that have leveraged ALA perform   adversarial training at the sequence level ( Guzman-   Nateras et al . , 2022 ) . That is , they only present   the discriminator with sequence - level representa-   tions ( e.g. , the representation for the [ CLS ] token in   mBERT ) . However , in this work we leverage token-   level adversarial training which has been found to   be more effective at generating language - invariant   representations ( Chen et al . , 2021 )   We again use a two - layer FFNN for the discrim-   inator network D. Instead of a softmax operation   to generate a probability distribution , we employ   a sigmoid function σto predict the associated lan-   guage l(Eq . 4 ) .   D(l ) = σ(WReLU ( Wh ) ) ( 4 )   where WandWare parameter matrices to be   learned and D(l)is a scalar ∈[0,1]that indicates   how likely it is that the current token representation   hbelongs to the source ( l= 0 ) or target ( l= 1 )   languages .   Thus , besides the ED classification loss Lde-   scribed in Equation 2 , adversarial training intro - duces the discriminator loss L(Eq . 5 ) as an addi-   tional training signal .   L= ( 5 )   1   n∗m / summationdisplay / summationdisplayl · D(h ) + ( 1 −l)(1− D(h ) )   Our adversarial training is achieved by minimiz-   ing the following term :   argmin / summationdisplay / summationdisplay(L(y|h)−λL(l|h))(6 )   We leverage a Gradient - Reversal Layer ( GRL )   ( Ganin and Lempitsky , 2015 ) to implement Equa-   tion 6 by applying the GRL to the discriminator   input vectors h. A GRL acts as the identity func-   tion during the forward pass and reverses the di-   rection of the gradients during the backward pass .   As such , the encoder parameters are trained in the   opposite direction to those of the discriminator , ef-   fectively learning to generate token representations   with language - invariant features .   Figure 1 shows the architecture of the Teacher   model.54172.2.3 Student Model   As described in the previous section , the teacher   model is trained using a direct transfer approach : it   learns to generate language - independent represen-   tations from the labeled source - language data so   that it can be directly applied to unlabeled target-   language data . However , in our proposed hybrid   knowledge transfer approach , we expect the stu-   dent model to reap the benefits of the data transfer   paradigm . Hence , we train the student model us-   ing target - language data so that it may learn from   syntactical features and word / label relations .   First , we apply the teacher model Teach to the   unlabeled target dataset Sto obtain a pseudo-   labeled training set S . Afterward , the student   model Student is trained in a supervised manner   using the obtained pseudo - labels .   The model architecture of our student model   mirrors the one of the teacher model : a pre - trained   multilingual language model as the encoder E   and a two - layer FFNN for a classifier C.   C(y ) = softmax ( WReLU ( Wh ) )   ( 7 )   Previous works on knowledge distillation have   found that using soft labels ( i.e. , probability distri-   butions over class types ) is beneficial for student   learning as they contain richer and more helpful in-   formation than hard labels ( Hinton et al . , 2015 ) . As   such , we train the student model to minimize the   Mean Squared Error ( MSE ) between the student-   predicted and teacher - generated event - type distri-   butions ( Eq . 8) .   L = ( 8)   1   n∗m / summationdisplay / summationdisplay(C(E(t))− C(E(t ) ) )   2.3 Student - Training Sample Selection   An important challenge in our teacher - student   framework is that the target pseudo - labels ob-   tained from the teacher model are prone to contain   noisy predictions . The teacher model is trained   with a direct transfer approach and , even though   its word representations are encouraged to be   language - independent through adversarial training ,   it learns task - related information exclusively from   the source - language labels . We argue this preventsthe teacher from learning task - specific information   in the target language as it is unable to exploit the   word - label relations specific to such language . Fur-   thermore , even though the student model should   be able to benefit from being trained in the target   language , any potential benefits can be nullified if   the quality of the teacher - generated pseudo - labels   is too poor .   To address the aforementioned issue , we argue   that the teacher model should produce more reli-   able pseudo - labels on target - language examples   that share some similarities ( structural or other-   wise ) with the source - language examples . Hence ,   we suggest improving the knowledge - distillation   process by restricting student - model training to   target - language examples with such desirable char-   acteristics . We implement this idea by designing   a two - step hierarchical sample - selection scheme :   First , we leverage Optimal Transport ( OT ) ( Vil-   lani , 2008 ) to generate an alignment score between   source and target samples and select samples above   a defined alignment threshold . Then , using the se-   lected source and target samples , we compute their   pairwise Cross - domain Similarity Scaling scores   ( CSLS , Conneau et al . , 2018 ) and only keep the   pairs with the highest similarities . The following   subsections describe each step in further detail .   Figure 2 presents an overview of our teacher-   student framework .   2.3.1 Optimal - Transport - based Selection   Recent research efforts have successfully leveraged   OT for cross - lingual language adaptation ( Phung   et al . , 2021 ; Guzman - Nateras et al . , 2022 ) and   word - label alignment for event detection ( Pouran   Ben Veyseh and Nguyen , 2022 ) . OT relies on a   distance - based cost function to compute the most   cost - effective transformation between two discrete   probability distributions by solving the following   optimization problem :   π(x , z ) = min / summationdisplay / summationdisplayπ(x , z)D(x , z )   ( 9 )   s.t.x∼P(x)andz∼P(z )   In Eq . 9 , Dis a cost function that maps XtoZ ,   D(x , z),X ×Z −→ R , P(x)andP(z)are prob-   ability distributions for the XandZdomains , and   π(x , z)is the optimal joint distribution over the   set of all joint distributions / producttext(x , z)(i.e . , the opti-   mal transformation between XandZ).5418   For our work , we consider the source and target   languages as the XtoZdomains to be aligned .   Each training sample corresponds to a data point   in a distribution and is represented by its sentence-   level encoding h. Following prior work ( Pouran   Ben Veyseh and Nguyen , 2022 ) , we estimate prob-   ability distributions P(x)andP(z)using a single-   layer FFNN and use Euclidean distance as the cost   function :   D(h , h ) = ||h−h|| ( 10 )   where his the i - th source - language sample and   his the j - th target - language sample .   Once the OT algorithm converges , we leverage   the solution matrix πto compute an overall simi-   larity score kfor each sample hby averaging the   optimal cost of transforming it to the other domain :   k=/summationtextπ(h , h )   m(11 )   Finally , a hyperparameter αdetermines the pro-   portion of samples with the highest similarity   scores kto be selected for use in the next step .   2.3.2 CSLS - based Selection   The OT - based similarity score described previously   captures the global alignment of a sample with   the alternate language , e.g. , how well a source-   language sample aligns with the target language   and vice versa . Nonetheless , we propose to further   refine our sample selection by considering the pair-   wise similarity between source and target samples .   To this end , we make use of the CSLS simi-   larity measure which was originally designed to   improve word - matching accuracy in word - to - word   translation ( Wu et al . , 2020b ) . CSLS addresses a   fundamental issue of pair - matching methods based   on Nearest Neighbors ( NN ): NNs are asymmetric   by nature , i.e. if ais a NN of b , bis not necessarilya NN of a. In high - dimensional spaces , this asym-   metry leads to hubness , a detrimental phenomenon   for pair matching : samples in dense areas have high   probabilities of being NN to many others , while   samples that are isolated will not be a NN to any   other sample ( Conneau et al . , 2018 ) .   As such , when computing the similarity between   a pair of samples , CSLS ( Eq . 12 ) computes mean   similarity rof a sample to its neighborhood N   ( i.e. , its Knearest neighbors ) in the alternate lan-   guage and leverages it to increase the similarity   scores of isolated samples while decreasing the   scores of so - called hubsamples . For example , the   mean similarity rfor source sample his com-   puted with its target neighborhood N(Eq . 13 ) .   CSLS ( h , h ) = ( 12 )   2cos(h , h)−r(h)−r(h )   r(h ) = 1   |N|/summationdisplaycos(h , h)(13 )   r(h ) = 1   |N|/summationdisplaycos(h , h)(14 )   where cosis the cosine similarity . In our work , the   source Nand target Nneighborhoods are de-   fined as the corresponding sample sets kept by the   previous selection step . Again , we keep a propor-   tion of the samples with the best pairwise similarity   scores determined by a hyperparameter β .   Figure 3 presents an overview of our proposed   hierarchical sample - selection strategy .   3 Experiments   3.1 Datasets and Hyperparameters   For our experiments , we leverage the   ACE05 ( Walker et al . , 2006 ) and ACE05-   ERE ( Song et al . , 2015 ) datasets as they are   the most commonly used datasets for CLED .   ACE05 contains ED annotations in 3 languages:5419   English ( En ) , Chinese ( Zh ) , and Arabic ( Ar )   while ACE05 - ERE annotates data for English   and Spanish ( Es ) . In addition , we evaluate   our model on the recently released MINION   dataset ( Pouran Ben Veyseh et al . , 2022 ) , which   contains annotations for 8 morphologically and   syntactically distinct languages : English , Spanish ,   Hindi ( Hi ) , Japanese ( Ja ) , Korean ( Ko ) , Polish ( Pl ) ,   Portuguese ( Pt ) , and Turkish ( Tr ) . For a fair   comparison , we follow the same train / val / test   splits as prior work ( M’hamdi et al . , 2019 ; Pouran   Ben Veyseh et al . , 2022 ) .   We tune all hyperparameters on the validation   sets . In particular , we use AdamW ( Loshchilov and   Hutter , 2017 ) as the optimizer . We approximate   the solution to the intractable problem described   by Equation 9 by solving its entropy - based relax-   ation via the Sinkhorn iterative algorithm ( Cuturi ,   2013 ) . Following prior works ( Wu et al . , 2020b ) ,   we freeze the embeddings and first three layers of   the XLM - R encoder for student training . Learn-   ing rates for the transformer and non - transformer   parameters are set at 2eand1erespectively .   Theαandβhyperparameters are set at 0.5and   0.75respectively . We employ a batch size of 32   for the experiments on ACE05 and a batch size   of 16 for the experiments on MINION . The size   of the hidden feed - forward layers is 300 . We use   a learning rate linear scheduler with 5 warm - up   epochs for teacher models and 10 warm - up epochs   for student models . We use a parameter weight   decay of 0.5for transformer parameters and 1e   for non - transformer parameters . Finally , we train   the teacher model for 20 epochs and the student   model for 100 epochs .   3.2 Main results   In order to evaluate our Hybrid Knowledge Trans-   fer for Cross - Lingual Event Detection ( HKT - CLED ) model , we first present our results on the   ACE05 and ACE05 - ERE datasets in Table 1 . We   compare against 6 recent CLED efforts including   the current state - of - the - art model ( Guzman - Nateras   et al . , 2022 ) . All the baseline results are taken di-   rectly from the original papers and our model ’s re-   sults are the average of 5 runs with different seeds .   English is used as the sole source language and Ara-   bic , Chinese , and Spanish are employed as target   languages . Following previous works , we report   F1 scores .   Our proposed approach obtains new state - of-   the - art performance across all 3 target languages   with improvements of +0.58,+1.51 , and +0.89   F1 points for Chinese , Arabic , and Spanish , re-   spectively . We believe these results demonstrate   the importance of hybrid knowledge transfer as it   gives HKT - CLED an edge over previous works that   follow a direct transfer approach ( M’hamdi et al . ,   2019 ; Majewska et al . , 2021 ; Nguyen et al . , 2021 ;   Guzman - Nateras et al . , 2022 ) .   To validate the effectiveness and general appli-   cability of our approach , Table 2 presents the per-   formance of our HKT - CLED model on the more   diverse MINION dataset . Once again , we employ5420   English as the source language and test our model ’s   performance on the remaining 7 languages . For a   fair comparison , we use their best XLM - R results .   Our model consistently outperforms their reported   baseline with an average performance improvement   of+7.74F1 points for all target languages ( +5.25   if the highest and lowest improvements are not   considered ) . In the case of Japanese , HKT - CLED   obtains a massive performance improvement of   over 25 F1 points . Also of note is that HKT - CLED   performance is a lot more uniform across target lan-   guages than the baseline . There is a difference of   23.43F1 points between the best - performing ( Pt ,   77.28 ) and the worst - performing ( Tr , 53.85 ) target   languages , as opposed to a 37.65point difference   in the baseline case ( Pt , 72.77and Ja , 35.12 ) .   3.3 Analysis   3.3.1 Ablation Study   We first explore the contribution of each model   component by performing an ablation study ( Ta-   ble 3 ) . In particular , we evaluate the impact of   three aspects : teacher adversarial training , OT-   based sample selection , and CSLS - based sample   selection . The Teacher ( Vanilla ) results were ob-   tained with a standard sequence - labeling model   without any adversarial training . Its performance   leaves room for improvement as its word repre-   sentations do not display any language - invariant   qualities . A considerable improvement is achieved   when training the teacher model with token - level   adversarial training ( Teacher + Adv ) . Then , the   Student ( Vanilla ) row shows the result of training a   student network on the teacher - generated pseudo-   labels without any sample selection . We argue its   performance is worse than the adversarially - trained   teacher due to the noisy pseudo - labels . By incor-   porating OT - based selection , Student + OT is able   to outperform its teacher . However , it is only by   performing our hierarchical sample selection that   the student model achieves new state - of - the - art per - formance .   3.3.2 Impact of Sample - Selection Ratios   Figure 4 shows the impact of hyperparameter αon   model performance . αdetermines the proportion   of student - training samples kept by the OT - based   selection step . An α= 1value performs no sample   selection and α= 0.25only keeps a fourth of the   batch samples with the highest similarity scores .   Best results are obtained when half of the sam-   ples are kept ( α= 0.5 ) exemplifying the impor-   tance of removing training examples with poten-   tially noisy pseudo - labels . However , if too few   samples are chosen ( e.g. , α= 0.25 ) the student per-   formance drops below its vanilla version ( α= 1 ) .   Similarly , Figure 5 presents the effect on per-   formance of hyperparameter βwhich defines the5421proportion of samples kept by the CSLS - selection   step . A β= 1value uses all of the samples selected   by the previous step .   Removing about a quarter ( β= 0.75 ) of the   previously - selected samples improves performance   across all languages . Of note is the fact that the OT   and CSLS similarity scores complement each other .   From Figure 4 it would seem that removing more   than half of the training samples would only hurt   performance . However , given CSLS pairwise fo-   cus , it is able to effectively remove some remaining   noisy samples and obtain better results .   4 Related Work   Event detection ( ED ) is an active research area in   NLP ( Nguyen and Grishman , 2015 , 2018 ; Pouran   Ben Veyseh et al . , 2021 ) , featuring cross - lingual   ED as a recent direction with growing interests .   The work by Liu et al . ( 2019 ) presents a data trans-   fer method that learns a mapping between mono-   lingual word embeddings , translates the source   training data on a word - by - word basis and uses   a graph convolutional network to generate order-   independent representations . M’hamdi et al . ( 2019 )   leverage mBERT as an encoder to perform zero-   shot transfer learning and a CRF layer to account   for label dependency . Lu et al . ( 2020 ) present   a cross - lingual structure transfer approach that   represents sentences as language - universal struc-   tures ( trees , graphs ) . In their work , Majewska   et al . ( 2021 ) argue that event triggers are usu-   ally related to the verb in a sentence and pro-   pose to incorporate external verb knowledge by   pre - training their encoder to classify whether two   verbs belong to the same class according to two   distinct ontologies VerbNet , ( Kipper et al . , 2006 )   and FrameNet , ( Baker et al . , 1998 ) . Model prim - ing(Fincke et al . , 2021 ) is a simple , yet effective   method that consists in augmenting the encoder   inputs by concatenating a candidate trigger to the   input sentence so that the encoder learns to generate   task - specific representations . Nguyen et al . ( 2021 )   leverage class information and word categories as   language - independent sources of information and   condition their encoder to generate representations   that are consistent in both the source and target   languages . Finally , Guzman - Nateras et al . ( 2022 )   propose to optimize standard adversarial language   adaptation by restricting the language discriminator   training to informative examples .   Our approach is also closely related to knowl-   edge distillation models for cross - lingual Named   Entity Recognition ( NER ) . Wu et al . ( 2020a ) were   the first to train a NER student model on the la-   bel distributions obtained from a teacher model .   Wu et al . ( 2020b ) improved upon this initial ap-   proach with a multi - step training method that in-   volved fine - tuning the teacher model with pseudo-   labeled data and generating hard labels that were   later used for student training . More recent propos-   als improve the knowledge distillation with either   reinforcement learning ( Liang et al . , 2021 ) or ad-   versarial training ( Chen et al . , 2021 ) . Nonetheless ,   our approach is the first to leverage a knowledge   distillation framework for CLED , and our novel hi-   erarchical training - sample selection scheme further   differentiates our work from previous efforts .   5 Conclusion   In this work , we present the first effort to lever-   age a hybrid knowledge - transfer approach for the   cross - lingual event detection task . We propose a   teacher - student framework complemented by a hi-   erarchical training - sample selection scheme that   effectively constrains the student - training process   to pseudo - labeled target - language samples that   are similar to their source - language counterparts .   Our HKT - CLED model sets a new state - of - the - art   performance on the most popular benchmarking   datasets ACE05 and ACE05 - ERE , and obtains sub-   stantial performance improvements on the recently-   released , and more diverse , MINION dataset with   an average improvement of +7.74F1 points across   7 distinct target languages . We believe these results   demonstrate our model ’s robustness and applica-   bility and validate our claim that combining the   benefits of the direct transfer and data transfer ap-   proaches is beneficial for cross - lingual learning.5422Limitations   We strived to make this work as accessible and ap-   plicable as possible . However , as with any other   research effort , it suffers from several limitations   stemming from preconceived assumptions . We be-   lieve that the most important limitation of our work   is the assumption of the existence of a pre - trained   multilingual language model , to be used as an en-   coder , that supports both the desired source and   target languages . Though most modern multilin-   gual language models support over a hundred lan-   guages , with over 7000 spoken languages in the   world , the vast majority of languages remain un-   supported . That being said , language models are   trained in an unsupervised manner , meaning that   only unlabeled data is required for training pur-   poses . As such , a suitable encoder could be trained   provided there is access to enough unlabeled data .   This leads to what we consider to be the second   biggest limitation of our work : the assumption of   the availability of unlabeled target - language data .   In general , raw unlabeled data is easy to obtain   for most languages . However , it can represent a   challenge for extremely low - resource languages .   In these special cases , training an effective encoder   can be an impossibility which , in turn , limits the   applicability of our approach . Other limitations   stem from our constrained time and computational   resources . Our method requires a GPU with a large-   enough memory to fit the transformer - based en-   coder which is usually more than what a personal   computer GPU provides . Depending on the dataset   and selected batch size , our model requires between   15 and 32 GB of GPU memory . We performed all   our experiments on a Tesla V100 GPU with 32 GB .   Finally , additional experiments on a more diverse   set of source / target language pairs could certainly   provide a more comprehensive overview of our   method ’s strengths and weaknesses .   Acknowledgement   This research has been supported by the Army Re-   search Office ( ARO ) grant W911NF-21 - 1 - 0112 ,   the NSF grant CNS-1747798 to the IUCRC Center   for Big Learning , and the NSF grant # 2239570 .   This research is also supported in part by the Office   of the Director of National Intelligence ( ODNI ) ,   Intelligence Advanced Research Projects Activity   ( IARPA ) , via the HIATUS Program contract 2022-   22072200003 . The views and conclusions con-   tained herein are those of the authors and shouldnot be interpreted as necessarily representing the   official policies , either expressed or implied , of   ODNI , IARPA , or the U.S. Government . The U.S.   Government is authorized to reproduce and dis-   tribute reprints for governmental purposes notwith-   standing any copyright annotation therein .   References542354245425ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 6 Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . We currently do not identify any potential risks inherently associated with our work .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Section 1 Introduction   /squareA4 . Have you used AI writing assistants when working on this paper ?   We did not use any AI writing assistants .   B / squareDid you use or create scientiﬁc artifacts ?   Section 2 describes our model which we created using the Pytorch library . The appendix B contains   our hyperparameter values and discusses additional implementation details .   /squareB1 . Did you cite the creators of artifacts you used ?   Not limited to a speciﬁc section . We cite all the original papers from artifacts such as the multilingual   pre - trained language models we use as econders .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We did not explicitly discuss the license or terms of use of the artifacts as they are publically available   on the original sites .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   We did not explicitly discuss in or work on its intended use . We intend to release our work publicly   under Apache License 2.0 .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Our datasets are widely used in previous research for Multilingual Event Detection . We do not   observe concerns for private information or offensive content in our datasets in previous work .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 2 Model , Section 3 Experiments , Section 4 Analysis , Appendix A and Appendix B   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3 Experiments , Appendix A , and Appendix B5426C / squareDid you run computational experiments ?   Section 3 Experiments   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix B Implementation details   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix B Implementation details   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 3 Experiments , Section 4 Analysis   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3 Experiments , Appendix B   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   We did not use any human annotators or research with human objects .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.5427