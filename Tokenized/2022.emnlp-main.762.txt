2   Zhihua Jiang , Guanghui Ye , Dongning Rao , Di Wang , Xin MiaoDepartment of Computer Science , Jinan University , Guangzhou 510632 , P. R. ChinaSchool of Computer , Guangdong University of Technology , Guangzhou 510006 , P. R. ChinaSchool of Computer Science , Wuhan University , Wuhan 430072 , P.R. China   tjiangzhh@jnu.edu.cn , yghljf@stu2020.jnu.edu.cn , raodn@gdut.edu.cn ,   windi@stu2020.jnu.edu.cn , miaoxin@whu.edu.cn   Abstract   Evaluation metrics shine the light on the best   models and thus strongly influence the research   directions , such as the recently developed dia-   logue metrics USR , FED , and GRADE . How-   ever , most current metrics evaluate the dia-   logue data as isolated and static because they   only focus on a single quality or several qual-   ities . To mitigate the problem , this paper pro-   poses an interpretable , multi - faceted , and con-   trollable framework IM(Interpretable and   Multi - category Integrated Metric ) to combine   a large number of metrics which are good at   measuring different qualities . The IMframe-   work first divides current popular dialogue qual-   ities into different categories and then applies   or proposes dialogue metrics to measure the   qualities within each category and finally gen-   erates an overall IMscore . An initial ver-   sion of IMwas submitted to the AAAI 2022   Track5.1@DSTC10 challengeand took the   2place on both of the development and test   leaderboard . After the competition , we develop   more metrics and improve the performance of   our model . We compare IMwith other 13   current dialogue metrics and experimental re-   sults show that IMcorrelates more strongly   with human judgments than any of them on   each evaluated dataset .   1 Introduction   Because human evaluation for natural language   generation ( NLG ) systems is both expensive and   time - consuming , relevant and meaningful auto-   matic evaluation metrics that strongly correlate   with human judgments are crucial . However , asthe one - to - many natures of dialogue makes stan-   dard automatic language evaluation metrics ( e.g. ,   BLEU and METEOR ) ineffective for evaluating   open - domain dialogue systems ( Liu et al . , 2016 ) ,   many automatic evaluation metrics specifically de-   signed for dialogue have been recently proposed   ( Lan et al . , 2020 ; Sinha et al . , 2020 ; Huang et al . ,   2020 ; Ghazarian et al . , 2020 ; Li et al . , 2021 ; Mehri   and Eskénazi , 2020b ; Zhang et al . , 2020a ; Pang   et al . , 2020 ; Phy et al . , 2020 ) .   Although these dialogue metrics correlate with   human evaluation , they focus on a single qual-   ity or a few qualities , thus evaluating the dia-   logue data as isolated and static , e.g. , GRADE   ( Huang et al . , 2020 ) evaluates the topic coher-   ence of dialogue and PredictiveEngage ( Ghazar-   ian et al . , 2020 ) estimates the user engagement .   Therefore , multi - quality metrics are preferred , e.g. ,   FED ( Mehri and Eskénazi , 2020a ) measures 9   turn - level qualities and 11 dialogue - level qualities   for predicting the overall impression score . How-   ever , the generalization capability of existing multi-   quality metrics is questionable , e.g. , FED corre-   lates poorly with human judgments when scoring   other dialogues outside its own data . Recently , the   Track5.1@DSTC10 challenge ( Zhang et al . , 2021c )   just ended , whose purpose is to develop effective   automatic open - ended dialogue evaluation metrics   that perform robustly across a range of dialogue   tasks . No individual metric will be competitive .   Therefore , recent work attempted to combine   dialogue evaluation metrics : 1 ) combining USR   ( Mehri and Eskénazi , 2020b ) , GRADE ( Huang   et al . , 2020 ) , PONE ( Lan et al . , 2020 ) and Pre-   dictiveEngage ( Ghazarian et al . , 2020 ) through   simple - averaging has been reported in a compre-   hensive assessment of dialogue evaluation metrics   ( Yeh et al . , 2021 ) ; 2 ) USL - H ( Phy et al . , 2020 )   divides dialogue qualities into three categories   ( viz . U , S , L ) and linearly combines them ; 3 )   the Track5.1@DSTC10 baseline , Deep AM - FM11091(Zhang et al . , 2020a ) , is a simply combined metric   which measures the Adequacy Metric ( AM ) and the   Fluency Metric ( FM ) simultaneously . However , the   above combinations are straightforward , and thus   exploring more sophisticated combination mecha-   nisms has been claimed as an important direction   for future work ( Yeh et al . , 2021 ) .   On that ground , this paper proposes a novel   metric framework named IM(Interpretable and   Multi - category Integrated Metric ) , which first di-   vides current dialogue qualities into three cate-   gories , and then applies or proposes dialogue met-   rics ( named sub - metrics ) to measure the qualities   within each category , and finally generates an over-   all evaluation score . The three quality categories   are : 1 ) NUF ( Natural , Understandable , and Fluent ) ,   which measures the basic quality of the response ;   2 ) CR ( Coherent and Relevant ) , which measures   the response ’s quality conditioned on the context ;   3 ) IES ( Interesting , Engaging , and Specific ) , which   measures the special property of the response . Par-   ticularly , IMleverages the multi - level integration ,   i.e. , first producing categorical metrics by integrat-   ing on sub - metrics and then producing the overall   metric by integrating on categorical metrics .   The contribution of this paper is two - fold :   1.We proposed a novel framework for combing auto-   matic dialogue evaluation metrics . The proposed   IMis : 1 ) reference - free , which does not need ref-   erence responses ; 2 ) interpretable , which integrates   fine - grained sub - metrics and meaningful categor-   ical metrics ; 3 ) flexible , which allows categorical   metrics to be used independently .   2.We submitted an early version of IMto the AAAI   2022 Track5.1@DSTC10 challenge and obtained   a high average Spearman correlation coefficient   0.3937 on the development datasets and 0.2819 on   the test datasets . After the competition , we fur-   ther improved the correlation score to 0.4645 and   0.3510 respectively , via developing more metrics .   2 Related Work   2.1 Dialogue Evaluation Metrics   This subsection describes individual dialogue met-   rics , which can be divided into two categories : rule-   based and model - based ( Yeh et al . , 2021 ) , where   rule - based metrics use heuristic rules to evaluatethe system response while model - based metrics are   trained on specific dialogue data .   Rule - based metrics have been proposed for stan-   dard language evaluation for at least two decades ,   e.g. , BLEU , METEOR , and ROUGE . BLEU ( Pap-   ineni et al . , 2002 ) is a popular metric that computes   the n - gram precision of the system responses using   human references and is often used to benchmark   NLG systems . Further , METEOR ( Banerjee and   Lavie , 2005 ) and ROUGE ( Lin , 2004 ) have been   proposed to address the shortcomings of BLEU ,   where METEOR incorporates stems and synonyms   into its calculation while ROUGE focuses on the   n - gram recall instead of precision .   In contrast , model - based dialogue metrics have   sprung up in recent years , e.g. , ADEM , RUBER ,   BERT - RUBER , PONE , MAUDE , GRADE , Pre-   dictiveEngage , FED , FlowScore , and DynaEval .   ADEM ( Lowe et al . , 2017 ) is an early metric de-   signed for dialogue , which uses a recurrent neural   network ( RNN ) to predict the cosine similarity be-   tween system and reference responses . RUBER   ( Tao et al . , 2018 ) uses a hybrid model which com-   prised both a referenced metric and an unreferenced   metric . Later , BERT - RUBER ( Ghazarian et al . ,   2019 ) is proposed to replace RNN with BERT ( De-   vlin et al . , 2019 ) . Based on BERT - RUBER , PONE   ( Lan et al . , 2020 ) uses a novel algorithm to sample   negative examples during training . MAUDE ( Sinha   et al . , 2020 ) is trained with Noise Contrastive Es-   timation . GRADE ( Huang et al . , 2020 ) models   topic transition dynamics in dialogue by construct-   ing a graph representation of the dialogue history .   PredictiveEngage ( Ghazarian et al . , 2020 ) incor-   porates an utterance - level engagement classifier .   FED ( Mehri and Eskénazi , 2020a ) uses DialoGPT   ( Zhang et al . , 2020b ) to measure fine - grained qual-   ities of dialogue . FlowScore ( Li et al . , 2021 ) con-   structs dynamic information flow from the dialogue   history . DynaEval ( Zhang et al . , 2021a ) evaluates   the dialogue in both turn - level and dialogue - level .   2.2 Metrics Combination   This subsection describes previous studies on com-   bining dialogue metrics , including Deep AM - FM ,   HolisticEval , USR , and USL - H. Deep AM - FM   ( Zhang et al . , 2020a ) measures two aspects of dia-   logue quality through adequacy and fluency . Holis-   ticEval ( Pang et al . , 2020 ) evaluates more qualities   of dialogue : context coherence , language fluency ,   response diversity , and logical self - consistency.11092However , both Deep AM - FM and HolisticEval are   simply combined . To the best of our knowledge ,   the most related work to ours is USR and USL - H.   They exploit a comparatively complex combination   mechanism . USR ( Mehri and Eskénazi , 2020b )   trains three models to evaluate different dialogue   qualities : a language model which measures the   fluency ; a dialogue model which determines the rel-   evance ; a selection model which checks the knowl-   edge use . USL - H ( Phy et al . , 2020 ) splits dialogue   qualities into three groups : Understandability ( U ) ,   Sensibleness ( S ) , and Likability ( L ) . Then it com-   posites these groups in a linear hierarchy ( H ) . For   more details on the above - mentioned dialogue met-   rics , we refer the readers to ( Yeh et al . , 2021 ) .   Although both USL - H and IMdivide dialogue   metrics into three categories , the differences are   specific qualities in categories , the relationship be-   tween categories , and the integration mechanism .   USL - H decomposes the structure of a response   quality in a hierarchy and supposes that understand-   ability is the basis of the whole dialogue quality .   If a dialogue is not understandable , then one can-   not measure its sensibleness or likability . On the   contrary , our categories are designed independently   and integrated at multiple levels . See Table 13 in   Appendix A.3 for more comparisons .   3 Problem Statement   The proposed framework is reference - free , which   scores the system response without human refer-   ence(s ) . Formally , given a dialogue context cand   a system response r , the goal is to learn a scoring   function f : ( c , r)→sthat evaluates the gener-   ated response . Dialogue metrics are assessed by   comparing them to human judgments . Concretely ,   a human annotator or several annotators score the   quality of a response conditioned on the dialogue   context : ( c , r)→q . Given the scores produced   by a metric , S={s , ... , s } , and the correspond-   ing human quality annotations , Q={q , ... , q } ,   we can measure the performance of the metric by   calculating the correlation between SandQ.   4 The IMFramework   4.1 The Overall Architecture   As shown in Figure 1 , the IMframework pro-   duces an overall evaluation score given by a   context - response pair . Training and evaluating   our model with the standard development data of   Track5.1@DSTC10 , we divide the quality metricsof the released development datasets into three cat-   egories : NUF , CR , and IES . The NUF category   measures the response ’s naturalness , understand-   ableness , and fluency , the CR category measures   the response ’s coherency and relevance conditioned   on the context , and the IES category measures the   response ’s interestingness , engagement , and speci-   ficity . Table 12 in Appendix A.3 exhibits more   detailed descriptions of these qualities .   Through extensive experiments that specify di-   alogue metrics ( i.e. , sub - metrics ) to measure the   qualities within each category , we notice that ap-   plying or adapting existing metrics is not sufficient   to improve the combined - metric ’s performance   greatly . Therefore , we proposed new sub - metrics   that can be trained on the evaluation data and deter-   mine three sub - metrics for each quality category ,   as shown in Table 1 . The many - to - many relation-   ships between sub - metrics and qualities are also   illustrated in Figure 1 .   4.2 The Categorical Data   For better training new metrics models , we   generate three categorical datasets named the   NUF , CR , and IES data , and one Overall data ,   from the 14 released development datasets of   Track5.1@DSTC10 . Specifically , for any category ,   if an original dataset is human - annotated with at   least one member quality , all of its dialogue will be   collected into the corresponding categorical data .   Comparatively , the NUF / CR / IES data is used to   train sub - metrics , while the Overall data is used   to train the overall metric . See Appendix A.4 for   more details of categorical data generation .   4.3 The Sub - metrics   This subsection describes how to train sub - metrics   used in IM . As shown in Table 1 , a sub - metric   can be directly applied , adapted with a little modi-   fication , or proposed by ourselves . There are three   pre - trained language models ( PTMs ) used in our   training : BERT ( Devlin et al . , 2019 ) , RoBERTa   ( Liu et al . , 2019 ) , and DialogGPT ( Zhang et al . ,   2020b ) . For most sub - metrics , we try each PTM   and choose the best - performing one as the final11093   choice . The further discussion on how the PTM   choice affects our training result ( Zhang et al . ,   2021b ) will be left as a future work .   •GRADE ( Huang et al . , 2020 ) . We run GRADE via   following its original settings .   •AB - BA . We propose this metric to enhance the   coherence prediction by using the negative sam-   pling . Given a positive example < A , B > composed   of the context Aand the response B , we construct   a negative example < B , A > by shuffling AandB.   The new pair < B , A > is incoherent regarding the   original sentence order . Specifically , we train Di-   alogGPT on the pre - processed DailyDialog . Un-   like GRADE , AB - BA predicts the sentence - level   coherence instead of the topic - level coherence .   •AB - AC . Similar to AB - BA , we propose this metric   to enhance the relevance prediction by using neg-   ative sampling . Given the context Aand its true   response B , instead of random generation , we select   other dialogue ’s response Cwhich has the largest   cosine similarityregarding B , as a false response .   Because the chosen Cis coming from different dia-   logue , it is statistically but not assured to be false .   We train BERT on the same pre - processed Daily-   Dialog as that for AB - BA . Figure 3 in Appendix   A.5 shows an example for training AB - BA .   •LSC ( logical self - consistency ) . We propose thismetric to evaluate the naturalness . It is difficult to   give a clear definition of naturalness , e.g. , for hu-   man annotators with different culture background .   In our opinion , a sentence will be natural if it is   smooth and does not contain cause - and - effect er-   rors . Thus , we split a response sentence rinto   sub - sentences { r , ... r}separated by punctuation   marks and pack every two adjacent sub - sentences   randrinto a pair < r , r > . We send these   pairs to the well - trained AB - BA model , which uses   the coherence to check the smoothness , and take   the average of all AB - BA scores as the LSC score .   •5 - NUF ( 5 - class NUF metric ) . We propose this met-   ric to evaluate the NUF categorical quality , by sim-   ulating the human ’s 5 - point annotation scheme . We   train a 5 - class classifier on the NUF - data instead   of the released development data . Specifically , we   train RoBERTa via adding a top three - layer fully-   connected network and use Mean Square Error as   the training objective .   •VUP ( valid utterance prediction ) . This metric was   proposed by USL - H ( Phy et al . , 2020 ) . The au-   thors trained a model based on BERT to capture   the understandability of an utterance by classifying   whether it is valid . For doing this , they applied   many rules to get a negative sample , e.g. , word   reorder , word drop , and words repeat . We run VUP   via following the original setting .   •Dist - n. Dist - n measures the response ’s interesting-   ness by detecting unique words , where the more   unique words there are , the more interesting the   response is . Our adaption for this metric is to build11094a word list for each dialogue dataset , which records   the occurrences of each word in dialogue utterances   and thus is used to calculate the n - gram entropy of   the response , i.e. , the Dist - n score .   •D - MLM ( MLM for dialogue ) . Inspired by the   masked language model ( MLM ) prediction task   of BERT , we propose the D - MLM metric to mea-   sure the specificity . One word at a time , each word   in the response is masked , and its log - likelihood   is computed . Then , the normalized scores on all   words is the D - MLM score of the response sen-   tence . We fine - tune RoBERTa on PersonaChat and   TopicalChat , the joint use of which brings a higher   gain than using a single one .   •5 - IES ( 5 - class IES metric ) . Similar to 5 - NUF , we   propose this metric to evaluate the IES categorical   quality . The training details are identical except   that using the IES - data .   4.4 The Integration Mechanism   Using bi - linear regression , the IMscore is :   Where the weight coefficients w - wandα-   αare learnable . The linear function describes the   interpretability of the proposed framework .   4.5 The Selection Mechanism   Since IMcontains categorical metrics which can   be integrated separately , we design two different   strategies to use metrics for evaluation :   1.OVERALL . For any quality , we use the IM-   metric as a whole to measure it .   2.SELECTIVE . For a specific quality q , we select   the most appropriate metric to measure it .   The selection rules are :   •ifq∈NUF , we use the NUF - metric ;   •ifq∈CR , we use the CR - metric ;   •ifq∈IES , we use the IES - metric ;   •otherwise , we use the IM - metric .   Particularly , when qisoverall or an unseen qual-   ity , we will use the IM - metric . Further , the SE-   LECTIVE strategies can be applied to other com-   bined metrics only if their metric members can be   used independently . Table 13 in Appendix A.3   compares IMwith other combined metrics.5 Experiments   5.1 Datasets and Setup   There are 14 released development datasets and   5 hidden test datasets on the Track5.1@DSTC10   challenge ( Sedoc et al . , 2019 ; Zhang et al . , 2021c ) .   We train and evaluate on the development data and   verify the model ’s generality on the test data . See   Appendix A.2 for the details of the datasets .   We ran all metrics on a workstation which is   equipped with Linux , a single NVIDIA Tesla 32 GB   GPU , and Python 3.7 . About the training time ,   all sub - metrics were trained within 20 - 40 minutes ,   e.g. , AB - BA ( 35 minutes ) or 5 - IES ( 20 minutes ) .   The training time is pertinent to the dataset size .   About the running time , all sub - metrics ran for   about 2 minutes on a single dataset , except for   GRADE which ran longer ( 5 minutes ) .   5.2 Primary Results   We report our experimental results on the released   development datasets in Table 3 , along with the   official results ( the SOTA teams and our team ) in   Table 4 , for comparison . The weight coefficients   which lead to the results of IMare shown in Table   2 . It reveals that each component has a contribution   on the overall performance . There is 13 compared   other metrics in Table 3 , including 8 single metrics   and 5 combined metrics . All of them have been   introduced in Related Work Section .   We ran all metrics , including our IMand other   compared metrics , on each dataset . Some repro-   duction details are stated as follows :   •The correlation score on each dataset is the average   of correlation scores on evaluated qualities .   •Referred to ( Yeh et al . , 2021 ) , we calculate the   average of context coherence , language fluency   andlogical self - consistency , as the overall score   for HolisticEval , because response diversity is not   available on Track5.1@DSTC10 datasets .   •We reproduced ‘ PE+GRADE+USR ’ according to   ( Yeh et al . , 2021 ) .   •We experimented with the SELECTIVE strategy   on USL - H. The variant is named USL - H - selective ,   while the original is named USL - H - overall .   •The results of OVERALL and SELECTIVE are   same on D6 , GD , and ZP because they only contain   the ‘ overall ’ quality .   •‘- ’ means no score . The reasons are : ( 1 ) PONE and   BERT - RUBER can not score on PC because when   using their unreferenced - metrics , the correlation11095   coefficient can be calculated only if the dialogue   has a human - annotated “ relevance ” or “ coherence ”   score ; ( 2 ) FlowScore only scores dialogues with   more than 3 utterances , so it can not be used on ZD .   Experimental findings for Table 3 and 4 are :   •Even though not outperforming the SOTA - dev   team , which performed very poorly on test data   in Table 6 , IMperformed much better than all   other compared metrics on each dataset .   •The ‘ A VG ’ column reveals that the top-3 met-   rics are IM - selective , IM - overall , and USL-   H - selective , showing that SELECTIVE is more   effective than OVERALL , even for USL - H.   •Apart from IM , PE+GRADE+USR and GRADE   performed better than the others . However , they   are not stable , e.g. , the Pearson correlation of USR   is 0.4452 on UP , but 0.0974 on ED .   5.3 Ablation Studies   Performance on Hidden Test Datasets . We re-   port equivalent results on the hidden test datasets   in Table 5 , along with the official results ( the SOTA   teams and our team ) in Table 6 , for comparison .   The weight coefficients which lead to the results   ofIMon test data are same as those on develop-   ment data . We excluded 4 out of 14 previous met-   rics because they performed badly on development   data ( e.g. , their average Spearman correlation score   was smaller than 0.1 ) . There are two interesting   findings : 1 ) both IM - overall and IM - selective   outperformed the SOTA - test team ; 2 ) the gain of   SELECTIVE over OVERALL on test data is not   as significant as on development data . It is because   the test data were unseen during the training . Further , to validate the transferability of IM   across domains , we evaluate IMand other 6   competitive metrics on 2 truly unseen test sets :   Holistic ( Pang et al . , 2020 ) and dstc9 ( Gunasekara   et al . , 2020 ) . The former was proposed by the   HolisticEval metric and the latter was used for   Track3@DSTC9 . As in Table 7 , new results show   thatIMoutperforms all the others significantly ,   justifying its generalization performance .   Categorical Metrics . To verify the effectiveness   of categorical metrics , we conducted the ablation   study on categorical datasets . As shown in Table   8 , each categorical metric performed better than its   sub - metrics on categorical data .   Correlation to Qualities . We tested the cor-   relations of metrics to different annotation qual-   ities on one test dataset ( DSTC10 - Persona ) and   one development dataset ( FED ) , respectively . Take   DSTC10 - Persona as an example . Specifically , we   select the NUF metric for the grammar quality , the   CR metric for the relevance quality , the IES metric   for the content quality , and the IMmetric for the   appropriateness quality . The results on DSTC10-   Persona are shown in Figure 2 . For the space limit ,   the results on FED are shown in Figure 4 in Ap-   pendix A.6 . Results show that categorical metrics   were good at evaluating their specific qualities and   IMstrongly correlated to most qualities .   Most - appropriate Metrics . We conducted the   most - appropriate - metric test in this part . The re-   sult is shown in Table 9 . Each most - appropriate   metric was parenthesized following the combined   metric . This test validated the effectiveness of the   SELECTIVE strategy.1109611097   Linear Weighting vs. Simple Averaging . We   compared two approaches for setting weight co-   efficients : simple averaging and linear weighting .   The former took the arithmetic mean , while the   latter used the weight distribution in Table 2 . As   shown in Table 10 , either for IMor any categori-   cal metric , the linear regression obtained a higher   correlation score . It reveals that linear weighting is   more effective than simple averaging.11098   6 Conclusion   This paper explores the sophisticated mechanism   for combining dialogue metrics and proposed a   novel framework , IM . The experimental results   show that IMstrongly correlates with human   judgments and outperforms all compared metrics .   Further , our work reveals that training a perfect   metric model for all dialogue datasets is difficult ,   but selecting the most appropriate metric for differ-   ent dialogues is promising .   There are many future works . First , we will pay   more attention to challenge dialogue datasets , such   as those with lengthy context . Second , we will   merge qualities for newest competition tasks , such   as Track4@DSTC11 ( Robust and Multilingual Au-   tomatic Evaluation Metrics for Open - Domain Di-   alogue Systems ) . Third , we will attempt more   powerful dialogue systems , such as PLATO-2 ( Bao   et al . , 2021 ) which directs towards building an   open - domain Chatbot , and with the help from   theIMevaluation scores more human - style re-   sponses might be generated .   7 Limitations   Conversational AI is one of the most popular NLP   applications and developing flexible evaluation   frameworks that can emphasize different aspects of   quality is important . This paper proposes a novel   evaluation framework , which we call IM , for de-   livering exactly that . We conduct a comprehensive   set of experiments on this year ’s DSTC10 challenge   data , verifying the effectiveness of our model em-   pirically . However , there are two limitations in our   current work : ( 1 ) we utilize pretrained models such   as DialogGPT , BERT and RoBERTa for training   our sub - metrics and choose the best - performing   one as the final metric model . While , a deep - in   analysis of how the pretrained model choice affects   our training result following ( Zhang et al . , 2021b )   is unexplored . ( 2 ) we linearly combine various sub-   metrics and categorical metrics to generate the final   IMscore for the interpretability . However , a non-   linear combination mechanism such as training a   small neural network may bring more promising   results , which we leave as one of future works .   Ethics Statement   We use standard datasets which are publicly avail-   able . There is no ethics statement for this paper .   Acknowledgements   This paper is supported by Guangdong Basic and   Applied Basic Research Foundation , China ( Grant   No . 2021A1515012556).11099References1110011101   A Appendix   A.1 The Track5.1@DSTC10 challenge   The challenge goal is to seek effective automatic   dialogue evaluation metrics that exhibit the correla-   tion to human judgments and the explainability of   the evaluation behaviors . The submitted metric will   be ranked according to the average correlation on   all 14 open - domain dialogue development datasets .   Each team can submit at most five submissions   and use at most five metrics in each submission .   The metric baseline is Deep AM - FM . The leader-   board ( https://chateval.org/dstc10 ) shows names of   submissions and their corresponding Spearman cor-   relation coefficients for each development dataset   and each hidden test dataset .   We submitted an early version of IM - selective   ( team ID : T8 ) , which integrates four sub - metrics   ( VUP , GRADE , AB - BA , and D - MLM).A.2 Released development datasets   The development datasets of the   Track5.1@DSTC10 challenge consist of the   following 14 components :   •Twitter - DSTC6 ( D6 ) ( Hori and Hori , 2017 ) ;   •Reddit - DSTC7 ( D7 ) ( Galley et al . , 2019 ) ;   •Persona - see ( PC ) ( See et al . , 2019 ) ;   •Persona - USR ( UP ) ( Mehri and Eskénazi , 2020b ) ;   •Topical - USR ( TP ) ( Mehri and Eskénazi , 2020b ) ;   •FED - Turn ( FT ) ( Mehri and Eskénazi , 2020a ) ;   •FED - Dial ( FC ) ( Mehri and Eskénazi , 2020a ) ;   •DailyDialog - Zhao ( ZD ) ( Zhao et al . , 2020 ) ;   •Persona - Zhao ( ZP ) ( Zhao et al . , 2020 ) ;   •DailyDialog - Gupta ( GD ) ( Gupta et al . , 2019 ) ;   •DailyDialog - Huang ( ED ) ( Huang et al . , 2020 ) ;   •ConvAI2 - GRADE ( EC ) ( Huang et al . , 2020 ) ;   •Empathetic - GRADE ( EE ) ( Huang et al . , 2020 ) ;   •HUMOD ( HU ) ( Merdivan et al . , 2020 ) .   Many of these datasets were collected in differ-   ent settings . For example , DailyDialog consists of   causal conversations about daily life while Topi-   calChat consists of knowledge - grounded conver-   sations . The FED dataset provides human - system   dialogs that were collected in an interactive setting .   Specifically , FED data incorporates two state - of-   the - art dialogue systems , Meena ( Adiwardana et al . ,   2020 ) and Mitsuku . For more detailed descrip-   tions on the above - mentioned dialogue datasets ,   we refer the readers to ( Zhang et al . , 2021c ) .   A.3 Comparing IMwith other metrics   Table 12 describes all qualities used in our frame-   work . Table 13 compares IMagainst the above-   mentioned combined metrics from the number of   sub - metrics , qualities , PTMs , and training datasets .   A.4 Categorical data generation   We collect the dialogues from the   Track5.1@DSTC10 datasets to generate the   NUF / CR / IES / Overall data . To take the full   advantage of the original datasets , we make a   slight extension to the NUF / CR / IES category   via relaxing the types of qualities , as shown in   Table 11 . However , the Overall data is only   annotated with the overall quality . Comparatively ,   the NUF / CR / IES data is used to train and linear-   regress the sub - metrics , while the Overall data is   used to linear - regress the categorical metrics.11102   A.5 Example for training AB - BA   For AB - BA and AB - AC , we tested three pretrained   models ( DialogGPT , BERT and RoBERTa ) and   found that there were only slight differences be-   tween the results . We used the best - performing   one as the final model for each sub - metric . In par-   ticular , we added a fully - connected layer on the   top of DialogGPT to determine whether a gener-   ated response is coherent . An example for training   AB - BA is shown in Figure 3 .   A.6 The Correlation - to - qualities Test on FED   We tested the correlations of metrics to different   annotation qualities on one test dataset ( DSTC10-   Persona ) and one development dataset ( FED ) . The   results are shown in Figure 2 and 4 , respectively.11103