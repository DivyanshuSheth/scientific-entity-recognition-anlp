  Artem Vazhentsev , Gleb Kuzmin , Artem Shelmanov , Akim Tsvigun ,   Evgenii Tsymbalov , Kirill Fedyanin , Maxim Panov , Alexander Panchenko ,   Gleb Gusev , Mikhail Burtsev , Manvel Avetisian , and Leonid ZhukovAIRI , Skoltech , MIPT , HSE , Sber AI Lab , FRC CSC RAS , ISP RAS Research Center for Trusted Artiﬁcial Intelligence   { vazhentsev , kuzmin , shelmanov , tsvigun , gusev , burtsev , manvel , zhukov}@airi.net   { evgenii.tsymbalov , m.panov , k.fedyanin , a.panchenko}@skoltech.ru   Abstract   Uncertainty estimation ( UE ) of model   predictions is a crucial step for a variety of   tasks such as active learning , misclassiﬁcation   detection , adversarial attack detection , out - of-   distribution detection , etc . Most of the works   on modeling the uncertainty of deep neural   networks evaluate these methods on image   classiﬁcation tasks . Little attention has been   paid to UE in natural language processing .   To ﬁll this gap , we perform a vast empirical   investigation of state - of - the - art UE methods   for Transformer models on misclassiﬁcation   detection in named entity recognition and   text classiﬁcation tasks and propose two   computationally efﬁcient modiﬁcations , one   of which approaches or even outperforms   computationally intensive methods .   1 Introduction   Machine learning methods are naturally prone to   errors as they typically have to deal with ambiguous   and incomplete data during both training and   inference . Unreliable predictions hinder the   application of these methods in domains , where   the price of mistakes is very high , such as clinical   medicine . Even in more error - tolerant domains   and tasks , such as intent recognition in general-   purpose chatbots , one would like to achieve a better   trade - off between expressiveness of a model and   its computational performance during inference .   Since mistakes are inevitable , it is crucial   to understand whether model predictions can   be trusted or not and abstain from unreliable   decisions . Uncertainty estimation ( UE ) of model   predictions aims to solve this task . Ideally ,   uncertain instances should correspond to erroneousobjects and help in misclassiﬁcation detection .   Besides misclassiﬁcation detection , UE is a crucial   component for active learning ( Settles , 2009 ) ,   adversarial attack detection ( Lee et al . , 2018 ) ,   detection of out - of - distribution ( OOD ) instances   ( Van Amersfoort et al . , 2020 ) , etc .   Some classical machine learning models , e.g.   Gaussian processes ( Rasmussen , 2003 ) , have   built - in UE capabilities . Modern deep neural   networks ( DNNs ) usually take advantage of a   softmax layer , which output can be considered   as a prediction probability and be used for   UE . However , the softmax probabilities are   usually unreliable and produce overconﬁdent   predictions ( Guo et al . , 2017 ) . Some previously   proposed techniques such as deep ensemble   ( Lakshminarayanan et al . , 2017 ) are known for   producing good UE scores but require a large   additional memory footprint for storing several   versions of weights and multiply an amount of   computation for conducting several forward passes .   Reliable UE of DNN predictions that does not   introduce high computational overhead is an open   research question ( Van Amersfoort et al . , 2020 ) .   In this work , we investigate methods for UE   of DNNs based on the Transformer architecture   ( Vaswani et al . , 2017 ) in misclassiﬁcation detection .   We consider two of the most common NLP tasks :   text classiﬁcation and named entity recognition   ( NER ) . The latter has been overlooked in the   literature on UE . To our knowledge , this work is   the ﬁrst to consider UE for NER .   We propose two novel computationally cheap   methods for UE of Transformer predictions . The   ﬁrst method is the modiﬁcation of the Monte   Carlo dropout with determinantal point process   sampling of dropout masks ( Shelmanov et al . ,   2021 ) . We introduce an additional step for   making masks more diverse , which helps to8237achieve substantial improvements and approach   the performance of computationally - intensive   methods on NER . The second method leverages   Mahalanobis distance ( Lee et al . , 2018 ) but also   adds a spectral normalization of the weight matrix   in the classiﬁcation layer ( Liu et al . , 2020 ) . This   method achieves the best results on most of the   datasets and even outperforms computationally-   intensive methods . We also investigate recently   proposed regularization techniques in combination   with other UE methods . The contributions of this   paper are the following :   •We propose two novel computationally cheap   modiﬁcations of UE methods for Transformer   models . The method based on Mahalanobis   distance with spectral normalization   approaches or even outperforms strong   computationally intensive counterparts .   •This work is the ﬁrst to investigate UE   methods on the NER task .   •We conduct an extensive empirical evaluation ,   in which we investigate recently proposed   regularization techniques in combination with   other UE methods .   2 Related Work   It is well known that reliable uncertainty scores can   be obtained simply by constructing an ensemble   of decorrelated neural networks ( deep ensemble )   ( Lakshminarayanan et al . , 2017 ) . However ,   such a straightforward approach is coupled with   substantial computational and memory overhead   during training an ensemble , performing inference   of all its components , and storing multiple versions   of weights . This overhead is a serious obstacle to   deploying ensemble - based uncertainty estimation   methods in practice .   Uncertainty estimation is a built - in capability   of Bayesian neural networks ( Blundell et al . ,   2015 ) . However , such models have similar issues   as ensembles and also require special training   procedures . Recently , it was shown by Gal and   Ghahramani ( 2016 ) that dropout , a well - known   regularization technique , is formally equivalent   to approximate variational inference in a deep   Gaussian process if it is activated during prediction .   This method , known as Monte Carlo ( MC ) dropout ,   uses the approximating variational distribution with   Bernoulli variables related to network units . MC   dropout does not impose any overhead duringtraining , introduces no additional parameters , and   thus does not require any additional memory .   The main disadvantage of this method is that it   usually requires many forward - pass samplings for   approximating predictive posterior , which makes it   also computationally expensive .   Recently , many works have investigated   the approximate Bayesian inference for neural   networks using deterministic approaches : Lee et al .   ( 2018 ) ; Liu et al . ( 2020 ) ; Van Amersfoort et al .   ( 2020 ) ; Mukhoti et al . ( 2021 ) ; Shen et al . ( 2021 ) ,   etc . These methods do not introduce notable   overhead for inference , storing weights , and usually   require compatible training time . However , most   of the research in this area is accomplished for   computer vision tasks .   For text classiﬁcation , a series of works   investigates UE methods for the OOD detection   task ( Liu et al . , 2020 ; Podolskiy et al . , 2021 ;   Zeng et al . , 2021 ; Hu and Khan , 2021 ) . In this   work , we focus on a more challenging task –   misclassiﬁcation detection . While OOD detection   requires to model only the epistemic uncertainty   inherent to the model and caused by a lack   of training data , misclassiﬁcation detection also   requires to model aleatoric uncertainty caused by   noise and ambiguity in data ( Mukhoti et al . , 2021 ) .   We consider recently proposed methods in this area   that are evaluated in text processing .   Three recent works propose techniques for   misclassiﬁcation detection based on an additive   regularization of a training loss function . Zhang   et al . ( 2019 ) suggest adding a penalty that reduces   the Euclidean distance between training instances   of the same class and increases the distance   between instances of different classes . He et al .   ( 2020 ) suggest using two components in the   loss function that reduce the difference between   outputs from two versions of a model initialized   with different weights . They also use mix - up   ( Thulasidasan et al . , 2019 ) to generate additional   training instance representations that help to   capture aleatoric uncertainty , self - ensembling , MC   dropout , and a distinctiveness score to measure the   epistemic uncertainty . Xin et al . ( 2021 ) introduce a   regularizer that penalizes overconﬁdent instances   with high loss . In another recent work , Shelmanov   et al . ( 2021 ) propose to combine MC dropout with   a Determinantal Point Process ( DPP ) to improve   the diversity of predictions by considering the   correlations between neurons and sampling the8238diverse neurons for activation in a dropout layer .   In this work , we conduct a systematic empirical   investigation of UE methods on NLP tasks . We   evaluate combinations of methods that have not   been tested before and propose two modiﬁcations ,   one of which achieves the best results among   computationally cheap methods . The previous   work focuses on text classiﬁcation tasks , while this   work is the ﬁrst to investigate UE also for NER .   3 Background and Methods   In this section , we describe the baselines and   propose novel uncertainty estimation techniques .   3.1 Softmax Response   Softmax Response ( SR ) ( Geifman and El - Yaniv ,   2017 ) is a trivial baseline for UE that uses the   probabilities generated via the output softmax layer   of the neural network . SR is based on the maximum   probabilityp(yjx)over classes y = c2C. The   smaller this probability is , the more uncertain   model is :   u(x ) = 1 maxp(y = cjx ): ( 1 )   3.2 Monte Carlo Dropout   Standard Monte Carlo Dropout ( MC Dropout )   Consider we have conducted Tstochastic forward   passes with activated dropout . In this work , we use   the following ways to quantify uncertainty with   methods based on MC dropout :   • Sampled maximum probability ( SMP ) is :   u= 1 max1   TXp ; ( 2 )   wherepis the probability of the class cfor   thet - th stochastic forward pass .   •Probability variance ( PV ; Gal et al . ( 2017 ) ;   Smith and Gal ( 2018 ) ) is :   u=1   CX   1   TX(p p ) !   ; ( 3 )   wherep = Ppis the probability for a   classcaveraged across Tstochastic forward   passes .   •Bayesian active learning by disagreement   ( BALD ; Houlsby et al . ( 2011 ) ) is :   u =  Xplogp+1   TXplogp :   ( 4)The two former techniques are speciﬁcally   designed for estimation of the epistemic ( model )   uncertainty arising from the lack of knowledge and   ignore the aleatoric uncertainty related to ambiguity   and noise in the data , while the latter method can   be seen as a measure of total uncertainty ( Malinin   and Gales , 2018 ) .   Transformers contain multiple dropout layers   ( after the embedding layer , in each attention head ,   and before the last classiﬁcation layer ) . It is   shown in previous work that the standard MC   dropout outperforms the baseline SR only when all   dropout layers are activated in a model ( Shelmanov   et al . , 2021 ) . Therefore , we follow this setting   for experiments in this work . We note that due to   activating all dropout layers , multiple stochastic   predictions are required for the whole network ,   which introduces a large computational overhead .   Similar UE scores are used in deep ensemble   ( Lakshminarayanan et al . , 2017 ) , where instead   of multiple stochastic predictions we train and   infer several model versions with different sets of   weights .   Diverse Determinantal Point Process Monte   Carlo Dropout ( DDPP MC dropout ) ( Ours )   Determinantal point processes ( DPPs ; Kulesza and   Taskar ( 2012 ) ) are used for sampling a subset   of diverse objects from a given set . Recently ,   Shelmanov et al . ( 2021 ) have combined the   MC dropout with a determinantal point process   ( DPP ) for sampling neurons in a dropout layer   and demonstrated that using stochasticity in the   last dropout layer ( in a classiﬁcation head of   Transformer ) only is enough to improve upon SR   in misclassiﬁcation detection . This method is   less computationally expensive than the standard   MC dropout since it requires multiple stochastic   predictions only for the top classiﬁcation layer of   the network with a small number of parameters ,   while all other layers are inferred only once .   Consider the similarity matrix Cbetween   neurons of the h - th hidden layer ( in particular , we   use a correlation matrix between output values of   neurons on the training set ) . Then one can construct   the DPP - based dropout masks MusingC   as a likelihood kernel for the DPP : M   DPP ( C ) . That gives the following probability to   select a setSof activations on the layer h :   Ph   M = Si   = det(C )   det(C+I ) ; ( 5)8239whereCis the square submatrix of Cobtained   by keeping only rows and columns indexed by the   sampleS.   In this work , we improve this method by   increasing the diversity of the sampled DPP masks .   After multiple dropout masks are pre - generated   via DPP in the inference step as in the original   DPP MC dropout , we make an additional step , in   which we select a diverse set of masks from this   pre - generated pool using one of two strategies :   •DDPP ( + DPP ) : We sample a set of “ diverse ”   masks that activate different sets of neurons .   For this purpose , we apply DPP sampling   again to the pool of pre - generated masks . As   a similarity kernel in this step , we use an RBF-   similarity matrix of mask vectors .   •DDPP ( + OOD ) : We sample a set of masks   that generate diverse predictions . For this   purpose , we select the masks that yield the   highest PV scores on the given OOD dataset .   After a new set of Tmasks is selected , we use   them as in the standard MC dropout to obtain   stochastic predictions . Increasing the diversity of   masks in the proposed modiﬁcation is motivated   by the ﬁnding of Jain et al . ( 2020 ) that improving   the diversity of elements in an ensemble leads to   better uncertainty estimates .   We note that in masks generated with DPP ,   usually , less than 50 % of neurons are activated ,   which makes predictions poorly calibrated . To   mitigate this problem , for each constructed mask ,   we perform a temperature - scaling calibration ( Guo   et al . , 2017 ) using a held - out dataset .   3.3 Deterministic Uncertainty Estimation   Spectral - normalized Neural Gaussian Process   ( SNGP ) Liu et al . ( 2020 ) suggest replacing the   typical dense output layer of a network with a   layer that implements a Gaussian process with   an RBF kernel , whose posterior variance at a   given instance is characterized by its Ldistance   from the training data in the hidden vector space   constructed by underlying layers of a network . The   authors propose an approximation based on random   Fourier feature expansion , which enables end - to-   end training and makes the inference feasible .   However , this method requires hidden   representations to be distance - preserving in order   to make it work . While the distance between   instances in the hidden space does not alwayshave a meaningful correspondence to the distance   in the input space , authors prove that to keep   hidden representations distance - preserving , the   transformation should satisfy the bi - Lipschitz   condition . For ResNets ( He et al . , 2016 ) , this   requirement is satisﬁed if weight matrices for the   nonlinear residual blocks have a spectral norm   ( i.e. , the largest singular value ) bounded from   above by a constant . Therefore , to enforce the   aforementioned Lipschitz constraint , they apply a   spectral normalization ( SN ) on weight matrices .   For Transformers , they normalize the matrix of the   penultimate classiﬁcation layer only .   Mahalanobis Distance ( MD ) Mahalanobis   distance is a generalisation of the Euclidean   distance , which takes into account the spreading   of instances in the training set along various   directions in a feature space . Lee et al . ( 2018 )   suggest estimating uncertainty by measuring the   Mahalanobis distance between a test instance and   the closest class - conditional Gaussian distribution :   u= min(h )(h );(6 )   wherehis a hidden representation of a i - th   instance,is a centroid of a class c , and is   a covariance matrix for hidden representations of   training instances .   Recently , the Mahalanobis distance has been   adopted for out - of - distribution detection with   Transformer networks by Podolskiy et al . ( 2021 ) .   Mahalanobis Distance with Spectral-   normalized Network ( MD SN ) ( Ours ) Since   the UE method based on the Mahalanobis distance   utilizes the idea of a proximity of a tested   instance hidden representation to the training   distribution , we expect this method to beneﬁt from   distance - preserving representations . Therefore ,   we propose the modiﬁcation of the method of   Lee et al . ( 2018 ) and Podolskiy et al . ( 2021 )   that enforces the bi - Lipschitz constraints on   transformation implemented by the network . We   perform spectral normalization of the weight   matrix of the linear layer in the classiﬁcation head   of Transformer as it is suggested in SNGP ( Liu   et al . , 2020 ) . At each training step , a spectral norm   is estimated using the power iteration method   =kWk , and a normalized weight matrix is   obtained : ~W=. At the inference step , hidden   representations are calculated using the normalized8240matrix ~h(x ) = ~Wx+band are used for computing   the Mahalanobis distance .   3.4 Training Loss Regularization   Additive regularization is another approach to   improving UE of neural network predictions .   Usually , the training loss combines the original   task - speciﬁc loss L(e.g . cross - entropy ) and   a regularization component Lthat facilitates   producing better calibrated UEs :   L = L+L ; ( 7 )   whereis a hyperparameter that controls the   regularization strength .   The positive side of such techniques is that ,   besides SR , they can be used to improve other   methods like MC dropout and deterministic   methods . The drawback is that regularization   affects the training procedure and can decrease the   model quality .   Conﬁdent Error Regularizer ( CER ) Xin et al .   ( 2021 ) propose a regularizer that adds a penalty for   an instance with a bigger loss than other instances   and , at the same time , bigger conﬁdence :   L = X 1[e > e ] ; ( 8)   = maxf0;maxp maxpg;(9 )   wherekis the number of instances in a batch and   eis an error of the i - th instance : eis 1 if the   prediction of the classiﬁer matches the true label ,   andeis 0 otherwise . The authors evaluate this   type of regularization only in conjunction with the   SR baseline .   Metric Regularizer Zhang et al . ( 2019 ) propose   a regularizer that aims to shorten the intra - class   distance and enlarge the inter - class distance :   L = Xn   L(c)+"XL(c;k)o   ; ( 10 )   L(c ) = 2   jSj jSjXD(h;h ) ;   ( 11 )   L(c;k)=1   jSjjSjX [    D(h;h ) ] ;   ( 12)D(r;r ) = 1   djjh hjj ; ( 13 )   wherehis a feature representation of an instance   ifrom a penultimate layer of a model with a   dimensiond , Sis the set of instances from class   c , jSjis the number of elements in S,"and   are   positive hyperparameters , [ x]=max(0;x ) .   4 Experimental Setup   In the experiments , we train a model on a given   dataset and perform inference on a separate test   set to compute both predictions and UE scores u.   We are interested in how the scores correlate with   the mistakes ~eof the model on the test set . For   text classiﬁcation , mistakes are computed in the   following way :   ~e=1 ; y6= ^y ;   0 ; y= ^y;(14 )   whereyis a true label , ^yis a predicted label .   For NER , we use two evaluation options : token-   level and sequence - level . For the token - level   evaluation , individual tokens are considered as   separate instances as in the text classiﬁcation .   For the sequence - level evaluation , mistakes are   computed in the following way :   ~e=1;9j2f1;:::;ng ; y6= ^y ;   0;8j2f1;:::;ng ; y= ^y;(15 )   wherenis a sequence length , yis a true label , ^y   is a predicted label of a j - th token in a sequence . In   the sequence - level evaluation , UE of a sequence is   aggregated from UEs of tokens by taking maximum   ( for MD methods ) or by summation ( for others ) .   4.1 Metrics   El - Yaniv and Wiener ( 2010 ) suggest evaluating   the quality of UE using the area under the risk   coverage curve ( RCC - AUC ) . The risk coverage   curve demonstrates the cumulative sum of loss due   to misclassiﬁcation ( cumulative risk ) depending   on the uncertainty level used for rejection of   predictions . The lower area under this curve   indicates better quality of the UE method .   Xin et al . ( 2021 ) propose a reversed pair   proportion ( RPP ) metric . They note that instances   with higher conﬁdence should have a lower loss l.   RPP measures how far the uncertainty estimator ~u   is to ideal , given the labeled dataset of size n :   RPP = 1   nX1[~u(x)>~u(x);l < l]:(16)8241   This metric has an upper bound of 1 ; for   convenience , the reported values are multiplied by   100 . Similar to Xin et al . ( 2021 ) , for both metrics ,   lis an indicator loss function .   We conduct each experiment six times   with different random seeds , obtaining the   corresponding metric values , and report their mean   and standard deviation .   We also present the results using the accuracy   rejection curve . This curve is drawn by varying   the rejection uncertainty level ( horizontal axis ) and   presenting the corresponding accuracy obtained   when all rejected instances are labeled with an   oracle ( vertical axis ) . This emulates the work of   a human expert in conjunction with a machine   learning system . The higher the curve , the smaller   amount of labor is needed to achieve a certain level   of performance and the better is the UE method . A   similar evaluation approach in a table form is used   in ( Zhang et al . , 2019 ) . A similar curve but without   oracle labeling is used in ( Lakshminarayanan et al . ,   2017 ; Filos et al . , 2019 ) .   4.2 Datasets   For experiments with text classiﬁcation , we use   three datasets from the GLUE benchmark ( Wang   et al . , 2018 ) that were previously leveraged   by Shelmanov et al . ( 2021 ) and Xin et al .   ( 2021 ) for the same purpose : Microsoft Research   Paraphrase Corpus ( MRPC ) ( Dolan and Brockett,2005 ) , Corpus of Linguistic Acceptability ( CoLA )   ( Warstadt et al . , 2019 ) , and Stanford Sentiment   Treebank ( SST-2 ) ( Socher et al . , 2013 ) . Similar to   ( Shelmanov et al . , 2021 ) , we randomly subsample   SST-2 to 10 % to emulate a low - resource setting .   The experiments with NER were performed   on the widely - used CoNLL-2003 task ( Tjong   Kim Sang and De Meulder , 2003 ) . For this dataset ,   we also subsample the training part to 10 % .   As an out - of - domain dataset for DDPP MC   dropout , we use the IMDB binary sentiment   classiﬁcation dataset ( Maas et al . , 2011 ) . We   randomly select 5,000 instances from its test part   and use them to select DPP - generated masks .   The dataset statistics are provided in Table 4 in   Appendix A.   4.3 Model Choice and Hyperparameter   Selection   For experiments , we use two modern Transformers :   the pre - trained ELECTRA model ( Clark et al . ,   2020 ) with 110 million parameters and DeBERTa   ( He et al . , 2021 ) with 138 million parameters .   They achieve higher performance on the GLUE   benchmark in comparison with previous models ,   such as BERT ( Devlin et al . , 2019 ) and RoBERTa   ( Liu et al . , 2019 ) .   The optimal hyperparameter values for each   triple < Dataset , Regularization Type , Spectral   Normalization Usage > are presented in Table 68242   in Appendix A. For the optimal hyperparameter   search , we split the original training data into   training and validation subsets in a ratio of 80   to 20 and apply Bayesian optimization with early   stopping . For text classiﬁcation , we use accuracy   as an objective metric , and for sequence tagging ,   we use span - based F1 - score ( Tjong Kim Sang and   De Meulder , 2003 ) . Sets of pre - deﬁned values   for each hyperparameter are given in the caption   of Table 6 . After the hyperparameter search is   completed , we train the model on the original   training set using the optimal values .   The hyperparameters for UE methods are   presented in Table 9 in Appendix A. The values   for the DDPP MC dropout and MD SN are chosen   using a grid search , while validating on the held - out   validation dataset with RCC - AUC as an objective .   For deep ensemble , we use random subsampling of   the training set with a ﬁxed ratio of 90 % .   The hardware conﬁguration for experiments is   provided in Table 5 in Appendix A.   5 Results and Discussion   5.1 Monte Carlo Dropout and Regularization   The results of methods based on MC dropout   and loss regularization are presented in Table 1   ( for ELECTRA ) . The standard computationally   intensive MC dropout achieves big improvements   over the SR baseline on all text classiﬁcation   datasets and the sequence - level CoNLL-2003   benchmark . For token - level CoNLL-2003 , none of   the considered methods substantially outperform   the baseline . Uncertainty estimation scores BALD   and PV have similar results , outperforming SMP   on SST-2 , while SMP has a slight advantage over   them on CoLA and CoNLL-2003 .   The DDPP MC dropout method does not   outperform the MC dropout . However , DDPP   ( + DDPP ) demonstrates a notable advantage overthe SR baseline on text classiﬁcation datasets SST-   2 and CoLA , while both DDPP ( + DDPP ) and   DDPP ( + OOD ) outperform the baseline on the   sequence - level CoNLL-2003 benchmark . The   main advantage of the proposed DDPP MC dropout   method consists in its much faster inference   compared to the computationally expensive   standard MC dropout . The DDPP MC dropout has   the same computational overhead during inference   as the original DPP MC dropout , which is only   less than 0.5 % of the overhead introduced by the   standard MC dropout ( Shelmanov et al . , 2021 ) .   We conduct an ablation study of the proposed   modiﬁcations for the original DPP MC dropout .   The experimental results of this study presented in   Table 12 in Appendix C demonstrate the beneﬁts of   using calibration and introducing diversity in mask   generation .   Both metric regularization and CER achieve a   substantial advantage over the baseline on text   classiﬁcation datasets SST-2 and MRPC . However ,   regularization appears to be malignant for NER .   Adding loss regularization to MC dropout usually   helps to achieve better results on text classiﬁcation .   The best results on SST-2 and CoLA are achieved   using metric regularization , while the best result   for MRPC is obtained using CER . Regularization   and DDPP MC dropout usually complement each   other , the results of their combination are slightly   better than when they are applied individually for   all datasets except CoNLL-2003 .   5.2 Deterministic Methods   The results for deterministic methods are presented   in Table 2 ( for ELECTRA ) . SNGP gives substantial   improvements on the text classiﬁcation datasets   MRPC and SST-2 but signiﬁcantly falls behind   the trivial baseline on CoNLL-2003 . The low   performance of SNGP for NER can be attributed to   the fact that it is initially designed for classiﬁcation8243   tasks rather than sequence tagging . MD yields   much bigger improvements over the SR baseline   on all datasets and signiﬁcantly outperforms SNGP .   MD SN is able to improve the misclassiﬁcation   detection performance even further for MRPC ,   SST-2 , and CoLA .   We also conduct an ablation study ( Table 2 ) , in   which we use the spectral normalization without   MD . We see that SN on its own , as expected , mostly   does not improve the UE performance ; the results   usually are even slightly worse than the baseline .   Regularization also helps to improve the results   of methods based on the Mahalanobis distance .   For both MD and MD SN , regularization helps on   CoLA and CoNLL-2003 . For MD , it also helps on   SST-2 , while for MD SN , regularization improves   the results on MRPC . We note that regularization   reduces the gap between MD and MD SN on   text classiﬁcation datasets and even gives a slightadvantage to MD over MD SN on CoNLL-2003 .   The best results across all deterministic methods   for text classiﬁcation datasets are achieved by MD   SN . The biggest improvements are obtained on   MRPC , where regularized MD SN reduces RCC-   AUC by more than 46 % compared to the baseline .   5.3 Best Results   Table 3 and Figure 1 compare results of the best   methods in each group for ELECTRA . Table 11   and Figure 3 in Appendix B show the best   results for DeBERTa . In these tables and ﬁgures ,   we also present the results of deep ensemble   ( Lakshminarayanan et al . , 2017 ) , which is a strong   yet computationally intensive baseline ( Ashukha   et al . , 2020 ) , and results of another recently   proposed computationally intensive method called   MSD ( He et al . , 2020 ) that leverage “ mix - up ”   ( Thulasidasan et al . , 2019 ) , “ self - ensembling ” , MD,8244   and the MC dropout ( all layers are activated ) .   We can see that it is possible to substantially   improve misclassiﬁcation detection performance   and achieve even better results than MC dropout ,   deep ensemble , or MSD almost with no overhead   in terms of memory consumption and amount   of computation . For text classiﬁcation and for   both models , computationally cheap methods   are either better or on par with the expensive   counterparts . However , for NER , we see that   the latter methods seriously fall behind deep   ensemble and MC dropout . On the token - level   CoNLL-2003 benchmark , only deep ensemble   substantially outperforms the SR baseline . On   the sequence - level CoNLL-2003 benchmark , MD   with CER , DDPP ( + DDP ) PV , and DDPP ( + OOD )   PV improve upon SR , but only approach the   performance of computationally intensive methods .   The proposed in this work MD SN method   outperforms all other computationally efﬁcient   alternatives on text classiﬁcation datasets . For   both models , it even substantially outperforms all   computationally expensive methods on the CoLA   dataset , while on other text classiﬁcation datasets   it is on par with them . Another method proposed   in this work , DDPP MC dropout , empowered with   regularization techniques , is able to substantially   reduce the gap between the SR baseline and   computationally intensive UE methods , while   introducing only a fraction of their overhead .   Figure 2 also presents accuracy rejection curves   for selected methods on MRPC . The ﬁgure shows   that if we reject 20 % of instances using UE   obtained with MC dropout and ask human experts   to label these uncertain objects , the accuracy score   of such a human - machine hybrid system will   increase from 88.4 % to 96.0 % , which is 1.3%better than the SR baseline . Such an additional gain   over the SR baseline can be crucial for safe - critical   applications . Deep ensemble and MD SN are close   to each other and achieve 95.6 % and 95.2 % of   accuracy correspondingly . Rejecting 40 % of most   uncertain instances gives 98.2 % of accuracy for the   computationally - intensive deep ensemble , while   the proposed cheap MD SN method yields even   better result with 98.5 % of accuracy , which is 1.7 %   higher than the result of the SR baseline .   6 Conclusion   Our extensive empirical investigation on   text classiﬁcation and NER tasks shows that   computationally cheap UE methods are able to   substantially improve misclassiﬁcation detection   for Transformers , performing on par or even   better than computationally intensive MC dropout   and deep ensemble . The proposed in this work   method based on the Mahalanobis distance   and spectral normalization of a weight matrix   ( MD SN ) achieves the best results among   other computationally cheap methods on text   classiﬁcation datasets and is on par with expensive   methods . This method does not require seriously   modifying a model architecture , extra memory   storage , and introduces only a little amount of   additional computation during inference .   We also show that our modiﬁcation of   DPP MC dropout that leverages the diversity   of generated dropout masks , which is also   a computationally cheap method , is able to   outperform the softmax response baseline and   approach the computationally intensive methods   on NER . Finally , we ﬁnd that regularization can   slightly improve the results of methods based on   MC dropout and the Mahalanobis distance in text   classiﬁcation .   The spectral normalization is theoretically   proven to ensure bi - Lipschitz constraint on the   transformation deﬁned by the standard residual   connection network ( Liu et al . , 2020 ) . However ,   the self - attention blocks in Transformers have   a more complicated architecture than the layers   of standard ResNets , which means that the   theoretical guarantees for them do not hold in   general . In future work , we are looking forward   to investigating other techniques to ensure bi-   Lipschitz constraint on self - attention blocks , which   might further improve deterministic methods for   uncertainty estimation of Transformers.8245Acknowledgements   We thank anonymous reviewers for their insightful   suggestions to improve this paper . The work was   supported by a grant for research centers in the   ﬁeld of artiﬁcial intelligence ( agreement identiﬁer   000000D730321P5Q0002 dated November 2 , 2021   No . 70 - 2021 - 00142 with ISP RAS ) .   References824682478248A Dataset Statistics , Hyperparameter Values , and Hardware Conﬁguration824982508251B Additional Experimental Results with DeBERTa   C Additional Ablation Studies for DDPP8252