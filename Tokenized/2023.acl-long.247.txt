  Satoshi Akasaki   Yahoo Japan Corporation   sakasaki@yahoo - corp.jpNaoki Yoshinaga   Institute of Industrial Science ,   The University of Tokyo   ynaga@iis.u - tokyo.ac.jpMasashi Toyoda   Institute of Industrial Science ,   The University of Tokyo   toyoda@iis.u-tokyo.ac.jp   Abstract   We make decisions by reacting to changes in   the real world , particularly the emergence and   disappearance of impermanent entities such   as restaurants , services , and events . Because   we want to avoid missing out on opportunities   or making fruitless actions after those entities   have disappeared , it is important to know when   entities disappear as early as possible . We thus   tackle the task of detecting disappearing enti-   ties from microblogs where various informa-   tion is shared timely . The major challenge is   detecting uncertain contexts of disappearing en-   tities from noisy microblog posts . To collect   such disappearing contexts , we design time-   sensitive distant supervision , which utilizes en-   tities from the knowledge base and time - series   posts . Using this method , we actually build   large - scale Twitter datasets of disappearing en-   titiesfor English and Japanese . To ensure   robust detection in noisy environments , we re-   fine pretrained word embeddings for the detec-   tion model on microblog streams in a timely   manner . Experimental results on the Twitter   datasets confirmed the effectiveness of the col-   lected labeled data and refined word embed-   dings ; the proposed method outperformed a   baseline in terms of accuracy , and more than   70 % of the detected disappearing entities in   Wikipedia are discovered earlier than the up-   date on Wikipedia , with the average lead - time   is over one month .   1 Introduction   Our daily actions depend on the state of the   real world and its changes , especially changes in   the entities available to us . Among the various   changes , the beginning of entities , or emerging   entities ( Akasaki et al . , 2019 ) such as new songs ,   movies , and events , are useful for understandingFigure 1 : Tweets of entities with non - disappearing and   disappearing contexts .   the trends in interests . At the same time , we want   to be made aware of the end or disappearance of   entities , such as closing stores or discontinuing ser-   vices ( Figure 1 ) , as soon as possible so that we can   avoid missing out on opportunities or taking fruit-   less actions after the entities are no longer avail-   able . Portal sites also need to address this issue   by quickly removing unavailable companies , loca-   tions , and events from their services . Moreover ,   it is important to collect these entities to maintain   knowledge bases ( KBs ) in which information about   the entities is continuously accumulated .   Whereas disappearing entities remain to be   discovered , studies on discovering out - of - KB or4507emerging entities ( Lin et al . , 2012 ; Färber et al . ,   2016 ; Wu et al . , 2016 ; Akasaki et al . , 2019 ) have   been successful to an extent , as most of these enti-   ties have distinct names and can be characterized by   mentions to their unseen names . In contrast , we do   not have such clues to detect disappearing entities ,   since most of them continue to be mentioned with   the same names even after they actually disappear .   Given such a situation , we take on the new   task of discovering disappearing entities from mi-   croblogs where news and personal experiences are   widely shared . To detect the entities ’ disappear-   ance , we exploit the specific expressions that peo-   ple use when mentioning disappearing entities in   microblogs ( Figure 1 ) ( § 2 ) . By capturing these   contexts , we can discover a variety of entities in the   early stage even before they disappear . To develop   a dataset of disappearing entities and contexts ,   we use time - sensitive distant supervision ( Akasaki   et al . , 2019 ) , which collects specific contexts of en-   tities by utilizing KB entities and timestamps of mi-   croblogs . Because this method requires the timing   of the desired contexts , we extract the year of disap-   pearance for each entity described in Wikipedia and   incorporate it into the distant supervision ( § 4.1 ) .   We then train a named entity recognition ( NER )   model on the collected entities and contexts to dis-   cover and type disappearing entities . However , the   NER model performs poorly for microblogs ( Der-   czynski et al . , 2017 ) where posts are short and   noisy , and the training data collected by the distant   supervision also contains noise , making it difficult   to train a reliable model . We address this issue by   considering that multiple posts are likely to men-   tion the target entity when it disappears . Concretely ,   we utilize these posts to refine pretrained word em-   beddings and incorporate them into the NER model   ( § 4.2 ) . This enables the model to consider the to-   kens that frequently appear among multiple posts   and to recognize disappearing entities robustly .   To evaluate our method , we built large - scale En-   glish and Japanese datasets from Twitter using the   proposed time - sensitive distant supervision method   ( § 5.1 ) . The experimental results ( § 5.4 ) demon-   strated that our method outperformed the baseline ,   which simply collected the latest burst of posts   about the disappearing entities as the disappearing   contexts using the original version of time - sensitive   distant supervision and used them to train the NER   model . In addition , the evaluation of relative re-   call indicated that our method successfully foundmore than 70 % of the target disappearing entities   in Wikipedia . Except for entities whose disappear-   ances are sudden and receive media exposure ( e.g. ,   persons ) , our method detected entities such as ser-   vices , facilities , and events on average more than   100 days earlier than the update of the disappear-   ance in Wikipedia .   2 Definition of Disappearing Entity   In this section , we define the meaning of the term   disappearing entity in this study . We consider the   entities ’ disappearance to be the disappearance of   its existence from the real world or official an-   nouncement of its discontinuation , with reference   to the list of ending entities in Wikipedia .   To reach a solid definition of disappearing en-   tities , we refer to the definition of emerging en-   tities ( Akasaki et al . , 2019 ) . They reported that   emerging entities have a specific process from their   first appearance to when they become known to   the public ; during this process , they appear with   specific expressions , i.e. , emerging contexts . We   define disappearing entities and contexts by focus-   ing on the fact that specific expressions that indicate   plans and signs of disappearance appear in contexts   not only at the time of disappearance but also in   the process leading up to that time as follows :   Disappearing contexts . Contexts in which the   writers assumed the readers do not know the disap-   pearance of the entities .   Non - disappearing contexts . Contexts other than   disappearing contexts .   Disappearing entities . Entities still being ob-   served in disappearing contexts .   Figure 1 lists examples of disappearing entities   and their disappearing and non - disappearing con-   texts . By properly identifying disappearing con-   texts , we can detect mentioned disappearing enti-   ties in the early stages even before they actually   disappear . This is especially important for enti-   ties such as stores that are closing or services and   events that are ending so that we can take action be-   fore they are gone . We later confirm the solidness   of these definitions by evaluating the inter - rater   agreement of disappearing entities acquired from   microblogs and by demonstrating that the disap-4508pearing entities can be detected before the update   of the disappearance in Wikipedia ( § 5.4 ) .   3 Related Work   There are no studies that detect disappearing en-   tities in a timely manner . We briefly review the   existing studies related to our task .   3.1 Entity Extraction   Because existing entity - related tasks such as   NER ( Nadeau and Sekine , 2007 ; Lample et al . ,   2016 ; Akbik et al . , 2018 , 2019 ; Yu et al . , 2020 ;   Cui et al . , 2021 ) and entity linking ( Shen et al . ,   2014 ; Kolitsas et al . , 2018 ; Martins et al . , 2019 ;   Oba et al . , 2022 ) do not take into consideration   whether the entities have already disappeared from   the real world , it is difficult to detect disappearing   entities by using these techniques simply .   As a complementary task to detecting disappear-   ing entities , Akasaki et al . ( 2019 ) attempted to find   emerging entities . They exploited the fact that peo-   ple use expressions that suggest novelty when men-   tioning emerging entities , and defined the emerg-   ing entities based on these expressions ( contexts ) .   They proposed a variant of the distant supervision   method ( Mintz et al . , 2009 ) called time - sensitive   distant supervision to collect emerging contexts   efficiently by utilizing KB entities and microblog   timestamps . They then developed an NER model   using the obtained data to detect emerging entities .   Although this study has some similarities to ours ,   finding emerging entities is a more easy task since   the initial occurrences of entities could be easily   regarded as emerging contexts . We can not apply   the analogous assumption to disappearing entities   because entities will be mentioned even after their   disappearance . We thus propose another method of   time - sensitive distant supervision for disappearing   entities ( § 4.1 ) to properly capture disappearing   contexts . We compare our method to Akasaki et al .   ( 2019 ) in the experiments and demonstrate its ef-   fectiveness . Also , unlike them , we experimented   with multiple languages ( English and Japanese ) and   even jointly performed entity typing rather than the   cascaded pipeline ( Akasaki et al . , 2021 ) for broader   applications.3.2 Temporal Event Extraction   As part of information extraction , researchers have   tackled the task of extracting an event ( e.g. , birth   of a person ) and its predefined attributes and argu-   ments ( such as time and places ) from the text ( Rit-   ter et al . , 2012 ; Nguyen and Grishman , 2015 ; Liu   et al . , 2020 ) . Although it is possible to use these   techniques to detect disappearing entities , they rely   on manual annotations ( Doddington et al . , 2004 ;   Aguilar et al . , 2014 ) and only a few entity types   such as ‘ person ’ and ‘ military conflict ’ are chosen   for the target entity types ; it is thus unrealistic to   cover a wide range of entity types .   KBP2011 ( Ji et al . , 2010 ; McClosky and Man-   ning , 2012 ) introduced the temporal slot filling   task , in which the duration of an event is identi-   fied from the given text , entities ( e.g. , Steve Jobs ) ,   and their events ( e.g. ,become CEO ) . The events are   attributes defined in Freebase , and they include the   disappearance of certain entities such as when a per-   son dies . However , the types of entities handled in   this task are also only a few ( ‘ person ’ and ‘ organiza-   tion ’ ) . Moreover , although target entities are given   in advance for this task , we have to detect mentions   of the entities in the settings of microblogs .   4 Proposed Method   To accurately collect contexts of disappearing enti-   ties ( § 2 ) , we adopt the framework of time - sensitive   distant supervision ( Akasaki et al . , 2019 ) , which ef-   ficiently collects specific contexts of given entities   utilizing microblog timestamps . Here , we utilize   the timings of the entities ’ disappearance described   in Wikipedia to refine the original time - sensitive   distant supervision since the original method is de-   signed for emerging entities and it is difficult to   apply the method to disappearing entities directly .   To ensure that a detection model can make ro-   bust predictions using the noisy dataset constructed   using distant supervision and for noisy microblog   posts , we refine pretrained word embeddings to   acquire features from multiple occurrences of dis-   appearing entities and feed them into the detection   model . As a microblog , we target Twitter , where   various sources including news articles and per-   sonal posts , are extensively and instantly shared .   4.1 Time - sensitive Distant Supervision   The original time - sensitive distant supervision in-   volves two main steps ( Akasaki et al . , 2019 ) ; 1 )   Collecting candidates of emerging entities by as-4509   sociating the time - stamps of registration with ti-   tles of articles in Wikipedia , and 2 ) collecting con-   texts of emerging entities by retrieving early - stage   microblog posts posted before the time - stamps of   registration . To prevent overfitting , non - emerging   contexts for the same entities with enough time   elapsed are collected as negative examples .   While this method was able to gather contexts   of emerging entities by collecting the posts when   they first appeared , it is difficult to directly apply   this implementation of time - sensitive distant super-   vision to disappearing entities because mentions of   entities are made even after they actually disappear ,   and thus the timing of disappearance is not clear .   Therefore , we explicitly feed the timing of the en-   tities ’ disappearance extracted from Wikipedia to   time - sensitive distant supervision to collect disap-   pearing entities and their contexts more accurately   ( Figure 2 ) . The specific procedure is as follows :   Step 1 : Collecting disappearing entities We   first collect candidates of disappearing entities   while excluding noise . To collect only entities that   have actually disappeared , we refer to the list of   ending entities in Wikipediaand gather the titles   of articles , categories , and their year of disappear-   ance . We excluded entities whose year of their first   appearance on Twitter was the same as the year of   their disappearance since they could be emerging   entities and contaminate the contexts . We also re-   move entities that have the ambiguity page so that   the contexts are not contaminated by homographic   entities which share the same namings with other   entities ( e.g. , “ Go ” can refer to a programming lan-   guage , a board game , or a verb ) . To acquire entity   types , we manually map the category of the article   to the coarse - grained type ; for example , the entity   ‘ Daft Punk ’ is mapped to the type ‘ G . ’ Weadopted this method instead of Akasaki et al . ( 2019 ,   2021 ) , which utilized the DBpedia mappings be-   cause of few mappings of disappearing entities .   Step 2 : Collecting disappearing contexts In   contrast to emerging entities , where the first ap-   pearance of the entity is often the emerging con-   text , the latest posts of disappearing entities contain   noisy unrelated contexts because they continue to   be mentioned in microblogs even after they have   disappeared . Therefore , for each collected entity ,   we utilize the year of disappearance and frequency   of appearance on Twitter to gather their disappear-   ing contexts . Specifically , we randomly collect k   posts of the day with the highest number of occur-   rences in the given year , assuming that the timing   that received the most attention in the year of the   disappearance includes the disappearing contexts .   For each collected entity , Akasaki et al . ( 2019 )   extracted contexts that differed from the positive   examples as negative examples to avoid overfitting   the detection model when recognizing mentions of   positive examples . Thus , we similarly collected   random knon - disappearing contexts as negative   examples from posts prior to the year in which we   collected positive examples for each entity . This   also enables the model to discriminate between   disappearing contexts and other contexts .   We adopt the BILOU scheme ( Ratinov and Roth ,   2009 ) for the NER tags ; we label the disappearing   entities in the positive examples with BILU and   their entity types and label the rest with O.   4.2 Finding Disappearing Entities   We train an NER model for recognizing and typ-   ing disappearing entities using the collected data .   Because we target short and noisy microblog posts   and build the dataset using distant supervision , it is4510   difficult to train the model stably . Thus , we focus   on the fact that multiple posts mentioning the disap-   pearance of the entities often appear when entities   disappear from microblogs . By obtaining features   from these posts , we can enhance the training and   prediction of the model even with noisy microblog   posts . To do this , we propose an unsupervised   method of refining pretrained word embeddings as   features from multiple posts of a Twitter stream .   We also devise a sequence labeling model for find-   ing disappearing entities using the refined word   embeddings as additional input ( Figure 3 ) .   4.2.1 Refining Pretrained Word Embeddings   Our objective is to extract features from multiple   posts in microblogs . However , because the surface   of the target entity is unknown at the prediction   time of NER , it is difficult to collect only relevant   posts of that entity from the massive Twitter stream .   To address this issue , we use , as the additional em-   bedding layers of our NER model , pretrained word   embeddings further fine - tuned on the posts on the   day of detecting disappearing entities . This enables   the refined word embeddings to reflect the tokens   and their co - occurrences in the Twitter stream of   the target day without the need for post selection .   The specific procedure is as follows :   First , we train the base word embeddings v   using the posts prior to the period in which we   collected the data in § 4.1 . We use fastText ( Bo-   janowski et al . , 2017 ) , a method of constructing   word embeddings , to deal with unknown words .   Next , we use the Twitter stream from each date   dof the posts collected in § 4.1 to retrain the word   vector vfor obtaining v. The resulting vcan   be interpreted as capturing the temporary semanticchange of von date d , and it can be treated   as an auxiliary input of models for various tasks .   Refining takes about 5 minutes per 2 M tweets and   can be done in parallel without using GPU .   4.2.2 NER with Refined Word Embeddings   We adopt BERT ( Devlin et al . , 2019 ) model with   softmax layer as an NER model . Since the pre-   trained BERT models available on the web may   have learned text with future timestamps than the   training data ( § 4.1 ) and suffer from data leaks , we   pretrain BERT from scratch using tweets from be-   fore the time at which we construct the training and   test data .   Based on this model , we input each post of   the constructed data into BERT and obtain the se-   quence of hiddan states . We then concatenate those   hidden states with the refined word embeddings v   corresponding to the date of the input post . We   finally feed them into the softmax layer and pre-   dict labels . This enables the model to consider   the global information of the given Twitter stream   other than the input post .   5 Experiments   We performed our task of discovering disappearing   entities using datasets built from Twitter .   5.1 Data   We built the Twitter datasets by using our time-   sensitive distant supervision . We targeted English   and Japanese , which are the top two languages   used on Twitter ( Alshaabi et al . , 2021 ) , and use   our archive of Twitter posts that were retrieved4511   using APIs . The archive contains more than 50B   posts , 32 % in English and 20 % in Japanese , a trend   similar to the actual data ( Alshaabi et al . , 2021 ) . We   tokenized posts using spaCy ( ver . 2.0.12)with the   en_core_web_sm model for English and MeCab   ( ver . 0.996)with ipadic ( ver . 2.7.0 ) for Japanese .   We removed URLs and usernames from the text . In Step 1 of § 4.1 , we collected article titles of   ending entities in Wikipedia from 2012 to 2019 us-   ing the Wikipedia dump from June 20th , 2021 . We   undersampled the types P andC to 1,000 entities as they are much larger   than the other types . We then excluded entities as   described and carried out Step 2 by setting kto 100 ,   as in ( Akasaki et al . , 2019 ) .   We split the collected data into training data   ( 2012 - 2018 ) and test data ( 2019).For the training   data , we obtained a total of 163,700 English and   150,204 Japanese tweets , including the same num-   ber of disappearing and other contexts for 3,213   English entities and 1,906 Japanese entities , respec-   tively . For model selection , we used 10 % of the   training data as the development data .   For the test data , from the collected positive ex-   amples of disappearing entities , we randomly se-   lected three posts for each entity and asked three   annotators ( the first author and two graduate stu-4512dents who are Japanese native speakers and fluent   in English ) to judge whether each context is ac-   companied by a disappearing context . We adopt   the positive contexts with the answers agreed upon   by two or more annotators . Then , for each entity   of the resulting data , we asked the annotators to   determine the non - disappearing contexts using the   collected negative examples of the entity and se-   lected the same number of posts as the positive   examples . We obtained an inter - rater agreement   of 0.722 for English and 0.786 for Japanese by   Fleiss ’s Kappa ( Fleiss and Cohen , 1973 ) , both of   which show substantial agreement . As a result ,   we obtained a total of 1,922 English and 1,326   Japanese tweets for 357 English entities and 235   Japanese entities , respectively , as the test data .   Tables 1 and 2 show the resulting disappearing   entities and disappearing contexts of training data   ( English ) and test data ( English and Japanese ) , re-   spectively . The entity types that are manually cat-   egorized into P andG account for a   large proportion . These types of entities , occupied   by persons , musical groups , and companies , are   more likely to disappear . We see that disappearing   contexts could be diverse according to the type of   entity they include . We thus have to capture those   contexts properly . Because we use Wikipedia as   the list of disappearing entities , there is a bias in   the composed categories , and some categories are   absent in certain languages . However , we can still   detect entities of the missing categories because if   the coarse type of entities is the same , their contexts   tend to be somewhat similar regardless of their cat-   egory ( e.g. , L types tend to be mentioned   with the term close ) .   Note that the test data for and are scarce . This is due to the absence of   disappearing contexts in the data collected by dis-   tant supervision . In the case of , there are   many entities that appear and then disappear within   a year , and many of them are filtered out ( § 4.1 ) .   Regarding , the disappearance of   these entities is actually uncertain for the nature of   the type . For example , even when the final episode   of a television drama airs on TV , it remains in var-   ious other media . It is thus difficult to determine   their disappearing contexts .   5.2 Models   We implemented the three models for comparison :   Proposed ( TDS + Emb ): As the proposed method , we implemented BERT ( Devlin et al . , 2019 ) as an   NER model and refined word embeddings ( Emb )   using the training data constructed by the proposed   time - sensitive distant supervision ( TDS ) . We re-   fined pretrained fastText embeddings for each in-   put post using tweets on the day of the input post   ( about 1 M to 2 M tweets for each day from 2012   to 2019 ) and additionally fed them into the BERT   model at the time of fine - tuning and the test time .   Proposed ( TDS ): To verify the effectiveness of   refined word embeddings , we removed the part   related to Emb ( the refined word embeddings ; the   green part in Figure 3 ) from Proposed ( TDS + Emb )   and used with the same training data , optimization ,   and parameters . Hence , this method does not con-   sider the multiple posts of the target day when   recognizing entities .   Baseline : To verify the effectiveness of the con-   structed training data , we collected the latest posts   of disappearing entities using the original version   of time - sensitive distant supervision ( Akasaki et al . ,   2019 ) , which does not consider the timing of the en-   tities ’ disappearance . Specifically , for each ending   entity in Wikipedia from 2012 to 2018 , we col-   lected up to 100 retweets of the last day ( through   2018 ) in which the entity appeared more than ten   times as positive examples . For negative exam-   ples , we obtained the same number of posts from   more than one year before the date when we col-   lected the positive examples . By using the collected   25,920 posts for 2867 entities in English and 16,777   posts for 1733 entities in Japanese , we fine - tuned   BERT using the same optimization and parameters   as Proposed ( TDS ) . Since this method does not   consider the timing of the entities ’ disappearance ,   many noisy contexts may be collected .   5.3 Settings   Implementation and model hyperparameters   We use TensorFlow 2 ( ver . 2.6.0)for implement-   ing the models and ran on NVIDIA RTX A6000   GPU with AMD EPYC 7313 @ 3.0GHz CPU . For   the BERT model , we use bert - base - cased with-   out next sentence predictionand pretrained from   scratch using 2B English tweets for English and   800 M Japanese tweets , respectively , both posted   from Mar. 11th , 2011 to Dec. 31st , 2011 . Using   the same tweets , we pretrained 300 - dimensional4513   word embeddings using fastTextwith default pa-   rameters . We refine them and feed to the BERT   model at the time of fine - tuning . We fine - tuned all   the models using AdamW ( Loshchilov and Hutter ,   2019 ) and chose the model in the epoch with the   highest F - score on the development data . The hy-   perparameters of fine - tuning are listed in Table 3 .   Evaluation methods To evaluate the accuracy ,   we apply the models to each post in the test data   built in § 5.1 . We utilize CoNLL-2003 ( Tjong   Kim Sang and De Meulder , 2003 ) schema , which   measures precision , recall , and F - score . We per-   formed training and test five times and reported the   average performances .   To evaluate the relative recall and the detec-   tion immediacy of our method , we follow the ex-   periments designed for evaluating emerging enti-   ties ( Akasaki et al . , 2019 ) . Specifically , for each   ending entity in Wikipedia that disappeared in 2019   ( 2608 for English and 763 for Japanese ) , we ap-   plied our method to all the posts in 2019 in which   each entity appeared ( 437,816 for English and   202,666 for Japanese ) . For each entity , we con-   sider an entity to have been discovered if the model   was able to recognize the target string at least once .   Then we determined how many target entities could   be discovered from the posts and how much earlier   those entities could be detected before the corre-   sponding Wikipedia articles were categorized as   ending using edit histories .   5.4 Results and Analysis   Overall accuracy of models Table 4 shows per-   formances for all models . Both the proposed meth-   ods outperformed the baseline , which collected   training data without considering the timing of   the entities ’ disappearance . The performance of   the baseline is low because it was trained with   data including many noisy contexts . This shows   that our time - sensitive distant supervision success-   fully collected the disappearing contexts . Our Pro-   posed ( TDS + Emb ) detected the entities with the   highest accuracy , which means that the refined   word embeddings worked effectively . In partic-   ular , the recall was improved in both Japanese and   English , indicating that entities that could not be   recognized by only using the features of a single   post can be detected by utilizing multiple posts .   Detailed accuracy of optimal model Table 5   shows the performances for each type to analyze   the behavior of Proposed ( TDS + Emb ) . The ac-   curacy of P type entities is high in both   English and Japanese . This is likely because nu-   merous entities of this type exist in the training   data and the person ’s names themselves are easy to   recognize from the surface . The C   type is not present in Japanese , and the accuracy for   that type is low in English because the disappear-   ance of these entities is uncertain for the nature of4514   the type as described in § 5.1 . This causes the train-   ing data to be contaminated with diverse contexts ,   resulting in low accuracy . The accuracy for E   type entities is the lowest in Japanese . This is likely   because the number of training data is small and   thus the model could not be sufficiently trained .   Relative recall and detection immediacy Ta-   ble 6 shows the distribution of the entity types ,   detection ratio , and lead time against the Wikipedia   update time for both languages . Overall , Proposed   ( TDS + Emb ) detected 1971 ( 75.57 % ) English and   563 ( 73.78 % ) Japanese disappearing entities . More   than 80 % of P entities were detected in   both languages , while the other types were found   45.32 % for English and 56.45 % for Japanese . Note   that some of the target entities are low frequency in   our Twitter archive and do not appear in disappear-   ing contexts , which may affect the performance .   Also , it is difficult to discover entities that are men-   tioned without disappearing contexts because our   method utilizes such disappearance signals .   For detection immediacy , we confirmed that   76.71 % of the discovered English entities ( 1,512   out of 1,971 ) and 83.33 % of the discovered   Japanese entities ( 485 out of 582 ) were detected   earlier than their update in Wikipedia . Remainingentities were often either mentioned infrequently   with disappearing contexts or their updates were   unusually fast . The mean ( and median ) lead days   of the first day when our method detected each en-   tity against their update date were 53 ( and 0.15 ) for   English and 69 ( and 0.47 ) days for Japanese . In par-   ticular , for other types of entities other than person ,   the lead days were 143 days ( and 78 ) for English   and 128 days ( and 98 ) for Japanese . This demon-   strates the detection immediacy of our method .   Our method could discover entities such as   Durgin - Park ( restaurant ) , Rolling Acres Mall ( shop-   ping mall ) , and CiteULike ( Internet property ) ,   which are the types of entities whose disappear-   ance is important to know in advance . This should   prevent us from making fruitless actions or missing   out on opportunities because the average lead time   for these types is more than 100 days . Interestingly ,   the updates for P type entities in Wikipedia   are faster for both languages . Since this kind of un-   controlled disappearance ( death ) occurs suddenly   and often receives media exposure , it is difficult to   detect these entities before they actually disappear .   It also shows that only certain types of entities are   updated faster in Wikipedia . Therefore , detecting   disappearing entities not only helps avoid missing   opportunities but also serves as a means to quickly   update the knowledge base .   6 Conclusion   In this paper , we have introduced the task of dis-   covering disappearing entities in microblogs ( § 1 ,   § 2 , and § 3 ) . To deal with the uncertainty of entity   disappearance , we considered the year of disap-   pearance in Wikipedia and fed it into time - sensitive   distant supervision ( § 4.1 ) . To perform the detec-   tion from noisy microblog posts , we proposed the   method of refining pretrained word embeddings   using the Twitter stream ( § 4.2 ) . We actually con-   structed a multi - lingual dataset in Japanese and   English using the proposed time - sensitive distant   supervision ( § 5.1 ) . Experimental results ( § 5.4 )   demonstrated that our method outperformed the   baseline method and successfully found more than   70 % of the target disappearing entities in Wikipedia   and they were detected more than a month earlier   than the update of the disappearance in Wikipedia .   We release the dataset used in our experiments .   We plan to extract information such as the spe-   cific time of disappearance to make the discovered   entities more useful for various applications.45157 Limitations   Our method can not discover entities that are men-   tioned without disappearing contexts since we uti-   lize such disappearance signals .   Although our experiments focused solely on   Wikipedia entities , they did not sufficiently cover   certain entities , such as stores and food products ,   for which disappearance is important for us . There-   fore , it is necessary to collect and conduct experi-   ments specifically for these entities .   8 Ethical Considerations   The dataset was collected using Twitter ’s official   APIand in compliance with Twitter ’s terms of   use . Only the tweet IDs of the tweets used in the   experiments will be made public , and we ensure   that their redistribution is in compliance with Twit-   ter ’s developer policy . Researchers can not collect   deleted tweets or tweets of private users , thus pro-   tecting user privacy .   We paid the annotators $ 0.03 per post they   checked ( approximately 3,000 to 5,000 posts ) .   Although the first author is affiliated with Yahoo   Japan Corporation , all the data used in the exper-   iments belong to the affiliation of the second and   third authors .   Acknowledgments   This work was partially supported by JSPS KAK-   ENHI Grant Number JP21H03494 , JP21H03445   and by JST , CREST Grant Number JPMJCR19A ,   Japan .   References451645174518ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   7   /squareA2 . Did you discuss any potential risks of your work ?   8   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   8   /squareB1 . Did you cite the creators of artifacts you used ?   8   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   8   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   8   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   8   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   5   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   5   C / squareDid you run computational experiments ?   5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   54519 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   5   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   5   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   5   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   5   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   5   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   5   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   5   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   54520