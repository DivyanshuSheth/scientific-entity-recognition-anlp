  Leyang Cui , Sen Yang , Yue ZhangZhejiang UniversitySchool of Engineering , Westlake UniversityInstitute of Advanced Technology , Westlake Institute for Advanced Study   { cuileyang , zhangyue}@westlake.edu.cn senyang.stu@gmail.com   Abstract   Thanks to the strong representation power of   neural encoders , neural chart - based parsers   have achieved highly competitive performance   by using local features . Recently , it has been   shown that non - local features in CRF struc-   tures lead to improvements . In this paper , we   investigate injecting non - local features into the   training process of a local span - based parser ,   by predicting constituent n - gram non - local   patterns and ensuring consistency between   non - local patterns and local constituents . Re-   sults show that our simple method gives bet-   ter results than the self - attentive parser on both   PTB and CTB . Besides , our method achieves   state - of - the - art BERT - based performance on   PTB ( 95.92 F1 ) and strong performance on   CTB ( 92.31 F1 ) . Our parser also achieves bet-   ter or competitive performance in multilingual   and zero - shot cross - domain settings compared   with the baseline .   1 Introduction   Constituency parsing is a fundamental task in nat-   ural language processing , which provides useful   information for downstream tasks such as machine   translation ( Wang et al . , 2018 ) , natural language in-   ference ( Chen et al . , 2017 ) , text summarization ( Xu   and Durrett , 2019 ) . Over the recent years , with   advance in deep learning and pre - training , neu-   ral chart - based constituency parsers ( Stern et al . ,   2017a ; Kitaev and Klein , 2018 ) have achieved   highly competitive results on benchmarks like Penn   Treebank ( PTB ) and Penn Chinese Treebank ( CTB )   by solely using local span prediction .   The above methods take the contextualized rep-   resentation ( e.g. , BERT ) of a text span as input , and   use a local classiﬁer network to calculate the scores   of the span being a syntactic constituent , together   with its constituent label . For testing , the output   layer uses a non - parametric dynamic programmingFigure 1 : An example of the non - local n - gram pat-   tern features : the 3 - gram pattern ( 3;11;fVBD NP PPg )   is composed of two constituent nodes and one part-   of - speech node ; the 2 - gram pattern ( 7;11;fNP PPg)is   composed of two constituent nodes .   algorithm ( e.g. , CKY ) to ﬁnd the highest - scoring   tree . Without explicitly modeling structured depen-   dencies between different constituents , the methods   give competitive results compared to non - local dis-   crete parsers ( Stern et al . , 2017a ; Kitaev and Klein ,   2018 ) . One possible explanation for their strong   performance is that the powerful neural encoders   are capable of capturing implicit output correlation   of the tree structure ( Stern et al . , 2017a ; Gaddy   et al . , 2018 ; Teng and Zhang , 2018 ) .   Recent work has shown that modeling non - local   output dependencies can beneﬁt neural structured   prediction tasks , such as NER ( Ma and Hovy ,   2016 ) , CCG supertagging ( Cui and Zhang , 2019 )   and dependency parsing ( Zhang et al . , 2020a ) .   Thus , an interesting research question is whether   injecting non - local tree structure features is also   beneﬁcial to neural chart - based constituency pars-   ing . To this end , we introduce two auxiliary train-   ing objectives . The ﬁrst is Pattern Prediction . As   shown in Figure 1 , we deﬁne pattern as the n - gram   constituents sharing the same parent . We ask the   model to predict the pattern based on its span rep-   resentation , which directly injects the non - local2065constituent tree structure to the encoder .   To allow stronger interaction between non - local   patterns and local constituents , we further pro-   pose a Consistency loss , which regularizes the co-   occurrence between constituents and patterns by   collecting corpus - level statistics . In particular , we   count whether the constituents can be a sub - tree of   the pattern based on the training set . For instance ,   both NNSandNPare legal to occur as sub - trees of   the 3 - gram pattern fVBD NP PPgin Figure 1 , while   SorADJP can not be contained within this pattern   based on grammar rules . Similarly , for the 2 - gram   patternfNP PPghighlighted in Figure 1 , both IN   andNPare consistent constituents , but JJis not .   TheConsistency loss can be considered as inject-   ing prior linguistic knowledge to our model , which   forces the encoder to understand the grammar rules .   Non - local dependencies among the constituents   that share the same pattern are thus explicitly mod-   eled . We denote our model as Injecting Non - local   Features for neural Chart - based parsers ( NFC ) .   We conduct experiments on both PTB and CTB .   Equipped with BERT , NFC achieves 95.92 F1 on   PTB test set , which is the best reported perfor-   mance for BERT - based single - model parsers . For   Chinese constituency parsing , NFC achieves highly   competitive results ( 92.31 F1 ) on CTB , outperform-   ing the baseline self - attentive parser ( 91.98 F1 ) and   a 0 - th order neural CRF parser ( 92.27 F1 ) ( Zhang   et al . , 2020b ) . To further test the generalization   ability , we annotate a multi - domain test set in En-   glish , including dialogue , forum , law , literature   and review domains . Experiments demonstrate   that NFC is robust in zero - shot cross - domain set-   tings . Finally , NFC also performs competitively   with other languages using the SPMRL 2013/2014   shared tasks , establishing the best reported results   on three rich resource languages . We release our   code and models at https://github.com/   RingoS / nfc - parser .   2 Related Work   Constituency Parsing . There are mainly two   lines of approaches for constituency parsing .   Transition - based methods process the input words   sequentially and construct the output constituency   tree incrementally by predicting a series of local   transition actions ( Zhang and Clark , 2009 ; Cross   and Huang , 2016 ; Liu and Zhang , 2017 ) . For   these methods , the sequence of transition actions   make traversal over a constituent tree . Althoughtransition - based methods directly model partial tree   structures , their local decision nature may lead   to error propagation ( Goldberg and Nivre , 2013 )   and worse performance compared with methods   that model long - term dependencies ( McDonald and   Nivre , 2011 ; Zhang and Nivre , 2012 ) . Similar to   transition - based methods , NFC also directly mod-   els partial tree structures . The difference is that   we inject tree structure information using two addi-   tional loss functions . Thus , our integration of non-   local constituent features is implicit in the encoder ,   rather than explicit in the decoding process . While   the relative effectiveness is empirical , it could po-   tentially alleviate error propagation .   Chart - based methods score each span indepen-   dently and perform global search over all possible   trees to ﬁnd the highest - score tree given a sentence .   Durrett and Klein ( 2015 ) represented nonlinear fea-   tures to a traditional CRF parser computed with a   feed - forward neural network . Stern et al . ( 2017b )   ﬁrst used LSTM to represent span features . Kitaev   and Klein ( 2018 ) adopted a self - attentive encoder   instead of the LSTM encoder to boost parser perfor-   mance . Mrini et al . ( 2020 ) proposed label attention   layers to replace self - attention layers . Zhou and   Zhao ( 2019 ) integrated constituency and depen-   dency structures into head - driven phrase structure   grammar . Tian et al . ( 2020 ) used span attention   to produce span representation to replace the sub-   traction of the hidden states at the span boundaries .   Despite their success , above work mainly focuses   on how to better encode features over the input sen-   tence . In contrast , we take the encoder of Kitaev   and Klein ( 2018 ) intact , being the ﬁrst to explore   new ways to introduce non - local training signal   into the local neural chart - based parsers .   Modeling Label Dependency . There is a line of   work focusing on modeling non - local output depen-   dencies . Zhang and Zhang ( 2010 ) used a Bayesian   network to encode the label dependency in multi-   label learning . For neural sequence labeling , Zhou   and Xu ( 2015 ) and Ma and Hovy ( 2016 ) built a   CRF layer on top of neural encoders to capture   label transition patterns . Pislar and Rei ( 2020 ) in-   troduced a sentence - level constraint to encourage   the model to generate coherent NER predictions .   Cui and Zhang ( 2019 ) investigated label attention   network to model the label dependency by produc-   ing label distribution in sequence labeling tasks .   Gui et al . ( 2020 ) proposed a two - stage label de-   coding framework based on Bayesian network to2066model long - term label dependencies . For syntac-   tic parsing , Zhang et al . ( 2020b ) demonstrated that   structured Tree CRF can boost parsing performance   over graph - based dependency parser . Our work is   in line with these in the sense that we consider   non - local structure information for neural struc-   ture prediction . To our knowledge , we are the ﬁrst   to inject sub - tree structure into neural chart - based   encoders for constituency parsing .   3 Baseline   Our baseline is adopted from the parsing model of   Kitaev and Klein ( 2018 ) and Kitaev et al . ( 2019 ) .   Given a sentence X = fx ; : : : ; xg , its correspond-   ing constituency parse tree Tis composed by a set   of labeled spans   T = f(i ; j ; l)gj ( 1 )   where iandjrepresent the t - th constituent   span ’s fencepost positions and lrepresents the   constituent label . The model assigns a score s(T )   to tree T , which can be decomposed as   s(T ) = Xs(i ; j ; l ) ( 2 )   Following Kitaev et al . ( 2019 ) , we use BERT   with a self - attentive encoder as the scoring function   s(i ; j; ) , and a chart decoder to perform a global-   optimal search over all possible trees to ﬁnd the   highest - scoring tree given the sentence . In particu-   lar , given an input sentence X = fx ; : : : ; xg , a list   of hidden representations H = fh;h ; : : : ; hg   is produced by the encoder , where his a hidden   representation of the input token x. Following pre-   vious work , the representation of a span ( i ; j)is   constructed by :   v = h h ( 3 )   Finally , vis fed into an MLP to produce real-   valued scores s(i ; j;)for all constituency labels : ( 4 )   where W , W , bandbare trainable parame-   ters , W2Rcan be considered as the con-   stituency label embedding matrix ( Cui and Zhang ,   2019 ) , where each column in Wcorresponds to   the embedding of a particular constituent label . jHj   represents the hidden dimension and jLjis the size   of the constituency label set .   Training . The model is trained to satisfy the   margin - based constraints   s(T)s(T ) +  ( T ; T ) ( 5 )   where Tdenotes the gold parse tree , and is   Hamming loss . The hinge loss can be written as(6 )   During inference time , the most - optimal tree   ^T= argmaxs(T ) ( 7 )   is obtained using a CKY - like algorithm .   4 Additional Training Objectives   We propose two auxiliary training objectives to   inject non - local features into the encoder , which   rely only on the annotations in the constituency   treebank , but not external resources .   4.1 Instance - level Pattern Loss   We deﬁne n - gram constituents , which shares the   same parent node , as a pattern . We use a triplet   ( i ; j ; l)to denote a pattern span beginning from   thei - th word and ending at j - th word . lis the   corresponding pattern label . Given a constituency   parse tree in Figure 1 , ( 3;11;fVBD NP PPg)is a   3 - gram pattern .   Similar to Eq 4 , an MLP is used for transforming   span representations to pattern prediction probabil-   ities:(8 )   where W , W , bandbare trainable param-   eters , W2Rcan be considered as the2067pattern label embedding matrix , where each col-   umn in Wcorresponds to the embedding of a   particular pattern label . jLjrepresents the size of   the pattern label set . For each instance , the cross-   entropy loss between the predicted patterns and the   gold patterns are calculated as   L= XXplog ^p ( 9 )   We use the span - level cross - entropy loss for pat-   terns ( Eq 9 ) instead of the margin loss in Eq 6 ,   because our pattern - prediction objective aims to   augment span representations via greedily classify-   ing each pattern span , rather than to reconstruct the   constituency parse tree through dynamic program-   ming .   4.2 Corpus - level Consistency Loss   Constituency scores and pattern probabilities are   produced based on a shared span representation ;   however , the two are subsequently separately pre-   dicted . Therefore , although the span representa-   tions contain both constituent and pattern infor-   mation , the dependencies between constituent and   pattern predictions are not explicitly modeled . Intu-   itively , constituents are distributed non - uniformly   in patterns , and such correlation can be obtained   in the corpus - level statistic . We propose a consis-   tency loss , which explicitly models the non - local   dependencies among constituents that belong to the   same pattern . As mentioned in the introduction , we   regard all constituent spans within a pattern span   as being consistent with the pattern span . Take 2-   gram patterns for example , which represents two   neighboring subtrees covering a text span . The con-   stituents that belong to the two subtrees , including   the top constituent and internal sub constituents ,   are considered as being consistent . We consider   only the constituent labels but not their correspond-   ing span locations for this task .   This loss can be understood ﬁrst at the instance   level . In particular , if a constituent span ( i ; j ; l )   is a subtree of a pattern span ( i ; j ; l ) , i.e.i>=   iandj<=j , where l = L[a](thea - th con-   stituent label in L ) andl = L[b](theb - th pattern   label in L ) , we deﬁne L[a]andL[b]to be con-   sistent ( denoted as y= 1 ) . Otherwise we con-   sider it to be non - consistent ( denoted as y= 0 ) .   This yields a consistency matrix Y2R   for each instance . The gold consistency matrix Yprovides information regarding non - local depen-   dencies among constituents and patterns .   An intuitive method to predict the consistency   matrix Yis to make use of the constituency label   embedding matrix W(see Eq 4 for deﬁnition ) ,   the pattern label embedding matrix W(see Eq 8   for deﬁnition ) and the span representations V(see   Eq 3 for deﬁnition ):   ^Y= Sigmoid    ( WUV)(VUW)   ( 10 )   where U;U2Rare trainable parame-   ters .   Intuitively , the left term , WUV , integrates   the representations of the pattern span and all pos-   sible constituent label embeddings . The second   term , VUW , integrates features of the span   and all pattern embeddings . Each binary element   in the resulting ^Y2Rdenotes whether   a particular constituent label is consistent with a   particular pattern in the given span context . Eq 10   can be predicted on the instance - level for ensur-   ing consistency between patterns and constituent .   However , this naive method is difﬁcult for training ,   and computationally infeasible , because the span   representation matrix V2Ris composed   ofnspan representations v2Rand the   asymptotic complexity is :   O   ( jLj+jLj)(jHj+njHj ) + jLjjLjn   ( 11 )   for a single training instance .   We instead use a corpus - level constraint on the   non - local dependencies among constituents and   patterns . In this way , Eq 10 is reduced to be inde-   pendent of individual span representations :   ^Y= Sigmoid    WUW   ( 12 )   where U2Ris trainable .   This trick decreases the asymptotic complexity   toO(jLjjHj+jLjjLjjHj ) . The cross - entropy   loss between the predicted consistency matrix and   gold consistency labels is used to optimize the   model :   L= XXylog ^y ( 13 )   The corpus - level constraint can be considered   as a prior linguistic knowledge statistic from the   treebank , which forces the encoder to understand   the grammar rules.2068   4.3 Training   Given a constituency tree , we minimize the sum of   the three objectives to optimize the parser :   L = L+L+L ( 14 )   4.4 Computational Cost   The number of training parameters increased by   NFC is W2R , W2R , b2   Randb2Rin Eq 8 and U2R   in Eq 12 . Taking training model on PTB as an   example , NFC adds less than 0.7 M parameters   to 342 M parameters baseline model ( Kitaev and   Klein , 2018 ) based on BERT - large - uncased dur-   ing training . NFC is identical to our baseline self-   attentive parser ( Kitaev and Klein , 2018 ) during   inference .   5 Experiments   We empirically compare NFC with the baseline   parser in different settings , including in - domain ,   cross - domain and multilingual benchmarks .   5.1 Dataset   Table 1 shows the detailed statistic of our datasets .   In - domain . We conduct experiments on both En-   glish and Chinese , using the Penn Treebank ( Mar-   cus et al . , 1993 ) as our English dataset , with stan-   dard splits of section 02 - 21 for training , section 22   for development and section 23 for testing . For Chi-   nese , we split the Penn Chinese Treebank ( CTB )   5.1 ( Xue et al . , 2005 ) , taking articles 001 - 270 and   440 - 1151 as training set , articles 301 - 325 as devel-   opment set and articles 271 - 300 as test set . Cross - domain . To test the robustness of our   methods across difference domains , we further an-   notate ﬁve test set in dialogue , forum , law , literature   and review domains . For the dialogue domain , we   randomly sample dialogue utterances from Wiz-   ard of Wikipedia ( Dinan et al . , 2019 ) , which is a   chit - chat dialogue benchmark produced by humans .   For the forum domain , we use users ’ communi-   cation records from Reddit , crawled and released   by Völske et al . ( 2017 ) . For the law domain , we   sample text from European Court of Human Rights   Database ( Stiansen and V oeten , 2019 ) , which in-   cludes detailing judicial decision patterns . For the   literature domain , we download literary ﬁctions   from Project Gutenberg . For the review domain ,   we use plain text across a variety of product genres ,   released by SNAP Amazon Review Dataset ( He   and McAuley , 2016 ) . After obtaining the plain text ,   we ask annotators whose majors are linguistics to   annotate constituency parse tree by following the   PTB guideline . We name our dataset as Multi-   domain Constituency Treebank ( MCTB ) . More   details of the dataset are documented in Yang et al .   ( 2022 ) .   Multi - lingual . For the multilingual testing , we   select three rich resource language from the   SPMRL 2013 - 2014 shared task ( Seddah et al . ,   2013 ): French , German and Korean , which include   at least 10,000 training instances , and three low-   resource language : Hungarian , Basque and Polish .   5.2 Setup   Our code is based on the open - sourced code   of Kitaev and Klein ( 2018 ) . The training pro-   cess gets terminated if no improvement on de-   velopment F1 is obtained in the last 60 epochs .   We evaluate the models which have the best F1   on the development set . For fair comparison ,   all reported results and baselines are augmented   with BERT . We adopt BERT - large - uncased   for English , BERT - base for Chinese and   BERT - multi - lingual - uncased for other   languages . Most of our hyper - parameters are   adopted from Kitaev and Klein ( 2018 ) and Fried   et al . ( 2019 ) . For scales of the two additional losses ,   we set the scale of pattern loss to 1.0 and the scale   of consistency loss to 5.0 for all experiments .   To reduce the model size , we ﬁlter out those non-2069   local pattern features that appear less than 5 times   in the PTB training set and those that account for   less than 0.5 % of all pattern occurrences in the CTB   training set . The out - of - vocabulary patterns are   set as < UNK > . This results in moderate pattern   vocabulary sizes of 841 for PTB and 514 for CTB .   For evaluation on PTB , CTB and cross - domain   dataset , we use the EV ALB script for evaluation .   For the SPMRL datasets , we follow the same setup   in EV ALB as Kitaev and Klein ( 2018 ) .   5.3 In - domain Experiments   We report the performance of our method on the   test sets of PTB and CTB in Table 2 and 3 , respec-   tively . Compared with the baseline parser ( Kitaev   and Klein , 2018 ) , our method obtains an absoluteimprovement of 0.20 % F1 on PTB ( p<0.01 ) and   0.33 % F1 on CTB ( p<0.01 ) , which veriﬁes the   effectiveness of injecting non - local features into   neural local span - based constituency parsers . Note   that the proposed method adds less than 0.7 M pa-   rameters to the 342 M parameter baseline model   using BERT - large .   The parser trained with both the pattern loss   ( Section 4.1 ) and consistency loss ( Section 4.2 )   outperforms the one trained only with pattern loss   by 0.14 % F1 ( p<0.01 ) . This suggests that the con-   straints between constituents and non - local pattern   features are crucial for injecting non - local features   into local span - based parsers . One possible expla-   nation for the improvement is that the constraints   may bridge the gap between local and non - local   supervision signals , since these two are originally   separately predicted while merely sharing the same   encoder in the training phase .   We further compare our method with the re-   cent state - of - the - art parsers on PTB and CTB . Liu   and Zhang ( 2017 ) propose an in - order transition-   based constituency parser . Kitaev and Klein ( 2018 )   use self - attentive layers instead of LSTM layers   to boost performance . Zhou and Zhao ( 2019 )   jointly optimize constituency parsing and depen-   dency parsing objectives using head - driven phrase   structure grammar . Mrini et al . ( 2020 ) extend Zhou   and Zhao ( 2019 ) by introducing label attention lay-   ers . Zhang et al . ( 2020b ) integrate a CRF layer to   a chart - based parser for structural training ( with-   out non - local features ) . Tian et al . ( 2020 ) use span   attention for better span representation .   Compared with these methods , the proposed   method achieves an F1 of 95.92 % , which exceeds   previous best numbers for BERT - based single-   model parsers on the PTB test set . We further   compare experiments for ﬁve runs , and ﬁnd that   NFC signiﬁcantly outperforms Kitaev and Klein   ( 2018 ) ( p<0.01 ) . The test score of 92.31 % F1 on   CTB signiﬁcantly outperforms the result ( 91.98 %   F1 ) of the baseline ( p<0.01 ) . Compared with the   CRF parser of Zhang et al . ( 2020b ) , our method   gives better scores without global normalization in   training . This shows the effectiveness of integrat-   ing non - local information during training using our   simple regularization . The result is highly competi-   tive with the current best result ( Mrini et al . , 2020 ) ,   which is obtained by using external dependency   parsing data.2070   5.4 Cross - domain Experiments   We compare the generalization of our methods with   baselines in Table 4 . In particular , all the parsers   are trained on PTB training and validated on PTB   development , and are tested on cross - domain test   in the zero - shot setting . As shown in the table , our   model achieves 5 best - reported results among 6   cross - domain test sets with an averaged F1 score   of 89.85 % , outperforming our baseline parser by   2.97 % points . This shows that structure informa-   tion is useful for improving cross - domain perfor-   mance , which is consistent with ﬁndings from pre-   vious work ( Fried et al . , 2019 ) .   To better understand the beneﬁt of pattern fea-   tures , we calculate Pearson correlation of n - gram   pattern distributions between the PTB training set   and various test sets in Figure 3 . First , we ﬁnd that   the correlation between the PTB training set and   the PTB test set is close to 1.0 , which veriﬁes the   effectiveness of the corpus - level pattern knowledge   during inference . Second , the 3 - gram pattern corre-   lation of all domains exceeds 0.75 , demonstrating   thatn - gram pattern knowledge is robust across do-   mains , which supports the strong performance of   NFC in the zero - shot cross - domain setting . Third ,   pattern correlation decreases signiﬁcantly as nin-   creases , which suggests that transferable non - local   information is limited to a certain window size of   n - gram constituents .   5.5 Multilingual Experiments   We compare NFC with Kitaev and Klein ( 2018 )   and Nguyen et al . ( 2020 ) on SPMRL . The results   are shown in Table 5 . Nguyen et al . ( 2020 ) use   pointer network to predict a sequence of pointing   decisions for constituency parsing . As can be seen ,   Nguyen et al . ( 2020 ) do not show obvious advan-   tages over Kitaev and Klein ( 2018 ) . NFC outper-   forms these two methods on three rich resource   languages . For example , NFC achieves 89.07 % F1   on Korean , outperforming Kitaev and Klein ( 2018 )   by 0.27 % F1 , suggesting that NFC is generally ef-   fective across languages . However , NFC does not   give better results compared with Kitaev and Klein   ( 2018 ) on low - resource languages . One possible   explanation is that it is difﬁcult to obtain prior lin-   guistic knowledge from corpus - level statistics by   using a relatively small number of instances .   6 Analysis   6.1n - gram Pattern Level Performance   Figure 4 shows the pattern - level F1 before and   after introducing the two auxiliary training objec-   tives . In particular , we calculate the pattern - level   F1 by calculating the F1 score for patterns based2071   on the constituency trees predicted by CKY de-   coding . Although our baseline parser with BERT   achieves 95.76 % F1 scores on PTB , the pattern-   level F1 is 80.28 % measured by 3 - gram . When   testing on the dialogue domain , the result is re-   duced to only 57.47 % F1 , which indicates that   even a strong neural encoder still has difﬁculties   capturing constituent dependency from the input   sequence alone . After introducing the pattern and   consistency losses , NFC signiﬁcantly outperforms   the baseline parser measured by 3 - gram pattern   F1 . Though there is no direct supervision signal   for 2 - gram pattern , NFC also gives better results   on pattern F1 of 2 - gram , which are subsumed by   3 - gram patterns . This suggests that NFC can effec-   tively represent sub - tree structures .   6.2 F1 against Span Length   We compare the performance of the baseline and   our method on constituent spans with different   word lengths . Figure 5 shows the trends of F1   scores on the PTB test set as the minimum con-   stituent span length increases . Our method shows   a minor improvement at the beginning , but the gap   becomes more evident when the minimum span   length increases , demonstrating its advantage in   capturing more sophisticated constituency label de-   pendency .   6.3 Exact Match   Exact match score represents the percentage of sen-   tences whose predicted trees are entirely the same   as the golden trees . Producing exactly matched   trees could improve user experiences in practical   scenarios and beneﬁt downstream applications on   other tasks ( Petrov and Klein , 2007 ; Kummerfeld   et al . , 2012 ) . We compare exact match scores of2072NFC with that of the baseline parser . As shown in   Figure 6 , NFC achieves large improvements in ex-   act match score for all domains . For instance , NFC   gets 33.40 % exact match score in the review do-   main , outperforming the baseline by 10.2 % points .   We assume that this results from the fact that NFC   successfully ensures the output tree structure by   modeling non - local correlation .   6.4 Model Efﬁciency   As mentioned in Section 4.4 , NFC only introduces   a few training parameters to the baseline model ( Ki-   taev and Klein , 2018 ) . For PTB , NFC takes about   19 hours to train with a single RTX 2080Ti , while   the baseline takes about 13 hours . For CTB , the   approximate training time is 12 hours for NFC and   7 hours for the baseline . Our inference time is the   same as that of the baseline parser , since no further   computational operations are added to the infer-   ence phase . Both take around 11 seconds to parse   the PTB section 23 ( 2416 sentences , an average of   23.5 tokens per sentence ) .   7 Conclusion   We investigated graph - based constituency parsing   with non - local features – both in the sense that fea-   tures are not restricted to one constituent , and in   the sense that they are not restricted to each train-   ing instance . Experimental results verify the effec-   tiveness of injecting non - local features to neural   chart - based constituency parsing . Equipped with   pre - trained BERT , our method achieves 95.92 %   F1 on PTB and 92.31 % F1 on CTB . We further   demonstrated that the proposed method gives better   or competitive results in multilingual and zero - shot   cross - domain settings .   Acknowledgements   We appreciate the insightful comments from the   anonymous reviewers . We thank Zhiyang Teng for   the insightful discussions . We gratefully acknowl-   edge funding from the National Natural Science   Foundation of China ( NSFC No.61976180 ) .   Ethical Considerations   As mentioned in Section 5.1 , we collected the raw   data from free and publicly available sources that   have no copyright or privacy issues . We recruited   our annotators from the linguistics departments   of local universities through public advertisement   with a speciﬁed pay rate . All of our annotators aresenior undergraduate students or graduate students   in linguistic majors who took this annotation as   a part - time job . We manually shufﬂed the data so   that all batches of to - be - annotated data have similar   lengths on average . An annotator could annotate   around 25 instances per hour . We pay them 50   CNY an hour . The local minimum salary in the   year 2021 is 22 CNY per hour for part - time jobs .   Our annotated data only involves factual infor-   mation ( i.e. , syntactic annotation ) , but not opinions ,   attitudes or beliefs . Therefore , the annotation job   does not belong to human subject research ; and   IRB approval is not required .   References207320742075