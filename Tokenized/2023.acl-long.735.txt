  Mourad Heddaya   University of ChicagoSolomon Dworkin   University of ChicagoChenhao Tan   University of Chicago   Rob Voigt   Northwestern UniversityAlexander Zentefis   Yale University   Abstract   Leveraging an established exercise in negoti-   ation education , we build a novel dataset for   studying how the use of language shapes bilat-   eral bargaining . Our dataset extends existing   work in two ways : 1 ) we recruit participants   via behavioral labs instead of crowdsourcing   platforms and allow participants to negotiate   through audio , enabling more naturalistic in-   teractions ; 2 ) we add a control setting where   participants negotiate only through alternating ,   written numeric offers . Despite the two con-   trasting forms of communication , we find that   the average agreed prices of the two treatments   are identical . But when subjects can talk , fewer   offers are exchanged , negotiations finish faster ,   the likelihood of reaching agreement rises , and   the variance of prices at which subjects agree   drops substantially . We further propose a tax-   onomy of speech acts in negotiation and enrich   the dataset with annotated speech acts . We   set up prediction tasks to predict negotiation   success and find that being reactive to the argu-   ments of the other party is advantageous over   driving the negotiation .   1 Introduction   Bilateral bargaining , in the sense of a goal - oriented   negotiation between two parties , is a fundamental   human social behavior that takes shape in many   areas of social experience . Driven by a desire to   better understand this form of interaction , a rich   body of work in economics and psychology has   evolved to study bargaining ( Rubin and Brown ,   1975 ; Bazerman et al . , 2000 ; Roth , 2020 ) . How-   ever , this work has seldom paid careful attention   to the use of language and its fine - grained impacts   on bargaining conversations ; indeed , many studies   operationalize bargaining as simply the back - and-   forth exchange of numerical values . Meanwhile ,   there is growing interest in bargaining in NLP ori-   ented towards the goal of building dialogue systems   capable of engaging in effective negotiation ( Zhanet al . , 2022 ; Fu et al . , 2023 ) . In this work , we aim to   bridge these two lines of work and develop a com-   putational understanding of how language shapes   bilateral bargaining .   To do so , building on a widely used exer-   cise involving the bargaining over the price of a   house used in negotiation education , we develop a   controlled experimental environment to collect a   dataset of bargaining conversations . The treatment   in our experiment is the manner in which subjects   communicate : either through alternating , written ,   numeric offers ( the alternating offers or AO condi-   tion ) or unstructured , verbal communication ( the   natural language or NL condition ) . Furthermore ,   to encourage naturalistic interactions , we recruit   participants via behavioral labs and allow partici-   pants to negotiate in a conversational setting using   audio on Zoom instead of crowdingsourcing text   conversations as prior work has done ( Asher et al . ,   2016 ; Lewis et al . , 2017 ; He et al . , 2018 ) . In total ,   we collect a dataset with 230 alternating - offers ne-   gotiations and 178 natural language negotiations .   In contrast with He et al . ( 2018 ) ’s Craigslist nego-   tiation dataset , our natural language negotiations   have an average of over 4x more turns exchanged   during each conversation , so our dataset represents   a richer source to explore linguistic aspects of bar-   gaining behavior than has been presented by exist-   ing work in this area .   In addition , we enrich the dataset by annotat-   ing all the conversations with a set of negotiation-   specific speech acts . Inspired by prior work on   rhetorical strategies in negotiations ( Chang and   Woo , 1994 ; Weigand et al . , 2003 ; Twitchell et al . ,   2013 ) , we create a simplified taxonomy of what   we term bargaining acts and hire undergraduate   research assistants to provide annotations . To the   best of our knowledge , our dataset of speech acts   in negotiations is an order of magnitude larger than13161existing datasets .   We first provide descriptive results based on   our dataset . Although the AO and NL conditions   are conducted via different communication mecha-   nisms , they reach the same average agreed prices .   However , when subjects can talk , fewer offers are   exchanged , negotiations finish faster , the likelihood   of reaching agreement rises , and the variance of   prices at which subjects agree drops substantially .   These observations suggest that the use of language   facilitates collaboration . We also find differences   in how buyers and sellers employ bargaining acts .   Recorded and transcribed speech provides more   direct access to the intuitive attitudes and behaviors   of the buyers and sellers . This enables us to iden-   tify subtle types of expression that are predictive   of negotiation outcomes and reveal underlying dy-   namics of negotiation . Other findings corroborate   conclusions from Lee and Ames ( 2017 ) , who dis-   tinguish the effectiveness of negotiators ’ different   expressions of the same rationale .   We set up prediction tasks to predict the outcome   of a negotiation based on features of the conversa-   tion and analyze the important features contribut-   ing to class differentiation . Our results show that   LIWC features provide consistently strong perfor-   mance and even outperform Longformer ( Beltagy   et al . , 2020 ) given the beginning of a negotiation .   Important features reveal that successful sellers   address buyers directly with the pronoun “ you ”   and prompt them to divulge their considerations   and preferences , while successful buyers convey a   sense of emotional indifference and thoughtfulness   through filler words and curt acknowledgements of   the sellers ’ arguments .   In summary , we make the following contribu-   tions :   •We build a novel dataset of bargaining and pro-   vide annotations of bargaining acts ;   •We demonstrate that the ability to communicate   using language facilitates cooperation .   •Our work reveals linguistic signals that are pre-   dictive of negotiation outcomes . For instance , it   is advantageous to be patient and reactive to the   arguments of the other party , rather than the one   driving the negotiation .   2 Related Work   Negotiation is a growing area of study in computer   science . Zhan et al . ( 2022 ) provide an excellent   survey of research on negotiation dialogue systems . Lewis et al . ( 2017 ) train recurrent neural networks   to generate natural language dialogues in negoti-   ations . He et al . ( 2018 ) propose a modular gener-   ative model based on dialogue acts . Our focus is   on deriving computational understanding of how   language shapes negotiation .   Several research disciplines have studied bilat-   eral bargaining from different perspectives and us-   ing different tools . Economic theory has investi-   gated the role of incomplete information ( Ausubel   et al . , 2002 ) and highlighted the role of explicit   communication ( Crawford , 1990 ; Roth , 2020 ) .   Bazerman et al . ( 2000 ) and Pruitt ( 2013 ) provide   an overview of the psychology literature on negoti-   ation . However , these studies tend to overlook the   content of the communication , with some notable   exceptions ( Swaab et al . , 2011 ; Jeong et al . , 2019 ;   Lee and Ames , 2017 ) .   The most related work to ours is Lee and Ames   ( 2017 ) , who study how bargaining outcomes are   affected by the way a rationale is expressed . They   find that expressions that hint at a constraint ( e.g. ,   “ I ca n’t pay more ” ) are more effective at shaping a   seller ’s views of the buyer ’s willingness to pay than   critique rationales ( e.g. , “ it ’s not worth more ” ) .   3 Dataset   The first contribution of our work is building the   first transcript dataset of spoken natural language   bargaining between lab experiment participants .   Our dataset extends existing datasets in four ways :   1.Negotiation happens in spoken language , and is   thus more fluid and natural , akin to real - world   bargaining scenarios , such as price haggling in   vendor markets , union negotiations , or diplo-   macy talks , while existing work is largely based   on written exchanges ( Asher et al . , 2016 ; Lewis   et al . , 2017 ; He et al . , 2018 ) ;   2.Our work is the first one to introduce a control   condition without the use of natural language ;   3.Participants are recruited through behavioral   labs at universities and their incentive struc-   ture is more high - powered ( i.e. , bonus earn-   ings based on outcomes and payment exceeding   the typical $ 12 hourly wage ) than for a crowd-   worker on Amazon Mechanical Turk ;   4.We supplement the transcripts with manual an-   notation of speech acts ( see § 4 ) .   While contributing greatly to our understand-   ing of negotiation , existing bargaining datasets are13162somewhat limited in being based on written ex-   changes ( He et al . , 2018 ) , often in the context of a   highly structured game ( Asher et al . , 2016 ; Lewis   et al . , 2017 ) .   Experiment design . We conducted a controlled   experiment whose setting reflected a common life   experience : the purchase or sale of a house . We   adapted the setting in “ Buying a House ” by Sally   Blount , a popular exercise from the Dispute Resolu-   tion Research Center ( DRRC ) of Northwestern Uni-   versity ’s Kellogg School of Management ( Blount ,   2000).We randomly paired participants and each   was assigned the role of buyer or seller . In each   pairing , buyer and seller negotiated a price of the   house anonymously . Both buyer and seller were   aware of the listing price of $ 240,000 and shared   the same descriptions of the house and surrounding   area , along with recent sales prices of comparable   homes . However , each participant was given a pri-   vate valuation of the house ( $ 235,000 for the buyer   and $ 225,000 for the seller ) .   Participant bonus earnings depended on bargain-   ing outcomes to incentivize subjects to engage in   realistic negotiating behavior . If no agreement was   reached , neither party earned bonus money . On   an hourly basis , compensation seemed significant   enough to influence participant behavior ( i.e. , at   least $ 40 / hour was on the table per round ) . On av-   erage , subjects earned roughly $ 23.25 / hour . More   details can be found in Appendix B.   Each subject participated in two bargaining   rounds . In the first round , a buyer - seller pair com-   municated via alternating offers ( AO ) in an online   chat that only accepted numeric entries . Each par-   ticipant could choose to accept or counter each   offer they received . In the second round , partici-   pants played the same role , either buyer or seller ,   but were assigned a new partner . In this round ,   each pair communicated in natural language ( NL )   via audio only on Zoom ( videos were monitored   to be turned off to avoid signals from gesture and   facial expressions ) . The subjects were restricted   from disclosing their respective private value and   compensation structure and informed that doing so   would result in forfeiture of their earnings . Our ex-   periment is approved by the IRB at an anonymous   institution .   Preprocessing . We transcribed the audio from   the Zoom negotiation settings using Amazon Tran-   scribe . Transcription produces strictly alternating   seller and buyer turns , without sentence segmenta-   tion . We use the resulting transcripts for the anno-   tation and analyses described in this paper . We trim   the end of each negotiation at the point of agree-   ment on a final price for the house , discarding any   interaction that occurs subsequently . We describe   in § 4 the annotation procedures that allowed us to   reliably identify the point of agreement .   Descriptive statistics . Table 1 provides descrip-   tive statistics of the AO and NL treatments . Since   a failed negotiation results in no bonus for both   sides , most negotiations end with a successful   sale . Nevertheless , the probability of agreement   is roughly 7 percentage points higher under NL   than AO ( 97.2 % versus 90.0 % ) . A two - tailed t-   test with heteroskedasticity - robust standard errors   shows that the difference in agreement probability   is significant . Moreover , in contrast with the AO   treatment , the NL treatment produces negotiations   that , on average , have ∼1.5x more turns , but NL   turns are over 50 % shorter in duration , and NL ne-   gotiations are roughly 30 % shorter in total duration   and feature about 74 % fewer offers .   Surprisingly , without the ability to communicate   using language , buyers and sellers are less efficient   in reconciling their differences . In the AO treat-   ment , the combination of fewer turns that are each ,   individually , longer in duration is telling . Interlocu-   tors are spending more time silently strategizing13163   and considering their next act . However , this time   invested is not fruitful individually nor at the level   of coordination , as exemplified by a lower proba-   bility of agreement and equivalent agreed prices   among successful negotiations , likely due to an   impoverished channel of communication .   Figure 1 shows that the distributions of agreed   prices largely overlap between the two treatments ,   but the distribution in prices under NL is substan-   tially narrower than under AO . Between the two   treatments , the mean agreed price conditional on   reaching agreement is identical ( $ 229.8 thousand ) .   However , the standard deviation of agreed prices   under NL is about one - third of that under AO ( 3.1   versus 10.4 ) . A Fligner - Killeen ( FK ) ( Fligner and   Killeen , 1976 ) two - sample scale test shows that the   standard deviation of the AO price distribution is   statistically larger than the NL counterpart .   4 Bargaining Act Annotation   Previous researchers have recognized the inher-   ently speech - act - like character of negotiations   ( Chang and Woo , 1994 ; Weigand et al . , 2003 ;   Twitchell et al . , 2013 ) . Many or most utterances   in a bargaining context can be thought of as tak-   ing some action with reference to the negotiation .   Here we propose and present a simplified ontol-   ogy of negotiation - oriented speech acts ( hereafter ,   bargaining acts ) relevant to the present context of   negotiation . Two trained undergraduate research   assistants annotated all transcripts according to five   bargaining acts : 1 ) new offer , 2 ) repeat offer , 3 )   push , 4 ) comparison , 5 ) allowance , and 6 ) end . Ta-   ble 2 provides definitions and examples . Note that   each turn can include multiple bargaining acts . In   addition , each speech act is also annotated with a   numerical offer , if applicable .   Twenty - four transcripts were annotated by both   annotators to allow agreement to be calculated . Us-   ing MASI distance weighting ( Passonneau , 2006 ) ,   we found a Krippendorff ’s alpha ( Hayes and Krip-   pendorff , 2007 ) of 0.72 , representing a high degree   of agreement for a pragmatic annotation task .   Figure 2 shows that new offers , pushes , and com-   parisons are relatively more frequent and appear   more consistently in all the negotiations than al-   lowances andrepeat offers . We note in Table 1   thatrepeat offers are dramatically more common   in the AO condition than the NL condition ( 11.3   vs. 1.56 per negotiation ) . With linguistic context ,   negotiators are less likely to engage in fundamen-   tally uncooperative behavior by simply repeating   past offers over again .   Comparing buyers to sellers , we observe that   buyers make on average 1 more new offers per   negotiation than sellers ( independent sample , het-   eroskedasticity robust t - test , p= 0.02 ) . We find no   statistically significant differences between roles   for the other five bargaining acts .   The bargaining act annotations allow us to de-   scribe a negotiation as a sequence of offers pro-   posed by the buyer and seller . We compare how   the frequency and pattern of numerical offers differ   across 1 ) experimental treatments ( NL vs. AO ) and   2 ) negotiation outcomes . We characterize differ-   ent properties of the negotiations as well as their   trajectories over the course the interaction .   Figure 3 reveals three general patterns on offer   trajectories . First , both AO and NL bargaining   feature a similar range of new offers exchanged in   the early stages of the negotiation . Early on , buyers13164   in both treatments present new offers as low as 170 ;   and sellers , as high as 270 . But extreme offers are   more prevalent in AO than NL bargaining . Second ,   both the AO and NL trajectories exhibit a rhythmic   pattern of low and high offers , which is familiar   to real - world negotiations . The buyer ’s low offer   is countered by the seller ’s high offer , which is   then countered by the buyer ’s slightly increased   low offer , and so on . Third , NL bargaining takes   far fewer new offers to reach agreement than AO   bargaining . Figure 3b clearly demonstrates that NL   negotiations converge quicker , with consecutive   offers converging to within $ 5 K after 6 new offers .   AO negotiations take over 40 new offer exchanges   to reach a similar convergence .   5 Predicting Negotiation Outcomes   Finally , we set up prediction tasks to understand   the relationship between the use of natural lan-   guage and negotiation success . Overall , our models   demonstrate performance gains over majority class   in most settings . Surprisingly , Logistic Regression   using bag - of - words and LIWC category features   outperform the neural model . We observe differ-   entiation between classification accuracy on seller   only and buyer only speech , and highlight features   that explain this difference .   5.1 Experimental Setup   Task . We consider a binary classification task   with two classes : 1 ) “ seller win ” and 2 ) “ buyerwin ” , where a negotiation is classified by whether it   concluded with an agreed price greater than $ 230 K   or less than $ 230 K , respectively . We focus on ne-   gotiations that end with an advantage for either the   buyer or seller to better understand the dynamics   that produce an asymmetric outcome . Hence , we   omit the negotiations that ended with $ 230 K or that   did not reach an agreed price . This leaves us 119   negotiations .   As the predictive task may become trivial if   we see the entire exchange , we build 10 versions   of each negotiation by incrementally adding pro-   portions of the negotiation to the input with a   step size of 10 % . Thus , we obtain input / output   pairs ( X , y)for a given negotiation , where k=   { 10 % , . . . , 100 % } , and each kcorresponds to a   different prediction task ; namely , whether the ne-   gotiation outcome can be predicted by the first k   percentage of the interaction .   Methods . We test two setups for our task . The   first is a standard linear model with logistic re-   gression . The second is an end - to - end approach   using Longformer , a transformer - based model for   encoding and classifying long sequences . In par-   ticular , we use the encoder and output modules of   LongformerEncoderDecoder ( LED ) ( Beltagy et al . ,   2020 ) , a variant of the original Longformer model ,   which can encode sequences up to 16,384 tokens   in length . This exceeds the maximum input length   in our dataset .   In the logistic regression experiments , we treat13165   the numerical offers as an oracle and consider three   other feature sets : 1 ) Transcription texts ; 2 ) Bar-   gaining acts ; 3 ) LIWC categories ( Tausczik and   Pennebaker , 2010).We represent each negotiation   as a binary bag - of - words encoding of the features   listed above . For bargaining acts , we construct   the vocabulary based on unigrams and bigrams ; for   the other feature sets , we only include unigrams .   We include bigrams for bargaining acts to capture   local combinations of bargaining acts . To main-   tain a reasonable vocabulary size , we only consider   unigrams from the transcribed text that occur in   at least 5 negotiations ( see Appendix C for total   feature counts ) . We replace numbers mentioned   in the text with a generic [ NUM ] token to elim-   inate the strongly predictive signal of new offers   and focus on language instead . In experiments with   LED , we add two special tokens [ SELLER ] and   [ BUYER ] that we concatenate to the start of each   turn depending on who is speaking . We make no   other changes to the transcribed text . The input to   LED is the concatenation of all the turns .   Evaluation . We use accuracy as our main evalua-   tion metric . In all experiments , due to the relatively   small size of our dataset , we use nested five - fold   cross validation for both inner and outer cross val-   idations . For logistic regression , we grid search   the best ℓcoefficient within { 2 } , where xranges   over 11 values evenly spaced between – 10 and   1 . We further concatenate the speaker ( ‘ buyer ’ or   ‘ seller ’ ) and the turn position within the negotiation .   We treat these as hyper - parameters . We represent   the position as k , where kcorresponds to a fraction   of the conversation , as defined earlier . For example ,   the word “ house ” spoken by the seller in the first   10 % of turns in a negotiation would be tokenized   as “ s1 - house ” . In the LED experiments , we omit   the inner cross validation and use a batch size of 4 ,   the largest possible batch size given our memory   constraints . We select the best performing learn-   ing rate out of { 5e−5,3e−4,3e−3}and early   stop based on training loss convergence.13166   5.2 Results   Predictive performance . We start by looking   at the overall predictive performance . Figure 4   presents results for all models . For the oracle con-   dition ( numerical ) , as expected , prediction accu-   racy increases monotonically and steadily as the   fraction of the conversation and the corresponding   numerical offers in the input increases from 10 % to   100 % of the conversation . As the buyer and seller   converge towards an agreed price , the offers made   provide strong signal about the outcome .   However , this task proves much more challeng-   ing for other models where we do not include nu-   merical offers provided by annotators . One intrigu-   ing observation is that LED consistently under-   performs logistic regression . Within logistic regres-   sion , LIWC categories outperform other features   and achieve 63.1 % accuracy whereas text - based   BOW features achieve a best score of 58.9 % . Fur-   thermore , there is no clear trend of performance   growing as the fraction of negotiation increases .   While bargaining actions under - perform other fea-   tures overall , there is a notable jump in accuracy at   fraction 30 % , which we will revisit later .   Buyer vs. seller . In bilateral bargaining , an inter-   esting question is which party drives the negotia-   tion , and to what effect ? To further understand the   role of buyer vs. seller , we only consider features   of buyer texts or seller texts .   Although the performance of LIWC does not   vary much for buyer and seller texts ( Figure 5a ) ,   Figures 5b and 5c show contrasting differences   in prediction accuracy for sellers and buyers   at various fractions of a negotiation . Seller   transcription text achieves ~10 % higher accu-   racy than buyer and buyer + seller at fractions   20 % ( p= 0.01),30 % ( p= 0.01),90 % ( p=0.001),100 % ( p= 0.01 ) . Meanwhile , buyer bar-   gaining acts outperform seller acts throughout and   are particularly effective at 40 % ( p= 0.008 ) and   50 % ( p= 0.03)of the negotiation .   Important features . To understand in greater de-   tail which features are more helpful for prediction ,   we compare the fitted logistic regression models ’   feature coefficients . Coefficients with the largest   absolute values are associated with more discrimi-   nating features .   We first discuss features from LIWC , our best   performing feature set ( Table 3a ) . Second - person   pronouns ( “ 2 - you ” ) occurring around 20 % and 30 %   into the negotiation are strongly predictive of a win   for either party . An example use by the seller is “ i   think we can negotiate and talk about it but yeah   you can start by giving me a price ” . Similarly , it   appears to be effective for buyers who bide their   time and allow the seller to drive the negotiation by   using filler words such as “ mhm ” , “ k ” , “ yep ” , and   “ huh ” ( LIWC category “ netspeak ” ) , especially near   the beginning of the conversation . One interpreta-   tion could be that the buyer signals a comfort with   silence and a lack of eagerness , effectively transfer-   ring the onus of driving the conversation onto the   seller . This opens up the opportunity for the seller   to consider the possibility of ceding ground on their   asking price . For both buyer and seller , playing the   ball into the other person ’s court appears effective .   Furthermore , LIWC category “ space ” is associ-   ated with seller success . This category consists   of seller - spoken words early in the negotiation   like “ small ” , “ location ” , “ big ” , and “ room ” , among   many others , which are used in reference to various   aspects of the house . Early discussion of this sub-13167   ject by the seller establishes a baseline justification   for the housing price . This appears to play in favor   of the seller as it clarifies the pretense that the ask-   ing price in question is substantive and sets the tone   for a negotiation based on concrete considerations .   One of the more salient LIWC features for buyer   is “ b4 - negemo ” , predictive of a seller win . Promi-   nent member words include “ unfortunately ” , “ prob - lem ” , “ sorry ” , “ lower ” , and “ risk ” , suggesting that   it is ineffective to frame the negotiation explicitly   as a conflict resolution rather than a positive co-   operative process . Given that the buyer requires   movement on the asking price to succeed , they   should avoid language that explicitly acknowledges   that the seller may be compromising their interests .   This result echoes the important role of negative ex-   pressions on negotiation outcomes by Barry ( 2008 ) .   Another notable observation is that buyer - only   bargaining acts are more predictive . To better make   sense of this observation , Table 3b shows important   features when predicting only with buyer bargain-   ing act unigrams and bigrams . Most notably , new   offers andpushes followed by comparisons consis-   tently appear as two of the most influential features   predictive of seller wins .   We present an example excerpt in Table 4 to   illustrate such sequences . In this case , the com-   parison is serving the role of justifying the buyer ’s   new offer of $ 218,000 . This scenario often oc-   curs the first time that a comparison is made by13168either party : It provides the seller the opportunity   to provide counter - evidence in favor of dismissing   or devaluing the buyer ’s offer . Tying an offer to a   rationalization based on extrinsic evidence is inef-   fective for the buyer since its relative importance   may be debated and new evidence may be intro-   duced to weaken the effect of the buyer ’s argument .   This conclusion complements the finding that , in   contrast to the buyer , the seller is advantaged when   discussing details of the property , as evidenced by   the LIWC feature “ s2 - space ” .   Further Evaluation . As an additional experi-   ment , we train a logistic regression model on the   C B dataset ( He et al . , 2018 )   and test it on our dataset . We include seller and   buyer text , and use the same text encoding proce-   dure described in § 5.1 . In the C B- dataset , the seller asking price is considered   to be the seller ’s private value for the item being   sold and the buyer ’s private value is separately spec-   ified . We consider the negotiation to be a seller   win if the agreed price is higher than the midpoint   between the two private values and a buyer win   otherwise . Despite C B having   a significantly larger training dataset , the maxi-   mum test accuracy across all 10 fractions of our   negotiations dataset is 54 % , whereas we achieve a   maximum of 60 % accuracy when we train and test   on our dataset . This experiment underscores the   distinctiveness of our dataset and suggests that it   may contain relevant linguistic differences to other   datasets within the bargaining domain .   6 Conclusion   In this work we design and conduct a controlled   experiment for studying the language of bargaining .   We collect and annotate a dataset of alternating of-   fersandnatural language negotiations . Our dataset   contains more turns per negotiation than existing   datasets and , since participants communicate orally ,   our setting facilitates a more natural communica-   tion environment . Our dataset is further enhanced   with annotated bargaining acts . Our statistical anal-   yses and prediction experiments confirm existing   findings and reveal new insights . Most notably ,   the ability to communicate using language facili-   tates cooperation and results in higher agreement   and faster convergence . Both sellers and buyers   benefit from maintaining a neutral role and provid-   ing the space for the other to formulate and justify   new offers . Buyers should further avoid 1 ) usinghouse comparisons as primary rationales for their   interests and 2 ) suggesting that by negotiating the   asking price down , they are introducing conflict   into the interaction .   Limitations   We note several important limitations of this work .   Perhaps most importantly , our dataset is " natural-   istic , " but not actually " natural " in the sense of   independently occurring in the world . Though the   interactions between our participants are real , the   task itself is ultimately artificially constructed . In a   real - world negotiation over something as valuable   and significant as a house , the negotiating parties   will be much more invested in the outcome than our   experimental participants , whose actions change   their outcome to the order of a few dollars . This   difference in turn could lead real - world negotiating   parties to speak differently and possibly employ   substantially different strategies than we observe .   Methodologically , our study has a few impor-   tant limitations . Firstly our analyses are based   entirely on language that has been automatically   transcribed ( with some manual checks ) , and while   this helps with expense and scale , these transcripts   could be missing important subtleties that influ-   ence the outcome . Koenecke et al . ( 2020 ) uncover   an important limitation of these systems , finding   significant racial disparities in the quality of ASR   transcriptions . The linguistic feature analysis we   perform should be treated as largely exploratory ,   and provides suggestive and correlational rather   than causal evidence for the relationship between   language in the interactions and negotiation out-   comes .   Lastly , there are further linguistic and interac-   tional phenomena at play that we have not yet   integrated into the analysis presented here . For   one , we have access to the audio channel of par-   ticipants ’ actual speech , but we have not analyzed   it in this work . There could very well be acoustic   cues in participants ’ speech that are as significant   to the interactions as the textual features analyzed   here , particularly speech prosody which has been   shown to communicate social meanings that could   be highly relevant to negotiation like friendliness   ( Jurafsky et al . , 2009 ) . This particularly extends   to more interactional questions of not simply who   said what , but what was said in response to what   and in what way . For instance , existing research   has shown that acoustic entrainment in dialog ( e.g. ,13169interlocutor adaptation to one another in terms of   prosody ) has important social associations with di-   alogue success ( Levitan et al . , 2012 ) . We leave a   deeper investigation of these phenomena for future   work .   Broader Impacts   This research , collectively with prior and future   related work , has the potential to advance our un-   derstanding of negotiation , a ubiquitous human   activity . Our dataset can enable future research   into the dynamics of human bargaining as well as   interpersonal interactions more broadly . By em-   ploying the findings and insights gained from such   research , individuals may be able to enhance their   ability to negotiate effectively in various settings ,   such as salary negotiations , personal relationships ,   and community initiatives . Meanwhile , we must   acknowledge that while a better understanding of   language as an instrument in social interaction can   be empowering , it may also be used as a tool for   manipulation .   Acknowledgements   We are grateful to Jessica Halten for helping us run   the experiment through the Yale SOM Behavioral   Lab . The experiment also would not have been   possible without the excellent study session coor-   dination by Sedzornam Bosson , Alexandra Jones ,   Emma Blue Kirby , Vivian Wang , Sherry Wu , and   Wen Long Yang . We thank Rajat Bhatnagar for   developing the web application used in the study .   The human subjects experiment in this research   was deemed exempt by the Yale University Insti-   tutional Review Board ( IRB # 2000029151 ) . We   thank Allison Macdonald and Sammy Mustafa for   their effort in the data annotation process . Their   work was an invaluable contribution to the success   of this research project . We thank all members of   the Chicago Human+AI Lab and LEAP Workshop   for feedback on early versions of this work . Fi-   nally , we thank all anonymous reviewers for their   insightful suggestions and comments .   References1317013171Appendix   A Negotiation Excerpts   B Controlled Experiment   Compensation details summary . Each subject received $ 10 for showing up and could earn additional   bonus money per round . Bonus earnings depended on bargaining outcomes to incentivize subjects to   engage in realistic negotiating behavior . Buyers could earn $ 1 in bonus for every $ 1,000 that the agreed   sale price was below the buyer ’s private value of $ 235,000 , up to a maximum of $ 10 in bonus money .   Sellers could earn $ 1 in bonus for every $ 1,000 that the agreed sale price was above the seller ’s private   value of $ 225,000 , up to a maximum of $ 10 . Given the private values of buyers and sellers , $ 10 of surplus   was available to split . No party earned bonus money in a round if an agreement was not reached .   C Logistic Regression Features13172D Hyperparameters1317313174E Recruitment and instruction material   Table 9 reports select demographic attributes of study subjects .   15131751317613177131781317913180131811318213183ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   7   /squareA2 . Did you discuss any potential risks of your work ?   7   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   3 , 4 , 5   /squareB1 . Did you cite the creators of artifacts you used ?   5   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   1   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   We provide a link to a webpage for requesting access to the artifacts we create . This form includes   information on the intended use .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   At time of publication , we are in the process of preparing the data for restricted release via form . We   will detail additional information as appropriate in the form for requesting access .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   1 , 2 , 7   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Left blank .   C / squareDid you run computational experiments ?   5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   We report the GPU used in section 5 . We cite the model we used for experimentation , additional   details may be found in the original paper introducing the LED model.13184 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   5 and Appendix D   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   3,4,5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   We do n’t use any existing package for signiﬁcant processing or evaluation . We do use hugginface   tokenizer and cite the model we use .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   3   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix E   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   3   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   3   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   3   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   313185