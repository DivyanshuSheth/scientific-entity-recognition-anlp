  Bishal Santra   IIT KharagpurSumegh Roychowdhury   IIT KharagpurAishik Mandal   IIT Kharagpur   Vasu Gurram   IIT KharagpurAtharva Naik   IIT KharagpurManish Gupta   Microsoft , IndiaPawan Goyal   IIT Kharagpurbsantraigi ✉ gmail.com , ♣ gmanish ✉ microsoft.com , ♦ pawang ✉ cse.iitkgp.ac.in   Abstract   Although many pretrained models exist for text   or images , there have been relatively fewer   attempts to train representations specifically   for dialog understanding . Prior works usually   relied on finetuned representations based on   generic text representation models like BERT   or GPT-2 . But such language modeling pre-   training objectives do not take the structural   information of conversational text into consid-   eration . Although generative dialog models can   learn structural features too , we argue that the   structure - unaware word - by - word generation is   not suitable for effective conversation model-   ing . We empirically demonstrate that such rep-   resentations do not perform consistently across   various dialog understanding tasks . Hence , we   propose a structure - aware Mutual Information   based loss - function DMI ( Discourse Mutual   Information ) for training dialog - representation   models , that additionally captures the inher-   ent uncertainty in response prediction . Exten-   sive evaluation on nine diverse dialog modeling   tasks shows that our proposed DMI - based mod-   els outperform strong baselines by significant   margins .   1 Introduction   Representation learning has transformed how we   can apply machine learning to solve real - world   problems . However , despite a vast body of research   on pretrained language representations , there have   been relatively fewer attempts to train representa-   tions specifically for dialog understanding . Prior   works mostly relied on finetuned representations   based on generic models like BERT ( Devlin et al . ,   2019 ) or GPT-2 ( Radford et al . , 2019 ) . In our exper-   iments , we demonstrate that such representations   do not perform uniformly across various dialog un-   derstanding tasks such as dialog - act classification ,   intent detection or dialog evaluation .   On the other hand , prior works on pretraining   large - scale dialog models focused mainly on open-   domain generation . These works evaluated theirmodels only on dialog generation ( Zhang et al . ,   2020 ; Roller et al . , 2021 ; Adiwardana et al . , 2020 )   or tasks related directly to the pretraining objective   ( Henderson et al . , 2020 ; Gao et al . , 2020 ) . Their ef-   fectiveness on other dialog understanding tasks like   act classification or intent detection remains unex-   plored . So we ask the following research question :   Can we learn enriched representations directly at   the pretraining phase that are specifically helpful   for dialog understanding ?   Existing language modeling ( causal or masked )   pretraining objectives unfortunately are not the best   to model dialogs for these reasons : ( 1 ) The model   is not directly trained to learn the content discourse   structure ( e.g. , context - response in dialogs ) . ( 2 )   Such models are trained to generate the response   word - by - word rather than predicting a larger unit .   ( 3 ) The inherent one - to - many nature of dialog gen-   eration implies that the encoding model should be   able to capture uncertainty in the response predic-   tion task , that such models ignore .   Hence , in this paper , we propose pretraining ob-   jectives for improved dialog modeling that turn   the discourse - level organizational structure of texts   from natural sources ( e.g. , documents , dialogs , or   monologues ) into a learnable objective . We call   this objective the Discourse Mutual Information   ( DMI ) . The key insight towards the design of our   pretraining objective is to capture representations   that can account for a meaningful conversation out   of a specific ordered sequences of utterances . We   hope that a discourse - level pretraining objective   with conversational data would guide the model   to learn complex context - level features . For exam-   ple , in Fig . 1 , we illustrate the differences between   standard language modeling ( causal or masked )   based pretraining objectives and a discourse - level   reasoning task .   The second research question that we ask is   whether discourse - level features learned using self-   supervised pretraining outperform word - level pre-1718   training objectives for downstream dialog under-   standing tasks . Experimentally , we show that rep-   resentations learned using the proposed objective   function are highly effective compared to both ex-   isting discriminative as well as generative dialog   models . In terms of various dialog understanding   tasks , our models achieve state - of - the - art perfor-   mances in several tasks ( absolute improvements up   to 8.5 % and 3.5 % in task accuracies in probing and   finetuning setups , resp . ) and perform consistently   well across a variety of dialog understanding tasks ,   whereas baseline models usually have a rather im-   balanced performance across tasks .   Overall , our main contributions are as follows .   •We propose DMI , a novel information-   theoretic objective function for pretraining di-   alog representation .   •We release pretrained dialog - representation   models in three different sizes ( small , medium   and base ) based on our proposed self-   supervised learning objectives .   •We extensively evaluate our DMI based rep-   resentations on multiple open - domain down-   stream tasks like intent detection , dialog - act   classification , response retrieval , dialog rea-   soning , and response - generation evaluation ,   and beat state - of - the - art across nine tasks in   both probe as well as finetune setups.2 Literature Review   2.1 Dialog System Pretraining   There have been quite a few efforts towards uti-   lizing existing representations or developing new   pretrained models for dialog systems . While BERT   ( Devlin et al . , 2019 ) , ELMo ( Peters et al . , 2018 ) ,   GPT-2 ( Radford et al . , 2019 ) and other general pur-   pose large - scale pretrained networks are not spe-   cific to dialogs , transfer learning from such models   could be reasonable . Basic language understanding   capability available through these representations   helps to get decent performance on many dialog-   understanding tasks ( Hosseini - Asl et al . , 2020 ) .   On the other hand , there have been various works   on pretraining dialog specific representations or   large - scale generation models . We summarize the   properties of various previously proposed dialog-   representation learning models in Table 1 . Di-   aloGPT ( Zhang et al . , 2020 ) , Meena ( Adiwardana   et al . , 2020 ) and Blenderbot ( Roller et al . , 2021 ) are   large - scale Transformer - based language models ,   which are trained to generate the gold - response ( as   per the dataset ) given a dialog context . ContextPre-   train ( Mehri et al . , 2019 ) , ConveRT ( Henderson   et al . , 2020 ) and ConvFiT ( Vuli ´ c et al . , 2021 ) are   trained on the response retrieval task using Multi-   Woz or Reddit conversations . DEB or Dialog Eval-   uation using BERT ( Sai et al . , 2020 ) is a model   based on extended pretraining of the BERT archi-   tecture using Reddit data . DialogRPT ( Gao et al . ,   2020 ) , on the other hand , is pretrained to predict   human - feedback ( e.g. , upvotes and downvotes ) on   comments to Reddit threads . This model is initial-1719   ized using the weights of DialoGPT model . Wu   et al . ( 2020 ) thoroughly investigate these existing   pretrained representations , both generic and dialog   specific , for understanding their effectiveness on   various goal - oriented dialog - understanding tasks .   2.2 Self - supervised Representation Learning   with InfoMax   Mutual Information maximization ( InfoMax ) is one   of the popular approaches for self - supervised learn-   ing , first used by Oord et al . ( 2018 ) and Belghazi   et al . ( 2018 ) . Oord et al . ( 2018 ) proposed InfoNCE   loss which is an estimator for lower bound to mu-   tual information ( MI ) between two continuous-   valued random variables . InfoNCE has also been   used for other NLP applications like training sen-   tence embeddings ( SIMCSE ( Gao et al . , 2021 ) ) ,   question answering ( QA - InfoMax ( Yeh and Chen ,   2019 ) ) , etc . Other estimators for mutual informa-   tion have also been proposed like MINE ( Mutual In-   formation Neural Estimator ) ( Belghazi et al . , 2018 )   and SMILE ( Song and Ermon , 2020 ) . In general ,   these estimators are also broadly studied in con-   trastive Learning ( CL ) literature for training both   self - supervised ( Mikolov et al . , 2013 ; Devlin et al . ,   2019 ; Liu et al . , 2019 ; Gao et al . , 2021 ; Hender-   son et al . , 2020 ; Vuli ´ c et al . , 2021 ) and supervised   models ( Schroff et al . , 2015 ; Gunel et al . , 2020 ) .   Some prior works in the dialog generation domain   have used the concept of mutual information to   design loss functions or scoring mechanisms to   improve specificity of the generated responses ( Li   et al . , 2016 ; Yoo et al . , 2020 ) . These works pre-   dominantly used MI either as a regularizer , along   with cross entropy loss , or as a scoring function for   ranking generated responses in a post - processing   step . In the next section , we derive our pretraining   loss function DMI for conversational texts from an   information - theoretic perspective.3 Discourse Mutual Information   We define Discourse Mutual Information ( DMI ) as   the mutual informationbetween two random vari-   ables representing two different segments within   the same discourse . This is a general concept that   can be applied to any form of discourse , no matter   the domain or type of signal . In this paper , we focus   on dialog type discourses and representation learn-   ing for conversational texts . We define two random   variables for the contexts ( C ) and responses ( R )   that jointly construct a valid conversation . Conver-   sations between humans represent samples from   the joint distribution PofCandR. We pose   the following learning problem , “ learn continuous   representations for the textual random variables   CandRsuch that the true mutual information   between CandRcan be closely estimated . ”   In the remainder of this section we show that ,   if the lower bound on MI estimated by some rep-   resentations of context and response is close to   the true value , the representation of the context   would be as predictive of the response as the natu-   ral language form itself . Existing generative train-   ing objectives as used in DialoGPT or Blenderbot   are extremely focused on predicting target response   only . Per - word cross - entropy loss , used for training   these models , fails to take into account the inherent   uncertainty in the context - to - response generation   function . Adapting context representations so as   to predict the target responses optimally , helps our   proposed DMI - based models learn better dialog   representations applicable to a versatile set of dia-   log understanding tasks.1720   Objective Function Formulation LetEand   Ebe the representationsforCandRbased on   some encoder . Using the data processing inequality   from Information theory ( Cover , 1999 ) , we have   I(C;R)≥I(E;E ) ( 1 )   This tells us that MI between any encoded version   ofCandRwill always be less or equal than the   true mutual information . The equality will hold if   EandEare both fully - invertible encoding pro-   cesses ( as opposed to representations which are   lossy or compressive , and inversion is thus not pos-   sible ) . However , neural networks generally embed   the data points in a low dimensional manifold by   learning robust features that can represent the data   points efficiently . Because of this , neural represen-   tations are usually not invertible . Now , the exact   computation of MI is not possible for continuous-   valued random variables . In recent years , various   variational lower bounds have been proposed for   estimating MI between continuous - valued random   variables . Including the MI estimator ( ˆI ) , the over-   all relation becomes   I(C;R)≥I(E;E)≥ˆI(E;E ) ( 2 )   This leads us to the proposed learning objective   DMI :   maxˆI(E;E ) ( 3 )   where ˆI(E;E)is a variational lower bound esti-   mate of I(E;E)(Equation 1 ) parametrized by θ   andϕdenotes the parameters of the encoder used   for encoding C(orR ) toE(orE).Loss function For training our models , we min-   imize a loss function depending on the estimator   being used .   min / bracketleftig   L(C , R ) = −ˆI ( E , E)/bracketrightig   We experimented with various MI estimators from   literature , namely , MINE ( Belghazi et al . , 2018 ) ,   InfoNCE ( Oord et al . , 2018 ) , JSD ( Hjelm et al . ,   2019 ) and SMILE ( Song and Ermon , 2020 ) . These   MI estimators generally compute samples of E   andEusing CandRdrawn from the joint dis-   tribution P. Based on our preliminary experi-   ments , we found that InfoNCE estimator produces   better representations . The InfoNCE MI - estimate   is computed as ,   I(C;R)≥logN−L ( 4 )   L=−1   2E / bracketleftigg   loge   /summationtexte / bracketrightigg   where Ndenotes the batch size , and f(c , r)is a   scoring function for the ⟨c , r⟩pair .   InfoNCE - S : The original InfoNCE formulation   only considers negative samples for one of the ran-   dom variables , but does not pose any constraint on   which of the variables should be considered for neg-   ative sampling . As identifying the true response ,   from a pool of negative samples , would require dif-   ferent reasoning than identifying the true context   out of a pool , we consider both these cases and   create a symmetric version of the InfoNCE loss   function . The final expression of this loss is given   in Equation 5 and we refer to it as InfoNCE - S. This   considerably improves the speed of training and   convergence , and also gives a boost to downstream   task performance.1721L=−1   2E / bracketleftigg   loge   /summationtexte / bracketrightigg   −1   2E / bracketleftigg   loge   /summationtexte / bracketrightigg   ( 5 )   For other loss functions , more detailed discussion   can be found in the Appendix .   Comparison with ConveRT ( Henderson et al . ,   2020 ) : There are a couple of differences between   ConveRT ’s contrastive loss and our DMI objective .   ConveRT models the problem as a response selec-   tion task and focuses on modeling cosine similarity   between the context and the response . On the other   hand , we propose a generic similarity computation   function f(c , r)in Eqn . 4 and 5 . Another differ-   ence is in encoding the input . ConveRT splits the   context into previous turns and current query , and   encodes them independently . Our model encodes   the entire context jointly and hence is capable of   better learning the correlations between previous   turns and current query .   4DMI vs. Language Modeling Objectives   In this work , we focus on utilizing DMI for pre-   training dialog representations incorporating strong   discourse - level features . But why should the DMI   objective learn better discourse - level features than   models trained on conversational data using MLM   or LM objectives ? We can find the answer by look-   ing at various LM - based objectives through the lens   of InfoMax , as shown by Kong et al . ( 2020 ) . They   connected various pretraining objectives for natural   language representations , including the ones used   for training Skipgram , BERT and XLNet , to the   InfoMax learning principle .   If we consider an input text Tand a masking   function Mthat returns a masked text ˜Tand the   masked word w , the MLM objective is equivalent   toL = −ˆI(E(˜T ) , e)where , Eis the   language encoder ( e.g. , a Transformer encoder )   andeis the embedding of the token w. Simi-   larly , in the case of auto - regressive LMs like GPT-   2 , the InfoMax objective equivalent to the loss is   L = −ˆI(E(T ) , e ) , where T   is the input sequence till t−1token and Tis the   ttoken .   Compared to these LM objectives , DMI focuses   on optimizing I(E , E ) , where candrare two   structural components from the discourse with des - ignated roles . This enables DMI to discover more   important features at the discourse level .   5 Experiments   5.1 Architecture   The exact encoder architecture and the pretraining   pipeline has been shown in Figure 2 . We use a   dual encoder architecture for encoding the contexts   and responses separately . We observe that sharing   parameters between the two encoders leads to a   more efficient learning process and faster conver-   gence . We use vanilla transformer - based encoders   ( Vaswani et al . , 2017 ) for encoding the natural lan-   guage inputs . The first tokens for both context and   response sequences are the special [ CLS ] tokens   whose contextual embeddings from the encoder   are used as the context or response representations .   The utterances in the context are delimited by an-   other special token [ EOU ] ( for end - of - utterance ) .   We construct the context using as many utterances   from the dialog history as possible up to a maxi-   mum of 300 subword tokens . We use the Word-   Piece tokenizer from BERT for tokenizing the input   texts , with a vocabulary size of 30,522 .   The scoring function f(c , r)in Eqs . 4 and 5   is implemented using a Bilinear dot product be-   tween the context and response representations :   f(c , r ) = eWewhere , Wis a square weight   matrix trained along with other parameters in the   model . This function can take any real value , pos-   itive or negative , thus allowing the ˆI(E;E )   function to take any positive real value . While   any complicated function with that range could be   chosen , we chose this as a simple formulation sat-   isfying the range constraint and left most of the   learning to the transformer and the projection ma-   trix W.   5.2 Model Variants   We train three different scales of the DMI model :   DMI_Small with 6 layers , DMI_Medium   with 8 layers , and DMI_Base with 12 layers .   All configurations use 12 attention heads and   768 - dimensional embeddings . DMI_Small   is initialized with “ google / bert_uncased_L-   6_H-768_A-12 " , DMI_Medium is initialized   with “ google / bert_uncased_L-8_H-768_A-12"1722and DMI_Base is initialized by weights from   “ RoBERTa - base ” pretrained checkpoint , and further   pretrained on the pretraining dataset ( see § 5.4 ) . All   of these models are trained using the InfoNCE - S   estimator , unless specified otherwise .   5.3 Hyper - parameter Settings   We use Adam optimizer with a linear learning rate   schedule for training both the models . Learning   rate is first linearly increased to a max value of 5×   10during the warm - up phase ( first 1000 steps ) .   Following this , in the remaining training period ,   learning rate is linearly decayed down back to zero .   Before training DMI_Base , we reset the parameters   of the 12th self - attention layer , and it is trained   again from scratch along with the weight matrix   Wusing our DMI objective . The embedding layer   and initial 11 self - attention layers of the RoBERTa-   base encoder are finetuned at a slower learning rate   ( 5×10 ) during our pretraining phase .   As the mutual information value obtained by   the InfoNCE loss is upper bounded by log(N),N   being the batch size , we try to keep the value of N   as large as possible . Both 8 and 12 - layer models are   trained on 4 - GPU ( 4x32 GB V100s ) systems with   overall batch size of 480 and 384 , respectively .   All the trained models will be publicly shared upon   publication .   5.4 Pretraining Dataset   We pretrained all our models using the Reddit cor-   pus ( Reddit-727 M conversational - data ) released   by Henderson et al . , 2019 . We ran the scripts re-   leased by the authors to recreate the dataset of   727 M English conversations . Out of these 727 M   conversations , we utilize around 7.5 % to 10 % of   the dataset to train our models , after which the val-   idation loss generally saturates . In the rest of this   paper , we will refer to this dataset as rMax , in   short .   Dialog Unrolling for Pretraining For training   our models , we need samples of context - response   ( CR ) pairs . Each dialog is unrolled to create   context - response pairs with each utterance in the   dialog as a response , except the first one . Hence ,   for each dialog D={U , U , . . . , U } , we gen-   erate the following set of samples S={(C : U , . . . , U;R : U ) : t∈[2 , T ] } . If we process   the full rMax dataset , this leads to , approximately ,   2.7B CR pairs .   5.5 MI Estimation   During pretraining , we compare the checkpoints   from different epochs and across hyperparame-   ter settings in terms of the bits of mutual infor-   mation extracted by the trained representation on   an unseen set of dialogs . This is calculated as   MI= log ( N)−L(see § 3 for more details ) .   As per the Information Bottleneck theory ( Tishby   et al . , 2000 ) , the mutual information learned be-   tween the two observed random variables can be   factorized into two components , namely , predictive   and redundant information . Predictive information   generally identifies whether the features learned by   the representation are useful for a downstream task .   The redundant information is caused by features   that do not help in any downstream tasks . Such   features can exist due to noise or spurious corre-   lation in the dataset , or even overfitting . Hence ,   we train our final models on a fraction of the rMax   dataset but only for one epoch ( i.e. , we never re-   peat the samples ) which removes any possibility of   overfitting .   Predictive features identified based on a fixed set   of downstream tasks ( Tishby et al . , 2000 ; Alemi   et al . , 2017 ) may not be a sufficient to assess other   features learned in the training process . Since , ide-   ally , we want to maximize the amount of predictive   information in the representation , we compare the   bits of MI on the training set against the bits of   MI on an unseen validation set , as captured by the   learned representation . To make sure that we do not   assume anything about the domain or the conver-   sation topics , we use the validation set of dialogs   from the open - domain Daily Dialog dataset ( Li   et al . , 2017 ) .   5.6 Downstream Tasks   Instead of focusing on a single downstream task   like many previous works on dialog representation   learning , we consider a more versatile range of   tasks to evaluate the learned representations from   DMI or the baseline models . To find out whether a   certain representation is effective for some down-   stream task , we evaluate in two setups : probe and   finetune . In both cases , the pretrained model is   used along with an MLP classifier of fixed com-   plexity ( Pimentel et al . , 2020 ) . In probing setup ,   we only train the parameters of the MLP classifier.1723   In finetuning setup , we also train the pretrained   model parameters along with the MLP classifier   parameters . We use the context and response repre-   sentations from our models as the input to the MLP   classifier .   For downstream tasks , we have two reasoning   tasks based on the MuTual dataset ( Cui et al . , 2020 ) ,   three classification tasks based on conversational   intent detection ( Casanueva et al . , 2020 ) , emotion   detection ( Welivita and Pu , 2020 ) and act classifi-   cation ( Stolcke et al . , 1998 ) , and four dialog eval-   uation tasks based on the DailyDialog++ dataset   ( DD++ , Sai et al . , 2020 ) . Table 2 shows dataset de-   tails and metrics for these nine tasks . Both MuTual   and DailyDialog++ datasets have an adversarial   configuration for the respective tasks , which allows   us to assess each of the evaluated models in adver-   sarial settings also .   6 Results and Discussions   6.1 Pretraining DMI based Representations   During pretraining , we used “ Validation MI ” to   evaluate model checkpoints . As the goal of our   models is to learn a representation that captures   maximum MI between the context and the response   texts , this metric tracks how well the learned repre-   sentation captures the mutual information between   contexts and responses of unseen dialogs .   We use the validation split from Daily Dialog   dataset as our validation set for evaluation the   model during pretraining . It is not specific to a   domain and , hence , covers a versatile range of top-   ics . This set comprises 1,000 full conversations   between two persons which on unrolling leads to   7,069 context - response ( CR ) pairs . We illustrate   the variation in validation - MI metric against train-   ing steps in Fig . 3 in the Appendix.6.2 Comparison of Representations on   Downstream Task Performance   In this set of experiments , we probe / finetune the   DMI models with various downstream tasks that re-   quire knowledge of many different types of dialog-   understanding features . The results of our probing   and finetuning experiments are shown in Table 3 .   We have used two types of models as our   baselines : generic pretrained models and dialog-   specific pretrained models . RoBERTa , BERT ,   T5 ( Raffel et al . , 2019 ) , GPT-2 are all trained on   large corpora of generic web - crawled English text .   But , since these models were not specifically pre-   trained on any dialog corpus , they may suffer from   poor performance on certain dialog understanding   tasks . Hence , we consider DialoGPT , DialogRPT ,   DEB and ConveRT models , which were trained   on conversational data . For DialogRPT , we used   “ human - vs - rand ” checkpoint released by authors .   All models are 12 - layer except Blender_Small ( 8   layers ) , ConveRT ( 6 layers ) , DialogRPT ( 24 lay-   ers ) and DMI_Medium ( 8 layers ) . We used the   publicly available model checkpoints for all base-   lines , wherever possible . The ConveRT model ’s   checkpoint has been removed from Githubby its   authors . Hence , it was only possible for us to MLP-   probe the representations , without finetuning of the   model , based on a cached version released by an-   other user under a valid license . Pretrained check-   points for Meena , ContextPretrain and ConvFiT are   not available , and hence we do not compare with   them .   6.2.1 Results in Probing Setup   We observe that , on average , DEB and ConveRT   have good performance among the baselines . How-1724   ever , the RoBERTa model outperforms all other   baselines on the MuTual task by a significant mar-   gin . In the MuTual Plus task , the DEB model out-   performs other models in the R@1 and MRR met-   rics . ConveRT performs the best among all base-   lines on the other tasks . ConveRT ’s loss function   is also contrastive in nature and is similar to ours .   This explains the model ’s generally high strength   across the tasks among all the baselines .   Our DMI_Base beats ConveRT on all the tasks ,   and DMI_Medium beats the baseline on 7 out of   9 tasks . We believe DD++ tasks to be the most   demanding ones with respect to context - level un-   derstanding . Here , all non - dialog baselines have   a weaker performance , with DEB and ConveRT   being the best of the bunch . These are also the   tasks where our models excel the most , with both   DMI_Medium and DMI_Base beating all baselineswith strong margins . DD++/cross is the most diffi-   cult among all four DD++ tasks . Here , the model   is trained on random negative samples and tested   on a dataset with human - curated adversarial neg-   atives . Our DMI_Base beats the best baseline on   DD++/cross by 8.54 points . This shows the su-   perior quality of context representations from our   models .   6.2.2 Results in Finetuning Setup   In the finetuning setup , on average , RoBERTa and   DialogRPT have good performance among the   baselines . DialogRPT performs well for DD++   tasks while Blender works well for the MuTual   task . For all other tasks , RoBERTa is the best base-   line , even outperforming models especially trained   for dialog tasks ( like DialoGPT ) .   Similar to the probe setup , DMI_Base beats base-1725line methods by significant margins . In general ,   finetune results are better than probe results across   all models , as expected .   Our large - scale RoBERTa - initialized DMI_Base   model outperforms the best baseline for all tasks ,   by a considerable margin . Additionally , our DMI-   based models are able to perform well uniformly   across all tasks , unlike even baselines like Di-   aloGPT , DialogRPT and Blenderbot models which   are explicitly trained on dialog data . This makes   DMI the best overall model for dialog related tasks .   Across multiple tasks , we show qualitative exam-   ples where our proposed DMI - based models pro-   vide accurate results , in the Appendix .   6.3 Ablations   We evaluate the importance of using RoBERTa   based pretraining as well as the symmetric ver-   sion of the InfoNCE loss in Table 4 . We observe   that RoBERTa based pretraining helps significantly   across all tasks . The symmetric InfoNCE improves   performance for SWDA and MuTual Plus tasks .   7 Conclusions and Future work   In this paper , we proposed the concept of Dis-   course Mutual Information ( DMI ) which is bet-   ter suited for learning dialog - specific features in   a self - supervised manner . Using the InfoMax   principle we formulated a pretraining method for   dialog - specific representation learning . Across 9   downstream dialog understanding tasks , our 12-   layer model outperforms state - of - the - art methods .   Further , we showed that on most of these tasks ,   even our 8 - layer model outperforms standard 12-   layer pretrained models . These experiments show   the potential of the proposed DMI objective to-   wards building dialog understanding models . We   will make the code and pretrained model check-   points available on request , instructions can be   found here https://bsantraigi.github .   io / DMI . Although we experimented only with   dialog modeling in this paper , we believe that the   proposed DMI objective is generic enough to be   applied to any type of discourse in any domain . In   the future , we would like to explore how to harness   DMI representations for generative conversation   modeling .   8 Acknowledgments   This work was partially supported by Microsoft   Academic Partnership Grant ( MAPG ) 2021 . Thefirst author was also supported by Prime Minister ’s   Research Fellowship ( PMRF ) , India .   9 Ethical considerations   Like many other pretrained language representa-   tion models , the proposed model may also have   learned patterns associated with exposure bias . In-   terpretability associated with the output is rather   limited , hence users should use the outputs care-   fully . The proposed model ranks possible response   candidates , and does not filter out any “ problem-   atic ” candidates . Thus , for applications , where   candidate responses could be problematic , ( e.g. ,   offensive , hateful , abusive , etc . ) , users should care-   fully filter them out before providing them as input   to our model .   All the datasets used in this work are publicly   available . We did not collect any new dataset as   part of this work .   Banking77 Casanueva et al . , 2020   has been obtained from https :   //github.com / PolyAI - LDN/   task - specific - datasets . It is avail-   able under a Creative Commons Attribution 4.0   International license with details here .   SWDA Stolcke et al . , 1998 : The dataset   has been obtained from http://compprag .   christopherpotts.net/swda.html .   This work is licensed under a Creative Commons   Attribution - NonCommercial - ShareAlike 3.0   Unported License .   E - Intent Welivita and Pu , 2020 : The dataset was   downloaded from https://github.com/   anuradha1992 / EmpatheticIntents .   The original dataset is available at https :   //github.com / facebookresearch/   EmpatheticDialogues which is under the   Creative Commons Attribution 4.0 International   license .   MuTual and MuTual - plus Cui et al . , 2020 : The   datasets have been downloaded from https://   github.com/Nealcly/MuTual . Licensing   is unclear ; the authors do not mention any license   information or terms of use .   DailyDialog++ Sai et al . , 2020 : The dataset   was downloaded from https://github.com/   iitmnlp / DailyDialog - plusplus . The   data is available under the MIT License.1726rMax or Reddit-727 M conversational-   data Henderson et al . , 2019 : the   dataset has been obtained from https :   //github.com / PolyAI - LDN/   conversational - datasets / tree/   master / reddit . The dataset is available under   the Apache License Version 2.0 .   References17271728   A Mutual Information Estimators   In this paper , we experiment with various different   MI estimators , and found InfoNCE - S to be the best   ( both in terms of accuracy as well as training speed ) .   The mathematical formulation of these estimators   is provided below .   1.InfoNCE was proposed by Oord et al . ( 2018 ) .   It connects to the mutual information value   I(X;Y)as ,   I(X;Y)≥log(N)−L   L=−E / bracketleftigg   loge   /summationtexte / bracketrightigg   2 . MINE ( Belghazi et al . , 2018 )   I(X;Y)≥sup / bracketleftbig   I(X;Y ) =   E[T(x , y)]−logE[e]/bracketrightbig   3 . JSD ( Hjelm et al . , 2019 )   I(X;Y ) = E[−sp(−T(x , y ) ) ]   −E[sp(T(x , y ) ) ]   4 . SMILE ( Song and Ermon , 2020 )   I(X;Y ) = E[T(x , y ) ]   −logE[clip(e , e , e ) ]   Use of the InfoMax objective for self - supervised   learning has been more prevalent in the computer   vision domain than in NLP . Although as Kong et al.(2020 ) have previously shown , many existing loss   functions used for training NLP models can be de-   rived directly from the InfoMax framework . Kong   et al . ( 2020 ) had only focused on various language   model objectives that focus on words given the sur-   rounding context . The authors showed that this   objective translates to maximizing mutual infor-   mation between the context and the missing word   within the context .   In dialog domain also , InfoMax - equivalent loss   functions have been used . First , Henderson et al .   ( 2020 ) used contrastive formulation of the response   selection task as a pretraining objective for dialog   representation . Other prior works on response se-   lection models often used a binary - cross entropy   loss for training . Both these loss functions are actu-   ally equivalent to various lower bound estimators   for mutual information . In the QAInfoMax model   ( Yeh and Chen , 2019 ) , the authors used the Deep-   InfoMax loss function ( Hjelm et al . , 2019 ) as a   regularizer and showed that representations learned   with or in - presence of an InfoMax regularizer are   more resilient to adversarial attacks while maintain-   ing the same level of task performance . We also   observe the same effect in our DD++/cross exper-   iments . This is because of the self - supervised yet   task - specific nature of the loss function .   B Response Retrieval Experiment   We wanted to investigate if the proposed model   can rank good responses higher compared to more   generic / bland ones . Hence to test against an ex-   treme setting we simulate a response selection task   for a very large pool using the test set of Daily Dia-   log ( Li et al . , 2017 ) dataset . We took all the ∼7000   responses from test set of the daily dialog dataset   as the response pool . Next , for a few randomly   selected context examples , we illustrate the top two   ranked as well as ground truth responses for two   full conversations in Tables 5 and 6 . Of course , the1729ground truth response was removed from the pool   for each context . The ranking of responses were   done using the f(c , r)function from the trained   DMI_Base model . From the examples of response   selection , we can observe that the model is able to   both avoid blend responses and select responses   that are relevant to the current context even from   such a large pool . This shows the usefulness of dia-   log specific pretrained representation trained using   the DMI objective .   CPrediction Samples and Error Analysis   In Table 7 , we show sample predictions from the   DailyDialog++/cross task ( Sai et al . , 2020 ) . As   DialoGPT has the best performance in the probe   setup , on this task among the baselines , we choose   it for error analysis . We randomly sampled 11 in-   stances where the DialoGPT model made a mistake   and observed the behavior of our DMI_Base model   on these samples . We see that our model correctly   predicts for all 6 out of 6 negative samples and out   of the 5 positive samples DMI_Base predicts the   label of 2 samples correctly ( overall 8/11 correct   predictions by our model ) . This shows that our   model has a better understanding of the context   and response inputs , which makes it robust against   the adversarial negative samples . As can be seen   in samples 2 , 3 , 5 and 6 , the incorrect predictions   by the DialoGPT model might have been caused   by presence of common or similar meaning tokens   ( cook , food ; million ; long ; employee ) between con-   text and response . This means that DialoGPT often   relies on weak token - based cues for prediction .   For error analysis on the Empathetic - Intent ( E-   Intent ) task ( Welivita and Pu , 2020 ) , we chose the   ConveRT model as the baseline to compare against   predictions from our DMI_Base model . First , we   randomly select 10 samples from the test set of the   E - Intent task where the baseline ConveRT model   makes a mistake . Then the predictions from the   DMI_Base model are observed on these 10 sam-   ples . The input utterances , true labels and the pre-   dictions made by the model are shown in Table 8 .   Out of these 10 samples , DMI_Base is able to pre-   dict the labels for 6 instances correctly . We notice   that though sample inputs often contain more than   one emotion , the one denoted by the gold label is   generally the primary one . Our model is able to   capture this emotion correctly more often than the   baseline , with such mixed - emotion samples .   Fig . 4 shows the confusion matrix for ourDMI_Base model for the Empathetic - Intent task .   The accuracy is highest for afraid , acknowledging   and questioning classes ( each above 95 % ) . Some   of the most confusing pairs of classes are ( annoyed ,   wishing ) , ( anxious , apprehensive ) , ( caring , confi-   dent ) , ( content , grateful ) , ( content , lonely).17301731173217331734