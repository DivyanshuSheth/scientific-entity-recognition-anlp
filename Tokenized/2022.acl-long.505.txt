  Ryokan RiIkuya YamadaYoshimasa Tsuruoka   Abstract   Recent studies have shown that multilingual   pretrained language models can be effectively   improved with cross - lingual alignment infor-   mation from Wikipedia entities . However , ex-   isting methods only exploit entity information   in pretraining and do not explicitly use enti-   ties in downstream tasks . In this study , we   explore the effectiveness of leveraging entity   representations for downstream cross - lingual   tasks . We train a multilingual language model   with 24 languages with entity representations   and show the model consistently outperforms   word - based pretrained models in various cross-   lingual transfer tasks . We also analyze the   model and the key insight is that incorporat-   ing entity representations into the input allows   us to extract more language - agnostic features .   We also evaluate the model with a multilingual   cloze prompt task with the mLAMA dataset .   We show that entity - based prompt elicits cor-   rect factual knowledge more likely than using   only word representations . Our source code   and pretrained models are available at https :   //github.com / studio - ousia / luke .   1 Introduction   Pretrained language models have become crucial   for achieving state - of - the - art performance in mod-   ern natural language processing . In particular , mul-   tilingual language models ( Conneau and Lample ,   2019 ; Conneau et al . , 2020a ; Doddapaneni et al . ,   2021 ) have attracted considerable attention particu-   larly due to their utility in cross - lingual transfer .   In zero - shot cross - lingual transfer , a pretrained   encoder is ﬁne - tuned in a single resource - rich lan-   guage ( typically English ) , and then evaluated on   other languages never seen during ﬁne - tuning . A   key to solving cross - lingual transfer tasks is to ob-   tain representations that generalize well across lan-   guages . Several studies aim to improve multilin-   gual models with cross - lingual supervision such asbilingual word dictionaries ( Conneau et al . , 2020b )   or parallel sentences ( Conneau and Lample , 2019 ) .   Another source of such information is the cross-   lingual mappings of Wikipedia entities ( articles ) .   Wikipedia entities are aligned across languages via   inter - language links and the text contains numer-   ous entity annotations ( hyperlinks ) . With these   data , models can learn cross - lingual correspon-   dence such as the words Tokyo ( English ) and    3(Japanese ) refers to the same entity . Wikipedia   entity annotations have been shown to provide rich   cross - lingual alignment information to improve   multilingual language models ( Calixto et al . , 2021 ;   Jiang et al . , 2022 ) . However , previous studies only   incorporate entity information through an auxiliary   loss function during pretraining , and the models do   not explicitly have entity representations used for   downstream tasks .   In this study , we investigate the effectiveness   of entity representations in multilingual language   models . Entity representations are known to en-   hance language models in mono - lingual settings   ( Zhang et al . , 2019 ; Peters et al . , 2019 ; Wang et al . ,   2021 ; Xiong et al . , 2020 ; Yamada et al . , 2020 )   presumably by introducing real - world knowledge .   We show that using entity representations facili-   tates cross - lingual transfer by providing language-   independent features . To this end , we present a   multilingual extension of LUKE ( Yamada et al . ,   2020 ) . The model is trained with the multilingual   masked language modeling ( MLM ) task as well   as the masked entity prediction ( MEP ) task with   Wikipedia entity embeddings .   We investigate two ways of using the entity rep-   resentations in cross - lingual transfer tasks : ( 1 ) per-   form entity linking for the input text , and append   the detected entity tokens to the input sequence .   The entity tokens are expected to provide language-   independent features to the model . We evaluate   this approach with cross - lingual question answer-   ing ( QA ) datasets : XQuAD ( Artetxe et al . , 2020)and MLQA ( Lewis et al . , 2020 ) ; ( 2 ) use the entity   [ MASK ] token from the MEP task as a language-   independent feature extractor . In the MEP task ,   word tokens in a mention span are associated with   an entity [ MASK ] token , the contextualized rep-   resentation of which is used to train the model to   predict its original identity . Here , we apply similar   input formulations to tasks involving mention - span   classiﬁcation , relation extraction ( RE ) and named   entity recognition ( NER ): the attribute of a mention   or a pair of mentions is predicted using their con-   textualized entity [ MASK ] feature . We evaluate   this approach with the RELX ( Köksal and Özgür ,   2020 ) and CoNLL NER ( Tjong Kim Sang , 2002 ;   Tjong Kim Sang and De Meulder , 2003 ) datasets .   The experimental results show that these entity-   based approaches consistently outperform word-   based baselines . Our analysis reveals that entity   representations provide more language - agnostic   features to solve the downstream tasks .   We also explore solving a multilingual zero - shot   cloze prompt task ( Liu et al . , 2021 ) with the entity   [ MASK ] token . Recent studies have shown that we   can address various downstream tasks by querying   a language model for blanks in prompts ( Petroni   et al . , 2019 ; Cui et al . , 2021 ) . Typically , the answer   tokens are predicted from the model ’s word - piece   vocabulary but here we incorporate the prediction   from the entity vocabulary queried by the entity   [ MASK ] token . We evaluate our approach with the   mLAMA dataset ( Kassner et al . , 2021 ) in various   languages and show that using the entity [ MASK ]   token reduces language bias and elicits correct fac-   tual knowledge more likely than using only the   word [ MASK ] token .   2 Multilingual Language Models with   Entity Representations   2.1 Model : mulitlingual LUKE   To evaluate the effectiveness of entity representa-   tions for cross - lingual downstream tasks , we in-   troduce a new multilingual language model based   on a bidirectional transformer encoder : Multilin-   gual LUKE ( mLUKE ) , a multilingual extension of   LUKE ( Yamada et al . , 2020 ) . The model is trained   with the masked language modeling ( MLM ) task   ( Vaswani et al . , 2017 ) as well as the masked entity   prediction ( MEP ) task . In MEP , some of the input   entity tokens are randomly masked with the spe-   cial entity [ MASK ] token , and the model is trained   to predict the original entities . Note that the entity[MASK ] token is different from the word [ MASK ]   token for MLM .   The model takes as input a tokenized text   ( w;w;:::;w ) and the entities appearing in the   text ( e;e;:::;e ) , and compute the contextualized   representation for each token ( h;h;:::;h   andh;h;:::;h ) . The word and entity tokens   equally undergo self - attention computation ( i.e. , no   entity - aware self - attention in Yamada et al . ( 2020 ) )   after embedding layers .   The word and entity embeddings are computed   as the summation of the following three embed-   dings : token embeddings , type embeddings , and   position embeddings ( Devlin et al . , 2019 ) . The   entity tokens are associated with the word tokens   through position embeddings : the position of an   entity token is deﬁned as the positions of its cor-   responding word tokens , and the entity position   embeddings are summed over the positions .   Model Conﬁguration . The model conﬁgurations   of mLUKE follow the base andlarge conﬁgura-   tions of XLM - RoBERTa ( Conneau et al . , 2020a ) , a   variant of BERT ( Devlin et al . , 2019 ) trained with   CommonCrawl data from 100 languages . Before   pretraining , the parameters in common ( e.g. , the   weights of the transformer encoder and the word   embeddings ) are initialized using the checkpoint   from the Transformers library .   The size of the entity embeddings is set to 256   and they are projected to the size of the word em-   beddings before being fed into the encoder .   2.2 Training Corpus : Wikipedia   We use Wikipedia dumps in 24 languages ( Ap-   pendix A ) as the training data . These languages   are selected to cover reasonable numbers of lan-   guages that appear in downstream cross - lingual   datasets . We generate input sequences by splitting   the content of each page into sequences of sen-   tences comprising 512 words with their entity   annotations ( i.e. , hyperlinks ) . During training , data   are sampled from each language with nitems with   the following multinomial distribution :   p = nn ; ( 1 )   where  is a smoothing parameter and set to 0.7   following multilingual BERT .   Entity Vocabulary . Entities used in mLUKE are   deﬁned as Wikipedia articles . The articles from dif-   ferent languages are aligned through inter - language   linksand the aligned articles are treated as a sin-   gle entity . We include in the vocabulary the most   frequent 1.2 M entities in terms of the number of hy-   perlinks that appear across at least three languages   to facilitate cross - lingual learning .   Optimization . We optimize the models with   a batch size of 2048 for 1 M steps in total us-   ing AdamW ( Loshchilov and Hutter , 2019 ) with   warmup and linear decay of the learning rate . To   stabilize training , we perform pretraining in two   stages : ( 1 ) in the ﬁrst 500 K steps , we update only   those parameters that are randomly initialized ( e.g. ,   entity embeddings ) ; ( 2 ) we update all parameters   in the remaining 500 K steps . The learning rate   scheduler is reset at each training stage . For further   details on hyperparameters , see Appendix A.   2.3 Baseline Models   We compare the primary model that we investi-   gate , multilingual LUKE used with entity repre-   sentations ( mLUKE - E ) , against several baselines   pretrained models and an ablation model based on   word representations :   mBERT ( Devlin et al . , 2019 ) is one of the earliest   multilingual language models . We provide these   results as a reference .   XLM - R ( Conneau et al . , 2020a ) is the model that   mLUKE is built on . This result indicates how our   additional pretraining step and entity representa - tion impact the performance . Since earlier studies   ( Liu et al . , 2019 ; Lan et al . , 2020 ) indicated longer   pretraining would simply improve performance ,   we train another model based on XLM - Rwith   extra MLM pretraining following the same conﬁg-   uration of mLUKE .   mLUKE - W is an ablation model of mLUKE - E.   This model discards the entity embeddings learned   during pretraining and only takes word tokens as   input as with the other baseline models . The results   from this model indicate the effect of MEP only   as an auxiliary task in pretraining , and the com-   parison with this model will highlight the effect of   using entity representations for downstream tasks   in mLUKE - E.   The above models are ﬁne - tuned with the same   hyperparameter search space and computational   budget as described in Appendix B.   We also present the results of XLM - K ( Jiang   et al . , 2022 ) for ease of reference . XLM - K is based   on XLM - Rand trained with entity information   from Wikipedia but does not use entity representa-   tions in downstream tasks . Notice that their results   are not strictly comparable to ours , because the   pretraining and ﬁne - tuning settings are different .   3 Adding Entities as Language - Agnostic   Features in QA   We evaluate the approach of adding entity embed-   dings to the input of mLUKE - E with cross - lingual   extractive QA tasks . The task is , given a question   and a context passage , to extract the answer span   from the context . The entity embeddings provide   language - agnostic features and thus should facili-   tate cross - lingual transfer learning .   3.1 Main Experiments   Datasets . We ﬁne - tune the pretrained models with   the SQuAD 1.1 dataset ( Rajpurkar et al . , 2016 ) , and   evaluate them with the two multilingual datasets :   XQuAD ( Artetxe et al . , 2020 ) and MLQA ( Lewis   et al . , 2020 ) . XQuAD is created by translating a   subset of the SQuAD development set while the   source of MLQA is natural text in Wikipedia . Be-   sides multiple monolingual evaluation data splits ,   MLQA also offers data to evaluate generalized   cross - lingual transfer ( G - XLT ) , where the question   and context texts are in different languages .   Models . All QA models used in this experiment   follow Devlin et al . ( 2019 ) . The model takes the   question and context word tokens as input and pre-   dicts a score for each span of the context word to-   kens . The span with the highest score is predicted   as the answer to the question .   mLUKE - E takes entity tokens as additional fea-   tures in the input ( Figure 1 ) to enrich word repre-   sentations . The entities are automatically detected   using a heuristic string matching based on the orig-   inal Wikipedia article from which the dataset in-   stance is created . See Appendix C for more details .   Results . Table 1 summarizes the model ’s F1 scores   for each language . First , we discuss the base mod-   els . On the effectiveness of entity representations ,   mLUKE - Eperforms better than its word - based   counterpart mLUKE - W(0.6 average points im-   provement in the XQuAD average score , 0.1 points   in MLQA ) and XLM - K ( 0.2 points improvementin MLQA ) , which indicates the input entity tokens   provide useful features to facilitate cross - lingual   transfer . The usefulness of entities is demonstrated   especially in the MLQA ’s G - XLT setting ( full re-   sults available in Appendix F ) ; mLUKE - Eex-   hibits a substantial 1.6 point improvement in the   G - XLT average score over mLUKE - W. This   suggests that entity representations are beneﬁcial   in a challenging situation where the model needs   to capture language - agnostic semantics from text   segments in different languages .   We also observe that XLM - Rbeneﬁts from   extra training ( 0.4 points improvement in the av-   erage score on XQuAD and 2.1 points in MLQA ) .   The mLUKE - Wmodel further improves the   average score from XLM - Rwith extra training   ( 1.2 points improvement in XQuAD and 2.1 points   in MLQA ) , showing the effectiveness of the MEP   task for cross - lingual QA .   By comparing large models , we still observe   substantial improvements from XLM - R to the   mLUKE models . Also we can see that mLUKE-   E overall provides better results than mLUKE-   W ( 0.4 and 0.3 points improvements in the   MLQA average and G - XLT scores ; comparable   scores in XQuAD ) , conﬁrming the effectiveness of   entity representations .   3.2 Analysis   How do the entity representations help the model   in cross - lingual transfer ? In the mLUKE - E model , the input entity tokens annotate mention spans on   which the model performs prediction . We hypothe-   size that this allows the encoder to inject language-   agnostic entity knowledge into span representa-   tions , which help better align representations across   languages . To support this hypothesis , we compare   the degree of alignment between span representa-   tions before and after adding entity embeddings in   the input , i.e. , mLUKE - W and mLUKE - E.   Task . We quantify the degree of alignment as   performance on the contextualized word retrieval   ( CWR ) task ( Cao et al . , 2020 ) . The task is , given   a word within a sentence in the query language , to   ﬁnd the word with the same meaning in the context   from a candidate pool in the target language .   Dataset . We use the MLQA dev set ( Lewis et al . ,   2020 ) . As MLQA is constructed from parallel sen-   tences mined from Wikipedia , some sentences and   answer spans are aligned and thus the dataset can   be easily adapted for the CWR task . As the query   and target word , we use the answer spananno-   tated in the dataset , which is also parallel across   the languages . We use the English dataset as the   query language and other languages as the target .   We discard query instances that do not have their   parallel data in the target language . The candidate   pool is all answer spans in the target language data .   Models . We evaluate the mLUKE - W and   mLUKE - Emodels without ﬁne - tuning . The   retrieval is performed by ranking the cosine simi-   larity of contextualized span representations , which   is computed by mean - pooling the output word vec-   tors in the span .   Results . Table 2 shows the retrieval performance   in terms of the mean reciprocal rank score . We   observe that the scores of mLUKE - Eare higher   than mLUKE - Wacross all the languages . This   demonstrates that adding entities improves the de-   gree of alignment of span representations , which   may explain the improvement of mLUKE - E in the   cross - lingual QA task.4 The Entity MASK Token as Feature   Extractor in RE and NER   In this section , we evaluate the approach of using   the entity [ MASK ] token to extract features from   mLUKE - E for two entity - related tasks : relation   extraction and named entity recognition .   We formulate both tasks as the classiﬁcation of   mention spans . The baseline models extract the   feature of spans as the contextualized representa-   tions of word tokens , while mLUKE - E extracts the   feature as the contextualized representations of the   special language - independent entity tokens associ-   ated with the mentions ( Figure 1 ) . We demonstrate   that this approach consistently improves the perfor-   mance in cross - lingual transfer .   4.1 Relation Extraction   Relation Extraction ( RE ) is a task to determine the   correct relation between the two ( head and tail ) enti-   ties in a sentence . Adding entity type features have   been shown to be effective to cross - lingual transfer   in RE ( Subburathinam et al . , 2019 ; Ahmad et al . ,   2021 ) , but here we investigate an approach that   does not require predeﬁned entity types but utilize   special entity embeddings learned in pretraining .   Datasets . We ﬁne - tune the models with the En-   glish KBP-37 dataset ( Zhang and Wang , 2015 ) and   evaluate the models with the RELX dataset ( Köksal   and Özgür , 2020 ) , which is created by translating   a subset of 502 sentences from KBP-37 ’s test set   into four different languages . Following Köksal   and Özgür ( 2020 ) , we report the macro average of   F1 scores of the 18 relations .   Models . In the input text , the head and tail enti-   ties are surrounded with special markers ( < ent > ,   < ent2 > ) . The baseline models extract the feature   vectors for the entities as the contextualized vector   of the ﬁrst marker followed by their mentions . The   two entity features are concatenated and fed into a   linear classiﬁer to predict their relation .   For mLUKE - E , we introduce two special enti-   ties,[HEAD ] and[TAIL ] , to represent the head   and tail entities ( Yamada et al . , 2020 ) . Their em-   beddings are initialized with the entity [ MASK ]   embedding . They are added to the input sequence   being associated with the entity mentions in the   input , and their contextualized representations are   extracted as the feature vectors . As with the word-   based models , the features are concatenated and   input to a linear classiﬁer .   4.2 Named Entity Recognition   Named Entity Recognition ( NER ) is the task to   detect entities in a sentence and classify their type .   We use the CoNLL-2003 English dataset ( Tjong   Kim Sang and De Meulder , 2003 ) as the training   data , and evaluate the models with the CoNLL-   2003 German dataset and the CoNLL-2002 Span-   ish and Dutch dataset ( Tjong Kim Sang , 2002 ) .   Models . We adopt the model of Sohrab and Miwa   ( 2018 ) as the baseline model , which enumerates all   possible spans in a sentence and classiﬁes them into   the target entity types or non - entity type . In this   experiment , we enumerate spans with at most 16   tokens . For the baseline models , the span features   are computed as the concatenation of the word   representations of the ﬁrst and last tokens . The span   features are fed into a linear classiﬁer to predict   their entity type .   The input of mLUKE - E contains the entity   [ MASK ] tokens associated with all possible spans .   The span features are computed as the contextual-   ized representations of the entity [ MASK ] tokens .   The features are input to a linear classiﬁer as with   the word - based models .   4.3 Main Results   The results are shown in Table 3 . The mLUKE - E   models outperform their word - based counterparts   mLUKE - W in the average score in all the compara-   ble settings ( the base andlarge settings ; the RE and   NER tasks ) , which shows entity - based features are   useful in cross - lingual tasks . We also observe that   XLM - Rbeneﬁts from extra training ( 1.8 aver-   age points improvement in RE and 0.3 points in   NER ) , but mLUKE - E still outperforms the results .   4.4 Analysis   The performance gain of mLUKE - E over mLUKE-   W can be partly explained as the entity [ MASK]de es fr tr   mLUKE - W 0.71 0.74 0.74 0.84   mLUKE - E 0.25 0.28 0.24 0.36   token extracts better features for predicting entity   attributes because it resembles how mLUKE is pre-   trained with the MEP task . We hypothesize that   there exists another factor for the improvement in   cross - lingual performance : language neutrality of   representations .   The entity [ MASK ] token is shared across lan-   guages and their contextualized representations   may be less affected by the difference of input   languages , resulting in features that generalize   well for cross - lingual transfer . To ﬁnd out if the   entity - based features are actually more language-   independent than word - based features , we evaluate   themodularity ( Fujinuma et al . , 2019 ) of the fea-   tures extracted for the RELX dataset .   Modularity is computed for the k - nearest neigh-   bor graph of embeddings and measures the degree   to which embeddings tend to form clusters within   the same language . We refer readers to Fujinuma   et al . ( 2019 ) for how to compute the metric . Note   that the maximum value of modularity is 1 , and 0   means the embeddings are completely randomly   distributed regardless of language .   We compare the modularity of the word fea-   tures from mLUKE - Wand entity features from   mLUKE - Ebefore ﬁne - tuning . Note that the   features here are concatenated vectors of head and   tail features . Table 4 shows that the modularity of   mLUKE - Eis much lower than mLUKE - W ,   demonstrating that entity - based features are more   language - neutral . However , with entity - based fea-   tures , the modularities are still greater than zero . In   particular , the modularity computed with Turkish ,   which is the most distant language from English   here , is signiﬁcantly higher than the others , indi-   cating that the contextualized entity - based features   are still somewhat language - dependent .   5 Cloze Prompt Task with Entity   Representations   In this section , we show that using the entity repre-   sentations is effective in a cloze prompt task ( Liu   et al . , 2021 ) with the mLAMA dataset ( Kassner   et al . , 2021 ) . The task is , given a cloze template   such as “ [ X ] was born in [ Y ] ” with [ X ] ﬁlled   with an entity ( e.g. ,Mozart ) , to predict a correct   entity in [ Y ] ( e.g. , Austria ) . We adopt the typed   querying setting ( Kassner et al . , 2021 ) , where a   template has a set of candidate answer entities and   the prediction becomes the one with the highest   score assigned by the language model .   Model . As in Kassner et al . ( 2021 ) , the word - based   baseline models compute the candidate score as the   log - probability from the MLM classiﬁer . When   a candidate entity in [ Y ] is tokenized into multi-   ple tokens , the same number of the word [ MASK ]   tokens are placed in the input sequence , and the   score is computed by taking the average of the log-   probabilities for its individual tokens .   On the other hand , mLUKE - E computes the log-   probability of the candidate entity in [ Y ] with   the entity [ MASK ] token . Each candidate entity   is associated with an entity in mLUKE ’s entity   vocabulary via string matching . The input sequence   has the entity [ MASK ] token associated with the   word [ MASK ] tokens in [ Y ] , and the candidate   score is computed as the log - probability from the   MEP classiﬁer . We also try additionally appending   the entity token of [ X ] to the input sequence if the   entity is found in the vocabulary .   To accurately measure the difference betweenword - based and entity - based prediction , we restrict   the candidate entities to the ones found in the en-   tity vocabulary and exclude the questions if their   answers are not included in the candidates ( results   with full candidates and questions in the dataset are   in Appendix G ) .   Results . We experiment in total with 16 languages   which are available both in the mLAMA dataset   and the mLUKE ’s entity vocabulary . Here we only   present the top-1 accuracy results from 9 languages   on Table 5 , as we can make similar observations   with the other languages .   We observe that XLM - Rperforms notably   worse than mBERT as mentioned in Kassner et al .   ( 2021 ) . However , with extra training with the   Wikipedia corpus , XLM - Rshows a signiﬁcant   9.3 points improvement in the average score and   outperforms mBERT ( 27.8 vs. 27.2 ) . We conjec-   ture that this shows the importance of the training   corpus for this task . The original XLM - R is only   trained with the CommonCrawl corpus ( Conneau   et al . , 2020a ) , text scraped from a wide variety of   web pages , while mBERT and XLM - R + training   are trained on Wikipedia . The performance gaps   indicate that Wikipedia is particularly useful for   the model to learn factual knowledge .   The mLUKE - Wmodel lags behind XLM-   R+ extra training by 1.7 average points but we   can see 5.4 points improvement from XLM - R   + extra training to mLUKE - E([Y ] ) , indicating   entity representations are more suitable to elicit   correct factual knowledge from mLUKE than word   representations . Adding the entity corresponding   to[X ] to the input ( mLUKE - E([X ] & [ Y ] ) )   further pushes the performance by 11.7 points to   44.9 % , which further demonstrates the effective-   ness of entity representations .   Analysis of Language Bias . Kassner et al . ( 2021 )   notes that the prediction of mBERT is biased by   the input language . For example , when queried in   Italian ( e.g. , “ [ X ] e stato creato in [ MASK ] . ” ) , the   model tends to predict entities that often appear in   Italian text ( e.g. , Italy ) for any question to answer   location . We expect that using entity representa-   tions would reduce language bias because entities   are shared among languages and less affected by   the frequency in the language of questions .   We qualitatively assess the degree of language   bias in the models looking at their incorrect pre-   dictions . We show the top incorrect prediction for   the template “ [ X ] was founded in [ Y ] . ” for each   model in Table 6 , together with the top- 1incor-   rect ratio , that is , the ratio of the number of the   most common incorrect prediction to the total false   predictions , which indicates how much the false   predictions are dominated by few frequent entities .   The examples show that the different models ex-   hibit bias towards different entities as in English   and French , although in Japanese the model consis-   tently tends to predict Japan . Looking at the degree   of language bias , mLUKE - E([X ] & [ Y ] ) ex-   hibits lower top- 1incorrect ratios overall ( 27 % in   fr , 44 % in ja , and 30 % in fr ) , which indicates us-   ing entity representations reduces language bias .   However , lower language bias does not necessarily   mean better performance : in French ( fr ) , mLUKE-   E([X ] & [ Y ] ) gives a lower top- 1incorrect   ratio than mBERT ( 30 % vs. 71 % ) but their num-   bers of total false predictions are the same ( 895 ) .   Language bias is only one of several factors in the   performance bottleneck .   6 Related Work   6.1 Multilingual Pretrained Language   Models   Multilingual pretrained language models have re-   cently seen a surge of interest due to their effective-   ness in cross - lingual transfer learning ( Conneau   and Lample , 2019 ; Liu et al . , 2020 ) . A straight-   forward way to train such models is multilingual   masked language modeling ( mMLM ) ( Devlin et al . ,   2019 ; Conneau et al . , 2020a ) , i.e. , training a single   model with a collection of monolingual corpora   in multiple languages . Although models trainedwith mMLM exhibit a strong cross - lingual abil-   ity without any cross - lingual supervision ( K et al . ,   2020 ; Conneau et al . , 2020b ) , several studies aim   to develop better multilingual models with explicit   cross - lingual supervision such as bilingual word   dictionaries ( Conneau et al . , 2020b ) or parallel sen-   tences ( Conneau and Lample , 2019 ) . In this study ,   we build a multilingual pretrained language model   on the basis of XLM - RoBERTa ( Conneau et al . ,   2020a ) , trained with mMLM as well as the masked   entity prediction ( MEP ) ( Yamada et al . , 2020 ) with   entity representations .   6.2 Pretrained Language Models with Entity   Knowledge   Language models trained with a large corpus con-   tain knowledge about real - world entities , which is   useful for entity - related downstream tasks such as   relation classiﬁcation , named entity recognition ,   and question answering . Previous studies have   shown that we can improve language models for   such tasks by incorporating entity information into   the model ( Zhang et al . , 2019 ; Peters et al . , 2019 ;   Wang et al . , 2021 ; Xiong et al . , 2020 ; Févry et al . ,   2020 ; Yamada et al . , 2020 ) .   When incorporated into multilingual language   models , entity information can bring another ben-   eﬁt : entities may serve as anchors for the model   to align representations across languages . Multi-   lingual knowledge bases such as Wikipedia often   offer mappings between different surface forms   across languages for the same entity . Calixto et al .   ( 2021 ) ﬁne - tuned the top two layers of multilin-   gual BERT by predicting language - agnostic en-   tity ID from hyperlinks in Wikipedia articles . As   our concurrent work , Jiang et al . ( 2022 ) trained   a model based on XLM - RoBERTa with an entity   prediction task along with an object entailment pre-   diction task . While the previous studies focus on   improving cross - lingual language representations   by pretraining with entity information , our work in-   vestigates a multilingual model not only pretrainedwith entities but also explicitly having entity repre-   sentations and how to extract better features from   such model .   7 Conclusion   We investigated the effectiveness of entity repre-   sentations in multilingual language models . Our   pretrained model , mLUKE , not only exhibits strong   empirical results with the word inputs ( mLUKE - W )   but also shows even better performance with the   entity representations ( mLUKE - E ) in cross - lingual   transfer tasks . We also show that a cloze - prompt-   style fact completion task can effectively be solved   with the query and answer space in the entity vocab-   ulary . Our results suggest a promising direction to   pursue further on how to leverage entity represen-   tations in multilingual tasks . Also , in the current   model , entities are represented as individual vec-   tors , which may incur a large memory footprint in   practice . One can investigate an efﬁcient way of   having entity representations .   ReferencesAppendix for “ mLUKE : The Power of Entity Representations in Multilingual Pretrained   Language Models ”   A Details of Pretraining   Dataset . We download the Wikipedia dumps from December 1st , 2020 . We show the 24 languages   included in the dataset on Table 7 , along with the data size and the number of entities in the vocabulary .   Language Code Size # entities in vocab Language Code Size # entities in vocab   ar 851 M 427,460 ko 537 M 378,399   bn 117 M 62,595 nl 1.1 G 483,277   de 3.5 G 540,347 pl 1.3 G 489,109   el 315 M 135,277 pt 1.0 G 537,028   en 6.9 G 613,718 ru 2.5 G 529,171   es 2.1 G 587,525 sv 1.1 G 390,313   ﬁ 480 M 300,333 sw 27 M 30,129   fr 3.1 G 630,355 te 66 M 14,368   hi 90 M 54,038 th 153 M 100,231   i d 327 M 217,758 tr 326 M 297,280   it 1.9 G 590,147 vi 516 M 263,424   ja 2.3 G 369,470 zh 955 M 332,970   Total 31.4 G 8,374,722   Optimization . We optimize the mLUKE models for 1 M steps in total using AdamW ( Loshchilov and   Hutter , 2019 ) with learning rate warmup and linear decay of the learning rate . The pretraining consists   of two stages : ( 1 ) in the ﬁrst 500 K steps , we update only those parameters that are randomly initialized   ( e.g. , entity embeddings ) ; ( 2 ) we update all parameters in the remaining 500 K steps . The learning rate   scheduler is reset at each training stage . The detailed hyper - parameters are shown in Table 8 .   Maximum word length 512 Mask probability for entities 15 %   Batch size 2048 The size of word token embeddings 768   Peak learning rate 1e-4 The size of entity token embeddings 256   Peak learning rate ( ﬁrst 500 K steps ) 5e-4 Dropout 0.1   Learning rate decay linear Weight decay 0.01   Warmup steps 2500 Adam   0.9   Mask probability for words 15 % Adam   0.999   Random - word probability for words 10 % Adam 1e-6   Unmasked probability for words 10 % Gradient clipping none   Computing Infrastructure . We run the pretraining on NVIDIA ’s PyTorch Docker container 19.02 hosted   on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs . The training   takes approximately 2 months . B Details of Downstream Experiments   Hyperparameter Search . For each downstream task , we perform hyperparameter searching for all the   models with the same computational budget to ensure a fair comparison . For each task , we use the ﬁnal   evaluation metric on the validation split of the training English corpus as the validation score . The models   are optimized with the AdamW optimizer ( Loshchilov and Hutter , 2019 ) with the weight decay term set   to 0.01 and a linear warmup scheduler . The learning rate is linearly increased to a speciﬁed value in the   ﬁrst 6 % of training steps , and then gradually decreased to zero towards the end . Table 9 summarizes the   task - speciﬁc hyperparameter search spaces .   QA   ( SQuAD)Relation Classiﬁcation   ( KBP37)NER   ( CoNLL 2003 )   Learning rate 2e-5 2e-5 2e-5   Batch size { 16 , 32 } { 4 , 8 , 16 } { 4 , 8 , 16 }   Epochs 2 5 5   # of random seeds 3 3 3   Validation metric F1 F1 F1   Computing Infrastructure . We run the ﬁne - tuning on a server with a Intel(R ) Core(TM ) i7 - 6950X CPU   and 4 NVIDIA GeForce RTX 3090 GPUs .   C Detecting Entities in the QA datasets   For each question – passage pair in the QA datasets , we ﬁrst create a mapping from the entity mention   strings ( e.g. , “ U.S. ” ) to their referent Wikipedia entities ( e.g. , United States ) using the entity hyperlinks on   the source Wikipedia page of the passage . We then perform simple string matching to extract all entity   names in the question and the passage and treat all matched entity names as entity annotations for their   referent entities . We ignore an entity name if the name refers to multiple entities on the page . Further , to   reduce noise , we also exclude an entity name if its link probability , the probability that the name appears   as a hyperlink in Wikipedia , is lower than 1 % .   The XQuAD datasets are created by translating English Wikipedia articles into target languages . For   each translated article , we create the mention - entity mapping from the source English article by the   following procedure : for all the entities found in the source article , we ﬁnd the corresponding entity in the   target language through inter - language links , and then collect its possible mention strings ( i.e. , hyperlinks   to the entity ) from a Wikipedia dump of the target language ; the entity and the collected mention strings   form the mention - entity mapping for the translated article . D The Model Size   # of layers hidden size # of heads vocabulary size # of parameters   mBERT 12 768 12 120 K 177 M   XLM - R 12 768 8 250 K 278 M   mLUKE - E 12 768 8 250 K 585 M   XLM - R 24 1024 16 250 K 559 M   mLUKE - E 24 1024 16 250 K 867 M   E Ablation Study of Entity Embeddings   In Section 3 and 4 , we have shown that using entity representations in mLUKE improves the cross-   lingual transfer performance in QA , RE , and NER . Here we conduct an additional ablation study to   investigate whether the learned entity embeddings are crucial to the success of our approach . We train an   ablated model of mLUKE - E whose entity embeddings are re - initialized randomly before ﬁne - tuning ( -   ablation ) . Table 11 and Table 12 show that the ablated model performs signiﬁcantly worse than the full   model ( mLUKE - E ) , indicating that using pretrained entity embeddings is crucial rather than applying our   approach during ﬁne - tuning in an ad - hoc manner without entity - aware pretraining .   XQuAD en es de el ru tr ar vi th zh hi avg .   mLUKE - E 86.3 78.9 78.9 73.9 76.0 68.8 71.4 76.4 67.5 65.9 72.2 74.2   - ablation 84.3 76.8 76.4 71.9 74.3 67.4 70.2 75.3 67.1 64.4 68.4 72.4   MLQA en es de ar hi vi zh avg . G - XLT avg .   mLUKE - E 80.8 70.0 65.5 60.8 63.7 68.4 66.2 67.9 55.6   - ablation 80.3 69.4 64.5 59.1 59.2 66.5 63.6 66.1 50.7   RE NER   en de es fr tr avg . en de du es avg .   mLUKE - E 69.3 64.5 65.2 64.7 68.7 66.5 93.6 77.2 81.8 77.7 82.6   - ablation 62.5 59.3 60.7 61.0 60.5 50.8 93.0 76.3 80.8 76.1 81.6F Full Results of MLQAG Full Results of mLAMA   Table 5 shows the results from the setting where the entity candidates not in the mLUKE ’s entity vocabulary   are excluded . Here we provide in Table 21 the results with the full candidate set provided in the dataset   for ease of comparison with other literature . When the candidate entity is not found in the mLUKE ’s   entity vocabulary , the log - probability from the word [ MASK ] tokens are used instead .   ar bn de el en es ﬁ fr   mBERT 15.1 12.7 28.6 19.4 34.8 30.2 19.2 27.1   XLM - R 14.9 7.5 18.4 12.7 24.2 18.5 14.5 16.1   + extra training 20.7 14.0 29.3 18.2 31.6 26.4 19.2 25.0   mLUKE - W 21.3 12.9 25.7 17.5 27.1 23.3 15.9 23.0   mLUKE - E([Y ] ) 25.6 21.6 32.9 25.2 34.9 28.5 24.7 27.7   mLUKE - E([X ] & [ Y])37.3 32.3 43.7 34.4 43.2 36.4 35.3 34.2   i d ja ko pl pt ru vi zh avg .   mBERT 37.4 14.2 17.8 21.9 32.0 17.4 36.5 24.2 24.3   XLM - R 24.6 11.4 10.9 16.6 22.2 12.6 23.0 15.5 16.5   + extra training 38.2 19.1 21.4 20.5 29.6 20.6 33.8 28.1 24.7   mLUKE - W 36.6 18.0 17.9 20.2 29.4 19.6 31.0 26.9 22.9   mLUKE - E([Y ] ) 35.3 27.2 26.3 25.7 34.7 23.8 39.1 29.5 28.9   mLUKE - E([X ] & [ Y])47.6 37.7 41.6 37.7 44.8 31.4 50.1 41.6 39.3