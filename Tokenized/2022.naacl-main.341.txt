  Peter WestChandra BhagavatulaJack HesselJena D. Hwang   Liwei JiangRonan Le BrasXiming LuSean WelleckYejin ChoiPaul G. Allen School of Computer Science & Engineering , University of WashingtonAllen Institute for Artificial Intelligence   Abstract   The common practice for training common-   sense models has gone from – human – to – corpus –   to – machine : humans author commonsense   knowledge graphs in order to train common-   sense models . In this work , we investigate   an alternative , from – machine – to – corpus – to –   machine : general language models author these   commonsense knowledge graphs to train com-   monsense models .   Our study leads to a new framework , Sym-   bolic Knowledge Distillation . As with prior   art in Knowledge Distillation ( Hinton et al . ,   2015 ) , our approach uses larger models to teach   smaller models . A key difference is that we   distill knowledge symbolically – as text – in ad-   dition to the resulting neural model . We distill   only one aspect – the commonsense of a general   language model teacher , allowing the student   to be a different type of model , a common-   sense model . Altogether , we show that careful   prompt engineering and a separately trained   critic model allow us to selectively distill high-   quality causal commonsense from GPT-3 , a   general language model .   Empirical results demonstrate that , for the first   time , a human - authored commonsense knowl-   edge graph is surpassed by our automatically   distilled variant in all three criteria : quantity ,   quality , and diversity . In addition , it results in   a neural commonsense model that surpasses   the teacher model ’s commonsense capabilities   despite its 100x smaller size . We apply this to   theA resource , and will share our new   symbolic knowledge graph and commonsense   models .   1 Introduction   Prior works have suggested that pre - trained lan-   guage models possess limited understanding of   commonsense knowledge ( Merrill et al . , 2021 ; Tal-   mor et al . , 2021 ; Davis and Marcus , 2017 ) despiteFigure 1 : Symbolic knowledge distillation extracts the   commonsense from the large , general language model   GPT-3 , into 2 forms : a large commonsense knowledge   graph A , and a compact commonsense model   C. The quality of this knowledge can be con-   trolled and improved by adding a critic model , making   GPT-3 a stronger teacher .   otherwise stellar performance on leaderboards . As   a result , symbolic commonsense knowledge graphs   ( Speer et al . , 2017 ; Sap et al . , 2019 ; Hwang et al . ,   2021 ) and corresponding neural representations   ( Bosselut et al . , 2019 ; Hwang et al . , 2021 ; Zhang   et al . , 2020b ) have supplemented past models with   commonsense capabilities . This has enabled di-   verse downstream applications , including interac-   tive learning through a conversational interface   ( Arabshahi et al . , 2021 ) , persona- and affect - aware   conversation models ( Kearns et al . , 2020 ) , figura-   tive language understanding ( Chakrabarty et al . ,   2020 , 2021 ) , story telling ( Ammanabrolu et al . ,   2021a ) and fantasy games ( Ammanabrolu et al . ,   2021b ) .   The common practice for commonsense knowl-   edge graph construction sees humans spell out   as many pieces of knowledge as possible . This   pipeline goes from – human – to – corpus – to – machine ,   with commonsense models trained from human-4602authored knowledge graphs . Yet , high - quality ,   human - authored knowledge is expensive to scale ,   limiting coverage ; this motivates an alternative :   from – machine – to – corpus – to – machine . Prior ef-   forts toward automatic commonsense knowledge   graphs have resulted in considerably lower qual-   ity than human - written data ( Hwang et al . , 2021 ;   Zhang et al . , 2020b ) , which in turn leads to less   reliable neural models ( Hwang et al . , 2021 ) . Broad   literature consistently shows machine - authored   knowledge graphs underperform human - authored   graphs ( Etzioni et al . , 2011 ; Mitchell et al . , 2015 ;   Bollacker et al . , 2008 ) .   In this work , we propose Symbolic knowledge   distillation , a new conceptual framework towards   high - quality automatic knowledge graphs for com-   monsense , leveraging state - of - the - art models and   novel methodology . Most prior art for automatic   knowledge graph construction extracts knowledge   from raw text ( Bhakthavatsalam et al . , 2020 ; Zhang   et al . , 2020a ; Zhou et al . , 2020 ; Zhang et al . , 2020b ;   Li et al . , 2020 ) . In contrast , our approach is mo-   tivated by knowledge distillation ( Hinton et al . ,   2015 ) wherein a larger teacher model transfers   knowledge to a compact student model ( § 2.1 ) . Our   method differs from prior knowledge distillation in   key ways : we distill a symbolic knowledge graph   ( i.e. , generated text ) in addition to a neural model ,   and we distill only a selective aspect of the teacher   model . This selectively allows the student model   to be of a different type ( commonsense model ) ,   compared to the teacher ( general language model ) ,   enriching the scope of distillation . An added ben-   efit is that knowledge distilled as text is human   readable : it can be understood and evaluated .   A general language model – GPT-3 in our case – is   an imperfect commonsense teacher on its own , and   the ability to evaluate distilled knowledge is useful   in improving it . We empirically demonstrate that ,   by training a separate critic model to judge sym-   bolic generation quality , a more precise teacher can   be defined . Knowledge from this critical teacher   is higher quality – even exceeding human - authored   knowledge . Yet even before training a critic , our   study makes the unexpected finding that the student   model surpasses the commonsense of GPT-3 , our   knowledge source .   To test symbolic knowledge distillation against   thehuman – to – corpus – to – machine paradigm , we   compare with A(Hwang et al . , 2021 ) ,   which is a human - authored commonsense knowl - edge graph . We find that A , our machine-   generated corpus , exceeds the human generated   corpus in scale , accuracy , anddiversity with re-   spect to 7 commonsense inference types that we   focus on in this study . The resulting commonsense   model , C , not only surpasses the human-   trained equivalent C , but is also smaller ,   more efficient , and produces commonsense at a   higher accuracy than its own teacher – GPT-3 .   Symbolic knowledge distillation offers a promis-   ing new role for general language models , as com-   monsense knowledge sources , and humans , as   small - scale evaluators to train critic models rather   than authors of commonsense knowledge . Our   work demonstrates that humans and LMs can be   effective collaborators for curating commonsense   knowledge graphs and training efficient and perfor-   mant commonsense models .   2 Overview and Key Findings   Throughout our work , we describe the machine –   to – corpus – to – machine methodology of symbolic   knowledge distillation . We first go machine – to –   corpus ( § 3 ) , by decoding from GPT-3 , then im-   prove our knowledge with a specialized critic   model ( § 4 ) , and finally distill this knowledge   into an efficient commonsense model ( § 5 ) , going   corpus – to – machine . Throughout this process , we   evaluate against a human knowledge source , com-   paring our automatic knowledge graph A   and commonsense model Cto the human-   authored Aand resulting model C   ( Hwang et al . , 2021 ) .   2.1 Symbolic Knowledge Distillation   Our proposed methodology parallels knowledge   distillation ( Hinton et al . , 2015 ) , a method for com-   pressing a large or complicated teacher distribution   Pinto a smaller / simpler student distribution P.   Key to knowledge distillationis the notion of min-   imizing the cross - entropy between PandP :   H(P , P ) = −/summationdisplayP(y ) logP(y ) ( 1 )   Knowledge is transferred to the student by encour-   aging it to match teacher predictions . Hinton et al .   ( 2015 ) apply this to conditional classification : for4603   each training input , PandPare model predic-   tions over label set Y. Typically Yis a tractable set ,   over which this sum can reasonably be calculated .   For distilling the knowledge of generative mod-   els , we can think of an unconditional language   model ( LM e.g. GPT-3 ) as P. This makes Ythe   set of all strings , over which LMs define probability .   Unfortunately Yis an exponential set , intractable   to sum over in Eq 1 . Kim and Rush ( 2016 ) address   this problem by simply taking the mode of Pover   Y , truncating most of the teacher distribution to the   most likely sequence and discarding information .   Instead , we consider a sampling - based interpre-   tation of the same objective :   H(P , P ) = E[−logP(y ) ] ( 2 )   which exactly equals the cross - entropy of Eq 1 , at   the limit under pure sampling from P.   Yet distilling all knowledge from the teacher may   not be desirable – our work is specifically focused   on distlling commonsense knowledge from GPT-   3 . The ideal teacher Pis a commonsense expert ,   but GPT-3 can approximate such a teacher , off - the-   shelf , via prompting . This ability to select informa-   tion is one explicit benefit of the sampling - based   interpretation of Eq 2 : while Eq 1 uses continu-   ous logits over existing data , sampling gives dis-   crete control over transferred information , by se-   lecting which samples are elicited and used . For   the general language model GPT-3 , We encour-   age domain / quality with prompting , and sample   truncation ( Holtzman et al . , 2020 ) . We call this   theloose teacher P – knowledge is generated and   transferred from GPT-3 , but without critical assess-   ment of correctness ( § 3).In fact , sampling knowledge in Eq 2 offers even   more control , as generations can be individually   interpreted and judged . Given an indicator function   A(x)for which knowledge xiscorrect , we can   define a stronger teacher model . Using a Product of   Experts ( Hinton , 2002 ) between the loose teacher   Pand and the critic A(x ) , we define a critical   teacher :   P(x)∝P(x|p)·A(x ) ( 3 )   In practice , A(x)is a textual classifier learned on   human judgements , 1 for knowledge predicted to   be correct and 0 otherwise . Thus , the critic gives   control over the correctness and confidence of the   knowledge that is transferred ( § 4 ) .   2.2 Key Findings   Applying symbolic knowledge distillation in prac-   tice results in promising and surprising findings :   1 . Learning symbolic knowledge from language   models can be framed as a symbolic extension   to knowledge distillation . In § 2.1 , we describe   learning commonsense as a symbolic extension to   knowledge distillation , with GPT-3 a knowledge   source . We elaborate on this process with positive   results in § 3,4 , and 5 .   2 . Symbolic knowledge distillation constructs   a high quality knowledge graph at scale . Our   method naturally yields a machine - generated com-   monsense knowledge graph , which can achieve   impressive quality ( § 4 ) , beyond that of human-   authored data . An effective critic which filters   incorrect generated knowledge is key .   3 . A critical teacher results in a higher quality   student . In § 4 , we show that making the teacher   more critical results in higher quality knowledge ,   even as it reduces the scale of knowledge trans-   ferred . This demonstrates that quality matters , not4604justquantity , as higher quality knowledge results in   a higher quality commonsense model in § 5 despite   smaller scale data .   4 . Critical teacher or not , a student can outper-   form the knowledge source . In § 5 , we show the   unexpected result that all student models exceed   the quality of GPT-3 , the knowledge source .   5 . Machines can win over humans for automatic   knowledge graph construction . In § 4 and § 5 ,   we show that machine generated knowledge and the   resulting commonsense model can outperform their   equivalents that use a human knowledge source .   Our symbolic knowledge exceeds humans at scale ,   quality , and diversity . The resulting commonsense   model achieves the most accurate commonsense   KG completions .   3 Machine - to - Corpus Verbalization   Symbolic knowledge distillation begins by going   machine – to – corpus , i.e. generating many com-   monsense facts , which results in a commonsense   knowledge graph . § 2.1 frames this as sampling   to estimate the knowledge distillation objective – a   student commonsense model learns from the gen-   erations of a teacher ( GPT-3 ) .   We start with a loose teacher , transferring knowl-   edge by prompted generation with truncated sam-   pling alone – this is in contrast to the critical teacher   ( § 4 ) which explicitly judges and filters the gen-   erated samples . The loose teacher uses few - shot   prompting as in Brown et al . ( 2020 ) . We use a   few - shot template :   where < EX - INP > /<EX - OUT > are human-   authored , natural language A entries ,   and < TASK - PROMPT > is a description of the   problem . Given such a prompt , GPT-3 generates   themissing piece , output < EX - OUT > for input   < EX - INP > , following the pattern of earlier   examples ( 1 to N-1 ) . We find important aspects for   producing high - quality commonsense knowledge :   •Examples should be numbered . e.g.   < EX - INP > might begin with " 5 ) " to in-   dicate it is the 5th example.•The format of < EX - INP > and < EX - OUT >   should linguistically imply the relationship be-   tween them . See below for examples .   •<TASK - PROMPT > can be used to give extra   specification to complicated problems .   3.1 Data : A   We demonstrate symbolic knowledge distillation   on the A if - then resource ( Sap et al . , 2019 ) .   This follows an event - relation - inference ( triple ) for-   mat . The corpus links events ( e.g. X attacks Y ) to   relations , e.g. HinderedBy which describes what   might hinder an event . For a relation / event , the   goal is to generate a resulting inference , e.g. X   attacks Y HinderedBy X is restrained .   Of the 23 relations from the most recent version –   A – we limit our investigation to 7 relations   that correspond to causal commonsense knowl-   edge : xAttr ( how X is perceived after event ) , xRe-   act(how X reacts in response to event ) , xEffect   ( what X does after event ) , xIntent ( X ’s intent in   event ) , xWant ( what X wants after event ) , xNeed   ( what X needed for event to take place ) and Hin-   deredBy . We describe how verbalization is ap-   plied to A data in 2 steps : generating under-   lying events ( heads ) , then full examples ( inference   given event ) .   3.2 Event Generation   Events are context - free premises in A   involving PersonX ( and sometimes a second   PersonY ) in various scenarios . These events form   heads in knowledge graph triples . We generate   events by filling in the elements of our template :   The format is simple , as events are generated un-   conditionally . We use 100 high - quality events from   theAcorpus for our prompt , selected   to avoid grammatical or logical errors , and min-   imize semantic overlap . We randomly sample 10   of these seed events for each generation batch , re-   sulting in randomized prompts . We use nucleus   sampling ( p= 0.9 ) ( Holtzman et al . , 2020 ) , and   presence / frequency penalties of 0.5 from the GPT-   3 interface . We generate 165 K unique events using   the 175B - parameter Davinci modelfrom Brown4605et al . ( 2020 ) ( human - authored Acontains   only 6.2 K events ) .   3.3 Inference Generation   Generating A inferences requires reasoning   about events and relations together . We design ver-   balization templates fo reach relation , with iterative   design and small - scale verification by the authors   e.g. we prompt the xNeed relation as follows :   . . .   . . .   The language of this template implies the relation-   specific task , both " Prerequisites : " and beginning   with " for this to happen " suggest the xNeed re-   lation . As well , we include an xNeed - specific   < TASK - PROMPT > . We use 10 few - shot examples   for each prompt .   For each event / relation ( 165 K X 7 ) we gener-   ate 10 inferences with the Curie GPT-3 model   and earlier hyperparameters . Removing duplicate   and degenerate ( e.g. fewer than 3 characters ) gen-   erations yields 6.46 M A -style data triples   ( examples in Figure 2 ) . We call this A ,   as it contains an order of magnitude more triples   than Afor the 7 relations we study .   3.4 Evaluating a Generated Commonsense   Knowledge Graph   Machine generation enables a large scale of unique   generations at a much lower cost than human-   authored knowledge ( Table 1 ) , but what kind of   examples are produced by GPT-3 , and how does   it differ from knowledge produced by humans ? In   this section , we conduct an in - depth analysis to   answer these questions .   Lexical Differences : Diversity and Uniqueness   Recent work finds that machine generations can be   repetitive and lack diversity ( Welleck et al . , 2020 ;   Holtzman et al . , 2020 ) ; one way generated knowl-   edge may differ from human - authored is less cre-   ative word choice , diversity , or more repetition .   To test this , we begin with lexical diversity   ( i.e. unique words used , Table 2 ) . While there   is variation by relation , the diveristy of A   actually exceeds Ahere , 5.2 M unique   words to 1.5M. In addition , it contains significantly   more strictly unique generated inferences ( Table 2 ,   unique tails ) .   BLEU Soft Uniqueness . Exact match ( above )   fails to capture the notion of similar text . Follow-   ing the intuition of self - BLEU ( Zhu et al . , 2018 ) ,   we define soft uniqueness to describe diversity of   generations in a corpus . An inference xis softly-   unique if :   BLEU(C , x)<0.5   where Cis the set of inferences for a given in-   put ( in our case , event + relation ) , and 0.5 is an   empirical threshold . To find soft - uniqueness of a   corpus , we iteratively remove examples until all   are softly unique , i.e. low mutual lexical over-   lap ; higher diversity means more such examples   ( thus a larger softly unique corpus is preferable ) .   Softly - unique corpus sizes are given in Table 4   ( “ Size ( div ) ” ) . Ahas a smaller fraction   of softly - unique examples than A , yet it   contains many more such examples . A   contains 4.38 M such examples ( full size 6.5 M ) vs.   A , which has 560 K ( full size 600K).4606   Model - based Diversity Measurement . Lexical   notions of diversity reward differences in surface   form , which may not always reflect diversity of   information , only format . Thus , we next study   information - theoretic measures for diversity . In-   tuitively , diverse information should be less pre-   dictable , or higher entropy . With GPT-2 XL mod-   els finetuned on AandA(§5 )   we estimate entropy – roughly , how difficult it is   for a model to capture the corpus information ( Ta-   ble 3 ) . This is 4 times higher for A ,   suggesting more content from a modeling per-   spective . We also estimate cross - entropy – how   well a model trained on one corpus describes the   other . From AtoA , this is 9.31 ,   only 2 points higher than its entropy suggesting   Ais describable with information from   A. In reverse , this is 41.48 suggesting   much of Ais not captured by A –   Ais surprising given only information   from A.   Human Evaluation of Quality . Perhaps most   importantly , we study the quality of knowledge in   each corpus . We conduct human evaluation with   Amazon Mechanical Turk . 3 annotators rate each   triple resulting in “ accepted ” , “ rejected ” or “ no   judgement ” . We evaluate 3000 examplesfrom   A , and 1000 from A(Table 4 ) .   We find Fleiss ’ kappa ( Fleiss , 1971 ) of 40.8 indicat-   ing moderate agreement ( Landis and Koch , 1977 ) ,   and 90.5 % accuracy agreement . We require work-   ers meet an Amazon Mechanical Turk qualification   for annotation quality based on past commonsense   evaluations . We compensate workers $ 0.17per   task , which we estimate require 30 seconds . Fur-   ther details and task template are in appendix § A.   For the loose teacher , consider the top row of   Ain Table 4 ( other rows add the critic   § 4 ) . Aexceeds Ain scale , but   is somewhat less acceptable by human raters – by   roughly 8 percentage points . Yet , the larger scale of   Aimplies a significantly higher number   of accurate examples . Increasing the proportion of   these is the main objective of the critic ( § 4 ) .   How do Knowledge Sources Compare ? To un-   derstand the robustness of our approach , we assess   other language models as the knowledge source   ( i.e. loose teacher ): GPT - J ( Wang and Komat-   suzaki , 2021 ) and T5 - 11B adapted for language   modelling ( Lester et al . , 2021 ) . We substitute both   for GPT-3 as in § 3.2,3.3 , generating a small - scale   corpus to evaluate . We conduct human evaluation   on 1000 examples as above ( Table 4 ) . Both mod-   els attain roughly 72 % accuracy , 6 points below   GPT-3 ( 78.5 ) . This suggests strong potential , but   higher quality from GPT-3 . We explore this further   in Appendix B.46074 Making the Teacher More Critical   Symbolic knowledge distillation requires a strong   teacher model to maximize the quality of the gener-   ated knowledge graph and resulting student model   ( § 5 ) . While the loose teacher ( GPT-3 alone ) re-   sults in a viable commonsense knowledge graph ,   evaluation shows this is n’t a perfect commonsense   teacher . Thus , we multiply in a critic model , to fil-   ter lower - quality knowledge , correcting the teacher   ( § 2.1 ) . With modest supervision ( a small - scale hu-   man evaluation ) we train a classifier to predict and   discriminate unacceptable examples . We multiply   this with the loose teacher § 3 , creating a critical   teacher product of experts . In practice this means   filtering Ato create new corpora that are   higher quality , yet still larger scale than human-   authored A.   Training a knowledge critic We gather a train-   ing set of correct vs. incorrect human judgments   on a randomly - sampled set of 10 K entries of   A , as in § 3.4 but with one annotation per   example . We take a ( random ) train / dev / test split of   8k/1k/1k . While this step requires human annota-   tion , humans take on the role of high - level supervi-   sors here – critiquing a small number of generations   rather than authoring the entire knowledge graph   as in previous work . Indeed , the cost / complexity   of this step is similar to a typical human evaluation ,   making it far cheaper / easier than eliciting human-   authored knowledge in past work .   We train binary classifiers ( critics ) for human ac-   ceptability using RoBERTa - Large ( Liu et al . , 2019 ) .   We find pretraining on MNLI results in the best   model in terms of precision and recall , and we sug-   gest this technique for future studies . We give more   detail in Appendix C , including baselines . Our best   model vastly improves the accuracy of A   ( Table 4 ) , demonstrating that a small amount of   human supervision can consistently help to correct   GPT-3 ’s mistakes .   Size - accuracy trade - off Using our critic to fil-   ter knowledge results in a natural trade - off be-   tween size and accuracy . We test several cut-   offs for A , i.e. confidence at which   the critic rejects examples . We report human-   measured accuracy ( Accept / Reject column Ta-   ble 4 ) following § 3.4 . We compare the loose   teacher ( unfiltered ) to critical teachers . Discard-   ing 20 % of instances that the critic judges as least   acceptable ( reducing corpus size from 6.5 M toRandom Inf Event EMAP Full   AP 79.3 81.9 86.2 87.1 94.0   5.1 M ) , A ’s accuracy rises 78.5 →88.4 ;   human - authored Acontains 600 K entries   at 86.8 % accuracy . Reducing to total size to 2.5 M   examples ( 38 % of full size ) , we attain 96.4 % accu-   racy , nearly 10 points above Awhile still   4X larger .   What gets filtered out ? We qualitatively identify   two types of filtered triples : 1 ) logical misalign-   ments , events / inferences joined in an inconsistent   manner . Recognizing these requires understand-   ing events - inference interactions , e.g. , X can not   find his shirt as a result X is wearing a shirt ; 2 )   awkward phrasings , in which events / inferences are   individually incoherent e.g. PersonX has a fire in   the bath – resulting triples are invalid as the event is   implausible .   To understand what is filtered , we ablate the   critic ( Table 5 ): our full model is compared to a   random predictor , event - only model , and inference-   only model . We also compare to an EMAP ( Hessel   and Lee , 2020 ) version , i.e. an ensemble of event   and inference - only , without interactions between   event / inference ( needed for logical misalignments ) .   We find GPT-3 produces both independent   awkwardly - phrased events / inferences ( filtered by   X - only models ) and logical misalignments . The   classifier , trained on validated knowledge triples ,   helps in both cases . The EMAP of our full model   ( identifies only awkward phrasings ) achieves 87 %   AP , and our full model ( which additionally identi-   fies logical misalignments ) improves to 94 % AP .   Does filtering hurt diversity ? One concern is   that the critic may keep only similar “ safe ” ex-   amples , lacking novelty . We repeat our diversity   analysis ( § 3.4 ) for critical corpora ( Table 4 , “ Size   ( div ) ” , higher = better ) . As we filter , we surprisingly   observe proportionally more diverse examples : full   Ahas a diverse subset 68 % of its size ;   rising to 80 % with the most extreme filtering . One   possibility is that GPT-3 gravitates towards com-4608   mon sentence structures for inconsistent knowl-   edge . These would be recognizable to the critic ,   and removing them would increase both quality   and diversity . This surprising result warrants fur-   ther study .   5 Corpus - to - Machine : Distillation   The final step of symbolic knowledge distillation   trains a compact model on the generated natural   language knowledge graph . Our base model is   GPT2 - XL trained on all of A : we denote   this model by C. We additionally train the   model on critical versions of A – crit   denotes training on the corpus achieving 91.5 % ac-   curacy , and criton the 96.4 % accuracy corpus .   Models are trained for 1 epoch , with default param-   eters using the Huggingface Transformers library   ( Wolf et al . , 2019 ) .   5.1 Evaluating a Symbolically Distilled Model   Evaluation follows past work ( Hwang et al . , 2021 ;   Bosselut et al . , 2019 ; Sap et al . , 2019 ) testing the   ability of models to do knowledge base completion ,   i.e. generating inferences for test events , specif-   ically from the Atest set . We use hu-   man evaluationfollowing Section 3.4 , on 1000   inputs ( event + relation ) , with results in Table 6 . We   compare to the GPT2 - XL - based Cmodel   trained on human - generated A , and GPT-   3 using the same generation method as § 3 – in ef-   fect , comparing the student Cto the loose   teacher GPT-3 . We omit the critical teacher ( GPT-   3 + critic ) , which is not assured to produce an in - ference for each input , as the critic may reject all   tails for some inputs . We also compare to zero - shot   GPT2 - XL ( Radford et al . , 2019 ) using the same   methodology ( Table 6 ) .   How does Ccompare to GPT-3 ? In   knowledge distillation , the student model often de-   teriorates in performance ( Hinton et al . , 2015 ; Kim   and Rush , 2016 ) compared to its teacher . Compar-   ing our base teacher – GPT-3 – to the simplest version   ofC(top - row Cof Table 6 ) sur-   prisingly shows the student surpasses GPT-3 , the   model that generates its training data . We posit   that the superior performance of Cmay   have to do with mistakes of GPT-3 being filtered by   verbalization and training of GPT-2 , and possibly   the focus of Con one commonsense do-   main while GPT-3 covers a more general domain .   We leave further study of this effect for future work .   How does Ccompare to human knowl-   edge ? While Cwithout the critic is   slightly outperformed by Cin terms of ac-   curacy , this reverses with the critic . For both cutoffs   tested , Csurpasses C , with more   filtering resulting in a wider gap .   Usefulness of C For on - demand infer-   ence , where a single high quality inference for   some input event / relation is required , C   is the best available model : the most performant   version surpasses Cby 5 points and GPT-3   by over 10 . The critical teacher ( GPT-3 + critic )   yields a more accurate corpus , but may filter all   inferences for an input , giving no output .   Limits and Future Work The success of   symbolic knowledge distillation is a first step –   demonstrating superior performance to human au-   thoring on the commonsense relations tested here .   No aspect of our approach is specific to these rela-   tions , yet further work is needed to explore the fea-   sibility of generation for other aspects of common-   sense and knowledge , beyond these relations , to   concepts like physical or temporal commonsense .   6 Related Work   Commonsense Knowledge Graphs ( CKG )   CKGs provide knowledge for commonsense rea-   soning . Some are manually constructed , e.g.4609A ( Sap et al . , 2019 ; Hwang et al . , 2021 ) .   ConceptNet ( Speer et al . , 2017 ) contains taxonomy   and physical commonsense , authored by humans   or compiled from such sources . Some CKGs are   automatically constructed : TransOMCS ( Zhang   et al . , 2020a ) extracts 18.48 M tuples from syntactic   parses and CausalBank ( Li et al . , 2020 ) extracts   314 M cause - effect pairs by pattern - matching . In   contrast , we generate commonsense .   Extracting Knowledge from LMs Past work   uses models for automatic knowledge graph com-   pletion ( Bosselut et al . , 2019 ; Hwang et al . , 2021 ;   Li et al . , 2020 ) . Yet , models are trained on existing   resources ; Ais generated without these .   Other works mine factual / commonsense knowl-   edge directly from off - the - shelf LMs ( Petroni et al . ,   2019 ; Davison et al . , 2019 ; Xiong et al . , 2020 ) , but   not resulting in the quality at scale of A.   Knowledge Distillation Other works use knowl-   edge distillation ( Hinton et al . , 2015 ) for genera-   tion . ( Sanh et al . , 2019 ) follow a label smoothing   formulation , while Kim and Rush ( 2016 ) follow a   similar formulation to us ( § 2.1 ) , use the mode of   the teacher distribution rather than sampling . Our   work is unique in distilling specific information   ( commonsense ) from a general language model .   Data Generation While manual dataset creation   is expensive and complex ( Schwartz et al . , 2017 ;   Agrawal et al . , 2018 ; Tsuchiya , 2018 ; Bras et al . ,   2020),crowdsourcing is the most popular method   for goal - oriented , high quality / coverage datasets .   Past automatic data mainly use extractive ap-   proaches , e.g. syntactic parsing ( Zhang et al . ,   2020a ) or pattern matching ( Li et al . , 2020 ) from   unstructured text ( Lehmann et al . , 2015 ; Buck et al . ,   2014 ) . These scale , but are noisy and limited in   format – A knowledge will not appear simply   in natural text . Some works explore automatic data   synthesis / expansion by finetuning LMs on existing   labeled data ( Anaby - Tavor et al . , 2020 ; Papaniko-   laou and Pierleoni , 2020 ; Kumar et al . , 2020 ; Yang   et al . , 2020 ) , but are limited by data quality .   7 Conclusions   We introduce symbolic knowledge distillation , a   machine – to – corpus – to – machine pipeline for com-   monsense that does not require human - authored   knowledge – instead , using machine generation .   Knowledge is transferred from a large , general   model to a compact commonsense model , througha commonsense corpus – yielding a commonsense   knowledge graph and model . Our resulting sym-   bolic knowledge graph has greater scale , diversity ,   and quality than human authoring . symbolic knowl-   edge distillation offers an alternative to human-   authored knowledge in commonsense research .   Acknowledgments   This work was funded in part by the Natural Sci-   ences and Engineering Research Council of Canada   ( NSERC ) ( funding reference number 401233309 ) ,   DARPA MCS program through NIWC Pacific   ( N66001 - 19 - 2 - 4031 ) , and the Allen Institute for   AI .   Ethical Considerations   One aspect of our work with the potential for ethi-   cal pitfalls is large - scale generation from pretrained   language models , in constructing A. Re-   cent work ( Bender et al . , 2021 ) has highlighted the   risks of models trained on massive text resources ,   as GPT-3 ( Brown et al . , 2020 ) is , which we use   for generation . Indeed , open generations from pre-   trained language models can often contain harmful ,   biased , or offensive aspects . We argue here that   this risk is largely mitigated in our work , mainly   due to the narrow and constrained nature of our   generations . The goal of our work is characterising   simple and generic anonymous situations , specifi-   cally in terms of commonsense causes and effects .   We ensure generations are focused on these top-   ics through careful prompting , which we found to   be quite effective at keeping these generations on-   topic . As such , the potential for harmful generation   is very low ; indeed , in a manual inspection of 100   generated examples , we found none that were sig-   nificant harmful , besides one that contained adult   content .   A related concern is the potential for large mod-   els and training sets to make automated oppression   or exploitation possible , for instance in surveillance   or generating fake news . As above , we argue that   the generic , commonsense nature of our data and   models makes this concern less relevant here . Our   data does not contain any information directly re-   lated to these harmful domains ( e.g. social media   or fake news generation ) . While our data may as-   sist machines in understanding basic situations , this   is unlikely to be useful for harmful models given   the simplicity of our data and still - flawed com-   monsense capabilities of even the most advanced4610models .   Finally , we note that we ensure fair and gener-   ous compensation for all human evaluators we hire   through Amazon Mechanical Turk . Based on our   estimates of time required per task , we ensure that   the effective pay rate is at least $ 15 per hour .   References461146124613A Human Evaluation Details   We conduct human evaluations on Amazon Me-   chanical Turk using the template of Figures 4,5 .   Workers are presented with A -style triples ,   replacing relations with natural language templates   ( e.g. HinderedBy becomes “ can be hindered by ” ) .   3 annotators rate each triple , with options for ac-   ceptability : “ always / often ” , “ sometimes / likely ” ,   “ farfetched / never ” , “ invalid ” , or “ too unfamiliar to   judge ” . The first two are considered “ accepted ” ,   the second two “ rejected ” and the final is “ no judge-   ment ” . For reporting acceptance rates , and training   a critic model , we only distinguish between “ ac-   cepted ” and not “ accepted ” .   Workers are compensated $ 0.17 per task ( i.e.   completing all questions in the evaluation template   Figures 4,5 ) . We estimate an upper bound of 30s to   complete a single task , which gives an hourly rate   of $ 20.4 . Workers are selected based on an Amazon   Mechanical Turk qualification , specifically filtering   for workers with high accuracy on past knowledge   base triple evaluations . We follow the same setup   for all evaluations , besides number of annotators .   This setup is shown to result in consistent and reli-   able annotations , with an inter - annotator agreement   given by Fleiss ’ kappa ( Fleiss , 1971 ) of 40.8 when   evaluating with 3 annotators , in § 3.4 .   B Using Alternate Models as Knowledge   Sources   One natural question that arises from the strong   performance of symbolic knowledge distillation is   whether other sources of knowledge ( i.e. language   models ) would similarly benefit from this method .   In this section , we particularly measure the capacity   of other language models to serve as the “ loose   teacher ” which generated the base knowledge of   the resulting corpus .   We expand our study beyond GPT-3 here ( the   model used in our work ) , to include 2 contempo-   rary large language models , GPT - J ( Wang and Ko-   matsuzaki , 2021 ) and T5 - 11B ( Lester et al . , 2021 )   finetuned for language modelling . For knowledge   generation ( verbalization ) we follow the same pro-   cedure as § 3 along with simple adjustments to im-   prove quality . We are investigating the effect of   the critic on knowledge precision here , so we also   include Ato probe the usefulness of auto-   matic filtering for human - authored knowledge .   For each knowledge source , we follow the hu-   man evaluation setup in § 3.4 to obtain quality an - notations of 2000 examples , with 1 annotation per   example . This follows a similar setup to § 4 – indeed ,   we are replicating the earlier critic experiments but   at a smaller scale ( 2000 annotations vs. 10000 )   to allow for more knowledge sources . For each   knowledge source , we randomly split into sizes of   1400/300/300 for train , dev , and test sets . We fol-   low § 4 to train a critic model for each knowledge   source .   We plot different thresholds ( % of corpus fil-   tered ) against the resulting precision ( proportion   of corpus that is judged to be “ valid ” knowledge )   in Figure 3 , and give numbers at various sizes   in Table 7 . One striking aspect is that a critic   model can raise the precision of any of these knowl-   edge sources to approximately 90 % while retaining   30 % of the original corpus size . While this dis-   cards a significant portion of the original generated   knowledge , it raises the exciting prospect of using   more cost - effective models at a large scale to gener-   ate strong commonsense corpora like A.   GPT - J and T5 - 11B can both be run locally by   researchers , unlike GPT-3 which uses a pay - per-   generation API . Thus , one can imagine producing   a large and high - quality corpus like Aat   a lower cost by instead generating a larger volume   of knowledge from such an accessible model , and   simply filtering to a greater extent .   Another interesting aspect is how the various   knowledge sources diverge . Under little to no criti-   cal filtering ( i.e. corpus size = 1.0 ) , the precision   of various knowledge sources is widely spread . Be-   fore applying a critic , quality of knowledge source   is very important . Indeed , precision is ordered   by cost of generation : human Ahas the   highest precision while being the most expensive ,   followed by GPT-3 ( used here ) which is pay - per-   generation , and finally the two publicly available   models . Another point of divergence is for extreme   filtering ( at approximately 20 % of the original cor-   pus size . All knowledge sources but GPT-3 plateau   at approximately 90 % accuracy , while GPT-3 rises   towards 100 % . Indeed , this supports our use of   GPT-3 in this work , as a high - quality automatic   knowledge source .   C Critic Model   We train binary classifiers ( critics ) for human ac-   ceptability using RoBERTa - Large ( Liu et al . , 2019 ) ,   fine - tuning all parameters , along with a 2 - layer   MLP on the [ CLF ] representation . We conduct4614Precision atCorpus Size ( % )   Knowledge Source 100 90 80 70 60 50 40 30 20 10   A 84.0 86.3 87.9 89.0 88.3 88.7 91.7 90.0 90.0 90.0   GPT - J 71.7 76.7 81.7 83.8 86.7 88.0 88.3 87.8 93.3 90.0   T5 - 11B 64.7 66.7 70.8 74.8 79.4 84.7 89.2 92.2 91.7 93.3   GPT-3 curie 79.3 81.5 85.0 86.2 88.3 90.7 91.7 90.0 98.3 100.0a small grid search on the validation set finding   batch size 128 , dropout .1 , and Adam ( Kingma   and Ba , 2015 ) learning rate 5e-6 to be effective .   We use early stopping and decay learning rate on   validation performance plateauing , to maximize   R@80 % on the validation set . We find RoBERTa   pretrained on MNLI ( Williams et al . , 2018 ) effec-   tive , outperforming other options . As well , we   substitute randomly - sampled names in for person   designations “ X”/“Y ” . We include as a baseline an   unsupervised filtration metric inspired by ( Davison   et al . , 2019 ): they propose a model estimate of   PMI to score mined commonsense triples . In our   case , we use Negative Log - Likelihood ( NLL ) and   token - mean - NLL from GPT-3 itself .   The validation precision / recall of our best per-   forming model , the baselines , and the in - optimal   hyperparameter configurations are given in Fig-   ure 6 . Once fixing our model , we applied it to the   test set ( also in Fig 6 ) , verifying that it generalizes   toAentries . Overall , our trained critic   model is more effective than the baselines in iden-   tifying high and low quality teacher generations at   all levels of precision and recall . This result demon-   strates that a small amount of human supervision   can consistently help to correct GPT-3 ’s mistakes .   D AGeneration Prompts   We include example prompts for all generations   we do , from Table 8 to 15 . Note that elements   of generation prompts are randomized for each   batch . For event generation , the few - shot examples   and order are randomly sampled from a seed set   of 100 high - quality examples from Ain   each batch . For inference generation , the natural   names used for PersonX and PersonY are randomly   sampled from a small predefined set of names.4615461646171 . Event : PersonX unwraps PersonY ’s hands   2 . Event : PersonX overcomes evil with good   3 . Event : PersonX is fed up with the present situation   4 . Event : PersonX breaks PersonX ’s back   5 . Event : PersonX calls no one   6 . Event : PersonX never gets angry   7 . Event : PersonX does not learn from PersonY   8 . Event : PersonX refuses to touch PersonY ’s hands   9 . Event : PersonX looks at flowers   10 . Event : PersonX unloads an atomic bomb   11 . Event:4618Next , how are people seen in each situation ? Examples :   Situation 1 : Devin bullies Jean .   Devin is seen as dominant .   Situation 2 : Jamie moves to another city .   Jamie is seen as adventurous .   Situation 3 : Sydney changes Ryan ’s mind .   Sydney is seen as influential .   Situation 4 : Lindsay writes a story .   Lindsay is seen as creative .   Situation 5 : Rowan covers Pat ’s expenses .   Rowan is seen as wealthy .   Situation 6 : Lee takes time off .   Lee is seen as carefree .   Situation 7 : Riley advises Noel .   Riley is seen as informed .   Situation 8 : Adrian bursts into tears .   Adrian is seen as depressed .   Situation 9 : Hunter deals with problems .   Hunter is seen as responsible .   Situation 10 : Sam follows Charlie .   Sam is seen as suspicious .   Situation 11 : Alex makes Chris wait .   Alex is seen as4619Next , what do situations make people do ? Examples :   Situation 1 : Devin gets a divorce .   As a result , Devin dates someone new .   Situation 2 : Jamie lifts weights .   As a result , Jamie has sore muscles .   Situation 3 : Sydney takes Ryan to a bar .   As a result , Sydney gets drunk .   Situation 4 : Lindsay decides to hire a tutor .   As a result , Lindsay gets better grades .   Situation 5 : Rowan buys Pat drinks .   As a result , Rowan is thanked by Pat .   Situation 6 : Lee hears bad news .   As a result , Lee begins to cry .   Situation 7 : Riley buys a chocolate bar .   As a result , Riley gets change .   Situation 8 : Adrian does a lot of work .   As a result , Adrian gets mental fatigue .   Situation 9 : Hunter attends a concert .   As a result , Hunter hears a new song .   Situation 10 : Sam gets the job done .   As a result , Sam gets more responsibilities .   Situation 11 : Alex makes Chris wait .   As a result , Alex4620For each situation , describe the intent . Examples :   Situation 1 : Devin gets the newspaper .   Devin intends to read the newspaper .   Situation 2 : Jamie works all night .   Jamie intends to meet a deadline .   Situation 3 : Sydney destroys Ryan .   Sydney intends to punish Ryan .   Situation 4 : Lindsay clears her mind .   Lindsay intends to be ready for a new task .   Situation 5 : Rowan wants to start a business .   Rowan intends to be self sufficient .   Situation 6 : Lee ensures Ali ’s safety .   Lee intends to be helpful .   Situation 7 : Riley buys lottery tickets .   Riley intends to become rich .   Situation 8 : Alex makes Chris wait .   Alex intends4621Next , we will discuss what people need for certain situations . Examples :   1 . Before Devin makes many new friends , Devin has to spend time with people .   2 . Before Jamie gets a date , Jamie has to ask someone out .   3 . Before Sydney changes Ryan ’s mind , Sydney has to think of an argument .   4 . Before Lindsay gets a job offer , Lindsay has to apply .   5 . Before Rowan takes a quick nap , Rowan has to lie down .   6 . Before Lee tries to kiss Ali , Lee has to approach Ali .   7 . Before Riley rides Noel ’s skateboard , Riley has to borrow it .   8 . Before Adrian eats the food , Adrian has to prepare a meal .   9 . Before Hunter watches Netflix , Hunter has to turn on the TV .   10 . Before Sam has a baby shower , Sam has to invite some friends .   11 . Before Alex makes Chris wait , Alex has4622Next , how do people feel in each situation ? Examples :   Situation 1 : Devin lives with Jean ’s family .   Devin feels loved .   Situation 2 : Jamie expects to win .   Jamie feels excited .   Situation 3 : Sydney comes home late .   Sydney feels tired .   Situation 4 : Lindsay sees dolphins .   Lindsay feels joyful .   Situation 5 : Rowan causes Pat anxiety .   Rowan feels guilty .   Situation 6 : Lee goes broke .   Lee feels embarrassed .   Situation 7 : Riley has a drink .   Riley feels refreshed .   Situation 8 : Adrian has a heart condition .   Adrian feels scared about their health .   Situation 9 : Hunter shaves Avery ’s hair .   Hunter feels helpful .   Situation 10 : Sam loses all of Charlie ’s money .   Sam feels horrible .   Situation 11 : Alex makes Chris wait .   Alex feels4623Next , what do people want in each situation ? Examples :   Situation 1 : Devin mows the lawn .   Devin wants to take a shower .   Situation 2 : Jamie is going to a party .   Jamie wants to take an Uber home .   Situation 3 : Sydney bleeds a lot .   Sydney wants to go to the ER .   Situation 4 : Lindsay works as a cashier .   Lindsay wants to find a better job .   Situation 5 : Rowan gets dirty .   Rowan wants to do a load of laundry .   Situation 6 : Lee stays up all night studying .   Lee wants to rest .   Situation 7 : Riley gets Noel ’s autograph .   Riley wants to tell some friends .   Situation 8 : Adrian sees Taylor ’s point .   Adrian wants to agree with Taylor .   Situation 9 : Hunter leaves Avery ’s bike .   Hunter wants to keep the bike safe .   Situation 10 : Sam wants a tattoo .   Sam wants to find a tattoo design .   Situation 11 : Alex makes Chris wait .   Alex wants4624Next , what can hinder each situation ? Examples :   Situation 1 : Devin makes a doctor ’s appointment ,   This is hindered if Devin ca n’t find the phone to call the doctor .   Situation 2 : Jamie rubs Wyatt ’s forehead ,   This is hindered if Jamie is afraid to touch Wyatt .   Situation 3 : Sydney eats peanut butter ,   This is hindered if Sydney is allergic to peanuts .   Situation 4 : Lindsay looks perfect ,   This is hindered if Lindsay ca n’t find any makeup .   Situation 5 : Rowan goes on a run ,   This is hindered if Rowan injures her knees .   Situation 6 : Lee takes Ali to the emergency room ,   This is hindered if Ali has no health insurance to pay for medical care .   Situation 7 : Riley spends time with Noel ’s family ,   This is hindered if Noel ’s family does n’t like spending time with Riley .   Situation 8 : Adrian moves from place to place ,   This is hindered if Adrian ca n’t afford to move .   Situation 9 : Hunter protests the government ,   This is hindered if Hunter is arrested .   Situation 10 : Sam has a huge fight ,   This is hindered if Sam does not like confrontation .   Situation 11 : Alex makes Chris wait ,   This is hindered if4625