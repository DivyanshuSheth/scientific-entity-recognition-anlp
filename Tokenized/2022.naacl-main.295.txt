  Chaitanya Agarwal , Vivek Gupta , Anoop Kunchukuttan , Manish ShrivastavaLTRC , IIIT Hyderabad;University of Utah;AI4Bharat;Microsoft   chaitanya.agarwal@research.iiit.ac.in ; vgupta@cs.utah.edu ;   ankunchu@microsoft.com ; m.shrivastava@iiit.ac.in   Abstract   Existing research on Tabular Natural Language   Inference ( TNLI ) exclusively examines the   task in a monolingual setting where the   tabular premise and hypothesis are in the   same language . However , due to the uneven   distribution of text resources on the web across   languages , it is common to have the tabular   premise in a high resource language and the   hypothesis in a low resource language . As   a result , we present the challenging task of   bilingual Tabular Natural Language Inference   ( bTNLI ) , in which the tabular premise and   a hypothesis over it are in two separate   languages . We construct EI - ITS : an   English - Indic bTNLI dataset by translating   the textual hypotheses of the English TNLI   dataset ITSinto eleven major Indian   languages . We thoroughly investigate how pre-   trained multilingual models learn and perform   onEI - ITS . Our study shows that the   performance on bTNLI can be close to its   monolingual counterpart , with translate - train ,   translate - test and unified - train being strongly   competitive baselines .   1 Introduction   Tabular Natural Language Inference ( TNLI ) is the   task of classifying whether a textual hypothesis is   an entailment , contradiction or a neutral extension   of the given tabular premise . The task requires a   broad range of reasoning abilities , including but   not limited to the ability to make lexical , spatio-   temporal , and semantic deductions . Recently   published datasets , TabFact ( Chen et al . , 2020b )   andITS(Gupta et al . , 2020 ) , have enabled   the examination of the TNLI task . Moreover ,   sophisticated models based on deep contextual   embeddings like BERT ( Devlin et al . , 2019 ) ,   RoBERTa ( Liu et al . , 2019 ) , etc . trained under   Figure 1 : Tabular premise followed by human written   hypotheses ( H1 , H2 , H3 ) . H1 is entailed entirely from   the premise , H2 is neither entailed nor contradictory , and   H3 is contradictory . H1 , H2 , and H3   are the transliterations of Hindi translations of the   former , released as a part of our EI - ITSdataset .   supervision on heuristic adaptations of these   datasets perform adequately .   Typically , and to the authors ’ best knowledge ,   fact verification tasks , specifically TNLI , have been   examined only in a monolingual setting wherein ,   the tabular premise and the textual hypothesis   are in the same language . However , many semi-   structured / tabular data sources exist only in English   but require verification of hypotheses over those   data sources in other languages , as discussed in   § 2 . Therefore , we examine a modified tabular NLI   task by introducing bilinguality within the premise4018and hypothesis pair . To understand this modified   task , consider the example in Figure 1 . The table   presented in the figure has been extracted from   the English Wikipedia article on Joe Strummer ,   a well known musician . Following which , are   transliterated hypotheses in Hindi ( hi ) ( and their   English ( en ) translation ) which are related to the   information presented in the given table . We   show transliterated hypotheses only for the ease   of comprehension . We use native scripts for   each language in the EI - ITSdataset . For   bilingual tabular NLI , a reasoning model should be   able to predict the inference label entail for H1 ,   neutral for H2and contradict for H3given the   English table as the primary context . In summary ,   our contributions are as follows :   •We introduce the task of bilingual tabular NLI   ( bTNLI ) wherein the tabular premise is in a high   resource language , while the textual hypothesis is   in a low resource language . This is a practical ,   real world setting for fact verification on semi-   structured tabular data which is further illustrated   in § 2 .   •We create EI - ITS , a dataset consisting   of machine translated hypotheses in 11 Indian   languages , while retaining the English tabular   premises from the ITSdataset . Through   extensive studies shown in § 3 , we confirm that   EI - ITSis of good quality , and preserves   properties important to study the bTNLI task .   •We explore several multilingual models for   the bTNLI task , and establish strong baselines   and share findings about their performance   across multilingual models , languages , train-   eval techniques , tabular reasoning categories ,   adversarial test splits , and both datasets   ( ITS and EI - ITS ) .   Overall , EI - ITSdataset and our proposed   train - eval strategies enable thorough examination   of the challenging task of bTNLI . Furthermore ,   the former aslo serve as a quality benchmark   for evaluating the robustness of multilingual   models . The dataset and the associated scripts ,   are available at https://enindicinfotabs .   github.io .   2 Motivation   Why Tabular NLI ? Tabular data is termed as   semi - structured as it is neither truly unstructureddata like raw text , nor is it entirely structured like a   database . Although semi - structured data is based   on a structured scaffold , the content can be free-   form text with variable length and type . Moreover ,   unlike a database , there is no homogeneity across   various data points in a shared context . Such   structural ambiguity imposes a significant cognitive   load while reasoning about it . However , such data   is ubiquitous in the real world ( e.g. web pages , fact   sheets , information tables ) and we frequently make   inferences from it .   Chen et al . ( 2020b ) argue that reasoning about   semi - structured data is broadly two - fold in nature .   It consists of ( a. ) Linguistic Reasoning : a semantic   deconstruction of the semi - structured data ( b. ) ,   and Symbolic Reasoning : a symbolic execution   on the tabular structure . For instance , Hin Figure   1 requires linguistic reasoning over the phrase   “ became a guitar player ” from the “ Occupation ” ,   and the “ Instruments ” rows of the concerned   table . Hrequires symbolic reasoning in the   form of conditional and arithmetic operations on   the “ Labels ” and “ Instruments ” rows . Whereas ,   Hrequires a combination of the two types of   reasoning . Such interwoven reasoning criteria   makes it challenging to model Tabular NLI task .   Why Indic Languages ? Indian society is largely   multilingual and consists of 122 major and 1599   other languages and dialects spanning 6language   families with over 1.3billion native speakers . Out   of these , 30languages have more than 1million   native speakers each and over 1billion speakers   cumulatively . Moreover , India has the second   largest online presence with over 749 million   internet users and is expected to grow to over   1.5billion users by 2040 . So , development of   competent reasoning models for the Indic context   is essential .   However , due to unfair linguistic bias on the   web ( Miquel - Ribé and Laniado , 2020 ; Joshi et al . ,   2020 ) , there is a disproportionate distribution   of text resources for Indian languages . Indian   languages have a limited number of internet   resources . Thus , they are often known as low web   resource languages ( LRL ) ( Khemchandani et al . ,   2021 ) . For instance , Wikipedia entries in Hindi are   just2%of those in English , and Wikipedia entries   in Assamese and Oriya are 7times lesser than those   in Hindi . This implies that a significant fraction of4019articles and sometime even complete categories are   discussed only in the English language Wikipedia   ( Bao et al . , 2012 ) .   Although , efforts have been made to bridge this   gap ( Adar et al . , 2009 ; Kumaran et al . , 2010 ) ,   there still exist several limitations : ( a. ) table   extraction for an article across languages is a   challenge due to absence of Wikipedia page links ,   their infobox tables or important keys of tables ,   ( b. ) even if tabular data exists , infobox tables in   Indian languages are not updated as regularly as   their English equivalents ( Minhas et al . , 2022 )   which leaves us with outdated and untrustworthy   tabular data for inference , ( c. ) and lastly , table   translation while maintaining the intent , context ,   and the same quality of the source English language   is difficult . Often , accurate translation requires the   distinction of a language specific domain expert .   Due to above reasons , tabular data is mostly absent   from Indic Wikipedia articles .   Thus , fact verification in a bilingual setting   wherein , the premise is in English and the   claim / hypothesis is in an Indic language , is of   great significance . Moreover , recent advances   in multilingual language models ( Khanuja et al . ,   2021a ; Kunchukuttan , 2020 ) , datasets ( Roark et al . ,   2020 ; Ramesh et al . , 2022 ) , and translation systems   ( Ramesh et al . , 2022 ) for Indian languages have   enabled quality examination of several Indic NLU   tasks which serves as additional motivation to   evaluate the task of bTNLI for Indic languages   before other low resource languages .   3EI - ITSDataset   EI - ITSis an English - Indic bTNLI   extension of ITS(Gupta et al . , 2020 ) ,   an English TNLI dataset . ITSconsists   of23,738 pairs of tabular premises and textual   hypotheses . The hypotheses are human written   short assertions with an accompanying NLI label ,   and the tabular premises are based on 2,540   Wikipedia infoboxes from 12diverse categories .   Moreover , it consists of additional adversarial test   sets apart from αwhich is the standard test set   and is lexically and topically similar to the train   set -αis the lexically adversarial test set which   maintains topical similarity and αis the topically   adversarial test set . The dev and test sets ( α , α ,   α ) cumulatively consist of 7200 table - hypothesis   pairs equally splits on all four sets .   EI - IT extends it by providing machinetranslated hypotheses in 11major Indic languages   namely Assamese ( as ) , Bengali ( bn ) , Gujarati ( gu ) ,   Hindi ( hi ) , Kannada ( kn ) , Malayalam ( ml ) , Marathi   ( mr ) , Odia ( or ) , Punjabi ( pa ) , Tamil ( ta ) , and   Telugu ( te ) for each tabular premise . In this section ,   we discuss the EI - ITSconstruction and   verification .   3.1 EI - ITSConstruction   To construct EI - ITS , we machine translated   the English hypotheses provided in ITS   to11major Indian languages as described   earlier . We use IndicTrans ( Ramesh et al . ,   2022 ) , an open - sourced state - of - the - art Indic   NMT model . IndicTrans is trained on the   Samanantar dataset ( Ramesh et al . , 2022 ) , which   is the largest publicly available parallel corpus   for Indic languages . Moreover , it outperforms   ( a ) commercial NMT systems like Google-   Translateand Bing Microsoft Translator , and   ( b ) open - source multilingual models like OPUS-   MT ( Tiedemann and Thottingal , 2020 ) , mBART50   ( Liu et al . , 2020 ) and mT5 ( Xue et al . , 2021 ) .   3.2 EI - ITSVerification .   Given the absence of Indic reference data , it   becomes challenging to measure the quality of the   translations , and subsequently , of EI - ITS .   In this section , we describe our robust quality   estimation approach to validate EI - ITS .   Automatic Evaluation . We use BERTScore   ( Zhang * et al . , 2020 ) , an automatic scoring   metric for sentence similarity , between the source   and back - translated English sentences . We use   IndicTrans to generate Indic to English back-   translated data .   BERTScore is known to correlate better with   human judgment at the sentence level ( Zhang *   et al . , 2020 ) compared to conventionally used MT   evaluation metrics like BLEU ( Papineni et al . ,   2002 ) and ROUGE ( Lin , 2004 ) . BERTScore   calculates word level semantic similarity whereas   the conventional MT metrics focus on word overlap .   The results are presented in Table 1 . We notice high   semantic similarity scores for all the languages .   However , when we analyse the examples with   low scores , we note that the scores are almost   always low due to the error added during the back-   translation phase . The back - translation introduces4020errors due to incorrect transliteration of Named   Entities . Consider the following example :   •Femme aux Bras Croisés is open for public   viewing .   •Back - translated : The Ox Brass Crossox is open   to the public   •Hindi Translation(Transliterated ): fem auksa br ¯as   kroisaiksa janat ¯a ke lie khul ¯a hai   The Hindi translation of the original sentence   is perfect , however , the named entity “ Femme aux   Bras Croisés ” when back - translated becomes “ Ox   Brass Crossox ” and yields a low BERTScore of   0.86 . This is broadly identified as qualitative   feedback for most of the sentences with low   scores across all the languages . Around 20 %   of the examples yield a BERTScore of 1.0and   are deemed perfect translations when reviewed by   native speakers .   Human Evaluation . Broadly , we follow the   guidelines recommended in ( Agirre et al . , 2016 )   to conduct human evaluation . We ( a. ) diversely   sample source - translation pairs in each language ,   ( b. ) prepare a common Direct Assessment ( Graham   et al . , 2013 ) scoring strategy , and ( c. ) get the   sampled data evaluated on the basis of that strategy .   Diverse Sampling . We sample 50diverse   hypotheses from the dev split of EI - ITSfor   each Indic language . Using the k - DPP algorithm   ( Kulesza and Taskar , 2011 ) over the mBERT   sentence representations , we ’re able to achieve   syntactically and semantically diverse samples   spanning the different table categories .   Direct Assessment . We adopt the human evaluation   strategy for low resource machine translation laid   out in ( Guzmán et al . , 2019 ) . We ask native Indic   language speakers proficient in English to score   a source - translation pair from 0 - 100 . The score   highlights the perceived translation quality of the   source - translation pair . For each language , we get   the samples annotated by two different annotators .   In Table 1 , we report the average scores for   each language along with the Pearson correlation   coefficient ( r ) as a measure for inter - rater reliability .   For more details on human evaluation strategy refer   to Appendix § C.   Discussion . We report our evaluation results   in Table 1 . Automatic evaluation and our   corresponding analysis on it shows that EI-   ITSconsists of fluent , semantically accurate   translations across all Indic languages . Moreover ,   we note competitive Direct Assessment scores   for each language , and a positive rvalue which   indicates that the native speakers agree on the good   quality of EI - ITS .   4 Experimental Pipeline   We design our experimental pipeline along the lines   of the research question : How well do existing pre-   trained multlilingual language models perform on   the bTNLI task ? In this section , we propose various   modeling strategies and examine how they might   address the challenges and nuances of the proposed   inference bTNLI task .   4.1 Table Representations   It is necessary to linearize semi - structured tabular   data into a textual premise in order to reduce the   task of Tabular Inferencing to a standard NLI   task for which existing state - of - the - art language   models can be adapted directly . We use and   compare the previously proposed linearization   methods ( a. ) Better Paragraph Representations   ( BPR ) ( Neeraja et al . , 2021 ) , ( b. ) and Premise as   Structure - TabFact ( Chen et al . , 2020b ; Gupta   et al . , 2020 ) ( cf . Appendix § A ) . Henceforth , by   premise , we refer to the linearized representation   of the tabular premise i.e. the infobox table.4021   4.2 Multilingual Models   Owing to the multilingual setting of this task , we   utilise pre - trained multilingual models to encode   the linearized English tabular premise along with   the Indic hypothesis into contextual representations   for classification . We consider two kinds of pre-   trained multilingual models ( a. ) Indic Specific   which includes IndicBERT and MuRIL due to their   indic specific pre - training , and ( b. ) Generic which   includes mBERT and XLM - Roberta due to their   pre training in more than hundred languages . For   more details refer to Appendix § B.   4.3 Training and Evaluation Strategies   In order to examine the inter - woven relationships   among the 11languages , and the corresponding   impact on multilingual models ’ performance , we   design a set of train - eval strategies for this task . Translate - Train : We fine - tune and evaluate the   models on E - Ipremise - hypothesis pairs where   Iis one of the 11Indic languages . This baseline   evaluates the performance of the multilingual   models on EI - ITSwhen fine - tuned on Indic   hypotheses . We also evaluated these models across   all languages i.e. cross lingual zero - shot setting   Translate - Train - X .   Translate - Test : We fine - tune the multilingual   models on E - Epremise - hypothesis pairs from   theITSdataset and evaluate on E - I   premise - hypothesis pairs . This baseline evaluates   the Zero - shot Cross - Lingual Transfer ability of   the reasoning models from ITStoEI-   ITS .   Bilingual - Train : We fine - tune the multilingual   models on both E - Eand E - Ipremise-   hypothesis pairs , and evaluate on E - Ipremise-4022hypothesis pairs . This baseline evaluates whether   addition of English hypotheses while fine - tuning   aids the performance of the multilingual models   prepared in Translate - Train . We also evaluated   these models across all languages i.e. cross lingual   zero - shot setting Bilingual - Train - X .   Multilingual - Train : We fine - tune the multilingual   models on all available training data across all Indic   languages and the English language . We evaluate   the models on E - Ipremise - hypothesis pairs on   each 11 Indic languages . This baseline assesses   if fine - tuning on several languages to produce a   unified multilingual model improves performance .   EnTranslate - Test : We fine - tune the multilingual   models on E - Epremise - hypothesis pairs from   ITSand evaluate on E - EIpremise-   hypothesis pairs where EIrepresents Ito   Eback - translated hypotheses . This approach   evaluate the translate then test baseline on the EI-   ITS dataset .   5 Results and Analysis   In this section , we discuss and analyse the results   obtained on conducting the experiments as per   the various strategies laid out in § 4 . We present   the results in Table 2 for each experiment on the   αtest set using the BPR linearization algorithm .   The values represent classification accuracy . We   analyze the findings thoroughly across multilingual   models , languages , train - eval techniques , tabular   reasoning categories , adversarial test splits , and   both datasets ( ITS and EI - ITS ) .   5.1 Across Multilingual Models   We observe that MuRIL performs best across all   languages and experiments except EnTranslate-   Test , beating IndicBERT and mBERT . MuRIL ’s   superior performance can be justified on the   grounds of ( a ) the large size of the hidden layers ,   ( b ) Indic specific pre - training data , and ( c ) Indic   specific pre - training objectives ( Khanuja et al . ,   2021a ) . MuRIL ’s architecture consists of 237 M   parameters , compared to mBERT ’s 167 M and   IndicBERT ’s 33 M , which makes it extremely   competitive on any Indic NLU task . IndicBERT ’s   relatively small size explains why it performs the   worst , even though it is pre - trained on Indic specific   data . mBERT comes in a close second to MuRIL ,   failing to perform adequately only on Odia ( or ) .   mBERT is n’t pre - trained on Assamese ( as ) or Odia   which justifies its extremely low performance onOdia . However , we note competitive results on   the Assamese language . This could be attributed   to the fact that Assamese is closely related to   Bengali ( bn ) linguistically . They both share   the Bengali - Assamese script and are mutually   intelligible ( Khemchandani et al . , 2021 ) .   mBERT ’s performance gets boosted in   EnTranslate - Test as mBERT is pre - trained on a   significant amount of English data which makes   it extremely competitive in modeling English   NLU tasks . MuRIL performs similarly even   though it is trained on lesser amount of English   data . This could be due to Indic artifacts like   sentence structure and inadequately transliterated   named entities being present in the back - translated   sentences which MuRIL has been trained to handle   better than mBERT .   5.2 Across Languages   We observe that the models perform best on   Hindi ( hi ) and Bengali . This is expected as   they are high resource languages in the Indic   context . Additionally , as explained in § 5.1 , we   note that pre - training or fine - tuning on Bengali   aids the performance on Assamese due to their   high degree of relatedness . Table 3 shows the   measure of agreement across the languages . We   note that almost all languages agree on 55 % of   the predictions on the dev set and the αtest   set . This reduces to 38 % on the αtest set .   This indicates that for a majority of examples   from the non - adversarial test sets , MuRIL   performs uniformly across languages . However ,   its performance across languages starts varying   more on the adversarial test sets ( αandα ) .   5.3 Train - Eval Strategies   Translate - Train ’s results show that the multilingual   models converge and perform adequately when   fine - tuned on EI - ITS . Moreover , when   fine - tuned along with English data - as described4023   in Bilingual - Train - mBERT and IndicBERT   perform marginally better while MuRIL does n’t   report a change in performance . MuRIL ,   when fine - tuned on all languages as described   in Multilingual - Train , performs best on EI-   ITSand forms the benchmark for this   task . mBERT and IndicBERT , however , perform   worse on Multilingual - Train when compared   to Bilingual - Train . This indicates that these   models fail to generalise their reasoning ability   across all languages and are n’t as multilingually   robust as MuRIL . The results on Translate - Test   are the lowest across all train / eval strategies   which indicates a poor Zero - shot Cross - Lingual   Transfer from ITStoEI - ITS .   However , the performance of MuRIL on Translate-   Test is comparable with its performance on   Translate - Train unlike mBERT and IndicBERT .   This indicates that MuRIL can generalize well   across English and Indic languages which are   linguistically distinct .   Translate - Train - X and Bilingual - Train - X   evaluate the average Cross - Lingual Transfer   performance of the models trained in Translate-   Train and Bilingual - Train . We observe higher   performance in Bilingual - Train - X over   Translate - Train - X which indicates that addition   of English training data aids the Cross Lingual   Transfer from one Indic language to another .   Moreover , the average performance of MuRIL   on Bilingual - Train - X is comparable to that on   Translate - Train which suggests that MuRIL   robustly generalises across Indic languages .   Both , Bilingual - Train - X and Translate - Train - X   perform better than Translate - Test due to high   language relatedness among Indic languages   when compared with English . The results on   EnTranslate - Test are extremely promising for   both MuRIL and mBERT . Their performance is   very close to that of the best performing model ,   MuRIL , on Translate - Train . This indicates that   back - translation does n’t lead to a significant loss   in information required for the bTNLI task .   5.4 Tabular Reasoning Categories   We conduct a fine - grained analysis on how our   best model , MuRIL ( Multilingual - Train ) , performs   on various reasoning categories . We present the   results in Figure 2 for Hindi and Odiya . We   observe that MuRIL performs similarly on EI-   ITSas RoBERTa does on ITS   for entity type , named entity , negation , numerical ,   quantification and simple lookup reasoning types .   Additionally , MuRIL performs better for the co-   reference resolution reasoning type . This is broadly4024   observed across all the Indic languages . Both   RoBERTa and MuRIL perform poorly for   knowledge and common sense , multi - row , co-   reference , and temporal reasoning types .   5.5 Across Adverserial Test Splits   The results for the other evaluation sets αand   αare provided in Appendix § E. Across all   the experiments , we note that the fine - tuned   models perform best on α , followed by α   andαrespectively . Moreover , we note that   on most baselines , the average performance   of a fine - tuned model drops by roughly 10 %   when tested on αorα . This is similar to   the observations reported on ITS(Neeraja   et al . , 2021 ) and presented in Table 4 . Low   performance of the multilingual models on the   αtest set of EI - ITSindicates that   ( a. ) multilingual models learn shallow lexical   features to make inferences on EI - ITSjust   like the monolingual models do on ITS ,   ( b. ) and IndicTrans carefully captures the lexical   adversity in the αtest set of ITS.This   commends the ability of IndicTrans to handle   lexical nuances . Low performance on αtest set   ofEI - ITSsuggests that the multilingual   models learn categorical features and perform   adversely when evaluated on unseen category .   5.6 EI - ITSv / sITS   Table 4 reports the human benchmarks and the   baselines with the BPR linearization algorithm   on each validation set in ITS . We observe   that the baselines on EI - ITSare within an   absolute margin of 10 % when compared to those   on ITS.This suggests that EI - ITS   is more challenging than ITSwhich wasexpected due to the presence of ( a. ) bilinguality   within the premise - hypothesis pair , and ( b. ) the   low resource nature of Indic languages .   Figure 3 reports the consistency of predictions   of MuRIL on the αtest set of Hindi EI-   ITSwhen compared against that of   RoBERTa on the αtest set of ITS .   We observe that MuRIL behaves noticeably   different than RoBERTa . MuRIL disagrees   with RoBERTa on47%of examples with the   Contradiction and Entailment labels . However , for   Neutral labels , it only disagrees on around 36 %   of the examples . Moreover , from our discussion   in § 5.4 , we observe that MuRIL outperforms   RoBERTa on certain reasoning categories .   However , the models fine - tuned on EI-   ITSbroadly mimic the performance of   RoBERTa onITS . Figure 4 presents   the confusion matrix of MuRIL ’s predictions on   theαtest set of Hindi . We observe a similar   distribution across all Indic languages . As noted in   Gupta et al . ( 2020 ) , MuRIL also tends to predict   Neutral hypotheses with the highest confidence   as they mostly contain out of table or subjective   information terms . Moreover , both models confuse   Entailment with Contradiction inference label and   vice - versa . We observe that the model predictions   onEI - ITSis similar to RoBERTa   predictions on ITS .   6 Further Discussion   EI - ITSis the first Tabular NLI dataset   in the Indic context which enables preliminary   studies in this field . Moreover , it introduces   bilinguality for fact verification scenarios which   is of huge significance in low resource contexts .   It motivates the development of cross - lingual   reasoning models , and helps in evaluation of   robustness of multilingual models . For instance ,   our experiments on EI - ITSclearly indicate4025that MuRIL is a significantly more robust   multilingual model when compared to mBERT as   it is able to generalize its reasoning ability across   all Indic languages .   Although , we explain how machine translation   does n’t affect the semantics of the hypotheses , it   does come with a few challenges . We identified   a few instances wherein the IndicTrans model   translates named entities , instead of transliterating   them . This is observed only , but not always , when   a named entity has an English dictionary word in   it . For instance , “ Death Proof ” , name of a movie ,   gets translated and not transliterated in two out   of nine hypotheses containing the phrase . This is   mostly observed in the Movies category . However ,   this does n’t affect our reasoning models and they   perform on par on this category when compared   with RoBERTa ’s performance on ITS .   This is so because such translations when shallow   parsed indicate that the translated entity still acts   as the Noun Phrase in the sentence . This helps the   translation , though technically imperfect , retain the   intended semantic structure .   7 Related Work   Tabular Reasoning . Tabular NLI has been of   keen interest recently . Datasets like TabFact ( Chen   et al . , 2020b ) , ITS(Gupta et al . , 2020 )   were the first resources on TNLI and they enabled   a fine - grained examination of the task . Beyond   NLI , there has been a thorough examination of   various other NLP tasks on semi - structured data .   For instance , question answering ( Abbas et al . ,   2016 ; Chen et al . , 2020c ; Zayats et al . , 2021 ; Oguz   et al . , 2020 ; Chen et al . , 2021 , and others ) , semantic   parsing and retrieval ( Krishnamurthy et al . , 2017 ;   Sun et al . , 2016 ; Pasupat and Liang , 2015 ; Lin   et al . , 2020 , and others ) , tabular probing ( Gupta   et al . , 2021 ) , generative tasks including table - to-   text ( Parikh et al . , 2020 ; Nan et al . , 2021 ; Yoran   et al . , 2021 ; Chen et al . , 2020a , d , and others ) . Other   works have explored creating task - independent   representations for Wikipedia infoboxes ( Herzig   et al . , 2020 ; Yin et al . , 2020 ; Zhang et al . , 2020 ; Iida   et al . , 2021 ; Pramanick and Bhattacharya , 2021 ;   Glass et al . , 2021 , and others ) , and boosting tabular   reasoning by pre - training and external knowledge   incorporation ( Neeraja et al . , 2021 ; Varun et al . ,   2022 , and others ) .   Multilingual Models . Multilingual , and   specifically Cross - Lingual transfer ( Deshpandeet al . , 2021 ; Patil et al . , 2022 , and other ) , has   been widely discussed in the context of low   resource languages . Several datasets ( Conneau   et al . , 2018 ; Yang et al . , 2019 ; Ponti et al . , 2020 ;   Artetxe et al . , 2020 ; Nivre et al . , 2016 ; Lewis et al . ,   2021 , and others ) , benchmarks and leaderboards   ( Hu et al . , 2020 ; Liang et al . , 2020 ; Ruder et al . ,   2021 ; Khanuja et al . , 2021b , and others ) , and   evaluation frameworks ( Tarunesh et al . , 2021 ; K   et al . , 2021 ; Srinivasan et al . , 2021 ) have emerged   which focus entirely on evaluation of multilingual   NLU . Further , multilingual language models   have been developed for ( a. ) Natural Language   Understanding ( Devlin et al . , 2019 ; Conneau and   Lample , 2019 ; Conneau et al . , 2020 ; Chi et al . ,   2021 ; Chung et al . , 2021 , and others ) , ( b. ) and   Natural Language Generation ( Xue et al . , 2021 ;   Fan et al . , 2021 , and others ) .   Indic Resources . Indic NLP , recently , has seen   a recent surge in the number of datasets ( Ramesh   et al . , 2022 ; Roark et al . , 2020 ; Haddow and Kirefu ,   2020a ; Abadji et al . , 2022 ; Kolluru et al . , 2021 , and   others ) , multilingual models ( Dabre et al . , 2021 ;   Kakwani et al . , 2020 ; Khanuja et al . , 2021a , and   others ) , toolkits ( Arora , 2020 ; Bhat et al . , 2015 ;   Jain et al . , 2020 , and others ) , translation systems   ( Ramesh et al . , 2022 ) , and dedicated benchmarks   for evaluation ( Kakwani et al . , 2020 ; Krishna et al . ,   2021 ) . This has enabled the Indian NLP research   community to construct competent models for a   variety of challenging NLP tasks .   8 Conclusion   We motivate and introduce the bilingual tabular   NLI for fact verification tasks , and release EI-   ITS- a first of its kind tabular NLI dataset   for making inferences in 11 Indic languages over   English tabular data . Our robust quality estimation   experiments show that the machine translated   datasets closely preserve the semantics of the   source and are fluent . We show that pre - trained   multilingual models find this task challenging ,   however , still perform close to the benchmarks on   ITSwith Translate - test and Translate - train   providing good performance . The analysis also   shows the similarity of inference capabilities across   languages . The dataset offers immense potential as   it opens up avenues in ( a ) multilingual tabular NLI ,   ( b ) bilingual claim verification , ( c ) and evaluation   of multilingual models.40269 Ethical Considerations   In terms of demographic and socioeconomic   characteristics , we attempted to establish a   balanced , bias - free dataset . The EI - ITS   dataset is derived from the ITSdataset ,   which is devoid of bias . The only possible source   of prejudice can be the translation pipeline . Our   qualitative analysis indicates that translation quality   is reasonably good and there are n’t any observable   biases like gender in the translation . The dataset is   intended and useful for studying language model   representations in a cross - lingual and structured   data setting . The paper points out that low-   resource languages can benefit from reasoning   over structured data in other languages . This is   a relatively new research topic and further work   will help understand limitations as well as uncover   new directions . Hence , we recommend the use of   this dataset at this point exclusively for scholarly ,   non - commercial purposes .   Acknowledgement   We thank members of the Utah NLP group for   their valuable insights and suggestions at various   stages of the project ; and reviewers their helpful   comments . The authors also thank Manila Devaraj ,   Arka Singha , Anagh Chattopadhyay , Maitrey   Mehta , Souvik Banerjee , Rupambara Padhi , Jayant   Duneja , Nithila Prakash , Tanvi Kamble , Anirudh   Palutla for helping with annotations for quality   estimation . Additionally , we appreciate the   inputs provided by Vivek Srikumar and Ellen   Riloff . Vivek Gupta acknowledges support from   Bloomberg ’s Data Science Ph.D. Fellowship .   References4027402840294030   A Details : Table Representation   1.Premise as Paragraph : ( Chen et al . ,   2020b ) , ( Gupta et al . , 2020 ) employ universal   templates to construct close to natural   language sentences for isolated cells in a4031   row , and then , concatenate them to obtain   a single paragraph representation . ( Gupta   et al . , 2020 ) suggest constructing sentences   of the form " The koftisv"for a cell   having key k , value vin a table with title   t. E.g. in figure 1 for the row Born   the premise sentence would be " The born   ofJoe Strummer is21 August 1952   ( 1952 - 08 - 21 ) Ankara , Turkey "   However , ( Neeraja et al . , 2021 ) identify   that such templates can often lead to   ungrammatical sentences and propose the   Better Paragraph Representation ( BPR )   approach . BPR utilises type specific templates   based on the entity type of a key , and   the overall category of the table itself   resulting in grammatical sentences . ( Neeraja   et al . , 2021 ) note a significant increase in   performance while employing BPR over the   universal template . We adopt BPR as one   of our representation approaches . E.g. for   same Born key in figure 1 the premise   sentence with BPR representation would be   " Joe Strummer wasborn onAugust   21 , 1952 ( 1952 - 08 - 21 ) atAnkara ,   Turkey "   2.Premise as Structure : Unlike the natural   language like Premise as Paragraph   representations , here , we try to represent the   row as structural text as proposed by ( Chen   et al . , 2020b ) . Every isolated cell in a row is   represented as " k : v"where kis the key , and   vis the value of the cell . A row ’s structural   representation is a semi - colon " ; " separated   sequence of the structural representations   of all the isolated cells in that row . E.g. for   the same Born key in figure 1 the premise   sentence will be represented as " Born :   August 21 , 1952 ( 1952 - 08 - 21 ) ,   Ankara , Turkey " B Details : Multilingual Models   Indic Specific : This class of multilingual models   are pre - trained entirely on Indic language data   along with English . We use MuRIL Base   ( Khanuja et al . , 2021a ) , and IndicBERT ( kak )   pre - trained multilingual models . MuRIL is a   BERT ( Devlin et al . , 2019 ) based model trained   with Masked Language Modeling ( Taylor , 1953 )   andTranslation Language Modeling ( CONNEAU   and Lample , 2019 ) objectives . It is trained   on ( a. ) Common Crawl OSCAR corpusand   Wikipediamonolingual data for 16 Indic   languages along with the English language ,   ( b. ) PMIndia ( Haddow and Kirefu , 2020b ) along   with other in - house parallel corpora , ( c. ) and   the Dakshina Dataset ( Roark et al . , 2020 ) along   with other parallel in - house transliterated corpora .   IndicBERT is an ALBERT ( Lan et al . , 2019 ) based   model trained on IndicCorp ( kak ) .   Generic : This class of multilingual models are   pre - trained on a wide array of languages from   around the world . We use mBERT - cased ( Devlin   et al . , 2019 ) and XLM - RoBERTa ( con ) pre - trained   multilingual models .   C Human Evaluation Strategy   We requested our colleagues who are native   speakers and are proficient in English to help us   with this task while disclosing the intentions . We   provide them with instructions adopted from the   Direct Assessment ( Graham et al . , 2013 ) strategy   for low resource machine translation in ( Guzmán   et al . , 2019 ) . We sample 50 pairs of source ,   translation pairs and ask the annotators to provide   a continuous score between 0 to 100 . 0–10   range represents a translation that is completely   incorrect and inaccurate . 70–90 range represents a   translation that closely preserves the meaning of the   source sentence while the 90–100 range represents   a perfect translation.4032D Model Hyper - Parameters   Table 5 reports the hyper - parameters used for fine-   tuning the multilingual models on EI - ITS .   We use the Huggingface Transformerslibrary   to script these experiments . We were unable to   successfully converge XLM - RoBERTa in multiple   runs spanning a distinctive set of hyper - parameters .   Figure 5 shows the loss plots for XLM - RoBERTa   and mBERT when fine - tuned on EI - ITS .   It is distinctively visible that XLM - RoBERTa   is unable to converge on EI - ITSon a   significant amount of steps unlike mBERT .   Fine - Tuning Settings . We follow the   conventionally used pipeline for fine - tuning   BERT for Sequence Classification ( Jiang and   de Marneffe , 2019 ) . We concatenate the premise   and the hypothesis strings using a [ SEP ] token   in between them , prepend this sequence with a   [ CLS ] token , tokenize this sequence using the   pre - trained tokenizer for the respective model ,   and provide the obtained sequence as input to   the pre - trained model . We attach a three - way   classification head with cross - entropy loss on top   of the pooled output obtained from the previous   step . With an initial learning rate of 5e-05with   AdamW optimizer ( Loshchilov and Hutter , 2018 ) ,   we fine - tune each model on 4 1080 Ti GPUs with   a batch size of 32 per GPU over 10 epochs . E Performance on the αandα   Adversarial Sets   Tables 6 and 7 report the results for the adverserial   test sets αandαrespectively using the BPR   linearization method .   F Zero Shot Cross - Lingual Transfer   Tables 8 and 9 report the performance of MuRIL on   Translate - Train - X and Bilingual - Train - X. We note   that models trained on linguistically closer pairs   of languages are able to admirably transfer their   performance to each other . Notably , Assamese   ( ‘ as ’ ) and Bengali ( ‘ bn ’ ) being immensely closely   related , support this hypothesis . Moreover , we   note the same for closely related Indo - European   languages Bengali , Hindi , Gujarati ( ‘ gu ’ ) , and   Marathi ( ‘ mr ’ ) . Models trained on these languages   distinctively transfer their performance better   on each other compared to languages from   the Dravidian language family - Malayalam   ( ‘ ml ’ ) , Telugu ( ‘ te ’ ) , Tamil ( ‘ ta ’ ) , Kannada ( ‘ kn ’ ) .   Dravidian languages are not as closely related due   to differences in scripts and sentence structures   which is observed in the results as well.40334034403540364037