  Mengze Li , Tianbao Wang , Haoyu Zhang , Shengyu Zhang , Zhou Zhao ,   Jiaxu Miao , Wenqiao Zhang , Wenming Tan , Jin Wang , Peng Wang ,   Shiliang Pu , Fei WuZhejiang UniversityShanghai Institute for Advanced Study of Zhejiang UniversityShanghai AI LaboratoryHikvision Research InstituteNorthwestern Polytechnical University   Abstract   Natural language spatial video grounding aims   to detect the relevant objects in video frames   with descriptive sentences as the query . In   spite of the great advances , most existing meth-   ods rely on dense video frame annotations ,   which require a tremendous amount of hu-   man effort . To achieve effective grounding   under a limited annotation budget , we inves-   tigate one - shot video grounding , and learn to   ground natural language in all video frames   with solely one frame labeled , in an end - to-   end manner . One major challenge of end - to-   end one - shot video grounding is the existence   of videos frames that are either irrelevant to   the language query or the labeled frames . An-   other challenge relates to the limited super-   vision , which might result in ineffective rep-   resentation learning . To address these chal-   lenges , we designed an end - to - end model via   Information T ree for O ne - Shot video ground-   ing ( IT - OS ) . Its key module , the information   tree , can eliminate the interference of irrele-   vant frames based on branch search and branch   cropping techniques . In addition , several self-   supervised tasks are proposed based on the in-   formation tree to improve the representation   learning under insufÔ¨Åcient labeling . Experi-   ments on the benchmark dataset demonstrate   the effectiveness of our model .   1 Introduction   Natural language spatial video grounding is a vi-   tal task for video - text understanding ( Luo and   Shakhnarovich , 2017 ; Zhou et al . , 2019 ; Hu et al . ,   2019 ; Zhang et al . , 2020b ; Li et al . , 2021 ) , which   aims to detect the objects described by the naturalFigure 1 : An example of spatially grounding natural   language in video frames .   language query from each video frame , as shown   in Figure 1 . There is a substantial and rapidly-   growing research literature studying this problem   with dense annotations ( Li et al . , 2017 ; Yamaguchi   et al . , 2017 ; Sadhu et al . , 2020 ) , where each frame   that contains objects relevant to the language query   will be manually labeled with bounding boxes . Ob-   viously , such annotations require tremendous hu-   man effort and can hardly be satisÔ¨Åed in real - world   scenarios . Recently , some works have investigated   weakly - supervised video grounding with solely the   video - text correspondence rather than object - text   annotations ( Huang et al . , 2018 ; Chen et al . , 2019a ;   Shi et al . , 2019 ; Chen et al . , 2019b ; Zhou et al . ,   2018 ) . However , the performance is less satisÔ¨Åed   with such weak supervision . In practice , we are   more likely to have a limited annotation budget   rather than full annotation or no annotation . In   addition , as humans , after experiencing the lan-   guage query and one frame object paired together   for the Ô¨Årst time , we have the ability to generalize   this Ô¨Ånding and identify objects from more frames .   Towards this end , we investigate another practical   problem setting , i.e. , one - shot spatial video ground-   ing , with solely one relevant frame in the video   labeled with bounding boxes per video .   Existing methods that are devised for supervised   video grounding are not directly applicable to this   novel setting . We summarize several critical chal-   lenges :   ‚Ä¢On the one hand , most of them incorporate a   multi - stage training process , i.e. , Ô¨Årstly training   a clip localization module , and training an object8707localization module in the second stage . How-   ever , in one - shot spatial video grounding , there   are no temporal annotations , which indicate the   start / end time of the relevant clip , to train the   clip localization module . Moreover , many of   them extract video features in a pre - processed   manner using feature extractor or object detec-   tor pretrained on large - scale datasets . However ,   independent modeling limits the cooperation of   different modules , especially when the labels are   few . Therefore , it is in urgent need to derive   an end - to - end training framework for one - shot   spatial video grounding .   ‚Ä¢On the other hand , there are video frames that are   either irrelevant to the natural language query or   the labeled frames . These irrelevant frames might   increase the computation complexity of end - to-   end training , and bring confounding between the   frame label and ( irrelevant ) visual features .   ‚Ä¢Lastly , with fewer supervision signals , deep rep-   resentation learning might become error - prone   or easily under-Ô¨Åtting , especially for end - to - end   training .   To address these challenges , we devise an end-   to - end model via the Information Tree for the One   Shot natural language spatial video grounding ( IT-   OS ) . Different from previous works , we design a   novel tree structure to shield off the one - shot learn-   ing from frames that are irrelevant to either the   language query or the labeled frame . We devise   several self - supervised tasks based on the tree struc-   ture to strengthen the representation learning under   limited supervision signals . SpeciÔ¨Åcally , the cal-   culation processes of the key module , information   tree , contains four steps : ( 1 ) To construct the in-   formation tree , we view video frame features as   nodes , and then compress the adjacent nodes to   non - leaf nodes based on the visual similarity of   themselves and the semantic similarity with the   language query ; ( 2 ) We search the information tree   and select branch paths that are consistently rele-   vant to the language query both in the abstractive   non - leaf node level and in the Ô¨Åne - grained leaf   node level ; ( 3 ) We drop I ) the leaf nodes that do   not belong the same semantic unit with the labeled   node ; and II ) the non - leaf nodes on the low rel-   evance branch paths . We also down - weight the   importance of the leaf nodes that belong to the   same semantic unit with the labeled node but are   on the low relevance paths ; ( 4 ) Finally , we inputthe extracted and weighted information to the trans-   former , and conduct training with the one - shot label   and self - supervised tasks , including masked feature   prediction and video - text matching . We note that   both the information tree and the transformer are   jointly trained in an end - to - end manner .   We conduct experiments on two benchmark   datasets , which demonstrate the effectiveness of   IT - OS over state - of - the - arts . Extensive analysis   including ablation studies and case studies jointly   demonstrate the merits of IT - OS on one - shot video   grounding . Our contributions can be summarized   as follows :   ‚Ä¢To the best of our knowledge , we take the ini-   tiative to investigate one - shot natural language   spatial video grounding . We design an end - to-   end model named IT - OS via information tree to   address the challenges brought by limited labels .   ‚Ä¢By leveraging the language query , several novel   modules on the information tree , such as tree   construction , branch search , and branch crop-   ping , are proposed . Moreover , to strengthen   the deep representation learning under limited   supervision signals , we introduce several self-   supervised tasks based on the information tree .   ‚Ä¢We experiment with our IT - OS model on two   benchmark datasets . Comparisons with the state-   of - the - art and extensive model analysis jointly   demonstrate the effectiveness of IT - OS .   2 Related works   Natural Language Video Grounding . Among   numerous multimedia understanding applica-   tions ( Zhang et al . , 2020a , c , 2021d , c , 2020d ; Kai   et al . , 2021 ; Zhang et al . , 2020e ) , natural language   video grounding has attracted the attention of more   and more researchers recently . There are mainly   three branches , temporal grounding[(Ross et al . ,   2018 ; Lu et al . , 2019 ; Zhang et al . , 2019 ; Lin et al . ,   2020a , b ; Zhang et al . , 2021a ; Li et al . , 2022 ; Gao   et al . , 2021 ; Yang et al . , 2021 ) ] , spatio - temporal   grounding[(Tang et al . , 2021 ; Zhang et al . , 2020f , g ;   Su et al . , 2021 ) ] , and spatial grounding . We focus   on the last one .   Deep neural network has convincingly demon-   strated high capability in many domains ( Wu et al . ,   2020 , 2022 ; Guo et al . , 2021 ; Li et al . , 2020b , c , a ) ,   especially for video related tasks ( Miao et al . , 2021 ;   Miao et al . ; Xiao et al . , 2020 , 2021 ) , like video8708grounding . For example,(Li et al . , 2017 ) use the   neural network to detect language query related ob-   jects in the Ô¨Årst frame and track the detected object   in the whole video . Compared to it , ( Yamaguchi   et al . , 2017 ) and ( Vasudevan et al . , 2018 ) go further .   They extract all the object proposals through the   pretrained detector , and choose the right proposal   described in the text .   Supervised training for the natural language   video object detection needs high labeling costs .   To reduce it , some researchers pay attention to   weakly - supervised learning fashion using multi-   ple instances learning(MIL ) method ( Huang et al . ,   2018 ; Chen et al . , 2019a ; Shi et al . , 2019 ; Chen   et al . , 2019b ; Zhou et al . , 2018 ; Wang et al . ,   2021a)transfers contextualized knowledge in cross-   modal alignment to release the unstable training   problem in MIL . Based on contrastive learning   ( Zhang et al . , 2022 ) , ( Da et al . , 2021 ) proposes an   AsyNCE loss to disentangle false - positive frames   in MIL , which allows for mitigating the uncertainty   of from negative instance - sentence pairs . Weakly   supervised false - positive identiÔ¨Åcation based on   contrastive learning has witnessed success in some   other domains ( Zhang et al . , 2021b ; Yao et al . ,   2022 )   One - shot Learning for Videos . One - shot learn-   ing has been applied in some other video tasks .   ( Yang et al . , 2018 ) proposes a meta - learning - based   approach to perform one - shot action localization   by capturing task - speciÔ¨Åc prior knowledge . ( Wu   et al . , 2018 ) investigates the one - shot video person   re - identiÔ¨Åcation task by progressively improving   the discriminative capability of CNN via stepwise   learning . Different from these works , ( Caelles et al . ,   2017 ) and ( Meinhardt and Leal - Taix√© , 2020 ) deÔ¨Åne   the one - shot learning as only one frame being la-   beled per video . SpeciÔ¨Åcally , ( Caelles et al . , 2017 )   use a fully convolutional neural network architec-   ture to solve the one - shot video segmentation task .   ( Meinhardt and Leal - Taix√© , 2020 ) decouple the de-   tection task , and uses the modiÔ¨Åed Mask - RCNN   to predict local segmentation masks . Following   this setting , we investigate one - shot natural lan-   guage spatial video grounding , and devise a novel   information - tree based end - to - end framework for   the task.3 Method   3.1 Model Overview   Problem Formulation . Given a video V=   fvgand a natural language query C , spa-   tial video grounding aims to localize the query-   described object from all the objects O=   fogfor each frame . Idenotes the frame   number of the video , and the Jis the object number   in the video . In one - shot spatial video grounding ,   solely one frame vin videoVis labeled with the   region boxes of the target objects O.   Pipeline of IT - OS . As shown in Figure 2 , there   are mainly four steps involved in the end - to - end   modeling of IT - OS :   ‚Ä¢Firstly , we extract the features from the input   video and the input caption . SpeciÔ¨Åcally , for the   video , we use ResNet-101(He et al . , 2016 ) as the   image encoder to extract the frame feature maps ;   for the language query , we employ a language   model Roberta(Liu et al . , 2019 ) . Both the vision   encoder and the language encoder are jointly op-   timized with the whole network .   ‚Ä¢Secondly , we build the information tree to get   the representation of the video . The information   tree is built upon the frame feature maps , which   are the leaf nodes . Leaf nodes will be further   merged based on the relevance between node-   node and node - query to have non - leaf and root   nodes . Nodes on unnecessary branches will be   deleted conditioned on the language query .   ‚Ä¢Thirdly , we utilize the transformer encoder to   reason on the remaining nodes and language fea-   tures . Upon the transformer , we devise two self-   supervised tasks , i.e. , masked feature modeling ,   and video - text matching , which enhances the rep-   resentation learning under limited labels .   Prediction and Training . We follow the common   prediction and training protocol of visual transform-   ers used in other object detection models ( Wang   et al . , 2021b ) . We input the embedding parameters   Eand the multi - model features Fgenerated   by the transformer encoder into the transformer   decoderD. Then , the decoder Doutputs possible   prediction region features for each frame . For each   possible region , a possibility Pand a bounding box   Bare generated .   P;B = D(F;E ) ; ( 1)8709   We choose the box Bwith the highest possibility   valuePfor each frame as the target box .   During the training process , we Ô¨Årst calculate the   possible prediction regions . Then , we match the   possible regions with the target boxes , and choose   the best match for each frame . Finally , use the   match to train our IT - OS model .   3.2 Information Tree Module   In this section , we will elaborate the information   tree modules in detail . We will illustrate how to   construct the information tree , how to extract criti-   cal information from it and how to design the self-   supervised learning based on the tree . To ease the   illustration , we take the 6frames as an example ,   and show the process in Figure 2 .   3.2.1 Tree Construction   Given the frame features generated by the CNN ,   we build the information tree by merging adjacent   frame features in the speciÔ¨Åed order . SpeciÔ¨Åcally ,   the frame features output by the image encoder are   the leaf nodes N = fng . A sliding window   of size 2 and step 2 is applied on these nodes andnodes in the window are evaluated to be merged or   not .   We calculate the semantic relevance difference   between each node pair with the language query ,   and get the visual relevance between the nodes   in each pair . For the visual relevance calculation ,   we max - pool the feature maps of the inode pair to   have the feature vector f andf . And then , we   compute the cosine similarity rbetweenf   andfto be the visual relevance . Next , we calcu-   late the semantic relevance r andrbetween   the text feature fand the nodes of inode pair :   r=((wf)(wf ) ) ; ( 2 )   r=((wf)(wf ) ) ; ( 3 )   where thewandware learnable parameters , and   is the sigmoid activation function .   The semantic relevance difference dbetween   theith paired nodes is :   d = jr rj+   r ; ( 4 )   where the   is the hyperparameter.8710With the relevant difference value , we rank the   node pairs and pick out the top . Theis a hy-   perparameter , which can be set as a constant or a   percentage . We merge the node pairs :   n = w(n+n ) + b;(5 )   where thewandbare trainable . Finally , The   new nodenreplace the old nodes nand   nin the queue . Repeat the process until there is   only one node in the queue . Saving all nodes in   the process and the composite relationship between   nodes generated in the merging process , we get the   information tree .   3.2.2 Branch Search   We use a branch to denote a subtree . To Ô¨Ålter   critical local and global information , we perform   branch search and selection . We Ô¨Årstly select   branches that contain leaf nodes less than    and more than .andare hyperpa-   rameters . We calculate the semantic relevance of   branches ‚Äô root nodes and the language query based   on Equation 2 .   Training . During training , we directly select the   branch that contains the labeled leaf node and the   root node with the highest semantic relevance . This   selection improves the training efÔ¨Åciency .   Inference . During inference , all frames should   be processed . We conduct an iterative search with   multiple search steps . For each step , we select the   branch with the highest semantic relevance and   remove the selected branch from the information   tree . After the search , we have multiple selected   branches and each branch will be forwarded to the   following processes .   3.2.3 Branch Cropping   Note that not all the non - leaf nodes in the selected   branches are closely related to the input caption .   We remove non - leaf nodes that are with semantic   relevance less than  , which is a hyperparameter .   Their descendant non - leaf nodes are also removed .   To reserve enough frame nodes for training , we do   not remove the descendant leaf nodes . Instead , we   down - weight them with = 0:5 . For other leaf   nodes,= 1 . The remaining leaf nodes and non-   leaf nodes represent the critical local information   and the global information , respectively . We multi-   ply the feature of node iand the node ‚Äôs semantic   relevancer :   f = fr ; ( 6)wherefis the feature vector input into the   transformer . As such , Equation 6 considers both   local relevance rand global relevance with the   language query .   3.2.4 Self - supervised Tasks   We leverage a transformer encoder for these ex-   tracted information and the language query . As   shown in the Figure 2 , we design two self-   supervised tasks as : 1 ) predicting the masked text   features , and masked local / global video informa-   tion ; 2 ) judging whether the text and the video   match . For the transformer , the input tokens F   consist of the local information , the global infor-   mation and the text features , which are three types   of tokens . We further introduce 2 - D position em-   bedding for video tokens and type embedding for   all tokens , which are added to the tokens ‚Äô features .   Then , the features Fare input into the trans-   former encoder E. After encoding , the fusion fea-   turesFare output :   F = E(F ): ( 7 )   We predict the original features for masked lan-   guage tokens and masked video tokens ( leaf / non-   leaf nodes in the selected branch ) using multilayer   perceptrons .   ^f = MLP(f);^f = MLP(f);(8 )   where theMLPandMLPare the multilayer per-   ceptrons for text and video features , respectively .   We view masked token modeling as feature regres-   sion and adopt L2 distance as the loss function .   In addition , there will be a mismatched language   query at the rate of 50 % . We propose to predict   whether the video and language are matched , i.e. ,   whether the video contains the event described by   the language query , based on the output represen-   tation of token [ CLS ] . When the video and the   language are not matched , we will not train the   model with the one - shot label .   4 Experiments   4.1 Experimental Setup   Datasets We consider two video grounding bench-   marks for evaluation : ( 1 ) VidSTG ( Zhang et al . ,   2020 g ) is a large - scale benchmark dataset for   video grounding , which is constructed based on   VidOR ( Shang et al . , 2019 ) dataset . VidSTG con-   tains 10;000videos and 99;943sentences with8711MethodDeclarative Sentence Grounding Interrogative Sentence Grounding   0.4 0.5 0.6 Avg 0.4 0.5 0.6 Avg   GroundeR 24.56 18.22 13.73 18.85 25.28 18.87 14.39 19.52   STPR 25.68 20.07 14.64 19.89 27.09 21.04 16.00 21.38   STGRN 27.57 20.91 16.25 21.50 28.51 21.89 17.20 22.47   VOGnet 32.08 24.38 19.91 25.75 33.08 25.54 20.85 26.72   OMRN 34.43 27.57 21.91 27.96 35.69 28.74 23.03 29.14   VOGnet * 36.42 29.37 21.95 29.25 36.98 28.35 22.57 29.30   OMRN * 39.54 30.02 22.34 30.64 38.89 30.53 24.10 31.17   IT - OS 46.75 35.81 23.23 35.26 46.16 34.55 25.19 35.30   Method 0.4 0.5 0.6 Avg   GroundeR 32.09 27.80 24.25 28.05   STPR 33.40 28.92 25.37 29.23   STGRN 35.45 30.41 26.31 30.72   VOGnet 38.81 32.65 26.87 32.78   OMRN 40.11 34.51 28.36 34.35   VOGnet * 41.23 35.82 29.48 35.51   OMRN * 45.52 37.69 30.41 37.87   IT - OS 51.87 42.91 33.58 42.79   55;135interrogative sentences and 44;808declar-   ative sentences . These sentences describe 79   types of objects appearing in the videos . We   follow the ofÔ¨Åcial dataset split of ( Zhang et al . ,   2020 g ) . ( 2 ) VID - sentence ( Chen et al . , 2019b ) is   another widely used video grounding benchmark   constructed based on the VID ( Russakovsky et al . ,   2015 ) dataset . There are 30 categories and 7;654   video clips in this dataset . We report the results   of all methods on the validation set for the VID-   sentence dataset . We obtain similar observations   and conclusions on the test set .   Implementation Detail For video preprocessing ,   we random resize the frames , and set the max size   is640640 . The other data augmentation methods ,   such as random horizontal Ô¨Çip and random size   cropping are used at the same time . During train-   ing , the learning rate is by default 0:00005 , and   decays by a factor of 10for every 35epochs . The   batch size is 1and the maximum training epoch   is100 . We implement IT - OS in Pytorch and trainit on a Linux server . For model hyperparameters ,   we set= 60 % , and  = 0:7 . Most of the nat-   ural language spatial video grounding models use   the pretrained detection model as the backbone .   Thus , like them , we choose the ofÔ¨Åcial pretrained   MDETR ( Kamath et al . , 2021 ) as the parameter   basis for target detection of our IT - OS .   Evaluation Metrics We follow the evaluation pro-   tocol of ( Chen et al . , 2019b ) . SpeciÔ¨Åcally , we   compute the Intersection overUnion ( IoU ) met-   ric for the predicted spatial bounding box and the   ground - truth per frame . The prediction for a video   is considered as " accurate " if the average IoU of all   frames exceeds a threshold  . The  is set to 0:4 ,   0:5 , and 0:6during testing .   Baselines Since existing video grounding methods   are not directly applicable to the one - shot setting ,   we extend several state - of - the - arts as the baselines .   SpeciÔ¨Åcally , to have a comprehensive compari-   son , we consider 1)fully supervised models , includ-   ingVOGnet ( Sadhu et al . , 2020 ) , OMRN ( Zhang   et al . , 2020f ) and STGRN ( Zhang et al . , 2020 g ) ;   and 2 ) other widely known methods , including   video person grounding STPR ( Yamaguchi et al . ,   2017 ) , and visual grounding method , GroundeR   ( Rohrbach et al . , 2016 ) .   4.2 Performance Comparison   The experimental results for one - shot video ground-   ing on VidSTVG and VID - sentence datasets are   shown in Table 1 and 2 , respectively . According to   the results , we have the following observations :   ‚Ä¢Not surprisingly , although extended to the video   grounding setting , baselines that belong to other   domains , including video person grounding   STPR and visual grounding GroundeR , achieve8712inferior results on video grounding benchmarks .   They lack domain - speciÔ¨Åc knowledge and might   fail to effectively model the spatial - temporal re-   lationships of videos and language queries .   ‚Ä¢IT - OS consistently achieves the best performance   on two benchmarks and multiple experimental   settings with a large margin improvement . Re-   markably , IT - OS boosts the performance ( Avg )   of the previous state - of - the - art OMRN from   nearly 28:0=29:1=34:4to35:3=35:3=42:8on   VidSTVG and VID - sentence , respectively . It   demonstrates the superiority of IT - OS on one-   shot video grounding .   ‚Ä¢The baselines are implemented with the back-   bones used in their original papers , which   are different from ours . To further disentan-   gle the sources of performance improvement ,   we re - implement the best - performing baselines   ( VOGnet * , and OMRN * ) with the same ob-   ject detection backbone , MDETR , as IT - OS . Al-   though there is performance improvement with   the new backbone , the best - performing baseline   OMRN * , still underperforms IT - OS by over 4   points for the average accuracy on all datasets .   It further reveals the effectiveness of our novel   model designs eliminating interference with dif-   ferent pre - training parameters . We attribute the   improvement to the end - to - end modeling , where   different modules can simultaneously beneÔ¨Åt   from each other . In addition , the proposed infor-   mation tree alleviates the negative effects of irrel-   evant frames , and effectively models the interac-   tions between the video global / local information   and the language query . Several self - supervised   learning tasks based on the information tree en-   hance the representation learning under limited   one - shot labels .   4.3 Comparison with Fully Supervised   Methods   We are interested in 1 ) how different baselines per-   form under fully supervised settings ; 2 ) how one-   shot IT - OS perform compared to these baselines .   Towards this end , we train multiple baselines and   IT - OS with all labels on the VID - sentence dataset .   The experiment results are shown in Table 3 . From   the table , we have the following Ô¨Åndings :   ‚Ä¢Remarkably , the performance gap between one-   shot IT - OS and the fully supervised OMRN isMethod 0.4 0.5 0.6 Avg   GroundeR 42.72 33.77 27.05 34.51   STPR 47.95 36.19 30.41 38.18   STGRN 49.25 44.03 34.89 42.72   VOGnet 53.17 43.47 33.77 43.47   OMRN 55.22 46.64 37.50 46.45   IT - OS ( OS ) 51.87 42.91 33.58 42.79   less than 4 % . Such a minor gap demonstrates   the effectiveness of IT - OS on learning with lim-   ited annotations . This is signiÔ¨Åcant and practical   merit since we are more likely to have a limited   annotation budget in real - world applications .   ‚Ä¢Surprisingly , one - shot IT - OS can still outper-   form some weak baselines such as GroundeR   and STPR . These results reveal the necessity of   end - to - end modeling for video grounding .   4.4 Ablation Study   We are interested in how different building blocks   contribute to the effectiveness of IT - OS . To this end ,   we surgically remove several components from   IT - OS and construct different architectures . The   investigated components include information tree   (   ) , the branch cropping (   ) , and the self-   supervised training (   ) . It is worth noting that   the other components can not be deleted indepen-   dently except the branch cropping . Thus , we do n‚Äôt   conduct an ablation study for them . Results on   VidSTG and VID - sentence datasets are shown in   Table 4 and Table 5 , respectively . There are several   observations :   ‚Ä¢Overall , removing any component incurs a perfor-   mance drop , demonstrating the necessity and ef-   fectiveness of the information tree , branch search   & cropping , and self - supervised training .   ‚Ä¢Stacking multiple components outperform the   architecture with a single component . This result   reveals that the proposed components can beneÔ¨Åt   from each other in end - to - end training and jointly   boost one - shot video grounding .   4.5 Case Study   We conduct a case study to visually reveal the abil-   ity of the IT - OS in detail . SpeciÔ¨Åcally , we random8713Declarative Sentence Grounding Interrogative Sentence Grounding         0.4 0.5 0.6 Avg 0.4 0.5 0.6 Avg   39.00 30.52 17.61 29.05 38.78 28.75 19.67 29.07   X 40.52 32.32 18.83 30.56 40.82 31.44 20.66 30.97   X 42.34 32.65 20.35 31.78 42.26 32.02 21.89 32.06   X X 44.16 33.38 21.11 32.89 44.55 33.78 23.19 33.84   X X 44.77 34.62 22.93 34.11 44.30 33.23 24.17 33.90   X X X 46.75 35.81 23.23 35.26 46.16 34.55 25.19 35.30       0.4 0.5 0.6 Avg   44.40 35.07 27.24 35.57   X 46.64 36.38 28.54 37.19   X 47.95 38.99 29.85 38.93   X X 49.44 40.30 31.16 40.30   X X 50.19 40.49 32.46 41.04   X X X 51.87 42.91 33.58 42.79   sample 3videos from the datasets , and sample 6   frames from each video to visualize .   We compare our IT - OS model with the base-   line method , OMRN , and the fundamental ablation   model of the IT - OS , which is removed from the   self - supervised module and the information tree .   As shown in Figure 3 , we have the following key   Ô¨Åndings : ( 1 ) The IT - OS detects the more accu-   rate one from all objects of the video than the best   performing previous method . It demonstrates the   better representation extraction and analysis capa-   bilities of our model . ( 2 ) Even if the target object   is selected correctly , the IT - OS localizes a more   precise spatial area compared with the previous   two stages method . The results reÔ¨Çect the end-   to - end model , IT - OS , has more accurate domain   knowledge through training the whole model on   the target dataset . ( 3 ) After adding the informa-   tion tree and the self - supervised module , the IT - OS   outputs more precise bounding boxes . It reveals   that combining the two modules introduce stronger   supervision signals for model training so that the   model has stronger detection ability .   5 Conclusion   In this paper , we introduce the one - shot learning   into the natural language spatial video grounding   task to reduce the labeling cost . To achieve the   goal , the main point is to make full use of only   one frame label for each video . The invalid frames   unrelated to the input text and target objects bring   confounding to the one - shot training process . We   design an end - to - end model ( IT - OS ) via the infor-   mation tree to avoid it . SpeciÔ¨Åcally , the information   tree module merges frames with similar semantics   into one node . Then , by searching the tree and   cropping the invalid nodes , we can get the com-   plete and valid semantic unit of the video . Finally ,   two self - supervised tasks are used to make up the   insufÔ¨Åcient supervision.8714Acknowledgements   This work is supported in part by the National   Natural Science Foundation of China ( Grant   No.62037001 , No.61836002 , No.62072397 ) . This   work is also partially funded by Hangzhou Hikvi-   sion Digital Technology .   References871587168717