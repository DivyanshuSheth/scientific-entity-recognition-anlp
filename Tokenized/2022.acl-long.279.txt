  Lukas Galke   University of Kiel , Germany   MPI for Psycholinguistics , the Netherlands   lukas.galke@mpi.nlAnsgar Scherp   University of Ulm , Germany   ansgar.scherp@uni-ulm.de   Abstract   Graph neural networks have triggered a resur-   gence of graph - based text classiﬁcation meth-   ods , deﬁning today ’s state of the art . We show   that a wide multi - layer perceptron ( MLP ) us-   ing a Bag - of - Words ( BoW ) outperforms the re-   cent graph - based models TextGCN and Hete-   GCN in an inductive text classiﬁcation set-   ting and is comparable with HyperGAT . More-   over , we ﬁne - tune a sequence - based BERT and   a lightweight DistilBERT model , which both   outperform all state - of - the - art models . These   results question the importance of synthetic   graphs used in modern text classiﬁers . In   terms of efﬁciency , DistilBERT is still twice   as large as our BoW - based wide MLP , while   graph - based models like TextGCN require set-   ting up anO(N)graph , where Nis the   vocabulary plus corpus size . Finally , since   Transformers need to compute O(L)atten-   tion weights with sequence length L , the MLP   models show higher training and inference   speeds on datasets with long sequences .   1 Introduction   Text categorization is the task of assigning topical   categories to text units such as documents , social   media postings , or news articles . Research on text   categorization is a very active ﬁeld as just the sheer   amount of new methods in recent surveys shows   ( Bayer et al . , 2021 ; Li et al . , 2020 ; Zhou et al . ,   2020 ; Kowsari et al . , 2019 ; Kadhim , 2019 ) .   There are approaches based on a Bag of Words   ( BoW ) that perform text categorization purely   on the basis of a multiset of tokens . Among   them are Deep Averaging Networks ( DAN ) ( Iyyer   et al . , 2015 ) , a deep Multi - Layer Perceptron   ( MLP ) model with nlayers that relies on averag-   ing the BoW , Simple Word Embedding Models   ( SWEM ) ( Shen et al . , 2018 ) that explores different   pooling strategies for pretrained word embeddings ,   and fastText ( Bojanowski et al . , 2017 ) , which uses   a linear layer on top of pretrained word embed - dings . These models count the occurrence of all   tokens in the input sequence , while disregarding   word position and order , and then rely on word em-   beddings and fully connected feedforward layer(s ) .   We call these BoW - based models .   Among the most popular recent methods for   text categorization are graph - based models such   as TextGCN ( Yao et al . , 2019 ) that ﬁrst induce a   synthetic word - document co - occurence graph over   the corpus and subsequently apply a graph neural   network ( GNN ) to perform the classiﬁcation task .   Besides TextGCN , there are follow - up works like   HeteGCN ( Ragesh et al . , 2021 ) , TensorGCN ( Liu   et al . , 2020 ) , and HyperGAT ( Ding et al . , 2020 ) ,   which we collectively call graph - based models .   Finally , there is the well - known Trans-   former ( Vaswani et al . , 2017 ) universe with models   such as BERT ( Devlin et al . , 2019 ) and its size-   reduced variants such as DistilBERT ( Sanh et al . ,   2019 ) . Here , the input is a ( ﬁxed - length ) sequence   of tokens , which is then fed into multiple layers of   self - attention . Lightweight versions such as Distil-   BERT and others ( Tay et al . , 2020 ; Fournier et al . ,   2021 ) use less parameters but operate on the same   type of input . Together with recurrent models such   as LSTMs , we call these sequence - based models .   In this paper , we hypothesize that text catego-   rization can be very well conducted by simple but   effective BoW - based models . We investigate this   research question in three steps : First , we conduct   an in - depth analysis of the literature . We review   the key research in the ﬁeld of text categorization .   From this analysis , we derive the different families   of methods , the established benchmark datasets ,   and identify the top performing methods . We de-   cide for which models we report numbers from the   literature and which models we run on our own .   Overall , we compare 16 different methods from   the families of BoW - based models ( 8 methods ) ,   sequence - based models ( 3 methods ) , and graph-   based models ( 5 methods ) . We run our own experi-4038ments for 7 of these methods on 5 text categoriza-   tion datasets , while we report the results from the   literature for the remaining methods .   The result is surprising : Our own BoW - based   MLP , called the WideMLP , with only one wide hid-   den layer , outperforms many of the recent graph-   based models for inductive text categorization ( Yao   et al . , 2019 ; Liu et al . , 2020 ; Ragesh et al . , 2021 ) .   Moreover , we did not ﬁnd any reported scores   for BERT - based methods from the sequence - based   family . Thus , we ﬁne - tuned our own BERT ( Devlin   et al . , 2019 ) and DistilBERT ( Sanh et al . , 2019 ) .   These models set a new state of the art . On a meta-   level , our study shows that MLPs have largely been   ignored as competitor methods in experiments . It   seems as if MLPs have been forgotten as baseline   in the literature , which instead is focusing mostly   on other advanced Deep Learning architectures .   Considering strong baselines is , however , an impor-   tant means to argue about truescientiﬁc advance-   ment ( Shen et al . , 2018 ; Dacrema et al . , 2019 ) .   Simple models are also often preferred in industry   due to lower operational and maintenance costs .   Below , we introduce our methodology and re-   sults from the literature study . Subsequently , we in-   troduce the families of models in Section 3 . There-   after , we describe the experimental procedure in   Section 4 . We present the results of our exper-   iments in Section 5 and discuss our ﬁndings in   Section 6 , before we conclude .   2 Literature on Text Categorization   Methodology In a ﬁrst step , we have analyzed re-   cent surveys on text categorization and comparison   studies ( Minaee et al . , 2021 ; Bayer et al . , 2021 ; Li   et al . , 2020 ; Zhou et al . , 2020 ; Kowsari et al . , 2019 ;   Kadhim , 2019 ; Galke et al . , 2017 ; Zhang et al . ,   2016 ) . These cover the range from shallow to deep   classiﬁcation models . Second , we have screened   for literature in key NLP and AI venues . Finally ,   we have complemented our search by checking re-   sults and papers on paperswithcode.com . On the   basis of this input , we have determined three fam-   ilies of methods and benchmark datasets ( see Ta-   ble 2 ) . We focus our analysis on identifying models   per family showing strong performance and select   the methods to include in our study . For all mod-   els , we have veriﬁed that the same train - test split   is used . We check whether modiﬁed versions of   the datasets have been used ( e. g. , fewer classes ) ,   to avoid bias and wrongfully giving advantages . BoW - based Models Classical machine learning   models that operate on a BoW - based input are ex-   tensively discussed in two surveys ( Kowsari et al . ,   2019 ; Kadhim , 2019 ) and other comparison stud-   ies ( Galke et al . , 2017 ) . Iyyer et al . ( 2015 ) proposed   DAN , which combine word embeddings and deep   feedforward networks . It is an MLP with 1 - 6 hid-   den layers , non - linear activation , dropout , and Ada-   Grad as optimization method . The results suggest   to use pretrained embeddings such as GloVe ( Pen-   nington et al . , 2014 ) over a randomly initialized   neural bag of - words ( Kalchbrenner et al . , 2014 ) as   input . In fastText ( Bojanowski et al . , 2017 ; Joulin   et al . , 2017 ) a linear layer on top of pretrained em-   beddings is used for classiﬁcation . Furthermore ,   Shen et al . ( 2018 ) explore embedding pooling vari-   ants and ﬁnd that SWEM can rival approaches   based on recurrent ( RNN ) and convolutional neural   networks ( CNN ) . We consider fastText , SWEM ,   and a DAN - like deeper MLP in our comparison .   Note that those approaches that rely on logistic   regression on top of pretrained word embeddings ,   e. g. , fastText , share a similar architecture as an   MLP with one hidden layer . However , the standard   training protocol involves pretraining the word em-   bedding on large amounts of unlabeled text and   then freezing the word embeddings while training   the logistic regression ( Mikolov et al . , 2013 ) .   Graph - based Models Using graphs induced   from text for the task of text categorization has   a long history in the community . An early work   is the term co - occurrence graph of the KeyGraph   algorithm ( Ohsawa et al . , 1998 ) . The graph is split   into segments , representing the key concepts in the   document . Co - occurence graphs have also been   used for automatic keyword extraction such as in   RAKE ( Rose et al . , 2010 ) and can be also used for   classiﬁcation ( Zhang et al . , 2021 ) .   Modern approaches exploit this idea in combi-   nation with graph neural networks ( GNN ) ( Hamil-   ton , 2020 ) . Examples of GNN - based methods op-   erating on a word - document co - occurence graph   are TextGCN ( Yao et al . , 2019 ) and its succes-   sor TensorGCN ( Liu et al . , 2020 ) as well as Hete-   GCN ( Ragesh et al . , 2021 ) , HyperGAT ( Ding et al . ,   2020 ) , and DADGNN ( Liu et al . , 2020 ) . We brieﬂy   discuss these models : In TextGCN , the authors   set up a graph based on word - word connections   given by window - based pointwise mutual informa-   tion and word - document TF - IDF scores . They use   a one - hot encoding as node features and apply a4039two - layer graph convolutional network ( Kipf and   Welling , 2017 ) on the graph to carry out the node   classiﬁcation task . HeteGCN combines ideas from   Predictive Text Embedding ( Tang et al . , 2015 ) and   TextGCN and split the adjacency matrix into its   word - document and word - word sub - matrices and   fuse the different layers ’ representations when re-   quired . TensorGCN uses multiple ways of convert-   ing text data into graph data including a semantic   graph created with an LSTM , a syntactic graph cre-   ated by dependency parsing , and a sequential graph   based on word co - occurrence . HyperGAT extended   the idea of text - induced graphs for text classiﬁca-   tion to hypergraphs . The model uses graph atten-   tion and two kinds of hyperedges . Sequential hyper-   edges represent the relation between sentences and   their words . Semantic hyperedges for word - word   connections are derived from topic models ( Blei   et al . , 2001 ) . Finally , DADGNN is a graph - based   approach that uses attention diffusion and decou-   pling techniques to tackle oversmoothing of the   GNN and to be able to stack more layers .   In TextGCN ’s original transductive formulation ,   the entire graph including the test set needs to be   known for training . This may be prohibitive in prac-   tical applications as each batch of new documents   would require retraining the model . When these   methods are adapted for inductive learning , where   the test set is unseen , they achieve notably lower   scores ( Ragesh et al . , 2021 ) . GNNs for text classiﬁ-   cation use corpus statistics , e. g. , pointwise mutual   information ( PMI ) , to connect related words in a   graph ( Yao et al . , 2019 ) . When these were omitted ,   the GNNs would collapse to bag - of - words MLPs .   Thus , GNNs have access to more information than   BoW - MLPs . GloVe ( Pennington et al . , 2014 ) also   captures PMI corpus statistics , which is why we   include an MLP on GloVe input representations .   Sequence models : RNN and CNN Recurrent   neural networks ( RNN ) are a natural choice for any   NLP task . However , it turned out to be challeng-   ing to ﬁnd numbers reported on text categoriza-   tion in the literature that can be used as references .   The bidirectional LSTM with two - dimensional max   pooling BLSTM-2DCNN ( Zhou et al . , 2016 ) has   been applied on a stripped - down to 4 classes ver-   sion of the 20ng dataset . Thus , the high score of   96:5reported for 4ng can not be compared with   papers applied on the full 20ng dataset . Also Text-   RCNN ( Lai et al . , 2015 ) , a model combining re-   currence and convolution uses only the 4 majorcategories in the 20ng dataset . The results of Text-   RCNN are identical with BLSTM-2DCNN . For the   MR dataset , BLSTM-2DCNN provides no infor-   mation on the speciﬁc split of the dataset . RNN-   Capsule ( Wang et al . , 2018 ) is a sentiment analysis   method reaching an accuracy of 83:8on the MR   dataset , but with a different train - test split . Lyu and   Liu ( 2020 ) combine a 2D - CNN with bidirectional   RNN . Another work applying a combination of a   convolutional layer and an LSTM layer is by Wang   et al . ( 2019b ) . The authors experiment with ﬁve   English and two Chinese datasets , which are not   in the set of representative datasets we identiﬁed .   The authors report that their approach outperforms   existing models like fastText on two of the ﬁve   English datasets and both Chinese datasets .   Sequence models : Transformers Surprisingly ,   only few works consider Transformer models   for text categorization . A recent work shows   that BERT outperforms classic TF - IDF BoW ap-   proaches on English , Chinese , and Portuguese   text classiﬁcation datasets ( González - Carvajal and   Garrido - Merchán , 2020 ) . We have not found any   results of transformer - based models reported on   those text categorization datasets that are com-   monly used in the graph - based approaches .   Therefore , we ﬁne - tune BERT ( Devlin et al . ,   2019 ) and DistilBERT ( Sanh et al . , 2019 ) on those   datasets ourselves . BERT is a large pretrained lan-   guage model on the basis of Transformers . Dis-   tilBERT ( Sanh et al . , 2019 ) is a distilled version   of BERT with 40 % reduced parameters while re-   taining 97 % of BERT ’s language understanding   capabilities . TinyBERT ( Jiao et al . , 2020 ) and Mo-   bileBERT ( Sun et al . , 2020 ) would be similarly   suitable alternatives , among others . We chose Dis-   tilBERT because it can be ﬁne - tuned independently   from the BERT teacher . Its inference times are 60 %   faster than BERT , which makes it more likely to be   reusable by labs with limited resources .   Summary From our literature survey , we see that   all recent methods are based on graphs . BoW - based   methods are hardly found in experiments , while ,   likewise surprisingly , Transformer - based sequence   models are extremely scarce in the literature on   topical text categorization . The recent surveys on   text categorization include both classical and Deep   Learning models , but none considered a simple   MLP except for the inclusion of DAN ( Iyyer et al . ,   2015 ) in Li et al . ( 2020).4040   3 Models for Text Categorization   We formally introduce the three families of mod-   els for text categorization , namely the BoW - based ,   graph - based , and sequence - based models . Table 1   summarizes the key properties of the approaches :   whether they require a synthetic graph , whether   word position is reﬂected in the model , whether   the model can deal with arbitrary length text , and   whether the model is capable of inductive learning .   3.1 BoW - Based Text Categorization   Under pure BoW - based text categorization , we de-   note approaches that are not order - aware and op-   erate only on the multiset of words from the in-   put document . Given paired training examples   ( x;y)2 D , each consisting of a bag - of - words   x2R and a class label y2Y , the goal   is to learn a generalizable function ^y = f(x )   with parameters such that arg max ( ^y)preferably   equals the true label yfor input x.   As BoW - based model , we consider a one hidden   layer WideMLP ( i. e. , two layers in total ) . We ex-   periment with pure BoW , TF - IDF weighted , and   averaged GloVe input representations . We also use   a two hidden layers WideMLP-2 . We list the num-   bers for fastText , SWEM , and logistic regression   from Ding et al . ( 2020 ) in our comparison .   3.2 Graph - Based Text Categorization   Graph - based text categorization approaches ﬁrst   set up a synthetic graph on the basis of the text   corpusDin the form of an adjacency matrix   ^A:= make - graph(D ) . For instance , in TextGCN   the graph is set up in two parts : word - word connec-   tions are modeled by pointwise mutual information   and word - document edges resemble that the word   occurs in the document . Then , a parameterized   functionf(X;^A)is learned that uses the   graph as input , where Xare the node features .   The graph is composed of word and document   nodes , each receiving its own embedding ( by set - tingX = I ) . In inductive learning , however , there   is no embedding of the test documents . Note that   the graph - based approaches from the current liter-   ature such as TextGCN also disregard word order ,   similar to the BoW - based models described above .   A detailed discussion of the connection between   TextGCN and MLP is provided in Appendix B.   We consider top performing graph - based models   from the literature , namely TextGCN along with   its successors HeteGCN , TensorGCN , HyperGAT ,   DADGNN , as well as simpliﬁed GCN ( SGC ) ( Wu   et al . , 2019 ) . We do not run our own experiments   for the graph - based models but rely on the original   work and extensive studies by Ding et al . ( 2020 )   and Ragesh et al . ( 2021 ) .   3.3 Sequence - Based Text Categorization   We consider RNNs , LSTMs , and Transformers as   sequence - based models . These models are aware   of the order of the words in the input text in the   sense that they are able to exploit word order infor-   mation . Thus , the key difference to the BoW - based   and graph - based families is that the word order is   reﬂected by sequence - based model . The model sig-   nature is ^y = f(hx;x;:::;xi ) , where   kis the ( maximum ) sequence length . Word posi-   tion is modeled by a dedicated positional encoding .   For instance , in BERT each position is associated   with an embedding vector that is added to the word   embedding at input level .   For the sequence - based models , we run our own   experiments with BERT and DistilBERT , while   reporting the scores of a pretrained LSTM from   Ding et al . ( 2020 ) for comparison .   4 Experimental Apparatus   4.1 Datasets   We use the same datasets and train - test split as   in TextGCN ( Yao et al . , 2019 ) . Those datasets   are 20ng , R8 , R52 , ohsumed , and MR . Twenty4041Newsgroups ( 20ng)(bydate version ) contains long   posts categorized into 20 newsgroups . The mean   sequence length is 551 words with a standard de-   viation ( SD ) of 2,047 . R8 and R52 are subsets   of the Reuters 21578 news dataset with 8 and 52   classes , respectively . The mean sequence length   and SD is 119128words for R8 , and 126133   words for R52 . Ohsumedis a corpus of medical   abstracts from the MEDLINE database that are cat-   egorized into diseases ( one per abstract ) . The mean   sequence length is 285123words . Movie Re-   views ( MR)(Pang and Lee , 2005 ) , split by Tang   et al . ( 2015 ) , is a binary sentiment analysis dataset   on sentence level ( mean sequence length and SD :   2511 ) . Table 2 shows the dataset characteristics .   4.2 Inductive and Transductive Setups   We distinguish between a transductive and an in-   ductive setup for text categorization . In the trans-   ductive setup , as used in TextGCN , the test doc-   uments are visible and actually used for the pre-   processing step . In the inductive setting , the test   documents remain unseen until test time ( i. e. , they   are not available for preprocessing ) . We report the   scores of the graph - based models for both setups   from the literature , where available . BoW - based   and sequence - based models are inherently induc-   tive . Ragesh et al . ( 2021 ) have evaluated a variant   of TextGCN that is capable of inductive learning ,   which we include in our results , too .   4.3 Procedure and Hyperparameter Settings   We have extracted accuracy scores from the liter-   ature according to our systematic selection from   Section 2 . Below , we provide a detailed descrip-   tion of the procedure for the models that we have   run ourselves . We borrow the tokenization strategyfrom BERT ( Devlin et al . , 2019 ) along with its un-   cased vocabulary . The tokenizer relies primarily on   WordPiece ( Wu et al . , 2016 ) for a high coverage   while maintaining a small vocabulary .   Training our BoW - Models . Our WideMLP has   one hidden layer with 1,024 rectiﬁed linear units   ( one input - to - hidden and one hidden - to - output   layer ) . We apply dropout after each hidden layer ,   notably also after the initial embedding layer . Only   for GloVe+WideMLP , neither dropout nor ReLU   is applied to the frozen pretrained embeddings but   only on subsequent layers . The variant WideMLP-   2 has two ReLU - activated hidden layers ( three lay-   ers in total ) with 1;024hidden units each . While   this might be overparameterized for single - label   text classiﬁcation tasks with few classes , we rely   on recent ﬁndings that overparameterization leads   to better generalization ( Neyshabur et al . , 2018 ;   Nakkiran et al . , 2020 ) . In pre - experiments , we   realized that MLPs are not very sensitive to hyper-   parameter choices . Therefore , we optimize cross-   entropy with Adam ( Kingma and Ba , 2015 ) and its   default learning rate of 10 , a linearly decaying   learning rate schedule and train for a high amount   of steps ( Nakkiran et al . , 2020 ) ( we use 100 epochs )   with small batch sizes ( we use 16 ) for sufﬁcient   stochasticity , along with a dropout ratio of 0:5 .   Fine - tuning our BERT models . For BERT and   DistilBERT , we ﬁne - tune for 10 epochs with a lin-   early decaying learning rate of 510and an   effective batch size of 128 via gradient accumula-   tion of 8 x 16 batches . We truncate all inputs to 512   tokens . To isolate the inﬂuence of word order on   BERT ’s performance , we conduct two further abla-   tions . First , we set all position embeddings to zero   and disable their gradient ( BERT w/o pos ids ) . By   doing this , we force BERT to operate on a bag - of-   words without any notion of word order or position .   Second , we shufﬂe each sequence to augment the   training data . We use this augmentation strategy   to increase the number of training examples by a   factor of two ( BERT w/ shuf . augm . ) .   4.4 Measures   We report accuracy as evaluation metric , which is   equivalent to Micro - F1 in single - label classiﬁca-   tion ( see Appendix C ) . We repeat all experiments   ﬁve times with different random initialization of   the parameters and report the mean and standard   deviation of these ﬁve runs.40425 Results   5.1 Effectiveness   Table 3 shows the accuracy scores for the text cate-   gorization models on the ﬁve datasets . All graph-   based models in the transductive setting show sim-   ilar accuracy scores ( maximum difference is 2   points ) . As expected , the scores decrease in the in-   ductive setting up to a point where they are matched   or even outperformed by our WideMLP .   In the inductive setting , the WideMLP models   perform best among the BoW models , in partic-   ular , TFIDF+WideMLP and WideMLP on an un-   weighted BoW. The best - performing graph - based   model is HyperGAT , yet DADGNN has a slight   advantage on R8 , R52 , and MR . For the sequence-   based models , BERT attains the highest scores ,   closely followed by DistilBERT .   The strong performance of WideMLP rivals all   graph - based techniques reported in the literature ,   in particular , the recently published graph - inducing   methods . MLP only falls behind HyperGAT , which   relies on topic models to set up the graph . Another   observation is that 1hidden layer ( but wide ) is suf-   ﬁcient for the tasks , as the scores for MLP variants   with2hidden layers are lower . We further observe   that both pure BoW and TF - IDF weighted BoW   lead to better results than approaches that exploit   pretrained word embeddings such as GloVe - MLP ,   fastText , and SWEM .   With its immense pretraining , BERT yields the   overall highest scores , closely followed by Distil-   BERT . DistilBERT outperforms HyperGAT by 7   points on the MR dataset while being on par on   the others . BERT outperforms the strongest graph-   based competitor , HyperGAT , by 8 points on MR ,   1.5 points on ohsumed , 1 point on R52 and R8 , and   0.5 points on 20ng .   Our results further conﬁrm that position embed-   dings are important for BERT with a notable de-   crease when those are omitted . Augmenting the   data with shufﬂed sequences has led to neither a   consistent decrease nor increase in performance .   5.2 Efﬁciency   Parameter Count of the Models Table 4 lists   the parameter counts of the models . Even though   the MLP is fully - connected on top of a bag - of-   words with the dimensionality of the vocabulary   size , it has only half of the parameters as Distil-   BERT and a quarter of the parameters of BERT .   Using TF - IDF does not change the number ofmodel parameters . Due to the high vocabulary   size , GloVe - based models have a high number of   parameters , but the majority of those is frozen , i. e. ,   does not get gradient updates during training .   Runtime Performance of the Models We pro-   vide the total running times in Table 5 as observed   while conducting the experiments on a single   NVIDIA A100 - SXM4 - 40 GB card . All WideMLP   variants are an order of magnitude faster than Dis-   tilBERT when considering the average runtime per   epoch . DistilBERT is twice as fast as the original   BERT . The transformers are only faster than BoW   models on the MR dataset . This is because the   sequences in the MR dataset are much shorter and   lessO(L)attention weights have to be computed .   6 Discussion   Key Insights Our experiments show that our   MLP models using BoW outperform the recent   graph - based models TextGCN and HeteGCN in   an inductive text classiﬁcation setting . Further-   more , the MLP models are comparable to Hyper-   GAT . Only transformer - based BERT and Distil-   BERT models outperform our MLP and set a new   state - of - the - art . This result is important for two   reasons : First , the strong performance of a pure   BoW - MLP questions the added value of synthetic   graphs in models like TextGCN to the text cat-   egorization task . Only HyperGAT , which uses   the expensive Latent Dirichlet Allocation for com-   puting the graph , slightly outperforms our BoW-   WideMLP in two out of ﬁve datasets . Thus , we   argue that using strong baseline models for text   classiﬁcation is important to assess the true scien-   tiﬁc advancement ( Dacrema et al . , 2019 ) .   Second , in contrast to conventional wis-   dom ( Iyyer et al . , 2015 ) , we ﬁnd that pretrained   embeddings , e. g. , GloVe , can have a detrimen-   tal effect when compared to using an MLP with   one wide hidden layer . Such an MLP circumvents   the bottleneck of the small dimensionality of word   embeddings and has a higher capacity . Further-   more , we experiment with more hidden layers ( see   WideMLP-2 ) , but do not observe any improvement   when the single hidden layer is sufﬁciently wide . A   possible explanation is that already a single hidden   layer is sufﬁcient to approximate any compact func-   tion to an arbitrary degree of accuracy depending   on the width of the hidden layer ( Cybenko , 1989 ) .   Finally , a new state - of - the - art is set by the trans-   former model BERT , which is not very surpris-4043   ing . However , as our efﬁciency analysis shows ,   the MLPs require only a fraction of the parameters   and are faster in their combined training and infer-   ence time except for the MR dataset . The attention   mechanism of ( standard ) Transformers is quadratic   in the sequence length , which leads to slower pro-   cessing of long sequences . With larger batches , the   speed of the MLP could be increased even further .   Detailed Discussion of Results Graph - based   models come with high training costs , as not only   the graph has to be ﬁrst computed , but also a GNN   has to be trained . For standard GNN methods , the   whole graph has to ﬁt into the GPU memory and   mini - batching is nontrivial , but possible with ded-   icated sampling techniques for GNNs ( Fey et al . ,   2021 ) . Furthermore , the original TextGCN is inher - ently transductive , i. e. , it has to be retrained when-   ever new documents appear . Strictly transductive   models are effectively useless in practice ( Lu et al . ,   2019 ) except for applications , in which a partially   labeled corpus needs to be fully annotated . How-   ever , recent extensions such as HeteGCN , Hyper-   GAT , and DADGNN already relax this constraint   and enable inductive learning . Nevertheless , word-   document graphs require O(N)space , where N   is the number of documents plus the vocabulary   size , which is a hurdle for large - scale applications .   There are also tasks where the natural structure   of the graph data provides more information than   the mere text , e. g. , citations networks or connec-   tions in social graphs . In such cases , the perfor-   mance of graph neural networks is the state of the   art ( Kipf and Welling , 2017 ; Velickovic et al . , 2018 )   and are superior to MLPs that use only the node   features and not the graph structure ( Shchur et al . ,   2018 ) . GNNs also ﬁnd application in various NLP   tasks , other than classiﬁcation ( Wu et al . , 2021 ) .   An interesting factor is the ability of the mod-   els to capture word order . BoW models disregard   word order entirely and yield good results , but still   fall behind order - aware Transformer models . In an   extensive study , Conneau et al . ( 2018 ) have shown4044   that memorizing the word content ( which words   appear at all ) is most indicative of downstream   task performance . Sinha et al . ( 2021 ) have experi-   mented with pretraining BERT by disabling word   order during pretraining and show that it makes sur-   prisingly little difference for ﬁne - tuning . In their   study , word order is preserved during ﬁne - tuning .   In our experiments , we have conducted comple-   mentary experiments : we have used a BERT model   that is pretrained with word order , but we have de-   activated the position encoding during ﬁne - tuning .   Our results show that there is a notable drop in per-   formance but the model does not fail completely .   Other NLP tasks such as question answering ( Ra-   jpurkar et al . , 2016 ) or natural language infer-   ence ( Wang et al . , 2019a ) can also be regarded   as text classiﬁcation on a technical level . Here ,   the positional information of the sequence is more   important than for pure topical text categorization .   One can expect that BoW - based models perform   worse than sequence - based models .   Generalizability We expect that similar obser-   vations would be made on other text classiﬁca-   tion datasets because we have already covered a   range of different characteristics : long and short   texts , topical categorization ( 20ng , Reuters , and   Ohsumed ) and sentiment prediction ( MR ) in the   domains of forum postings , news , movie reviews ,   and medical abstracts . Our results are in line with   those from other ﬁelds , who have reported a resur-   gence of MLPs . For example , in business predic-   tion , an MLP baseline outperforms various other   Deep Learning models ( Venugopal et al . , 2021 ;   Yedida et al . , 2021 ) . In computer vision , Tolstikhin   et al . ( 2021 ) and Melas - Kyriazi ( 2021 ) proposed   attention - free MLP models that are on par with   the Vision Transformer ( Dosovitskiy et al . , 2021 ) .   In natural language processing , Liu et al . ( 2021a )   show similar results , while acknowledging that a   small attention module is necessary for some tasks . Threats to Validity We acknowledge that the ex-   perimental datasets are limited to English . While   word order is important in the English language , it   is notable that methods that discard word order still   work well for text categorization . Another possi-   ble bias is the comparability of the results . How-   ever , we carefully checked all relevant parameters   such as the train / test split , the number of classes in   the datasets , if datasets have been pre - processed in   such a way that , e. g. , makes a task easier like reduc-   ing the number of classes , the training procedure ,   and the reported evaluation metrics . Regarding our   efﬁcency analysis , we made sure to report num-   bers for the parameter count and a measure for   the speed other than FLOPs , as recommended by   Dehghani et al . ( 2021 ) . Since runtime is heavily de-   pendant on training parameters such as batch size ,   we complement this with asymptotic complexity .   Practical Impact and Future Work Our study   has an immediate impact on practitioners who seek   to employ robust text categorization models in re-   search projects and in industrial operational en-   vironments . Furthermore , we advocate to use an   MLP baseline in future text categorization research ,   for which we provide concrete guidelines in Ap-   pendix A. As future work , it would be interesting to   analyze multi - label classiﬁcation tasks and to com-   pare with hierarchical text categorization methods   ( Peng et al . , 2018 ; Xiao et al . , 2019 ) . Another in-   teresting yet challenging setting would be few - shot   classiﬁcation ( Brown et al . , 2020 ) .   7 Conclusion   We argue that a wide multi - layer perceptron en-   hanced with today ’s best practices should be consid-   ered as a strong baseline for text classiﬁcation tasks .   In fact , the experiments show that our WideMLP   is oftentimes on - par or even better than recently   proposed models that synthesize a graph structure   from the text.4045The source code is available online :   https://github.com/lgalke/   text - clf - baselines   Ethical Considerations   The focus of this work is text classiﬁcation . Poten-   tial risks that apply to text classiﬁcation in general   also apply to this work . Nonetheless , we present   alternatives to commonly used pretrained language   models , which suffer from various sources of bias   due to the large and poorly manageable data used   for pretraining ( Bender et al . , 2021 ) . In contrast ,   the presented alternatives render full control over   the training data and , thus , contribute to circumvent   the biases otherwise introduced during pretraining .   References4046404740484049A Practical Guidelines for Designing a   WideMLP   On the basis of our results , we provide recommen-   dations for designing a WideMLP baseline .   Tokenization We recommend using modern sub-   word tokenizers such as BERT - like WordPiece or   SentencePiece that yield a high coverage while   needing a relatively small vocabulary .   Input Representation In contrast to conven-   tional wisdom ( Iyyer et al . , 2015 ) , we ﬁnd that   pretrained embeddings , e. g. , GloVe , can have a   detrimental effect when compared to using an MLP   with one wide hidden layer . Such an MLP circum-   vents the bottleneck of the small dimensionality of   word embeddings and has a higher capacity .   Depth vs. Width In text classiﬁcation , width   seems more important than depth . We recommend   to use a single , wide hidden layer , i. e. , one input - to-   hidden and one hidden - to - output layer , e. g. , with   1,024 hidden units and ReLU activation . While   this might be overparameterized for single - label   text classiﬁcation tasks with few classes , we rely   on recent ﬁndings that overparameterization leads   to better generalization ( Neyshabur et al . , 2018 ;   Nakkiran et al . , 2020 ) .   We further motivate the choice of using wide   layers with results from multi - label text classiﬁca-   tion ( Galke et al . , 2017 ) , which has shown that a   ( wide ) MLP outperforms all tested classical base-   lines such as SVMs , k - Nearest Neighbors , and lo-   gistic regression . Follow - up work ( Mai et al . , 2018 )   then found that also CNN and LSTM do not sub-   stantially improve over the wide MLP .   Having a fully - connected layer on - top of a bag-   of - words leads to a high number of learnable pa-   rameters . Still , the wide ﬁrst input - to - hidden layer   can be implemented efﬁciently by using an embed-   ding layer followed by aggregation , which avoids   large matrix multiplications .   In our experiments , we did not observe any im-   provement with more hidden layers ( WideMLP-2 ) ,   as suggested by Iyyer et al . ( 2015 ) , but it might be   beneﬁcial for other , more challenging datasets .   Optimization and Regularization We seek to   ﬁnd an optimization strategy that does not   require dataset - speciﬁc hyperparameter tuning .   This comprises optimizing cross - entropy with   Adam ( Kingma and Ba , 2015 ) and default learningrate10 , a linearly decaying learning rate sched-   ule and training for a high amount of steps ( Nakki-   ran et al . , 2020 ) ( we use 100 epochs ) with small   batch sizes ( we use 16 ) for sufﬁcient stochasticity .   For regularization during this prolonged train-   ing , we suggest to use a high dropout ratio of 0:5 .   Regarding initialization , we rely on framework de-   faults , i. e. ,N(0;1)for the initial embedding layer   and random uniform U( p   d;p   d ) for   subsequent layers ’ weight and bias parameters .   B Connection between BoW - MLP and   TextGCN   TextGCN uses the PMI matrix to set up edge   weights for word - word connections . A single layer   Text - GCN is a BoW - MLP , except for the docu-   ment embedding . The one - hop neighbors are words   which are aggregated after a nonlinear transform .   The basic GCN equation H=(^AXW ) reveals   that the order of transformation and neighborhood   aggregation is irrelevant . The document embed-   ding implies that TextGCN is a semisupervised   technique . Truly new documents , as in inductive   learning scenarios , would need a special treatment   such as using an all zero embedding vector .   A two - layer MLP can be characterized by the   equation ^y = W(Wx+b ) + b. On   bag - of - words inputs , the ﬁrst layer Wx+b   can be replaced by an equivalent embedding layer   with weighting ( e. g. , TF - IDF or length normal-   ization ) being applied during aggregation of the   embedding vectors .   The ﬁrst layer of TextGCN is equivalent to ag-   gregating embedding vectors . A standard GCN   layer with shared weights has the form ( assuming   self - loops have been inserted )   h = XaWx+b   Now in TextGCN node features are given by the   identity , such that xis one - hot . Then we can   rewrite the ﬁrst layer of Text - GCN as an aggrega-   tion of embeddings E. We gain   h = XaE   asWx + bmay again be replaced by an embed-   ding matrix if applied to one hot vectors x. Now E   contains two types of embedding vectors : word em-   beddings and document embeddings corresponding   to word nodes and document nodes . We see that the4050ﬁrst layer of TextGCN is essentially an aggregation   of word embeddings plus the document embedding .   Only with a second layer , TextGCN considers the   embedding of other documents whose words are   connected to the present documents ’ words .   C Equivalence of Micro - F1 and   Accuracy in Multiclass Classiﬁcation   In multiclass classiﬁcation , we have a single true   label for each instance and the predictions are con-   strained to a single prediction per instance . As a   consequence , the measures accuracy and Micro - F1   coincide to the same formula .   Micro - F1 aggregates true positives ( TP ) , true   negatives ( TN ) , false positives ( FP ) , and false neg-   atives ( FN ) globally . It can be expressed as :   Micro -F=2PTP   2PTP+PFP+PFN ;   whereciterates over all classes .   While the accuracy can be expressed as :   Acc = PTP+PTNPTP+PTN+PFP+PFN   In multiclass classiﬁcation , every true positive   is also a true negative for all other classes . When   summing those up over the entire dataset , we obtain   XTP = XTN :   Thus , we can rewrite   2XTP = XTP+XTN   and see that the Micro - F1 and accuracy are equiva-   lent in the multiclass ( a.k.a . single - label ) case.4051