  Huihan LiTianyu GaoManan Goenka Danqi Chen   Department of Computer Science , Princeton University   { huihanl,tianyug,mgoenka,danqic}@princeton.edu   Abstract   Conversational question answering aims to   provide natural - language answers to users in   information - seeking conversations . Existing   conversational QA benchmarks compare mod-   els with pre - collected human - human conver-   sations , using ground - truth answers provided   in conversational history . It remains unclear   whether we can rely on this static evalua-   tion for model development and whether cur-   rent systems can well generalize to real - world   human - machine conversations . In this work ,   we conduct the ﬁrst large - scale human evalua-   tion of state - of - the - art conversational QA sys-   tems , where human evaluators converse with   models and judge the correctness of their an-   swers . We ﬁnd that the distribution of human-   machine conversations differs drastically from   that of human - human conversations , and there   is a disagreement between human and gold-   history evaluation in terms of model rank-   ing . We further investigate how to improve   automatic evaluations , and propose a question   rewriting mechanism based on predicted his-   tory , which better correlates with human judg-   ments . Finally , we analyze the impact of var-   ious modeling strategies and discuss future di-   rections towards building better conversational   question answering systems .   1 Introduction   Conversational question answering aims to build   machines to answer questions in conversations and   has the promise to revolutionize the way humans in-   teract with machines for information seeking . With   recent development of large - scale datasets ( Choi   et al . , 2018 ; Saeidi et al . , 2018 ; Reddy et al . , 2019 ;   Campos et al . , 2020 ) , rapid progress has been made   in better modeling of conversational QA systems .   Current conversational QA datasets are collected   by crowdsourcing human - human conversations , where the questioner asks questions about a speciﬁc   topic , and the answerer provides answers based on   an evidence passage and the conversational history .   When evaluating conversational QA systems , a set   of held - out conversations are used for asking mod-   els questions in turn . Since the evaluation builds   on pre - collected conversations , the gold history of   the conversation is always provided , regardless of   models ’ actual predictions ( Figure 1(b ) ) . Although   current systems achieve near - human F1 scores on   this static evaluation , it is questionable whether this   can faithfully reﬂect models ’ true performance in   real - world applications . To what extent do human-   machine conversations deviate from human - human   conversations ? What will happen if models have no   access to ground - truth answers in a conversation ?   To answer these questions and better understand   the performance of conversational QA systems ,   we carry out the ﬁrst large - scale human evalua-   tion with four state - of - the - art models on the QuAC   dataset ( Choi et al . , 2018 ) by having human eval-   uators converse with the models and judge the   correctness of their answers . We collected 1,446   human - machine conversations in total , with 15,059   question - answer pairs . Through careful analy-   sis , we notice a signiﬁcant distribution shift from   human - human conversations and identify a clear in-   consistency of model performance between current   evaluation protocol and human judgements .   This ﬁnding motivates us to improve automatic   evaluation such that it is better aligned with hu-   man evaluation . Mandya et al . ( 2020 ) ; Siblini   et al . ( 2021 ) identify a similar issue in gold - history   evaluation and propose to use models ’ own predic-   tions for automatic evaluation . However , predicted-   history evaluation poses another challenge : since   all the questions have been collected beforehand ,   using predicted history will invalidate some of the   questions because of changes in the conversational   history ( see Figure 1(c ) for an example ) .   Following this intuition , we propose a question8074   rewriting mechanism , which automatically detects   and rewrites invalid questions with predicted his-   tory ( Figure 4 ) . We use a coreference resolution   model ( Lee et al . , 2018 ) to detect inconsistency   of conference in question text conditioned on pre-   dicted history and gold history , and then rewrite   those questions by substituting with correct men-   tions , so that the questions are resolvable in the   predicted context . Compared to predicted - history   evaluation , we ﬁnd that incorporating this rewriting   mechanism aligns better with human evaluation .   Finally , we also investigate the impact of differ-   ent modeling strategies based on human evaluation .   We ﬁnd that both accurately detecting unanswer-   able questions and explicitly modeling question de-   pendencies in conversations are crucial for model   performance . Equipped with all the insights , we   discuss directions for conversational QA modeling .   We release our human evaluation dataset and hope   that our ﬁndings can shed light on future develop-   ment of better conversational QA systems .   2 Preliminary   2.1 Evaluation of conversational QA   Evaluation of conversational QA in real - world con-   sists of three components : an evidence passage   P , a ( human ) questioner Hthat has no access to   P , and a modelMthat has access to P. The   questioner asks questions about Pand the model   answers them based on Pand the conversational   history thus far ( see an example in Figure 1(a ) ) .   Formally , for the i - th turn , the human asks a ques - tion based on the previous conversation ,   QH(Q;A;:::;Q;A ) ; ( 1 )   and then the model answers it based on both the   history and the passage ,   AM ( P;Q;A;:::;Q;A;Q);(2 )   whereQandArepresent the question and the   answer at the i - th turn . If the question is unanswer-   able fromP , we simply denote AasCANNOT   ANSWER . The modelMis then evaluated by the   correctness of answers .   Evaluating conversational QA systems requires   human in the loop and is hence expensive . Instead ,   current benchmarks use automatic evaluation with   gold history ( Auto - Gold ) and collect a set of human-   human conversations for automatic evaluation . For   each passage , one annotator asks questions with-   out seeing the passage , while the other annotator   provides the answers . Denote the collected ques-   tions and answers as QandA. In gold - history   evaluation , the model is inquired with pre - collected   questionsQand the gold answers as history :   AM ( P;Q;A;:::;Q;A;Q);(3 )   and we evaluate the model by comparing AtoA   ( measured by word - level F1 ) . This process does   not require human effort but can not truly reﬂect   the distribution of human - machine conversations ,   because unlike human questioners who may ask   different questions based on different model predic-   tions , this static process ignores model predictions   and always asks the pre - collected question .   In this work , we choose the QuAC dataset ( Choi   et al . , 2018 ) as our primary evaluation because it is8075closer to real - world information - seeking conversa-   tions , where the questioner can not see the evidence   passage during the dataset collection . It prevents   the questioner asking questions that simply over-   laps with the passage and encourages unanswerable   questions . QuAC also adopts extractive question   answering that restricts the answer as a span of text ,   which is generally considered easier to evaluate .   2.2 Models   For human evaluation and analysis , we choose the   following four conversational QA models with dif-   ferent model architectures and training strategies :   BERT . It is a simple BERT ( Devlin et al . , 2019 )   baseline which concatenates the previous two turns   of question - answer pairs , the question , and the pas-   sage as the input and predicts the answer span .   This model is the same as the “ BERT + PHQA ”   baseline in Qu et al . ( 2019a ) .   GraphFlow . Chen et al . ( 2020 ) propose a recur-   rent graph neural network on top of BERT em-   beddings to model the dependencies between the   question , the history and the passage .   HAM . Qu et al . ( 2019b ) propose a history atten-   tion mechanism ( HAM ) to softly select the most   relevant previous turns .   ExCorD. Kim et al . ( 2021 ) train a question rewrit-   ing model on CANARD ( Elgohary et al . , 2019 ) to   generate context - independent questions , and then   use both the original and the generated questions   to train the QA model . This model achieves the   current state - of - the - art on QuAC ( 67.7 % F1 ) .   For all the models except BERT , we use the orig-   inal implementations for a direct comparison . We   report their performance on both standard bench-   mark and our evaluation in Table 2 .   3 Human Evaluation   3.1 Conversation collection   In this section , we carry out a large - scale human   evaluation with the four models discussed above .   We collect human - machine conversations using   100 passages from the QuAC development set on   Amazon Mechanical Turk . We also design a setof qualiﬁcation questions to make sure that the an-   notators fully understand our annotation guideline .   For each model and each passage , we collect three   conversations from three different annotators .   We collect each conversation in two steps :   ( 1 ) The annotator has no access to the passage   and asks questions . The model extracts the an-   swer span from the passage or returns CANNOT   ANSWER in a human - machine conversation inter-   face . We provide the title , the section title , the   background of the passage , and the ﬁrst question   from QuAC as a prompt to annotators . Annotators   are required to ask at least 8 and at most 12 ques-   tions . We encourage context - dependent questions ,   but also allow open questions like “ What else is   interesting ? ” if asking a follow - up question is difﬁ-   cult . ( 2 ) After the conversation ends , the annotator   is shown the passage and asked to check whether   the model predictions are correct or not .   We noticed that the annotators are biased when   evaluating the correctness of answers . For ques-   tions to which the model answered CANNOT   ANSWER , annotators tend to mark the answer as   incorrect without checking if the question is an-   swerable . Additionally , for answers with the cor-   rect types ( e.g. a date as an answer to “ When was   it ? ” ) , annotators tend to mark it as correct without   verifying it from the passage . Therefore , we asked   another group of annotators to verify question an-   swerability and answer correctness .   3.2 Answer validation   For each collected conversation , we ask two addi-   tional annotators to validate the annotations . First ,   each annotator reads the passage before seeing the   conversation . Then , the annotator sees the question   ( and question only ) and selects whether the ques-   tion is ( a ) ungrammatical , ( b ) unanswerable , or ( c )   answerable . If the annotator chooses “ answerable ” ,   the interface then reveals the answer and asks about   its correctness . If the answer is “ incorrect ” , the an-   notator selects the correct answer span from the   passage . We discard all questions that both anno-   tators ﬁnd “ ungrammatical ” and the correctness is   taken as the majority of the 3 annotations .   3.3 Annotation statistics   In total , we collected 1,446 human - machine con-   versations and 15,059 question - answer pairs . We   release this collection as an important source that8076   complements existing conversational QA datasets .   Numbers of conversations and question - answer   pairs collected for each model are shown in Table 1 .   The data distribution of this collection is very differ-   ent from the original QuAC dataset ( human - human   conversations ): we see more open questions and   unanswerable questions , due to less ﬂuent conversa-   tion ﬂow caused by model mistakes , and that mod-   els can not provide feedback to questioner about   whether an answer is worth following up like hu-   man answerers do ( more analysis in § 6.2 ) .   Deciding the correctness of answers is challeng-   ing even for humans in some cases , especially   when questions are short and ambiguous . We mea-   sure annotators ’ agreement and calculate the Fleiss ’   Kappa ( Fleiss , 1971 ) on the agreement between   annotators in the validation phase . We achieve   = 0:598(moderate agreement ) of overall anno-   tation agreement . Focusing on answerability anno-   tation , we have = 0:679(substantial agreement ) .   4 Disagreements between Human and   Gold - history Evaluation   We now compare the results from our human evalu-   ation and gold - history ( automatic ) evaluation . Note   that the two sets of numbers are not directly com-   parable : ( 1 ) the human evaluation reports accuracy ,   while the automatic evaluation reports F1 scores ;   ( 2 ) the absolute numbers of human evaluation are   much higher than those of automatic evaluations .   For example , for the BERT model , the human eval-   uation accuracy is 82.6 % while the automatic eval-   uation F1 is only 63.2 % . The reason is that , in auto-   matic evaluations , the gold answers can not capture   all possible correct answers to open - ended ques-   tions or questions with multiple answers ; however ,   the human annotators can evaluate the correctness   of answers easily . Nevertheless , we can compare   relative rankings between different models .   Figure 2 shows different trends between human   evaluation and gold - history evaluation ( Auto - Gold ) .   Current standard evaluation can not reﬂect model   performance in human - machine conversations : ( 1 )   Human evaluation and Auto - Gold rank BERT and   GraphFlow differently ; especially , GraphFlow per-   forms much better in automatic evaluation , but   worse in human evaluation . ( 2 ) The gap between   HAM and ExCorD is signiﬁcant ( F1 of 65.4 % vs   67.7 % ) in the automatic evaluation but the two   models perform similarly in human evaluation ( ac-   curacy of 87.8 % vs 87.9 % ) .   5 Strategies for Automatic Evaluation   The inconsistency between human evaluation and   gold - history evaluation suggests that we need bet-   ter ways to evaluate and develop our conversational   QA models . When being deployed in realistic sce-   narios , the models would never have access to the   ground truth ( gold answers ) in previous turns and   are only exposed to the conversational history and   the passage . Intuitively , we can simply replace   gold answers by the predicted answers of models   and we name this as predicted - history evaluation   ( Auto - Pred ) . Formally , the model makes predic-   tions based on the questions and its own answers :   AM ( P;Q;A;:::;Q;A;Q):(4 )   This evaluation has been suggested by several re-   cent works ( Mandya et al . , 2020 ; Siblini et al . ,   2021 ) , which reported a signiﬁcant performance   drop using predicted history . We observe the same   performance degradation , shown in Table 2 .   However , another issue naturally arises with pre-   dicted history : Qs were written by the dataset an-   notators based on ( Q;A;:::;Q;A ) , which8077   may become unnatural or invalid when the history   is changed to ( Q;A;:::;Q;A ) .   5.1 Predicted history invalidates questions   We examined 100 QuAC conversations with the   best - performing model ( ExCorD ) and identiﬁed   three categories of invalid questions caused by pre-   dicted history . We ﬁnd that 23 % of the questions   become invalid after using the predicted history .   We summarize the types of invalid questions as   follows ( see detailed examples in Figure 3 ):   •Unresolved coreference ( 44.0 % ) . The question   becomes invalid for containing either a pronoun   or a deﬁnite noun phrase that refers to an entity   unresolvable without the gold history .   •Incoherence ( 39.1 % ) . The question is incoher-   ent with the conversation ﬂow ( e.g. , mention-   ing an entity non - existent in predicted history ) .   While humans may still answer the question us-   ing the passage , this leads to an unnatural conver-   sation and a train - test discrepancy for models .   •Correct answer changed ( 16.9 % ) . The an-   swer to this question with the predicted history   changes from when it is based on the gold history .   We further analyze the reasons for the biggest   “ unresolved coreference ” category and ﬁnd that the   model either gives an incorrect answer to the previ-   ous question ( “ incorrect prediction ” , 39.8 % ) , or the   model predicts a different ( yet correct ) answer to   an open question ( “ open question ” , 37.0 % ) , or the   model returns CANNOT ANSWER incorrectly ( “ no   prediction ” , 9.5 % ) , or the gold answer is longer   than prediction and the next question depends on   the extra part ( “ extra gold information ” , 13.6 % ) .   Invalid questions result in compounding errors ,   which may further affect how the model interprets   the following questions .   5.2 Evaluation with question substitution   Among all the invalid question categories , “ unre-   solved coreference ” questions are the most criti-   cal ones . They lead to incorrect interpretations   of questions and hence wrong answers . We pro-   pose to improve our evaluation by ﬁrst detecting   these questions using a state - of - the - art coreference   resolution system ( Lee et al . , 2018 ) , and then sub-   stituting them with either rewriting the questions in-   place and replacing the questions with their context-   independent counterparts .   Detecting invalid questions . We make the as-   sumption that if the coreference model resolves   mentions in Qdifferently between using gold his-   tory(Q;A;:::;A;Q)and predicted history   ( Q;A;:::;A;Q ) , thenQis identiﬁed as hav-   ing an unresolved coreference issue .   The inputs to the coreference model for Qare   the following:8078   whereBG is the background , SandSdenote   the inputs for gold and predicted history . After the   coreference model returns entity cluster informa-   tion givenSandS , we extract a list of entities   E = fe;:::;egandE = fe;:::;eg . We   sayQisvalid only ifE = E , that is ,   jEj = jEjande = e;8e2E ;   assumingeandehave a shared mention in Q.   We determine whether e = eby checking if   F1(s;s)>0 , wheresandsare the ﬁrstmen-   tion ofeanderespectively , and F1is the word-   level F1 score , i.e. , e = eas long as their ﬁrst   mentions have word overlap . The reason we take   the F1 instead of exact match to check whether the   entities are the same is stated in Appendix A.   Question rewriting through entity substitution .   Our ﬁrst strategy is to substitute the entity names   inQwith entities in E , ifQis invalid . The   rewritten question , instead of the original one , will   be used in the conversation history and fed into   the model . We denote this evaluation method   asrewritten - question evaluation ( Auto - Rewrite ) ,   and Figure 4 illustrates a concrete example .   To analyze how well Auto - Rewrite does in de-   tecting and rewriting questions , we manually check   100 conversations of ExCorD from the QuAC de-   velopment set . We ﬁnd that Auto - Rewrite can   detect invalid questions with a precision of 72 %   and a recall of 72 % ( more detailed analysis in Ap-   pendix B ) . An example of correctly detected and   rewritten question is presented in Figure 4 .   Question replacement using CANARD . Another   strategy is to replace the invalid questions with   context - independent questions . The CANARDdataset ( Elgohary et al . , 2019 ) provides such a re-   source , which contains human - rewritten context-   independent version of QuAC ’s questions . Recent   works ( Anantha et al . , 2021 ; Elgohary et al . , 2019 )   have proposed training sequence - to - sequence mod-   els on such dataset to rewrite questions ; however ,   since the performance of the question - rewriting   models is upper bounded by the human - rewritten   version , we simply use CANARD for question re-   placement . We denote this strategy as replaced-   question evaluation ( Auto - Replace ) . Because col-   lecting context - independent questions is expensive ,   Auto - Replace is limited to evaluating models on   QuAC ; it is also possible to be extended to other   datasets by training a question rewriting model , as   demonstrated in existing work .   6 Automatic vs Human Evaluation   In this section , we compare human evaluation with   all the automatic evaluations we have introduced :   gold - history ( Auto - Gold ) , predicted - history ( Auto-   Pred ) , and our proposed Auto - Rewrite and Auto-   Replace evaluations . We ﬁrst explain the metrics   we use in the comparison ( § 6.1 ) and then discuss   the ﬁndings ( § 6.2 and § 6.3 ) .   6.1 Agreement metrics   Model performance and rankings . We ﬁrst con-   sider using model performance reported by differ-   ent evaluation methods . Considering numbers of   automatic and human evaluations are not directly   comparable , we also calculate models ’ rankings   and compare whether the rankings are consistent   between automatic and human evaluations . Model   performance is reported in Table 2 . In human eval-   uation , GraphFlow < BERT < HAMExCorD ;   in Auto - Gold , BERT < GraphFlow < HAM < Ex-   CorD ; in other automatic evaluations , GraphFlow   < BERT < HAM < ExCorD.8079   Statistics of unanswerable questions . Percent-   age of unanswerable questions is an important as-   pect in conversations . Automatic evaluations using   static datasets have a ﬁxed number of unanswerable   questions , while in human evaluation , the percent-   age of unanswerable questions asked by human   annotators varies with different models . The statis-   tics of unanswerable questions is shown in Table 3 .   Pairwise agreement . For a more ﬁne - grained eval-   uation , we perform a passage - level comparison for   every pair of models . More speciﬁcally , for ev-   ery single passage we use one automatic metric   to decide whether model Aoutperforms model B   ( or vice versa ) and examine the percentage of pas-   sages that the automatic metric agrees with human   evaluation . For example , if the pairwise agreement   of BERT / ExCorD between human evaluation and   Auto - Gold is 52 % , it means that Auto - Gold and hu-   man evaluation agree on 52 % passages in terms of   which model is better . Higher agreement means the   automatic evaluation is closer to human evaluation .   Figure 5 shows the results of pairwise agreement .   6.2 Automatic evaluations have a signiﬁcant   distribution shift from human evaluation   We found that automatic evaluations have a signiﬁ-   ca nt distribution shift from human evaluation . Wedraw this conclusion from the following points .   •Human evaluation shows a much higher model   performance than all automatic evaluations , as   shown in Table 2 . Two reasons may cause this   large discrepancy : ( a ) Many conversational QA   questions have multiple possible answers , and it   is hard for the static dataset in automatic eval-   uations to capture all the answers . It is not an   issue in human evaluation because all answers   are judged by human evaluators . ( b ) There are   more unanswerable questions and open questions   in human evaluation ( reason discussed in the next   paragraph ) , which are easier — for example , mod-   els are almost always correct when answering   questions like “ What else is interesting ? ” .   •Human evaluation has a much higher unanswer-   able question rate , as shown in Table 3 . The   reason is that in human - human data collection ,   the answers are usually correct and the question-   ers can ask followup questions upon the high-   quality conversation ; in human - machine interac-   tions , since the models can make mistakes , the   conversation ﬂow is less ﬂuent and it is harder   to have followup questions . Thus , questioners   chatting with models tend to ask more open or   unanswerable questions .   •All automatic evaluation methods have a pairwise   agreement lower than 70 % with human evalua-   tion , as shown in Figure 2 . This suggests that   all automatic evaluations can not faithfully reﬂect   the model performance of human evaluation .   6.3 Auto - Rewrite is closer to human   evaluation   First , we can clearly see that among all automatic   evaluations , Auto - Gold deviates the most from the   human evaluation . From Table 2 , only Auto - Gold   shows different rankings from human evaluation ,   while Auto - Pred , Auto - Rewrite , and Auto - Replace   show consistent rankings to human judgments .   In Figure 2 , we see that Auto - Gold has the lowest   agreement with human evaluation ; among others ,   Auto - Rewrite better agrees with human evaluation   for most model pairs . Surprisingly , Auto - Rewrite   is even better than Auto - Replace — which uses   human - written context independent questions — in   most cases . After checking the Auto - Replace con-   versations , we found that human - written context in-   dependent questions are usually much longer than   QuAC questions and introduce extra information8080   into the context , which leads to out - of - domain chal-   lenges for conversational QA models ( example in   Appendix C ) . It shows that our rewriting strategy   can better reﬂect real - world performance of con-   versational QA systems . However , Auto - Rewrite is   not perfect — we see that when comparing G / E or   G / H , Auto - Pred is better than Auto - Rewrite ; in all   model pairs , the agreement between human evalua-   tion and Auto - Rewrite is still lower than 70 % . This   calls for further effort in designing better automatic   evaluation in the future .   7 Towards Better Conversational QA   With insights drawn from human evaluation and   comparison with automatic evaluations , we discuss   the impact of different modeling strategies , as well   as future directions towards building better conver-   sational question answering systems .   Modeling question dependencies on conversa-   tional context . When we focus on answerable   questions ( Table 2 ) , we notice that GraphFlow ,   HAM and ExCorD perform much better than   BERT . We compare the modeling differences of   the four systems in Figure 6 , and identify that all   the three better systems explicitly model the ques-   tion dependencies on the conversation history and   the passage : both GraphFlow and HAM highlight   repeated mentions in questions and conversation   history by special embeddings ( turn marker and   PosHAE ) and use attention mechanism to select   the most relevant part from the context ; ExCorD   adopts a question rewriting module that generates   context - independent questions given the history   and passage . All those designs help models better   understand the question in a conversational con-   text . Figure 7 gives an example where GraphFlow ,   HAM and ExCorD resolved the question from long   conversation history while BERT failed .   Unanswerable question detection . Table 4demonstrates models ’ performance in detecting   unanswerable questions . We notice that Graph-   Flow predicts much fewer unanswerable questions   than the other three models , and has a high pre-   cision and a low recall in unanswerable detection .   This is because GraphFlow uses a separate network   for predicting unanswerable questions , which is   harder to calibrate , while the other models jointly   predict unanswerable questions and answer spans .   This behavior has two effects : ( a ) GraphFlow ’s   overall performance is dragged down by its poor   unanswerable detection result ( Table 2 ) . ( b ) In   human evaluation , annotators ask fewer unanswer-   able questions with GraphFlow ( Table 3)—when   the model outputs more , regardless of correctness ,   the human questioner has a higher chance to ask   passage - related followup questions . Both suggest   that how well the model detects unanswerable ques-   tions signiﬁcantly affects its performance and the   ﬂow in human - machine conversations .   Optimizing towards the new testing protocols .   Most existing works on conversational QA model-   ing focus on optimizing towards Auto - Gold eval-   uation . Since Auto - Gold has a large gap from the   real - world evaluation , more efforts are needed in   optimizing towards the human evaluation , or Auto-   Rewrite , which better reﬂects human evaluation .   One potential direction is to improve models ’ ro-   bustness given noisy conversation history , which   simulates the inaccurate history in real world that   consists of models ’ own predictions . In fact , prior   works ( Mandya et al . , 2020 ; Siblini et al . , 2021 )   that used predicted history in training showed that it   beneﬁts the models in predicted - history evaluation .   8 Related Work   Conversational question answering . In recent   years , several conversational question answering   datasets have emerged , such as QuAC ( Choi8081   et al . , 2018 ) , CoQA ( Reddy et al . , 2019 ) , and   DoQA ( Campos et al . , 2020 ) , as well as a few   recent works focusing on conversational open-   domain question answering ( Adlakha et al . , 2021 ;   Anantha et al . , 2021 ; Qu et al . , 2020 ) Different   from single - turn QA datasets ( Rajpurkar et al . ,   2016 ) , conversational QA requires the model to   understand the question in the context of conver-   sational history . There have been many methods   proposed to improve conversational QA perfor-   mance ( Ohsugi et al . , 2019 ; Chen et al . , 2020 ; Qu   et al . , 2019b ; Kim et al . , 2021 ) and signiﬁcant im-   provements have been made on conversational QA   benchmarks . Besides text - based conversational QA   tasks , there also exist conversational QA bench-   marks that require external knowledge or other   modalities ( Saeidi et al . , 2018 ; Saha et al . , 2018 ;   Guo et al . , 2018 ; Das et al . , 2017 ) .   Only recently has it been noticed that the current   method of evaluating conversational QA models is   ﬂawed . Mandya et al . ( 2020 ) ; Siblini et al . ( 2021 )   point out that using gold answers in history is not   consistent with real - world scenarios and propose to   use predicted history for evaluation . Different from   prior works , in this paper , we conduct a large scale   human evaluation to provide evidence for why gold-   history evaluation is sub - optimal . In addition , wepoint out that even predicted - history evaluation has   issues with invalid questions , for which we propose   rewriting questions to further mitigate the gap .   Automatic evaluation of dialogue systems . Au-   tomatically evaluating dialogue systems is difﬁcult   due to the nature of conversations . In recent years ,   the NLP community has cautiously re - evaluated   and identiﬁed ﬂaws in many popular automated   evaluation strategies of dialogue systems ( Liu et al . ,   2016 ; Sai et al . , 2019 ) , and have proposed new eval-   uation protocols to align more with human evalua-   tion in a real - world setting : Huang et al . ( 2020 ) ; Ye   et al . ( 2021 ) evaluate the coherence of the dialogue   systems ; Gupta et al . ( 2019 ) explore to use multi-   ple references for evaluation ; Mehri and Eskenazi   ( 2020 ) propose an unsupervised and reference - free   evaluation ; Lowe et al . ( 2017 ) ; Tao et al . ( 2018 ) ;   Ghazarian et al . ( 2019 ) ; Shimanaka et al . ( 2019 ) ;   Sai et al . ( 2020 ) train models to predict the related-   ness score between references and model outputs ,   which are shown to be better than BLEU ( Papineni   et al . , 2002 ) or ROGUE ( Lin , 2004 ) .   9 Conclusion   In this work , we carry out the ﬁrst large - scale   human evaluation on conversational QA systems .   We show that current standard automatic evalua-   tion with gold history can not reﬂect models ’ per-   formance in human evaluation , and that human-   machine conversations have a large distribution   shift from static conversational QA datasets of   human - human conversations . To tackle these prob-   lems , we propose to use predicted history with   rewriting invalid questions for evaluation , which   reduces the gap between automatic evaluations and   real - world human evaluation . Based on the insights   from the human evaluation results , we also nalyze   current conversational QA systems and identify   promising directions for future development.8082Acknowledgements   We thank Alexander Wettig and other members of   the Princeton NLP group , and the anonymous re-   viewers for their valuable feedback . This research   is supported by a Graduate Fellowship at Princeton   University and the James Mi * 91 Research Innova-   tion Fund for Data Science .   References80838084A Invalid Question Detection   In question rewriting , we use F1 instead of exact   match to check whether two entites are the same .   The reason is that sometimes the prediction may   mention the same entity as the gold answer does ,   but with different names . Figure 8 gives an exam-   ple . Thus to avoid the false positive of detecting   invalid questions , we take the F1 metric .   B Quality of Rewriting Questions   Detection . After manually checking 100 conversa-   tions of ExCorD from the QuAC development set ,   we ﬁnd that Auto - Rewrite can detect invalid ques-   tions with a precision of 72 % and a recall of 72 % .   We notice that the coreference model sometimes   detects the pronoun of the main character in the pas-   sage as insolvable , although it almost shows up in   every question . This issue causes the low precision   but is not a serious problem in our case – whether   rewriting the pronoun of the main character does   not affect models ’ prediction much , because the   model always sees the passage and knows who the   main character is .   Rewriting . Among all correctly detected invalid   questions , we further check the quality of rewrit-   ing , and in 68 % of the times Auto - Rewrite gives a   correct context - independent questions . The most   common error is being ungrammatical . For exam-   ple , using the gold history of " ... Dee Dee claimed   that Spector once pulled a gun on him " , the origi-   nal question " Did they arrest him for doing this ? "   was rewritten to " Did they arrest Phillip Harvey   Spector for doing pulled ? " While this creates a dis-   tribution shift on question formats , it is still better   than putting an invalid question in the ﬂow .   C Issue with Context Independent   Questions   Figure 9 shows an example where extra informa-   tion in context - independent questions confuses the   model and leads to incorrect prediction.8085