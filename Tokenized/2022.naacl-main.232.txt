  Xinyu Wang , Min Gui , Yong Jiang , Zixia Jia , Nguyen Bach , Tao Wang ,   Zhongqiang Huang , Fei Huang , Kewei TuSchool of Information Science and Technology , ShanghaiTech UniversityShanghai Engineering Research Center of Intelligent Vision and ImagingDAMO Academy , Alibaba GroupShopee , SingaporeMicrosoft   { wangxy1,jiazx,tukw}@shanghaitech.edu.cn , min.gui@shopee.com   { yongjiang.jy,z.huang,f.huang}@alibaba-inc.com   nguyenbach@microsoft.com   Abstract   Recently , Multi - modal Named Entity Recog-   nition ( MNER ) has attracted a lot of attention .   Most of the work utilizes image information   through region - level visual representations ob-   tained from a pretrained object detector and   relies on an attention mechanism to model the   interactions between image and text represen-   tations . However , it is difficult to model such   interactions as image and text representations   are trained separately on the data of their re-   spective modality and are not aligned in the   same space . As text representations take the   most important role in MNER , in this paper ,   we propose Image- textAlignments ( ITA ) to   align image features into the textual space , so   that the attention mechanism in transformer-   based pretrained textual embeddings can be   better utilized . ITA first aligns the image into   regional object tags , image - level captions and   optical characters as visual contexts , concate-   nates them with the input texts as a new cross-   modal input , and then feeds it into a pretrained   textual embedding model . This makes it easier   for the attention module of a pretrained textual   embedding model to model the interaction be-   tween the two modalities since they are both   represented in the textual space . ITA further   aligns the output distributions predicted from   the cross - modal input and textual input views   so that the MNER model can be more practi-   cal in dealing with text - only inputs and robust   to noises from images . In our experiments ,   we show that ITA models can achieve state - of-   the - art accuracy on multi - modal Named Entity   Recognition datasets , even without image in-   formation.1 Introduction   Named Entity Recognition ( NER ) ( Sundheim ,   1995 ) has attracted increasing attention in natu-   ral language processing community . It has been   applied to a lot of domains such as news ( Tjong   Kim Sang , 2002 ; Tjong Kim Sang and De Meulder ,   2003 ) , E - commerce ( Fetahu et al . , 2021 ) , social me-   dia ( Strauss et al . , 2016 ; Derczynski et al . , 2017 )   and bio - medicine ( Do ˘gan et al . , 2014 ; Li et al . ,   2016 ) . Several recent studies focus on improving   the accuracy of NER models through utilizing im-   age information ( MNER ) in tweets ( Zhang et al . ,   2018 ; Moon et al . , 2018 ; Lu et al . , 2018 ) . Most ap-   proaches to MNER use the attention mechanism to   model the interaction between image and text repre-   sentations ( Yu et al . , 2020 ; Zhang et al . , 2021a ; Sun   et al . , 2021 ) , in which image representations are   from a pretrained feature extractor , i.e. ResNet ( He   et al . , 2016 ) , and text representations are extracted   from pretrained textual embeddings , i.e. BERT   ( Devlin et al . , 2019 ) . Since these models are sep-   arately trained on datasets of different modalities   and their feature representations are not aligned , it   is difficult for the attention mechanism to model   the interaction between the two modalities .   Recently , pretrained vision - language ( V+L )   models such as LXMERT ( Tan and Bansal , 2019 ) ,   UNITER ( Chen et al . , 2020 ) and Oscar ( Li et al . ,   2020b ) have achieved significant improvement on   several cross - modal tasks such as image caption-   ing , VQA ( Agrawal et al . , 2015 ) , NLVR ( Young   et al . , 2014 ) and image - text retrieval ( Suhr et al . ,   2019 ) . Most pretrained V+L models are trained   on image - text pairs and simply concatenate text   features and image features as the input of pretrain-   ing . There are , however , two problems . First , texts   in these datasets mainly contain common nouns3176instead of named entitieswhich leads to an induc-   tive bias over common nouns and images . Second ,   despite its important role in pretraining V+L mod-   els , the image modality only plays an auxiliary   role in MNER for disambiguation , and can some-   times even be discarded . These problems make   pretrained V+L models perform weaker than pre-   trained language models for MNER .   Pretrained textual embeddings such as BERT ,   XLM - RoBERTa ( Conneau et al . , 2020 ) and LUKE   ( Yamada et al . , 2020 ) have achieved state - of - the - art   performance on various NER datasets through sim-   ple fine - tuning of pretrained textual embeddings .   Since most of the transformer - based pretrained tex-   tual embeddings are trained over long texts , re-   cent work ( Akbik et al . , 2019 ; Schweter and Akbik ,   2020 ; Yamada et al . , 2020 ; Wang et al . , 2021 ) has   shown that introducing document - level contexts   can significantly improve the accuracy of a NER   model . The attention mechanism in transformer-   based pretrained textual embeddings can utilize   contexts to improve the token representation of a   sequence . Moreover , pretrained V+L models such   as Oscar and VinVL ( Zhang et al . , 2021b ) can use   object tags detected in images to significantly ease   the alignments between text and image features .   Therefore , the images in MNER can be converted   to texts as well so that the image representations   can be aligned to the space of text representations .   As a result , the attention module of the pretrained   textual embeddings have the capability to easily   model the interactions between aligned image and   text representations , without introducing a new at-   tention module . In this paper , we propose ITA ,   a simple but effective framework for Image- Text   Alignments . ITA converts an image into visual   contexts in textual space by multi - level alignments .   We concatenate the NER texts with the visual con-   texts as a new cross - modal input view and then   feed it into a pretrained textual embedding model   to improve the token representations of NER texts ,   which are fed into a linear - chain CRF ( Lafferty   et al . , 2001 ) layer for prediction . In practice , a   MNER model should be robust when there is only   text information , as images may be unavailable or   can introduce noises . Sometimes it is even unde-   sirable to use images as image feature extraction   can be inefficient in online serving . Therefore , we   further propose to utilize the cross - modal inputview to improve the accuracy of textual input view ,   based on cross - view alignment that minimizes the   KL divergence over the probability distributions of   the two views .   ITA can be summarized in four aspects :   1.Object Tags as Local Alignment : ITA locally   extracts object tags and its corresponding at-   tributes of image regions from an object detec-   tor .   2.Image Captions as Global Alignment : ITA sum-   marizes what the image is describing through   predicting image captions from an image cap-   tioning model .   3.Optical Character Alignment : ITA extracts the   texts presented in the image via optical character   recognition ( OCR ) .   4.Cross - View Alignment : we calculate the KL   divergence between the output distributions of   two input views .   We show in experiments that ITA can significantly   improve the model accuracy on MNER datasets   and achieve the state - of - the - art . The cross - view   alignment module can significantly improve both   the cross - modal and textual input views , and bridge   the performance gap between the two views .   2 Approaches   We consider the NER task as a sequence labeling   problem . Given a sentence w={w , · · · , w }   withntokens and its corresponding image I , an   sequence labeling model aims to predict a label   sequence y={y , · · · , y}at each position . In   our framework , we focus on incorporating visual   information to improve the representations of the   input tokens by aligning visual and textual informa-   tion effectively . We use a visual context generator   to convert the image Iinto texts forming visual   contexts w={w , · · · , w}withmtokens . We   then concatenate the input text and visual contexts   as a cross - modal text+image ( I+T ) input view in-   stead of the text ( T ) input view . We feed the I+T   input into a pretrained textual embeddings model to   get stronger token representations of the input sen-   tence . Then the token representations are fed into   a linear - chain CRF layer to get the label sequence   y. To further improve the model accuracy of both   input views , we use the cross - view alignment mod-   ule to align the output distributions of I+T andT3177   input views during training . The architecture of our   framework is shown in Figure 1 .   2.1 NER Model Architecture   We use a neural model with a linear - chain CRF   layer , a widely used approach for the sequence la-   beling problem ( Huang et al . , 2015 ; Akbik et al . ,   2018 ; Devlin et al . , 2019 ) . The input is fed   into a transformer - based pretrained textual embed-   dings model and the output token representations   { r,···,r}are fed into the CRF layer :   p(y|w ) = /producttextψ(y , y , r )   /summationtext / producttextψ(y , y , r )   where θis the model parameters , Y(w)is the set   of all possible label sequences given the input w.   Given the gold label sequence ˆyin the training   data , the objective function of the model for the T   input view is :   L(θ ) = −logp(ˆy|w ) ( 1 )   The loss can be calculated using Forward algo-   rithm .   2.2 Image - text Alignments   The transformer - based pretrained textual embed-   dings have strong representations over texts . There-   fore , ITA converts the image information into tex-   tual space through generating texts from the im-   age so that the learning of the self - attention in thetransformer - based model can be significantly eased   compared with simply using image features from   an object detector . We propose a local ( LA ) , a   global ( GA ) and an optical character alignment   ( OCA ) approaches for alignments .   Object Tags as Local Alignment Given an im-   age , the image information can be decomposed into   a set of objects in local regions . The object tags of   each region textually describe the local information   in the image . To extract the objects , we use an ob-   ject detector ODto identify and locate the objects   in the image :   a , o = OD(I);where   a={a , a,···,a}ando={o , o , · · · , o }   The attribute predictions from the object detector   contain multiple attribute tags afor each object   o. We linearize and sort the objects in a descend-   ing order based on the confidences of the detection   model . For each object , we heuristically keep 0 to   3 attributes with confidence scores above a thresh-   oldm . We linearize the attributes and put the at-   tributes before the corresponding objects since the   attributes are the adjectives describing the object   tags . As a result , we take the predicted lobject tags   oand their attribute tags afrom the object detector   as the locally aligned visual contexts w :   w={a , o , a , o,···,a , o }   Image Captions as Global Alignment Though   the local alignment can localize the image into3178objects , the objects can not fully describe the of   the whole image . Image captioning is a task that   predicts the meaning of an image . Therefore , we   align the image into kimage captions by an image   captioning model IC :   { w , w,···,w}=IC(I )   where { w , w,···,w}are captions generated   from beam search with kbeams . We concatenate   thekcaptions together with a special separate to-   ken [ X ] to form the aligned global visual contexts   w :   w=[w,[X],w,[X],···,[X],w ]   The exact label ( e.g. “ [ SEP ] ” in BERT ) of the   special [ X ] token depends on the selection of em-   beddings .   Optical Character Alignment Some image con-   tain text when they are created to enrich the seman-   tic information that the images want to convey . In   order to better understand this type of image , we   use an OCR model to identify and extract the texts   in the image :   w = OCR ( I )   where ware the texts extracted by the OCR   model . Note that wmay be an empty text if   there is no text in the image .   We concatenate the input sentence and our   aligned visual contexts to form the I+Tinput view   ˆw= [ w;w ] , where wcan be one of w , w ,   wor the concatenation of all ( we denote it as   All ) . The transformer - based embeddings are fed   with the I+T input view and then output image-   text fused token representations for each token   { r,···,r } . The token representations are fed   into the CRF layer to get the probability distri-   bution p(y|ˆw ) . Similar to Eq . 1 , the objective   function of the model for the I+T input view is :   L(θ ) = −logp(ˆy|ˆw ) ( 2 )   Cross - View Alignment There are several limita-   tions in incorporating images into NER prediction :   1 ) the images may not available in testing ; 2 ) align-   ing images to texts requires several pipelines in   pre - processing instead of an end - to - end manner ,   which is so time - consuming that it is not applicable   to some time - critical scenes such as online serving ;   3 ) the noises in the image can mislead the MNERmodel to make wrong predictions . To alleviate   these issues , we propose Cross - View Alignment   ( CV A ) , which targets at reducing the gap between   theI+T andTinput views over the output distri-   butions so that the MNER model can better utilize   the textual information in the input . During train-   ing , CV A minimizes the KL divergence over the   probability distribution of I+T andTinput views :   L(θ)=KL(p(y|ˆw)||p(y|w ) ) ( 3 )   Since the I+T input view has additional visual in-   formation in the input and we want the Tinput   view to match the accuracy of I+T input view , we   only back - propagate through p(y|w)in Eq . 3 .   Therefore , Eq . 3 is equivalent to calculating the   cross - entropy loss over the two distributions :   L(θ)=/summationdisplayp(y|ˆw ) logp(y|w ) ( 4 )   As the set of all possible label sequences Y(x)is   exponential in size , we calculate the posterior dis-   tributions of each position p(y|w)andp(y|ˆw )   through forward - backward algorithm to approxi-   mate Eq . 4 :   p(y|∗)∝/summationdisplay / productdisplayψ(y , y , r )   ×/summationdisplay / productdisplayψ(y , y , r )   L(θ)=/summationdisplayp(y|ˆw ) logp(y|w ) ) ( 5 )   where rrepresents either rorr .   Training During training , we jointly train Tand   I+T input views with the training objective in Eq .   1 and 2 together with the CV A alignment training   objective in Eq . 5 . As a result , the final training   objective for ITA is :   L = L+L+L   3 Experiments   We conduct experiments on two MNER datasets .   To show the effectiveness of our approaches , we   use two embedding settings and compare our ap-   proaches with previous multi - modal approaches.31793.1 Settings   Datasets We show the effectiveness of our ap-   proaches on Twitter-15 , Twitter-17 and SNAP   Twitter datasetscontaining 4,000/1,000/3,357 ,   3,373/723/723 and 4,290/1,432/1,459 sentences   in train / development / test split respectively . The   Twitter-15 dataset is constructed by Zhang et al .   ( 2018 ) . The SNAP dataset is constructed by Lu   et al . ( 2018 ) and the Twitter-17 dataset is a filtered   version of SNAP constructed by Yu et al . ( 2020 ) .   Model Configuration For token representations ,   we use BERT base model to fairly compare with   most of the recent work ( Yu et al . , 2020 ; Zhang   et al . , 2021a ; Sun et al . , 2021 ) . Recently , XLM-   RoBERTa has achieved state - of - the - art accuracy on   various NER datasets by feeding the input together   with contexts to the model . To further utilize the   visual contexts in transformer - based embeddings ,   we use XLM - RoBERTa large ( XLMR ) model as   another embedding in our experiments . To extract   object tags and image captions of the image , we   use VinVL ( Zhang et al . , 2021b ) , which is a pre-   trained V+L model based on a newly pretrained   large - scale object detector based on the ResNeXt-   152 C4 architecture . We use the object detection   module of VinVL to predict object tags and their   corresponding attributes . The number of object   tags and attributes varies over the images and is no   more than 100 . We set the threshold mto be 0.1   for keeping the attributes of each object . For image   captions , we use VinVL large model finetuned on   MS - COCO ( Lin et al . , 2014 ) captionswith CIDEr   optimization ( Rennie et al . , 2017 ) . In our exper-   iments , we use a beam size of 5with at most 20   tokens for prediction and keep all the 5captions   as the visual contexts . For OCR , we use Tesser-   act OCR(Smith , 2007 ) , which is an open source   OCR engine . We use the default configuration of   the engine to extract texts in the image .   Training Configuration During training , we   finetune the pretrained textual embedding model   by AdamW ( Loshchilov and Hutter , 2018 ) opti-   mizer . In experiments we use the grid search to   find the learning rate for the embeddings within   [ 1×10,5×10 ] . For BERT embeddings , we   finetune the embeddings with a learning rate of   5×10with a batch size of 16 . For XLMR   embeddings , we use a learning rate of 5×10   and a batch size of 4instead . For the learning   rate of the CRF layer , we use a grid search over   [ 0.05,0.5]and[0.005,0.05]for BERT and XLMR   respectively . The MNER models are trained for 10   epochs and we report the average results from 5   runs with different random seeds for each setting .   3.2 Results   In Table 1 , we compare our approaches with our   baselines with different training and evaluation   modalities ( Tfor the text - only input view and I+T3180for the multi - modal input view ) . Results show   that ITA models are significantly stronger than our   BERT - CRF andXLMR - CRF baselines ( Student ’s   t - test with p < 0.05 ) . For the aligned visual con-   texts , LA , GA and OCA are competitive in most   of the cases . To show the effectiveness of CV A ,   we report the evaluation results of both input views   in evaluation . With CV A , the accuracy of both   input views can be improved , especially the Tin-   put view . CV A can improve the Tinput view to   be competitive with I+T input view . Moreover ,   the combination of all the alignments ITA - All   can further improve the model accuracy in most   of the cases . The accuracy of the MNER models   can be significantly improved if we use XLMR em-   beddings , which shows the importance of the text   modality in MNER . With XLMR embeddings , the   model accuracy can be further improved with ITA .   The relative improvements over the baseline mod-   els are sometimes higher with XLMR than with   BERT , which shows that the visual contexts can be   further utilized with stronger embeddings .   In Table 2 , we compare ITA with previous state-   of - the - art approaches . For previous approaches ,   we report the results including OCSGA , UMT ,   RIV A , RpBERT , UMGF , which are the proposed   approaches of Wu et al . ( 2020 ) , Yu et al . ( 2020 ) ,   Sun et al . ( 2020 ) , Sun et al . ( 2021 ) and Zhang et al .   ( 2021a ) respectively . For fair comparison , we re-   port the results of these models based on the BERT   base embeddings . Moreover , since most of these   previous approaches report the best model accuracy   instead of the averaged model accuracy , we use the   best model accuracy of ITA - All over5runs .   We also report our reproduced results of UMT , Rp-   BERT andUMGF on the corresponding datasets .   The results show that ITA - All outperforms   all of the previous approaches . On the SNAP   dataset , the reported accuracy of RpBERTis   competitive with ITA - All . However , we find   that the accuracy of our reproduced RpBERT   is significantly lower than the reported accuracy ,   even after careful check of the source code and   hyper - parameter tuning . Moreover , the fact that   ourBERT - CRF baseline achieves competitive ac-   curacy with previous state - of - the - art multi - modal   approaches shows that most of the previous work   has not fully explored the strength of the text repre-   sentations for the task .   Discussion about Textual Modules As we have   shown in Table 1 and 2 , the textual baselines ( i.e.   BERT - CRF ) of previous work are significantly   lower than that of ours . In most of the previ-   ous MNER architectures , the textual modules are   mainly based on the baseline architectures with   some modifications . We further show the baselines   of previous work are not well - trained and how the   multi - modal approaches perform with stronger tex-   tual modules . In Table 3 , we rerun the BERT - CRF   baseline based on the released codes of UMT .   Based on the code of UMT , we tried to improve   the baseline models in the code by using the same   loss function as ours . The accuracy of BERT-   CRF models in the code are significantly improved   but the UMT models based on the improved code   are not improved and even get worse in Twitter-   17 . Therefore , we suspect the UMT model can-   not be further improved even with stronger textual   modules . Zhang et al . ( 2021a ) also reported the   baseline based on the implementation of Yu et al .   ( 2020 ) , so we suspect the UMGF model can not   be improved as well . Therefore , the under - trained   textual baselines of previous work make the effec-   tiveness of the images unclear and we show that   some of the MNER models perform even weaker   than our BERT - CRF model .   3.3 Comparison with Other Variants   To further show the effectiveness of ITA , we per-   form several comparisons between ITA and the   following variants of the MNER model in Table 4 :   ITA - Random : We generate random image - text   pairs for the model . For each sentence , we ran-   domly select the image in the dataset and generate   the corresponding visual contexts . The noises of   random visual contexts make the model accuracy3181   drop slightly comparing with our BERT - CRF base-   line , which shows the improvement of our approach   is from the visual contexts rather than extending   the input sequence length the embeddings .   ITA - Joint : It is an ablated model of ITA-   All . We train the ITA - All model for both input   views without the CV A loss in Eq . 5 . The model   accuracy is improved moderately with only the T   input view while our ITA - All can improve   both input views significantly , which shows the   effectiveness of the CV A module of ITA .   ITA - LAand ITA - GA : We conduct experi-   ments to see how the accuracy changes when using   weaker image features . We use Bottom- Up features   proposed by Anderson et al . ( 2018 ) for object detec-   tion and image captioning . The captioning model is   a pretrained image captioning modelproposed by   Luo et al . ( 2018 ) with the Bottom - Up features and   self - critical training ( Rennie et al . , 2017 ) . Results   show that there is no significant difference between   the visual contexts from Bottom - Up features and   VinVL features . Therefore , our approaches can   utilize other off - the - shelf vision models to extract   visual contexts .   ITA - OCA : We conduct experiments to see   how the accuracy changes when using stronger   OCR models . We use PaddleOCRfor the exper-   iment , which is one of the newest open resource   lightweight OCR system . Results show that the   model accuracy can be slightly improved compar-   ing with ITA - OCA , which shows the ITA models   can be improved by using better OCR models .   BERT - CRF : Instead of ITA , we can di-   rectly feed the image region features generated   from an object detector into the BERT . We use   ResNet-152 model to generate region features and   then feed the features into a linear layer to project   the region features into the same space of text   features in the BERT . Moreover , we compare the   model with RpBERT w/o Rp , which is an ablated   model of RpBERT and is equivalent to BERT-   CRF+ over the usage of BERT embeddings .   Sun et al . ( 2021 ) showed RpBERT w/o Rp can   improve the model accuracy compared with their   baseline . However , our results show that the model   accuracy slightly drops comparing with our BERT-   CRF , which shows that it is difficult for the atten-   tion module of BERT to learn the relations of the   unaligned representations of two modalities .   VinVL - CRF : To show how the pretrained V+L   models perform on the NER task , we use VinVL   since it is a very recent state - of - the - art pretrained   V+L model on a lot of multi - modal tasks . We   feed the VinVL model with texts and images in the   MNER datasets and finetune the model over the   task . We take the text representations output from   VinVL as the input of the CRF layer . The accuracy   of the finetuned VinVL model drops significantly   compared to the BERT model , which shows that   the inductive bias of the pretrained V+L model   hurts the model accuracy on MNER .   BERT+VinVL - CRF : As the VinVL model may   lead to an inductive bias over the common nouns   and the image , we jointly finetune the BERT and   VinVL models and concatenate the output text rep-   resentations of the two models . The accuracy is   improved on a moderate scale , which shows BERT   is complementary to VinVL for MNER.3182   3.4 Analysis   Effect of the Number of Captions Using more   captions output from the captioning model can im-   prove diversities of the visual contexts but can add   noises to them as well . To better understand how   the number of captions affects the model accu-   racy , we change the beam size and keep all the   sentences output from the captioning model . The   trends in Figure 2 show that the model accuracy   increases until 5captions for all the datasets and   gradually drops when the number of captions fur-   ther increases for Twitter-15 and 17 datasets . The   observation shows that using 5captions keeps a   good balance between the diversities and correct-   ness of the captions .   How ITA Eases the Cross - Modal Alignments   Previous work such as Moon et al . ( 2018 ) ; Sun   et al . ( 2021 ) visualized modality attention in sev-   eral cases to show the effectiveness of their ap-   proaches . However , visualizing the multi - layer   attention in transformer - based embeddings is rela-   tively difficult . Instead of studying special cases ,   we statistically calculate the averaged L2 distance   between token representations randrfrom two   input modalities to show how the token representa-   tions depend on image information . In Figure 3 , the   L2 distance ITA - All is significantly larger than that   ofBERT - CRF+ImgFeat . Besides , the standard   deviation of BERT - CRF+ImgFeat is very large .   The observations show the image region features   make the alignment become difficult and unstable   while our visual contexts can significantly ease the   cross - modal alignments . Moreover , with CV A , the   L2 distance becomes much smaller and stable as   CV A aligns the two input views to reduce the depen-   dence on images , which shows the MNER model   can better utilize the textual information with CV A.   How Images Affect the NER Prediction To   study the effectiveness of the images over eachlabel , we show a comparison between our model   and our baselines in Table 5 . When the relative   improvement of the F1 score is larger than 0.5 , the   relative improvement of precision is larger than that   of recall . The observation shows that the main im-   provement of MNER is mainly because the images   can help the model to reduce false - positive predic-   tions for disambiguation on uncertain entities .   4 Related Work   Multi - modal Named Entity Recognition Most   of the previous approaches to MNER focus on   the interaction between image and text features   through attention mechanisms . Moon et al . ( 2018 )   proposed a modality attention network to fuse the   text and image features before the input to the BiL-   STM layer . Lu et al . ( 2018 ) additionally used a   visual attention gate for the output features of the   BiLSTM layer . Zhang et al . ( 2018 ) proposed an   adaptive co - attention network after the BiLSTM   layer to model the interaction between image and   text . Recently , Wu et al . ( 2020 ) proposed OCSGA ,   which use object labels to model the interaction   between image and object labels in an additional   dense co - attention layer . Compared with the work ,   we show a simpler and more effective way to uti-   lize object labels and additionally use other align-   ment approaches to further improve the model ac-   curacy . Yu et al . ( 2020 ) proposed UMT , which   utilized a multi - modal interaction module and an   auxiliary entity span detection module for MNER .   Zhang et al . ( 2021a ) proposed UMGF , which uti-   lizes a pretrained parser to create the graph connec-   tion between visual object tags and textual words .   They used a graph attention network to fuse the   textual and visual features . In order to better   model whether the image is related to the text , Sun   et al . ( 2021 ) proposed RpBERT , which addition-   ally trains on a text - image relation classification   dataset proposed by Vempala and Preo¸ tiuc - Pietro   ( 2019 ) to prevent the negative effect of noisy im-   ages . Comparing with RpBERT , we use CV A to let   the NER model better utilize the input sentences   without such kinds of supervision . All of these   approaches focus on fusing the image and text fea-   tures through the attention mechanism but ignore   the gap between the image and text features while   we propose to fully utilize the attention mecha-   nism in the pretrained textual embeddings through3183   aligning image features into textual space . Besides ,   some cross - media research also shows the effective-   ness of OCR texts ( Chen et al . , 2016 ; Wang et al . ,   2020 ) and object tags ( Wu et al . , 2016 ) have been   shown . Most of the approaches introduced a new   attention module over cross - modal features while   in comparison ITA effectively utilizes the attention   module in the pretrained textual embeddings .   Pretrained Vision - Language Models Inspired   by related work on language model pretraining ,   visual - language pretraining ( VLP ) has recently at-   tracted a lot of attention ( Li et al . , 2019 ; Lu et al . ,   2019 ; Chen et al . , 2020 ; Tan and Bansal , 2019 ; Li   et al . , 2020a ; Yu et al . , 2021 ; Zhang et al . , 2021b ) .   The pretrained V+L models are pretrained on large-   scale image - text pairs and have achieved state - of-   the - art accuracy over various vision - language tasks   such as image captioning , VQA , NLVR and image-   text retrieval . Recently , Li et al . ( 2020a ) proposed   Oscar to add object tags in pretraining so that self-   attention can learn the image - text alignments easily .   Following Oscar , Zhang et al . ( 2021b ) proposed   VinVL to train a large - scale object detector to im-   prove the pretrained V+L model ’s accuracy . Com-   paring with VLP , MNER is a totally different task .   Firstly , the image - caption pairs are given in VLP   and the image and text are equally important in   pretraining for general representations . Therefore ,   using global alignment is meaningless for VLP but   makes sense for MNER . In MNER , the input text   is not the caption of the image and the image may   not adds additional information to the input text .   Secondly , though captions and object tags are of-   ten utilized in VLP , how to effectively utilize the   captions and object tags of the image in MNER is   rarely considered . Finally , besides the local andglobal alignments , another aspect of ITA is the op-   tical character alignment and cross - view alignment ,   which is rarely considered in VLP .   5 Conclusion   In this paper , we propose Image - Text Alignments   for multi - modal named entity recognition , which   convert images into object labels , captions and   OCR texts to align the image representations into   textual space in a multi - level manner and form   a cross - modal input view . The model can effec-   tively utilize attention module of the transformer-   based embeddings . Considering noises , availability   of images and inference speed for practical use ,   we propose cross - view alignment , which let the   MNER models better utilize the text information   in the input . In our experiments , we show that ITA   significantly outperforms previous state - of - the - art   approaches on MNER datasets . We also show that   most of the previous work failed to train a good   textual baseline while our textual baseline can eas-   ily match or even outperform previous multi - modal   approaches . In analysis , we further analyze how   ITA eases the cross - modal alignments and how the   images affect the NER prediction .   Acknowledgements   This work was supported by the National Natu-   ral Science Foundation of China ( 61976139 ) and   by Alibaba Group through Alibaba Innovative Re-   search Program .   References318431853186   A Appendix   A.1 Details of Experiment Settings   We run our code on Tesla V100 GPU with 16 GB   memory . It takes about two hours to train a model .   The size of model parameter is approximately equal   to size of BERT / XLMR embeddings .   A.2 Details of OCA   Table 6 shows that the OCR system only finds about   26 % sentences have texts in the image and the   extracted texts have an average of 28 tokens . The   statistics show that ITA - OCA can help to improve   the model accuracy with only 26 % of the samples   have OCR texts .   A.3 Case Study   Despite that images can generally help to improve   the accuracy of the NER model , there are a lot of   cases that the images may contain misleading infor-   mation to hurt the model prediction . We study two   cases for LA nad GA : 1 ) the entities are wrongly   predicted by BERT - CRF baseline but are correctlypredicted by ITA ; 2 ) the entities are wrongly pre-   dicted by ITA without CV A but are correctly pre-   dicted by the baseline and ITA with CV A. Figure 4   shows the two cases with two samples for each .   Figure 4 ( a ) shows the first case , which shows   the importance of the visual contexts . The base-   line model failed to recognize the person entities   “ TWICE ” and “ Harry Potter ” possibly because the   two words are usually an adverb and a book name   respectively . For the I+T input view , our MNER   model is able to recognize the hints such as “ two   girls ” , “ young girl ” , “ a couple of young men ” and   “ woman ” in the visual contexts and then correctly   predict the two entities . Figure 4 ( b ) shows the   second case , which shows how the noises from the   image mislead the model predictions . There are   three- and two - person entities in gold labels but   the visual contexts indicate that the top right image   has “ two baseball players ” and the bottom right   image has only “ a woman ” . As a result , ITA with-   out CV A only predict two and one person entities   according to the visual contexts in the two sam-   ples respectively . However , with CV A , ITA takes   a good balance in utilizing the textual and visual   information and correctly predicts the entity labels   in both TandI+T input views .   For OCA , we study how the extracted texts can   help model prediction . In the upper sample of Fig-   ure 5 , there are two “ Donald ” words in the image .   The baseline model failed to identify the latter one   while ITA - OCA can successfully identify both of   them . In the bottom of Figure 5 , the texts in the im-   age are mainly talking about “ HARRY STYLES ” ,   which helps the model prediction .   A.4 Discussion   In our paper , we use the captioning and object   detection model based on MSCOCO and visual   genome . The model performance could be im-   proved if we use domain - specific models ( Twitter   domain ) . For OCA , the model accuracy may be   poor if the OCR system does not support a certain   language .   A.5 Loss Function Comparison with UMT   In the codes of UMT , the BERT embeddings tok-   enize the token in a sentence into subtokens . The   codes use the first subtoken as the token represen-   tation to predict the corresponding label . However ,   for the other subtokens , the codes use a special   label “ PAD ” for prediction . Therefore , the target   labels are changed . For example , the original label31873188   sequence is “ B - X , I - X , O , B - X , O , O ” but now it   becomes “ B - X , PAD , PAD , I - X , O , B - X , O , PAD ,   O ” . As a result , the exact training objective changes   compared with the training objective in the paper   of UMT . We improve the code by removing all   the “ PAD ” labels and just use the first subtoken   of each token as the token representation . Our im-   proved baseline model is significantly improved ,   while the accuracy of UMT model in the improved   code can not be further improved.3189