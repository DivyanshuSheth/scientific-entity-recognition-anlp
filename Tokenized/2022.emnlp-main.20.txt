  Jannis Bulian Christian Buck Wojciech Gajewski   Benjamin B ¨orschinger Tal Schuster   Google Research   { jbulian , cbuck , wgaj , bboerschinger , talschuster } @google.com   Abstract   The predictions of question answering ( QA )   systems are typically evaluated against man-   ually annotated ﬁnite sets of one or more an-   swers . This leads to a coverage limitation   that results in underestimating the true perfor-   mance of systems , and is typically addressed   by extending over exact match ( EM ) with pre-   deﬁned rules or with the token - level Fmea-   sure . In this paper , we present the ﬁrst system-   atic conceptual and data - driven analysis to ex-   amine the shortcomings of token - level equiva-   lence measures .   To this end , we deﬁne the asymmetric no-   tion of answer equivalence ( AE ) , accepting an-   swers that are equivalent to or improve over   the reference , and publish over 23k human   judgments for candidates produced by multi-   ple QA systems on SQuAD.Through a care-   ful analysis of this data , we reveal and quan-   tify several concrete limitations of the Fmea-   sure , such as a false impression of graduality ,   or missing dependence on the question .   Since collecting AE annotations for each eval-   uated model is expensive , we learn a BERT   matching ( BEM ) measure to approximate this   task . Being a simpler task than QA , we ﬁnd   BEM to provide signiﬁcantly better AE ap-   proximations than F , and to more accurately   reﬂect the performance of systems .   Finally , we demonstrate the practical utility of   AE and BEM on the concrete application of   minimal accurate prediction sets , reducing the   number of required answers by up to ×2.6 .   1 Introduction   Automatically assessing the answers given by ques-   tion answering ( QA ) systems for correctness can   be nontrivial . This was already recognized in the   early large - scale QA evaluation work of V oorheesand Tice ( 2000 ) , leading them to suggest string-   matching patterns for approximation , yet recogniz-   ing their limitations :   “ it is quite difﬁcult to determine auto-   matically whether the difference between   a new string and a judged string is sig-   niﬁcant with respect to the correctness of   the answer . ”   Despite the early recognition of the importance   and difﬁculty of evaluation for question answering ,   surprisingly little progress has been made . As of   today , QA research in the NLP community relies   almost exclusively on two token - level metrics , Ex-   act Match ( EM ) and Token F(F ) . Unfortunately ,   both fall short of capturing the difference between   signiﬁcant and insigniﬁcant span differences . In   Section 2 , we ﬁrst document these limitations , pro-   viding a detailed analysis and examples from the   SQuAD dataset . Moreover , we extend this analysis   with our new ﬁndings : the inherent asymmetry of   the task and the reliance on the question , also pro-   viding examples from the SQuAD dataset . We also   identify cases where the context is important .   One obvious limitation of token - level measures   is their direct dependence on the diversity of the   reference answers collected for the dataset ( Chen   et al . , 2019 ) . While this could be addressed by   extending the annotations , this is both expensive   and has diminishing returns as the true collection   of all correct answers might be large . In contrast ,   we focus on improving the equivalence measure   beyond token matching and thereby increase the   answer inclusiveness over any reference set .   To facilitate research on this issue , we introduce   a well - deﬁned Answer Equivalence ( AE ) task along   with the release of a new dataset ( Section 3 ) . We   collect human annotations on SQuAD examples ,   comparing gold answers with model predictions .   We utilize this data in several ways . First , we   use the human judgments to better understand how291well Fand EM capture equivalence in the machine   reading setting . We demonstrate that ( 1 ) both met-   rics underestimate the quality of candidate answers ,   ( 2 ) are highly reliant on the number of available ref-   erences , and ( 3 ) that Fgives the false impression   of a gradual rating , while really there is a similar   ratio of equivalent to non - equivalent answers for   non - zero F.   We further propose to learn a measure that ap-   proximates the AE relations ( Section 4 ) . To study   this , we introduce BERT matching ( BEM ) , that   uses a BERT model trained on our data to predict   equivalence . Through multiple experiments with   three QA systems , we show that BEM better corre-   lates with human judgments , compared to F.   Finally , we demonstrate the utility of AE and   BEM on the concrete application of returning small   and accurate answers sets ( Section 5 ) . Build-   ing on the expanded admission conformal predic-   tionframework ( Fisch et al . , 2021 ) , we show that   AE and BEM signiﬁcantly reduce the number of   answers in the prediction sets while including a   correct answer with arbitrarily high probability .   Thanks to its simplicity , BEM can easily replace F   in many other applications that leverage QA predic-   tions ( e.g. , Honovich et al . , 2021 , 2022 ; Eyal et al . ,   2019 ; Fabbri et al . , 2021 ; Schuster et al . , 2021a ) .   Our main contributions include :   •Deﬁning the answer equivalence ( AE ) task .   •Releasing a large dataset with AE annotations .   •A data - driven analysis of the shortcomings of   EM andFas evaluation measures for QA .   •A learned AE measure ( BEM ) that better corre-   lates with human judgments and enables practi-   cal improvements for QA - based applications .   2 Common token - level metrics : EM & F   Notation . We refer to the “ gold ” answers from   humans as reference answers . Each question is   paired with a set of one or more gold answers . We   refer to the predicted answers produced by question   answering models as candidate answers .   The most popular metrics are Token F(F ) and   Exact Match ( EM ) , deﬁned as follows :   Deﬁnition 2.1 ( Exact Match ) .Given a set of ref-   erencesAand a candidate answer c , cis an exact   match forAiffc∈A.   Deﬁnition 2.2 ( Token F).Given a set of refer-   encesA , a candidate answer cand a tokenizationfunctiont , the Token Fofcwith respect to Aand   tis the maximum over the token - wise Fscores   betweencand eacha∈A , i.e. :   max2|t(a)|   |t(a)∩t(c)|+|t(c)|   |t(a)∩t(c)|   It is also common to remove stop words and   punctuation before computing either EM or F.   2.1 Limitations of EM and F   We brieﬂy recount and provide examples for known   short - comings . EM and Fimperfectly capture the   answer equality and can over- , or underestimate   the performance of models ( Kocmi et al . , 2021 ;   Gehrmann et al . , 2021 ; Chen et al . , 2019 , 2020 ) .   Strictness . EM is often too strict , especially   when only a few gold answers are available . Con-   sider Example 1 in Table 1 : even though the dif-   ference is a minor surface variation , the candidate   receives an EM - score of 0.0 – and also an Fof0.0   ( unless further tokenization is done ) .   Granularity . Both EM and Fdo not distinguish   between signiﬁcant and insigniﬁcant span differ-   ences . This can be misleading and sometimes sur-   faces in surprising ways . Example 4 in Table 1   shows a reference / candidate pair receiving a rela-   tively high Fscore of 0.67 – with a completely   wrong candidate answer .   Assessment of numbers and units . Answers   can be equivalent when they express identical val-   ues in different units ( and the question does not   specify a speciﬁc unit ) , e.g. Example 8 in Table 1 .   Similar and frequent problems arise from approxi-   mate quantities ( e.g. the population of a country ) ,   metric vs. imperial units , percentages and absolute   values , and spelled out numbers .   2.2 Further limitations   We ﬁnd Fhas several more speciﬁc shortcomings   that make it less suitable for QA evaluation .   Asymmetry . A candidate answer can improve   over the reference by adding relevant information ,   such as Example 3 in Table 1 . In that case it should   get credit in evaluation , even though it is not strictly   equivalent . Conversely , omitting relevant informa-   tion ( which is present in the reference answer ) in   the candidate answer should be discouraged .   On the other hand , if the candidate answer re-   moves irrelevant or misleading information , it also   improves the reference answer , and , even though292   it is not strictly equivalent , should get full credit in   evaluation . Both EM and Ffail to recognize such   cases , because they are symmetric .   The question matters . Whether two answers are   equivalent might depend on the question . In Exam-   ple 6 in Table 1 ‘ secondary school ’ is equivalent   to ‘ secondary school teachers ’ only because the   question explicitly asked for teachers .   Note that this is a particular example of a fused-   head construction , which occur frequently in ma-   chine reading datasets . Elazar and Goldberg ( 2019 )   discuss this phenomenon for numerical cases . The context matters . More rarely than with   questions , the context can determine whether two   answers are equivalent . Consider Example 7 in   Table 1 . ‘ Queen Bees ’ only qualiﬁes as a possible   match for the reference answer ‘ women ’ , because   it is used as a metaphor in the context.2933 The AE task deﬁnition and dataset   To address the issues mentioned in Section 2 , we   require the answer equivalence relation to be asym-   metric and to be conditional on both question and   context . We want an ideal metric to give credit for   a candidate answer that is at least as good as the   reference , i.e. it should capture all the important   information in the reference and not add irrelevant ,   or worse , misleading information . A candidate   answer that improves over the reference by either   removing misleading or irrelevant information , or   adding more relevant information , should receive   full credit . More formally , we deﬁne :   Deﬁnition 3.1 ( Answer Relation ) .Letqbe a query   and leta , abe answers contained in a context c.   Thenais a good answer in place of aif both the   following are satisﬁed :   ( i)adoes contain at least the same ( or more )   relevant information as a , taking into account   qandc ; in particular it does not omit any   relevant information present in a.   ( ii)acontains neither misleading or excessive   superﬂuous information not present in a , tak-   ing into account qandc .   Note that this approach does not aim to replace   the regular QA annotations , but expands on them to   create larger , more inclusive sets of acceptable an-   swers . Most studies rely on token - level measures to   approximate this expansion . However , as detailed   in Section 2 , such measures are inadequate .   3.1 Rating task   We design the rating task for answer equivalence   as follows : the raters are presented with ( i ) the   question , ( ii ) context from Wikipedia that contains   the answer text , ( iii ) the reference answer ( referred   to as ‘ ﬁrst answer ’ ) , and ( iv ) a candidate answer   ( referred to as ‘ second answer ’ ) . They are then   asked the following yes / no questions in sequence :   If a rater answers ‘ yes ’ to the ﬁrst question , the   rating is terminated and the following questions are   not shown . Similarly , if a rater answers ‘ yes ’ to the   second question , the rating ends and Q3 / Q4are not   shown . Otherwise , all four questions are answered .   3.2 Data collection   We use the above task to annotate examples gen-   erated from the SQuAD dataset ( Rajpurkar et al . ,   2016 ) , labelling examples from both train anddev .   For the training examples , we partition the   SQuAD train set 5 - ways at random , and train ﬁve   Albert models ( Lan et al . , 2019 ) , each excluding   one of the partitions ( i.e. training on 80 % of the   available data ) . We then use each model to generate   predictions for the unseen examples in its excluded   partition , thereby generating predictions for the en-   tire SQuAD train set . We rate all examples where   the prediction does not match the reference .   Three different models are used to make predic-   tion on the SQuAD dev set : BiDAF ( Seo et al . ,   2016 ) , XLNet ( Yang et al . , 2019 ) , and Luke ( Ya-   mada et al . , 2020 ) . As before we remove predic-   tions that match any of the ( up to 6 ) reference   answers . Otherwise we pair the prediction with   reference and annotate all combinations .   For XLNet predictions we collect up to 4 anno-   tations ; for Luke and BiDAF we obtain only a   single annotation . Overall , we collect 14,170an-   notations for 8,565(question , context , reference ,   candidate)-tuples for 4,369non - EM predictions .   See the Appendix ( Table 10 ) for detailed statistics .   3.3 Train / Dev / Test splits   We provide two partitionings of the dev data : ei-   ther split by the system producing the candidate   as introduced above , see Table 10 , or in a 30/70   dev / test split , see Table 2 . For the latter we select   examples with no document overlap between dev   and test sets . In either case , the training data is the   same.294   3.4 Answer Equivalence deﬁnition   Each annotation consists of four binary answers   to the questions given in Section 3.1 . We deﬁne   the candidate to be equivalent to the reference , if   it is rated as ( 1 ) not completely different ( i.e. Q1   is answered ‘ no ’ ) and ( 2 ) containing at least as   much relevant information and not more irrelevant   information as the gold answer ( i.e. Q2is answered   ‘ yes ’ ) . In all other cases we deﬁne the candidate to   benot equivalent to the reference .   3.5 Quantitative analysis   Overall our raters consider 55 % of the pairs that are   not exact matches to be equivalent . At 69.9 % that   rate is higher for the candidates from SQuAD train   but also differs across systems . As expected and   shown in Table 11 , ratings for AE examples pro-   duced by BiDAF are less likely to be rated equiva-   lent than those produced by XLNet or Luke .   We collect a total of 14,170ratings on SQuAD   dev , for 8,565AE - examples , i.e. ( context , ques-   tion , reference , candidate)-tuples ( Table 10 in the   Appendix ) . 6,062examples have a single rating ,   17have two , 1,870have three and 616have four .   We aggregate the ratings per example using ma-   jority voting . Among all examples with multiple   annotations , over 88 % have full agreement between   raters . Similarly , selecting a random pair of ratings   for the same example , has a 92 % chance of agree-   ment . Finally , we compute Krippendorff ’s αat   0.84 , conﬁrming good inter - annotator agreement .   As expected , Figure 1 shows the vast majority of   completely different answers are cases of no token   overlap between reference and candidate answer .   Interestingly , all buckets with F>0contain a   sizeable portion of equivalent answers .   3.6 Qualitative analysis   Comparing raters ’ judgment on the Answer Equiv-   alence task with F1 scores shows typical disadvan-   tages , and limitations of F1 . In the area of high F1   scores ( i.e.>0.6 ) , raters ﬁnd answers with signiﬁ-   cantly changed meaning , despite having substantial   overlap with the reference answer . For example , ‘ O   notation ’ can not necessarily be understood as ‘ Big   O notation ’ ( F1 = 0.8 ) , or ‘ June ’ is not equivalent   to‘every June ’ ( F1 = 0.66 ) .   On the other hand , in the area of low F1 scores   ( e.g.<0.3 ) , we can see answers that convey the   same core content as the reference but with added   information that an informed human would likely   give . For example , for a question ‘ In the most   basic sense what did a Turing machine emulate ? ’ ,   an answer ‘ a very robust and ﬂexible simpliﬁcation   of a computer ’ is more informative than the bare   minimum reference ‘ a computer ’ .   Moreover , the candidate can add crucial infor-   mation , thereby improving over the reference : e.g.   for the question : ‘ What acquired condition results   in immunodeﬁciency in humans ? ’ , raters rate posi-   tively an answer ‘ HIV / AIDS , or the use of immuno-   suppressive medication ’ , despite the reference be-   ing only ‘ HIV / AIDS ’ .   3.7 Quantifying limitations of F   In our data we observe three further issues with F.   Firstly , having values on a scale from 0to1gives   the impression of a gradual rating , with e.g. 0.6   being twice as correct as0.3 . Almost all of the   answers with a rating greater than 0were rated as   equivalent .   Secondly , we observe that Fsystematically un-   derestimates model performance compared to hu-   man judgment . Table 6 shows that both EM and F   severely underestimate the quality of predictions .   Lastly , Fis highly dependant on the number of   available references . Table 7 quantiﬁes how Fbe-   comes a better estimator of human judgment when   more references become available . This is an issue295because additional references are not available or   expensive and laborious to generate .   4 Predicting Answer Equivalence   In the previous section we deﬁned the AE task   and discussed the value of such annotations . In   practice , however , obtaining human ratings is slow   and expensive . This is especially true for open-   domain or generative QA models , where the space   of possible answers is inﬁnite . Instead , we train a   classiﬁer to predict AE for any answer pair .   We ﬁnd the classiﬁer ’s score to be more accurate   ( § 4.1.2 ) , and provide practical gains ( § 5 ) . We con-   jecture that the high performance of the classiﬁer   is due to the easier nature of the AE task com-   pared to QA . While , under some assumptions , the   question - answering problem is NP - complete ( We-   ston et al . , 2015 ) , the veriﬁcation of a candidate   answer against an already given true answer is sig-   niﬁcantly simpler .   The classiﬁcation task . Given the four - tuple   ( context , question , reference answer , candidate an-   swer ) , predict the equivalence of the answers .   4.1 Bert Match ( BEM )   To achieve the approximate AE predictions , we   train a BERT - Base ( Devlin et al . , 2019 ) model on   the AE training set . ( Training details can be found   in Appendix B. )   4.1.1 Comparing input variations   We experiment with three different settings for in-   corporating the question and context :   ( i)Only answers : Just using the two answers as   model input ; more precisely : [ CLS ] candidate   [ SEP ] reference [ SEP ]   ( ii)Question and answers : We present the ques-   tion and the two answers ; i.e. [ CLS ] candidate   [ SEP ] reference [ SEP ] question [ SEP ]   ( iii ) Context , question and answers : We present   all available information : [ CLS ] candidate   [ SEP ] reference [ SEP ] question [ SEP ] con-   text [ SEP ]   The classiﬁcation accuracy results comparing   the three different models are shown in Table 3 .   A model that just sees both answers ( i ) already   performs well on the task , improving over other   metrics ( Table 5 ) . Adding the question ( ii ) furtherMode Accuracy   Only answers 89.27   Question and answers 90.70   Context , question and answers 89.53   improves the classiﬁcation accuracy , possibly due   to the need of disambiguating the relevance of the   answer . For example , in line 6 of Table 1 both   answers with or without the word “ teachers ” are   equivalent since the context is clear from the ques-   tion . If the question was about schools the two   answers would not be equivalent .   Finally , adding the context ( iii ) degrades the per-   formance compared to ( ii ) . We hypothesize that   this is for three reasons : First , the number of train-   ing examples where the context contains pertinent   information is insufﬁcient to be used productively   in our dataset . Second , cases that require assess-   ment of the context are harder examples , both for   the model , and for humans , which may lead to less   consistent annotations and add noise during the   learning process . Third , the context is the longest   part of the input and may contain many irrelevant   parts . We leave further exploration to future re-   search and use variant ( ii ) in the following .   4.1.2 Accuracy of BEM predictions   Table 4 shows that our BEM model achieves high   accuracy and correlation on the task of predicting   the human equivalence ratings , signiﬁcantly im-   proving over the baselines . The gain in accuracy is   consistent across the choice of QA system as shown   in Table 5 . We include BERTScore ( Zhang et al . ,   2020 ) , Bleurt Sellam et al . ( 2020b ) , and LERC   Chen et al . ( 2020 ) as additional baselines .   To compute accuracy we either threshold at 0.5   or tune a threshold such that accuracy on the train   set is optimal.296   Metric Accuracy , predictions from   BiDAF XLNet Luke   EM 61.23 44.26 41.07   F 79.26 77.33 74.64   BERTScore 69.06 74.47 73.83   BEM 88.83 90.70 89.48   4.2 AE for QA performance evaluation   As previously discussed , accurately assessing the   performance of QA models is a signiﬁcant chal-   lenge . Even in the presence of multiple gold an-   swers and predeﬁned normalization rules , models   might output correct answers outside of the anno-   tated answer bank . Thus EM provides a pessimistic   lower bound on the true accuracy , and AE metrics   can provide a more realistic assessment .   Metric BiDAF XLNet Luke   EM 71.6288.6089.76   F 80.7994.1594.99   BEM 84.6096.0097.03   Human 83.2596.6796.76   In order to evaluate the validity of the approx-   imate metrics , we compute the accuracy of three   representative models according to each equiva-   lence metric against the SQuAD gold answer set .   In addition , we use AE annotations to measure the   true model performance . As Table 6 shows , indeed   the EM is signiﬁcantly lower than the true accuracy .   Fprovides a slightly more optimistic evaluation ,   though still below the true score in about 2 - 3 points .   The learned BEM - based accuracy computation is   closest to the true performance of the model .   Next we show that the learned metric is much   more robust with respect to the number of available   references ( Table 7 ) . At the time of writing , Luke   ( rank 1 ) outperforms XLNet ( rank 2 ) on the the   SQuAD 1.1 leaderboard . However , if only one   gold reference was available , a higher degree of   accuracy would be needed to distinguish them with   Fcompared to the learned metric . Even with just a   single reference , BEM is much closer to the human   judgment shown in Table 6 .   To assess how well BEM generalizes to more   challenging tasks , we run a baseline system on NQ-   Open ( Lee et al . , 2019 ) consisting of a BM25 - based   retriever and a BERT - based extractive reader . We   sample 300 predictions from the dev set , ignoring   exact matches . In 87 % of these examples we ﬁnd   our manual independent assessment to agree with   BEM . This is evidence , that BEM does not just gen-   eralize well from Albert answers to other models   on SQuAD , but also to other more difﬁcult datasets   and QA settings such as open - domain .   5 Example application : Returning small   and accurate prediction sets   Beyond the value of accurate performance assess-   ment , identifying when the model answers are right   can provide immediate practical gains . We demon-   strate this on the common setting of constructing   prediction sets ( e.g. , the top- kmodel ’s predictions ) .   Here , the accuracy is measured by whether a cor-297   rect answer is included in the predicted set . The   size of the set ( k ) is typically calibrated on an eval-   uation set , to the desired accuracy . Reliably iden-   tifying high - ranking correct answers allows us to   effectively reduce the size of the sets ( smaller k )   while keeping them accurate .   Here , we use the conformal prediction ( CP )   framework ( Fisch et al . , 2022 ; Shafer and V ovk ,   2007 ) to construct provably accurate prediction   sets . Unlike top- k , CP set size is dynamically   determined per input with instance - wise hypothesis   testing to marginally satisfy the user - speciﬁed tar-   get accuracy . Speciﬁcally , given an exchangeable   calibration set of pairs of questions and correct   answers , we measure their nonconformity scores   ( here , the negative predicted probability ) . Then , for   a new question , CP returns a set of candidate an-   swers by including all answers with nonconformity   scores smaller than the inﬂated αQuantile of the   calibration scores , where αis the target accuracy .   While CP was originally deﬁned for a single   label per input , Fisch et al . ( 2021 ) recently pre-   sented expanded admission CP to support multiple   answers . This extension allows leveraging an an-   swer equivalence function to reduce the size of the   CP sets while preserving the accuracy guarantee .   We follow the setting of Fisch et al . ( 2021 ) and   use the collected AE labels to construct accurate   CP sets for SQuAD 1.1 with Luke ’s scores . As   Table 8 shows , this leads to signiﬁcantly smaller   sets , sometimes less than half the size . For example ,   to achieve 90 % accuracy , calibrating with SQuAD   labels results in an average of 11.31 answers per   question . Expanding the admission to include AE   labels reduces this size to only 4.31 answers .   Fisch et al . ( 2021 ) also discuss the use of an ap-   proximate admission function ( see their appendix   B ) . This setting is useful for when high - quality an-   notations for calibration are missing . We evaluate   this here and experiment with using either For   BEM AE predictions . As Table 8 indicates , we ﬁnd   BEM to be as effective as the AE evaluation labels   for reducing the number of required answers per   target accuracy level .   Figure 2 shows the results over the full range of   target accuracy ( α ) averaged over 50 trials , with 16   and 84th percentiles shown in shaded color . Addi-   tional details are available in Appendix C.2986 Related Work   Answer equivalence . Most similar to our work ,   Risch et al . ( 2021 ) annotate 1k answer pairs from   the SQuAD gold answers with a similarity score   and train a model that classiﬁes the concatenated   strings . They focus on the symmetric string-   similarity problem whereas we aim for asymmetric   equivalence conditioned on the answer .   Another similar effort is the MOCHA dataset   Chen et al . ( 2020 ) . The authors also collect anno-   tations on answer candidates and train a learned   metric . However , they focus on generative question   answering . Moreover , their methods for collecting   candidate answers , the selection of datasets and the   rating task differ considerably from our work .   Chen et al . ( 2021 ) use natural language inference   to verify predictions from QA systems by convert-   ing question and answer into a statement . They use   this to improve predictions in a setting where no   gold answer is known , but one could potentially   employ similar methods to compare a predicted   answer to a gold answer .   Text similarity . To our knowledge , Breck et al .   ( 1999 ) ﬁrst used Token Ffor automatic evaluation   of their “ Sys called Qanda ” . Chen et al . ( 2019 ) ﬁnd   thatFis a reasonable metric for extractive QA   tasks but fails to account for the higher variability   of surface form in generative QA .   Besides Token Fand Exact Match , other pop-   ular metrics for text comparison have been tried   for question answering , but are not widely in use :   BLEU ( Papineni et al . , 2002 ) , Rouge ( Lin , 2004 )   and METEOR ( Banerjee and Lavie , 2005 ) .   Yang et al . ( 2018 ) identify the need for meth-   ods that go beyond lexical overlap , trying to adapt   ROUGE and BLEU to better ﬁt answer comparison ,   but focusing on just “ yes - no ” and “ entity ” ques-   tions . Using a different approach , Si et al . ( 2021 )   propose to expand entities in gold answers with   aliases from Freebase ( Bollacker et al . , 2008 ) to   improve exact match reliability .   A good overview of different string distance met-   rics , in the context of name - matching tasks , can be   found in ( Cohen et al . , 2003 ) . Metrics based on   the Wasserstein distance have also been proposed   ( Kusner et al . , 2015 ; Clark et al . , 2019 ) .   In addition to automatic metrics , the NeurIPS   2020 EfﬁcientQA Competition ( Min et al . , 2020a )   uses manual annotations to reward correct answers   not contained in the gold answers . In contrast , wefocus on only rewarding answers that are equivalent   to one of the gold answers .   An important topic that is tangentially related to   our work is the question of ambiguous questions   ( cf . Min et al . ( 2020b , a ) ) . Our approach to answer   equivalence could be useful for detecting the pres-   ence of multiple clusters of equivalent answers ,   suggesting an ambiguous question .   Learned metrics . Recently string based metrics   have been replaced by learned metrics for various   Natural Langauge Generation ( NLG ) tasks . Exam-   ples include BLEURT ( Sellam et al . , 2020a ) and   COMET ( Rei et al . , 2020 ) for machine translation   evaluation , which Kocmi et al . ( 2021 ) ﬁnd to cor-   relate better with human judgments than e.g. the   popular BLEU metric ( Papineni et al . , 2002 ) .   For question answering evaluation , Chen et al .   ( 2019 ) propose a variant of BERTScore ( Zhang   et al . , 2020 ) where the answer tokens are contextu-   alized by adding the question and context but ca n’t   show improvements over Ffor extractive QA .   7 Conclusion   We present a systematic data - driven analysis of the   shortcomings of token - level measures , EM and F ,   for evaluating QA systems against a set of reference   answers .   We design an answer equivalence ( AE ) task that   directly captures the desired relation between can-   didate and reference answer . Also , we collect a   large number of annotations for both evaluating   and training equivalence models , as well as quanti-   tatively assessing the performance of QA systems .   Beyond relying on human AE annotations for   evaluation , we trained a BERT matching ( BEM )   model and showed that it generalized well to new   QA models and evaluation questions . Speciﬁcally ,   BEM allowed a signiﬁcantly better performance   assessment for the QA systems compared to the   token - level or other similarity measures . We also   demonstrated the value of AE on a practical appli-   cation beyond QA performance assessment .   We hope releasing our data will contribute to   further development of better metrics and improve   the evaluation and usability of QA systems .   Acknowledgments   We thank Adam Fisch for helpful feedback on the   conformal prediction experiments , and Costanza   Conforti for helping us to add the dataset to TFDS.299Ethical Considerations   As our work involves human participants , we point   out that all annotators provided informed consent   and no personally identiﬁable information ( ) was   collected or will be released . The collected data has   been vetted for presence ofas well as offensive   language through heuristics and random sampling .   The annotators received fair compensation with   respect to local markets , but said compensation   was not tied to speed or accuracy to prevent distort-   ing the motivation . Intrinsic motivation has been   shown to produce higher quality results ( Gneezy   and Rustichini , 2000 ) .   The released data and the experiments we con-   ducted are in English , therefore we do not claim   generalization of our ﬁndings across languages .   However , we believe that the proposed methods   could be applied in other languages using other   available corpora as sources .   Limitations   Our current work has several limitations that we   hope will inspire future research and extensions to   our framework .   One limitation is our focus on short answers ,   which is in line with much of the current QA re-   search . Some datasets ( e.g. , Natural Questions )   have introduced long answers . Our setting would   not be directly applicable there , as equivalence be-   tween long answers could be harder to determine .   However , BEM could be a building block in a more   complex aggregated comparison ( e.g. , “ summariz-   ing ” the long answer with a machine reading model   and comparing the “ summaries ” ) .   A further limitation , that is discussed in more de-   tail in Section 4.1.1 , is our ﬁnal BEM is not using   the context available for determining equivalence .   This is an advantage as it allows BEM to be readily   extended to settings where context is unavailable .   On the other hand , we expect AE on certain QA   domains to perform better with context , and en-   courage future research to explore when context is   useful .   The research was done on English language   datasets ( SQuAD , Natural Questions ) , but the same   methodology for data collection and model training   should extend to other languages given the exis-   tance of high - quality QA datasets for the languages   in question . A further limitation may be multi-   lingual answers . Translation alone may not be   enough to establish equivalence , given potentialsubtle differences in semantics between languages .   Also , our main experiments focus on machine   reading models . In Section 4.2 we discuss exten-   sions to NQ - Open and show promising initial re-   sults .   A more speciﬁc challenge regarding Open QA is   the use of generative models for QA . Here the gen-   erative model may add more speciﬁc details ( e.g. ,   a middle name , or a more precise date ) that is not   supported by the available context and may poten-   tially include false hallucinations . Since our model   has been trained only with extracted answers , we   saw in a small experiment that it will generally   accept these more speciﬁc answers as equivalent   answers . Note that this is also a more general chal-   lenge when annotating answers for QA datasets and   not only for answer equivalence — possibly requir-   ing speciﬁc background knowledge and/or extra   time / resources when verifying an answer .   Our approach does not directly address the ( po-   tential ) temporal or spatial dimension of ques-   tions / answers . While our deﬁnition allows us to   handle answers that vary by time / location , in rare   cases the equivalence of an answer may depend   on the time / location it is given . For example , the   two answers “ February 2022 ” and “ 4 months ago ”   to the question “ When were the last Olympics ? ”   would only be equivalent in June 2022 . This is a   limitation of the QA setting , and could be solved   by recording the exact date / time and location of the   annotation and adding it to the context .   Lastly , some more speciﬁc question answering   tasks are out of scope for us and left for future   work . For example , conversational question an-   swering ( e.g. , CoQA ) may need different process-   ing of questions and context .   References300301302303Metric Threshold AE Dev AE Test   F 0.2 85.06 82.47   LERC 2.52 81.97 80.74   BEM 0.56 89.99 89.80   Appendix   A Additional dataset statistics   We provide additional statistics about the collected   annotations in Table 10 and Table 11 .   B BEM training details   For BEM training we ﬁnetune the published BERT-   Base ( uncased , 12 - layer , 768 - hidden , 12 - heads ,   110 M parameters ) checkpoint on the training ex-   amples for one epoch , using a JAX - based BERT   implementation . We use a batch size of 64 and a   learning rate of 1e-4 with the Adam optimizer . We   did not perform a search for optimal hyperparam-   eters . The training on a TPU v2 takes less than 5   minutes .   C Conformal prediction sets   experimentation details   The experiments in section 5 use the Conformal   Prediction ( CP ) framework ( Angelopoulos and   Bates , 2021 ; Fisch et al . , 2022 ; V ovk et al . , 2005 ) .   While recent work found CP to be useful in many   practical applications such as medical image seg-   mentation ( Bates et al . , 2021 ) and adaptive compu-   tation Transformers ( Schuster et al . , 2021b , 2022 ) ,   one of CP challenges is in reducing the size of the   prediction sets while maintaining the compelling   accuracy guarantees . In this work , we follow the   expanded admission CP extension of Fisch et al .   ( 2021 ) that leverages the existence of equally cor-   rect answers to improve the statistical efﬁciency of   the calibration . We refer the reader to Fisch et al .   ( 2021 ) for the description and theoretical analysis   of the method , and detail the exact setting below .   We use the top 20 predictions of the Luke model   on SQuAD as candidate answers per question and   use the conformal - cascades repository for running   the calibration experiments . We only utilize the   expanded admission functionality of the code , and   do n’t use a cascade here as we directly use the neg-   ative span score from Luke as the nonconformity   measure . We run 50 calibration trials and report   the average results , as well as visualize the 16 and   84th percentiles in Figure 2 . In each trial , we ran-   domly partition the data into 80 % calibration and   20 % test examples . Reference results for different   target accuries are provided in Table 8 .   CP prediction sets are computed as a function of   the calibration examples and a user deﬁned target   accuracy . Following Fisch et al . ( 2021 ) we empiri-   cally verify the validity of the sets ( i.e. , marginally   meeting the target accuracy ) , and measure the size   of the sets . The goal is to minimize the size of   the prediction sets while satisfying validity . Our   experiments evaluate different equivalence terms   for the admission expansion function .   For exact expanded admission , we experiment   with either using the original SQuAD labels , or   including our AE annotations . For approximate   expanded admission , we try both Fand our BEM   equivalence metrics . We follow the method in Ap-   pendix B of Fisch et al . ( 2021 ) to statistically cor-   rect for the approximation errors of the metrics . We   use only 10 % of the calibration data to compute   the empirical FPR of the metric ( i.e. , the ratio that   the top answer that was accepted by the metric was   incorrect ) . We assume that the rest of the calibra-   tion data is lacking AE labels , hence the need for   the approximation . We use binomial conﬁdence   intervals to get an upper bound of the true error ,   resulting in a two - level probabilistic guarantee.304Then , we apply the correction by dividing the p-   value of each candidate by the lower bound of the   TPR.305