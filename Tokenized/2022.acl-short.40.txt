  Sicheng YuQianru SunHao ZhangJing JiangSingapore Management University , SingaporeNanyang Technological University , SingaporeCentre for Frontier AI Research , A*STAR , Singapore   scyu.2018@phdcs.smu.edu.sg , hzhang26@outlook.com   { qianrusun,jingjiang}@smu.edu.sg   Abstract   Translate - train is a general training approach   to multilingual tasks . The key idea is to use   the translator of the target language to gener-   ate training data to mitigate the gap between   the source and target languages . However , its   performance is often hampered by the artifacts   in the translated texts ( translationese ) . We   discover that such artifacts have common pat-   terns in different languages and can be mod-   eled by deep learning , and subsequently pro-   pose an approach to conduct translate - train   using Translationese Embracing the effect of   Artifacts ( TEA ) . TEA learns to mitigate such   effect on the training data of a source lan-   guage ( whose original and translationese are   both available ) , and applies the learned mod-   ule to facilitate the inference on the target lan-   guage . Extensive experiments on the multi-   lingual QA dataset TyDiQA demonstrate that   TEA outperforms strong baselines .   1 Introduction   Cross - lingual transfer has drawn wide attention in   recent years ( Hu et al . , 2020 ; Liang et al . , 2020 ) . It   has great potentials to be applied in advanced in-   dustries and real applications such as for improving   dialog and advertisement systems in multilingual   countries ( Schuster et al . , 2019 ; Yu et al . , 2021 ) .   It aims to reuse NLP models trained on a source   language for the task of a target language . The   most intuitive method is transfer learning by lever-   aging pre - trained multilingual language models   ( LMs ) such as mBERT ( Devlin et al . , 2019 ) and   XLM - R ( Conneau et al . , 2020 ) . These pre - trained   LMs encode different languages into a joint space   of multilingual representations ( Wu and Dredze ,   2019 ; Lauscher et al . , 2020 ) , and they perform well   especially for zero - shot cross - lingual tasks ( Wu   and Dredze , 2019 ; Lauscher et al . , 2020 ) . An-   other method orthogonal to this is called translate-   train ( Hu et al . , 2020 ; Fang et al . , 2021 ) . It trans-   lates training data from the source language intoFigure 1 : Monolingual QA performance comparison   between ( i ) training using originals and ( ii ) training us-   ing translated texts on TyDiQA . Note that for each lan-   guage , training data and test data are in the same lan-   guage . ‚Äú EM ‚Äù stands for Exact Match .   the target language and uses the translated texts for   training . Our paper focuses on this second method .   Translate - train mitigates the language gap be-   tween the source and the target languages in mul-   tilingual tasks in a straightforward manner as it   directly generates the needed target language train-   ing samples . However , the translation process in-   troduces artifacts in the translated texts ( i.e. , trans-   lationese ) . It has been observed that translationese   often exhibits features such as stylistic ones that   are different from text written directly in the same   language ( which we call the originals ) and thus   can mislead model training ( Selinker , 1972 ; V olan-   sky et al . , 2015 ; Bizzoni et al . , 2020 ) . In Figure 1 ,   we show that even in a monolingual setting where   training and test data are in the same language ,   when the test data are original texts , using transla-   tionese to train a QA model results in substantial   performance drop compared with using originals   for training .   To tackle the issue with translationese artifacts ,   inspired by domain mapping techniques ( Zhu et al . ,   2017 ) , we explore the idea of projecting originals362   and translationese into a common embedding space   to close their gap . Since only the originals of the   source language are available under the translate-   train setting , whether this idea is feasible depends   on whether the projection function is learnable   from the source language and transferable to other   languages . Therefore , we Ô¨Årst conduct experiments   to investigate if translationese artifacts , or patterns   of differences between orginals and translationese ,   are recognizable and transferable across languages   by deep learning models . SpeciÔ¨Åcally , we train   a binary classiÔ¨Åer to distinguish English originals   from English translationese . We then test the effec-   tiveness of this binary classiÔ¨Åer on other languages .   Our intuition is that ( 1 ) if the model converges , it   suggests that the patterns of translationese artifacts   can be potentially learned to some extent , and 2 )   if the trained model recognizes the translationese   of other languages , it means the model can likely   transfer the learned patterns across different lan-   guages . Our results in Figure 2 validate both : 1 )   The model converges well and achieves 97 % ac-   curacy on English , the training language . ( 2 ) It   also performs reasonably well on other languages   ( 77%91 % ) .   Based on the above intuition and validation ,   we propose a Translationese Embracing Artifacts   ( TEA ) method that projects originals and transla-   tionese into a common space to mitigate the trans-   lationese artifacts . TEA explicitly learns a map-   ping function from originals to translationese us-   ing originals and translationese of the source lan - guage ( English in our experiments ) , where learn-   ing is through minimizing the distance between   the mapped representation of originals and of the   corresponding translationese . TEA then applies   this mapping function to the originals of the target   language during the testing stage . For evaluation ,   we conduct experiments on multilingual QA us-   ing the TyDiQA dataset ( Clark et al . , 2020 ) . Our   results show that TEA outperforms translate - train   baselines and SOTA translationese mitigation meth-   ods designed for machine translation ( Marie et al . ,   2020 ; Wang et al . , 2021 ) .   2 Related Work   The effect of translationese has been widely stud-   ied in translation tasks ( Lembersky et al . , 2012 ;   Zhang and Toral , 2019 ; Edunov et al . , 2020 ; Gra-   ham et al . , 2020 ; Freitag et al . , 2020 ) . Some works   focus on mitigating or controlling the effect of   translationese , e.g. , tagged training ( Marie et al . ,   2020 ; Riley et al . , 2020 ; Wang et al . , 2021 ) , which   are adopted as baselines in our paper . In the Ô¨Åeld   of cross - lingual transfer , there are very few works   about translationese . Artetxe et al . ( 2020 ) is the   only attempt for translate - test and zero - shot learn-   ing . In contrast , we focus on translate - train and   aim to mitigate the artifacts in translationese .   Our research is also related to domain adaptation   ( DA ) that aims to transfer the knowledge from a   source domain to target domains . Our original - to-   translationese projection function can be seen as   something similar to projecting source domain and   target domain data into a common space , which   has been used before for domain adaptation ( Zhu   et al . , 2017 ; Shen et al . , 2017 ) .   3 Our Approach ( TEA )   Letxrepresent the input text and yrepresent the   output label . Xdenotes the domain ( i.e. , all pos-   sible values ) of xandYis the set of labels . The   input xcomes from different languages , and it can   be either originals or translationese during training .   SpeciÔ¨Åcally , we use X to denote the domain   ofsource language originals , and deÔ¨Åne X   andX in a similar way ( whererefers   to the target language andrefers to transla-   tionese ) . We further use back - translation ( Sennrich   et al . , 2016 ) to generate source language trans-363lationese ( i.e. , the source language originals are   Ô¨Årst translated into a pivot language and then trans-   lated back into the source language ) , denoted as   X , for the purpose of learning a mapping   function to project originals and translationese into   the same space .   We now present our TEA method . Our ultimate   goal is to learn a mapping function fX    Y , which takes target language originals as input .   However , we only have D"XYand   D"XYduring training . The chal-   lenge is that an flearned from either D or   D may not work effectively on X be-   cause of the differences between the source and the   target languages and between originals and trans-   lationese . To mitigate the differences between the   source and the target languages , we rely on pre-   trained multilingual language models , as many ex-   isting works do . As for the differences between   originals and translationese , based on the idea dis-   cussed in Section 1 , we propose to mitigate the   translationese artifacts of the target language us-   ing an original - to - translationese mapping function ,   and because of the lack of target originals , we pro-   pose to learn the original - to - translationese mapping   function from the source language .   To concretely illustrate our idea , we break down   the mapping from XtoYinto the following steps :   Multilingual Projection ( MP ): First , input xis   projected into a language - agnostic multilingual   space by using a pre - trained multilingual LM . We   useXto denote the projected multilingual space ,   andfis a multilingual projection ( i.e. , the multi-   lingual LM ) that maps an input xin any language   intoX.   Original - to - Translationese Projection ( OTP ):   Suppose Xconsists of two subspaces : X    XX , where X andX   denote the multilingual representations of any orig-   inals and translationese , respectively . To close the   gap between originals and translationese , we de-   Ô¨Åne an original - to - translationese projection func-   tionfX X to convert the vec-   tor representation of a piece of originals to its cor-   responding representation of translationese .   Language - Agnostic QA ( QA ): The last step is a   language - agnostic classiÔ¨Åer for the QA task itself .   We use fX Yto denote it .   Given an input x , depending on whether it is   from originals or translationese , we use differentcompositions of the functions above to map xtoy :   y vf`f`f xx"X ;   f`f x x"X :   Here`represents the composition of two func-   tions , i.e. ,f`g x f g x , and¬òdenotes   source language or target languages . More con-   cretely , for  x;y"D , we use X    X  X Y ; for x;y "   D , we use X X Y.   As discussed in Section 1 , we make use of the   source language originals and translationese to   learn f. SpeciÔ¨Åcally , for  x;y"D ,   we use x"X to represent its corre-   sponding translationese , i.e. , generated by back-   translation ( Sennrich et al . , 2016 ) through a pivot   language . Let r x;xx"D denotes all the   pairs of originals and translationese in the source   language . Then , we minimize the distance between   f f xandf xto optimize f.   In summary , the loss function consists of the   following three components :   L =l f`f`f x;y   =l f`f x;y   = 1g x;x ;   where g x;x cos f f x ; f x.   l  ; is standard cross entropy loss and cos  ;    is the cosine similarity function .   Model Details . Forf , we use XLM - R ( Conneau   et al . , 2020 ) . For f , we utilize a transformer   layer ( Vaswani et al . , 2017 ) . fis implemented   by a linear layer .   4 Experiments   Dataset . We conduct experiments on Ty-   DiQA ( Clark et al . , 2020 ) . SpeciÔ¨Åcally , we evaluate   our approach on the gold - passage subtask of Ty-   DiQA , which includes 9 languages . We set English   as source language and others as target languages ,   and report the results on target languages . During   training , we utilize translated training data in all   target languages for joint training . We use Exact   Match ( EM ) and F1 scores as evaluation metrics .   Implementation . Translations of English training   data for target languages are from XTREME ( Hu364   et al . , 2020 ) and translationese English is trans-   lated by Google Cloud Translation . German ( de )   is selected as the default pivot language in back-   translation . More details are in the Appendix B.   Baselines . We compare our model with the   following baselines : ( 1 ) Standard Translate-   Train ( STT ) ( Devlin et al . , 2019 ) . ( 2 ) FIL-   TER ( Fang et al . , 2021 ) is an advanced translate-   train method fully utilizing the parallel data . ( 3 )   Tagging ( TAG ) ( Marie et al . , 2020 ) , which distin-   guishes originals and translationese by adding a   tag for machine translation . ( 4 ) Two - Stage Train-   ing ( TST ) ( Wang et al . , 2021 ) , which is another   approach to address the gap between translationese   and originals for machine translation . It Ô¨Årst uses   the combination of them for training followed by   another round of training only on originals . ( 5 ) Gra-   dient Reversal Layer ( GRL ) ( Ganin and Lempitsky ,   2015 ) , which is a general DA method .   Main results . Table 1 summarizes the compari-   son between our TEA and the baselines . We make   the following observations : ( 1 ) TEA outperforms   all baselines . For instance , TEA surpasses STT   by2:4%(EM ) and 1:5%(F1 ) on average , which   demonstrates the effectiveness of our method . ( 2 )   Methods considering translationese artifacts gen-   erally perform better than methods without such   design , which reinforces the importance of miti-   gating translationese artifacts . ( 3 ) Compared to   the baselines for translationese artifacts , TEA still   shows its superiority . We highlight that our OTP   module with explicit projection is better than im-   plicit DA approaches . E.g. , TAG only uses different   tags to distinguish the translationese from origi-   nals . ( 4 ) The improvements from TEA across dif-   ferent languages are different . For high - resource   target languages , TEA brings more gains on the   languages in Indo - European family , e.g. , ru , and   marginal gains on others , e.g. , ar . For low - resource   target languages , the performance improvements   are obvious , e.g. , sw . It is because both language   model and machine translation model are of lower   quality on low - resource languages , and thus mit-   igating the gap between translationese and orig-   inals shows more effectiveness in such scenario .   For high - resource languages , TEA prefers Indo-   European languages , which are closer to English .   Ablation studies . We conduct in - depth ablation   studies to analyze TEA . SpeciÔ¨Åcally , we explore   the following settings : ( 1 ) Since we use 11 % more   data in TEA ( unlabeled X ) compared to   STT , here we add labeled X in STT . ( 2 )   Since we use additional 0:38 % parameters ( OTP )   in our method compared to STT , here we add   the same OTP module in STT . ( 3 ) We replace   the Original - to - Translationese Projection ( OTP ) by   Translationese - to - Original Projection ( TOP ) . ( 4 )   We replace the self - attention layer in OTP with a   multi - layer perceptron ( MLP ) . ( 5 ) We replace the   cosine distance function in loss with mean square   function . The results are summarized in Table 2 .   Compared to the variants , our full method performs   best over all settings . ( 1)/(2 ) incorporate additional   data / parameters , which demonstrates the improve-   ment of our method is not caused by the two factors.365   ( 3 ) proves that TOP still mitigates the artifacts , but   OTP obtaining better performance . We argue that   it is because most of the training data is transla-   tionese . ( 4 ) and ( 5 ) demonstrate the effectiveness   of our loss function and architecture .   Pivot Languages Analysis . Here we study the ef-   fect of pivot language used in generating X .   SpeciÔ¨Åcally , we select four pivot languages , i.e. ,   German ( de ) , Scottish ( gd ) , Korean ( ko ) and Chi-   nese ( zh ) , for evaluation . We Ô¨Åx our approach and   only replace the X used in OTP . The results   are reported in Table 3 . We observe that pivot lan-   guages from Indo - European family are superior to   that from other language families . We believe it is   because the training data of other target languages   in translate - train is translated from English , while   English belongs to the Indo - European family .   5 Conclusions   We aim to mitigate the translationese artifacts when   training translate - train models . After varifying the   transferability of the translationese patterns across   languages , we propose the TEA that mitigates ar-   tifacts using a source language and to facilitate   the prediction on unseen target languages . Our ap-   proach is simple and generic . Moreover , our results   on multilingual QA show its effectiveness .   Ethical Considerations   Although our method requires Ô¨Åne - tuning of the   pre - trained multilingual language model , the com-   putational cost of our experiments is not high . We   utilize two pieces of NVIDIA V100 and it takes   around 1 hour for the Ô¨Åne - tuning process . This   is partly due to the relatively small QA training   dataset used for Ô¨Åne - tuning . It is possible that if our   method is applied to either a much larger training   dataset for Ô¨Åne - tuning or a much larger pre - trained   language model , the computational cost and power   consumption will go up . To reduce such costs , one   way is to Ô¨Åne - tune only part of the pre - trained lan-   guage model . Another way is to apply the recently   proposed Adapter method ( Houlsby et al . , 2019 ) toÔ¨Åne - tune the language model .   Our method relies on machine translation sys-   tems . It has been found in a previous study that in-   dustrial MT systems as well as SOTA academic MT   systems may suffer from gender bias ( Stanovsky   et al . , 2019 ) , and it would not be surprising if other   types of societal biases and stereotypes are also   found in machine translated texts . If our method   uses translationese containing societal biases , our   learned original - to - translationese projection func-   tion will likely also contain such biases , which may   affect the fairness of the Ô¨Ånal trained system . How-   ever , this is not due to our method but rather the   translated text we use . Nevertheless , this is some-   thing we need to keep in mind if our method is   adopted for real applications .   Acknowledgments   This research is supported by the National Research   Foundation , Singapore under its Strategic Capabili-   ties Research Centres Funding Initiative , and par-   tially supported by the Agency for Science , Tech-   nology and Research ( A*STAR ) under its AME   YIRG Grant ( Project No . A20E6c0101 ) and the   A*STAR Career Development Fund ( Project No .   C210812033 ) . Any opinions , Ô¨Åndings and conclu-   sions or recommendations expressed in this mate-   rial are those of the author(s ) and do not reÔ¨Çect the   views of National Research Foundation , Singapore .   References366367   A Training Pipeline   Figure 3 illustrates the training pipelines our   method . The goal is to map the originals and trans-   lationese domains into the same embedding space   for prediction . SpeciÔ¨Åcally , the module on the top   of the Ô¨Ågure is to train the D andD ,   i.e. , the Ô¨Årst two terms of loss function , while the   module at the bottom aims to map the originals-   translationese pairs in D into same space ,   i.e. , the last term of loss function . B Implementation   General Implementation . We adopt the Hugging-   Face Transformers ( Wolf et al . , 2020 ) toolkit to im-   plement the pre - trained language model , i.e. , XLM-   R. The maximal input length , i.e. , concatenation   of question and passage tokens , is set as 384 . We   also utilize a document sliding window with stride   length of 128to tackle the long passage issue . The   learning rate and batch size are set as 2e5and32 ,   respectively . We use back - translate ( Sennrich et al . ,   2016 ) to generate the translationese . Back - translate   means to translate the source language to a pivot   language and then translate back to the source lan-   guage . By doing this , we are able to obtain the   translationese of source language .   Implementation of Experiment in Figure 1 . Ty-   DiQA ( Clark et al . , 2020 ) provides both training   and testing datasets of originals for all languages .   Here we adopt the originals training data to gener-   ate the corresponding translationese training data   through back - translation . The results in Fig-   ure 1 are obtained by training originals and gener-   ated translationese data for en , ar and Ô¨Å , respec-   tively . Note all the translationese is generated   by the Google Cloud Translationservice , where   the English translationese is generated by back-   translationese with de as pivot language , the trans-   lationeses of ar and Ô¨Å use en as pivot language . The   The test set is originals of each language .   Implementation of Experiment in Figure 2 .   Similarly , we generate the translationese of the orig-   inals for each language using Google Cloud Trans-   lation service , where en is set as pivot language for   non - English languages , and German for English   language . We split the originals - translationese   pairs of English into two groups , where 80 % sam-   ples are used for training , and the rest 20 % samples   together with all pairs of other languages are used   for evaluation . As the originals and translationese   are paired , a random guess could achieve 50 % ac-   curacy for all languages ideally .   Implementation of TEA . It is worth noting that   we can only access the originals of English and   the translationese of other target languages during   training . We use the translationese data , i.e. , the   target language data translated from English , from368   XTREME ofÔ¨Åcial website , while the translated   data from XTREME is utilized in translate - train   for all previous works ( Fang et al . , 2021 ) . Besides ,   we also augment translationese of English , which is   generated by back - translation , in our TEA . Again ,   we resort to the Google Cloud Translation service   to generate the translationese for all experiments   in Section 4 , where the German is set as pivot lan-   guage by default .   C Data and Parameters   The sample sizes of the data sets in all 9 languages   are equal , since they are all translated from En-   glish originals training data . The standard translate-   train ( STT ) directly adopt the data samples of 9   languages for training . In addition to the data sam-   ples of 9 languages , we also incorporate the En-   glish translationese , leading to 11 % more samples   used compared to SST . Besides , our Original - to-   Translationese Projection ( OTP ) module also intro-   duce additional parameters compared to SST .   D Additional Experiments   Main Results . In this part , we replenish the Ty-   DiQA results of two advanced multilingual lan-   guage models , i.e. , VECO ( Luo et al . , 2021 ) and   HICTL ( Wei et al . , 2020 ) in Table 4 .   Originals - Translationese Pair Sample . In Fig-   ure 4 , we list examples of originals - translationese   pair in English used for TEA training .   Effect of Translation Quality on Translationese   English . Here we conduct an ablation study aboutthe effect of translation quality on translationese   English used in cosine distance loss . Due to the   limited resource , we are unable to train a machine   translation model from scratch by ourselves . In-   stead , we select the free Google Translate toolkit   ( compared to the paid Google Cloud service ) as the   proxy of low - quality translator . We Ô¨Åx all the im-   plementation settings and change the translationese   English data only . Consequently , we obtain the av-   erage performance of EM ¬© F1 58:0 ¬© 73:9 . The   result indicates that a better translator is more effec-   tive for the translationese English generation . It is   because that the low - quality translator may create   more translation errors , then those errors are propa-   gated during training , which hinders the learning   of the originals to translationese mapping.369370