  Yukun FengFeng LiPhilipp KoehnJohns Hopkins UniversityUniversity of Illinois Urbana - Champaign   { yfeng55 , phi}@jhu.edu , fengl3@illinois.edu   Abstract   Multilingual pretrained models have shown   strong cross - lingual transfer ability . Some   works used code - switching sentences , which   consist of tokens from multiple languages , to   enhance the cross - lingual representation fur-   ther , and have shown success in many zero - shot   cross - lingual tasks . However , code - switched   tokens are likely to cause grammatical inco-   herence in newly substituted sentences , and   negatively affect the performance on token-   sensitive tasks , such as Part - of - Speech ( POS )   tagging and Named - Entity - Recognition ( NER ) .   This paper mitigates the limitation of the code-   switching method by not only making the token   replacement but considering the similarity be-   tween the context and the switched tokens so   that the newly substituted sentences are gram-   matically consistent during both training and   inference . We conduct experiments on cross-   lingual POS and NER over 30 + languages , and   demonstrate the effectiveness of our method by   outperforming the mBERT by 0.95 and original   code - switching method by 1.67 on F1 scores .   1 Introduction   Recent studies(Devlin et al . , 2019 ; Lample and   Conneau , 2019 ; Conneau et al . , 2020a ) have shown   the success of multilingual corpus pre - training for   cross - lingual knowledge transfer . Some works(Qin   et al . , 2020 ; Yang et al . , 2021 ) further addressed   the effectiveness of code - switching on improving   the performance of multilingual models on zero-   shot cross - lingual tasks(Conneau et al . , 2018 ; Hu   et al . , 2020 ) . Though code - switching has shown   great potential and strong generalization ability on   the semantic representation , the newly switched se-   quence fails to consider the token - level coherence .   Specifically , a code - switched sentence consists of   tokens from various languages , and such words are   likely to cause grammatical incoherence , resulting   in an inconsistent context space for the newly sub-   stituted sentence . Also , code - switched tokens areFigure 1 : Overview of the training and inference for   POS in code - switching and our settings .   conditioned on the original context , for which , dur-   ing the optimization phase , the embedding of sub-   stituted tokens is greatly affected . However , such   updates do not consider the token - level dependency   in their own language and will likely cause inconsis-   tent embedding space during inference . Therefore ,   even though code - switching benefits most of the   sentence - level cross - lingual tasks , it still limits the   performance of token - level prediction .   To address such issues , we use an alignment strat-   egy to reduce the gap between original context and   code - switched tokens during training . Specifically ,   code - switched tokens are mapped to the original   context so that the newly switched sentences are ex-   pected to be grammatically coherent and maintain   the same contextual space in the same language .   Meanwhile , our approach enables the substituted   tokens not to significantly affect the token - level co-   herence in their own language and keep a consistent   embedding space during inference . We conduct ex-   periments on zero - shot cross - lingual POS and NER   and outperforms both the mBERT and the original   code - switching method by 0.77 and 2.08 on NER   and 1.13 and 1.27 on POS , respectively . We also   make further analysis regarding the performance on   low - resource languages , code - switch contribution ,   substitution strategy , and token - level coherence.5966   2 Approach   We conduct cross - lingual tasks in the zero - shot set-   ting , in which only English labeled sentences with   code - switching augmentation are used for training ,   and evaluation is performed in all other languages .   2.1 Code - Switching Augmentation   Multilingual Dictionary Construction To build   the dictionary that maps English tokens to other   languages in Table 1 , we adopt the parallel sen-   tences from CCMatrix(Schwenk et al . , 2021 ) , and   use fast_align(Dyer et al . , 2013 ) to align tokens   from parallel sentences . To keep languages be-   ing equally considered , we sample no more than   1 million sentence pairs for each language . For   each language lg , a bilingual dictionary Dis   built as a one - to - many structure based on extracted   alignment , where each English token thas sev-   eral candidates as C={c , ... c } , and the cor-   responding sampling probability are defined as   β={β , ... β } . We merge such bilingual dic-   tionaries into one unified dictionary Dwith respect   to their keys of English tokens . Then for each   English token t , we have its corresponding can-   didates as C={c , ... c } , and sampling proba-   bility as β={β , ... β } , where nrefers to the   number of candidates among all languages , and the   sampling rates are normalized after the merge .   Token Substitution Given the English training   sentences , we decide whether to substitute an En-   glish token based on a substitution ratio α . For   an English token tthat is selected for substitu-   tion , the candidate token tis sampled following   the probability distribution in dictionary D. We   adopt the dynamic substitution to sample different   substitution sequences for each epoch . We present   analysis of the substitution strategy in Section 3.3 .   2.2 Cross - Lingual Transfer   Our approach is built on both the code - switched   sentence S={S , ... S } , and original English   sentence S={S , ... S } . To avoid the influ-   ence of the grammatical incoherence , we use an   alignment network to align substituted tokens with   original English context . Token - Level Alignment We map the code-   switched sentence to the English context by com-   puting the similarity between substituted tokens   and English tokens . The similarity scores are fur-   ther used as the weights to aggregate the embedding   of English tokens , as the calculated potential for   substituted tokens in the switched sequence .   We use the multilingual BERT ( mBERT ) to en-   code English and code - switched sentences into con-   textualized embedding as H= mBERT(S ) ,   H= mBERT(S)respectively . For any token   S , the similarity score scoreis computed as :   score= H∗H(1 )   The final potential of the code - switched token /tildewideH   is the weighted sum of the contextualized embed-   dingH , calculated along the time axis .   α = exp ( score )   /summationtextexp ( score)(2 )   /tildewideH=/summationdisplayαH ( 3 )   Eventually , the newly aligned code - switched se-   quence of tokens is represented as /tildewideH.   Training and Inference During training , the   alignment is applied before the layer input in the   mBERT . We compute /tildewideHfrom the original layer   input pairs ( H , H ) as in Eq.3 , where irefers   to the layer index in mBERT . /tildewideHwill be the   new layer input for code - switched sentence . As   lower layers in BERT learns about the syntax in-   formation , we only apply the alignment in lower   six layers to better align the representations for   switched tokens . To optimize the effect of the   ground - truth labels on both the English and code-   switched sequences during training , the final loss   is described as : Loss = ,   where Lrefers to the loss function . In the inference ,   we do not use any code - switch to sentence tokens .   3 Experiment   3.1 Settings   We conduct experiments on two widely - used cross-   lingual datasets for token - level prediction , Univer-   sal Dependencies for POS and WikiANN for NER   ( Pan et al . , 2017 ) . We adopt mBERTwith the5967Model af ar bg nl en et fi fr de el he hi hu i d it ja kk   mBERT 87.3 66.4 87.5 87.6 95.3 84.6 82.9 79.5 85.6 53.2 76.8 75.1 80.9 74.3 85.4 58.5 72.5   M 88.7 70.3 88.8 87.4 95.0 83.7 82.5 79.4 85.8 51.0 78.6 75.7 81.7 73.6 84.6 58.2 72.9   M 87.6 69.0 87.7 88.1 95.0 84.5 83.2 79.7 85.9 57.7 80.2 77.0 80.0 74.4 85.6 58.9 74.6   ko zh mr fa pt ru es tl ta te th tr ur vi yo Avg Diff   mBERT 57.4 68.6 70.5 67.4 88.9 84.1 87.8 83.0 63.9 73.5 53.5 70.3 65.2 62.8 61.2 74.73 -   M 57.8 68.1 65.4 66.6 88.9 83.2 87.9 82.8 63.8 72.5 52.0 69.2 69.5 60.4 60.8 74.59 -0.14   M 57.1 68.9 75.0 69.5 89.4 84.4 89.0 82.1 66.1 74.7 57.2 70.0 68.2 62.7 64.6 75.86 +1.13   Model af ar bg bn de el en es et eu fa fi fr he   mBERT 81.4 74.6 84.0 78.7 83.8 77.1 89.1 84.1 83.4 77.8 80.8 81.3 86.0 76.0   M 80.6 74.4 81.8 82.0 83.1 74.3 89.2 77.1 81.9 76.3 77.2 81.3 86.2 75.1   M 82.2 74.5 84.4 79.5 84.9 79.5 89.2 82.8 84.5 79.6 79.2 81.7 86.0 76.1   hi hu i d it ja jv ka kk ko ml mr ms my nl   mBERT 80.2 80.6 85.0 87.4 43.5 73.7 77.9 69.4 72.4 71.9 75.0 72.7 67.0 85.6   M 78.0 77.6 85.2 87.4 48.9 74.9 74.2 67.9 71.2 65.1 70.8 75.6 64.8 84.5   M 79.3 81.7 85.3 87.7 46.5 75.9 76.9 74.3 72.8 71.2 76.2 76.7 68.0 85.7   pt ru sw ta te th tl tr ur vi yo zh Avg Diff   mBERT 86.0 76.9 79.3 75.1 71.1 20.6 74.4 78.3 73.6 85.6 59.6 64.2 75.62 -   M 83.1 74.6 78.7 73.0 67.4 20.3 80.6 75.1 66.0 85.6 58.3 63.6 74.31 -1.31   M 85.6 78.7 80.3 73.8 72.0 21.1 77.4 78.8 70.4 86.6 62.8 65.8 76.39 +0.77   Training Size ( GB ) # Languages Avg Gain   [ 0.006,0.354 ] 13 +1.91   [ 0.354,1.414 ] 10 +1.03   [ 1.414,5.657 ] 8 +0.62   base configuration . The learning rate is set as 1e   for pretrained parameters and 9efor newly ini-   tialized parameters . We use a batch - size of 32 ,   warmup of 200 steps and patience of 3 on all exper-   iments . We follow the zero - shot setting , in which   we have only the English set for training and all   languages for evaluation . We compare our method   with the original mBERT and code - switching . The   substitution ratio is set to 15 % for main experi-   ments . Seed numbers in all experiments keep the   same so that the substituted tokens on both settings   are the same for each run . To reduce the influence   of the randomness caused by the token substitution ,   results are averaged by three runs .   3.2 Results   In Table 2 and 3 , the code - switching negatively   affects the mBERT by 0.14 on POS and 1.31 onNER , while our method makes improvement of   1.13 on POS and 0.77 on NER . Our method suc-   cessfully eliminates the code - switching noise and   demonstrates the effectiveness over 30 + languages .   3.3 Analysis   In this section , we perform the analysis on the POS   task , with the default substitution ratio of 15 % .   Metric Breakdown To determine how well our   model performs on each language , we breakdown   the evaluation scores based on how much pretrain-   ing data the mBERT used for each language . We   follow the range of training size for each language   as described in Wu and Dredze ( 2020 ) , and aver-   age the performance gain on the POS task of our   model compared to the original mBERT . As shown   in Table 4 , we notice our model has better perfor-   mance gain especially on languages that are less   trained in mBERT . For example , we have achieved   3.8 and 9.6 performance gain on yo and mr , re-   spectively , for which the languages have less than   0.1 GB data in the mBERT training . It indicates   that our alignment strategy enriches the represen-   tation of low - resource tokens , leading to a better   performance on the low - resource languages.5968Model Switched Original   Code - Switch 5.69 % 4.05 %   Ours 13.84 % 2.07 %   Switching Effectiveness To evaluate whether our   model actually benefits from the switched tokens ,   we adopt the gradient attribution test ( Ancona et al . ,   2018 ) . Specifically , we evaluate the importance of   each token to the model ’s prediction by calculating   the gradient for each test input . As in Table 5 , we   see the ratio of the received gradient for switched   tokens in our method is much greater than original   English tokens . Also , our model has shown greater   relative importance of the switched tokens than the   vanilla code - switching method . It indicates that   such substituted tokens significantly contribute to   our model ’s prediction and benefit the performance .   Substitution Strategy We compare the effect of   different token substitution ratios for original code-   switching and our method . As shown in Figure 2 ,   our method has consistent greater scores on differ-   ent substitution ratios from 5 % to 90 % . However ,   the performance of original code - switching method   decreases as the substitution percentage increases .   Such results show the stable performance of our   method , in which the code - switched tokens keep   benefiting the cross - lingual knowledge transfer .   Token - Level Coherence We plot the degree of   dispersion between tokens in a sentence to further   analyze whether our method keeps the consistent   context space in both training and inference . Specif-   ically , for both the switched and original tokens ,   we retrieve the token embedding from intermediate   ( 6 - th ) layers and use the top feature calculated from   the Principle Component Analysis ( PCA ) . In Fig-   ure 3 , our model has shown a more compact space   for the sequence of tokens and the switched tokens   are also inside the space of the context . However ,   the original code - switching method entirely sepa-   rates the substituted tokens apart from the original   context . Also , Figure 4 shows the effectiveness of   our model on keeping a consistent context space   during inference . Tokens in all three languages are   very close in our method but separated apart in the   original code - switched approach . Our model has   demonstrated the effectiveness on keeping a con-   sistent embedding space in training and inference .   4 Related Works   Previous studies(Huang et al . , 2019 ; Liu et al . ,   2020a ; Gritta and Iacobacci , 2021 ; Luo et al . , 2021 )   trained language models on either monolingual or   cross - lingual corpus to learn the multilingual repre-   sentation . Recent works(Wu and Dredze , 2019 ; Hu   et al . , 2020 ; Conneau et al . , 2020b ) have proved the   effective zero - shot transferable ability of multilin-   gual models . Researchers(Zhang et al . , 2019 ; Yang   et al . , 2020b , a ; Qin et al . , 2020 ; Liu et al . , 2020b ;   Yang et al . , 2021 ) tried to use code - switched sen-   tences to enhance the representation among vari-   ous languages and have been proved the success on   many cross - lingual tasks . We believe our approach   further addresses the limitation the code - switching   on token - level classification.59695 Conclusion   This paper introduces an alignment strategy to map   the code - switched tokens to original context and   solves the grammatically incoherence in the embed-   ding space of code - switching . Experimental results   on POS and NER along with comprehensive analy-   sis have demonstrated the effectiveness of our ap-   proach on the token - level classification . We think   this work could further address other cross - lingual   tasks and multilingual pretraining .   Acknowledgements   We thank all reviewers ’ valuable suggestions . We   also appreciate the helpful feedback from Boyuan   Zheng , and Shuai Zhang . The computing resources   used by this work are supported by the Center for   Language and Speech Processing ( CLSP ) at Johns   Hopkins University ( JHU ) .   Limitation   We do not use any neural - based aligner e.g the   awesome - aligner(Dou and Neubig , 2021 ) , because   we want to make the aligning part simple and ef-   ficient . We believe that some modern methods   or involving grammatical information could help   achieve better aligning results but it is not the point   of this paper . Although the construction of the dic-   tionary is much less computationally expensive , it   must be completed before the training and requires   additional parallel data , which might cause incon-   sistency with the domain of the training text . The   randomness introduced by the code - switching sub-   stitution may affect the overall performance , even   though our method has considered the correlation   between switched tokens and original context .   References59705971