  Zhiyi Fu , Wangchunshu Zhou , Jingjing Xu , Hao Zhou , Lei LiPeking UniversityByteDance AI LabUniversity of California , Santa Barbara   ypfzy@pku.edu.cn   { zhouwangchunshu.7 , zhouhao.nlp}@bytedance.com   { jingjingxu , leili}@cs.ucsb.edu   Abstract   How do masked language models ( MLMs )   such as BERT learn contextual representa-   tions ? In this work , we analyze the learn-   ing dynamics of MLMs . We ﬁnd that MLMs   adopt sampled embeddings as anchors to es-   timate and inject contextual semantics to rep-   resentations , which limits the efﬁciency and   effectiveness of MLMs . To address these is-   sues , we propose TACO , a simple yet effec-   tive representation learning approach to di-   rectly model global semantics . TACO extracts   and aligns contextual semantics hidden in con-   textualized representations to encourage mod-   els to attend global semantics when gener-   ating contextualized representations . Exper-   iments on the GLUE benchmark show that   TACO achieves up to 5x speedup and up to   1.2 points average improvement over existing   MLMs . The code is available at https://   github.com/FUZHIYI/TACO .   1 Introduction   In the age of deep learning , the basis of repre-   sentation learning is to learn distributional seman-   tics . The target of distributional semantics can be   summed up in the so - called distributional hypoth-   esis ( Harris , 1954 ): Linguistic items with similar   distributions have similar meanings . To model   similar meanings , traditional representation ap-   proaches ( Mikolov et al . , 2013 ; Pennington et al . ,   2014 ) ( e.g. , Word2Vec ) model distributional seman-   tics by deﬁning tokens using context - independent   ( CI ) dense vectors , i.e. , word embeddings , and di-   rectly aligning the representations of tokens in the   same context . Nowadays , pre - trained language   models ( PTMs ) ( Devlin et al . , 2019 ; Radford et al . ,   2018 ; Qiu et al . , 2020 ) expand static embeddings   into contextualized representations where each to-   ken has two kinds of representations : context-   independent embedding , and context - dependentFigure 1 : Illustration of the proposed token - alignment   contrastive objective . It extracts and aligns the global   semantics hidden in contextualized representations via   the gap between contextualized representations and cor-   responding static embeddings .   ( CD ) dense representation that stems from its em-   bedding and contains context information . Al-   though language modeling and representation learn-   ing have distinct targets , masked language model-   ing is still the prime choice to learn token represen-   tations with access to large scale of raw texts ( Pe-   ters et al . , 2018 ; Devlin et al . , 2019 ; Raffel et al . ,   2020 ; Brown et al . , 2020 ) .   It naturally raises a question : How do masked   language models learn contextual representa-   tions ? Following the widely - accepted understand-   ing ( Wang and Isola , 2020 ) , MLM optimizes two   properties , the alignment of contextualized repre-   sentations with the static embeddings of masked   tokens , and the uniformity of static embeddings in   the representation space . In the alignment property ,   sampled embeddings of masked tokens play as an   anchor to align contextualized representations . We   ﬁnd that although such local anchor is essential   to model local dependencies , the lack of global   anchors brings several limitations . First , experi-   ments show that the learning of contextual repre-   sentations is sensitive to embedding quality , which   harms the efﬁciency of MLM at the early stage of2701training . Second , MLM typically masks multiple   target words in a sentence , resulting in multiple em-   bedding anchors in the same context . This pushes   contextualized representations into different clus-   ters and thus harms modeling global dependencies .   To address these challenges , we propose a novel   Token- Alignment Contrastive Objective ( TACO )   to directly build global anchors . By combing lo-   cal anchors and global anchors together , TACO   achieves better performance and faster convergence   than MLM . Motivated by the widely - accepted be-   lief that contextualized representation of a token   should be the mapping of its static embedding on   the contextual space given global information , we   propose to directly align global information hid-   den in contextualized representations at all posi-   tions of a natural sentence to encourage models   to attend same global semantics when generating   contextualized representations . Concerning possi-   ble relationships between context - dependent and   context - independent representations , we adopt the   simplest probing method to extract global informa-   tion via the gap between context - dependent and   context - independent representations of a token for   simpliﬁcation , as shown in Figure 1 . To be speciﬁc ,   we deﬁne tokens in the same context ( text span ) as   positive pairs and tokens in different contexts as   negative pairs , to encourage the global information   among tokens within the same context to be more   similar compared to that from different contexts .   We evaluate TACO on GLUE benchmark . Ex-   periment results show that TACO outperforms   MLM with average 1.2 point improvement and 5x   speedup ( in terms of sample efﬁciency ) on BERT-   small , and with average 0.9 point improvement and   2x speedup on BERT - base .   The contributions of this paper are as follows .   •We analyze the limitation of MLM and pro-   pose a simple yet efﬁcient method TACO to   directly model global semantics .   •Experiments show that TACO outperforms   MLM with up to 1.2 point improvement and   up to 5x speedup on GLUE benchmark .   2 Understanding Language Modeling   2.1 Objective Analysis   The key idea of MLM is to randomly replace a   few tokens in a sentence with the special token   [ MASK ] and ask a neural network to recover the   original tokens . Formally , we deﬁne a corruptedsentence as x , x,,x , and feed it into a   Transformers encoder ( Vaswani et al . , 2017 ) , the   hidden states from the ﬁnal layer are denoted as   h , h,,h . We denote the embeddings of the   corresponding original tokens as e , e,,e .   The MLM objective can be formulated as :   whereMdenotes the set of masked tokens and jVj   is the size of vocabulary V.mis hidden state of   the last layer at the masked position , and can be   regarded as a fusion of contextualized representa-   tions of surrounding tokens . Following the widely-   accepted understanding ( Wang and Isola , 2020 ) ,   Eq.1 optimizes : ( 1 ) the alignment between contex-   tualized representations of surrounding tokens and   the context - independent embedding of the target   token and ( 2 ) the uniformity of representations in   the representation space .   In the alignment part , MLM relies on sampled   contextual - independent embeddings of masked to-   kens as anchors to align contextualized represen-   tations in contexts , as shown in Figure 2 . Local   anchor is the key feature of MLM . Therefore , the   learning of contextualized representations heavily   relies on embedding quality . In addition , multiple   local anchors in a sentence tend to pushing con-   textualized representations of surrounding tokens   closer to different clusters , encouraging models to   attend local dependencies where global semantics   are neglected .   2.2 Empirical Analysis   To verify our understanding , we conduct compre-   hensive experiments to investigate : How does em-   bedding anchor affect the learning dynamics of   MLM ? We re - train a BERT - small ( Devlin et al . ,   2019 ) model with the MLM objective solely and   analyze the changes in its semantic space during2702pre - training . The training details are described in   Appendix A.   Contextualized representation evaluation . In   general , if contextualized representations are well   learned , the contextualized representations in a   same context will have higher similarity than that of   in different contexts . Naturally , we use the gap be-   tween intra - sentence similarity and inter - sentence   similarity to evaluate contextual information in con-   textualized representations . We call this gap as con-   textual score . The similarity can be evaluated via   probing methods like L2 distance , cosine similarity ,   etc . We observe similar ﬁndings on different prob-   ing methods and only report cosine similarity here   for simpliﬁcation . Figure 3(b ) shows how contex-   tual score changes during training . Other statistical   results are listed in Appendix A.   Embedding similarity evaluation . To observe   how sampled embeddings affect contextualized   representation learning , we evaluate the embed-   ding similarity between co - occurrent tokens . Moti-   vated by the target that co - occurrent tokens should   have similar representations , we use the similar-   ity score calculated by cosine similarity between   co - occurrent words labeled by humans ( sampled   from the WordSim353 dataset ( Agirre et al . , 2009 ) )   as the evaluation metric . Figure 3(a ) shows how   embedding similarity between co - occurrent tokens   changes during training .   The learning of contextualized representations   heavily relies on embeddings similarity . As we   can see from Figure 3(a ) , the embedding similarity   between co - occurrent tokens ﬁrst decreases during   the earliest stage of pre - training . It is because all   embeddings are randomly initialized with the same   distribution and the uniformity feature in MLM   pushes tokens far away from each other , thus result-   ing in the decrease of embedding similarity . Mean-   while , the contextual score , i.e. , the gap between   intra - context similarity and inter - context similar-   ity in Figure 3(b ) , does not increase at the earliest   stage of training . It shows that random embeddings   provide little help to learn contextual semantics .   During 5K-10 K iterations , only when embeddings   become closer , contextualized representations in   the same context begin to have similar features . At   this stage , the randomly sampled embeddings from   the same sentence , i.e. , the same context , usually   have similar representations and thus MLM can   push contextualized tokens closer to each other .   We further verify the effects of embedding qual-   ity in Figure 4 . To this end , we train two BERT   models whose embedding matrices are frozen and   initialized with the ones from different pre - training   stage . We can see the model initialized with ran-   dom embedding fails to teach contextualized repre-   sentations to attend sentence meanings and repre-   sentations from different contexts have almost the   same similarity . However , the variant with well-   trained but frozen embeddings learns to distinguish   different contexts early at around 4k steps . These   statistical observations verify that embedding an-   chors bring the efﬁciency and effectiveness prob-   lem .   Surprisingly , embedding anchors reduce global   contextual information in contextualized repre-   sentation at the later stage of training . Fig-   ure 3(a ) shows that embedding similarity begins   to drop after 8k steps . It shows that the model   learns the speciﬁc meanings of co - occurrent to-   kens and begins to push them a little bit far away .   Since MLM adopts local anchors , these local em-2703beddings push contextualized representations into   different clusters . The contextual score begins to   decrease too . This phenomenon proves the embed-   ding bias problem where the learning of contextu-   alized representations is decided by the selected   embeddings where the global contextual semantics   are neglected .   3 Proposed Approach : TACO   To address the challenges of MLM , we propose   a new method TACO to combine global anchors   and local anchors . We ﬁrst introduce TC , a token-   alignment contrastive loss which explicitly models   global semantics in Section 3.1 , and combine TC   with MLM to get the overall objective for training   our TACO model in Section 3.2 .   3.1 Token - alignment Contrastive Loss   To model global semantics , the objective is ex-   pected to be capable of explicitly capturing infor-   mation shared between contextualized representa-   tion of tokens within the same context . Therefore ,   a natural solution is to maximize the mutual infor-   mation of contextual information hidden in contex-   tualized representations in the same context . Toextract shared contextual information , we ﬁrst de-   ﬁne a rule to generate contextual representations   of tokens by combining embeddings and global   information . Formally ,   wherefis a probing algorithm and eis the embed-   ding and gis the global bias of a concrete context .   In this paper , we adopt a straightforward probing   method to get global information hidden in contex-   tualized representations , where   Given contextualized representations of an token   xand its nearby tokens cin the same context , we   usegandgto represent global semantics hidden   in these representations . The mutual information   between the two global bias gandgis   According to van den Oord et al . 2019 , the In-   foNCE loss serves as an estimator of mutual infor-   mation of xandc :   whereL(g;g)is deﬁned as :   where cis thek - th negative sample of xandKis   the size of negative samples . Hence minimizing the   objectiveL(g;g)is equivalent to maximizing the   lower bound on the mutual information I(g;g ) .   This objective contains two parts : positive pairs   f(g;g)andnegative pairs f(g;g ) .   Previous study ( Chen et al . , 2020 ) has shown that   cosine similarity with temperature performs well   as the score function fin InfoNCE loss . Following   them , we take   f(g;g ) = 1   gg   kgkkgk(7 )   where  is the temperature hyper - parameter and   kk is`-norm function .   Contextualized representation : To get global   biasgandgfollowing Eq . 3 , we adopt the   widely - used Transformer ( Vaswani et al . , 2017 )   as the encoder and take the last hidden states as2704the contextualized representations handh . For-   mally , suppose a batch of sequences fsgwhere   i2f1;;Ng . We feed it into the Transformer   encoder to obtain contextualized representations ,   h , h,,hwhere h2R.   Positive pairs : Given each token x , we randomly   sample a positive sample cfrom nearby tokens in   the same context ( sequence ) within a window span   whereWis the window size .   Negative pairs : Given each token x , we ran-   domly sample Ktokens from other sequences in   this batch as negative samples c.   To sum up , the Token - alignment Contrastive   ( TC ) loss is applied to every token in a batch as :   whereNis the number of sequences of this batch ;   sis thei - th sequence ; jandjcare tokens in s   wherejc6 = j;gis the global semantics hidden in   contextualized representation of token s.gand   gare generated via :   g = h e ( 9 )   g = h e(10 )   where handeare the contextualized represen-   tation and static embedding of the anchor token ,   respectively . handeare the contextualized   representation and static embedding of the sampled   positive token in the same context .   3.2 Training Objective   As described before , the token - alignment con-   trastive lossLis designed to model global de-   pendencies while MLM is able to capture local   dependencies . Therefore , we can better model con-   textualized representations by combining the token-   alignment contrastive loss Land the MLM loss   to get our overall objective L :   L = L+L ( 11 )   We implement it in a multi - task learning manner   where all objectives are calculated within one for-   ward propagation , which only introduces negligible   extra computations .   4 Experiments   4.1 Experimental Settings   Training Following BERT ( Devlin et al . , 2019 ) ,   we select the BooksCorpus ( 800 M words afterWordPiece tokenization ) ( Zhu et al . , 2015 ) and En-   glish Wikipedia ( 4B words ) as pre - training corpus .   We pre - train two variants of BERT models : BERT-   small and BERT - base . All models are equipped   with the vocabulary of size 30,522 , trained with   15 % masked positions for MLM . The maximum   sequence length is 256 and batch size is 1,280 .   We adopt optimizer AdamW ( Loshchilov and Hut-   ter , 2019 ) with learning rate 1e-4 . All models are   trained until convergence . To be speciﬁc , the small   model is trained up to 250k steps with a warm - up   of 2.5k steps . The base model is trained up to 500k   steps with a warm - up of 10k steps . For TACO , we   set the positive sample window size Wto 5 , the   negative sample number Kto 50 , and the tempera-   ture parameter  to 0.07 after a slight grid - search   via preliminary experiments . More pre - training   details can be found in Appendix A.   During ﬁne - tuning models , we conduct a grid   search over batch sizes of { 16 , 32 , 64 , 128 } , learn-   ing rates of { 1e-5 , 2e-5 , 3e-5 , 5e-5 } , and training   epochs of { 4 , 6 } with an Adam optimizer ( Kingma   and Ba , 2015 ) . We use the open - source pack-   ages for implementation , including HuggingFace   Datasetsand Transformers . All the experiments   are conducted on 16 GPU chips ( 32 GB V100 ) .   Evaluation We evaluate methods on the GLUE   benchmark ( Wang et al . , 2019 ) . Speciﬁcally , we   test on Microsoft Research Paraphrase Matching   ( MRPC ) ( Dolan and Brockett , 2005 ) , Quora Ques-   tion Pairs ( QQP)and STS - B ( Conneau and Kiela ,   2018 ) for Paraphrase Similarity Matching ; Stan-   ford Sentiment Treebank ( SST-2 ) ( Socher et al . ,   2013 ) for Sentiment Classiﬁcation ; Multi - Genre   Natural Language Inference Matched ( MNLI - m ) ,   Multi - Genre Natural Language Inference Mis-   matched ( MNLI - mm ) ( Williams et al . , 2018 ) , Ques-   tion Natural Language Inference ( QNLI ) ( Ra-   jpurkar et al . , 2016 ) and Recognizing Textual En-   tailment ( RTE ) ( Wang et al . , 2019 ) for the Natural   Language Inference ( NLI ) task ; The Corpus of   Linguistic Acceptability ( CoLA ) ( Warstadt et al . ,   2019 ) for Linguistic Acceptability .   Following Devlin et al . ( 2019 ) , we exclude   WNLI ( Levesque , 2011 ) . We report F1 scores for   QQP and MRPC , Spearman correlations for STS-   B , and accuracy scores for the other tasks . For   evaluation results on validation sets , we report the2705   average score of 4 ﬁne - tunings with different ran-   dom seeds . For results on test sets , we select the   best model on the validation set to evaluate .   Baselines We mainly compare TACO with MLM   on BERT - small and BERT - base models . In ad-   dition , we also compare TACO with related con-   trastive methods : a sentence - level contrastive   method BERT - NCE and a span - based contrastive   learning method INFOWORD , both from Kong   et al . ( 2020 ) . We directly compare TACO with the   results reported in their paper .   4.2 Results on BERT - Small   Table 1 and Figure 5 show the results of TACO   on BERT - small . As we can see , compared with   MLM with 250k training steps ( convergence steps ) ,   TACO achieves comparable performance with only   1/5 computation budget . By modeling global de-   pendencies , TACO can signiﬁcantly improve the   efﬁciency of contextualized representation learning .   In addition , when pre - trained with the same steps ,   TACO outperforms MLM with 1.2 average score   improvement on the validation set .   In addition to convergence , we also compare   TACO and MLM on fewer training data . The re-   sults are shown in Table 2 . We sample 4 tasks with   the largest amount of training data for evaluation .   As we can see , TACO trained on 25 % data can   achieve competitive results with MLM trained on   full data . These results also verify the data efﬁ-   ciency of our method , TACO .   4.3 Results on BERT - Base   We also compare TACO with MLM on base - sized   models , which are the most commonly used mod-   els according to the download data from Hugging-   face(Wolf et al . , 2020 ) . First , from Table 3 ,   we can see that TACO consistently outperforms   MLM under all pre - training computation budgets .   Notably , TACO-250 kachieves comparable perfor-   mance with MLM-500 k , which saves 2x computa-   tions . Similar results are observed on TACO-100 k   and BERT-250 k. These results demonstrate that   TACO can achieve better acceleration over MLM .   It is also a signiﬁcant improvement compared to   previous methods ( Gong et al . , 2019 ) focusing on   accelerating BERT but only with slight speedups .   In addition , as shown in Table 4 , TACO achieves   competitive results compared to BERT - NCE and   INFOWORD , two similar contrastive methods.2706   5 Discussion   5.1 TACO and MLM   To better understand how TACO works , we con-   duct a quantitative comparison on the learning dy-   namic for BERT and TACO . Similar to Section 2.2 ,   we plot the Cosine similarity among contextual-   ized representations of tokens in the same context   ( intra - context ) and different contexts ( inter - context )   in Figure 6 . We ﬁnd that the learning dynamic   of TACO signiﬁcantly differs from that of MLM .   Speciﬁcally , for TACO , the intra - context represen-   tation similarity remains high and the gap between   intra - context similarity and inter - context similarity   remains large at the later stage of training . This con-   ﬁrms that TACO can better fulﬁll global semantics ,   which may contribute to the superior downstream   performance .   5.2 Ablation Study   TACO is implemented as a token - level contrastive   ( TC ) loss along with the MLM loss . Therefore , the   improvement of TACO might come from two as-   pects , including 1 ) denser supervision signals from   the all - token objective and 2 ) the beneﬁts of the   contrastive loss to strengthen global dependencies .   It is helpful to ﬁgure out which factor is more im-   portant . To this end , we design two variants for   ablation . One is a concentrated TACO , where the   contrastive loss is built on the 15 % masked posi-   tions only , keeping the same density of supervision   signal with MLM . The other is an extended MLM ,   where not only 15 % masked positions are askedto predict the original token , so do the rest 85 %   unmasked positions . The extended MLM has the   same dense supervision with TACO but loses the   beneﬁts of modeling the global dependencies . The   results on small models are shown in Figure 6 .   As we can see , the performance of TACO de-   creases if we sample a part of token positions to   implement TC objectives . It shows that more su-   pervision signals beneﬁt the ﬁnal performance of   TACO . However , simply adding more supervision   signals by predicting unmasked tokens does not   help MLM too much . Even equipped with the ex-   tra 85 % token prediction ( TP ) loss , MLM+TP does   not show signiﬁcant improvements and it is notice-   able that the performance of MLM+TP starts to   drop after 150k steps . This further conﬁrms the   effectiveness of TC loss by strengthening global   dependencies .   6 Related Work   6.1 Language Representation Learning   Classic language representation learning meth-   ods ( Mikolov et al . , 2013 ; Pennington et al . , 2014 )   aims to learn context - independent representation   of words , i.e. , word embeddings . They gener-   ally follow the distributional hypothesis ( Harris ,   1954 ) . Recently , the pre - training then ﬁne - tuning   paradigm has become a common practice in NLP   because of the success of pre - trained language   models like BERT ( Devlin et al . , 2019 ) . Context-   dependent ( or contextualized ) representations are   the basic characteristic of these methods . Many2707   existing contextualized models are based on the   masked language modeling objective , which ran-   domly masks a portion of tokens in a text sequence   and trains the model to recover the masked tokens .   Many previous studies prove that pre - training with   the MLM objective helps the models learn syntac-   tical and semantic knowledge ( Clark et al . , 2019 ) .   There have been numerous extensions to MLM . For   example , XLNet ( Yang et al . , 2019 ) introduced the   permutated language modeling objective , which   predicts the words one by one in a permutated or-   der . BART ( Lewis et al . , 2020 ) and T5 ( Raffel et al . ,   2020 ) investigated several denoising objectives and   pre - trained an encoder - decoder architecture with   the mask span inﬁlling objective . In this work , we   focus on the key MLM objective and aim to explore   how MLM objective helps learn contextualized rep-   resentation .   6.2 Contrastive - based SSL   Apart from denoising - based objectives , contrastive   learning is another promising way to obtain self-   supervision . In contrastive - based self - supervised   learning , the models are asked to distinguish the   positive samples from the negative ones for a given   anchor . Contrastive - based SSL method was ﬁrst   introduced in NLP for efﬁcient learning of word   representations by negative sampling , i.e. , SGNS   ( Word2Vec ( Mikolov et al . , 2013 ) ) . Later , sim-   ilar ideas were brought into CV ﬁeld for learn-   ing image representation and got prevalent , such   as MoCo ( He et al . , 2020 ) , SimCLR ( Chen et al . ,   2020 ) , BYOL ( Caron et al . , 2020 ) , etc .   In the recent two years , there have been manystudies targeting at reviving contrastive learning   for contextual representation learning in NLP . For   instance , CERT ( Fang et al . , 2020 ) utilized back-   translation to generate positive pairs . CAPT ( Luo   et al . , 2020 ) applied masks to the original sentence   and considered the masked sentence and its origi-   nal version as the positive pair . DeCLUTR ( Giorgi   et al . , 2020 ) samples nearby even overlapping spans   as positive pairs . INFOWORD ( Kong et al . , 2020 )   treated two complementary parts of a sentence as   the positive pair . However , the aforementioned   methods mainly focus on sentence - level or span-   level contrast and may not provide dense self-   supervision to improve efﬁciency . Unlike these   approaches , TACO regards the global semantics   hidden in contextualized token representations as   the positive pair . The token - level contrastive loss   can be built on all input tokens , which provides a   dense self - supervised signal .   Another related work is ELECTRA ( Clark et al . ,   2020 ) . ELECTRA samples machine - generated to-   kens from a separate generator and trains the main   model to discriminate between machine - generated   tokens and original tokens . ELECTRA implicitly   treats the fake tokens as negative samples of the   context , and the unchanged tokens as positive sam-   ples . Unlike this method , TACO does not require   architectural modiﬁcations and can serve as a plug-   and - play auxiliary objective , largely improving pre-   training efﬁciency .   7 Conclusion   In this paper , we propose a simple yet effective ob-   jective to learn contextualized representation . Tak-2708ing MLM as an example , we investigate whether   and how current language model pre - training ob-   jectives learn contextualized representation . We   ﬁnd that the MLM objective mainly focuses on   local anchors to align contextualized representa-   tions , which harms global dependencies modeling   due to an “ embedding bias ” problem . Motivated   by these problems , we propose TACO to directly   model global semantics . It can be easily combined   with existing LM objectives . By combining lo-   cal and global anchors , TACO achieves up to 5    speedups and up to 1.2 improvements on GLUE   score . This demonstrates the potential of TACO   to serve as a plug - and - play approach to improve   contextualized representation learning .   Acknowledgement   We thank the anonymous reviewers for their help-   ful feedback . We also thank the colleagues from   ByteDance AI Lab for their suggestions on our   experiment designing and paper writing .   References270927102711A Experiment Details   A.1 Pre - training Hyper - parameters   All pre - training approaches involved in experi-   ments use the same pre - training hyper - parameters   but do not include BERT - NCE and INFOWORD .   Results of BERT - NCE and INFOWORD are di-   rectly cited from the original paper ( Kong et al . ,   2020 ) . Following Liu et al . ( 2019 ) , we do not use   the next sentence prediction ( NSP ) objective and   use dynamic masking for MLM with a 15 % mask   ratio , where the masked positions are decided on   the ﬂy .   TACO introduces three extra hyper - parameters ,   including negative sample size K , positive sample   window size Wand temperature  . We set the tem-   perature  as a small value , 0.07 , following Fang   et al . ( 2020 ) . By searching for the best Kout of   { 10 , 50 } and Wout of { 3 , 5 , 10 , 50 } on the small   TACO model , we found that TACO with K=50 and   W=5 performs best , so we also apply these hyper-   parameter choices for base - sized TACO . The full   set of pre - training hyper - parameters are listed in   Table 5 . Actually , TACO outperforms MLM under   most cases in our preliminary experiments . How-   ever , we still also ﬁnd some extreme cases whichmight harm the effectiveness of TACO . If the size   of negative samples Kis too small , e.g. , smaller   than 10 , the performance of TACO degenerates   nearly to the level of BERT baseline . Similar con-   clusions are also mentioned in related works ( He   et al . , 2020 ; Chen et al . , 2020 ) . Also , if the positive   window size Wis too large , e.g. , bigger than 50 ,   the performance of TACO degrades , too . We sus-   pect the over - large positive window brings more   false - positive samples , which makes the sequence   meaning ambiguous , thus harms the performance .   A.2 Fine - tuning Details   For small - sized models , we ﬁne - tune all saved   checkpoints ( 5k , 10k , 20k , 30k , 40k , 50k , 100k ,   150k , 200k , 250k - step ) of different pre - trained   models ( TACO and its ablations ) with the same   hyper - parameters on each task . Considering the   large amount of pre - training checkpoints , we just   adopt the default ﬁne - tuning hyper - parameters and   repeat ﬁne - tuning 4 times with different random   seeds . Then the best performed ﬁne - tuned models   on validation sets are used for testing . This setting   helps make a fair comparison among models and   avoids a large amount of grid - search runs . The task-   speciﬁc hyper - parameters for small - sized models2712   are listed in Table 7 . The general ﬁne - tuning hyper-   parameters are listed in Table 6 .   For base - sized models , we save checkpoints at   100k , 250k , and 500k steps , respectively . During   ﬁne - tuning , we also conduct multiple ﬁne - tuning   runs with different task - speciﬁc hyper - parameter   combinations as shown in Table 8 . Concretely , we   randomly sample 6 different hyper - parameter com-   binations and report the average score for validation   results . Then we select the best - performing run of   500k - step checkpoints ( converged ) for testing .   A.3 Statistic Details   Embedding Similarity We calculate cosine sim-   ilarity of 20 randomly sampled pairs of fre-   quently co - occurrent words from the WordSim353   dataset ( Agirre et al . , 2009 ) labeled by human an-   notators to plot the average similarity curve in Fig-   ure 3(b ) . Corresponding embeddings are obtained   from the embedding layer of the BERT model and   variant models mentioned in Section 2.2 .   Intra-/Inter - context Similarity For every token   win the corpus , we randomly sample a positive   tokenwwithin the same context ( sentence ) and   another token wfrom other sentences . As men-   tioned in Section 2.2 , we take BERT ( Devlin et al . ,   2019 ) as our encoder to get contextualized represen-   tations through the last hidden states h. We mainlyadopt the cosine similarity as the measurement and   calculate the average intra - context similarity ( be-   tween handh ) and the average inter - context   similarity ( between handh ) over all tokens in   the corpus . It is worth noticing that we do use any   masks here when generating a token ’s contextual-   ized representation for statistics .   Other Measurements We observe the same ﬁnd-   ings for MLM under other measurements , though   the statistics before are mainly based on cosine sim-   ilarities . We tried other similarities or distances ,   e.g. , L1 distance , L2 distance and L10 distance , to   evaluate the discrepancy between contextualized   representations from the same context and different   contexts . Speciﬁcally , we make intra - context and   inter - context statistics under speciﬁc measurement   at different pre - training checkpoints , then calcu-   late the ratio of intra - context measurement over   the inter - context one . Table 9 shows the statistical   results . As we can see , when the ratio of L1 dis-   tance decreases , the ratio of cosine similarity and   the dot - production similarity increase , vice versa .   B Extra Experiments   In the standard implementation of BERT , the pa-   rameters of input embeddings are shared with out-   put embeddings . All experiments and analyses in   this paper are based on this assumption . To further2713   conﬁrm the effectiveness of TACO , we conduct the   extra experiments without embedding sharing on   BERT - small . The results are showed in Table 10 .   It is unexpected that the variants without embed-   ding sharing perform worse compared their counter-   parts due to lack of regularization of weight sharing .   From the results , we can see that the TACO without   embedding sharing performs slightly worse than   TACO with embedding sharing . However , com-   pared to the MLM , it is still better than MLM than   0.9 average GLUE score when convergence . These   results prove the effectiveness of TACO even when   embeddings are not sharing.2714