  Min Xiao , Junnan Zhu , Haitao Lin , Yu Zhou , Chengqing ZongState Key Laboratory of Multimodal Artificial Intelligence Systems ,   Institute of Automation , CAS , Beijing , ChinaSchool of Artificial Intelligence , University of Chinese Academy of Sciences , Beijing , ChinaFanyu AI Laboratory , Zhongke Fanyu Technology Co. , Ltd , Beijing , China   { min.xiao , junnan.zhu , haitao.lin , yzhou ,   cqzong } @nlpr.ia.ac.cn ,   Abstract   Multimodal summarization usually suffers   from the problem that the contribution of the   visual modality is unclear . Existing multimodal   summarization approaches focus on design-   ing the fusion methods of different modalities ,   while ignoring the adaptive conditions under   which visual modalities are useful . Therefore ,   we propose a novel Coarse - to- Fine contribu-   tion network for multimodal Sum marization   ( CFSum ) to consider different contributions of   images for summarization . First , to eliminate   the interference of useless images , we propose   a pre - filter module to abandon useless images .   Second , to make accurate use of useful images ,   we propose two levels of visual complement   modules , word level and phrase level . Specif-   ically , image contributions are calculated and   are adopted to guide the attention of both tex-   tual and visual modalities . Experimental results   have shown that CFSum significantly outper-   forms multiple strong baselines on the standard   benchmark . Furthermore , the analysis verifies   that useful images can even help generate non-   visual words which are implicitly represented   in the image .   1 Introduction   With the information explosion , the internet is   flooded with various multimodal information . Mul-   timodal summarization ( MMS ) can help generate   more abundant and comprehensive summary infor-   mation than unimodal based on extra visual infor-   mation . Existing studies on multimodal summariza-   tion include multimodal sentence summarization   ( Li et al . , 2018b ) , multimodal summarization withFigure 1 : Experiments on existing mainstream multi-   modal summarization models . The performance is not   affected by masking images . “ Concat ” is the concate-   nate fusion method , and “ Attn ” is the attention - based   fusion method .   multimodal output ( Zhu et al . , 2018 ) , multimodal   meeting summarization ( Li et al . , 2019 ) and so on .   In this paper , we focus on the task that generat-   ing a text summary based on the input of a text   and an image . It has been proved that integrating   multimodal data can help improve the quality of   the summary ( Li et al . , 2018b ; Jangra et al . , 2020 ;   Palaskar et al . , 2019 ; Yu et al . , 2021 ) .   However , it is unclear whether the visual modal-   ity can indeed benefit the process of summariza-   tion . Thus , we conduct an experiment to explore   the influence of masking images on the summary .   As shown in Figure 1 , the solid lines mean the   performance of summary generated by masking   portions of images , and the dashed lines indicate   the origin performance . It can be observed that   the dashed and the solid lines roughly coincide ,   which indicates that masking images do not affect   the performance of the multimodal model . Some   masking rates can even raise the ROUGE-1 value   of the summary . It indicates that existing models   do not make effective use of image information for8538the summary .   Existing approaches have two major problems .   First , existing studies focus on multimodal fusion ,   such as concatenate , attention - based , and gate-   based fusion ( referring to Related Work ) . However ,   they ignore the adaptive conditions under which   visual modalities are helpful . Thus they are poor at   extracting useful visual information . Furthermore ,   all fusion methods do not explicitly model the im-   age complementarity for the summary . Especially   for the attention - based method , the inter - attention   is not accurate enough , which leads to inefficient   use of the image . Second , in many samples , the   image may introduce noise , while existing fusion   methods assume that all images are helpful for the   summary without considering the interference of   useless images . As analyzed above , we believe that :   1 ) It is essential to eliminate the influence of the   useless image . 2 ) The contributions of the image   to the summary need to be clarified . In particular ,   it is necessary to consider the complementarity of   visual information relative to textual information .   Although we notice the lack of image contribu-   tions , it is difficult to detach various roles of images   from a single fusion layer . Thus , in this work , we   propose a novel Coarse - to - Fine contribution net-   work for multimodal Summarization ( CFSum ) to   extract the role of the image at different stages .   First , we apply a pre - filter module to abandon use-   less images . It coarsely obtains helpful images   for the summary . Specifically , the consistency of   content between image and text is calculated . If   the consistency is low , the image will be masked   in subsequent encoding . Second , when the image   is coarsely useful , the complement module is em-   ployed to finely guide the fusion of text with the   image . To consider image contributions for text   with different granularities , the complement mod-   ule consists of two levels , word level and phrase   level . For the word level complement module , to   obtain the image complementarity over the text , the   difference between bi - modal and uni - modal inputs   is measured through a classification task . Then we   add a loss to guide the attention between words   and the image . For the phrase level complement   module , similar to the word level , the image com-   plementarity on phrases is acquired to guide the   attention between phrases and the image . Through   these modules , the model can acquire more explicit   image contributions and provide better multimodal   encoding for summary generation . Our contributions are as follows :   ( 1 ) We propose a Coarse - to - Fine contribution   network for multimodal Summarization ( CFSum )   to model different contributions of images for sum-   marization .   ( 2 ) We innovatively design a pre - filter module   to coarsely reduce the interference of the useless   images and develop two visual complement mod-   ules to finely obtain image complementarity over   the summary .   ( 3 ) Experimental results show that our model out-   performs strong baselines . Besides , extensive anal-   ysis proves that useful image even contributes to   non - visual words which are implicitly represented   in the image .   2 Related Work   Multimodal Summarization Tasks . In the field   of multimodal summarization , there are usually   three steps . First , different feature extractor mod-   ules are adopted to extract the features of the text   and the image , respectively . Second , the different   features are fused at the fusion layer . Finally , the   fused context features are fed into the text decoder   to generate a summary . Existing studies focus on   multimodal fusion . Specifically , the fusion meth-   ods consist of concatenate , attention - based , and   gate - based . The concatenate fusion directly con-   catenates multimodal features into a fusion context   ( Li et al . , 2018b , 2020a ) . It can fully extract high-   level features of different modalities , but there is a   large gap between high - dimensional spaces . The   attention - based methods fuse all multimodal fea-   tures with attention mechanism ( Atri et al . , 2021 ;   Palaskar et al . , 2019 ; Kitada et al . , 2022 ) , which   can get the correlations between each unit of text   and image . Gate - based methods take text as the   central modality ( Jangra et al . , 2021 ) and exploit   images to help focus on the core information ( Liu   et al . , 2020 ; Li et al . , 2020b ) . In summary , ( 1 ) all   fusion methods do not explicitly model the image   complementarity for the summary , which leads to   inefficient use of the image . ( 2 ) concatenate and   attention - based can not eliminate the influence of   useless images in the fusion layer .   Cross - modal tasks . Some studies have noted the   contributions of modalities and explored the cross-   modal influence in other multimodal tasks . Zeng   et al . ( 2021 ) propose loss modulation to explore the   contribution of individual modalities and devise a   modality filter to reduce modality noise , which con-8539   siders consistency and complementarity between   different modalities . Zhu et al . ( 2018 ) propose   multi - task summarization : the method also selects   the image that best matches the summary when gen-   erating a text summary . It guarantees the positive   effect of images on the summary . Li et al . ( 2022 )   exploit ReLu - based cross - attention to align visual   features to textual representation , which abandons   low - value attention scores for those unaligned vi-   sual features . Inspired by the above studies , we   propose CFSum , which considers various image   contributions for better encoding input text and   generating the final summary .   3 Proposed Methods   3.1 Overview   In this section , we introduce the details of CF-   Sum . Given a dataset consisting of ntriplets   ( t , v , s)with a text t , an image v , and   a summary s , the multimodal summarization task   aims at generating sbased on tandv .   As depicted in Figure 2 , the CFSum takes bi-   modal and uni - modal streams as input parallelly .   It builds coarse and fine image contributions with   three modules ( Coarse - to - Fine Structure ) . First , the   pre - filter module coarsely filters the images incon-   sistent with texts ( Pre - filter Module ) . Second , two   levels of visual complement modules consisting of   word level ( Word - level Complement ) and phrase   level ( Phrase - level Complement ) make accurate   use of useful images.3.2 Coarse - to - Fine Structure   We build our model based on the multimodal trans-   former UNITER ( Chen et al . , 2020 ) and GRU   ( Chung et al . , 2014 ) encoder - decoder architectures .   We refer the model to UniG. As shown in Figure   2(a ) , in order to evaluate the complementarity of   different modalities , the bi - modal and uni - modal   inputs are operated parallelly with the same en-   coder . The two parallel streams can catch the gain   of the image . Additionally , we generate a summary   relying on bi - modal encoding . Uni - modal encod-   ing assists in measuring various contributions and   guiding the bi - modal encoding .   Specifically , the multimodal encoder consists of   L= 12 multimodal transformer layers . We serve   theLlayers as a hierarchical structure and divide   Llayers into three parts as shown in Figure 2(a ) .   L , L , Lmark as the starting layer of the pre-   filter , the word - level complement , and the phrase-   level complement modules , respectively . Existing   studies assume all images benefit summary gener-   ation or input text encoding , resulting in damage   from unnecessary images . The pre - filter module is   utilized to eliminate the interference of misleading   images in advance . Next , the word - level comple-   ment module is developed to model the gain of   the image on input words for the summary . Then   the image gain guides the subsequent attention be-   tween words and the image . Finally , similar to   the word level , the phrase - level complement mod-   ule concentrates on phrases at higher layers . Each   component will be elaborated in the following sub-   sections.85403.3 Pre - filter Module   The bi - modal and uni - modal features from the i   layer are encoded as m∈R , u∈R ,   where i∈[1 , L ] , andC , T denote the lengths of bi-   modal and uni - modal tokens . Hdenotes the hidden   dimension . The bi - modal self - attention matrix in   theilayer is A= ( a)∈R.   The pre - filter module aims at filtering images   that are unnecessary to the summary . As shown   in Figure 2(a ) , given two encoded features m   andufrom the Llayer , the goal of the filter-   ing module is to select those useless images and   guide the self - attention of all subsequent layers .   We believe that if the bi - modal feature has low con-   sistency with the uni - modal feature , the image may   introduce interferential information . Specifically ,   we first calculate the consistency ∆between uni-   modal feature uand bi - modal feature mas   follows :   We define the indicator function as :   which represents the text attending to the image , the   image attending to the text , and the image attending   to itself shown in Figure 2(a ) . Then we calculate   the new subsequent self - attention nawith :   By correcting the attention matrix , the image with   a large deviation in content is cropped out . In other   words , the multimodal inconsistency features de-   generate into text - only features through this pro-   cess . The simple method has been shown to be   effective in our experiments .   3.4 Word - level Complement   This section introduces a word - level complement   module , considered as an auxiliary task during the   training process . First , we measure the image gain   on input words for the summary . Then the image   gain is applied to guide the attention between words   and the image ( as shown in Figure 2(b)).Image gain measurement . Intuitively , the text   tokens should concern the image which is help-   ful for the summary . In previous attention - based   studies , inter - modality correlation can be modeled   assoftmax ( ) V.Q , K , V are the projected fea-   tures from the bi - modal input . However , it does   not explicitly model the image complementarity   for the summary , which leads to inefficient use of   the image .   Following the motivation above , we hope to cal-   culate the image gain on the summary with mutual   information . In other words , we want to measure   whether generating summaries based on bi - modal   feature mis more deterministic than generating   summaries based on uni - modal feature u. Thus ,   we expect to calculate the image gain on the k - th   word of the reference summary :   However , we intend to obtain GIbefore gener-   ating summary Sand encoding m. Thus GIcan   be beneficial for generating Sand encoding m.   To this end , we define Copy Classification task Y   to approximate the summary task S : for each in-   put text token t , the target is to binary categorize   whether it appears in the reference summary . If   the token appears in the reference summary , it is   classified as ˆy= 1 ; otherwise , ˆy= 0 . Next , the   GIis given by :   where u , mdenote the uni - modal and the bi-   modal feature acquired by Llayer . Finally , we   measure the gain that the image brings to predict   whether a word appears correctly in the summary   as follows :   Derivation details can refer to the Appendix B. In   addition , to ensure the correct gain direction , we   add a binary cross - entropy loss to train the Copy   Classification Task Y :   Image gain application . We introduce diver-   gence loss to restrain that the image with greater   gain should receive more textual attention . In suc-   cessive ilayer , the average inter-   attention between each text token tand the image   is:8541   where a , arepresent the attention of image - to-   text and text - to - image , respectively .   Finally , an attention divergence loss is added to   restrain the inter - attention scores T2VwithGI :   By minimizing the divergence loss , the text token   attends to the image according to the gain it brings .   Interaction between word gain and inter - attention   learns to pay attention to the useful image . Ap-   pendix C provides examples to figure out the word-   level complement .   3.5 Phrase - level Complement   Considering the image contribution to text of dif-   ferent granularities , we put forward a phrase - level   complement module similar to the word level ( as   shown in Figure 2(c ) ) .   Image gain measurement . Different from copy   classification task at the word level , we define   Copy Scorer task to measure the image gain on   phrases : We obtain phrases { p , ... , p ... }from the   text with StandfordNLP.{l , ... , l ... }is the num-   ber of words in the phrases . The task targets scor-   ing the proportion of words that appear in both the   phrase and the reference summary :   where Scorer is a MLP . The ground truth proportion   is obtained with the following :   where Countdenotes the number of words   that appear in both the phrase pand the reference   summary . Therefore , the image gain on phrase can   be acquired as :   Similarly , to guarantee the correctness of phrase   gain , we add a squared loss for the Copy Scorer   task : Especially , for the convenience of applying phrase   gainGS , we project it to token gain GSas :   Image gain application . Second , we introduce a   phrase attention divergence loss to restrain that the   image with greater phrase gain should receive more   textual attention . We obtain the inter - attention   score T2Vfromi∈[L+1 , L+3 ] layers as for-   mula 10 . Finally , we restrain it with the following :   The phrase - level restraint guarantees the image con-   tributing to the text of phrase granularity .   3.6 Training and Inference   In the training phase , to ensure the accuracy of   the information difference between bi - modal and   uni - modal , we initialize the model only with the   summary generation loss . We apply negative log-   likelihood for the target word sequence as the over-   all loss :   Then the model is finetuned with the hierarchical   modules ’ objectives :   In the inference phase , we only maintain the pre-   filter module . L andL are added to let   the model learn how to fuse multimodal informa-   tion . Hence , differences in training and inference   phases would not hurt the generation .   4 Experiment   4.1 Settings   We experiment with the multimodal sentence sum-   marization dataset(Li et al . , 2018a ) . It contains8542   66,000 samples in total . And each sample is a   triplet of < sentence , image , summary > . Some sta-   tistical information is shown in Table 1 . Appendix   D gives the categories of test images .   We set both the text embedding dimension and   hidden dimension as 768 . We apply “ bert - base-   uncased ” ( Devlin et al . , 2019 ) vocabulary with   28,996 tokens . The dropout ( Srivastava et al . , 2014 )   rate is set to 0.1 . Besides , the batch size is set to   8 . For texts , we use the max text encoding length   of 60 , and the minimum text decoding length is 8 .   For images , the object detection tool BUTD ( An-   derson et al . , 2018 ) is applied to extract the image   feature , with the maximum boxes as 36 . We use the   Adam ( Kingma and Ba , 2014 ) optimizer and set the   learning rate as 5e−05 , momentum parameters as   β= 0.9 , β= 0.98 . The model is initially trained   with the summary generation loss for 35 epochs .   To obtain our final model , we train for a further 15   epochs with the hierarchical framework . In the test   phase , we employ beam search and set the beam   size as 4to generate the summary . The parameter   αin the pre - filter module is set as α= 0.65 .   4.2 Comparative Methods   Lead : Exploiting the first eight words as the sum-   mary .   Compress ( Clarke and Lapata , 2008 ): It uses in - teger linear programming to infer global optimal   compressions .   ABS ( Rush et al . , 2015 ): It utilizes an attention-   based model to generate words of summary condi-   tioned on the input text .   SEASS ( Zhou et al . , 2017 ): It constructs a second-   level sentence representation with a sentence en-   coder and a selective gate for summarization .   Multi - Source ( Libovický and Helcl , 2017 ): It com-   bines multiple source modalities based on the hier-   archical attention mechanisms over each modality   for solving the multimodal machine translation .   Doubley - attentive ( Calixto et al . , 2017 ): It uses   two separate attention mechanisms to incorporate   the visual feature , which minified the gap between   the image and the translation .   MAtt ( Li et al . , 2018b ): It proposes modality atten-   tion and image filtering for multimodal summariza-   tion .   MSE ( Li et al . , 2020b ): It proposes to apply the vi-   sual selective gates to multimodal summarization .   UniG : It is our base model with multimodal trans-   former UNITER and GRU decoder .   UniG ( T ) : UniG fed only with textual modality .   4.3 Automatic Evaluation Results   Our methods are reported with six automatic met-   rics , including ROUGE-1 , ROUGE-2 , ROUGE-   L ( Lin and Hovy , 2002 ) , BLEU ( Papineni et al . ,   2002 ) , BERTScore ( Zhang * et al . , 2020 ) , and   MoverScore ( Zhao et al . , 2019 ) . More details of8543   evaluation scripts are given in Appendix A.   Comparisons with Baselines . We compare our   work with our baselines and other work on the   multimodal sentence summarization dataset . Ta-   ble 2 shows the results of different models . The   results show that UniG performs comparably with   UniG ( T ) .CFSum s build on UniG , and introduces   coarse - to - fine contribution network . “ F ” , “ W ” , and   “ P ” represent the pre - filter , the word - level comple-   ment , and the phrase - level complement modules   contained in the CFSum . The footnote is the lo-   cation of the corresponding module . For exam-   ple , CFSum - Fcontains a pre - filter module with   L= 3 . Generally , our methods CFSum s outper-   form the baselines UniG ( T ) andUniG . The best   methods is CFSum - FWP . And it achieves 1.64   higher points on ROUGE-1 than UniG . We also   conduct ablation experiments by applying one or   two kinds of contributions . The results demonstrate   that each image contribution benefits the model . In   addition , combining all image contributions brings   greater gains than a single contribution . Therefore ,   it can be concluded that different contributions are   complementary to the summary . Besides , we con-   duct ablation studies by placing the pre - filter mod-   ule at the beginning ( L= 3 ) or the end of the   hierarchical layers ( L= 9 ) . In comparison , plac-   ing the pre - filter module at the beginning ( CFSum-   FWP ) yields better performance .   4.4 Human Evaluation Results   We randomly select 50 samples from the test   dataset and invite three postgraduates to score 1-   5 for the summary quality . The evaluation met-   rics include informativeness , fluency , and non - redundancy . ( 1 ) Informativeness : Does the system   summary contain comprehensive reference con-   tent ? ( 2 ) Fluency : Is the system summary grammat-   ically correct and readable ? ( 3 ) Non - Redundancy :   Does the system summary not have redundant or   incorrect information relative to the reference sum-   mary ? Table 3 shows the human evaluation re-   sults . We run the inter - annotator agreement study   on three volunteers ’ scores and achieve reasonable   scores , 0.47 , 0.39 , and 0.43 on informativeness , flu-   ency , and non - redundancy , respectively . The results   show that our method CFSum - FWSachieves   the best performance on all three aspects over UniG   ( T)andUniG baselines . Thus we conclude that our   method is also effective through human evaluation .   4.5 Further Analysis   4.5.1 Complement Modules Analysis   In other multimodal tasks such as image caption-   ing and multimodal translation , their models learn   to attend to the image more for visual words like   “ red ” , “ rose ” and “ woman ” ( Lu et al . , 2017 ; Calixto   et al . , 2017 ) . Since our proposed complement mod-   ules aim at extracting complementary information   relative to textual modality , we want to know which   word or phrase the image provides gains on . As   shown in Figure 3 , we visualize the complement   gain value for the input words . We manually align   the reference summary and the input text . The   word highlighted with a red box indicates that it   appears in the reference summary generativelyor   extractively .   First , we find that words with positive image   gain can basically cover the reference summary in-   formation . It proves that our calculated gain helps   in generating the target summary words . Second ,   it can be observed that different complement mod-   ules bring positive gains in different areas , which   means different levels of complement modules are   complementary . It further explains that multiple8544   contributions are better than a single contribution in   the experimental results . At last , it is worth noting   that some words are gained from the image but are   not visible in the image , i.e. , “ relatives ” and “ vic-   tims ” . Therefore , we believe the image brings gain   in both visible and invisible words . We explain   further in Gainable Images .   4.5.2 Pre - filter Module Analysis   Since we believe that images should provide mean-   ingful contributions instead of robustness enhance-   ments in multimodal summarization , we wonder   whether unpaired multimodal data may affect the   performance of our model . Therefore , we try gen-   erating the summary based on the unpaired image   and text .   In the test set , most of the images are highly sim-   ilar in theme and content . Generating unpaired data   with automatic shuffling is not significant for analy-   sis . Therefore , we manually exchange vandvin   pairs < t , v > , < t , v > , where v , vhave different   themes or contents .   We exchange 20 pairs from 100 pairs of test   samples . And we conduct experiments with differ-   ent sampling for three times . The mean and stan-   dard deviation reports as Table 4 . “ Paired ” repre-   sents ROUGE-1 on test set , “ Unpaired ” represents   ROUGE-1 on the unpaired set . “ CFSum ( filter - off ) ”   represents turning down the pre - filter mechanism .   The results show different trends in the two mod-   els . For UniG , unpaired multi - modalities do not   affect the performance . We guess UniG does not ex-   ploit meaningful image information while relying   only on text to generate the summary . In contrast ,   CFSum hurt more severely from unpairing . The   difference exists because CFSum depends on the   image and text . Thus , the unpaired image would   reduce the correct information that CFSum gets .   However , CFSum still performs better than UniG ,   proving that it is fault - tolerant . Furthermore , CF-   Sum ( filter - off ) significantly suffers from unpaired   data , showing that pre - filter can eliminate useless   images .   4.5.3 Ablation Study   One of the most important hyperparameters in CF-   Sum is the location of different contribution mod-   ules . Because the three modules ’ order in the net-   work is fixed , we change their absolute position   in the encoder layers and report the corresponding   performance in Figure 5 . wdenotes the number   of layers between two modules , and the Xaxis   denotes the starting layer of the pre - filter module .   The results show that the different layer settings   achieve comparable performance . It is noticeable   thatw= 2weakens the model . This is due to the   fact that the network with small wloses the advan-   tage of a hierarchical structure in the encoder .   4.5.4 Gainable Images   We select three gained words and corresponding   gainable images to show in Figure 4 . Consistent   with our perception , images bring gains on visual   words , such as “ earthquake ” . More importantly ,   they bring gains on non - visual words such as “ cel-   ebrate ” and “ victims ” . For example , “ celebrate ”   may be used in competitions , events , and diplo-   macy as shown in Figure 4 . Multimodal tasks such   as image captioning or multimodal question an-   swering focus on establishing associations between   visual words and images . However , multimodal   summarization also needs to pay attention to the   associations between non - visual words and images .   In other words , image contributes to both visual   and non - visual words.85455 Conclusion   Based on the observation that existing multimodal   summary models do not take full advantage of use-   ful image information , this paper focuses on mod-   eling different contributions of images for summa-   rization . Therefore , we propose a novel framework   CFSum consisting of pre - filter , word - level comple-   ment , and phrase - level complement modules . The   pre - filter coarsely eliminates the impact of useless   images . The two - level visual complement modules   measure different aspects of image gains and guide   the fusion of different modalities . Experimental   results have shown that CFSum can significantly   improve the summary . More importantly , the com-   plement modules make images contribute to visual   words and non - visual words .   Limitations   Since our method constructs on the multimodal   transformer , it can not be migrated to the dual-   stream model . Experiment results show that CF-   Sum can achieve comparable performance with   strong baselines . But it still can not surpass the   SOTA of some dual - stream large models .   Acknowledgements   The research work has been supported by the Natu-   ral Science Foundation of China under Grant No .   62106263 .   References85468547A Experiment details   Here , we will introduce some detailed settings for   our experiments . All methods are run on NVIDIA   GeForce RTX 3090 . UniG has 139 M parameters .   When the batch size is 8 , it takes 20 hours to train   for 50 epochs with a single GPU .   We also provide evaluation scripts for reproduc-   tion . For ROUGE score , we use file2rougewith   default settings . For BERTScore , we use the of-   ficial API . It exploits the pre - trained contextual   embeddings from BERT to calculate the similarity   between the hypothesis sentences and the refer-   ence sentences . For MoverScore , we use mover-   score_v2 , which leverages BERT and Earth Mover   Distance to measure the similarity .   B Derivation details   Derivation detail of formula 8 is :   Thus the gain is simplified to entropy difference .   C Examples of Complement Modules   We will provide some examples to explain further   Word - level Complement . For one of the input   words t , we assume that it appears in the refer-   ence summary . Then the ground truth of the copy   classification is ˆy= 1 . We list hypothetical classi-   fication results of bi - modal and uni - modal in Table   5 .   Then , the GIis calculated as :   which means the image may give the input word t   a gain of 0.405 . Furthermore , the image brings a   positive gain . Thus in the attention layer , the text   word tshould give the image a higher attention   score .   D Impact of image category   To further analyze the impact of our approach on   different categories of images . We categorize the   test images with VGG19 and show the performance   of each type of image . As shown in Figure 6 , there   are 380 categories in the test images , and we list   the top 10 categories with the highest proportion .   It can be seen that the image is evenly distributed .   The line charts also show that CFSum is superior   to UniG in all categories . Therefore there is no   category bias in our method .   E Guided Attention   We visualize ( 1 ) the attention matrix from the 8   encoder layer of CFSum - FWS , whose layer is   under the word - level guidance . ( 2 ) the attention   matrix from the 11encoder layer of CFSum-   FWS , whose layer is under the phrase - level   guidance . The attention matrix is renormalized   after removing [ CLS ] and [ SEP ] . They are shown   in Figure 7 and Figure 8 .   From the attention under the word - level guid-   ance , we can observe that some input words which   generatively or extractively occur in the reference   summary will attend to the image , such as “ crash ”   and “ relatives ” . From the attention under the   phrase - level guidance , we can observe that some   input phrases which generatively or extractively   occur in the reference summary attend to the im-   age more . Above all , it also proves that two visual   complement modules succeed in providing better   encoding to generate summaries.8548854985508551ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Our paper is foundational research and not tied to particular applications .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Section 1 Introduction .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4 Experiment .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4.3 Automatic Evaluation Results .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4.1 Settings .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4.1 Settings .   C / squareDid you run computational experiments ?   Section 4 Experiment   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A Experiment details8552 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4.1 Settings .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4.3 Automatic Evaluation Results .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3 Proposed Methods and Section 4 Experiments   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 4.4 Human Evaluation Results .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Section 4.4 Human Evaluation Results .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Section 4.4 Human Evaluation Results .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.8553