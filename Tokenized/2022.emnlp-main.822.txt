  Viktor Hangya , Hossain Shaikh SaadiandAlexander FraserCenter for Information and Language Processing , LMU Munich , GermanyMunich Center for Machine Learning , GermanyTechnical University of Munich , Germany   { hangyav , fraser}@cis.lmu.de   shaikh.saadi@tum.de   Abstract   Pre - trained multilingual language models are   the foundation of many NLP approaches , in-   cludingcross - lingualtransfersolutions . How-   ever , languages with small available mono-   lingual corpora are often not well - supported   bythesemodels leadingto poorperformance .   We propose an unsupervised approach to im-   provethecross - lingualrepresentationsoflow-   resourcelanguagesbybootstrappingwordtrans-   lationpairsfrommonolingualcorporaandus-   ing them to improve language alignment in pre-   trained language models . We perform experi-   mentsonninelanguages , usingcontextualword   retrievalandzero - shotnamedentityrecognition   to measure both intrinsic cross - lingual word   representationqualityanddownstreamtaskper-   formance , showingimprovementsonbothtasks .   Ourresultsshowthatitispossibletoimprove   pre - trained multilingual language models by   relying only on non - parallel resources .   1 Introduction   Pre - trainedlanguagemodels(LMs)havereplaced   static word embeddings , such as word2vec , fast-   Textor GloVe ( Mikolov etal . , 2013a;Bojanowski   etal . ,2017;JameelandSchockaert,2016),dueto   theirsuperiorcontextualizedrepresentations . Ap-   proaches such as mBERT ( Devlin et al . , 2019 ) or   XLM - R(Conneauetal . ,2020a)aretrainedonmul-   tiple languages simultaneously resulting in mul-   tilingual models which can be used for various   cross - lingualtransferlearningtasks(Conneauetal . ,   2018b ; Schuster et al . , 2019 ; Artetxe et al . , 2020 ,   inter alia ) .   However , multilingualLMsmainlyfocusonhigh   resource languages , e.g. , mBERT supports the top   104 languages based on Wikipedia sizes , while   XLM - R supports the top 100 based on Common-   Crawl data . Additionally , many of these languages   areunderrepresentedleadingtolowmodelperfor-   mance in both monolingual and cross - lingual se-   tups . Due to the small data sizes of low - resourcelanguages , subword tokenizers trained jointly on   multiple languages tend to over - split the tokens   of such languages and LMs are not able to learn   goodqualityrepresentationsforthem . Recentwork   have shownthat pre - trained LMs canbe improved   on low - resource languages using vocabulary ex-   tensionandmodelfine - tuning(Wangetal . ,2020 ) .   The cross - lingual quality of LMs can further be   improved by learning an additional alignment of   languagepairs(AldarmakiandDiab,2019;Wang   et al . ,2019 ) orfine - tuningthe wholemodel ( Cao   et al . , 2020 ) . However these methods require cross-   lingual data , often in the form of word aligned par-   allelsentences , toimprovecross - lingualitywhich   is often missing for low - resource languages .   In this work we propose an unsupervised ap-   proach to improve the language support of low-   resource languages without any parallel data . Rely-   ing on the initial cross - lingual quality of mBERT   weminewordtranslationpairsfrommonolingual   dataofthesourceandtargetlanguagepairsbylever-   agingcontextualizedcross - lingualwordrepresenta-   tions ( CCWRs ) . More precisely , we build CCWRs   for each token in the source and target corpora and   look for the most similar token pairs by calculating   theircosinesimilarity . Eventhoughourapproach   doesnotrelyonparallelcorpora , weshowthatthere   are enough sentences with similar contexts ( topics )   containingatleastonewordtranslationpairthatare   detected by our mining approach . We then use the   CCWRs of the mined word pairs to make their rep-   resentations more similar , this way aligning source   and target pairs . We use dedicated linear layers for   both of the languages of the considered language   pairtolearnthealignmentandwekeeptheLM ’s   core frozen . An important contrast with previous   work(AldarmakiandDiab,2019;Wangetal . ,2019 ;   Cao et al . , 2020 ) is that we mine word pairs from   sentences of similar contexts , while they were only   able to extract them from parallel sentences , which   areoftennotavailableforlowresourcelanguages.11993Despitethis , weshowthatCCWRscanbeimproved   using our mined word pairs .   We conduct experiments on nine low - resource   languagesandtestontwotasks : contextualcross-   lingualwordretrieval(Caoetal . ,2020)andzero-   shotnamedentityrecognition(Rahimietal . ,2019 ) .   Our results show improved CCWRs for each of the   languages andimprovedNER Fscores for eight   out of the nine languages . Our analysis reveals our   approach to be robust even when we mine noisy   wordpairs , becauseitbenefitsmorefromalarger   quantity than a better quality of bootstrapped train-   ing set . Additionally , we experiment with vocab-   ulary extension techniques ( Wang et al . , 2020 ) in   case of languages using scripts other than Latin or   Cyrillic , sincethetokenizersofLMstendtoover-   split the text of such languages . This step improves   CCWR quality of the initial LMs which gives a   further boost to our bootstrapping method , leading   tothebestperformanceontheselanguages . Finally ,   werunpreliminaryexperimentsonfurtherfourvery   low - resource languages that were not used for LM   pre - training and show that the initial LM quality   on such unseen languages is very low , which our   approachcanmarginallyimprove . Ourimplemen-   tation is publicly available .   2 Related Work   Pre - trainedLMs(Devlinetal . ,2019;Conneauetal . ,   2020a ) provide the core of many NLP solutions .   They are trained using accessible monolingual cor-   poraofmultiplelanguagesresultinginmultilingual   LMs allowing them to be used in zero - shot trans-   fer setups mitigating the issues of missing down-   streamtasktrainingdataformanylow - resourcelan-   guages(Conneauetal . ,2018b;Schusteretal . ,2019 ;   Artetxeetal . ,2020 ) . Althoughnoparalleldatais   used , thesemodelsshowremarkablecross - lingual   quality , i.e. ,wordswithsimilarmeaningindiffer-   entlanguagesarerepresentedsimilarlybythemod-   els . Previousworkinvestigatedthereasonsofthis   phenomenon . Ketal.(2020)foundthatthestruc-   tural similarity of languages is an important factor ,   whilebothConneauetal.(2020b)andArtetxeetal .   ( 2020)showedthatasharedvocabularyisnotneces-   sary . DufterandSchütze(2020)identifiedessential   elements for multilinguality , such as shared special   tokens or comparable training corpora .   On the other hand , multilingual LMs are less ef-   fective on low - resource languages . It was shownthat a fixed sized model can only support up to   a certain number of languages efficiently , while   addingmorelanguagesdeterioratesitsperformance   ( Conneau et al . , 2020a ) . The small size of the   available monolingual data for low - resource lan-   guagesdecreasesmodelperformancefurther(Wu   and Dredze , 2020a ; Lauscher et al . , 2020 ) . Addi-   tionally , data size imbalance of the used languages   leadstoanimbalancedsubwordvocabularyaswell   ( Rust et al . , 2021 ) . To mitigate tokenization issues   Wang et al . ( 2020 ) extend the vocabulary of pre-   trainedLMswithlanguagespecifictokens , while   Pfeiffer et al . ( 2021 ) propose to learn language   specific embeddings for low - resource languages .   Theyshowthatwithbettersubwordvocabularyand   model fine - tuning the model ’s performance can be   improved on low - resource languages . We also rely   on these techniques in our work .   To improve the cross - lingual quality of LMs var-   ious authors proposed steps on top of model pre-   training . Basedonembeddingmappingapproaches   ( Mikolovetal . ,2013b;Conneauetal . ,2018a)itwas   shownthatrepresentationsofmonolingualLMscan   also be aligned ( Schuster et al . , 2019 ) . In addition ,   mapping approaches can be applied to the repre-   sentationsofmultilingualLMsaswellforfurther   improvements(AldarmakiandDiab,2019;Wang   et al . , 2019 ; Hämmerl et al . , 2022 ) . In contrast to   the above approaches , Cao et al . ( 2020 ) proposes a   wordalignmentbasedobjectivefunctionthatfine-   tunes the whole model in order to build more simi-   lartokenrepresentationsforthealignedwordpairs ,   while Chi et al . ( 2021 ) introduced the denoising   wordalignmenttaskandWuandDredze(2020b )   relied on a contrastive alignment objective to en-   couragebettercross - lingualperformance . Others   leverage cross - lingual training signals already in   the model pre - training phase ( Conneau and Lam-   ple , 2019 ; Hu et al . , 2021 ) . However , the above   approaches require parallel data which is not avail-   able for many low - resource languages . In contrast ,   weshowthatLMscanbeimprovedbyusingonly   monolingualcorporawheresourceandtargetlan-   guagesentenceswithsimilarcontextscanbefound .   3 Unsupervised Language Alignment   In order to improvethecross - lingual quality of pre-   trainedmultilingualLMsweminewordtranslation   pairsusingonlymonolingualcorporaofthesource   andtargetlanguages . Werelyonthetokenrepresen-   tations given by LMs to look for the most similar11994   sourceandtargetlanguagewordpairsinthemining   step . Mined pairs are then used as training samples   intheupdatestep , wherethetrainingobjective is   tomaketheirrepresentationsmoresimilar . Weiter-   ate the mining and update steps until convergence .   Eventhoughourapproachdoesnotrequireparallel   sentences , weshowthatitispossibletomineuse-   fulwordpairsfromsentenceswithsimilarcontexts .   WedepictourapproachinFigure1anddetailthe   two main steps in the following .   3.1 Word Pair Mining   For the word translation pair extraction we assume   that we have monolingual corpora for the source   and target languages , DandD. We build con-   textualizedcross - lingualwordrepresentationsfor   each token in each sentence of both corpora by tak-   ing the corresponding output vectors of the used   model ’s final layer . As explained in Section 3.2 we   applyalinearlayerontopoftheusedLMforalign-   ment purposes . We take the output of this linear   layerasCCWRinsteadoftheaverageofmultiple   LMlayersincontrasttopreviouswork(Vulićetal . ,   2020 ) in order to directly benefit from model up-   dates as our method progresses . If a word is split   into multiple subwords by the LM ’s tokenizer we   take the representation of the last subword token   based on the findings of ( Ács et al . , 2021).For   each token w∈Dwe look for the most similar   token w∈Dby calculating :   w= arg maxcos(E , E)(1 )   where wisthejtokenofthe isentencein D ,   E(w)is its CCWR and cosis the cosine similar - ity of two vectors . Finally , we filter out low quality   wordpairsbykeepingonlythosepairswhichhavea   similarity value larger than a given threshold value   ( th= 0.2 ) . Weminewordpairsinbothlanguagedi-   rections , i.e. ,weminepairsforeach w∈Dand   inthereversedirectionaswellforeach w∈D   this way boosting the number of training examples .   Duetothequadraticnatureoftheminingprocess ,   instead ofusing the full sourceand target corpora ,   werandomlysample 1Ksentencesineachiteration   fromDandDresultingin ˜Dand˜Drespectively   and use them for the word alignment instead of the   full corpora . Calculating with 20tokens per sen-   tenceonaveragemeansthatthereare 20Ksource   tokensfor whichwelook forthemost similarpair   inthe 20Kcandidatetargettokensineachiteration .   Furthermore , wekeeptheminingprocesssimple ,   fast and memory efficient , i.e. , we use cosine simi-   larity instead of CSLS ( Conneau et al . , 2018a ) and   do not rely on previously introduced word align-   ment techniques , such as SimAlign ( Jalili Sabet   et al . , 2020 ) . We present further analysis of the   miningqualityinSection5.3andexperimentswith   upto 100Ksamplesizeinsteadof 1Kperiteration   and employ the efficient search method of Faiss   ( Johnson et al . , 2019 ) in Appendix C.   3.2 Model Training   Motivated by mapping approaches ( Mikolov et al . ,   2013b ; Schuster et al . , 2019 ) we use a dedicated   linearlayerontopofthepre - trainedLMforboth   sourceandtargetlanguagesinthegivenlanguage   pair to improve cross - lingual quality . More pre-   cisely , we feed the output representations of the11995LM ’s last hidden layer to the dedicated linear layer   dependingonthelanguageoftheinputword , which   hasinput andoutputsizesmatchingtheLM ’s hid-   den size and has no bias term . Since we want to   exploit the improved cross - linguality of the model   intheminingstepineachiteration , weinitializethe   dedicatedlinearlayerswiththeidentitymatrixand   use the output of the linear layers as CCWRs for   word pair mining . Given themined word pairs we   updatetheparametersofthededicatedlinearlayers   whilekeeping theparametersof theLM frozenby   minimizing the following loss function based on   the work of Cao et al . ( 2020 ):   L=/summationdisplay / vextenddouble / vextenddouble / vextenddoubleE−E / vextenddouble / vextenddouble / vextenddouble(2 )   where D⊆˜D×˜Disthedatasetcontainingthe   mined word pairs from step 1 .   Wenotethatweexperimentedwiththeproposed   model of Cao et al . ( 2020 ) , i.e. , updating the full   model using no linear layer on top of the origi-   nal LM architecture but it resulted in significant   degradation of the model performance when using   our mined training samples . The main difference   withtheoriginalmethodofCaoetal.(2020)isthat   theyuseparallelsentenceswheremostofthewords   arealigned , whileinourcasemostsentencepairs   thatareextractedbyourmethodcontainonlyone   alignedwordpairandtherestareunaligned , thus   not usedfor training . Our conjectureis thatdue to   the smaller information ratio per sentence pair and   the smaller number of covered unique words in the   training process ( see Section 5.3 ) the full model   updateismoresusceptibletoover - fitting , thusthe   use of language specific linear layers is crucial for   our approach . This shows that the simpler linear   layer and the static LM is needed to prevent over-   fitting and degrading the quality of monolingual   language subspaces in case of the mined training   data . Wesummarizeourcompleteiterativemethod   in Algorithm 1 .   4 Experimental Setup   4.1 Datasets and Model Parameters   Wetestourproposedmodelonawiderangeoflan-   guages : Bengali , Basque , Macedonian , Malayalam ,   Afrikaans , Swahili , Kannada , GujaratiandNepali .   For each of the languages we used 1MrandomlyAlgorithm 1 High level pseudo - code of our pro-   posed method .   Require : D , Dthefullmonolingualcorporaof   the source and target languages ; Θpre - trained   model parameters ; Nnumber of update steps ;   thminimum word similarity threshold   fori in0 .. Ndo   ˜D,˜D←sample ( D ) , sample ( D )   D←mining ( ˜D,˜D , Θ , th )   Θ←update ( D , Θ )   end for   selectedsentences fromthefull Wikipediadumps   asmonolingualcorpora ( DandD)whichwetok-   enized with the IndicNLP toolkit ( Kakwani et al . ,   2020)incaseoftheIndiansubcontinentlanguages   or with the Mosestoolkit ( Koehn et al . , 2007 ) in   case of the others . We show language and dataset   statistics in Table 1 . In Appendix D we present   simulated low - resource experiments indicating the   effectiveness of our approach when only a small   amount of monolingual sentences are available .   As the pre - trained LM we use bert - base-   multilingual - cased ( Devlinetal . ,2019)whichwe   refer to as mBERT in the following . To try to   strengthenthebaselinesforunderrepresentedlan-   guagesweperformvocabularyextensionandmodel   fine - tuningusingthemonolingualdataofthetarget   low - resourcelanguage . Wefollowtheapproachand   suggested model parameters of Wang et al . ( 2020 ) ,   i.e. , weextend mBERT ’s originalvocabulary with   the most frequent 10Ksubword tokens of the low-   resource language and run 100Kfine - tuning steps   onthefullWikipediadumpsusingonlythemasked   language modeling objective . We refer to the vo-   cabulary extended models as eBERT .   Forourapproachwetunemodelparametersus-   ingthedevelopmentsetofthewordretrievaltask   ( seeSection4.2)on Nepalionlyandusethe same   parameters for the rest of the languages as well .   The used parameters are the following : number   of model update steps ( N)5Kwith batch size 2 ,   gradientaccumulationsteps 6(whichmeans 60 K   extracted sentence pairs ) , 1Kwarm - up steps and   learning rate 5×10 . We used the Hugging-   facelibrary for the implementation of our tech-   niques ( Wolf et al . , 2020 ) . The runtime of our   method ranges between 0.5and2hours using a11996   single GeForce GTX 1080 Ti .   4.2 Evaluation   Contextual cross - lingual word retrieval Asan   intrinsic evaluation of cross - lingual quality of LMs   we perform the word retrieval task as defined by   Caoetal.(2020 ) . Givenawordalignedparalleltest   corpus for the source and target languages the task   is for each aligned token in the source corpus to   retrieve its pair , i.e. , the target word it is aligned to ,   givenalltokensinthetargetcorpus . Similarlyasin   ourproposedminingprocess , wefollowEquation1   forwordretrievalwiththeonlyexceptionthatwe   useCSLSinsteadofcosineasthevectorsimilarity   function , sinceitslongerruntimeandlargermem-   oryfootprintisnotanissuefortheevaluation . Note   thatsinceagivenwordtypeiscontainedinmultiple   sentences on both the source and target language   sides , thusit has multipleCCWRs , Equation 1im-   plicitlyinvolvesretrievingtheparallelsentencepair   of the source sentence and the aligned word pair   of the source word just by measuring CSLS sim-   ilarity of CCWRs . Accuracy is measured by the   ratio of correctly retrieved aligned word pairs . We   measureaccuracyinbothsourcetotargetandtarget   to source directions and report their average .   In our experiments we consider English as the   target language and the already mentioned low-   resource languages as the source and use var-   ious parallel corpora including the corpora of   Bangla - NMT ( Hasan et al . , 2020 ) , OpenSubtitles   ( Lison and Tiedemann , 2016 ) , the Samanantar   ( Ramesh et al . , 2021 ) and GoURMET ( Sánchez-   Martínez et al . , 2020 ) projects , as well as the Bible   ( Christodouloupoulos and Steedman , 2015 ) . Moredetails are shown in Table 1 . We use the first 1024   sentence pairs as test . We reserved the next 1024   asdevelopment(recallthatweonlyusedtheNepali   developmentdataasdiscussedabove ) . Weusedthe   restonlyfortrainingthesupervisedbaselinemodels   ( Caoetal . ,2020 ) . Wetokenizedthedatasetswith   the tools discussed in Section 4.1 , performed word   alignment using fastAlign ( Dyer et al . , 2013 ) and   kept only the one - to - one pairs in the intersection ,   in order to obtain a high quality test set .   Named entity recognition To test the useful-   nessof ourapproachon downstreamtasks aswell   we perform zero - shot cross - lingual NER , since it   was shown to reflect the cross - lingual quality of   LMs well ( Wang et al . , 2020 ) . We use the multi-   lingual WikiANN dataset which supports a large   setoflanguages(Rahimietal . ,2019 ) . Wekeepour   sequence tagger simple so that the quality of token   representations is the mostinfluential factor in the   final results . Weonly apply adropout ( probability   0.1 ) and a single linear layer as the classification   head . Additionally , we freeze all model parame-   ters except the final classifier layer during training .   We train our models on English with batch size 32 ,   learningrate 5×10,warm - upsteps 1K , using   early stopping on the development set of the target   language . We report Fscores as our final results .   Compared systems We compare our ap-   proach to the off - the - shelf mBERTmodel ( De-   vlin et al . , 2019 ) and additionally to the vocabu-   lary extended eBERT(Wang et al . , 2020 ) model   for languages not written using Latin or Cyril-   lic scripts . Additionally we evaluated the su-   pervised model of Cao et al . ( 2020 ) which fine-   tunes the whole model using the training por-   tionoftheparallelcorpora ( mBERT_full_sup and   eBERT_full_sup ) . To test the effectiveness of using   just a linear layer for language alignment instead   of full model update in the supervised setup we   runexperimentswithourproposedarchitecturebut   with parallel data as training instead of running   the unsupervised mining step ( mBERT_linear_sup   andeBERT_linear_sup ) . Finally , we refer   to our systems as mBERT_linear_unsup and   eBERT_linear_unsup . AsmentionedinSection3.2   mBERT_full_unsup andeBERT_full_unsup ( full   model update with mined training data ) did not   converge , thus we omit it from our final results .   Due to environmental considerations we report the   results of a single run for each setup .   On top of pure BERT - based models in case of11997   the NER task , weexperiment with aligning BERT   representation using VecMap(Artetxe et al . , 2018 )   similarly to Schuster et al . ( 2019 ) and Liu et al .   ( 2019 ) . Moreprecisely , i)webuildtype - levelrep-   resentations ( anchors ) for each of the most fre-   quent 50Kwords of a given language . We ran-   domly sample 100sentencescontaining a given   word ( w ) , build its CCWRs ( E ) and take their   dimension - wise average as the type - level repre-   sentation . Then ii ) we train an orthogonal align-   mentofthesourceandtargetlanguagetype - level   embedding spaces with VecMap using iterative-   refinement . We use two types of training sig-   nals : identical word pairs ( mBERT_vecmap_id   andeBERT_vecmap_id ) which similarly to our   approach does not need explicit cross - lingual   resources ; and word translation pairs from   the MUSE project ( Conneau et al . , 2018a ) as   the supervised setup ( mBERT_vecmap_sup and   eBERT_vecmap_sup ) . Finally , iii ) we initialize the   weightsofthelinearlayerontopofBERTwiththe   learned alignment before NERtraining to transfer   it to the downstream task .   5 Results   5.1 Contextual Word Retrieval   Weshowaccuracyresultsofthewordretrievaltask   on the low - resource languages in Table 2 . On a   high level it can be seen that both baseline models ,   mBERTandeBERT , were improved by all of the   used methods for each of the languages .   Additionally , LMs updated with our unsuper-   vised mining method ( mBERT_linear_unsup andeBERT_linear_unsup ) show large improvements   comparedtothebaselinesalthoughnoparalleldata   wasused . Thisshowsthatusefulwordtranslation   pairscanbeminedautomaticallyrelyingontheini-   tialcross - lingualqualityofmultilingualLMsand   that parallel data is not necessary . We show mined   examples in Section 5.3 .   We built vocabulary extended mBERTmodels   ( eBERT)forlanguagesthatusedascriptotherthan   LatinorCyrillic . eBERTiseffectiveonthelower   resourced languages ( Gu and Ne ) . Still our min-   ing approach improves over eBERTachieving best   scores onGu and Newhile mBERT_linear_unsup   achieves best scores on the others .   Thesupervisedapproaches , whichcanbeconsid-   ered as oracle systems since they assume the avail-   abilityofgoodqualityparalleldata , achievelarge   improvements over our unsupervised approach .   This is not surprising , since these approaches do   not rely on the initial cross - lingual quality of the   usedLMwhichislowfortheconsideredlanguages .   Additionally , the aligned word pairs in the used   parallel data cover a larger portion of the given lan-   guage’svocabularywhichmeansmoreinformation   forthetrainingprocess . Incontrast , theminingpro-   cesscoverslessuniquewordpairs(seeSection5.3 ) .   Among the two variations the fully fine - tuned   model ( mBERT_full_sup andeBERT_full_sup )   achieves best performance , while the supervised   model with linear layer ( mBERT_linear_sup and   eBERT_linear_sup ) liesbetweenourproposedun-   supervised models and the supervised fully fine-   tuned models . As mentioned before , the informa-   tion density of parallel sentences is much higher   thanthatoftheminedpairs , sincemostofthewords   are alignedin case ofthe former , while inmost of   thecasesonlyonewordpairiscontainedpersen-   tence pair in case of the latter . This shows that11998   paralleldatacanbeexploitedbetterbyupdatingthe   fullLM , whilethesimplerlinearlayerandfrozen   LM parameters are needed to prevent over - fitting   in case of the mined training data .   5.2 Named Entity Recognition   Weshow Fscoresofourzero - shotcross - lingual   experiments in Table 3 . Similarly to the contex-   tualized word retrieval task the best scores were   achieved by fine - tuning the baseline models . Fol-   lowing the trend in the contextual word retrieval   results , eBERTiseffectiveincaseofthelowerre-   sourced languages ( Gu and Ne ) and the supervised   methodsusingastrongcross - lingualsignalinthe   form of a word aligned parallel data achieve best   results . However , thelatterisonlyapplicableifpar-   allel data exists for the low - resource language . Our   miningbasedapproachimprovedonthebaselines   on all languages except Afrikaans .   Similarly to Cao et al . ( 2020 ) , we found the   mapping based approach ( VecMap ) to be ineffec-   tive.mBERT_vecmap_id which uses no explicit   cross - lingual training signal to learn the align-   ment , only identical word pairs , achieves lower   performance than mBERTespecially in case of   thelower - resourcespectrum(Kn , GuandNe ) . In   contrast , eBERT_vecmap_id is competitive with   eBERTand even outperforms it on Kannada and   Nepali . This shows that extending the vocab-   ulary of mBERTis an important step to make   VecMap based on identical pairs effective even   on the higher - resource languages . On the other   hand , our unsupervised mining approach achieves   betterscoresthanVecMapwiththeexceptionsof   Bengali and Kannada where eBERT_vecmap_idperforms better than eBERT_linear_unsup , how-   evermBERT_linear_unsup isthe mosteffectivein   these cases . Finally , it can be seen that the super-   vised variations of the VecMap based approach   ( mBERT_vecmap_sup andeBERT_vecmap_sup )   are not able to benefit further from the stronger   cross - lingual training signal . This is in line with   the findings of previous work which show identi-   calpairsoftentobecompetitivewithdictionaries   ( Artetxe et al . , 2017 ; Søgaard et al . , 2018 ; Severini   et al . , 2022 ) .   We present our preliminary results on unseen   languages , i.e. ,languages which werenot used for   LM pre - training , in Appendix A.   5.3 Analysis   Mined word pairs In Table 4weshow Englishto   Macedonianminedwordpairexamples . Itcanbe   seenthatthesentencepairsselectedbyourmethod   areindeednotparallelbuttheircontextsaresimilar .   In example 1 both sentences mention Eastern Eu-   ropeanarmies , example2discussescontroversial   events , while example 3 mentions various English   and German well - known individuals . The simi-   lar context of the sentences helps to build similar   CCWRs for the selected word pairs . As we men-   tioned before , most of the mined sentence pairs   containonlyonealignedwordpairasinexample1 ,   howeverthereareafewsentencepairs(about 25 % )   containing multiple word pairs , such as example 2 .   Finally , example 3 shows a mined word pair ( iden-   tity – име(name))whichisnotacorrecttranslation ,   howeverthewordsasusedhavesimilarmeanings .   Suchexamplesindicatethatourapproachisableto   leverage word pairs with similar meanings as well.11999   As discussed in Section 4.1 the complete train-   ing process extracts 60Ksentence pairs due to the   fixedbatchsize , gradientaccumulationnumberand   model update steps . The numbers of unique mined   word types however are relatively small . For En-   glishitvariesbetween 11Kand15K , whileforthe   low - resourcelanguagesitvariesbetween 8Kand   14K. Covered word types mainly involve frequent   words of the given language ’s vocabulary . When   a given language has less monolingual resources   the number of words having good quality vector   representation decreases as well due to the lower   frequency of these words ( making the pairing of   the word difficult ) . However , the number of mined   word types is still larger than the frequently used   5Kpairsformappingapproaches(Conneauetal . ,   2018a ) . Weshowtheexactnumberofuniquewords   per languagemined byour methodin Table 9,Ap-   pendix B.   Quality vs. Quantity Wecomparedtwovari-   ationsofourwordpairminingmethodinTable5 .   Wecallourmethodwhichwasdiscussedpreviously   forward - backward , since it mines word pairs in the   source to target and target to source language di-   rections using the method discussed in Section 3.1 .   Wesimplytaketheunionoftheoutputofthetwo   directions as the final set of mined word pairs . Incontrast , in order to increase the quality of word   pairs at the expense of quantity we take mutually   aligned word pairs in the two directions . We call   this approach the intersection mining method .   TheresultsinTable5showsthatthelargerbut   less precise set of mined word pairs resulted by the   forward - backward methodoutperformsthesmaller   but more precise set of intersection in 7 out of 9   cases on the named entity recognition task . This   showsthatthealignmentmethodisrobustagainst   incorrectlyalignedpairsandcansuccessfullylever-   age pairs that are not direct translations but are   nevertheless similar in meaning . This also leads to   a larger number of mined unique word types .   6 Conclusions   Low - resource languages are underrepresented in   pre - trained multilingual LMs . In contrast to pre-   viousworkusingaparalleldatasettoimprovethe   language support of low - resource languages , we   presented an unsupervised method to align lan-   guage pairs by relying only on monolingual cor-   pora . We showed that word translation pairs can   be extracted from non - parallel sentence pairs by   leveraging the cross - lingual contextualized repre-   sentations of words which in turn can be used to   align the vector spaces of languages . We tested our12000approach on the intrinsic contextual word retrieval   andthedownstreamnamedentityrecognitiontasks .   Our results showed improved cross - lingual quality   ofthefine - tunedLMs . Ouranalysisrevealedthat   the quantity of mined word pairs matters over their   qualityandthatthevocabularyextensionmethodis   importantforperformanceboostincaseofthelow-   estresourcelanguages . Asfutureworkweaimat   leveragingeasilyaccessiblecross - lingualresources   for better unseen language support .   Limitations   Our preliminary experiments on unseen languages   in Appendix A show that our approach improves   the baselines for these languages as well but the   achieved performance is still low . The main reason   for this is the initial low quality of mBERT and   eBERT , thustheminingusingpoorCCWEsisinef-   fective . The supervised experiments using parallel   sentencesshowthatwithalargerquantityandmore   precisely aligned word pairs further improvements   can be achieved . However , since such resources   are often unavailable for low - resource languages   furthermethodsrelyingonmoreeasilyaccessible   cross - lingual resources should be considered in fu-   ture work .   Acknowledgements   Wethanktheanonymousreviewersfortheirhelpful   feedback . The work was supported by the Euro-   pean Research Council ( ERC ) under the European   Union ’s Horizon 2020 research and innovation pro-   gramme(No.640550)andbytheGermanResearch   Foundation ( DFG ; grant FR 2829/4 - 1 ) .   References120011200212003   A Unseen languages   We ran initial experiments on unseen languages   ( Sindhi , Faroese , Upper Sorbian and Maori ) that   were not used for pre - training mBERT . Other than   the mentioned parallel datasets in Table 1 we used   Tanzil ( Tiedemann , 2012 ) for Sindhi . There are no   available parallelcorpora aligned with Englishfor   FaroeseandUpperSorbian . Datasetstatisticscan   beseeninTable6 . Wefollowthesamedataprepro-   cessing(weuse IndicNLP forSindhiand Mosesfor   the others ) and use the same hyper - parameters as   for the seen languages .   The results in Table 7 show that mBERTper-   forms below 1%accuracy on the two unseen lan-   guages in terms of word retrieval which is im-   proved by our mining approach . Vocabulary ex-   tension is an important step as eBERTperforms   better than mBERT_linear_unsup on Sindhi . The   updatedeBERT_linear_unsup modelachievesfur-   therimprovementscomparedto eBERT.However ,   theseresultsarestilllowandfurtherimprovements   are needed . The Fscores on the named entity   recognition task shown in Table 8 are inconsistent .   OurapproachachievesimprovementsonSindhiand   Faroese but not on Upper Sorbian and Maori , al-   though it achieves improvements on all unseen lan-   guagesintermsofwordretrievalaccuracy(wedo   nothaveFaroeseandUpperSorbianretrievalresults   due to the lack of parallel data with English ) . Simi-   larlyas incase ofthe seenlanguages , theVecMap   based approach performs lower than the baseline   whennovocabularyextensionisperformed . Incon-   trast , eBERT_vecmap_id outperforms eBERTbut   noteBERT_linear_unsup .   Thesupervisedmethodsusingparallelcorpora   for training show that with good quality word pairs   even unseen languages can be improved signifi-   cantly , indicating that in case of low initial LM   qualityonagivenlanguageastrongercross - lingual   signal is needed for meaningful model improve-   ment . Ontheotherhand , paralleldatasetsarenot   available for many low - resource languages .   Finally , Table 9 shows that the number of mined   unique words for the unseen languages are lower   than the numbers for seen languages , ranging be-   tween 5Kand10K. Due to smaller monolingual   corpora word types are less frequent and there is   stronger oversplitting by the subword tokenizer ,   leading to fewer words with high quality CCWRs .   B Mined Words   Weshowthenumberofuniquewordtypesthatwere   mined by our approach for both seen and unseen   languages in Table 9 .   C Size of Mining Candidates   Inourmainexperimentsweusecosinesimilarity   forwordpairminingandrandomlysample 1Ksen-   tencesasthesourceandtargetdatasets ( ˜Dand˜D )   in each iteration . We experimented further with   largersamplingsizesandamoreefficientCCWR   similaritymethodontopofcosinewhich , although   quadratic in runtime and memory requirements ,   can be efficiently performed on GPUs using batch-   ing . Theruntime ofour cosine - basedapproach on12004   Nepali ranges between only 0.5(|˜D|= 1 K ) and   15(|˜D|= 100 K ) hours . Since the main goal of   using larger data samples is to increase the chance   of including the translation of a given source word   inthecandidateset , wekeepthesizeof ˜Dat1 K   ( sincewesampleanewsetineveryiteration)and   onlyvary thesizeof ˜D. Asthe alternativesearch   methodweuse Faiss , alibraryforefficientvector   similarity calculation ( Johnson et al . , 2019 ) . More   precisely , we use an inverted file index with 100   lists for faster search and product quantizer with   8bitsencodingformemoryefficiency . Therun-   timeoftheFaissbasedsystemrangesbetween 20   minutes and 1.5hours .   Table 10 shows our findings on Nepali with|˜D|cosine Faiss   17.16 6.50   57.15 6.45   107.10 6.49   506.99 6.46   1006.97 6.51   |D|Ne   57.19   107.27   507.21   1007.14   5007.18   1,0007.16   eBERT_linear_unsup . Itcanbeseenthatasthesize   ofcandidatesincreasestheperformanceslightlyde-   creasesincaseofcosineandstaysonthesamelevel   in case of Faiss . As expected , we found that the se-   tups with larger target candidate sizes mine more   pairsperiteration , thusourconjectureisthatitis   bettertoupdatethemodelwithafewexamplesin   theinitialtrainingstepswhichpositivelyinfluences   the mining quality in the later stages . Secondly ,   cosine similarity outperforms Faiss due to its exact   search in exchange for computational efficiency .   D Size of Monolingual Data   We also run simulated low - resource experiments   where we assume that the size of the overall mono-   lingual corpus ( D ) , from which we sample 1 K   sentencesineachiteration ( ˜D),islimited . Table11   shows the results on simulated Nepali contextual   cross - lingualwordretrieval . Recallthatinourmain   experiments we used 1,000Ksentences for both   DandD. It can be seen that there is a negligible   differencebetweenthedifferentdatasetsizes , show-   ingtherobustnessofourapproachagainstlackof12005data for mining . As discussed in Section 4.1 the   complete training process extracts 60Ksentence   pairsduetothefixedbatchsize , gradientaccumu-   lation number and model update steps , thus even   thelowestsetuphasenoughpossiblesentencepairs   to mine from ( 5K×1,000K).12006