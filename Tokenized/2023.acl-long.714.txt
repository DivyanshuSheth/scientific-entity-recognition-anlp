  Fangyu LiuFrancesco PiccinnoSyrine KricheneChenxi PangKenton Lee   Mandar JoshiYasemin AltunNigel CollierJulian Martin EisenschlosGoogle DeepMindUniversity of Cambridge   Abstract   Visual language data such as plots , charts ,   and infographics are ubiquitous in the hu-   man world . However , state - of - the - art vision-   language models do not perform well on   these data . We propose MC(Math   reasoning and Chart derendering pretraining )   to enhance visual language models ’ capabili-   ties in jointly modeling charts / plots and lan-   guage data . Speciﬁcally we propose several   pretraining tasks that cover plot deconstruc-   tion and numerical reasoning which are the   key capabilities in visual language modeling .   We perform the MCpretraining starting   from Pix2Struct , a recently proposed image-   to - text visual language model . On standard   benchmarks such as PlotQA and ChartQA , the   MCmodel outperforms state - of - the - art   methods by as much as nearly 20 % . We also   examine how well the MCpretraining   transfers to domains such as screenshots , text-   book diagrams , and document ﬁgures and ob-   serve overall improvement , verifying the use-   fulness of MCpretraining on broader vi-   sual language tasks .   1 Introduction   Visual language is the system that uses tightly inte-   grated textual and visual elements to convey mean-   ing ( Horn , 1998 ) . It is ubiquitous in the human   world with typical examples being charts , plots and   diagrams existing in places such as textbooks , sci-   entiﬁc papers web pages and many more . Visual   language is also highly complex – besides texts ,   its structural units can include line , shape , color ,   orientation , scale , angle , space , etc . One needs   to recognize patterns from these structural units ,   and perform spatial grouping and/or alignment to   extract information for reasoning . Whilst being prevalent and important , there is   little research on visual language understanding   from the machine learning community . Vision-   language models pretrained on natural images or   image - text pairs crawled from the web perform   badly on visual language tasks such as ChartQA   ( Masry et al . , 2022 ) and PlotQA ( Methani et al . ,   2020 ) due to the high complexity of jointly model-   ing language and symbols ( more evidence in exper-   iments ) . Pix2Struct ( Lee et al . , 2023 ) is a recently   proposed pretraining strategy for visually - situated   language that signiﬁcantly outperforms standard   vision - language models , and also a wide range of   OCR - based pipeline approaches . Pix2Struct de-   signs a novel masked webpage screenshot pars-   ing task and also a variable - resolution input repre-   sentation for pretraining an image - to - text encode-   decoder Transformer ( Vaswani et al . , 2017 ) . In this   work , we use Pix2Struct as the base model and   further pretrain it with chart derendering and math   reasoning tasks .   We argue that visual language understanding   needs two key ingredients : ( 1 ) layout understand-   ing ( including number extraction and their orga-   nizations ) and ( 2 ) mathematical reasoning . ( 1 ) is   required to discover the underlying patterns of the   image and organize the elements in the image in   a logical form . ( 2 ) is needed to operate on the el-   ements extracted from ( 1 ) and derive meaningful   information demanded by a task or query . Based   on these observations , we propose two complemen-   tary pretraining tasks for enhancing visual language   understanding : chart derendering andmath rea-   soning . In chart derendering , given a plot / chart ,   the image - to - text model is required to generate its   underlying data table or the code used to render   it . The second task is math reasoning pretraining .   We pick two numerical reasoning dataset MATH   ( Saxton et al . , 2019 ) and DROP ( Dua et al . , 2019 ) ,   render the input into images and the image - to - text   model needs to decode the answers.12756   We use a suite of visual language tasks to test the   effectiveness of our method . Most importantly , we   test on ChartQA and PlotQA which are QA datasets   about plots and charts . On both datasets , MC   surpasses even the SOTA model assuming access   to charts ’ underlying data tables and can beat the   prior SOTA without gold data table by as much as   20 % . We also test MCon chart - to - text sum-   marization tasks and observe clear improvements   over Pix2Struct and achieves SOTA on Chart - to-   Text ( Kantharaj et al . , 2022 ) Pew split . Last but   not least , to examine if the MCpretraining   generalizes to datasets beyond the standard plots   and charts domain , we also test MCon four   additional domains where Pix2Struct was evalu-   ated on : documents , illustrations , user interfaces ,   and natural images ( including datasets , such as   textbook QA , Widget Captioning , etc . ) . We demon-   strate consistent improvement on most additional   datasets compared with the base model Pix2Struct .   To summarize , our contributions are : ( 1 ) propos-   ing a set of effective pretraining tasks for visual   language learning ( 2 ) demonstrating consistent im-   provements across all evaluated tasks and SOTA re-   sults on ChartQA , PlotQA , and Chart - to - Text sum - marization ( Statista set ) without accessing the gold   data tables ; ( 3 ) verify that MCpretraining   transfers to visual language benchmarks beyond   the chart & plot domains and achieve SOTA across   a wide range of datasets beyond the chart domain   such as textbook VQA and Widget Captioning ; ( 4 )   comprehensive ablation and analyses to understand   the effect of each pretraining component and its   impact to downstream performance .   2 Related Work   Vision - language research and a lack of atten-   tion on visual language . Research on vision-   and - language has predominately been focusing   on natural images . Visually - grounded reasoning   datasets such as NLVR2 ( Suhr et al . , 2019 ) and   MaRVL ( Liu et al . , 2021 ) are mostly in the natu-   ral image domain . Synthesized datasets such as   SHAPES ( Andreas et al . , 2016 ) , NLVR ( Suhr et al . ,   2017 ) , and CLEVR ( Johnson et al . , 2017 ) can be   seen as in the visual language domain . However ,   their visual language systems are signiﬁcantly sim-   pler than those in the real world such as plots and   charts . As a result , information extraction from   these synthesized datasets is straightforward . Be-12757sides , the queries in the synthesized datasets are   relatively naive and do not require complex rea-   soning ( e.g. , questions can usually be on spatial   relations or counting objects ) . Consequently , cur-   rent vision - language models can handle the above   mentioned synthesized visual reasoning datasets   quite well . However , they do not perform well   on real - world visual language datasets where both   the information extraction and reasoning becomes   much more complex ( we will show this in § 4 ) .   OCR - based & end - to - end methods for visually-   situated language . LayoutLM ( Xu et al . , 2020 ;   Huang et al . , 2022 ) leverages a patch - OCR align-   ment loss to inject an external OCR systems ’   knowledge into the Transformer model . PresSTU   ( Kil et al . , 2022 ) and PaLI ( Chen et al . , 2023 ) also   design OCR - aware pretraining objectives where   the model needs to predict texts obtained from off-   the - shelf OCR systems . ChartBERT ( Akhtar et al . ,   2023 ) relies on OCR text and positions to train   a transformer encoder . While OCR systems can   be helpful for accurately extracting texts , running   them is not cheap . Also , OCR systems do not cover   visual language systems that do not explicitly use   text . As examples , plots and charts do not always   have numbers written explicitly . In our concurrent   work DP ( Liu et al . , 2023 ) , we explore com-   bining a chart - to - text translation module ( without   OCR ) with large language models .   Donut ( Kim et al . , 2022 ) , Dessurt ( Davis et al . ,   2023 ) , and Pix2Struct ( Lee et al . , 2023 ) are end - to-   end pretrained models for visual language where   Donut and Dessurt focus on document understand-   ing and Pix2Struct aim to provide a generic pre-   trained checkpoint for all visual language tasks .   MC ’s architecture is identical to Pix2Struct   – we continually pretrain a Pix2Struct checkpoint   with new objectives .   Learning to reason by designing novel pretrain-   ing tasks . MCis related to the literature   of designing better pretraining objectives to help   the language models ( LMs ) to reason better since   the skill is hard to require through naive languagemodeling objectives only ( e.g , masked language   modeling and autoregressive language modeling   on raw text ) . Geva et al . ( 2020 ) ; Eisenschlos et al .   ( 2020 ) generate additional pretraining data focused   on ( numerical ) reasoning through human - written   templates . Pi et al . ( 2022 ) synthesize data and pro-   grams , and then use program executors to simulate   answers . LMs are pretrained to predict the answers   given data and programs . Wu et al . ( 2022 ) explore a   wide range of synthetic pretraining tasks and found   that even just injecting knowledge as simple as in-   duction and deduction rules could teach LMs to   reason . We teach an image - to - text model to reason   through mapping charts to data and code , and also   directly learning textual math reasoning datasets .   3 Method   We argue that layout understanding and basic math   operation capabilities are the key elements for per-   forming visual language understanding / reasoning .   We inject such capabilities to the model by propos-   ing two pretraining tasks : chart derendering   ( § 3.1 ) and math reasoning ( § 3.2 ) which we in-   troduce in detail in the following sections .   3.1 Chart Derendering   Plots and charts are usually generated by an under-   lying data table and a piece of code . Code decides   the overall layout of the ﬁgure ( e.g. , type , direction ,   color / shape scheme of the chart ) and the underly-   ing data table decides the actual numbers and the   groupings of them . Both the data and code are   sent to a compiler / rendering engine to create the   ﬁnal image . To understand a chart one needs to dis-   cover the visual patterns in the image , effectively   parse and group them to extract the key informa-   tion . Reversing the plot rendering process demands   all such capabilities and can thus serve as a perfect   pretraining task .   In practice , it is challenging to simultaneously   obtain charts , their underlying data tables , and their   rendering code . To collect sufﬁcient pretraining   data , we independently accumulate ( chart , code )   and ( chart , table ) pairs . For ( chart , code ) , we crawl   all GitHub IPython notebooks with appropriate li-   censes and extract blocks with ﬁgures . A ﬁgure and   the code block right before it are saved as a ( chart ,   code ) pair . For ( chart , table ) pairs , we explored12758two sources . First is to manually write code for con-   verting web - crawled Wikipedia tables from Herzig   et al . ( 2020 ) to charts . We randomly combine sev-   eral plotting options . The key random variables   include : using either matplotlib orseaborn as   the plotting package ; using either bar , line , or pie   chart ; styles and colors of the charts ; whether to   show numbers explicitly on the graph ; font and   size of the texts . Besides our own synthetic data ,   we also add chart - table pairs generated by Methani   et al . ( 2020 ) ( from PlotQA ) to diversify the pre-   training corpus . The second source is web - crawled   chart - table pairs . Websites such as Statista provides   both . We directly use the chart - table pairs crawled   by Masry et al . ( 2022 ) ( from ChartQA ) , contain-   ing around 20k pairs in total from four websites :   Statista , Pew , Our World in Data , and OECD .   Note that to avoid leaking test information for   the PlotQA and ChartQA tasks which use the same   chart data as pretraining , we only use the chart-   table pairs in the training sets for pretraining and   test tables / charts are strictly excluded . In ablation   study ( § 5.1 ) , we will show that chart - table from   both sources are useful and having a diverse set of   chart - table pairs is always better . However , using   only our synthetic data brings very signiﬁcant im-   provement already , suggesting that the concept of   chart derendering can be easily transferred to charts   of other domains ( including real - world charts ) .   3.2 Math Reasoning   Reasoning over visual language requires ( 1 ) ef-   fective recognition and grouping of the visual el-   ements and also ( 2 ) applying mathematical oper-   ations ( such as sorting , min / max , averaging , etc . )   on top of them . Plot derendering addresses ( 1 ) but   ( 2 ) is still lacking in the current pretraining frame-   work . As a result , we propose to explicitly inject   numerical reasoning knowledge to the image - to-   text model by learning from textual math datasets .   We use two existing textual math reasoning   datasets , MATH ( Saxton et al . , 2019 ) and DROP   ( Dua et al . , 2019 ) for pretraining . MATH is syn-   thetically created , containing two million train-   ing examples per module ( type ) of questions ( see   Appx . § A for a comprehensive listing of modules   included in MCpretraining ) . DROP is a   reading - comprehension - style QA dataset where the   input is a paragraph context and a question . DROPhas 96k question and answer pairs over 6.7 K para-   graphs . To solve questions in DROP , the model   needs to read the paragraph , extract relevant num-   bers and perform numerical computation to predict   the answer . We found both datasets to be comple-   mentarily helpful . MATH contains large amounts   of questions and is categorized which helps us iden-   tify math operations needed to explicitly inject to   the model . DROP ’s reading - comprehension format   resembles the typical QA format where models   need to simultaneously perform information extrac-   tion and reasoning . In practice , we render inputs of   both datasets into images ( concatenating the con-   text and question for DROP ) . The image - to - text   model is trained to decode the answer given the   redered image . Examples of MATH and DROP can   be found in Figure 1 ( in light red ) .   Besides the two newly proposed pretraining   strategies , to prevent catastrophic forgetting , we   also keep applying the screenshot parsing pretrain-   ing from Pix2Struct ( Lee et al . , 2023 ) . Speciﬁcally ,   given screenshot of a website ( where parts of the   website is masked ) , the image - to - text transformer   needs to predict the underlying simpliﬁed HTML   code that could render the original unmasked web-   site screenshot . The ﬁnal pretraining task is a mix-   ture of all aforementioned tasks . We discuss the   mixture weights in § 4.1 .   4 Experiment   We detail our experimental setup in § 4.1 , introduce   the main results in § 4.2 , and results on additional   Pix2Struct tasks in § 4.3 .   4.1 Experimental Setups   Pretraining datasets / tasks . Overall , we create   a mixture of pretraining task that has 40 % of   math reasoning , 40 % of chart derendering , and   20 % screenshot parsing . The weight for speciﬁc   task / dataset is listed in Table 1 . For chart derender-   ing , we have four sources of data : ( 1 ) chart - table   pairs synthesized by ourselves , ( 2 ) from ChartQA ,   ( 3 ) synthesized in PlotQA , and ( 4 ) chart - to - code   data . We initially assigned equal weight to the   four tasks however noticed training instability since   chart - to - code is very hard ( the pretraining data is   noisy ) . We thus lower chart - to - code to 4 % and   increase all chart - to - table tasks to 12 % . For math   reasoning , we assign equal weights to MATH and12759   DROP ( both are 20 % ) .   For pretraining dataset ablation studies , see § 5.1 .   Evaluation datasets . We evaluate MCon   multimodal English QA and generation tasks in-   cluding ChartQA ( Masry et al . , 2022 ) , PlotQA   ( Methani et al . , 2020),and Chart - to - Text sum-   marization ( Kantharaj et al . , 2022 ) . Both ChartQA   and PlotQA are chart domain QA datasets where   the input is an image of a chart and a query and   the target is an answer string . ChartQA has two   subsets : ( 1 ) augmented and ( 2 ) human where the   augmented set is machine generated and thus more   extractive and the human set is human written and   requires more complex reasoning . PlotQA also has   two sets v1 and v2 . Similarly , v1 focuses more   on extractive questions and v2 requires more nu-   merical reasoning . However , both v1 and v2 are   machine generated . Chart - to - Text has two sets as   well . They are “ Pew ” and “ Statista ” where the   names describe the source of the image examples .   For Pew , the gold summaries are automatically ex-   tracted from areas around the image . For Statista ,   the summaries are human written . The sizes of   each dataset are described in Table 2 .   Beyond chart domain datasets , we additionally   evaluate on other datasets used in Pix2Struct ( Lee   et al . , 2023 ) . We follow the exact same setups and   protocols of Pix2Struct by rerunning Pix2Structexperiments but replacing the initial checkpoint   with MC . See Lee et al . ( 2023 ) for more   experimental details .   Metrics . For ChartQA and PlotQA , following   previous works ( Masry et al . , 2022 ; Methani et al . ,   2020 ; Lee et al . , 2023 ) , we use relaxed correct-   ness ( exact match but tolerating 5 % of numerical   error ) . For Chart - to - Text , we use BLEU4 . For all   Pix2Struct experiments , we use identical metrics   introduced in Lee et al . ( 2023 ) .   Training and inference details . We save check-   point every 200 steps and keep the checkpoint   that produces the highest validation score . Fol-   lowing Lee et al . ( 2023 ) , we ﬁnetune models on the   ChartQA aug . and human sets together ( i.e. , one   checkpoint for two sets ) and use the checkpoint   selected on human val set as the ﬁnal checkpoint   for testing . For PlotQA and Chart - to - Text , we train   standalone models for v1 , v2 , Pew , and Statista   sets . For pretraining , we use a batch size of 512   and max sequence length of 192 . We pretrain for   100k steps and the ﬁnal MCcheckpoint is   selected at the 90k step ( where the average exact   match validation score is the highest ) . For down-   stream tasks ﬁnetuning , we use a batch size of 256   and max sequence length of 128 . For ChartQA   and Chart - to - Text we ﬁnetune for 10k steps and   for PlotQA we ﬁnetune for 20k steps ( since it is   signiﬁcantly larger ) . Setups for Pix2Struct tasks   are the same as the original paper . As for the PaLI   baselines , we use the larger 17B variant and ﬁne-   tune for 5k steps and save checkpoints every 1000   steps . All MCand Pix2Struct models are pre-   trained/ﬁnetuned with 64 GCP - TPUv3 while PaLI   models are ﬁnetuned with 128 GCP - TPUv4 .   Note that since MCis an image - to - text   model ( without a textual input branch ) , whenever   it is required to input text to the model , the text   is rendered as an image . As an example , for QA   tasks , we prepend the question as a header above   the chart and input the image with question header   as a whole to the model .   4.2 Main Results   We summarize the main results in Table 3 where   we compare MCwith a wide range of base-   lines and SOTA modelsacross three chart / plot-   domain benchmarks ChartQA , PlotQA , and Chart-   to - Text Summarization . On ChartQA , MC12760   beats the previous SOTA ( without access to the   underlying gold data table ) Pix2Struct by 8.2 % .   Even if we consider models that do assume the   existence of gold data tables , they generally un-   derperform MCby 3 - 5 % . The best perform-   ing baseline VisionTaPas has a specialized module   for modeling tables but still lags behind MC   by 2.4 % . On PlotQA , MCis again the best   performing model overall . On the v1 set , VL - T5   with access to underlying data table performs better   than MCby≈4%which is intuitive since   PlotQA is a synthetic dataset thus containing rel-   ative simple queries and the v1 is the extractive   set where queries are even more straightforward .   On v2 where questions are related to numerical   reasoning , MCoutperforms all models in-   cluding the models with access to underlying gold   tables . On Chart - to - Text summarization , MC   improves upon Pix2Struct on both Pew and Staista   and is the new SOTA on Pew . However , MC   underperforms PaLI-17B ( res . 588 ) on Statista .   Overall , MCis clearly the best - performing   model with SOTA or competitive performance on   every setup and all tasks . All baselines without   access to gold tables lag behind signiﬁcantly –   MCoutperforms the strongest baseline with-   out gold table access Pix2Struct by ≈10 % if we   average the performance scores across all datasets .   Among the baselines , we would like to highlight   PaLI which is the SOTA for a series of multimodal   text - image tasks such as VQA and captioning on   natural images and is of a much larger size ( i.e. ,   17B parameters vs. 300 M in MC ) . PaLI   fails signiﬁcantly on ChartQA and PlotQA since   the challenge in the visual language is distinctlydifferent from that in the natural image domain .   Increasing input resolution substantially helps the   model ’s performance ( likely due to the better text   reading with higher resolution ) but this also in-   creases the sequence length ( thus also memory and   compute ) quadratically . PaLI performs reasonably   well in Chart - to - Text . We believe this is because   the Chart - to - Text task ( evaluated by BLEU4 ) might   be more sensitive to textual ﬂuency but less sensi-   tive to factuality as compared with the other two   QA tasks . It is expected that PaLI trained with a   language modeling objective on natural text will   have more advantage under this evaluation setup .   4.3 Results on Pix2Struct Tasks   Besides chart / plot domain datasets , we would also   like to examine if MCtransfers to other vi-   sual language datasets such as documents , user   interfaces , and natural images . We rerun all   Pix2Struct ﬁnetuning experiments with a MC   checkpoint and the results are shown in Table 4 .   On average across all tasks , MCoutper-   forms Pix2Struct by 2.3 % . Besides ChartQA ,   the improvement is also observed in AI2D ( QA   on textbook diagrams ) , Widget Captioning ( rec-   ognizing and captioning widgets in screenshots ) ,   DocVQA ( QA on scanned documents ) , etc . Even   if we exlucde ChartQA , MCcan outperform   Pix2Struct by 1.6 % on average , suggesting that   knowledge learned through MCpretraining   can be transferred to visual language domains out   side of plots / charts.12761   5 Analyses and Discussions   In this section , we ﬁrst conduct pretraining abla-   tions in § 5.1 to understand the usefulness of each   pretraining component , then in § 5.2 we conduct   ﬁne - grained analysis and error analysis to probe   MC ’ strengths and weaknesses .   5.1 Ablation Study   We conduct two types of ablations . First , we re-   move a whole type of pretraining datasets . For   example , ‘ no math reasoning ’ means removing   the whole math reasoning component and drops   the MATH and DROP datasets . The weights of   other datasets in the mixture are proportionally in-   creased . Second , we remove an individual dataset   within a component . For example , ‘ no MATH   dataset ’ means removing just MATH dataset but   keep other datasets in the math reasoning com-   ponent untouched . In this scenario , we increase   the weight of other math datasets ( in this case   just DROP ) proportionally to maintain the over-   all weight of the component in the mixture . To   reduce compute used , we train one full MC   model and all its ablated models with 50k steps ( the   original full MCis trained for 100k steps ) .   As a result the MCmodel performance in   Table 5 is slightly lower than the 100k model ( 63.0   vs. 64.2 ) . The pretrained models are then ﬁnetunedand evaluated on ChartQA only . The full ablation   study table is shown in Table 5 where the ﬁrst half   is component - level ablations and the second half is   individual dataset ablation .   The impact of each pretraining component .   On the component - level , we found that removing   any major component ( math reasoning , chart deren-   dering , and screenshot parsing ) would cause a per-   formance drop . The most important component is   chart derendering , the removal of which causes a   decrease of≈4%averaging across the two sets .   Removing math reasoning decreases the avg . score   by 2.4 % and removing the continual pretraining   of screenshot parsing causes a drop of 1.6 % . We   notice that math reasoning is more important to   the human set while chart derendering is more im-   portant on the augmented set . The ﬁndings are   likely due to the fact that the human set contains   more numerical reasoning questions while the aug-   mented set contains more extractive questions . We   also conducted ablations of speciﬁc datasets / tasks   which we discuss in paragraphs below .   MATH vs. DROP dataset for learning to rea-   soning . We have used two datasets , i.e. MATH   and DROP , for injecting numerical reasoning ca-   pabilities to MC . According to Table 5 , we   observe that DROP seems to be more important   ( the removal of which causes a performance drop   of 1.7 % vs. a drop of 0.5 % from the removal   of MATH ) . We conjecture that it is because the   reading - comprehension - QA format of DROP is   more similar to the downstream task of QA on   visual language , where information extraction and   reasoning needs to be jointly performed .   Synthetic vs. real - world corpus as pretraining   chart - table pairs . We perform another ablation   to justify the choice of chart derendering pretrain-   ing corpus . Real - world chart - table pairs can in-   crease the diversity and coverage of chart deren-   dering pretraining however we need to explicitly   scrape such data from the web . We are interested   in understanding to what extent our manually syn-12762thesized charts and plots with existing libraries can   improve model ’s performance . The row ‘ no real-   world chart - table pairs ’ shows results of only using   synthesized chart - table data by us ( i.e. , no ChartQA   and PlotQA chart - table data ) . The overall perfor-   mance drops by 2 % . Interestingly , for the aug-   mented set , the performance only drops 1.2 % but   almost 3 % is dropped on the human set . This indi-   cates that extractive questions can usually be solved   with synthetic pretraining but the more diverse real-   world data ( also usually having more sophisticated   layout ) can beneﬁt reasoning learning more .   The impact of chart - to - code pretraining .   While much of the information in a chart is   provided by data table , the code that is used to   render the table decides the visual layout ( e.g. ,   type of chart and orientation ) and attributes ( e.g. ,   color ) of the data . To test the importance of the   chart - to - code pretraining component , we remove   it in an ablated pretrained model and the model   performance on ChartQA drops by 1.1 % overall .   The drop is mainly on the human set where more   complex reasoning is required .   5.2 Fine - grained Analysis and Error Analysis   Fine - grained analysis . To understand the spe-   ciﬁc aspects of strengths and weaknesses of the   models and breakdown the challenges into ﬁne-   grained categories , we sample 100 test examples   from ChartQA ( both augmented and human sets )   for further analyses . Speciﬁcally , we summa-   rize the challenges of ChartQA into three cat-   egories : ( 1 ) data extraction ( where the model   needs to parse a complex query with sophisticated   coreference resolution or needs to read numbers   when numbers are not explicitly written out ) , ( 2 )   math reasoning ( where the model needs to per-   form one or multiple numerical operations such as   min / max / sort / average / etc . ) , and ( 3 ) plot attributes   ( where the query asks about color / shape / location of   speciﬁc objects / labels ) . We manually classify the   100 examples into the three categories and allow   an instance to belong to multiple categories when   the challenge is multifaceted . After excluding 7   annotation errors , we ﬁnd 55.9 % questions need   complex data extraction , 45.2 % involve math rea-   soning , and 7.5 % concern plot attributes . We plot   the per - category performance of PaLI ( res . 588 ) ,   Pix2Struct and MCin Figure 2 . Overall , all   models perform the best on data extraction while   math reasoning and plot attributes are more chal - lenging . When compared across models , MC   improves Pix2Struct in every category and beats   PaLI in both data extraction and math reasoning .   However , for plot attributes , MClags behind   PaLI . This is not signiﬁcantly reﬂected in the over-   all ChartQA performance since plot attribute only   concerns less than 10 % of the examples .   Error analysis . Similar to the ﬁne - grained anal-   ysis , we sample 100 errors made by MCon   ChartQA test set and manually classify the 100   errors into the three categories . After exluding   21 annotation errors , we ﬁnd 48.3 % of the er-   rors are related to math reasoning , 43.4 % are re-   lated to data extraction , and 8.0 % concern plot at-   tributes . We conclude that math reasoning remains   to be the major challenge even if MChas   improved its math reasoning capability compared   with Pix2Struct and PaLI . We notice that MC   still struggles with sophisticated math reasoning   questions or numerical computation that requires   high precision . An example is shown in Appendix   Table 8 .   Case study . To concretely understand what type   of questions MCcan do better than the base-   lines , we present several case studies . In Table 6 ,   we show an example which requires computing   average of multiple numbers . Besides MC ,   PaLI and Pix2Struct ’s answers are far from the   ground truth . In Table 7 , we demonstrate an exam-   ple that requires resolving complex coreference res-   olution of multiple data points . The model needs to   accurately parse the query and ﬁnd the referenced   data points in the chart , then perform a simple nu-   merical computation . MCis the only model   that gets the correct answer .   Besides cases where MCsucceeded , we12763   also present an example where all models have   failed ( Table 8) . Questions which require very ac-   curate numerical computation are still very chal-   lenging to MC .   Continue pretraining Pix2Struct with its origi-   nal objective . It is commonly known that BERT   ( Devlin et al . , 2019 ) is undertrained and simply   continuing training BERT with the same objective   and on the same data for longer can slightly im-   prove a model ’s performance ( Liu et al . , 2019 ) .   To understand whether such phenomenon persists   forMCand to what extent does continue   pretraining on Pix2Struct screenshot parsing task   would improve the model ’s performance , we con-   tinue pretraining Pix2Struct with its original objec-   tive and data for 50k steps . We found that continue   pretraining indeed improves Pix2Struct ’s perfor-   mance ( 56.0→57.0 on ChartQA ) but is to a much   less extent without using the MCpretraining   components ( improving from 56.0 to 64.2 ) .   6 Conclusion   We have proposed a pretraining method MC   for visual language tasks . MCinjects chart   understanding and reasoning knowledge to an   image - to - text transformer model by learning to   ( 1 ) predict the underlying data tables and code   given chart images and ( 2 ) decode the answers of   math questions ( rendered in the form of images ) .   MCestablishes new SOTA on 5 out of 6   setups across three chart domain benchmarks cov-   ering both QA and summarization tasks . On visual   language tasks beyond the chart domain ( e.g. , text-   book QA and DocVQA ) , MCimproves upon   Pix2Struct , indicating that the learned knowledge   inMCpretraining can be transferred outside   of the pretraining domain . We conduct comprehen-   sive ablation studies to identify the actual impact   of each pretraining component and task and ﬁnd   that chart derendering is essential for extractive   questions while math pretraining is important for   queries that requires complex reasoning.12764Limitations   Though we have injected math reasoning skills   toMC , error analysis shows that there is   still room for improvement on queries requiring   complex reasoning . Besides , it remains debatable   whether doing math calculation in weight space in   a purely end - to - end manner is the most promising   path forward .   Besides math reasoning , Figure 2 shows that   plot attributes is an area where MCunder-   performs PaLI . We conjecture that it is due to   MC ’s lack of massive scale grounded image-   text pretraining with rich semantics ( which PaLI   has using web - scale image - text pairs ) . While chart-   to - code pretraining provides certain level of plot   attribute grounding , such plot features are mostly   using default options in plotting packages but not   explicitly written out in code .   In terms of experimental setup , the reported num-   ber is result of a single run . Pretraining is ex-   tremely costly especially when there exists more   than twenty ablation setups and downstream eval-   uation tasks . We have collected pretraining and   evaluation data points from multiple aspects on var-   ious scenarios to verify the robustness of MC .   However , we do acknowledge that the paper can   beneﬁt from reporting multiple runs given sufﬁ-   cient compute .   Last but not least , it is also worth noting that   visual language is an umbrella term . There are   other visual language systems beyond the ones dis-   cussed in this paper . As an example , comics / manga   have their distinct visual lexicon or even grammars   ( Cohn , 2013 ) .   Ethics Statement   To the best of our knowledge , MChas not   been trained on sensitive private information and   should be of low risk to generate harmful contents .   All pretraining and ﬁnetuning data are either syn-   thetically created using rules or publicly available   data on the web with appropriate permissive li-   censes .   References1276512766   A More Details on Datasets Used   Chart - table pairs from the web . The data was   originally collected by Masry et al . ( 2022 ) and   came from the below four sources :   • Statista : www.statista.com   • Pew : www.pewresearch.org   • Our World in Data : ourworldindata.org   • OECD : www.oecd.org   Modules of MATH questions included . We ex-   clude overly complex math questions and only se-   lect the basic modules that would help with nu-   merical reasoning . They are from the two areas of   Arithmetic and Comparison . The individual mod-   ules included are   • Arithmetic   – add_or_sub   – add_sub_multiple   – div   – mixed   – mul   – mul_div_multiple   • Comparison   – closest   – closest_composed   – kth_biggest   – kth_biggest_composed   – pair   – pair_composed   – sort   – sort_composed   Please see Saxton et al . ( 2019 ) for detailed de-   scriptions about each module and how they are   generated .   B Details of Baselines   We introduce below the details of the baselines   used in Table 3 .   T5 is an encode - decoder Transformer model pro-   posed by Raffel et al . ( 2020 ) . The baseline model   T5 takes the concatenation of a linearized table   ( and a query , when the task is QA ) as input , and   aims to decode the target ( answer or summariza-   tion ) . When the gold table is availible , the gold   table is used as the input and the chart image is not   used directly . VL - T5 proposed by Cho et al . ( 2021 )   is similar to T5 but also takes a visual input ( i.e. ,   the chart image ) on the encoder side . VisionTaPas   ( Masry et al . , 2022 ) is modiﬁed from TaPas ( Herzig12767et al . , 2020 ) to incorporate the visual modality by   adding a ViT model ( Dosovitskiy et al . , 2021 ) and   cross - modal fusion layers . T5 - OCR , VL - T5 - OCR ,   and VisionTaPas - OCR are the same model as T5 ,   VL - T5 , and VisionTaPas , respectively . However ,   they do not assume the existence of gold table but   use an OCR - based system to extract the data ta-   ble from the chart image . The above mentioned   models and their performance numbers are all ex-   tracted from Masry et al . ( 2022 ) and Kantharaj et al .   ( 2022 ) . Please see the original paper for more de-   tails . Classiﬁcation - Regression Chart Transformer   ( CRCT ) ( Levy et al . , 2022 ) is the best performing   model on PlotQA according to the PlotQA bench-   mark on paperswithcode.com . It uses a detector   that extracts all textual and visual elements of chart   then processes these elements with a multimodal   Transformer.12768ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   After the conclusion section ( § 6 )   /squareA2 . Did you discuss any potential risks of your work ?   Ethics Statement   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   § 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   § 4   /squareB1 . Did you cite the creators of artifacts you used ?   § 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   § 4   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Ethics Statement   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Ethics Statement   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   § 4   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   § 4   C / squareDid you run computational experiments ?   § 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   § 412769 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   § 4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   § 4   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   § 4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   § 5   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   We annotated 100 examples for analysis within the authors ourselves .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   § 5 , We annotated 100 examples for analysis within the authors ourselves .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   We annotated 100 examples for analysis within the authors ourselves .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   We annotated 100 examples for analysis within the authors ourselves .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   We annotated 100 examples for analysis within the authors ourselves.12770