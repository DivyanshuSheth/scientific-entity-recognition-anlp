  Boxi Cao , Hongyu Lin , Xianpei Han , Fangchao Liu , Le SunChinese Information Processing LaboratoryState Key Laboratory of Computer Science   Institute of Software , Chinese Academy of Sciences , Beijing , ChinaUniversity of Chinese Academy of Sciences , Beijing , ChinaBeijing Academy of Artificial Intelligence , Beijing , China   { boxi2020,hongyu,xianpei,fangchao2017,sunle}@iscas.ac.cn   Abstract   Prompt - based probing has been widely used in   evaluating the abilities of pretrained language   models ( PLMs ) . Unfortunately , recent stud-   ies have discovered such an evaluation may be   inaccurate , inconsistent and unreliable . Fur-   thermore , the lack of understanding its inner   workings , combined with its wide applicability ,   has the potential to lead to unforeseen risks for   evaluating and applying PLMs in real - world   applications . To discover , understand and quan-   tify the risks , this paper investigates the prompt-   based probing from a causal view , highlights   three critical biases which could induce biased   results and conclusions , and proposes to con-   duct debiasing via causal intervention . This pa-   per provides valuable insights for the design of   unbiased datasets , better probing frameworks   and more reliable evaluations of pretrained lan-   guage models . Furthermore , our conclusions   also echo that we need to rethink the criteria for   identifying better pretrained language models .   1 Introduction   During the past few years , the great success of   pretrained language models ( PLMs ) ( Devlin et al . ,   2019 ; Liu et al . , 2019 ; Brown et al . , 2020 ; Raffel   et al . , 2020 ) raises extensive attention about eval-   uating what knowledge do PLMs actually entail .   One of the most popular approaches is prompt-   based probing ( Petroni et al . , 2019 ; Davison et al . ,   2019 ; Brown et al . , 2020 ; Schick and Schütze ,   2020 ; Ettinger , 2020 ; Sun et al . , 2021 ) , which   assesses whether PLMs are knowledgable for a   specific task by querying PLMs with task - specific   prompts . For example , to evaluate whether BERT   knows the birthplace of Michael Jordan , we could   query BERT with “ Michael Jordan was born in   [ MASK ] ” . Recent studies often construct prompt-   based probing datasets , and take PLMs ’ perfor - Figure 1 : The illustrated procedure for two kinds of   evaluation criteria .   mance on these datasets as their abilities for the   corresponding tasks . Such a probing evaluation   has been wildly used in many benchmarks such   as SuperGLUE ( Wang et al . , 2019 ; Brown et al . ,   2020 ) , LAMA ( Petroni et al . , 2019 ) , oLMpics ( Tal-   mor et al . , 2020 ) , LM diagnostics ( Ettinger , 2020 ) ,   CAT ( Zhou et al . , 2020 ) , X - FACTR ( Jiang et al . ,   2020a ) , BioLAMA ( Sung et al . , 2021 ) , etc .   Unfortunately , recent studies have found that   evaluating PLMs via prompt - based probing could   be inaccurate , inconsistent , and unreliable . For   example , Poerner et al . ( 2020 ) finds that the per-   formance may be overestimated because many in-   stances can be easily predicted by only relying   on surface form shortcuts . Elazar et al . ( 2021 )   shows that semantically equivalent prompts may   result in quite different predictions . Cao et al .   ( 2021 ) demonstrates that PLMs often generate un-   reliable predictions which are prompt - related but   not knowledge - related .   In these cases , the risks of blindly using prompt-   based probing to evaluate PLMs , without under-   standing its inherent vulnerabilities , are significant .   Such biased evaluations will make us overestimate   or underestimate the real capabilities of PLMs , mis-   lead our understanding of models , and result in5796wrong conclusions . Therefore , to reach a trustwor-   thy evaluation of PLMs , it is necessary to dive into   the probing criteria and understand the following   two critical questions : 1 ) What biases exist in cur-   rent evaluation criteria via prompt - based probing ?   2 ) Where do these biases come from ?   To this end , we compared PLM evaluation via   prompt - based probing with conventional evalua-   tion criteria in machine learning . Figure 1 shows   their divergences . Conventional evaluations aim   to evaluate different hypotheses ( e.g. , algorithms   or model structures ) for a specific task . The tested   hypotheses are raised independently of the train-   ing / test data generation . However , this indepen-   dence no longer sustains in prompt - based prob-   ing . There exist more complicated implicit con-   nections between pretrained models , probing data ,   and prompts , mainly due to the bundled pretraining   data with specific PLMs . These unaware connec-   tions serve as invisible hands that can even dom-   inate the evaluation criteria from both linguistic   and task aspects . From the linguistic aspect , be-   cause pretraining data , probing data and prompts   are all expressed in the form of natural language ,   there exist inevitable linguistic correlations which   can mislead evaluations . From the task aspect , the   pretraining data and the probing data are often sam-   pled from correlated distributions . Such invisible   task distributional correlations may significantly   bias the evaluation . For example , Wikipedia is a   widely used pretraining corpus , and many probing   data are also sampled from Wikipedia or its exten-   sions such as Yago , DBPedia or Wikidata ( Petroni   et al . , 2019 ; Jiang et al . , 2020a ; Sung et al . , 2021 ) .   As a result , such task distributional correlations   will inevitably confound evaluations via domain   overlapping , answer leakage , knowledge coverage ,   etc .   To theoretically identify how these correlations   lead to biases , we revisit the prompt - based prob-   ing from a causal view . Specifically , we describe   the evaluation procedure using a structural causal   model ( Pearl et al . , 2000 ) ( SCM ) , which is shown   in Figure 2a . Based on the SCM , we find that   the linguistic correlation and the task distributional   correlation correspond to three backdoor paths in   Figure 2b - d , which lead to three critical biases :   •Prompt Preference Bias , which mainly   stems from the underlying linguistic corre-   lations between PLMs and prompts , i.e. , the   performance may be biased by the fitness ofa prompt to PLMs ’ linguistic preference . For   instance , semantically equivalent prompts will   lead to different biased evaluation results .   •Instance Verbalization Bias , which mainly   stems from the underlying linguistic correla-   tions between PLMs and verbalized probing   datasets , i.e. , the evaluation results are sensi-   tive and inconsistent to the different verbaliza-   tions of the same instance ( e.g. , representing   the U.S.A. with the U.S. or America ) .   •Sample Disparity Bias , which mainly stems   from the invisible distributional correlation   between pretraining and probing data , i.e. ,   the performance difference between different   PLMs may due to the sample disparity of their   pretraining corpus , rather than their ability   divergence . Such invisible correlations may   mislead evaluation results , and thus lead to   implicit , unaware risks of applying PLMs in   real - world applications .   We further propose to conduct causal interven-   tion via backdoor adjustments , which can reduce   bias and ensure a more accurate , consistent and re-   liable probing under given assumptions . Note that   this paper not intends to create a “ universal cor-   rect ” probing criteria , but to remind the underlying   invisible risks , to understand how spurious correla-   tions lead to biases , and to provide a causal toolkit   for debiasing probing under specific assumptions .   Besides , we believe that our discoveries not only   exist in prompt - based probing , but will also influ-   ence all prompt - based applications to pretrained   language models . Consequently , our conclusions   echo that we need to rethink the criteria for identi-   fying better pretrained language models with the   above - mentioned biases .   Generally , the main contributions of this paper   are :   •We investigate the critical biases and quan-   tify their risks of evaluating pretrained lan-   guage models with widely used prompt - based   probing , including prompt preference bias , in-   stance verbalization bias , and sample disparity   bias .   •We propose a causal analysis framework ,   which can be used to effectively identify ,   understand , and eliminate biases in prompt-   based probing evaluations.5797   •We provide valuable insights for the design of   unbiased datasets , better probing frameworks ,   and more reliable evaluations , and echo that   we should rethink the evaluation criteria for   pretrained language models .   2 Background and Experimental Setup   2.1 Causal Inference   Causal inference is a promising technique for iden-   tifying undesirable biases and fairness concerns in   benchmarks ( Hardt et al . , 2016 ; Kilbertus et al . ,   2017 ; Kusner et al . , 2017 ; Vig et al . , 2020 ; Feder   et al . , 2021 ) . Causal inference usually describes the   causal relations between variables via Structural   Causal Model ( SCM ) , then recognizes confounders   and spurious correlations for bias analysis , finally   identifies true causal effects by eliminating biases   using causal intervention techniques .   SCM The structural causal model ( Pearl et al . ,   2000 ) describes the relevant features in a system   and how they interact with each other . Every   SCM is associated with a graphical causal model   G={V , f } , which consists of a set of nodes rep-   resenting variables V , as well as a set of edges   between the nodes representing the functions fto   describe the causal relations .   Causal Intervention To identify the true causal   effects between an ordered pair of variables ( X , Y ) ,   Causal intervention fixes the value of X = xand   removes the correlations between Xand its prece-   dent variables , which is denoted as do(X = x ) . In   this way , P(Y = y|do(X = x))represents the true   causal effects of treatment Xon outcome Y(Pearl   et al . , 2016 ) .   Backdoor Path When estimating the causal ef-   fect of XonY , the backdoor paths are the non-   causal paths between XandYwith an arrow intoX , e.g. , X←Z→Y. Such paths will confound   the effect that Xhas on Ybut not transmit causal   influences from X , and therefore introduce spuri-   ous correlations between XandY.   Backdoor Criterion The Backdoor Criterion is   an important tool for causal intervention . Given an   ordered pair of variables ( X , Y ) in SCM , and a set   of variables Zwhere Zcontains no descendant of   Xand blocks every backdoor path between Xand   Y , then the causal effects of X = xonYcan be   calculated by :   P(Y = y|do(X = x ) ) =   XP(Y = y|X = x , Z = z)P(Z = z),(1 )   where P(Z = z)can be estimated from data or   priorly given , and is independent of X.   2.2 Experimental Setup   Task This paper investigates prompt - based prob-   ing on one of the most representative and well-   studied tasks – factual knowledge probing ( Liu   et al . , 2021b ) . For example , to evaluate whether   BERT knows the birthplace of Michael Jordan ,   factual knowledge probing queries BERT with   “ Michael Jordan was born in [ MASK ] ” , where   Michael Jordan is the verbalized subject men-   tion , “ was born in ” is the verbalized prompt of rela-   tionbirthplace , and [ MASK ] is a placeholder   for the target object .   Data We use LAMA ( Petroni et al . , 2019 ) as our   primary dataset , which is a set of knowledge triples   sampled from Wikidata . We remove the N - M rela-   tions ( Elazar et al . , 2021 ) which are unsuitable for   the P@1 metric and retain 32 probing relations in   the dataset . Please refer to the appendix for detail.5798Pretrained Models We conduct probing experi-   ments on 4 well - known PLMs : BERT ( Devlin et al . ,   2019 ) , RoBERTa ( Liu et al . , 2019 ) , GPT-2 ( Rad-   ford et al . , 2019 ) and BART ( Lewis et al . , 2020 ) ,   which correspond to 3 representative PLM archi-   tectures , including autoencoder ( BERT , RoBERTa ) ,   autoregressive ( GPT-2 ) and denoising autoencoder   ( BART ) .   3 Structural Causal Model for Factual   Knowledge Probing   In this section , we formulate the SCM for factual   knowledge probing procedure and describe the key   variables and causal relations .   The SCM is shown in Figure 2a , which con-   tains 11 key variables : 1 ) Pretraining corpus   distribution D ; 2)Pretraining corpus C , e.g. ,   Webtext for GPT2 , Wikipedia for BERT ; 3 ) Pre-   trained language model M ; 4)Linguistic distri-   bution L , which guides how a concept is verbal-   ized into natural language expression , e.g. , rela-   tion to prompt , entity to mention ; 5 ) Relation R ,   e.g. ,birthplace , capital , each relation cor-   responds to a probing task ; 6 ) Verbalized prompt   Pfor each relation , e.g , xwas born in y ; 7)Task-   specific predictor I , which is a PLM combined   with a prompt , e.g. , < BERT , was born in > as a   birthplace predictor ; 8) Probing data distri-   bution D , e.g. , fact distribution in Wikidata ; 9 )   Sampled probing data Tsuch as LAMA , which   are sampled entity pairs ( e.g. , < Q41421 , Q18419 >   in Wikidata ) of relation R ; 10 ) Verbalized in-   stances X , ( e.g. , < Michael Jordan , Brooklyn >   from < Q41421 , Q18419 > ) ; 11 ) Performance Eof   the predictor IonX.   The causal paths of the prompt - based probing   evaluation contains :   •PLM Pretraining . The path { D , L } →   C→Mrepresents the pretraining procedure   for language model M , which first samples   pretraining corpus Caccording to pretraining   corpus distribution Dand linguistic distribu-   tionL , then pretrains MonC.   •Prompt Selection . The path { R , L } → P   represents the prompt selection procedure ,   where each prompt Pmust exactly express   the semantics of relation R , and will be influ-   enced by the linguistic distribution L.   •Verbalized Instances Generation . The path   { D , R } → T→X←Lrepresents thegeneration procedure of verbalized probing   instances X , which first samples probing data   Tof relation Raccording to data distribution   D , then verbalizes the sampled data Tinto   Xaccording to the linguistic distribution L.   •Performance Estimation . The path   { M , P } → I→E←Xrepresents the   performance estimation procedure , where the   predictor Iis first derived by combining PLM   Mand prompt P , and then the performance   Eis estimated by applying predictor Ion ver-   balized instances X.   To evaluate PLMs ’ ability on fact extraction ,   we need to estimate P(E|do(M = m ) , R = r ) .   Such true causal effects are represented by the path   M→I→Ein SCM . Unfortunately , there exist   three backdoor paths between pretrained language   model Mand performance E , as shown in Fig-   ure 2b - d. These spurious correlations make the   observation correlation between MandEcannot   represent the true causal effects of MonE , and   will inevitably lead to biased evaluations . In the   following , we identify three critical biases in the   prompt - based probing evaluation and describe the   manifestations , causes , and casual interventions for   each bias .   4 Prompt Preference Bias   In prompt - based probing , the predictor of a spe-   cific task ( e.g. , the knowledge extractor of rela-   tionbirthplace ) is a PLM Mcombined with a   prompt P(e.g . , BERT + was born in ) . However ,   PLMs are pretrained on specific text corpus , there-   fore will inevitably prefer prompts sharing the same   linguistic regularity with their pretraining corpus .   Such implicit prompt preference will confound the   true causal effects of PLMs on evaluation perfor-   mance , i.e. , the performance will be affected by   both the task ability of PLMs and the preference   fitness of a prompt . In the following , we investigate   prompt preference bias via causal analysis .   4.1 Prompt Preference Leads to Inconsistent   Performance   In factual knowledge probing , we commonly as-   sign one prompt for each relation ( e.g. , X was   born in Y forbirthplace ) . However , dif-   ferent PLMs may prefer different prompts , and it is   unable to disentangle the influence of prompt pref-   erence from the final performance . Such invisible5799   prompt preference will therefore lead to inconsis-   tent conclusions .   To demonstrate this problem , we report the   performance variance on LAMA using different   prompts for each PLM . For each relation , we follow   Elazar et al . ( 2021 ) ; Jiang et al . ( 2020b ) and design   at least 5 prompts that are semantically equivalent   and faithful but vary in linguistic expressions .   Prompt selection significantly affects perfor-   mance . Figure 3 illustrates the performance on   several relations , where the performances of all   PLMs vary significantly on semantically equiv-   alent prompts . For instance , by using different   prompts , the Precision@1 of relation languages   spoken dramatically changing from 3.90 % to   65.44 % on BERT - large , and from 0.22 % to 71.94 %   on BART - large . This result is shocking , because   the same PLM can be assessed from “ knowing   nothing ” to “ sufficiently good ” by only changing   its prompt . Table 1 further shows the quantitative   results , for BERT - large , the averaged standard de-   viation of Precision@1 of different prompts is 8.75 .   And the prompt selection might result in larger per-   formance variation than model selection : on more   than 70 % of relations , the best and worst prompts   will lead to > 10 point variation at Precision@1 ,   which is larger than the majority of performance   gaps between different models .   Prompt preference also leads to inconsistent   comparisons . Figure 4 demonstrates an exam-   ple , where the ranks of PLMs are significantly   changed when applying diverse prompts . We also   conduct quantitative experiments , which show that   the PLMs ’ ranks on 96.88 % relations are unstable   when prompt varies .   All these results demonstrate that the prompt   preference bias will result in inconsistent perfor-   mance . Such inconsistent performance will fur-   ther lead to unstable comparisons between different   PLMs , and therefore significantly undermines the   evaluations via prompt - based probing .   4.2 Cause of Prompt Preference Bias   Figure 2b shows the cause of the prompt preference   bias . When evaluating the ability of PLMs on spe-   cific tasks , we would like to measure the causal ef-   fects of path M→I→E. However , because the   prompt Pand the PLM Mare all correlated to the   linguistic distribution L , there is a backdoor path   M←C←L→P→I→Ebetween PLM M   and performance E. Consequently , the backdoor   path will confound the effects of M→I→E   withP→I→E.   Based on the above analysis , the prompt prefer-   ence bias can be eliminated by blocking this back-   door path via backdoor adjustment , which requires   a prior formulation of the distribution P(P ) . In   Section 7 , we will present one possible causal in-   tervention formulation which can lead to more con-   sistent evaluations .   5 Instance Verbalization Bias   Apart from the prompt preference bias , the under-   lying linguistic correlation can also induce bias in   the instance verbalization process . Specifically , an   instance in probing data can be verbalized into dif-   ferent natural language expressions ( e.g. , verbalize5800   Q30 in Wikidata into America orthe U.S. ) ,   and different PLMs may prefer different verbaliza-   tions due to mention coverage , expression prefer-   ence , etc . This will lead to instance verbalization   bias .   5.1 Instance Verbalization Brings Unstable   Predictions   In factual knowledge probing , each entity is verbal-   ized to its default name . However , different PLMs   may prefer different verbalizations , and such under-   lying correlation is invisible . Because we could n’t   measure how this correlation affects probing per-   formance , the evaluation may be unstable using   different verbalizations .   Table 2 shows some intuitive examples . When   we query BERT “ The capital of the U.S. is   [ MASK ] ” , the answer is Washington . Mean-   while , BERT would predict Chicago if we re-   place the U.S. to its alias America . Such un-   stable predictions make us unable to obtain reliable   conclusions on whether or to what degree PLMs   actually entail the knowledge .   To quantify the effect of instance verbalization   bias , we collect at most 5 verbalizations for each   subject entity in LAMA from Wikidata , and cal-   culate the verbalization stability on each relation ,   i.e. , the percentage of relation instances whose pre-   dictions are unchanged when verbalization varies .   The results in Figure 5 show the average verbaliza-   tion stabilities of all four PLMs are < 40 % , which   demonstrate that the instance verbalization bias   will bring unstable and unreliable evaluation .   5.2 Cause of Instance Verbalization Bias   Figure 2c shows the cause of instance verbalization   bias : the backdoor path M←C←L→X→E ,   which stems from the confounder of linguistic dis-   tribution Lbetween pretraining corpus Cand ver-   balized probing data X. Consequently , the ob-   served correlation between MandEcouldn’t faith-   fully represent the true causal effect of MonE ,   but is also mixed up the spurious correlation caused   by the backdoor path .   The instance verbalization bias can be eliminated   by blocking this backdoor path via causal interven-   tion , which requires a distribution formulation of   the instance verbalization , i.e. , P(X ) . We will   present a possible intervention formulation in Sec-   tion 7 .   6 Sample Disparity Bias   Besides the biases induced by linguistic correla-   tions , the distributional correlations between pre-   training corpus and task - specific probing data can   also introduce sample disparity bias . That is , the   performance difference between different PLMs   may due to the sample disparity of their pretraining   corpus , rather than their ability divergence .   In conventional evaluation , the evaluated hy-   potheses are independent of the train / test data gen-   eration , and all the hypotheses are evaluated on   training data and test data generated from the same   distribution . Therefore , the impact of correlations   between training data and test data is transparent ,   controllable , and equal for all the hypotheses . By   contrast , in prompt - based probing , each PLM is   bundled with a unique pretraining corpus , the corre-   lation between pretraining corpus distribution and   probing data distribution can not be quantified . In   the following we investigate this sample disparity   bias in detail .   6.1 Sample Disparity Brings Biased   Performance   In factual knowledge probing , LAMA ( Petroni   et al . , 2019 ) , a subset sampled from Wikidata,5801   is commonly used to compare different PLMs .   Previous work claims that GPT - style models are   with weaker factual knowledge extraction abili-   ties than BERT because they perform worse on   LAMA ( Petroni et al . , 2019 ; Liu et al . , 2021c ) .   However , because PLMs are pretrained on differ-   ent pretraining corpus , the performance divergence   can stem from the spurious correlation between   pretraining corpus and LAMA , rather than their   ability difference . For example , BERT ’s superior   performance to GPT-2 may stem from the diver-   gence of their pretraining corpus , where BERT ’s   pretraining corpus contains Wikipedia , while GPT-   2 ’s pretraining corpus does n’t .   To verify the effect of sample disparity bias , we   further pretrain BERT and GPT-2 by constructing   pretraining datasets with different correlation de-   grees to LAMA , and report their new performances   on LAMA . Specifically , we use the Wikipedia snip-   pets in LAMA and collect a 99k - sentence dataset ,   named WIKI - LAMA . Then we create a series of   pretraining datasets by mixing the sentences from   WIKI - LAMA with WebText(the pretraining cor-   pus of GPT2 ) . That is , we fix all datasets ’ size   to 99k , and a parameter γis used to control the   mixture degree : for each dataset , there are γ%in-   stances sampled from WIKI - LAMA and 1−γ%   instances sampled from WebText . Please refer to   the appendix for pretraining detail .   Table 3 demonstrates the effect of sample dis-   parity bias . We can see that 1 ) Sample disparity   significantly influences the PLMs ’ performance :   the larger correlation degree γwill result in better   performance for both BERT and GPT-2 ; 2 ) Sample   disparity contributes to the performance difference .   We can see that the performance gap between GPT-   2 and BERT significantly narrows down when theyare further pretrained using the same data . Besides ,   further pretraining BERT on WebText ( γ=0 ) would   significantly undermine its performance . These re-   sults strongly confirm that the sample disparity will   significantly bias the probing conclusion .   6.2 Cause of Sample Disparity Bias   The cause of sample disparity bias may diverge   from PLMs and scenarios due to the different   causal relation between pretraining corpus distribu-   tionDand probing data distribution D. Never-   theless , sample disparity bias always exist because   the backdoor path will be M←C←D→   D→T→X→Ewhen Dis the ancestor of   D , orM←C←D←D→T→X→E   when Dis the descendant of D. Figure 2d shows   a common case when the pretraining corpus dis-   tribution Dis an ancestor of probing data dis-   tribution D. For example , the pretraining data   contains Wikipedia and probing data is a sampled   subset from Wikipedia ( e.g. , LAMA , X - FACTR ,   BioLAMA ) . As a result , there is a backdoor path   between MandE , which will mislead the evalua-   tion .   7 Bias Elimination via Causal   Intervention   This section describes how to eliminate the above-   mentioned biases by blocking their corresponding   backdoor paths . According to the Backdoor Cri-   terion in Section 2.1 , we need to choose a set of   variables Zthat can block every path containing an   arrow into Mbetween MandE. Since the linguis-   tic distribution L , pretraining corpus distribution   Dand probing data distribution Dare unobserv-   able , we choose Z={P , X}as the variable set   for blocking all backdoor paths between ( M , E ) in   the SCM by conducting backdoor adjustment :   P(E|do(M = m ) , R = r ) =   XXP(p , x)P(E|m , r , p , x ) .(2 )   Equation 2 provides an intuitive solution . To   eliminate the biases stemming from the spurious   correlations between pretraining corpus , probing   data and prompts , we need to consider the natu-   ral distribution of prompts and verbalized probing   data regardless of other factors . Consequently , the   overall causal effects between PLM and evaluation   result are the weighted averaged effects on all valid   prompts and probing data.5802   Unfortunately , the exact distribution of P(x , p )   is intractable , which needs to iterate over all valid   prompts and all verbalized probing data . To ad-   dress this problem , we propose a sampling - based   approximation . Specifically , given a specific as-   sumption about P(x , p)(we assume uniform distri-   bution in this paper without the loss of generality ) ,   we sample Kprompts for each relation and K   kinds of verbalization for each instance according   toP(x , p ) , and then these samples are used to es-   timate the true causal effects between MandE   according to Equation 2 .   To verify whether causal intervention can im-   prove the evaluation consistency and robustness ,   we conduct backdoor adjustment experiments on   8 different PLMs . We randomly sample 1000 sub-   sets with 20 relations from LAMA , and observe   whether the evaluation conclusions were consis-   tent and stable across the 1000 evaluation runtimes .   Specifically , we use rank consistency as the evalu-   ation metric , which measures the percentage of the   most popular rank of each model in 1000 runtimes .   For example , if BERT ranks at 3place in 800   of the 1000 runtimes , then the rank consistency of   BERT will be 80 % .   Table 4 shows the results . We can see that causal   intervention can significantly improve the evalu-   ation consistency : 1 ) The consistency of current   prompt - based probing evaluations is very poor on   all 8 PLMs : when we randomly select prompts andverbalizations during each sampling , the overall   rank consistency is only 5.5 % ; 2 ) Causal interven-   tion can significantly improve overall rank consis-   tency : from 5.5 % to 68.5 % ; 3 ) Casual intervention   can consistently improve the rank consistency of   different PLMs : the rank of most PLMs is very   stable after backdoor adjustment .   The above results verify that causal intervention   is an effective technique to boost the stability of   evaluation , and reach more consistent conclusions .   8 Related Work   Prompt - based Probing Prompt - based probing   is popular in recent years ( Rogers et al . , 2020 ;   Liu et al . , 2021b ) for probing factual knowl-   edge ( Petroni et al . , 2019 ; Jiang et al . , 2020a ; Sung   et al . , 2021 ) , commonsense knowledge ( Davison   et al . , 2019 ) , semantic knowledge ( Ettinger , 2020 ;   Sun et al . , 2021 ; Brown et al . , 2020 ; Schick and   Schütze , 2020 ) and syntactic knowledge ( Ettinger ,   2020 ) in PLMs . And a series of prompt - tuning   studies consider optimizing prompts on training   datasets with better performance but may under-   mine interpretability ( Jiang et al . , 2020b ; Shin et al . ,   2020 ; Haviv et al . , 2021 ; Gao et al . , 2021 ; Qin and   Eisner , 2021 ; Li and Liang , 2021 ; Zhong et al . ,   2021 ) . Because such prompt - tuning operations will   introduce additional parameters and more unknown   correlations , this paper does not take prompt - tuning   into our SCM , delegate this to future work .   Biases in NLP Evaluations Evaluation is the cor-   nerstone for NLP progress . In recent years , many   studies aim to investigate the underlying biases and   risks in evaluations . Related studies include inves-   tigating inherent bias in current metrics ( Cough-   lin , 2003 ; Callison - Burch et al . , 2006 ; Li et al . ,   2017 ; Sai et al . , 2019 , 2020 ) , exploring dataset   artifacts in data collection and annotation proce-   dure ( Lai and Hockenmaier , 2014 ; Marelli et al . ,   2014 ; Chen et al . , 2018 ; Levy and Dagan , 2016 ;   Schwartz et al . , 2017 ; Cirik et al . , 2018 ; McCoy   et al . , 2019 ; Liu et al . , 2021a ; Branco et al . , 2021 ) ,   and identifying the spurious correlations between   data and label which might result in catastrophic   out - of - distribution robustness of models ( Poliak   et al . , 2018 ; Rudinger et al . , 2018 ; Rashkin et al . ,   2018 ) .   Most previous studies demonstrate the evalua-   tion biases empirically , and interpret the underlying   reasons intuitively . However , intuitive explanations   are also difficult to critical and extend . In contrast,5803this paper investigates the biases in prompt - based   probing evaluations from a causal view . Based on   the causal analysis framework , we can identify , un-   derstand , and eliminate biases theoretically , which   can be extended and adapted to other evaluation   settings in a principled manner . We believe both   the causal analysis tools and the valuable insights   can benefit future researches .   9 Conclusions and Discussions   This paper investigates the critical biases and quan-   tifies their risks in the widely used prompt - based   probing evaluation , including prompt preference   bias , instance verbalization bias , and sample dispar-   ity bias . A causal analysis framework is proposed   to provide a unified framework for bias identifica-   tion , interpretation and elimination with a theoreti-   cal guarantee . Our studies can promote the under-   standing of prompt - based probing , remind the risks   of current unreliable evaluations , guide the design   of unbiased datasets , better probing frameworks ,   and more reliable evaluations , and push the bias   analysis from empirical to theoretical .   Another benefit of this paper is to remind the   evaluation criteria shifts from conventional ma-   chine learning algorithms to pretrained language   models . As we demonstrate in Figure 1 , in conven-   tional evaluation , the evaluated hypotheses ( e.g. ,   algorithms , architectures ) are raised independently   of the train / test dataset generation , where the im-   pact of correlations between training data and test   data is transparent , controllable , and equal for all   the hypotheses . However , in evaluations of pre-   trained language models , the pretraining corpus is   bundled with the model architecture . In this case ,   it is significant to distinguish what you need to as-   sess ( architecture , corpus , or both ) , as well as the   potential risks raised by the correlations between   pretraining corpus and test data , which most current   benchmarks have ignored . Consequently , this pa-   per echoes that it is necessary to rethink the criteria   for identifying better pretrained language models ,   especially under the prompt - based paradigm .   In the future , we would like to extend our causal   analysis framework to fit prompt - tuning based prob-   ing criteria and all PLM - based evaluations .   Acknowledgments   We sincerely thank all anonymous reviewers for   their insightful comments and valuable sugges - tions . This research work is supported by the Na-   tional Natural Science Foundation of China under   Grants no . 62122077 , the Strategic Priority Re-   search Program of Chinese Academy of Sciences   under Grant No . XDA27020200 , and the National   Natural Science Foundation of China under Grants   no . 62106251 and 62076233 .   Ethics Consideration   This paper has no particular ethic consideration .   References5804580558065807A Datasets Construction Details   Instance Filtering We follow the data construc-   tion criteria as LAMA , we remove the instances   whose object is multi - token or not in the intersec-   tion vocabulary of these 4 PLMs .   Relation Selection We remove all the N - M re-   lations in LAMA such as “ share border with ” or   “ twin city ” . Because in these relations , there are   multiple object entities corresponding to the same   subject entity . In that case , the metric Precision@1   is not suitable for evaluating PLMs in such rela-   tions . In addition , due to the completeness limita-   tion of knowledge bases , it ’s impossible to find all   the correct answers for each subject . Therefore , we   do not include these relations in our experiments .   Prompt Generation Because of the difference   between the pretraining tasks of these 4 PLMs ( au-   toencoder , autoregressive and denoising autoen-   coder ) , we design prompts where the placeholder   for the target object is at the end , e.g. , The birth-   place of xisyinstead of yis the birthplace of x.   We follow the instruction from Wikidata , combine   the prompts from Elazar et al . ( 2021 ) and Jiang   et al . ( 2020b ) , and manually filter out the prompts   with inappropriate semantics . All the prompts are   created before any experiments and fixed afterward .   B Further Pretraining Details   We further pretrain BERT with masked language   modeling ( mask probability= 15 % ) and GPT2 with   autoregressive language modeling task respectively .   Training was performed on 840G - A100 GPUs for   3epochs , with maximum sequence length 512 . The   batch sizes for BERT - base , BERT - large , GPT2-   base , GPT2 - medium are 256,96,128,64respec-   tively . All the models is optimized with Adam   using the following parameters : β= 0.9 , β=   0.999 , ϵ= 1e−8and the learning rate is 5e−5   with warmup ratio= 0.06.5808