  Liyan XuChenwei ZhangXian LiJingbo ShangJinho D. ChoiPattern Recognition Center , WeChat AIEmory UniversityAmazon.comUniversity of California , San Diego   liyan.xu@emory.edu { cwzhang , xianlee } @amazon.com   Abstract   We present a new task setting for attribute   mining on e - commerce products , serving as   a practical solution to extract open - world at-   tributes without extensive human intervention .   Our supervision comes from a high - quality   seed attribute set bootstrapped from existing   resources , and we aim to expand the attribute   vocabulary of existing seed types , and also   to discover any new attribute types automat-   ically . A new dataset is created to support   our setting , and our approach Amacer is pro-   posed specifically to tackle the limited supervi-   sion . Especially , given that no direct super-   vision is available for those unseen new at-   tributes , our novel formulation exploits self-   supervised heuristic and unsupervised latent   attributes , which attains implicit semantic sig-   nals as additional supervision by leveraging   product context . Experiments suggest that our   approach surpasses various baselines by 12 F1 ,   expanding attributes of existing types signifi-   cantly by up to 12 times , and discovering values   from 39 % new types . Our data and code can be   found at https://github.com/lxucs/woam .   1 Introduction   Attribute mining ( or product attribute extraction )   is to extract values of various attribute types ( e.g.   colors , flavors ) from e - commerce product descrip-   tion , which is a foundational piece for product un-   derstanding in online shopping services , enabling   better search and recommendation experience .   Within this task regime , different settings have   been studied . Most pioneer works deem it as a   closed - world setting , where models are trained to   identify a fixed set of pre - defined attribute types   ( Ghani et al . , 2006 ; Putthividhya and Hu , 2011 ;   Zheng et al . , 2018 ) , similar to the standard named   entity recognition ( NER ) . Recent works start to   step up towards the open - world aspect that supportsFigure 1 : Illustration of our task setting on one prod-   uct : given light supervision from seed attributes , our   approach Amacer aims to expand attribute vocabulary   of seed types , and to also discover values of any new   types ( Shelf Life , Origin ) not covered by seeds . The   outputs on all products are thus attribute clusters with   diverse values . Evaluation is based on clustering met-   rics , as new clusters are not named beforehand .   extraction of new attribute types unseen in training .   Particularly , several works have focused on the   zero - shot perspective ( Xu et al . , 2019 ; Yang et al . ,   2022 ) , enabling extraction of a new attribute type   during inference if given a name or description of   this new type , which is a more realistic setting to   this task , as new types of products and attributes   are constantly emerging in the real world .   In this work , we formulate the attribute mining   task one step further towards the ultimate open-   world setting : given product - related description ,   the objective is to identify as many new values of   existing attribute types , as well as any new types   that could be considered as reasonable attributes   but not covered in training . As such , our setting   automatically discovers new attributes , unlike the   zero - shot setting that requires explicit specification   of new types of interest . In addition , we also aim   the model to work under limited supervision , by   introducing only a relatively small seed attribute   set in training , thereby remaining practical when   only a few values are known for a certain attribute,12223also for the fact that it is untenable to keep up   high - coverage human annotations of ever - changing   attributes , especially in e - commerce domain .   Figure 1 illustrates our overall task setting ,   where the model expands the attribute vocabulary   of existing types , and discovers any new attributes ,   yielding numerous attribute clusters . A new dataset   dubbed WM(Weakly - supervised Open - world   Attribute Mining ) is created to accommodate our   setting , as described in Section 2 . Targeting to-   wards realistic open - world setting , our dataset cov-   ers full product horizons including titles and de-   tailed description , where the latter provides rich   context and is shown to contain more unseen at-   tribute types than titles by 66 % ( Table 1 ) . More-   over , distinguished from previous datasets that ei-   ther require substantial annotation efforts ( Zheng   et al . , 2018 ) or noisy distant - supervised data ( Xu   et al . , 2019 ; Yang et al . , 2022 ; Zhang et al . , 2022 ) ,   our training supervision comes from a high - quality   seed attribute set constructed hybridly , combining   data - driven and light human curation . Overall , our   setting achieves good trade - offs with reasonable   human interventions , under a practical scope with   decent coverage on attributes .   We then propose our approach for this setting ,   dubbed Amacer ( Attribute mining with adaptive   clustering and w eakregularization ) . To overcome   the challenge of limited supervision , we first in-   troduce our approach to generate diverse spans of   candidate attribute values from corpus ( § 3 ) ; then fo-   cus on representation learning by utilizing explicit   supervision from seed attributes ( § 4 ) , followed by   the last step that performs grouping on candidate   spans using refined , attribute - aware embeddings   ( § 5 ) . New formulations to mine more implicit se-   mantic signals from product context are also pro-   posed for new attribute discovery ( § 6 ) .   Experiments on WMsuggest that our ap-   proach outperforms various baselines by up to 12.5   F1 . Furthermore , our novel formulation to lever-   age self - supervised and unsupervised semantic sig-   nals is shown effective to both existing and new   attributes , especially boosting new attribute dis-   covery by a good margin of 6.4 F1 . Despite the   limited amount of seed values , our model is able to   expand the seed attribute vocabulary by up to 12   times ( Table 15 ) , and to discover values from 39 %   unseen attribute types on our test set . Overall , our   contributions can be summarized as follows :   •We address a new setting in attribute mining asa practical paradigm to extract open - world at-   tributes under light human intervention .   •A new dataset is created , covering 66 attribute   types with 42 % unseen types from the seed set .   •A new approach is proposed to support our   unique task setting , especially exploiting self-   supervised and unsupervised semantic signals ,   which has not been explored by previous works .   2 Data   Our dataset WMconsists of three parts , includ-   ing : 1 ) text corpus ; 2 ) seed attribute set for training ;   3 ) human - annotated test set for evaluation . Full   statistics of our dataset are provided in Table 11 ,   and more details are provided in Appendix B.   Corpus Four common e - commerce product cat-   egories are included in our corpus : Tea , Vitamin ,   Sofa , Phone Case . For each category , we sampled   9,000 + products publicly listed on Amazon.com   with full description available in English . Each   product record can be represented as a tuple : ( iden-   tifier , category , title , bullet points ) .   Seed Set For each category , the seed set consists   of a few applicable attribute types ( avg . 16.5 types   per category ) and their values ( avg . 22 values per   type ) . We adopt a hybrid approach for the con-   struction : existing resources are first utilized to   bootstrap the seed set , and human curation is per-   formed upon to overcome the noisy issue existed   in previous datasets ( example shown in Table 10 ) .   Specifically , two steps are applied as below :   Automatic Sanitizing : we collect the raw prod-   uct profiles that contain certain attributes provided   by Amazon retailers , and perform frequency - based   heuristics to heavily sanitize noisy attributes . First ,   long - tail attribute types that have fewer than 10   values are removed . Second , for each product cat-   egory , if a unique value appears under multiple   attributes types , we restrict it to only belong to its   most common type . Lastly , for each attribute type ,   we only keep at most 100 values based on the top   frequency , so to discard the tail values that we are   less confident on . The resulting seed set thereby   has a relatively small size but of higher quality after   above three steps .   Human Curation : as the attribute set after san-   itizing is relatively small , human curators can go   through the entire set rather quickly and consoli-   date the final seed set ( < 40min per product cat-   egory ) . Concretely , remaining noisy values are12224   spotted and removed from their attribute types . Fur-   thermore , granularity is adjusted such that ambigu-   ous or coarse attribute types are split into multiple   newly defined fine - grained types ; similar attribute   types are also merged into one type .   After we obtain the final seed set , we perform   string match to obtain their occurrences in corpus ,   ready to be used for training . A development set is   separately created that consists of sanitized profile   attributes solely for hyperparameter tuning . Over-   all , our training supervision is built practically that   balances between scalability and quality .   Test Set For each category , we collect additional   products not covered in the raw corpus as the test   set . Two in - house annotators are asked to annotate   all spans that appear as reasonable attribute values   of either an existing type from the seed set , or   a brand - new type that fits the context . As with   previous works , we do not allow overlapping spans :   more complete spans are preferred over shorter and   incomplete spans ; each span is assigned a single   attribute type that best describes its property .   Table 1 briefly specifies unique characteristics   of our dataset . It is clear that most gold values   are new values unseen from the seed set . Espe-   cially , bullet points have a higher ratio of new at-   tribute types / values than titles , while those values   are harder to extract due to longer text , sparser val-   ues , and more complex language structures . For   comparison , our setting poses greater challenges   than the most related previous dataset from a recent   work OA - Mine ( Zhang et al . , 2022 ) , which is un-   der a much limited scope that consists of only titles   with sparser and noisier seed attributes ( detailed   comparison is provided in Appendix A ) .   Our proposed approach for this dataset is pre-   sented in the following Section 3 - 6 . Specifically ,   Section 3 - 5 introduce the overall pipeline depicted   in Figure 2 that utilizes explicit signals from seed   attributes , and Section 6 introduces our novel for - mulation to exploit implicit signals beyond the lim-   ited seed attributes .   3 Candidate Span Generation   The first stage of our approach is to generate spans   from product description that could be qualified as   attribute values , producing a set of non - overlapping   candidate spans , serving as a foundational step for   this attribute extraction task .   With weak supervision in mind , this step should   not simply rely on signals from the seed set ; oth-   erwise , it would become hard to generalize and   lose diverse attribute expressions during inference .   Therefore , directly employing a supervised model   can be suboptimal . It is also tempting to use off - the-   shelf phrase extraction tools such as AutoPhrase   ( Shang et al . , 2018 ) , however , the domain shift on   e - commerce description of varied categories can   severely affect recall , as observed by Zhang et al .   ( 2022 ) . The close work OA - Mine regards this stage   as an unsupervised sentence segmentation task on   product titles through language model probing ( Wu   et al . , 2020 ) , regarding each segment as a candi-   date span . Nonetheless , two shortcomings still   remain . First , unlike titles , segmentation may not   be suitable for bullet points , as most segments from   bullet points would be noisy spans , demonstrated   by the lower value density ( 13.8 % ) in Table 1 . Sec-   ond , being completely unsupervised , there is no   task - specific adjustment in this process , suffering   inadequate candidate quality .   In this work , we instead resort to a basic yet   effective strategy that overcomes above issues , by   using syntax - oriented patterns : we collect valid   Part - of - Speech ( POS ) patterns for attribute values ,   and simply obtain all spans in the corpus that fit   into those patterns as candidate spans , followed   by rudimentary stopword filtering and overlapping   span removal ( prioritizing longer spans ) , yielding   a smaller but higher - quality candidate set than that   from sentence segmentation .   Valid POS patterns are acquired in a data - driven   fashion without human intervention : we leverage   the product profiles again , and obtain all POS se-   quences of their attribute values . These raw se-   quences are further compacted by removing con-   secutive duplicate POS tags , such that healthy clean   water ( [ ADJ , ADJ , NOUN ] →[ADJ , NOUN ] ) will share   the same POS pattern as clean water ( [ ADJ , NOUN ] ) .   The resulting set of collected POS patterns serves to   identify spans as well - formed or ill - formed phrases.12225   Examples of our POS patterns are shown in Ta-   ble 2 . They regulate spans based on their syntactic   features , without sole reliance on semantic supervi-   sion from the limited seed set , hence being able to   capture diverse attribute expressions of vast variety .   Overall , they serve as the quality guardrail for can-   didate spans , while reaping additional advantages :   1 ) easy to perform manual domain - specific adjust-   ment ; 2 ) scalable towards other product categories ,   as being data - driven ; 3 ) efficient to run in practice .   As we depend on external tools to identify POS ,   this process is not without noises . Nonetheless ,   we find the empirical performance to be quite ro-   bust qualitatively . Moreover , it can be augmented   with other techniques to mitigate noise in scenarios   tailored to specific applications .   4 Explicit Signals for Seed Expansion   With both seed attribute values and candidate spans   in - place , our next objective is to perform represen-   tation learning that refines the geometry of embed-   ding space , such that values of similar attributes   should have a closer embedding representation , and   vice versa , as the key property to leverage in later   grouping stage . In this section , we introduce the   utilization of available seed attributes as explicit   supervision , primarily targeting the vocabulary ex-   pansion of existing attribute types .   For each seed value or candidate span , we can   have an initial representation on the embeddingspace via encoding through pretrained language   models such as BERT ( Devlin et al . , 2019 ) . Con-   cretely , we feed each text sequence ( either a title   or bullet point ) to BERT , and obtain the contextu-   alized representation of each span by averaging its   token embedding , without introducing extra encod-   ing parameters .   Supervised Contrastive Learning Contrastive   learning is a natural fit to consume task signals   from the seed set : for an anchor seed value v , a   positive seed vfrom the same attribute , and a neg-   ative seed vfrom a different attribute , contrastive   learning enforces ( v , v)to be more similar than   ( v , v)on the embedding space . OA - Mine adopts   a triplet loss ( Schroff et al . , 2015 ) for the supervised   contrastive learning , as well as another regression   loss ( Reimers and Gurevych , 2019 ) that directly   pushes the similarity of positive / negative pairs , re-   quiring careful sampling and tuning . In our work ,   we simplify this supervised process by only using   an in - batch negative contrastive loss ( Khosla et al . ,   2020 ) . Let Ibe all seed value indices , P(i)be   the indices of positive seeds that belong to the same   attribute as seed i , N(i ) = I\P(i)be the cor-   responding negative seeds . gis the L2 - normalized   embedding of seed ifrom the last layer of BERT   encoding . The loss can then be denoted as :   L=/summationdisplay−1   |P(i)|/summationdisplayloge   /summationtexte   τis the temperature hyperparameter . As all em-   beddings are L2 - normalized , g·gis effectively   the cosine similarity as a distance measurement of   two span representation . Lpushes seed values   of the same attribute to have a similar representa-   tion , while pulling away seed values from different   attribute types on the embedding space.122265 Candidate Span Grouping   After representation learning , a grouping stage   upon candidate spans is followed . Each resulting   cluster represents an attribute type , with each span   inside being its attribute value . Unlike most re-   lated works that employ off - the - shelf clustering   algorithms such as HAC , K - Means or DBSCAN   ( Elsahar et al . , 2017 ; Zhao et al . , 2021 ; Zhang et al . ,   2022 ) , we propose a more fine - grained grouping   strategy , which first explicitly addresses the expan-   sion of existing seed attributes , then discovers new   potential attributes , as described below .   Adaptive Expansion on Existing Attributes We   borrow the concept from few - shot learning , and   regard each existing seed attribute set as a support   set . The distance between each candidate span c   and each support set Sis measured by D , which is   the averaged cosine distance between the candidate   and each seed values , as in Eq ( 1 ) . A candidate c   is added to an attribute jifD(c , S)<t , where   tis a threshold calculated adaptively based on its   support set , as in Eq ( 2 ) . Particularly , δ∈(0,1]is   a hyperparameter to relax the threshold that can be   tuned on the development set .   D(c , S ) = 1   |S|/summationdisplaycosine ( c , s ) ( 1 )   t = δ·1   |S|/summationdisplaycosine ( s , s ) ( 2 )   More Attribute Coverage For remaining can-   didate spans , more clusters are mined to increase   coverage primarily for potential new attributes . We   also resort to off - the - shelf DBSCAN that can auto-   matically discover clusters and distinguish noises   based on the pairwise cosine distance .   The union of clusters from the above two stages   serve as the final result of the candidate grouping .   6 Implicit Signals for New Discovery   Since the seed set only provides semantic signals   regarding seed attributes , the majority of candidate   spans lack proper supervision , as most of them   are absent from the seed set , especially for those   new attributes that have no direct supervision dur-   ing representation learning . Therefore , it is desir-   able to exploit additional implicit signals towards   more new - attribute - friendly embedding space , and   we propose novel methods to tackle the challenge   by fully leveraging product context through self-   supervised andunsupervised regularization .6.1 Self - Supervised Contrastive Learning   To utilize the product context , we formulate a self-   supervised contrastive heuristic similar to skip-   gram in word2vec ( Mikolov et al . , 2013 ) . We re-   gard each bullet point as a window : pushing two   candidate spans within the same window ( same   bullet point ) to have closer representation than   two spans not in the same window ( different bul-   let points of a product ) . It is based on the gen-   eral observation that different bullet points usually   discuss different product perspectives , but within   each point , similar attributes or topics are usually   mentioned . Though noisy , useful semantic signals   could still be revealed given enough corpus , similar   to the skip - gram training .   LetIbe all candidate span indices in bullet   points , P(i)be the indices of positive spans within   the same bullet point as i , N(i)be the correspond-   ing negative spans from different bullet points of   the same product . The self - supervised contrastive   loss is denoted as :   L=/summationdisplay−1   |P(i)|/summationdisplayloge   /summationtexte   We regard Las a form of regularization , assign-   ing a small coefficient during training . The final   loss is described as in Eq ( 8) .   6.2 Unsupervised Latent Attributes   More useful signals could still be revealed from   product context in addition to the bullet point   heuristic . Inspired from topic modeling , e.g. Latent   Dirichlet Allocation ( LDA ) ( Blei et al . , 2003 ) , a   classic generative method that discovers latent top-   ics unsupervisely from bag - of - words documents ,   here we propose a formulation of latent attributes   to regulate the embedding space , providing implicit   signals based on the semantic distribution of corpus ,   especially beneficial to new attribute discovery that   has no direct supervision . We adapt the neural LDA   work from Miao et al . ( 2017 ) ; Dieng et al . ( 2020 ) ,   and regard topics as attributes in our setting . The   main idea is that each product can be rendered as a   composition of spans ( equivalently , bag - of - spans )   generated from different latent attributes based on   the following two distributions .   Product - to - Attribute Distribution Given the   context of a product , the model predicts a distribu-   tion over Klatent attributes , where Kis a hyper-   parameter . Latent attributes of higher probabilities12227play a larger role in a product ’s semantics . Since   learning the true distribution is intractable , varia-   tional inference is applied such that we posit the   distribution family to be multivariate Gaussian with   diagonal covariance matrix , and fix the prior dis-   tribution as standard Gaussian ( Dieng et al . , 2020 ) .   Hence , the posterior Product - to - Attribute distribu-   tion can be obtained by simply predicting the mean   and variance of multivariate Gaussian . Let prep-   resent a product , hbe its context representation ,   µ/σbe its mean / variance for the latent attribute   kpredicted by the model . A sampled probability   of attribute kfor product pcan be denoted as α :   µ/σ = W·h(3 )   /tildewideα∼ N(µ , σ ) ( 4 )   α = softmax ( /tildewideα)| ( 5 )   Wis a learned parameter to predict mean and   variance . For h , we use the averaged CLS repre-   sentation of its product title and all bullet points .   Attribute - to - Span Distribution For each latent   attribute , the model also learns a distribution over   candidate spans ; spans of high probabilities are the   representatives of this attribute . Following Dieng   et al . ( 2020 ) , rather than building an explicit distri-   bution , the model instead simply learns an attribute   embedding , so that the distribution can be obtained   by measuring the similarity of the attribute em-   bedding and span embeddings . Let hbe the k’th   attribute embedding learned by the model , gbe   the representation of a candidate span c , andCbe   all unique candidate spans from all products in a   training batch . The distribution of an attribute k   over candidates Ccan be denoted as :   β = softmax ( h·g)| ( 6 )   Optimization Given the above two distributions   for a product p , the model can easily get the   Product - to - Span distribution P(c|p)by marginal-   izing out the latent attributes , as in Eq ( 7 ) , which   can then be used to optimize a reconstruction objec-   tive , such that spans actually appeared in product p   should have higher probability than those who do   not . Let V(p)be the candidate spans in a product   p , mbe the total number of products . The unsuper-   vised reconstruction loss Lcan be estimated byevidence lower bound ( ELBO ) as :   P(c|p ) = /summationdisplayα·β ( 7 )   L=−/summationdisplay / parenleftbig / summationdisplaylogP(c|p ) + KL(/tildewideα∥ˆα)/parenrightbig   L = L+λ · L+λ · L(8 )   where ˆαis the fixed standard Gaussian ( prior   Product - to - Attribute distribution ) . The first term   ofLis the log - likelihood to encourage higher   probability for actually appeared candidate spans   in a product , and the second KL - divergence term   regularizes the posterior attribute distribution /tildewideαto   be close to the standard Gaussian ˆα .   The final loss Lduring representation learning is   constituted by three losses ; λandλare hyper-   parameters that control the regularization strength .   7 Experiments   Experiments are conducted on our dataset in mul-   tiple model settings , including various baselines .   Three different types of models are examined based   on how attribute spans are obtained :   ( 1 ) Closed - world models based on sequence-   tagging that extract spans upon predicted BIO tags   of existing attributes , which do not support new   attribute discovery natively . Two models are ex-   perimented : Tx - CRF , a generic Transformers - CRF   tagging model ; SU - OpenTag ( Xu et al . , 2019 ) , a   popular tagging - based attribute extraction model .   ( 2 ) Open - world models that rely on sentence   segmentation to obtain candidate spans . We use   the code released from OA - Mine to obtain all text   segments for our dataset . Two settings are included :   OA - Mine ( Zhang et al . , 2022 ) ; Amacer , a stripped   version of our approach removing regularization   and directly taking segments as candidates .   ( 3 ) Open - world models that employ our syntax-   based candidate generation ( § 3 ) . Five settings   are included : DBSCAN that directly performs DB-   SCAN clustering without representation learning ;   DBSCAN+AE that adds our proposed adaptive expan-   sion ( § 4 ) ; OA - Minethat substitutes segmentation   with our candidate spans ; Amacer , our full pro-   posed approach ; and Amacerthat only utilizes   seed supervision without regularization in § 6 .   For candidate span generation , we use spaCyto   obtain POS tags ; a total of 96 valid POS patterns12228   are acquired from product profiles ( Section 3 ) . The   same BERT - Large is used as the encoder for all   models . Our detailed hyperparameter settings are   provided in Appendix C.   Evaluation Metrics Standard clustering evalua-   tion metrics are used : Jaccard , Adjusted Rand In-   dex ( ARI ) , Normalized Mutual Information ( NMI ) ,   to compare the attribute assignments on gold spans ;   Recall , to evaluate gold cluster coverage . As above   metrics are consistent with OA - Mine , the evalua-   tion adopts exact - match on predicted / gold spans .   However , it could become over - restrictive as span   boundaries can be quite subjective in this open-   world setting , losing the information of near - correct   predictions . Thus , we also provide a relaxed evalu-   ation that allows partial - match on spans , such that   a predicted span is considered an attribute value if   more than half of the span falls into a gold value .   To assess the overall performance of a model ,   we roughly regard the averaged number of Jaccard ,   ARI and NMI as pseudo precision , and derive a   single pseudo- F1score based on the clustering pre-   cision andrecall , serving as the main evaluation   metric of each approach .   Results Table 3 shows the evaluation results by   all model settings . Our full proposed approach   Amacer surpasses both SU - OpenTag andOA - Mine   by a large margin ( 10 + Exact / Partial - F1 ) , achieving   the best performance on this task . Further observa-   tions and ablation study can be obtained as below .   •Open - world models identify more attributes   than closed - world models . The two tagging - based   models underperform OA - Mine - based models andour Amacer - based models , with noticeably lower   recall . It can be attributed to two factors . First ,   as all spans are obtained through tagging learned   solely from the seed set , they lack the ability to   accept more diverse attribute values not covered in   training , not being able to generalize well under   limited supervision . Second , new attributes are left   untouched , unlike the open - world counterparts .   •Adaptive expansion on seed attribute types is   effective for candidate grouping . By simply com-   paring DBSCAN with DBSCAN+AE , adaptive expan-   sion is shown greatly improving the recall by 13-   23 % and overall performance by 10+% . On a side   note , there is still a huge gap between DBSCAN+AE   andAmacer , demonstrating the necessity to refine   embedding space by representation learning .   •Syntax - oriented generation obtains candi-   date spans of higher quality than segmentation .   Both OA - MineandAmacerthat apply syntax-   oriented candidates outperform their segmentation-   based counterparts OA - Mine andAmacer , espe-   cially for exact - match that brings a gap of 4 + F1 .   Notably , our generation step takes under 10 min-   utes to process each category on CPUs , while the   segmentation requires several hours on a GPU .   Qualitatively , we found that the segmentation often   over - divides sentences , yielding many noisy and   incomplete phrases .   •Seed supervision is more efficiently utilized   by in - batch negative contrastive loss . Compared   to the triplet loss and regression loss adopted in   OA - Mine , the in - batch loss is not only simpler but   also improves 5 + F1 in this task . We found the   regression loss that pushes cosine similarity to 1/-112229   for pos / neg pairs can be too harsh for the embed-   ding space , as certain attribute types are indeed   more related and not completely independent .   •Regularization ( § 6 ) is able to bring ad-   ditional semantic signals useful to shape the   attribute - aware embedding space , as shown by   the 2.2 Partial F1 improvement of Amacer upon   Amacer , where the unsupervised latent attribute   formulation contributes around 70 % improvement .   We provide further quantitative and qualitative in-   sights in Section 8 - 9 .   8 Quantitative Analysis   To quantify the unique challenges of this task , we   decompose the evaluation to examine two perspec-   tives specifically :   •Performance on new attribute types ( only open-   world evaluation ) compared to seed types ( only   closed - world evaluation ) .   •Performance on attribute values in bullet points   compared to titles .   Table 4 shows that all models suffer performance   degradation on new attribute types unseen in train-   ing , comparing with those existing seed types ,   which corroborates the expectation that open - world   discovery remains a tough challenge owing to no di-   rect supervision . It is noteworthy that our approach   brings significant improvement on new attributes ;   especially , our proposed regularization in Amacer   boosts performance on existing types by relatively   2.3 % upon Amacer , while the improvement onnew types is 16.1 % , which fulfills our motivation   to provide semantic supervision for those new at-   tributes . Compared to OA - Mine , our approach ex-   hibits smaller relative gap between existing and   new types , discovering 39 % new types ( Recall in   Table 12 ) .   For more traits of our corpus , all models strug-   gle to keep up the performance on bullet points   compared to titles , showing that they are indeed   harder to extract from due to their characteristics   ( Table 1&9 ) . Interestingly , our proposed regular-   ization is also able to reduce the gap from 4.3   to 2.2 Partial - F1 , which can be credited to both   self - supervised heuristic and unsupervised latent   attributes , as they both leverage the product context   mainly from bullet points .   To detach the impact of candidate generation ,   we provide additional views to assess the repre-   sentation learning and grouping performance . The   last column of Table 4 shows evaluation by using   gold values as candidate spans directly . It clearly   strengthens the advantage of our proposed repre-   sentation learning methods , as Amacer outperforms   OA - Mineby 10 + Partial - F1 .   Table 5 further evaluates span extraction of pre-   dicted values against gold values . All models are   shown quite low Exact - F1 scores ( < 37 ) and low   precision ( < 34 ) , leaving room for future improve-   ment to extract more correct candidate spans under   limited supervision .   9 Qualitative Analysis   Seed Attributes : our approach performs gener-   ally well on seed attribute types . Table 8 shows   examples of discovered new values on a seed type   Flavor Profile ( also see Table 15 ) . Amacer is able to   extract sensible and diverse expressions , given only   6 seed values as supervision . Each proposed com-   ponent makes evident contribution : the candidate   generation can capture unseen long - tail spans , such   asfloral with honey notes , delicate zesty , while   the representation learning and grouping together   are effective recognizing similar attribute values .   Nearly 80 new flavor values are identified on our   test set , expanding its vocabulary by 12 times .   New Attributes : it is inevitably difficult to dis-   cover values of new types , as models possess little   prior knowledge as regards . For error analysis , we   found that for most of these new types , their values   are either absent in the predictions , or grouped as   other existing attributes mistakenly . Table 6 shows12230   an example of the latter case ; however , it is still en-   couraging that these new values are extracted and   recognized as certain attributes , rather than being   neglected by the model , which partially achieves   the open - world discovery objective .   Latent Attributes : Table 7 shows examples of   learned latent attributes resulted by contrastive loss   and topic modeling . They resemble certain “ con-   cepts ” that regulate towards more attribute - friendly   embedding space . However , we also observe that   certain learned attributes are repetitive , such that   their attribute embeddings have high cosine similar-   ity . This behavior aligns with the previously discov-   ered issue known as topic collapsing ( Srivastava   and Sutton , 2017 ) , leading to deficient discovery .   We do not particularly address it in this work , andleave it for future research .   10 Conclusion   In this work , we present a new task setting as a prac-   tical solution to mine open - world attributes without   extensive human intervention . A new dataset is cre-   ated accordingly , and our proposed approach is   designed for light supervision , especially by utiliz-   ing a high - quality seed set , as well as exploiting   self - supervised and unsupervised semantic signals   from the context . Empirical results show that our   approach effectively improves discovery upon base-   lines on both existing and new attribute types .   11 Limitations   The scope of our approach is intended for our spe-   cific task setting , which is proposed as a practi-   cal solution to mine open - world attributes without   heavy supervision , and has not been studied pre-   viously . Our approach does require an external   dependency of a POS tagger , and assumes high   POS tagging quality on English . Thankfully , there   are POS tools publicly available with high perfor-   mance , and are quite robust against domain shift ,   mostly fulfilling the assumption .   Our current candidate generation that utilizes   syntax - oriented patterns does not check the seman-   tics , which can be another limitation . It introduces   noisy spans in the process , such as “ supports joint12231health & overall ” ( in Table 15 ) . Future works   could consider combining syntax with semantics   to alleviate noisy spans .   References1223212233A Previous Work   As the most related previous work to our proposed   task setting is OA - Mine ( Zhang et al . , 2022 ) , we   found that their released dataset is not ideal nor   practical to serve as the testbed for this setting , due   to three drawbacks :   •The seed attribute set is too sparse : there are   only five seed values provided for each attribute   type , leading to insufficient attribute extraction   and discovery .   •The seed attributes can be quite noisy ; espe-   cially , certain values appear under multiple at-   tribute types , presenting noise and ambiguity to   the model training ( example shown in Table 10 ) .   •The corpus only consists of product titles , and   lacks the full product description taxonomy such   as bullet points , which can provide richer in-   formation regarding attributes and also require   stronger inference capability . Detailed statistics   of bullet point description compared to titles are   provided in Table 9 .   Our dataset explicitly addresses above issues ,   and is constructed to provide higher quality and   richer context , as introduced in Section 2.B Dataset   Full statistics of our new dataset WMare pro-   vided in Table 11 . Our dataset is publicly available   under the Apache 2.0 License .   Corpus Our corpus consists of e - commerce prod-   uct description from selected product categories ,   collected under permissions . We do not find con-   cerns regarding privacy issues or discriminatory   content .   Product Profiles In addition , we also document   three detailed issues existed in product profiles that   are addressed in our seed set construction : data   sparsity , noisy attributes , coarse granularity . Thus ,   the raw profiles are unable to serve as the full su-   pervision directly for this attribute extraction task .   •Our preliminary study shows that 80 - 90 % human-   identified attribute values are missing from the   product profiles ; along with the missing values ,   around 40 % identified attribute types are also   absent in the profiles , which aligns with the pre-   vious observations from Zhang et al . ( 2022 ) . The   sparsity of product profiles further cultivates our   research motivation to enrich the product profiles   by discovering new attributes automatically .   •Attribute values resided in profiles can be quite   noisy , as there are no restrictions on what values   that sellers could provide regarding their prod-   ucts . In extreme cases , many irrelevant values   may be provided by sellers in efforts to boost   their product search performance , which can dis-   rupt the training and make the model insensible .   •Certain attributes may not be used directly due   to their coarse granularity . For example , an at-   tribute type STYLE can be too ambiguous for sell-   ers such that it essentially becomes a superset   of more fine - grained attribute values including   colors , flavors , visual styles , materials , etc.12234   C Experimental Settings   For representation learning , BERT - Large ( Devlin   et al . , 2019 ) is adopted as the encoder and we freeze   all layers except for the last four layers , allowing   for a larger batch size and faster training , which   we found performs similar to finetuning the entire   BERT . We use a batch size as 128 , learning rate as   2×10 , linear - decay learning rate scheduler with   warm - up ratio as 0.01 , max gradient clipping norm   as1 .   Other hyperparameters are searched on the de-   velopment set ; in our final Amacer model , we set   the temperature τ= 0.1 in the contrastive loss , and   the number of latent attributes K= 50 ( Section 6 ) .   In the final loss Eq ( 8) , we set λ= 0.01and   λ= 0.02 , regarding them as weak regulariza-   tion that mines additional semantic signals .   At the grouping stage , we set the relaxation δ=   0.8 in adaptive expansion Eq ( 2 ) . For DBSCAN ,   we use the implementation from sklearn , and set   eps as 0.05 , min_samples as 4 .   All training is conducted on a Nvidia Tesla V100   GPU with 32 GB memory , and takes around 1 hour   to finish each model .   D Quantitative Analysis   Full evaluation metrics are provided in Table 12   and 13 , in regard to the quantitative analysis in   Section 8 . In particular , Table 12 separately shows   the detailed evaluation results on existing seed at-   tribute types only or on new attribute types only .   Table 13 separately shows the detailed evaluation   results on product titles only , or on bullet pointdescription only .   Table 14 shows the full evaluation metrics when   using gold spans as candidate spans directly . Since   all resulting spans will be gold values , the evalua-   tion scores are the same for either partial - match or   exact - match.122351223612237ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 10 .   /squareA2 . Did you discuss any potential risks of your work ?   Our work is only intended for our task scope that extracts attributes from product description , and   only requires publicly available e - commerce data without targeting speciﬁc groups . Our work does   not require large computational resources neither . Therefore , we do n’t see risks directly related to   our work .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract , Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 2 , Section 6 , Appendix C.   /squareB1 . Did you cite the creators of artifacts you used ?   Section 6   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix B. We will also add speciﬁc copyright and terms upon dataset release .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Appendix B. Our dataset will be publicly available .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Appendix B.   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   We covered the domains and languages in Section 2 . Our dataset does not involve speciﬁc demo-   graphic groups .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 2 , Appendix B.12238C / squareDid you run computational experiments ?   Section 6 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix C.   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix C.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 6 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix C.   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 2 .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   We will publish these details upon dataset release .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Our annotators are in - house researchers involved in this work .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   We use in - house data collected under permissions .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   We have obtained approval from an ethics review board .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Our dataset consists of e - commerce products and does not have direct concerns regarding speciﬁc   groups / people.12239