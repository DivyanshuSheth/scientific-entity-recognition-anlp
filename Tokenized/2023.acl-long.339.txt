  Tassilo Klein   SAP AI Research   tassilo.klein@sap.comMoin Nabi   SAP AI Research   m.nabi@sap.com   Abstract   This paper presents miCSE , a mutual   information - based contrastive learning   framework that significantly advances the state-   of - the - art in few - shot sentence embedding .   The proposed approach imposes alignment   between the attention pattern of different views   during contrastive learning . Learning sentence   embeddings with miCSE entails enforcing   the structural consistency across augmented   views for every sentence , making contrastive   self - supervised learning more sample efficient .   As a result , the proposed approach shows   strong performance in the few - shot learning   domain . While it achieves superior results   compared to state - of - the - art methods on   multiple benchmarks in few - shot learning ,   it is comparable in the full - shot scenario .   This study opens up avenues for efficient   self - supervised learning methods that are more   robust than current contrastive methods for   sentence embedding .   1 Introduction   Measuring sentence similarity has been challenging   due to the ambiguity and variability of linguistic ex-   pressions . The community ’s strong interest in the   topic can be attributed to its applicability in numer-   ous language processing applications , such as sen-   timent analysis , information retrieval , and semantic   search ( Pilehvar and Navigli , 2015 ; Iyyer et al . ,   2015 ) . Language models perform well on these   tasks but typically require fine - tuning on the down-   stream task and corpora ( Reimers and Gurevych ,   2019 ; Devlin et al . , 2018 ; Pfeiffer et al . , 2020 ; Mos-   bach et al . , 2021 ) . In terms of sentence embed-   dings , contrastive learning schemes have already   been adopted successfully ( van den Oord et al . ,   2018 ; Liu et al . , 2021 ; Gao et al . , 2021 ; Carlsson   et al . , 2021 ) . The idea of contrastive learning is   that positive and negative pairs are generated givena batch of samples . Whereas the positive pairs   are obtained via augmentation , negative pairs are   often created by random collation of sentences .   Following the construction of pairs , contrastive   learning forces the network to learn feature rep-   resentations by pushing apart different samples   ( negative pairs ) or pulling together similar ones   ( positive pairs ) . While some methods seek to op-   timize for selecting “ hard ” negative for negative   pair generation ( Zhou et al . , 2022a ) , others investi-   gated better augmentation techniques for positive   pair creation . In this regard , many methods have   been proposed to create augmentations to boost   representation learning . Standard approaches for   the augmentation aim at input data level ( a.k.a dis-   crete augmentation ) , which comprises word level   operations such as swapping , insertion , deletion ,   and substitution ( Xie et al . , 2017 ; Coulombe , 2018 ;   Wei and Zou , 2019 ) . In contrast to that , contin-   uous augmentation operates at the representation   level , comprising approaches like interpolation or   “ mixup ” on the embedding space ( Chen et al . , 2020 ;   Cheng et al . , 2020 ; Guo et al . , 2019 ) . Most re-   cently , augmentation was also proposed in a more   continuous fashion operating in a parameter level   via simple techniques such as drop - out ( Gao et al . ,   2021 ; Liu et al . , 2021 ; Klein and Nabi , 2022 ) or ran-   dom span masking ( Liu et al . , 2021 ) . The intuition   is that “ drop - out ” acts as minimal data augmenta-   tion , providing an expressive semantic variation .   However , it will likely affect structural alignment   across views . Since positive pairs are constructed   from identical sentences , we hypothesize that the   structural dependency over the views should be pre-   served by utilizing drop - out noise . Building on this   idea , we maximize the structural dependence by   enforcing distributional similarity over the atten-   tion values across the augmentation views . To this   end , we employ maximization of the mutual infor-   mation ( MI ) on the attention tensors of the positive   pairs . However , since attention tensors can be very6159high - dimensional , computing MI can quickly be-   come a significant burden if not intractable . This   paper proposes a simple solution to alleviate the   computational burden of MI computation , which   can be deployed efficiently . Similar to ( Fan et al . ,   2020 ) , we adopt the Log - Normal distribution to   model attention . Empirical evidence confirms this   model as a good fit while facilitating the optimiza-   tion objective to be defined in closed form . In   this case , mutual information can be provably re-   formulated as a function of correlation , allowing   native GPU implementation . As discussed above ,   the proposed approach builds upon the contrastive   learning paradigm known to suffer from model col-   lapse . This issue becomes even more problematic   when enforcing MI on the attention level , as it tight-   ens the positive pairs via regularizing the attention .   Therefore the selection of negative pairs becomes   more critical in our setup . To this end , we utilize   momentum contrastive learning to generate harder   negatives ( He et al . , 2020 ) . A “ tighter ” binding on   positive pairs and repulsion on ” harder ” negative   pairs empowers the proposed contrastive objective ,   yielding more powerful representations .   Combining ideas from momentum contrastive   learning and attention regularization , we propose   miCSE , a conceptually simple yet empirically pow-   erful method for sentence embedding , with the   goal of integrating semantic and structural informa-   tion of a sentence in an information - theoretic and   Transformer - specific manner . We conjecture the   relation between attention maps and a form of syn-   tax to be the main driver behind the success of our   approach . We speculate that our proposed method   injects structural information into the model as an   inductive bias , facilitating representation learning   with fewer samples . The adopted structural induc-   tive biases provide a “ syntactic ” prior as an implicit   form of supervision during training ( Wilcox et al . ,   2020 ) , which promotes few - shot learning capabil-   ities in neural language models . To validate this ,   we introduced a low - shot setup for training sen-   tence embeddings . In this benchmark , we finetune   the language model only with a small number of   training samples . Note that this is a very chal-   lenging setup . The inherent difficulty can be at-   tributed to the need to mitigate the domain shift   in the low - shot self - supervised learning scheme .   We emphasize the importance of this task , as in   many real - world applications , only small datasets   are often available . Such cases include NLP forlow - resource languages or expert - produced texts   ( e.g. , medical records by doctors ) , personalized LM   for social media analysis ( e.g. , personalized hate   speed recognition on Twitter ) , etc . Our proposed   method significantly improves over the state - of - the-   art in the low - shot sentence embedding benchmark .   This is the first work that explores how to combine   semantic and structural information through atten-   tion regularization and empirically demonstrates   this benefit for low - shot sentence embeddings .   Previous works : Recently , VaSCL ( Zhang et al . ,   2022a ) , ConSERT ( Yan et al . , 2021a ) , PCL ( Wu   et al . , 2022a ) and ( Chuang et al . , 2022 ) proposed   contrastive representation learning with diverse   augmentation strategies on positive pair . However ,   we proposed a principled approach for enforcing   alignment in positive pairs at contrastive learning   without discretely augmenting the data . Similar to   us , ESimCSE ( Wu et al . , 2021 ) and MoCoSE ( Cao   et al . , 2022a ) proposed to exploit a momentum   contrastive learning model with negative sample   queue for sentence embedding to boost uniformity   of the representations . However , unlike us , they   do not enforce any further tightening objective on   the positive pairs nor consider few - shot learning .   Very recently , authors in InforMin - CL ( Chen et al . ,   2022 ) and InfoCSE ( Wu et al . , 2022b)proposed   information minimization - based contrastive learn-   ing . Specifically , the authors propose to minimize   the information entropy between positive embed-   dings generated by drop - out augmentation . Our   model differs from this paper and the method in   ( Bachman et al . , 2019 ; Yang et al . , 2021 ; Zhang   et al . , 2020 ; Sordoni et al . , 2021 ; Wu et al . , 2020 ) ,   which focuses on using mutual information for self-   supervised learning . A key difference compared to   these methods is that they estimate MI directly on   the representation space . In contrast , our method   computes the MI on attention . Other related work   include ( Zhang et al . , 2022b ; Zhou et al . , 2022b ;   Zhang et al . , 2022c ; Liu et al . , 2022 ) .   The contributions of the proposed work are :   First , we propose to inject structural information   into language models by adding an attention - level   objective . Second , we introduce Attention Mu-   tual Information ( AMI ) , a sample - efficient self-   supervised contrastive learning . Third , we intro-   duce low - shot learning for sentence embedding .   We show that our method performs comparably   to the state - of - the - art in the full - shot scenario and   significantly better in few - shot learning.61602 Method   The proposed approach aims to exploit the structure   of the sentences in a contrastive learning scheme .   Compared to conventional contrastive learning that   solely operates at the level of semantic similarity   in the embedding space , the proposed approach in-   jects structural information into the model . This is   achieved by regularizing the attention space of the   model during training . We let Ddenote a dataset   consisting of string sequences ( sentences ) from cor-   pusXwithD={x , x , ... , x } , where we as-   sume xto be a tokenized sequence of length nwith   x∈N. For mapping the input data to the embed-   ding space , we use a bi - encoder fparametrized   byθ . Bi - encoders entail the computation of em-   beddings for similarity comparison , whereby each   sentence in a pair is encoded separately . Hence , the   instantiation of a bi - encoder on augmented input   data induces multiple views . For the following , we   letv∈ { 1 , 2}denote the index of the view , where   each view corresponds to a different augmentation .   Consequently , encoding a data batch Dyields em-   bedding matrices E∈R , where Udenotes   the dimensionality of the embeddings . Employing   a Transformer , encoding the input data yields the   embedding matrices and the associated attention   tensors W. Then learning representation of the   proposed approach entails the optimization of a   joint loss :   minL(E , E ) + L(W , W ) ( 1 )   with(E , W ) , ( E , W ) = f(D ) . Here , Lis   responsible for the semantic alignment , correspond-   ing to the standard InfoNCE ( van den Oord et al . ,   2018 ) loss that seeks to pull positive pairs close   together while pushing away negative pairs in the   embedding space . In contrast , Lis responsible   for the syntactic alignment , operating on the at-   tention space . However , in comparison to Lis   employed only on positive pairs ’ attention tensors .   2.1 Embedding - level Momentum - Contrastive   Learning ( InfoNCE )   The InfoNCE - loss seeks to pull positive pairs to-   gether in the embedding space while pushing nega-   tive pairs apart . Specifically , InfoNCE on embed-   dings pushes for the similarity of each sample and   its corresponding augmented embedding . Nega-   tives pairs are constructed in two ways , reflected by   the two terms in the denominator of Eq . 2 . First , in-   batch negative pairs are constructed by pairing eachsentence with another random sentence ( sharing no   semantic similarity ) , pushing for dissimilarity . Sec-   ond , using embeddings obtained from a momentum   encoder known as MoCo ( He et al . , 2020 ; Cao et al . ,   2022a ) . The momentum encoder is a replication   of the encoder f , whose parameters are updated   more slowly . Specifically , while the parameters of   fencoder are updated via back - propagation , the   parameters of the momentum encoder are updated   using an exponential moving average from the for-   mer . The negative embeddings are produced from   samples from previous batches , which are stored   in queue Qand are forward - passed through the   momentum encoder . Then the InfoNCE ( van den   Oord et al . , 2018 ) loss ( L)is defined as :   −/summationdisplaylogd(e , e)/summationtextd(e , e ) + /summationtextd(e , q),(2 )   where e∈Eande∈Edenote the embed-   dings of different augmentations of x. Further-   more , d(x , y ) = exp ( sim(x , y)/τ)withsim ( . )   the cosine similarity metric , qdenoting repre-   sentations obtained from momentum encoder , and   τ∈Ris a temperature scalar .   2.2 Attention - level Mutual Information ( AMI )   Preliminaries and notations : We first briefly   review the attention mechanism and explain   the notation used in the rest of this section . A   Transformer stack consists of a stack of Llayers ,   with input data cascading up the layer stack .   Each layer comprises a self - attention module   and a feed - forward network in its simplest form .   Passing sentences through the encoder stack entails   simultaneous computation of attention weights .   These attention weights indicate the relative   importance of every token . To this end , key - value   pairs are computed for each token of the input   sequence within each self - attention module . This   entails the computation of three different matrices :   key matrix K , value matrix V , and query matrix Q.   The values of the attention weights Ware obtained   according to W= softmax ( f(Q , K))∈R ,   where f(.)is a scaled dot - product . Output features   are then generated as obtained according to WV .   To attend to different sub - spaces ( Vaswani et al . ,   2017 ) simultaneously , the attention mechanism   is replicated Htimes , referred to as multi - head   attention . During training the encoder , the self-   attention tensors Wvalues are subject to a random6161   deterministic process , with randomness arising due   to drop - out . Hence , the proposed approach seeks   to optimize structural alignment by maximizing   mutual information between the attention tensors   W= [ w , ... , w]of the augmentation views .   We propose a four - step pipeline to regularize the   joint attention space . For a schematic illustration   of the AMI pipeline , see Fig . 1 .   1 ) Attention Tensor Slicing : Given that augmen-   tation has different effects on the attention distri-   bution depending on the depth ( layer ) and the po-   sition ( head ) in the Transformer stack , we propose   to slice the attention tensor . Chunking the atten-   tion has multiple advantages . On the one hand ,   this allows for preserving the locality of distribu-   tion change . This is important as it can be empiri-   cally observed that distribution divergence between   views decreases with increasing depth in the en-   coding stack . On the other hand , restricting the   space permits using a simple distributional model   such as bivariate distribution compared to a mixture   distribution for the whole stack .   For the sake of economy in notation and   avoid notational clutter , we will restrict the   attention tensor of a single encoded sample in   the following . To this end , a slicing function   π : R→Rcuts the attention   tensor for each input sample into R(indexed )   elements : π(w ) = [ w , ... , w]∈Rwith   w= ( w)andr∈R. For a schematic   illustration of how the attention tensor is sliced   into tiles , see Fig . 2.2 ) Attention Sampling : Different sentences in   the batch are typically in token sequences of differ-   ent lengths . To accommodate the different lengths   and facilitate efficient training , sequences are typ-   ically padded with [ PAD ] -token for length equal-   ity . Although this allows for efficient batch encod-   ing on GPU , attentions arising from [ PAD ] -tokens   have to be discarded when looking at statistical   relationships . To accommodate for the different   lengths of tokenized sequences , perform a sam-   pling step for attention values within each grid cell   w. To this end , we leverage multinomial distribu-   tionP(p , .. , p ) , where scorrespond to the6162   number of non - padding tokens with 1≤s≤n .   Specifically , we sample from the sattention val-   ues pool , each with a probability of , with the   remaining elements associated with probability 0 .   As a result , we obtain a set J={j , ... , j}con-   sisting of mindices of the attention tensors for each   slicer∈R :   J∼P ( 1 / s , ... , 1 /s   /bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright ,   /bracehtipdownleft / bracehtipupright / bracehtipupleft / bracehtipdownright   0 , ... , 0 ) ( 3 )   It should be noted that for the same slice racross   the views , the same index set is used for sampling :   ˜w=/uniontextw[j]and˜w=/uniontextw[j ] .   3 ) Attention Mutual Information Estimation :   We propose using mutual information to measure   the similarity of attention patterns for different   views . Specifically , we follow ( Fan et al . , 2020 )   and adopt the Log - Normal distribution for mod-   eling the attention distribution , which is prudent   for several reasons . First , Empirical observation   confirms attention asymmetry . Second , by utilizing   a non - symmetric distribution , it becomes possible   to break down the attention tensor WintoKand   Q , thereby allowing for non - symmetrical attention .   Third , adopting the log - normal models facilitates   the optimization objective to be defined in closedform and hence easy to optimize , particularly on   GPUs . Mutual information for two normally dis-   tributed tuple vectors ( z , z)can be written as a   function of correlation ( I.M. and A.M. , 1957 ):   I(z , z ) = −1   2log(1−ρ ) ( 4 )   where ρcorresponds to the correlation coefficient   computed from from zandz . Hence , we com-   pute the mutual information for each slice rand   sample xasMI = I(log ( ˜w ) , log(˜w ) ) . The   log(.)function accommodates the Log - Normal   to Normal random variable transformation . For   details on the implementation , see Alg . 1 .   4 ) Mutual Information Aggregation : To com-   pute the loss component for attention regulariza-   tion , we need to aggregate the distributional similar-   ities for the entire tensor . Aggregation is obtained   by averaging the individual similarities obtained   for each slice r∈Rand each sample xin the   batch . With λ∈Rsome weighting scalar , the   attention alignment loss term is :   L(W , W ) = −λ   |R| · |D|/summationdisplay / summationdisplayMI(5 )   3 Experiments   In this section , we describe the experimental setting   used for the evaluation , present our main results ,   and discuss different aspects of our method by pro-   viding several empirical analyses .   3.1 Experimental Setup   Model and Hyperparameters : Training is started   from a pre - trained transformer LM . Specifically ,   we employ the Hugging Face ( Wolf et al . , 2020 )   implementation of BERT . For each approach   evaluated , we follow the same hyperparameters pro-   posed by the authors . In the InfoNCE loss , we set   τ= 0.05 . In order to determine the hyperparam-   eterλa coarse grid search { 1.0 , 0.1 , ... , 1.0 e−5 }   was conducted to assess the magnitude . Upon de-   termination , a fine grid search was conducted once   with 10 steps . We set λ= 2.5 e−3for training   100 % of the data in a single episode with a batch   size of 50 at a learning rate of 3.0e−5and250   warm - up steps . The number of optimization steps   is kept constant for training the different dataset   sizes . For the training set of size 10(= 100 % ) ,   we train for 1 epoch ; for the size of 10(= 10 % ) , 6163   we train for 10 epochs , etc . The training was con-   ducted using an NVIDIA V100 with a training time   of around 1.5h . The overall GPU budget from ex-   perimentation and hyperparameter optimization is   estimated to be around 500 GPU / hours . The mo-   mentum encoder is associated with a sample queue   of size |Q|= 384 . The momentum encoder param-   eters are updated with a factor of 0.995 , except for   the MLP pooling layer , which is kept identical to   the online network . Additionally , we increase the   drop - out for the momentum encoder network from   the default rate ( 0.1 ) to0.3 .   Data and Evaluation : Following ( Gao et al . ,   2021 ) , we train the model unsupervised on sen-   tences from Wikipedia . We create random sample   sets of different sizes { 10 , 10 , 10 , 5.0·10 , 10 }   to train the model in a few - shot learning scenario .   We repeated the training set creation for each size   5 times with different random seeds .   Mutual Information Estimation : Following the   observations in ( V oita et al . , 2019 ) , we restrict the   computation of the mutual information to the up-   per part of the layer stack . Specifically , we se-   lect the layers between 8 and 12 (= last layer inBERT ) . To accommodate input sequences of   varying lengths and make computation more ef-   ficient , we pool together pairs of adjacent heads   ( without overlap ) while preserving the layer sepa-   ration . From each of the ( 4× ) chunks of pooled   attentions , we random sample 150joint - attention   pairs for each embedding of the bi - encoder .   3.2 Experimental Results   Unsupervised Sentence Embedding : We com-   pare miCSEto previous state - of - the - art sentence em-   bedding methods on STS tasks . For comparisons ,   we favored comparable architectures ( bi - encoder )   that facilitate seamless integration of the proposed   approach and methods of comparable backbone .   We also added methods that employ explicit dis-   crete augmentation to provide a full picture of ex-   isting techniques for sentence embedding .   For semantic text similarity , we evaluated on   7 STS tasks : ( Agirre et al . , 2012 , 2013 , 2014 ,   2015 , 2016 ) , STS Benchmark ( Cer et al . , 2017 )   and SICK - Relatedness ( Marelli et al . , 2014 ) . These   datasets come in sentence pairs with correlation   labels in the range of 0 and 5 , indicating the se-6164Algorithm 1 Mutual Information estimation   Input : BatchD , encoder f , multinomial sampler p   Output : Average mutual information / summationtextMI   ( E , W ) , ( E , W)←f(D ) ▷Transformer encoding creating views   fori←1 ... |D|do   w , w← ( W , W , i ) ▷Extract attention tensor for each sample   { w , ... , w } ←π(w ) ▷Slicing the attention tensors   s←number of text tokens in x   forr←1 ... |R|do   J←p(1 / s , ... 1 / s , 0 ) ▷Sampling indices of valid attentions   MI←AMI(/uniontextw[j],/uniontextw[j ] )   end for   end for   procedure AMI(w , w )   z , z←log(w ) , log(w ) ▷Log - Normal to Normal transform   ρ←cos(z−¯z , z−¯z ) ▷Compute correlation coefficient on centered   attentions   Return −(1−ρ ) ▷Mutual information for tensor slice   end procedure   mantic relatedness of the pairs . Specifically , we   employ the SentEval toolkit ( Conneau and Kiela ,   2018 ) for evaluation . All our STS experiments   are conducted in a fully unsupervised setup , not   involving any STS training data . The benchmark   measures the relatedness of two sentences based   on the cosine similarity of their embeddings . The   evaluation criterion is Spearman ’s rank correlation   ( ρ ) . For comparability , we follow the evaluation   protocol of ( Gao et al . , 2021 ) , employing Spear-   man ’s rank correlation and aggregation on all the   topic subsets . Results for the sentence similarity   experiment are presented in Tab . 1 . As can be   seen , the proposed approach is slightly lower in   terms of average performance than state - of - the - art   algorithms such as DiffCSE . However , it should   be noted that these aforementioned methods use   extensive discrete augmentation techniques , such   as word repetition , deletion , and others , while the   proposed method in this work does not employ   any form of discrete data augmentation . This ren-   ders the proposed method more general and less   ad - hoc in nature . While it is technically feasible for   our method to incorporate discrete augmentation ,   it was deliberately excluded in this study for the   sake of generalization with the intention of further   exploration in future research . A more in - depth   analysis shows the best performance on the SICK-   R benchmark , where it outperforms the second - bestapproach SCD by ( +0.44 ) and third - best PCL by   ( +0.87 ) . We highlight the comparison to the clos-   est method SimCSE , where the proposed approach   has an average gain of ( +3.94 ) . This improvement   is due to the two additional components ( i.e. , AMI   and MoCo ) we add to this baseline method .   Low - shot Sentence Embedding : In this experi-   ment , the performance of several SOTA sentence   embedding approaches is benchmarked elabora-   tively . Similar to Sec . 3.2 , we evaluate 7 STS   tasks , STS Benchmark , and SICK - Relatedness   with Spearman ’s ρrank correlation as the evalu-   ation metric . However , in contrast to the previous   section , models are trained on different subsets of   the data , namely { 100 % , 10 % , 1 % , 0.1 % } of the   Wikipedia dataset used in ( Gao et al . , 2021 ) . Re-   sults for the low - shot sentence similarity experi-   ment can be presented in Fig . 3 . As can be seen , the   proposed approach gains by increasing the training   set size and consistently outperforms all the base-   lines in all training subsets . Interestingly , our pro-   posed method reaches the performance of SimCSE   trained on the entire dataset with only 0.5 % of the   data . We believe it shows the impact of exploiting   structural information for data augmentation during   training . It should be noted that the performance   gain is most significant when conducted on a single   token rather than token averaging . We attribute   this to token averaging , which to a certain degree,6165Semantic Textual Similarity   Model 0.1 % 1 % 10 % 100 %   CT ( Carlsson et al . , 2021 ) 68.46±2.33 66.21 ±4.06 72.06 ±1.46 72.69   AMI+CT 71.12±1.11 72.20±0.49 73.20±0.78 73.55   Mirror - BERT ( Liu et al . , 2021 ) 40.13±5.08 42.17 ±1.69 42.47 ±3.66 43.32   AMI+Mirror - BERT 43.99±1.26 45.26±2.60 44.72±1.36 47.48   Mirror ( avg . ) ( Liu et al . , 2021 ) 71.48±1.19 71.80 ±1.18 70.38 ±1.18 69.81   AMI+Mirror - BERT ( avg . ) 71.49±0.95 72.54±0.49 70.68±1.19 71.34   SimCSE ( Gao et al . , 2021 ) 67.94±1.16 74.96 ±0.65 75.76 ±0.24 76.15   AMI+SimCSE 73.85±0.49 76.21±0.28 76.31±0.46 76.88   miCSE 73.68±0.89 76.40±0.48 76.38 ±0.35 78.13   is equivalent to attention regularization . On the   extremely low data regime , the proposed approach   shows very strong performance up ( +11 ) compared   to SimCSE - see Fig . 4a . It suggests resilience of   our method to very small batch training .   3.3 Experimental Analysis of components   Given that AMI is a regularizer on Transformer   attention , we evaluate the applicability in conjunc-   tion with other contrastive learning methods . We   evaluate the following approaches CT ( Carlsson   et al . , 2021 ) , Mirror - BERT ( Liu et al . , 2021 ) , and   SimCSE ( Gao et al . , 2021 ) . Evaluation is con-   ducted on 7 STS tasks , STS Benchmark , and SICK-   Relatedness with Spearman ’s ρrank correlation as   a metric . Results for the low - shot sentence simi-   larity experiment are presented in Tab . 2 . As can   be seen , our proposed AMI can boost the perfor - mances of all approaches in all settings . Addition-   ally , it shows the most significant boost in perfor-   mance in combination with SimCSE . In addition ,   we observe that the impact of AMI grows with de-   clining training set size . Combined with SimCSE ,   AMI leads to a performance gain of up to ( +5.91 )   at0.1 % of the data . We also observe that adding   AMI to all the approaches significantly reduces the   variance for all methods . This can probably be   attributed to the regularization effect of the pro-   posed AMI component . In addition , we conducted   an ablation study to assess the effect of AMI and   MoCo w.r.t . the baseline SimCSE - see Tab . 3 .   As shown in Fig . 4b , AMI and MoCo improve the   baseline at different data ratios . Again , AMI pro-   vides a particularly strong performance boost in   the low - data regime . In contrast , the impact of   MoCo diminishes with decreasing training set size.6166Semantic Textual Similarity   Model 0.1 % 1 % 10 % 100 %   SimCSE ( Gao et al . , 2021 ) 67.94±1.16 74.96 ±0.65 75.76 ±0.24 76.15   AMI+SimCSE 73.85±0.49 76.21±0.28 76.31 ±0.46 76.88   MoCo+SimCSE 69.54±1.61 75.73 ±0.91 76.73±0.29 76.81   miCSE 73.68±0.89 76.40±0.48 76.38±0.35 78.13   We emphasize that our approach gets the best of   both worlds by integrating these two components .   This can be directly exploited for different few - shot   setups by adjusting the hyper - parameter λ .   Discussion on the Structure andAttention : The   proposed approach aligns the attention patterns   for drop - out augmented input pairs . We posit   that conducting such a regularization enforces con-   straints w.r.t . the structure ( e.g. , syntax ) of the   sentence embeddings . This is motivated by recent   literature findings , which suggest that the Trans-   former 's attention captures structural information   such as syntactic grammatical relationships of the   sentences ( Ravishankar et al . , 2021 ; Clark et al . ,   2019 ; Raganato et al . , 2018 ; V oita et al . , 2019 ) .   Additionally , recent research explicitly targets the   extraction of topologies from attention maps for   diverse tasks on syntactic and grammatical struc-   ture ( Kushnareva et al . , 2021 ; Cherniavskii et al . ,   2022 ; Perez and Reinauer , 2022 ) . Although no   “ one - to - one ” mapping connects syntactic structures   and attention patterns , the attention tensor , at the   bare minimum , encodes a “ holistic notion ” of the   syntactic structure of sentences . While this study   refrains from making any definitive claim on the   matter , a preliminary analysis wrt . role of syntax in   our proposed method is conducted ( see Appendix ) .   Discussion on the discrete argumentation : Dis-   crete augmentation serves as a suitable strategy   for expanding datasets to enhance learning robust-   ness and partially address the issue of data scarcity .   Although augmentation contributes to improved ro-   bustness , additional measures are required to tackle   the information gap challenge in few - shot learning   scenarios . Therefore , our current study deliber-   ately excluded discrete augmentation to minimize   any interference it may have with our low - shot   learning algorithm . The primary rationale behind   this decision is that while discrete augmentation   is known to alleviate data scarcity by replicatingmissing information , it often leads to a superficial   correlation between test and training data , rather   than enhancing the model ’s few - shot learning ca-   pability . Consequently , we excluded augmentation   to maintain control over miCSE ’s behavior and val-   idate its effectiveness without any negative con-   sequences . The significant superiority of miCSE   over augmentation - based approaches ( such as Dif-   fCSE ) in the low - shot setup is evident from Fig .   3 . Nevertheless , the proposed approach inherently   facilitates the integration of discrete augmentation ,   offering the potential to enhance results in both   few and full - shot learning scenarios . However , it is   crucial to acknowledge that their structural similari-   ties must be respected when applying augmentation   strategies to positive pairs . One promising option is   to utilize the augmentation strategies proposed by   ESimCSE ( Wu et al . , 2021 ) , which involve word   duplication anddeletion to address length biases .   This can be followed by enforcing AMI on the   shared attention subspaces of the augmented in-   stances . Although we do not explore this approach   in our current paper , it presents an intriguing av-   enue for future research .   4 Conclusion   We proposed a method to inject structural similarity   into language models for self - supervised represen-   tation learning for sentence embeddings . The pro-   posed approach integrates the inductive bias at the   level of Transformer attention by enforcing mutual   information on positive pairs obtained by drop - out   augmentation . Leveraging attention regularization   makes the proposed approach much more sample   efficient . Consequently , it outperforms methods   with a significant margin in low - shot learning sce-   narios while having state - of - the - art performance in   full - shot to comparable approaches.61675 Limitations   The proposed AMI component is effective in the   low - data regime but can not be generalized to all   cases . Future work will investigate the role of syn-   tax in the structural regularization of attention and   the extension of the proposed approach to discrete   augmentation .   References616861696170   A Appendix   In the following sections , we add additional details   omitted in the main paper due to space restrictions .   First , we show an analysis of the relationship be-   tween syntactic structure and semantics . Next , we   illustrate the cosine similarity distribution accord-   ing to human judgment ( ground truth ) in Sec . C.   Next , in Sec . D , we visualize the 2D histogram   of joint distributions between views . In Sec . E ,   we present detailed results of the few - shot perfor-   mance of miCSE in contrastive and non - contrastive   setup . Finally , the exact relation between mutual   information and correlation is presented in Sec . F.   B Analysis on Structure vs. Semantic   In light of the lack of a rigorous benchmark for   analyzing structure(syntax ) in sentence embedding ,   we performed two qualitative analyses visualized   in Fig . 5 and Fig 6 .   Let us consider the following three sentences   and their linearized syntax tree to understand bet-   ter the notions of negatives and ( dis-)similar syntax .   Anchor / Positive :   Life is good   Negative ( similar Syntax ):   Good is expensive   Negative ( dissimilar Syntax ):   Live a good life   For each sentence , we computed the depen-   dency tree . Subsequently , we linearize the tree   structure for comparison , as can be done with tools   such as spaCy . Positive samples have an identical   tree and negative samples have non - identical trees   with their part - of - speech tags :   Anchor / Positive :   nsubj(1,0 ) – ROOT(1,1 ) – acomp(1,2 ) –   punct(1,3 ) .   Negative ( similar Syntax ): nsubj(1,0 ) – ROOT(1,1 ) – acomp(1,2 ) –   punct(1,3 ) .   Negative ( dissimilar Syntax ):   ROOT(0,0 ) – det(3,1 ) – amod(3,2 ) –   npadvmod(0,3 ) – punct(0,4 ) .   Herensubj corresponds to " nominal sub-   ject,"acomp to " adjectival complement , " detto   " determiner , " npadv to " noun phrase as adverbial   modifier " and punct to " punctuation . "   Our empirical observations are :   Observation ( i ) There is a higher semantic and   syntactic similarity between positive pairs com-   pared to the negative pairs : Our contrastive learn-   ing approach assumes that positive pairs exhibit   more syntactic similarity than negative pairs ( i.e. ,   syntactic inductive bias ) . To validate this hypothe-   sis , we plot the semantic similarity against syntac-   tic similarity for both positive and negative pairs .   Specifically , we analyzed the embeddings and atten-   tion values of the trained model with SimCSE and   the proposed approach . Input to the models was   randomly sampled sentences from Wikipedia . In-   terestingly enough , although training the proposed   model involves maximization of MI over the at-   tention w.r.t . positive pairs , we also observe the   reflection of syntactic information in the negative   pairs . As shown in Fig . 5 , the negative pairs end   up in the low left corner , whereas the positive pairs   are in the upper right corner .   Observation ( ii ) : Negative pairs with similar   syntax show higher attention similarity , compared   to pairs with dissimilar syntax : For a more in - depth   analysis of this , we further sub - divided the nega-   tive pairs into two groups : a)negative pairs with   similar dependency trees , b)negative pairs with   dissimilar dependency trees . For simplicity , we   adopted a binary similarity scheme - “ similar ” im-   plies an identical dependency tree , whereas “ dis-   similar ” corresponds to a non - identical dependency   tree . To highlight the inter - group syntax similarity ,   samples of each group were normalized w.r.t . the   centroid of the opposite group . As shown in Fig 6   ( by the increased distance between the cluster cen-   ters ) , the proposed approach encodes a notion of   syntactic similarity . Note that this margin appeared   solely due to enforcing the AMI on attention for   the positive pairs , leading to a notion of “ syntax ”   on negative pairs.6171   C Cosine - similarity Distribution   To directly show the strengths of our approaches   on STS tasks , we illustrate the cosine similarity on   embeddings distributions of STS - B pairs in com-   bination with human ratings in Fig . 7 . The STS   dataset comes in sentence pairs with correlation   labels in the range of 0 and 5 , indicating the seman-   tic relatedness of the pairs . Here , the x - axis is the   sample similarity of sentences according to human   judgment ( ground truth ) , and the y - axis represents   the cosine similarity between pairs using embed-   dings . Color coding corresponds to ground - truth   similarity . Compared to the baseline model ( Sim-   CSE ) , miCSE better distinguishes sentence pairs   with different levels of similarities , as can be seen   from the stronger correlation between embedding   distance and human rating . This property leads to   better performance on STS tasks . In addition , we   observe that miCSE generally shows a more scat-   tered distribution while preserving a lower variance   on semantically similar sentence pairs . This obser-   vation further validates that miCSE can potentially   achieve a better alignment - uniformity balance . D Visualization of Joint Distribution   To analyze the impact of the proposed approach   compared to the baseline SimCSE at the attention   level , we visualized the joint distribution of the at-   tention values created by the two views created by   the bi - encoder . The joint distribution and mutual   information are closely related . More specifically ,   given two random variables XandY , the associ-   ated mutual information can be expressed in terms   of the joint distribution as :   I(X , Y ) = /summationdisplayp(x , y ) logp(x , y )   p(x)p(y),(6 )   where p(x , y)denotes the joint - distribution and   p(x),p(y)the marginals . Assuming random vari-   ables are normally distributed , the joint distribution   of random variables is distinctly shaped depending   on the correlation coefficient ρ . See Sec . F details   on the relationship between entropy and the cor-   relation coefficient . In the extreme case of totally   unrelated marginals ρ= 0 , the joint distribution   assumes a circular shape having the lowest possi-   ble mutual information . On the other end of the6172   spectrum , in the case of perfect correlation , the   joint distribution assumes collinearity ( 45diago-   nal ) , with mutual information assuming maximal   value . We sliced the attention tensor into 12 slices   to avoid visual clutter , pooling together every 3   adjacent heads and every 4 adjacent layers . Slicing   the tensor at a higher resolution leads to visually   very similar results . The axes of the joint distribu-   tion ( 2d histogram ) correspond to the marginals ’   distribution . As miCSE maximizes the mutual infor-   mation , one can observe a reduction in the scatter   of the joint distribution compared to SimCSE .   E Detailed Comparison with SimCSE   Our proposed method is built on top of contrastive   learning . Thus it intrinsically relies on the exis-   tence of the negative pairs . To complement the   performance comparison of contrastive learning   in Fig . 4a , we designed an experiment to analyze   the extent to which attention regularization alone   ( AMI ) can compensate for the lack of negative   pairs . To that end , we conducted training with pos-   itive pairs only . See Tab . 4 and Fig . 9 for results .   The integration of mutual attention information   boosts the performance by up to ( +15 ) across all   training set sizes . It suggests the potential appli-   cation of our proposed attention regularization for   non - contrastive learning .   F Bivariate Normal Mutual Information   General Log - Normal Properties : Similar to   the normal distribution , the log - normal distribu-   tionlogN(w|µ,σ)has two parameters µand   σcapturing mean and variance . It follows that ap-   plying the logtransformation on a random variable   w , we yield random variable z= log ( w ) , which isnormally distributed : z∼ N(µ,σ ) .   Mutual Information : Given a vectors of tuples   ( X , X)containing i.i.d . points sampled the   joint bivariate normal distribution of p(A , B ) =   N(µ , Σ ) withµ∈R , Σ∈R. It can be   shown that there exists an exact relationship be-   tween mutual information and the correlation co-   efficient ρ(I.M. and A.M. , 1957 ) derived from X   andX. To that end , we expand the notation :   µ=/parenleftbig   µµ/parenrightbig   , Σ = /parenleftbiggσρσσ   ρσσσ / parenrightbigg   ( 7 )   The marginal and the joint entropy terms for   Gaussian distributed variables can be written as :   H(X ) = 1   2log(2πeσ ) =   1   2 + 1   2log(2π ) + log ( σ),i∈ { 1 , 2}(8 )   H(X , X ) = 1   2log / bracketleftbig   ( 2πe)|Σ|/bracketrightbig   =   1 + log(2 π ) + log ( σσ ) +1   2(1−ρ).(9 )   Given that Mutual Information can be written in   terms of entropy as :   I(X , X ) = H(X ) + H(X)−H(X , X )   ( 10 )   Then it follows by inserting Eq . 8,9 in Eq . 10 :   I(X , X ) = −1   2(1−ρ ) ( 11)6173   Semantic Textual Similarity   Model 0.1 % 1 % 10 % 100 %   SimCSE ( with negatives ) 66.69±1.03 74.08 ±0.81 75.01 ±0.23 76.15   ∗miCSE(with negatives ) 73.85±0.49 76.21 ±0.28 76.31 ±0.46 78.13   SimCSE ( w/o negatives ) 43.02±4.48 41.30 ±1.63 42.56 ±6.87 40.18   ∗miCSE(w / o negatives ) 57.00±1.32 56.41 ±3.38 53.38 ±4.70 54.3461746175ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 5   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . No potential risk   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Grammarly for grammar correction and spelling correction   B / squareDid you use or create scientiﬁc artifacts ?   Section 3.1 , 3.2   /squareB1 . Did you cite the creators of artifacts you used ?   Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . All open - source   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3.2   C / squareDid you run computational experiments ?   Section 3.1 + Section 3.2   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 3.1 + Section 3.26176 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 3.1 + Section 3.2   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 3.1 + Section 3.2   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Not applicable . Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.6177