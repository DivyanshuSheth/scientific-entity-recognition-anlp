  David Ifeoluwa Adelani , Jesujoba Oluwadara Alabi , Angela Fan , Julia Kreutzer ,   Xiaoyu Shen , Machel Reid , Dana Ruiter , Dietrich Klakow , Peter Nabende ,   Ernie Chang , Tajuddeen R. Gwadabe , Freshia Sackey , Bonaventure F. P. Dossou ,   Chris Chinenye Emezue , Colin Leong , Michael Beukman ,   Shamsuddeen H. Muhammad , Guyo D. Jarso , Oreen Yousuf , Andre N. Rubungo ,   Gilles Hacheme , Eric P. Wairagala , Muhammad U. Nasir , Benjamin A. Ajibade ,   Tunde Oluwaseyi Ajayi , Yvonne Wambui Gitau , Jade Abbott , Mohamed Ahmed ,   Millicent Ochieng , Anuoluwapo Aremu , Perez Ogayo , Jonathan Mukiibi ,   Fatoumata Ouoba Kabore , Godson Koffi Kalipe , Derguene Mbaye   Allahsera Auguste Tapo , Victoire M. Koagne , Edwin Munkoh - Buabeng ,   Valencia Wagner , Idris Abdulmumin , Ayodele Awokoya , Happy Buzaaba ,   Blessing Sibanda , Andiswa Bukula , Sam Manthalu   Abstract   Recent advances in the pre - training of language   models leverage large - scale datasets to create   multilingual models . However , low - resource   languages are mostly left out in these datasets .   This is primarily because many widely spoken   languages are not well represented on the web   and therefore excluded from the large - scale   crawls used to create datasets . Furthermore ,   downstream users of these models are restricted   to the selection of languages originally cho-   sen for pre - training . This work investigates   how to optimally leverage existing pre - trained   models to create low - resource translation sys-   tems for 16 African languages . We focus on   two questions : 1 ) How can pre - trained mod-   els be used for languages not included in the   initial pre - training ? and 2 ) How can the re-   sulting translation models effectively transfer   to new domains ? To answer these questions ,   we create a new African news corpus cover-   ing 16 languages , of which eight languages   are not part of any existing evaluation dataset .   We demonstrate that the most effective strategy   for transferring both to additional languages   and to additional domains is to fine - tune large   pre - trained models on small quantities of high-   quality translation data .   1 Introduction   Enormous efforts have been invested in making   language and translation models more multilingualwhile leveraging the maximal amount of data for   training , most prominently large crawls of mono-   lingual and parallel data from the web ( El - Kishky   et al . , 2020 ; Schwenk et al . , 2021b , a ; Xue et al . ,   2021b ) . The resulting models are now capable of   translating between hundreds of languages , includ-   ing language pairs that in isolation do not have   large collections of parallel data ( Tang et al . , 2020 ;   Xue et al . , 2021a ; Fan et al . , 2021b ) . For example ,   M2M-100 ( Goyal et al . , 2021 ) can translate ( with   low accuracy ) between Hausa and Yorùbá , two of   the most widely spoken languages in Nigeria , even   though there is barely any parallel data available for   training . For languages that are not included in the   set of training languages , the model would have no   knowledge on how to generate translations . Does   this mean there is no hope for languages that do not   have large presence on the web and are therefore   not included in these pre - trained models ?   We investigate how large - scale pre - trained mod-   els can be leveraged for the translation of unseen   low - resource languages and domains . We address   this question by studying 16 African languages that   are largely underrepresented in NLP research ( Joshi   et al . , 2020 ; ∀et al . , 2020 ) and further have little   to no training data available ( § 3 ) . These languages   provide an ideal testbed for two challenging knowl-   edge transfer tasks : ( 1)How can pre - trained mod-   els create translations for languages unseen at train-3053ing time ? and ( 2)Since training data may only   exist in single domain ( i.e. religious texts ) , how   can a model be trained in one domain and translate   another effectively at test time ?   These questions are extremely relevant for our   chosen languages because they all have millions of   native speakers and a massive need for translation   technologies . For example , news concerning the   African continent are almost exclusively published   in English , French , or Arabic , and thereby inacces-   sible for speakers of only native African languages .   This creates a bottleneck for information transmis-   sion , which becomes even more critical in times of   crises ( Öktem et al . , 2020 ; Anastasopoulos et al . ,   2020 ; Öktem et al . , 2021 ) . Furthermore , the task   of translating news has historically played a central   role in translation research , e.g. in shared tasks   since 2008 ( Callison - Burch et al . , 2008 ) and as a   test for determining human parity ( Hassan et al . ,   2018 ; Läubli et al . , 2018 ; Toral et al . , 2018 ) . To   spur the development of dedicated news translation   models for Africa , we construct a benchmark of   news translation for translating between 16 native   African languages and English or French ( § 4 ) .   This allows us to compare three approaches to   leveraging large - scale multilingual models for the   translation of previously unseen languages : ( 1 )   zero - shot transfer , ( 2)continual pre - training on   monolingual data , and ( 3)multi - domain fine - tuning   on parallel data ( § 5 ) . We find that fine - tuning pre-   trained models on a few thousand sentences of high   quality bitext is remarkably effective , and can be   further augmented with continual pre - training on   African languages and fine - tuning on news domain   data ( § 6 ) . Our contributions are the following :   1.We create a new African news corpus for   machine translation ( following principles of   participatory research ∀et al . ( 2020 ) ) covering   16 African languages .   2.Weadapt several multilingual pre - trained   models ( MT5 , ByT5 , mBART , M2M-100 ) to   these largely unseen languages , and evaluate   their quality on news translation .   3.We quantify the effectiveness of small in-   domain translation sets by measuring do-   main transfer effects and comparing fine-   tuning strategies . We find that having a targeted collection of trans-   lations is surprisingly effective , showcasing the   power of local knowledge in so - called “ zero-   resource ” scenarios ( Bird , 2020 ) . This paints a   promising picture for the development of NLP   technology for understudied languages : being able   to customize these models for new language of   interest with as little as 2k sentences and a few   fine - tuning steps , MT developers and users from   any language community are less dependent on   choices and monetary interest of industry power-   houses from the Global North ( Paullada , 2020 ) .   2 Related Work   African MT Datasets . One of the major chal-   lenges of developing MT models for African lan-   guages is lack of data . There are many attempts to   automatically crawl and align sentences from the   web ( Schwenk et al . , 2021a , b ) . Nevertheless , the   resulting corpora for many African languages are   typically small and of poor quality ( Kreutzer et al . ,   2021 ) . Other cleaner parallel sources are mostly   from religious sources , like the Bible covering   over 1600 languages ( McCarthy et al . , 2020 ) and   JW300 ( Agi ´ c and Vuli ´ c , 2019 ) from JW.org with   over 343 languages , including over 100 African lan-   guages . Apart from the training dataset , evaluation   datasets are needed to test the performance of mul-   tilingual MT models . The FLORES-101 ( Goyal   et al . , 2021 ) evaluation set , sourced from Wikipedia   and manually translated , covers the largest num-   ber of languages , including 20 African languages .   Finally , while other evaluation datasets for trans-   lating into or from African languages have been   developed ( Siminyu et al . , 2021 ; Emezue and Dos-   sou , 2020 ; Azunre et al . , 2021b ; Nyoni and Bassett ,   2021 ; Gezmu et al . , 2021 ; Ali et al . , 2021 ) , unfortu-   nately there are only a few African languages with   evaluation datasets in the news domain ( Adelani   et al . , 2021a ; Mabuya et al . , 2021 ; Ezeani et al . ,   2020 ) but ours covers 11 African languages ( § 4 ) .   Low - resource MT . Interest in low - resource MT   has been increasing both within the MT research   community ( Haddow et al . , 2021 ) , as well as in   native speaker communities ( ∀et al . , 2020 ; Azunre   et al . , 2021a ; Mager et al . , 2021 ) . On the model-   ing side , many techniques have been developed :   unsupervised MT ( Lample et al . , 2018 ) leverages   monolingual data , single multilingual models capa-   ble of translating between many languages ( Firat   et al . , 2016 ; Johnson et al . , 2017 ; Aharoni et al . ,3054   2019 ; Fan et al . , 2021a ) , multilingual unsupervised   models leverage a related language ( with paral-   lel data ) to assist translating the low - resource lan-   guage that might not even have any monolingual   data ( Ko et al . , 2021 ) . Unfortunately , unsupervised   MT typically performs poorly on low - resource lan-   guages ( Marchisio et al . , 2020 ) .   Transfer learning from high - resource languages   has achieved more promising results : Transfer from   multilingual pre - trained language models ( PLM ) ,   like mBART50 ( Tang et al . , 2020 ) and MT5 ( Xue   et al . , 2021b ) , and large - scale multilingual MT of-   ten outperforms bilingual MT ( Tran et al . , 2021 ;   Yang et al . , 2021 ) . For low - resource languages this   strategy outperforms the baseline ( Transformer )   models ( Birch et al . , 2021 ; Adelani et al . , 2021a ;   Lee et al . , 2022 ) . The performance can be further   improved by large scale pre - training ( Reid et al . ,   2021 ; Emezue and Dossou , 2021 ) .   3 Focus Languages and Their Data   Focus Languages . We focus on 16 African   languages with varying quantities of available   data ( Joshi et al . , 2020 ) , including moderately low-   resource languages such as Swahili and Hausa , and   very low - resource languages such as Ghomálá ’   with the Bible being its largest available corpus . Ta-   ble 1 provides an overview of the focus languages ,   including the language families , location and num-   ber of speakers , and the source and original lan-   guage for our corpus . The languages are from four   language families : Afro - Asiatic ( e.g. Hausa ) , Nilo-   Saharan ( e.g. Luo ) , English Creole ( e.g. Nigerian-   Pidgin / Naija ) and Niger - Congo . Most of the lan-   guages ( 13 out of 16 ) are from the Niger - Congofamily , which is the largest language family in   Africa . Six of the languages are predominantly   spoken in Francophone countries of Africa , while   the remainder are predominantly spoken in Anglo-   phone countries of Africa . In contrast to previous   work ( ∀et al . , 2020 ; Gowda et al . , 2021 ) , we do   not focus exclusively on translation to / from En-   glish since this is not the primary language of the   Francophone Africa community . All languages are   spoken by at least one million speakers .   Language Characteristics . All languages are   written in Latin script , using letters of the basic   Latin alphabet with a few omissions ( e.g “ c ” , “ q ” ,   “ x ” , “ z ” ) and additions ( e.g. “ E ” , “ O ” , “ N ” , “ o. ” , in-   cluding digraphs like “ gb ” , “ kp ” , “ gh ” , and some-   times more than two - character letters ) . 13 of the   languages are tonal , and about nine make use of   diacritics . Many African languages are morpholog-   ically rich . For example , all Bantu languages are   agglutinative . Fon , Mossi , and Yorùbá are highly   isolating . All languages follow the Subject - Verb-   Object sentence structure like English and French .   Table C provides more details .   Existing Parallel Corpora . We curate publicly   available parallel data for our focus languages ,   which consists primarily of text in the religious do-   main . For most African languages , the largest avail-   able parallel corpora is JW300 ( Agi ´ c and Vuli ´ c ,   2019 ) , sourced from jw.org , which publishes bib-   lical texts as well as lifestyle and opinion columns .   Varying quantities of data are available for 11 of the   16 focus languages . Éwé , Igbo , Swahili , Setswana ,   Twi , Yorùbá , and isiZulu have over 400 K parallel   sentences . Hausa and Mossi have slightly more   than 200 K parallel sentences , while Fon and Naija   have around 30 K sentences . For the remaining3055five languages that are not in the JW300 corpus ,   we make use of the Bible . We aligned the sen-   tences automatically by the verses ( around 31k in   total ) . Ghomálá ’ only has the New Testament with   8k verses . Bambara and Wolof are missing some   verses and books , leading to a total size of 28 K and   22K. Table 1 summarizes this information about   the religious ( REL ) corpora .   4 MAFAND - MT African News Corpus   4.1 Data Collection Process   We introduce our newly translated news corpus ;   MAFAND - MT — Masakhane Anglo & Franco   Africa NewsDataset for Machine Translation . Ta-   ble 1 gives the news source and data splits for 11   African languages which includes six languages   ( bam , bbj , ewe , fon , mos , wol ) spoken pre-   dominantly in Francophone Africa and five lan-   guages ( lug , luo , pcm , tsn , twi ) spoken pre-   dominantly in Anglophone Africa . The MAFAND-   MT corpus was created in three steps :   1.Crawling and preprocessing of news web-   sites from local newspapers that are publish-   ing in English and French . Raw texts from   the web were segmented into sentences . Most   languages were crawled from one or two sites ,   except for Wolof and Fon that were crawled   from four and seven news websites respec-   tively due to local French language newspa-   pers having very few articles . We also ensured   that the articles came from a variety of topics   e.g. politics , sports , culture , technology , soci-   ety , religion , and education . This was carried   out by native speakers of the target language   with source language proficiency .   2.Translation of 5k–8k sentences by profes-   sional translators . The translation process took   one to four months depending on the availabil-   ity of the translators .   3.Quality control was provided by native speak-   ers , who discussed and , if possible , fixed prob-   lematic translations and ran automatic checks   to detect misspellings , duplicated sentences ,   and alignment problems . Following the recommendations of ∀et al . ( 2020 ) ,   we design the process to be participatory : Ev-   eryone involved in the corpus creation is a na-   tive speaker of the respective target languages and   has societal knowledge about the communities that   speak those languages . This is particularly impor-   tant for curation and quality control to ensure that   the resulting material is appropriate and relevant   for stakeholders of the final MT models ( ∀et al . ,   2020 ; Kreutzer et al . , 2021 ) . Furthermore , every-   one received appropriate remuneration . To enable   cross - disciplinary knowledge transfer between par-   ticipants in the individual steps , every language   was assigned a coordinator . The coordinator con-   ducted the initial curation in the first step , and com-   municated with translators and quality checkers   throughout the following steps .   Other Available Parallel Corpora . We found   five African languages with available parallel texts   in the news domain : Hausa , Igbo ( Ezeani et al . ,   2020 ) , Swahili , Yorùbá ( Adelani et al . , 2021a ) ,   and isiZulu ( Mabuya et al . , 2021 ) . Table 1 provides   news source , the TRAIN , DEV andTEST splits .   Appendix B provides details on the pre - processing   of the available news corpora .   4.2 Monolingual News Corpus   To adapt available multilingual pre - trained models   via continued pre - training to African languages ,   we curated texts from the 17 highest - resourced   African languages and three non - native African   languages that are widely spoken on the conti-   nent ( Arabic , English , and French ) . The selec-   tion of African languages is based on their cover-   age in mC4 ( Xue et al . , 2021b ) , AfriBERTa cor-   pora ( Ogueji et al . , 2021 ) , and other publicly avail-   able news websites like VOA and BBC . We limited   the size of the corpus extracted from mC4 to the   first 30 million sentences ( roughly 1 GB of data ) for   Afrikaans , Amharic , Arabic , English , French , and   Swahili . In total , we collected about 12.3 GB of   data . Appendix C provides more details about the   pre - training corpus .   5 Models and Methods   5.1 Baseline Models   We experiment with pre - trained multilingual mod-   els and our own bilingual MT baselines . We focus3056   on pre - trained models that are approximately 500 M   parameters , both for computational feasibility and   comparability across various different models .   Transformer Baseline . We train Trans-   former ( Vaswani et al . , 2017 ) sequence - to-   sequence models from scratch for each language   pair using JoeyNMT ( Kreutzer et al . , 2019 ) . We   tokenize the bitext using a joint SentencePiece   unigram model ( Kudo , 2018 ) , with a character   coverage of 1.0 and a maximum sentence length   of 4096 tokens and create a vocabulary of 10 K   subwords . Models are trained on the concatenation   ofREL andNEWS corpora for each language .   Pre - trained Models . We consider three language   models , MT5 ( Xue et al . , 2021b ) , ByT5 ( a token-   free T5 ) ( Xue et al . , 2021a ) , mBART50 ( Tang   et al . , 2020 ) , and the multilingual translation model   M2M-100 ( Fan et al . , 2021b ) for our experiments .   We use MT5 - base and ByT5 - base , and M2M-100   with 418 M parameters . Table 2 gives the pre-   trained model size , number of African languages   covered , and the focus languages supported .   5.2 Transfer Learning Across Languages   We describe two methods for adding new lan-   guages to existing models : continual pre - training   and many - to - many multilingual translation .   Continual Pre - training . The effectiveness of   PLMs is limited on extremely low - resource lan-   guages because they rarely , if ever , occur in the pre-   training corpus ( Wang et al . , 2020 ; Liu et al . , 2021 ) .   As shown in Table 2 , even for MT5 and M2M-100 ,   which cover 100 languages , less than half of the   African languages under study are included . To   adapt the existing PLMs to our languages corpora   and domains , we apply continual pre - training ( Gu-   rurangan et al . , 2020 ; Liu et al . , 2021 ) using our   collected monolingual corpus . Specifically , before   fine - tuning on the parallel MT data , models are pre-   trained with their original training objective and vo - cabularyon the monolingual corpus . Pre - training   parameters can be found in the appendix . We re-   fer to the models adapted to African languages as   AfriMT5 , AfriByT5 , and AfriMBART .   Many - to - Many Translation . We fine - tuned   M2M-100 for African multilingual translation to   create English- and French - centric models . For the   English - centric model , the M2M-100 model was   fine - tuned on the news data for en–{hau , ibo ,   lug , luo , pcm , swa , tsn , twi , yor , zul }   while the French - centric model is trained on fr –   { bam , bbj , ewe , fon , mos , wol } . Languages   not included in the pre - trained M2M-100 model   were assigned the language code of a language in-   cluded in M2M-100 but excluded from our study .   5.3 Transfer Learning Across Domains   As there is very limited MT data on the news do-   main , we compare different methods that combine   thelarge data from the religious domain ( REL ) and   thesmall data from the NEWS domain ( NEWS ) to   fine - tune M2M-100 :   1.REL+NEWS : Fine - tuning on the aggregation   ofREL andNEWS .   2.REL→NEWS : Training on REL , followed by   fine - tuning on NEWS .   3.REL+NEWS →NEWS : REL+NEWS , followed   by additional fine - tuning on NEWS .   Each fine - tuning stage lasts for three epochs . We   evaluate translation quality with BLEU ( Papineni   et al . , 2002 ) using SacreBLEU ( Post , 2018)and   ChrF ( Popovi ´ c , 2015 ) .   6 Results and Discussion   We successfully adapt several multilingual pre-   trained models to previously unseen African lan-   guages and quantify the effectiveness of small in-   domain translation datasets . We discuss the effects   of domain shift and analyze mitigation strategies .   6.1 Adaptation to the Focus Languages   We demonstrate that fine - tuning with a few thou-   sand high - quality bitext is effective for adding new   languages to pre - trained models . Further , contin-   uing to pre - train to specialize models to African   languages further improves performance.3057   Zero - Shot Translation . Table 3 and Table 4   gives the result of zero - shot evaluation on NEWS .   We evaluate only on the M2M-100 dataset because   it has been pre - trained on parallel texts with a few   of our focus languages . We observe very poor per-   formance ( < 5BLEU ) on the languages except   forzul ( > 13BLEU ) and swa ( > 20BLEU )   in both translation directions . For swa , its likely   that the performance is reasonable because M2M-   100 has seen more bitext during pre - training ( 2.4 M   sentences in CCAligned ( El - Kishky et al . , 2020 ) ) .   Other African languages except for Afrikaans have   less than 600 K sentences in CCAligned , and are   also of a lower quality ( Kreutzer et al . , 2021 ) which   affect overall zero - shot performance .   Performance after Fine - tuning . We found im-   pressive performance after fine - tuning PLMs and   M2M-100 on few thousand sentences ( mostly 2 K –   7 K sentences , except for swa with 30 K sentences ) ,   including languages not seen during pre - training .   Foren / fr - xx , MT5 has a poor transfer performance   with average BLEU of 7.2 , despite being pre-   trained on 101 languages . ByT5 outperforms MT5   by over 3BLEU on average , even though their per-   formances were reported to be similar in previous   work ( Xue et al . , 2021a ) . This indicates that ByT5   might be preferable over MT5 when translating   low - resource languages . Surprisingly , mBART50   that was only pre - trained on 50 languages and 2   African languages outperformed MT5 and ByT5   which are pre - trained on 101 languages . Overall ,   we found M2M-100 to be the best model , mostlikely because it was pre - trained on a translation   task . In general , BLEU scores are relatively low   ( < 15BLEU for 9 out of 16 languages for en / fr - xx   and 7 in xx - en / fr ) even when fine - tuning M2M-100   on in - domain data , which suggests that developing   more effective methods for fine - tuning might be a   promising future direction . The languages with the   best quality according to BLEU on the target side   arepcm , swa andtsn , and pcm , zul , and swa   on the source side .   BLEU scores are higher when translating from   an African language , which is expected due to the   more frequent exposure to English and French on   the target side during pre - training , and BLEU being   penalized more for morphologically rich languages   likebbj , lug , swa , tsn , and zul ) . The ChrF   metric works better for them . For example , fine-   tuning M2M-100 on NEWS and evaluating on zul   has a BLEU of 21.0inen / fr - xx , and BLEU of 37.8   in the xx - en / fr showing a large gap in performance   in both directions . However , with the ChrF , we find   a smaller performance gap ( 51.2inen / fr - xx and   55.5 in the xx - en / fr .   Continual Pre - training . We observe an improve-   ment in BLEU when we utilize AfriMT5 and   AfriByT5 , for languages included in our continual   pre - training corpus ( Appendix C ) . Other languages   also benefit despite not being seen during continual   pre - training , possibly due to language similarity .   For example , AfriByT5 on fr - bam improved by 1.9   BLEU over ByT5 and AfriMT5 on en - tsn improved   by3.6BLEU over MT5 . On average , AfriMT5 im-3058   proved over MT5 by 1.3BLEU in en / fr - xx and   2.4BLEU in the xx - en / fr . The improvement for   AfriByT5 was much smaller : 0.6and0.9BLEU   inen / fr - xx andxx - en / fr translation directions . For   AfriMBART , we did not see any improvement on   average , only the performance of hau ( 1.5BLEU )   andibo ( 0.7BLEU ) improved in en / fr - xx direc-   tion . However , in the xx - en / fr direction , fon , tsn ,   twi , andzul improved by 2.7–6.0 BLEU .   Many - to - Many Multilingual MT . Training on   the combined news corpus from all languages that   use French or English separately does not appear to   help much . We see slight improvements for most   languages only in the xx - en / fr direction .   6.2 Adaptation to the News Domain   To improve over the baseline performance on   NEWS , we train bilingual Transformer models ( as a   baseline ) and M2M-100 on a combination of REL   andNEWS . We chose M2M-100 because it was the   best performing model . Table 5 gives the BLEU   on three settings : REL+NEWS , REL→NEWS , and   REL+NEWS →NEWS . In general , the improvement   depends on the size of REL corpus . For languages   trained on the Bible such as bbj , bam , lug , luo ,   andwol , the improvement is minimal . For M2M-   100 , the REL+NEWS performance does not im-   prove over NEWS despite the larger quantity of   training data . This demonstrates that increasing the   size in the target domain is the most helpful strategy   ( see Figure 2 ) . Similarly , combining REL+NEWS   is not very helpful for xx - en / fr . An alternative ap-   proach is REL→NEWS , which allows the model   to develop a good understanding of the desired   language before adapting to the news domain . We   observe an increase on 1.1BLEU over REL+NEWS   in the en / fr - xx direction . However , the best strat-   egy is REL+NEWS →NEWS , especially for xx - en / fr   where it yields an improvement over NEWS and   REL+NEWS by2.0and1.5BLEU , respectively .   6.3 Analysis of Domain Shift   Is a small in - domain set essential for fine-   tuning ? If we train models only on previously   available religious data , they are not capable of   translating news well due to the strong domain   bias . This is illustrated in Figure 1 : All models   perform much worse on NEWS than on the REL do-3059   main . When the quantity of religious training data   is small , the loss in translation performance on the   news test set is largest , c.f . bbj ( 8k of REL data )   with a drop of -95.5 % BLEU or bam ( -93.5 % , 28k )   andluo ( -93.5 % , 31k ) . This indicates that whentheREL training data is sparse , it is insufficient to   teach the M2M-100 model a more general under-   standing required for translating NEWS . However ,   when the religious training data is larger , this loss   is reduced , c.f . when translating to zul ( 667k , -   67 % ) , swa ( -69.3 % , 872k ) , and tsn ( -71 % , 870k ) .   While this is the general trend , pcm , whose reli-   gious training data is small ( 23k ) , has the lowest   drop in performance ( -59.3 % ) , which may be due   to the strong similarity to its source language .   How many sentences in the target domain are   required ? Figure 2 shows how for three selected   language pairs with a large ( fr - bam ) , medium   ( eng - ibo ) and relatively small ( eng - swa ) do-   main gap , the quality of target domain translations   improves as we increase the size of the target do-   main corpus . For all three pairs , fine - tuning M2M-3060   100 or ByT5 on 2.5 ksentence pairs of in - domain   data ( NEWS ) is sufficient to outperform the bilin-   gual Transformer baselines that were additionally   trained on larger amounts of out - of - domain data   ( REL ) . Surprisingly , this procedure not only works   for languages included during pre - training ( swa ) ,   but also for previously unseen languages ( ibo ,   bam ) . M2M-100 tends to adapt to the new data   more quickly than ByT5 , but in all cases , models   continue to learn with additional in - domain data .   This shows how much more effectively a small   number of in - domain translations can be used when   they serve for fine - tuning multilingual pre - trained   models rather than training bilingual MT models   from scratch .   Examples of Domain Bias . To illustrate the chal-   lenge of overcoming domain bias , we show exam-   ples translating from bam andlug in Table 7 . The   M2M-100 model fine - tuned only on REL succeeds   in roughly capturing the meaning of the sources ,   but using biblical terms , such as “ scroll ” instead   of “ novel ” . Adding our news corpus to fine - tuning   resolves these issues ( e.g. “ book ” ) .   How general is our news corpus ? Table 8 shows   the zero - shot evaluation of M2M-100 fine - tuned   on our small NEWS corpora on other domains : reli - gious ( REL ) and Wikipedia ( FLORES ) . We evalu-   ated the Wikipedia domain on the FLORES devtest   and the REL domain on either JW300 or Bible   ( lug , luo , wol ) . As a baseline , we evaluated   the zero - shot performance of M2M-100 ( not fine-   tuned , ✗ ) on FLORESusing spBLEU ( i.e. sen-   tencepiece BLEU ( Goyal et al . , 2021 ) ) . We noticed   very poor performance except for Swahili — as   discussed in § 6.1 . After fine - tuning on our new   data ( ✓ ) , transfer is largely improved across the   bench ( up to +17 BLEU for en - ibo ) . The same   trend holds for the religious domain . This shows   that even though our data comes from the news   domain , it helped the model generalize to other   domains . Hence , expanding African news corpora   and developing better MT models for news pays   off even for other domains of interest .   7 Conclusion   We have created MAFAND - MT , a corpus of 16   African languages to study translation systems for   low - resource languages in the news domain . We in-   vestigate how to most effectively adapt large - scale   pre - trained models to incorporate new languages   and new domains . Our findings suggest that as   little as 2k sentences are sufficient for fine - tuning ,   with an improved performance , paving the way for   others to create new translation systems without   relying on large collections of web - sourced text .   This has strong implications for languages that are   spoken by millions but lack presence on the web .   In the future , we hope to expand our cover-   age to additional under - resourced languages , and   to develop even more effective fine - tuning objec-   tives . Currently , we are extending our corpus to   Chichewa , Kinyarwanda , Shona , and isiXhosa , in-   cluding an expansion of the Hausa corpus , they will   be released under MAFAND - MT dataset name.30618 Acknowledgment   This work was carried out with support from   Lacuna Fund , an initiative co - founded by   The Rockefeller Foundation , Google.org , and   Canada ’s International Development Research   Centre . David Adelani acknowledges the EU-   funded Horizon 2020 projects : COMPRISE   ( http://www.compriseh2020.eu/ ) under   grant agreement No . 3081705 and ROXANNE   under grant number 833635 . We thank Chester   Chester Palen - Michel and Constantine Lignos for   providing the VOA corpus for this research , and   Google for providing GCP credits to run some of   the experiments . Finally , we thank Davor Orli ˇc   and Knowledge4All for their administrative sup-   port throughout the project .   References3062306330643065   A Language Characteristics   Table 9 provides the details about the language   characteristics . B Available Parallel Corpora   We found Five African languages with publicly   available parallel texts in the news domain : Hausa ,   Igbo , Swahili , Yorùbá , and isiZulu . Table 1 pro-   vides news source , the TRAIN , DEV andTEST   splits .   Hausa The Hausa Khameneicorpus contains   5,898 sentences , we split them into TRAIN ( 3,098 ) ,   DEV ( 1,300 ) , and TEST split ( 1,500 ) .   Igbo The Igbo corpus ( Ezeani et al . , 2020 ) has   9,998 sentences , we extract 6,998 sentences for   TRAIN , and the remaining for DEV andTEST   splits .   Swahili The Global V oicescorpus contains   30,782 sentences , which we use for the TRAIN   split . We additionally crawled newer ( 2019–2021 )   publications of Swahili articles from the Global   V oices website , this gives a total of 3,626 sentences ,   they were aligned and manually verified by Swahili   speakers . They are split into the DEV andTEST   splits .   Yorùbá The MENYO-20k ( Adelani et al . , 2021a )   corpus contains sentences from different domains   ( TED talks , books , software localization , proverbs ,   and news ) , from which we select the news domain   sentences for the TRAIN , DEV andTEST splits .   isiZulu The Umsuka corpus ( Mabuya et al . ,   2021 ) contains 9,703 training sentences and 1,984   evaluation sentences . 4,739 training sentences were   translated from English - isiZulu , and the remaining   from isiZulu - English . We only keep the training   sentences translated into isiZulu , and split them   into 3,500 for TRAIN and 1,239 sentences for DEV .   From the existing evaluation set we select only the   998 English - isiZulu translations for TEST . Um-   suka provides two translations for each English   sentence , but we use only the first .   CMonolingual Corpus PLMs adaptation   Table 10 provides the details about the Mono-   lingual corpus used to adapt the pre - trained lan-   guage models ( PLMs ) , their size and source   of corpora . The African languages pre - trained   are : Afrikaans , Amharic , Hausa , Igbo , Malagasy ,   Chichewa , Oromo , Naija , Kinyarwanda , Kirundi,3066   Shona , Somali , Sesotho , Swahili , isiXhosa , Yorùbá ,   and isiZulu .   D Model Hyper - parameters and   Reproducibility of Results   For the pre - trained models , we fine - tune the models   using HuggingFace transformer tool ( Wolf et al . ,   2020 ) with the default learning rate ( 5e−5 ) , batch   size of 10 , maximum source length & maximum   target length of 200 , beam size of 10 , and number   of epochs is 3except for models trained on only   NEWS which we set to 10 . We make All the exper-   iments were performed on a single GPU ( Nvidia   V100 ) .   For fine - tuning pre - trained models , especially   for mBART50 that only supports two African lan-   guages , the target language is required to be spec-   ified during decoding from among those that themodel has seen during pre - training , we follow past   works ( Madaan et al . , 2020 ; Cahyawijaya et al . ,   2021 ; Lee et al . , 2022 ) in selecting another closely-   related language that is represented in the pre-   trained model . For convenience , we make use   of Swahili ( sw ) as the target language when an   African language is not represented since Swahili is   represented in all the pre - trained models . The only   exception is Nigerian - Pidgin , where we make use   of French ( fr ) since it is closely related to English .   When a language is represented in the pre - trained   model like M2M-100 has seen Yorùbá ( yo ) , we   make use of the correct language code .   To train AfriMT5 and ByT5 , we start with MT5   and ByT5 . We pre - train with the learning rate   1e−4,10,000warm up steps and a batch size   of2048 for one epoch . For mBART50 , we pre-   train with learning rate of 5e−5for50,000steps3067   using Fairseq ( Ott et al . , 2019 ) without modify-   ing the mBART50 vocabulary . Table 11 has the   names of all the models that are publicly avail-   able on HuggingFace Model Hub . In total , we   have 357 models from 22 x 16 bilingual mod-   els , two English / French - centric models , and three   adapted models to African languages ( i.e AfriMT5 ,   AfriByT5 , and AfriMBART ) .   E BLEU vs spBLEU   Table 12 and Table 13 compares BLEU and sp-   BLEU metric for the domain transfer experiments .   We observe that spBLEU gives higher scores than   BLEU especially in the direction of en / fr - xx , which   shows that it may be better for evaluating African   languages . Although , further analysis and human   evaluation are still needed to show that spBLEU is   generally better . On the other hand , in the xx - en / fr ,   there is no much difference in the scores between   BLEU and spBLEU .   F Qualitative Analysis   The following examples from the Fon - to - French   translations of the test set illustrate the advantage   of multilingual modeling and its limitations :   •Source ( fon ): Louis Guy Alimanyi ãokpo   kpódÍssa Etchlekoun kpó O , sín az ˇan m Okpán   ãyeO , yeãò wˇuvEsè w Etawun ãò agbaza m E ,   có ye ká tuun fí é az On nElEEgosin é Oˇa .   •Reference ( fr ): Les faits Louis Guy Ali-   magnidokpo et Issa Etchlekoun se plaignent   depuis quelques jours de multiples douleurs ,   ignorant l’origine réelle de leurs maux .   •Bilingual Transformer ( REL+NEWS ,   fon→fr ): on ne peut pas avoir une trentaine   d’années ni un jeune homme ni un jeune   homme d’âge pour un jeune homme qui soit   12 ans .   •M2M-100 ( REL+NEWS →NEWS , fon→fr ):   Louis Guy Alimanyion et Issa Etchlekoun ont   depuis plusieurs jours souffert d’une maladie   grave malgré les conséquences de cette mal-   adie qu’ils ne connaissent pas .   •M2M-100 ( REL+NEWS →NEWS , fr→fon ):   Sín az ˇan y OywEywE ãéãyeãokpóo w ´ Enˇu   è kàn Louis Guy Alimagnidokpo kpódó Issa   Etchl Ek´En kpán ãè ´ O ãò xó ãOw´E ã´Owˇuv´Egege   w´E , ye ká tuun n ˇu è wú w ˇuv´EyetOnãè ´ Oˇa .   The translation of the bilingual Transformer model3068   is very poor and far from the Fon source , high-   lighting how poorly the model generalized from   the few thousand training sentences . The M2M-   100 model gives a more meaningful and adequate   translation . M2M-100 makes a surprising but beau-   tiful move , switching se plaignent depuis quelques   jours de multiples douleurs ( sín az ˇan m Okpán ãye   O , ye ãò wˇuvEsè w Etawun ãò agbaza m E)toont   depuis plusieurs jours souffert d’une maladie grave .   The BLEU score here might be low but the mean-   ing is conserved and even more detailed than the   French reference . In fact , in this source context ,   wˇuv ¢means souffrir , souffrance ( suffer , suffering ) :   the French reference made use of se plaignent   ( complaining ) which makes less sense than souf-   fertused in the M2M-100 prediction . M2M-100   also learned the style of the sentence : có ye ká   tuun fí é az On nElEEgosin ( but they do know the   origin of their sufferings ) é Oˇa ( NOT ) - this last   part is crucial for the meaning of the entire sen-   tence . Given the structural and morphological dif-   ferences between Fon and French , we expected it   to be more complicated to predict . However , this   translation is structurally wrong even though any   French native speaker would understand the con-   veyed message quickly and easily . In the M2M-100   translation , the word malgré is at the wrong place ,   corrupting syntax and logic of the second clause .   A perfect translation ( in the idea to be expressed )   would be : " Louis Guy Alimanyion et Issa Etch-   lekoun ont depuis plusieurs jours souffert d’une   maladie grave malgré ( do nt ) ils ne connaissent pas   lesconséquences ( causes / raisons ) decette maladie   qu’ils neconnaissent pas . "   In the opposite translation direction , fr→fon ,   M2M-100 ( REL+NEWS →NEWS ) still preserved   some sense of logical reasoning and predicted the   last part right ye ká tuun n ˇu è wú w ˇuv´EyetOn ( theydo know why they are suffering ) ãè´Oˇa ( NOT ) . How-   ever , the model had some limitations : the names   which are part of the translation are not spelled   correctly . Some expressions are incomplete : For   instance sín az ˇan + number means since xxx days   butyEywEis not a number , and do not have any   meaning in this context .   G Limitations and Risks   Despite the promising results , our work has the   following limitations :   1.Translation quality : Even the best model   scores low BLEU on some of the reported lan-   guages ( bbj , mos , zul ) , in particular when   translating into them .   2.Evaluation : Our evaluation is focused on   BLEU . We report ChrF results as well , but   without a deeper human evaluation , we can-   not make claims about the absolute quality   of the translations . Manual inspections of   translations like the example discussed in Sec-   tion F gave us the impression that translations   are surprisingly fluent and make good use of   language - specific expressions when translat-   ing into English or French , but that errors in   grammar and logic can be easily overlooked .   Automatic reference - based metrics like BLEU   and ChrF might not be able to capture the   semantic relatedness to the reference suffi-   ciently , as well potentially being tricked by   word matches in incoherent phrases .   3.Language bias : We have shown that even   when not included in pre - training , and with-   out large out - of - domain data , significant gains   in translation quality can be achieved . How-   ever , language - specific biases , in terms of re-   sourcedness , morphology , standardization , in-   clusion in pre - trained models and available   corpora , or relatedness to other languages , still   affect the relative quality of translations , and   require more efforts to be overcome .   4.Domain limitations : While we showed a   rapid adaptation to the news domain and the   auxiliary benefit of the religious domain , our   study also revealed how automatically esti-   mated translation quality drops when the test   domain is narrow . Therefore , future work   should aim to expand the study to multiple   test domains and develop systematic methods3069for distilling knowledge from multiple narrow   domains .   5.Language coverage : Africa has thousands   of other languages that are not covered in our   study but deserve the same attention . We hope   that our work is encouraging enough to inspire   native speakers of those languages not covered   here to collect translations , run our code , and   report their findings to the NLP research com-   munity , so that we can make joint progress   in developing language technology for more   people .   We believe that our translation models carry sim-   ilar risks of causing harm by inaccurate and bi-   ased translations as the underlying large pre - trained   models . M2M-100 is trained on large collections   of texts crawled from the web , and the quality   for most of the languages studied here is ques-   tionable ( Kreutzer et al . , 2021 ) . Our fine - tuning   successes show that some obvious biases can be   overcome when the quality of the fine - tuning set   is controlled ( see the examples in Section 6.3 ) , but   we can not guarantee that biases prevailing in the   pre - training corpus or more subtle biases will not   occur with other inputs . Together with a careful   human evaluation , this should be the main con-   cern for future work on the produced models . The   methodology of rapid fine - tuning might also be mis-   used to tune the models towards harmful content or   purposes that harm the speakers of the languages   presented here.3070