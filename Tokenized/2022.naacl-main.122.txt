  Pieter Delobelle , Ewoenam Kwaku Tokpo , Toon CaldersandBettina BerendtDepartment of Computer Science , KU Leuven ; Leuven . AIDepartment of Computer Science , University of AntwerpFaculty of Electrical Engineering and Computer Science , TU Berlin ; Weizenbaum Institute   Abstract   An increasing awareness of biased patterns in   natural language processing resources such as   BERT has motivated many metrics to quantify   ‘ bias ’ and ‘ fairness ’ in these resources . How-   ever , comparing the results of different metrics   and the works that evaluate with such metrics   remains difﬁcult , if not outright impossible .   We survey the literature on fairness metrics for   pre - trained language models and experimen-   tally evaluate compatibility , including both bi-   ases in language models and in their down-   stream tasks . We do this by combining tra-   ditional literature survey , correlation analysis   and empirical evaluations . We ﬁnd that many   metrics are not compatible with each other and   highly depend on ( i ) templates , ( ii ) attribute   and target seeds and ( iii ) the choice of embed-   dings . We also see no tangible evidence of in-   trinsic bias relating to extrinsic bias . These re-   sults indicate that fairness or bias evaluation re-   mains challenging for contextualized language   models , among other reasons because these   choices remain subjective . To improve fu-   ture comparisons and fairness evaluations , we   recommend to avoid embedding - based metrics   and focus on fairness evaluations in down-   stream tasks .   1 Introduction   With the popularization of word embeddings by   works such as Word2vec ( Mikolov et al . , 2013 ) ,   GLoVe ( Pennington et al . , 2014 ) and , more re-   cently , contextualized variants such as ELMo ( Pe-   ters et al . , 2018 ) and BERT ( Devlin et al . , 2019 ) ,   Natural Language Processing ( NLP ) has seen sig-   niﬁcant growth and advancement . Word embed-   dings and later language models have been adopted   by many applications . Many of these embeddings   have been probed by researchers for biases such as   gender stereotypes .   Word embeddings are generally trained on real-   world data such that they model statistical proper-   ties from the training data . Hence , they pick upbiases and stereotypes that are typically present   in the data ( Garrido - Mu ˜noz et al . , 2021 ) . Al-   though Kurita et al . ( 2019 ) and Webster et al . ( 2020 )   opine that this can pose signiﬁcant challenges in   downstream applications , this view has been ques-   tioned , especially for non - contextualized word em-   beddings ( Goldfarb - Tarrant et al . , 2021 ) .   Early works such as Bolukbasi et al . ( 2016 ) ;   Caliskan et al . ( 2017 ) ; Gonen and Goldberg ( 2019 )   widely explored fairness in non - contextualized em-   bedding methods . In non - contextualized embed-   dings such as Word2vec and GLoVe embeddings ,   models are trained to generate vectors that map   directly to dictionary words and hence are inde-   pendent of the context in which the word is used .   In contrast , contextualized word embeddings take   polysemy ( words could have multiple meanings ,   e.g. ‘ a stick ’ vs ‘ let ’s stick to ’ ) into considera-   tion . Thus different embeddings are generated for   a given word depending on the context in which   it appears . Because of such differences between   the two approaches , popular techniques for detect-   ing and measuring bias in non - contextualized word   embeddings , such as WEAT ( Caliskan et al . , 2017 ) ,   do not apply naturally to contextualized variants .   Many techniques have been proposed to mea-   sure bias in contextualized word embeddings , ei-   ther as a standalone method ( May et al . , 2019 ; Bartl   et al . , 2020 ) or as an additional contribution to eval-   uate fairness interventions ( Webster et al . , 2020 ;   Lauscher et al . , 2021 ; Kurita et al . , 2019 ) . This   broad selection of methods makes it difﬁcult for   NLP practitioners to select an appropriate and reli-   able set of metrics to quantify bias and to compare   results . This is further exacerbated as these quan-   tifying techniques also involve different choices   for attribute and target words , commonly jointly   referred to as seed words , templates for context ,   and different methods for measuring similarity .   In this paper , we combine literature survey and   experimental comparisons to compare fairness met-1693rics for contextualized language models . We are   guided by the following research questions :   •Which fairness measures exist for contextu-   alized language models such as BERT ? ( Sec-   tion 3 )   •What challenges do languages other than En-   glish pose ? ( § 3.3 )   •What are the relationships between fairness   measures , the templates these measures use ,   embedding methods , and intrinsic vs extrinsic   measures ? ( Section 4 )   •Which set of measures do we recommended   to evaluate language resources ? ( Section 7 )   2 Background   Static word embeddings have typically been used   with recurrent neural networks ( RNNs ) , option-   ally with an attention mechanism ( Bahdanau et al . ,   2014 ) . The transformer architecture ( Vaswani   et al . , 2017 ) introduced a new paradigm relying   only on attention , which proved faster and more   accurate than RNNs and did not rely on static   word embeddings . The transformer consists of   two stacks of attention layers , the encoder and the   decoder , with each layer consisting of multiple par-   allel attention heads . BERT ( Devlin et al . , 2019 )   is based on the encoder from this transformer and   obtained state - of - the - art results for multiple NLP   tasks using transfer learning with a pre - training   step and a second ﬁnetuning step .   The pre - training task is to reconstruct missing   words in a sentence , called masked language   modeling ( MLM ) , which helps capture interesting   semantics . The training objective for a model   with parameters θis to predict the the original   token on the position of a randomly masked token   xbased on the positional - dependent context   x = x, ... ,x , x, ... ,x , following   max / summationtext1log / parenleftbig   P / parenleftbig   x|x;θ / parenrightbig / parenrightbig   with   1as indicator function . After training , the   language model can infer the probability that   a token occurs on the masked position . As an   illustration with the original BERT model , the   sentence ‘ [ MASK ] is a doctor . ’ is ﬁlled in with   the token ‘ He ’ ( 62 % ) , followed by ‘ She ’ ( 32 % ) .   Because the MLM task relies on co - occurrences ,   this example illustrates how this task captures   stereotypes that are present in pre - training datasets ,   which is referred to as intrinsic bias .   As a second step , this pre - trained model can be   ﬁnetuned on a new task , most commonly either   sentence classiﬁcation , which uses the contextu-   alized embeddings of the ﬁrst token x= [ CLS ] ,   or token classiﬁcation , for which the embeddings   of each respective token position are used . These   embeddings are obtained from output states of the   penultimate layer , after which a single linear layer   is added and trained . This ﬁnetuning is typically   done with different datasets that are labeled for   the task at hand and here we can observe extrin-   sicbias with allocational harms ( Goldfarb - Tarrant   et al . , 2021 ; Blodgett et al . , 2020 ) , e.g. gender   imbalances in co - reference resolution ( see § 3.2 ) .   Many models improved on the original BERT   architecture and training setup , e.g. RoBERTa ( Liu   et al . , 2019 ) was trained on signiﬁcantly more   data for a longer period and without a second pre-   training objective , next sentence prediction . AL-   BERT ( Lan et al . , 2019 ) used parameter sharing   between attention layers to obtain a smaller model   without signiﬁcant performance degradation . Sanh   et al . ( 2019 ) also created a smaller BERT varia-   tion , DistilBERT , by using knowledge distillation .   All these models are MLMs , so this gives us the   opportunity to compare bias metrics across models .   2.1 Fairness in word embeddings   Fairness in machine learning has a long standing   history and a general introduction is out of scope   for this paper , so we refer the reader to Barocas   et al . ( 2019).Typical metrics , e.g. demographic par-   ity , are not directly applicable to tasks dealing with   natural language . Furthermore , many NLP applica-   tions ﬁnetune existing language models , which in-   tertwines extrinsic and intrinsic biases as discussed   earlier in Section 2.1694Early methods for evaluating bias in non-   contextualized embeddings like Word2vec , are   WEAT ( Caliskan et al . , 2017 ) and a direct bias   metric ( Bolukbasi et al . , 2016 ) . The latter demon-   strated that word embeddings contain a ( lin-   ear ) biased subspace , where for example ‘ man ’   and ‘ woman ’ can be projected on the same gen-   der axis as ‘ computer programmer ’ and ‘ home-   maker ’ ( Bolukbasi et al . , 2016 ) . These analogies   are calculated using cosine distance between vec-   tors to deﬁne similarity and also to evaluate the   authors ’ proposed debiasing strategies . In addi-   tion , pairs of gendered words were also evaluated   using Principal Component Analysis ( PCA ) . This   showed that most of the variance stemming from   gender could be attributed to a single principal com-   ponent ( Bolukbasi et al . , 2016 ) .   In parallel , the Word Embeddings Association   Test ( WEAT ; Caliskan et al . , 2017 ) was devel-   oped based on the Implicit Association Tests ( IAT ;   Greenwald et al . , 1998 ) from social sciences .   WEAT measures associations between two sets of   target wordsX , Y , e.g. male and female names ,   and another two sets of attribute words A , B , e.g.   career and family - related words , following   s(X , Y , A , B ) = /summationdisplayu(x , A , B)−/summationdisplayu(y , A , B )   with a similarity measure u(x , A , B)that mea-   sures the association between one word embedding   xand the word vectors of attributes a∈A , b∈   B , deﬁned as ( x , A , B ) = meancos ( x , a)−   meancos ( x , b ) . This method relies on a vector   representation for each word and by providing a   representation from a contextualized model , WEAT   can also be adapted for contextualized language   models , which we discuss in Section 3 and § 4.3 .   3 Measuring fairness in language models   3.1 Intrinsic measures   Discovery of Correlations ( DisCo ) . Webster   et al . ( 2020 ) presented an intrinsic measure Dis-   covery of Correlations ( DisCo ) that uses templates   with two slots such as ‘ likes to [ MASK ] . ’ , we   provide a complete list in § A.1 . The ﬁrst slot ( )   is ﬁlled with words based on a set of e.g. ﬁrst   names or nouns related to professions . The sec-   ond masked slot is ﬁlled in by the language modeland the three top predictions are kept . If these pre-   dictions differ between sets , this is considered an   indication of bias . Lauscher et al . ( 2021 ) slightly   modiﬁed this method by ﬁltering predictions with   P(x|T)>0.1instead of the top - three items .   Log Probability Bias Score ( LPBS ) . This bias   score presented by Kurita et al . ( 2019 ) is a template-   based method that is similar to DisCo , but also cor-   rects for the prior probability of the target attribute ,   as for example the token ‘ He ’ commonly has a   higher prior than ‘ She’ . The reasoning is that correc-   tion ensures that any measured difference between   attributes can be attributed to the attribute and not   to the prior of this token . Bartl et al . ( 2020 ) in-   troduced an alternative dataset speciﬁcally for this   evaluation method , called bias evaluation corpus   with professions ( BEC - Pro ) , with templates and   seeds in both English and German . We will revisit   the German results in § 3.3 .   Sentence Embedding Association Test ( SEAT ) .   A limitation of WEAT ( Caliskan et al . , 2017 ) is   that the method does not work directly on contex-   tualized word embeddings , which SEAT solves by   using context templates ( May et al . , 2019 ) . These   templates are semantically bleached , so there are   no words in there that affect bias measurements ,   for instance ‘ is a[MASK ] . ’ We will investigate   this concept further in § 4.2 .   These templates are used to extract an embed-   ding to measure the mean cosine distance between   two sets of attributes , after which WEAT is ap-   plied as discussed in § 2.1 . This embedding is   obtained from the [ CLS ] token in BERT . May   et al . ( 2019 ) implemented three tests from WEAT .   In addition , the authors also made new tests for   double binds ( Stone and Lovejoy , 2004 ) and angry   Black woman stereotypes . An approach inspired by   SEAT was taken by Lauscher et al . ( 2021 ) using to-   ken embeddings from the ﬁrst four attention layers   instead of the [ CLS ] embedding in the last layer ,   following Vulic et al . ( 2020 ) . Tan and Celis ( 2019 )   also adapted SEAT by relying on the embedding   of the token of interest in the last layer , instead of   the[CLS ] token . We will discuss these different   embedding methods in § 4.3 .   Contextualized Embedding Association Test   ( CEAT ) . Another extension of WEAT ( Caliskan   et al . , 2017 ) was presented by Guo and Caliskan   ( 2021 ) . CEAT uses Reddit data ( up to 9 tokens )   as context templates , which provide more realistic1695   contexts compared to other WEAT extensions ( May   et al . , 2019 ; Lauscher et al . , 2021 ; Tan and Celis ,   2019 ; May et al . , 2019 ) . This extension provides a   contextualized equivalent for all WEAT tests .   Context Association Test ( CAT ) . Nadeem et al .   ( 2021 ) created StereoSet , a dataset with stereotypes   with regard to professions , gender , race , and reli-   gion . Based on this dataset , a score , CAT , is cal-   culated that reﬂects ( i ) how often stereotypes are   preferred over anti - stereotypes and ( ii ) how well   the language model predicts meaningful instead of   meaningless associations . Blodgett et al . ( 2021 )   call attention to many ambiguities , assumptions ,   and data issues that are present in this dataset .   CrowS - Pairs . CrowS - Pairs ( Nangia et al . , 2020 )   takes a similar approach as SteroSet / CAT ,   but the evaluation is based on pseudo - log-   likelihood ( Salazar et al . , 2020 ) to calculate a   perplexity - based metric of all tokens in a sentence   conditioned on the stereotypical tokens ( e.g. ‘ He ’ ) .   All samples consist of pairs of sentences where one   has been modiﬁed to contain either a stereotype or   an anti - stereotype . ALBERT and RoBERTa both   had better scores compared to BERT , but these ﬁnd-   ings might be limited , since this dataset also has   data quality issues ( Blodgett et al . , 2021 ) .   All Unmaksed Likelihood ( AUL ) . Kaneko and   Bollegala ( 2021 ) modify the above CrowS - Pairs   measure to consider multiple correct predictions ,   instead of only testing if the target tokens are pre-   dicted . In addition , the authors also argue against   evaluations biases using [ MASK ] tokens , sincethese tokens are not used in downstream tasks .   PCA - based methods . Both Basta et al . ( 2019 ) ;   Zhao et al . ( 2019 ) analyzed gender subspaces in   ELMo using a method that is very similar to Boluk-   basi et al . ( 2016 ) . This approach was then applied   to BERT - based models ( Sedoc and Ungar , 2019 ) .   We do not further compare to these methods , since   they are less suited to obtain numerical bias scores   as they rely on identifying a unique gender axis .   3.2 Extrinsic measures   Extrinsic measures are used to measure how bias   propagates in downstream tasks such as occupation   prediction and coreference resolution . These typ-   ically involve ﬁnetuning the pre - trained language   model on a downstream task and subsequently eval-   uating its performance with regard to sensitive at-   tributes such as gender and race . As elsewhere in   the bias literature , most evaluations focus on gender   bias due to the relative availability of gender - related   datasets and the relatively widespread concern for   gender - related biases .   BiasInBios . De - Arteaga et al . ( 2019 ) developed   an English dataset as a classiﬁcation benchmark for   measuring bias in language models , which has been   adopted as an extrinsic measure ( Webster et al . ,   2020 ; Zhao et al . , 2020 ) . The task is to predict   professions based on biographies of people . Bias   is quantiﬁed as the true positive rate difference be-   tween male and female proﬁles . We will investigate   BiasInBios as a fairness metric in ( § 4.4 ) .   Winograd schemas . The Winograd schema   ( Levesque et al . , 2012 ) , originally designed to test1696machine intelligence based on anaphora resolution ,   has been adapted in various works into benchmark   datasets for bias evaluation . These benchmark   datasets have nuances that make them suitable for   measuring biases in different scenarios and con-   texts ( Rudinger et al . , 2018 ) . Prominent among   these are WinoBias ( Zhao et al . , 2018 ) , Winogen-   der ( Rudinger et al . , 2018 ) and WinoGrande ( Sak-   aguchi et al . , 2021 ) . GAP ( Webster et al . , 2018 ) is   another benchmark dataset which closely relates   to the Winograd family . It has also been used to   measure bias in pronoun resolution methods .   The WinoBias dataset covers 40 occupations and   is used to measure the ability of a language model   to resolve coreferencing of gender pronouns ( fe-   male and male ) in the context of pro - stereotype   and anti - stereotype jobs . A pro - stereotype setting   is when , for instance , a male pronoun is linked   to a male - dominated job , whereas a female pro-   noun being linked to that same job will be an anti-   stereotype . E.g. Pro - stereotype : [ The janitor ]   reprimanded the accountant because [ he ] got less   allowance . Anti - stereotype : [ The janitor ] rep-   rimanded the accountant because [ she ] got less   allowance . The usual approach is to adapt the lan-   guage model to the OntoNotes dataset ( Weischedel   et al . , 2013 ) . A model is said to pass the WinoBias   test if resolution is done with the same level of per-   formance for pro - stereotype and anti - stereotype in-   stances . This is quantiﬁed with an Fscore for two   types of sentences , of which type 1 is the most chal-   lenging because resolution relies on world knowl-   edge ( Rudinger et al . , 2018 ) . Using this approach ,   de Vassimon Manela et al . ( 2021 ) extended Wino-   Bias to include skew towards one gender , follow-   ing(|F−F|+|F−F| ) . In   ( § 4.4 ) , we will also investigate WinoBias ( type 1 )   and the skew variant as implemented by de Vassi-   mon Manela et al . ( 2021 ) .   3.3 Measuring biases in other languages   Many languages have some sort of grammatical   gender , which can interfere with fairness evalua-   tion metrics presented in § 3.1 that focus mostly   on gender stereotyping by measuring associations .   The assumption is that there should be no associa-   tion between e.g. professions and gender . However ,   these associations can be expected in gendered lan-   guages . We provide a brief overview of some meth-   ods that address languages beyond English . Delobelle et al . ( 2020 ) and Ch ´ avez Mulsa and   Spanakis ( 2020 ) evaluated RobBERT , a Dutch lan-   guage model . Delobelle et al . ( 2020 ) did this vi-   sually with three templates ( § A.5 ) . Associations   between gendered pronouns and professions were   not considered an indicator of bias , since this is   expected in Dutch . Instead , a prior towards male   pronouns was viewed as an indication , contrasting   with LPBS ( Kurita et al . , 2019 ) .   For German , Bartl et al . ( 2020 ) evaluated BEC-   Pro . The authors found that the scores for male   and female professions were very similar , likely   because of the gender system .   Finally , Nozza et al . ( 2021 ) presented a multi-   lingual approach using HurtLex ( Bassignana et al . ,   2018 ) , focusing on six European languages ( En-   glish , Italian , French , Portuguese , Romanian , and   Spanish ) with BERT and GPT-2 . Both models repli-   cated multiple stereotypes and reproduced deroga-   tory words across languages , leading the authors to   question the suitability for public deployment .   4 On the compatibility of measures   In this section , our goal is to objectively investigate   the consistency in indicating bias between various   techniques used by previous works . As mentioned   earlier , besides the metric choice , three primary fac-   tors are important when measuring intrinsic bias in   an embedding model : ( i ) choice of seed words , ( ii )   choice of templates and ( iii ) how representations   for seed words are generated .   Recent works investigating bias in language   models have found issues with inconsistencies be-   tween seed words ( Antoniak and Mimno , 2021 ) ,   unvoiced assumptions and data quality issues in   StereoSet and CrowS - Pairs templates ( Blodgett   et al . , 2021 ) , and issues with semantically bleached   templates ( Tan and Celis , 2019 ) . These issues raise   some questions for the remaining two factors , for   example whether or not the choice of template and   technique for selecting embeddings to represent   seed words matters in measuring bias ? And are   “ semantically bleached ” templates really semanti-   cally bleached ? Meaning , do they not affect bias   measurements ? Or in the extreme , can bias in em-   bedding model stay hidden by picking the “ wrong ”   templates or representations ? These are questions   we seek to answer with a series of experimental   analysis where we measure correlations between   various approaches to test if these templates and   representations measure the same bias.1697   4.1 Methodology   We conduct correlation analyses between differ-   ent templates ( § 4.2 ) and between representation   methods ( § 4.3 ) , as well as between measures them-   selves ( § 4.4 ) . To create a context and to help draw   concise conclusions , we focus all our experiments   on binary gender bias with respect to professions .   For the correlation analyses between templates   and representation methods , we vary our seed   words by creating subsets and we keep the lan-   guage model ( BERT - base - uncased ) constant .   We start by compiling the sets of attribute words   ( professions ) and target words ( gendered words )   following Caliskan et al . ( 2017 ) and Zhao et al .   ( 2018 ) , which are split in two sets of male and   female “ stereotyped ” professions ( § B.1 ) and we   create female and male sets of target words ( § B.2 ) .   We generate 20 subsets { a, ... ,a}by randomly   sampling 10 professions for each set of attributes ,   thus for male and female professions ( see § B.1 for   the full list ) . We expect that some subsets will show   higher levels of bias than others and that given two   “ accurate ” fairness metrics MandM , ifMin-   dicates thatacontains less bias than awhich in   turn contains less bias than a , Mshould likewise   indicate bias for the three subsets . Caliskan et al .   ( 2017 ) ; May et al . ( 2019 ) ; Lauscher et al . ( 2021 ) ;   Tan and Celis ( 2019 ) used a similar approach to   calculate distributional properties and quantify the   variance . In our experiments , we use Pearson corre-   lation coefﬁcients .   For the third correlation experiment between fair - ness metrics ( § 4.4 ) , we use ﬁve language mod-   els , where the different language models replace   the need for subsets . We assume that different   language models have different levels of biases ,   because of different training setups on different   datasets , which was observed for metrics that were   evaluated on multiple models ( Nangia et al . , 2020 ) .   We also use the templates and seed words for each   metric as described in the original papers , since we   compare the metrics as they are used .   4.2 Compatibility between templates   The choice of template for creating contexts for   seed words plays a very important role in measur-   ing bias in contextual word embeddings . Many   papers propose the use of “ semantically bleached ”   sentence templates for context which should con-   tain no semantic meaning so that the embedding   generated by inserting a seed word into such a tem-   plate should only represent the seed word . May   et al . ( 2019 ) ; Tan and Celis ( 2019 ) indicated that   semantically bleached templates might still contain   some semantics , at least related to the bias .   If these templates are semantically bleached with   regard to a gender bias , all these templates should   have a high correlation with other bleached tem-   plates . We test the bleached SEAT templates ( May   et al . , 2019 ) , listed in Table 2 ( T−T ) . We also   compare with the masked template of used by Ku-   rita et al . ( 2019 ) for their SEAT implementation   ( T ) , and add 2 semantically unbleached templates   from Tan and Celis ( 2019 ) ( T−T ) as control   templates . We test both the [ CLS ] embedding as1698   sentence representation May et al . ( 2019 ) and the   target token embedding ( Tan and Celis , 2019 ) .   We test our hypothesis with a correlation anal-   ysis as described in § 4.1 and we additionally test   how the distribution differs between templates . We   operationalize semantically bleached templates as   two templates T1,T2having the same contextual-   ized probability for a set of tokens on position x ,   followingP(x|T ) = P(x|T ) .   To quantify the distance between both distribu-   tions , we calculate relative entropy ( Kullback and   Leibler , 1951 ) between every template and tem-   plateT , which we expect to be lower for the se-   mantically bleached templates compared to the un-   bleached templates . We perform this relative en-   tropy experiment twice : ( i ) once with all tokens   in the model ’s vocabulary and ( ii ) once with a set   of gendered tokens ( see § B.2 ) . Both sets aim to   evaluate how the contextualized distributions of   the masked token t = P(x|T)differ , but we   expect a lower divergence in particular for the gen-   dered subset . Figure 2a and Table 2 present our   results for the correlation analysis and difference in   distributions , where we make three observations .   Firstly , the choice of “ semantically bleached ”   template could signiﬁcantly vary the measure of   bias . Although templates T−Tare all bleached ,   there are weak and sometimes even negative corre-   lations ( e.g. T ) . The fact that we do not get ( close   to ) perfect correlation among these templates con-   ﬁrms the observation made by May et al . ( 2019 )   on the possible impact that “ semantically bleached ”   templates could have on fairness evaluations .   Secondly , semantically and syntactically similar   templates do not necessarily correlate strongly . E.g.   “ There is the . ” ( T ) and “ The is there . ” ( T ) con-   tain the same words which are believed to carry no   relevant information , yet the correlation is lower .   Thirdly , the distributional distances between   Tand all other templates , as measured by the   Kullback - Leiber divergence and shown in Table 2 ,   highlight that the different templates are indeed not   completely semantically bleached . However , this   deﬁnition does have some merit , as the distance is   signiﬁcantly less for all than bleached sentences   the two unbleached sentences .   Based on the above observations , we conclude   that semantically bleached templates need to be   used cautiously , and any results stemming from the   use of such templates can not be objectively main-   tained so long as there does not exist a standardized   and validated scheme of selecting such templates .   4.3 Compatibility between representations   Word representations or embeddings could also be   a source of inconsistency in evaluating contextual-   ized language models . Since many techniques use   templates , it is natural to use the entire sentence   representation as the representation of the word   in question , e.g. by mean - pooling over all target   tokens or using the [ CLS ] embedding . We test   these methods and some additional combinations   that have been used in the literature , yet not nec-   essarily for bias evaluations . A complete list with   explanations can be found in Appendix C.   Firstly , we investigate whether there are incon-   sistencies between methods by conducting corre-1699   lation analysis of bias scores produced by SEAT   on scores from the subset of attribute words . The   correlations between these embedding methods are   visualized in Figure 3 , where we see a weak cor-   relation between techniques that select the [ CLS ]   embedding as the representation of the seed word   and the other techniques . The weak correlation   among the [ CLS ] techniques themselves conﬁrms   the claim that semantically bleached contexts have   signiﬁcant inﬂuence on the word representation .   Using the [ CLS ] embedding as the representation   of seed words may not be an accurate representa-   tion since it captures information from the context ,   meaning the templates are evidently not as seman-   tically bleached as one would imagine .   Secondly , we explore how other embedding se-   lection methods withstand semantic inﬂuence from   the context / templates . Tan and Celis ( 2019 ) pro-   pose using the contextual word representation of   the token of interest instead of [ CLS ] . We inves-   tigate the effectiveness of this approach by repli-   cating the experiment in Figure 2a . The results   on the correlations between template types show   that using only the embeddings of the target word   ( Figure 2b ) produces more consistent results than   using the [ CLS ] embedding as the representation   ( Figure 2a ) . Thus , using only the embeddings of   the target word produces more stable results across   templates and is more resilient to a context that   may not be semantically bleached , which justiﬁes   the embedding approach of Tan and Celis ( 2019).4.4 Compatibility between metrics   In this section , our goal is to ( i ) see if there is a   general relationship between intrinsic and extrinsic   bias measures and ( ii ) how individual bias metrics   correlate with extrinsic bias . To do this , we test   three extrinsic metrics , BiasinBios ( De - Arteaga   et al . , 2019 ) , WinoBias Zhao et al . ( 2018 ) , and   skew ( de Vassimon Manela et al . , 2021 ) . and we   evaluate ﬁve popular language models . For Wino-   Bias , we adapt the models to the OntoNotes 5.0   dataset ( Weischedel et al . , 2013 ) , which is standard   practice for WinoBias and we follow the training   setup of de Vassimon Manela et al . ( 2021 ) .   We performed a correlation analysis between   the results of the three extrinsic measures and a   set of intrinsic fairness measures from Section 3 ;   the results are presented in Figure 4 . We observe   that most correlations with the extrinsic BiasInBios   measure are negative — which is expected since   this measure gives a higher score if more bias is   present — but still strongly correlated with some   intrinsic measures , like a WEAT variant by Tan   and Celis ( 2019 ) . However , other measures , like   CrowS - pairs ( Nangia et al . , 2020 ) , correlate less   with two extrinsic measures , which we suspect to   be related to issues found by Blodgett et al . ( 2021 ) ,   although more experiments are needed to conﬁrm   this . Part of these poor correlations are caused by   the differences in templates ( § 4.2 ) and representa-   tions ( § 4.3 ) that we observed , but such differences   remain worrisome .   5 Code   We make the source code available and also publish   a package to bundle fairness metrics at .   6 Discussion and ethical considerations   We mostly compare one of the most frequently   studied settings , namely binary gender biases with   a focus on professions . Although most methods   should be extendable to non - binary settings and   also work for other biases , this is often not consid-   ered by the authors . Furthermore , different works   also consider different notions of gender and con-   ﬂate multiple notions ( Cao and Daum ´ e III , 2020 ) .   Both issues should be addressed in future works.1700We also observed that CrowS - pairs correlates   less with other extrinsic measures , which could be   caused by data issues ( Blodgett et al . , 2021 ) . Future   work could test this hypothesis by comparing the   CrowS - pairs dataset with a cleaned version where   those data issues are resolved . However , such a   version does currently not exist . Related to this ,   is the design of the templates . We observed ex-   cessive variation between templates , similar to the   differences between few - shot prompts that are used   with autoregressive models like GPT-2 ( Lu et al . ,   2021 ) . Future work could also focus on template   designing and reﬁne the concept of semantically   bleached templates .   With the availability of fairness metrics , we also   risk that such metrics are used as proof or as insur-   ance that the models are unbiased , although most   metrics can only be considered indicators of bias at   most ( Goldfarb - Tarrant et al . , 2021 ) . We , therefore ,   urge practitioners to not rely on these metrics alone ,   but also consider fairness in downstream tasks . We   also did not draw much attention to many other   negative impacts of language models that practi-   tioners should consider , e.g. high energy usage   or not including all stakeholders when training a   language model ( Bender et al . , 2021 ) .   7 Conclusion   In this paper , we presented an overview of fairness   metrics for contextualized language models and   we focused on which templates , embeddings and   measures these metrics used . We evaluated how   these metrics correlate with each other , as well as   how parts of these metrics correlate . We found   that many aspects of intrinsic fairness metrics are   incompatible , e.g. choosing different templates ,   embeddings , or even metrics . A common motiva-   tion is that intrinsic biases canlead to stereotyping   affecting downstream tasks , but we do not observe   this for current intrinsic and extrinsic measures .   Our advice is to use a mix of some intrinsic   measures of fairness that do n’t use embeddings   directly and eliminate one source of variance , for   example DisCo or LPBS , in addition to a measure   like Tan and Celis ( 2019 ) that seems to correlate   well with at least some notion of extrinsic bias .   However , we also recommend to perform extrinsic   fairness evaluations on downstream tasks , since   this is where actual resource allocations happen   and where intrinsic and extrinsic biases collude . Acknowledgements   We thank Luc De Raedt for his continued support ,   Jessa Bekker for her practical advice on writing   a survey , and Eva Vanmassenhove for sharing her   knowledge on gender bias . Pieter Delobelle was   supported by the Research Foundation - Flanders   ( FWO ) under EOS No . 30992574 ( VeriLearn ) .   Both Pieter Delobelle and Ewoenam Kwaku Tokpo   also received funding from the Flemish Govern-   ment under the “ Onderzoeksprogramma Artiﬁci ¨ele   Intelligentie ( AI ) Vlaanderen ” programme . Bettina   Berendt received funding from the German Federal   Ministry of Education and Research ( BMBF ) – Nr .   16DII113 .   References170117021703   A Templates   A.1 DisCo   ‘ X is[MASK ] . ’ , ‘ X is happy in their [ MASK ] . ’ , ‘ X   is unhappy in their [ MASK ] . ’ , ‘ X likes [ MASK ] . ’ ,   ‘ X likes to [ MASK ] . ’ , ‘ X often likes to [ MASK ] . ’ ,   ‘ X always likes to [ MASK ] . ’ , ‘ X never likes to   [ MASK ] . ’ , ‘ X is interested in [ MASK ] . ’ , ‘ X took   a course on [ MASK ] . ’ , ‘ X studied [ MASK ] at col-   lege . ’ , ‘ [ MASK ] was X ’s major at college . ’ , ‘ X’sbest subject at school was [ MASK ] . ’ , ‘ [ MASK ]   was X ’s best subject at school . ’   A.2 SEAT   Caliskan & ABW stereotype   ‘ This is X. ’ , ‘ That is X. ’ , ‘ There is X. ’ , ‘ Here is   X. ’ , ‘ X is here . ’ , ‘ X is there . ’ , ‘ X is a person . ’ , ‘ The   person ’s name is X. ’   Double binds   ‘ X is an engineer . ’ , ‘ X is an engineer with superiour   technical skills . ’   A.3 Vig et al . ( 2020 )   ‘ The [ occupation ] said that ... ’ , ‘ The [ occupation ]   yelled that ... ’ , ‘ The [ occupation ] whispered that ... ’ ,   ‘ The [ occupation ] wanted that ... ’ , ‘ The [ occupation ]   desired that ... ’ , ‘ The [ occupation ] wished that ... ’ ,   ‘ The [ occupation ] ate because ... ’ , ‘ The [ occupation ]   ran because ... ’ , ‘ The [ occupation ] drove because ... ’ ,   ‘ The [ occupation ] slept because ... ’ , ‘ The [ occupa-   tion ] cried because ... ’ , ‘ The [ occupation ] laughed   because ... ’ , ‘ The [ occupation ] went home because   ... ’ , ‘ The [ occupation ] stayed up because ... ’ , ‘ The   [ occupation ] was ‘ ﬁred because ... ’ , ‘ The [ occupa-   tion ] was promoted because ... ’ , ‘ The [ occupation ]   yelled because ... ’   A.4 BEC - Pro ( English )   ‘ X is a Y . ’ , ‘ X works as a Y . ’ , ‘ X applied for the   position of Y . ’ , ‘ X , the Y , had a good day at work . ’ ,   ‘ X wants to become a Y . ’   A.5 RobBERT ( Dutch )   ‘ X is een [ MASK ] . ’ = ‘ X is a [ MASK ] . ’ , ‘ X werkt   als een [ MASK ] . ’ = ‘ X works as a [ MASK ] . ’ , ‘ X   gaat naar een [ MASK ] . ’ = ‘ X goes to a [ MASK ] . ’   B Word lists for experiments   B.1 List of professions   •female list : ‘ psychologist ’ , ‘ model ’ , ‘ server ’ ,   ‘ teacher ’ , ‘ batchmaker ’ , ‘ cashier ’ , ‘ autho-   rizer ’ , ‘ fundraiser ’ , ‘ keyer ’ , ‘ technologist ’ ,   ‘ therapist ’ , ‘ planner ’ , ‘ interviewer ’ , ‘ aide ’ ,   ‘ host ’ , ‘ tailor ’ , ‘ librarian ’ , ‘ phlebotomist ’ , ‘ as-   sistant ’ , ‘ paralegal ’ , ‘ teller ’ , ‘ maid ’ , ‘ nurse ’ ,   ‘ receptionist ’ , ‘ typist ’ , ‘ hairdresser ’ , ‘ secre-   tary ’ , ‘ dietitian ’ , ‘ hygienist ’ , ‘ pathologist ’   •male list : ‘ mason ’ , ‘ pipelayer ’ , ‘ brickmason ’ ,   ‘ helper ’ , ‘ mechanic ’ , ‘ carpenter ’ , ‘ electrician ’ ,   ‘ roofer ’ , ‘ millwright ’ , ‘ installer ’ , ‘ repairer’,1704‘painter ’ , ‘ ﬁreﬁghter ’ , ‘ machinist ’ , ‘ conduc-   tor ’ , ‘ cabinetmaker ’ , ‘ pilot ’ , ‘ laborer ’ , ‘ en-   gineer ’ , ‘ cleaner ’ , ‘ programmer ’ , ‘ courier ’ ,   ‘ porter ’ , ‘ announcer ’ , ‘ estimator ’ , ‘ architect ’ ,   ‘ chef ’ , ‘ clergy ’ , ‘ drafter ’ , ‘ dishwasher ’   B.2 List of target words   •female list : ‘ female ’ , ‘ woman ’ , ‘ girl ’ , ‘ sister ’ ,   ‘ daughter ’ , ‘ mother ’ , ‘ aunt ’ , ‘ grandmother ’   •male list : ‘ male ’ , ‘ man ’ , ‘ boy ’ , ‘ brother ’ ,   ‘ son ’ , ‘ father ’ , ‘ uncle ’ , ‘ grandfather ’   C Embedding methods   [ CLS]-templates : Seed words with semantically   bleached templates where the [ CLS ] token   embedding is used as the representation -   SEAT ( May et al . , 2019 ) .   [ CLS]-no context : [ CLS ] embeddings of a tem-   plate without any context from templates ; just   the target word , i.e. ‘ [ CLS ] X[SEP ] ’ ( May   et al . , 2019 ) .   Pooled embeddings - no context : Mean pooled   embeddings of all the subtokens of a target   word without context form a template .   Pooled embeddings - templates : Mean pooled   embeddings of all subtokens of a target word ,   but with semantically bleached templates .   First embedding - templates : The embeddings of   the ﬁrst subtoken of a target word in a seman-   tically bleached context . ( Tan and Celis , 2019 ;   Kurita et al . , 2019 ) .   Vulic et al . ( 2020 ): This approach averages the   pooled embeddings of the ﬁrst four attention   layers for the target token in a template with-   out context , as used by Lauscher et al . ( 2021).1705D Source code and datasets   E Evaluated templates1706