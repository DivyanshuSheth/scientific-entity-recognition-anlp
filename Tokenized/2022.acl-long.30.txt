  Yang Li , Cheng Yu , Guangzhi Sun , Hua Jiang , Fanglei Sun , Weiqin Zu ,   Ying Wen , Yang Yang , Jun WangShanghaiTech University , Cambridge University , Neurowave Ai Limited , Shanghai Jiao Tong University , University College London   Abstract   Modelling prosody variation is critical for syn-   thesizing natural and expressive speech in end-   to - end text - to - speech ( TTS ) systems . In this pa-   per , a cross - utterance conditional V AE ( CUC-   V AE ) is proposed to estimate a posterior proba-   bility distribution of the latent prosody features   for each phoneme by conditioning on acoustic   features , speaker information , and text features   obtained from both past and future sentences .   At inference time , instead of the standard Gaus-   sian distribution used by V AE , CUC - V AE al-   lows sampling from an utterance - specific prior   distribution conditioned on cross - utterance in-   formation , which allows the prosody features   generated by the TTS system to be related to   the context and is more similar to how hu-   mans naturally produce prosody . The perfor-   mance of CUC - V AE is evaluated via a quali-   tative listening test for naturalness , intelligibil-   ity and quantitative measurements , including   word error rates and the standard deviation of   prosody attributes . Experimental results on LJ-   Speech and LibriTTS data show that the pro-   posed CUC - V AE TTS system improves natural-   ness and prosody diversity with clear margins .   1 Introduction   Recently , abundant research have been performed   on modelling variations other than the input text   in synthesized speech such as background noise ,   speaker information , and prosody , as those directly   influence the naturalness and expressiveness of the   generated audio . Prosody , as the focus of this pa-   per , collectively refers to the stress , intonation , and   rhythm in speech , and has been an increasingly   popular research aspect in end - to - end TTS systems   ( van den Oord et al . , 2016 ; Wang et al . , 2017 ; Stan-   ton et al . , 2018 ; Elias et al . , 2021 ; Chen et al . , 2021 ) .   Some previous work captured prosody features ex - plicitly using either style tokens or variational au-   toencoders ( V AEs ) ( Kingma and Welling , 2014 ;   Hsu et al . , 2019a ) which encapsulate prosody in-   formation into latent representations . Recent work   achieved fine - grained prosody modelling and con-   trol by extracting prosody features at phoneme   or word - level ( Lee and Kim , 2019 ; Sun et al . ,   2020a , b ) . However , the V AE - based TTS system   lacks control over the latent space where the sam-   pling is performed from a standard Gaussian prior   during inference . Therefore , recent research ( Dah-   mani et al . , 2019 ; Karanasou et al . , 2021 ) employed   a conditional V AE ( CV AE ) ( Sohn et al . , 2015 ) to   synthesize speech from a conditional prior . Mean-   while , pre - trained language model ( LM ) such as   bidirectional encoder representation for Transform-   ers ( BERT ) ( Devlin et al . , 2019 ) has also been ap-   plied to TTS systems ( Hayashi et al . , 2019 ; Kenter   et al . , 2020 ; Jia et al . , 2021 ; Futamata et al . , 2021 ;   Cong et al . , 2021 ) to estimate prosody attributes im-   plicitly from pre - trained text representations within   the utterance or the segment . Efforts have been de-   voted to include cross - utterance information in the   input features to improve the prosody modelling of   auto - regressive TTS ( Xu et al . , 2021 ) .   To generate more expressive prosody , while   maintaining high fidelity in synthesized speech , a   cross - utterance conditional V AE ( CUC - V AE ) com-   ponent is proposed , which is integrated into and   jointly optimised with FastSpeech 2 ( Ren et al . ,   2021 ) , a commonly used non - autoregressive end - to-   end TTS system . Specifically , the CUC - V AE TTS   system consists of cross - utterance embedding ( CU-   embedding ) and cross - utterance enhanced CV AE   ( CU - enhanced CV AE ) . The CU - embedding takes   BERT sentence embeddings from surrounding ut-   terances as inputs and generates phoneme - level CU-   embedding using a multi - head attention ( Vaswani   et al . , 2017 ) layer where attention weights are de-   rived from the encoder output of each phoneme as   well as the speaker information . The CU - enhanced391CV AE is proposed to improve prosody variation   and to address the inconsistency between the stan-   dard Gaussian prior , which the V AE - based TTS   system is sampled from , and the true prior of   speech . Specifically , the CU - enhanced CV AE is   a fine - grained V AE that estimates the posterior of   latent prosody features for each phoneme based on   acoustic features , cross - utterance embedding , and   speaker information . It improves the encoder of   standard V AE with an utterance - specific prior . To   match the inference with training , the utterance-   specific prior , jointly optimised with the system , is   conditioned on the output of CU - embedding . La-   tent prosody features are sampled from the derived   utterance - specific prior instead of a standard Gaus-   sian prior during inference .   The proposed CUC - V AE TTS system was eval-   uated on the LJ - Speech read English data and the   LibriTTS English audiobook data . In addition to   the sample naturalness measured via subjective lis-   tening tests , the intelligibility is measured using   word error rate ( WER ) from an automatic speech   recognition ( ASR ) system , and diversity in prosody   was measured by calculating standard deviations of   prosody attributes among all generated audio sam-   ples of an utterance . Experimental results showed   that the system with CUC - V AE achieved a much   better prosody diversity while improving both the   naturalness and intelligibility compared to the stan-   dard FastSpeech 2 baseline and two variants .   The rest of this paper is organised as follows .   Section 2 introduces the background and related   work . Section 3 illustrates the proposed CUC - V AE   TTS system . Experimental setup and results are   shown in Section 4 and Section 5 , with conclusions   in Section 6 .   2 Background   Non - Autoregressive TTS . Promising progress has   taken place in non - autoregressive TTS systems to   synthesize audio with high efficiency and high fi-   delity thanks to the advancement in deep learn-   ing . A non - autoregressive TTS system maps the   input text sequence into an acoustic feature or   waveform sequence without using the autoregres-   sive decomposition of output probabilities . Fast-   Speech ( Ren et al . , 2019 ) and ParaNet ( Peng et al . ,   2019 ) requires distillation from an autoregressive   model , while more recent non - autoregressive TTS   systems , including FastPitch ( La’ncucki , 2021 ) ,   AlignTTS ( Zeng et al . , 2020 ) and FastSpeech2 ( Ren et al . , 2021 ) , do not rely on any form of   knowledge distillation from a pre - trained TTS sys-   tem . In this paper , the proposed CUC - V AE TTS   system is based on FastSpeech 2 . FastSpeech 2   replaces the knowledge distillation for the length   regulator in FastSpeech with mean - squared error   training based on duration labels , which are ob-   tained from frame - to - phoneme alignment to sim-   plify the training process . Additionally , FastSpeech   2 predicts pitch and energy from the encoder output ,   which is also supervised with pitch contours and   L2 - norm of signal amplitudes as labels respectively .   The pitch and energy prediction injects additional   prosody information , which improves the natural-   ness and expressiveness in the synthesized speech .   Pre - trained Representation in TTS . It is be-   lieved that prosody can also be inferred from lan-   guage information in both current and surrounding   utterances ( Shen et al . , 2018 ; Fang et al . , 2019 ;   Xu et al . , 2021 ; Zhou et al . , 2021 ) . Such informa-   tion is often entailed in vector representations from   a pre - trained LM , such as BERT ( Devlin et al . ,   2019 ) . Some existing work incorporated BERT   embeddings at word or subword - level into autore-   gressive TTS models ( Shen et al . , 2018 ; Fang et al . ,   2019 ) . More recent work ( Xu et al . , 2021 ) used the   chunked and paired sentence patterns from BERT .   Besides , a relational gated graph network with pre-   trained BERT embeddings as node inputs ( Zhou   et al . , 2021 ) was used to extract word - level seman-   tic representations , thus enhancing expressiveness .   V AEs in TTS . V AEs have been widely adopted   in TTS systems to explicit model prosody varia-   tion . The training objective of V AE is to max-   imise p(x ) , the data likelihood parameterised by   θ , which can be regarded as the marginalisation   w.r.t . the latent vector zas shown in Eq . ( 1 ) .   p(x ) = Z   p(x|z)p(z)dz . ( 1 )   To make this calculation tractable , the marginalisa-   tion is approximated using evidence lower bound   ( ELBO ):   L(x ) = E[logp(x|z ) ]   −βD(q(z|x)∥p(z)),(2 )   where q(z|x)is the posterior distribution of   the latent vector parameterized by ϕ,βis a hy-   perparameter , and D(·)is the Kullback - Leibler   divergence . The first term measures the expected   reconstruction performance of the data from the392   latent vector and is approximated by Monte Carlo   sampling of zaccording to the posterior distribu-   tion . The reparameterization trick is applied to   make the sampling differentiable . The second term   encourages the posterior distribution to approach   the prior distribution which is sampled from during   inference , and βweighs this term ’s contribution .   A large body of previous work on V AE - based   TTS used V AEs to capture and disentangle data   variations in different aspects in the latent space .   Works by Akuzawa et al . ( 2018 ) leveraged V AE to   model the speaking style of an utterance . Mean-   while , Hsu et al . ( 2019a , b ) explored the disentan-   glement between prosody variation and speaker   information using V AE together with adversarial   training . Recently , fine - grained V AE ( Sun et al . ,   2020a , b ) was adopted to model prosody in the la-   tent space for each phoneme or word . Moreover ,   vector - quantised V AE was also applied to discrete   duration modelling by Yasuda et al . ( 2021 ) .   CV AE is a variant of V AE when the data gener-   ation is conditioned on some other information y.   In CV AE , both prior and posterior distributions are   conditioned on additional variables , and the data   likelihood calculation is modified as shown below :   p(x|y ) = Z   p(x|z , y)p(z|y)dz.(3 )   Similar to V AE , this intractable calculation can beconverted to the ELBO form as   L(x|y ) = E[logp(x|z , y ) ]   −βD(q(z|x , y)∥p(z|y ) ) .   To model the conditional prior , a density network   is usually used to predict the mean and variance   based on the conditional input y.   3 CUC - V AE TTS System   The proposed CUC - V AE TTS system , which is   adapted from FastSpeech 2 as shown in Fig . 1 ,   aims to synthesize speech with more expressive   prosody . Fig . 1 describes the model architecture ,   which has two components : CU - embedding and   CU - enhanced CV AE . The CUC - V AE TTS system   takes as input [ u,···,u,···,u],sand   x , where [ u,···,u,···,u]is the cross-   utterance set that includes the current utterance u   and the Lutterances before and after u. Each u   represents the text content of an utterance . Note   thatsis the speaker ID , and xis the reference   mel - spectrogram of the current utterance u. In   this section , the two main components of the CUC-   V AE TTS system will be introduced in detail .   3.1 Cross - Utterance Embedding   The CU - embedding encodes not only the phoneme   sequence and speaker information but also cross-   utterance information into a sequence of mixture393encodings in place of a standard embedding . As   shown in Fig . 1 , the first Lutterances and the   lastLutterances surrounding the current one , u ,   are used as text input in addition to the current   utterance and speaker information . Same as the   standard embedding , an extra G2P conversion is   first performed to convert the current utterance into   phonemes P= [ p , p , · · · , p ] , where Tis the   number of phonemes . Then , a Transformer encoder   is used to encode the phoneme sequence into a se-   quence of phoneme encodings . Besides , speaker   information is encoded into a speaker embedding   swhich is directly added to each phoneme en-   coding to form the mixture encodings Fof the   phoneme sequence .   F= [ f(p),f(p),···,f(p ) ] , ( 4 )   where frepresents resultant vector from the addi-   tion of each phoneme encoding and speaker em-   bedding .   To supplement the text information from the   current utterance to generate natural and expres-   sive audio , cross - utterance BERT embeddings   together with a multi - head attention layer are   used to capture contextual information . To be-   gin with , 2Lcross - utterance pairs , denoted as C ,   are derived from 2L+ 1 neighboring utterances   [ u,···,u,···,u]as :   where c(u , u ) = { [ CLS],u,[SEP],u } ,   which adds a special token [ CLS ] at the beginning   of each pair and inserts another special token [ SEP ]   at the boundary of each sentence to keep track of   BERT . Then , the 2Lcross - utterance pairs are fed   to the BERT to capture cross - utterance information ,   which yields 2LBERT embedding vectors by tak-   ing the output vector at the position of the [ CLS ]   token and projecting each to a 768 - dim vector for   each cross - utterance pair , as shown below :   B= [ b , b,···,b ] ,   where each vector binBrepresents the BERT   embedding of the cross - utterance pair c(u , u ) .   Next , to extract CU - embedding vectors for each   phoneme specifically , a multi - head attention layer   is added to combine the 2LBERT embeddings into   one vector as shown in Eq . ( 6 ) .   G = MHA ( FW , BW , BW),(6)where MHA ( · ) denotes the multi - head attention   layer , W , WandWare linear projection   matrices , and Fdenotes the sequence of mixture   encodings for the current utterance which acts as   the query in the attention mechanism . For simplic-   ity , we denote Eq . ( 6)asG= [ g , g,···,g ]   from the multi - head attention being of length T   and each of them is then concatenated with its cor-   responding mixture encoding . The concatenated   vectors are projected by another linear layer to   form the final output Hof the CU - embedding ,   H= [ h , h,···,h]of the current utterance ,   as shown in Eq . ( 7 ) .   h= [ g , f(p)]W , ( 7 )   where Wis a linear projection matrix . Moreover ,   an additional duration predictor takes Has inputs   and predicts the duration Dof each phoneme .   3.2 Cross - Utterance Enhanced CV AE   In addition to the CU - embedding , a CU - enhanced   CV AE is proposed to conquer the lack of prosody   variation of FastSpeech 2 and the inconsistency   between the standard Gaussian prior distribution   sampled by the V AE based TTS system and the   true prior distribution of speech . Specifically , the   CU - enhanced CV AE consists of an encoder mod-   ule and a decoder module , as shown in Fig . 1 . The   utterance - specific prior in the encoder aims to learn   the prior distribution zfrom the CU - embedding   output Hand predicts duration D. For conve-   nience , the subscript iis omitted in this subsection .   Furthermore , the posterior module in the encoder   takes as input reference mel - spectrogram x , then   model the approximate posterior zconditioned on   utterance - specific conditional prior z. Sampling   is done from the estimated prior by the utterance-   specific prior module and is reparameterized as :   z=µ⊕σ⊗z , ( 8)   where µandσare estimated from conditional   posterior module to approximate posterior distri-   bution N(µ,σ),zis sampled from the learned   utterance - specific prior , and ⊕,⊗are elementwise   addition and multiplication operation . Furthermore ,   the utterance - specific conditional prior module is   conducted to learn utterance - specific prior with   CU - embedding output HandD. The reparame-   terization is as follows :   z=µ⊕σ⊗ϵ , ( 9)394where µ,σare learned from the utterance-   specific prior module , and ϵis sampled from the   standard Gaussian N(0,1 ) . By substituting Eq . ( 9 )   into Eq . ( 8) , the following equation can be derived   for the total sampling process :   z=µ⊕σ⊗µ⊕σ⊗σ⊗ϵ. ( 10 )   During inference , sampling is done from the   learned utterance - specific conditional prior distri-   bution N(µ,σ)from CU - embedding instead of   a standard Gaussian distribution N(0,1 ) . For sim-   plicity , we can formulate the data likelihood calcu-   lation as follows , where the intermediate variable   utterance - specific prior zfromD , Hto obtain z   is omitted :   ( 11 )   In Eq . ( 11),ϕ , θare the encoder and decoder mod-   ule parameters of the CUC - V AE TTS system .   Moreover , the decoder in CU - enhanced CV AE   is adapted from FastSpeech 2 . An additional pro-   jection layer is firstly added to project zto a high   dimensional space so that zcould be added to H.   Next , a length regulator expands the length of in-   put according to the predicted duration Dof each   phoneme . The rest of Decoder is same as the De-   coder module in FastSpeech 2 to convert the hid-   den sequence into an mel - spectrogram sequence   via parallelized calculation .   Therefore , the ELBO objective of the CUC - V AE   can be expressed as ,   ( 12 )   where ϕ , ϕare two parts of CUC - V AE encoder ϕ   to obtain zfromz , xandzfromD , Hrespec-   tively , β , βare two balance constants , p(z)is   chosen to be standard Gaussian N(0,1 ) . Mean-   while , zandzcorrespond to the latent represen-   tation for the n - th phoneme , and Tis the length of   the phoneme sequence .   4 Experimental Setup   4.1 Dataset   To evaluate the proposed CUC - V AE TTS system ,   a series of experiments were conducted on a singlespeaker dataset and a multi - speaker dataset . For the   single speaker setting , the LJ - Speech read English   data ( Ito and Johnson , 2017 ) was used which con-   sists of 13,100 audio clips with a total duration of   approximately 24 hours . A female native English   speaker read all the audio clips , and the scripts were   selected from 7 non - fiction books . For the multi-   speaker setting , the train - clean-100 and train - clean-   360 subsets of the LibriTTS English audiobook   data ( Zen et al . , 2019 ) were used . These subsets   used here consist of 1151 speakers ( 553 female   speakers and 598 male speakers ) and about 245   hours of audio . All audio clips were re - sampled at   22.05 kHz in experiments for consistency .   The proposed CU - embedding in our system   learns the cross - utterance representation from sur-   rounding utterances . However , unlike LJ - Speech ,   transcripts of LibriTTS utterances are not arranged   as continuous chunks of text in their correspond-   ing book . Therefore , transcripts of the LibriTTS   dataset were pre - processed to find the location of   each utterance in the book , so that the first Land   lastLutterances of the current one can be effi-   ciently obtained during training and inference . The   pre - processed scripts and our code are available .   4.2 System Specification   The proposed CUC - V AE TTS system was based   on the framework of FastSpeech 2 . The CU-   embedding utilised a Transformer to learn the cur-   rent utterance representation , where the dimension   of phoneme embeddings and the size of the self-   attention were set to 256 . To explicitly extract   speaker information , 256 - dim speaker embeddings   were also added to the Transformer output . Mean-   while , the pre - trained BERT model to extract cross-   utterance information had 12 Transformer blocks   and 12 - head attention layers with 110 million pa-   rameters . The size of the derived embeddings of   each cross - utterance pair was 768 - dim . Note that   the BERT model and corresponding embeddings   were fixed when training the TTS system . Net-   work in CU - enhanced CV AE consisted of four 1D-   convolutional ( 1D - Conv ) layers with kernel sizes   of 1 to predict the mean and variance of 2 - dim   latent features . Then a linear layer was added to   transform the sampled latent feature to a 256 - dim   vector . The duration predictor which consisted of   two convolutional blocks and an extra linear layer395to predict the duration of each phoneme for the   length regulator in FastSpeech 2 was adapted to   take in CU - embedding outputs . Each convolutional   block was comprised of a 1D - Conv network with   ReLU activation followed by a layer normaliza-   tion and dropout layer . The Decoder adopted four   feed - forward Transformer blocks to convert hidden   sequences into 80 - dim mel - spectrogram sequence ,   similar to FastSpeech 2 . Finally , HifiGAN ( Kong   et al . , 2020 ) was used to synthesize waveform from   the predicted mel - spectrogram .   4.3 Evaluation Metrics   In order to evaluate the performance of our pro-   posed component , both subjective and objective   tests were performed . First of all , a subjective   listening test was performed over 11 synthesized   audios with 23 volunteers asked to rate the natural-   ness of speech samples on a 5 - scale mean opinion   score ( MOS ) evaluation . The MOS results were   reported with 95 % confidence intervals . In addi-   tion , an AB test was conducted to compare the   CU - enhanced CV AE with utterance - specific prior   and normal CV AE with standard Gaussian prior .   23 volunteers were asked to choose the preference   audio generated by different models in the AB test .   For the objective evaluation , Fframe error   ( FFE ) ( Chu and Alwan , 2009 ) and mel - cepstral dis-   tortion ( MCD ) ( Kubichek , 1993 ) were used to mea-   sure the reconstruction performance of different   V AEs . FFE combined the Gross Pitch Error ( GPE )   and the V oicing Decision Error ( VDE ) and was   used to evaluate the reconstruction of the Ftrack .   MCD evaluated the timbral distortion , which was   computed from the first 13 MFCCs in our experi-   ments . Moreover , word error rates ( WER ) from an   ASR model trained on the real speech from the Lib-   riTTS training set were reported . Complementary   to naturalness , the WER metric showed both the   intelligibility and the degree of inconsistency be-   tween synthetic speech and real speech . The ASR   system used in this paper was an attention - based   encoder - decoder model trained on Librispeech 960-   hour data , with a WER of 4.4 % on the test - clean set .   Finally , the diversity of samples was evaluated by   measuring the standard deviation of two prosody   attributes of each phoneme : relative energy ( E )   and fundamental frequency ( F ) , similar to Sun   et al . ( 2020b ) . Relative energy was calculated as   the ratio of the average signal amplitude within a   phoneme to the average amplitude of the entire sen - tence , and fundamental frequency was measured   using a pitch tracker . In this paper , the average   standard deviation of EandFof three phonemes   in randomly selected 11 utterances was reported to   evaluate the diversity of generated speech .   5 Results   This section presents the series of experiments for   the proposed CUC - V AE TTS system . First , abla-   tion studies were performed to progressively show   the influence of different parts in the CUC - V AE   TTS system based on MOS and WER . Next , the   reconstruction performance of CUC - V AE was eval-   uated by FFE and MCD . Then , the naturalness and   prosody diversity using CUC - V AE were compared   to FastSpeech 2 and other V AE techniques . At last ,   a case study illustrated the prosody variations with   different cross - utterance information as an exam-   ple . The audio examples are available on the demo   page .   5.1 Ablation Studies   Ablation studies in this section were conducted on   the LJ - Speech data based on the subjective test and   WER . First , to investigate the effect of the differ-   ent number of neighbouring utterances , CUC - V AE   TTS systems built with L= 1,3,5were evaluated   using MOS scores , as shown in Table 1 .   The effect of the different number of neighbour-   ing utterances on the naturalness of the synthesized   speech can be observed by comparing MOS scores   which is the higher the better . The CUC - V AE with   L= 5 achieved highest score 3.95 compared to   system with L= 1andL= 3 . Since only marginal   MOS improvements were obtained using more than   5neighbouring utterances , the rest of experiments   were performed using L= 5 .   Then we investigated the influence of each part   of CUC - V AE on performance . The baseline was396our implementation of Fastspeech 2 . For the sys-   tem denoted as Baseline + fine - grained V AE which   served as a stronger baseline , the pitch predictor   and energy predictor of FastSpeech 2 were replaced   with a fine - grained V AE with 2 - dim latent space .   Based on the fine - grained V AE baseline , the CV AE   was added without the CU - embedding to the sys-   tem , referred to as Baseline+CV AE to verify the   function of CV AE on the system , which conditions   on the current utterance . Again , MOS was com-   pared among these systems as shown in Table 2 .   As shown in Table 2 , MOS progressively in-   creased when fine - grained V AE , CV AE , and CU-   embedding were added in consecutively . The pro-   posed CUC - V AE TTS system achieved the highest   MOS 3.95 compared to baselines . The results indi-   cated that CUC - V AE module played a crucial role   in generating more natural audio .   To verify the importance of the utterance-   specific prior to the synthesized audio , the same   CUC - V AE system was used , and the only differ-   ence is whether to sample latent prosody features   from the utterance - specific prior or from a stan-   dard Gaussian distribution . A subjective AB test   was performed which required 23 volunteers to pro-   vide their preference between audios synthesized   from the 2 approaches . Moreover , WER was also   compared here to show the intelligibility of the   synthesized audio . As shown in Table 3 , the pref-   erence rate of using the utterance - specific prior is   0.52 higher than its counterpart , and a 4.9 % abso-   lute WER reduction was found , which confirmed   the importance of the utterance - specific prior in our   CUC - V AE TTS system .   5.2 Reconstruction Performance   FFE and MCD were used to measure the re-   construction performance of V AE systems . An   utterance - level prosody modelling baseline which   extract one latent prosody feature vector for an   utterance was added for more comprehensive com-   parison , and is referred to as the Global V AE .   Table . 4 shows the reconstruction performance   on the LJ - Speech dataset and LibriTTS dataset ,   respectively . Baseline had the highest value of FFE   and MCD on the LJ - Speech dataset and LibriTTS   dataset . The value of FFE and MCD decreased   when the global V AE was added and was further   reduced when the fine - grained V AE was added to   the baseline . Our proposed CUC - V AE TTS system   achieved the lowest FFE and MCD across the table   on both the LJ - Speech and LibriTTS datasets . This   indicated that richer prosody - related information   entailed in both cross - utterance and conditional   inputs was captured by CUC - V AE .   5.3 Sample Naturalness and Diversity   Next , sample naturalness and intelligibility were   measured using MOS and WER respectively on   both LJ - Speech and LibriTTS datasets . Comple-   mentary to the naturalness , the diversity of gener-   ated speech from the conditional prior was evalu-   ated by comparing the standard deviation of Eand   Fsimilar to ( Sun et al . , 2020b ) .   LJ - Speech experiments were shown in left part   of Table . 5 . Compared to the global V AE and fine-   grained V AE , the proposed CUC - V AE received   the highest MOS and achieved the lowest WER.397   Although both FandEof the CUC - V AE TTS   system were lower than the baseline + fine - grained   V AE , the proposed system achieved a clearly higher   prosody diversity than the baseline and baseline   + global V AE systems . The fine - grained V AE   achieved the highest prosody variation as its latent   prosody features were sampled from a standard   Gaussian distribution , which lacks the constraint   of language information from both the current and   the neighbouring utterances . This caused extreme   prosody variations to occur which impaired both   the naturalness and the intelligibility of synthesized   audios . As a result , the CUC - V AE TTS system was   able to achieve high prosody diversity without hurt-   ing the naturalness of the generated speech . In   fact , the adequate increase in prosody diversity im-   proved the expressiveness of the synthesized audio ,   and hence increased the naturalness .   The right part of Table . 5 showed the results   on LibriTTS dataset . Similar to the LJ - Speech   experiments , the CUC - V AE TTS system achieved   the best naturalness measured by MOS , the best   intelligibility measured by WER , and the second-   highest prosody diversity across the table . Overall ,   consistent improvements in both naturalness and   prosody diversity were observed on both single-   speaker and multi - speaker datasets .   5.4 A Case Study   To better illustrate how the utterance - specific   prior influenced the naturalness of the synthesized   speech under a given context , a case study was   performed by synthesizing an example utterance ,   “ Mary asked the time ” , with two different neigh-   bouring utterances : “ Who asked the time ? Mary   asked the time . ” and “ Mary asked the time , and was   told it was only five . ” Based on the linguistic knowl-   edge , to answer the question in the first setting , an   emphasis should be put on the word “ Mary ” , while   in the second setting , the focus of the sentence is   “ asked the time ” . The model trained on LJ - Speech   dataset was used to synthesize the utterance and   the results were shown in Fig . 2 .   Fig . 2 showed the energy and pitch of the two   utterance . Energy of the first word “ Mary ” in   Fig . 2(a ) changed significantly ( energy of “ Ma- ”   was much higher than “ -ry ” ) , which reflected an   emphasis on the word “ Mary ” , whereas in Fig . 2(b ) ,   energy of “ Mary ” had no obvious change , i.e. , the   word was not emphasized . On the other hand ,   the fundamental frequency of words “ asked ” and   “ time ” stayed at a high level for a longer time in the   second audio than the first one , reflecting another   type of emphasis on those words which was also   coherent with the given context . Therefore , the   difference of energy and pitch between the two ut-   terances demonstrated that the speech synthesized   by our model is sufficiently contextualized.3986 Conclusion   In this paper , a non - autoregressive CUC - V AE TTS   system was proposed to synthesize speech with bet-   ter naturalness and more prosody diversity . CUC-   V AE TTS system estimated the posterior distribu-   tion of latent prosody features for each phone based   on cross - utterance information in addition to the   acoustic features and speaker information . The   generated audio was sampled from an utterance-   specific prior distribution , approximated based on   cross - utterance information . Experiments were   conducted to evaluate the proposed CUC - V AE TTS   system with metrics including MOS , preference   rate , WER , and the standard deviation of prosody   attributes . Experiment results showed that the pro-   posed CUC - V AE TTS system improved both the   naturalness and prosody diversity in the generated   audio samples , which outperformed the baseline in   all metrics with clear margins .   References399400