  Mobashir Sadat Cornelia Caragea   Computer Science   University of Illinois Chicago   msadat3@uic.edu cornelia@uic.edu   Abstract   Automatic topic classiﬁcation has been stud-   ied extensively to assist managing and index-   ing scientiﬁc documents in a digital collection .   With the large number of topics being avail-   able in recent years , it has become necessary to   arrange them in a hierarchy . Therefore , the au-   tomatic classiﬁcation systems need to be able   to classify the documents hierarchically . In ad-   dition , each paper is often assigned to more   than one relevant topic . For example , a paper   can be assigned to several topics in a hierarchy   tree . In this paper , we introduce a new dataset   for hierarchical multi - label text classiﬁcation   ( HMLTC ) of scientiﬁc papers called SciHTC ,   which contains 186,160papers and 1,233cat-   egories from the ACM CCS tree . We estab-   lish strong baselines for HMLTC and propose   a multi - task learning approach for topic clas-   siﬁcation with keyword labeling as an auxil-   iary task . Our best model achieves a Macro-   F1 score of 34.57 % which shows that this   dataset provides signiﬁcant research opportu-   nities on hierarchical scientiﬁc topic classiﬁca-   tion . We make our dataset and code available   on Github .   1 Introduction   With the exponential increase of scientiﬁc docu-   ments being published every year , the difﬁculty in   managing and categorizing them in a digital col-   lection is also increasing . While the enormity of   the number of papers is the most important reason   behind it , the problem can also be assigned to the   large number of topics . It is a very difﬁcult task to   index a paper in a digital collection when there are   thousands of topics to choose from . Fortunately ,   the large number of topics can be arranged in a   hierarchy because , except for a few general top-   ics , all topics can be seen as a sub - area of another   topic . After arranging the topics in a hierarchy tree ,   the task of categorizing a paper becomes muchsimpler since now there are only a handful of top-   ics to choose from at each level of the hierarchy .   However , manual assignment of topics to a large   number of papers is still very difﬁcult and expen-   sive , making an automatic system of hierarchical   classiﬁcation of scientiﬁc documents a necessity .   After arranging the topics in a hierarchy , the   classiﬁcation task no longer remains a multi - class   classiﬁcation because in multi - class classiﬁcation ,   each paper is classiﬁed into exactly one of sev-   eral mutually exclusive classes or topics . However ,   if the topics are arranged in a hierarchy , they no   longer remain mutually exclusive : a paper assigned   to a topic node ain the hierarchy also gets assigned   to topic node b , wherebis a node in the parental   history ofa . For example , if a paper is classiﬁed   to the area of natural language processing ( NLP ) ,   it is also assigned to the topic of artiﬁcial intelli-   gence ( AI ) given that AI is in the parental history of   NLP . This non - mutual exclusivity among the topics   makes the task a multi - label classiﬁcation task .   Despite being an important problem , hierarchical   multi - label topic classiﬁcation ( HMLTC ) has not   been explored to a great extent in the context of sci-   entiﬁc papers . Most works on hierarchical and/or   multi - label topic classiﬁcation focus on news arti-   cles ( Banerjee et al . , 2019 ; Peng et al . , 2018 ) and   use the RCV-1 dataset ( Lewis et al . , 2004 ) for eval-   uation . This is partly because of a lack of datasets   for hierarchically classiﬁed scientiﬁc papers , which   hinders progress in this domain . Precisely , the ex-   isting multi - label datasets of scientiﬁc papers are   either comparatively small ( Kowsari et al . , 2017 ) or   the label hierarchy is not deep ( Yang et al . , 2018 ) .   Therefore , we address the scarcity of datasets   for HMLTC on scientiﬁc papers by introducing a   new large dataset called SciHTC in which the pa-   pers are hierarchically classiﬁed based on the ACM   CCS tree . Our dataset is large enough to allow   deep learning exploration , comprising 186,160re-8923search papers that are organized into 1,233top-   ics , which are arranged in a six - level deep hierar-   chy . We establish several strong baselines for both   hierarchical and ﬂat multi - label classiﬁcation for   SciHTC . In addition , we conduct a thorough in-   vestigation on the usefulness of author speciﬁed   keywords in topic classiﬁcation . Furthermore , we   show how multi - task learning with scientiﬁc doc-   ument classiﬁcation as the principal task and its   keyword labeling as the auxiliary task can help im-   prove the classiﬁcation performance . However , our   best models with SciBERT ( Beltagy et al . , 2019 )   achieve only 34.57 % Macro - F1 score which shows   that there is still plenty of room for improvement .   2 Related Work   To date , several datasets exist for topic classiﬁ-   cation of scientiﬁc papers . Kowsari et al . ( 2017 )   created a hierarchically classiﬁed dataset of sci-   entiﬁc papers from the Web of Science ( WoS ) .   However , their hierarchy is only two levels deep   and the size of their dataset is 46,985 , which is   much smaller than its counterpart from news source   data . In addition , there are only 141topics in the   entire hierarchy . The Coradataset introduced by   McCallum et al . ( 2000 ) is also hierarchically clas-   siﬁed with multiple labels per paper and contains   about 50,000papers . The hierarchy varies in depth   from one to three and has 79topics in total . How-   ever , the widely used version of Coracontains   only2,708papers ( Lu and Getoor , 2003 ) and is   not hierarchical . Similarly , the labeled dataset for   topic classiﬁcation of scientiﬁc papers from Cite-   Seer(Giles et al . , 1998 ) is also very small in size   containing only 3,312papers with no hierarchy   over the labels . Yang et al . ( 2018 ) created a dataset   of55,840arXiv papers where each paper is as-   signed multiple labels using a two - level deep topic   hierarchy containing a total of 54topics . Simi-   lar to us , Santos and Rodrigues ( 2009 ) proposed   a multi - label hierarchical document classiﬁcation   dataset using the ACM category hierarchy . How-   ever , our dataset is much larger in size than this   dataset ( which has ≈15,000documents in their   experiment setting ) . Furthermore , the dataset by   Santos and Rodrigues ( 2009 ) is not available onlineand can not be reconstructed as the ACM paper IDs   are not provided .   Recently , Cohan et al . ( 2020 ) released a dataset   of25,000papers collected from the Microsoft Aca-   demic Graph(MAG ) as part of their proposed eval-   uation benchmark for document level research on   scientiﬁc domains . Although the papers in MAG   are arranged in a ﬁve level deep hierarchy ( Sinha   et al . , 2015 ) , only the level one categories ( 19top-   ics in total ) are made available with the dataset . In   contrast to the above datasets , SciHTC has1,233   topics arranged in a six level deep hierarchy . The   total number of papers in our dataset is 186,160   which is signiﬁcantly larger than all other datasets   mentioned above . The topic hierarchy of each pa-   per is provided by their respective authors . Since   each paper in our dataset is assigned to all the top-   ics on the path from the root to a certain topic in   the hierarchy tree , our dataset can be referred to as   a multi - label dataset for topic classiﬁcation .   For multi - label classiﬁcation , there are two ma-   jor approaches : a ) training one model to predict   all the topics to which each paper belongs ( Peng   et al . , 2018 ; Baker and Korhonen , 2017b ; Liu et al . ,   2017 ) ; and b ) training one - vs - all binary classiﬁers   for each of the topics ( Banerjee et al . , 2019 ; Read   et al . , 2009 ) . The ﬁrst approach learns to classify   papers to all the relevant topics simultaneously ,   and hence , it is better suited to leverage the inter-   label dependencies among the labels . However ,   despite that it is simpler and more time efﬁcient ,   it struggles with data imbalance ( Banerjee et al . ,   2019 ) . On the other hand , the second approach   gives enough ﬂexibility to deal with the different   levels of class imbalance but it is more complex   and not as time efﬁcient as the ﬁrst one . In gen-   eral , the second approach takes additional steps   to encode the inter - label dependencies among the   co - occurring labels . For example , in hierarchical   classiﬁcation , the parameters of the model for a   child topic can be initialized with the parameters of   the trained model for its parent topic ( Kurata et al . ,   2016 ; Baker and Korhonen , 2017a ; Banerjee et al . ,   2019 ) . In this work , we take both approaches and   compare their performance .   Besides these approaches , another approach for   hierarchical and/or multi - label classiﬁcation in re-   cent years is based on sequence - to - sequence mod-   els ( Yang et al . , 2018 , 2019 ) , which we explored in   this work . However , these models failed to show8924satisfactory performance on our dataset . We also   explored the hierarchical classiﬁcation proposed   by Kowsari et al . ( 2017 ) where a local classiﬁer   is trained at every node of the hierarchy , but this   model also failed to gain satisfactory performance .   Onan et al . ( 2016 ) proposed the use of keywords   together with traditional ensemble methods to clas-   sify scientiﬁc papers . However , since ground truth   keywords were not available for the papers in their   dataset , the authors explored a frequency based key-   word selection , which gave the best performance .   Therefore , their application of keyword extraction   methods for the classiﬁcation task can be seen as a   feature selection method .   OurSciHTC dataset , in addition to being very   large , multi - labeled , and hierarchical , contains the   author - provided keywords for each paper . In this   work , we present a thorough investigation of the   usefulness of keywords for topic classiﬁcation and   propose a multi - task learning framework ( Caruana ,   1993 ; Liu et al . , 2019 ) that uses keyword labeling   as an auxiliary task to learn better representations   for the main topic classiﬁcation task .   3 The SciHTC Dataset   We constructed the SciHTC dataset from papers   published by the ACM digital library , which we   requested from ACM .   Precisely , the dataset provided by ACM has more   than300,000papers . However , some of these pa-   pers did not have their author - speciﬁed keywords ,   whereas others did not have any category informa-   tion . Thus , we pruned all these papers from the   dataset . Finally , there were 186,160papers which   had all the necessary attributes and the category   information . The ﬁnal dataset was randomly di-   vided into train , development and test sets in an   80 : 10 : 10 ratio . The number of examples in each   set can be seen in Table 1 .   The category information of the papers in our   dataset were deﬁned based on the category hierar-   chy tree created by ACM . This hierarchy tree is   named CCS or Computing Classiﬁcation System .   The root of the hierarchy tree is denoted as ‘ CCS ’   and there are 13nodes at level 1 which represent   Topic Index   topics such as “ Hardware , ” “ Networks , ” “ Security   and Privacy , ” etc . Note that CCS itself does not   represent any topic ( or category ) . It is simply the   root of the ACM hierarchy tree . There are 6 lev-   els in the hierarchy tree apart from the root ‘ CCS ’ .   That is , the maximum depth among the leaf nodes   in the tree is 6 . However , note that the depths of   different sub - branches are not uniform and there   are leaf nodes in the tree with a depth less than 6 .   Each paper in our dataset is assigned to one or   more sub - branches of the hierarchy tree by their   respective authors with different depth levels and   relevance scores among { 100,300,500 } with 500   indicating the most relevant . The authors also pro-   vide a set of keywords relevant to their paper . Table   2 shows the assigned sub - branches and keywords   of an example paper , both of them being provided   by the authors . Among the author - speciﬁed sub-   branches , we only consider the sub - branch with   the highest relevance score for each paper . Thus ,   the categories in the ﬁrst sub - branch ( bolded line )   in Table 2 are selected as the labels for the paper .   However , considering all relevant sub - branches can   present a more interesting and challenging task8925which can be explored in future work .   There are 1,233different topics in total in our   ﬁnal dataset . However , we ﬁnd that the distribution   of the number of papers over the topics is very im-   balanced and a few topics ( especially in the deeper   levels of the hierarchy ) had extremely low support   ( i.e. , rare topics ) . Thus , for our experiments , we   only consider the topics up to level 2 of the CCS hi-   erarchy tree which had at least 100examples in the   training set . Figure 1 shows the number of papers   in each of the 95topics up to level 2 of the hierar-   chy tree in our dataset . We also report the explicit   topic distribution ( i.e. , topic name vs. support ) in   Appendix A. Note that since there are 12topics   ( among the 95topics up to level 2 of the hierarchy )   with less than 100examples in the training set , we   remove them and experiment with the remaining   83topics . Although we do not use the topics with   low support in our experiments , we believe that   they can be potentially useful for hierarchical topic   classiﬁcation of rare topics . Therefore , we make   available not only the two - level hierarchy dataset   used in our experiments but also all relevant topics   for each paper from the six - level hierarchy tree .   4 Methodology   This section describes the hierarchical and ﬂat   multi - label baselines used in our experiments   ( § 4.1 ) ; after that , it introduces our simple incor-   poration of keywords into the models ( § 4.2 ) ; lastly ,   it presents our multi - task learning framework for   topic classiﬁcation ( § 4.3 ) .   Problem Deﬁnition Letpbe a paper , tbe a topic   from the set of all topics T , andnbe the number of   all topics inT ; and let xdenote the input text and   ydenote the label vector of size ncorresponding   top . For our baseline models , xis a concatenation   of the title and abstract of p. The goal is to predict   the label vector ygiven xsuch that , ifpbelongs   to topict , y= 1 , andy= 0 otherwise , i.e. ,   identify all topics relevant to p.   4.1 Baseline Modeling   We establish both ﬂat and hierarchical classiﬁcation   approaches as our baselines , as discussed below .   4.1.1 Flat Multi - Label Classiﬁcation   We refer to the classiﬁers that predict all relevant   topics of a paper with a single model as ﬂat multi-   label classiﬁers . Although these models leverage   the inter - label dependencies by learning to predictall relevant labels simultaneously , they do not con-   sider the label hierarchy structure . In the models ,   all layers are shared until the last layer during train-   ing . Instead of softmax , the output layer consists   ofnnodes , each with sigmoid activation . Each   sigmoid output represents the probability of a topic   tbeing relevant for a paper p , witht= 1,···,n .   The architecture is illustrated in Appendix B.   We use the following neural models to obtain   representations of the input text : neural model Bi-   LSTM ( Hochreiter and Schmidhuber , 1997 ) , and   pre - trained language models — BERT ( Devlin et al . ,   2019 ) and SciBERT ( Beltagy et al . , 2019 ) .   Traditional Neural Models We use a BiLSTM   based model similar to Banerjee et al . ( 2019 ) as our   traditional neural baseline . Speciﬁcally , we take   three approaches to obtain a single representation   of the input text from the hidden states of the Bi-   LSTM and concatenate them before they are sent   to the fully connected layers . These approaches are :   element - wise max pool , element - wise mean pool ,   and an attention weighted context vector . The atten-   tion mechanism is similar to the word level atten-   tion mechanism from Yang et al . ( 2016 ) . After the   Bi - LSTM , we use one fully connected layer with   ReLU activation followed by the output layer with   sigmoid activation . The obtained representations   are projected with nweight matrices W∈R.   We also explore a CNN based model as another   neural baseline and report its performance and ar-   chitectural design in Appendix C.   Pre - trained Language Models We ﬁne - tune   base BERT ( Devlin et al . , 2019 ) and SciBERT   ( Beltagy et al . , 2019 ) using the HuggingFacetrans-   formers library . We use the “ bert - base - uncased "   and “ scibert - scivocab - uncased " variants of BERT   and SciBERT , respectively . Both of these language   models are pre - trained on huge amounts of text .   While BERT is pre - trained on the BookCorpus   ( Zhu et al . , 2015 ) and Wikipedia , SciBERT is pre-   trained exclusively on scientiﬁc documents . After   getting the hidden state embedded in the [ CLS ]   token from these models , we send them through   a fully connected output layer to get the classiﬁ-   cation probability . That is , we project the [ CLS ]   token withnweight matrices W∈R. The   language model and classiﬁcation parameters are   jointly ﬁne - tuned.8926[CLS ] Tok1 Tok2 · · · TokN···BERT · · ·   FF FF FF FF···L(θ ) = αL(θ ) + βL(θ )   4.1.2 Hierarchical Multi - Label Classiﬁcation   In this approach , we train none - vs - all binary clas-   siﬁers . As with ﬂat multi - label classiﬁcation , we   use both traditional neural models based architec-   tures and pre - trained language models , which are   similar to the ﬂat architectures described in § 4.1.1   with two key differences . First , the output layer no   longer contains nnumber of nodes . Since we train   binary classiﬁers , we change the architectures by   having output layers with only one node with sig-   moid activation . Second , to leverage the inter - label   dependencies we initialize the model parameters of   a child node in the topic hierarchy tree by its parent   node ’s trained model parameters similar to Kurata   et al . ( 2016 ) ; Baker and Korhonen ( 2017a ) ; Baner-   jee et al . ( 2019 ) . An illustration of this method of   leveraging the topic hierarchy to learn inter - label   dependencies can be seen in Appendix B.   4.2 Incorporating Keywords   We aim to improve upon the baseline models de-   scribed above by incorporating the keywords speci-   ﬁed by the authors of every paper into the model .   The keywords of a paper can provide ﬁne - grained   topical information speciﬁc to a paper and at the   same time are indicative about the general ( coarse-   grained ) topics of the paper . Thus , the keywordscan be seen as a bridge between the general topics   of a paper and the ﬁne details available in it ( see Ta-   ble 2 for examples of general topics and keywords   of a paper for the ﬁne nuances of each ) .   We incorporate the keywords by a simple con-   catenation approach . The input text xis extended   with the keywords kspeciﬁed by the authors of p.   x:= [ x , k ] ( 1 )   We use the same network architectures as in § 4.1 ,   in both ﬂat and hierarchical settings .   Although this approach strikes by its simplicity   and , as we will see in experiments , improves over   the baselines in § 4.1 that use only the title and ab-   stract as input , it is often the case that at test time   the keywords of a paper are not always available ,   which affects the results . Our aim is to build mod-   els that are robust enough even in the absence of   keywords for papers at test time . Our proposal is   to explicitly teach the model to learn to recognize   the keywords in the paper that are indicative of its   topics , using multi - task learning .   4.3 Multi - Task Learning with Keywords   We propose a neural multi - task learning framework   for topic classiﬁcation of scientiﬁc papers where   the main task of topic classiﬁcation is informed by   the keyword labeling auxiliary task which aims to   identify the keywords in the input text .   Keyword Labeling Given an input sequence   x={x,···,x}(e.g . , title and abstract ) , the   objective is to predict a sequence of labels z=   { z,···,z } , where each label zis 1 ( a keyword )   or 0 ( not a keyword ) , i.e. , predict whether a word   in the sequence is a keyword or not . During train-   ing , we do an exact match of the tokenized author-   speciﬁed keywords in the tokenized input text ( ti-   tle+abstract ) and set the keyword label zas 1 for   the positions where we ﬁnd a match in the input   text and 0 otherwise .   Multi - Task Learning Model The architecture   of our multi - task learning model is shown in Figure   2 . It jointly learns two tasks : topic classiﬁcation   and keyword labeling . As can be seen from the ﬁg-   ure , the model has shared layers across both tasks   at the bottom and task - speciﬁc layers at the top . In   the ﬁgure , we show BERT as the encoder to avoid   clutter , but in experiments we use all encoders de-   scribed in § 4.1.8927Shared layers . The input of the model is the se-   quence xofNwords . These words are ﬁrst   mapped into word embedding vectors ( e.g. , by sum-   ming word and positional embeddings in BERT ) ,   which are then fed into the encoder block that pro-   duces a sequence of contextual embeddings ( one   for each input token , including the [ CLS ] token   in the transformer - based models ) .   Task - speciﬁc layers . There are two task - speciﬁc   output layers . The topic classiﬁcation output layer   works the same as discussed in § 4.1 . On the other   hand , the output layer for keywords labeling con-   sists of a fully connected layer with sigmoid acti-   vation which predicts whether a word is a keyword   or not by using each token contextual embeddding .   Training The model is optimized based on both   tasks . During training , two losses are calculated   for the two tasks ( main and auxiliary ) and they are   combined together as a sum . This summed loss is   used to optimize the model parameters . The overall   lossL(θ)in our model is as follows :   L(θ ) = αL(θ ) + βL(θ ) ( 2 )   whereL(θ)is the loss for topic classiﬁcation and   L(θ)is the loss for keyword labeling . αandβare   hyperparameters to scale the losses with different   weights .   5 Experiments and Results   We perform the following experiments . First , we   study the difﬁculty of classifying topics for scien-   tiﬁc papers from our dataset in comparison with   related datasets ( § 5.1 ) . Second , we show the impact   of the hierarchy of topics on models ’ performance   and how incorporating keywords can help improve   the performance further ( § 5.2 ) . Third , we evaluate   the performance of our proposed multi - task learn-   ing approach ( § 5.3 ) . Implementation details are   reported in Appendix D.   5.1 SciHTC vs. Related Datasets   We contrast SciHTC with three related datasets :   Cora Research Paper Classiﬁcation ( McCallum   et al . , 2000 ) , WoS-46985 ( Kowsari et al . , 2017 )   and the Microsoft Academic Graph ( MAG ) dataset   released by Cohan et al . ( 2020 ) .   The WoS dataset does not have the titles of the   papers . Therefore , only the abstracts are used as   the input sequence . For the Cora and MAG datasets   as well as our SciHTC dataset , we use both title   and abstract as the input sequence . We use the train ,   test and validation splits released by Cohan et al .   ( 2020 ) for the MAG dataset but we split the other   two datasets ( Cora and WoS ) in a 80:10:10 ratio   similar to ours because they did not have author   deﬁned splits . For this experiment , our goal was   to compare the degree of difﬁculty of our dataset   with respect to the other related datasets . We thus   choose to experiment with both ﬂat and hierarchi-   cal baseline Bi - LSTM models with 300D Glove   embeddings . On the MAG dataset , we only report   the Flat Bi - LSTM performance since only level 1   categories are made available by the authors ( with   no hierarchy information ) . We experiment with the   categories up to level 2 of the hierarchy tree for the   other datasets . Table 3 shows the Macro F1 scores   of these models on the four datasets along with the   number of topics in each dataset and the size of   each dataset . We ﬁnd that :   SciHTC is consistently more challenging com-   pared with related datasets . As we can see   from Table 3 , both models ( ﬂat and hierarchical )   show a much lower performance on SciHTC com-   pared with the other datasets . It is thus evident   that the degree of difﬁculty is much higher on our   dataset , making it a more challenging benchmark   for evaluation .   An inspection into the categories of the related   datasets revealed that these categories are more   easily distinguishable from each other . For exam-   ple , the categories in WoS and MAG cover broad   ﬁelds of science with small overlaps between them .   They range from Psychology , Medical Science ,   Biochemistry to Mechanical Engineering , Civil   Engineering , and Computer Science . The vocab-   ularies used in these categories/ﬁelds of science   are quite different from each other and thus , the   models learn to differentiate between them more   easily . On the other hand , in our dataset , all papers   are from the ACM digital library which are related   to Computer Science and are classiﬁed to more   ﬁne - grained topics than the ones from the above   datasets . Examples of topics from our dataset in-   clude Network Architectures , Network Protocols,8928   Software Organization and Properties , Software   Creation and Management , Cryptography , Systems   Security , etc . Therefore , it is more difﬁcult for the   models to learn and recognize the ﬁne differences   in order to classify the topics correctly resulting in   lower performance compared to the other datasets .   5.2 Impact of Hierarchy and Keywords   Next , we explore the usefulness of the hierarchy   of topics and keywords for topic classiﬁcation on   SciHTC . We experiment with all of our baseline   models ( ﬂat and hierarchical ) described in § 4.1 and   with the incorporation of keywords described in   § 4.2 . Precisely , each model is evaluated twice : ﬁrst   using only the input sequence ( title+abstract ) with-   out the keywords and second by concatenating the   input sequence with the keywords as in Eq . 1 . We   run each experiment three times and report their   average and standard deviation in Table 4 . As we   can see , the standard deviations of the performance   scores shown by the models are very low . Thisillustrates that the models are stable and easily re-   producible . We make the following observations :   The hierarchy of topics improves topic classiﬁ-   cation . We can observe from Table 4 that all hi-   erarchical models show a substantially higher per-   formance than their ﬂat counterparts regardless of   using keywords or not . Given that the ﬂat models   learn to predict all relevant labels for each docu-   ment simultaneously , it is possible for them to learn   inter - label dependencies to some extent . However ,   due to the unavailability of the label hierarchy , the   nature of the inter - label dependencies is not speci-   ﬁed for the ﬂat models . As a result , they can learn   some spurious patterns among the labels which are   harmful for overall performance . In contrast , for   the hierarchical models we can specify how the   inter - label dependencies should be learned ( by ini-   tializing a child ’s model with its parent ’s model )   which helps improve the performance as we can   see in our results.8929Incorporating keywords brings further im-   provements . From Table 4 , we can also see that   the performance of all of our baseline models in-   creases when keywords are incorporated in the in-   put sequence . These results illustrate that indeed ,   the ﬁne - grained topical information provided by   the keywords of each paper is beneﬁcial for pre-   dicting its categorical labels ( and thus capture an   add - up effect for identifying the relevant coarser   topics ) . Moreover , keywords can provide addi-   tional information which is unavailable in the title   and the abstract but is relevant of the rest of the   content and indicative of the topic of the paper .   This additional information also helps the models   to make better predictions .   Transformer - based models consistently outper-   form Bi - LSTM models and SciBERT performs   best . BERT and SciBERT show strong perfor-   mance across all settings ( hierarchical vs. ﬂat and   with keywords vs. without ) in comparison with   the BiLSTM models . Interestingly , even the ﬂat   transformer based models outperform all BiLSTM   based models ( including hierarchical ) . We believe   that this is because BERT and SciBERT are pre-   trained on a large amount of text . Therefore , they   are able to learn better representations of the words   in the input text . Comparing the two transformer   based models ( BERT and SciBERT ) , SciBERT   shows the better performance . We hypothesize that   this is because SciBERT ’s vocabulary is more rel-   evant to the scientiﬁc domain and it is pre - trained   exclusively on scientiﬁc documents . Hence , it has   a better knowledge about the language used in sci-   entiﬁc documents .   5.3 Multi - task Learning Performance   The results in Table 4 show that the keywords are   useful for topic classiﬁcation but it is assumed that   these keywords are available for papers not only   during training but also at test time . However , of-   ten at test time the keywords of a paper are not   available . We turn now to the evaluation of models   when keywords are not available at test time . We   compare our multi - task approach ( § 4.3 ) with the   models trained with concatenating the keywords   in the input sequence ( during training ) but tested   only on the input sequence without keywords . The   motivation behind this comparison is to understand   the difference in performance of the models which   leverage keywords during training in a manner dif-   ferent from our multi - task models but not at testtime ( same as the models trained with our multi-   task approach ) . These results are shown in Table 5 .   We found that :   Multi - task learning effectively makes use of   keywords for topic classiﬁcation . A ﬁrst ob-   servation is that not making use of gold ( author-   speciﬁed ) keywords at test time ( but only during   training KWthrough concatenation using Eq . 1 )   decreases performance ( see Table 4 bottom half and   Table 5 top half ) . Remarkably , the multi - tasking   models ( which also do not use gold keywords at   test time ) are better at classifying the topics than   the models that use keywords only during train-   ing through concatenation . In addition , comparing   the models that do not use keywords at all and the   multi - task models ( top half of Table 4 and bottom   half of Table 5 ) , we can see that the multi - task mod-   els perform better . Furthermore , the performance   of the multi - tasking models is only slightly worse   compared with that of the models that use gold   ( author - speciﬁed ) keywords both during train and   test ( see bottom halves of Tables 4 and 5 ) . These   results indicate that the models trained with our   multi - task learning approach learn better represen-   tations of the input text which help improve the   classiﬁcation performance , thereby harnessing the   usefulness of author - speciﬁed keywords even in   their absence at test time .   6 Analysis and Discussion   From our experiments , it is evident that all of our hi-   erarchical baselines can outperform their ﬂat coun-   terparts . But it is not clear whether the performance   gain comes from using the hierarchy to better learn   the parent - child dependency or it is because we   allow the models to focus on each class individu-   ally by training one - vs - all binary classiﬁers in our   hierarchical setting as opposed to one ﬂat model   for all the classes . In addition , our experiments   also show that keywords can be used in multiple   ways to improve topic classiﬁcation performance .   However , it is unclear whether or not keywords by   themselves can achieve the optimal performance .   Thus , we analyze our models in these aspects with   the following experiments .   Hierarchical vs. n - Binary We conduct an ex-   periment with SciBERT where we train a binary   classiﬁer for each class similar to the hierarchical   SciBERT model but do not initialize it with its par-   ent ’s model parameters , i.e. , we do not make use of   the topic hierarchy . We compare the performance8930   of this n - binary - SciBERT model with HR - SciBERT   model in Table 6 .   We can see that the non - hierarchical approach   withnbinary models has more than 2percentage   points lower Macro F1 . The performance of deep   learning models depends partly on how their param-   eters are initialized ( Bengio et al . , 2017 ) . For the   n - binary approach , since we initialize the model   parameters for each class with a SciBERT model   pre - trained on unsupervised data , it is forced to   learn to distinguish between the examples belong-   ing to this class and the examples from all other   classes from scratch . In contrast , when the model   parameters for a node in the topic hierarchy are   initialized with its parent node ’s trained model ( for   HR models ) , we start with a model which already   knows a superset of the distinct characteristics of   the documents belonging to this node ( i.e. , the char-   acteristics of the papers which belong to its parent   node ) . In other words , the model does not need   to be trained to classify from scratch . Therefore ,   the hierarchical classiﬁcation setup acts as a better   parameter initialization strategy which leads to a   better performance .   With Keywords vs. Only Keywords We exper-   iment with ﬂat BiLSTM , BERT and SciBERT mod-   els with only keywords as the input . A comparison   of these only keywords models with the models   which use title , abstract and keywords can be seen   in Table 7 . We can see a decline of ≈12%,≈8 %   and≈5%in Macro F1 for BiLSTM , BERT and   SciBERT , respectively , when only keywords are   used as the input . Therefore , we can conclude that   keywords are useful in topic classiﬁcation but that   usefulness is evident when other sources of input   are also available .   7 Conclusion   In this paper , we introduce SciHTC , a new dataset   for hierarchical multi - label classiﬁcation of scien-   tiﬁc papers and establish several strong baselines .   Our experiments show that SciHTC presents a   challenging benchmark and that keywords can play   a vital role in improving the classiﬁcation perfor-   mance . Moreover , we propose a multi - task learning   framework for topic classiﬁcation and keyword la-   beling which improves the performance over the   models that do not have keywords available at test   time . We believe that SciHTC is large enough for   fostering research on designing efﬁcient models   and will be a valuable resource for hierarchical   multi - label classiﬁcation . In our future work , we   will explore novel approaches to further exploit the   topic hierarchy and adopt few - shot and zero - shot   learning methods to handle the extremely rare cate-   gories . We will also work on creating datasets from   other domains of science with similar characteris-   tics as SciHTC to allow further explorations .   8 Limitations   One limitation of our proposed dataset could po-   tentially be that all of our papers are from the com-   puter science domain and therefore , it does not pro-   vide coverage to papers from other scientiﬁc areas .   However , we see this as a strength of our dataset   rather than a weakness . There are other datasets   already available which cover a diverse range of   scientiﬁc areas ( e.g. , WoS ) . In contrast , we address   the lack of a resource which can be used to study hi-   erarchical classiﬁcation among ﬁne - grained topics ,   with potential confusable classes . SciHTC can be   used as a benchmark for judging the models ’ abil-   ity in distinguishing very subtle differences among   documents belonging to closely related but differ-   ent topics which will lead to development of more   sophisticated models .   Acknowledgements   This research is supported in part by NSF CAREER   award # 1802358 , NSF CRI award # 1823292 , NSF   IIS award # 2107518 , and UIC Discovery Partners   Institute ( DPI ) award . Any opinions , ﬁndings , and   conclusions expressed here are those of the authors   and do not necessarily reﬂect the views of NSF and   DPI . We thank AWS for computational resources .   We also thank our anonymous reviewers for their   constructive feedback.8931References89328933BERT   Input Paper , pσ(FF)σ(FF)σ(FF)···ˆy= 0/1ˆy= 0/1 ˆy= 0/1 · · ·   A Label Distribution   We can see the explicit label distribution showing   the number of topics belonging to each topic up to   level 2 of the category hierarchy tree in Table 9 .   B Flat & Hierarchical Model   Architectures   Figure 3 illustrates the architecture of our ﬂat multi-   label classiﬁcation baselines . Here , we show BERT   as the encoder to avoid clutter but we also use BiL-   STM , XML - CNN and SciBERT as encoders as we   describe in Section 4 . We can see that the encoder   is shared by all topics and there is one feed - forward   layer for each topic , t= 1,2, ... ,n . A sigmoid acti-   vation is applied to the feed - forward layers ’ output   to predict whether each corresponding topic is rele-   vant to an input paper or not ( 1 or 0 ) .   We can also see an example of leveraging topic   hierarchy to learn inter - label dependencies in Fig-   ure 4 . Here , all models are binary classiﬁers for a   single label from the topic hierarchy . θ , θandθ   represent the model parameters of topics a , bandc ,   respectively where ais the parent topic of bandc   in the hierarchy tree . Both θandθare initialized   withθto encode inter - label dependencies and then   ﬁne - tuned to predict whether topic band topicc   are relevant to an input paper or not.θ 0/1   θ:=θ 0/1θ:=θ 0/1   θ 0/1θ 0/1   C CNN Models and Results   We follow the XML - CNN architecture proposed   by Liu et al . ( 2017 ) , which consists of three convo-   lution ﬁlters on the word embeddings of the input   text . The outputs from the convolution ﬁlters are   pooled with a certain window . The pooled output   then goes trough a bottleneck layer where the di-   mensionality of the output is reduced to make it   computationally efﬁcient . The output from the bot-   tleneck layer is then sent to the output layer for   topic prediction .   Note that Bi - LSTM , BERT and SciBERT give   a contextual representation for every word in the   input text which can be used for sequence labeling .   This is not necessarily true for CNN . To ensure we   have a representation of every word in the input   text from CNN ﬁlters , the ﬁlter sizes are selected   in such a way that the number of output vectors   match the length of the input text , as presented in   ( Xu et al . , 2018 ) . Having a corresponding represen-   tation for each token is necessary for our multi - task   objective . We can see the results of this model in   Table 8 .   D Implementation Details   We started pre - processing our data by converting   title , abstract and keywords to lower case letters .   Then , we removed the punctuation marks for the   LSTM and CNN models . The text was tokenized   using the NLTK tokenizer . After tokenizing the8934   text , we stemmed the tokens using Porter stem-   mer . Finally , we masked the words which oc-   cur less than two times in the training set with an   < unk > tag . The rest of the unique words were   used as our vocabulary .   We address the imbalance of classes in our   data by assigning the following weights to the   examples of the positive class while training our   CNN and LSTM based hierarchical classiﬁers :   1,3,5,10,15 ... 40 . The best weight was chosen based   on the model ’s F1 score on the validation set . How-   ever , for the ﬂat multi - label classiﬁers , we could   not try this method because ﬁnding the optimal   combination of weights would take exponential   time . We also did not try this approach for hi-   erarchical BERT and Sci - BERT because they are   already very time consuming and expensive . We   tuned the sigmoid thresholds from 0.1−0.9on   the validation data and the thresholds with the   highest performance scores were chosen for every   class separately . We tune the loss scaling param-   eters [ α , β]for our multi - task objective with the   following values : [ 0.3,0.7],[0.4,0.6],[0.5,0.5 ] ,   [ 0.6,0.4],[0.7,0.3],[1,1]on the development set   and found that the models show the best perfor-   mance with [ 1,1 ] .   For all our experiments , the maximum lengths   for input text ( title+abstract ) sequence and key-   words sequence was set to 100 and 15 respectively .   We used pre - trained 300 dimensional Gloveem-   beddings to represent the words for LSTM and   CNN based models . The hidden state size for the   bidirectional LSTMs were kept at 72 across all   our models . The fully connected layer after the bi-   LSTM layer has size 16 for the hierarchical models   and 72 for the ﬂat models . We tried to keep them   both at size 16 . However , the ﬂat LSTM model   showed very unstable performance with a hiddenlayer of size 16 . The ﬁlter sizes for the XML - CNN   was chosen as 3 , 5 and 9 and the number of ﬁlters   for each of the sizes were set at 64 . The input text   was padded by 1 , 2 and 4 units for each of the ﬁlter   windows , respectively . The pooling window was   set at 32 and the bottleneck layer converted the   pooled output to a vector of length 512 .   We used binary cross - entropy as our loss func-   tions for both classiﬁcation and keyword labeling   tasks in all our models . Adam optimizer ( Kingma   and Ba , 2014 ) was used to train the models with   mini - batch size 128 . Except the transformer based   models , the initial learning rate for all other mod-   els was kept at 0.001 . For BERT and Sci - BERT ,   the learning rate was set to 2e . The hierarchical   LSTM and CNN based models were trained for   10 epochs each . We employed early stopping with   patience equal to 3 . On the other hand , ﬂat - LSTM   and ﬂat - XML - CNN models were trained for 50   epochs with patience 10 . The ﬂat and hierarchical   transformer based models were ﬁne tuned for 5 and   3 epochs , repectively . We ran our experiments on   NVIDIA Tesla K80 GPUs . The average training   time was 2 days for the hierarchical LSTM models   and additional∼24 hours with the multi - task ap-   proach . Hierarchical CNN models took ∼24 hours   to train with additional 4/5 hours more with the   multi - task approach . The ﬂat models took less than   1 hour to train for both LSTM and CNN . The ﬂat   transformer based models took ∼14 hours to train   on one GPU . We used 8 of the same NVIDIA Tesla   K80 GPUs to train the hierarchical transformer   based models . It took ∼6days to train all 83   binary models.893589368937