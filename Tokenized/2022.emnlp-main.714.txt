  Pei Zhou Hyundong Cho Pegah Jandaghi Dong - Ho Lee Bill Yuchen Lin   Jay Pujara Xiang Ren   Department of Computer Science and Information Sciences Institute   University of Southern California   Abstract   Human communication relies on common   ground ( CG ) , the mutual knowledge and be-   liefs shared by participants , to produce coher-   ent and interesting conversations . In this paper ,   we demonstrate that current response gener-   ation ( RG ) models produce generic and dull   responses in dialogues because they act reflex-   ively , failing to explicitly model CG , both due   to the lack of CG in training data and the stan-   dard RG training procedure . We introduce Re-   flect , a dataset that annotates dialogues with   explicit CG ( materialized as inferences approxi-   mating shared knowledge and beliefs ) and solic-   its 9k diverse human - generated responses each   following one common ground . Using Reflect ,   we showcase the limitations of current dialogue   data and RG models : less than half of the re-   sponses in current data is rated as high quality   ( sensible , specific , and interesting ) and models   trained using this data have even lower qual-   ity , while most Reflect responses are judged   high quality . Next , we analyze whether CG can   help models produce better quality responses   by using Reflect CG to guide RG models . Sur-   prisingly , we find that simply prompting GPT3   to “ think ” about CG generates 30 % more qual-   ity responses , showing promising benefits to   integrating CG into the RG process .   1 Introduction   Human communication is a collaborative ef-   fort ( Grice , 1975 ; Allwood , 1976 ; Bohm et al . ,   2004 ) where participants strive to achieve com-   mon ground ( CG ) , consisting of mutual beliefs and   common knowledge ( Stalnaker , 1978 ; Clark and   Schaefer , 1989 ; Clark and Brennan , 1991 ) . Conver-   sational AI systems , while able to produce fluent   texts , often generate generic and dull dialogue re-   sponses ( Serban et al . , 2017 ; Zhao et al . , 2017 ) ,   potentially because they do not explicitly modelFigure 1 : A motivating example . We aim to help RG   models produce more human - like responses instead of   generic ones . We argue that integrating common ground   by making inferences is crucial .   CG in communication ( as illustrated in Figure 1 ) .   Specifically , existing models mostly follow a dia-   logue history →response training paradigm since   such data can be easily obtained in the wild , skip-   ping an important middle step that builds common   ground , which naturally and universally exists in   human communication , i.e. , dialogue history →   common ground →response . Moreover , the same   history can yield numerous responses , predicated   on the CG and intent of the responder . We conjec-   ture that the omission of modeling CG explicitly   is a crucial bottleneck in RG models because they   are directly trained to produce responses without   learning how and why those responses are uttered .   Modeling common ground between speakers ,   however , is challenging due to its implicit and   subjective nature during conversations(Clark and   Schaefer , 1989 ) . Prior work on representing   CG either mines noisy commonsense knowledge   triples between dialogue history and existing re-10450sponses ( Zhou et al . , 2022 ) or collects human in-   ferences after reading the whole dialogue as a by-   stander ( Ghosal et al . , 2022 ) . Such approaches   provide useful augmentation , but post - hoc analy-   sis can not mirror the generative process and intent   of diverse human dialogue . Figure 2 illustrates   three paradigms for RG . We argue that truly model-   ing this generative process requires ( 1 ) articulating   CG prior to the response ; ( 2 ) generating responses   conditioned on CG ; ( 3 ) differentiating response   generation based on different types of CG .   To this end , we formalize common ground in   dialogues as inferences made by oneparticipant to   approximate potential beliefs shared by other par-   ticipants , as shown in Figure 1 . In this work , we in-   stantiate inferences as question - answer ( QA ) pairs   in natural language ( NL ) such as “ What might hap-   pen later ? ” “ They might need to clean the floor ”   to elicit others ’ beliefs , inspired by inquiry - based   dialogic learning ( Bruner , 1961 ; Habermas , 1985 ;   Wells , 2000 ) . Another critical aspect of CG is its   multi - dimensional nature , i.e. , given the same dia-   logue context , different plausible inferences can be   made , which then lead to different responses . Fol-   lowing these principles , we create a novel dialogue   resource with multiple explicitly human - annotated   common ground , each of which is further substanti-   ated as a next - turn response continuing the conver-   sations ( an example of expanded CG and responses   for one context shown in Figure 3 ) .   We design a two - stage data collection process   by first asking crowdsourcing workers to answer   different inference questions eliciting beliefs about   CG ( e.g. , what is the speaker feeling right now ? )   Answers rely on common sense , and adopt the   point of view of the conversational respondent .   We use these QA pairs to approximate various   ( non - exhaustive ) inference dimensions to extend   the common ground ( e.g. , empathy and event   causality ) . Our second step converts these CG   into dialogue responses by asking different work-   ers to write a coherent response based on the an-   swer / inference collected in the first stage . Our col-   lected data Reflect contains 9k diverse responses   from 600 dialogue contexts , based on 5 inference   dimensions for CG .   Using Reflect , we first test our hypothesis that ex-   plicitly modeling CG and using CG to construct re-   sponses creates more engaging conversations . We   conduct human evaluation to compare the qual-   ity of responses between Reflect and “ reflex ” style   datasets and models in terms of sensibility , speci-   ficity , and interestingness . We find that , com-   pared to reflex - prone human - written and machine-   generated dialogues , our two - stage data collection   process results in more responses that are sensible ,   specific , and interesting as rated by humans . This   highlights limitations of existing data collection   procedures and models trained on the data .   Next , we look to study the potential of explic-   itly modeling CG in dialogue systems to help build   models that can create more engaging conversa-   tions . As a case study , we use the inference di-   mensions from Reflect and test two simple ways   to guide RG using CG . We surprisingly find that   simple approaches such as appending an inference   question to the dialogue context before the response   in the few - shot ( FS ) in - context examples ( from Re-   flect ) help GPT3 - 175B ( Brown et al . , 2020 ) gen-   erate almost 30 % more responses that are deemed   sensible , specific , and interesting than vanilla FS   learning GPT3 ( no inference question ) . We demon-   strate that , when prompted to “ think ” about an in-   ference question ( approximated CG ) , large models   such as GPT-3 can create more engaging conversa-   tions . We also find that such effect is only shown   in large models like GPT-3 as we find BlenderBot-   440 M ( Roller et al . , 2021 ) benefits from fine - tuning   onReflect , but appending inference questions does   not further increase response quality .   In summary , our contributions are as follows : 1 )   we operationalize theories about common ground   and formalize them for dialogue ; 2 ) we collect   the first large - scale ( 9k responses ) dialogue dataset   with diverse responses guided by CG and release   this resource to facilitate training and evaluation ; 3)10451we show important limitations of existing dialogue   data and RG models that detract from engaging   communication ; 4 ) we demonstrate that CG can   dramatically improve RG quality even with simple   prompting , boosting quality by 30 % . The resources   and results from this work promise to enable the re-   search community to create and evaluate common   ground - aware RG models .   2 Inference - Based Common Ground   We formally introduce the notion of common   ground in conversations as the implicit variable   conditioned on dialogue history and provides con-   ditions to the next - turn response .   2.1 Grounding in Communication   Successful collaborative communication activity   relies on mutual understanding of shared knowl-   edge and beliefs ( Clark and Brennan , 1991 ; Bohm   et al . , 2004 ) called common ground . However , due   toleast collaborative effort ( Grice , 1975 ; Clark and   Schaefer , 1989 ) where communication participants   try to minimize the effort spent on contributing to   the interaction , establishing CG relies on signals   other than the surface communication information   ( i.e. , actual utterances in a conversation ) . While hu-   mans in face - to - face communication receive some   information from non - verbal signals such as ges-   tures and facial expressions , virtual systems such   as chatbots often do not have access to such signals .   Thus , we argue that they have to rely heavily on   another crucial way of getting signals for establish-   ing CG : making inferences based on the surface   communication utterances and common sense , in   order to approximate two humans talking to create   engaging conversations .   Furthermore , building CG by making relevant   inferences also connects closely with the “ dual pro-   cess ” theories of human reasoning ( Stanovich and   West , 2000 ; Evans , 2003 ; Kahneman , 2011 ) . We   argue that the “ reflexive ” RG is mostly modeling   “ System 1 ” which is intuitive and associative , but a   more deliberative and logical “ System 2 ” is lacking .   2.2 Formulating CG in Dialogue   Consider three high - level components in commu-   nication efforts : context C(often materialized as   dialogue history consisting of a sequence of ncon-   tributions C = c , ... , c ) , common ground G , and   a new contribution continuing the context ( often   referred to as a “ response ” c. Specifically , for   common ground G , we focus on signals gained   from inferences and thus materialize Gas a list   ofmpotential inferences G = I , ... , Icondi-   tioned on the context . We furthermore materialize   each inference as a QA pair in NL I= ( Q , A )   ( examples included in Figure 3 between Stage 1   and 2 ) . We use QA format to express inferences   to mimic inquiry - based dialogic learning ( Bruner ,   1961 ; Habermas , 1985 ; Wells , 2000 ) and follow   empirical evidence that neural models take in QA-   format knowledge effectively ( Shwartz et al . , 2020 ;   Zhou et al . , 2022 ) .   3 Collecting Reflect Data   Here we describe how we collect Reflect , a novel   large - scale dialogue dataset with diverse human-   annotated inference - based CG and grounded re-   sponses . An overview of the procedure with ex-   amples are shown in Figure 3 . We first select base   dialogues from a dataset that is constructed with-   out considering CG and only has one plausible   response for each context ( 3.1 ) . Then we aim to   expand and collect multiple responses based on   different inference dimensions . We introduce a   two - stage process to first crowdsource potential in-   ferences people make in conversations ( 3.2 ) and   then ask a second cohort of workers to generate di-   verse responses based on the inferences ( 3.3 ) . We   designed a two - stage data collection to 1 ) collect   multiple , diverse responses based on each CG ; 2 )   to allow response writers to validate CG as high   quality , generic common sense inferences . Finally ,   we include discussions of data quality assurance   ( 3.4 ) .   3.1 Pre - Collection : Selecting Base Dialogue   Turns for Expansion   Our first step is to select base dialogues and dia-   logue turns to expand on , in terms of both inference-10452   based CG and more potential responses following   the CG . One important criterion for base turns is   that they should not be “ social glue ” turns such   as “ You are welcome ” in responding to “ Thank   you ! ” We aim at expanding turns that have   semantically - rich dialogue context , enabling dif-   ferent plausible inferences to be made . After inves-   tigation of existing datasets , we use dialogues from   Commonsense - Focused Dialogues ( Zhou et al . ,   2021 ) that are converted to dialogues from So-   cialIQA ( Sap et al . , 2019b ) contexts . We chose this   dialogue data because SocialIQA ( crowdsourced   from ATOMIC ( Sap et al . , 2019a ) , an if - then in-   ferential commonsense knowledge base ) contains   everyday situations where people can make various   inferences on . Then , to select what turns to expand   on , we use simple heuristics and select the turn   that has the largest semantic overlap with the event   in SocialIQA using SentenceBERT ( Reimers and   Gurevych , 2019 ) .   3.2 Stage 1 . Inference Collection   Our first goal is to collect potential inferences peo-   ple might make ( e.g. “ they might be feeling bad ” )   given conversation contexts Cto approximate com-   mon ground . Each inference Iis further material-   ized as a QA pair ( Q , A)along multiple inference   dimensions as formulated in Section 2.2 .   Inference Knowledge Schema We adopt infer-   ence dimensions from ATOMIC2020 ( Hwang et al . ,   2021 ) since it focuses on social commonsense in-   ferences based on everyday scenarios . Specifi-   cally , we conduct a pilot study to choose 5 dimen-   sions from the 14 dimensions , consolidating those   that overlap ( e.g. , “ what might happen later ” and   “ what would others likely want to do after ” ) in the   context of dialogues . Our final five dimensions   for conversation - based inference dimensions are   shown in Table 1 .   Crowdsourcing Our Stage 1 crowdsourcing task   is : given a dialogue context , imagine that you are   participating as the responder and write answers   to the 5 inference questions ( more details in Ap-   pendix ) . We recruit a group of around 30 crowd-   sourcing workers from Amazon ’s Mechanical Turk   platform ( AMT ) who are native English speakers   and provide detailed feedback . Specifically , after   carefully reading collected inferences from pilot   studies , we provide feedback to turkers by stress-   ing on several principles to make the inferences   collected more closely approximate CG , shown in   Figure 4 .   3.3 Stage 2 . Response Collection   After the first stage , we have collected 5 inferences   ( approximating CG ) in the form of QA pairs for   each dialogue context . Our next step is to collect   next - turn responses given both the dialogue con-   text and the collected inference - based CG along   different dimensions . To account for diversity in   responses , for each dialogue context we ask three   Turkers to write a next - turn response based on each   of the given inferences , yielding 15 responses for   each dialogue context . Similarly to Stage 1 , we   communicate our collection principles to workers   to improve the collected data quality ( Figure 4 ) .   Both Stage 1 and Stage 2 UI and positive / negatives   examples for workers are included in Appendix .   3.4 Quality Control and Analysis   Quality check for Inference Collection In our   second stage for response collection , we ask work-   ers an additional question before writing a response :   “ do you think the shown inference answer is a valid10453reaction from the responder ? ” as a way to check   the quality of the first collection stage results . We   find that less than 7 % ( 200/3000 ) of the inferences   are deemed implausible by second stage workers   and only keep the inferences where most workers   agree that the inferences are plausible .   Quality check for Response Collection To   check quality for our stage 2 response results ,   we randomly sampled around 5 % of collected re-   sponses ( 500 ) and conduct a manual in - house check   for two criteria : 1 ) is it a sensible continuation   from the dialogue context ? and 2 ) is the response   based on the inference given ? We find that around   93 % of the responses are a sensible continuation   and 89 % are following the inferences given . Fur-   ther human ratings of our collected grounded dia-   logue responses showing that our data improves the   sensibility , specificity , and interestingness aspects   compared to the base responses are included and   discussed in Section 4 .   Comparison to prior work on representing CG   We compare CG inferences from Reflect with   TBS ( Zhou et al . , 2022 ) and CICERO ( Ghosal et al . ,   2022 ) , two prior work that aims to represent CG   in dialogues using either ConceptNet ( Speer et al . ,   2017 ) knowledge triples or post - hoc human anno-   tations , respectively . Note we only compare in-   ferences ( CG ) since neither collects new dialogue   responses grounded in the inferences , and only con-   sider a single response per context . Comparison   results on sampled 100 inferences for each resource   are shown in Table 2 where we find that inferences   inReflect are rated as make more sense and relevant   to dialogue context than the prior dataset .   4 Limitations of Reflex - Prone Dialogue   Data and Models   Most existing open - domain dialogue datasets are   either crowdsourced by workers who do not have   strong incentives to create engaging conversa-   tions ( Rashkin et al . , 2019 ; Zhou et al . , 2021 )   or crawled from language learning websites and   exams ( Li et al . , 2017 ; Cui et al . , 2020 ) . Both   lack explicit CG . These collection processes can   fail to capture engaging human - like conversations   through under - specified response criteria . Accord-   ingly , RG models trained on these data may mimic   generic patterns . This section aims to demonstrate   such limitations by comparing responses from Re-   flect with responses from both the original dialogue   dataset we expand on and models trained on the   dialogue history →response regime .   4.1 Human Evaluation Dimensions - SSI   We evaluate the quality of each response by head-   to - head comparing across systems along several   evaluation criteria . We follow the protocol used   by LaMDA ( Thoppilan et al . , 2022 ) and measure   SSI : sensibleness , specificity , and interestingness .   Examples of positive and negative responses are   shown in Table 3 . Our assumption is that responses   that contribute to more engaging conversations   should satisfy allthree dimensions and we refer   to them as quality responses . We do not consider   automatic metrics since they do not yet reliably re-   place human judgements on open - ended responses ,   especially for fine - grained evaluation dimensions .   4.2 Comparing Original vs Reflect Responses   First , we compare the quality of responses in previ-   ous dialogue datasets with our Reflect responses to   analyze the effects of explicitly incorporating CG   inhuman RG . Here we present results by adopt-   ing the aforementioned evaluation protocol on hu-   man dialogues , both from the original base dia-   logues ( Zhou et al . , 2021 ) and from our Reflect   dataset , derived from the same dialogues . We sam-   pled 300 dialogue contexts and asked 3 crowd-   sourcing workers to rate the three SSI criteria ,   using majority voting to get final scores ( Fleiss-   kappa ( Fleiss , 1971 ) agreement is around 0.67 ) .   We compare the original next - turn response from   the contexts with a randomly sampled one from our   Reflect responses .   Reflect contains more specific and interesting   responses than original dialogues From human   evaluation shown in Figure 5 , we observe that   our collected Reflect data , consists of dialogue re-   sponses that are on average more specific ( 20 % )   and interesting ( 13 % ) than the original data , while   having slightly lower sensibility ( 4 % ) ratings . One   possible contributor to the lower sensibility may   be 2 - stage collection where a new worker contin-   ues dialogues constrained by a specific inference   generated by another person . Specifically , when10454comparing the percentages of responses that sat-   isfy all three criteria , i.e. ,quality responses , we   find that there are substantially more ( 18 % ) such   responses in Reflect than in original data . This ob-   servation raises an interesting question : “ do exist-   ing dialogue training datasets capture high quality   dialogues ? ” Without sensible , specific , and inter-   esting responses to learn from , RG models will   necessarily be limited in the quality of their output .   4.3 Comparing Reflex RG vs Reflect Data   We now compare Reflect with RG models trained   on dialogue data that lacks explicit CG and to di-   rectly generate an utterance given a context .   Reflexive model baselines Specifically , we con-   sider models from two categories : medium - sized   RG models pre - trained on dialogue data such as   BlenderBot ( 440 M parameters)(Roller et al . ,   2021 ) and large - sized language models ( LLM ) pre-   trained on general texts such as GPT3 - DaVinci   ( 175B parameters)(Brown et al . , 2020 ) . We   directly use off - the - shelf Blender since it is pre-   trained on dialogue data ( Blender ) . For GPT3-   175B , we apply few - shot in - context learning by   providing 3 examples of dialogue context and re-   sponse from existing data ( GPT3 - FS ) . We manu-   ally examine these responses to ensure their quality   as demonstrating examples . Then we present a dia-   logue context from our test data and prompt GPT3   to generate a next - turn response . More details in   Appendix A.   Models with no common ground struggle Un-   surprisingly , as shown in Figure 6 , we find a similar   trend as comparing Reflect with original dialogue   data : both BlenderBot - FT and GPT3 - FS generate   much fewer quality responses ( 53 % and 38 % , re-   spectively ) that satisfy all criteria and particularly   on specificity . This further supports the hypothe-   sis that RG models that learn from no - grounding   dialogue responses struggle to capture what consti-   tuted meaningful conversations .   5 A Little CG Goes a Long Way   After showing that explicitly integrating inference-   based CG helps humans produce more specific and   interesting dialogue responses , we now test if this   also holds for neural RG models . We take the non-   exhaustive inference dimensions we used in Re-   flect as case studies to see how CG could improve   the quality of existing RG systems ’ responses , in   terms of the SSI human evaluation ( Thoppilan et al . ,   2022 ) .   5.1 Experiment Setup   Inference - Guided reflect models We attempt to   shift models from “ reflexive ” RG to “ reflective ” RG   by taking into account of plausible inferences that   humans use to build common ground during com-10455   munication . Since both BlenderBot and GPT3 are   trained to generate responses directly without in-   tegrating common ground , a non - trivial challenge   is how to adapt them to use inference - based com-   mon ground before RG . Here we present our two   intuitive and simple approaches .   For BlenderBot-440 M , we follow the common   practice of fine - tuning models to adapt to a new   task format . We split our Reflect data into 60/10/30   for train / valid / test and first fine - tune BlenderBot-   440 M ( Blender - FT ) on only the collected re-   sponses to show potential benefits of training from   inference - guided human responses . Then we fine-   tune BlenderBot but modify the training task from   outputting responses from contexts to inference-   guided RG . Inspired by modular generation in dia-   logue RG ( Adolphs et al . , 2021 ; Zhou et al . , 2022 ;   Shuster et al . , 2022 ) , our training task is : given dia-   logue context and one of the five inference dimen-   sion questions , generate the answer as well as the   response collected in Reflect ( Blender - FT - InfQ ,   indicating that the model is given the Inference   Question ) . More details in Appendix C.   For GPT-175B , we follow the few - shot in-   context learning approach with one small addition   in input : we append the dialogue context with an   inference question and ask the model to generate a   response . Our pilot studies show that GPT3 tends   to generate directly an answer to the question , not   a next - turn response to the dialogue context , thus   we format the question into a prompt for GPT3and stress that the end goal is RG . Specifically , we   append the text “ Think about this when respond-   ing : ” and then one of our inference questions after   the dialogue context to prompt GPT3 to generate a   response by reflecting on the questions ( GPT3 - FS-   InfQ ) . Illustrative figures for prompting GPT3 are   shown in Appendix A Figures 10 .   To compare and analyze the effects of each infer-   ence dimension , we randomly sample one response   for each of the five inference dimensions for GPT3-   FS - InfQ and Blender - FT - InfQ and take their aver-   age . For GPT3 - FS , Blender , and Blender - FT , we   pick the top 5 responses generated using their de-   fault decoding strategy ( beam search for GPT3 and   nucleus sampling for Blender ) and aggregate their   evaluation results . In total , we evaluate 250 re-   sponses from each model following the procedure   in Section 4.1 .   5.2 Experimental Results   Prompting GPT3 to “ think ” about common   ground improves response quality by 30 % Fig-   ure 7 presents results when comparing models that   has no access to inference - guided Reflect data with   those that do . We test the hypothesis that whether   guiding RG models with inference questions about   common ground is helpful for generating more   human - like responses . We find that with inferences ,   GPT3 - FS - InfQ outperforms GPT3 - FS on alleval-   uation dimensions . Specifically , inference - guided   GPT3 produces almost 25 % more specific and 30%10456   more quality responses . Moreover , 54 % quality   ( sensible , specific , and interesting ) responses al-   ready surpasses quality of human - written responses   in original dialogues ( 49 % ) , but still lags behind   Reflect ( 58 % ) as shown in Figure 5 .   Fine - tuning Blender on Reflect generates 26 %   more quality responses For BlenderBot-400 M ,   we find that fine - tuning on inference - guided hu-   man responses from Reflect helps generate almost   50 % more specific and 26 % more quality responses .   In contrast to GPT3 , BlenderBot with inference-   guided fine - tuning does not seem to improve much .   We speculate that model size might play a role in   how much model is influenced by CG inferences ,   leaving future work for more inference - customized   fine - tuning on moderate - sized models .   5.3 Analysis   Which inference dimension helps models the   most ( and which the least ) ? Figure 8 shows   the percentages of quality responses separated by   the inference dimension we use to prompt humans   and models . Interestingly , we find that on some   dimensions , GPT3 - FS - InfQ can produce signifi-   cantly better responses than human responses from   Reflect , especially event - based : “ What might have   happened before ” and “ what might happen after ? ”   and emotion - based CG about the other speaker   “ What is A ( speaker1 ) feeling now ? ” . However , on   “ How would you describe A ” , humans responses   grounded on this question are much better . This   dimension - specific analysis provides evidence that   neural models ’ capability to generate quality re-   sponses may depend on what types of CG we use   to guide them .   Prompting GPT3 - 175B with complete human   inferences To show how well GPT3 can make   use of complete human - annotated common ground ,   we further append the inference answer after the   question from Reflect data and prompt GPT3 togenerate a response given the fully materialized   common ground . As expected , we observe fur-   ther improvements in response quality especially in   specificity ( 15 % more ) and general quality ( 16.7 %   more ) . This analysis shows promises to make   reflect - style models produce better responses by   providing quality inference answers for CG .   6 Related Work   We have presented discussion of previous work   representing CG ( Ghosal et al . , 2022 ; Zhou et al . ,   2022 ) in Section 1 and relevant communication   theory and psycholinguistic literature in Section 2 .   Here we provide additional discussions . Recent   advances on neural RG models mainly focused   on fine - tuning large pre - trained transformer mod-   els ( Zhang et al . , 2020 ; Roller et al . , 2021 ; Thop-   pilan et al . , 2022 ) on huge number of dialogue   data . However , few of the data provides explicit   common grounding . Modular RG ( Adolphs et al . ,   2021 ; Shuster et al . , 2022 ) aims to generate rele-   vant knowledge first by retrieving from the web   and the generate knowledge - grounded responses .   Compared to these work , we focus on inferences   based on common sense instead of external knowl-   edge . Another closely related work by Cho and   May ( 2020 ) examined incorporating dialogue data   with techniques from improvisational theater to   teach models to implicitly build common ground .   7 Conclusion   We introduce Reflect , a dataset with diverse   inference - grounded responses inspired by CG and   communication theories . We carefully design our   two - stage collection process and apply quality con-   trol . Then we demonstrate limitations of existing   dialogue data and models trained on it . Finally ,   we present promising signs that guiding models   with CG results in more engaging conversations .   We hope to encourage more work on improving   RG quality by looking at how humans use CG   and adapt the communication process to machine   learning models . Future directions include provid-   ing a ranking of inference dimensions depending   on dialogue context and train models to generate   responses following the most suitable dimension .   Reflect also enables potential automated metrics   to evaluate response since more responses per di-   alogue might help gauge the plausible response   space given a context.10457Acknowledgments   We thank anonymous reviewers for providing in-   sightful feedback along with Brendan Kennedy ,   Peifeng Wang , and members from INK and   JAUNTS lab . This research is supported in part   by the DARPA MCS program under Contract No .   N660011924033 , the Defense Advanced Research   Projects Agency with award NSF IIS 2048211 ,   NSF SMA 182926 , and support from Google .   Ethics and Broader Impact   We collect a new dialogue dataset in English , which   benefits English speakers more . We use Amazon   Mechanical Turk to recruit crowdsourcing workers   and we pay workers over $ 15 / hour on average , well   above the highest state minimum wage and engage   in constructive discussions if they have concerns   about the process . We also give each annotation   instance enough time so that we do not pressure   annotators . In our quality assurance process for this   dataset , we also examine potential harmful biases   and aggressive languages in responses and remove   them in the final dataset . We also acknowledge that   the generated responses from our experimented   models might contain biases .   8 Limitations   Our first limitation in modeling CG is that we are   using inferences from one speaker to approximate   CG during the communication process . To truly   represent CG , we need to recollect dialogues and as   participants continue the conversations , we should   ask both of them the same inference questions and   perform post - hoc analysis on the answers to the   questions .   Our second limitation is the lack of explicitly   modeling communicative intents . In future work ,   we plan to heuristically link each inference dimen-   sion to a general communication goal . For example ,   making inferences about “ speaker emotion states ”   is helpful to build emotional connections with the   other speaker .   References104581045910460A Data Collection Details   We engage in active discussions with them in the   TurkerNationSlack channel and provide detailed   feedback after multiple rounds of pilot study to   ensure the data quality .   A.1 Inference Collection   Here we present more detailed feedback for AMT   workers on Stage 1 . inference collection : First , we   stress that the goal of these answers is to help with   generating a response to continue the conversation   instead of any inferences that might not be useful   for directly generating engaging responses , such   as “ spaghetti is a type of food ” for the example   in Figure 1 . Secondly , the answers should not be   a direct copy - paste of some parts in the dialogue   context as those would be trivial to collect , violate   the least collaborative principle and the maxim of   quantity ( Grice , 1975 ) , and and should not be worth   making inferences over . Finally , we remind them   that the inferences written should be considered   as “ common sense ” so that the approximated CG   is more likely to become shared knowledge and   beliefs among the dialogue participants . Collection   UI and provided examples for turkers are shown in   Figures 12 and 13 .   A.2 Response Collection   We specifically stress on several points to work-   ers : 1 ) to collect more engaging and interesting   responses , response should not directly paraphrase   the inference such as “ I think you are feeling re-   lieved ” from inference QA pair “ What is speaker   feeling now ? Speaker is feeling relieved ” ; 2 ) the   response should be both coherent to the dialogue   context as what would be naturally uttered by the re-   sponder and based on the reactions to lead the con-   versation in an interesting direction ; 3 ) Ultimately ,   we want responses that lead the conversations that   are more enjoyable and engaging . Collection UI   and provided examples for turkers are shown in   Figures 14 and 15 .   B Human Evaluation Details   Specifically , a sensible response is one that is rea-   sonable in context . A specific response is one that   relates closely to the given dialogue context , in-   stead of a generic one that can be applied in dozens   of different contexts . An interesting response can“catch someone ’s attention or arouse their curiosity ,   or if it is unexpected , witty , or insightful . ” ( Thop-   pilan et al . , 2022 ) . For more detailed instructions ,   please refer to Thoppilan et al . ( 2022 ) . Evaluation   UI and provided examples for turkers are shown in   Figures 16 and 17 .   C Model Implementation Details   We use two base models in our paper : BlenderBot-   440 M and GPT3 - 175B. For BlenderBot , we use   the ParlAI ( Miller et al . , 2017 ) package for pre-   trained modeling and fine - tuning . The format for   fine - tuning BlenderBot on inference questions is :   input sequence is “ < speaker1 > ... < speaker2 > ...   < speaker1 > ... < infq > What might have hap-   pened before ? ” and output sequence is “ < infa > ...   < speaker2 > ... ” , where we use “ < infq > ” , “ < infa > ”   to indicate the start of an inference question and an-   swer , respectively . We fine - tune BlenderBot-440 M   for 3 epochs with batch size 16 and set the learning   rate to be 1e-06 . We perform gradient accumulation   for 8 steps and gradient clipping with a max norm   of 1.0 and optimize using the Adam optimizer . For   decoding , we use top - p nucleus sampling ( Holtz-   man et al . , 2019 ) with temperature T ( p = 0.9 and T   = 0.7 ) , and a maximum decoding length of 300 to-   kens . BlenderBot-440 M models are mostly trained   on 4 Quadro RTX 8000 GPUs and take around 9   hours .   We use OpenAI - APIto access GPT3 - DaVinci   ( 175B ) and include prompting formats for GPT3-   FS and GPT3 - FS - InfQ in Figures 9 and 10 , respec-   tively .   D Additional Experimental Results   D.1 Inference - Separated Fine - Grained   Evaluation Results   Inference dimension - separated full results are   shown in Figure 11.1046110462104631046410465104661046710468