  Liming Wang , Siyuan Feng , Mark Hasegawa - Johnson , Chang D. YooDepartment of Electrical and Computer Engineering ,   University of Illinois at Urbana - ChampaignMultimedia Computing Group , Delft University of TechnologyArtiﬁcial Intelligence and Machine Learning Lab , KAIST   Abstract   Phonemes are deﬁned by their relationship   to words : changing a phoneme changes the   word . Learning a phoneme inventory with lit-   tle supervision has been a longstanding chal-   lenge with important applications to under-   resourced speech technology . In this paper ,   we bridge the gap between the linguistic and   statistical deﬁnition of phonemes and propose   a novel neural discrete representation learning   model for self - supervised learning of phoneme   inventory with raw speech and word labels .   Given the availability of phoneme segmen-   tation and some mild conditions , we prove   that the phoneme inventory learned by our ap-   proach converges to the true one with an expo-   nentially low error rate . Moreover , in experi-   ments on TIMIT and Mboshi benchmarks , our   approach consistently learns a better phoneme-   level representation and achieves a lower er-   ror rate in a zero - resource phoneme recog-   nition task than previous state - of - the - art self-   supervised representation learning algorithms .   1 Introduction   Thanks to recent developments in self - supervised   speech representation learning ( van den Oord et al . ,   2017 , 2019 ; Chorowski et al . , 2019 ; Baevski et al . ,   2020 ) , there is new hope for the development of   speech processing systems without the need for   full textual transcriptions . Supervised speech pro-   cessing systems for tasks such as automatic speech   recognition ( ASR ) rely on a large amount of tex-   tual transcriptions , but self - supervised systems can   be applied to under - resourced languages in which   such annotation is either scarce or unavailable . A   key task of the self - supervised system is to learn a   discrete representation . While it is possible to dis-   cretize the speech solely on the basis of its acoustic   properties , a more desirable discrete representa-   tion would serve as a bridge from the continuous   acoustic signal toward higher - level linguistic struc-   tures such as syntax and semantics . Such a rep - resentation would make it possible to repurpose   algorithms developed for written languages so that   they could be used for unwritten languages in tasks   such as speech translation and spoken language   understanding . Words are the obvious choice for   a discrete , semantic - driven speech representation ,   but a practical speech understanding system needs   at least thousands of words ; learning them in an un-   supervised manner may be challenging . Phonemes   may be a more learnable representation . According   to the standard linguistic deﬁnition , phonemes are   closely linked to words :   Deﬁnition 1 . ( Linguistic deﬁnition of   phonemes ( Swadesh , 1934 ) ) Phonemes are   the smallest units in speech such that given a   correct native word , the replacement of one or   more phonemes by other phonemes ( capable of   occurring in the same position ) results in a native   word other than that intended , or a native - like   nonsense word .   For example , the sentences “ he thinks ” and “ he   sinks ” differ by exactly one phoneme but have very   different meaning . The optimal compactness of a   phoneme inventory as speciﬁed in the deﬁnition   leads to three advantages . First , learning phonemes   requires lower sample complexity than learning   words since the number of distinct phonemes is   much smaller than the number of distinct words in   a language . Second , the phonemes are much more   abundant and more balanced in classes than words   within a speech corpus , which makes sample com-   plexity less of an issue when learning phonemes .   Third , phonemes are more generalizable in the   sense that knowing the phoneme inventory allows   the learner to memorize previously unseen words   as sequences of phonemes , and , having memo-   rized them , to begin seeking clues to their mean-   ing . Motivated by the semantic - driven deﬁnition of   phonemes , we formulate the problem of learning   a phoneme inventory as a self - supervised learning   problem , where a small amount of semantic su-8027pervision is available . The required supervision   speciﬁes which acoustic segments are instances of   the same word , and which are instances of differ-   ent words . Such supervision might be acquired in   a naturalistic setting by asking native speakers to   name objects in a set of standardized images , as is   commonly done in primary education classrooms ,   or by asking for the translations of common words   in a second language , a common baseline approach   in dialectology and historical linguistics ( Swadesh ,   1952 ) . Our contributions are threefold : ( 1 ) we   propose a computationally tractable deﬁnition of   phoneme that is almost equivalent to the linguis-   tic deﬁnition . ( 2 ) We propose a ﬁnite - sample ob-   jective function for learning phoneme - level units   and prove that when the phoneme segmentation is   available and under mild conditions , the empirical   risk minimizer ( ERM ) of this objective will ﬁnd   the correct phoneme inventory with exponentially   low error rate . ( 3 ) We propose a novel neural net-   work called information quantizer to optimize the   proposed objective , which achieve state - of - the - art   results in the phoneme inventory discovery task   on the TIMIT and low - resourced Mboshi bench-   marks with much less training data than previous   approaches .   2 Related works   Due to the challenge of learning phonemes , early   works on unsupervised speech representation learn-   ing ( Park and Glass , 2005 ; Lee and Glass , 2012 ;   Ondel et al . , 2016 ) focus on learning speech   segments sharing similar acoustic properties , or   phones , without taking into account the meaning of   the speech they are part of . There are two main ap-   proaches in this direction . One approach is to learn   discrete phone - like units without anytextual labels   by modeling phone labels of the speech segments   as latent variables . In particular , ( Park and Glass ,   2005 ; Jansen et al . , 2010 ) ﬁrst detect segments with   recurring patterns in the speech corpus followed by   graph clustering using the similarity graph formed   by the segments . ( Lee and Glass , 2012 ; Ondel   et al . , 2016 ; Kamper et al . , 2016 ) develop prob-   abilistic graphical models to jointly segment and   cluster speech into phone - like segments . An exten-   sion to the latent variable approach is to introduce   additional latent variables such as speaker iden-   tity ( Ondel et al . , 2019 ) or language identity ( Yusuf   et al . , 2020 ) and develop mechanisms to disentan-   gle these variables . With the advance of deep learning , neural net-   work models have also been proposed to learn   unsupervised phone - level representation either by   ﬁrst learning a continuous representation ( Chung   et al . , 2019 ; Feng et al . , 2019 ; Nguyen et al . , 2020 )   followed by off - line clustering , or by learning a   discrete representation end - to - end with Gumbel   softmax ( Eloff et al . , 2019b ; Baevski et al . , 2020 )   or vector - quantized variational autoencoder ( VQ-   V AE ) ( van den Oord et al . , 2017 ; Chorowski et al . ,   2019 ; Baevski et al . , 2019 ) . However , codebooks   learned by the neural approaches tend to be much   larger than the number of phonemes ( Baevski et al . ,   2020 ) , leading to low scores in standard phoneme   discovery metrics . The second approach utilizes   weak supervision such as noisy phone labels pre-   dicted by a supervised , multilingual ASR system   trained on other languages . Along this direction ,   early works ( Schultz and Waibel , 1998 ; Lööf et al . ,   2009 ; Swietojanski et al . , 2012 ) have showed that   phonetic knowledge gained from one language can   be leveraged to develop ASR systems for another   language using an HMM - based or DNN - HMM   hybrid approach . Instead of using phone labels ,   ( Stuker et al . , 2003 ) explores the use of articula-   tory features as supervision for the multilingual   ASR . Recently , ( ˙Zelasko et al . , 2020a , b ; Feng   et al . , 2021a ) systematically study the performance   of zero - shot crosslingual ASR on 13 languages   trained with international phonetic alphabet ( IPA )   tokens and found that the system tends to perform   poorly on unseen languages . Instead , ( Feng et al . ,   2021b ) is able to discover phone - like units by clus-   tering bottleneck features ( BNF ) from a factorized   time - delay neural network ( TDNN - f ) trained with   phone labels predicted by a crosslingual ASR ( Feng   et al . , 2021a ) .   Several works have since shifted focus toward   the more challenging phoneme discovery prob-   lem by formulating it as a self - supervised learn-   ing problem where the semantics of the speech   are known , such as from translation , phoneme-   level language models or other sensory modali-   ties such as vision . ( Jansen , 2013 ) has studied   the use of pairwise word identity labels for train-   ing phoneme discovery models based on Gaus-   sian mixture models ( GMM ) ; ( Harwath and Glass ,   2019 ) analyzes the hidden layers of a two - branch   neural network trained to retrieve spoken captions   with semantically related images and ﬁnds strong   correlation between segment representation and8028phoneme boundaries . ( Harwath et al . , 2020 ) adds   hierarchical vector quantization ( VQ ) layers in the   same retrieval network and is able to ﬁnd a much   smaller codebook than the unsupervised neural ap-   proach ( Baevski et al . , 2020 ) , and achieve high cor-   relation with the phoneme inventory . ( Godard et al . ,   2018 ; Boito et al . , 2019 ) has studied the possibility   of learning semantic units using an attention - based   speech - to - text translation system , though the units   appear to correlate more with words . Works on un-   supervised speech recognition ( Chen et al . , 2019 )   attempt to learn to recognize phonemes by lever-   aging the semantic information from a phoneme   language model unpaired with the speech , typi-   cally by matching the empirical prior and posterior   distributions of phonemes either using cross en-   tropy ( Yeh et al . , 2019 ) or adversarial loss ( Chen   et al . , 2019 ; Baevski et al . , 2021 ) . Such models ,   however , have a slightly different objective as they   assume knowledge about the phoneme inventory of   the language and instead tries to ﬁnd the alignment   between the speech and phonemes , rather than in-   duce the phoneme inventory from scratch .   3 Semantic - driven Phoneme Discovery   3.1 Notation   Throughout the paper , we use Pfgto denote proba-   bility . We use capital letters to denote random vari-   ables and lower - case letters to represent samples of   random variables . We use P:=PfX = xgto de-   note both probability mass and density functions of   random variable X , depending on whether it is con-   tinuous or discrete . Further , denote P(yjx ) : =   PfY = yjX = xgas the true conditional proba-   bility distribution of random variable Y = ygivenrandom variable X = x. The probability simplex   inRis denoted as .   3.2 Statistical Deﬁnition of Phonemes   The linguistic deﬁnition of phonemes can be   rephrased as follows . Deﬁne Xto be the set of   all physical acoustic segments that can ever be pro-   duced as instances of the phonemes of a given lan-   guage . Deﬁnition 1 can be phrased as follows :   Two sequences of segments x= [ x;;x]and   x= [ x;x;x ] , differing only in that   x6 = x , are instances of different words , y6 = y ,   if and only if xandxare instances of different   phonemes . In order to design effective algorithms ,   we will work with a relaxation of this deﬁnition ,   which we call the statistical deﬁnition of phonemes .   Deﬁnition 2 . ( Statistical deﬁnition of phonemes )   LetXbe the set of all speech segments in a lan-   guage , and let Xbe a random vector taking val-   ues in XandYbe a random variable represent-   ing the word of which Xis one segment . The   phoneme inventory of a language is the minimal   partition Z = fZ;;ZgofX(i.e . ,X=   [ Z;Z\Z=;;81j;kK ) , such   that if a speech segment pair ( x;x)2Xsatisﬁes   ( x;x)2Zfor somek2f1;;Kg , then their   conditional distributions satisfy   P = P : ( 1 )   In other words , given only the knowledge that two   acoustic sequences contain instances of the same   phoneme , the resulting conditional distributions   across possible word labels are the same .   The fundamental intuition of Deﬁnition 2 is   that different phonemes have different distributions   across the words of the language . Two instances   of the same phoneme , xandx , might have dif-   ferent likelihoods PandP , e.g. , be-   cause of allophony ; but their posteriors P   andPcannot be different without violating   Deﬁnition 1 . The relationship between Deﬁnition 1   and Deﬁnition 2 is given by the following proposi-   tion , whose proof is in Appendix A.3 .   Proposition 1 . LetZ=[Zbe a partition of   X. If , for all possible fPg , for any spo-   ken word x= [ x;;x ] , and for any segment   pairs ( x;x)2Z;k2f1;;Kg , changingx8029   toxdoes not alter the identity of the word , i.e. ,   arg maxP(yjx;x;x )   = arg maxP(yjx);(2 )   but for any segment pairs x2Z;x2Zfor   k6 = l , changingxtoxalters the identity of the   word , i.e. ,   arg maxP(yjx;x;x )   6= arg maxP(yjx);(3 )   thenZis a phoneme inventory from Deﬁnition 2 .   Deﬁne the phoneme assignment function z :   X!f1;;Kgsuch thatz(x ) = kifx2Z.   Suppose a segment Xis randomly chosen from X   with probability distribution Pand its phoneme   label is another random variable Z:=z(X ) , then   by Deﬁnition 2 , for any pair x;x2Xsuch that   z(x ) = z(x ) , we haveP = P=   P. The phoneme inventory is thereby com-   pletely characterized by the phoneme label function   z()as well as the set of distributions associated   with each class P.   3.3 Problem Formulation   Letz()be the phoneme assignment function from   Deﬁnition 2 and assume the size of the phoneme   inventory is known to be K.   Given a training set D = f(x;y)g ,   where each xis an acoustic segment extracted   from a spoken word , and each y2Yis the cor-   responding word label , a semantic - driven phoneme   discovery ( SPD ) system tries to ﬁnd an assign-   ment function that minimizes the token error rate   ( TER ) :   P(^z ) : = minPfz(X)6=(^z(X))g;(4)where is the set of all permutations of length   K , which is used because the problem is unsuper-   vised andz()is not available during training . An   assignment function ^zis said to achieve exact dis-   covery ifP(^z ) = 0 . It can be easily shown that   TER is equivalent to standard evaluation metrics   for phoneme discovery such as normalized mutual   information ( NMI ) ( Yusuf et al . , 2020 ; Harwath   et al . , 2020 ; Feng et al . , 2021b ) and token F1 ( Dun-   bar et al . , 2017 ) , as presented in Appendix A.2 .   Thus , to provide guarantees for NMI and token F1 ,   it sufﬁces to provide a guarantee for TER .   4 Information Quantizer   We solve the SPD problem using a novel type   of neural network called an information quan-   tizer ( IQ ) , depicted in Figure 2 . An IQ ( ;q)2   Qconsists of four main components : A pre-   segmentation network , a speech encoder e( ) , a   word posterior c()and a quantizer q :  !   C = fQ;;Qg , where [ ; ] = andCis   thedistribution codebook andQ ’s are called the   code distributions ofq .   4.1 Phoneme inventory discovery with IQ   IQ performs phoneme discovery in three stages .   The pre - segmentation stage takes a raw speech   waveform as input and extracts phoneme - level   segments x= [ x;;x]in a self - supervised   fashion ( Kreuk et al . , 2020 ) . Afterwards , in the   joint distribution learning stage , the speech encoder   extracts phoneme - level representations e(x ) =   [ e(x);;e(x)]before passing them into   the word posterior network to estimate the distri-   bution of word labels , Y , given the presence in the   word of acoustic phonetic segment X = x :   P = c(e(x));1tT : ( 5)8030Note that it is crucial that no recurrent connection   exists between segments since our goal is to learn   the probability of a word label given the presence of   one phoneme segment . Finally , in the quantization   stage , the quantizer creates the phoneme inventory   by assigning each segment xan integer index via   codeword assignment function ^z(x)such that   ^z(x ) = kifq(P ) = Q.   4.2 Training   The loss function that IQ minimizes has two goals :   learn a good estimator for the conditional distribu-   tionPand learn a good quantization function   q( ) . The ﬁrst goal is achieved by minimizing the   cross entropy loss :   L(P; ) : =  1   nXlogP(yjx);(6 )   wherePis the empirical joint distribution . The   second goal is achieved by minimizing the KL-   divergence between the estimated conditional dis-   tribution before and after quantization :   L(~P;;q ) : =   1   nXD(Pjjq(P));(7 )   where   ~P:=1   nXP   is the smoothed version of the empirical distribu-   tion . The ﬁnal loss function of IQ for SPD is then :   L(P;;q ) : = L(P; ) + L   ~P;;q   ;   ( P )   where > 0is some hyperparameter set to ap-   proximately 1for most experiments . Further , we   restrictqto be nearest - neighbor so that :   q(P ) = arg minD(PjjQ ): ( 8)   This restriction does not increase the loss ( P ) and   serves as a regularization during phoneme discov-   ery , as shown in Appendix A.3 .   4.3 Theoretical Guarantee   We show that when the phoneme segmentation is   available and under mild assumption , IQ is able   to achieve exact discovery of phoneme inventory .   First , let us state the main assumptions of the paper . Assumption 1 . ( boundedness of the density ra-   tio ) There exist universal constants C < C   such that82;8q2Q;8(x;y)2X   Y;log2[C;C];log2   [ C;C ] .   Assumption 2 . ( log - smoothness of the density   ratio ) There exists  > 0such that8;2   ;x;y2XY ;  log  k k .   Assumption 3 . ( realizability ) There exists a   nonempty subset such thatP=   P;82.   Assumption 4 . The true prior of the phoneme in-   ventory is known to be P(z ) = ; 1zK.   The ﬁrst two assumptions are similar to the ones   in ( Tsai et al . , 2020 ) . Assumption 3 assumes that   the true probability measure is within the function   class , which combined with Assumption 1 requires   the true distribution to share the same support as the   estimated one . However , such assumption can be   relaxed so that D(PjjP);82   for some small enough  > 0 , which does not   affect the essential idea behind our analysis and   can be achieved by some rich class of universal ap-   proximators such as neural networks ( Hornik et al . ,   1989 ) . The last assumption ensures the inventory   to be identiﬁable by assuming knowledge of the   prior of the phoneme inventory .   Next , we will state the theoretical guarantee be-   fore giving some intuitive explanation .   Theorem 1 . Given Assumption 1 - 4 , let the infor-   mation quantizer ( ^;^q)with assignment function ^z   be an empirical risk minimizer ( ERM ) of ( P ):   L(P;^;^q ) = minL(P;;q):(9 )   For any2(0;1 ] , with probability at least 1     , the cluster assignment function ^zof the ERM   information quantizer ^qachievesP(^z ) = 0 if   the sample size nsatisﬁes :   nO   log   minf;logg !   ; ( 10 )   where   = minc(z;z)D(PjjP )   for some constants c(z;z)>0;1z;zK   independent of n;,O(x)is such that O(x)8031  xfor some   > 0andD(PjjQ ) : = D   Pjj   + D   Qjj   is the   Jensen - Shannon divergence .   The bound in Theorem 1 captures two main fac-   tors determining the sample complexity of exact   phoneme discovery : the ﬁrst factor is how close   the word distributions of phonemes are from each   other as measured by their Jensen - Shannon ( JS )   divergence , and the second factor is how hard it   is for the training data to cover all the phonemes .   The theorem works essentially because ( P ) can   be viewed as an approximation of the mutual in-   formation between the codeword ^z(X)and word   typeY , I(^z(X);Y ) . SupposePPand   letH(j)denotes conditional entropy , we have :   L(P;^;^q )   H(YjX ) + D(Pjj^q(P ) )   / I(X;Y ) + D(Pjj^q(P ) )   =  I(^z(X);Y ) ;   which is minimized if ^q(P ) = P. In fact ,   we prove that ^zfor such ^qis equivalent to z()up   to a permutation in Appendix A.3 .   5 Experimental Setup   Datasets We construct four training datasets   consisting of spoken words only . The vocabu-   lary set withjYj= 224 is selected from head   words of noun phrases from the Flickr30kEntities   dataset ( Hodosh et al . , 2010 ) that appear at least   500 times . For the Flickr audio word dataset , spo-   ken words in the vocabulary are extracted from   Flickr audio dataset ( Harwath and Glass , 2015 ) .   For the Librispeech and TIMIT word dataset with   jYj= 224 , spoken words are extracted from Lib-   rispeech ( Vassil et al . , 2015 ) 460 - hour train - clean   subset , resulting in a dataset of about 6 hours and   0.1 hours ; for Librispeech and TIMIT word dataset   withjYj= 524 andjYj= 824 , we supplement the   dataset with the speech for the top 300 frequent   words and top 600 frequent words respectively ( ex-   cluding the visual words ) in Librispeech , resulting   in datasets of about 15 and 21 hours . For Mboshi   dataset , we found only about 20 actual words occur   more than 100 times , so instead we use n - grams   with eithern3(all except uni- and bi - grams ) or   n2(all except unigrams ) that occur more than   100 times as “ words ” , resulting in a vocabulary size   of 161 and 377 respectively . Note that the amount   of labeled data we need is much lower than previ-   ous works ( Yusuf et al . , 2020 ): around 30 hours ,   ( Feng et al . , 2021b ): around 600 hours ) and the   vocabulary size used is much smaller than the total   vocabulary size in the language . More details of the   sets can be found in Appendix B. We also test our8032models on two standard phoneme discovery bench-   marks , which contain whole - sentence utterances   with many words unseen during training . The ﬁrst   dataset is TIMIT ( Garofolo et al . , 1993 ) , an En-   glish corpus consisting of about 5 hours speech and   Mboshi ( Godard et al . , 2017 ) , which contains about   2.4 hours speech from a low - resource language .   For both datasets , we follow the split in ( Yusuf   et al . , 2020 ) , ( Feng et al . , 2021b )   Baselines For phoneme discovery from seg-   mented words , we compare our model ( IQ ) to   four baselines . The ﬁrst two baselines use con-   tinuous representation : the CPC+k - means model   performs k - means clustering on the segment - level   CPC features , and the k - means model performs   k - means clustering after the model is trained on   the word recognition task . The last two baselines   use discrete representations : the Gumbel varia-   tional information bottleneck ( Alemi et al . , 2017 )   ( Gumbel VIB ) is a neural model with a Gumbel   softmax ( Jang et al . , 2016 ) layer to approximate   the codebook assignment function z( ) , and we   set  = 0:001and decay the temperature of the   Gumbel softmax from 1to0:1linearly for the ﬁrst   300000 steps , keeping it at 0:1afterwards , which   works best in our experiments ; the deterministic   information bottleneck ( DIB ) , a generalization of   ( Strouse and Schwab , 2016 ) for continuous feature   variableX , which assumes the same deterministic   relation between speech Xand codebook unit Zas   ours , but optimizes the models in a pipeline fashion   ( ﬁrst the speech encoder and then the quantizer ) by   performing clustering on the learned conditional   distributions . The CPC features used are trained   in a self - supervised fashion on the 960 - hour Lib-   riSpeech dataset and released by ( Nguyen et al . ,   2020 ) . All models share the same speech encoder   as IQ . For the whole - sentence datasets , we com-   pare our models to three phoneme discovery sys-   tems , namely , the unsupervised H - SHMM trained   with multilingual speech ( Yusuf et al . , 2020 ) , the   ResDA VEnet - VQ ( Harwath et al . , 2020 ) with vi-   sual supervision and the TDNN - f system by ( Feng   et al . , 2021b ) trained with multilingual speech . To   study how well our model performs in extreme   low - resource speech recognition compared to other   neural speech representation learning models , we   compare our models to wav2vec ( Schneider et al . ,   2019 ) , wav2vec 2.0 ( Baevski et al . , 2020 ) ( small ,   trained on the 960 - hour LibriSpeech ) , vq - wav2vec   with Gumbel softmax and k - means as discretiza - tion strategies ( Baevski et al . , 2019 ) , CPC ( van den   Oord et al . , 2019 ) and VQ - CPC ( van Niekerk et al . ,   2020 ) , using the pretrained models released by the   authors . Implementation details of the baselines   and our models are in Appendix C.   Evaluation metrics Standard metrics are used   such as NMI and boundary F1 for the quality of   codebook and segmentation respectively with the   same implementation as in prior works ( Yusuf   et al . , 2020 ; Feng et al . , 2021b ) . In addition , token   F1 ( Dunbar et al . , 2017 ) is also reported . To exam-   ine the beneﬁt of using our discovered phoneme   inventory for low - resource speech recognition , we   also evaluate using equivalent phone error rate   ( equiv . PER : Ondel et al . 2019 ) . This metric can   be viewed as a proxy for phone error rate ( PER )   applicable beyond supervised speech recognizers .   6 Results   6.1 Word - level Phoneme Discovery   The results on visual word - only test sets of Flickr   audio and Librispeech are shown in Table 1 . On   both datasets , IQ outperforms both Gumbel VIB   and DIB in terms of all metrics , especially on Flickr8033   audio , which has more phonemes than Librispeech   and a larger test set . Moreover , the performance of   IQ is very robust to the codebook size , achieving   good results even when the codebook size is very   different from the size of the true phoneme inven-   tory , suggesting our theory may be able to work   with a relaxed Assumption 4 .   6.2 Sentence - level Phoneme Discovery   The results on TIMIT and Mboshi are shown in   Table 2 and Table 3a respectively . On TIMIT , our   model is able to outperform the visually grounded   baseline ( Harwath et al . , 2020 ) for all training vo-   cabulary , and all three baselines for jYj= 524 and   jYj= 824 with and without gold segmentation in   terms of all three metrics . Further , we also empiri-   cally verify the sample complexity bound in Theo-   rem 1 as IQ performs better in Token F1 and NMI   as the training vocabulary size get larger , which   generally increases the JS divergence . On Mboshi ,   IQ with CPC feature consistently outpeforms ( Feng   et al . , 2021b ) in token F1 and boundary F1 , and IQ   with CPC+BNF features consistently outperform   ( Feng et al . , 2021b ) in all three metrics under vari-   ous level of word supervision . The performance of   our model on Mboshi compared with other neural   self - supervised models are shown in Table 3b . We   found that IQ outperforms the best self - supervised   model , CPC+k - means in equiv . PER by 34 % and   20 % absolute with and without gold segmentation   respectively and 12 % absolute in terms of boundaryF1 , suggesting that IQ is able to learn consistent   phoneme - like sequence useful for zero - resource or   extremely low - resource speech recognition .   Effect of segmentation and codebook size The   use of unsupervised phoneme segmentation dete-   riorates the NMI by about 18 % and 28 % absolute   on TIMIT and Mboshi respectively for our models   since the distributional property of phonemes does   not apply exactly to non - phoneme segments . On   the other hand , in Appendix F we show that the   quality of codeword assignments by IQs is very   robust against varying codebook size , after exper-   imenting with codebook size from 30 to 70 on   TIMIT and Mboshi .   Multilingual and word supervision are compli-   mentary In all vocabulary sizes , concatenating   the multilingual BNF from ( Feng et al . , 2021b ) to   the CPC output representation from the segmental   speech encoder in Figure 2 signiﬁcantly improves   token F1 and NMI to allow our best models to   outperform baselines in all three metrics .   6.3 Analysis   IQ codebook resembles true phonemes From   Figure 3b , we observe that the codeword as-   signments by IQ correlates well with the actual   phonemes , but tends to confuse the most between   phonemes within the same manner class , such as   nasals /n/ and /m/. This is also conﬁrmed by the   t - SNE plot in Figure 3a , where the embeddings   of most manner classes are well - clustered , except   for related manner classes such as affricate and   fricative , or glide and vowel . Further , from the   examples shown in Figure 4 , we can see that IQ is   not only better at grouping segments of the same8034phonemes but also at detecting segment boundaries   than the baselines . Also , across different examples ,   IQ assign the same codes to phonemes such as /a/   ( 31 ) and /s/ ( 7 ) more consistently than other mod-   els do . Please check Appendix G for more speech   examples .   Limitation While our theory predicts that with   gold segmentation , the TER of IQ is asymptotically   zero , in practice TER is nonzero due to the viola-   tion of Assumption 4 , i.e. , the phonemes are not   uniformly distributed for languages such as Mboshi .   As a result , the model often discards information of   the rare phonemes by merging them into a more fre-   quent phoneme cluster . Evidently , from Figure 5 ,   where we use ABX accuracy ( Munson and Gard-   ner , 1950 ) to score how reliable the IQ codebook   can identify segments of the same phoneme , we   observe a strong correlation is observed between   ABX accuracy and the frequency of the phonemes .   7 Conclusion   Motivated by the linguistic deﬁnition of phonemes ,   we propose information quantizer ( IQ ) , a new neu-   ral network model for self - supervised phoneme   discovery that can take advantage of word - level su-   pervision . We demonstrate in two ways that word-   level supervision is beneﬁcial for phoneme inven-   tory discovery : theoretically , we prove that IQ can   achieve zero token error rate asymptotically with   the help of word labels ; empirically , we show that   IQ out - performs various speech - only algorithms in   phoneme discovery tasks under both simulated ( En-   glish ) and realistic ( Mboshi ) low - resource settings .   In the future , we would like to apply the discovered   phoneme inventory to develop better low - resource   speech technologies such speech translation and   speech synthesis systems .   References80358036   A Proofs of Theoretical Results   A.1 Statistical deﬁnition of phonemes   Proof of Proposition 1 . Without loss of generality ,   suppose ( x;x)2X , suppose there exists y   such that   P(yjx)>P(yjx ) ;   then there exists ysuch that   P(yjx)<P(yjx ) ;   which means there exists 0  ;  1 ;  +   1 , such that   P(yjx )   P(yjx)   < P(yjx )   P(yjx ):   Now , since Equation 2 holds for arbitrary   P2;s6 = t , we can set   P(yjx ) =  ; P(yjx ) =  ;   P(yjx ) = P(yjx ) = 1   2;8t>2 ;   in which case Equation 2 boils down to   arg max  P(yjx ) =   arg max  P(yjx ):   However , by the choice of  ’s , the left - hand side   isysince  P(yjx ) >  P(yjx)and   the right - hand side is ysince  P(yjx ) >   P(yjx ) , and therefore Equation 2 can-   not hold . Therefore , Equation 2 is true only if   P(yjx ) = P(yjx);8(x;x)2X;y2   Y.8037A.2 Equivalence of TER and standard   phoneme discovery metrics   Consider the groundtruth assignment z()and a   codebook assignment ^z()with ^Kcode words , the   NMI of ^zis deﬁned as :   NMI(^z ) = 2I(z(X ) ; ^z(X ) )   H(z(X ) ) + H(^z(X ) ) ; ( 11 )   whereH()denotes the entropy and I(;)denotes   the mutual information .   which is also related to the token F1 used   for acoustic unit discovery ( Dunbar et al . , 2017 ) .   Since SPD is an unsupervised learning problem   and ground truth phoneme labels are not avail-   able , matching between codebook indices and   phoneme units is needed . When computing to-   ken F1 , we consider two different many - to - one   mappings:f1;;Kg!f 1;;^Kgand   :f1;;^Kg!f 1;;Kgto compute the   token recall and precision respectively as :   Rec(^z ) : = maxPf^z(X ) = (z(X))g(12 )   Prec(^z ) : = maxPfz(X ) = (^z(X))g;(13 )   before computing the harmonic mean between the   two to obtain token F1 : F1(^z ) : = .   The following proposition relates TER with token   F1 and NMI .   Proposition 2 . For any assignment function ^z :   f1;;Kg!f 1;;Kg , P(^z ) = 0 if and   only if F1 ( ^z ) = NMI(^z ) = 1 .   Proof . First of all , for such ^z , we have   1F1(^z)minfPrec(^z);Rec(^z)g   1 P ( ^z ) = 1 ;   where the third inequality comes from the fact that   the set of permutations is a smaller set than the set   of all many - to - one mappings :f1;;Kg !   f1;;Kg . Further , using the fact that zand^zare   functions of each other when P(^z ) = 0 , it can   be shown that NMI ( ^z ) = =   2H(z(X))=2H(z(X ) ) = 1 .   A.3 Exact Discovery Guarantee   First , we prove the claim made in Section 4.2 about   nearest neighbor information quantizers . Recall the   deﬁnition of general and nearest - neighbor informa-   tion quantizers as follows . Deﬁnition 3 . ( Information quantizer ) A K - point   information quantizer is a function q :  !   C = fQ;;Qg  , where Cis called   the codebook and Q ’s are called the code distri-   butions . Further , deﬁne Qto be the class of such   functions .   Deﬁnition 4 . ( Nearest - neighbor Information quan-   tizer ) AK - point information quantizer is called   nearest - neighbor if , 8P2;D(Pjjq(P ) ) =   minD(PjjQ ) . Further , deﬁne Q   Qto be the class of such functions .   Then we have the following lemma .   Lemma 1 . There exists an information quantizer   ^2;^q2Qsuch that   L(P;^;^q ) = minL(P;;q ):   ( 14 )   Therefore , ( ^;^q)is an ERM of ( P ) .   Proof of Lemma 1 . Notice that only the Lterm   of EquationPdepends onq , so it sufﬁces to show   thatminL(~P;q)minL(~P;q ) .   This is true since   minL(~P;q )   = minE[D(Pjjq(P ) ) ]   E [ minD(PjjQ ) ]   = minE[D(Pjjq(P ) ) ]   = minL(~P;q ) ;   where the inequality holds since   D(Pjjq(P))minD(PjjQ )   for anyq2Q.   Next , we show under the condition P=   Pandn!1 , ( P ) recoversz()up to a per-   mutation .   Proposition 3 . The pair ( z;P)is a minimizer   to the following optimization problem :   maxI(^z(X);Y ) ; ( P )   if and only if zis equal to the true assignment   functionzup to a permutation.8038Proof . ): First , z()is a feasible solution by deﬁ-   nition . By data processing inequality , we have   I(z(X);Y)I(X;Y ) = I(z(X);Y ):   Therefore , z()is also the optimal solution .   (: Suppose there exists some optimal ( ^z;^P )   with ^P6 = Pfor at least one x2X .   Since such discrepancies are independent with each   other , it sufﬁces to show that each such discrepancy   leads to lower I(Z;Y ) . Indeed , for ( ^z;^P)with   ^P6 = Ponly atx ,   I(^z(X);Y) I(z(X);Y )   = P(x)XP(yjx ) log^P   P   =  P(x)D(Pjj^P)<0 ;   which contradicts the optimality of ^z . Therefore ,   ^P = Pfor all optimal solution of ( P ) .   To prove Theorem 1 , we also need the following   lemma .   Lemma 2 . Under Assumption 3 , for any bounded   parameter set  , there exists    > 0and some   optimal parameter 2such that   D(PjjP)   k k;82 :   Proof . We prove the lemma by contradiction . First ,   we assume62since the inequality satisﬁes   trivially for any 2. By boundedness , there   exists some R > 0such thatkkR. Suppose   for any    > 0 , there exists some 2such that   D(PjjP)   k k  2   R , then   we haveD(PjjP)inf   R= 0 .   However , since D(PjjP)0 , we have   D(PjjP ) = 0 , which implies 2   and leads to contradiction .   Note it is crucial that the parameter set is   bounded , which is the case for neural nets . Fur-   ther , Assumption 3 is needed or the inequality can   be easily violated when the optimal parameter set   is empty .   Next , we need the following lemma , which is   based on ( Tsai et al . , 2020 ):   Lemma 3 . Under Assumptions 1 - 3 , and consider   ^to be part of the ERM of ( P ) with conditionaldistribution ^P:=P. Then for any  > 0 ,   the following inequality holds :   P   supD(Pjj^P)>   2  N(;   4 )  exp       n   2(C C)   ; ( 15 )   whereN(A;)is the-net of setA.   Proof . For notational ease , we drop the depen-   dence ofLonPif the context is clear . Using As-   sumption 3 , let P = P. DeﬁneD(PjjQ )   as the empirical KL divergence . Further , notice that   forP , Lcan always be made 0 and therefore ,   the ERM ofPneeds to satisfyL(^)L( ) .   As a result ,   D(Pjj^P )   : = E "   logP(YjX )   ^P(YjX ) #   = L(^) L()0 :   Note thatD(PjjQ)is an unbiased estimator of   the conditional KL divergence between distribu-   tionsPandQ : EElog=   D(PjjQ ) . Therefore , let ( ) : =   D(PjjP) D(PjjP ) ,   Pn   D(Pjj^P)>o      Pn   D(Pjj^P) D(Pjj^P)>o   Pfj()j>gP   supj()j>   :   To bound the last probability , consider an-   net in the parameter space N(;)and =   [  , where is the - ball surrounding   2N( ;) , we have82 ,   P   supj()j>   XP (   supj()j> )      N(;   4 )  supP (   supj()j> )   :   ( 16)8039Further , by Assumption 2 , we have   supj() ()j   sup  D(PjjP) D(PjjP )   +  D(PjjP) D(PjjP )  =   E  logP(YjX )   P(YjX )   + E  logP(YjX )   P(YjX )  2k k   2 :   As a result ,   P (   supj()j> )   P (   j()j+ supj() ()j> )   Pn   j()j>   2o   2 exp    n   2(C C)   ;   by Assumption 1 and Hoeffding ’s inequality . Plug-   ging this into ( 16 ) , we arrive at   Pn   D(Pjj^P)>o   2  N(;   4 )  exp    n   2(C C)   :( 17 )   To prove uniform convergence , use Assumption 2   to conclude that :   D(Pjj^P )   = XP(yjx ) logP(yjx )   P(yjx )   sup  logP(yjx )   P(yjx )  k ^k ;   for some2. Therefore , using Lemma 2 , we   arrive at the desired result :   P   supD(Pjj^P)   P   k ^k      P   D(Pjj^P)         2  N(;   4 )  exp       n   2(C C)   : Next , we prove the following lemma by perform-   ing a perturbation analysis on ( P ) inspired by ( Qiu   et al . , 2019 ) .   Lemma 4 . Consider some subset of speech seg-   mentsD  Xsuch that for any 1z   K , there exists x2Xsuch thatz(x ) = z.   Further , suppose there exists  > 0such that   k^P Pk;8x2 D . Then ,   8x2X , k^q(^P) Pkcfor   some constant c>0 .   Proof . We ﬁrst prove the statement for the seg-   ments from the set D. By the deﬁnition of ERM ,   L(P;^q) L(P;q ) ( 18 )   = E "   logP(YjX )   ^q(^P(YjX ) ) #   0 : ( * )   From the condition in the lemma , we have   ^P = P+  for some2[0;1 ]   and  2R ;  1= 0;k  k1;8x2D .   Further , suppose q(^P ) = P+ for some   2[0;1]and 2R ; 1= 0;k k   1;8x2X. Using Assumption 1 and the inequality   log(1 + x)x ;8x2( 1;1 ] , we have   X^P(yjx ) logP(yjx )   ^q(^P(yjx ) )   =  XP(yjx ) log   1 +  ( y )   P(yjx)    X  ( y ) logP(yjx )   ^q(^P(yjx ) )   X ( y )   4P(yjx) Ck ( y)k   4      4jYj C ;   for everyx2D. Therefore , to maintain ( 18 ) , we   need4CjYjfor the training examples X   and the inequality in the lemma holds for examples   fromDwith coefﬁcient c:= 2p   CjYj .   To show the same claim holds for any unseen seg-   mentsx2XnD , we ﬁrst use Lemma 1 to conclude   that there always exists a nearest - neighbor infor-   mation quantizer ^qthat is an ERM . Further , since   every phoneme class occurs in D , we can always   ﬁndx2D such thatz(x ) = z(x ) . Therefore , us-   ing the inequality log(1+x)x ;8x> 1,8040we have   1   2k^P ^q(^P)k   D(^Pjj^q(^P ) )   D(^Pjjq(^P ) )   D(Pjjq(^P ) )   + jD(Pjjq(^P ) )    D(Pjj^P)j   D(Pjjq(^P ) ) + (C C )   X ( y )   ^P(yjx)+(C C )   e   minP(yjz(x ) )   + (C C)a ;   where   a:=ec   minP(yjz(x ) )   + C C > c :   Notice that the minimum is taken over y ’s with   nonzero probabilities due to the boundedness con-   ditions in Assumption 1 , which asserts  ( y ) =    ( y)0fory ’s with zero probabilities . Finally ,   using triangular inequality :   kP ^q(^P)k   k^P ^q(^P)k+   k^P Pk   p   2a+cp   wherec:=p2a+ 1 is the coefﬁcient in the   lemma .   Now we are ready to prove Theorem 1 .   Proof of Theorem 1 . Deﬁne the event C:=   fsupD(Pjj^P)<g . Further , sup-   pose is within the ball of radius RinR. By   Lemma 3 , we have :   P(C)1 exp( cn+c( ) ) ; ( 19 )   wherec:=;c( ) : = dlogR(1 + ) + log 2log 2jN(;)j(see e.g. , ( Ver-   shynin , 2018 ) , Section 4.2 ) . For the subsequent   discussion , suppose Coccurs . To prove that^zachieves zero TER , it sufﬁces to prove that   ^z(x ) = ^z(x),z(x ) = z(x);8x;x2X. To   prove the “ ) ” direction , suppose for some seg-   ment pairs ( x;x)2X,^z(x ) = ^z(x ) = z   butz(x ) = z6 = z(x ) = z. Invoke Lemma   4 and write Q = P+ ; =   c ; 1= 0;k k1;j2f1;2 g. Use   the inequality log(1 + x)x ;8x> 1we   have   D(PjjQ )   =  XP(yjx ) log   1 +  ( y )   P(yjx)   Xe ( y )   P(yjx)a(z;z) ;   where a(z;z ) =   maxe = minP(yjz ) .   As a result ,   2a(z;z)   D(PjjQ ) + D(PjjQ)   2D(PjjP);(20 )   which can not be true if  ,   or.   To prove the other direction , we use “ ) ” to con-   clude that every phoneme occurs in at least one dis-   tinct cluster from other classes , since every cluster   in^Ccontains only a unique phoneme class . Fur-   ther , deﬁneE=minP1= 0 	  . Us-   ing Sanov ’s theorem ( see e.g. , ( Cover and Thomas ,   2006 ) ) , we have :   P(E)   ( n+ 1)exp    nminD(PjjP)   ;   where P:=fP2 : minP(z ) = 0 g. Use   Assumption 4 and optimize the bound , we obtain   minD(PjjP )   = minD   Pjj1   K1   = logK maxH(P ) = logK   K 1   and   P(E)exp    nlogK   K 1+Klog(n+ 1)   : 8041As a result , phonemes of each class occur at least   once in the training set with high probability . If this   is the case and if there exists some x;x2Xsuch   thatz(x ) = z(x)but^z(x)6= ^z(x),^Ccontains at   leastK+1clusters , which contradicts Assumption   4 . Therefore , deﬁne the event R:=f^z(X ) =   ^z(X),z(X ) = z(X)g , the token error rate can   be upper bounded as   P(^z )   P(C\E)PfRjC\Eg+P(C[E )   = exp (  nminfe(n;);e(n;K)g ) ;   where   := minD(PjjP )   c(z;z)a(z;z )   = : minc(z;z)D(PjjP )   e( ) : = c c( )   n   e:= logK   K 1 Klog(n+ 1 )   n :   Therefore , P(^z)amounts to   cn c()log1      nlogK   K 1 Klog(n+ 1)log1    :   The ﬁrst inequality implies   nlogc( ) + ( 1= )   c=O   log ( )    !   :   For the second inequality , rearranging the terms we   obtain :   nK   loglogn+log   log ; ( 21 )   which by Lemma A.2 from ( Shalev - Shwartz and   Ben - David , 2014 ) holds if   n4Klog+ 2 log   log   = O   log   log !   :( 22 )   Combining Equation 21 and Equation 22 proves   the theorem . B Collection Process and Statistics of the   Spoken Word Datasets   The dataset statistics of all the datasets used for our   experiments are shown in Table 4 . We collect all   the spoken word datasets from existing datasets in   the following steps :   1.Decide the train - test split : For Flickr audio ,   we use the original training and validation set   to extract spoken words for the training set   and the test set to extract words for test set ;   for LibriSpeech , we use train - clean-100 and   train - clean-360 for training set and dev - clean   for test set ; for TIMIT and Mboshi , we use   the whole dataset without SAutterances to   extract spoken words , to be consistent with   prior works . For the latter , it will not lead to   overﬁtting since our setting is unsupervised in   a sense that the target label , phoneme , is not   available during training .   2.Decide the phoneme inventory : The phoneme   inventory of the English corpora such as Flickr   audio , LibriSpeech and TIMIT are the stan-   dard 61 phonemes from TIMIT merged into   39 classes for LibriSpeech and 44 classes   for Flickr Audio , due to slightly different   phoneme set required for the forced align-   ment systems used to extract phoneme and   word boundaries . The phoneme inventory of   Mboshi is provided in ( Godard et al . , 2017 ) .   3.Decide the vocabulary : For English corpora ,   we use a neural dependency parser ( Gardner   et al . , 2017 ) to extract head words of noun   phrases from the Flickr30kEntities and choose   those with frequency more than 500 times   in the entire Flickr30k corpus . For Mboshi ,   we use the bigrams and trigrams as proxy for   words .   4.Word and phoneme boundary detection : For   evaluation purposes , we need to extract   word and phoneme boundaries for the ut-   terances . While TIMIT and Mboshi has   provided frame - level phoneme transcriptions ,   such labels are not available for Flickr Au-   dio and LibriSpeech . Therefore , we use the   Montreal forced aligner to extract word and   phoneme boundaries for LibriSpeech and an-   other HMM - DNN hybrid ASR system to ex-   tract segment boundaries for Flickr audio8042   5.Extract spoken word utterances : To keep the   dataset as balanced as possible , we set a cutoff   on the maximal number of word utterances   per class , which is set to be 200 for Flickr   Audio and 1000 for LibriSpeech , TIMIT and   Mboshi .   C Model Implementation   For the pre - segmentation stage in Figure 2 of   IQ , we use the self - supervised model proposed in   ( Kreuk et al . , 2020 ) to predict the phoneme - level   segmentation for English datasets , and the segmen-   tation generated by one of our baselines ( Feng et al . ,   2021a ) for experiments on Mboshi language . The   segmental speech encoder e()is a CPC model   pretrained on the whole 960h LibriSpeech ( Nguyen   et al . , 2020 ) with 256 - dimensional representation   for each 10ms frame followed by averaging across   each segments . The word posterior c()for the   joint distribution learning stage consists of four hid-   den layers and 512 ReLU units per layer with layer   normalization and one softmax output layer . All   our models are trained for 20 epochs using Adam   optimizer ( Kingma and Ba , 2014 ) with learning   rate of 0.001 decayed by 0.97 every 2 epochs and a   batch size of 8 . We slightly modify ( P ) analogous   to the VQ - V AE ( van den Oord et al . , 2017 ) to make   it more suitable for gradient - based optimization :   L ( P;;q ) : = L(P;)+   E[D(sg[P]jjq(P))+   D(Pjjsg[q(P ) ] ) ]   where sg [ ]denotes the stop - gradient operation and   = 0:5for all experiments . Exponential moving   average ( EMA ) codebook update is used with a   decay rate of 0:999to optimize the ﬁrst KL term .   Each code distribution is initialized using a sym - metric Dirichlet distribution with a concentration   parameter of 100 .   For CPC , wav2vec and wav2vec 2.0 , we extract   discrete units using the same predicted and gold   segmentations as our IQ model using k - means clus-   tering with the same number of clusters ( K= 31 ) .   D Convergence Plot for Word - Level   Phoneme Discovery   The convergence plot of Token F1 during train-   ing of IQ on Flickr Audio compared to the baselines   is shown in Figure 6 .   E Further Analysis of Representations   Learned by IQ   The visualizations of the estimated distributions   Pusing t - SNE ( van der Maaten and Hinton ,   2008 ) on Mboshi are shown in Figure 7 . We again   observe that IQ is capable of clustering phonemes   from the same manner class as shown in the t - SNE   plots for TIMIT in the main text . We also show the   most confusing phoneme pairs for both datasets in   Table 8a and Table 8b respectively , where the error8043probability for a phoneme pair is deﬁned as the   probability that segments of different phonemes in   the pair are assigned to the same cluster . While we   can see that most phoneme pairs confused by the   model are acoustically very similar such as ( /ae/ ,   /aa/ ) , ( /z/ , /s/ ) in TIMIT and ( /e/ , /a/ ) , ( /bv/ , /b/ ) in   Mboshi , we also observe some non - obvious pairs   such as the pair ( /ch/ , /ah/ ) in TIMIT . From the   confusion matrix shown in Fig 33b , we can see that   this is due to the high variability and potentially   lack of samples for the vowel /ah/ , which makes its   cluster more likely to be merged by other bigger   clusters . A more general reason for the model to   confuse between such non - obvious pairs may be   that distinguishing such phonemes is not very use-   ful in discriminating those words used during the   IQ training , which is possible since the vocabulary   size during training is relatively small ( < 1000 ) .   F Effect of Codebook Size for IQ   The phoneme discovery results of IQ with different   codebook sizes on Mboshi and TIMIT are shown   in Table 5 and Table 6 respectively . As discussed   in the paper , our IQ model achieving equally good   NMI and boundary F1 and is thus robust to the   codebook size on both datasets .   G More Speech Examples   Lastly , we provide eight more spoken utterances   annotated with phoneme discovery results.804480458046Codebook size 30 40 50 60 70   jYj= 224Token F1 51.21:050.90:850.30:649.01:249.00:4   NMI 43.00:743.40:943.60:343.10:743.50:5   Boundary F1 77.70:578.60:478.20:378.10:678.30:6   jYj= 524Token F1 53.50:853.90:353.00:952.00:952.50:7   NMI 46.80:646.70:246.70:446.90:347.30:2   Boundary F1 80.40:280.40:280.30:180.20:180.30:1   jYj= 824Token F1 53.70:554.40:453.30:452.60:850.70:9   NMI 47.10:447.50:247.30:247.40:447.10:4   Boundary F1 80.60:080.50:180.40:180.30:080.30:0   Codebook size 30 40 50 60 70   jYj= 161Token F1 54.21:054.20:251.10:954.00:745.90:8   NMI 45.10:444.00:444.70:244.30:744.30:5   Boundary F1 67.50:067.40:167.30:167.30:166.80:0   jYj= 377Token F1 57.11:057.21:156.71:656.81:155.20:4   NMI 49.30:349.00:149.80:249.60:449.50:6   Boundary F1 67.30:167.30:167.30:167.10:267.00:08047