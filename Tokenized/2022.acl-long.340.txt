  Donghan Yu , Chenguang Zhu , Yuwei Fang , Wenhao Yu , Shuohang Wang ,   Yichong Xu , Xiang Ren , Yiming Yang , Michael ZengCarnegie Mellon UniversityMicrosoft Cognitive Services Research GroupUniversity of Notre DameUniversity of Southern Californiadyu2@cs.cmu.edu , chezhu@microsoft.com   Abstract   Current Open - Domain Question Answering   ( ODQA ) models typically include a retriev-   ing module and a reading module , where the   retriever selects potentially relevant passages   from open - source documents for a given ques-   tion , and the reader produces an answer based   on the retrieved passages . The recently pro-   posed Fusion - in - Decoder ( FiD ) framework is   a representative example , which is built on   top of a dense passage retriever and a gener-   ative reader , achieving the state - of - the - art per-   formance . In this paper we further improve   the FiD approach by introducing a knowledge-   enhanced version , namely KG - FiD. Our new   model uses a knowledge graph to establish the   structural relationship among the retrieved pas-   sages , and a graph neural network ( GNN ) to   re - rank the passages and select only a top few   for further processing . Our experiments on   common ODQA benchmark datasets ( Natural   Questions and TriviaQA ) demonstrate that KG-   FiD can achieve comparable or better perfor-   mance in answer prediction than FiD , with less   than 40 % of the computation cost .   1 Introduction   Open - Domain Question Answering ( ODQA ) is the   task of answering natural language questions in   open domains . A successful ODQA model relies   on effective acquisition of world knowledge . A   popular line of work treats a large collection of   open - domain documents ( such as Wikipedia arti-   cles ) as the knowledge source , and design a ODQA   system that consists of a retrieving module and a   reading module . The retriever pulls out a small   set of potentially relevant passages from the open-   source documents for a given question , and the   reader produces an answer based on the retrieved   passages ( Karpukhin et al . , 2020 ; Guu et al . , 2020 ;   Izacard and Grave , 2020 ) . An earlier example of   this kind is DrQA ( Chen et al . , 2017 ) , which usedan traditional search engine based on the bag of   words ( BoW ) document representation with TF-   IDF term weighting , and a neural reader for extract-   ing candidate answers for each query based on the   dense embedding of the retrieved passages . With   the successful development of Pre - trained Lan-   guage Models ( PLMs ) in neural network research ,   dense embedding based passage retrieval ( DPR )   models ( Karpukhin et al . , 2020 ; Qu et al . , 2021 )   have shown superior performance over BoW / TF-   IDF based retrieval models due to utilization of   contextualized word embedding in DPR , and gen-   erative QA readers ( Lewis et al . , 2020 ; Roberts   et al . , 2020 ) usually outperform extraction based   readers ( Devlin et al . , 2019 ; Guu et al . , 2020 ) due   to the capability of the former in capturing lexical   variants with a richer flexibility .   The recently proposed Fusion - in - Decoder ( FiD )   model ( Izacard and Grave , 2021 ) is representative   of those methods with a DPR retriever and a gen-   erative reader , achieving the state - of - the - art results   on ODQA evaluation benchmarks . FiD also signif-   icantly improved the scalability of the system over   previous generative methods by encoding the re-   trieved passages independently instead of encoding   the concatenation of all retrieved passages ( which   was typical in previous methods ) .   Inspired by the success of FiD , this paper aims   further improvements of the state of the art of   ODQA in the paradigm with a DPR retriever and   a generative reader . Specifically , we point out two   potential weaknesses or limitations of FiD as the   rooms for improvements , and we propose a novel   solution namely KG - FiD to address these issues   with FiD. The two issues are :   Issue 1 . The independent assumption among   passages is not justified . Notice that both the   DPR retriever and the generative reader of FiD   perform independent encoding of the retrieved pas-   sages , which means that they can not leverage the   semantic relationship among passages for passage4961embedding and answer generation even if such re-   lational knowledge is available . But we know that   rich semantic connections between passages often   provide clues for better answering questions ( Min   et al . , 2019 ) .   Issue 2 . Efficiency Bottleneck . For each in-   put question , the FiD generative reader receives   about 100 passages from the DPR module , with a   relatively high computational cost . For example ,   the inference per question takes more than 6 tril-   lion floating - point operations . Simply reducing the   number of retrieved passages sent to the reader will   not be a good solution as it will significantly de-   crease the model performance ( Izacard and Grave ,   2021 ) . How to overcome such inefficient computa-   tion issue is a challenging question for the success   of FiD in realistic ODQA settings .   We propose to address both of the above is-   sues with FiD by leveraging an existing knowledge   graph ( KG ) to establish relational dependencies   among retrieved passages , and employing Graph   Neural Networks ( GNNs ) to re - rank and prune re-   trieved passages for each query . We name our new   approach as KG - FiD.   Specifically , KG - FiD employs a two - stage pas-   sage reranking by applying GNN to model struc-   tural and semantic information of passages . Both   stages rerank the input passages and only a few   top - reranked passages are fed into subsequent mod-   ules . The first stage reranks passages returned by   the retriever , where we use the passage embeddings   generated by DPR as the initial GNN node repre-   sentation . This allows reranking a much larger set   of initial candidate passages to enhance coverage   of answers . The second stage performs joint pas-   sage reranking and answer generation , where the   node embeddings are initialized by the embeddings   of passage - question pairs output from the reader   encoder . This stage operates on a smaller candi-   date set but aims for more accurate reranking and   passage pruning .   To improve the efficiency , in the second - stage   reranking , our GNN model adopts representations   from the intermediate layer in the reader encoder   instead of the final layer to initiate passage node em-   beddings . Then only a few top reranked passages   will be passed into the higher layers of encoder   and the decoder for answer generation , while other   passages will not be further processed . This is cou-   pled with a joint training of passage reranking and   answer generation . As shown in Section 4.3 , thesestrategies significantly reduce the computation cost   while still maintaining a good QA performance .   Our experiments on ODQA benchmark datasets   Natural Questions and TriviaQA demonstrate that   KG - FiD can achieve comparable or better perfor-   mance in answer prediction than FiD , with only   40 % of the computation cost of FiD.   2 Related Work   ODQA with text corpus ODQA usually as-   sumes that a large external knowledge source is   accessible and can be leveraged to help answer   prediction . For example , previous works ( Chen   et al . , 2017 ; Karpukhin et al . , 2020 ; Izacard and   Grave , 2021 ) mainly use Wikipedia as knowledge   source which contains millions of text passages .   In this case , current ODQA models mainly con-   tains a retriever to select related passages and a   reader to generate the answer . Thus , the follow - up   works mainly aim to : ( 1 ) Improve the retriever :   from sparse retrieval based on TF - IDF or BM25   ( Chen et al . , 2017 ; Yang et al . , 2019 ) to dense re-   trieval ( Karpukhin et al . , 2020 ) based on contextual-   ized embeddings generated by pre - trained language   models ( PLMs ) . Moreover , some further improve-   ment are also proposed such as better training strat-   egy ( Qu et al . , 2021 ) , reranking based on retrieved   passages ( Wang et al . , 2018 ; Nogueira and Cho ,   2019 ; Mao et al . , 2021 ) , and knowledge distilla-   tion from reader to retriever ( Izacard and Grave ,   2020 ) ; ( 2 ) Improve the reader : changing from   Recurrent Neural Network ( Chen et al . , 2017 ) to   PLMs such as extractive reader BERT ( Karpukhin   et al . , 2020 ; Iyer et al . , 2021 ; Guu et al . , 2020 ) and   generative reader BART and T5 ( Izacard and Grave ,   2021 ; Lewis et al . , 2020 ) . Besides , some works   ( Guu et al . , 2020 ; Lewis et al . , 2020 ; Sachan et al . ,   2021 ) have shown that additional unsupervised   pre - training on retrieval - related language model-   ing tasks can further improve ODQA performance .   However , none of these methods modeled the rela-   tionships among different passages .   ODQA with knowledge graph Besides the un-   structured text corpus , world knowledge also exists   in knowledge graphs ( KGs ) , which represent enti-   ties and relations in a structural way and have been   used in a variety of NLP tasks ( Xu et al . , 2021b ; Yu   et al . , 2020 ; Xu et al . , 2021a ) . Some works ( Berant   et al . , 2013 ; Sun et al . , 2018 , 2019 ; Xiong et al . ,   2019 ) restrict the answer to be entities in the knowl-   edge graph , while our work focus on more general4962ODQA setting where the answer can be any words   or phrases . Under this setting , some recent efforts   have been made to leverage knowledge graphs for   ODQA ( Min et al . , 2019 ; Asai et al . , 2020 ; Zhou   et al . , 2020 ) . For example , UniK - QA ( Oguz et al . ,   2020 ) transforms KG triplets into text sentences   and combine them into text corpus , which loses   structure information of KG . Other works use KG   to build relationship among passages similar to   ours . KAQA ( Zhou et al . , 2020 ) use passage graph   to propagate passage retrieve scores and answer   span scores . Graph - Retriever ( Min et al . , 2019 )   iteratively retrieve passages based on the relation-   ship between passages , and also use passage graph   to improve passage selection in an extractive reader .   However , applying KG to improve the recent ad-   vanced FiD framework remains unstudied .   3 Method   In the following sections , we first introduce how to   apply KG to build a graph structure among the re-   trieved passages ( Section 3.1 ) . Then we show how   we adopt the graph - based stage-1 reranking with   DPR retriever to improve passage retrieval ( Section   3.2 ) . Next we introduce joint stage-2 reranking and   answer generation in the reading module ( Section   3.3 ) . Finally we illustrate the improvement of effi-   ciency by using intermediate layer representation   for stage-2 reranking ( Section 3.4 ) . The overview   of our framework is illustrated in Figure 1 .   3.1 Construct Passage Graph using KG   The intuition behind using KG is that there ex-   ists the structural relationship among the retrieved   passages which can be captured by the KG . Sim-   ilar to ( Min et al . , 2019 ) , we construct the pas-   sage graph where vertices are passages of text and   the edges represent the relationships that are de-   rived from the external KGs as KG={(e , r , e ) } ,   where e , r , eare the head entity , relation and tail   entity of a triplet respectively .   First , we formalize the definition of a passage .   Following previous works ( Wang et al . , 2019 ;   Karpukhin et al . , 2020 ) , each article in the text   corpus is split into multiple disjoint text blocks   of 100 words called passages , which serve as the   basic retrieval units . We assume there is a one-   one mapping between the KG entities and articles   in the text corpus . Specifically , we use English   Wikipedia as the text corpus and English Wiki-   data ( Vrande ˇci´c and Krötzsch , 2014 ) as the knowl - edge graph , since there exists an alignment between   the two resources . For example , for the article ti-   tled with “ New York Yankees ” , it contains passages   such as “ The New York Yankees are an American   professional baseball team ... ” . The article also   corresponds to a KG entity with the same name as   “ New York Yankees ” .   Then we define the mapping function e = f(p ) ,   where the KG entity ecorresponds to the article   which pbelongs to . Note that one passage can only   be mapped to one entity , but multiple passages   could be mapped to the same entity . The final   passage graph is defined as G={(p , p ) } , where   passages pandpare connected if and only if their   mapped entities are directly connected in the KG ,   i.e. ,(f(p ) , r , f(p))∈ KG .   Since the total number of passages is very large ,   e.g. , more than 20 M in Wikipedia , constructing   and maintaining a graph over all the passages is   inefficient and memory - consuming . Thus , we build   a passage graph on the fly for each question , based   on the retrieved passages .   3.2 Passage Retrieving & Stage-1 Reranking   DPR Retriever : Our framework applies DPR   ( Karpukhin et al . , 2020 ) as the retriever , which   uses a BERT based passage encoder to encode all   theNpassages in the text corpus { p , p , · · · , p } .   Suppose all the passage embeddings are fixed and   stored in memory as M∈Rwhere Dis the   hidden dimension :   M = BERT ( p)fori∈ { 1,2,···N}(1 )   For an input question q , DPR applies another BERT-   based question encoder to obtain its representation   Q , then it builds on FAISS ( Johnson et al . , 2019 ) to   conduct fast dot - product similarity search between   QandM , and returns N(N≪N ) passages   with the highest similarity scores .   Stage-1 Reranking : We see that the DPR re-   triever returns Npassages which are indepen-   dently retrieved based on the similarity between   the question and each passage , without considering   inter - passage relationship . Thus instead of directly   retrieving Npassages for the reader , we propose to   first retrieve N(N > N ) passages , then rerank   them and output top- Nreranked passages into the   reader .   Following Section 3.1 , we construct a graph   among the Nretrieved passages denoted as G.4963   We aim to rerank the retrieved passages based on   both the structural information and the textual se-   mantic information of them .   To represent the semantic information of pas-   sages , one can use another pre - trained language   model to encode the passage texts , but this will not   only include lots of additional model parameters ,   but also incur heavy computational cost as Ncan   be large . To avoid both additional memory and   computation cost , we propose to reuse the offline   passage embeddings Mgenerated from the DPR   retriever in Equation 1 as the initial node representa-   tion : E = Mwhere { r|i∈ { 1,2 , · · · , N } }   is the set of retrieved passage indices .   Then we employ a graph attention network   ( GAT ) ( Velickovic et al . , 2018 ) with Llayers   as GNN model to update representations for each   node based on the passage graph and initial repre-   sentation . The l - th layer of the GNN model updates   the embedding of node ias follows :   E = h(E,{E } ) ( 2 )   where his usually a non - linear learnable function   which aggregates the embeddings of the node it-   self and its neighbor nodes . The reranking score   for each passage pis calculated by s = QE , where Qis the question embedding also   generated by the DPR retriever . Then we sort the re-   trieved passages by the reranking scores , and input   the top- Npassages into the reader . The training   loss of passage ranking for each question is :   L = −Xylog   exp(s )   Pexp(s ) !   ( 3 )   where y= 1ifpis the gold passagethat con-   tains the answer , and 0 otherwise .   As we only add a lightweight graph neural net-   work and reuse the pre - computed and static DPR   passage embeddings , our reranking module can   process a large number of candidate passages effi-   ciently for each question . In experiments , we set   N= 1000 andN= 100 .   3.3 Joint Stage-2 Reranking and Answer   Generation   In this section , we briefly introduce the vanilla   FiD reading module before illustrating our joint4964reranking method . We suppose the reader takes N   retrieved passages { p , p , · · · , p}as input .   Vanilla FiD Reading Module : We denote the   hidden dimension as Hand number of encoder   layers and decoder layers as L , FiD reader first   separately encodes each passage pconcatenated   with question q :   P = T5 - Embed ( q+p)∈R , ( 4 )   P = T5 - Encoder(P)∈R , ( 5 )   where Tis the sequence length of a passage con-   catenated with the question . T5 - Embed ( · ) is the   initial embedding layer of T5 model ( Raffel et al . ,   2019 ) and T5 - Encoder(·)is the l - th layer of its   encoder module . Then the token embeddings of   all passages output from the last layer of the en-   coder are concatenated and sent to the decoder to   generate the answer tokens A :   A = T5 - Decoder   [ P;P;···;P]   ( 6 )   Stage-2 Reranking : Note that vanilla FiD   reader neglect the cross information among pas-   sages , and the joint modeling in the decoding pro-   cess makes it vulnerable to the noisy irrelevant   passages . Thus , we propose to leverage the pas-   sage graph to rerank the input Npassages during   the encoding and only select top- N(N < N )   reranked passages into the decoder , which is named   as stage-2 reranking .   Similar to stage-1 reranking , the reranking   model is based on both the structural information   and the textual semantic information of passages .   We denote the passage graph as G , which is a   subgraph of G. To avoid additional computation   and memory cost , we propose to reuse the encoder-   generated question - aware passage representation   from FiD reader for passage reranking as it is al-   ready computed in Equation 5 . Specifically , the ini-   tial node embeddings Zfor passage pcomes   from the first token embedding of the final layer in   the FiD - Encoder , i.e. , Z = P(0)∈R. Then   same as stage-1 reranking , we also employ a GAT   ( Velickovic et al . , 2018 ) with Llayers as the graph   neural network ( GNN ) model to update represen-   tations for each node based on the passage graph ,   similar to Equation 2 : Z = GAT(Z , G ) .   The reranking score of passage pis calculated   bys = WZ where Wis a trainable   model parameter . After reranking , only the finaltop - N(N < N ) passages are sent for decoding .   Suppose their indices are { g , g , · · · , g } , the   decoding process in Equation 6 becomes :   A = T5 - Decoder   [ P;P;···;P]   ( 7 )   where Ais the generated answer . Similar to stage-1   reranking , the training loss of passage ranking for   each question is :   L = −Xylog   exp(s )   Pexp(s ) !   ( 8)   where y= 1ifpis the gold passage that contains   the answer , and 0 otherwise .   The passage reranking and answer generation are   jointly trained . We denote the answer generation   loss for each question is L , then the final training   loss of our reader module is L = L+λL ,   where λis a hyper - parameter which controls the   weight of reranking task in the total loss .   Note that the first stage reranking is based on   DPR embeddings , which are are high - level ( one   vector per passage ) and not further trained . While   the second stage is based on reader - generated   passage - question embeddings , which are semantic-   level and trainable as part of the model output .   Thus the second stage can better capture semantic   information of passages and aims for more accu-   rate reranking over a smaller candidate set . In the   experiment , we set N= 100 andN= 20 .   3.4 Improving Efficiency via Intermediate   Representation in Stage-2 Reranking   Recall that in the stage-2 reranking , we take the   passage representation from the last layer of reader   encoder for passage reranking . In this section , we   propose to further reduce the computation cost by   taking the intermediate layer representation rather   than the last layer . The intuition is that answer gen-   eration task is more difficult than passage reranking   which only needs to predict whether the passage   contains the answer or not . Thus we may not need   the representation from the whole encoder module   for passage reranking .   Suppose we take the representation from the L-   th layer ( 1≤L < L ) , i.e. , Z = P(0)for   i∈ { 1,2 , · · · , N } , and the reranking method re-   mains the same . Then only the top N(N < N )   reranked passages will go through the rest lay-   ers of FiD - encoder . Suppose their indices are4965I={g , g , · · · , g } , forl≥L+ 1 :   P= (   T5 - Encoder(P)ifi∈I   Stop - Computing else(9 )   Then P , P,···,Pare sent into the de-   coder for answer generation as in Equation 7 . In   Section 4.3 , we demonstrate this can reduce 60 %   computation cost than the original FiD while keep-   ing the on - par performance on two benchmark   datasets .   3.5 Analysis on Computational Complexity   Here we analyze the theoretical time complexity of   our proposed KG - FiD compared to vanilla FiD.   More practical computation cost comparison is   shown in Appendix A.5 . Because both the compu-   tations of DPR retrieving and stage-1 reranking are   negligible compared to the reading part , we only   analyze the reading module here .   Suppose the length of answer sequence Ais de-   noted as Tand the average length of the passage   ( concatenated with question ) is T. For vanilla   FiD reader , the time complexity of the encoder   module is O(L·N·T ) , where L , Ndenote   the number of encoder layers and the number of   passages for reading . The square comes from the   self - attention mechanism . The decoder time com-   plexity is O(L·(N·T·T+T ) ) , where N·T·T   comes from the cross - attention mechanism . For our   reading module , all the Ncandidate passages are   processed by the first Llayers of encoder . But   only Npassages are processed by the remain-   ingL−Lencoder layers and sent into the de-   coder . Thus , the encoder computation complexity   becomes O((L·N+(L−L)·N)·T ) , and the   decoder computation takes O(L·(N·T·T+T ) ) .   Because L < L , N < N , both the encoding   and decoding of our method is more efficient than   vanilla FiD.   Furthermore , the answer is usually much shorter   than the passage ( which is the case in our experi-   ments ) , i.e. , T≪T. Then the decoding compu-   tation can be negligible compared to the encoding .   In this case , the approximated ratio of saved com-   putation cost brought by our proposed method is :   S= 1−(L·N+ ( L−L)·N)·T   L·N·T   = ( 1−L   L)(1−N   N)This shows that we can reduce more computation   cost by decreasing LorN. For example , if set-   tingL = L/4 , N = N/5 , we can reduce about   60 % of computation cost . More empirical results   and discussions will be presented in Section 4.3 .   4 Experiment   In this section , we conduct extensive experiments   on two most commonly - used ODQA benchmark   datasets : Natural Questions ( NQ ) ( Kwiatkowski   et al . , 2019 ) which is based on Google Search   Queries , and TriviaQA ( Joshi et al . , 2017 ) which   contains questions from trivia and quiz - league web-   sites . We follow the same setting as ( Izacard and   Grave , 2021 ) to preprocess these datasets , which is   introduced in Appendix A.1 . All our experiments   are conducted on 8 Tesla A100 40 GB GPUs .   4.1 Implementation Details   Knowledge Source : Following ( Karpukhin et al . ,   2020 ; Izacard and Grave , 2021 ) , we use the English   Wikipedia as the text corpus , and apply the same   preprocessing to divide them into disjoint passages   with 100 words , which produces 21 M passages in   total . For the knowledge graph , we use English   Wikidata . The number of aligned entities , relations   and triplets among these entities are 2.7 M , 974 and   14 M respectively .   Model Details : For the retrieving module , we   use the DPR retriever ( Karpukhin et al . , 2020 )   which contains two BERT ( base ) models for encod-   ing question and passage separately . For the GNN   reranking models , we adopt 3 - layer Graph Atten-   tion Networks ( GAT ) ( Velickovic et al . , 2018 ) . For   the reading module , same as ( Izacard and Grave ,   2021 ) , we initialize it with the pretrained T5 - base   and T5 - large models ( Raffel et al . , 2019 ) , and we   name the former one as KG - FiD ( base ) and the   latter one as KG - FiD ( large ) . Our implementa-   tion is based on the HuggingFace Transformers   library ( Wolf et al . , 2019 ) . For number of passages ,   we set N= 1000 , N= 100 , N= 20 . The   training process of our method is introduced in Ap-   pendix A.3 . More results about model design and   hyper - parameter search is in Appendix A.4 .   Evaluation : We follow the standard evaluation   metric of answer prediction in ODQA , which is the   exact match score ( EM ) ( Rajpurkar et al . , 2016 ) . A   generated answer is considered correct if it matches   any answer in the list of acceptable answers after4966normalization . For all the experiments , we con-   duct 5 runs with different random seeds and report   the averaged scores .   4.2 Baseline Methods   We mainly compare KG - FiD with the baseline   model FiD ( Izacard and Grave , 2021 ) . For other   baselines , we compare with representative meth-   ods from each category : ( 1 ) not using external   knowledge source : T5 ( Roberts et al . , 2020 ) and   GPT-3 ( Brown et al . , 2020 ) ; ( 2 ) reranking - based   methods : RIDER ( Mao et al . , 2021 ) and RECON-   SIDER ( Iyer et al . , 2021 ) ; ( 3 ) leveraging knowl-   edge graphs or graph information between pas-   sages : Graph - Retriever ( Min et al . , 2019 ) , Path-   Retriever ( Asai et al . , 2020 ) , KAQA ( Zhou et al . ,   2020 ) , and UniK - QA ( Oguz et al . , 2020 ) . We also   compare with methods ( 4 ) with additional large-   scale pre - training : REALM ( Guu et al . , 2020 ) ,   RAG ( Lewis et al . , 2020 ) and Joint Top - K ( Sachan   et al . , 2021 ) .   4.3 Main Results   Comparison with Baselines : Table 1 shows the   results of our method and all baselines . We see that   our proposed model KG - FiD consistently and sig-   nificantly improves FiD on both NQ and TriviaQA   datasets over both base and large model . Specifi-   cally , for large model , KG - FiD improves FiD by   1.5%and1.1%on two datasets respectively , which   has larger improvement compared to base model .   We think the reason is that more expressive reader   will also benefit the stage-2 reranking since the   initial passage embeddings are generated by the   reader encoder module . We also see that our pro-   posed method outperforms all the baseline meth-   ods except UniK - QA ( Oguz et al . , 2020 ) . How-   ever , UniK - QA uses additional knowledge source   Wikipedia - Table for retrieval , which is highly re-   lated with the NQ dataset and makes it unfair to   directly compare with our method .   Efficiency & Accuracy : Table 2 show the de-   tailed comparison between our method and FiD   in the large model version . The results of base   model version is shown in Appendix A.4 . Be-   sides EM score , we also report the ratio of compu-   tation flops ( # FLOPs ) and inference latency ( per   question ) . The detailed calculation of # FLOPs is   shown in Appendix A.5 . From table 2 , we seeModel # params NQ TriviaQA   T5 11B 36.6 -   GPT-3 ( few - shot ) 175B 29.9 -   RIDER 626 M 48.3 -   RECONSIDER 670 M 45.5 61.7   Graph - Retriever 110 M 34.7 55.8   Path - Retriever 445 M 31.7 -   KAQA 110 M - 66.6   UniK - QA990 M 54.064.1   REALM 330 M 40.4 -   RAG 626 M 44.5 56.1   Joint Top - K 440 M 49.2 64.8   FiD ( base ) 440 M 48.2 65.0   FiD ( large ) 990 M 51.4 67.6   Our Implementation   FiD ( base ) 440 M 48.8 66.2   KG - FiD ( base ) 443 M 49.6 66.7   FiD ( large ) 990 M 51.9 68.7   KG - FiD ( large ) 994 M 53.4 69.8   that ( 1 ) for KG - FiD , decreasing Lcan improve   the computation efficiency as analyzed in Section   3.4 , while increasing Lcan improve the model   performance . We think the performance improve-   ment comes from the noise reduction of passage   filtering . For a larger L , the passage embeddings   for reranking will have a better quality so that the   gold passages are less likely to be filtered out . ( 2 )   Simply reducing the number of passages Ninto   vanilla FiD reader can reduce computation cost ,   but the performance will also drop significantly   ( from 51.9 to 50.3 on NQ dataset ) . ( 3 ) Our model   can achieve the performance on par with FiD with   only 38 % of computation cost . When consum-   ing the same amount of computations ( L= 24 ) ,   our model significantly outperforms FiD on both   NQ and TriviaQA datasets . These experiments   demonstrate that our model is very flexible and can   improve both the efficiency and effectiveness by   changing L.   4.4 Ablation Study   Effect of Each Reranking Stage : Since our pro-   posed graph - based reranking method are applied   in both retrieving stage ( Section 3.2 ) and reading   stage ( Section 3.3 ) . We conduct ablation study4967Model # FLOPsNQ TriviaQA   EM Latency ( s ) EM Latency ( s )   FiD ( N=40 ) 0.40x 50.3 0.74 ( 0.45x ) 67.5 0.73 ( 0.44x )   FiD ( N=100 ) 1.00x 51.9 1.65 ( 1.00x ) 68.7 1.66 ( 1.00x )   KG - FiD ( N=100 , L=6 ) 0.38x 52.0 0.70 ( 0.42x ) 68.9 0.68 ( 0.41x )   KG - FiD ( N=100 , L=12 ) 0.55x 52.3 0.96 ( 0.58x ) 69.2 0.94 ( 0.57x )   KG - FiD ( N=100 , L=18 ) 0.72x 52.6 1.22 ( 0.74x ) 69.8 1.22 ( 0.73x )   KG - FiD ( N=100 , L=24 ) 0.90x 53.4 1.49 ( 0.90x ) 69.8 1.48 ( 0.89x )   ModelNQ TriviaQA   base large base large   FiD 48.8 51.9 66.2 68.7   KG - FiD 49.6 53.4 66.7 69.8   w/o Stage-1 49.3 53.1 66.2 69.5   w/o Stage-2 49.4 52.3 66.5 69.2   to validate the effectiveness of each one . Table   3 shows the experiment results by removing each   module . We see the performance of KG - FiD drops   when removing any of the two reranking modules ,   demonstrating both of them can improve model   performance . Another thing we observe is that   stage-1 reranking is more effective in base model   while stage-2 reranking is more effective in large   model . This is reasonable since stage-2 reranking   relies on the effectiveness of reader encoder mod-   ule , where the large model is usually better than   the base model .   Passage Ranking Results : We additionally   show that our proposed GNN reranking method   can improve the passage retrieval results . This is   demonstrated in Figure 2 , where we report Hits@K   metric over NQ test set , measuring the percentage   of top - K retrieved passages that contain the gold   passages ( passages that contain the answer ) . We   see that DPR+stage-1 reranking consistently out-   performs DPR for all the K∈ { 10,20,50,100 } .   With two stages of reranking , the retrieval results   are further improved for K∈ { 10,20}(We only   cares about K≤20for stage-2 reranking since   N= 20 ) . This shows that such reranking can   increase the rank of gold passages which are previ-   ously ranked lower by DPR retriever and improve   the efficacy of passage pruning .   5 Conclusion   This work tackles the task of Open - Domain Ques-   tion Answering . We focus on the current best   performed framework FiD and propose a novel   KG - based reranking method to enhance the cross-   modeling between passages and improve compu-   tation efficiency . Our two - stage reranking meth-   ods reuses the passage representation generated   by DPR retriver and the reader encoder and ap-   ply graph neural networks to compute reranking   scores . We further propose to use the intermedi-   ate layer of encoder to reduce computation cost   while still maintaining good performance . Exper-   iments on Natural Questions and TriviaQA show   that our model can significantly improve original   FiD by 1.5%exact match score and achieve on - par   performance with FiD but reducing over 60 % of   computation cost.49686 Acknowledgements   We thank all the reviewers for their valuable com-   ments . We also thank Woojeong Jin , Dong - Ho   Lee , and Aaron Chan for useful discussions . Dong-   han Yu and Yiming Yang are supported in part by   the United States Department of Energy via the   Brookhaven National Laboratory under Contract   No . 384608 .   References49694970   A Appendix   A.1 Dataset   The datasets we use are Natural Questions ( NQ )   and TriviaQA . The open - domain version of NQ is   obtained by discarding answers with more than 5   tokens . For TriviaQA , its unfiltered version is usedfor ODQA . We also convert all letters of answers   in lowercase except the first letter of each word on   TriviaQA . When training on NQ , we sample the an-   swer target among the given list of answers , while   for TriviaQA , we use the unique human - generated   answer as generation target . For both datasets , we   use the original validation data as test data , and   keep 10 % of the training set for validation .   A.2 Preliminary Analysis   We conduct preliminary analysis on the graph con-   structed among passages . Note that for each ques-   tion , we first apply the retriever to retrieve a few   candidate passages , then build edge connection   only among the retrieved passages , which means   that the passage graph is question - specific . Since   the passage graph depends on the retrieved pas-   sages , before further utilizing the graph , we need   avoid two trivia situations : ( 1 ) all the retrieved   passages come from the same article ; ( 2 ) The num-   ber of graph edges is very small . Thus we con-   duct statistics of the passage graphs on two ODQA   benchmark datasets , which is shown in Figure 3 .   For each question , the number of retrieved passages   is 100 . We see that the two trivia situations only   happen for a small portion of questions .   A.3 Training Process   For training our framework , we adopt the separate-   training strategy to avoid out - of - memory issue : we   first train the DPR model following its original   paper , then freeze the DPR model to train the stage-   1 reranking module , and finally jointly train stage-2   reranking and reader part . For the training of stage-   1 reranking , the optimizer is AdamW ( Loshchilov   and Hutter , 2019 ) with learning rate as 1e-3 and   linear - decay scheduler . The weight decay rate is   0.01 . Batch size is set as 64 . The number of total   training steps is 15k , and the model is evaluated   every 500 steps and the model with best validation   results is saved as the final model . For the training   of reading part , we adopt the same training setting   except that the learning rate is 1e-4 for the base   model and 5e-5 for the large model . We also adopt   learning rate warm up with 1000 steps .   A.4 Additional Experiment Results   We show additional experiment results in this sec-   tion , which includes the efficiency and performance   comparison between FiD ( base ) and KG - FiD ( base )   shown in Table 4 , and hyper - parameter search re-   sults listed below:4971   Model # FLOPsNQ TriviaQA   EM Latency ( s ) EM Latency ( s )   FiD ( N=40 ) 0.40x 47.2 0.27 ( 0.47x ) 64.1 0.27 ( 0.46x )   FiD ( N=100 ) 1.00x 48.8 0.58 ( 1.00x ) 66.2 0.59 ( 1.00x )   KG - FiD ( N=100 , L=3 ) 0.38x 48.4 0.27 ( 0.47x ) 65.6 0.26 ( 0.44x )   KG - FiD ( N=100 , L=6 ) 0.56x 49.0 0.35 ( 0.60x ) 66.1 0.34 ( 0.58x )   KG - FiD ( N=100 , L=9 ) 0.73x 49.3 0.43 ( 0.74x ) 66.3 0.43 ( 0.73x )   KG - FiD ( N=100 , L=12 ) 0.91x 49.6 0.50 ( 0.86x ) 66.7 0.49 ( 0.83x)4972Model H@1 H@5 H@10 H@20   GCN 49.1 69.7 75.7 79.9   GAT 50.1 70.1 76.1 80.2   # Layers   1 49.0 69.7 75.8 79.8   2 49.6 70.0 76.0 80.2   3 50.1 70.1 76.1 80.2   4 49.5 69.9 76.1 80.1   GNN Model Design : We conduct tuning on   the model type and number of layers of our GNN   based reranking model . For efficiency , we rerank   100 passages returned by DPR retriever and search   them based on the passage retrieval results . Table 5   shows the Hits scores for different choices . We see   that GAT outperforms vanilla GCN model ( Kipf   and Welling , 2017 ) which is reasonable since GAT   leverage attention to reweight neighbor passages by   their embeddings . The best choice for the number   of GNN layers is 3 . Note that other GNN models   such as GIN ( Xu et al . , 2019 ) , DGI ( Velickovic   et al . , 2019 ) can also be applied here and we leave   the further exploration of GNN models as future   work .   Nandλ . For the stage-2 reranking part in Sec-   tion 3.3 , we also conduct hyper - parameter search   on the number of passages after filtering : N∈   { 10,20,30}and the weight of reranking loss when   training the reading module : λ∈ { 0.01,0.1,1.0 } .   As shown in Table 6 , N= 20 achieves better re-   sults than N= 10 , but further increasing Ndoes   not bring performance gain while decreasing the   efficiency of model since the number of passages   to be processed by the decoder is increased . Thus   we choose N= 20 . For the loss weight λ , we   found that with its increment , the performance first   increases then significantly drops . This shows that   it ’s important to balance the weight of two training   losses , as we want the model to learn better pas-   sage reranking while not overwhelming the training   signal of answer generation .   A.5 FLOPs Computation   In this section we compute the FLOPs of each mod-   ule . The results are shown in Table 7 and 8 forModel N=10 N=20 N=30   KG - FiD 47.6 48.0 48.0   λ=0.01 λ=0.1 λ=1.0   KG - FiD 47.7 48.0 46.6   base model and large model respectively . Before   the computation , we first show some basic statistics   on two benchmark datasets : the average question   length is 20 , and the average answer length is 5 .   For the reading part , the length of concatenated pas-   sage question pair is 250 , number of input passages   isN= 100 .   We first calculate the number of FLOPs of   vanilla FiD model . For the retrieving part , it con-   tains both question encoding and passage similarity   search . We only consider the former part as the   latter part depends on the corpus size and search   methods and is usually very efficient . The question   encoding flops by BERT - based model is about 4.4   Gigaflops ( GFLOPs ) . For the reading part , the en-   coding of each question passage pair takes about   57/174 GFLOPs for base / large model , and the en-   coding of 100 passages takes 5772/17483 GFLOPs .   The decoder part only costs 714.2/2534.5 GFLOPs   for base / large model since the average length of   answer is very small . In summary , vanilla FiD   base / large model costs 6491.0/20022.0 GFLOPs .   For our model , the computation cost of retriev-   ing part is the same as vanilla FiD. Since we set   N= 1000 andN= 100 , the GAT ( Velickovic   et al . , 2018 ) computation in stage-1 reranking takes   about 3.5 GFLOPs , and the stage-2 reranking takes   only 0.4/0.6 GFLOPs for base / large model . For the   reader encoding part , the computation cost depends   onLandN , which is analyzed in Section 3.5 .   For the reader decoding part , where cross attention   takes most of the computation , KG - FiD only takes   about N / N= 1/5cost of vanilla FiD , which is   143.9/510.0 for base / large model respectively . The   detailed flops are shown in Table 7 and 8.4973Model RetrievingStage-1   RerankingReader   EncodingStage-2   RerankingReader   DecodingAll   FiD 4.4 - 5772.3 - 714.2 6491.0 ( 1.00x )   KG - FiD ( L=3 ) 4.4 3.5 2308.9 0.4 143.9 2461.1 ( 0.38x )   KG - FiD ( L=6 ) 4.4 3.5 3463.4 0.4 143.9 3615.5 ( 0.56x )   KG - FiD ( L=9 ) 4.4 3.5 4617.9 0.4 143.9 4770.0 ( 0.73x )   KG - FiD ( L=12 ) 4.4 3.5 5772.3 0.4 143.9 5924.5 ( 0.91x )   Model RetrievingStage-1   RerankingReader   EncodingStage-2   RerankingReader   DecodingAll   FiD 4.4 - 17483.2 - 2534.5 20022.0 ( 1.00x )   KG - FiD ( L=6 ) 4.4 3.5 6993.3 0.6 510.0 7511.8 ( 0.38x )   KG - FiD ( L=12 ) 4.4 3.5 10489.9 0.6 510.0 11008.4 ( 0.55x )   KG - FiD ( L=18 ) 4.4 3.5 13986.5 0.6 510.0 14505.1 ( 0.72x )   KG - FiD ( L=24 ) 4.4 3.5 17483.2 0.6 510.0 18001.7 ( 0.90x)4974