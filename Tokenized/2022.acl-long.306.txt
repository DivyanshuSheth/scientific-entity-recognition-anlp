  Johannes Kiesel   Bauhaus - Universität Weimar   johannes.kiesel@uni - weimar.deMilad Alshomary   Paderborn University   milad.alshomary@upb.de   Nicolas Handke   Universität Leipzig   nicolas.handke@gmx.deXiaoni Cai   Technische Universität München   caix@in.tum.de   Henning Wachsmuth   Paderborn University   henningw@upb.deBenno Stein   Bauhaus - Universität Weimar   benno.stein@uni-weimar.de   Abstract   This paper studies the ( often implicit ) human   values behind natural language arguments , such   as to have freedom of thought or to be broad-   minded . Values are commonly accepted an-   swers to why some option is desirable in the   ethical sense and are thus essential both in real-   world argumentation and theoretical argumen-   tation frameworks . However , their large va-   riety has been a major obstacle to modeling   them in argument mining . To overcome this   obstacle , we contribute an operationalization of   human values , namely a multi - level taxonomy   with 54 values that is in line with psychologi-   cal research . Moreover , we provide a dataset   of 5270 arguments from four geographical cul-   tures , manually annotated for human values .   First experiments with the automatic classiﬁ-   cation of human values are promising , with   F - scores up to 0.81 and 0.25 on average .   1 Introduction   How come people disagree on the best course for-   ward in controversial issues , even if they use the   same information to form their opinion ? A way   to get to the bottom of such disagreement is to   repeatedly ask them why they see something as   desirable . We observe that people have different   beliefs and priorities of what is generally worth   striving for ( e.g. , personal achievements vs. humil-   ity ) and how to do so ( e.g. , being self - directed vs.   respecting traditions ) , often referred to as ( human )   values ( Searle , 2003 ) . Some values tend to conﬂict   and others to align ( see Figure 1 ) , which can cause   disagreement on the best course forward , but also   the support , if not formation , of political parties   that promote the respective highly revered values .   Moreover , one can observe different value priori-   ties between cultures and disagreement thereon . Figure 1 : The levels of this paper ’s consolidated taxon-   omy of 54 values ( shown as black dots ) that are cate-   gorized on the more abstract levels 2–4 ( cf . Section 3 ) .   Categories that tend to conﬂict are placed on opposite   sites . Illustration adapted from ( Schwartz et al . , 2012 ) .   Due to their outlined importance , human values   are studied both in the social sciences ( Schwartz ,   1994 ) and in formal argumentation ( Bench - Capon ,   2003 ) for decades . According to the social sciences ,   a “ value is a ( 1 ) belief ( 2 ) pertaining to desirable   end states or modes of conduct , that ( 3 ) transcends   speciﬁc situations , ( 4 ) guides selection or evalu-   ation of behavior , people , and events , and ( 5 ) is   ordered by importance relative to other values to   form a system of value priorities . ” As Schwartz   continues , these features “ make it possible to con-   clude that security and independence are values,4459whereas thirst and a preference for blue ties are   not . ” Consider the following example :   “ Social media is good for us . Though it might make   people less polite , it makes our lives much easier . ”   To understand the pragmatics of this argument , a   reader has to acknowledge the belief ( Point 1 in the   deﬁnition above ) that the “ end state ” ( 2 ) of having   a comfortable life is desirable in general ( 3 ) . To   concur with the statement ( 4 ) , the reader further   has to prefer having a comfortable life over being   polite ( 5)—ignoring other arguments on the topic   for the sake of the example . Within computational   linguistics , human values thus provide the context   to categorize , compare , and evaluate argumenta-   tive statements , creating several possibilities : to   inform social science research on values through   large - scale datasets ; to assess argumentation with   respect to scope and strength ; to generate or select   arguments based on the value system of a target au-   dience ; and to identify opposing and shared values   on both sides of a controversial topic .   However , the task to identify values in arguments   seems daunting due to their large number , often im-   plicit use in arguments , and vague deﬁnitions . On   the other hand , the creation of larger argumenta-   tion datasets , advancements in natural language   understanding , and the decade - long rigorous tax-   onomization of values by social scientists has put   such an automatic identiﬁcation within reach .   As a ﬁrst endeavor on the automatic identiﬁ-   cation of values in written arguments , this paper   makes three contributions : ( 1 ) a consolidated multi-   level taxonomy of 54 human values taken from four   authoritative cross - cultural social science studies   ( Section 3 ) ; ( 2 ) a dataset of 5270 arguments from   the US ( most arguments ) , Africa , China , and India ,   each of which manually annotated for all values   by three annotators , corresponding to about 850k   human judgments ( Section 4 ) ; and ( 3 ) ﬁrst clas-   siﬁcation results per taxonomy level , establishing   a baseline and revealing promising results both   within and across cultures ( Section 5 ) .   2 Background   Human values are of concern to most if not to all   social sciences ( Rokeach , 1973 ) and have also been   integrated into computational frameworks of argu-   mentation ( Bench - Capon , 2003 ) . In NLP , values   have been analyzed for personality proﬁling ( Ma-   heshwari et al . , 2017 ) , but not yet for argument   mining , as considered here.2.1 Values in Social Science   Rokeach ( 1973 ) already described the two concepts   of ( 1 ) a value as a belief pertaining to desirable end   states or modes of conduct and ( 2 ) a value system   as prioritization of values based on cultural , social ,   and personal factors . These deﬁnitions attribute   values to persons rather than to objects , facilitating   a systematic analysis ( Rokeach , 1973 ) . The paper   at hand follows these deﬁnitions and targets the per-   sonal values behind arguments , that is , the values   that the arguments , mostly implicitly , resort to .   Several proposed value schemes are domain-   independent and hence suited to analyze generic   argumentation . Our consolidated value taxonomy   ( Section 3 ) is thus based on these schemes . Com-   bining research from anthropology , sociology , phi-   losophy , and psychology , Rokeach ( 1973 ) esti-   mates the total number of human values to be fewer   than hundreds , and develops a practical survey of   36 values that distinguishes between values pertain-   ing to desirable end states and desirable behavior .   Speciﬁcally for cross - cultural analysis , Schwartz   et al . ( 2012 ) derived 48 value questions from the   universal needs of individuals and societies , in-   cluding obeying all the laws andto be humble .   Moreover , Schwartz ( 1994 ) proposes a relatedness   of values by their tendency to be compatible in   their pursuit ( see Figure 1 ) . This relatedness re-   ﬂects two “ higher order ” conﬂicts : ( 1 ) openness to   change / own thoughts vs. conservation / submission ,   and ( 2 ) self - transcension ( directed towards oth-   ers / the environment ) vs. self - enhancing ( directed   towards one ’s self ) , allowing to analyse values at   several levels . Cheng and Fleischmann ( 2010 ) con-   solidates 12 schemes into a “ meta - inventory ” with   16 values , such as honesty andjustice , revealing a   large overlap in schemes across ﬁelds of research .   However , as the meta - inventory is strictly more   coarse - grained than Schwartz et al . ’s theory we do   not investigate it further in this paper .   Other schemes , however , pertain to speciﬁc pur-   poses , making them less suited for our study . We   give an overview for completeness . England ( 1967 )   suggested 66 values related to management deci-   sions , such as high productivity andprestige , and   categorized them by relevant entity , for example   business organizations andindividuals . Brown and   Crace ( 2002 ) looked at 14 values for counseling   and therapy , such as responsibility andspirituality ,   and Kahle et al . ( 1988 ) at nine for consumer re-   search , such as warm relationships andexcitement .44602.2 Values in Argumentation Research   Formal argumentation employs value systems to   model audience - speciﬁc preferences , that is , an ar-   gument ’s strength depends on the degree to which   the audience reveres the values the argument resorts   to . Examples include value - based argumentation   schemes ( van der Weide et al . , 2009 ) , defeasible   logic programming ( Teze et al . , 2019 ) , and the   value - based argumentation framework of Bench-   Capon ( 2003 ) . The latter is an extension of the   abstract argumentation framework of Dung ( 1995 )   that has already been applied manually to analyze   interactions with reasoning and persuasion subject   to a speciﬁc value system ( Atkinson and Bench-   Capon , 2021 ) . This paper presents a ﬁrst step to-   wards the large - scale automatic application of these   works as it takes values to argument mining .   Feldman ( 2021 ) recently showed the strong con-   nection between values and the moral foundation   theory ( Haidt , 2012 ) . Like personal values , this   theory analyzes ethical reasoning behind human   choices , but considers ﬁve rather abstract “ founda-   tions : ” care , fairness , loyalty , authority , and purity .   Alshomary and Wachsmuth ( 2021 ) hypothesized   that the foundations could be used for audience-   speciﬁc argument generation . Kobbe et al . ( 2020 )   tried to classify arguments by foundations , but   noted a low human agreement due to the vague-   ness of the foundations . We assume values can   here contribute to the classiﬁcation by foundations .   Values overlap with idea of framing in commu-   nication , that is , the selection and emphasis of spe-   ciﬁc aspects of ( perceived ) reality to promote a par-   ticular problem , causal interpretation , ethical eval-   uation , and/or recommendation ( Entman , 1993 ) . In   frames , values can deﬁne the costs and beneﬁts   of options ( Entman , 1993 ) , while common value   systems are used for evaluation . Framing has of-   ten been studied computationally for news ( Naderi   and Hirst , 2015 ; Chen et al . , 2021 ) , but also for   political speech ( De Vreese , 2005 ) , and argumenta-   tion ( Ajjour et al . , 2019 ) . In the latter , some values   are so prevalent that they constitute frames of their   own , indicating a potential use of values in frame   identiﬁcation . For example , 14 out of 54 values we   use are also frames in the dataset of Ajjour et al .   Values may be considered as aspects under   which to group arguments . Some researchers have   mined aspects from text ( Trautmann , 2020 ) or used   them to control argument generation ( Schiller et al . ,2021 ) . Others have studied the task of opinion sum-   marization in arguments ( Egan et al . , 2016 ; Misra   et al . , 2016 ; Chen et al . , 2019 ) , aiming at the most   important aspects discussed in a debate . Related ,   the task of key point analysis ( Bar - Haim et al . ,   2020 ; Friedman et al . , 2021 ) is to generate a small   set of concise statements that each represent a dif-   ferent aspect . We argue that analyzing the values   found in a collection of arguments provides a new   perspective to aspects in argumentation , focusing   on the “ why ” behind an argument ’s reasoning .   3 Taking Values to Argument Mining   Human values have been considered in formal ar-   gumentation since about 20 years ( Bench - Capon ,   2003 ) . However , to the best of our knowledge , our   paper is the ﬁrst that aims at identifying the values   behind arguments computationally . The term “ be-   hind ” reﬂects the fact that many arguments do not   explicate values ; for example , in the argument “ no   matter they felt forced to commit it : anyone who   commits a crime should be prosecuted ” no value   is mentioned literally . The argument gains its per-   suasive strength when being connected to values ,   which can be both desirable behavior ( behaving   properly ) or end states ( a safe country ) . By putting   forward an argument , its proponent wants the audi-   ence to connect the argument with its values . For-   mally , values are connected speciﬁcally with the   argument ’s premise . However , automatic models   might still improve when incorporating the textual   conclusion as context for the textual premise . The   task studied in this paper is to draw this connection   between arguments and values automatically .   The heart of a value - based argumentation frame-   work is a value taxonomy ( or a set of values ) that is   both accepted and relevant . The research presented   in this paper is largely based on the reﬁned theory   of Schwartz et al . ( 2012),which , however , has   been extended by us : Comparing Schwartz et al . ’s   reﬁned theory with three other widespread value   lists against a sample of our dataset , we decided   to add and integrate nine values ( see Table 1 ) . We   also asked the annotators to comment on suppos-   edly missing values ( see Section 4 ) . For most of the   additional 48 value descriptions that we received   ( be humane , be fair , be modern , etc . ) , we identiﬁed   existing values or value combinations in the taxon-   omy that subsume them , suggesting to extend the   value descriptions rather than adding new values.44614462Only two of the added values are not directly   related to the universal needs that Schwartz ( 1994 )   based the value categories on . The proposed cate-   gory universalism : objectivity integrates well be-   tween the outward thinking of universalism : toler-   ance and the free thinking of self - direction : thought   ( see Figure 1 ) . We adopt a uniform naming scheme   where the value names reﬂect the distinction of   Rokeach ( 1973 ) into instrumental ( be . . . ) and ter-   minal ( have . . . ) values , and are easy to embed in   sentences , for example , “ it is good to be creative . ”   The taxonomy levels are chosen based on use-   fulness in social science research . The values at   Level 1 are intended to be the items in surveys   ( Schwartz , 1994 ) , which is why we also suggest to   use them for dataset annotation . Moreover , Level 1   values can still be classiﬁed into being either in-   strumental or terminal . One could , however , create   arbitrarily coarse- and ﬁne - grained levels .   The close connection of our taxonomy to social   science research enables studies of value systems   across disciplines that are beyond the scope of this   paper . The grouping of values at higher levels al-   lows for classiﬁcations at coarser levels of granu-   larity , enabling investigations such as , whether a   speciﬁc set of arguments focus on persons or soci-   ety mainly , or whether they imply a rather anxiety-   free or a rather anxiety - avoiding background ( cf .   Figure 1 ) . Also , the circular organization of the tax-   onomy enables the analysis of major “ directions ” in   a collection of arguments , which can , for example ,   be used to study value differences in argumentation   datasets of different cultures . In addition , for the   41 values with a link to the World Values Survey   ( the WVS column in Table 1 , Haerpfer et al . , 2020 ) ,   the corresponding dataset contains information on   people ’s value priorities ( i.e. , value systems ) col-   lected rigorously for 51 territories , with the earliest   survey from 1981 and the latest from 2020 . These   links allow comparing value distributions identiﬁed   in regional datasets with survey data .   4 A Dataset of Values behind Arguments   This section presents the ﬁrst dataset for study-   ing human values behind arguments . Each of the   5270 arguments included was annotated by three   crowdworkers for all 54 values from Section 3 . The   dataset , taxonomy description , and annotation inter-   face are available online as Webis - ArgValues-22.4.1 Argument Sources of Different Cultures   Following the aspiration of a cross - cultural value   taxonomy and using territories as a proxy for cul-   tures , the dataset is composed of four parts : Africa ,   China , India , and USA . Each argument consists   of one premise , one conclusion , and a stance at-   tribute indicating whether the premise is in favor of   ( pro ) or against ( con ) the conclusion . As existing   argument datasets are almost exclusively from a   Western background , we had to collect new suit-   able arguments for the non - US parts , drastically   limiting their size . The respective non - US sources   were recommended to us for their authenticity by   students from the respective territory that work with   our groups . Note that this data is not intended to   represent the respective culture , but to train and   benchmark classiﬁers across sources .   Africa We manually extracted 50 arguments   from recent editorials of the debating ideas sec-   tion of a pan - African news platform , African Argu-   ments .Premises could often be extracted literally ,   but conclusions were mostly implicit and had to be   compiled from several source sentences .   China We extracted 100 arguments from the   recommendation and hotlist section of a Chinese   question - answering website , Zhihu .We manually   identiﬁed key points ( premises and conclusions ) in   the answers and manually translated them to En-   glish using automated translation for a ﬁrst draft .   India We extracted 100 arguments from the con-   troversial debate topics 2021 section of Group Dis-   cussion Ideas .This blog collects pros and cons on   various topics from Indian news to support discus-   sions . Premises and conclusions were used as - is .   USA We took 5020 arguments with a manual   argument quality rating of at least 0.5 from the   30,497 arguments of the IBM - ArgQ - Rank-30kArgs   dataset ( Gretz et al . , 2020 ) . For the dataset , crowd-   workers wrote one pro and one con argument   for one of 71 common controversial topics . We   rephrased the topics to represent conclusions .   Due to the difﬁculty of collecting datasets from   various cultures , the number of respective argu-   ments ( 250 ) is small compared to the US part .   However , we will mainly use them for testing the   robustness of identifying values in arguments.4463   Table 2 shows one example from each part . Note   that we do not see any part as representative for   the respective culture , but rather as a necessary   approximation ( see Section 7 for a discussion ) . Ta-   ble 3 provides an overview of the dataset . Premises   are longer than conclusions , with USA having the   lowest average for both . The Africa part has the   fewest premises per conclusion ( 2.2 ) and the US   part the most ( 70.7 ) . The skew between pros and   cons is highest for Africa with a ratio of about 3:1 .   All these observations are results of the collection   process and are natural variations for arguments .   4.2 Crowdsourcing of Value Annotations   We employed a custom three - part annotation in-   terface , optimized for speed and task expertise ac-   quisition through keyboard shortcuts and a clear   template - like structure ( see Appendix A for screen-   shots ) . Besides instructions and example argu-   ments , a brief explanation of speciﬁc terms was   given if needed ( e.g. , for the “ 996 overtime system ”   mentioned in several arguments from China ) . Be-   low this introductory material , the main part of the   interface consists of three panels . The ﬁrst panelplaces the argument to be annotated in a scenario :   Imagine someone is arguing [ in favor of / against ]   “ [ conclusion ] ” by saying : “ [ premise ] . ”   The second panel formulated the annotation task   for a value as a yes / no question . The question   follows the operationalization of Section 3 :   If asked “ Why is that good ? ” , might this be their   justiﬁcation ? “ Because it is good to [ value ] ” .   For illustration , example implications of matching   arguments were provided . Instructions stated that   one to ﬁve values are typical for an argument , and   more than 10 should be avoided . A third panel   shows the annotation progress . Annotators could   write feedback on both arguments and values .   The crowdsourcing ran on the MTurk platform ,   with annotators taking 2:40 minutes per argument   on average , and totaling 90 days of 8 - hour work .   We required them to have an approval rate of at   least 98 % , at least 100 approved work tasks , and —   for language proﬁciency — being located in the US .   No further personal information was gathered . The   annotators were ﬁrst restricted to three annotation   tasks . Manual quality checks at this stage resulted   in 154 work rejections ( 5 % rejection rate ) due to   ignored instructions . We then selected 27 annota-   tors for annotating the bulk of arguments , ensuring   at least 3 annotations per argument . As mandatory   for MTurk , annotators were paid on a task basis ,   which led to an average hourly wage of $ 8.12 ( cur-   rent US federal minimum wage : $ 7.25 ) . Addition-   ally , we paid bonuses of total $ 65.65 , especially to   annotators who wrote extensive comments.4464   We employed MACE ( Hovy et al . , 2013 ) to fuse   the annotations into a single ground truth , apply-   ing it value - wise as suggested by the author for   multi - label annotations . Despite the difﬁculty of   the annotation task , the crowdworker annotators   reached an average value - wise agreement  of 0.49   ( Krippendorff , 2004 ) . We found most disagreement   arose from the complexity of annotating 54 values   at once , with annotators sometimes confusing val-   ues despite the descriptions . For follow - up datasets ,   one could likely reduce such problems by training   annotators on the arguments of our dataset with   highest disagreement . One step we implemented   for quality assurance is that we manually checked   the 48 arguments ( < 1 % ) to which MACE assigned   more than 10 values , reducing their values to the   most prevalent 5–7 ones . The right side of Table 1   shows the frequency of each value in each dataset   part , revealing that each value occurs at least once .   A value in the ground truth also automatically   led to an assignment of all parent labels in the tax-   onomy ( see Figure 1 ) . Figure 2 shows the resulting   level - wise distribution of labels per argument . As   the majority of arguments are assigned both labels   for Levels 4a and b , these base dichotomies for   values are hence mostly not dichotomous for argu-   ments . So , like the value systems of people , many   arguments seem to resort to a broad spectrum of val-   ues from the value continuum at once . For example ,   the ﬁrst argument in Table 2 resorts to both having   a comfortable life ( personal focus , self - protection )   andhaving equality ( social focus , growth ) . Sim-   ilar to observations of Rokeach ( 1973 , p. 50f ) on   value systems , this example showcases an interac-   tion between values that change their psychological   signiﬁcance , where having equality gives having a   comfortable life a social focus . We believe that our   dataset enables scholars to study such interactions   for arguments in the future.5 Identifying Values behind Arguments   This section presents a ﬁrst attempt at automati-   cally identifying human values using standard ap-   proaches . The ﬁrst experiment focuses on the   USA dataset part alone , the second on a cross-   cultural setting . We compare three approaches ,   for which we provide our implementation online :   BERT . Fine - tuned multi - label bert - base - uncased   with batch size 8 and learning rate 2(20 epochs ) .   SVM . A linear kernel scikit - learn support vector   machine trained label - wise with C= 18 .   1 - Baseline . Classiﬁes each argument as resorting   to all values . Thus always achieves a recall of 1 .   Our evaluation focuses on the label - wise F-   score and its mean over all labels ( macro - average ) ,   as well as its constituents precision and recall . We   report accuracy for completeness , though the heav-   ily skewed label distribution makes it less suited .   The evaluation employs macro - averages for all met-   rics to give the same weight to all values . Note that   the 1 - Baseline is especially strong for the F - score   since it always achieves a recall of 1 . By deﬁnition   this baseline achieves at least as high — and in most   cases higher — F - scores than label - wise random   guessing according to the label frequency . For cal-   culating the p - values when comparing approaches   we employ the Wilcoxon signed rank signiﬁcance   test ( Wilcox , 1996 ) . As detailed in Section 4 , most   arguments actually have both labels of the base di-   chotomies ( Levels 4a and b ) assigned to them , so   we do not discuss these levels deeper here .   5.1 Results on the USA Part   We ﬁrst report results on the main part of our   dataset ( USA ) as an experiment with matching   training and test set . The approaches are trained on   the arguments from 60 unique conclusions ( 4240 ar-   guments , ~85 % ) , validated on 4 ( 277 , ~5 % ) , and   tested on 7 ( 503 , ~10 % ) . The conclusions were   selected so that the different sets contain roughly   the speciﬁed percentage of arguments . Unfortu-   nately , this process led to different value distribu-   tions in the different sets . However , we deemed   the conclusion - wise split more important for our   experiments , as we want to test whether classiﬁers   generalize to unseen conclusions . Only one very   rare value , be neat and tidy ( 0.2 % of arguments in   USA part ) , does not occur in the test set . We thus   exclude this value from evaluation.4465   Table 4 shows the results averaged over all la-   bels . BERT performs best according to F - score   for Level 1 ( p= 0:007vs . SVM and p= 0:001vs .   1 - Baseline ; n= 53 ) and for Level 2 ( p= 0:153   andp= 0:117;n= 20 ) , but is worse than or at   the baseline for higher levels ( ntoo small for test ) .   The comparably bad performance at higher lev-   els is somewhat surprising , as it indicates that the   categories at these higher levels are harder to sepa-   rate by state - of - the - art language - based approaches .   Maybe hierarchical classiﬁcation approaches ( e.g. ,   Babbar et al . , 2013 ) can address this comparably   weak performance by utilizing signals at each level   of the hierarchy simultaneously . Moreover , while   a F - score of 0.25 at Level 1 is encouraging for   largely out - of - the - box approaches , clearly more   work is needed . Though a recall of 0.19 may be   acceptable for applications that not rely on com-   pleteness , a precision of 0.40 is clearly too low for   practical uses . As Figure 3 shows , however , considerably higher   F - scores are reached by BERT for several values   and value categories . Speciﬁcally , the identiﬁcation   works exceptionally well for the value have good   health ( F : 0.81 ) and the value - category security :   personal ( F : 0.78 ) that contains it . Other value cat-   egories with F0:5areuniversalism : concern ,   self - direction : action , achievement , and benevo-   lence : caring . The out - of - the - box models thus per-   form reasonably well for a few selected values and   categories within the USA part . Moreover , Figure 3   indicates some correlation of value frequency ( grey   bars ) with classiﬁer performance ( colored lines ) .   One reason for this correlation could be that the   dataset is too small for training reliable classiﬁers   on the infrequent values . Another reason might be   that there is a more developed vocabulary concern-   ing frequent values , making it easier for classiﬁers   to identify these values . The results are distributed   alongside the dataset for follow - up analyses.4466   5.2 Results Across Culture   For testing classiﬁcation robustness , we here apply   the same approaches without re - training to all test   sets . The non - US parts are considerably smaller   and as a result ~28 % of the values are lacking ar-   guments ( cf . Table 1 ) . However , the 1 - Baseline is   equally affected by this lack , thus providing for a   comparison with the previous setting .   Table 5 shows the F - scores for each test set   averaged over all labels . Once more , BERT per-   formed best by the F - score for Level 1 ( p= 0:006   vs. SVM and p < 0:001vs . 1 - Baseline ; n= 169 )   and Level 2 ( both p < 0:001;n= 74 ) , whereas   no signiﬁcant difference was found for Level 3   ( p= 0:179 andp= 0:856;n= 16 ) . BERT   and SVM perform on Level 1 and 2 similar across   parts . Maybe due to the clarity of its editored argu-   ments , BERT performs best for India , despite the   1 - Baseline performing best for USA .   These ﬁndings constitute ﬁrst evidence that us-   ing a cross - cultural value taxonomy could result in   robust methods for identifying the values behind   arguments , even though more data and research   seem necessary to get there .   6 Conclusion   A computational identiﬁcation of human values be-   hind arguments is a challenging but also necessary   task . With our research we contribute ( 1 ) a multi-   level taxonomy with 54 values based on social sci-   ence research , ( 2 ) a labeled dataset comprised of   5270 arguments from four sources , and ( 3 ) empiri-   cal analyses that cover multiple value granularity   levels and compare different cultures .   Based on this work a logical next step are anal-   yses that fully exploit relationships between la-   bels . Hierarchical classiﬁcation approaches ap-   pear promising here ( e.g. , Babbar et al . , 2013 ) ;   learning rules for multi - label classiﬁcation ( e.g. ,   Loza Mencía and Jannsen , 2016 ) can provide in-   sights into value - relationships .   Moreover , the dataset should be extended to in - clude data from more cultures or territories , genres   ( e.g. , blog posts ) , modalities ( ofﬂine and spoken   argumentation ) , and languages . Probably an auto-   mated translation with manual assurance , as we did   for the dataset ’s China part , may not be sufﬁcient .   Though we optimized the annotation process , the   argument acquisition requires a community effort   to ensure the widest variety of data . Employing   annotators from different cultures is a requirement   to analyze and mitigate potential sources of bias . A   subsequent step of ranking the annotated values by   importance can be beneﬁcial for certain use cases ,   especially when using the higher taxonomy levels .   Values are a major contributor to argument   strength ( Bench - Capon , 2021 ) , and the large - scale   mining from web data could improve all of argu-   ment categorization , assessment , and generation .   For example , matching values between arguments   could be effective for both supporting and coun-   tering arguments . Clearly expressing values be-   hind arguments could avoid misunderstandings be-   tween humans and automated argumentation sys-   tems ( Kiesel et al . , 2021 ) . Similarly , an “ objective ”   highlighting of common values behind arguments   across political camps could be a step towards re-   solving seemingly fundamental disagreements .   Finally , the analysis of values in large - scale text   corpora can also be of interest of social science   scholars . How are values expressed online ? Com-   bined with Internet archive data , one could even   analyse references to values over time . We thus   hope that this work can serve as a ﬁrst step towards   a better understanding of how the public sees and   saw human values in everyday ( digital ) life .   7 Ethics Statement   Identifying values in argumentative texts could   be used in various applications like argument   faceted search , value - based argument generation ,   and value - based personality proﬁling . In all these   applications , an analysis of values has the oppor-   tunity to broaden the discussion ( e.g. , by present-4467ing a diverse set of arguments covering a wide   spectrum of personal values in search or inviting   people with underrepresented value - systems to dis-   cussions ) . At the same time , a value - based analysis   could risk to exclude people or arguments based on   their values . However , in other cases , for example   hate speech , such an exclusion might be desirable .   While we tried to include texts from different   cultures in our dataset , it is important to note that   these samples are not representative of their re-   spective culture , but intended as a benchmark for   measuring classiﬁcation robustness across sources .   A more signiﬁcant community effort is needed to   collect more solid datasets from a wider variety of   sources . To facilitate the inclusivity of different   cultures , we adopted a personal value taxonomy   that has been developed targeting universalism and   tested across cultures . However , in our study , the   annotations have all been carried out by annotators   from a western background . Even though the value   taxonomy strives for universalism , a potential risk   is that an annotator from a speciﬁc culture might   fail to correctly interpret the implied values in a   text written by people from a different culture .   Finally , as mentioned in Section 4 , we did not   gather any personal information in our annotation   studies , and we ensured that all our annotators get   paid more than the minimum wage in the U.S.   References4468   A Annotation Interface   Figure 4 and 5 show screenshots of the custom an-   notation interface . Its source code is distributed as   part of the dataset at https://github.com/   webis - de / ACL-22 .446944704471