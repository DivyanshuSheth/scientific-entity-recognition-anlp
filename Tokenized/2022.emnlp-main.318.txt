  Huanran Zheng , Wei Zhu , Pengfei Wang , Xiaoling WangEast China Normal University , Shanghai , China   { hrzheng,wzhu,pfwang}@stu.ecnu.edu.cn   { xlwang}@cs.ecnu.edu.cn   Abstract   Non - autoregressive translation ( NAT ) model   achieves a much faster inference speed than   the autoregressive translation ( AT ) model be-   cause it can simultaneously predict all tokens   during inference . However , its translation qual-   ity suffers from degradation compared to AT .   And existing NAT methods only focus on im-   proving the NAT model ’s performance but do   not fully utilize it . In this paper , we propose   a simple but effective method called “ Candi-   date Soups , ” which can obtain high - quality   translations while maintaining the inference   speed of NAT models . Unlike previous ap-   proaches that pick the individual result and dis-   card the remainders , Candidate Soups ( CDS )   can fully use the valuable information in the   different candidate translations through model   uncertainty . Extensive experiments on two   benchmarks ( WMT’14 EN – DE and WMT’16   EN – RO ) demonstrate the effectiveness and gen-   erality of our proposed method , which can sig-   nificantly improve the translation quality of var-   ious base models . More notably , our best vari-   ant outperforms the AT model on three transla-   tion tasks with 7.6×speedup .   1 Introduction   Autoregressive translation ( AT ) models based on   Transformer ( Vaswani et al . , 2017 ; So et al . , 2019 ;   Sun et al . , 2022 ; Zhu et al . , 2021a ) , where each gen-   eration step depends on the previously generated to-   kens , achieve state - of - the - art ( SOTA ) performance   on most datasets for machine translation tasks . AT   model can better model the process of translation   generation but leads to a massive limitation of its   inference speed .   Therefore , the non - autoregressive translation   ( NAT ) ( Gu et al . , 2018 ) model is proposed , which is   15.6 times faster than AT model . NAT assumes that   the generated tokens are conditionally independentFigure 1 : Efficiency ( Speedup ) and Translation quality   ( BLEU ) of NAT models in the WMT’14 EN - DE trans-   lation dataset . A cross “ × ” represents our Candidate   Soups ( CDS ) variants . Its base model is shown in the   shape “ • ” , and its correspondence is represented by an   arrow . CDS ( mE - nD ) refers to the AT model for re-   scoring that has m encoder layers and n decoder layers .   given the source sentence , so the translation can be   generated in parallel , significantly improving its in-   ference speed compared to AT . However , due to the   strong independence assumption , the ability of the   NAT model modeling sequence generation is weak-   ened . So NAT model usually has multimodality   problem ( Gu et al . , 2018 ) in the inference process ,   resulting in its performance worse than AT models .   Several methods have been proposed to alleviate   the multimodality problem and improve the perfor-   mance of the NAT model , such as the iteration-   based NAT model ( Ghazvininejad et al . , 2019 ;   Gu et al . , 2019 ; Kasai et al . , 2020 ) and the semi-   autoregressive translation model ( Wang et al . , 2018 ;   Ran et al . , 2020 ) .   Most of the previous methods are modified from   the model ’s perspective , either modifying the struc-   ture of the model ( Shu et al . , 2020 ; Huang et al . ,   2021 ; Zhu et al . , 2021b ) or modifying the training   method of the model ( Du et al . , 2021 ; Qian et al . ,   2021 ) . Different from the previous methods , in this   paper , we propose a simple but effective method:4811Src Die Beschaffung des erforderlichen Personalausweises kostet oft über hundert Dollar .   Candidate 1 It often costs over a hundred dollars to obtain the require identity card .   Candidate 2 It often cost over a hundred dollars to obtain the required identity card .   NPD It often cost over a hundred dollars to obtain the required identity card .   CDS It often costs over a hundred dollars to obtain the required identity card .   Candidate Soups , which can significantly improve   the translation quality without any modification to   the model . Moreover , Candidate Soups is a general   approach that can be used by any NAT model that   can generate multiple candidate results , such as   Vanilla NAT ( Gu et al . , 2018 ) , GLAT ( Qian et al . ,   2021 ) , etc .   The conventional recipe for maximizing trans-   lation quality through candidate results is noisy   parallel decoding ( NPD ) ( Gu et al . , 2018 ) , which   regards each candidate translation as an indepen-   dent individual and ultimately only selects one of   them as the final result and discards others . There-   fore NPD can not utilize the valuable information   in all the candidate translations . For example , there   are a total of two candidate translations . The first   candidate translation has the wrongly translated   word in the second half , and the second candidate   translation has the wrongly translated word in the   first half . Using the NPD algorithm , in this case ,   can not get the correct translations ( Figure 2 ) .   However , Candidate Soups will effectively use   the valuable information of all the candidate trans-   lations to fuse the different candidate results and ob-   tain a higher - quality translation ( Figure 2 ) . Specifi-   cally , Candidate Soups first finds the common sub-   sequence among all candidate results . Based on   the uncertainty of the model , we consider the com-   mon subsequence to be the most confident part   of the model ’s predictions , so we make it part of   the final translation and use it to align the candi-   date results . For the remaining parts , we select the   part with the highest average log - probability among   all candidate results to add to the final translation .   Candidate Soups can be regarded as an implicit   model ensemble method , which generates multiple   different results by introducing uncertainty in the   inference process , and further enhances the transla-   tion quality by making full use of the information   of multiple results .   We conduct extensive experiments in two   datasets commonly used in machine translation , WMT’14 EN – DE and WMT’16 EN – RO . The re-   sults demonstrate that our proposed method can   significantly improve the base models ’ translation   quality on different tasks while maintaining the fast   inference speed of the NAT model . Remarkably ,   our best variant achieves better performance than   the AT teacher model on three translation tasks with   7.6×speedup . Figure 1 demonstrates the quality-   speed trade - off compared with AT and recent NAT   models . And relevant background knowledge is   introduced in Appendix A.   2 Related Work   Since the NAT model was proposed , it has attracted   the attention of many researchers due to its superior   inference speed . However , its translation quality   suffers from degradation compared to AT model .   Therefore various methods have been proposed to   bridge the performance gap between NAT and AT   model .   Some researchers constrain the distribution of   NAT model outputs by introducing various latent   variables ( Gu et al . , 2018 ; Shu et al . , 2020 ; Ran   et al . , 2021 ) . Through latent variables , the diversity   of the NAT model output space can be significantly   reduced so that the model can better handle the de-   pendencies between output words and alleviate the   multimodality problem . Such methods can usually   maintain the efficient inference speed of the NAT   model , but the improvement in translation quality   is relatively small .   Some other researchers have proposed iterative   decoding methods ( Ghazvininejad et al . , 2019 ; Gu   et al . , 2019 ; Kasai et al . , 2020 ) , which continuously   optimize the model ’s output by introducing more   information in the iterative process . For example ,   Ghazvininejad et al . ( 2019 ) mask the partial to-   ken of the previous output result and then use it   as the input of the decoder for the next round of   iteration . Although such models can achieve high   performance , multiple iterations can also signifi-   cantly affect the inference speed of the NAT model.4812   Recently , Qian et al . ( 2021 ) borrowed ideas from   curriculum learning and proposed a novel way to   train NAT models , which let the model starts from   learning the generation of sequence fragments and   gradually moving to whole sequences . Huang et al .   ( 2021 ) proposes to predict the result at each de-   coder layer and input it to the next layer together   with the output of the current decoder layer to mod-   ify the result of the subsequent prediction .   The above methods are all improvements from   the model perspective , and their purpose is to allow   the model to generate higher quality translations .   Unlike previous work , Candidate Soups wants to   explore how to make the most of an existing model ,   and it can be applied to all NAT models that can   generate multiple candidate results .   3 Candidate Soups   This section describes the details of the proposed   method in the paper . We first show the problem def-   inition and the general idea of Candidate Soups in   Section 3.1 , then introduce implementation details   of the Candidate Soups in Section 3.2 , followed by   the example in Section 3.3 .   3.1 Problem Definition   By introducing uncertainty into the NAT model ,   we can get a list of candidate results R = Algorithm 1 : Candidate Soups   [ R , . . . , R ] , and each candidate result may have   correctly and incorrectly translated parts that do not   completely overlap . Thus , our goal is to find the   optimal combination in R , which has the highest   average log - probability re - scored by an AT model .   Because the word order in the original translations   must be kept , we first use Rto build a Lattice   ( Figure 3a ) . Each node represents a token , and   each edge represents the change of average log-   probability caused by adding the next token . So   each path from the beginning node [ BOS ] to the   end node [ EOS ] represents a possible translation .   Therefore , our goal is to find the best path which   has the highest average log - probability in this Lat-   tice .   However , because the initial Lattice contains too   many paths , we can not calculate the values of all   edges . Furthermore , due to the dislocation caused   by the different lengths of the candidate results ,   most of the paths in the initial Lattice have word   order errors , such as edges between the same to-   kens ( Figure 3a ) . So we simplified and aligned the   original Lattice . Specifically , after introducing the   length uncertainty , we consider tokens that appear   in all candidate results with the same word order as   certain components ( Gal and Ghahramani , 2016).4813   Thus , we find the common subsequence of all can-   didate results and directly use it as part of the final   translation ( Figure 3b ) . This way , the alignment   of candidate translations can be achieved , and the   original complex Lattice can be simplified into the   connection of multiple simple Lattices .   For the remaining simple Lattice , the cost of cal-   culating each edge value is still unbearable . There-   fore , we fuse the nodes belonging to the same can-   didate result in each Lattice into a single node ( Fig-   ure 3c ) and ignore the influence of previous Lattice   results on subsequent Lattice . In this way , we can   quickly calculate each edge value by using the AT   model to re - score each candidate translation . Then   we only need to calculate the best path in each sim-   ple Lattice and obtain the final translation through   a simple greedy algorithm .   3.2 Implementation   Algorithm 1 lists the process of Candidate Soups .   We generate the final translation while looking for   the common subsequence .   First , for the input candidate results set Rand   the corresponding log - probability score set S , we   will remove the adjacent repeated tokens and their   corresponding scores for each sentence . Then we   initialize a pointer set Ithat each pointer points to   position 0 for each candidate translations and usethese pointers to traverse simultaneously . If all the   current pointers point to the same token , the token   is added to the final translation , and all pointers are   moved one step to the right . Otherwise , Candidate   Soups will look for the next sequence of pointers   Ithat satisfies the above conditions and move all   pointers there . At the same time , the segment with   the highest average log - probability score among all   segments generated by the pointer traverse from I   toIis added to the final translation .   Experimental results show that Candidate Soups   can significantly improve final translation quality ,   requiring only 3 to 7 candidate translations . More-   over , the time required by Candidate Soups is al-   most negligible compared to the inference time of   the NAT model .   3.3 Example   Figure 4 shows how Candidate Soups fuses the   valuable information in each candidate translation   to generate high - quality translations during the   traversal process .   First , the NAT model predicts three candidate   translations for the input sentence by introducing   different lengths ( t = 0 ) . Afterward , through the   traversal of the pointers , Candidate Soups found   that the first two tokens ( “ The Republican ” ) in   the candidate results were the same , so they were4814added to the final translation ( t = 1 ) . When there   is a disagreement between candidate results , Can-   didate Soups will find the next token ( “ extend ” )   that all candidate translations predict in common   and get three different segments ( “ authorities were   quick , ” “ authorities were quick to , ” and “ and the   authority ” ) . Then Candidate Soups will select the   candidate segment with the highest average log-   probability scores ( “ authorities were quick to ” ) and   add it to the final translation ( t = 2 ) . Similarly , in   the subsequent traversal process , if the tokens are   predicted jointly by all candidate results , Candi-   date Soups will add them to the final translation ( t   = 3 , t = 5 ) . Otherwise , Candidate Soups will select   the tokens segment with the highest log - probability   score to join the final translation ( t = 4 ) . Ultimately ,   we get higher - quality translations by combining all   valuable information in the candidate results .   From this example , we can find that only se-   lecting an independent candidate result as the fi-   nal translation is not effective enough for the NAT   model . Because different lengths introduce uncer-   tainty into the NAT model , there is diversity among   candidate translations , but the previous methods do   not take advantage of this . Proudly , through the cer-   tainty and confidence of the NAT model ’s output ,   Candidate Soups makes full use of the candidate   results , takes the essence and removes the dross ,   and further improves the final translation quality   without affecting the inference speed .   4 Experiments   In this section , we first introduce the settings of our   experiments in Section 4.1 , then report the main   results in Section 4.2 . Ablation experiments and   analysis are presented in Section 4.3 .   4.1 Experimental Setup   Dataset and Evaluation We evaluate our method   on the two most recognized machine translation   benchmarks : WMT’14 English – German ( 4.0 M   sentence pairs)and WMT’16 English – Romanian   ( 610 K pairs ) . We use BLEU ( Papineni et al . ,   2002 ) to evaluate the translation quality . And the   beam size is set to 5 for AT model during infer-   ence . Moreover , for a fair comparison , we ob-   tain the Huang et al . ( 2021 ) open - source corpus   whose tokenization and vocabulary are the same   as previous work : Zhou et al . ( 2020 ) for WMT’14EN – DE which contains 39.8k subwords , and Lee   et al . ( 2018 ) for WMT’16 EN – RO which contains   34.6k subwords . Both the implementation and eval-   uation of our method are performed using the open   source fairseq(Ott et al . , 2019 ) .   Knowledge Distillation Using AT model ’s out-   put to train the NAT model can significantly im-   prove the performance of the NAT model . Fol-   lowing previous work ( Gu et al . , 2018 ; Lee et al . ,   2018 ; Ghazvininejad et al . , 2019 ) , we also em-   ploy sequence - level knowledge distillation for all   datasets . All the distillation data we use is open   sourced by Huang et al . ( 2021 ) .   Hyperparameters Our model architecture is   Transformer - base ( Vaswani et al . , 2017 ): a 6 - layer   encoder and a 6 - layer decoder , 8 attention heads   per layer , 512 attention modules dimensions , 2048   feedforward modules hidden dimensions . We adopt   the Adam optimizer ( Kingma and Ba , 2015 ) with   β= ( 0 .9,0.98 ) . To train the models , we use   a batch size of 64 K tokens , with a maximum   300 K updates . For regularization , we use dropout   ( WMT’14 EN - DE : 0.1 , WMT’16 EN - RO : 0.3 ) ,   0.01 weight decay and 0.1 label smoothing .   Base Models Our Candidate Soups is a general   algorithm that can be applied to various NAT mod-   els . Therefore , to evaluate whether our proposed   method can perform well on different NAT models ,   we selected the following four base models :   ( 1)Vanilla NAT ( Gu et al . , 2018 ) , which predicts   length instead of fertility sequence .   ( 2)CMLM ( Ghazvininejad et al . , 2019 ) , whose   training strategy follows a masked language   model approach similar to BERT ( Devlin   et al . , 2019 ) . And it can perform iterative   decoding during inference . We trained one   CMLM model for each translation task and re-   spectively iterated decoding once and iterated   decoding five times as two baselines .   ( 3)GLAT ( Qian et al . , 2021 ) , which trains NAT   model step - by - step in a curriculum learning   manner .   ( 4)GLAT & DSLP ( Huang et al . , 2021 ) , whose   decoder layers can get the prediction result of   the previous layer .   The prediction patterns and performance of these   base models are quite different , so through them,4815   we can verify whether Candidate Soups can be   applied to various NAT models . In the future ,   we will test Candidate Soups on more NAT mod-   els , such as CTC ( Libovick ` y and Helcl , 2018 ) and   CTC+V AE ( Gu and Kong , 2021 ) .   4.2 Main Results   Generality of Candidate Soups Table 1 shows   the performance improvement of our method for   four base models on four translation tasks . Here ,   our number of candidate results is set to 5 . And   we use the Transformer - Base as the architecture   of the AT model for re - scoring . The results show   that for each NAT model and translation task , using   Candidate Soups can achieve an average of 2.51-   2.92 BLEU higher than the base model . Even com-   pared with the NPD , Candidate Soups improves   BLEU by an average of 0.76 - 1.39 BLEU , which   is a considerable improvement in machine transla-   tion task . Impressively , using the Candidate Soups   on a strong baseline ( GLAT & DSLP ) can achieve   superior performance than the AT teacher . Fur-   thermore , when AT models are used for re - scoring ,   they can perform parallel decoding as fast as train-   ing ( Gu et al . , 2018 ) . So the inference latency isonly roughly doubled , which is still much faster   than the AT model .   In conclusion , the above experimental results   show that Candidate Soups is a general approach   that can significantly improve translation quality   while maintaining fast inference speed .   Comparing with the State of the Art To   evaluate the best performance Candidate Soups   can achieve , we compare our best variant   ( GLAT+DSLP+Candidate Soups ) with previous   state - of - the - art NAT models , including Iterative   NAT and Fully NAT . As shown in Table 2 , com-   pared with the Iterative NAT , we produce a very   competitive translation quality with approximately   2×-4×faster inference speed . Compared with   Fully NAT , our best variant is better than all ex-   isting models in two translation tasks ( EN →DE ,   RO→EN ) and is close to the current state - of - the - art   performance in the remaining two translation tasks .   More encouragingly , our approach even performed   better than AT teacher on three translation tasks   and achieved very comparable performance on the   remaining one translation tasks , which extensively   validated the effectiveness of Candidate Soups .   In addition , we also try to use two smaller AT   models for re - scoring to accelerate the inference   speed further . These two models have the same   hyperparameters as Transformer - base , except for   the number of layers of decoder and encoder . AT4816   4E-2D contains 4 encoder layers and 2 decoder   layers , and AT 3E-1D contains 3 encoder layers   and 1 decoder layer . Moreover , they were trained   using the same distillation data as the NAT model .   Surprisingly , even when the small AT models were   used for re - scoring , our method maintained a simi-   lar performance to the previous model ( AT 6E-6D ) ,   and its inference speed was 10.1×-11.5×that of   the AT model . This result further proves that Candi-   date Soups can well balance the trade - off between   translation quality and inference speed .   4.3 Ablation Study and Analysis   Influence of the Candidate Number In order   to analyze the effect of the candidate translation   number on the Candidate Soups , we conduct exper-   iments with different candidate numbers . Figure   5 shows the relationship between translation qual-   ity and the number of candidate results . Specifi-   cally , with the increased candidate numbers , the   quality of the translation generally maintains a   growth trend . Especially when the candidate results   number is less than 4 , the Candidate Soups perfor-   mance is significantly improved when the number   increases . However , when the number increases to   a certain threshold , the quality of the translation be-   gins to fluctuate , even showing a slight downward   trend . We guess this is because when the number   is larger than the threshold , the quality of the can-   didate translations may gradually decrease due to   the gap between the predicted length and the actual   length becoming larger . Furthermore , there may be   duplication between the candidate results . There-   fore , using 3 - 7 candidate translations is enough   for Candidate Soups to significantly improve the   quality of the final translation.4817   Influence of the Autoregressive Teacher To an-   alyze the effect of whether using the AT model to   re - score on our proposed method , we conducted   experiments on the WMT’14 EN - DE dataset . Fig-   ure 6 demonstrates that Candidate Soups has a   different dependence on AT model in different   NAT models . For a model with weaker perfor-   mance , such as Vanilla NAT , when AT model is   not used for re - scoring , Candidate Soups ’ perfor-   mance degrades significantly . However , for GLAT   and GLAT+DSLP , which can produce high - quality   translations , Candidate Soups can still increase   approximately 1.89 - 1.63 BLEU even without re-   scoring with the AT model . Notably , using Can-   didate Soups in this case hardly increases the in-   ference time of the NAT model . Moreover , after   using AT model for re - scoring , the effect of Can-   didate Soups can be further improved , which in-   creases 2.64 and2.31 BLEU on the GLAT and   GLAT+DSLP , respectively . In addition , the results   in Table 2 show that we can further improve the   inference speed on the premise of guaranteeing   translation quality by using a smaller AT model for   re - scoring .   Influence of the Source Input Length To ana-   lyze the influence of source sentence length on Can-   didate Soups ’ performance , we divide the source   sentence after BPE into different intervals by length   and calculate the BLEU score of each interval . The   histogram of results is presented in Figure 7 . It   can be seen that the performance of Vanilla NAT   degrades significantly as the length of the source   sentence increases . Although NPD can improve   the overall translation quality , the translation qual-   ity of long source sentences is still inferior . How-   ever , Candidate Soups can dramatically improve   Vanilla NAT ’s performance and enables long sen-   tences to achieve much higher BLEU than short .   Impressively , the BLEU score of the source sen-   tence length ranging from 40 to 60 increases by   5.01 , and the BLEU score of the source sentences   longer than 60 increases by 7.51 .   We believe this is because the NAT model tends   to generate more uncertain and diverse candidate   results for longer source sentences . This feature   enables Candidate Soups to obtain more useful   information in the candidate results to generate   higher - quality translations . These experimental   results further verify the potential of the Candidate   Soups in translating complex long sentences . More   experimental results and analyses are presented in   the Appendix B.   5 Conclusion   In this paper , we propose “ Candidate Soups , ”   which can discover and fuse valuable informa-   tion from multiple candidate translations based on   model uncertainty . This approach is general and   can be applied to various NAT models . Extensive   experimental results prove that the translation qual-   ity of the NAT model can be significantly improved   by using Candidate Soups , especially for long sen-   tences that are difficult to translate . And the trade-   off between translation quality and inference speed   is well controlled and balanced by Candidate Soups .   Furthermore , our best variant can achieve better re-   sults on three translation tasks than the AT teacher   while maintaining NAT ’s high - speed inference.4818Limitations   Although our proposed method can significantly   improve the performance of non - autoregressive   translation ( NAT ) models , it relies on trained au-   toregressive translation ( AT ) models to a certain   extent . Not using the AT model for re - scoring can   lead to poorer quality of translations generated by   Candidate Soups , especially when using it for the   poorer performing NAT model . Although using a   small AT model is sufficient for Candidate Soups to   achieve decent performance , it still results in a drop   in inference speed and more GPU resources being   used for translation . In addition , the performance   of the AT model may limit the upper bound of the   Candidate Soups ’ capability . Therefore , we will   explore new methods that can be effective without   AT re - score in the future .   Ethics Statement   Our work has potentially positive implications for   various non - autoregressive machine translation ap-   plications . It is a general method that can be ap-   plied to virtually all existing non - autoregressive   translation models to improve their performance   while maintaining their high inference speed . Our   work can facilitate the implementation of non-   autoregressive translation models in commercial   companies and humanitarian translation services in   the future and promote cultural exchanges between   different languages and different races .   Acknowledgements   This work was supported by NSFC grants ( No .   62136002 ) , National Key RD Program of China   ( 2021YFC3340700 ) and Shanghai Trusted Industry   Internet Software Collaborative Innovation Center .   References48194820A Background   A.1 Autoregressive Translation   The autoregressive translation ( AT ) model achieves   sort - of - the - art performance on multiple machine   translation tasks ( Song et al . , 2019 ; Sun et al . ,   2020 ) . Given a source sentence X =   ( x , x , . . . , x)and the target sentence Y=   ( y , y , . . . , y ) , the AT model decomposes the   target distribution of translations according to the   chain rule :   p(Y|X;θ ) = /productdisplayp(y|y , X;θ ) ( 1 )   where ydenotes generated previous tokens be-   fore the tposition . During the training process ,   the AT model is trained via the teacher - forcing   strategy that uses ground truth target tokens as pre-   viously decoded tokens so that the output of the   decoder can be computed in parallel .   However , during inference , the AT model still   needs to generate translations one by one from   left to right until the token that represents the end   [ EOS ] is generated . Although AT model has good   performance , its autoregressive decoding method   dramatically reduces the decoding speed and be-   comes the main bottleneck of its efficiency .   A.2 Non - Autoregressive Translation   To improve the inference speed , the non-   autoregressive translation ( NAT ) model is pro-   posed ( Gu et al . , 2018 ) , which removes the order   dependency between target tokens and can generate   target words simultaneously :   p(Y|X;θ ) = /productdisplayp(y|X;θ ) ( 2 )   where mdenotes the length of the target sentence .   Generally , NAT models need to have the ability   to predict the length because the entire sequence   needs to be generated in parallel . A common prac-   tice is to treat it as a classification task , using the   information from the encoder ’s output to make pre-   dictions .   However , this superior decoding speed is   achieved at the cost of significantly sacrificing   translation quality . Because NAT is only condi-   tioned on source - side information , but AT can ob-   tain the strong target - side context information pro-   vided by the previously generated target tokens ,   there is always a gap in the performance of NAT   compared with AT.4821A.3 Noisy Parallel Decoding   Noisy parallel decoding ( NPD ) ( Gu et al . , 2018 )   is a stochastic search method that can draw sam-   ples from the length space and compute the best   translation for each length as a candidate result .   Then NPD selects the translation with the highest   average log - probability as the final result :   Y = argmax1   m / summationdisplaylogp(y|X;θ)(3 )   where Yis the translation predicted by the   NAT model based on the length m. NPD also can   use the AT model to identify the best translation :   Y= argmax1   m / summationdisplaylogp(y|y , X;θ )   Notably , when an AT model is used for re-   scoring , it can be decoded in parallel as it does   at training time . Moreover , since all search sam-   ples can be computed independently , even with   AT model for re - scoring , the latency of the NPD   process is only doubled compared to computing a   single translation .   B Additional Analysis Experiment   B.1 Influence of the Knowledge Distillation   Compared with the original data , the distillation   data generated by the AT model has less noise   and is more deterministic , which can effectively   alleviate the multimodality problem of the NAT   model . Therefore , almost all the existing NAT   model adopts the method of Knowledge Distilla-   tion ( KD ) for training . However , generating distil-   lation data tends to consume significant computing   resources and time , and using distillation data to   train NAT models may limit the translation capa-   bilities of NAT models .   In order to analyze whether our proposed method   can still be effective in the scenarios where knowl-   edge distillation is not used , we conducted exper-   iments on the WMT’14 EN - DE dataset . Figure   8 shows the performance of Candidate Soups on   the NAT model that does not use knowledge dis-   tillation . For Vanilla NAT and GLAT , the perfor-   mance trained with raw data is significantly re-   duced compared to that trained with knowledge   distillation . However , after using Candidate Soups ,   the BLEU of Vanilla NAT and GLAT respectively   increased by 4.54 and4.88 , which was only 5.68   and1.28 lower than the performance with knowl-   edge distillation . We believe that this significant   performance improvement may be since NAT mod-   els without knowledge distillation may produce   more diverse candidate translations , thus enabling   Candidate Soups to fully play its role and obtain   higher - quality translations from different candidate   translations . The experimental results show that   the Candidate Soups can significantly improve the   performance of the NAT model without knowledge   distillation , which proves the potential of the Can-   didate Soups in this scenario .   B.2 Influence of introducing uncertainty   methods   In addition to introducing uncertainty through   length , we propose two other methods for generat-   ing different candidate translations :   •Use the prediction results of different decoder   layers . DSLP ( Huang et al . , 2021 ) is a general   method that can be applied to various NAT   models , and it needs to predict the translations   in each decoder layer . Therefore , Candidate   Soups can be combined with DSLP , and any   NAT model using DSLP can use Candidate   Soups to fuse the results of different layers . In   this experiment , we use the results generated   by the last 5 layers of the decoder .   •Generate different translations by maintaining   dropout during inference ( Gal and Ghahra-   mani , 2016 ) . Even with the same input , the   model can produce different outputs since4822   dropout activates different neurons each time .   In this experiment , the dropout probability at   inference is set to 0.02 .   Table 3 shows the performance of Candidate   Soup under three different ways of introducing   uncertainty . The experimental results show that ,   compared with the other two methods , when un-   certainty is introduced by length , Candidate Soups   improves the translation quality more significantly .   We speculate that this is because the length uncer-   tainty can ensure that the generated translations are   more diverse under the premise of high quality .   However , for layer uncertainty , the quality of   the translations produced by the first layer will   be significantly lower than that of the last layer .   These low - quality candidate translations are of lit-   tle help to Candidate Soups and even affect the   performance of Candidate Soups . For dropout   uncertainty , the candidate translations generated   will be affected by the dropout probability . On   the one hand , if the dropout probability is set too   high , it may reduce the overall quality of the candi-   date translations . On the other hand , the generated   candidate translations will be less diverse if the   dropout probability is low . So we further need   to spend time searching for the optimal dropout   probability setting for different NAT models and   tasks . However , these two methods can still achieve   about 1 BLEU improvement on the strong baseline   ( GLAT+DSLP ) , and their generalization ability is   stronger than the length - based method . In addi-   tion , Candidate Soups can also be used as a new   model ensemble method to enhance the final trans-   lation quality by using the output from multiple   NAT models . We will discuss this in future work.4823