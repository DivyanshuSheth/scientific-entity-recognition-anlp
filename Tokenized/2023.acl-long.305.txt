  Nayeon Kim , Yinhua Piao , and Sun KimInterdisciplinary Program in Artificial Intelligence , Seoul National UniversityDepartment of Computer Science and Engineering , Seoul National UniversityInstitute of Computer Technology , Seoul National UniversityAIGENDRUG Co. , Ltd.   { ny_1031 , 2018 - 27910 , sunkim.bioinfo } @snu.ac.kr   Abstract   Leveraging knowledge from electronic health   records ( EHRs ) to predict a patient ’s condition   is essential to the effective delivery of appropri-   ate care . Clinical notes of patient EHRs contain   valuable information from healthcare profes-   sionals , but have been underused due to their   difficult contents and complex hierarchies . Re-   cently , hypergraph - based methods have been   proposed for document classifications . Directly   adopting existing hypergraph methods on clin-   ical notes can not sufficiently utilize the hier-   archy information of the patient , which can   degrade clinical semantic information by ( 1 )   frequent neutral words and ( 2 ) hierarchies with   imbalanced distribution . Thus , we propose a   taxonomy - aware multi - level hypergraph neural   network ( TM - HGNN ) , where multi - level hy-   pergraphs assemble useful neutral words with   rare keywords via note and taxonomy level hy-   peredges to retain the clinical semantic infor-   mation . The constructed patient hypergraphs   are fed into hierarchical message passing layers   for learning more balanced multi - level knowl-   edge at the note and taxonomy levels . We val-   idate the effectiveness of TM - HGNN by con-   ducting extensive experiments with MIMIC - III   dataset on benchmark in - hospital - mortality pre-   diction .   1 Introduction   With improvement in healthcare technologies , elec-   tronic health records ( EHRs ) are being used to mon-   itor intensive care units ( ICUs ) in hospitals . Since   it is crucial to schedule appropriate treatments for   patients in ICUs , there are many prognostic models   that use EHRs to address related tasks , such as in-   hospital mortality prediction . EHRs consist of three   types of data ; structured , semi - structured , and un-   structured . Clinical notes , which are unstructured   data , contain valuable comments or summary of theFigure 1 : ( a ) Examples of patient clinical notes with   difficult contents ( e.g. jargons and abbreviations ) and   complex structures . Patient powns notes of radiol-   ogy taxonomy ( pink ) and nursing taxonomy ( blue ) . ( b )   Differences between existing hypergraphs and our pro-   posed multi - level hypergraphs .   patient ’s condition written by medical professionals   ( doctors , nurses , etc . ) . However , compared to struc-   tured data , clinical notes have been underutilized in   previous studies due to the difficult - to - understand   contents and the complex hierarchies ( Figure 1(a ) ) .   Transformer - based ( Vaswani et al . , 2017 ) methods   like ClinicalBERT ( Alsentzer et al . , 2019 ; Huang   et al . , 2019a , 2020 ) have been proposed to pre-   train on large - scale corpus from similar domains ,   and fine - tune on the clinical notes through transfer   learning . While Transformer - based methods can   effectively detect distant words compared to other   sequence - based methods like convolutional neural   networks ( Kim , 2014 ; Zhang et al . , 2015 ) and re-   current neural networks ( Mikolov et al . , 2010 ; Tai   et al . , 2015 ; Liu et al . , 2016 ) , there are still limita-   tions of increasing computational complexity for   long clinical notes ( Figure 2 ) .   Recently , with the remarkable success of the   graph neural networks ( GNNs ) ( Kipf and Welling,55592017 ; Veli ˇckovi ´ c et al . , 2018 ; Brody et al . , 2021 ) ,   graph - based document classification methods have   been proposed ( Yao et al . , 2019 ; Huang et al . ,   2019b ) that can capture long range word depen-   dencies and can be adapted to documents with dif-   ferent and irregular lengths . Some methods build   word co - occurrence graphs by sliding fixed - size   windows to model pairwise interactions between   words ( Zhang et al . , 2020 ; Piao et al . , 2022 ; Wang   et al . , 2022 ) . However , the density of the graph in-   creases as the document becomes longer . Besides ,   there are also some methods apply hypergraph for   document classification ( Ding et al . , 2020 ; Zhang   et al . , 2022a ) , which can alleviate the high den-   sity of the document graphs and extract high - order   structural information of the documents .   Adopting hypergraphs can reduce burden for   managing long documents with irregular lengths ,   but additional issues remain when dealing with   clinical notes : ( 1 ) Neutral words deteriorate clin-   ical semantic information . In long clinical notes ,   there are many frequently written neutral words   ( e.g. " rhythm " ) that do not directly represent the   patient ’s condition . Most of the previous methods   treat all words equally at the learning stage , which   may result in dominance of frequent neutral words ,   and negligence of rare keywords that are directly   related to the patient ’s condition . Meanwhile , the   neutral word can occasionally augment information   of rare keywords , depending on the intra - taxonomy   context . Taxonomy represents the category of the   clinical notes , where implicit semantic meaning   of the words can differ . For example , " rhythm "   occurred with " fibrillation " inECG taxonomy can   represent serious cardiac disorder of a patient , but   when " rhythm " is written with " benadryl " inNurs-   ingtaxonomy , it can hardly represent the serious   condition . Therefore , assembling intra - taxonomy   related words can leverage " useful " neutral words   with rare keywords to jointly augment the clini-   cal semantic information , which implies the ne-   cessity of introducing taxonomy - level hyperedges .   ( 2)Imbalanced distribution of multi - level hyper-   edges . There are a small number of taxonomies   compared to notes for each patient . As a result ,   when taxonomy - level and note - level information   are learned simultaneously , note - level information   can obscure taxonomy - level information . To learn   more balanced multi - level information of the clini-   cal notes , an effective way for learning the multi-   level hypergraphs with imbalanced distributed hy-   peredges is required .   To address the above issues , we propose TM-   HGNN ( Taxonomy - aware Multi - level HyperGraph   Neural Networks ) , which can effectively and effi-   ciently utilize the multi - level high - order seman-   tic information for patient representation learn-   ing . Specifically , we adopt patient - level hyper-   graphs to manage highly unstructured and long clin-   ical notes and define multi - level hyperedges , i.e. ,   note - level and taxonomy - level hyperedges . More-   over , we conduct the hierarchical message passing   from note - level to taxonomy - level hyperedges us-   ing edge - masking . To hierarchically learn word   embeddings without mixture of information be-   tween note and taxonomy , note and taxonomy hy-   peredges are disconnected . Note - level word em-   beddings are learned only with intra - note local in-   formation . The following taxonomy - level propa-   gation introduce clinical semantic information by   assembling the intra - taxonomy words and separat-   ing inter - taxonomy words for better patient - level   representation learning . The contributions of this   article can be summarized as follows ( Figure 2 ):   •To address issue 1 , we construct multi - level   hypergraphs for patient - level representation   learning , which can assemble " useful " neutral   word with rare keyword via note and taxon-   omy level hyperedges to retain the clinical   semantic information.5560•To address issue 2 , we propose hierarchical   message passing layers for the constructed   graphs with imbalanced hyperedges , which   can learn more balanced multi - level knowl-   edge for patient - level representation learning .   •We conduct experiments with MIMIC - III clin-   ical notes on benchmark in - hospital - mortality   task . The experimental results demonstrate   the effectiveness of our approach .   2 Related Work   2.1 Models for Clinical Data   With the promising potential of managing medical   data , four benchmark tasks were proposed by Haru-   tyunyan et al . ( 2019 ) for MIMIC - III ( Medical Infor-   mation Mart for Intensive Care - III ) ( Johnson et al . ,   2016 ) clinical dataset . Most of the previous works   with MIMIC - III dataset focus on the structured data   ( e.g. vital signals with time - series ) for prognostic   prediction tasks ( Choi et al . , 2016 ; Shang et al . ,   2019 ) or utilize clinical notes combined with time-   series data ( Khadanga et al . , 2019 ; Deznabi et al . ,   2021 ) . Recently , there are approaches focused on   clinical notes , adopting pre - trained models such as   BERT - based ( Alsentzer et al . , 2019 ; Huang et al . ,   2019a ; Golmaei and Luo , 2021 ; Naik et al . , 2022 )   and XLNet - based ( Huang et al . , 2020 ) or utilizing   contextualized phenotypic features extracted from   clinical notes ( Zhang et al . , 2022b ) .   2.2 Graph Neural Networks for Document   Classification   Graph neural networks ( Kipf and Welling , 2017 ;   Veliˇckovi ´ c et al . , 2018 ; Brody et al . , 2021 ) have   achieved remarkable success in various deep learn-   ing tasks , including text classification . Initially ,   transductive graphs have been applied to docu-   ments , such as TextGCN ( Yao et al . , 2019 ) . Trans-   ductive models have to be retrained for every re-   newal of the data , which is inefficient and hard to   generalize ( Yao et al . , 2019 ; Huang et al . , 2019b ) .   For inductive document graph learning , word co-   occurrence graphs initialize nodes with word em-   beddings and exploit pairwise interactions between   words . TextING ( Zhang et al . , 2020 ) employs   the gated graph neural networks for document-   level graph learning . Following TextGCN ( Yao   et al . , 2019 ) which applies graph convolutional net-   works ( GCNs ) ( Kipf and Welling , 2017 ) in trans-   ductive level corpus graph , InducT - GCN ( Wanget al . , 2022 ) applies GCNs in inductive level where   unseen documents are allowed to use . TextSSL   ( Piao et al . , 2022 ) captures both local and global   structural information within graphs .   However , the density of word co - occurrence   graph increases as the document becomes longer ,   since the fixed - sized sliding windows are used to   capture local pairwise edges . In case of hyper-   graph neural networks , hyperedges connect multi-   ple number of nodes instead of connecting words   to words by edges , which alleviates the high den-   sity of the text graphs . HyperGAT ( Ding et al . ,   2020 ) proposes document - level hypergraphs with   hyperedges containing sequential and semantic in-   formation . HEGEL ( Zhang et al . , 2022a ) applies   Transformer - like ( Vaswani et al . , 2017 ) multi - head   attention to capture high - order cross - sentence re-   lations for effective summarization of long docu-   ments . According to the reduced computational   complexity for long documents ( Figure 2 ) , we   adopt hypergraphs to represent patient - level EHRs   with clinical notes . Considering issues of exist-   ing hypergraph - based methods ( Figure 2 ) , we con-   struct multi - level hypergraphs at note - level and   taxonomy - level for each patient . The constructed   graphs are fed into hierarchical message passing   layers to capture rich hierarchical information of   the clinical notes , which can augment semantic   information for patient representation learning .   3 Method   3.1 Problem Definition   Our task is to predict in - hospital - mortality for   each patient using a set of clinical notes . Given   a patient p∈ P with in - hospital - mortality label   y∈ Y , patient powns a list of clinical notes N=   [ n , ... , n , ... ] , and each clinical note n∈ N   with taxonomy t∈ Tcontains a sequence of   words W= [ w , ... , w , ... ] , where j , kand   idenote the index of clinical note n , taxonomy t   and word wof patient p. The set of taxonomies can   be represented by T={t , t , ... , t , ... } .   Our goal is to construct individual multi - level   hypergraphs Gfor each patient pand learn patient-   level representation Gwith the multi - level knowl-   edge by hierarchical message passing layers for   in - hospital - mortality prediction task . Since our   model is trained by inductive learning , patient pis   omitted throughout the paper.5561   3.2 Multi - Level Hypergraph Construction   We construct multi - level hypergraphs for patient-   level representation learning , which can address   the issues that are mentioned in introduction 1 . A   hypergraph G= ( V , E)consists of a set of nodes   Vand hyperedges Ewhere multiple nodes can be   connected to single hyperedge e∈ E. A multi - level   hypergraph G={V,{E∪ E}}is constructed   from patient ’s clinical notes , where EandE   denote note - level and taxonomy - level hyperedges ,   respectively . A word node vexists in note nwith   the taxonomy of tcan be represented by { v∈   n , n∈t } . A note - level hyperedge is denoted as e ,   and a taxonomy - level hyperedge is denoted as e.   Multi - level Positional Encoding There are three   types of entries in the multi - level hypergraph G ,   such as word nodes V , note - level hyperedges E   and taxonomy - level hyperedges E. To distinguish   these entries , we propose multi - level positional en-   coding to introduce more domain - specific meta-   information to the hypergraph G. The function   of multi - level positional encodingcan be   defined as :   where entry x∈ { V , E , E } , and function   τ : x∝ ⇕ ⊣√∫⊔≀→ { 0,1,2}maps entry xto a single type   among nodes , note - level and taxonomy - level hy - peredges . Functions I(·),I ( · ) , andI(·)maps   entry xto positions in the word , note and taxonomy-   level , respectively . To initialize embedding of node   v , we concatenate embeddingfrom multi-   level position encoding and word2vec ( Mikolov   et al . , 2010 ) pre - trained embedding z. Since shal-   low word embeddings are widely used to initial-   ize node embeddings in graph - based document   representation ( Grohe , 2020 ) , we use word2vec   ( Mikolov et al . , 2010 ) embedding . A word node   embedding his constructed as follows :   where⊕denotes concatenation function .   3.2.1 Hyperedge Construction   To extract multi - level information of patient - level   representation using clinical notes , we construct   patient hypergraphs with two types of hyperedges ,   one at the note - level hyperedge Eand the other   at the taxonomy - level hyperedge E. A word node   vin note nwith taxonomy tis assigned to one   note - level hyperedge eand one taxonomy - level   hyperedge e , which can be defined as :   Note - level Hyperedges We adopt linear embed-   ding function fand obtain the index embedding5562usingI(n ) . To preserve time - dependent sequen-   tial information of clinical note n , we simply add   time information t(n)to the embedding . Then ini-   tial embedding of note - level hyperedge hwithcan be defined as :   where θ∈Rdenotes the parameter matrix of   function f. Notably , we set the value of word in-   dexI(n)as -1 since the note nrepresents higher   level information than word v.   Taxonomy - level Hyperedges Taxonomy - level   hyperedges eare constructed by taxonomy index   I(t)through linear layers fconcatenated withfunction , which can be defined as :   where θ∈Rdenotes the parameter matrix   of function f. Like note - level hyperedge , we set   I(t)andI(t)as -1 since the level of taxonomy   tis higher than the levels of note and word .   3.3 Hierarchical Message Passing   To leverage the characteristics of two types of hy-   peredges , we propose a hierarchical hypergraph   convolutional networks , composed of three layers   that allow message passing from different types of   hyperedges . In general , we define message passing   functions for nodes and hyperedges as follows :   where Fdenotes message passing function   for word nodes and Fdenotes message passing   function for hyperedges with type τ∈ { 1,2 } , i.e. ,   note - level hyperedges and taxonomy - level hyper-   edges , respectively . Function Fupdates word   node embedding hby aggregating embeddings of   connected hyperedges E(v ) . Function Fupdates   hyperedge embedding hby aggregating embed-   dings of connected word nodes V(e).σis the non-   linear activation function such as ReLU , θ∈R   is the weight matrix with dimension dwhich can be   differently assinged and learned at multiple levels .   Then we can leverage these defined functions to   conduct hierarchical message passing learning at   the note level and at the taxonomy level . Statistics   # of patients 17,927   # of ICU stays 21,013   # of in - hospital survival 18,231   # of in - hospital mortality 2,679   # of notes per ICU stay 13.29 ( 7.84 )   # of words per ICU stay 1,385.62 ( 1,079.57 )   # of words per note 104.25 ( 66.82 )   # of words per taxonomy 474.75 ( 531.42 )   Initialization Layer Due to the complex struc-   ture of the clinical notes , the initial multi - level hy-   pergraph constructed for each patient has a large   variance . To prevent falling into local optima in ad-   vance , we first use an initialization layer to pre - train   the entries of hypergraphs by learning the entire   patient graph structure . In this layer , message pass-   ing functions are applied to all word nodes v∈ V   and hyperedges e∈ E={E∪ E } . Thus , em-   beddings of node v , hyperedges eandeat both   levels can be defined as :   Note - level Message Passing Layer Then we ap-   ply note - level message passing layer on hyper-   graphs with only word nodes v∈ V and note - level   hyperedges e∈ E , and the taxonomy - level hy-   peredges are masked during message passing . In   this layer , the word nodes can only interact with   note - level hyperedges , which can learn the intra-   note local information .   Taxonomy - level Message Passing Layer The   last layer is the taxonomy - level message passing   layer , where all word nodes v∈ V and taxonomy-   level hyperedges e∈ Ecan be updated . In   this layer , we block the hyperedges at the note   level . The node representations with note - level in-   formation are fused with taxonomy information5563via taxonomy - level hyperedges , which can assem-   ble the intra - taxonomy related words to augment   semantic information .   3.3.1 Patient - Level Hypergraph Classification   After all aforementioned hierarchical message   passing layers , node and hyperedge embed-   dings h(v ) , h(e ) , h(e)∈Hfollow mean-   pooling operation which summarizes patient - level   embedding z , which is finally fed into sigmoid   operation as follows :   where ˆydenotes the probability of the predicted   label for in - hospital - mortality of the patient . The   loss function for patient - level classification is de-   fined as the binary cross - entropy loss :   where ydenotes the true label for in - hospital-   mortality . The proposed network , TM - HGNN , can   be trained by minimizing the loss function .   4 Experimental Settings   4.1 Dataset   We use clinical notes from the Medical Informa-   tion Mart for Intensive Care III ( MIMIC - III ) ( John-   son et al . , 2016 ) dataset , which are written within   48 hours from the ICU admission . For quantita-   tive evaluation , we follow Harutyunyan et al . ’s   ( 2019 ) benchmark setup for data pre - processing   and train / test splits , then randomly divide 20 % of   train set as validation set . All patients without   any notes are dropped during the data preparation .   To prevent overfitting into exceptionally long clin-   ical notes for a single patient , we set the maxi-   mum number of notes per patient into 30 from the   first admission . Table 1 shows the statistics of pre-   processed MIMIC - III clinical note dataset for our   experiments . We select top six taxonomies for ex-   periments , since the number of notes assigned to   each taxonomy differs in a wide range ( Appendix   B Table 3 ) . In addition , we select two chronic   diseases , hypertension and diabetes , to compare   prediction results for patients with each disease.4.2 Compared Methods   In our experiments , the compared baseline methods   for end - to - end training are as follows :   •Word - based methods : word2vec ( Mikolov   et al . , 2013 ) with multi - layer perceptron clas-   sifier , and FastText ( Joulin et al . , 2017 ) .   •Sequence - based methods : TextCNN ( Kim ,   2014 ) , Bi - LSTM ( Hochreiter and Schmidhu-   ber , 1997 ) , and Bi - LSTM with additional at-   tention layer ( Zhou et al . , 2016 ) .   •Graph - based methods : TextING ( Zhang et al . ,   2020 ) , InducT - GCN ( Wang et al . , 2022 ) , and   HyperGAT ( Ding et al . , 2020 ) . In particu-   lar , HyperGAT represents hypergraph - based   method , and the other graph - based methods   employ word co - occurrence graphs .   4.3 Implementation Details   TM - HGNN is implemented by PyTorch ( Paszke   et al . , 2019 ) and optimized with Adam ( Kingma   and Ba , 2015 ) optimizer with learning rate 0.001   and dropout rate 0.3 . We set hidden dimension d   of each layer to 64 and batch size to 32 by search-   ing parameters . We train models for 100 epochs   with early - stopping strategy , where the epoch of 30   shows the best results . All experiments are trained   on a single NVIDIA GeForce RTX 3080 GPU .   5 Results   Since the dataset has imbalanced class labels for   in - hospital mortality as shown in Table 1 , we use   AUPRC ( Area Under the Precision - Recall Curve )   and AUROC ( Area Under the Receiver Operating   Characteristic Curve ) for precise evaluation . It is   suggested by Davis and Goadrich ( 2006 ) to use   AUPRC for imbalanced class problems .   5.1 Classification Performance   Table 2 shows performance comparisons of TM-   HGNN and baseline methods . Sequence - based   methods outperform word - based methods , which   indicates capturing local dependencies between   neighboring words benefits patient document clas-   sification . Moreover , all graph - based methods out-   perform sequence - based and word - based methods .   This demonstrates ignoring sequential information   of words is not detrimental to clinical notes . Fur-   thermore , hypergraphs are more effective than pre-   vious word co - occurrence graphs , indicating that5564   it is crucial to extract high - order relations within   clinical notes . In particular , as TM - HGNN outper-   forms HyperGAT ( Ding et al . , 2020 ) , exploiting   taxonomy - level semantic information which rep-   resents the medical context of the notes aids pre-   cise prediction in patient - level . Another advantage   of our model , which captures multi - level high or-   der relations from note - level and taxonomy - level   with hierarchy , can be verified by the results in Ta-   ble 2 where TM - HGNN outperforms T - HGNN . T-   HGNN indicates the variant of TM - HGNN , which   considers note - level and taxonomy - level hyper-   edges homogeneous . Likewise , results from hyper-   tension and diabetes patient groups show similar   tendencies in overall .   5.2 Robustness to Lengths   To evaluate the performance dependencies to   lengths , we divide clinical notes in patient - level   into three groups by lengths , which are short ,   medium , and long ( Appendix B , Figure 8) . For   test set , the number of patients is 645 , 1,707 , and   856 for short , medium , and long group each , and   the percentage of mortality is 6.98 % , 10.72 % , and   15.89 % for each group , which implies patients in   critical condition during ICU stays are more likely   to have long clinical notes . Figure 4 shows per-   formance comparisons for three divided groups   with TextING ( Zhang et al . , 2020 ) which utilizes   word co - occurrence graph , HyperGAT ( Ding et al . ,   2020 ) , a ordinary hypergraph based approach , and   our multi - level hypergraph approach ( TM - HGNN ) .   All three models were more effective to longer   clinical notes , which demonstrates graph based   models are robust to long document in general .   Among the three models , our proposed TM - HGNN   mostly performs the best and HyperGAT ( Ding   et al . , 2020 ) follows , and then TextING ( Zhang   et al . , 2020 ) . The results demonstrate that our TM-   HGNN , which exploits taxonomy - level semantic   information , is most effective for clinical notes   regardless of the lengths , compared to other graph-   based approaches.5565   5.3 Ablation Study   Effect of Multi - level Hypergraph In order to   validate the effect of multi - level hypergraphs ,   we ignore taxonomy - level and note - level hyper-   edges respectively . w/o taxonomy , which ignores   taxonomy - level hyperedges , deteriorates the per-   formance most significantly . w/o note shows de-   graded performance as well . Thus , effectiveness   of multi - level hypergraph construction for patient   representation learning can be verified ( Figure 5 ) .   Effect of Hierarchical Message Passing Figure   5 demonstrates that hierarchical message passing   ( note - level to taxonomy - level ) for multi - level hy-   pergraphs is effective than learning without hier-   archies , since w/o hierarchy shows inferior per-   formance compared to TM - HGNN . w/o hierarchy   represents T - HGNN from Table 2 , which consid - ers every hyperedge as homogeneous . Degraded   performance from w/o initialization shows the ef-   fectiveness of the initialization layer before hierar-   chical message passing , which indicates that pre-   training on the entire multi - level hypergraphs first   benefits the patient - level representation learning .   5.4 Case Study   Hierarchical Message Passing We visualize the   learned node representations based on principal   component analysis ( PCA ) ( Jolliffe , 2002 ) results ,   as hierarchical message passing continues in TM-   HGNN . In Figure 6(a ) , " rhythm " from ECG and   Nursing / other taxonomy are mapped closely for   initial word embeddings , since they are literally   same words . As the patient - level hypergraphs are   fed into a global - level , note - level , and taxonomy-   level convolutional layers in order , words in the   same taxonomies assemble , which can be found   in Figure 6(b ) , ( c ) , and ( d ) . As a result , " rhythm "   of ECG represents different semantic meanings   from " rhythm " of Nursing / other , as it is learned   considerably close to " fibrillation " from the same   taxonomy .   Importance of Taxonomy - level Semantic In-   formation To investigate the importance of   taxonomy - level semantic information extraction ,   we visualize PCA results of the learned node em-   beddings from the baseline method and the pro-   posed TM - HGNN . We select patient with hospi-   tal admission i d ( HADM_ID ) 147702 for case   study since TM - HGNN successfully predicts the   true label for in - hospital - mortality , which is pos-5566itive , but the other baseline methods show false   negative predictions . As in Figure 7 , HyperGAT   learns " rhythm " without taxonomy - level semantic   information , since it is not assembled with other   words in the same taxonomy . But TM - HGNN sepa-   rately learns " rhythm " from ECG and " rhythm "   from Nursing / other based on different contexts ,   which results in same taxonomy words aligned   adjacently , such as " fibrillation " of ECG and " be-   nadryl " of Nursing / other . Therefore , in case of   TM - HGNN , frequently used neutral word " rhythm "   from ECG with a word " fibrillation " means an ir-   regular " rhythm " of the heart and is closely related   to mortality of the patient , but " rhythm " from Nurs-   ing / other with another nursing term remains more   neutral . This phenomenon demonstrates that con-   textualizing taxonomy to frequent neutral words   enables differentiation and reduces ambiguity of   the frequent neutral words ( e.g. " rhythm " ) , which   is crucial to avoid false negative predictions on   patient - level representation learning .   6 Conclusion   In this paper , we propose a taxonomy - aware multi-   level hypergraph neural networks , TM - HGNN , a   novel approach for patient - level clinical note repre-   sentation learning . We employ hypergraph - based   approach and introduce multi - level hyperedges   ( note and taxonomy - level ) to address long and   complex information of clinical notes . TM - HGNN   aims to extract high - order semantic information   from the multi - level patient hypergraphs in hierar-   chical order , note - level and then taxonomy - level .   Clinical note representations can be effectively   learned in an end - to - end manner with TM - HGNN ,   which is validated from extensive experiments .   Limitations   Since our approach , TM - HGNN , aggregates every   note during ICU stays for patient representation   learning , it is inappropriate for time - series predic-   tion tasks ( e.g. vital signs ) . We look forward to   further study that adopts and applies our approach   to time - series prediction tasks .   Ethics Statement   In MIMIC - III dataset ( Johnson et al . , 2016 ) , every   patient is deidentified , according to Health Insur-   ance Portability and Accountability Act ( HIPAA )   standards . The fields of data which can identify thepatient , such as patient name and address , are com-   pletely removed based on the identifying data list   provided in HIPAA . In addition , the dates for ICU   stays are shifted for randomly selected patients ,   preserving the intervals within data collected from   each patient . Therefore , the personal information   for the patients used in this study is strictly kept   private . More detailed information about deiden-   tification of MIMIC - III can be found in Johnson   et al . ( 2016 ) .   Acknowledgements   This work was supported by Institute of Informa-   tion & communications Technology Planning &   Evaluation ( IITP ) grant funded by the Korea gov-   ernment ( MSIT ) [ NO.2021 - 0 - 01343 , Artificial In-   telligence Graduate School Program ( Seoul Na-   tional University ) ] and the Bio & Medical Tech-   nology Development Program of the National Re-   search Foundation ( NRF ) funded by the Ministry   of Science & ICT ( RS-2023 - 00257479 ) , and the   ICT at Seoul National University provides research   facilities for this study .   References55675568A Detailed Statistics of MIMIC - III   Clinical Notes   Table 3 shows the number of clinical notes assigned   to 15 predefined taxonomies in MIMIC - III dataset .   Since the number of notes varies in a wide range   for each taxonomy , we select top six taxonomies   for experiments : Radiology , ECG , Nursing / other ,   Echo , Nursing , and Physician .   Figure 8 shows histogram for the number of   words per patient - level clinical notes in train set .   Since 682 , 1,070 , and 1,689 are the first , second ,   and third quantile of the train data , we select 600   and 1,600 as the boundaries to divide test set into   3 groups ( short , medium , and long ) , which is used   to validate proposed TM - HGNN ’s robustness to   lengths .   B Node Representations from Other   Methods   Figure 9 shows PCA results of learned node rep-   resentations from three different models . Accord-   ing to Figure 9(a ) and 9(b ) , word co - occurrence   graphs ( TextING ) and homogeneous single - level   hypergraphs ( HyperGAT ) show node representa-   tions ambiguous to discriminate by taxonomies ,   since every taxonomy has been shuffled . In Fig-   ure 9(c ) , node embeddings are aligned adjacently   and arranged with similar pattern for the same tax-   onomies . This verifies the effectiveness of the pro-   posed TM - HGNN which captures intra- and inter-   taxonomy semantic word relations for patient - level   representation learning . Example words ( voltage ,   lvef , benadryl , and obliteration ) which are gener-   ally used in each taxonomy are shown in Figure   9 to emphasize that the keywords from each tax-   onomy are learned adjacently to words similar in   context within taxonomies in case of TM - HGNN ,   but not for other methods .   C Explanation of the Medical Terms   •Fibrillation : Fibrillation refers to rapid and   irregular contractions of the muscle fibers , es-   pecially from the heart . It can lead to serious   heart conditions .   •Benadryl : Brand name for the drug Diphen-   hydramine , which is an antihistamine . Be-   nadryl is one of the over - the - counter drugs ,   and generally used for alleviating the allergic   symptoms.5569   •Lvef : Abbreviation of left ventricular ejection   fraction , which is the ratio of stroke volume   to end - diastolic volume . Lvef is known as the   central measure for the diagnosis and manage-   ment of heart failure .   •Obliteration : In Radiology , obliteration refers   to the disappearance of the contour of an or-   gan , due to the same x - ray absorption from   the adjacent tissue .   D Additional Performance Comparison   We conduct additional experiments using LSTM   based on 17 code features selected by Johnson   et al . ( 2016 ) , and Transformer - based ClinicalXL-   Net ( Huang et al . , 2020 ) without pre - training for   in - hospital mortality prediction . Table 4 shows that   TM - HGNN outperforms approaches using struc-   tured data and Transformer - based model without   pre - training .   In addition , we train our model on acute kidney   injury prediction task ( MIMIC - AKI ) following Li   et al . ( 2023 ) . Table 5 shows comparative results of   our TM - HGNN to Clinical - Longformer ( Li et al . ,   2023 ) that justify TM - HGNN can effectively utilize   high - order semantics from long clinical notes , with   much less computational burden compared to long   sequence transformer models.55705571ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section " Limitations "   /squareA2 . Did you discuss any potential risks of your work ?   Section " Ethics Statement "   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section " Introduction "   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4.1 , Section 4.2   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Section 4.1   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 4.2   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section " Ethics Statement "   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4.1   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4.1   C / squareDid you run computational experiments ?   Section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4.35572 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4.3   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5.1 , Section 5.2 , Section 5.3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3.2 , Section 4.1   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.5573