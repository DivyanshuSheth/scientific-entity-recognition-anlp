  Tianshu Wang , Faisal Ladhak , Esin Durmus , He HeNew York University , Columbia University , Stanford University   { tw2112 , hhe}@nyu.edu , faisal@cs.columbia.edu , esdurmus@stanford.edu   Abstract   Current abstractive summarization systems   tend to hallucinate content that is unfaithful   to the source document , posing a risk of mis-   information . To mitigate hallucination , we   must teach the model to distinguish halluci-   nated summaries from faithful ones . However ,   the commonly used maximum likelihood train-   ing does not disentangle factual errors from   other model errors . To address this issue ,   we propose a back - translation - style approach   to augment negative samples that mimic fac-   tual errors made by the model . Speciﬁcally ,   we train an elaboration model that generates   hallucinated documents given the reference   summaries , and then generates negative sum-   maries from the fake documents . We incorpo-   rate the negative samples into training through   a controlled generator , which produces faith-   ful / unfaithful summaries conditioned on the   control codes . Additionally , we ﬁnd that   adding textual entailment data through multi-   tasking further boosts the performance . Ex-   periments on three datasets ( XSum , GigaWord ,   and WikiHow ) show that our method consis-   tently improves faithfulness without sacriﬁc-   ing informativeness according to both human   and automatic evaluation .   1 Introduction   Despite the fast progress in ﬂuency and coherence   of text summarization systems , a common chal-   lenge is that the generated summaries are often   unfaithful to the source document , containing hal-   lucinated , non - factual content ( Cao et al . , 2018 ;   Falke et al . , 2019 , inter alia ) . Current summa-   rization models are usually trained by maximum   likelihood estimation ( MLE ) , where unfaithful and   faithful summaries are penalized equally if they   both deviate from the reference . As a result , whenFigure 1 : Overview of CoFE . The original and fabri-   cated document - summary pairs are shown in blue and   red respectively . The trained elaborator ﬁrst generates   fake documents from the summary . Then , the summa-   rizer generates summaries from the fake documents ,   which are likely to contain hallucinated information   ( underlined ) . A controlled generator is then trained to   produce the original ( faithful ) and the fabricated ( un-   faithful ) summaries depending on the control codes .   the model fails to imitate the reference , it is likely   to “ over - generalize ” and produce hallucinated con-   tent .   In this work , we address the issue by explicitly   teaching the model to discriminate between posi-   tive ( groundtruth ) and negative ( unfaithful ) sum-   maries . The key challenge is to generate realis-   tic negative samples . Existing work on negative   data augmentation mainly focuses on corrupting   the reference ( e.g. , replacing entities ) or sampling   low - probability model outputs ( Cao and Wang ,   2021 ; Kryscinski et al . , 2020 ; Kang and Hashimoto ,   2020 ) . However , the synthetic data often does   not resemble actual hallucinations from the model   ( Goyal and Durrett , 2021 ) and many methods rely   on external tools such as NER taggers .   To generate unfaithful summaries , we propose a   simple method inspired by back - translation ( Sen-   nrich et al . , 2016 ) ( Fig . 1 ) . Speciﬁcally , we ﬁrst   generate fake documents using an elaboration   model that is trained to produce a document given   the summary . We then generate summaries from11913the fake documents , which are assumed to be un-   faithful since they are likely to contain hallucinated   information in the fake documents . Given the refer-   ence summaries and the augmented negative sam-   ples , we train a controlled generation model that   generates either faithful or unfaithful summaries   conditioned on a faithfulness control code . At in-   ference time , we control the model to generate   only faithful summaries . We call our approach   CoFE ( Controlled Faithfulness via Elaboration ) .   The controlled generation framework allows us to   incorporate additional data easily : jointly training   on natural language inference ( NLI ) datasets to   generate entailed ( faithful ) and non - entailed ( un-   faithful ) hypotheses further improves the result .   We evaluate CoFE on three summarization   datasets : XSum ( Narayan et al . , 2018 ) , GigaWord   ( Graff et al . , 2003 ) , and WikiHow ( Koupaee and   Wang , 2018 ) . Both automatic metrics and human   evaluation show that our method consistently out-   performs previous methods in terms of faithfulness   and content similarity to the reference , without sac-   riﬁcing abstractiveness ( Ladhak et al . , 2022 ) .   2 Approach   To learn a summarization model , the commonly   used MLE aims to imitate the reference and does   not distinguish different types of errors , thus the   model may be misaligned with the desired behavior   in downstream applications . For example , a faith-   ful summary missing a detail would be preferred   over a summary with hallucinated details , even if   both have low likelihood under the data distribu-   tion . Therefore , additional inductive bias is needed   to specify what unfaithful summaries are . There-   fore , we augment negative examples and jointly   model the distributions of both faithful and unfaith-   ful summaries . At decoding time , we generate the   most likely faithful summary .   Negative data augmentation . The key chal-   lenge in generating negative summaries is to simu-   late actual model errors . Prior approaches largely   focus on named entities errors . However , differ-   ent domains exhibit diverse hallucination errors   ( Goyal and Durrett , 2021 ) ; in addition , certain do-   mains may not contain entities that can be easily   detected by off - the - shelf taggers ( e.g. , stories or in-   structions ) . Our key insight is that the reverse sum-   marization process — expanding a summary into a   document — requires the model to hallucinate de-   tails , thus provides a domain - general way to pro - duce unfaithful information . Instead of manipulat-   ing the reference summary directly , we expand it   into a fake document , and generate negative sum-   maries from it using the summarization model .   More formally , given a set of document-   summary pairs ( x , y ) , we train a backward elab-   oration model p(x|y)as well as a forward   summarization model p(y|x ) . Then , given   a reference summary y , we ﬁrst generate a fake   document ˆxfromp , then generate the negative   sampleyfrom ˆxusingp , forming a pair of   positive and negative samples ( x , y)and(x , y ) .   To avoid data leakage ( i.e. training models and gen-   erating summaries on the same data ) , we split the   training data into Kfolds ; the negative examples   in each fold are generated by elaboration and sum-   marization models trained on the rest K−1folds .   We useK= 5 in the experiments .   Controlled generation . Given the positive and   negative samples , we would like the model to learn   to discriminate faithful summaries from unfaithful   ones . Inspired by controlled generation methods   ( Keskar et al . , 2019 ) , we train the model to generate   faithful or unfaithful summaries conditioned on a   control code . In practice , we prepend a preﬁx at   the beginning of the document ( [ ENT ] for positive   examples and [ CON ] for negative examples ) . At in-   ference time , we always prepend [ ENT ] to generate   faithful summaries .   Training . Our training data consists of positive   examples ( i.e. the original dataset ) and gener-   ated negative samples , marked with different pre-   ﬁxes . LetL , Ldenote negative log - likelihood   ( NLL ) losses on the positive and negative examples .   We use a multitasking loss that is a weighted sum   of the two losses to balance the contribution from   different types of examples : L = L+λL.   Adding NLI datasets . We hypothesize that in-   corporating NLI data through multitasking would   transfer knowledge of entailment to the generator ,   helping it better model faithful and unfaithful sum-   maries . The NLI sentence pairs can be naturally   incorporated into controlled generation . Speciﬁ-   cally , given the premise as input , we generate en-   tailed and non - entailed hypotheses with control   codes [ ENT ] and[CON ] , respectively . With the   additional NLI data , The loss function becomes :   L = L+λL+λL , whereLde-   notes the NLL loss on the auxiliary NLI examples.119143 Experiments   Datasets . We evaluate our approach on 3   datasets , including : ( i ) XSum ( Narayan - Chen et al . ,   2019 ) , a dataset of BBC news articles paired with   one - sentence summaries ; ( ii ) GigaWord ( Rush   et al . , 2015 ) , a headline generation dataset com-   piled from the GigaWord corpus ( Graff et al . ,   2003 ) ; and ( iii ) WikiHow ( Koupaee and Wang ,   2018 ) , a dataset of how - to articles compiled from , each paired with paragraph headlines   as the summary . For the auxiliary NLI data , we   useSNLI ( Bowman et al . , 2015 ) and MultiNLI   ( Williams et al . , 2018 ) , both containing pairs of   premise and hypothesis sentences .   Baselines . We compare with three baselines : ( i )   maximum likelihood estimation ( MLE ) ; ( ii ) Loss   Truncation ( LT ) ( Kang and Hashimoto , 2020 ) that   adaptively removes high - loss examples , which are   assumed to be noisy / unfaithful ; and ( iii ) CLIFF   ( Cao and Wang , 2021 ) , a contrastive learning   method based on generated negative samples .   Implementation . All generation models ( includ-   ing the baselines ) are ﬁne - tuned BART - large mod-   els ( Lewis et al . , 2019 ) . We train all CoFE models   using Fairseq ( Ott et al . , 2019 ) with a learning rate   of 3e-5 . For decoding , we use beam search with   a beam size of 6 . We train the elaborators using   the same model and learning hyperparameters . We   generate one negative sample per document using   beam search except for WikiHow where we use   top-5sampling . To ensure that the negative sum-   maries are different from the references , we further   remove the top 10 % summaries ranked by their   edit distances to the reference . To train the con-   trolled generator , we set coefﬁcients ( λ , λ ) of   the loss terms such that the reweighted number of   examples in the original dataset , the negative sam-   ples , and optionally the NLI datasets have the ratio   1 : 0.5 : 0.5 . Details for other baselines are given   in Appendix B.   Metrics . A good summary must cover important   content , be faithful to the document , and be suc-   cinct . We evaluate the generated summaries from   the following aspects . ( 1 ) Content selection . Weuse similarity to the reference as a proxy mea-   sure , and report ROUGE ( Lin , 2004 ) and BertScore   ( Zhang et al . , 2020 ) . ( 2 ) Faithfulness . For auto-   matic evaluation , we use QuestEval ( Scialom et al . ,   2021 ) , a QA - based metric , which shows better cor-   relation with human judgment on system ranking   in our preliminary experiments . We perform hu-   man evaluation on 100randomly selected examples   from each dataset . Given a document with the gen-   erated summaries from all systems ( including the   references ) , we ask annotators from Amazon Me-   chanical Turk to evaluate whether each summary is   supported by the document . Each output is evalu-   ated by 3annotators . If two or more annotators vote   “ supported ” , then we consider the output faithful .   The evaluation interface is described in Appendix   A. ( 3 ) Extractiveness .?show that it is important   to measure the extractiveness of the summaries to   determine whether a method improves faithfulness   mainly by copying from the document . Therefore ,   we also report coverage anddensity that measure   the percentage of the words and the average length   of text spans copied from the document ( Grusky   et al . , 2018 ) .   Results . Table 1 shows our main results . CoFE   outperforms the baselines in human evaluated faith-   fulness accuracy on 2 out of the 3 datasets . On   GigaWord , LT performs the best but it also incurs   the largest drop in ROUGE and BertScore and more   copying . CLIFF is good at ﬁxing entity errors , but   it has less advantage on datasets like WikiHow that   contain fewer entities detectable by off - the - shelf   taggers . On average , CoFE is less extractive than   CLIFF and LT , indicating that our faithfulness im-   provements are not simply due to more copying .   Finally , we ﬁnd that adding NLI brings a marginal   improvement on top of our negative samples .   Are generated negative summaries really un-   faithful ? Our method relies on the assumption   that the elaboration of summaries introduces hal-   lucinations , which results in unfaithful summaries .   To verify this , we assess whether our generated neg-   ative samples are true negatives . Speciﬁcally , we   evaluate the faithfulness of the negative summaries   generated by our method and CLIFF on 100ran-   domly sampled documents from each dataset . In   Table 2 , we report the QuestEval scores and human-   annotated faithfulness scores ( following the same   procedure described in Metrics ) . As a sanity check ,   the faithfulness scores of the negative samples are11915   much lower than those in Table 1 , suggesting a   qualitative difference between the generated nega-   tive samples and the positive samples . Compared   to CLIFF , our method achieves lower QuestEval   and human - annotated faithfulness scores across all   datasets , suggesting that our negative samples are   more often unfaithful ( true negatives ) . Example   negative summaries are shown in the Appendix   ( Table 4 ) .   Ablation study . Our approach consists of two   key ingredients : negative data generated through   elaboration and controlled generation . To disen - tangle the effect of data and modeling , we report   the result of using our negative data in CLIFF ’s   contrastive learning framework and using CLIFF ’s   negative data to learn our controlled generator   ( CLIFF(CoFE data ) and CoFE ( CLIFF data ) in   Table 1 ) . Consider the QuestEval score , which   has a higher correlation with human - judged sys-   tem rankings . Using our model with CLIFF data ,   the performance is consistently lower than CoFE ,   but improves over CLIFF on XSum and WikiHow .   On the other hand , CLIFF with our data does not   outperform CLIFF except on GigaWord . A closer   inspection suggests that the contrastive learning   method used by CLIFF is sensitive to the num-   ber of negative examples , which may explain the   performance drop using CoFE data . In summary ,   CoFE achieves similar or better performance with   a smaller amount of high - quality negative samples .   Is faithfulness controllable ? We use the con-   trolled generator to model distributions of both   faithful and unfaithful summaries . To verify the   effect of the control code , we measure the change   in ROUGE scores on XSum after toggling the   control code from faithful ( [ ENT ] ) to unfaithful   ( [ CON ] ) . As expected , we observe that R1 / R2 drops11916from 45.26/22.19 to 37.29/15.82 , indicating that   the model has learned to discriminate faithful and   unfaithful summaries .   4 Related Work   Recent work in automated factuality metrics   ( Kryscinski et al . , 2020 ; Durmus et al . , 2020 ; Wang   et al . , 2020 ; Goyal and Durrett , 2020 ) has spurred   interests in building more faithful systems . Prior   work has tackled the problem from the aspects of   data , modeling , and learning .   Data . Since most summarization datasets are   scraped online , there may be unfaithful summaries   in the training data . Thus , one approach is to ﬁl-   ter the training data to remove noisy summaries or   tokens . For example , Kang and Hashimoto ( 2020 )   drop high - loss examples from training , observing   that these examples are usually of lower quality .   Nan et al . ( 2021 ) discard sentences from gold sum-   maries if there is an entity that does not match   the entities in the document . Goyal and Durrett   ( 2021 ) take a more ﬁne - grained approach , and use   a dependency arc - based entailment metric ( Goyal   and Durrett , 2020 ) to ﬁlter noisy tokens from the   summary .   Modeling . Another line of work aims to impose   prior on how the summary should be generated   through better modeling . Existing work has in-   corporated structural information from the docu-   ment , such as relation triplets ( Cao et al . , 2018 ) ,   knowledge graphs ( Zhu et al . , 2021 ) , and topics   ( Aralikatte et al . , 2021 ) to bias the summary .   Learning . Liu et al . ( 2022 ) uses a scoring model   to suppress low - quality candidates during training .   Liu and Liu ( 2021 ) ; Cao and Wang ( 2021 ) focus   on generating negative summaries like us , but they   use the contrastive learning framework to incorpo-   rate negative summaries into learning . Other work   ﬁxes faithfulness errors through a post - processing   step by revising the generated outputs ( Dong et al . ,   2020 ; Chen et al . , 2021 ; Zhao et al . , 2020 ; Cao   et al . , 2020 ) . Our generation model is also related   to Filippova ( 2020 ) , which learns a similar con-   trolled generator , but with negative data from the   training set .   5 Conclusion   We present CoFE , a data construction and training   pipeline to improve faithfulness of summarizationsystem . In the negative sample generation stage ,   fabricated details are generated through the elab-   orator , and some of them will be kept by summa-   rizor in negative samples . In the training stage ,   CoFE adopts the preﬁx - control framework , which   is designed to provide conditions through differ-   ent preﬁxes , so as to make the model distinguish   between unfaithfulness and faithfulness . Our ex-   periments show that by add NLI data into training ,   faithfulness can be further enhanced .   Limitations   While our approach is not language - speciﬁc , the   experiments are limited to English datasets , as cur-   rent automatic faithfulness metrics work best on   English data . Future work should experiment with   non - English data .   Compared to other data augmentation baselines ,   our approach requires ﬁnetuning ﬁve elaboration   models for each dataset to avoid overﬁtting to the   training set ; thus , it uses more computational re-   sources . This is the most time - consuming part of   our method . For example , for XSum , it takes 40   hours on one 32 GB V100 . Follow - up work may   consider a more efﬁcient implementation .   Acknowledgment   The authors thank Chenghao Yang and the anony-   mous reviewers for the discussions and feedback   on this work . This work is partly supported by the   Samsung Advanced Institute of Technology ( Next   Generation Deep Learning : From Pattern Recogni-   tion to AI ) and a gift from AWS AI .   References119171191811919A Human Evaluation Setup   We use Amazon Mechanical Turk as the human   evaluation platform . The prompt is shown in Fig .   2 . We only hire annotators in the US with an HIT   acceptance rate of more than 98 % .   B Experiment Detail   Model details . For both the summarization   model , the elaboration model , and the controlled   generator , we ﬁne - tune a pre - trained BART model   ( Lewis et al . , 2019 ) using Fairseq ( Ott et al . , 2019 )   and the default learning rate 3e−5 . All summaries   are generated using beam search with a beam size   of6 . Linear - scale the maximum update steps of the   learning rate scheduler according to the number of   samples in the training data .   For hyperparameters , we follow the setting   of ﬁne - tuning BART on XSum ( Lewis et al . ,   2019 ) , which uses 8 cards , update freq is 4 ,   total numupdates is 20000 . Linear scale the   max - update - step by extra number of negative   data and NLI data . For the weights of differenttasks , an intuitive idea is to ﬁx ” the ratio of the   product of the number of samples and their weights   for different tasks ” . We set Product :   Product : Product = 1 : 0.5 : 0.5 .   For example , if we have 1000 positive and 1000   negative samples in the training set , the weight of   positive data is 1 , the weight of negative data is 0.5 .   If we ﬁlter half of the negative samples and reduce   it to 500 samples , then the weight of two tasks is 1 .   Other baselines : For MLE , the BART reposi-   tory releases hyperparameters and checkpoints for   XSum . Based on the hyperparameters for XSum ,   we scale the max - update - step linearly according to   the size of training set of GigaWord and WikiHow .   For Loss - truncation , besides the hyperparameters   in MLE , there are some hyperparameters for the   loss function . We follow the settings in their pa-   per . For CLIFF , we only use ” SysLowCon ” as the   negative data augmentation method , which is the   best single method they claimed in the paper . They   release the checkpoints of XSum and hyperparame-   ters in their github repository . We only re - scale the   max - update - step .   Computational resources . CoFE on one dataset   requires training 11 models , including 10 models   to generate negative samples , since each fold needs   an elaborator and a summarizer . On a 4 RTX8000   GPU node , each model needs 2 hours to ﬁne - tune .   It takes 22 hours to get the ﬁnal output . BART - large   has 400 M parameters .   Number of generated samples . For XSum and   GigaWord , the threshold is the 0.1 quantile of edit-   ing distance . For WikiHow the quantile is set to 0.2 ,   because the distribution of editing distance concen-   trates around 0 , so we ﬁlter out more low - quality   negative samples .   Examples of generated negative samples . To   illustrate qualitatively the difference between   CLIFF and CoFE data , we show some generated   negative summaries in Table 4.1192011921