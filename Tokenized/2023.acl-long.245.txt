  Xinyu ZhuJunjie WangLin ZhangYuxiang Zhang   Yongfeng HuangRuyi GanJiaxing ZhangYujiu YangTsinghua UniversityWaseda UniversityIDEAThe Chinese University of Hong Kong   Abstract   Large - scale pre - trained language models   ( PLMs ) bring new opportunities to challeng-   ing problems , especially those that need   high - level intelligence , such as the math   word problem ( MWPs ) . However , directly   applying existing PLMs to MWPs can fail   as the generation process lacks sufﬁcient   supervision and thus lacks fast adaptivity as   humans . We notice that human reasoning   has a dual reasoning framework that consists   of an immediate reaction system ( system 1 )   and a delicate reasoning system ( system 2 ) ,   where the entire reasoning is determined by   their interaction . This inspires us to develop   a cooperative reasoning - induced PLM for   solving MWPs , called Cooperative Reasoning   ( CoRe ) , resulting in a human - like reasoning   architecture with system 1 as the generator   and system 2 as the veriﬁer . In our approach ,   the generator is responsible for generating   reasoning paths , and the veriﬁers are used to   supervise the evaluation in order to obtain re-   liable feedback for the generator . We evaluate   our CoRe framework on several mathematical   reasoning datasets and achieve decent im-   provement over state - of - the - art methods , up   to9.6%increase over best baselines .   1 Introduction   Addressing math problems is a hallmark of human   intelligence , which allows reasoning and adapting   from limited data . We want neural models to be   able to do the same , however , quick and ﬂexible   reasoning is challenging to current neural models   as they must possess a certain level of prior expe-   rience from a limited amount of new data while   avoiding overﬁtting . The rapid growth of large-   scale Pre - trained Language Models ( PLMs ) offersFigure 1 : Comparing our CoRe with popular methods   in mathematical logic reasoning tasks .   unprecedented potential for this issue , often rely-   ing on well - designed trigger prompts ( Wei et al . ,   2022c ; Li et al . , 2022 ; Brown et al . , 2020 ) . Al-   though appealing in terms of efﬁciency , its success   relies on memorizing patterns with a sufﬁciently   large number of parameters ( ≥100 billion ) ( Wei   et al . , 2022b ) , differentiating it from the fast adap-   tivity in the human reasoning process .   Active disciplines like neuroscience and cogni-   tive science attempt to uncover the mechanism of   human reasoning , and agree that our learning pro-   cess is governed by an interaction mechanism , of-   ten referred to as System 1 and System 2 ( Evans ,   2003 ; Kahneman , 2011 ) . In particular , System 1   offers fast responses like human instinct , and Sys-   tem 2 performs deliberate reasoning . Interactions   between them are important for adapting to a con-   tinuously changing environment . PLMs behave   more like System 1 , according to the above theory ,   and thus lack the generalization ability in reason-   ing ( Nye et al . , 2021 ) .4471In this work , we explore a new line of zero - shot   math problem reasoning , using a human reasoning-   alike framework with feedback in the solution gen-   eration loop as opposed to pure PLM - based meth-   ods , called Cooperative Reasoning ( CoRe ) . In-   tuitively , System 1 and System 2 are embodied   as generators and veriﬁers , respectively , and they   are deﬁned as follows : generators for generating   reasoning paths , and veriﬁers for supervising the   paths ’ evaluation . Speciﬁcally , we train a LM be-   yond the question - answer paradigm by integrat-   ing in - the - loop reasoning , i.e. , we let the LM out-   put both the answer and the corresponding reason-   ing process for a given question . Meanwhile , we   introduce two types of veriﬁers , including token-   level and sentence - level , allowing us to provide   feedback in the whole solution generation life-   cycle . Notice that the solution path is generated   by selecting candidate tokens with some proba-   bility so that it is tree - alike and much coincides   with the tree search process of Monte Carlo Tree   Search ( MCTS ) ( Kocsis and Szepesvári , 2006 ) .   With this in mind , the veriﬁers can score tokens   along the solution generation process from start to   end when using the MCTS . Therefore , we can use   the score to evaluate the quality of the generation   process during inferring before ﬁnalizing the solu-   tion , making timely feedback available for super-   vising the generation process . With this , the evalu-   ation goes beyond the quality of the ﬁnal result at   the granularity of each reasoning step , extending   the supervision from the solution level to the path   level . We combine the solution score and the per-   plexity of its corresponding reasoning path to en-   courage the overall training towards high - quality   augmented solutions while aligning with the reli-   able reasoning process , aiming to improve gener-   alization ability .   Our experimentally evaluate CoRe on multi-   ple mathematical reasoning datasets in both zero-   shot and ﬁne - tuning settings . CoRe consistently   achieves better performance than competing base-   lines . Notably , CoRe has up to 9.6%improve-   ments on MultiArith over SoTA baselines , which   are dozens of times larger than our model .   In summary , our contributions are as follows .   •We propose a novel reasoning method   for mathematical problem solving , called   Cooperative Reasoning ( CoRe ) , that intro-   duces feedback in the loop during solution   generation as opposed to the sequential learn - ing process in the previous ones , resulting in   the ﬁrst method for this task that builds on   top of the learning mechanism in the human   brain .   •We develop a self - thinking strategy for fur-   ther boosting reasoning ability with gener-   ated data from the cooperation between Sys-   tem 1 and System 2 .   •We demonstrate the superiority of CoRe com-   paring to other zero - shot and ﬁne - tuning   methods , which has 9.6%improvements on   MultiArith over SoTA baselines .   2 Related Work   2.1 Dual Process System   Dual - process theory ( Evans , 2003 ; Kahneman ,   2011 ) argues there are two cognitive systems un-   derpinning human reasoning : System 1 and Sys-   tem 2 . The purpose of clarifying these systems is   that they have the potential to help us construct   artiﬁcial intelligence systems that beneﬁt from hu-   man ﬂexibility and methodical generalization .   Dual process system model guidance is not new .   Nye et al . ( 2021 ) simulated Systems 1 and 2 to   improve consistency and coherence of neural net-   works . Similar to several studies Cobbe et al .   ( 2021 ) ; Li et al . ( 2022 ) ; Scialom et al . ( 2021 ) , in   addition to System 1 for the generation , we de-   velop a distinct model as System 2 , called Veri-   ﬁer . The Veriﬁer checks the feasibility and correct-   ness of the generator ’s content and collaboratively   solves the reasoning task together .   2.2 Multi - step Reasoning   Many works exploit the multi - step reasoning abil-   ity of language models . Cobbe et al . ( 2021 )   showed that training a veriﬁer to score the solu-   tions generated by a ﬁne - tuned GPT-3 could im-   prove the performance compared to solely ﬁne-   tuning a GPT-3 . Nye et al . ( 2022 ) discovered that   asking the language model to write the intermedi-   ate process could achieve better results on various   NLP tasks . Likewise , Chain - of - Thought ( CoT )   prompts ( Wei et al . , 2022c ) prepended exemplars   with intermediate reasoning steps as prompts and   achieved SoTA on several reasoning benchmarks   by using large - scale PLMs . Wang et al . ( 2022 ) fur-   ther boosted CoT ’s performance by sampling a   bunch of possible solutions and then obtained the   ﬁnal answer by majority voting . DIVERSE ( Li   et al . , 2022 ) proved diverse CoT prompts and an4472extra veriﬁer were both helpful for PLMs to solve   reasoning problems . Kojima et al . ( 2022 ) found   that by simply adding “ Let ’s think step by step ”   after the question . PLMs could successfully step   by step solve the problems , called Zero - shot - CoT.   These above methods rely on extremely large   language models , resulting in high computa-   tional cost and time - consuming . Moreover , sev-   eral works ( Wei et al . , 2022c ; Kojima et al . , 2022 )   point out that neither CoT nor Zero - shot - CoT is   helpful to smaller models . While our method does   not necessarily require extremely large PLMs and   can work with models with different size scales ,   thus reducing computational cost and inference   time . Our approach has competitive zero - shot per-   formance thanks to the efﬁcient and collaborative   application of a dual - process system .   3 Cooperative Reasoning   In this section , we will present the proposed co-   operative reasoning framework , CoRe , that en-   forces System 1 and System 2 mutually cooperat-   ing , which includes 3sequential steps : cooperative   training , cooperative inference , and self - thinking .   3.1 Preparation   As discussed in Sec . 1 , we expect a PLM ( G ) to   fast generate multiple reasoning paths like System   1 . Then , considering that System 2 is responsible   for deliberate evaluations of the reasoning paths ,   we employ two modules : a step veriﬁer ( V ) for   reasoning steps , and a path veriﬁer ( V ) for rea-   soning paths .   3.2 Cooperative Training   Before applying System 1&2 to inference , a crit-   ical issue for them is learn how to generate rea-   soning paths and evaluate reasoning steps / paths .   Inspired by a widely - used training strategy for rea-   soners ( Cobbe et al . , 2021 ) , we present a cooper-   ative training method as shown in Fig . 2Step 1 .   Moreover , we discuss hyper - parameter conﬁgura-   tions and extra training details in Appendix B.1   and Appendix B.2 .   Step 1.1 : We ﬁrst ﬁne - tune Gon a dataset D=   { ( q , p , gt)}consisting of Nsamples . Each   sample xis composed of a question q , a reason-   ing path pand a ground truth answer gt . We ﬁne-   tuenGwith standard language modeling objectiveLas Eq . ( 1 ) .   Step 1.2 : Once Ghas learned how to generate   solutions , we employ it on questions qfrom D.   As a result , we obtain a new dataset D= { (   q , rp , a)}with Mgenerated rea-   soning paths ( rp ) and answers ( a ) for each q.   Step 1.3 : Different from the popular methods , we   train two veriﬁers to model human reasoning pro-   cedure with deliberate analysis for each step and   the whole path . To evaluate several reasoning steps   in a path , we desire a token - level scorer , which   is named step veriﬁer V. Therefore , we ﬁne-   tune a PLM with two tasks jointly : 1 ) the language   modeling task mentioned before ; 2 ) the veriﬁca-   tion task to predict a score for each token in the   solution . The veriﬁcation loss Lis calculated   as the Mean Squared Error ( MSE ) of the predicted   score with respect to the label as follows :   where , ( rp , a)from Dandgtwith same qfrom   D.   On the other hand , we need a path - level scorer   for reasoning paths . Different from step veriﬁer ,   we simply extract an overall presentation of the   reasoning path for prediction . Speciﬁcally , we em-   ploy a BERT - like model and take the [ CLS ] token   to calculate MSE loss Lsimilar to L.   In summary , the overall training objective for   veriﬁers is given by :   3.3 Cooperative Inference   After obtaining a generator and two veriﬁers , we   propose cooperative inference to generate solu-   tions for unseen questions . Instead of treating veri-   ﬁers as voters , we argue that veriﬁers should offer   appropriate guidance and feedback during the rea-   soning process . Therefore , we integrate a cooper-   ative search algorithm . In particular , we adopt the   popular Monte Carlo Tree Search ( MCTS ) ( Kocsis   and Szepesvári , 2006 ) to enable controlled reason-   ing . The cooperative inference starts from the root   node , which preserves question tokens . We detail   the cooperative inference process as follows .   Selection . If the current node has children , with   50 % probability , we select a node from its children4473   with the modiﬁed PUCT formula ( Czech et al . ,   2021 ) as Eq . ( 4 ) ,   where the state srepresents the sequence consist-   ing of all tokens in the current search path . And ,   N(s , n)means the times that node nhas been   selected in state s. Reward R(n)records all the   scores received from the backup . We perform se-   lection again with the selected node as the current   node . Otherwise , we perform expansion once and   choose the returned new node as current node .   Expansion . During expansion , the generator is re-   quired to generate a sequence of tokens based on   the current state . A new node is created to store the   generated tokens and added to the current node ’s   children . Then , Vevaluates the current reason-   ing path and predict a score score . Finally , the   new node is returned .   Roll - Out . After selection and expansion , we start   from the current node and let the generator com-   plete the reasoning path until it meets [ EOS ] to-   ken or reaches the max token length limit . Next ,   Vevaluates the whole reasoning path and pro-   duces a score score . Remember that Valso   provides a score scoreduring the expansion .   Therefore to leverage both scores , we introduce a   hyper - parameter αto adjust their contributions to   the node ’s reward ,   where sis the ﬁnal score that each node receives   by the backup .   Backup . We update the rewards back from the cur-   rent node to the root node . The scores producedAlgorithm 1 Self - Thinking   Input : Generator G ; Step veriﬁer V ; Path ver-   iﬁerV ; Dataset D.Combine generator and veriﬁers with a coop-   erative search algorithm.repeat Generate a new dataset Dfrom input   questions . Filter D. Merge DwithDin Step 1 . Do Step 1 . Do Step 2.until performance is saturated .   by veriﬁers are added to R(n)and the visited time   N(s , n)is increased by 1 .   3.4 Self - Thinking   It is challenging to ﬁne - tune models on the data   synthesized by themselves , which indicates they   have to be very conﬁdent in the content they gen-   erate . A proper self - training method can enhance   the robustness of the whole system and allow deep   data mining . Therefore , we introduce self - thinking   as described in Fig . 2Step 3 and Algorithm 1 . Con-   sidering the noise contained in generated data , we   build a ﬁlter by using scores from veriﬁers and per-   plexity ( PPL ) from the generator . In detail , we se-   lect high - quality reasoning paths by setting a score   threshold . Moreover , we only keep the reasoning   paths with no higher PPL than the ground truth   solutions . After ﬁltering , we merge Dwith D   and send it to Step 1 . Once the several iterations   are completed , we obtain a powerful System 1&2 .   More details can be found in Appendix B.3.44743.5 Zero - shot Inference   We simply perform cooperative inference as Fig . 2   Step 2 with trained System 1&2 on unseen   datasets . After obtaining several reasoning paths   with scores , we arrive at the ﬁnal answer by   weighted voting based on scores following ( Li   et al . , 2022 ) .   4 Experiments   4.1 Experimental Setup   4.1.1 Datasets   We consider several widely - used math word prob-   lem datasets : GSM8 K ( Cobbe et al . , 2021 ) ,   ASDiv - A ( Miao et al . , 2020 ) , SingleOp ( Roy et al . ,   2015 ) , SinlgeEq ( Koncel - Kedziorski et al . , 2015 )   and MultiArith ( Roy and Roth , 2015 ) . ( Details   in Appendix A ) . Following the general setting as   in ( Kojima et al . , 2022 ; Wei et al . , 2022c ) , we   employ accuracy as the evaluation metric for all   datasets .   4.1.2 Baselines   For comparison under the zero - shot setting , the re-   sults of Instruct GPT-3 ( 175B ) and PaLM ( 540B )   with their various methods are from Kojima et al .   ( 2022 ) . The zero - shotand zero - shot - CoTim-   ply not the standard prompt ( see details in Ap-   pendix B.4 ) . We also provide our generator as a   baseline when compared to previous ﬁne - tuning   methods . Regarding to sampling multiple solu-   tions , we search 40paths with the same setting as   Self - Consistency ( Wang et al . , 2022 ) .   For GSM8 K , we select various powerful PLMs   enhanced by the chain of thought prompt as base-   lines , including LaMDA ( 137B ) ( Thoppilan et al . ,   2022 ) , GPT-3 ( 175B ) ( Brown et al . , 2020 ) and   PaLM ( 540B ) ( Chowdhery et al . , 2022 ) . Except   for the few - shot methods , we also include a ﬁne-   tuned baseline that applies two GPT-3 ( 175B ) , one   as the generator and the other as veriﬁer ( Cobbe   et al . , 2021 ) .   4.1.3 Implementation Details   Since cooperative training requires a high-   quality dataset with reasoning paths , we treat   GSM8 K ( Cobbe et al . , 2021 ) as the seed dataset   Din Sec . 3.2 . Unless otherwise , we employ GPT-   J ( Wang and Komatsuzaki , 2021 ) as the generator   and the step veriﬁer , DeBERTa - large ( He et al . ,   2021 ) as the path veriﬁer . Since the default set-   ting consists of two GPT - J ( 6B ) and a DeBERTa-   large ( 0.4B ) , we note our backbone as “ GPT - J   12B ” , which implies around 12.4 billion parame-   ters in total . During generation , we apply calcula-   tor as assistant following Cobbe et al . ( 2021 ) . We   run all the experiments for 3 times and report the   best result , detailed hyper - parameters setting can   be found in Appendix B.1 . Our zero - shot setting   is similar to the transferring setting in T0 ( Sanh   et al . ,2022 ) and FLAN ( Wei et al . , 2022a ) . All the   training and testing procedures are done on a DGX   station with 8A100 GPUs .   4.2 Main Results   4.2.1 Zero - shot Results   Table 1presents main results on two mathemat-   ical reasoning datasets , demonstrating the zero-   shot generalization ability . CoRe achieves supe-   rior performance on both datasets , demonstrating   its capability of mathematical reasoning on unseen   datasets . Note that the baselines are several dozen   times larger than ours and still underperform our   model . The improvement might be explained by   two potential reasons . One is that applying the   CoRe framework on PLMs can activate their rea-   soning ability , even though their scales are small   ( ≤100B ) . Another one is that self - thinking can   provide valuable self - produced data to teach Sys-   tems 1&2 . Therefore , the results present the effec-   tiveness of cooperative working with System 1&2   and self - thinking .   4.2.2 Zero - shot v.s. Fine - tuning   We compare CoRe with previous ﬁne - tuned SoTA   baselines on four datasets , and results are pre-   sented in Table 2 . To show the importance of co-   operative reasoning , we apply our generator as a   baseline . The results demonstrate that without any   guidance generator underperforms previous meth-   ods on most datasets . Despite the gain from self-   consistency , it still lags behind other ﬁne - tuned So-   TAs . While after applying our method CoRe , it sur-4475   passes previous ﬁne - tuned SoTAs on all datasets in   a zero - shot setting . The results clearly demonstrate   the capability of CoRe to greatly boost PLMs ’ rea-   soning ability .   4.2.3 GSM8 K Results   Beyond improvements on zero - shot results , we ob-   serve that the ﬁne - tuning setting can beneﬁt a lot   from our CoRe framework , as shown in Table 3 .   Compared to previous ﬁne - tuned SoTA ( Cobbe   et al . , 2021 ) ( GPT-3 350B ) , CoRe outperforms it   with much fewer parameters , computation and in-   ference time . Note that it samples 100solutions   for each question while we only search 40paths .   For a comprehensive comparison , we include   few - shot results with large - scale PLMs due to a   limited number of “ ﬁne - tune ” competitors . With   regard to few - shot methods applied on large - scale   PLMs ( ≥100B parameters ) , CoRe only under-   performs PaLM-540B strengthened by chain of   thought prompt and self - consistency , further prov-   ing the effectiveness of our method .   4.3 Ablation Study   4.3.1 Is guidance important during path   searching reasoning ?   We argued that it is important to introduce guid-   ance in the loop during reasoning path searching .   To validate this argument , we adjust the weight   of reward provided by veriﬁers during reasoning .   The experiments are conducted using models with-   out self - thinking . Table 4summarizes the perfor-   mance on zero - shot datasets with different set-   tings of guidance . For “ w/o veriﬁers ” , the solu-   tions are predicted by a generator only and ap-   plied with “ Self - Consistency ” . As demonstrated   in Table 4 , guidance from Vcan provide per-   formance gains on SingleOp , with a 20.6%ab-   solute improvement . We further incorporate the   guidance from the step - level veriﬁer V. As de-   scribed in Eq . ( 5 ) , increasing the weight of reward   ( α ) from V , CoRe achieves a higher accuracy   on both SingleOp and MultiArith . Thanks to the   feedback and guidance during the reasoning stage ,   the generator tends to explore more often on a path   with a higher reward score . As a result , CoRe in-   creases the accuracy on SingleOP from 59.6%to   82.9%and MultiArith from 92.3%to96.8 % .   4.3.2 How much does self - thinking boost the   reasoning ability of a language model ?   To examine the effect of self - thinking , we explore   it along with two axes : 1 ) the number of itera-   tions and 2 ) the type of search strategy . Since we4476   apply the self - thinking procedure on the GSM8 K   dataset , we investigate the performance of mod-   els under different settings on GSM8 K , as shown   in Table 5 . First , increasing the number of itera-   tions can always improve the performance for both   greedy decode and self - consistency . Our CoRe   reaches saturation in one round , which might be   attributed to the fact that System 1&2 learns better   and faster on self - generated data by collaborative   working . Second , regardless of the search strategy ,   self - thinking consistently boost the model ’s perfor-   mance , which veriﬁes that self - thinking boost lan-   guage model ’s reasoning ability .   4.3.3 Do self - thinking generalize to other   datasets ?   We have performed self - thinking on GSM8 K and   proved that it improves the model ’s reasoning abil-   ity in 4.3.2 . Furthermore , we explore whether the   improvement on GSM8 K comes at the cost of per-   formance degradation on other datasets , i.e. the   model overﬁts the dataset . As presented in Table 6 ,   we vary the number of self - thinking iterations for   the generator and veriﬁers respectively and pro-   vide results on SingleOp and MultiArith . The re-   sults show that the performance of the generator   suffers a little , but veriﬁers can eliminate this un-   desirable effect and beneﬁt a lot from self - thinking .   The best results are obtained when only the ver-   iﬁers are further ﬁne - tuned , with the 2.3%and   0.7%absolute improvement on the two datasets   respectively . This observation implies that we can   economize on training costs and time where target   datasets are not included in self - thinking data .   4.3.4 How performance varies as the number   of search iterations for different search   strategies changes ?   As shown in Fig . 3 , accuracy on 4datasets consis-   tently increases along with the growth of search   iterations for both search strategies . However , the   scaling curves of self - consistency and CoRe are   quite different . The performance gain quickly sat-   urates with self - consistency . Sampling 40paths   can not further improve the accuracy , while the   scaling curve of CoRe is much sharper . Due to   the heuristic algorithm that requires the model to   continue exploring on the previously generated   paths , CoRe starts from a relatively lower level in   the beginning , whereas the accuracy quickly im-   proves as the number of search iterations increases .   The result demonstrates the effectiveness of CoRe   in searching reasoning paths , with a fast growth   curve and a slow saturation rate .   4.4 Case studies   4.4.1 Improvements from CoRe   A typical exemplar from GSM8 K is presented   in Table 7 . Greedy decode fails to ﬁnd a reasonable   path due to the limited exploration in the output   space . In contrast , self - consistency samples multi-   ple reasoning paths randomly , resulting in a richer   candidate set . Although it ﬁnds some right solu-4477   tions occasionally , without any guidance , it fails to   explore more frequently on the high - quality paths ,   thus ending up with a wrong answer obtained by   majority voting as shown in the fourth row .   As a comparison , results generated by CoRe are   listed with their scores . Similar to random sam-   pling , the reasoning paths might be partially illog-   ical , even though the ﬁnal answers happen to be   correct . Despite this challenge , CoRe is capable   of distinguishing those poor - quality paths from the   superior ones thanks to the veriﬁers . Adhering to   the philosophy of cooperative reasoning we have   emphasized , the veriﬁers managed to harness the   generator throughout the reasoning procedure with   the help of MCTS . Therefore , CoRe enjoys not   only the advantage of having a diverse candidate   set , but also the merit of being wiser and efﬁcient   during reasoning path searching .   4.4.2 Improvements from Self - Thinking   Table 8shows an example that the vanilla model   failed to solve the given question , whereas after   the self - thinking , the model rectiﬁed the faulty   parts and successfully addressed it . This displays   that self - thinking boosts language models ’ inner   reasoning ability regardless of the search strategy ,   which is also proved in Sec . 4.3.2 .5 Discussion   Although we only ﬁne - tune the language model   on GSM8 K due to the scarcity of QA datasets an-   notated with intermediate rationales , zero - shot re-   sults on several arithmetic datasets prove that basic   reasoning capability is transferable across datasets   within the same domain . This observation implies   that when it comes to a new domain , we only need   to collect a limited number of question - answer   pairs with reasoning paths , model ’s reasoning abil-   ity can generalize to other unseen datasets and can   be further strengthened by our approach CoRe ac-   cording to the experimental results .   6 Conclusions   In this work , we mimic the dual system of human   cognition to develop an effective reasoning frame-   work for solving the math word problems . The pro-   posed approach is consisting of two ingredients :   the generator as System 1 and the veriﬁers as Sys-   tem 2 , and overall reasoning is conducted based   on their mutual reinforcement . From the robust-   ness and generalization aspects , CoRe activates su-   perior reasoning ability of LMs , and thus outper-   forms PLMs that are dozens of times larger.4478Limitations   The outcome on multiple datasets veriﬁes the pow-   erful reasoning ability , which even works on mod-   els with only several billion parameters . How-   ever , our self - thinking procedure utilizes only one   dataset , GSM8 K , and the available training set   size is only 7.5K. The main reason is the scarcity   of high - quality datasets with rich reasoning paths .   And , collecting such data incurs huge computation   costs and expensive human resources . Another   limitation is that we have not conducted experi-   ments on bigger language models , such as GPT-3   and PaLM , due to the expensive usage costs and   the fact of no open - source codes . In a nutshell , in   the future , we will focus on collecting more high-   quality labeled data and exploring our method on   more powerful language models .   Ethics Statement   In this work , our CoRe shows impressive rea-   soning capability , however , it also comes with   social risks . Here , we summarize three possible   ethical impacts : i ) PLMs with bias , ii ) gener-   ated data with social stereotypes and iii ) prob-   lematic data environments . Considering utilizing   PLMs as backbones , several works present var-   ious potential risks in PLMs ( Lucy and Bam-   man,2021 ; Amin and Kabir , 2022 ) . Fortunately ,   our method supports the replacement of different   PLMs . Therefore , we encourage deploying some   risk - free PLMs , expecting to reduce the potential   ethical risks . Furthermore , once deploying harm-   ful PLMs , the self - thinking process might gener-   ate several undesired data and those data are fed   into language models , which deepens the bias and   causes unintended social impacts . For reducing the   aforementioned cases , we suggest recording gener-   ated sentences . In real - world applications , a good   choice is to monitor generated content and then   hand them over for human review . In addition to   the two risks posed by PLMs , the data in down-   stream tasks is of great concern . In particular , pri-   vate data might cause unpredictable inﬂuence be-   cause of their nature as a non - open source . There-   fore , we believe that a data cleaning workﬂow is   necessary to mitigate potential risks , such as Pri-   vateClean ( Krishnan et al . , 2016 ) . Finally , we en-   courage open debating about its utilization for in-   creasing transparency and reducing the potential   for misuse . Acknowledgements   This work was partly supported by the National   Key Research and Development Program of China   ( No . 2020YFB1708200 ) , the " Graph Neural Net-   work Project " of Ping An Technology ( Shenzhen )   Co. , Ltd. and the Shenzhen Science and Technol-   ogy Program ( JCYJ20220818101001004 ) .   References447944804481A Dataset Details   The mathematical reasoning datasets with details   are as follows ( Detailed description of the statis-   tics in Table 9 ) . We follow the licenses for their   papers .   The dataset in ﬁne - tuning :   GSM8 K ( Cobbe et al . , 2021)is a high - quality   dataset with reasoning paths . It consists of 8.8 K   grade school math problems created by human   writers , which are divided into a train set ( 7.5 K )   and a test set ( 1.3 K ) . The reasoning paths include   2to8steps with considering basic arithmetic oper-   ations . Furthermore , we conduct cooperative train-   ing and self - thinking on its training set .   The datasets in zero - shot inference :   ASDiv - A ( Miao et al . , 2020)includes diverse   math word problems , which are required to answer   a number for each question .   SingleOP ( Roy et al . , 2015)is proposed with ele-   mentary math problems of a single operation .   SingleEq ( Koncel - Kedziorski et al . , 2015)is con-   strued with both single - step and multi - step math   problems from mixed sources .   MultiArith ( Roy and Roth , 2015)includes ele-   mentary math problems with multiple steps .   B Experimental Settings   B.1 Hyper - parameters Setting   For the generator and the step veriﬁer , we train   them for two epochs . The batch size is set to 16 .   The learning rate ( LR ) is set to 1e−5at the ﬁrst   epoch and 1e−6at the second epoch for generator .   On the hand of step veriﬁer we apply the warmup   method then linearly decaying scheduler , LR is set   to1e−6and warmup ratio is 0.1 .   For the path veriﬁer , we train it for three epochs   with batch size set to 128and LR set to 1e−5 .   Same LR scheduler as the step veriﬁer has been   applied for the path veriﬁer . We set the gradient   clip norm to 1.0and the sampling temperature to   0.7 . The random seed is set to 19990303 through-   out the training process .   For MCTS , we set max search iterations to 40   during inference . In expansion , we search 20to-   kens each time . In order to avoid expanding too   many homogeneous children for the same node ,   we simply penalize the probability of ﬁrst token   if it has appeared in other child nodes . We set the   max token number to 300 in roll out and limit the   total token number of reasoning path to 400 .   B.2 Details of Training Veriﬁers   Before two veriﬁers are ﬁne - tuned , we utilize the   generator to sample 100 solutions for each ques-   tion following Cobbe et al . ( 2021 ) . Then we train   the two veriﬁers on the generated data as described   in Sec . 3.2Step 1.3 .   B.3 Details of Self - Thinking   In each iteration of self - thinking , we initialize the   model with the weights obtained from the previ-   ous round so as to save the computational costs .   Since we use cooperative inference rather than ran-   dom sampling to generate data for further training ,   solutions are expected more high - quality . Thus ,   the number of generated solutions Mmentioned   in Sec . 3.2is set to 50for saving computational   cost and time . Due to the ﬂexibility of MCTS , we   have also tried to limit the time for searching rather   than the number of iterations , which makes the to-   tal search time controllable and predictable . More-   over , this allows the model to adaptively adjust   the ﬁnal number of solutions searched for each   question , due to the different levels of difﬁculty   in questions . In our experiments , we observe that   setting the time limit to 320seconds provides bet-   ter results than setting the iteration limit to 50 ,   while maintaining approximately the same time   consumption . Therefore , we use time control to   generate data during self - thinking .   B.4 Baseline Settings   As shown in Table 1 , the Instruct GPT-3 is based   on text - davinci-002 version . Moreover , since Ko-   jima et al . ( 2022 ) provides difference prompt set-   ting , we list them in Table 10 . For few - shot scenar-   ios with the chain of thought prompts , we follow   the original paper ( Wei et al . , 2022c ) .   C Extended Experiments   This section we replicate the work of Cobbe et al .   ( 2021 ) with GPT - J and report the results in Ta-   ble11for comprehensive comparison . CoRe fully   surpasses Cobbe et al . ( 2021 ) when the number of4482   reasoning paths reaches 30 and maintains a faster   increasing rate after that . As a result , CoRe has   a superior performance over Cobbe et al . ( 2021 )   on all the datasets and achieves a 9.1%and8.3 %   improvement compared to it on ASDiv - A and Sin-   gleOp .   D Future Work   We focus on measuring our method in boosting the   language model ’s arithmetic reasoning ability in   this work . Nevertheless , we believe that our frame-   work can also be applied to other reasoning tasks   seamlessly , e.g. , commonsense reasoning and sym-   bolic reasoning . We choose arithmetic reasoning   because it is the fundamental type of reasoning   task . Additionally , we believe solving arithmetic   reasoning is the ﬁrst step toward a general cogni-   tive reasoning system . In the future , we will ex-   plore other reasoning tasks and put more effort   into low - resource scenarios.4483ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   After conclusion section   /squareA2 . Did you discuss any potential risks of your work ?   After limitations section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   4.1   /squareB1 . Did you cite the creators of artifacts you used ?   4.1   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix A   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Ethics Statement   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Ethics Statement   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Appendix A   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   4.4   C / squareDid you run computational experiments ?   4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   4.14484 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   4 , Appendix A   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   4.1   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.4485