  El Moatez Billah NagoudiAbdelRahim ElmadanyMuhammad Abdul - Mageed   Deep Learning and Natural Language Processing Group   The University of British Columbia   Abstract   Transfer learning with a uniﬁed Transformer frame-   work ( T5 ) that converts all language problems into   a text - to - text format was recently proposed as a   simple and effective transfer learning approach .   Although a multilingual version of the T5 model   ( mT5 ) was also introduced , it is not clear how well   it can fare on non - English tasks involving diverse   data . To investigate this question , we apply mT5 on   a language with a wide variety of dialects – Arabic .   For evaluation , we introduce a novel benchmark   forARabic language GEN eration ( ARGEN ) ,   covering seven important tasks . For model   comparison , we pre - train three powerful Arabic   T5 - style models and evaluate them on ARGEN .   Although pre - trained with 49 % less data , our   new models perform signiﬁcantly better than mT5   onallARGEN tasks ( in 52out of 59test sets ) and   set several new SOTAs . Our models also establish   new SOTA on the recently - proposed , large Arabic   language understanding evaluation benchmark   ARLUE ( Abdul - Mageed et al . , 2021 ) . Our models   are publicly available . We also link to individual   ARGEN datasets through our public repository .   1 Introduction   Due to their remarkable ability to transfer knowl-   edge from unlabeled data to downstream tasks ,   pre - trained Transformer - based language models   have emerged as important components of mod-   ern natural language processing ( NLP ) systems . In   particular , the uniﬁed framework that converts all   text - based language problems into a text - to - text for-   mat presented through the T5 model ( Raffel et al . ,   2019 ) is attractive . In addition to its simplicity ,   this approach is effective since it allows knowledge   transfer from high - resource to low - resource tasksFigure 1 :   without the need for changing model architecture .   Unlike models such as BERT ( Devlin et al . , 2019 ) ,   which are based on encoders only , the T5 model   is an encoder - decoder that can naturally be em-   ployed for natural language generation . Although   the T5 model , originally pre - trained for English ,   was recently extended to the multilingual setting as   mT5 ( Xue et al . , 2020 ) , it is not clear how suited   it is to individual languages ( and varieties of these   languages ) . In addition , systematic issues have   been discovered in multilingual corpora on which   language models have been trained ( Kreutzer et al . ,   2021 ) . In absence of comparisons with monolin-   gual pre - trained language models that serve differ-   ent non - English contexts , it remains unknown how   multilingual models really fare against language-   speciﬁc models .   In this work , we offer the ﬁrst comparison of the   mT5 model to similar encoder - decoder models ded-   icated to Arabic . We choose Arabic as our context   due to its large set of diverse varieties as well as its   wide use on social media . Our work aims at uncov-   ering the extent to which mT5 can serve Arabic ’s   different varieties . Our work also meets an existing   need for pre - trained Transformer - based sequence-   to - sequence models . In other words , while sev-   eral BERT - based models have been pre - trained for   Arabic ( Antoun et al . , 2020 ; Abdul - Mageed et al . ,6282021 ; Inoue et al . , 2021 ) , no such attempts have   been made to create sequence - to - sequence models   that we know of . Another motivation for our work   is absence of an evaluation benchmark for Arabic   language generation tasks . Apart from machine   translation where researchers are starting to pro-   pose benchmarks such as AraBench ( Sajjad et al . ,   2020 ) , there are no benchmarks that can be used   to methodically measure Arabic natural language   generation performance .   Our main contributions are as follows : ( 1)We   introduce three powerful variants of the text - to - text   transformer ( T5 ) model dedicated to Modern Stan-   dard Arabic ( MSA ) and a diverse set of Arabic   dialects . We include in our vocabulary 11lan-   guages other than Arabic ( e.g. , English , French ,   German , Russian ) , which also allows us to evaluate   our models under zero - shot pre - training conditions   involving these languages . ( 2)We propose a novel   uniﬁed benchmark for ARabic natural language   GEeneration ( ARGEN ) composed of seven tasks :   machine translation , code - switched text translation ,   summarization , news title generation , question gen-   eration , paraphrasing , and transliteration . ARGEN   is collected from a total of 19datasets , including   9new datasets proposed in this work . ( 3)To show   the utility of our new models , we evaluate them on   ARGEN under both fullandzero - shot pre - training   conditions . Our models set new SOTA on the ma-   jority of datasets in allseven tasks . ( 4)Although   the main focus of our work is language generation ,   we also show the effectiveness of our models on   Arabic language understanding by ﬁne - tuning our   new models on a large , recently proposed Arabic   language understanding benchmark . Again , our   models establish new SOTA on the majority of lan-   guage understanding tasks .   The rest of the paper is organized as follows :   Section 2 describes our Arabic pre - tained models .   In Section 3 , we introduce ARGEN , our new natu-   ral language generation benchmark . We evaluate   our models on ARGEN in Section 4 . Section 5 is   an analysis and discussion of our results . In Sec-   tion 6 , we provide an overview of related work . We   conclude in Section 7 . We now introduce our new   pre - trained models .   2 Our Models   2.1 Pre - Training Data   MSA Data . We use 70 GB of MSA text   ( 7:1B tokens ) from the following sources : AraNews ( Nagoudi et al . , 2020 ) , El - Khair El - Khair   ( 2016 ) , Gigaword , , OSCAR ( Suárez et al . , 2019 ) ,   OSIAN ( Zeroual et al . , 2019 ) , Wikipedia Arabic ,   and Hindawi Books .   Twitter Data . We randomly sample 1:5B Arabic   tweets ( 178 GB ) from a large in - house dataset of   10B tweets . We use string matching to only   include tweets with at least 3Arabic words , regard-   less whether the tweet has non - Arabic string or   not .   Our combined MSA and Twitter data make up   29B tokens , and hence is 49 % less than Arabic   tokens on which mT5 is pre - trained ( 57B Arabic   tokens ) . More information about our pre - training   data is in Table 1 .   MSA Vs . Dialect Distribution . In order to ana-   lyze MSA - dialect distribution in our Twitter data ,   we run the binary ( MSA - dialect ) classiﬁer intro-   duced in Abdul - Mageed et al . ( 2020b ) on a random   sample of 100 M tweets . We ﬁnd the data to in-   volve 28:39 % predicted dialect tweets and 71:61 %   predicted MSA . We also acquire country - level di-   alect labels using an in - house strong classiﬁer on   the dialectal portion of the data ( i.e. , 28:39mil-   lions tweets ) , ﬁnding dialectal tweets to be truly   geographically diverse as shown in Figure 2 .   Naturally - Occurring Code - Switching . Using   1 M random tweets from our data , we perform an   analysis of code - switching . For this , we employ   simple string matching to identify Arabic and run   the CLD3 language ID toolon the non - Arabic   string sequences . We ﬁnd the data to have 4:14 %   non - Arabic . These turn out to be almost always   natural code - switching involving many foreign lan-   guages ( e.g. , English , French , Korean , etc.).629   2.2 Pre - Processing and Vocabulary   We remove diacritics and replace URLs and user   mentions with < URL > and < USER > . We also clean   the data by removing HTML tags , elongation , and   the hash signs . Further , we reduce repetitive char-   acters , emojis , and emoticons to one . To create   our language model vocabulary , we use Sentence-   Piece ( Kudo , 2018 ) to encode text as WordPiece   tokens ( Sennrich et al . , 2016 ) with 110 K Word-   Pieces . To allow for further pre - training ( and/or   ﬁne - tuning ) on additional languages , we extract our   vocabulary as follows : 70 M MSA sentences , 200 M   Arabic twitter data , 15 M sentences from Wikipedia   English , and 5 M sentences from the Wikipedia of   10other languages ( Bulgarian , French , German ,   Greek , Italian , Portuguese , Russian , Spanish , Turk-   ish , Czech).In § 3.1.2 , we describe parallel data   from four of these languages on which we ﬁne - tune   our models for X!Arabic MT . Our respective re-   sults ( reported in Table 4.2 ) demonstrate the utility   of including foreign vocabulary in our models .   2.3 AraT5   Model Architecture . We leverage our unlabeled   MSA and Twitter data described in § 2.1 to pre-   train three models : AraT5on MSA data ,   AraT5on twitter data , and AraT5 on both   MSA and twitter data using the T5encoder-   decoder architecture ( Raffel et al . , 2019 ) . Each   of the encoder and decoder components is similar   in size and conﬁguration to BERT(Devlin et al . ,   2019 ) , with 12layers each with 12attention heads ,   and 768hidden units . In total , this results in a   model with220million parameters . Objective .   Raffel et al . ( 2019 ) pre - train T5using a self - supervised ( denoising ) objective . The main idea is   to feed the model with masked ( corrupted ) versions   of the original sentence , and train it to reconstruct   the original sequence . Inspired by BERT ’s objec-   tive ( Devlin et al . , 2019 ) , the denoising objective   ( Raffel et al . , 2019 ) works by randomly sampling   and dropping out 15 % of tokens in the input se-   quence . All consecutive spans of dropped - out to-   kens are then replaced by a single sentinel token .   Pre - Training . For all three of our pre - trained mod-   els , we use a learning rate of 0:01 , a batch size of   128sequences , and a maximum sequence length   of512 , except for AraT5where the maximum   sequence is 128.We pre - train each model for 1 M   steps . Pre - training of each model took 80days   on one Google Cloud TPU with 8cores ( v 3:8 ) from   TensorFlow Research Cloud ( TFRC).We now in-   troduce our language generation and understating   benchmarks .   3 ARGEN   In order to evaluate our pre - trained language mod-   els , we introduce our new benchmark for Ara-   bic language generation evaluation ARGEN . It in-   cludes 19 different datasets with 59 test splits and   covers seven tasks : machine translation ( MT ) , code-   switched translation ( CST ) , text summarization   ( TS ) , news title generation ( NGT ) , question gen-   eration ( QG ) , transliteration ( TR ) , and paraphras-   ing ( PPH ) . As such , ARGEN has wide - coverage   both in terms of the number of tasks and datasets .   It is also linguistically diverse as it covers both   MSA and various Arabic dialects , in addition to   Arabizi ( romanized Arabic in the TS task ) and code-   switching ( in the CST task ) . We now describe each   component of ARGEN .   3.1 Machine Translation   To design the MT component of ARGEN ,   ARGEN , we consolidate 7unique datasets with   46different test splits . The datasets come from   both MSA and Arabic dialects , and range between   600 - 138 K sentences ( details in Table C.2 in Ap-   pendix ) . We introduce each dataset brieﬂy here .   3.1.1 Arabic!English   ( 1 ) United Nations Parallel Corpus . Ziemski   et al . ( 2016 ) introduce this parallel corpus of man-630ually translated UN documents covering the six   ofﬁcial UN languages ( i.e. , Arabic , Chinese , En-   glish , French , Russian , and Spanish ) . The corpus   consists of development and test sets only , each of   which comprise 4;000sentences that are one - to-   one alignments across all ofﬁcial languages .   ( 2 ) IWSLT Corpus . Several Arabic - to - English   parallel datasets were released during IWSLT eval-   uation campaigns ( Federico et al . , 2012 ; Cettolo   et al . , 2013 , 2014 , 2016 ) . The datasets are mainly   extracted from transcriptions of TED talks between   2010 and 2016 , and the QCRI Educational Domain   Corpus ( QED 2016 ) ( Abdelali et al . , 2014 ) .   AraBench Datasets . Sajjad et al . ( 2020 ) introduce   AraBench , an evaluation suite for MSA and di-   alectal Arabic to English MT consisting of ﬁve   publicly available datasets : ( 3 ) ADPT : Arabic-   Dialect / English Parallel Text ( Zbib et al . , 2012 ) ,   ( 4 ) MADAR : Multi - Arabic Dialect Applications   and Resources dataset ( Bouamor et al . , 2018 ) , ( 5 )   QAraC : Qatari - English speech corpus ( Elmahdy   et al . , 2014 ) , and ( 6 ) Bible : The English Bible   translated into MSA , Moroccan , and Tunisian Ara-   bic dialects . For all these datasets , we use the   same splits as Sajjad et al . ( 2020 ) in our experi-   ments .   3.1.2 X!Arabic   To investigate ability of our models to generate Ara-   bic starting from foreign languages in our vocab-   ulary , we create an X ! Arabic benchmark of four   languages ( English , French , German , and Russian )   by extracting parallel data from OPUS ( Tiedemann ,   2012 ) . For each language , we pick 1 M sentences   for training and 5 K sentences for each of devel-   opment and test splits . This gives us our seventh   ARGENdataset , which we call ( 7 ) OPUS - X-   Ara .   3.2 Code - Switched Translation   There is rising interest in translating code - switched   data ( Nagoudi et al . , 2021 ) . Our purpose here is   to translate Arabic text involving code - switching   from a foreign language into ( i)that foreign lan-   guage as well as into ( ii)MSA . Hence we create   ARGEN , our code - switched translation bench-   mark component , using four sub - test sets . Two of   these are natural and two are synthetic , as follows :   Natural Code - Switched Data . We create two   human written ( natural ) code - switched paralleldatasets : ( 1 ) ALG - CST . This is collected from   Algerian Twitter and consists of code - switched   Arabic - French posts . We translate these manu-   ally into monolingual French . ( 2 ) JOR - CST . This   is collected from Jordanian Twitter and consists   of code - switched Arabic - English posts , which we   manually translate into monolingual English . Each   of ALG - CST and JOR - CST comprises 300tweets   ( total= 600 ) . Human translation is performed by   one native speaker from each dialect with semi-   native English / French ﬂuency .   Synthetic Code - Switched Data . We use the multi-   lingual sequence - to - sequence model mBART ( Liu   et al . , 2020 ) to create synthetic code - switched data   following Jawahar et al . ( 2021 ) . We exploit the   UN multi - parallel data ( Ziemski et al . , 2016 ) using   the Arabic - English and Arabic - French test splits   ( 4;000sentences each , described in § 3.1 ) to gen-   erate our two code - switched test sets ( 3 ) MSA - EN   and(4 ) MSA - FR . In each case , we use mBART to   translate30 % random Arabic n - grams into the   target language ( i.e. , English or French ) .   3.3 Text Summarization   To build our text summarization benchmark com-   ponent , ARGEN , we use the following :   Essex Arabic Summaries Corpus ( EASC ) .   EASC ( El - Haj et al . , 2010 ) contains 153Arabic   Wikipedia and newspaper articles , each with 5   human - generated extractive summaries ( total= 765   summaries ) . The summaries are crowdsourced via   Mechanical Turk .   WikiLingua . An abstractive summarization   dataset in 18languages , including Arabic   ( Faisal Ladhak and McKeown , 2020 ) . It contains   articles and their summaries from WikiHow . The   Arabic part includes summaries for 29:2 K articles ,   which we split into 80 % Train ( 23:4 K ) , 10 % Dev   ( 2:9 K ) , and 10 % Test ( 2:9 K ) .   3.4 News Title Generation   The purpose of the news title generation ( NTG )   task is to produce proper news article titles ( Liang   et al . , 2020 ) . We introduce NTG as a new task   for Arabic language generation . Given an article ,   a title generation model needs to output a short   grammatical sequence of words suited to the arti-   cle content . For this , we introduce ARGEN , a   novel NTG dataset exploiting 120 K articles along631with their titles extracted from AraNews ( Nagoudi   et al . , 2020).We only include titles with at least   three words in this dataset . We split ARGEN   data into 80 % Train ( 93:3 K ) , 10 % Dev ( 11:7 K ) ,   and 10 % Test ( 11:7 K ) . Details about ARGEN   are in Table C.1 ( Appendix ) . A sample of a news   article from our Test split and example titles gener-   ated by our models are in Table D.5 ( Appendix ) .   3.5 Question Generation   In the question generation ( QG ) task , a question   is produced for a passage ( Gehrmann et al . , 2021 ) .   Given the absence of an Arabic QG dataset , we   create a new Arabic QG dataset ( ARGEN ) us-   ing a publicly available Arabic question answering   ( QA ) resource . We follow Kriangchaivech and   Wangperawong ( 2019 ) who train a model to gen-   erate simple questions relevant to passages and   answers extracted from SQuAD ( Rajpurkar et al . ,   2016 ) . In our case , we build ARGENby extract-   ing96 K ( passage , answer , and question ) triplets   from ( 1)The Arabic QA dataset ARCD ( Mozan-   nar et al . , 2019 ) , and ( 2)three multi - lingual QA   datasets : XTREME benchmark ( Hu et al . , 2020 ) ,   MLQA ( Lewis et al . , 2019 ) , XQuAD ( Artetxe et al . ,   2020 ) , and TyDi QA ( Artetxe et al . , 2020 ) .   3.6 Paraphrasing   The main goal of this task is to produce for a given   Arabic sentence a paraphrase with the same mean-   ing . In order to build our paraphrasing benchmark   component ( ARGEN ) , we use the following   three datasets :   AraPara . We introduce AraPara , a new multi-   domain Arabic paraphrasing dataset we create us-   ing English - Arabic parallel OPUS data ( Tiede-   mann , 2012 ) . AraPara covers several domains such   as news , religion , politics , movies , and technol-   ogy . To create a high quality machine generated   paraphrase dataset , we follow four careful steps   involving human validation ( more details are of-   fered in Appendix C.1 ) . AraPara consists of 122 K   paraphrase pairs . We only use AraPara for model   development , and hence we split it into 116 K Train   and6 K Dev .   Arabic SemEval Paraphrasing ( ASEP ) . We also   create a new Arabic paraphrasing dataset using   three existing Arabic semantic similarity datasets   released during SemEval 2017 ( Cer et al . , 2017).These are MSR - Paraphrase ( 510 pairs ) , MSR-   Video ( 368pairs ) , and SMTeuroparl ( 203pairs ) .   The pairs are labeled with a similarity score on   a scale from 0to5 . For our purpose , we only   keep sentence pairs with a semantic similarity score   3:5which gives us 603pairs . We merge and   shufﬂe all three ASEP datasets for our use .   Arabic Paraphrasing Benchmark ( APB ) . APB   is created by Alian et al . ( 2019 ) . It consists of   1;010Arabic sentence pairs that are collected from   different Arabic books . Paraphrasing was per-   formed manually using six transformation proce-   dures ( i.e. , addition , deletion , expansion , permuta-   tion , reduction , and replacement ) .   3.7 Transliteration .   Transliteration involves mapping a text written   with orthographic symbols in a given script into   another ( Beesley , 1998 ) . We use the BOLT   Egyptian Arabic SMS / Chat and Transliteration   dataset ( Song et al . , 2014),a collection of   naturally - occurring chat and short messages ( SMS )   from Egyptian native speakers . The messages   ( sources ) were natively written in either romanized   Arabizi or Egyptian Arabic orthography . The target   is the Egyptian transliteration of these message .   For experiments , we use the same split proposed   by Shazal et al . ( 2020 ) ( 58:9 K for Train and 5:4 K   for Dev and Test each ) . We refer to this dataset as   ARGEN .   4 Evaluation on ARGEN   Baselines and Procedure . For all tasks , we com-   pare our models to models ﬁne - tuned with mT5 us-   ing the same training data . In addition , for MT , we   compare to a vanilla sequence - to - sequence ( S2S )   Transformer ( Vaswani et al . , 2017 ) trained from   scratch as implemented in Fairseq ( Ott et al . , 2019 ) .   For all models and baselines , across all tasks , we   identify the best model on the respective Dev data   and blind - test it on Test data . As a rule , we report   on both Dev and Test sets . All our Dev results are   in Section C.2 in the Appendix .   4.1 Machine Translation .   We train two S2S Transformers models on 2 M   ( S2S ) and 10 M ( S2S ) MSA - English paral-   lel sentences extracted from OPUS . We take these632633two models as our baseline I . We also ﬁne - tune our   three models as well as mT5 on the same OPUS 2 M   MSA - English parallel sentences used for baseline I.   Fine - tuned mT5 is our second baseline baseline II .   Arabic!English . Results of ARGENare re-   ported in Table 2 . Results show that our models   achieve best BLEU score in 37out of the 42tests   splits . AraT5 acquires best results in 32of   these test splits , outperforming all the baselines   ( S2S ) , ( S2S ) , and mT5 with + 5:25 , +4:99 ,   and + 0:45BLEU points . These results are strik-   ing since our language models are pre - trained on   Arabic data only ( although they include English vo-   cabulary and marginal amounts of code - switching ;   see § 2.1 ) . In other words , even under this arguably   zero - shot setting , the models perform very well .   In addition , our AraT5 model outperforms even   the S2S model trained with 5X more data . For   completeness , we also provide the current SOTA   on each of our datasets . We do not compare our   results to SOTA since these are acquired by models   ﬁne - tuned on much larger datasets than ours . For   example , Sajjad et al . ( 2020 ) exploit 42 M par-   ralel sentences to train their models . To limit GPU   needs during our experiments , especially given the   time - consuming ﬁne - tuning process typical of T5   models , we do not ﬁne - tune the models on the full   amounts of available parallel data . However , in the   future we plan to compare our models under the   full data setting .   X!Arabic . Our language models are not pre-   trained on foreign data , but we include vocabulary   from 11foreign languages . Our X ! Arabic exper-   iments here are hence zero - shot ( from the perspec-   tive of pre - training ) . Table 4.2 shows the results   of AraT5 and mT5 on OPUS - X - Ara . We ob-   serve that our model outperforms mT5 in the four X   ! Arabic sub - tasks with an average of + 1:12and   +0:86BLEU points on Dev and Test , respectively .   4.2 Code - Switched Translation .   For this task , we test on the two natural code-   switched translation ( CST ) test sets that we manu-   ally created , ALG - FR ! FR and JOR - EN!EN . We   also evaluate on our two synthetic CST datasets ,   MSA - EN and MSA - FR , one time with EN / FR as   target ( e.g. , MSA - EN ! EN ) and another with MSA   as target ( e.g. , MSA - EN ! MSA ) . We ﬁne - tuneour three pre - trained models as well as mT5 on   the OPUS - X - Ara segments involving English and   French ( each with 1 M parallel sentences , described   in § 3.1.2 ) , in both directions . Since these MT   models are only ﬁne - tuned on parallel monolin-   gual data , we refer to these experiments as zero-   shot . We test these models on both our natural and   synthetic code - switched data ( described in § 3.2 ) .   We report results in Table 3 . Our models achieve   best results in one out of the two natural test sets   ( with + 4:36BLEU points on ALG - FR ) and all   four synthetic test sets ( e.g. , + 4:55BLEU points   on MSA - EN!MSA ) . These results clearly show   our models ’ remarkable language generation abil-   ity especially in the Arabic direction .   4.3 Text Summarization   For the two ARGENdatasets , we ﬁne - tune and   identify the best model on the Train and Dev   splits of WikiLingua ( Faisal Ladhak and McKeown ,   2020 ) and test on all EASC and the Test of Wik-   iLingua . We report different ROUGE scores ( Lin ,   2004 ) in Table 5 . As the Table shows , AraT5ac-   quires best results on WikiLingua data , while mT5   outperforms us on EASC ( we hypothesize since   EASC is older data that is likely part of the mC4   on which mT5 was pre - trained ) . On both datasets ,   we establish new SOTA ( both with our pre - trained   models and mT5 ) .   4.4 News Title and Question Generation   For both tasks , we ﬁne - tune all our models on the   Train splits of ARGENand ARGEN , respec-   tively . As Table 6 shows , allour models outperform   mT5 on each of the two tasks . AraT5 excels   with 20:61 % BLEU on ARGENand AraT5 is   at16:99 % on ARGEN .   4.5 Paraphrasing and Transliteration   For the paraphrasing task , we ﬁne - tune and vali-   date on our new AraPra dataset and blind - test on   both APB and ASEP datasets ( described in§ 3.6).634   As Table 6 shows , AraT5is best on APB ( 17:52   BLEU ) and ASEP ( 19:38BLEU ) . For translit-   eration , we ﬁne - tune our models on the Train   split of ARGEN . As Table 6 shows , each of   AraT5 and AraT5 outperform mT5 . Notably ,   AraT5 is at 65:88BLEU , outperforming previ-   ous SOTA ( Shazal et al . , 2020 ) by 7:1points .   4.6 Evaluation on Arabic NLU   We also evaluate our new pre - trained models on   the recently proposed Arabic language understand-   ing and evaluation benchmark , ARLUE ( Abdul-   Mageed et al . , 2021 ) that involves six cluster tasks   ( i.e. , sentiment analysis , social meaning , topic   classiﬁcation , dialect identiﬁcation , named entity   recognition , and question answering ) . Our mod-   els establish new SOTA on the benchmark with an   ARLUE score of 77:52vs . the previous SOTA of76:53 , reported by ARLUE authors . We provide   results of this set of experiments in Appendix B.   5 Analysis and Discussion   5.1 Multilingual vs. Dedicated Models .   Our results conﬁrm the utility of dedicated lan-   guage models as compared to multilingual models   such as mT5 ( 101 + languages ) . Our AraT5 model   outperforms mT5 , even though it is pre - trained   with 49 % less data ( see § 2.1 ) . One reason might   be that massively multilingual models are more   prone to suffering from capacity issues . Data qual-   ity is another challenge for multilingual models .   As pointed out earlier , Kreutzer et al . ( 2021 ) ﬁnd   systematic issues with data representing several   languages ( including Arabic ) in the mC4 dataset   on which mT5 is pre - trained . We perform a data   quality study conﬁrming the ﬁndings of Kreutzer   et al . ( 2021 ) . We also ﬁnd Arabic mC4 data to be   less geographically diverse than our Twitter pre-   training data ( described in § 2.1 ) . Our mC4 data   study is in Appendix A.   Code - Switching . We also study code - switching   in both our Twitter dataset and the Arabic part of   mC4 . We ﬁnd that while our Twitter data involves   natural code - switching ( 4%of sequences ) , code-   switching in Arabic mC4 is very rare . This explains   the strong performance of our AraT5model   on the natural code - switched translation data on   French . We conjecture that mT5 good performance   on English code - switched data is due to it being   pre - trained on very large amounts of English rather   than natural code - switching .   5.2 Effect of Sample Length on MT .   We were inquisitive how MT models ﬁne - tuning   our pre - trained language models compare to mT5   under different length conditions . For this , we   ( 1)merge all MSA and dialectal Test datasets in   our Arabic!English experiments to form a single   dataset that we then ( 2)split into three bins / Test   sets based on sentence length as shown in Table D.1 .   As the Table shows , our AraT5 outperform   mT5 in allbut one condition ( where our model   acquires marginally less performance ) . We also   performed similar evaluation on the merged Dev   sets of all MSA and dialectal Arabic MT datasets   in the Arabic!English direction . We do not show   related results here , but we note our AraT5   outperforms mT5 on allconditions.635   5.3 Qualitative Analysis .   We also perform qualitative analyses of the outputs   of several of our models , including as to length   of MT source data ( Appendix D ) . In particular ,   our analyses are for the following tasks : machine   translation , code - switched translation , paraphras-   ing , transliteration , and news title generation . MT   Model . Table D.2 ( Appendix ) shows three exam-   ples of Arabic!English MT models . Sentence ( 1 )   is inMSA source , sentence ( 2 ) is in Levantine Ara-   bic source , and sentence ( 3 ) is in Egyptian source .   In all three examples , one or more of our models   generate(s ) more ﬂuent translations than mT5 . This   includes ability of our models to translate dialectal   sentences where mT5 seems to struggle ( e.g. , mT5   is not able to translate the equivalents of “ drive "   from Egyptian Arabic ) .   Code - Switched Translation Model . Table 7   shows two code - switched examples from   ARGEN . Sentence ( 1 ) is Algerian dialect at   source translated into French , while sentence ( 2 ) is   Jordanian dialect translated into English . In both   cases , our models not only handle the dialects but   also their use in code - switched contexts better than   mT5 .   Paraphrasing , Transliteration , and Title Gen-   eration . Each of Tables D.3 , D.4 , and D.5 ( Ap-   pendix D ) shows two output samples from our   paraphrasing , transliteration , and title generation   models , respectively . In each case , the samples   are high - quality , informative , and ﬂuent . Our para-   phrase samples also tightly capture the meaning of   the source sentences.6 Related Work   Multilingual LMs . mBERT is the multilingual   version of BERT ( Devlin et al . , 2019 ) , which is an   encoder model with bidirectional representations   from Transformers trained with a denoising ob-   jective . mBERT is trained on Wikipedia for 104   languages , including Arabic . XLM - R ( Conneau   et al . , 2020 ) is also a Transformer - based multilin-   gual masked language model pre - trained on more   than 2 TB of CommonCrawl ( CC ) data in 100lan-   guages , including Arabic ( 2:9B tokens ) . XLM - R   model uses the same masking objective as BERT ,   but not the next sentence prediction . mT5 ( Xue   et al . , 2020 ) is the multilingual version of Text-   to - TextTransfer Transformer model ( T5 ) ( Raffel   et al . , 2019 ) . T5 is an encoder - decoder Transformer   similar in conﬁguration and size to a BERT .   It is trained on mC4 , which is 26:76 TB for 101   languages generated from 71CC dumps .   Arabic LMs . AraBERT ( Antoun et al . , 2020 ) is   an Arabic pre - trained language model based on the   BERTarchitecture with 24 GB of MSA data .   ARBERT andMARBERT ( Abdul - Mageed et al . ,   2021 ) are two BERT - based models , with the ﬁrst   focused on MSA ( 61 GB ) and the second on both   MSA and dialects ( 128 GB ) . MARBERT achieves   SOTA on most Arabic NLU tasks . QARiB ( Abde-   lali et al . , 2021 ) is similarly a BERT - based model   covering both MSA and dialects . CamelBERT ( In-   oue et al . , 2021 ) is also a BERT - based model pre-   trained with MSA , dialectal , and classical Arabic .   7 Conclusion   We introduced three powerful Arabic - speciﬁc text-   to - text Transformer models trained on large MSA   and/or Arabic dialectal data . We also introduced   ARGEN , a uniﬁed benchmark for Arabic Natu-   ral Language generation evaluation composed of   seven tasks collected from a total of 19datasets .   Our models outperform mT5 on allARGEN tasks   ( 52out of 59test sets , i.e. , 88:14 % ) . This is true   even for MT involving four foreign languages from   which the models have seen marginal or no pre-   training data ( i.e. , zero- and few - shot pre - training ) .   Our models also set new SOTA on the large Ara-   bic language understanding evaluation benchmark   ARLUE . Our models involve vocabulary from 11   languages other than Arabic , and hence can easily   be further pre - trained/ﬁne - tuned in these languages .   Our models are publicly available , and ARGEN   datasets are accessible from our repository.636Acknowledgements   We gratefully acknowledge support from the Nat-   ural Sciences and Engineering Research Council   of Canada ( NSERC ; RGPIN-2018 - 04267 ) , the So-   cial Sciences and Humanities Research Council   of Canada ( SSHRC ; 435 - 2018 - 0576 ; 895 - 2020-   1004 ) , Canadian Foundation for Innovation ( CFI ;   37771 ) , Compute Canada ( CC ) , , UBC ARC-   Sockeye , and Advanced Micro Devices , Inc.   ( AMD ) . We thank the Google TFRC program for   providing us with free TPU access . Any opin-   ions , conclusions or recommendations expressed in   this material are those of the author(s ) and do not   necessarily reﬂect the views of NSERC , SSHRC ,   CFI , CC , ARC - Sockeye , AMD , or Google . We   thank Bashar Talafha for help with code - switching   data preparation .   Ethics Statement   Energy efﬁciency . Our models , similar to many   deep learning language models , take signiﬁcant   pre - training time and are not energy efﬁcient . We   acknowledge this important issue and believe work   on creating energy efﬁcient models should receive   scholarly attention .   Data . Our pre - training datasets are collected from   the public domain and cover diverse communities .   As we have demonstrated , our resulting models   are better equipped to power applications involving   several varieties of Arabic as well as code - switched   language use involving Arabic . From this perspec-   tive , we hope they add to ongoing efforts in the   community to design models that are fairer and   more representative .   ARGEN Benchmark Release . We design AR-   GEN using both existing datasets and new datasets   that we create for this work . In our accompanying   GitHub repository , we link to all existing publicly   available components of the benchmark with stan-   dard splits from source as well as components that   can be acquired from data organizations . In ad-   dition , we released all the new datasets we have   developed . While we have prioritized standardiz-   ing evaluation on as many uniﬁed and consolidated   datasets and tasks as possible , we also report per-   formance on individual test sets so as to enable the   community to replicate our work even on particular   parts or tasks of ARGEN if they so wish . AraT5 Models Release . All our pre - trained mod-   els are publicly available for non - malicious use .   We acknowledge our models may still be misused   in real world . However , we hope the models will   be deployed in domains such as education , disaster   management , health , recreation , travel , etc . in so-   cially beneﬁcial ways . These meaningful potential   use cases are behind our decision to release the   models .   References637638639640   A A Study of Arabic mC4 Data Quality   Xue et al . ( 2020 ) train mT5 on the mC4 dataset .   They report 57B Arabic tokens ( almost double our   token size ) from 53 M webpages , making 1:66 % of   all mT5 data . For our analysis , we randomly sam-   ple1 M paragraphs from the Arabic part of mC4 .   We use paragraphs rather than whole documents for   a more ﬁne - grained analysis that is more compara-   ble to our own data ( especially in the case of Twit-   ter ) . We ﬁrst perform language identiﬁcation using   CLD3 ( McCandless , 2010 ) on the data . We ﬁnd   a sizable amount of the data ( i.e. , 13:59 % ) to be   non - Arabic ( mostly English or French ) . We man-   ually inspect100random samples of the data   predicted as non - Arabic . We ﬁnd these are mostly   either non - linguistic content ( e.g. , java - script or   HTML code ) or non - Arabic text . The non - Arabic   text is sometimes foreign language advertising or   even full translation of the Arabic text in some   cases . In many cases , non - Arabic is also boilerplate   text such as that in web fora . Also , no samples of   the non - Arabic included real code - switching .   We also run an in - house MSA - dialect classiﬁer   on the same 1 M data sample . The classiﬁer pre-   dicts an overriding majority of the data ( 99:83 % )   as MSA . We again manually inspect 100sam-   ples from the small fraction predicted as dialects   ( i.e. , 0:17 % ) . While we ﬁnd some of these to be ac-   tual dialectal text ( usually short belonging to either   Egyptian or Saudi dialects ) from web fora , in the   majority of cases the text is simply names of soap   operas or advertisements . Our own pre - training   data in the case of Twitter , in comparison , involve   much more dialectal content ( 28:39 % as listed in   § 2.1 ) .   B Evaluation on Arabic NLU   B.1 ARLUE Benchmark   Recently , Abdul - Mageed et al . ( 2021 ) introduced   ARLUE , a natural language understanding bench-   mark for Arabic . ARLUE is composed of 42pub-   licly available datasets , making it the largest and   most diverse Arabic NLP benchmark . ARLUE   is arranged into the six cluster tasks of sentiment   analysis ( SA ) , social meaning ( SM ) , topic classi-   ﬁcation ( TC ) , dialect identiﬁcation ( DI ) , named   entity recognition ( NER ) , and question answering   ( QA ) . We methodically evaluate each cluster task , ultimately reporting a single ARLUE score follow-   ing Abdul - Mageed et al . ( 2021 ) . Table B.1 , shows   a summary of the ARLUE benchmark . We brieﬂy   describe ARLUE tasks next .   ARLUE.To construct this task cluster Abdul-   Mageed et al . ( 2021 ) merged 17MSA and DA   publicly available datasets .   ARLUE.ARLUErefers to eight social mean-   ing datasets covering prediction of age , dangerous   speech , emotion , gender , hate speech , irony , offen-   sive language , and sarcasm . used in this benchmark .   We will follow Abdul - Mageed et al . ( 2021 ) in not   merging the social meaning datasets , but rather re-   port performance on each individual dataset as well   as average performance across all tasks as part of   an overall ARLUE score .   ARLUE This benchmark component is a con-   catenationof three topic classiﬁcation datasets :   Arabic News Text ( ANT ) ( Chouigui et al . , 2017 ) ,   Khaleej ( Abbas et al . , 2011 ) , and OSAC ( Saad and   Ashour , 2010 ) .   ARLUEFive datasets are used for dialect clas-   siﬁcation . These are AOC Zaidan and Callison-   Burch ( 2014 ) , ArSarcasm(Farha and Magdy ,   2020 ) , MADAR ( sub - task 2 ) ( Bouamor et al . ,   2019 ) , NADI-2020 ( Abdul - Mageed et al . , 2020a ) ,   and QADI ( Abdelali et al . , 2020 ) .   ARLUEinvolve three categories , namely ,   ARLUE for MSA - dialect classiﬁcation ( bi-   nary).ARLUE , and ARLUE for the   region and country level classiﬁcation into four   classes ( region ) , and 21classes ( country ) respec-   tively .   ARLUE . Four Arabic and multilingual QA   datasets are concatenated to build ARLUE :   ARCD ( Mozannar et al . , 2019 ) MLQA ( Lewis   et al . , 2019 ) , XQuAD ( Artetxe et al . , 2020 ) , and   TyDi QA ( Artetxe et al . , 2020 ) .   B.2 ARLUE Evaluation   Baselines . For comparison , we ﬁne - tune a number   of models on the same training data as our new   models . These include the multilingual sequence-   to - sequence model mT5 ( Xue et al . , 2020 ) , and   the powerful Arabic - speciﬁc BERT - based model   MARBERT ( Abdul - Mageed et al . , 2021 ) . We note641that MARBERT achieves the SOTAacross the   majority of 6 cluster tasks of ARLUE , with the   highest ARLUE score .   Settings and Evaluation . We evaluate our models   on the language understanding benchmark , AR-   LUE , under two settings : ( i ) single task learning   and ( ii ) multi - task learning . We present results   on all the task clusters included in ARLUE ex-   cept for NER which is a token - level task that is   not straightforward with the text - to - text set up we   adopt . Table B.2 shows our evaluation results using   the relevant metric for each task .   Abdul - Mageed et al . ( 2021 ) introduced ARLUE   score , a metric used to score pre - trained language   model performance on multiple datasets . AR-   LUE score is a simply macro - average of the dif-   ferent scores across all task clusters , where each   task is weighted equally following ( Wang et al . ,   2018 ) . We compute the ARLUE score ( i.e. , overall   macro - average ) for each of our three models ( i.e. ,   AraT5 , AraT5 , and AraT5 ) and the baseline   ( mT5 ) .   Single Task . We ﬁne - tune our three models andmT5 individually on each of the six tasks of AR-   LUE . We typically ( i.e. , in allour experiments )   identify the best checkpoint for each model on   the development set , and report its performance   on both development and test data . As Table B.2   shows , our AraT5 model achieves the highest AR-   LUE score ( 77:52 ) , followed by AraT5 ( 77:50 )   and AraT5(75:33 ) . We note that all our models   outperform mT5 and the MARBERT ( SOTA ) by   +2:74and+1ARLUE score points , respec-   tively .   Multitask . We also investigate multitask learning   ( Caruana , 1997 ; Ruder , 2017 ) with our AraT5 mod-   els . This approach consists of training the model on   multiple tasks simultaneously ( i.e. , the model and   its parameters are shared across all tasks ) in order   to eventually improve performance on each indi-   vidual task . In our case , we ﬁne - tune our models   on many tasks at the same time using : ( i ) The three   dialect datasets : ARLUE , ARLUE , and   ARLUE and ( ii ) the social meaning datasets642of ARLUE . Table B.3 and Table B.4 show the   results of multi - task experiments for dialect set-   tings and social meaning , respectively . Our results   show that multi - task training outperforms single   task models in the majority of the dialects experi-   ments ( n= 7out of 9experiments , 77:78 % of the   tasks ) and half of the social meaning tasks ( n= 18   out of 36experiments , 50 % of the tasks ) . These   results are promising , and hence we plan to fur-   ther investigate multi - task learning with our new   models in the future .   C ARGEN   C.1 Arabic Paraphrase Data   AraPara . is a new multi - domain Arabic paraphras-   ing dataset we create using English - Arabic parallel   OPUS data ( Tiedemann , 2012 ) . To ensure high-   quality , we follow four careful steps : ( 1)We pick   1million English - Arabic parallel sentences from   OPUS ( Tiedemann , 2012 ) covering the different   domains . ( 2)We translate the English sentences   using a high - quality in - house English ! Arabic MT   model . ( 3)We run the multi - lingual semantic simi-   larity model from Yang et al . ( 2019 ) on the Arabic   machine translated sentences and the human trans-   lation ( i.e. , original Arabic sentences from OPUS ) ,   keeping only sentences with an arbitrary semantic   similarity score between 0:70and0:99 . This al-   lows us to ﬁlter out identical sentence pairs ( i.e. ,   similarity score = 1 ) and those that are not good   translations ( i.e. , those with a semantic similarity   score<0:70).(4)In order to maximize syntactic   and lexical diversity of the pairs of paraphrased sen-   tences , we perform an analysis based on word over-   lap between the semantically similar pair sentences   ( i.e. , the output of the previous step ) . We then   perform a manual analysis of the data , identify-   ing sentences with unigram token overlap between   35 % and70 % as sufﬁciently distinct paraphrase   pairs . This gives us 122 K paraphrase pairs . We   split these sentence pairs into 116 K for training   and6 K for validation .   C.2 Evaluation on DEV   In this section we describe the ARGENdatasets   splits and report the evaluation results in valida-   tion datasets . Details about ARGENare in Ta-   ble C.1 and ARGENdatasets splits are shown in   Table C.2 . Moreover , The evaluation on validation   datasets for ARGENare described in Table C.3   and C.4 , respectively . Finally , Table C.5 shows   the validation results of ARGEN , ARGEN ,   ARGEN , and ARGENdatasets .   D Qualitative Analysis of Models   In this section , we explore ability of our models   to generate MSA and dialectal Arabic under vari-   ous conditions . We now overview various types of   analyses in this regard . While samples presented   here are handpicked , we note that they are mostly   representative of outputs from our models since we   mainly chose them to demonstrate different linguis-   tic attributes that we believed would be relevant to   the analysis .   Effect of Sample Length on MT . We were inquis-   itive how MT models ﬁne - tuning our pre - trained   language models compare to mT5 under different   length conditions . For this , we ( 1)merge all MSA   and dialectal Test datasets in our Arabic ! English   experiments to form a single dataset that we then   ( 2)split into three bins / Test sets based on sentence   length as shown in Table D.1 . As the Table shows ,   our AraT5 outperform mT5 in allbut one con-   dition ( where our model acquires marginally less   performance ) . We also performed similar evalu-   ation on the merged Dev sets of all MSA and di-   alectal Arabic MT datasets in the Arabic ! English   direction . We do not show related results here , but   we note our AraT5 outperforms mT5 on all   conditions .   MT Model Output . Table D.2 shows three exam-   ples of Arabic!English MT models . Sentence ( 1 )   is inMSA source , sentence ( 2 ) is in Levantine Ara-   bic source , and sentence ( 3 ) is in Egyptian source .   In all three examples , on or more of our models   generate(s ) more ﬂuent translations than mT5 . This   includes ability of our models to translate dialectal   sentences where mT5 seems to struggle ( e.g. , mT5   is not able to translate the equivalents of “ drive "   from Egyptian Arabic ) .   Code - Switched Translation Model Output . Ta-   ble 7 shows two code - switched examples from   ARGEN . Sentence ( 1 ) is Algerian dialect at   source translated into French , while sentence ( 2)643644   Jordanian dialect translated into English . In both   cases , our models not only handle the dialects but   also their use in code - switched contexts better than   mT5 .   Paraphrasing , Transliteration , and Title Gen-   eration Output . Tables D.3 , D.4 , and D.5 each   shows two output samples from our paraphrasing ,   transliteration , and title generation models , respec-   tively . In each case , the samples are high - quality ,   informative , and ﬂuent . Our paraphrase samples   also tightly capture the meaning of the source sen-   tences.645646647