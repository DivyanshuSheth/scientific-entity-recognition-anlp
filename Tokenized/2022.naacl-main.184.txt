  Ori Shapira , Ramakanth Pasunuru ,   Mohit Bansal , Ido Dagan , and Yael AmsterdamerBar - Ilan UniversityUNC Chapel HillAmazon   obspp18@gmail.com   { ram,mbansal}@cs.unc.edu   { dagan,amstery}@cs.biu.ac.il   Abstract   Interactive summarization is a task that facil-   itates user - guided exploration of information   within a document set . While one would like   to employ state of the art neural models to im-   prove the quality of interactive summarization ,   many such technologies can not ingest the full   document set or can not operate at sufficient   speed for interactivity . To that end , we propose   two novel deep reinforcement learning models   for the task that address , respectively , the sub-   task of summarizing salient information that   adheres to user queries , and the subtask of list-   ing suggested queries to assist users throughout   their exploration . In particular , our models   allow encoding the interactive session state and   history to refrain from redundancy . Together ,   these models compose a state of the art solu-   tion that addresses all of the task requirements .   We compare our solution to a recent interac-   tive summarization system , and show through   an experimental study involving real users that   our models are able to improve informativeness   while preserving positive user experience .   1 Introduction   Integrating human interaction into NLP tasks has   been gaining the interest of the NLP community .   Human - machine cooperation can improve the gen-   eral quality of results , as well as provide a higher   sense of control for the targeted consumer . We   focus on the task of interactive summarization   ( IS : Shapira et al . , 2021b ) which enables   information exploration within a document set on   a topic , by means of user - guided summarization .   As illustrated in Figure 1 , a user can incrementally   expand on a summary by submitting requests to the   system , in order to expose the information of inter-   est within the topic . A proper exploration session   demands access to allinformation within the docu-   ment set , and fast reaction time for smooth humanFigure 1 : An IS system , ingesting a large doc-   ument set . A user interactively submits queries in or-   der to expand on the information . The system is re-   quired to process the full document set for compre-   hensive exploration , respond quickly , and expose non-   redundant salient information that also complies to the   input queries . See real example in Figure 5 .   engagement ( Anderson , 2020 ; Attig et al . , 2017 ) .   In addition , presented information must consider   the session history to refrain from repetitiveness .   While it is worthwhile to apply recent NLP ad-   vances that excel at extracting salient and query-   biased information , those advances usually come   at a cost of rather small input size limits or heavy   computation time . Indeed , all previous interactive   summarization systems we know of either apply   traditional methods or are inadequate for real - time   processing due to high latency ( § 2 ) . Our goal is to   overcome these obstacles , and leverage advanced   methods to improve information exposure while   keeping latency acceptable for interaction .   As depicted in Figure 1 , an IS system   provides an initial generic summary as an overview   of the topic , after which a user can iteratively is-   sue queries to the system for summary expansions   on subtopics of interest . To support querying , the   system offers a list of suggested queries , hinting at   information concealed within the document set .   We address the IS task components   through two subtasks : ( 1 ) generating the initial   summary and query responses , and ( 2 ) generating   lists of suggested queries . For each of the sub-   tasks we propose a deep reinforcement learning   ( RL ) algorithm that addresses the respective sub-2551task requirements . To enable comprehensive topic   exploration , our models speedily process the full   document set , as inspired by Mao et al . ( 2020 ) . Ad-   ditionally , they are able to peek at session history   to comply to the current state of the interaction .   The model for the query - assisted summarization   subtask , M , incorporates the query sequence   by ( 1 ) encoding a query into the contextual sen-   tence representations , ( 2 ) attending the represen-   tations using a new query - biased variant of the   maximal marginal relevance ( MMR : Carbonell and   Goldstein , 1998 ) function , and ( 3 ) a dual reward   mechanism for policy optimization ( Pasunuru and   Bansal , 2018 ) which we adapt to consider both ref-   erence summaries and the query ( § 3 ) . The model   for the suggested queries list generation subtask ,   M , works at the phrase level , as opposed to the   sentence level , to enable extraction of important   phrases that serve as suggested queries . Similarly   toM , the model learns importance with con-   sideration to session history , but without an input   query – as its role is to suggest such a query ( § 4 ) .   The models are trained on the DUC2007 multi-   document summarization ( MDS ) news - domain   dataset , with adaptions for our task setting . For   testing , we follow the IS evaluation frame-   work of Shapira et al . ( 2021b ) to run simulations ,   collect real user sessions , and assess the results ,   using DUC 2006 . In principle , summary informa-   tiveness , i.e. general salience , could potentially   come at the expense of query responsiveness , but   importantly , our results show that our RL - based so-   lution is able to significantly improve information   exposure over the baseline of Shapira et al . ( 2021b ) ,   without compromising user experience ( § 5 ) .   2 Background and Related Work   Interactive summarization facilitates user - guided   information navigation within document sets . The   task suffered from a lack of a methodological eval-   uation , until Shapira et al . ( 2021b ) formalized the   IS task with a framework consisting of a   benchmark , evaluation metrics , a session collection   process and baseline systems . This framework , that   we leverage , enables comparison and analysis of   systems , allowing principled research on the task   and accelerated development of algorithms .   To the best of our knowledge , all previous works   onIS have either applied more traditional   text - processing methods or require costly prepro - cessing of inputs to facilitate seamless interaction .   Leuski et al . ( 2003 ) used surface - form features   for processing content , and Baumel et al . ( 2014 )   adapted classic MDS algorithms like LexRank   ( Erkan and Radev , 2004 ) and KLSum ( Haghighi   and Vanderwende , 2009 ) . Christensen et al . ( 2014 )   optimized discourse graphs and Shapira et al .   ( 2017 ) relied on a knowledge representation , both   expensively pre - generating hierarchical summaries   that limit expansions to pre - prepared information   selections . Hirsch et al . ( 2021 ) applied advanced   coreference resolution algorithms that take several   hours for preprocessing a document set .   The two IS baseline systems of Shapira   et al . ( 2021b ) use sentence clustering or TextRank   ( Mihalcea and Tarau , 2004 ) for summarization , sen-   tence similarity heuristics for query - responses , and   n - gram frequency or TextRank for suggested query   extraction . Moreover , their query - response gen-   erators strictly consider a given query , ignoring   history or global informativeness . Our proposed   algorithms significantly improve information expo-   sure over the latter baselines , using advanced deep   RL methods , working in real time . We next review   some recent techniques in MDS , query - focused   summarization and multi - document keyphrase ex-   traction , all of which relate to the IS task   and our choice of algorithms .   The subtask of query - assisted summarization .   Non - interactive MDS has been researched exten-   sively , with few recent neural - based methods that   can handle relatively large inputs . For example ,   Wang et al . ( 2020 ) use graph neural networks   to globally score sentence salience , Xiao et al .   ( 2021 ) summarize using Longformers ( Beltagy   et al . , 2020 ) , and Pasunuru et al . ( 2021b ) combine   a Longformer with BART ( Lewis et al . , 2020 ) and   incorporate graphical representation of information .   Mao et al . ( 2020 ) apply deep RL for autoregressive   sentence selection , and , in contrast to most other   neural methods , can ingest the fulldocument set .   In the query - focused summarization ( QFS ) task   summaries are biased on a query . To accommo-   date a query , Xie et al . ( 2020 ) use conditional self-   attention to enforce dependency of the query on   source words . Pasunuru et al . ( 2021a ) and Kulka-   rni et al . ( 2021 ) hierarchically encode a query with   the documents . These and other QFS methods   require large training sets , and limit the allowed   input size ( Baumel et al . , 2018 ; Laskar et al . , 2020 ) .   Relatedly , incremental update summarization ( Mc-2552Creadie et al . , 2014 ; Lin et al . , 2017 ) marks query-   relevant information as reported texts stream in ,   avoiding repeating information marked earlier . In-   teractivity is not a constraining factor here , yielding   solutions with relatively high computation time .   With respect to the above related work , we de-   velop a model inspired by Mao et al . ( 2020 ) , which   is closest to our requirements . To facilitate an inter-   active setting , our model ( 1 ) enables query+history   injection , ( 2 ) supports full input processing , neces-   sary for complete information availability during   exploration , ( 3 ) has low latency at inference time ,   and ( 4 ) requires a relatively small training set .   The subtask of suggested - queries list genera-   tion . Extracting suggested queries on a document   set most resembles the multi - document keyphrase   extraction ( MDKE ) task since it aims to identify   salient keyphrases ( Shapira et al . , 2021a ) . MDKE   was mostly addressed using traditional heuristics   or graph - centrality algorithms applied over the doc-   uments ( e.g. Mihalcea and Tarau , 2004 ; Florescu   and Caragea , 2017 ) . In contrast to MDKE , the sug-   gested queries extraction subtask is a new paradigm   that updates “ keyphrases ” with respect to session   history . While previous methods for keyphrase   extraction could potentially be adapted for our dy-   namic setting , we choose to focus in this work on a   deep RL architecture for suggested queries that res-   onates our model for query - assisted summarization   and allows sharing insights between the models .   3 Query - Assisted Summarization Model   The subtask of query - assisted summarization cov-   ers two main components of the IS task :   the generators of an initial summary and of query-   responses . The initial summary concisely specifies   some central issues from the input topic ( not biased   on a query ) to initiate the user ’s understanding of   the topic and to motivate further exploration . Then ,   for each user submitted query , the query - response   generator non - redundantly expands on the previ-   ously presented information with topically salient   responses that are also biased around the query . We   next formally define the subtask and then describe   our RL model for it .   3.1 Subtask Formulation   The input to the query - assisted summarization sub-   task is tuple ( D , q , E , m ) , such that : Dis a docu-   ment set on a topic where the j - th sentence in the   concatenation of D ’s documents is denoted s;qis a query , and can be empty ( denoted _ ) for an   unbiased generic summary ; E={e , ... , e}is   a sequence of sentences from Dtermed the history ,   containing texts previously output in the session ;   andmis the number of sentences to output . The   output is sentence sequence E={e , ... , e }   fromD(extractive summarization ) . When in-   putting ( D , _ , { } , m ) , the output is a generic sum-   mary of msentences , that can serve as the initial   summary ; and when qandEare not empty , the   output is an expansion on Ein response to q ,   containing new salient information biased on q.   Dis paired with a set of generic reference sum-   maries R , which is used for training or as a part of   the evaluation effort .   3.2 Model Architecture   Our query - assisted summarization model , M ,   is autoregressive , outputting the requested number   of summary sentences one - by - one . At time step   t , a sentence eis output according to the cur-   rent query and an encoding of the summary - so - far   E={e , ... , e , e , ... , e}to prevent infor-   mation repetition . At inference time , M out-   puts the summary sentences with the given query   and history ( possibly empty ) . At train time , we   emulate a session by invoking M with a se-   quence of differing queries , Q={q , q , ... , q } ,   for which to generate the corresponding sequence   of output sentences . I.e. , output sentence eis   biased on query qand the summary - so - far Eat   time step t. We next describe the architectureof   M , also illustrated in Figure 2 .   Sentence encoding . The first step of the model is   hierarchically encoding the sentences of the docu-   ment set Dto obtain contextualized representation   cfor sentence s∀j . A CNN ( Kim , 2014 ) en-   codes son the sentence level and then a bi - LSTM   ( Huang et al . , 2015 ) forms representation con the   document level , given the CNN encodings .   Query encoding . Additionally , at each time step   twe prepare sentence+query representations c=   c⊕CNN(q ) , i.e. , obtained by concatenating a   sentence representation and the CNN - encoding of   the current query . This sentence+query represen-2553   tation influences the relevance of a sentence with   respect to the current input query .   Query - MMR score weighting . MMR has been   shown to be effective in MDS , where information   repeats across documents . It aims to select a salient   sentence for a summary , that is non - redundant to   previous summary sentences . We extend standard   MMR so that the importance of the sentence is in   regards to both the document set and the query .   Formally , the query - focused MMR function defines   a score mfor each sat time step tas follows :   m = λ·BS(s , D , q )   −(1−λ)·maxS(s , e)(1 )   BS(s , D , q ) = β·S(s , D )   + ( 1−β)·S(s , q)(2 )   where λ∈[0,1]balances salience and redundancy   andβ∈[0,1]balances a sentence ’s salience within   its document set and its resemblance to the current   query . S(x , y)measures the similarity of texts   xandy , andDis a fully concatenated version of   document set D. Following findings of Mao et al .   ( 2020 ) , Scomputes cosine similarity between   the two compared texts ’ TF - IDF vectors . Redun-   dancy to previous sentences is computed as the   highest similarity - score against any of the previous   sentences . We set λ= 0.6(following Lebanoff   et al . , 2018 ) and β= 0.5(see Appendix B.3).The query - focused MMR scores are incorpo-   rated into M by softly attending on the sen-   tence representations with their respective trans-   lated query - focused MMR scores :   µ=softmax ( MLP(m ) ) ( 3 )   ˆc=µc ( 4 )   State representation . At time t , a representa-   tionzof the summary - so - far is computed by ap-   plying an LSTM encoder on { c , ... , c ,   c , ... , c } , i.e. , on the plain sentence   representations of E , where idx(e)is the index   of sentence e. Then , a state representation gcon-   siders zand all sentence representations with the   glimpse operation ( Vinyals et al . , 2016 ):   a = vtanh(Wˆc+Wz ) ( 5 )   α = softmax ( a ) ( 6 )   g=/summationdisplayαWˆc ( 7 )   where v , WandWare model parameters , and   arepresents the vector composed of a.   Finally , a sentence sat time tis assigned a   selection probability softmax ( p)such that :   where v , WandWare model parameters .   Reinforcement learning . AsM ’s goal   is to incrementally generate a query - assisted2554summary , it should strive to optimize ( 1 ) non-   redundant salient - sentence extraction and ( 2 ) query-   to - sentence similarity , that can be appraised with   ROUGE ( Lin , 2004 ) and text - similarity metrics ,   respectively . A policy gradient - based RL approach   ( Williams , 1992 ) allows optimizing on such non-   differentiable metrics . Specifically , we adopt the   Advantage Actor Critic method ( Mnih et al . , 2016 )   for policy learning , and a dual - reward procedure   ( Pasunuru and Bansal , 2018 ) to alternate between   the summary and query - similarity rewards .   At time step t , for selected sentence e(based   onsoftmax ( p ) ) , reward ris computed and   weighted into M ’s loss function . The re-   ward function alternates , from one train batch   to the next , between ROUGE(e , E , R)andS(e , q ) . The former computes the ROUGE   difference before adding etoEand after :   A larger ROUGEvalue implies that econcisely   adds more information onto E , with respect to   topic reference summaries R. We use ROUGE- 1   Fas the ROUGE function here . The query-   similarity reward functionS(e , q ) =   avg ( ( e , q ) , ( e , q))(10 )   computes an average of semantic and lexical sim-   ilarities between the selected sentence and corre-   sponding query . computes the cosine sim-   ilarity between the average of word embeddings   ( spaCy : Honnibal and Montani , 2021 ) of eand   that of q. For lexical similarity , ( e , q ) =   avg(R(e , q ) , R(e , q ) , R(e , q))(11 )   is the average of ROUGE -1 , 2 and L precision   scores between sentence and query . By alternat-   ing between the two rewards , we train a sentence-   selection policy in M to balance summary   informativeness and adherence to queries .   Overall system . OurM model adopts its   base architecture from Mao et al . ( 2020 ) ( for   generic MDS ) . Chiefly , we modify their model for   handling an input query - sequence and a sentencehistory , and employ a different summarization re-   ward function . The query is incorporated in the   sentence representation , in the new query - focused   MMR function and in the dual - reward mechanism .   3.3 Model Training   Pre - training . To provide a warm start for train-   ingM , a reduced version of M is first   pre - trained for generic extractive single - document   summarization using the large - scale CNN / Daily   Mail corpus ( Hermann et al . , 2015 ) , as proposed   by Chen and Bansal ( 2018 ) . The reduced model   pre - trains the full model for contextual sentence   representation and for salient - sentence selection in   the single - document generic setting . See Appendix   B.1 for precise technical details .   Training data . After pre - training the reduced   version of M , we train the full model using   the DUC 2007 MDS dataset , with modifications for   our query - assisted MDS task . The dataset includes   45 topics ( split into 35/10 train / val ) , each contain-   ing 25 documents and 4 reference summaries .   For each topic , we generate an “ oracle ” extrac-   tive summary by greedily aggregating 10 sentences   fromD , that maximizes the ROUGE-1recall   against R. Then for each sentence , we extract   a bi- or trigram that is most lexically - unique to the   sentence , in comparison to all other sentences in D.   This yields a sequence of 10 “ queries ” that could   easily render the corresponding oracle summary .   The intuition for this approach is that it would teach   M that it is worthwhile to consider a given   query when selecting a sentence that is informa-   tive with respect to the reference summaries . This   further assists in fulfilling the dual requirements   of selecting a globally informative sentence that   also adheres to the query . Appendix B.3 discusses   usage of different query types for training .   Validation metric . As the interactive session pro-   gresses , a recall curve emerges , that maps the   ROUGE recall score ( here ROUGE -1 ) versus the   expanding summary token - length . Once the ses-   sion halts , the area under the curve indicates the   efficacy of the session for information exposure .   A higher value implies faster unveiling of salient2555information . Normalizing by the final summary   length allows approximate comparability between   different length sessions . We hence use the aver-   age ( over topics ) length - normalized area under the   recall curve for validating the training progress .   4 Suggested Queries Extraction Model   4.1 Subtask Formulation   We now consider the second subtask of IS :   generating lists of suggested queries . The list is   regenerated after every interaction , to yield queries   that focus on sub - topics that were not yet explored .   Reusing the notations of M in § 3 , we de-   fine a model , M , for suggested queries list   generation , that receives an input tuple ( D , E , m )   ( notice that a query is not needed here ) . Here , the j-   thphrase inDis denoted ρ , when the documents   inDare concatenated , and accordingly , history   Eis a list of phrases extracted from the session ’s   current accumulated summary . mis the number   of suggested queries to output . The model outputs   phrase sequence E={e , e , ... , e}from   D , accounting for history E. As in M ’s   setting , Dis paired with a set of generic reference   summaries R.   4.2 Model Architecture   We adopt and adjust the architecture in § 3.2 for this   subtask . Similar to M , M selects input   units one - by - one considering a history , with the   main difference being the absence of query injec-   tion . Additionally , inputs and outputs are processed   on the phrase- rather than the sentence level .   Phrase and state representation . For the given   document set , all noun phrases are extracted using a   standard part - of - speech regular expression method   ( Mihalcea and Tarau , 2004 ; Wan and Xiao , 2008 ) .   We obtain document - level contextual phrase em-   beddings , cfor phrase ρ , with the CNN and bi-   LSTM networks , and softly attend the embeddings   with a standard MMR score :   m = λ·S(ρ , D )   −(1−λ)·maxS(ρ , e)(12 )   The MMR - based phrase representations then   pass through the glimpse attention procedure ,   which culminates in the phrase probability distribu-   tion for selecting the next output phrase . Reinforcement learning . The policy in M is   trained with a single reward function that measures   how prominent the selected phrase is within the   reference summaries , and how different it is from   previously seen phrases . Formally , at time step t ,   the reward rof selected phrase eis :   r=(e , R)−γ·M(e , E , R )   −γ·M(e , E\E , R)(13)(e , R ) = avg(avg(w , r))(14)M(e , L , R ) = max(e∩e , R )   ( 15 )   where(w , r)is the relative frequency of word w   in reference summary r. Namely , computes the   average term frequency of a phrase over its words   and across the reference summaries , as an estimate   of the phrase importance within the topic . M   computes the highestagainst a list of phrases ,   which is used to lower the reward of a phrase that is   redundant to phrases used earlier . Different weights   are given to theMagainst the input history   ( γ ) and that of the phrases output so far ( γ ) .   4.3 Model Training   Similarly to M , we first pre - train the base   model to get a warm start on embedding formation   and salience detection . The reduced architecture of   M andM for pre - training are identical .   We use the same DUC 2007 training data , with   document sets and reference summaries , and ad-   ditionally prepare three “ histories ” per topic : one   empty and two non - empty . An empty history mim-   ics generating a session ’s initial list of suggested   queries , while a non - empty history trains the model   to consider previously known information . Train-   ing with two non - empty histories per topic prepares   a model for varying informational states . These are   curated from a generic summary ( from a trained   M model ) that is truncated at two random   sentence - lengths between 1 and 12 . Overall , the   model is trained on three versions of each topic ,   each time with a different history .   Similarly to M , validation is guided by   the average normalized area under the recall curve .   Here , the accumulating rscores from Equation 13   are used as the recall of the expanding suggested   queries list . I.e. , a higher reward means better sug-   gested queries are output earlier . The AUC is nor-   malized with the total token - length of all suggested   queries to mitigate for lengthy phrase extractions.25565 Experiments   We ran several experiments for the assessment   of our M andM models , applying the   IS evaluation framework of Shapira et al .   ( 2021b ) . The goals of the experiments are to com-   pare varying configurations of our models and to   evaluate against an IS baseline system .   The experiments include both simulations and in-   teractive sessions with human users .   5.1 Compared Algorithms   TheM model architecture ( § 3.2 ) has several   configurable components : encoding the query into   sentences , considering the query in the MMR func-   tion ( both at train and inference time ) , and the dual   reward mechanism . We compared several varia-   tions of these using simulations , presented in § 5.2 .   In addition , we compare , both via simulations   ( § 5.2 ) and real sessions ( § 5.3 ) , against the ( better-   performing ) baseline system in ( Shapira et al . ,   2021b ) , named S.S ’s initial summary algorithm   is TextRank , and the query - response generator ex-   tracts sentences via lexical+semantic similarity to   the query , somewhat resemblingSin Equation   10 , fully neglecting the summary - so - far , in contrast   toM .S ’s suggested queries list contains   TextRank ’s top salient topic phrases . Since these   too do not account for the summary - so - far , they   are computed at the session beginning and are not   updated along the session , in contrast to M.   5.2 Simulated Experiments   TheIS task involves human users by defi-   nition . Nevertheless , running on simulated query   lists and session histories is pertinent for efficient   system evaluation and comparison of methods .   To simulate the query - assisted summarization   algorithms , we utilize the real sessions recorded   by Shapira et al . ( 2021b ): 3 - 4 user sessions on 20   topics from DUC 2006 collected with S. In our   simulation , each summary - so - far from a recorded   session is fed as input to the system together   with the following recorded user query . We then   measure R(difference of ROUGE- 1recall in-   curred by the query response compared with the   input summary - so - far ) . Additionally , we use R   ( ROUGE- 1F ) for initial summary informative-   ness . Both are measured w.r.t . the reference sum-   maries , normalized by the output length , and aver-   aged per session recording , and then over all ses-   sions and topics , to get an overall system infor - mativeness score . We also measure system query-   responsiveness using theSmetric .   Table 1 presents a representative partial ablation   of the M model . All variants were config-   ured to output sentences of up to 30 tokens , initial   summaries are 75 tokens , and query responses are   2 sentences . Configurations i - ivuse the query in   training , while vandvido not . Each configuration   is measured for informativeness ( columns marked   with† ) , and for query - responsiveness ( Scol-   umn ) . Out of configurations i - iv , config . i , where   we employ all mechanisms for query inclusion ,   yields the best overall scores in both informative-   ness and query - responsiveness , despite the inher-   ent tradeoff between the two . In the second set of   configurations ( v - vi ) , we observe that ignoring the   query at train time substantially degrades query-   responsiveness , and this is expectedly further exac-   erbated when also ignoring the query at inference   time . However , disregarding the query gives more   informative expansions with respect to reference   summaries , since the model was trained only to   optimize content informativeness , and is less likely   to sidetrack to the query - related information .   Compared to S(last row ) , our model sig-   nificantly improves informativeness . Query-   responsiveness is better in the Sbaseline since   its query - response generator simply invokes a func-   tion similar toS , but for the price of lower   informativeness . Still , this does not lead to inferior   overall user experience , see § 5.3 .   5.3 Real Session Collection and Evaluation   We collect real user sessions via controlled crowd-   sourcing ( which provides high quality work , see   Appendix D ) with the use of an IS web   applicationrunning either our M + M   models or the Sbaseline algorithms , enabling a   comparative assessment of the two systems . No-   tably , our algorithms have the low latency required   for the interactive setting ( Attig et al . , 2017 ) , i.e. ,   responding almost immediately .   Using the DUC 2006 IS test set , we pre-   pared two complementing user sets of 20 topics ,   each with 10 of the topics to be run on our system   and the other 10 on the baseline . We apply the eval-   uation metrics of Shapira et al . ( 2021b ): ( 1 ) The2557   area under the sessions ’ ROUGE recall curves , in   a common word - length interval across all sessions   and topics , which demonstrates how fast salient in-   formation is exposed in sessions . ( 2 ) ROUGE Fat   the initial summary and at 250 tokens , that indicate   how effectively the interactive system can gener-   ate summaries at pre - specified , comparable lengths .   ( 3 ) Manually assigned query - responsiveness score   ( 1 to 5 scale ) , which expresses how well users think   the system responded to their requests . And ( 4 )   manual UMUX - Lite ( Lewis et al . , 2013 ) score for   system usability ( effectiveness and ease of use ) ,   where 68is considered “ acceptable ” and 80.3is   considered “ excellent ” . We also measure automatic   query - responsiveness withS.   We conducted two such comparative collec-   tion and assessment experiments , either employing   M configuration vori , namely the best of the   two configuration sets . In both cases , the M   model used was set with γ= 0.5andγ= 0.9   after some hyperparameter tuning ( Appendix B.4 ) .   The first experiment ( with configuration v ) is de-   scribed here , and the other in Appendix E.1 .   We hired 6 qualified workers using the controlled   crowdsourcing procedure , and collected 2 - 3 ses-   sions per topic per system ( 111 total sessions ) . Inthe sessions , users explore their given topic by sub-   mitting queries with a common generic informa-   tional goal in mind ( Appendix D ) .   Overall system assessment . Table 2 , presenting   average scores over the collected sessions , shows   that our system is significantly more effective for   exposing salient information , as depicted in the   first three rows . Users indicate a slight degradation   in query - responsiveness of our system , consistent   withSscores ( row 4 - 5 ) . Note that the observed   difference inSscores , between simulations   and user sessions , partly stems from the fact that   they were computed over different sets of queries .   The varying queries issued by the users in user   sessions form a less stable query responsiveness   comparison than the one in Table 1 , whereS   scores are computed using consistent queries for all   systems . Despite the gap inSscores between   our system and Sin Table 2 , the overall usability   scores are slightly better ( last row ) . This may sug-   gest that users appreciate the informativeness of   the produced summary even when they are aware   that the summary is less biased on their queries ;   thus our system improves informativeness while   still providing a favorable user experience .   Assessment of suggested queries functionality .   We analyzed the types of queries users submitted   throughout their sessions , to assess the utility of up-   dating suggested queries , with M , as opposed   to a static list of suggestions , with S. To that   end , we tallied suggested query clicks and query   submissions via other modes , binning the tallies   to three sequential temporal segments within their   respective sessions ( Appendix E.3 ) . We found that ,   on average , the usage of suggested query clicks in-   creased by ~13 % when nearing the end of a session   withM , and conversely decreased by ~24 %   withS. While the decrease in use of the static   list is expected , since appealing queries are likely   exhausted earlier in a session , it is encouraging to2558witness the usefulness of updated queries as the   session progresses . This behavior suggests that   the updated list contains suggested queries that are   indeed engaging for learning more about the topic .   6 Conclusion   Interactive summarization for information explo-   ration is a task that requires compliance to user   requests and session history , while comprehen-   sively handling a large input document set . These   requirements pose a challenge for advanced text   processing methods due to the need for fast reac-   tion time . We present novel deep reinforcement   learning based algorithms that answer to the task   requirements , improving salient information expo-   sure while satisfying user queries and keeping user   experience positive .   We note that while M is designed for the   IS task , it may potentially be serviceable   for standard MDS , QFS , update summarization and   combinations thereof . This can be accommodated   by a proper choice of input , e.g. , QFS can be ad-   dressed by giving M as input a query , an   empty history and target summary length . In fu-   ture work , we may study the performance of our   solutions for such tasks , as well as strive to fur-   ther improve their performance on both ends of the   IS task – selecting topically salient infor-   mation and responding to user queries .   Acknowledgements   We thank the anonymous reviewers for their con-   structive comments and suggestions . This work   was supported in part by Intel Labs ; by the Is-   rael Science Foundation ( grants no . 2827/21 and   2015/21 ) ; by a grant from the Israel Ministry of   Science and Technology ; by the NSF - CAREER   Award # 1846185 ; and by a Microsoft PhD Fellow-   ship .   References255925602561A Ethical Considerations   Datasets . The DUC 2006 and 2007 datasets were   obtained according to the DUC website ( duc .   nist.gov ) requirements . It was not possible for   others to reconstruct the document sets and refer-   ence summaries of the dataset from the crowdsourc-   ing tasks .   The datasets are composed of new articles   mainly from the late 1990s from large news out-   lets , compiled by NIST . All data exposed by our   systems are directly extracted from those articles .   For extraction , we do not intentionally add in any   rules for ignoring or boosting certain information   due to an opinion .   Crowdsourcing . Due to the need for English   speaking workers , a location filter was set on   the Amazon Mechanical Turk ( https://www .   mturk.com ) tasks for the US , UK and Australia .   All tasks paid according to a $ 10 per hour wage ,   according to the estimated required time of each   task . The payment was either paid per assignment ,   or as a combination with a bonus .   Compute resources . Our M and   M models required between 2 and 20   hours of training ( usually around 4 hours ) ,   depending on the configuration . We trained on   one NVIDIA GeForce GTX 1080 Ti GPU with   11 GB memory . The pretrained base model was   trained once and reused in all subsequent training .   Outputting at inference time is computationally   cheap : M runs upto about 1 second , but   mostly in a few hundred milliseconds , and   M runs upto about 7 seconds , but mostly in   under 4 seconds . Training with a batch size of 8   used about 3 GB GPU memory for M , and   about 9 GB memory for M ( since there are   many more input units per document set , i.e. , all   noun phrases versus sentences ) .   B Implementation Details   B.1 Pre - training Technicalities   To provide a warm start for training M and   M , a reduced version of the models , which is   the same for both , is first pre - trained for generic   extractive single - document summarization using   the CNN / Daily Mail corpus ( Hermann et al . , 2015 )   with about 287k samples , as proposed by Chen   and Bansal ( 2018 ) . In this reduced model , ˆcis   replaced by cin Equations 5 , 7 and 8 . Further - more , there is a single reward function for learn-   ing the policy , computed per selected sentence e   asROUGE -L Fw.r.t . the ( single ) reference sum-   mary ’s sentence at index t. The reduced model   pre - trains the full model for contextual sentence   representation and for salient - sentence selection in   the single - document generic setting . This allows   training M andM with a relatively small   dataset for their final purposes .   B.2 Training Technicalities   Following ( Mao et al . , 2020 ) , the pre - trained base   model is the rnn - ext + RL model from Chen and   Bansal ( 2018 ) , and is trained like in Lebanoff   et al . ( 2018 ) . Both M andM are further   trained on our adjusted DUC 2007 data using an   Adam optimizer with a learning rate of 5e-4 and no   weight decay . A discount factor of 0.99 is used for   the reinforcement learning rewards . The batch size   was 8 . Training was halted once 30 consecutive   epochs did not improve the validation score .   The MMR function within our models uses TF-   IDF vector cosine similarity for all Sinstances   ( in Equations 1 and 12 ) . The TF - IDF vectorizer   is initialized with the document set on which the   MMR score is computed .   As is commonly practiced , selection of an out-   put sentence / phrase eis done by sampling prob-   ability distribution p(in Equation 8) at train   time , and by extracting the maximum scoring sen-   tence / phrase at inference time .   The MLP in Equation 3 transforms the MMR   score with a feed - forward network with one - hidden   layer of dimension 80 following ( Mao et al . , 2020 ) .   B.3 Query - Assisted Summarization Model   Model configurations . The architecture of the   M model and its training allowed for much   creativity in the configuration process . Other than   the combinations mentioned in the paper in Table   1 , we also experimented with other components .   We list here many of the experiments , without for-   mal results . Anecdotes are taken by looking at   validation scores and some eyeballing .   ( 1 ) The βvalue in the query - focused MMR   function in Equation 2 , that impacts the weight of   the query on a sentence versus the document set   on the sentence . We tried out a few βvalues and   mainly noticed that a value of 0.5kept validation   results more stable across configurations , or kept   training time shorter . In our experiments , to cancel   out this component ( both at training and inference2562time ) , we simply set β= 1so that the query is not   considered .   ( 2 ) Different summary reward functions .   ROUGErecall ( instead of F ) was also a good   alternative , but gave somewhat less stable results   across configurations . ROUGE ( not as ∆ ) was also   less stable with recall and F , and gave too short   and irrelevant sentences with precision . We also   tried sentence level ROUGE -L , like in ( Mao et al . ,   2020 ) , eventually outputting sentences that were   much less compliant to queries .   ( 3 ) Using only the query similarity reward in-   stead of the dual reward mechanism worked sur-   prisingly well . This may be due to the queries on   which the model was trained on . These queries   were very relevant to the gold reference summaries ,   hence possibly implicitly providing a strong signal   to salient sentences within the document set . Still ,   this was less productive than our final choice of   reward .   ( 4)Adding training data ( additional DUC   MDS datasets ) did not impact the results . Impor-   tantly , since DUC 2007 is most similar to the test   DUC 2006 set , it seems to be more beneficial to   include DUC 2007 in the training set .   ( 5 ) We also tried representing the query in the   input by concatenating it ’s raw text to each input   sentence before get the sentence representations .   ( 6 ) To represent the sentences , we also tried   using average w2v vectors ( Honnibal and Montani ,   2021 ) and Sentence - BERT ( Reimers and Gurevych ,   2019 ) instead of the CNN network . These did   not show any apparent improvements , and were   notably expensive in terms of execution time .   ( 7 ) For the sentence similarity in the query-   MMR component , we tried w2v and Sentence-   BERT representations instead of TF - IDF vectors .   Similarly to ( 6 ) , they did not show improvements   over using TF - IDF , and were very time - costly .   ( 8) Instead of the dual - reward mechanism that   alternates between the two rewards from batch to   batch , we also considered using a weighted average   of the two rewards , consistently over all batches .   Further experimentation is required on this tech-   nique for a more conclusive judgment .   Queries used for training . The queries used for   training the M model can affect the way it   learns to respond to a query . Seemingly , the most   natural approach would be to train the model as   close as possible to the model ’s use at inference   time . This would mean training M withqueries from real sessions . However , a session ’s   queries are dependent on outputs previously pro-   duced by the used system . It is therefore not certain   that the sequence of queries from a different sys-   tem ’s usage would necessarily benefit the training   process when compared to a synthesized sequence   of queries . I.e. , it ’s not actually possible to train   with “ real sessions ” in a conventional way .   Also , as stated in § 3.3 , the synthetic queries we   eventually used direct the model to select salient   sentences , which can support our dual - objectives :   to get a sentence that is both globally salient to the   topic , as well as responsive to the query . We tried   training on other query types , synthesized with   various keyphrase extraction techniques , and found   that our final choice of queries more consistently   gave good results overall .   Sentence length . We segmented the sentences in   the document sets with the NLTKsentence tok-   enizer , and removed sentences that contain quotes   in them or do not end with a period .   During training we did not constrain the input   sentences in any way . Some of the configuration ex-   periments described above were done to check how   the configuration might influence the length of the   selected sentences . The best configurations , includ-   ing the one we eventually used in our tests , tended   to output somewhat longer sentences . Very long   sentences are usually tedious for human readers ,   and we hence limited the sentences to 30 tokens at   inference time . We found that this length constraint   caused a slight degradation in simulation score re-   sults of our models , however still gave superior   informativeness results compared to the baseline   system .   Initial summary length . Sentences are accumu-   lated until surpassing 75 tokens . Therefore sum-   maries are not shorter than 75 tokens , but mostly   not much longer than that .   B.4 Suggested Queries Extraction Model   Model configurations . We experimented with   different configurations and hyper - parameter fine-   tuning in the M model as well . Tuning was   performed in accordance to the validation scores   and generic keyphrase extraction scores on the   MK - DUC-01 multi - document keyphrase extraction   dataset of Shapira et al . ( 2021a).2563(1 ) In the reward function in Equation 13 , we   setγ= 0.5andγ= 0.9 , i.e. , the preceding   output phrases are more strongly accounted for than   the phrases in the session history . We tested several   values between 0 and 1 for both hyper - parameters .   ( 2 ) We implemented altered versions of the re-   ward function in Equation 13 . Instead of phrase   unigram - level frequency , we tried computing the   full phrase frequency and computing partial phrase   frequency , i.e. , a maximal phrase template match   within a reference summary . All functions tested   were adequate overall , though our final choice of   reward function was closest to the keyphrase ex-   traction task unigram overlap metric , and gave best   results overall .   ( 3 ) We also attempted noun phrase extraction   with the spaCynoun chunker and named entity   recognizer . This combined approach misses some   noun phrases within the text , but mainly is also   more computationally heavy than the simple POS   regex search that we use .   Extracting phrases with regular - expression .   We extracted all noun - phrases from the docu-   ment set by first mapping all tokens to their   part - of - speech tags , and then applying a regular-   expression chunker with regex : { ( < JJ > *   < NN . * > + < IN > ) ? < JJ > * < NN . * > + } .   These steps were accomplished with NLTK .   Phrase length . There is no limit set on the phrase   length . We tried training and inferring with a   phrase length constraint of 4 words , but found that   this gave worse results overall .   History sentences to phrases . M works on   thephrase level . Meanwhile , in our extractive in-   teractive setting , the history is a set of sentences   already presented to the reader . Therefore , when   extracting phrases from D , we also link each phrase   to its source sentence , and obtain Eby compiling   the phrases linked from the history sentences .   C Dataset Notes   While DUC 2006 ( our test set ) and 2007 ( our   train / validation set ) were originally designed for   the query - focused summarization task , they con-   tain excessive topic concentration due to their long   and descriptive topic queries ( Baumel et al . , 2016 ) .   Hence , their reference summaries can practically   be considered generic . D Session Collection   Controlled crowdsourcing protocol . We fol-   lowed the controlled crowdsourcing protocol of   Shapira et al . ( 2021b ) , which includes three steps :   ( 1 ) a trap task for finding qualified workers ; ( 2 )   practice tasks for explaining the interface and the   purpose , as well as reiterating the generic infor-   mation goal ( see below ) during exploration ; ( 3 )   the session collection tasks . We used the Amazon   Mechanical Turk HITs prepared by Shapira et al .   ( 2021b ) .   Process cost . We paid $ 0.40 for a trap task assign-   ment , with 400 assignments released , and $ 0.90 for   a practice task assignment , with 28 assignments   completed . The session collection assignment paid   $ 0.70 , and a bonus mainly according to the length   of interaction and additional comments provided .   The bonus was between $ 0.15 and $ 0.35 . A total   of 111 sessions were recorded from 6 high qual-   ity workers . The full process cost about $ 385 in   total ( including the Mechanical Turk fees ) for the   experiment including configuration vin Table 1 .   The second round of experiments done on an-   other variant of our system ( configuration i ) also   included 28 practice tasks and compiled 10 fi-   nal workers for a total of 180 collected sessions .   Bonuses ranged from $ 0.10 and $ 0.40 on the ses-   sion collection task . The full process cost of the   second experiment was about $ 475 in total ( includ-   ing the Mechanical Turk fees ) .   Session collection data preparation . We used   the same 20 test topics as Shapira et al . ( 2021b ) ,   and created 2 batches of tasks . For the first batch ,   in alternating order of topics , 10 topics were paired   with our system , and the other 10 were paired with   theSbaseline . The other batch consisted of the   complementing topic - system pairings . The work-   ers were assigned a batch to work on such that half   of the workers would work on each batch .   User informational goal . Since all sessions on   a topic are evaluated against the same reference   summaries , it is important that users aim to ex-   plore similar information . Following Shapira et al .   ( 2021b ) , during practice tasks all users received a   common informational goal to follow , so that the   sessions are comparable . The emphasized descrip-   tion was : “ produce an informative summary draft   text which a journalist could use to best produce an   overview of the topic”.2564Sessions filtering . In the first experiment , we   filtered out 7 sessions that accumulated less than   250 tokens ( from 2 different workers ) .   In the second experiment , 9 of the 10 workers   completed at least 19 of the 20 topics One worker   completed only 3 tasks and we disregarded those   sessions . We also threw away 9 sessions that accu-   mulated less than 250 tokens .   IS user interface . We used the same   user interface developed by Shapira et al . ( 2021b )   with a small change to enable suggested query list   updates after each interaction ( the interface was   designed for the baselines , where the suggested-   query list is static ) . To refrain from any possible   user experience bias , we made the UI change as   least apparent as possible .   System response time . M is able to gen-   erate summaries mostly in under a second , and   M prepares the list in a few seconds . The   summary expansion is hence presented to the user   almost immediately after query submission , and the   suggested queries list is shown shortly afterwords ,   before the user finishes reading the expansion . The   small delay in suggested query updating is hence al-   most unnoticed . The baseline summarizer responds   similarly fast to M , making response - time   difference unperceivable between the systems .   User feedback . Many of the users provided feed-   back about the session collection tasks after finish-   ing their assignment batch . The overall impression   was that there was no strong preference for either   system . For example , one user wrote : “ I did not   discern a consistent difference between the two   systems that would result in having a clear pref-   erence . ” This kind of comment was repeated by   several users . Generally , there were no explicit   comments about the difference in quality of the   summary outputs , and topics were mostly scored   or commented on similarly between the two sys-   tems since the complexity of the topic influenced   the ability of the systems to comply to the user .   A comment in favor of updating suggested   queries during interaction said : “ It was nice to   have a new list as you progressed through the task ,   it helped me think of where to go next if I got stuck ... ”   This specific comment was written by a user that   explored topics quite deeply . On the other hand ,   a user that explored more shallow liked that used   suggested queries in the static list were marked : “ I   did notice ... the red font color on the used queries . That was helpful . ” It therefore seems that updating   suggested queries are more useful for lengthy ex-   ploration , but for quick navigation , the static list   might naturally be enough .   E More Results   E.1 Overall System Assessment   We conducted two comparative session collec-   tion and analysis experiments , one using M   model configuration v(from Table 1 ) , as presented   in § 5.3 and Table 2 , and another with M   model configuration i. As explained in § 5.2 , these   two configurations performed best , on simulations ,   out of their respective configuration sets .   We show here results of the second experiment ,   where we used M model configuration i , with   the same M model as in the first experiment .   TheSbaseline was similarly used for compari-   son . We also kept the same AUC length limits ( 106   to 250 tokens ) for easy comparability to Table 2 .   Table 3 shows the results . Here too , while less   substantially , informativeness is improved with our   system without significantly harming the user expe-   rience . Overall , it seems that users were somewhat   more satisfied with the IS system that uses   M configuration vthan configuration i. Inter-   estingly , it seems the users may have appreciated   the slightly better informativeness of configuration   veven if the query - responsiveness was not as good   as in configuration i , as shown through theS   score . In addition , we see that absolute manual   scores in Table 3 are lower than in Table 2 , but   trends are generally similar . It is common that scal-   ing of manually supplied scores can fluctuate ( e.g.   Gillick and Liu , 2010 ) .   Figures 3 and 4 show the averaged ( per topic and   then over all topics ) recall curves of the collected2565sessions in the experiment described in § 5.3 and   above , respectively . The x - axis is the accumulat-   ing token - length of the session , and the y - axis is   theROUGE -1 recall . The points on the curve are   the average interpolated values from all the ses-   sions . The vertical dashed lines are the intersecting   bounds of the sessions , from 106 tokens to 250 .   The area under the curve ( AUC ) is computed for   each of the curves , and reported in the first row of   Tables 2 and 3 . The higher AUC scores obtained   from the recall curves of our models , compared   to those of the Sbaseline , highlight the ability   to expose more salient information earlier in the   session .   E.2 Execution Time of Systems   Systems that are made for interacting with humans   must respond quickly in order to keep the user ’s   engagement . The exact amount of time does not   affect the user experience as long as it does not   surpass some limit , after which the user starts los-   ing interest or feeling irritated ( Attig et al . , 2017 ;   Anderson , 2020 ) .   As mentioned in Appendix D , M generates   summaries in under a second and M prepares   the list in a few seconds . The baseline summarizer   also responds in under a second . The difference   between the systems is virtually unperceivable dur-   ing interaction . There were no comments from the   users in our experiments that stated any issue with   execution time . E.3 Assessment of Suggested Queries   Functionality   In this analysis , we assessed what modes of query   submission users relied on over the course of a   session . To that end , ( 1 ) we divided each session   to three segments ( first , second and third part of   the session ) , and counted the types of queries . The   types are “ suggested query ” , “ free - text ” , “ highlight ”   ( a span from the summary text ) and “ repeat ” ( re-   peating the last submitted query ) . ( 2 ) We then com-   puted the percentage of each mode in each segment .   ( 3 ) The percentages over all sessions and all topics   were computed for each of the three segments .   This process was conducted only for sessions   between 4 and 20 interactions , as the few long and   short sessions often show different behavior . For   the first experiment , this left 43 sessions with avg .   8.63 ( std . 2.32 ) interactions for our system , and 50   sessions with 8.44 ( 2.48 ) interaction for S. For the   second experiment , it left 72 sessions with 10.24   ( 4.82 ) interactions for our system , and 74 sessions   with 9.59 ( 4.42 ) interactions for S.   We focus here on the use of suggested queries   versus all other query types . In the first experiment   we observe a change of +9 % from the first to the   third segment in our system , and -20 % in S. In the   second experiment we see +18 % and -28 % in S.   As discussed in § 5.3 , this suggests the effectiveness   of updated suggested queries , especially by the end   of a session.2566F Further Explanations on Evaluation   Metrics   The normalized AUC score for the validation   metric ( explained in § 3.3 ) is computed over the   recall curve produced from the accumulating sum-   mary expansions . Each point on the curve marks   an accumulating token - length ( x - axis ) and an accu-   mulating recall score ( y - axis ) of an interactive state ,   as depicted in Figures 3 and 4 ( although these fig-   ures show the averaged session recall curves with   bounds , whereas during validation the curve is for   a single session and there are no bounds set ) . By   computing the area under the full curve , and divid-   ing by the full length , the normalized AUC score is   obtained . The normalization gives an approximate   absolute value that can be compared at different   lengths ( although at large length differences this is   not comparable due to the decaying slope of the   curve ) .   The manual query - responsiveness score , re-   ported in Tables 2 and 3 , is obtained by asking   users , at the end of a session , “ During the inter-   active stage , how well did the responses respond   to your queries ? ” , for which they rate on a 1 - to-5   scale . The scores are averaged over the topic and   then over all topics . This follows the evaluation   defined in Shapira et al . ( 2021b ) .   TheUMUX - Lite score ( Lewis et al . , 2013 ) , re-   ported in Tables 2 and 3 , is obtained by asking   users to rate ( 1 - to-5 ) two statements at the end of   a session : ( 1 ) “ The system ’s capabilities meet the   need to efficiently collect useful information for   a journalistic overview ” and ( 2 ) “ The system is   easy to use ” . The first question refers to the users ’   informational goal that they received , in order to   follow a consistent objective goal during their ex-   ploration . The final score is a function of these two   scores , and is used as a replacement for the popular   SUS metric ( Brooke , 1996 ) ( with a much longer   questionnaire ) , to which it shows very high cor-   relation , thus offering a cheaper alternative . This   also follows the evaluation defined in Shapira et al .   ( 2021b ) .   Allconfidence intervals in Tables 1 , 2 and 3   are computed as margins - of - error , on the topic-   level , over the standard error of the mean with 95 %   confidence .   Thetoken - length values in Table 1 are averages   with standard deviations . G A2C Policy Learning   A policy gradient - based reinforcement learning ap-   proach ( Williams , 1992 ) allows optimizing on non-   differentiable metrics , and eliminates the exposure   bias that occurs with traditional training methods ,   like cross - entropy , on generation tasks ( Ranzato   et al . , 2016 ) .   Specifically , we use the Advantage Actor Critic   ( A2C ) policy gradient training method . See tech-   nical explanations in the appendix of ( Chen and   Bansal , 2018 ) . At a high level , an output reward   ( subtracted by a baseline reward – computed on a   version of the model without MMR attention ) is   used to weight the output selection in the loss func-   tion . In so , outputs with higher rewards increase   the likelihood of those outputs and lower rewards   decrease the likelihood . Since the reward function   is not differentiable , it is used as a weight on the   probability of the selected output , which is then   given to the loss function .   H IS Example   We show in Figure 5 an example of an IS   system using the web application of Shapira et al .   ( 2021b ) and our our M ( configuration ifrom   Table 1 ) and M models in the backend.25672568