  Shiv Shankar   University of Massachusetts   sshankar@cics.umass.edu   Abstract   Information integration from different modal-   ities is an active area of research . Human be-   ings and , in general , biological neural systems   are quite adept at using a multitude of signals   from different sensory perceptive fields to in-   teract with the environment and each other . Re-   cent work in deep fusion models via neural   networks has led to substantial improvements   over unimodal approaches in areas like speech   recognition , emotion recognition and analysis ,   captioning and image description . However ,   such research has mostly focused on architec-   tural changes allowing for fusion of different   modalities while keeping the model complexity   manageable . Inspired by neuroscientific ideas   about multisensory integration and processing ,   we investigate the effect of introducing neu-   ral dependencies in the loss functions . Experi-   ments on multimodal sentiment analysis tasks   with different models show that our approach   provides a consistent performance boost .   1 Introduction   Human beings perceive the world as a unified   whole , not in individual sensory modalities . While   traditionally different sensory models have been   studied in isolation , it has been well recognized   that perception operates via integration of informa-   tion from multiple sensory modalities .   Research in multimodal fusion aims to achieve   a similar goal in artificial models : extract and in-   tegrate all information from different input modal-   ities . For example , if someone is sarcastic , the   facial expression and voice intonation provide in-   formation not directly decipherable from the ut-   tered words . If a model only looks at the text of   the interaction , then it is unlikely to classify this   interaction currently . Current research in deep mul-   timodal fusion primarily deals with architectural   improvements to create complex feature - rich , yet   efficient representations ( Zadeh et al . , 2017 ; Liu   et al . , 2018 ; Hazarika et al . , 2020 ) . The hope isthat more complex models will be able to integrate   the complementary information from different uni-   modal representations into a unified common rep-   resentation . Learning such unified representations ,   however , is a challenging task . Different modalities   can present the same information in radically dif-   ferent ways with emphasis on different aspects of   the content . These heterogeneities across different   modalities mean that learning multimodal represen-   tations must deal with feature shifts , distributional   effects , nuisance variation and a variety of related   challenges ( Baltrušaitis et al . , 2018 ) .   Inspiring from work in multisensory neural pro-   cessing , we define a loss regularizer that we call   synergy to train these models . Synergy has a spe-   cific meaning in information - theoretic literature   ( Cover , 1999 ) . The synergy between random vari-   ables XandYrefers to the unique mutual infor-   mation that Xprovides about Y. While our loss   function is not the same as information theoretic   synergy , the intuition behind our proposed loss is   the same as actual synergy ; to try to maximize de-   pendencies between the representations . As our   method uses neural networks or kernel - based meth-   ods to capture distributional divergences , we expect   that this method will allow our model to capture   complex dependencies which can not be captured   via techniques like subspace alignment .   We test our proposed training loss on dif-   ferent multimodal fusion architectures including   LFN(Zadeh et al . , 2017 ) , MFN ( Zadeh et al . ,   2018a ) , MAGBERT(Rahman et al . , 2020 ) and   MIM ( Han et al . , 2021 ) . Our experiments show   that training with synergy maximization improves   the result by a significant margin .   2 Preliminaries   In this section , we give an overview of the basic   ideas relevant to this work ; primarily mutual in-   formation , and existing work on deep multimodal   fusion and neural synergy.11672.1 Multimodal Fusion   The problem in the most abstract terms is a super-   vised learning problem . We are provided with a   dataset of Nobservations D= ( x , y ) . Allx   come from a space XandyfromY. We are pro-   vided a loss function L : Y × Y → Rwhich is the   task loss . Our goal is to learn a model F :X → Y   such that the total loss L = PL(F(x ) , y)is   minimized . In multimodal fusion the space of   inputs Xnaturally decomposes into Kdifferent   modalities X = QX . We use Xto repre-   sent random variables which form the individual   modality specific components of the input random   variable X.   A common way to learn such a multimodal func-   tion is to decompose it into two components : a ) an   embedding component Ewhich fuses information   into a high dimensional vector in Rand b ) a pre-   dictive component Pwhich maps vector from R   toY. Furthermore since the different modalities   are often no directly compatible with each other   ( for eg text and image ) , Eitself is decomposed into   a ) modality specific readers FX→Rwhich are   specifically designed for each individual modality   Xand b ) a fusion component F : QR→R   which fuses information from eah individual modal-   ity embedding . Fis provided with uni - modal rep-   resentations of the inputs X= ( X , X , . . . X )   obtained through embedding networks f. Fhas   to retain both unimodal dependencies ( i.e relations   between features that span only one modality ) and   multi - modal dependency ( i.e relationships between   features across multiple modalities ) .   This decomposition has two advantages a ) the   individual modality reader can be pre - trained on   the task at hand or even from a larger dataset ( for   example BERT ( Devlin et al . , 2018 ) for language ,   Resnet ( He et al . , 2016 ) for images ) which allows   us to leverage wider modality specific information   and b ) often but not always each individual modal-   ity is in principle enough to correctly predict the   output   2.2 Distributional Divergences   Divergence is a functional which characterizes the   distance or " discrepancy " between two probabil-   ity distributions on the same space . Divergence   however is a different notion than distance because   divergences are not necessarily symmetric . A com-   mon measure of discrepancy between two distribu-   tions is the Kullback - Liebler divergence ( KL diver - gence ) ( Cover , 1999 ) . The KL divergence of the   density prelative to the density qis given by   d(p;q ) = E   logp(x )   q(x)   This divergence is often also used implicitly for   estimating dependence between two random vari-   ables . Mutual information ( MI ) is a measures of   dependence between two random variable Xand   Ycapable of incorporating multiple types of rela-   tionships between them . If we have variables X   andY , then the mutual information between them   is given by   I(X;Y ) = KL[p(x , y)∥p(x)p(y ) ]   where pis the joint probability density of the   pair(X , Y ) , andp , pare the marginal probabil-   ity densities of X , Y respectively .   Estimation of Divergence Estimating entropic   differences between two distributions purely from   their samples is a difficult task ( Kinney and Atwal ,   2014 ) . As such there have been multiple types of   divergences proposed over the years ( Gretton et al . ,   2005 ; Studen ` y and Vejnarová , 1998 ) . Moreover   in recent years , several estimators have been pro-   posed for entropic divergences based on variational   methods ( Belghazi et al . , 2018 ; Hjelm et al . , 2018 ;   Amjad and Geiger , 2019 ) . These estimators use   flexible neural networks as a contrast function and   optimize a variational bound . We describe two   such methods which are used in our experiments   •Neural Mutual Information ( Belghazi et al . ,   2018 ) is a variational method to estimate   the KL divergence between two distribu-   tions . It is estimated via gradient ascent on   the Donsker - Varadhan bound ( Donsker and   Varadhan , 1985 ) . The Donsker Varadhan   bound shows that :   KL(P , Q)≥supE[g(X ) ]   −E[exp ]   The Young - Fenchel duality shows that the gap   is zero ; i.e. at the optima the right side of   the above expression matches the KL diver-   gence . Instead of a global maximization over   all functions one can instead use a family of1168functions parameterized via neural networks .   The bound obtained thus is necessarily lower   than the actual KL , but now one can use gra-   dient descent to optimize the network .   •Maximum Mean Discrepancy or MMD   ( Gretton et al . , 2012 ) is a kernel based es-   timator of divergence between distributions .   Mathematically the MMD between two distri-   butions PandQis given by the norm of the   difference of the mean embeddings of Pand   Qin the RKHS space of the chosen kernel .   Further extensions to MMD have been devel-   oped based on neural networks which provide   non - universal but more powerful kernel based   tests ( Liu et al . , 2020 ) .   MMD ( P , Q ) = ||µ−µ||   The above formula can be estimated purely   via samples by using the Kernel matrix   K(x , x ) = ϕ(x)ϕ(x)where ϕrepre-   sents the corresponding RKHS embedding   function The final monte carlo estimator is   given by :   MMD ( P , Q ) = XK(p , p )   + XK(q , q )   −X2K(p , q )   2.3 Kurtosis   Kurtosis is a statistical measure which is used to   categorize the behavior of the distribution tails . It is   more sensitive to rare events and hence is used for   distributions with " fatter tails " . For univariate vari-   ables , kurtosis is the standardized fourth moment   i.e   E[(X−µ ) ]   ( E[(X−µ ) ] )   It is often used to measure deviations from normal-   ity . Mardia ( 1970 ) defined a measure of multivari-   ate kurtosis as follows :   E[((X−µ])Σ(X−µ ) ) ]   where Xis ap×1dimensional random vector   andµ,Σare the mean and covariance matrix of   Xrespectively . Multivariate cokurtosis betweenrandom variables is also sometimes used as a mea-   sure of dependence between them . It is one of the   metrics used by Rosas et al . ( 2019 ) ; Barrett and   Seth ( 2011 ) to analyze neural complexity and brain   functional connectivity .   2.4 Other works on multimodal fusion   Earlier work on neural fusion models primarily   relied on an early fusion of features . These ap-   proaches simply concatenated inputs of different   modalities and used simple models to combine   requisite information . Despite their simplicity ,   such models often perform well and are robust   ( Narayanan et al . , 2019 ) . More modern methods ,   however , deploy fancier methods to induce informa-   tion aggregation . One set of models used gradient   descent to try to force different feature networks   to learn about each other and embed information   jointly . This process can be enhanced by adding   specific forms of regularization such as recon-   struction loss ( Mai et al . , 2020 ) , or auxiliary task   loss ( Chen et al . , 2017 ; Yu et al . , 2021 ) . Another   family of models uses linear algebra based methods   to combine unimodal representations . Methods like   those of Liu et al . ( 2018 ) ; Chen and Mitra ( 2018 ) ;   Chachlakis et al . ( 2019 ) try to fuse information via   tensor decomposition of high dimensional prod-   uct tensors of individual unimodal representations .   Other methods use subspace alignment ( Lee et al . ,   2019 ; Yu et al . , 2012 ) or correlation loss ( Sun et al . ,   2020 ; Hazarika et al . , 2020 ) to merge different rep-   resentations . However , in some form or other , these   models rely primarily on architectural changes . We ,   on the other hand , do not want to focus on such   changes . Instead , our goal was to use insights from   neuroscience to provide a methodology that can   be deployed atop any standard multimodal fusion   model .   3 Dependency Coding in Multisensory   Processing   A common and vital feature of nervous systems   is the integration of information arriving simul-   taneously from multiple sensory pathways . The   underlying neural structures have been found to be   related in both vertebrates and invertebrates . The   classic understanding of this process is that differ-   ent sensory modalities are processed individually   and then combined in various multimodal conver-   gence zones , including cortical and subcortical re-   gions ( Ghazanfar and Schroeder , 2006 ) , as well as1169   multimodal association areas ( Rauschecker et al . ,   1995 ) . Studies in the superior colliculus ( Meredith   et al . , 1987 ) showed that multiple sensory modal-   ities are processed in this brain stem region , with   some neurons being exclusively unimodal and oth-   ers being multimodal . Hypotheses of encoding of   multimodal information include changes in neu-   ronal firing rates ( Pennartz , 2009 ) or a combinato-   rial code in population of neurons ( Osborne et al . ,   2008 ; Rohe and Noppeney , 2016 ) .   Evidence shows that while multimodal represen-   tations are distinct from unimodal ones , there is   sufficient overlap between the set of neurons that   process different sensory modalities . For example ,   Follmann et al . ( 2018 ) show that even in a simple   crustacean organism , more than half the neurons in   the commissural ganglion are multimodal . More-   over , they show that in 30 % of these multimodal   neurons , responses to one modality were predictive   of responses to other modalities . Both these facts   suggest that the neural representations across dif-   ferent modalities have high information about each   other .   Studies of multisensory collicular neurons sug-   gest that their crossmodal receptive fields ( RF ) of-   ten overlap ( Spence et al . , 2004 ) . This pattern is   also found in multisensory neurons present in other   brain regions . As such , a spatiotemporal hypothe-   sis of multisensory integration has been suggested :   superadditive multimodal processing is observed   when information from different modalities comes   from spatiotemporally overlapping receptive fields(Recanzone , 2003 ; Wallace et al . , 2004 ; Stanford   et al . , 2005 ) . Since multimodal cortical neurons are   generally downstream of modality - specific regions ,   the information about RF overlap is present in their   input unimodal neural representations . Moreover ,   the sensory - specific nuclei of the thalamus have   been shown to feed multisensory information to   primary sensory specific - cortices ( Kayser et al . ,   2008 ) . This suggests the existence of explicit feed-   back connection from the multimodal representa-   tions to unimodal representations .   Cortical and subcortical networks often contain   clusters of strongly connected neurons . Function-   ally the existence of such cliques imply highly in-   tegrated pyramidal cells that handle a dispropor-   tionately large amount of traffic ( Harriger et al . ,   2012 ) . In cortical circuits , around 20 % of the neu-   rons account for 80 % of the information propa-   gation ( Nigam et al . , 2016 ; Van Den Heuvel and   Sporns , 2011 ) . Timme et al . ( 2016 ) ; Faber et al .   ( 2019 ) demonstrate that multimodal computation   tends to concentrate in such local cortical clusters .   They also found significantly lower kurtosis in such   clusters and that dependence between oscillations   was proportional to the amount of information flow .   Sherrill et al . ( 2020 ) show that highly kurtotic neu-   ral activity positively related when multiple exter-   nal stimuli are provided . Thus , kurtosis in neural fir-   ings is a representation of the dependence between   inputs . This suggests that when input kurtosis is   high there is more significant cognitive processing   and information flow required to extract relevant1170information .   4 Model   For our purposes we will limit ourselves to talk   about tasks similar to the MOSI dataset . In this   setting the input has three modalities viz audio ( a ) ,   visual ( v ) , and textual language ( l ) . The fusion   problem involves learning a representation M   that combined the uni - modal representations of the   inputs X= ( X , X , X ) .   4.1 Dependency Coding and C - Network   We modify the base neural architecture to incor-   porate the global structure explained in the last   section . We propose a way to incorporate such   changes without major architectural change into   current baseline designs . The key component is   the additional network ( colored in red ) in Figure 1   which we shall call as C - network . The C - network   takes as input the individual unimodal representa-   tions and the fused representation and attempts to   force a specific form of dependency as explained   below .   C - Network The purpose of the C - Network is   to try to enforce on the model the three primary   characteristics of real neural circuits explained in   the earlier section . We list them here and describe   how we attempt to incorporate those characteristics   in a more standard model .   •Individual uni - modal representations should   be predictive of other uni - modal representa-   tions . We try to achieve this by simply predict-   ing on modality representation by the combi-   nation of others . Qrefers to a modality as-   sociated neural network which attempts to re-   construct the unimodal representation Zfrom   the other representations Z. The error be-   tween the two is penalized in the form of a   reconstruction loss between modalities i.e. we   add a penalty of the form :   L=||Q(Z)−Z||   •Multimodal representation should be feedback   into input neurons to align and capture infor-   mation between them . Providing feedback   during inference time from the multimodal   representation would be ideal . However this   would make the overall prediction recurrent ,   something fundamentally different from mostcurrent architectures . Moreover given current   high dimensional encoders ; doing such pro-   cessing would be extremely resource inten-   sive . As such we aim to achieve this feedback   by treating the multimodal representation and   unimodal representation spaces as different   domains and adding a loss of the form :   L = d(p(g(Z ) ) , p(g(Z ) ) )   The purpose of the aforementioned loss is to   align the distributions of the features in the   same embedding space of the mapping from   the multimodal and unimodal domains . drep-   resents a measure that captures the discrep-   ancy between the distributions , grefers to   neural networks for projecting and aligning   the combined representation Zwith unimodal   representations Z , and pdenotes the empiri-   cal / sample distribution of the corresponding   features . In our experiments , for dwe use the   MMD discrepancy ( Gretton et al . , 2012 ) and   KL divergence as the metric ; though other di-   vergences can also be used . Note that this loss   by itself can be minimized by forcing the g   functions to ignore their inputs . We prevent   this by first doing a random projection of the   featuresinto a smaller dimensional vector   space and then apply an invertible neural net-   work . Such alignment losses have been used   in works on domain adaptation ( Motiian et al . ,   2017 ) under the name semantic loss or con-   fusion loss . We refer the readers to Motiian   et al . ( 2017 ) ; Li et al . ( 2019 ) for more details   on semantic losses .   Note that instead of aligning the features via   some kind of embedding based distributional   distance , one could try to maximize mutual   information between the embeddings as well .   We experiment with one such model in our   experiment and as the results show , found it   to be slightly worse than using MMD based   alignment loss .   •Individual unimodal and multimodal represen-   tations should have low kurtosis . To ensure   this condition we estimate the multivariate kur-   tosis by plugging in standard estimators for   the mean and covariates . The final kurtosis   estimator used is given by:1171κ=1   nX[((z−¯z)S(z−¯z ) ) ]   where zhere are samples from the Zfeatures   in the model ( where Zcan be unimodal   features like Zor fused final feature Z).¯z   refers to the empirical mean feature ¯z=   andSis the empirical covariance matrix   S=.   An important thing to note here is that high   dimensional kurtosis values can be highly sen-   sitive to outliers . As such we regularize the   estimate by doing three things : a ) We cap the   max norm of the difference vectors during   estimation . b ) We scale up the diagonal of   the covariance matrix to reduce its condition   number c ) Finally the covariance matrix itself   is computed via a decaying moving average   over a window of multiple batches to produce   smoother estimates before the inversion oper-   ation .   During training we add the regularization penal-   ties described earlier along with the usual max-   imum likelihood based objective . The different   loss components are weighted with seperate hyper-   parameters . Note that the C - Network is purely a   training time addition , and is not invoked during   inference . Hence the additional network invoke   zero additional time during testing . An algorithmic   description of the full method is presented in the   Appendix D   5 Experiments   5.1 Datasets   We empirically evaluate our methods on two com-   monly used datastes for multimodal training viz   CMU - MOSI and CMU - MOSEI .   CMU - MOSI ( Wöllmer et al . , 2013 ) is sentiment   prediction taks on a set of short youtube video   clips . CMU - MOSEI ( Zadeh et al . , 2018b ) is a   similar dataset consisting of around 23k review   videos taken from YouTube . The output in both   cases is a sentiment score in [ −3,3 ] . For each   dataset , three modalities are available ; audio , visual   frames , and language . Preliminary features on each   modality is obtained as follows:•Audio : Features are extracted from the sund   recordings using the method of Degottex et al .   ( 2014 ) .   •Language : The video transcripts are converted   to word embeddings using BERT ( Devlin   et al . , 2018 ) or Glove ( Pennington et al . , 2014 )   •Visual : Visual features are extracted using   FACET ( iMotion ) which provides facial ac-   tion units vectors .   5.2 Models   We run our experiments with the following archi-   tectures :   •FLSTM ( Narayanan et al . , 2019 ) is the base-   line early fusion LSTM architecture used by   Zadeh et al . ( 2017 )   •Tensor Fusion Network or TFN ( Zadeh et al . ,   2017 ) combined information via pooling of   a high dimensional tensor representation of   multimodal features . More specifically it does   a multimodal Hadamard product of the ag-   gregated features with RNN based language   features .   •Memory Fusion Network or MFN ( Zadeh   et al . , 2018a ) incorporate gated memory - units   to store multiview representations . It then per-   forms an attention augmented readout over   the memory units to combine information into   a single representation .   •MAGBERT ( Rahman et al . , 2020 ) is a trans-   former based architecture that uses the Wang   gate ( Wang et al . , 2019 ) . The multimodal   information is send to the multimodal gate   to compute modified embeddings which are   passed to a BERT ( Devlin et al . , 2018 ) based   model . This model achieves state - of the - art   results on multimodal sentiment benchmark   MOSI ( Wöllmer et al . , 2013 ) and MOSEI   ( Zadeh et al . , 2018c ) .   •MIM ( Han et al . , 2021 ) is a recent near SOTA   architecture . It combined BERT based text   embeddings with modality specific visual and   acoustic LSTMs ( Hazarika et al . , 2020 ) .   •Recently Colombo et al . ( 2021 ) conducted   experiments introducing a information regu-   larizer on existing architectures . The main   differences between the our method and their1172method are a ) our method focuses on synergy   terms whereas their proposal is optimizing   joint mutual information between different   unimodal representations ; and b ) they experi-   ment with variational measures of information .   We replicate our experiments with their best   performing model and present the results with   the label I.   Split CMU - MOSI CMU - MOSEI   Train 1284 16326   Validation 229 1871   Test 686 4659   All 2199 22856   5.3 Evaluation   We report both the Mean Absolute Error ( MAE )   and the correlation of model predictions with true   labels . In the literature , the regression task is also   turned into a binary classification task for polarity   prediction . We follow Rahman et al . ( 2020 ) Accu-   racyAccdenotes accuracy on 7 classes and Acc   the binary accuracy ) of our best performing models .   We also report the Mean Absolute Error ( MAE ) and   the correlation of model intensity predictions with   true values .   5.4 Results   We present and discuss here the results obtained in   our experiments . Results on MOSI are presented   in Table 2 while Table 3 present results for MOSEI   dataset . We trained each of the models with the   standard cross entropy loss ( labeled as NLL ) ; and   with cross entropy loss regularized with the syn-   ergy penalty discussed earlier . On both datasets ,   regularization via synergy leads to performance im-   provement . For example , a MFN on CMU - MOSI   trained with MMD based synergy ( NLL+ S )   outperforms by more than 4 points on Accthan   standard likelihood training . On CMU - MOSEI too   the gains are significant when trained with synergy   regularization . In general training via MMD syn-   ergy tends to be better than via KL synergy . This   might be the inherent behavior of the MMD depen-   dency which is always well defined ; or it might   reflect the hardness of information estimation . For   example it is well known that good bounds on stan-   dard mutual information are difficult to obtain ( Kin - AccAccMAE CORR   FLSTM   NLL 31.2 75.9 1.01 0.64   NLL+S 31.6 76.3 1.01 0.66   NLL+S 33.6 76.4 0.98 0.66   MFN   NLL 31.3 76.6 1.01 0.62   NLL+S 32.5 76.6 0.94 0.65   NLL+S 35.9 77.4 0.95 0.66   NLL+I 35.1 77.1 0.97 0.63   LFN   NLL 31.9 76.9 1.01 0.64   NLL+S 32.6 77.6 0.97 0.64   NLL+S 35.4 77.9 0.97 0.67   NLL+I 32.4 77.6 0.97 0.64   MAGBERT   NLL 40.2 83.7 0.79 0.80   NLL+S 41.9 84.1 0.76 0.82   NLL+S 41.9 85.6 0.76 0.82   NLL+I 41.8 84.2 0.76 0.82   MIM   NLL 46.3 83.7 0.77 0.76   NLL+S 46.4 83.7 0.74 0.75   NLL+S 46.7 84.2 0.72 0.79   NLL+I 46.6 84.2 0.75 0.79   ney and Atwal , 2014 ) ; while MMD estimator are   asymptotically consistent ( Gretton et al . , 2012 )   5.5 Modality Dropout   Zadeh et al . ( 2018a ) ; Rahman et al . ( 2020 ) have   demonstrated that while multimodal fusion does   improve performance , the primary modality con-   tinues to be textual data . Hence in this experi-   ment , we want to assess the effect of corruptions   of text modality in our model . Following Colombo   et al . ( 2021 ) we experiment with dropping the text   modality either by itself ( T ) or with one of the other   modalities ( T+V or T+A ) . The results are presented   in Table 4   Since the C - Networks forces a reconstruction   and distributional divergence loss between the uni-   modal and multimodal representations , one would   expect that models trained using our approach   would be more resistant to modality errors . This   is borne out in the experiments , where we see that1173AccAccMAE CORR   FLSTM   NLL 44.1 75.1 0.72 0.52   NLL+S 44.4 75.6 0.70 0.52   NLL+S 45.3 76.0 0.68 0.54   MFN   NLL 44.3 74.7 0.72 0.52   NLL+S 44.3 74.8 0.72 0.56   NLL+S 46.2 75.1 0.69 0.56   NLL+I 45.1 75.2 0.72 0.54   LFN   NLL 45.2 74.3 0.70 0.54   NLL+S 46.1 75.3 0.69 0.56   NLL+S 46.3 75.3 0.67 0.56   NLL+I 45.9 75.1 0.69 0.55   MAGBERT   NLL 46.9 83.9 0.59 0.77   NLL+S 47.4 85.3 0.59 0.79   NLL+S 47.9 85.4 0.59 0.79   NLL+I 47.2 85.0 0.59 0.78   MIM   NLL 53.3 79.6 0.54 0.75   NLL+S 53.5 80.3 0.54 0.77   NLL+S 54.3 82.4 0.52 0.77   NLL+I 53.5 82.1 0.53 0.77   training with synergy based loss has better perfor-   mance than training with simple max - likelihood .   Note that the C - network itself is not active at   test time ; instead this effect is due to the align-   ment forced by the network during training . An   interesting future direction would be to explicitly   use the C - network outputs to ameliorate modality   corruption .   Drop Modality None T T+V T+A   NLL 83.7 36.4 35.1 34.4   NLL+S 85.6 48.3 46.7 45.9   NLL+S 84.1 46.8 45.9 45.55.6 Ablation Study   Our overall proposal has multiple components viz   a ) the reconstruction loss ( also called Lloss ) ; b )   the distribution alignment loss ( which we call L   Loss ) ; and c ) the kurtosis loss L. As such we ran   experiments to assess the importance of each com-   ponent . Specifically we trained the model without   each of the three loss components prescribed in our   method , and assessed the test performance . The   results are presented in Appendix A.   First we note the performance improvement by   incorporating kurtosis in the regularization which   shows the efficacy of this term . Second one can   also note that removing any individual component   leads to reduction in performance , suggesting all   components act together in a synergistic way to   improve the results .   6 Concluding Remarks   In this paper , we used the idea of regularizing via a   term which we label neural synergy maximization .   This regularizer is inspired by neural cicruit design   in the vertebral cortex . We experimented with dif-   ferent measures of synergy based on discrepancy   measures such as KL and MMD . We also show that   training with synergy can produce benefit on even   SOTA architectures .   Limitations The most prominent limitation of   this approach , is that it is inherently limited by the   architecture with which it is being used . While our   additional loss did improve performance , one can   observe that the final performance is dependent on   the initial performance . For example , while we   tested on four architectures , the final performance   of each model was in the same range as the initial   performance . An entirely different architecture   can possibly improve over our results . On the other   hand our approach is model agnostic and applicable   on any model trained only via max - likelihood .   References117411751176A Ablation Study Results   ’   In this section we run ablation experiments to   assess the individual impact of each component   of the overall synergy loss . For this purpose we   use the MIM model on the MOSEI dataset , and   the divergence type was chosen to be MMD . We   activated each of the three loss components viz . (   L , L , L ) , trained the MIM model , and   report the test accuracies on all the metrics .   AccAccMAE CORR   MIM   MLE 53.3 79.6 0.54 0.75   + L 53.6 80.0 0.55 0.77   + L 53.1 80.3 0.57 0.73   + L 53.9 79.9 0.54 0.76   + S 54.3 82.4 0.52 0.77   We note the performance improvement caused   by adding the Kurtosis loss . We also note that   directly adding the distributional divergence while   mildly helpful can also degrade the performance .   Each component overall has some value to add over   others . We leave the exact nature of interactions   between these terms for future work .   B Additional Experiments   We present results on the UR_FUNNY dataset   which is another common affective sentiment pre-   diction dataset . The model is evaluated on accuracy   so higher numbers are better .   NLL NLL+ S NLL+ S   MISA 68.6 68.9 69.6   MFN 65.2 66.5 67.2   TFN 64.7 67.3 67.8   C Training details   We perform a grid - search for the best set of hyper-   parameters : batch size in { 32 , 64 } , learning rate1177 in { 1e-2 , 5e-3,1e-3 , 5e-4 , 1e-4 } . We did gradi-   ent clipping with clip value of 5 . Model selection   was done following ( Zadeh et al . , 2017 ) , by select-   ing the model with the best MAE on validation   data . Optimization was done using the AdamW   ( Loshchilov and Hutter , 2018 ) optimizer . For both   theQand the gfunction we used a four layer MLP   with LeakyRelu activation . The dataset statistics   are given in Table 1 . All our experiments were   conducted on Nvidia Titan X GPUs .   D Algorithm   As a reminder the input the fusion problem in-   volves learning a representation Mthat com-   bined the uni - modal representations Zof the input   X= ( X , X , .. , X)where Xare individual in-   put modalities . We shall denote observations as   Xand the fused representations as Z.Q , gare   multi - layer perceptrons.1178