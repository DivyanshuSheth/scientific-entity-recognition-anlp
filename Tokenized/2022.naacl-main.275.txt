  Max BartoloTristan ThrushSebastian Riedel   Pontus StenetorpRobin JiaDouwe KielaUCLUSCFacebook AI Research   m.bartolo@cs.ucl.ac.uk   Abstract   In Dynamic Adversarial Data Collec-   tion ( DADC ) , human annotators are tasked   with ﬁnding examples that models struggle   to predict correctly . Models trained on   DADC - collected training data have been   shown to be more robust in adversarial and   out - of - domain settings , and are considerably   harder for humans to fool . However , DADC   is more time - consuming than traditional data   collection and thus more costly per annotated   example . In this work , we examine whether   we can maintain the advantages of DADC ,   without incurring the additional cost . To that   end , we introduce Generative Annotation   Assistants ( GAAs ) , generator - in - the - loop   models that provide real - time suggestions   that annotators can either approve , modify , or   reject entirely . We collect training datasets in   twenty experimental settings and perform a   detailed analysis of this approach for the task   of extractive question answering ( QA ) for   both standard and adversarial data collection .   We demonstrate that GAAs provide signiﬁcant   efﬁciency beneﬁts with over a 30 % annotation   speed - up , while leading to over a 5x improve-   ment in model fooling rates . In addition , we   ﬁnd that using GAA - assisted training data   leads to higher downstream model perfor-   mance on a variety of question answering   tasks over adversarial data collection .   1 Introduction   Natural language processing has become increas-   ingly reliant on large datasets obtained using crowd   sourcing . However , crowdsourcing as an uncon-   strained annotation approach is known to result in   machine - exploitable annotator artefacts ( Jia and   Liang , 2017 ; Schwartz et al . , 2017 ; Gururangan   et al . , 2018 ; Geva et al . , 2019 ) , leading to poor out-   of - distribution generalisation ( Chen et al . , 2016 ;   Weissenborn et al . , 2017 ; Yogatama et al . , 2019 ;   McCoy et al . , 2019 ) . Dynamic Adversarial Data   Collection ( DADC ) aims to address these issuesFigure 1 : Example interaction between an annotator   and the models in the loop . The annotator selects an   answer from the passage , for which the Generative An-   notation Assistant ( GAA ) prompts a question . The an-   notator can then freely modify the question and/or an-   swer , or generate another prompt . In the adversarial   data collection setting , a model - in - the - loop provides   predictions with the aim of encouraging annotators to   ﬁnd model - fooling examples . In the answer prompting   setting , an answer suggestion is prompted by the assis-   tive model instead of being selected by the annotator .   by introducing state - of - the - art models into the data   collection loop and asking human annotators to   produce examples that these models ﬁnd challeng-   ing ( Kiela et al . , 2021 ) . The intuition behind this   approach is that it leads human annotators to bet-   ter explore the space of possible examples . Previ-   ous work has found that DADC leads to improved   model robustness on adversarial datasets ( Nie et al . ,   2020 ; Bartolo et al . , 2020 ) , increased sample di-   versity ( Bartolo et al . , 2020 ; Wallace et al . , 2021 ) ,   better training data ( Wallace et al . , 2021 ) and better   domain generalisation ( Bartolo et al . , 2021 ) .   Despite these advantages , a downside to DADC   is that it increases the human effort necessary to an-   notate a single example and thus the overall annota-   tion cost . In fact , to date , only a limited number of3754large - scale training datasets have been produced us-   ing DADC and its application has been primarily re-   stricted to producing challenge sets or as additional   training data to improve the performance of models   already trained on non - DADC curated datasets . To   make better use of DADC data , Bartolo et al . ( 2021 )   propose generating synthetic adversarial training   sets to further improve model robustness . However ,   this approach inevitably limits example diversity   as it relies on examples ultimately generated by a   model with no additional human input , and pro-   vides no guarantees that useful synthetic examples   would transfer across target adversary models of   varying capabilities or across annotation rounds .   In this work , we propose assisting annotators   by having generative models aid human annota-   tors in the data collection loop . Concretely , we   utilise a Generative Annotation Assistant ( GAA )   model that provides prompt suggestions to crowd-   workers , while allowing full ﬂexibility for edits   and rewrites to support example generation while   still allowing for human creativity as shown in Fig-   ure 1 . We explore GAAs in a broad range of exper-   imental settings , including standard and adversar-   ial data collection approaches , training on various   source datasets , and employing sampling method-   ologies based on likelihood , adversarial feedback ,   and uncertainty . We showcase the value of this   approach on the task of extractive question answer-   ing ( QA ) , and ﬁnd that GAAs can help improve   both the standard and adversarial data collection   paradigms . We ﬁnd considerable efﬁciency gains ,   with over 30 % observed annotation speed - ups , as   well as improved data effectiveness with up to a   6.1Fimprovement in downstream performance   over adversarial data collection .   2 Related Work   2.1 Dynamic Adversarial Data   Collection ( DADC )   There is a rich body of recent work showing the   beneﬁts of dynamic adversarial data collection in   model evaluation ( Yang et al . , 2017 ; Dua et al . ,   2019 ; Dinan et al . , 2019 ; Nie et al . , 2020 ; Bar-   tolo et al . , 2020 ; Kiela et al . , 2021 ; Wallace et al . ,   2021 ) , although the approach has been challenged   for not necessarily leading to better generalisation   on non - adversarial test sets ( Kaushik et al . , 2021 )   and being sensitive to the choice of model that   was used in the loop ( Bowman and Dahl , 2021 ;   Phang et al . , 2021 ) . This work builds on previ - ous work in adversarial data collection methods   for QA ( Bartolo et al . , 2020 ) , and work investigat-   ing the use of question generation models to create   synthetic adversarial data to improve QA model   robustness ( Bartolo et al . , 2021 ) .   2.2 Generative Model Annotation Support   A long line of prior work has trained generative   models for question answering ( Du et al . , 2017 ; Du   and Cardie , 2018 ; Zhao et al . , 2018 ; Lewis and Fan ,   2019 ; Alberti et al . , 2019 ; Puri et al . , 2020 ; Yang   et al . , 2020 ; Bartolo et al . , 2021 ; Lewis et al . , 2021 ) .   In many cases , these approaches ﬁlter out questions   that an external QA model gets wrong , in order   to ensure correctness of the generated questions ;   our ﬁltering strategies instead focus on generated   questions that QA models get wrong as we hypoth-   esise that these would serve as more useful initial   prompts to human annotators .   Generative models have also been used to aid   experts with writing contrast sets ( Wu et al . , 2021 ;   Ross et al . , 2021 ) , but to the best of our knowl-   edge , this is the ﬁrst work to investigate the use   of generative annotation assistants for crowdwork-   ers directly in the annotation loop for NLP . Recent   work on supporting crowdworkers for textual en-   tailment in a non - adversarial setting shows no im-   provements on downstream transfer performance   over baseline , albeit with reductions in previously   observed issues with annotation artefacts ( Bowman   et al . , 2020 ) . Subsequent work highlights the need   for further data collection efforts focusing on im-   proving writing - based annotation processes ( Vania   et al . , 2021 ) , which we aim to investigate in this   work . Separately , Ettinger et al . ( 2017 ) provide   breakers with the ability to minimally edit original   data to identify the boundaries of system capabil-   ities , while Potts et al . ( 2021 ) analyse the use of   prompts to assist crowdworkers in beating a model   in the loop for sentiment analysis . In both cases ,   prompts are sourced from existing datasets and are   not generated on the ﬂy .   2.3 Active Learning and Weak Supervision   Active learning approaches have been used to accel-   erate annotation ( Tsuruoka et al . , 2008 ) , although   this typically assumes access to a pool or stream   of unlabelled data for which the learning algorithm   can query labels ( Settles , 2009 ) . In our setting ,   no unlabelled questions are provided , necessitating   the use of a generative model to suggest questions   instead . Moreover , our annotators are free to edit3755   and browse generated questions , whereas annota-   tors in active learning typically only provide labels   and have no choice in what to label . Some of our   sampling and ﬁltering strategies based on entropy   are inspired by uncertainty sampling , a standard   active learning algorithm ( Lewis and Gale , 1994 ) .   3 Experimental Setup   Our study focuses on the effects of incorporating   generative annotation assistants , and understanding   their interactions with annotators and discrimina-   tive models - in - the - loop in a DADC context for QA .   We provide crowdworkers with a short passage   from Wikipedia and ask them to write ﬁve ques-   tions and highlight the span in the passage that best   answers the question for each ( see Figure 2 ) . We   pay workers equally across experiment modes to   avoid creating an incentive imbalance and pay out   an additional bonus for each question that success-   fully beats the discriminative QA model i.e. , for   each question that the model fails to answer cor-   rectly . Finally , we validate all collected examples   using a separate worker pool that also undergoes   rigorous onboarding and validation . We ask three   of these additional workers to report on the validity   of each annotated example .   Selected Passages We select passages from   KILT ( Petroni et al . , 2021 ) to allow for the pos-   sibility of future investigation into cross - domain   and task transfer . We restrict KILT passages tothose with between 100 and 600 tokens that are   used by at least 5 of the KILT tasks . Furthermore ,   we ﬁlter out any passages with any 8 - gram overlap   ( after normalisation ) to the SQuAD1.1 training or   development sets , seeking to ensure that all pas-   sages used in our study are novel and previously   unseen by the discriminative QA models in the   loop . This leaves a total of 10,109 passages from   421 Wikipedia pages . We retain and supply all   passage - relevant KILT metadata ( such as IDs and   provenances ) with our collected datasets to facili-   tate future work .   Model - in - the - Loop The discriminative QA   model in the loop is ELECTRA ( Clark et al . ,   2020 ) trained on SQuAD1.1 and AdversarialQA ,   and enhanced using SynQA to improve adversarial   robustness as investigated by Bartolo et al . ( 2021 ) .   This model represents the best - performing   model on the Dynabench ( Kiela et al . , 2021 )   leaderboard at the time of conducting this study ,   obtaining a word - overlap Fscore of 94.5 %   on the SQuAD1.1 dev set , and represents the   state - of - the - art on AdversarialQA achieving 77.6 %   on the D subset , 71.5 % on D , and   63.2 % on D .   Generator - in - the - Loop For our generative mod-   els , we use the fairseq ( Ott et al . , 2019 ) implemen-3756tation of BART ( Lewis et al . , 2020 ) , and ﬁne-   tune the decoder to generate questions conditioned   on the passage and the answer highlighted by the   annotator . To provide annotators with a diverse set   of questions , we decode using nucleus sampling   withtop= 0.75 , as decoding using standard beam   search results in questions which are more similar   to each other and therefore likely to be less use-   ful as question prompts to annotators . To speed   up inference and model - annotator interaction , we   preemptively identify answer candidates for each   passage and generate questions to build up a large   cache from which we serve questions during anno-   tation . Once there are no questions remaining in   the cache for a particular answer , or if the annota-   tor selects an answer that is not in the cache , we   fall back to querying the generative model in real-   time . In this work , we investigate generative assis-   tants trained on three different sources of questions :   SQuAD1.1 , AdversarialQA , and the combination   of both SQuAD and AdversarialQA .   Question Sampling We investigate three differ-   ent selection strategies for presenting the gener-   ated questions as prompts to annotators : i ) genera-   tor likelihood samples candidates in the order pre-   scribed by the generative model ’s associated likeli-   hood values ; ii ) adversarial sampling selects gener-   ated questions in order of the least word - overlap F   scores when queried against the discriminative QA   model ; and iii ) uncertainty sampling is inspired by   active learning and selects generated questions in   order of the least span selection conﬁdence when   queried against the QA model . The latter two pro-   vide an interesting trade - off for exploration as we   would expect the quality of the generated questions   to be worse than if sampled based on likelihood .   However , we hope that such prompts could serve to   inspire annotators and provide a “ starting point ” be-   yond the answering capabilities of the QA model ,   irrespective of correctness . We hypothesise that   modifying such examples might be a more effective   process for annotators to undertake than when start-   ing from higher quality but less model - confusing   prompts , and investigate this question thoroughly .   Answer Prompts We also investigate the effects   of abstracting away the answer selection task from   the annotator . To identify potential candidate an-   swers , we use Self - Attention Labelling ( SAL ) ( Bar-   tolo et al . , 2021 ) and investigate providing anno-   tators with both answer prompts as well as thecorresponding generated questions .   Experimental Settings In total , there are twenty   different experimental settings involving combi-   nations of the above - mentioned pipeline compo-   nents . We collect 2,000 validated training exam-   ples for each of these settings , for a total of 40,000   examples . For downstream evaluation we train   ELECTRA QA models on the training datasets   collected for each setting , and perform identical   model selection and hyper - parameter tuning .   Annotation Interface We use an adaptation of   the Dynabench ( Kiela et al . , 2021 ) QA interface   that allows annotators to interact with the models in   the loop , and further allows them to edit and mod-   ify generated questions and answers as required .   The same base interface is used across experimen-   tal settings and only varied minimally depending   on the current setting , for example by changing the   title and instructions in the adversarial annotation   setting , or by adding a “ Generate Question ” button   when the setting involves GAAs . In the GAA set-   tings , annotators are not informed what generative   model they are interacting with , or what sampling   mechanism is being used .   Crowdsourcing Protocol We use Amazon Me-   chanical Turk to recruit workers for this study and   run all experiments using Mephisto . To ensure   proﬁciency in English , crowdworkers are required   to be based in Canada , the UK , or the US . They   are also required to have a Human Intelligence   Task ( HIT ) Approval Rate greater than 98 % , have   previously completed at least 1,000 HITs , and un-   dergo a dedicated onboarding process . Workers   were randomly assigned to one of the possible ex-   periment modes and were all presented with pas-   sages sampled from the same set , for which they   were tasked with writing and answering ﬁve ques-   tions . All collected questions were than validated   for correctness by a separate group of crowdwork-   ers . We collect three validations per question and   use this information , along with manual veriﬁcation   of a subset of the annotated examples , to maintain   a high level of quality and remove examples from   workers with less than an 80 % validity rate . We   calculate the reliability of agreement between val-   idators using Fleiss ’ kappa at 0.46 . Workers were   provided an additional bonus for each example val-   idated as having successfully fooled the model in   the adversarial data collection settings . In total,3757   1,931 workers participated in the study , with 1,559   contributing to the ﬁnal datasets . We also continu-   ously validate both annotators and validators based   on signals such as repetitiveness , agreement , and   manual checks .   Evaluation We evaluate the outcomes in each of   the experimental settings by a selection of metrics :   i.median time per example as a measure of an-   notation efﬁciency and where a lower time   taken is better ;   ii.validated Model Error Rate ( vMER ) ( Bartolo   et al . , 2021 ) which evaluates the effective-   ness of annotators at generating valid question-   answer pairs that the QA model in the loop   fails to answer correctly ;   iii.median time per validated model - fooling ex-   ample which serves as a single metric incor-   porating both method efﬁciency and effective-   ness and thus provides a convenient metric for   comparison across the various experimental   settings ; and   iv.downstream effectiveness in which we eval-   uate the performance ( by word - overlap F   score ) of a QA model trained on the data   collected in each of the experimental modes   on the standard SQuAD1.1 benchmark , on   the AdversarialQA benchmark , and in terms   of domain generalisation ability on the   MRQA ( Fisch et al . , 2019 ) dev sets .   Lower values are better for the time - dependent met-   rics , however , from the perspective of training data   we consider a higher vMER to be better guided by   the performance beneﬁts observed for adversarial   over standard data collection . This is corroborated   by comparison with downstream results .   4 Results   Our study allows us to perform a thorough investi-   gation into both the efﬁciency and effectiveness ofthe different data annotation methodologies . It also   allows us to build on work investigating the various   differences between standard and adversarial data   collection ( Kaushik et al . , 2021 ) .   4.1 Standard versus Adversarial Data   Collection   The standard and adversarial data collection set-   tings we use as baselines do not make use of GAAs ,   and are designed to replicate the SQuAD1.1 ( Ra-   jpurkar et al . , 2016 ) and AdversarialQA ( Bartolo   et al . , 2020 ) annotation setups as closely as possi-   ble . However , in contrast to AdversarialQA , our   setting only provides annotators with a ﬁnancial in-   centive to tryto beat the model in the loop through   the use of a bonus , and does not restrict annotators   to only submitting model - fooling examples .   The results , shown in Table 1 , highlight the dif-   ferences between the two annotation approaches .   As expected , standard data collection is more efﬁ-   cient in terms of the time taken per example , as   there is no requirement for annotators to make   any effort to try to beat a model . However , the   efﬁciency differences are not as large as seen in   settings where annotators have to submit model-   fooling examples ( Bartolo et al . , 2020 ) .   We also ﬁnd considerable beneﬁts from adversar-   ial data collection in terms of the validated model   error rate and subsequent downstream performance .   As observed by Bartolo et al . ( 2020 ) , adversarial   data collection is more effective on adversarial test   sets and aids domain generalisation , with slight per-   formance degradation in the standard evaluation   setting , which may be mitigated by increasing the   amount of training data or combining with non-   adversarial training data ( for detailed results , refer   to Appendix B ) . The combined performance across   evaluation settings is considerably higher for adver-   sarial data collection .   We note that the training data sizes in both these   experimental settings are relatively small , and the   beneﬁts of adversarial data collection have been3758   shown to be more pronounced in the low data   regime , likely due to increased example diversity .   Furthermore , while the passages used in this study   are sourced from Wikipedia , there may exist charac-   teristic differences between these and the passages   used in SQuAD .   We also observe considerably lower ( i.e. ,   better ) adversarial human evaluation vMER   scores achieved for our synthetically - augmented   ELECTRA model - in - the - loop compared to the   8.8 % reported for RoBERTa by Bartolo et al .   ( 2021 ) . We hypothesise that this is primarily due   to two factors : the improved robustness of ELEC-   TRA in comparison to RoBERTa , and more tightly-   controlled example validation . For further evidence   of the improved adversarial robustness of ELEC-   TRA , refer to Appendix C.   4.2 Improving Standard Data Collection   We now investigate whether it might be possible   to improve standard data collection practices us-   ing generative assistants – can we achieve similar   performance to adversarial data collection without   access to any adversarial data ?   We therefore use a GAA trained on SQuAD1.1 ,   and investigate the three sampling techniques   namely : likelihood , adversarial , and uncertainty   sampling . Results are shown in Table 2 . We ﬁnd   that using a GAA with likelihood sampling con-   siderably improves the efﬁciency of the annotation   process in comparison to the standard data collec-   tion baseline in Table 1 . It also gives comparable   vMER results and downstream QA performance .   Furthermore , both the adversarial and uncer-   tainty sampling strategies prove effective . While   the reduction in time taken per example is not as   substantial as for standard likelihood sampling , and   is comparable to the standard data collection base-   line , the vMER – an indicator of the diversity of   the collected training data – is substantially im-   proved and outperforms the adversarial data col-   lection baseline . The downstream results are also   promising , providing slight improvements over thestandard data collection setting , particularly with   regards to domain generalisation . They start to   make progress towards the values obtained for the   adversarial data collection baseline although , de-   spite the improved vMER , overall downstream per-   formance is considerably higher in the adversarial   data collection setting .   In summary , these results shows that we can en-   courage annotators to come up with more challeng-   ing examples without requiring any adversarially-   collected data or an adversarial model in the loop ,   simply through the use of GAAs paired with an   appropriate sampling strategy . However , using ad-   versarial data collection still provides substantially   better downstream performance . These observa-   tions are in line with our initial hypothesis that   sampling generated prompts from regions of known   model uncertainty , or prompts that we know the   model ﬁnds challenging to answer , irrespective of   generated sample quality , provides annotators with   a better starting point for example creation .   4.3 Improving Adversarial Data Collection   Following the efﬁciency gains observed for stan-   dard data collection , we investigate whether it is   possible for GAAs to provide further improvements   over adversarial data collection . As for the pre-   vious experiments , we investigate GAAs trained   on three different datasets : SQuAD1.1 , Adversari-   alQA , and the combination of both . We combine   each of these with the three previously discussed   sampling strategies resulting in nine different ex-   perimental settings . Results are shown in Table 3 .   We ﬁnd that when annotators are incentivised to   try to beat an adversarial QA model - in - the - loop ,   the previously seen efﬁciency gains are not as clear   cut . In fact , annotators are slightly slower than for   the adversarial data collection baseline when us-   ing a SQuAD - trained GAA . When using a GAA   that has been trained on adversarially - sourced ques-   tions , standard likelihood sampling provides efﬁ-   ciency gains over the baseline , however , both ad-   versarial and uncertainty sampling ( which naturally3759   lead to more complex prompts that might be more   challenging to work with ) actually slow annotators   down , although they do provide improved validated   model error rates and overall better adversarial ex-   ample generation efﬁciency measured by the time   taken per validated model - fooling example .   In terms of downstream performance , there is no   clear best option , but the best settings consistently   outperform the adversarial data collection baseline   on the most challenging examples ( D and   D ) while providing comparable results in   the other evaluation settings . Surprisingly , we ﬁnd   that various settings , particularly those involving   a SQuAD - trained GAA can provide performance   gains over the standard data collection baseline   on SQuAD1.1 . We also observe that a SQuAD-   trained GAA with uncertainty sampling gives best   performance on the less challenging evaluation sets ,   while an AdversarialQA - trained GAA gives best   performance on the evaluation datasets collected   using a more performant adversary . This is also in   line with the observations made by Bartolo et al .   ( 2020 ) showing a distributional shift in question   type and complexity with an increasingly stronger   model - in - the - loop .   The general takeaway in terms of the ideal ex-   perimental setting from the perspective of down - stream performance is that it depends on the par-   ticular evaluation setting , with GAAs trained on   examples from a particular setting yielding better   performance when the downstream model is also   evaluated in similar conditions . Another key obser-   vation is that both the validated model error rate   and time per validated model - fooling example com-   fortably outperform the baselines across the board ,   highlighting the enhancements to the effectiveness   of the annotation process provided by incorporating   GAAs in the loop .   4.4 Investigating Answer Prompting   The settings explored in the previous sections fo-   cus on investigating the effects of assisting free - text   generation of the questions using GAAs . However ,   the QA crowdsourcing setting also involves annota-   tion of answer spans , which we also explore in the   search for efﬁciency gains . Here , we explore GAAs   trained on datasets with adversarially - sourced com-   ponents and the same three sampling strategies as   previously ( likelihood , uncertainty and adversar-   ial ) , while additionally providing annotators with   an answer suggestion .   In essence , this is similar to an answer and ques-   tion validation setting , with the difference that an-   notators have the ability to freely modify both an-3760swer and question , or request additional sugges-   tions . Results for our experiments involving answer   assistance are shown in Table 4 .   We ﬁnd that answer prompting is very effective   at improving annotation efﬁciency , providing gains   in all six experimental settings while also provid-   ing improved vMER results in all cases . We also   see very similar downstream performance result   patterns to the previous set of experiments – for   performance on the more challenging evaluation   sets ( D andD ) , an AdversarialQA-   trained GAA with likelihood sampling gives best   performance , while for performance on SQuAD , a   GAA trained on examples including SQuAD gives   the best results . As previously discussed and as   shown in Appendix B , adding SQuAD examples to   the training data mitigates this effect .   The consistency in performance patterns serves   to further highlight the previous observation that ,   while using GAAs provides considerable gains in   both the efﬁciency of the annotation process and   effectiveness in terms of downstream results , the   ideal annotation setup should be selected based   on the target downstream evaluation . It is also   worth highlighting the considerable performance   improvements on the more challenging Adversar-   ialQA evaluation sets observed when using an   AdversarialQA - trained GAA even over adversarial   data collection .   5 Annotator Interaction with GAAs   While we provide annotators with instructions ex-   plaining how they can use the GAAs to aid their   annotation , they are free to query the generative   models as many times as they like , if at all , during   annotation . We are interested to see how the three   main factors affecting interaction with the GAAs   that we explore – training data , sampling strategy ,   and answer prompting – affect the ways in which   annotators interact or use the GAAs .   Results , shown in Table 5 , indicate that anno-   tators query the GAA less frequently when being   shown simpler prompts i.e. those obtained using a   GAA trained on non - adversarially sourced exam-   ples , or selected using likelihood sampling which   tends to provide higher quality and less complex   generated texts . We also ﬁnd that annotators query   the GAA more frequently when an answer prompt   is also provided . We believe that this can be at-   tributed to the fact that the answer and question   prompt setting is more similar to a validation work-   ﬂow , allowing annotators to generate prompts until   a satisfactory one is found .   6 Discussion and Conclusion   In this work , we introduce Generative Annotation   Assistants ( GAAs ) and investigate their potential   to aid crowdworkers with creating more effective   training data more efﬁciently . We perform a thor-   ough analysis of how GAAs can be used for im-   proving QA dataset annotation in different settings ,   including different generative model training data ,   sampling strategies , and whether to also provide   annotators with answer suggestions .   We ﬁnd that GAAs are beneﬁcial in both the   standard and adversarial data collection settings . In   the standard data collection setting , and under the   assumption of no access to adversarially - collected   data , GAAs with prompts sampled based on likeli-   hood provide annotation speed - ups , while prompts   sampled by adversarial performance or uncertainty   metrics provide beneﬁts to both the model error   rates on the collected data as well as subsequent   downstream QA performance . We ﬁnd that while   GAAs are effective for improving standard data col-   lection , we still do not approach the performance   obtained when using adversarial data collection .   For adversarial data collection , we demonstrate   improved effectiveness of the annotation process   over the non - GAA baseline , although this comes at   a cost of reduced annotation efﬁciency . We show   that also aiding annotators with answer prompts   boosts data collection efﬁciency even beyond that   of standard data collection , while retaining overall   downstream performance .   We ﬁnd that the ideal annotation setting dif-   fers for different intended evaluations , with an   uncertainty - sampled GAA trained on data that was   not adversarially - collected providing best perfor-3761mance on simpler questions , while a GAA trained   on adversarially - collected data provides best down-   stream performance on more challenging evalua-   tion sets . However , we also ﬁnd that combining   with a small sample of SQuAD training examples   can boost performance on these less - challenging   questions , and that in this setting a likelihood-   sampled adversarially - trained GAA consistently   provides the best results .   In terms of efﬁciency , we see annotation speed-   ups over baseline of 34.4 % for standard data collec-   tion and 37.4 % for adversarial data collection . In   terms of effectiveness , we see over a 5x improve-   ment in vMER over adversarial data collection ,   along with downstream performance gains . We im-   prove over standard data collection on SQuAD   by up to 0.8Fand improve over adversarial data   collection by up to 6.1FonD , and 3.8F   onD . Furthermore , we see beneﬁts in   domain generalisation over standard data collec-   tion , and show that annotators interact with the   GAA more frequently when it has been trained   on adversarially - collected data , is sampled based   on adversarial or uncertainty feedback , and also   provides answer prompts .   While our analysis is limited by the size of the   collected data , we believe that GAAs can help drive   further innovation into improved data collection   methodologies based on these ﬁndings . We hope   that our analysis of various aspects of GAA incor-   poration into the annotation pipeline and the inter-   actions between annotators and multiple models   in the loop can help inform future work exploring   broader aspects of GAA use , such as for other NLP   tasks or for larger scale annotation efforts .   7 Ethical Considerations   We collect a training datasets as a part of the anal-   ysis in this work . The passages are sourced from   Wikipedia through KILT . As described in the main   text , our incentive structure is designed to ensure   that crowdworkers were fairly compensated . Our   datasets focus on the English language . As this data   is not collected for the purpose of designing NLP   applications , we do not foresee any risks associated   with the use of this data .   Acknowledgements   The authors would like to thank the Dynabench   team for their feedback and continuous support . References376237633764A Breakdown of MRQA Results   Table 6 shows the breakdown of results on the 12   MRQA in- and out- of domain evaluation sets .   B Combining with SQuAD1.1   Bartolo et al . ( 2020 ) and subsequent works ﬁnd   that the performance degradation in the original   evaluation setting when training on adversarially-   collected data only is mitigated by also including   some of the original training data . To investigate   this further , we combine and shufﬂe the training   datasets collected in each of the experimental set-   tings with 2k SQuAD1.1 examples for a total of 4k   training examples per experiment .   The baseline results in Table 7 show that this   results in similar performance , if slightly im-   proved on the SQuAD dev set , when using some   adversarially - collected data . We also show the re-   sults for the other experimental settings in Tables 8 ,   9 and 10 , noting very similar performance variation   between settings as those reported earlier .   C Adversarial Robustness of ELECTRA   and RoBERTa   Table 11 shows adversarial robustness perfor-   mance evaluated on the AddSent and AddOneSent   evaluation datasets introduced by Jia and Liang   ( 2017 ) . We observe that even when trained only on   SQuAD1.1 , ELECTRA performs considerably bet-   ter than RoBERTa in this setting , suggesting that it   is substantially more robust “ out of the box ” .   D Computational Resources   All experiments were run on single NVIDIA Tesla   P100 GPUs . Models were trained for up to 14   epochs each taking approximately 2 hours to com-   plete training . Best model checkpoints and hyper-   parameters were tuned for each experimental set-   ting . The ﬁnal model selected for each setting   was based on validation performance across the   SQuAD and AdversarialQA development sets . The   time taken for evaluation of the ﬁnal models on   each of the AdversarialQA test sets and the MRQA   datasets was dependent on the number of examples.37653766   Model Training Data SQuAD AddSent AddOneSent   BERTSQuAD 90.3 73.7 80.3   SQuAD + AdversarialQA 93.3 80.1 85.2   RoBERTaSQuAD 93.5 82.4 86.9   SQuAD + AdversarialQA 92.5 83.4 86.7   SQuAD + AdversarialQA + SynQA 94.8 86.0 89.0   SQuAD + AdversarialQA + SynQA 94.9 87.1 90.1   ELECTRASQuAD 94.4 85.0 89.0   SQuAD + AdversarialQA 94.7 86.1 89.9   SQuAD + AdversarialQA + SynQA 94.8 85.7 89.23767