  William Hogan Jiacheng Li Jingbo Shang   Department of Computer Science & Engineering   University of California , San Diego   { whogan,j9li,jshang}@ucsd.edu   Abstract   Recent relation extraction ( RE ) works have   shown encouraging improvements by conduct-   ing contrastive learning on silver labels gener-   ated by distant supervision before fine - tuning   on gold labels . Existing methods typically as-   sume all these silver labels are accurate and   treat them equally ; however , distant supervi-   sion is inevitably noisy — some silver labels   are more reliable than others . In this paper ,   we propose fine - grained contrastive learning   ( FineCL ) for RE , which leverages fine - grained   information about which silver labels are and   are not noisy to improve the quality of learned   relationship representations for RE . We first   assess the quality of silver labels via a sim-   ple and automatic approach we call “ learning   order denoising , ” where we train a language   model to learn these relations and record the   order of learned training instances . We show   that learning order largely corresponds to label   accuracy — early - learned silver labels have , on   average , more accurate labels than later - learned   silver labels . Then , during pre - training , we in-   crease the weights of accurate labels within   a novel contrastive learning objective . Exper-   iments on several RE benchmarks show that   FineCL makes consistent and significant per-   formance gains over state - of - the - art methods .   1 Introduction   Relation extraction ( RE ) , a subtask of information   extraction , is a foundational task in Natural Lan-   guage Processing ( NLP ) . The RE task is to deter-   mine a linking relationship between two distinct   entities from text , producing fact triples in the form   [ head , relation , tail ] . For example , reading the   Wikipedia page on Noam Chomsky , we learn that   Noam was “ born to Jewish immigrants in Philadel-   phia , ” which corresponds to the fact triple [ Noam   Chomsky , born in , Philadelphia ] . Fact triples play a   key role in downstream NLP tasks such as question - answering , search queries , dialog systems , and   knowledge - graph completion ( Xu et al . , 2016 ; Lin   et al . , 2015 ; Madotto et al . , 2018 ; Hogan et al . ,   2021 ; Li et al . , 2014 ) .   Current state - of - the - art RE models leverage a two-   phase training : a self - supervised pre - training fol-   lowed by a supervised fine - tuning . Popular pre-   trained language models ( PLM ) such as BERT ( De-   vlin et al . , 2019 ) and RoBERTa ( Liu et al . , 2019 )   feature a generic pre - training objective , namely   masked language modeling ( MLM ) , that allows   them to generalize to various downstream tasks .   However , recent RE works have shown impressive   performance gains by using a pre - training objective   designed specifically for relation extraction ( Soares   et al . , 2019 ; Peng et al . , 2020 ; Qin et al . , 2021 ) .   Recently , Peng et al . ( 2020 ) and Qin et al . ( 2021 )   used a contrastive learning loss function to learn   relationship representations during pre - training .   However , RE - specific pre - training requires large   amounts of automatically labeled data obtained   trough distant supervision for RE ( Mintz et al . ,   2009 ) which is inherently noisy — not all labels   from distantly supervised data are correct . Gao   et al . ( 2021 ) manually examined distantly super-   vised relation data and found that a significant ratio ,   53 % , of the assigned labels were incorrect . Further-   more , distantly supervised labels can go beyond   “ correct ” or “ incorrect”—they can have multiple   levels of correctness . Consider the following sen-   tences :   Pairing this text with the Wikidata knowledge   graph ( Vrande ˇci´c and Krötzsch , 2014 ) , distant su-   pervision labels each sentence as a positive instance   of [ Noam Chomsky , born in , Philadelphia ] ; how-   ever , only sentence ( 1 ) adequately expresses the re-   lationship “ born in . ” Sentence ( 2 ) is incorrectly la-1083   beled , and sentence ( 3 ) is , arguably , semi - accurate   since one may infer that someone was born in the   same place they were raised . Conventional con-   trastive learning for RE does not account for dif-   ferences in label accuracy — it treats all instances   equally . This can be problematic when learning ro-   bust and high - quality relationship representations .   This paper proposes a noise - aware contrastive   pre - training , Fine - grained Contrastive Learning   ( FineCL ) for RE , that leverages additional fine-   grained information about which instances are and   are not noisy to produce high - quality relationship   representations . Figure 1 illustrates the end - to - end   data flow for the proposed FineCL method . We   first assess the noise level of all distantly super-   vised training instances and then incorporate such   fine - grained information into the contrastive pre-   training . Less noisy , or clean , training instances are   weighted more relative to noisy training instances .   We then fine - tune the model on gold - labeled data .   As we demonstrate in this work , this approach   produces high - quality relationship representations   from noisy data and then optimizes performance   using limited amounts of human - annotated data .   There are several choices of methods to assess   noise levels . We select a simple yet effective   method we call “ learning order denoising ” that   does not require access to human annotated labels .   We train an off - the - shelf language model to predict   relationships from distantly supervised data and we   record the order of relation instances learned during   training . We show that the order in which instances   are learned corresponds to the label accuracy of   an instance : accurately labeled relation instances   are learned first , followed by noisy , inaccurately   labeled relation instances . We leverage learning - order denoising to improve   the relationship representations learned during pre-   training by linearly projecting the weights of each   relation instance corresponding to the order in   which the instance was learned . We apply higher   weights to relation instances learned earlier in train-   ing relative to those learned later in training . We   use these weights to inform a contrastive learning   loss function that learns to group instances of simi-   lar relationships .   We compare our method to leading RE pre - training   methods and observe an increase in performance   on various downstream RE tasks , illustrating that   FineCL produces more informative relationship   representations .   The contributions of this work are the following :   •We demonstrate that learning - order denoising is   an effective and automatic method for denoising   distantly labeled data .   •Applying a denoising strategy to a contrastive   learning pre - training objective creates more infor-   mative representations , improving performance   on downstream tasks .   •We openly provide all code , trained models , ex-   perimental settings , and datasets used to substan-   tiate the claims made in this paper .   2 Related Work   Early RE methods featured pattern - based algo-   rithms ( Califf and Mooney , 1997 ) followed by   advanced statistical - based RE methods ( Mintz   et al . , 2009 ; Riedel et al . , 2010 ; Quirk and Poon ,   2017 ) . Advances in deep learning led to neural-   based RE methods ( Zhang and Wang , 2015 ; Peng1084   et al . , 2017 ; Miwa and Bansal , 2016 ) . The trans-   former ( Vaswani et al . , 2017 ) enabled the devel-   opment of wildly successful large pre - trained lan-   guage models ( Radford and Narasimhan , 2018 ; De-   vlin et al . , 2019 ; Liu et al . , 2019 ) . At the time of   writing , all current leading models in RElever-   age large pre - trained language models via a two-   step training methodology : a self - supervised pre-   training followed by a supervised fine - tuning ( Xu   et al . , 2021 ; Xiao et al . , 2021 ) .   Building on BERT ( Devlin et al . , 2019 ) , Soares   et al . ( 2019 ) proposed MTB , a model featuring a   pre - training objective explicitly designed for the   task of relation extraction . MTB uses dot product   similarly to align pairs of randomly masked enti-   ties during pre - training . Its success inspired the de-   velopment of subsequent RE - specific pre - training   methods ( Peng et al . , 2020 ; Qin et al . , 2021 ) . Peng   et al . ( 2020 ) demonstrated the effectiveness of con-   trastive learning used to develop relationship repre-   sentations during pre - training . Their model , named   “ CP , ” featured a pre - training objective that com-   bined a relation discrimination task with BERT ’s   masked language modeling ( MLM ) task . Their   work inspired ERICA ( Qin et al . , 2021 ) , which   expanded the contrastive learning pre - training ob-   jective to include entity and relation discrimination ,   as well as MLM .   Wan et al . ( 2022 ) is a recent extension of Peng   et al . ( 2020 ) that proposes a weighted contrastive   learning ( WCL ) method for RE . The authors first   fine - tune BERT to predict relationships using goldtraining data and then use the fine - tuned model to   predict relationships from distantly labeled data .   Next , they use the softmax probability of each pre-   diction as a confidence value which they then apply   to a weighted contrastive learning function used for   pre - training . Lastly , they fine - tune the WCL model   on gold training data .   Our work is an extension of ERICA . We introduce   a more nuanced RE contrastive learning objective   that leverages additional , fine - grained data about   which instances are high - quality training signals .   Table 1 qualitatively compares recent pre - training   methods used for RE .   3 Methods   FineCL for RE consists of three discrete stages :   learning order denoising , contrastive pre - training ,   and supervised adaptation .   3.1 Learning Order Denoising   For learning order denoising , we automatically la-   bel large amounts of training data via distant su-   pervision for RE ( Mintz et al . , 2009 ) which we   use to train a PLM to predict relation classes using   multi - class cross - entropy loss .   L=−/summationdisplayy·log ( p(y ) ) ( 1 )   Where the number of classes Nis the number of   relation classes plus one for no relation , yis a   binary indicator that is 1if and only if iis the   correct classification for observation o , and p(y )   is the Softmax probability that observation ois of   class i.   During training , we record the order of training in-   stances learned . We consider an instance “ learned ”   upon the initial correct prediction . Likewise , an in-   stance is “ not learned ” if the model fails to predict   it correctly during training . Training instances are   evaluated by batch within each epoch , exposing the   model to all training data points the same number   of times . We refer to this method as batch - based   learning order .   Thus , the PLM effectively becomes a mapping   function that maps all training instances ( T ) into   two subsets : learned ( A ) and not learned instances   ( B ) such that A / uniontextB = TandA / intersectiontextB=∅.   The set of learned instances Ais further divided   into non - intersecting subsets of learned instances1085   Athrough Awhere kcorresponds to the epoch   in which an instance is learned .   A / uniondisplay   A ... /uniondisplay   A = A ( 2 )   A / intersectiondisplay   A=∅for all i̸=j ( 3 )   We use k= 15 epochs , resulting in k+1subsets of   instances — ksubsets of learned instances plus one   subset of not learned instances . Figure 2 shows the   percent of total training instances learned per epoch   during this phase on the DocRED ( Yao et al . , 2019 )   distantly labeled training set which contains 100k   documents , 1.5 M intra- and inter - sentence relation   instances , and 96 relation types ( not including no   relation ) .   More challenging relation classes may be underrep-   resented within the set of learned instances . Such   minority classes can be problematic during pre-   training since unlearned instances are weighted less   than learned ones , presenting a challenge for the   model to learn informative representations for mi-   nority classes . To account for this , we ensure that at   leastP%of instances of each relation class is con-   tained within the set of learned instances . During   training , we set P= 50 and observed that 2 % of re-   lation classes are underrepresented within the set of   learned instances . We upsample underrepresented   classes by randomly selecting unlearned instances   from the corresponding class , placing them into   one of the ksubsets of learned instances A. See   Figure 4 in the Appendix for a detailed chart show-   ing the ratio of learned instances by relation class   in each epoch .   Learning order metadata is then inserted into the   original training data T , creating a modified train-   ing set Tused for the contrastive pre - training.3.2 Contrastive Pre - training   This section introduces our pre - training method to   learn high - quality entity and relation representa-   tions . We first construct informative representation   for entities and relationships which we use to im-   plement a three - part pre - training objective that fea-   tures entity discrimination , relation discrimination ,   and masked language modeling .   3.2.1 Entity & Relation Representation   We construct entity and relationship representa-   tions following ERICA ( Qin et al . , 2021 ) . For   the document d , we use a pre - trained language   model to encode dand obtain the hidden states   { h , h , . . . , h } . Then , mean pooling is applied   to the consecutive tokens in entity eto obtain en-   tity representations . Assuming n andnare   the start index and end index of entity ein doc-   ument d , the entity representation of eis repre-   sented as :   m= MeanPool ( h , . . . , h)(4 )   To form a relation representation , we concatenate   the representations of two entities eande :   r= [ e;e ] .   3.2.2 Entity Discrimination   For entity discrimination , we use the same method   described in ERICA . The goal of entity discrimina-   tion ( E ) is inferring the tail entity in a document   given a head entity and a relation ( Qin et al . , 2021 ) .   The model distinguishes the ground - truth tail entity   from other entities in the text . Given a sampled in-   stance tuple tk= ( d , e , r , e ) , our model is   trained to distinguish the tail entity efrom other   entities in the document d. Specifically , we con-   catenate the relation name of r , the head entity   eand a special token [ SEP ] in front of dto get   d. Then , we encode dto get the entity represen-   tations using the method from Section 3.2.1 . The   contrastive learning objective for entity discrimina-   tion is formulated as :   where cos(·,·)denotes the cosine similarity be-   tween two entity representations and τis a tem-   perature hyper - parameter .   3.2.3 Relation Discrimination   To effectively learn representation for downstream   task relation extraction , we conduct a Relation Dis-   crimination ( R ) task during pre - training . R1086aims to distinguish whether two relations are se-   mantically similar ( Qin et al . , 2021 ) . Existing meth-   ods ( Peng et al . , 2020 ; Qin et al . , 2021 ) require   large amounts of automatically labeled data from   distant supervision which is noisy because not all   sentences will adequately express a relationship .   In this case , the learning order can be introduced to   make the model aware of the noise level of relation   instances . To efficiently incorporate learning order   into the training process , we propose fine - grained ,   noise - aware relation discrimination .   In this new method , the noise level of all distantly   supervised training instances controls the optimiza-   tion process by re - weighting the contrastive ob-   jective . Intuitively , the model should learn more   from high - quality , accurately labeled training in-   stances than noisy , inaccurately labeled instances .   Hence , we assign higher weights to earlier learned   instances from the learning order denoising stage .   In practice , we sample a tuple pair of relation   instance t= ( d , e , r , e , k)andt=   ( d , e , r , e , k)fromTandr = r ,   where dis a document ; eis a entity in d;ris the   relationship between two entities and kis the first   learned order introduced in Section 3.1 . Using the   method mentioned in Section 3.2.1 , we obtain the   positive relation representations randr . To   discriminate positive examples from negative ones ,   the fine - grained Ris defined as follows :   L=−/summationdisplayf(k ) logexp ( cos ( r , r)/τ )   Z   Z=/summationdisplayf(k ) exp ( cos ( r , r)/τ )   where cos(·,·)denotes the cosine similarity ; τis   the temperature ; Nis a hyper - parameter and tis   a negative instance ( r̸=r ) sampled from T.   Relation instances tandtare re - weighted by   function fwhich is defined as :   f(k ) = α ( 5 )   where α(α > 1 ) is a hyper - parameter of the func-   tionf;max andmin are maximum and minimum   first - learned order , respectively . We increase the   weight of negative tif it is a high - quality training   instance ( i.e. , kis small ) . Because all positives and   negatives are discriminated from instance t , we   control the overall weight by the learning order k.3.2.4 Overall Objective   We include the MLM task ( Devlin et al . , 2019 ) to   avoid catastrophic forgetting of language under-   standing ( McCloskey and Cohen , 1989 ) and con-   struct the following overall objective for FineCL :   L = L+L+L ( 6 )   3.3 Supervised Adaptation   The primary focus of our work is to improve rela-   tionship representations learned during pre - training   and , in doing so , improve performance on down-   stream RE tasks . To illustrate the effectiveness   of our pre - training method , we use cross - entropy   loss , as described in equation 1 , to fine - tune our   pre - trained FineCL model on document - level and   sentence - level RE tasks .   4 Experiments   4.1 Learning Order as Noise Level Hypothesis   We first seek to confirm our hypothesis that the   learning order automatically orders distantly su-   pervised data from clean , high - quality instances   to noisy , low - quality instances . However , given   the large amount of pre - training data , statistically   significant confirmation via manual annotation is   prohibitively expensive . So , we devise the follow-   ing experiment to test our hypothesis in lieu of a   significant manual annotation effort .   We begin with the assumption that a model trained   on a dataset without noise will perform better than   a model trained on a dataset with noise . Sup-   pose learning order denoising successfully orders   instances relative to their noise ; then , we should   observe a boost in performance by training on a   subset of early - learned instances compared to a   model trained on the complete , noisy dataset .   As reported by Gao et al . ( 2021 ) , up to 53 % of   relation instances labeled via distant supervision   are incorrect . Using this estimation , we attempt to   use learning order denoising to remove the roughly   50 % of instances that are noisy instances from the   DocRED ’s distantly supervised training set . To   do this , we first obtain the learning order of rela-   tion instances using the methodology described in   Section 3.1 . Without loss of generalization , we   choose RoBERTa ( Liu et al . , 2019 ) , specifically   theroberta - base checkpoint , as the base model to1087   develop the order of learned instances .   We observe that the set of training instances learned   viabatch - based learning order in the first epoch ,   A , consists of 45 % of the total training instances .   We use Ato construct a trimmed training set T.   We then compare performance in two settings : ( 1 )   RoBERTa trained with the complete distantly super-   vised training dataset Tand ( 2 ) RoBERTa trained   on the trimmed , denoised training data T. Ta-   ble 2 reports the results of this experiment . Signifi-   cantly , the denoised training set consisting of only   45 % training data outperforms the baseline model .   We also conduct an informal manual analysis of   the learning order . We randomly selected 120 in-   stances from the first six training epochs—60 cor-   rectly , and 60 incorrectly predicted instances . We   find that 93 % of the correct predictions have accu-   rate labels within the first three epochs . However ,   in epochs 4 through 6 , label accuracy drops to 53 %   among correct predictions . Furthermore , we find a   relatively low label accuracy of 50 % from the first   three epochs of incorrect predictions , illustrating   that the model struggles to learn noisy instances   compared to clean instances early in training . We   use these results and the results presented in Ta-   ble 2 to argue that learning order successfully or-   ders instances from clean , high - quality to noisy ,   low - quality instances .   4.2 Learning Order : Batch- vs. Epoch - based   We experiment with two methods of collecting   learning order data : batch - based andepoch - based   ( see Appendix A.1 for pseudo - code describing   these methods ) .   Batch - based : As previously mentioned , for batch-   based learning order we collect learned instances   per batch across each epoch during training . How-   ever , we recognize that this may bias the set oflearned instances by the random batch for which   they are selected . For example , accurately labeled   relation instances selected for the first few batches   during training may not be predicted correctly be-   cause the model has not learned much .   Epoch - based : To reduce potential selection order   bias from batch - based learning order , we experi-   ment with epoch - based learning order by evaluat-   ing the model on the entire training set at the end   of each epoch . We rerun the experiment detailed   in Section 4.1 using epoch - based learning order to   construct the trimmed dataset Tand present the   results in Table 2 .   Using epoch - based learning order , we observe that   the model learns 64.9 % of the training instances   within the first epoch , an increase compared to   the 45.0 % of learned instances from batch - based   learning order . However , training RoBERTa on the   epoch - based training subset , we obtain an F1 score   of 46.0 , which under - performs relative to the 46.6   F1 score from the batch - based learning order ex-   periment . We hypothesize that , while epoch - based   learning order may capture more learned instances ,   it leads to noisier instances leaking into the sets of   learned data because the model is more prone to   simply memorizing noisy labels encountered previ-   ously in the epoch .   Note that we do not use DocRED ’s human-   annotated training data in these learning order ex-   periments . Instead , we train on the distantly su-   pervised training data and test on human - annotated   data . This is done to assess the quality of the vari-   ous subsets of distantly labeled data . It is why the   performance of these tests is considerably lower   than the results from the experiments in Section 4.4   that leverage human - annotated training data .   4.3 Pre - training Details   To ensure a fair comparison and highlight the ef-   fectiveness of FineCL , we align our pre - training   data and settings to those used by ERICA . The   ERICA pre - training dataset is constructed using   distant supervision for RE by pairing documents   from Wikipedia ( English ) with the Wikidata knowl-   edge graph . This distantly labeled dataset creation   method mirrors the method used to create the dis-   tantly labeled training set in DocRED but differs   in that it is much larger and more diverse . It con-   tains 1 M documents , 7.2 M relation instances , and   1040 relation types compared to DocRED ’s 100k1088   documents , 1.5 M relation instances , and 96 rela-   tion types ( not including no relation ) . Additional   checks are performed to ensure no fact triples over-   lap between the training data and the test sets of   the various downstream RE tasks . Detailed pre-   training settings can found in Appendix A.2 .   4.4 Relation Extraction   Document - level RE : To assess our framework ’s   ability to extract document - level relations , we re-   port performance on DocRED ( Yao et al . , 2019 ) .   We compare our model to the following baselines :   ( 1 ) CNN ( Zeng et al . , 2014 ) , ( 2 ) BiLSTM ( Hochre-   iter and Schmidhuber , 1997 ) , ( 3 ) BERT ( Devlin   et al . , 2019 ) , ( 4 ) RoBERTa ( Liu et al . , 2019 ) , ( 5 )   MTB ( Soares et al . , 2019 ) , ( 6 ) CP ( Peng et al . ,   2020 ) , ( 7 & 8) ERICA & ERICA ( Qin   et al . , 2021 ) , ( 9 ) WCL ( Wan et al . , 2022 ) . We fine-   tune the pre - trained models on DocRED ’s human-   annotated train / dev / test splits ( see Appendix A.3.1   for detailed experimental settings ) . We implement   WCL with identical settings from our other pre-   training experiments and , for fair comparison , we   use RoBERTa instead of BERT as the base model   for WCL , given the superior performance we ob-   serve from RoBERTa in all other experiments . Ta-   ble 3 reports performance across multiple data re-   duction settings ( 1 % , 10 % , and 100 % ) , using an   overall F1 - micro score and an F1 - micro score com-   puted by ignoring fact triples in the test set that   overlap with fact triples in the training and develop-   ment splits . We observe that FineCL outperforms   all baselines in all experimental settings , offering   evidence that FineCL produces better relationship   representations from noisy data .   Given that learning - order denoising weighs ear-   lier learned instances over later learned instances ,   FineCL may be biased towards easier , or common   relation classes . The increase in F1 - micro per-   formance may result from improved predictions   on common relation classes at the expense of pre-   dictions on rare classes . To better understand the   performance gains , we also report F1 - macro and   F1 - macro weighted in Table 4 . The results show   that FineCL outperforms the top baselines in both   F1 - macro metrics indicating that , on average , our   method improves performance across all relation   classes . However , the low F1 - macro scores from   all the models highlight an area for improvement —   future pre - trained RE models should focus on im-   proving performance on long - tail relation classes .   Sentence - level RE : To assess our framework ’s abil-   ity to extract sentence - level relations , we report   performance on TACRED ( Zhang et al . , 2017 ) and   SemEval-2010 Task 8 ( Hendrickx et al . , 2010 ) . We1089compare our model to MTB , CP , BERT , RoBERTa ,   ERICA , ERICA , and WCL ( see Ap-   pendix A.3.2 for detailed experimental settings ) .   Table 5 reports F1 scores across multiple data re-   duction settings ( 1 % , 10 % , 100 % ) . Again , we   observe that FineCL outperforms all baselines in   all settings .   5 Ablation Studies   We conduct a suite of ablation experiments to un-   derstand how learning order denoising affects the   quality of relationship representations learned dur-   ing pre - training . We note that the FineCL method is   identical to ERICA when we remove fine - grained   data and treat all instances equally . As such , ER-   ICA can be considered an ablation experiment of   FineCL without fine - grained data .   5.1 Learning Order Epochs   In our first ablation experiment , we vary the num-   ber of training epochs ( k ) used to obtain learning or-   der data to determine how the different amounts of   batch - based learning order data affect pre - training .   We test k={1,3,5,10,15}as well as a baseline   that does not use learning order denoising . To re-   duce the high computational requirements for pre-   training , we use a shortened pre - training for these   experiments where we pre - train for 1000 training   steps compared to the full 6000 step training used   for our main experiments . We then fine - tune the   models using the same settings described in Sec-   tion 4.4 . Notably , our pre - trained model trained   at 1000 steps achieves an F1 score of 59.0 , which   is reasonably close to the 59.5 F1 score from the   FineCL trained for 6000 steps . Table 6 contains   the results from this ablation experiment . We ob-   serve that k= 15 epochs of learned instances pro-   duce the best performance , indicating that a more   extensive set of learned instances produces better   relationship representations .   5.2 Different Learning Order Models   We chose the RoBERTa base model for the first   stage of our FineCL framework to reduce the adop-   tion barrier for our methodology . Popular pre-   trained models such as roberta - base are easy to   implement and require fewer resources compared   to larger state - of - the - art ( SOTA ) RE models . How-   ever , given that RoBERTa is not a leading RE   model , we seek to answer the question — how do   sets of learned training instances differ between   RoBERTa and the SOTA RE model ? At the time of   writing , the leading RE model on DocREDis the   SSAN model ( Xu et al . , 2021 ) . Therefore , we com-   pare sets of learned instances from SSAN ( A ) and   RoBERTa ( A ) by epoch ( k ) using a cumulative   Jaccard Similarity Index :   J(A , A ) = /summationdisplay|A∩ A|   |A∪ A|   Figure 3 plots the cumulative Jaccard Similarity   Index ( JSI ) between sets of learned instances from   RoBERTa and SSAN . The total cumulative JSI be-   tween the two models after k= 15 epochs is 0.771 ,   showing high similarity between sets of learned   instances . While the sets are not perfectly aligned ,   we argue that this high similarity justifies using   the smaller and more convenient RoBERTa model   in determining learning order . We leave a more   thorough examination of the differences in sets   of learned instances obtained using various RE   models to future work and present our findings   as a proof of concept , demonstrating that obtaining   learning order from relatively small and convenient1090Metric F1 - micro   BERT 32.9   RoBERTa 35.8   ERICA 34.7   ERICA 34.4   WCL 35.7   FineCL 36.1   language models is sufficient in improving repre-   sentations learned during pre - training .   5.3 Performance Relative to Class Difficulty   As mentioned in Section 3.1 , it is possible that   learning - order denoising biases the model to eas-   ier relation classes , as easier classes may be over-   represented in the set of learned instances . To un-   derstand the effectiveness of our approach relative   to class difficulty , we assess the end - to - end per-   formance of FineCL on a set of difficult relation   classes .   We recognize that there are multiple ways to define   a “ difficult ” relation class . Difficult classes can be   classes with few training instances , classes with a   significant number of inaccurate or semi - accurate   labels , or classes that suffer from low overall ac-   curacy after training completes . For this ablation   study , we define the set of difficult relation classes   as classes that attain relatively low accuracy from   the training in Stage 1 of FineCL . We claim that   any class which achieves less than 80 % accuracy   after Stage 1 training completes is a “ difficult ” re-   lation class . This subset of the lowest - performing   classes from the DocRED dataset makes up 24 %   of all the classes in the dataset .   We compare the end - to - end performance of FineCL   to baselines that do not leverage fine - grained con-   trastive learning on the set of difficult relation   classes . Table 7 contains the results from this ex-   periment . We observe that FineCL achieves an F1   score of 36.1 % on the subset of difficult classes   compared to the best - performing baseline which   achieves 35.8 % . We argue that these results , as   well as the results from Table 4 , offer evidence that   the FineCL approach is capable of improving per-   formance on both difficult classes as well as easy   classes . However , the low overall performancefrom all models on difficult classes highlights an   area for future work .   6 Conclusion   In this work , we expand on contrastive learning   for relation extraction by introducing Fine - grained   Contrastive Learning for RE — a method that uses   additional , fine - grained information about distantly   supervised training data to improve relationship   representations learned during pre - training . These   improved representations lead to increases in per-   formance across a variety of downstream RE tasks .   This report shows that learning order denoising   effectively and automatically orders distantly su-   pervised training data from clean to noisy instances .   In future work , we hope to explore the usefulness   of this method when applied to manually annotated   data where learning order may instead reflect the   level of difficulty of training instances . This could   be an easy and automatic way to introduce curricula   learning within the fine - tuning training phase . We   also intend to explore the pairing of other denoising   methods with FineCL .   Acknowledgements   Thank you to the anonymous reviewers for their   thoughtful feedback . Our work is sponsored in   part by National Science Foundation Convergence   Accelerator under award OIA-2040727 as well as   generous gifts from Google , Adobe , and Teradata .   Any opinions , findings , and conclusions or rec-   ommendations expressed herein are those of the   authors and should not be interpreted as necessarily   representing the views , either expressed or implied ,   of the U.S. Government . The U.S. Government is   authorized to reproduce and distribute reprints for   government purposes not withstanding any copy-   right annotation hereon .   7 Limitations   The limitations of our method are as follows :   1.Our method requires access to a robust knowl-   edge graph to define the concepts and the rela-   tionships for distant supervision .   2.Our method minimizes the need for but still   requires human - annotated data , which is both   expensive and time - consuming to create .   3.The low F1 - macro scores of our model and all   other leading RE models highlight the need   to improve performance on long - tail relation   classes in future works.1091References10921093A Appendix   A.1 Learning order methods : batch- vs   epoch - based   Algorithm 1 : Batch - based learning orderk = 15 epochsfori= 0tokdo foreach batch of training data do predictions ←model(batch ) A.insert(correct predictions ) Calculate loss Back propagate   Algorithm 2 : Epoch - based learning orderk = 15 epochsfori= 0tokdo foreach batch of training data do Calculate loss Back propagate predictions ←model(all training data)A.insert(correct predictions )   A.2 Pre - training Settings   We initialize our model with roberta - base released   by Huggingface . The optimizer is AdamW and   we set the learning rate to 3×10 , weight decay   to1×10 , batch size to 768 and temperature τ   to5×10 . The hyper - parameter αthat controls   the weights of contrastive learning is e(the base   of natural logarithm ) . We randomly sample 64   negatives for each document . We train our model   with 3 NVIDIA Tesla V100 GPUs for 6,000 steps .   A.3 Downstream Training Settings   A.3.1 DocRED   We fine - tune our model on DocRED using the   following settings : batch size=32 , epochs=200 ,   max sequence length=512 , gradient accumula-   tion steps=1 , learning rate=4e-5 , weight decay=0 ,   adam epsilon=1e-8 , max gradient norm=1.0 , hid-   den size=768 , and a seed=42 . Results are reported   on the official DocRED test set as an average of   three runs .   A.3.2 SemEval and TACRED   We fine tune our moodel on SemEval and TA-   CRED using the following settings : batch size=64 ,   max sequence length=100 , learning rate=5e-5,adam epsilon=1e-8 , weight decay=1e-5 , max gra-   dient norm=1.0 , warm up steps=500 , and hidden   size=768 . We ran tests on training proportions   0.01/0.1/1.0 using 80/20/8 epochs and a dropout of   0.2/0.1/0.35 , respectively .   Results are reported as an average of five runs using   the following seed values : 42 , 43 , 44 , 45 , and 46.10941095