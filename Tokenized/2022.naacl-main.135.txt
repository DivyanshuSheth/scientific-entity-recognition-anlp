  Daniela Brook WeissPaul RoitOri ErnstIdo DaganComputer Science Department , Bar - Ilan University   Abstract   NLP models that process multiple texts of-   ten struggle in recognizing corresponding and   salient information that is often differently   phrased , and consolidating the redundancies   across texts . To facilitate research of such   challenges , the sentence fusion task was pro-   posed , yet previous datasets for this task were   very limited in their size and scope . In this   paper , we revisit and substantially extend pre-   vious dataset creation efforts . With careful   modiﬁcations , relabeling and employing com-   plementing data sources , we were able to   more than triple the size of a notable ear-   lier dataset . Moreover , we show that our ex-   tended version uses more representative texts   for multi - document tasks and provides a more   diverse training - set , which substantially im-   proves model performance .   1 Introduction   Despite recent advances reported in NLU bench-   marks for single document tasks , cross - document   tasks , such as multi - document summarization   ( MDS ) have not progressed with the same pace .   The handling of information across documents re-   quires effective measures for identifying overlap-   ping content . Moreover , generative tasks require   consolidating the relevant and redundant content   into a coherent utterance . In light of this , sev-   eral works proposed a focused sentence - level task ,   called sentence fusion , which focuses on summa-   rizing multiple sentences with overlapping con-   tent into a non - redundant one . This allows a ﬁne-   grained analysis of which information units are   shared among the input sentences , as well as con-   trol over different degrees of information inclusion   and exclusion .   However , the available datasets for fusing sen-   tences which exhibit signiﬁcant content overlap are   still lacking , with the most recent containing only   several hundreds of examples ( McKeown et al . ,   Table 1 : Sentence fusion example from Thadani and   McKeown ( 2013 ) . ( a - d ) are the input sentences , origi-   nating from different documents . Text spans ( in bold )   that are considered as contributing to the same unit   of content ( SCU ) are annotated with the same concise   label . The sentences where the spans appear in are   grouped to be input for sentence fusion , while the SCU   label becomes the fusion target .   2010 ; Thadani and McKeown , 2013 ) , impeding   further research . In this work , we follow Thadani   and McKeown ( 2013 ) and extend their described   sentence fusion dataset , which is derived from ex-   pertly written and annotated summaries based on   the Pyramid MDS evaluation method by Nenkova   and Passonneau ( 2004 ) . Table 1 illustrates an ex-   ample where the gold label is a summary of the   content intersection in the input sentences .   We ﬁnd that the heuristics and ﬁlters applied   by Thadani and McKeown ( 2013 ) result in short   and highly related sentences , which may not reﬂect   more complex and long sentences that are often   found in multi - text consolidation tasks . Moreover ,   their dataset uses exclusively sentences from ex-   pert summaries and exclude the actual source doc-   uments that are used in practice for summarization .   The resulting high similarity within input sentences   makes them amenable to extractive methods , where   a representative sentence can be selected as the   summary , curbing the efforts to develop an abstrac-   tive fusion of sentences .   In this work , we modify Thadani and McKeown   ( 2013 ) ’s pre - processing pipeline after careful anal-1854ysis , re - label a portion of the instances , and supple-   ment the data with source document sentences ( § 3 ) .   Our contribution therefore is an extended sentence-   fusion dataset , more than 4x times larger than its   original , with 18 % manually relabeled instances .   We show that our ﬁnal extended dataset better re-   ﬂects challenges in multi - source summarization   tasks ( § 4 ) , with highly redundant salient content ,   originating in more representative sentences from   the wild . In addition , we show ( § 5 ) that a contem-   porary generative model produces more abstractive   output after training on our extended training set   than on the original one . Similarly , it also out-   performs the latter on the original test set . Given   that sentence fusion was originally motivated as   a step in modular multi - document summarization   pipelines ( Barzilay and McKeown , 2005 ; Marsi   and Krahmer , 2005 ) , we hope that progress on sen-   tence fusion may contribute to broader contexts of   multi - document consolidation and fusion tasks .   2 Background   The sentence fusion task deals with combining mul-   tiple sentences with overlapping content into a sin-   gle summary sentence that represents the shared   information across the inputs ( Barzilay and McKe-   own , 2005 ; Filippova and Strube , 2008 ; Marsi and   Krahmer , 2005 ; McKeown et al . , 2010 ; Thadani   and McKeown , 2013 ) . Several other variants of   sentence fusion have also been explored , such as   sentence union – fusing the union of information   in the input ( Marsi and Krahmer , 2005 ) . In an-   other variant , “ disparate ” sentence fusion ( Elsner   and Santhanam , 2011 ; Geva et al . , 2019 ; Lebanoff   et al . , 2019 , 2020 ) , the input sentences do not ex-   hibit considerable content overlap but are rather   related in discourse . Such sentences often originate   in a single document and pose a different kind of   challenge to generate the right discourse structure   that will ﬂuently connect the inputs .   For pragmatic purposes , a “ loose ” variant of   sentence intersection is desired , since redundant   content is most likely salient , yet additional im-   portant but non - overlapping information may be   relevant for a ﬁnal summarized sentence . For this   reason , our extended dataset follows the fusion as   “ loose ” intersection approach applied by Barzilay   and McKeown ( 2005 ) , McKeown et al . ( 2010 ) and   Thadani and McKeown ( 2013 ) . The latter com-   piled a dataset for sentence fusion by leveraging   annotations made during post - hoc evaluation of   multi - document summarization systems .   2.1 From MDS Evaluation to Fusion   The Pyramid method ( Nenkova and Passonneau ,   2004 ) , is a well - known evaluation method for con-   tent selection in summarization , which was used in   the DUCand TACbenchmarks for MDS .   Applying this method , a set of reference sum-   maries per topic are written by expert annotators   and divided into informational units . Each unit ,   named Summary Content Unit ( SCU ) , denotes a   short statement . For example , the SCU labeled :   cyanide use by ﬁsherman decimates ﬁsh may be   expressed in multiple summaries and source docu-   ments under different manifestations . To compile a   list of content units for MDS evaluation , the anno-   tator marks text spans ( see bold spans in Table 1 )   across reference summaries with equivalent con-   tent that directly expresses or contributes to the   summary unit ( SCU contributors ) . Next , she labels   the content unit by writing a concise statement in   natural language , named SCU Label . The source   sentences of each contributing span may then be   grouped into a cluster bearing the same SCU la-   bel . Table 1 presents an example of such a cluster ,   containing four source sentences with contributing   spans ( in bold ) , along with their associated SCU   label that concisely summarizes them . Thadani and   McKeown ( 2013 ) creates a fusion instance by us-   ing each cluster ’s sentences as input , and the SCU   label as the target for fusion output.18553 Data Collection   After carefully analyzing Thadani and McKeown   ( 2013 ) ’s pre - processing pipeline described in sub-   section 3.1 , we decided to substantially modify it   ( subsection 3.2 ) , recovering signiﬁcantly more data .   We proceed with relabeling some of the targets , and   adding samples from source documents ( i.e. not   just from expertly written summaries ) that were   mapped to SCUs but overlooked in the past .   3.1 Previous Pre - processing pipeline   Thadani and McKeown ( 2013 ) applied several ﬁl-   tering steps to generate a fusion dataset ( termed   here PF ) from SCUs . Speciﬁc details re-   garding these ﬁlters as well as examples are in   Appendix A. While the original intention was to   reduce noisy samples , these steps removed a signiﬁ-   ca nt portion of challenging fusion instances . Poten-   tial clusters were removed for having differences   in length either between the source sentences to   the marked span contributions , or between the tar-   get SCU label and the marked span contributions ,   denoting possibly non - shared information that ap-   peared in the input , but not in the output . Another   ﬁltering criterion had been to discard all clusters   whose target label contains content words unused   by any input sentence , discouraging paraphrasing   between input and output . Such ﬁltering has left the   dataset , whose inputs and outputs are quite similar   in both length and content ( see § 4 ) , missing realis-   tic challenges in a multi - document setup , where lex-   ically differing and non - overlapping content may   appear . Moreover , such setup inadvertently biases   generative models to be more extractive ( see § 5 )   than abstractive , relying on a single input sentence   to convey all shared information in a cluster .   This dataset was the largest available source to   date for supervised sentence fusion focused on   multi - text , with a total of 1705 fusion instances .   3.2 Extending Fusion Dataset   We discovered that most of the above ﬁltering crite-   ria were safe to forgo save a few sanity checks . This   has recovered new fusion instances by either adding   back removed SCU labels or input sentences . Fol-   lowing , we noticed that 18 % of the input clusters   share more than one SCU label , mostly due to the   original Pyramid annotators splitting conjunctions   along different SCUs . For correctness , we manu-   ally re - labeled such clusters using all shared labels   into a single sentence ( see Appendix C ) .   Additionally , DUC also made available the SCU   Marked Corpus ( Copeck and Szpakowicz , 2005 ) ,   which automatically maps source document sen-   tences to SCU labels using lexical matching . We   use this resource to extend our dataset with docu-   ment sentences , which were overlooked in P-   F. Document sentences tend to be longer and   more varying than summary sentences , with 30   tokens vs. 20 on average . Clusters containing docu-   ment sentences also tend to have more inputs , since   reference summaries were limited to four , while   the number of source documents per summarized   topic is much higher . In total , we have extended   the fusion dataset from its original 1705 instances   to 7505 , with 37 % containing at least one docu-   ment source sentence , creating a much more varied   dataset , as analyzed next .   4 Data Analysis   We suggest that the additional instances previously   skipped would more closely resemble challenges   in a multi - document setting . To show that , we com-   pare our extended dataset PF++to its prede-   cessor PF , that uses closely knit sentence   clusters , and to DISPARATE ( Lebanoff et al . ,   2020 ) , that contains mostly non - overlapping within   document sentences . The latter allows to estimate   a lower bound for overlap for document sentences   with little shared content that still relate to each   other , making the bound tighter than for randomly   picked sentences ( some examples are shown in Ap-   pendix D ) . We denote by ∆-PFthe instances   that we added exclusively as part of our extension .   To assess content overlap empirically , we calculate   the micro - average of ROUGE ( Lin , 2004 ) word-1856Train Data Dev Test Test++   PF 36.4 40.9 28.5   PF++ 42 45.4 32.5   overlap between every sentence in the cluster to   its target label ( R ) and between every pair of   input sentences in the same cluster ( R ) .   The results in Table 3 show that the content over-   lap among input sentences ( R ) of our added   instances in ∆-PFis much closer to P-   Fthan to disparate sentences , indicating they   are viable and highly - related input examples . This   reinforces our claim that in a true multi - document   setting a system will be challenged with dealing   with signiﬁcantly more redundant information then   exhibited within a single document ( as in DIS-   PARATE ) , and this has to be speciﬁcally addressed   by a multi - document fusion dataset .   As expected , PFcontains a much higher   label to sentence content overlap ( R ) , given   that the original pre - processing explicitly removed   instances with less overlap between the SCU out-   put and the source sentences . In fact , our analysis   revealed that in PyrFus , extractive target labels ,   where the target sentence is an approximate copy   of one of the input sentences ( up to two words ) ,   account for 29 % of the clusters , while in our ex-   tended dataset they account only for 11 % . Overall   our new fusion clusters express high relatedness   between the source sentences and their label , while   exhibiting higher diversity .   5 Baselines and Data Effectiveness   We implement a modern baseline ( see Appendix E   for details ) for PF(Thadani and McKeown ,   2013 ) , which outperforms their pre - neural one . To   that end , we employ the pre - trained auto - encoder   BART ( Lewis et al . , 2020 ) as our end - to - end gen-   eration model due to its demonstrated performance   on summarization tasks .   Results , as shown in Table 4 , were measured   with the Rouge-2 F1 metric on the original P-   Fevaluation splits . These results show that   a fusion model trained on our extended data   ( PF++ ) signiﬁcantly outperforms the same   model trained on the original training data , by   roughly 5 Rpoints . Notably , the model trained on   PF++scored 13 points lower on its own test   set , indicating that the new dataset is much more   challenging , and yet enables the model to reach   better generalizations .   Examining the outputs of both models , we ﬁnd   that many are similar and are often extracted from   source sentences . To study the differences be-   tween model outputs , we ﬁrst sample 50 instances   where the PF++model performed worse . We   notice that in most ( 78 % ) of these cases the P-   F++model output is acceptable , while the lower   score stems from ROUGE artifacts due to sentence   rephrasing . Only 22 % do suffer from lack of salient   content . On the contrary , inspecting 50 instances   where the PFmodel performed worse , we   ﬁnd that only 54 % of these are acceptable , while   the rest suffer from lack of salient content . This   sample analysis suggests that the advantage of the   PF++model is even greater than reﬂected   by the ROUGE scores . Finally , an example for   missing information in the PFmodel output   appears in Table 5 , not including a critical detail   that all input sentences ( in Table 1 ) discuss – ﬁsh   decimation , while the PF++-trained model   correctly includes it . Such instances show the ne-   cessity of a large and realistic fusion dataset for   model training .   6 Conclusion   In this work we extended a sentence fusion dataset   by almost four times its original size , while rela-   beling some of the data . The new dataset includes   more complex and relevant training instances , bet-   ter reﬂecting those that could be found in “ the   wild ” , and thus facilitates further research on data   consolidation in multi - text tasks . In addition , we1857train baseline fusion models and show that when   trained on our extended data we achieve notably   better performance on the original available fusion   test set , while also generating qualitatively better   ( “ loose " ) sentence intersections .   Acknowledgements   The work described herein was supported in part   by grants from Intel Labs , Facebook , the Israel   Science Foundation grant 2827/21 , and by a grant   from the Israel Ministry of Science and Technol-   ogy .   References   A Modiﬁcations to the preprocessing   pipeline   Thadani and McKeown ( 2013 ) applied several pre-   processing steps to generate a fusion dataset from1858   SCUs . These include discarding all clusters that :   [ 1 ] have more than 4 contributing sentences ; [ 2 ]   have SCU labels that do n’t contain a verb after   the ﬁrst token ; [ 3 ] have SCU labels and source   sentences with less than 5 words or more than 100 ;   [ 4 ] have contributing spans that are shorter than   half of their source sentence ; [ 5 ] have SCU labels   that are shorter than half of the shortest contributing   span in the input ; and [ 6 ] have SCU labels with any   tokens not appearing in any of the source sentences .   Table 6 represents examples of fusion instances   that were ﬁltered out in PFdue to various   ﬁlters . First four examples were recovered in our   dataset , but the last two were deemed too short and   not speciﬁc enough , and were left out due to lack   of informativeness .   We found that certain ﬁlters were safe to remove .   We discard ﬁlter [ 2 ] since the majority of SCU la-   bels without a verb use a nominalization ( affecting   601 instances ) . Similarly , we ease the length re-   quirement of SCUs to be between 4 - 100 , as they   were found to be coherent and descriptive , affect-   ing 497 instances . Additionally , we allow SCU   clusters that have low overlap between their label   and their marked contributing spans , discarding   ﬁlters [ 4 ] and [ 5 ] , affecting 2410 instances ( see Ta-   ble 2 ) . And ﬁnally , we keep fusion instances whose   SCU label tokens are not fully covered by their in-   put sentences to allow paraphrases ( affecting 2410   instances ) .   B Pyramid - based Fusion Data   For the fusion instances containing summary   source sentences as inputs , we used the same years   reported in Thadani and McKeown ( 2013 ) ( years2005 - 2007 for DUC and 2008 - 2011 for TAC ) . The   source document sentences found in Copeck and   Szpakowicz ( 2005 ) were made available from 2005-   2008 . We made use of all the years except 2005 ,   since we found this year to be containing more   varied documents within a topic , which yielded   noisier automatic alignments between SCU labels   and source document sentences .   C Manual Target Re - annotation   Once we removed most of the ﬁltering pipeline   ofPF , we noticed that almost 20 % of the   fusion input clusters share more than one SCU la-   bel . To accommodate , we manually re - label such   clusters using all shared SCU labels into a single   sentence . For example , for the following two SCU   labels : Clinical trials typically involve three phases   andClinical trials involve an average of 200 pa-   tients per trial , a new merged fusion label would   be : Clinical trials typically involve three phases   and an average of 200 patients per trial .   D Examples of Fusion Instances   Table 7 presents 3 examples of fusion instances   originating from different datasets . As previously   mentioned , DISPARATE fusion involves the fu-   sion of input sentences that often originate from a   single document , containing little content overlap   but related in discourse . The data used was taken   from Lebanoff et al . ( 2020 ) and included 1599 sam-   ples . PFandPF++contain examples   originating from multi - text settings , while PF   contains only inputs from reference summaries ,   andPF++is enhanced with document source   sentences , along with more complex examples .   E Training a Fusion Baseline   As described in section 5 , we train two sentence   fusion baselines using a pre - trained auto - encoder   BART base model ( Lewis et al . , 2020 ) , on PF   andPF++respectively . We used the training   scriptmade available by the transformers library   ( Wolf et al . , 2020 ) with the following parameters :   4 training epochs and a learning rate of 3e-5 . A   “ steps ” evaluation parameter was used with 5000   evaluation steps and an evaluation beam of 6 . Max   source input was limited to 265 while max target   length was set to 30 . Minimum target length were1859   set to 4 , given our minimum requirements for fu-   sion labels . The ﬁnal evaluated score reported was   an average score over 20 different trained models .   This is due to BART being highly sensitive to the   ordering of the input sentences . Both baseline mod-   els were trained using the train / test splits that were   reported in Thadani and McKeown ( 2013 ) , using   DUC years 2005 - 2007 for test , TAC 2011 for dev ,   and TAC 2008 - 2010 for train.1860