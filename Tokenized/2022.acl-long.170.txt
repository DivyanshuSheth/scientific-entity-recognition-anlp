  Ningning Wang , Guobing Gan , Peng Zhang , Shuai Zhang ,   Junqiu Wei , Qun Liu , Xin JiangCollege of Intelligence and Computing , Tianjin University , Tianjin , ChinaThe Hong Kong Polytechnic University , ChinaHuawei Noah ’s Ark Lab , China   { w_ning1215,ganguobing,pzhang,szhang96}@tju.edu.cn   junwei@polyu.edu.hk,{qun.liu,Jiang.Xin}@huawei.com   Abstract   Recently , a lot of research has been carried   out to improve the efﬁciency of Transformer .   Among them , the sparse pattern - based method   is an important branch of efﬁcient Transform-   ers . However , some existing sparse methods   usually use ﬁxed patterns to select words , with-   out considering similarities between words .   Other sparse methods use clustering patterns   to select words , but the clustering process is   separate from the training process of the tar-   get task , which causes a decrease in effective-   ness . To address these limitations , we design a   neural clustering method , which can be seam-   lessly integrated into the Self - Attention Mech-   anism in Transformer . The clustering task   and the target task are jointly trained and op-   timized to beneﬁt each other , leading to signif-   icant effectiveness improvement . In addition ,   our method groups the words with strong de-   pendencies into the same cluster and performs   the attention mechanism for each cluster inde-   pendently , which improves the efﬁciency . We   veriﬁed our method on machine translation ,   text classiﬁcation , natural language inference ,   and text matching tasks . Experimental results   show that our method outperforms two typical   sparse attention methods , Reformer and Rout-   ing Transformer while having a comparable or   even better time and memory efﬁciency .   1 Introduction   Transformer ( Vaswani et al . , 2017 ) has been widely   used and achieved state - of - the - art results in a va-   riety of NLP tasks such as neural machine trans-   lation ( Bahdanau et al . , 2015 ) , text classiﬁcation ,   etc . Its good effectiveness beneﬁts from its core   component Self - Attention Mechanism which can   capture global dependencies well . However , the   large calculation and memory cost limit the fur-   ther application of Transformer on long sequencetasks due to the complexity of O(Nd)of Self-   Attention . As a result , many research works have   been carried out to improve the efﬁciency of Trans-   former ( Tay et al . , 2020b ) . These efﬁcient Trans-   formers can be roughly divided into two cate-   gories : approximation - based ( Tay et al . , 2020a ;   Katharopoulos et al . , 2020 ) and sparse pattern-   based methods ( Qiu et al . , 2020 ; Ho et al . , 2019 ;   Beltagy et al . , 2020 ; Liu et al . , 2018 ) .   Regarding approximation - based methods , some   works are based on low - rank approximation ( Tay   et al . , 2020a ; Wang et al . , 2020 ) while others   are based on kernels ( Katharopoulos et al . , 2020 ;   Choromanski et al . , 2020 ) . Speciﬁcally , Lin-   former ( Wang et al . , 2020 ) adopts a low - rank ap-   proximation idea and projects the length dimension   of keys and values to a lower dimension ( N!k ) .   This reduces the complexity to O(Nkd ) . However ,   the projection matrix in this method requires that all   input sequences must be ﬁlled to the same length N ,   which makes it can not handle the variable - length   sequence well . Linear Transformer ( Katharopou-   los et al . , 2020 ) uses kernels and the associative   property of matrix multiplication to linearize the   softmax attention , which reduces the complexity   toO(Nd ) . However , the approximation error to   the softmax matrix in Self - Attention can be large   in some cases ( Xiong et al . , 2021 ) .   Sparse pattern - based methods introduce sparse   patterns into the attention mechanism and limit   the number of key vectors that the query vector   should pay attention to . Prior work ( Child et al . ,   2019 ; Qiu et al . , 2020 ; Ho et al . , 2019 ; Beltagy   et al . , 2020 ; Liu et al . , 2018 ) proposed to use ﬁxed   sparse patterns to improve the efﬁciency of Self-   Attention . For example , Sparse Transformer ( Child   et al . , 2019 ) restricts the query to focus only on   keys that are nearby or at ﬁxed intervals . Such   ﬁxed sparse patterns do not consider the similarity   between the query and different keys , and directly   ﬁlter keys according to their location , which re-2390sults in a degradation of the model effectiveness .   More recently , the clustering sparse patterns are   proposed . Reformer ( Kitaev et al . , 2020 ) and Rout-   ing Transformer ( Roy et al . , 2021 ) use Locality   Sensitive Hashing ( LSH ) and K - Means algorithms ,   respectively , to divide the words in the sequence   into different clusters , and then the attention op-   eration is restricted within each cluster indepen-   dently . In this way , they reduce the complexity to   O(NlogNd)andO(Np   Nd ) , respectively . How-   ever , in Reformer and Routing Transformer , both   LSH and K - Means only play the role of cluster   partitioning , but run separately from the attention   network training . In addition , these two methods   also have the problem of inconsistency in the simi-   larity measure between the clustering and attention   operation . LSH and K - Means respectively use the   hash value obtained by random projections and the   negative Euclidean distance as the similarity mea-   sure between the input vectors while the attention   operations use the inner product . Therefore , such   a sparse pattern idea often results in the reduced   effectiveness .   To address the reduced effectiveness issue of   many efﬁcient Transformers , especially sparse   pattern - based methods , we propose Neural Clus-   tering Method to learn the sparse pattern of the   Attention . It can be seamlessly integrated into neu-   ral networks for joint training and optimization .   In our method , the cluster center ( centroid ) is up-   dated by a weighted sum of all word hidden states .   At the same time , the members of clusters are di-   vided according to the subordinate matrix of the   centroids and word hidden states . The optimiza-   tion of the clustering loss can guide the represen-   tation of word hidden states while learning cluster   centroids . The integration of the neural clustering   method and attention training enables our Neural   Clustering Attention to perform better than previ-   ous clustering - based sparse attention mechanisms .   Our Neural Clustering Method is a general cluster-   ing method , in the sense that in addition to being   integrated into the network of the speciﬁc task , it   is can handle clustering tasks alone .   Our overall model is called ClusterFormer and it   is obtained by replacing the Self - Attention Mecha-   nism in Transformer with Neural Clustering Atten-   tion Mechanism . In order to validate the beneﬁts   of ClusterFormer , we have carried out comparison   experiments of the efﬁciency and effectiveness re-   spectively . For efﬁciency , we provide a detailedanalysis about the time and memory on dataset   20NEWS of text classiﬁcation . Results show that   our model has a comparable or even better efﬁ-   ciency compared with two typical sparse attention   models , Reformer and Routing Transformer . Espe-   cially , when the sequence length exceeds 2000 , our   model , Routing Transfomer and Reformer reduce   the memory of Transformer by 53.8 % , 60.8 % and   31.8 % while reducing the training time by 51.4 % ,   41.8 % and 14.4 % , respectively on GPU . For ef-   fectiveness , we test it on machine translation , text   classiﬁcation , natural language inference , and text   matching tasks . Experimental results show that   on all tasks , our model consistently outperforms   Reformer and Routing Transformer . In particular ,   our method improves the accuracy by 15.6 % and   7.2 % on SciTail datasets of natural language infer-   ence task compared with Reformer and Routing   Transformer , respectively .   The major contributions of our work are as fol-   lows :   •We propose a general end - to - end fuzzy clus-   tering method based on neural network ,   named Neural Clustering Method , which can   dynamically learn weights of each word and   then update centroids by weighting all the in-   put words along with the training of speciﬁc   tasks .   •We design the Neural Clustering Attention   Mechanism based on our proposed cluster-   ing method to refactor Self - Attention Mecha-   nism . The experimental results show that our   method has comparable efﬁciency and bet-   ter effectiveness than typical sparse attention   models .   2 Related Work   2.1 Self - Attention Mechanism   The Self - Attention is the core component of Trans-   former ( Vaswani et al . , 2017 ) . It extracts sequence   features by processing the interaction of three se-   quence matrices Q , K , andV. Referring to the   standard Transformer , its function can be written   as follows :   whereX2R , Q , K;V2R ,   W;W2R , Nis the length of the se-2391   quence , d is the dimensionality of the model ,   anddis the dimensionality of the attention head . In   Self - Attention Mechanism , the interaction of Qand   Kgives theNNattention ( weight ) matrix , and   it leads to the complexity of O(Nd ) , which has   been one of the crucial limitation of Transformer .   2.2 Sparse variants of Transformer   Transformer has been developed into many variants   to reduce the complexity of the attention mecha-   nism . In these works , one of the main research   directions is to use a sparse attention to substitute   the quadratic - cost attention .   Some early works ( Qiu et al . , 2020 ; Ho et al . ,   2019 ; Beltagy et al . , 2020 ) have been proposed   to reduce the time complexity by restricting ev-   ery query to focus only on keys that are nearby or   at ﬁxed intervals . This method ﬁxes the sparsity   pattern without considering the similarity between   queries and keys , limiting its ability to assemble   critical information from large contexts . Different   from these works , our method attempts to automat-   ically aggregate critical keys for each query based   on dependency relationships .   Moreover , the clustering - pattern methods were   used in Self - Attention to implement a sparse atten-   tion . For example , Reformer ( Kitaev et al . , 2020 )   and Routing Transformer ( Roy et al . , 2021 ) in-   troduce Locality - Sensitive Hashing and K - Means   algorithms , respectively , to reduce complexity to   O(NlogNd)andO(Np   Nd ) . However , in this   kind of method , the clustering process and train-   ing process are separate , which is a limitation in   improving effectiveness . Based on previous re-   searches , we proposed a novel Neural Clustering   Method , which can be seamlessly integrated into   the network of speciﬁc tasks for joint training andoptimization to improve effectiveness .   3 Model   In this section , we ﬁrst introduce our Neural Clus-   tering Method . Then , we introduce our Neural   Clustering Attention Mechanism which combines   our clustering method and the Self - Attention Mech-   anism .   3.1 Neural Clustering Method   As shown in Figure 1 ( a ) , our clustering method   takes word hidden states X2Rand centroid   hidden states C2Ras inputs . Cis initialized   randomly in the ﬁrst layer . Then , we can get the   subordinate ( similarity ) matrix Ubetween word   vectors and centroid vectors . It can be deﬁned as :   wherekis the number of clusters and Nis the   length of sequence . W2R is a   parameter matrix .  ( · ) is a similarity measure func-   tion and it is the inner product operation in this   scenario . Xis the j - th row of matrix XandC   is the i - th row of matrix C. The subordinate value   U2[0;1]is the normalized similarity value be-   tween the i - th centroid vector and emphj - th word   vector , and it represents the degree of the word X   belonging to the centroid C.   Then , we get the updated centroids by weight-   ing all the word hidden states . The corresponding   formula is as follows:2392   whereiandjrepresent the index value of the cen-   troid and word , respectively . Then we group the   word vectors according to the subordinate matrix   U , as follows :   whereUis the j - th column of the matrix Uand   functionArgmax ( · ) assigns word hidden states to   the corresponding cluster according to the maxi-   mum subordinate value . Therefore , I2Rrepre-   sents the cluster index of all the word hidden states .   Then , we sort the word vectors according to the   cluster indexes I , as follows :   where the function Sort ( · ) is used to arrange word   hidden states belonging to the same cluster to ad-   jacent positions in ascending order of cluster in-   dex . X2R is the sorted word vectors .   I2Ris used to record the original positions   of shufﬂed word hidden states in the sequence and   will be used in Eq . 10 . Through the above process ,   we get the grouped and sorted word hidden states   X , as shown in Figure 1 ( a ) .   Clustering Loss : Clustering Loss ( L ) is the   mean of the negative similarity scores of word hid-   den states and their belonging centroids , and it   will give guidance to learn the optimal clustering   scheme . It is deﬁned as follows :   whereX , bCrepresent the j - th word hidden state   in the sequence and the updated centroid . The   function  ( · ) is a similarity measure function and   needs to be consistent with Eq . 2.From the above analysis , our Neural Clustering   Method is based on the soft clustering . There is a   subordinate value between each pair of word vec-   tors and centroid vectors , which can quantitatively   describe the fuzzy relationship , so that the cluster-   ing can be carried out objectively and accurately .   In addition , Neural Clustering Method is based on   the neural network , which is easy to integrate into   the network corresponding to the target task . The   reconstruction of centroid vectors depends on all   the word vectors and is based on the continuous   optimization for the clustering objective function   ( as shown in Eq . 6 ) and the task - speciﬁc objective   function to get better effectiveness .   In addition , we carried out a clustering compar-   ison experiments between our method and tradi-   tional clustering methods and observed improve-   ments of our method in effectiveness . See Ap-   pendix A for more details .   3.2 Neural Clustering Attention Mechanism   As described in Section 3.1 , our Neural Clustering   Method groups word vectors with strong depen-   dency into the same cluster and outputs the sorted   word vectors X. Then , we use different matrices   to projectXinto matrix Q , K , andV , as   follows :   whereW , WandW2Rare weight   matrices . Q , KandVare matrices Query ,   Value and Key , respectively .   The number of members in each cluster may not   be uniform , which makes it difﬁcult for all clusters   to perform the attention mechanism in parallel . For   parallel computing , after arranging word hidden   states in the same cluster to be in adjacent positions,2393we chunk them into equal blocks in order , as shown   in Figure 1 ( b ) ( essentially similar to the masking of   Reformer ) . The process can be written as follows :   whereQ2RandK2Rare the   i - th Query block and Key block respectively . w   ( w= ) is the number of members in each block .   MatrixVhas operations similar to K. Af-   ter chunking , Query contains one sequence block   while Key and Value consist of two contiguous   blocks , which corresponds to Lmentioned in   Eq . 11 . Each token in Query focuses on two blocks   of tokens so that the query can cover the words in   the same cluster as much as possible . Of course , it   does not have to be 2 , and can be adjusted .   Then , we perform the attention operation within   the sequence block in parallel and concatenate the   output of each block .   whereZ2RandZ2R.Zis the   output of the i - th sequence block after the attention   operation .   Finally , we recover the shufﬂed sequence ( out-   put ) to obtain the ﬁnal result , as follows :   where the function Resort ( · ) aims to recover shuf-   ﬂed sequence according to the original position   record vector Iobtained from the Eq . 5 . Z2   Ris the output of Neural Clustering Attention .   For the autoregressive modeling , we provide a   Masked Neural Clustering Attention Mechanism to   prevent the leftward information ﬂow . More details   can be found in Appendix B.   Centroid Sorting Loss : Centroid Sorting Loss   ( L ) is the mean of the negative similarity scores of   the adjacent centroid pairs . In Eq . 8 , each token in   Query block is expected to focus on two continuous   blocks of tokens . Lmakes word hidden states   belonging to adjacent clusters are also close to each   other . It is deﬁned as follows : wherekis the number of centroids , bCis the i-   th updated centroid , and the meaning of  ( · ) is   consistent with Eq . 6 .   In our method , Clustering Loss , Centroid Sorting   Loss , and the loss of target tasks of the model are   assigned different weights for joint optimization .   More details can be found in Appendix C.   3.3 Analysis of Complexity   The complexity of Neural Clustering Attention   Mechanism comes from two parts : ( i ) Neural Clus-   tering Method . In this part , we need to calculate the   subordinate matrix between centroid hidden states   C2Rand the word hidden states X2R ,   referring to the Eq . 2 , which leads to the complex-   ity ofO(Nkd ) . ( ii ) Attention Mechanism . For this   part , we compute attention within the Query block   ( 2R ) and Key block ( 2R ) , refer-   ring to the Eq . 9 , which leads to the complexity of   O(kwd)wherew=. In summary , the overall   complexity is O(Nkd + kwd ) . Whenkis set top   N , the complexity is approximately O(Np   Nd ) .   4 Experiments   In order to verify the effectiveness and efﬁciency   of our method , we carried out the following tasks .   We choose Transformer and its clustering - pattern   variants ( Reformer , Routing transformer ) as base-   line models . The implementations of the attention   layer of Reformer and Routing transformer refer   to the open source codes . For a fair comparison ,   our proposed method and baseline models have the   same architecture , except for the attention layer .   4.1 Machine Translation   We validate our model on IWSLT14 German-   English and WMT14 English - German benchmarks ,   which have been widely used for machine transla-   tion tasks . For IWSLT14 De - En , it contains about   160 K training sentence pairs and is pre - processed   by using prepare-iwslt14en2de.sh . For WMT14   En - De , it contains about 4.5 million training sen-   tence pairs and it is pre - processed by using prepare-   wmt14en2de.sh . We use the BLEU score as the   effectiveness evaluation metric . Some hyperparam-   eters are set : the number of encoder and decoder2394Model IWSLT14 De - En WMT14 En - De   Transformer(Vaswani et al . , 2017 ) 34.4 27.3 / 26.4   Reformer(Kitaev et al . , 2020 ) 34.0 26.3 / 25.4   Routing Transformer(Roy et al . , 2021 ) 32.5 24.3 / 23.6   ClusterFormer 34.9 27.4 /26.5   Model CR MR SUBJ MPQA 20NEWS Average   DiSAN ( Shen et al . , 2018 ) 84.8 – 94.2 90.1 – –   MPSAN ( Dai et al . , 2020 ) 85.4 – 94.6 90.4 – –   Transformer(Vaswani et al . , 2017 ) 86.2 81.8 95.4 89.9 83.6 87.38   Reformer(Kitaev et al . , 2020 ) 83.0 79.7 94.7 88.6 81.7 85.54   Routing Transformer(Roy et al . , 2021 ) 80.1 78.8 94.3 81.2 81.3 83.14   ClusterFormer 88.1 82.7 96.2 90.4 83.8 88.24   layersL= 6 , the number of centroids k= 3 . The   dimension of word embedding and model d   = 512 . Speciﬁcally , for IWSLT14 , the number of   heads is set to 4 and d= 1024 . For WMT14 , the   number of heads is set to 8 and d= 2048 .   As shown in Table 1 , our method boosts effec-   tiveness on both datasets . Speciﬁcally , the Tok-   enized BLEU score is improved by at least 1.5 %   compared with other models on IWSLT14 datasets .   Compared with the latest models Reformer and   Routing Transformer , ClusterFormer respectively   has 2.6 % and 7.4 % improvement . Our method   shows the same trend on WMT14 datasets . Es-   pecially , compared with Reformer and Routing   Transformer , the Tokenized BLEU score of Cluster-   Former respectively has 4.2 % and 12.8 % improve-   ment and the sacreBLEU score respectively has   4.3 % and 12.3 % improvement .   4.2 Text Classiﬁcation   We validate our model on ﬁve text classiﬁcation   tasks . CR ( Hu and Liu , 2004 ): Customer re-   views composed of positive or negative product   reviews ; MR ( Pang and Lee , 2004 ): Movie re-   views divided into positive and negative categories ;   SUBJ : Subjectivity dataset where the target is to   classify a text as being subjective or objective ;   MPQA ( Wiebe et al . , 2005 ): Opinion polarity de-   tection subtask . 20NEWS : A international standard   dataset for text classiﬁcation , text mining , and in-   formation retrieval research . The dataset collectsabout 20,000 newsgroup documents , divided into   a collection of newsgroups on 20 different topics .   Accuracy is used as the evaluation metric for these   datasets . In addition , for all datasets , word embed-   dings are initialized by GloVe ( Pennington et al . ,   2014 ) with 300 - dimension . Some hyperparameters   are set : The number of encoder layers L= 2 , the   dimension of model d= 300 , the number of heads   h= 4 , and the number of centroids kis adjusted   near the square root of the max length .   As shown in Table 2 , ClusterFormer outperforms   all baseline models and improves the test accu-   racy by at least 3.16 % , 1.70 % for CR and SUBJ   datasets , respectively . In addition , on the MPQA   dataset , ClusterFormer achieves a comparable re-   sult with MPSAN . We also carry out the text clas-   siﬁcation task on the long text dataset 20NEWS .   The accuracy for the 20NEWS dataset increases at   least 0.24 % compared with other models . In ad-   dition , compared with the latest models Reformer   and Routing Transformer , our model respectively   has 6.1 % , 3.8 % , 1.6 % , 2.0 % , 2.6 % and 10.0 % ,   4.9 % , 2.0 % , 11.3 % , 3.1 % improvement for CR ,   MR , SUBJ , MPQA and 20NEWS datasets .   4.3 Natural Language Inference ( NLI ) and   Text Matching   In this section , we conduct Natural Language   Inference tasks on SNIL , SciTail datasets , and   Text Matching tasks on Quora , WikiQA datasets .   SNLI ( Bowman et al . , 2015 ) is a benchmark dataset2395Model SNLI SciTail QuoraWikiQA   map mrr   DELTA ( Han et al . , 2019 ) 80.7 – – – –   Bigram - CNN ( Yu et al . , 2014 ) – – – 0.619 0.628   Transformer(Vaswani et al . , 2017 ) 83.7 76.6 85.4 0.601 0.613   Reformer(Kitaev et al . , 2020 ) 78.6 67.3 74.3 0.587 0.603   Routing Transformer(Roy et al . , 2021 ) 76.3 72.6 81.5 0.560 0.574   ClusterFormer 83.9 77.8 85.4 0.630 0.648   for natural language inference . There are 570k   human - annotated sentence pairs with four labels .   SciTail ( Khot et al . , 2018 ) is an entailment classiﬁ-   cation dataset constructed from science questions   and answers . Quora Question Pairs is a dataset for   paraphrase identiﬁcation with two classes indicat-   ing whether one question is a paraphrase of the   other . The evaluation metric for these three data   sets is Accuracy . WikiQA ( Yang et al . , 2015 ) is a   retrieval - based question answering dataset based on   Wikipedia , which is composed of 20.4k/2.7k/6.2k   ( train / dev / test ) samples . The mean average pre-   cision ( MAP ) and mean reciprocal rank ( MRR )   are used as the evaluation metrics . For SNIL and   Quora datasets , word embeddings are initialized by   GloVe ( Pennington et al . , 2014 ) with 300 dimen-   sions . For the rest , we use random word embedding   vectors with 300 dimensions . Some hyperparame-   ters are set : L= 1 , the number of heads h= 6 and   the number of centroids k= 3 .   As shown in Table 3 , our model achieves the best   results for most datasets . Speciﬁcally , the accuracy   of our model is at least 1.6 % higher than baseline   models on the SciTail dataset . On WikiQA , our   model improves the result by at least 1.8 % and   3.2 % in MAP and MRR evaluation metrics , respec-   tively . Our model and Transformer have consid-   erable effectiveness on SNLI and Quora datasets .   In addition , compared with the latest models Re-   former and Routing Transformer , our model has   6.7 % , 15.6 % , 14.9 % , and 10.0 % , 7.2 % , 4.8 % im-   provement for SNLI , SciTail , Quora datasets . For   the WikiQA dataset , the score increases 7.3 % and   7.5 % by our model in MAP and MRR compared to   Reformer . The score increases 12.5 % and 13.0 %   compared to Routing Transformer .   4.4 The choice of clustering numbers k   In this section , we test the effect of different cluster-   ing numbers ( k ) on the effectiveness and efﬁciency .   We test our model on the 20NEWS dataset of text   classiﬁcation tasks with a NVIDIA V100 ( 16 GB )   GPU . Some hyperparameters are set : the number of   encoder layers Lis 2 , the dimension of the model   dis 300 , the batch size is 64 , and the max sequence   lengthNis 1500 .   From Table 4 , we can draw the following conclu-   sions : ( i ) Accuracy of our model : In general , within   a certain range during the growth of k , the perfor-   mance of our model is relatively stable . When the   value ofkgoes beyond a certain threshold , the   performance of our model degrades ; ( ii ) Memory   cost of our model : As the number of centroids k   increases , the memory cost of the model decreases   ﬁrst and then increases ; ( iii ) Training time of our   model : As the number of centroids kincreases , the   training time of the model also decreases ﬁrst and   then increases . Therefore , according to this law ,   our method can simultaneously gain both the effec-   tiveness and efﬁciency of the model by determining   an appropriate kvalue through ﬁnite experiments .   4.5 Ablation study for Clustering Losses   In this section , we provide an ablation experiment   about the two kinds of clustering losses . We ver-   ify the effectiveness of the two loss modules by   assigning different weight . Some hyperparameters2396   are set : the number of encoder layers Lis 1 , the   dimension of model dis 300 , the batch size is 128   and the max sequence length Nis 500 .   From Table 5 , the experimental result shows that   bothLandLcontribute to the performance . For   example , on dataset SciTail , the accuracy with the   best result is improved by 1.46 % ( acc ) compared   with the result without the two losses . On dataset   WikiQA , the accuracy with the best result is im-   proved by 3.10 % ( map ) , 1.89 % ( mrr ) compared   with the result without the two losses .   4.6 Time and Memory Analysis   In this section , we provide a comparison experi-   ment on dataset 20NEWS about the time and mem-   ory cost for different models . About the dataset , its   average sequence length is approximately 280 and   the maximum sequence length exceeds 10,000 . To   compare time and memory cost , we set the range   of sequence length Nas ( 0 , 2000 ] and batch size   to 20 . We test the memory and time cost on a   NVIDIA V100 GPU . We take the time of 1000   steps forward propagation of the model as the in-   ference time , and the time of 1000 steps forward   and back propagation as the training time .   As shown in Figure 4 , as the sentence length in-   creases , both Routing Transformer and our model   can signiﬁcantly reduce memory cost compared to   Transformer . When Nexceeds 2000 , our model ,   Routing Transfomer and Reformer reduce the mem-   ory by 53.8 % , 60.8 % , and 31.8 % , respectively .   As shown in Figure 3 , the training time of Trans-   former increases signiﬁcantly with increasing se-   quence length , while our model and Routing Trans-   former have a relatively small increase on GPU de-   vices . When Nis 2000 , our model , Routing Trans-   fomer and Reformer reduce the training time by   51.4 % , 41.8 % , and 14.4 % . However , the inference   speed of these improvements is inferior compared   with Transformer , which may be caused by the de-   crease of the model parallelism . The above analysis   fully demonstrates the efﬁciency and effectiveness   of our proposed Neural Clustering Mechanism .   5 Conclusion   In this paper , we propose a Neural Clustering Atten-   tion Mechanism to address the reduced effective-   ness issue in sparse attention methods . This issue   is mainly caused by the introduction of a sparse pat-   tern that is separated from the target task or does   not consider the similarity between words . In our   method , we design a neural clustering algorithm to   better capture critical pairs of dependencies . We   integrate this clustering algorithm and the neural   network to jointly train and optimize with speciﬁc   tasks together to further contribute to the effective-   ness and efﬁciency . The experimental results show   that our model can achieve better effectiveness and   a comparable or even better efﬁciency , compared   with the latest typical sparse attention models , Re-   former and Routing Transformer.2397Acknowledgements   This work is supported in part by the state   key development program of China ( grant   No.2017YFE0111900 ) , Natural Science Founda-   tion of China ( grant No.61772363 ) , PolyU internal   fund ( grant No.1 - BD47 ) under the research project   ( P0039657 ) of the Hong Kong Polytechnic Univer-   sity , and the Beijing Academy of Artiﬁcial Intelli-   gence(BAAI ) .   References23982399A Comparison experiment for clustering   methods   In this section , we carry out the comparison experi-   ment between the Neural Clustering Method and   other clustering methods to verify the effectiveness   of our clustering method .   Firstly , according to the division mode , we intro-   duce the following two kinds of clustering methods .   Hard Clustering : Each element to be recog-   nized is strictly divided into a certain cluster . It   deﬁnes an either / or relationship R2f0;1gbe-   tween the element and clusters .   Soft Clustering ( Fuzzy Clustering ): Each ele-   ment to be recognized is subordinate to all clusters   ( with different subordinate values ) . It deﬁnes a   fuzzy relationship U2[0;1]between the element   and clusters .   Regarding the selection of the comparative clus-   tering method , we chose the classic hard clustering   algorithm K - means ( used for Routing Transformer )   and the soft clustering algorithm SOM , a compet-   itive neural network . In addition , since Locality   Sensitive Hashing ( used for Reformer ) can not con-   struct the loss function ( no iteration condition ) , it   can not be used for the following clustering task on   the MNIST dataset . In the experiment , we set the   number of centroids kto 10 , 50 , 100 , 200 , and 300   respectively .   As shown in Table 6 , our method consistently   has the best effectiveness in experiments with dif-   ferent centroids . In particular , when the number of   centroids exceeds 300 , the accuracy of our method   can reach 93.6 , which improved the accuracy by   0.54 % and 1.74 % compared with SOM and K-   Means , respectively . From the above analysis , we   have conﬁrmed that Neural Clustering Method is   a general clustering method . It can also achieve   better effectiveness compared with K - Means and   SOM when handling clustering tasks alone . B Masked Neural Clustering Attention   Mechanism   For autoregressive modeling , we provide a Masked   Neural Clustering Attention Mechanism to prevent   the leftward information ﬂow . We ﬁrst obtain the   original position indexes of the Q;Kmatrix .   The formula is as follows :   whereI2RandI2R.Iand   Iare the original position indexes of the i - th   sequence block QandKrespectively .   Then , we extend Iin the second dimension   to getM2R , and extend Iin the   ﬁrst dimension to get M2R. Therefore ,   we can obtain a mask matrix Mfor the i - th se-   quence block by comparing these position indexes ,   as follows :   whereM2Ris the mask matrix of the   i - th block and the Mis composed of either 0 or   1.Mis the value of the u - th row and v - th column   of the matrix M. Then , for each word in Query   block , it will mask the words whose index value   in Key block is greater than it according to mask   matrixM , as follows :   whereS2Ris the similarity matrix of   thei - th sequence block . The subsequent opera-   tions are the same as Neural Clustering Attention   Mechanism .   C Optimization of the multi - task   learning for ClusterFormer   As shown in Figure 1 , our model , ClusterFormer ,   consists of two joint training tasks , clustering tasks ,   and speciﬁc tasks of the model . Therefore , it con-   tains the loss from the two tasks . The loss functions   related to the clustering task are Clustering Loss   ( L ) and Centroid Sorting Loss ( L ) . Their equa-2400   tions are as follows :   whereXandCare respectively word vectors and   centroid vectors . Nis the length of the input and   kis the number of clusters . The loss function of a   speciﬁc task ( e.g. , text classiﬁcation ) is formulated   as follows :   where byis predictive value and yis the corre-   sponding target value . Then , the overall loss func-   tion can be written as :   whereC , W , andbare respectively centroid pa-   rameters , weight parameters , and bias parameters   in ClusterFormer . , , and0are non - negative   coefﬁcients , which are used to adjust the proportion   of the importance of corresponding tasks .   Therefore , we can obtain the optimal parameters   in the neural network by minimizing the loss func-   tionL through gradient descent , as follows :   wherec , wandbrepresent the element value of   the corresponding vectors . d is the dimension-   ality of the model . mandnrepresent the number   of weight and bias parameters , respectively . is   the learning rate . From the above , it can be seen   that the update of centroid , weight , and bias pa-   rameters is the result of multi - task joint learning in   ClusterFormer .   D Convergence Analysis   In this section , we provide a comparison experi-   ment on the SNLI dataset about the convergence   speed for standard Transformer and efﬁcient Trans-   formers during training , as shown in Figure 5 . In   our experiment , the epochs of different models to   achieve convergence is : Transformer has 21 epochs ,   Reformer has 24 epochs Routing Transformer has   29 epochs and ClusterFormer has 19 epochs . Com-   pared with Transformer , our model has a compar-   ative convergence rate and is more stable . In ad-   dition , compared with the latest model Reformer   and Routing Transformer , our model not only has a   faster and more stable convergence speed , but also   has better effectiveness .   E Comparison experiment of different   methods under the ultra - long sequence   condition   In this section , we have supplemented the experi-   ment of time and memory cost in different methods   on extremely long sequence tasks . We tested them   on text classiﬁcation of the 20NEWS dataset and   trained them on a NVIDIA V100 GPU . We set the2401   number of centroids k to the square root of the max   length and set the batch size to 2 ( constrained by   resources ) .   As shown in Table 7 , we can see that our method   has a better efﬁciency advantage on long sequences   compared with Transformer . And as the sequence   length increases , the advantage in memory and   training time are even more signiﬁcant .   F Pretraining experiment with the   Neural Clustering Attention   Mechanism   In this section , we pretrain a model with the Neural   Clustering Attention Mechanism with two unsu-   pervised tasks , masked language modeling ( MLM )   and next sentence prediction ( NSP ) . The parame-   ter settings of our pretraining model are similar to   BERT - small - uncased . Some hyperparameters are   set : the number of layers Lis 4 , the hidden size is   512 , and the number of heads his 8 .   For downstream tasks , we use the General Lan-   guage Understanding Evaluation ( GLEU ) bench-   mark which is a collection of diverse natural lan-   guage understanding tasks . We use a batch size   of 32 and ﬁne - tune On a scale of 3 to 10 epochs   over the data for all GLEU tasks . For each task ,   we selected the best ﬁne - tuning learning rate ( 5e-5 ,   4e-5 , 3e-5 , and 2e-5 ) on the Dev set .   As shown in Table 8 , experimental results   demonstrate that our method can have a good per-   formance improvement through Pretraining . Es-   pecially , compared with the Bert - Small - uncased ,   pretraining ClusterFormer respectively has 23.0 % ,   4.9 % / 7.0 % , and 2.2 % / 2.0 % improvement on   CoLA , STSB , and MRPC datasets . The experimen-   tal results show that our model has the potential   to do more NLP tasks including pretraining and   non - pretraining tasks.2402