  Nathan SchucherSiva ReddyHarm de VriesServiceNow ResearchMila / McGill UniversityFacebook CIFAR AI Chair   { nathan.schucher,harm.devries}@servicenow.com   Abstract   Prompt tuning has recently emerged as an effec-   tive method for adapting pre - trained language   models to a number of language understanding   and generation tasks . In this paper , we investi-   gate prompt tuning for semantic parsing — the   task of mapping natural language utterances   onto formal meaning representations . On the   low - resource splits of Overnight and TOPv2 ,   we find that a prompt tuned T5 - xl significantly   outperforms its fine - tuned counterpart , as well   as strong GPT-3 and BART baselines . We also   conduct ablation studies across different model   scales and target representations , finding that ,   with increasing model scale , prompt tuned T5   models improve at generating target represen-   tations that are far from the pre - training distri-   bution .   1 Introduction   With the widespread success of pre - trained lan-   guage models ( LMs ; Devlin et al . 2019 ; Raffel   et al . 2020 ; Bommasani et al . 2021 ) , it becomes   increasingly important to explore how such models   can be adapted to downstream tasks . One adap-   tation method which has recently attracted much   attention is prompt design ( Brown et al . , 2020 ; Shin   et al . , 2020 ) , which modulates the behaviour of a   LM through a task description and a few input-   output examples . Brown et al . ( 2020 ) show that   this adaptation strategy is increasingly effective for   larger LMs . However , prompt design is sensitive   to the exact phrasing of the prompt , and , more im-   portantly , performs worse than fine - tuning models   on task - specific examples ( Lester et al . , 2021 ) .   Prompt tuning has recently arisen as a strong   performing alternative adaption method ( Lester   et al . , 2021 ) . Rather than hand - designing discrete   prompts , prompt tuning optimizes the embeddings   of a number of task - specific prompt tokens . In   contrast to fine - tuning , this method keeps almost   all LM parameters frozen . On a set of languageFigure 1 : We show that the T5 prompt tuning perfor-   mance difference between target representations shrinks   as the number of parameters increase , with constrained   decoded T5 - xl achieving close to performance parity .   understanding tasks , Lester et al . ( 2021 ) show   that prompt tuning becomes competitive with fine-   tuning for the largest pre - trained T5 models ( Raffel   et al . , 2020 ) . Li and Liang ( 2021 ) also explore   a related parameter - efficient adaptation method   called prefix - tuning , finding that it outperforms   fine - tuning on low - resource natural language gen-   eration tasks .   In this paper , we investigate prompt tuning for   semantic parsing . This task is fundamentally differ-   ent from the aforementioned language understand-   ing and generation tasks , as it requires that mod-   els output formal meaning representations which   do not resemble the natural language distribution   seen during pre - training . In particular , we focus   on the low - resource setup because examples for   semantic parsing are difficult and expensive to col-   lect ( Wang et al . , 2015 ; Marzoev et al . , 2020 ) . We   therefore evaluate prompt tuning on two datasets :   the 200 - shot version of Overnight ( Wang et al . ,   2015 ; Shin et al . , 2021 ) and the low - resource splits   TOPv2 ( Chen et al . , 2020 ) . On both datasets , we   compare prompt tuning T5 against fine - tuning and   investigate the effect of canonicalizing the meaning148representation , i.e. to what extent naturalizing the   logical forms influences performance . In addition ,   we study the effect of T5 model scale on Overnight   as well as varying data regimes on TOPv2 . Our   main findings can be summarized as follows :   •For large T5 models , prompt tuning signifi-   cantly outperforms fine - tuning in the low - data   regime , resulting in an absolute improvement   of 6 % and 15 % on Overnight and TOPv2 , re-   spectively . This performance gap decreases   when more training data becomes available .   •With growing model size , prompt tuned T5   models are increasingly capable of outputting   diverse target representations ( see Figure 1 ) .   On Overnight , we find that the disparity be-   tween canonical and meaning representations   shrinks from 17 % to 4 % for T5 - small and   T5 - xl , respectively . On TOPv2 , prompt tuned   T5 - large models are much better at generating   out - of - vocabulary tokens than T5 - small .   2 Related work   Our work is related to recent work on semantic   parsing and prompt tuning , which we briefly de-   scribe below .   2.1 Semantic Parsing   Semantic parsing is the task of converting a nat-   ural language utterance u= ( u , . . . , u)to a   formal meaning representation z= ( z , . . . , z ) .   These meaning representations , also referred to   as logical forms , can be interpreted by machines   and executed in a real environment . For ex-   ample , ThingTalk ( Campagna et al . , 2019 ) and   TOP ( Gupta et al . , 2018 ) are meaning represen-   tations for executing commands of virtual as-   sistants , while SQL is a representation for in-   teracting with relational databases . In recent   years , neural sequence - to - sequence models have   become the dominant approach for semantic pars-   ing tasks ( Dong and Lapata , 2016 ) .   Canonicalization A common simplification step   in semantic parsing is to canonicalize the meaning   representations . That is , the meaning representa-   tionzis naturalized to a canonical form cthrough   a grammar or set of rules . Examples of the mean-   ing and canonical representation for Overnight and   TOPv2 ( Wang et al . , 2015 ; Chen et al . , 2020 ) can   be found in Fig . 2.When canonical representations are available ,   Berant and Liang ( 2014 ) argue that semantic pars-   ing can be seen as a paraphrase task . They propose   to use a paraphrase model — using e.g. word vectors   trained on Wikipedia — to find the best paraphrase   of utterance uamong a set of canonical utterances .   They show this paraphrase model improves re-   sults over directly generating logical forms on two   question - answering datasets . Marzoev et al . ( 2020 )   extends this work by showing that pre - trained lan-   guage models like BERT can be effective para-   phrasers . While Berant and Liang ( 2014 ) ; Marzoev   et al . ( 2020 ) use models to score canonical utter-   ances , Shin et al . ( 2021 ) propose to constrain the   generation process of autoregressive models like   BART and GPT-3 . On a number of few - shot seman-   tic parsing tasks , they demonstrate the benefit of   generating canonical representations over meaning   representations .   2.2 Prompt - tuning   Lester et al . ( 2021 ) evaluates prompt tuning on   SuperGLUE , a benchmark consisting of eight lan-   guage understanding tasks . They find that prompt   tuning becomes competitive with fine - tuning for   the largest T5 model . Li and Liang ( 2021 ) propose   prefix - tuning to adapt BART and GPT-2 for natu-   ral language generation tasks . This method differs   from Lester et al . ( 2021 ) in that it prepends train-   able embeddings for each layer of the language   model rather than introducing token embeddings at   the input layer . They demonstrate that pre - fix out-   performs fine - tuning baselines . Similarly , Liu et al .   ( 2021 ) also show encouraging results for prompt   tuning on natural language understand and gener-   ation tasks . Qin and Eisner ( 2021 ) also explores   prompt tuning but for a knowledge extraction task .   Inserting general adapter layers into pre - trained   language models is also proposed in Houlsby et al .   ( 2019 ) ; Mahabadi et al . ( 2021 ) . Related to our   work are also other few - shot adaptation techniques   like PET ( Schick and Schütze , 2021 ) . Moreover ,   adapter layers have also been explored in the com-   puter vision domain ( Rebuffi et al . , 2017 ; de Vries   et al . , 2017 ) .   3 Experiments   To evaluate low - resource prompt tuning , we com-   pare against fine - tuned variants of the same model   on two semantic parsing datasets with canonical   representations available . We compare both large149   and small variants of the T5 architecture on these   datasets and experiment with various canonicalized   representations .   3.1 Datasets   Overnight The Overnight semantic parsing   dataset ( Wang et al . , 2015 ) consists of 13,682 natu-   ral utterance , canonical form , meaning representa-   tion triples split across eight domains . To simulate   low - resource splits of this dataset , we follow Shin   et al . and create randomly subsampled splits of   200 training examples for each domain , using 20 %   of the remaining data for validation . We measure   and report denotation accuracy by evaluating all   predicted queries using the SEMPRE toolkit ( Be-   rant et al . , 2013 ) . We repeat each experiment on   Overnight with five different random splits .   TOPv2 Chen et al . ( 2020 ) introduce the TOPv2   dataset , a task - oriented semantic parsing dataset   with eight domains , two of which come with pre-   defined low - resource splits . The authors propose a   principled way of constructing low - resource train-   ing sets , samples per intent and slot ( SPIS ) , in-   tended to ensure equal exposure to ontology labels   across domains of varying complexity . We experi-   ment with the weather andreminder domains at the   10 , 25 , and 500 SPIS resource splits , performing   five runs on each model varying the random seed .   Thereminder domain is the most challenging with   19 intent labels , 32 slot labels , and with 21 % of the   programs having a depth greater than 2 . Weather in   comparison has 7 intent labels , 11 slot labels , and   no programs with depth greater than 2.3.2 Canonicalized Representations   3.2.1 Overnight   Overnight uses a context - free synchronous gram-   mar to generate canonical representations for the   logical forms . As can be seen in Fig . 2 , these canon-   ical representations resemble natural language .   3.2.2 TOPv2   Chen et al . apply a set of simple modifications   to the TOPv2 meaning representations to arrive   at a canonical form used in all their experiments .   Unlike Overnight , these pre - processing steps are   largely small encoding differences and do not   change the syntactic structure of the logical forms .   We adopt all of these canonicalization steps ( except   for lexicographic sorting of the semantic parse tree )   and add an ontology label shortening step . Exam-   ples of these transformations can be seen in Fig . 2   and are briefly described below .   Simplify removes redundant utterance tokens un-   necessary for interpreting the meaning repre-   sentation .   Out - of - Vocab adds the entire intent or slot label   to the tokenizer as a new single tokens with   a corresponding randomly initialized embed-   ding .   In - Vocab replaces the intent and slot labels with   a short unique identifier representable by the   pre - trained tokenizer .   We perform an ablation over these canonicaliza-   tion choices , repeating each experiment three times   with varying random seed.150   3.3 Models   We provide training details and hyperparameters   for all models in Appendix A. Below , we briefly   explain the prompt - tuning methodology .   3.3.1 Prompt Tuning   Prompt tuning , as proposed by Lester et al . ( 2021 ) ,   prepends a sequence of continuous embeddings   p= ( p , . . . , p)to the sequence input embed-   dings e(u ) = ( e(u ) , . . . , e ( u))before feeding   it to a language model with parameters θ . Dur-   ing prompt tuning we optimize the prompt embed-   dings ( p , . . . , p)exclusively , keeping the lan-   guage model parameters θand the pretrained vo-   cabulary embeddings fixed . Note that this process   still requires backpropagating gradients through the   full language model . Like fine - tuning models , we   maximize the likelihood of generating the output   sequence z.   4 Results   In Table 1 , we report Overnight results across four   T5 model scales and two target representations .   In Table 2 , we add constrained decoding ( see Ap-   pendix A ) to our best performing T5 model and   compare against previously reported Overnight re-   sults . In Table 3 , we display the results of T5 - large   on the three different SPIS - splits of TOPv2 , and   include the BART - CopyPtr results from Chen et al .   ( 2020 ) . In Table 4 , we summarize the results of the   canonicalization ablation study for TOPv2.4.1 Prompt tuning vs fine tuning   We find that prompt tuning improves over fine-   tuning for all large model configurations and tar-   get representations . On Overnight , prompt tuned   denotation accuracy exceeds fine - tuned counter-   parts by up to 5 points with T5 - large and T5 - xl .   For T5 - small and T5 - base , prompt tuning remains   competitive ( within 1 % average accuracy ) with   fine - tuning when predicting canonical forms . On   TOPv2 , prompt tuning achieves an absolute im-   provement of 15 % mean accuracy over fine - tuning   on the lowest SPIS split . This performance dispar-   ity lessens when training data increases ; however ,   prompt tuned T5 - large continues to beat its fine-   tuned counterpart by 5 points at 500 SPIS and the   BART - CopyPtr model by 1.4 points .   Our prompt tuning models outperform previ-   ously reported results on these datasets . On   Overnight , our best model — T5 - xl PT with canon-   ical representations and constrained decoding —   outperforms the BART FT model of Shin et al .   ( 2021 ) by 5 accuracy points , and GPT-3 by more   than 2 points . On the 25 SPIS split of TOPv2 , we   see an average improvement of more than 5 points   compared to the BART - CopyPTR of Chen et al .   ( 2020 ) .   4.2 Canonical vs meaning representations   Our main finding is that prompt tuned T5 models   become better at generating meaning representa-   tions with increased model size . On Overnight , we   see the absolute difference between canonical and   meaning representations shrink from 17.5 points151   for T5 - small to 3.4 points for T5 - xl ( Table 1 ) . This   gap shrinks another 18 % to 2.8 points when we   apply constrained decoding to T5 - xl ( Table 2 ) . By   contrast , Shin et al . ( 2021 ) reports an 11.7 point   difference when prompting GPT-3 . For our fine-   tuning baselines , we observe a small performance   gap of 4 points across target representations for   BART and T5 - xl , while we observe no gap for   T5 - small , T5 - base , and T5 - large models .   In our TOPv2 experiments we find similar ev-   idence of large T5 model flexibility for generat-   ing sequences far from the training distribution .   In particular , for our most intrusive canonicaliza-   tion scheme Out - of - Vocab , which adds novel   tokens to the vocabulary and leaves these embed-   dings un - trained , we find no significant reduction   in performance for T5 - large across all data resource   levels . T5 - small , in comparison , sees almost a 50 %   drop in performance relative to no canonicaliza-   tion ( None ) at the 10 SPIS level and continues to   underperform by 33 % at the 500 SPIS level .   Interestingly , we find that In - Vocab drasti-   cally reduces performance for T5 - small at the 10   SPIS level—30.9 % vs. 43.4 % for None — but   slightly outperforms it at 500 SPIS . We speculate   thatIn - Vocab effectively anonymizes the ontol-   ogy tokens , obscuring information that is useful   for prediction . In low - data regimes there is not   enough training data to learn the semantics of these   anonymized tokens , whereas with enough data this   problem vanishes .   5 Conclusion   We find that prompt tuning is an effective method   for adapting language models to the semantic pars-   ing task . Prompt tuning significantly outperforms   fine - tuning in low - data regimes , and remains com-   petitive in the fully supervised setting . We further-   more find that while canonicalizing meaning rep-   resentations can slightly improve performance , the   disparity between target representations decreases   when prompt tuning larger T5 models . This re-   sult differs from previous work ( Shin et al . , 2021 )   which suggested that pre - trained LMs are much   better equipped to output canonical than meaning   representations . However , a significant limitation   of prompt tuning is that it takes more time to con-   verge than fine - tuning . We believe one fruitful di-   rection for future research is to find ways to reduce   the compute required to prompt tune.1526Ethical Considerations and Limitations   There are two main limitations of this work . The   first is the limited analysis of the learned prompts .   While concurrent work has shown that interpreting   prompts is a difficult task , it is still an important   consideration and left for future work ( Khashabi   et al . , 2021 ) . Secondly , training prompts on mean-   ing representations requires substantially more   compute than fine - tuning . This may exacerbate   inequalities in regions where access to data and   compute are similarly limited ( Ahia et al . , 2021 ) .   References153154A Models   Here we provide all model details and hyperpa-   rameters to reproduce our results . We experiment   with BART and T5 ( Lewis et al . , 2020 ; Raffel   et al . , 2020 ) , two large pre - trained encoder - decoder   language models . BART is trained on the same   160 GB text dataset used to train RoBERTa ( Lewis   et al . , 2020 ) with a denoising objective . There are   two size configurations ( BART - base , BART - large )   and we experiment only with the 406 M parameter   BART - large on the Overnight dataset . T5 is trained   on the 750 GB C4 dataset ( Raffel et al . , 2020 ) with   a de - noising objective . We use the T5 - v1.1 check-   points from Lester et al . ( 2021 ) that were trained   for an additional 100 K steps with the Prefix - LM   objective . T5 - v1.1 has five configurations at vari-   ous scales : small , base , large , xl , xxl which have   60 M , 220 M , 770 M , 3B , and 11B parameters , re-   spectively . Here , we experiment with models up to   T5 - xl . All experiments were run with PyTorch ( v.   1.8.1 ) and the Huggingface Transformers ( v. 4.8.2 )   library ( Paszke et al . , 2019 ; Wolf et al . , 2020 ) .   Fine - tuning baseline We compare against base-   lines that fine - tune all parameters of BART and T5 .   We train the T5 models with AdaFactor ( Shazeer   and Stern , 2018 ) and BART with Adam ( Lewis   et al . , 2020 ; Kingma and Ba , 2015 ) . On TOPv2 ,   we use a learning rate of 10and batch size of   128 . On Overnight , we use a learning rate of 10   and a batch size of 64 across all sizes of T5 . On   both datasets , we train for 5000 epochs and perform   model selection by early stopping on the validation   set .   Prompt tuning We follow the prompt tuning pro-   cedure proposed by Lester et al . for T5 . We use 150   prompt tokens for all model sizes with a learning   rate of 0.3 optimized with AdaFactor . We train for   5000 epochs on most domains , although it some-   times took as many as 20000 epochs to converge on   the low - resource splits . Like the fine - tuned base-   line , we perform model selection with best exact   match accuracy on the validation set . We apply   the same method to BART and found that it did   not converge under a number of hyperparameter   configurations . We therefore exclude prompt tuned   BART models from our results . Constrained Decoding We implement grammar-   constrained decoding by building a prefix tree con-   taining all canonical or meaning representations in   the dataset as in Shin et al . ( 2021 ) . When doing   constrained decoding we perform a beam search   with 10 beams and use the prefix tree to look up   valid single token continuations of the decoded se-   quence .   B Results   For completeness , we provide all Overnight results   in Table 5 .   B.1 Training Times   Prompt tuned parameter efficiency comes at a   cost : we find that prompt tuning takes significantly   longer to train with early stopping than does fine-   tuning . On the Overnight dataset , fine - tuned mod-   els typically took 250 epochs before validation per-   formance plateaued . Our prompt tuned models   frequently took more than 1000 epochs when pre-   dicting canonical representations , and up to 5,000   when predicting meaning representations . In Fig-   ure 3 , we show example training curves for prompt   tuning and fine - tuning.155156