  Antonio Laverghetta Jr. andJohn Licato   Advancing Machine and Human Reasoning ( AMHR ) Lab   Department of Computer Science and Engineering   University of South Florida   Tampa , FL , USA   { alaverghett,licato}@usf.edu   Abstract   Reasoning using negation is known to be dif-   ﬁcult for transformer - based language models .   While previous studies have used the tools of   psycholinguistics to probe a transformer ’s abil-   ity to reason over negation , none have focused   on the types of negation studied in develop-   mental psychology . We explore how well trans-   formers can process such categories of nega-   tion , by framing the problem as a natural lan-   guage inference ( NLI ) task . We curate a set of   diagnostic questions for our target categories   from popular NLI datasets and evaluate how   well a suite of models reason over them . We   ﬁnd that models perform consistently better   only on certain categories , suggesting clear dis-   tinctions in how they are processed .   1 Introduction   Negation is an important construct in language for   reasoning over the truth of propositions ( Heine-   mann , 2015 ) , garnering interest from philosophy   ( Horn , 1989 ) , psycholinguistics ( Zwaan , 2012 ) ,   and natural language processing ( NLP ) ( Morante   and Blanco , 2020 ) . While transformer language   models ( TLMs ) ( Vaswani et al . , 2017 ) have   achieved impressive performance across many   NLP tasks , a great deal of recent work has found   that they do not process negation well , and often   make predictions that would be trivially false in   the eyes of a human ( Rogers et al . , 2020 ; Ettinger ,   2020 ; Laverghetta Jr. et al . , 2021 ) .   In developmental psychology , there has likewise   been a great deal of interest in how a child ’s abil-   ity to comprehend negation emerges in the early   years of life ( Nordmeyer and Frank , 2013 , 2018b ;   Reuter et al . , 2018 ; Grigoroglou et al . , 2019 ) . Un-   like in NLP , which typically treats negation as rep-   resenting a single monolithic competency , this re-   search has long understood that there are manykinds of negation used in everyday interactions   ( Bloom , 1970 ; Pea , 1982 ) . This ranges from using   negation to express a child ’s rejection of something   to clarifying a child ’s knowledge . These “ devel-   opmental ” categories of negation do not emerge   simultaneously ; children tend to start using certain   kinds before others ( Nordmeyer and Frank , 2018a ) .   Given that these categories represent some of   the earliest uses of negation among humans , un-   derstanding how well TLMs can master them is   important for building more human - like models of   language processing . Understanding how well mod-   els perform on different categories will indicate   whether they have mastery of some forms of nega-   tion , while also helping to identify failure points .   Another interesting question is whether the proﬁ-   ciency of TLMs on these categories is at all related   to competencies in human children ( e.g. , is the cat-   egory which models consistently perform the best   on the same that children most frequently employ ? ) .   However , to our knowledge , no prior work in NLP   has focused on how well models perform on the   forms of negation of interest to developmental psy-   chology .   In this short paper , we investigate how well a   suite of TLMs can process developmental nega-   tion , by framing the problem as a natural lan-   guage inference ( NLI ) task . We develop a rule-   based parser to extract problems from existing NLI   datasets , and evaluate our models on each cate-   gory , in order to determine ( i)whether certain cat-   egories are more solvable by our models than oth-   ers , and ( ii)what relationships exist among the   categories . We ﬁnd that models can consistently   achieve stronger performance only on certain cat-   egories , and that training on combinations or se-   quences of these categories does not substantially   improve a model ’s downstream performance.5452 Related Work   Negation is known to be frequently used in every-   day conversation . While this includes its logical   form , we primarily focus on negation ’s psycholin-   guistic forms , especially those that have been stud-   ied in the context of developmental psychology .   Negation emerges early in child development , with   ‘ no ’ sometimes being a child ’s ﬁrst word ( Schnei-   der et al . , 2015 ) , and even infants appear to under-   stand forms of negation ( Piaget , 1980 ; Hochmann   and Toro , 2021 ) . Preschool children use at least   three different kinds of negation ( Bloom , 1970 ) ,   but possibly as many as nine ( Choi , 1988 ) . As   noted by Nordmeyer and Frank ( 2018a ) , one of   the ﬁrst categories children use is rejection , where   a child rejects an object or activity . This is later   followed by existence , where a child might ex-   press the lack of an object , and later still denial ,   which a child uses to deny the truth of a claim .   Larger scale studies of child - directed speech have   found that truth - functional kinds of negation tend   to emerge later ( Liu and Jasbi , 2021 ) , but individual   children do vary in their speciﬁc order of acquisi-   tion ( Nordmeyer and Frank , 2018a ) . It is unknown   whether this ordering reﬂects any deeper depen-   dencies among the different categories , or whether   the ordering is reﬂected in how artiﬁcial language   models ( LMs ) learn negation .   In NLP , methods from psycholinguistics have   been used to probe the reasoning capabilities of   LMs . Results from some studies have indicated   that TLMs are not human - like in their processing   of negation ( Ettinger , 2020 ; Kassner and Schütze ,   2020 ) . A similar line of work has used the NLI   task to probe a model ’s ability to process negation   and found that TLMs will often alter their predic-   tions when negation is inserted or removed , even   when the negation does not alter the entailment re-   lationship ( Hossain et al . , 2020 ; Hartmann et al . ,   2021 ) . As argued by Kruszewski et al . ( 2016 ) , part   of the challenge of modeling purely logical nega-   tion is that a predicate often occurs in very similar   contexts regardless of whether it is being negated .   They argue that we should view negation as be-   ing a “ graded similarity function ” , and show that   distributional models can predict human plausibil-   ity judgments quite well , even in the presence of   negation . These works show that it is unclear how   well distributional models , especially TLMs , are   actually processing negation . We contribute to this   literature from a new perspective , by studying how   well models can reason over forms of negation   common in developmental psychology .   3 The Developmental Negation Corpus   We use the NLI task to study the negation reasoning   capabilities of our models . NLI problems consist   of two sentences : a premise ( p ) and hypothesis   ( h ) , and solving such a problem involves assessing   whether ptextually entails h. The generic structure   of the NLI task makes it suitable for studying a va-   riety of underlying reasoning skills , including nega-   tion . We speciﬁcally use the SNLI ( Bowman et al . ,   2015 ) and MNLI ( Williams et al . , 2018 ) datasets .   To automatically identify questions that contain   a speciﬁc kind of negation , we rely on the work   by Liu and Jasbi ( 2021 ) which studied how fre-   quently different kinds of developmental negation   occur in child - directed speech , using the data from   the CHILDES corpus ( MacWhinney , 2014 ) . To do   this , they created a simple rule - based parser to au-   tomatically tag each sentence in CHILDES with   the type of negation it contained ( if any ) . We re-   implement their parser , in some cases tweaking   the rules slightly to better suit the structure of the   NLI task . For each example across all the splits of   both datasets , we ﬁrst obtain a dependency parse   of both pandhusing the diaparser package ( Wang   et al . , 2019 ) , and check if either contains an explicit   negation marker ( “ no ” , “ not ” , or “ n’t ” ) . If one span   contains negation , we check if the syntactic struc-   ture obeys the rules of any of our categories . If the   span falls into a category , we mark it as belonging   to that category . We use these questions as the diag-   nostic set for our experiments , splitting out 1/3 of   the questions in each category as a diagnostic test   set , and leaving the remainder as a diagnostic train   set ( and we will refer to them as such ) . We place   the remaining NLI questions containing no nega-   tion in a separate NLI set , giving us about   730,000 examples we use to ﬁnetune our models   on the NLI task . We split out 9,000 questions from   this train set at random to use as a NLIset , bal-546   anced for each label . In the following , we describe   the precise rules used to determine which category   a negated example should be assigned to :   Possession ( PO)We require that the lemma of   the root be have , has , orhad , and that the root is   directly modiﬁed by both the negation and the verb   do .   Existence ( EX)We require that there occur in   the text and precede the negative marker and that   the negative marker directly modiﬁes a noun phrase ,   determiner , or an adverb .   Labeling ( L)We require that the sentence be-   gin with either That orIt , and that the root of the   sentence is a noun which is modiﬁed by isor ’s .   Prohibition ( PR)We require that the sentence   not contain a subject and that the negation is im-   mediately preceded by do . To not conﬂate this cat-   egory with others , we ﬁlter out cases where the   root contains one of the explicit markers of another   category ( e.g. , likeorwant in the case of rejection ) .   Inability ( I)We require that the negation di-   rectly modify the root of the sentence , and that   the word immediately before the negation is either   canorcould ( e.g. , can not do ) . Prior literature has   typically viewed inability from an egocentric per-   spective . However , we found that allowing only the   ﬁrst person severely restricted the number of ex-   amples extracted , and therefore chose to also allow   the second and third person .   Epistemic ( EP)We require that the root be re-   member , know , orthink , and that the root be directly   modiﬁed by the verb do .   Rejection ( R)We require that the lemma of the   root word be either likeorwant , and that the root   is modiﬁed by the negative marker .   After performing extraction , categories Land   PRcontained fewer than 1000 examples , which   we deemed was insufﬁcient to split into separate   train and test sets . To address this , we developeda simple data augmentation approach that utilized   the Wordnet database ( Miller , 1998 ) . From the de-   pendency parse of both pandh , we check if the   root of either parse occurs in both spans . If it does ,   we obtain all synonyms of the word in Wordnet and   replace the root in both spans with the synonym   ( doing this for every synonym ) . We found this sim-   ple approach increased the number of examples for   bothLandPRto at least 1500 . Note that we per-   formed no augmentation for the other categories , as   our parser extracted at least 1500 examples for all   other cases . Table 1 shows statistics for the dataset   after augmentation .   Table 2 shows extracted examples , along with   their category assignment . We generally found that   the extracted examples matched up with the pro-   totypical category quite well , although in some   cases their semantics differed slightly . For instance ,   consider a PRexample with p = don’t miss hav-   ing a ﬂick through the albums andh = The pic-   tures of old Madeira show a more interesting city   than now , which is an MNLI example originally   extracted from a travel guide . Although this tech-   nically counts as PR , it does not have quite the   same semantics as an actual command . Unfortu-   nately , these ambiguities are not easily resolved ,   given that negation takes on many forms and may   occur at any location within a sentence . We , there-   fore , opted to focus on forms of negation that can   be easily extracted , and leave improvements to our   dataset creation protocol for future work .   4 Experiments   Using the curated dataset , we performed a series of   exploratory experiments to help us understand how   well TLMs process each of the negation categories .   We use BERT ( Devlin et al . , 2019 ) , and RoBERTa   ( Liu et al . , 2019 ) , two popular transformer LMs   that have demonstrated impressive results on a   variety of language understanding tasks . We also   examine MiniBERTa ( Warstadt et al . , 2020 ) and   BabyBERTa ( Huebner et al . , 2021 ) , which are both547based on the RoBERTa architecture but were pre-   trained on a much smaller number of tokens ( 10   million and 5 million respectively ) , which is more   realistic to the amount of language a child is ex-   posed to in the ﬁrst few years of life . We use the   Huggingface implementation of all models ( Wolf   et al . , 2020 ) , and use both the base andlarge ver-   sion of BERT and RoBERTa , which differ only in   the number of trainable parameters .   Experiment 1 : We began by investigating   whether TLMs would master certain negation cate-   gories sooner than others over the course of train-   ing . We train our models on NLI for 10   epochs , using a learning rate of 1e 5 , a weight   decay of 0:01 , a batch size of 16 , and a maximum   sequence length 175.We selected these hyperpa-   rameters to be similar to those which were previ-   ously reported to yield strong results when train-   ing on NLI datasets ( Laverghetta Jr. et al . , 2021 ) .   We additionally evaluated the models on NLI ,   and found that they all achieved a Matthews Cor-   relation of at least 0.6 ( Matthews , 1975 ) , and thus   concluded that these hyperparameters were suit-   able . For every end of epoch checkpoint across all   models , we obtained evaluation results on each di-   agnostic test set . Importantly , the models are not   ﬁnetuned on any negated NLI questions for this ex-   periment , meaning that all knowledge of negation   comes from pre - training . Results are shown in Fig-   ure 1 . We see that the categories have similar rank-   ings in terms of accuracy . For example , LandPO   are among the top two best - performing categories ,   while Ris generally one of the worst - performing   ones , indicating clear distinctions in how LMs pro-   cess the categories . BabyBERTa , unlike other mod-   els , also shows stronger similarities to how children   acquire negation . For instance , while Ris thought   to be one of the ﬁrst categories children acquire ,   BabyBERTa is the only model where Ris one of   the highest - ranking categories in terms of accuracy .   Experiment 2 : One might expect that children   develop a more abstract understanding of negation   as they are exposed to different categories . This   was suggested by Pea ( 1978 ) who argued that more   abstract forms of negation develop from less ab-   stract ones , suggesting that mastering one form of   negation can lead to positive transfer on others . In   Experiment 2 , we examined how much positive   transfer could be obtained from training on one   of the negation categories , and then testing on the   others . We adopt a similar methodology to Pruk-   sachatkun et al . ( 2020 ) , who explored the condi-   tions that affect intermediate task transfer learning .   Using the models trained in Experiment 1 , we fur-   ther ﬁnetune these models for 25 epochs on each   diagnostic train set separately . We then evaluate the   ﬁnetuned models on each diagnostic test set , which   allows us to examine all possible pairwise interac-   tions among categories . Figure 2 shows the results   for all combinations of diagnostic categories for   training and testing . Surprisingly , we ﬁnd that posi-   tive transfer generally only occurs when a model is   trained on the same category it is being tested on .   Training on a different category has little to no ef-   fect on the target category . BabyBERTa is again an   exception , as we do see positive transfer for most   pairs , suggesting the model is generalizing across   categories   Experiment 3 : Building on Experiment 2 , we   examined how the performance of our models is   affected when trained on all diagnostic categories   in sequence . Assuming that no positive transfer   exists among the categories , we would expect to   see a model ’s performance on a particular cate-   gory improve only after it has been trained on that   same category , and even training on multiple other   categories should not substantially improve perfor-548   mance on the target . Using the models from Ex-   periment 1 , we ﬁnetune each model for 10 epochs   on every diagnostic train set , using the sequence of   categories shown in the x - axis of Figure 3 . Addi-   tionally , we under - sample all diagnostic train sets to   have the same number of questions as PR , so that   all categories contribute the same amount of data .   Figure 3 shows the results . For some categories ,   such as LandPR , we see the expected trend . The   largest accuracy gain for these categories occurs   whenever the model is trained on the same cate-   gory it is being tested on , and performance drops   slightly after being trained on others . However , for   categories such as R , the best performance gain   is not always after being trained on the same cat-   egory . We sometimes see the model continue to   improve on Rafter being trained on R , and in   some cases , training on Rcauses performance on   Rtodecrease .   5 Discussion and Conclusion   In this paper , we have explored how well trans-   formers process categories of developmental nega-   tion . We ﬁnd that performance rankings across cat-   egories are generally consistent , but that the cate-   gories seem to test for orthogonal skills in the ma-   jority of LMs . In BabyBERTa , we see signiﬁcant   similarities with the order of negation acquisition   in children . Two of the best performing categories   areRandL , while two of the worst are EX and   PR , which aligns quite well to the order observed   by Liu and Jasbi ( 2021 ) . It thus seems that TLMs   do at least partially reﬂect the order of negation   acquisition observed in children , although more   experiments would be needed to understand the   extent of this correlation . That we found category   rankings to generally be consistent across LMs may   have interesting implications , and understanding   why LMs struggle with certain categories may help   to improve the ability of LMs to process negation .   Future work can build on these experiments in   several ways . In Experiments 2 and 3 , we modeled   interactions among the negation categories in either   a pairwise or sequential fashion , which is unlikely   to reﬂect how children are exposed to negation .   More experiments , mixing all of the categories at   once in various proportions , might yield a more   realistic model of cognitive development . Our ap-   proach also requires that each category ﬁts into a   speciﬁc structure , which limits the amount of exam-   ples that can be extracted . Future work will need   to expand our ruleset to include more variations   in the negated utterances covered . Finally , while   we primarily focus on ﬁnetuning , pre - training is   likely to impact the proﬁciency of our models on   the categories as well . Future work should precisely   control the prevalence of each category in the pre-   training corpus , to observe what effect this has on   downstream performance.549References550551