  Sarawoot Kongyoung , Craig Macdonald , Iadh Ounis   University of Glasgow , UKs.kongyoung.1@research.gla.ac.uk{craig.macdonald,iadh.ounis}@glasgow.ac.uk   Abstract   To address the Conversational Question An-   swering ( ORConvQA ) task , previous work has   considered an effective three - stage architec-   ture , consisting of a retriever , areranker , and   areader to extract the answers . In order to ef-   fectively answer the users ’ questions , a number   of existing approaches have applied multi - task   learning , such that the same model is shared   between the reranker and the reader . Such ap-   proaches also typically tackle reranking and   reading as classification tasks . On the other   hand , recent text generation models , such as   monoT5 and UnifiedQA , have been shown to   respectively yield impressive performances in   passage reranking and reading . However , no   prior work has combined monoT5 and Uni-   fiedQA to share a single text generation model   that directly extracts the answers for the users   instead of predicting the start / end positions in   a retrieved passage . In this paper , we investi-   gate the use of Multi - Task Learning ( MTL ) to   improve performance on the ORConvQA task   by sharing the reranker and reader ’s learned   structure in a generative model . In particular ,   we propose monoQA , which uses a text genera-   tion model with multi - task learning for both the   reranker and reader . Our model , which is based   on the T5 text generation model , is fine - tuned   simultaneously for both reranking ( in order to   improve the precision of the top retrieved pas-   sages ) and extracting the answer . Our results on   the OR - QuAC and OR - CoQA datasets demon-   strate the effectiveness of our proposed model ,   which significantly outperforms existing strong   baselines with improvements ranging from   +12.31 % to +19.51 % in MAP and from +5.70 %   to +23.34 % in F1 on all used test sets .   1 Introduction   Research in Conversational Search and Conver-   sational Question Answering ( ConvQA ) is of   increasing interest . The ConvQA task ( Choi et al . ,   2018 ; Reddy et al . , 2019 ) consists in understandingFigure 1 : Overview of ( a ) reranker and extractive reader   and ( b ) reranker and generative reader .   the question based on a given conversational his-   tory , and extracting an answer from a given passage .   This task is an extractive type of QA , meaning   that the answer takes the form of a span in the   provided passage , and can be successfully tackled   by employing an extractive or generative reader .   Extractive reader models ( Qu et al . , 2019a , b ; Yeh   and Chen , 2019 ) are typically employed , where   the goal is to classify the start and end positions of   the answer span in the given passage . In contrast ,   generative readers ( Raffel et al . , 2020 ; Khashabi   et al . , 2020 ; Lewis et al . , 2020 ; Karpukhin et al . ,   2020 ) have demonstrated impressive results on the   extractive QA tasks , where the goal is to generate   tokens that are a subset of a passage . Recently ,   there has been more focus on retrieval as part of   the ConvQA pipeline , known as Open - Retrieval   Conversational Question Answering ( ORConvQA ) .   In this setting , the ORConvQA system needs to   apply the ConvQA model upon passages retrieved   from a large collection , given a question , before   actually extracting the answer .   To address the ORConvQA task , prior works ( Qu   et al . , 2020 , 2021 ; Liang et al . , 2022 ) have adopted   a three - stage architecture , including a retriever , a   reranker , and a reader to extract the answers . First ,   the retriever retrieves the top Krelevant passages   from the collection based on a question and its   conversation history . The reranker and the reader   then respectively rerank and identify an answer   in the top Kpassages . We also adopt this three-   stage architecture in our proposed model . How-   ever , in order to investigate the effectiveness of the7207cross - encoder reranker , we consider a two - stage   pipeline including a retriever and a reader , as a   baseline for comparison with our system . For the   retriever , existing works ( Qu et al . , 2020 , 2021 ;   Liang et al . , 2022 ; Xiong et al . , 2020 ; Yu et al . ,   2021 ) have focused on using bi - encoder dense re-   trieval ( a question encoder and a passage encoder ) ,   which applies neural contextual language mod-   els , such as ALBERT or BERT , for encoding the   question and passage into low - dimensional vec-   tors and computing their relevance scores . For ex-   ample , Yu et al . ( 2021 ) proposed ConvDR , which   encodes the question and its history in a dense vec-   tor learned with a teacher - student model to mimic   a dense representation of the manually rewritten   question . ConvDR has also been shown to out-   perform other retriever models for conversational   search such as sparse BM25 , and bi - encoders using   ALBERT ( Qu et al . , 2020 ) or BERT ( Xiong et al . ,   2020 ; Karpukhin et al . , 2020 ) . Due to the good   effectiveness of bi - encoder dense retrievers for pas-   sage retrieval , we adapt this type of retrieval model   as our retriever . We also consider other recent ex-   isting bi - encoder passage retrievers such as TCT-   ColBERT ( Lin et al . , 2021b , 2020a ) and CQE ( Lin   et al . , 2021a ) as baseline passage retrievers .   Recently , Multi - Task Learning ( MTL ) , which is   a method of learning multiple different but related   tasks at the same time , has become a popular ap-   proach for tackling several tasks using a uniform   model ( Qu et al . , 2019b ) . For instance , MTL has   been employed in order to efficiently answer the   questions posed by the users ( Qu et al . , 2020 , 2021 ) .   In this manner , the network structure is shared be-   tween the reranker and the reader . Doing this , ex-   isting works ( Qu et al . , 2020 , 2021 ) also typically   approach reranking and extractive reading as classi-   fication tasks , with two fully - connected layers ( one   for the reranker and reader , respectively ) added   to find an answer span for the retrieved passages   ( start / end positions ) as well as to predict the rele-   vance score of the question to the passage as shown   in Figure 1(a ) . In this paper , we use the multi - task   learning of the reranker and the extractive reader   as our strongest baseline .   On the other hand , Nogueira et al . ( 2020 ) pro-   posed monoT5 , a text generation model , which was   fine - tuned to generate the tokens “ true ” or “ false ”   depending on whether the document is relevant or   not to the query . Indeed , the monoT5 model has   been shown to outperform BERT - based models inpassage reranking ( Nogueira et al . , 2020 ) . In ad-   dition , many studies ( Raffel et al . , 2020 ; Khashabi   et al . , 2020 ; Lewis et al . , 2020 ; Karpukhin et al . ,   2020 ) have focused on developing a generative   reader which is fine - tuned as a text generation   model to extract the answer from the passage . In   particular , Khashabi et al . ( 2020 ) introduced the   UnifiedQA model , which has been shown to yield   impressive performances on many extractive QA   datasets . However , we show that compared to using   monoT5 and UnifiedQA separately , a joint learning   can enhance the learning efficiency and prediction   accuracy of a model for the ORConvQA task , since   by sharing the learning model the reranker and   reader can simultaneously predict the answer and   reranking score . Indeed , a joint learning by sharing   a single model trained using MTL reduces the   memory needs and speeds up inference ( Sun et al . ,   2020 ; Standley et al . , 2020 ) . In addition , we com-   bine the effective monoT5 ( to rerank the retrieved   passages ) and UnifiedQA ( to extract the answer   from the highest scored passage ) models into a   strong baseline . To the best of our knowledge , no   prior work has combined monoT5 and UnifiedQA   by sharing a single text generation model , in order   to directly extract the answers instead of predicting   the start / end positions in a retrieved passage .   In summary , in this work , we investigate the   use of Multi - Task Learning ( MTL ) to improve   performance on the ORConvQA task by sharing   the reranker and reader ’s learned structure . We   propose monoQA , which uses a text generation   model with multi - task learning for both the   reranker and reader . Our model , which is based on   the T5 ( Raffel et al . , 2020 ) text generation model ,   is fine - tuned simultaneously for both reranking ( in   order to improve the precision of the top retrieved   passages ) and extracting the answer . Unlike   previous work , monoQA makes predictions by   generating the first token for the passage reranking   task , followed by the other tokens for the answer   extraction task , as illustrated in Figure 1(b ) . Our   contributions are summarised as follows : ( 1 )   we leverage Multi - Task Learning with a text   generation model by sharing the reranker and   reader ’s learned structure to effectively addresses   the ORConvQA task ; ( 2 ) using two different   ORConvQA datasets , we compare our model   to two strong baselines from the literature , and   show that our MTL reranker and generative   reader approach yields the best F1 , Recall , MRR,7208   and MAP performance improvements over the   strongest baseline with statistically significant im-   provements ranging from +5.70 % to +23.34 % ; ( 3 )   the proposed MTL model combining the reranker   and generative reader significantly outperforms   and is twice as fast for inference than the individual   application of the monoT5 and UnifiedQA models   for reranking and extracting the answer .   2 A Three - Stage Pipeline for a   ORConvQA system   We first define the ORConvQA task in Section 2.1 .   An overview of the proposed MTL text generation   model follows in Section 2.2 . Then , we explain   how to fine - tune the monoQA model in Section 2.3 .   2.1 Open - Retrieval Conversational Question   Answering Task   Following Qu et al . ( 2020 ) , the ORConvQA task is   defined as follows : given a current question q , and   a conversation history H – consisting of a list of   c−1questions and the ground truth answer pairs ,   i.e. H= [ ⟨q , a⟩ ] – the task is to predict an   answer afor the current question qfrom a pas-   sage collection C. An example of the ORConvQA   task is shown in Figure 2 . It consists of a relevant   passage ( shown in bold ) , and a conversation history   of length c= 3 ( two previous pairs of questions   and answers ) . In order to answer question q , the   ORConvQA system needs to retrieve a list of rel-   evant passages in Cand leverage the conversation   history Hto understand and correctly predict an   answer afrom all relevant passages in C.   2.2 Model Overview   To tackle the task described in Section 2.1 , follow-   ing Qu et al . ( 2020 ) , our system consists of three   main components : ( 1 ) a retriever for relevant pas-   sages retrieval ; ( 2 ) a passage reranker for improv-   ing the precision of the top retrieved passages ; and   ( 3 ) a passage reader for generating the answer from   the top retrieved passage . In particular , we present   monoQA , which uses a text generation model with   Multi - Task Learning for both the reranker andreader . First , the retriever retrieves the top Krel-   evant passages from the collection based on a ques-   tion and its history as described in Figure 3 . In   order to produce the final answer , the reranker and   reader then re - score and identify the answer in the   top - Kpassages from the retriever . In doing so , a   single model application gives an answer to both   stages - i.e. , whether this is a relevant passage and   the position of the answer in the passage . We now   describe each component of our model in detail .   2.2.1 Retriever   The left part of Figure 3 presents the retriever com-   ponent . Following Yu et al . ( 2021 ) , we use a dual-   encoder model named ConvDR , which consists of   a question encoder and a passage encoder . The   ConvDR model first encodes the current question   concatenated with all previous questions and pas-   sage into the embedding space :   E = ConvDR ( q;q ) ,   E = ConvDR ( p ) ( 1 )   where q , qandpdenote the current question ,   the historical questions , and a passage , respectively .   EandEare the embeddings of qconcatenated   withqandp , respectively . ConvDR uses the   dot product of EandEto calculate the retrieval   score of a passage pfor the current question q ,   with historical questions q :   score(q;q , p ) = E·E ( 2 )   To retrieve the top Kpassages in the embedding   space , ConvDR uses FAISS ( Johnson et al . , 2021 ) ,   which is a library for efficient approximate nearest   neighbour search . Yu et al . ( 2021 ) provides further   details on ConvDR . Finally , we note that , in   practice , a rewritten formulation qof the current   question qcan be used to resolve ambiguities   such as coreference resolution .   2.2.2 monoQA : Reranker & Generative Reader   The right part of Figure 3 presents our proposed   model , which uses T5 , a large pre - trained language   models designed for text generation . In particu-   lar , text generation approaches can be trained to   generate a meaningful textual response based on   some input text . Moreover , like BERT ( Devlin   et al . , 2019 ) , the pre - trained BART ( Lewis et al . ,   2019 ) and T5 ( Raffel et al . , 2020 ) text generation   models can be fine - tuned to perform a variety of   downstream tasks . To adopt an MTL approach to   a text generation model for jointly learning from   both passage reranking and answer extraction , the7209   MTL model makes predictions by generating the   first token for the passage reranking task and the   follow - up tokens for the answer extraction task . In   particular , when fine - tuning the T5 model for a   downstream task , we use Prompt Learning , which   is a method to modify the model by using a task-   specific prompt together with the input ( Liu et al . ,   2021 ) . We deploy a T5 model to capture the rela-   tion between the rewritten question qof the current   question qand the passage pas shown in the right   part of Figure 3 . In particular , we define a monoQA   transformation function as monoQA ( · ) by taking   the input sequence as follows : ( 3 )   where f ( ) is a prompt function ( template ) to   format q , and the passage into an input sequence   for monoQA . The model is then fine - tuned to gen-   eratentarget tokens , as shown in Equation ( 3 ) , for   the token wnamely " true " or " false"depending   on whether the passage is relevant or not to question   q , while the follow - up tokens w , ... , ware the   output sequence for the answer of the question q.   At inference time , following Nogueira et al .   ( 2020 ) , we apply a softmax only on the logits of   the " true " and " false " tokens of the first generated   token wto calculate the reranker score as follows :   score= softmax ( w ) ( 4 )   2.3 monoQA Training   Given Kretrieved passages and a rewritten ques-   tionq(see Section 2.2.1 ) , our monoQA model   jointly trains the reranker and the reader as follows :   Joint training : We consider how to fine - tune   monoQA in order to generate the tokens for both   passage reranking and answer extraction . In   particular , the prompt function ( Equation ( 3 ) )   formats a question qand a passage pinto an input   sequence for monoQA and monoQA then outputs   the contextual representation h. After that , the   monoQA decoder takes the previously generated   tokens as input and performs attention over hand   then generates the next token . In particular , given a   tuple⟨q , p , a⟩ , the training objective is to minimisethe following loss function :   L=/summationdisplaylogP(a|h , a ) ( 5 )   where Mis the number of tokens in the ground   truth answer a , ais the itoken in a , and ais   the beginning of sequence token ( < s > ) .   We also consider relevance accuracy and word-   level F1 scores for selecting the best training model   checkpoint during the evaluation step with the de-   velopment set of the OR - QuAC dataset . Relevance   accuracy is defined as the percentage of correct   predictions for the first token generated from the   model . The target token is " true " ( the passage is   indeed relevant ) or " false " ( a non - relevant passage ) .   Following Qu et al . ( 2020 ) , the word - level F1 is   calculated by first removing stopwords and then   considering the overlapping portion of the words   in the prediction and ground truth answer .   Prompt : Recently , Prompt Learning , which is a   method to tailor pre - trained language models to   downstream tasks by using a task - specific prompt   together with the input , has recently become a   popular approach for tackling several tasks using a   uniform model ( Liu et al . , 2021 ) . To fine - tune the   monoQA model for passage reranking and answer   extraction , we use Prompt Learning to modify the   model input . By doing this , we investigate several   prompts in previous works ( Nogueira et al . , 2020 ;   Khashabi et al . , 2020 ) , and for completeness we   evaluate all of them in order to choose the most   effective . Details of the Prompt Learning and their   corresponding experiments and results are provided   in Appendix A.2 . We do not investigate Prompt   Learning for question reformulation since our   main contribution focuses on leveraging the output   of a generative model for re - ranking and reading .   Positive and negative passages : Selecting positive   and negative passages is a crucial step for training   monoQA . For instance , passages relevant to a ques-   tion are provided in the ORConvQA task . All other   passages in the collection , which are unjudged , can   be viewed as non - relevant by default . To cope with   this issue , following Yu et al . ( 2021 ) , we employ   a hard negative sampling technique by randomly   selecting the negative passage pfor the question7210q from the top Kretrieved passages by ConvDR .   For training monoQA , the output sequence for the   positive passage pbegins with the token " true "   followed by the ground truth answer ; the output se-   quence for the negative passage pbegins with the   token " false " followed by " CANNOTANSWER " .   Model initialisation : We consider the use of   different models to initialisation monoQA during   training , since we propose to combine monoT5 and   UnifiedQA to share a single text generation model .   Moreover , both monoT5 and UnifiedQA are   fine - tuned based on the t5 - base model . Therefore ,   we investigate which of monoT5 , UnifiedQA , and   t5 - base , are suitable for initialising monoQA ( see   details in Section 4.1 ) .   3 Experimental Setup   Our experiments address the three following   research questions : RQ1 : Which model to   use for initialising monoQA , namely which of :   monoT5 , UnifiedQA , and t5 - base , lead to the   best performance of monoQA on the ORConvQA   task ? RQ2 : How does monoQA compare to other   existing baselines , namely : ( 1 ) . the ORConvQA   system proposed by Qu et al . ( 2020 ) ; ( 2 ) . the   ORConvQA system proposed by Qu et al . ( 2020 )   but using ConvDR as a retriever ; and ( 3 ) . using   ConvDR as a retriever , monoT5 as a reranker ,   and UnifiedQA as a reader ? RQ3 : How does our   proposed system , which is a three - stage pipeline   ( retriever , reranker , and reader ) , compare to the   two - stage pipeline baselines ?   3.1 Datasets   To conduct our evaluation of monoQA , we choose   the OR - QuAC ( Qu et al . , 2020 ) and OR - CoQA ( Qu   et al . , 2021 ) datasets , which are extractive Ques-   tion Answering ( QA ) datasets . However , the OR-   CoQA dataset can be also considered as a genera-   tive QA dataset because it contains both span and   freeform answers . Indeed , in this paper , we focus   on extractive QA only since we train our monoQA   only on the OR - QuAC training set . In addition , fol-   lowing ( Qu et al . , 2021 ) , we remove unanswerable   questions from both datasets .   OR - QuAC : This dataset has been introduced   by Qu et al . ( 2020 ) , adapting the well - known   QuAC ( Choi et al . , 2018 ) dataset to an open-   retrieval setting . This dataset is an aggregation   of three existing datasets consisting of ( 1 ) the   QuAC ( Choi et al . , 2018 ) dataset , which isan information seeking dataset , ( 2 ) the CA-   NARD ( Elgohary et al . , 2019 ) dataset , which   contains questions that humans have re - written   from questions in the QuAC dataset , and ( 3 ) the   Wikipedia corpus , a large collection of over 11   million passages , which are used as the knowledge   source for actually answering a given question .   OR - CoQA : Qu et al . ( 2021 ) introduced this   dataset by aggregating the CoQA ( Reddy et al . ,   2019 ) dataset with the Wikipedia corpus from   the OR - QuAC dataset . In contrast to OR - QuAC ,   the gold passages for each question are not   included in the OR - CoQA dataset . Moreover ,   unlike OR - QuAC , there are no manually rewritten   questions in the OR - CoQA dataset . As a result , we   do not use OR - CoQA for training monoQA .   As exemplified in Figure 2 , a question in either   OR - QuAC and OR - CoQA can be ambiguous and   difficult to understand without its context ( e.g. , q3 :   " Did he release any other albums as a solo artist ? " ) .   At training time , we fine - tune monoQA by using a   manually rewritten query ( q ) , which is provided   by the OR - QuAC dataset . Thereafter , at infer-   ence time , following Dalton et al . ( 2020 , 2021 ) ;   Lin et al . ( 2020b ) , we employ another T5 model   trained using the CANARD dataset to rewrite the   OR - QuAC and OR - CoQA test set questions into   context - independent questions that can be used as   input for monoQA .   To validate our monoQA model during training ,   we use the OR - QuAC development set by selecting   only positive examples ( ground truth consisting of   an answer and the corresponding passage for the   question ) after removing the unanswerable ques-   tions following ( Qu et al . , 2021 ) . This development   set consists of 490 dialogues with 2808 questions   in total .   3.2 Baselines and Implementation Details   Baselines : To demonstrate the effectiveness of our   proposed monoQA model , we compare it with the   seven baseline systems listed as ( a)-(g)below :   Three - stage pipelines : ( a)The first ORCon-   vQA system has been proposed by Qu et al .   ( 2020 ) . It adopts a duo - ALBERT encoder as the   retriever and an MTL of the reranker and reader by   sharing a BERT encoder . We make use of the code   provided by Qu et al . ( 2020 ) ; ( b)This baseline is   adapted from ( a ) by replacing the duo - ALBERT   encoder passage retriever with ConvDR ( Yu   et al . , 2021 ) in a similar manner to our proposed7211ORConvQA system ( consisting of ConvDR and   monoQA ) . This is a crucial baseline to compare   with our monoQA model in order to evaluate the   reranker and reader performances . We reproduce   the MTL of the reranker and reader models and   its evaluation results provided by Qu et al . ( 2020 ) ;   ( c)This baseline uses ConvDR as the passage   retriever similarly to our ORConvQA system ,   monoT5 ( Nogueira et al . , 2020 ) as the passage   reranker , and UnifiedQA ( Khashabi et al . , 2020 )   as the passage reader . It is deployed by using   three models in the pipeline for comparison with   our ORConvQA system , which employs a MTL   of the reranker and reader . This comparison is   done in order to evaluate the performance of using   monoT5 and UnifiedQA separately in comparison   with the joint learning of the reranker and reader .   Two - stage pipelines : ( d)This baseline uses   ConvDR as the passage retriever , and our monoQA   reader as the passage reader without using the   reranking results from the monoQA reranker . The   reader directly identifies an answer in the top   passage from the retriever ; ( e)This baseline uses   ConvDR as the passage retriever and UnifiedQA   as the passage reader . ( f)This baseline uses   TCT - ColBERT ( Lin et al . , 2020a , 2021b ) as the   passage retriever and UnifiedQA as the passage   reader ; ( g)This baseline uses CQE ( Lin et al . ,   2021a ) as the passage retriever and UnifiedQA   as the passage reader . The results of this baseline   come from the previous work of Lin et al . ( 2021a ) .   Hyperparameter settings : Appendix A.1   describes in detail the hyper - parameter settings .   Evaluation metrics : Since we are using the OR-   QuAC dataset , we naturally adopt the two evalua-   tion metrics , namely the word - level F1 , and the hu-   man equivalence score ( HEQ ) . Word - level F1 , com-   monly used in the Machine Comprehension and   ConvQA tasks ( Rajpurkar et al . , 2016 , 2018 ; Choi   et al . , 2018 ) , evaluates the word overlap between   the system ’s prediction and the ground truth an-   swer span . Meanwhile , the HEQ metric is used to   evaluate the percentage of examples for which the   deployed model ’s F1 is equivalent to or higher than   the human F1 . Given nreferences ( ground - truth   answers ) for each question , human F1 is calculated   by averaging the maximum F1 from each n−1sub-   set with respect to the heldout reference ( Choi et al . ,   2018 ) . This metric is composed of HEQ - Q , com-   puted at the question level , and HEQ - D , computed   at the dialogue ( conversation ) level . To evaluate the   retrieval performance , following ( Qu et al . , 2020 ;   Yu et al . , 2021 ) , we use Mean Average Precision   ( MAP@10 ) , Mean Reciprocal Rank ( MRR@5 )   and Recall@5 as metrics for the reranker . For each   query , the top 100 passages are considered . Fi-   nally , we use the McNemar ’s test to test statistical   significance between the various readers ’ perfor-   mances and the paired t - test for testing significant   differences between the rerankers ’ performances .   4 Experimental Results   We now address RQs 1 - 3 ( see Section 3 ) and con-   clude with an efficiency analysis .   4.1 RQ1 : Model Initialisation   In this section , we examine the effectiveness of   the use of the models for initialising monoQA ,   namely monoT5 , UnifiedQA , and t5 - base , on the   test set of OR - QuAC . All models are trained on   the OR - QuAC training set by using positive pas-   sages pand negative passages pas described in   Section 2.3 . In particular , we use " Question An-   swering : { q } [ sep ] { p } " as the prompt function   because it performed the best according to the ex-   periments in Appendix A.2 . Table 1 presents the   results for each evaluated model on the retrieval   and question answering ( QA ) metrics .   From the table , we see that training monoQA   when initialised by monoT5 achieves the highest   performance on the retrieval metrics ( MAP@10 ,   Recall@5 , and MRR@5 ) . However , there are no   significant differences between all of the models ’   retrieval performances . For the QA performance ,   the best word - level F1 , HEQ - Q , and HEQ - D scores   are obtained by the models that use t5 - base ,   monoT5 , and UnifiedQA , respectively . In partic-   ular , in terms of word - level F1 , initialising from   monoT5 or t5 - base significantly outperforms the   model trained from UnifiedQA , but both monoT5   andt5 - base initialisations lead to comparable per-   formances .   Therefore , in response to RQ1 , we find that the   model initialised from monoT5 has the best overall7212   effectiveness , yielding statistically significant im-   provements in word - level F1 over using UnifiedQA   on the test set of the OR - QuAC dataset . In the fol-   lowing , we use monoT5 to initialise monoQA for   answering RQ2 and comparing with the baselines .   4.2 RQ2 : Effectiveness of monoQA   We investigate the performances of our monoQA   model in comparison to the baselines ( a)-(c ) ( de-   scribed in Section 3.2 ) on the test sets of the OR-   QuAC and the OR - CoQA datasets . In Table 2 , the   first row shows the results of ConvDR as the re-   triever only and the last row shows the results of   our monoQA model . Table 2 ( top - half ) also shows   the results of the existing baselines ( a)-(c ) . In the   table , on the OR - CoQA test set , we only include   the question answering ( QA ) results since the gold   passages for each question are not provided .   From the table , on the test sets of the OR-   QuAC and OR - CoQA datasets , we observe that our   monoQA model achieves the highest performance   by significantly outperforming all baselines on all   measures , excepting the baseline using monoT5 as   the reranker and UnifiedQA as the reader in terms   of HEQ - Q on OR - CoQA . From the table , we also   observe that the baselines ( b ) and ( c ) have lower   retrieval performances compared to the results of   using ConvDR as the retriever only . According to   these findings , the reranker of the baselines ( b ) and   ( c ) might have a negative impact on the retrieval   performance of the top retrieved passages . Hence ,   this might also lead to reducing the performances   of the reader of the ( b ) and ( c ) baselines . More-   over , we further analyse why our monoQA model   does not outperform the baseline ( c ) in terms of   HEQ - Q on OR - CoQA . We find that the average   number of tokens in the OR - CoQA ’s answer ( 2.6   tokens per answer ) is remarkably short compared   to that of the OR - QuAC ’s answer ( 14.7 tokens peranswer ) ( Qu et al . , 2021 ) , and the predicted answer   from the baseline ( c ) is shorter than that of our   monoQA model . This prediction may lead to our   monoQA model having a lower HEQ - Q score than   the baseline ( c ) . As described in Section 3.1 , our   proposed monoQA model is fine - tuned on the OR-   QuAC dataset and evaluated on the OR - QuAC and   OR - CoQA datasets . We postulate that this explains   why evaluating the model with OR - CoQA exhibits   a lower performance . Recall that OR - CoQA has no   relevance assessments for retrieval , and hence we   are unable to train a retrieval model for that dataset   ( which has shorter answers than OR - QuAC ) .   In answer to RQ2 , we conclude that our pro-   posed monoQA ’s joint learning of the reranker   and the reader by sharing a single text generation   model , does help to improve the overall perfor-   mance , yielding statistically significant improve-   ments over the baselines on both the OR - QuAC   and OR - CoQA datasets . It is also of note that such   a joint learning can enhance the performances of   the models on the ORConvQA task compared to   using monoT5 and UnifiedQA separately . Later   in Section 4.4 , we also analyse the efficiency of   our monoQA model compared with the individual   application of monoT5 and UnifiedQA .   The performances of baselines ( b ) and ( c ) on   OR - QuAC raise the question as to how do the base-   lines ( b ) and ( c ) compare to our monoQA model   when using the ground truth passages provided   in the OR - QuAC test set instead of using the re-   trieved passages . We provide such an analysis   in Appendix A.4 , which shows that our monoQA   reader achieves the best performance and signifi-   cantly outperforms the reader of the baselines ( b )   and ( c ) on all measures.72134.3 RQ3 : Effectiveness of using a Reranker   Finally , we examine the effectiveness of our pro-   posed system , which is a three - stage pipeline using   a jointly trained cross - encoder for the reranker and   reader , compared to the two - stage pipeline base-   lines ( d)-(g ) , which each uses a bi - encoder for re-   trieval as input into a reader ( see details in Sec-   tion 3.2 ) . This allows to establish the impact of the   reranker . Table 2 ( bottom - half ) presents the results   of the baselines ( d)-(g ) . From the table , we observe   that our system , which is a three - stage pipeline us-   ing ConvDR as the retriever and monoQA as both   the reranker and reader , achieves the highest perfor-   mance by significantly outperforming all two - stage   baselines on all measures – e.g. see row ( d ) vs. the   last row in Table 2 . In answer to RQ3 , we con-   clude that integrating the reranker in the pipeline   does help to improve both the retrieval and QA   performances , yielding statistically significant im-   provements over the two - stage pipeline baselines .   4.4 Efficiency of monoQA   In this section , we measure the efficiency of infer-   ence of monoQA , which jointly learns the reranker   and reader , in comparison with using monoT5 as   the reranker and UnifiedQA as the reader sepa-   rately on the test set of the OR - QuAC dataset . We   find that the average prediction time of monoQA is   23ms , whereas the average prediction time of using   monoT5 as the reranker and UnifiedQA as the   reader separately is 44ms . This is because monoQA   uses a single model application for addressing both   the reranker and reader stages . Indeed , we conclude   that , on the test set of OR - QuAC , our monoQA   model is approximately twice as fast in infer-   ence as the individual application of monoT5 and   UnifiedQA for reranking and extracting the answer .   5 Related Work   In the following , we discuss related work and posi-   tion our contribution in relation to Conversational   Question Answering and Multi - Task Learning .   Conversational Question Answering : This is a   conversational search task , where the system needs   to correctly interpret a question in the context of   an ongoing conversation . Most research on con-   versational QA focuses on conversational response   ranking tasks ( Dalton et al . , 2019 , 2020 , 2021 ) and   extractive QA tasks ( Choi et al . , 2018 ; Reddy et al . ,   2019 ; Qu et al . , 2019b ; Yeh and Chen , 2019 ) . Qu   et al . ( 2020 ) were first to define the task of Open - Retrieval Conversational Question Answering ( OR-   ConvQA ) , where the system is required to learn to   retrieve top relevant passages from a large collec-   tion before extracting answers from the passages .   To address the ORConvQA task , Qu et al . ( 2020 )   proposed a three - stage pipeline : ( 1 ) a retriever , ( 2 )   a reranker , and ( 3 ) a reader . Later , Qu et al . ( 2021 )   introduced a learned weakly - supervised training   approach to address the problem of accessing gold   passages during the training of the model on the   OR - CoQA dataset in ( Qu et al . , 2020 ) . One partic-   ular problem is coarse ranking , where the reranker   of Qu et al . ( 2020 ) only takes the top - ranked   passages as input . This makes the whole pipeline   suffers from this coarse ranking , especially for   situations when the golden passages are not   retrieved in the top - ranked passages . Liang et al .   ( 2022 ) addressed this issue by adding a post - ranker   module that can push more relevant passages to the   reader . However , these works typically approach   reranking and reading as classification tasks to find   an answer span for retrieved passages ( start / end   positions ) . Instead , we use a text generation model   with multi - task learning for both the reranker and   reader in order to directly extract the answers   for the users instead of predicting the start / end   positions in a retrieved passage .   Multi - Task Learning ( MTL ) in Conversational   QA : MTL methods have recently been effectively   implemented in existing Conversational QA   works ( Kongyoung et al . , 2020 ; Qu et al . , 2019b ;   Xu et al . , 2019 ; Yeh and Chen , 2019 ; Qu et al . ,   2020 , 2021 ) . However , all of the tasks in these   works leverage MTL by sharing the network   structure between an extractive reader and its   auxiliary tasks , which are typically classification   tasks , such as a yes / no question prediction or   a follow - up question prediction . For example ,   some existing works ( Qu et al . , 2020 , 2021 ) have   applied MTL on reranking and answer reading   by adding two fully - connected layers ( one for   the reranker and reader , respectively ) to find an   answer span for the retrieved passages ( start / end   positions ) . Instead , in this paper , we leverage   MTL on reranking and answer extraction by   sharing a single text generation model in order to   directly extract the answers instead of predicting   the start / end positions in a retrieved passage . On   the other hand , Ide and Kawahara ( 2021 ) recently   adopted an MTL approach that involves both   classification and text generation tasks . However,7214they addressed a completely different use case ,   consisting in detecting a user ’s emotion - aware   response , rather than conversational QA .   6 Conclusions   We proposed the Multi - Task Learning ( MTL ) of   passage reranking and answer extraction to share   a single text generation model , so as to improve   effectiveness on the Open - Retrieval Conversational   Question Answering ( ORConvQA ) task . Our ex-   periments on two datasets , namely the OR - QuAC   and OR - CoQA datasets , showed that our proposed   monoQA model has the best effectiveness on these   datasets , yielding statistically significant improve-   ments over several strong baselines from the litera-   ture , e.g. the ORConvQA system proposed by Qu   et al . ( 2020 ) and the individual application of the   monoT5 and UnifiedQA models . Our resulting sys-   tem improves the best baseline by up to 12 % MAP   and 23 % word - level F1 on the OR - QuAC and OR-   CoQA datasets . In addition , we demonstrated that   including the reranker in the pipeline helps to en-   hance the retrieval and QA performances , yielding   statistically significant improvements over the state-   of - the - art two - stage pipeline baselines . Further-   more , we showed that our MTL - based monoQA   model improves the efficiency of inference com-   pared to the individual application of the monoT5   and UnifiedQA models for separately reranking   and extracting the answer to the user ’s question .   Limitations and Future Work   Our work shows the effectiveness of the multi - task   learning of the reranker and a generative reader by   sharing a single text generation model on the OR-   ConvQA task . Indeed , while our generative model   is trained to generate only word sequences appear-   ing in the input passage , we observe that 1.5 % of   the generated tokens are not extracted from the in-   put . While this may not affect user satisfaction , the   extractive evaluation measures may underestimate   the model ’s utility . For this reason , it is also worth   investigating the multi - task learning of the reranker   and an extractive reader by sharing a single model .   Another limitation of our work is that the input of   monoQA is a rewritten question from another T5   model ( see Section 3.1 ) . Ideally , we would like a   single model to be able to use the original question   ( without needing it to be first rewritten ) . We leave   both these investigations to future work . References7215   A Appendices   Our code and data are publicly available   at the following URL : https://github.com/   terrierteam / monoQA .7216A.1 Hyperparameter Settings   For ConvDR , we reproduce the model and its   evaluation results provided by Yu et al . ( 2021 ) to   generate the offline passage embeddings from the   passage collection of the OR - QuAC dataset as   shown in Figure 3 . We implement the monoQA   model using the following PyTorch models   from HuggingFace ( Wolf et al . , 2020 ) , namely   t5 - base , castorini / monot5 - base - msmarco ,   and allenai / unifiedqa - t5 - base . Follow-   ing Qu et al . ( 2020 ) , these models are configured   as follows : the maximum sequence length is set to   512 , the number of training epochs is set to 10 , the   batch size is set to 16 , and the learning rate is set to   5e . The models are trained on a NVIDIA RTX   A6000 . The average training time of monoQA is   6.3 hours . The number of parameters in monoQA   is approximately 222 million parameters , i.e. the   same as monoT5 and other fine - tuned versions   oft5 - base . We save the checkpoints every   epoch and evaluate on the development set of the   OR - QuAC dataset . We provide the details of how   to select the best checkpoint in Appendix A.3 .   A.2 Prompt Learning   Recently , Prompt Learning , which is a method   to modify pre - trained language models to down-   stream tasks by using a task - specific prompt to-   gether with the input , has increasingly become a   popular approach for tackling several tasks in a   uniform model ( Liu et al . , 2021 ) . To fine - tune the   monoQA model for passage reranking and answer   extraction , we adopt Prompt Learning to modify   the model input . We have observed that several   prompts have previously been used in previous   work ( Nogueira et al . , 2020 ; Khashabi et al . , 2020 ) .   Below we list the templates f ( ) that we con-   sider in this study :   •monoT5 prompt : We adapt the monoT5 ’s tem-   plate by replacing the prefix word from “ Query : " to   “ Question : " , the separator token from “ Document : "   to “ Passage : " , without using the word “ Relevant : " :   “ Question : { q}Passage : { p } ” ( 6 )   •UnifiedQA prompt : UnifiedQA ( Khashabi et al . ,   2020 ) made use of a ‘ \n ’ between the current   question and the passage :   “ { q}\n{p } ” ( 7 )   However , under the standard T5 tokenizer , a   whitespace such as ‘ \n ’ does not result in a separate   token , so the end result of this formulation is a   simple concatenation of the question and passage :   “ { q } { p } ” ( 8)   •Our prompt : For comparison with the above   templates from the literature , we design a new tem-   plate using " Question Answering : " as a prefix and   a T5 - provided special tokens ( [ sep ] ) as a separator   token between the question and the passage . ( 9 )   Table 3 shows the results of each evaluated   model for each of the above prompts . From the   table , we see that the monoQA model trained by   using our designed prompt ( Question Answering :   { q } [ sep ] { p } ) has the highest performance on all   measures , except when it uses the monoT5 prompt   ( Question : { q } Passage : { p } ) for word - level F1 . To   conclude , we find that the model learned with our   designed prompt has the best overall effectiveness .   As a consequence , we use " Question Answering :   { q } [ sep ] { p } " as the prompt for training monoQA .   A.3 Selecting the Best Model   We further investigate how to identify the optimal   training checkpoint for our proposed monoQA   model on the OR - QuAC development set . The   model is trained on the OR - QuAC training set   by using the positive pand negative passages   pdescribed in Section 2.3 . In particular , we   use " Question Answering : { q } [ sep ] { p } " as the   prompt function and monoT5 to initialise monoQA .   In this appendix , we consider the performance   of each model checkpoint on both reranking and   answer extraction.7217   We identify the best checkpoint of the model for   each measure , namely validation loss , validation   relevance accuracy , and word - level F1 as discussed   in Section 2.3 . Figure 4 shows the best epochs of   the model in terms of validation loss , relevance   accuracy , and word - level F1 scores , which are 4 , 6 ,   and 9 , respectively . We then evaluate the models   obtained at these epochs ( 4 , 6 , and 9 ) on the OR-   QuAC test set , as depicted in Figure 5 . Figure 5   shows that the model checkpoint at epoch 9 has the   best performance in terms of MAP@10 , Recall@5 ,   MRR@5 , word - level F1 , and HEQ - Q , whereas in   HEQ - D the epoch 6 is the best . Indeed , the model   that exhibits the highest word - level F1 on the val-   idation set is also the best model when evaluated   on the test set in terms of MAP@10 , Recall@5 ,   MRR@5 , word - level F1 , and HEQ - Q.   A.4 Effect of Providing Ground Truth   Passages   In this section , we experiment to answer the ques-   tion concerning how do baselines ( b ) and ( c ) ( listed   in Section 3.2 ) compare to our monoQA modelwhen using the ground truth passages provided in   the OR - QuAC test set instead of using the retrieved   passages . In particular , recall that baseline ( b ) is the   MTL of the reranker and reader by sharing a BERT   encoder , while ( c ) is the individual application of   monoT5 and UnifiedQA . By doing this compari-   son , we can control the impact of the reranker , and   consider only the effectiveness of the reader . In-   deed , in this setting , all models predict the answer   by using the question and the ground truth passage .   Table 4 shows the results of each evaluated model   on the test set of OR - QuAC .   On analysing Table 4 , we observe that our pro-   posed monoQA model achieves the best perfor-   mance across all measures and significantly outper-   forms the baselines ( b ) and ( c ) in terms of word-   level F1 scores according to the McNemar ’s test ( p   < 0.05 ) . Indeed our monoQA model ’s joint learn-   ing of the reader and the reranker can indeed help   improve the performance of the answer extraction .   A.5 Reproducibility Criteria   Table 5 summarises our answers to the EMNLP   reproducibility criteria questions.7218