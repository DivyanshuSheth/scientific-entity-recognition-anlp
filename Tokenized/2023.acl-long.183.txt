  Pengshan Cai , Kaiqiang Song , Sangwoo Cho , Hongwei Wang ,   Xiaoyang Wang , Hong Yu , Fei Liu , Dong YuUniversity of Massachusetts , AmherstTencent AI Lab , Bellevue , WAUniversity of Massachusetts , LowellEmory University   { pengshancai,hongyu}@cs.umass.edu fei.liu@emory.edu   { riversong,swcho,hongweiw,shawnxywang,dyu}@global.tencent.com   Abstract   The potential choices for news article headlines   are enormous , and finding the right balance be-   tween conveying the essential message and cap-   turing the reader ’s attention is key to effective   headlining . However , presenting the same news   headline to all readers is a suboptimal strategy ,   because it does not take into account the differ-   ent preferences and interests of diverse readers ,   who may be confused about why a particular   article has been recommended to them and do   not see a clear connection between their inter-   ests and the recommended article . In this paper ,   we present a novel framework that addresses   these challenges by incorporating user profil-   ing to generate personalized headlines , and a   combination of automated and human evalua-   tion methods to determine user preference for   personalized headlines . Our framework utilizes   a learnable relevance function to assign person-   alized signature phrases to users based on their   reading histories , which are then used to person-   alize headline generation . Through extensive   evaluation , we demonstrate the effectiveness of   our proposed framework in generating person-   alized headlines that meet the needs of a diverse   audience . Our framework has the potential to   improve the efficacy of news recommendations   and facilitate creation of personalized content .   1 Introduction   Personalized news recommendation systems , such   as Google News and Yahoo News , help users dis-   cover articles that align with their interests ( Karimi   et al . , 2018 ) . However , these systems often present   the same article headline to all users , making it diffi-   cult for them to understand the connection between   their interests and the recommended article , poten-   tially reducing the effectiveness of the recommen-   dation system . To address this , we propose a new   framework for generating personalized , engagingheadlines that clearly show the connection between   a user ’s reading history and a recommended article .   Our framework has the potential to improve the ef-   ficacy of personalized news recommendations , and   recommendations for short videos , articles , recipes ,   etc.(Majumder et al . , 2019 ; Kanouchi et al . , 2020 ;   Gosangi et al . , 2021 )   Generating personalized headlines is a challeng-   ing task due to the constraints of conciseness and   the need to capture the reader ’s attention . A person-   alized headline should ( a ) effectively convey the   main message of the article and ( b ) provide a clear   link to the user ’s reading history , using only about   10 words on average ( Bernstein et al . , 2020 ) . There   are two main challenges in this task . First , a head-   line that entices users to click , but only presents   limited information and fails to convey the essential   story , becomes clickbait rather than a useful head-   line ( Bourgonje et al . , 2017 ; Potthast et al . , 2018 ) .   Second , it is difficult to find large scale annotated   datasets containing news articles , multiple person-   alized headlines , and associated user profiles . Such   a dataset would be useful in developing personal-   ized headlines , but it is currently unattainable .   The key to effective personalization is to develop   acomprehensive framework that enables us to ( a )   understand users ’ interests based on their reading   histories , ( b ) produce personalized headlines , and   ( c ) evaluate the effectiveness of these headlines   in terms of user preference . Previous studies on   headline generation have primarily focused on pro-   ducing headlines that accurately summarize a given   news article or its first sentence ( Song et al . , 2018 ;   Xu et al . , 2019 ; Matsumaru et al . , 2020 ; Song et al . ,   2021 ; Kanungo et al . , 2021 ) , but have not consid-   ered the potential benefits of personalization . In   this study , we propose a pipeline that incorporates   user profilingand a comprehensive synthesis of3265   automated and human evaluation methods for user   preference to produce personalized headlines that   cater to a varied audience .   Our approach focuses on learning a relevance   function that condenses a user ’s reading history   into a collection of signature phrases . This method   for user profiling is both efficient and adaptable ,   as the signature phrases can be easily updated as   the user ’s interests evolve ( Bansal et al . , 2015 ) .   These signature phrases are derived from news ar-   ticle based on the user ’s reading history through   contrastive learning without the need for annotated   data . For example , if the phrase Upper East Side   frequently appears in the user ’s reading history , it   could become a signature phrase for that user ( Fig-   ure 1 ) . These signature phrases do not need to   appear verbatim in the user ’s reading history and   can indicate broader interests , e.g. , if the phrases   Avengers andHulk appear in the user ’s reading his-   tory , it could indicate a love for Marvel movies and   Marvel Studios could be a signature phrase that   reflects this interest . We build a synthetic dataset   that trains the model to generate personalized head-   lines for a news article . Using signature phrases ,   our model is able to create a connection between   the recommended article and the user ’s interests ,   resulting in personalized headlines that are both   engaging and anchored to the article to avoid click-   bait .   Evaluating personalized news headlines presents   unique challenges ( Gligori ´ c et al . , 2021 ) . It would   be ideal to have human evaluators judge the effec-   tiveness of system headlines . Indeed , we have con-   ducted a human evaluation in this study . However ,   this process is time - consuming and costly , making   it impractical during the system development phase .   Thus , we propose a comprehensive synthesis of au-   tomated and human evaluation methods to assess   headline relevance and user preference . By using   signature phrases , we can synthesize user profilesof various types . We hypothesize that personal-   ized headlines generated for these user profiles will   be preferred by the same users over generic , non-   personalized headlines according to recommender-   driven metrics ( Karpukhin et al . , 2020 ; Wu et al . ,   2021a ) . We also experiment with a variety of auto-   matic metrics to assess headline quality in terms of   informativeness , relevance to the source article , and   content accuracy ( Kryscinski et al . , 2020 ; Fabbri   et al . , 2021 ) .   In this paper , we make the following contributions :   •we present a comprehensive framework for gener-   ating personalized news headlines that convey the   essential message of the article and capture the   reader ’s attention while also aligning with their   interests . Our framework utilizes a learnable rel-   evance function to derive signature phrases from   users ’ reading histories and uses them to person-   alize the headlines ;   •we thoroughly synthesize automated and human   evaluation methods to assess the effectiveness of   headlines in terms of their accuracy and user pref-   erence . We further compare our proposed frame-   work with strong headline generation baselines ,   present results on benchmark news datasets , and   identify promising directions for future research   through an in - depth analysis of system outputs .   2 Related Work   Automatic headline generation has made signif-   icant progress in recent years ( Matsumaru et al . ,   2020 ; Horvitz et al . , 2020 ; Laban et al . , 2021 ; Song   et al . , 2020 ; Goyal et al . , 2022 ) , thanks in part to   the development of large language models ( Lewis   et al . , 2020 ; Raffel et al . , 2020 ; Zhang et al . , 2020a ;   Brown et al . , 2020 ; Chowdhery et al . , 2022 ) and   the availability of benchmark news datasets such   as Gigaword , XSum , and Newsroom ( Rush et al . ,   2015 ; Narayan et al . , 2018 ; Grusky et al . , 2018).3266These datasets include a single headline for each   news article , serving as the groundtruth for the   models . In contrast to previous works , we aim to   personalize headline generation to improve content   recommendations , where a personalized headline   should convey the main points of the article and   capture the user ’s attention .   Personalization is a highly sought - after technique ,   and researchers have explored its use for tasks such   as headline generation , dialog response generation   and recipe creation ( Ao et al . , 2021 ; Majumder   et al . , 2019 ; Flek , 2020 ; Wu et al . , 2021b ; Dudy   et al . , 2021 ) . We anticipate that this technique to   continue to have a significant impact . For example ,   when a recommender system distributes news arti-   cles or short videos , personalizing the headline can   help users find a clear connection between their in-   terests and the recommended article / video ( Karimi   et al . , 2018 ; Bernstein et al . , 2020 ) , thus improving   their experience .   Evaluating personalized content is a largely   under - explored area , partly due to the lack of   ground truth for personalized content generation   ( Gligori ´ c et al . , 2021 ) . Without ground truth , it is   challenging to apply commonly used text genera-   tion evaluation metrics such as ROUGE , BLEU ,   BERTScore , MoverScore , BLEURT , etc . ( Lin ,   2004 ; Post , 2018 ; Zhang et al . , 2020b ; Zhao et al . ,   2019 ; Sellam et al . , 2020 ) . To leverage recent ad-   vances in data synthesis ( Pasunuru et al . , 2021 ;   Amplayo and Lapata , 2020 ; Magooda and Litman ,   2021 ) , we propose synthesizing user profiles of   various types . We then evaluate system headlines   against these profiles along multiple dimensions ,   including their alignment with user interests , rele-   vance to the source article , and content accuracy . In   the following , we provide details of our approach .   3 Our Approach   Our goal is to generate a user - engaging headline   that conveys the main idea of a given news article d   for a specific user u. To achieve this , we have devel-   oped a three - step framework : ( 1 ) Signature phrases   identification . Using a key - phrase generation mod-   ule , we identify a set of candidate signature phrases   Z={z , z , . . .}that cover various aspects of d   ( Section 3.1 ) ; ( 2 ) User signature phrases selection .   From the set of candidate signature phrases , we   select a subset Z⊆Zthat relates to user u ’s   interests as the user signature phrases ( Section 3.2 ) ;   ( 3)Signature - oriented headline generation . Based   on the news article dand the selected user signaturephrases Z , we generate a headline that introduces   the content of the article dfrom the perspective of   the user u ’s personalized interests ( Section 3.3 ) .   3.1 Signature Phrases Identification   We approach this task as a conditional text gen-   eration problem , in which the model takes a news   article or headline as input and outputs all candidate   signature phrases in the input sequence , separated   by semicolons . We use a BART model that has   been pretrained on the KPTimes dataset . KPTi-   mes ( Gallina et al . , 2019 ) is a large - scale dataset   containing 279 K news articles paired with editor-   curated signature phrases . Unlike other datasets   for signature phrase identification ( Meng et al . ,   2017 ; Krapivin et al . , 2009 ) that focus on scien-   tific research papers , KPTimes focuses on extract-   ing signature phrases in news articles , making it   well - suited for our task . The model is trained by   minimizing the cross - entropy loss between the pre-   dicted signature phrase sequences and the human-   curated signature phrase sequences .   3.2 User Signature Selection   In this step , we rank all candidate signature phrases   inZbased on their level of engagement with user   u ’s reading history H , and select the top kcandi-   date signature phrases as the user signature phrases .   Suppose that the user ’s history Hcan be defined   as a set of headlines of articles that the user has   previously read , i.e. , H={t , t , . . . } . We first   convert each signature phrase z∈Zinto a dense   vector zusing a signature phrase encoder . To cal-   culate the user - engaging scores for each candidate   signature phrase z , we consider two different en-   coding strategies for the user ’s history :   ( 1)Holistic history encoding . We concatenate all   headlines in the user ’s reading history Hwith ad-   ditional semicolons for headline separation . Then   we encode the concatenated headlines into a dense   vector husing a holistic history encoder . The   engaging score S(z , H)of a signature phrase   z∈Zfor user uis obtained by the dot prod-   uct of the two vectors :   S(z , H ) = zh . ( 1 )   ( 2)Individual history encoding . Each individual   headline t∈His encoded as a dense vector   tusing an individual headline encoder . The user-   engaging score is then defined as the maximum dot-   product relevance between the signature phrase z3267and each individual headline in the reading history :   S(z , H ) = maxzt . ( 2 )   In practice , we train the user signature phrase se-   lection model using an in - batch contrastive learn-   ing approach ( Radford et al . , 2021 ) . We consider   a batch of synthesized users { u , u , · · · , u }   where Nis the batch size , and each user uhas   exactly one user signature phrase z. The reading   history Hfor user uis then constructed by ran-   domly sampling news articles whose candidate sig-   nature phrases contain z , i.e. ,H={d|z∈Z } .   In this way , ( z , H)is considered as a positive pair ,   and(z , H)(i̸=j ) is considered as a negative   pair . The contrastive loss for this batch is defined   as follows :   L = 1   2 / parenleftigg / summationdisplaylogS(z , H)/summationtextS(z , H)+ ( 3)/summationdisplaylogS(z , H)/summationtextS(z , H)/parenrightigg   ( 4 )   3.3 Signature - Oriented Headline Generation   We model the user - specific headline generation pro-   cess as a conditional generation task . Given a news   article dand a user u , along with the user signa-   ture phrases Z⊆Z , our goal is to generate a   headline t= [ w , w , . . .]ford , where wis the   i - th token in t. The loss for this generation step   is calculated as the negative log - likelihood of the   conditional language generation :   L=−/summationdisplaylogPr ( w|w,···,w;Z , d)(5 )   Specifically , the input to the generator is the con-   catenation of the user signature phrases Zand   news article d , and the output is the signature - based   headline t. During the training stage , Zis identi-   fied from t , the ground - truth headline of d. During   the inference stage , Zis identified from ditself   and selected by user signature selection models ,   since the headline tis not available before gen-   eration . We use BART here as the generator for   headline generation .   4 Corpora Processing   In this section , we describe the corpora processing   step , including the creation of synthesized users   and the generation of signature phrase based head-   lines . Our data is sourced from two existing news   corpora : Newsroom ( Grusky et al . , 2018 ) and Gi-   gaword ( Rush et al . , 2015 ; Graff et al . , 2003 ) .   The Newsroom corpus contains 995,041 article-   headline pairs in its training set , 108,837 in its vali-   dation set , and 108,862 in its test set . The Gigaword   corpus contains 7,704,419 instances in its training   set , 394,390 in its validation set , and 381,045 in its   test set . For each corpus , we construct two datasets :   a synthesized user dataset and a headline genera-   tion dataset . The first dataset is used for training the   use signature phrase selection model ( Section 3.2 )   and evaluating the entire system , while the second   dataset is used for training the signature - oriented   headline generation model ( Section 3.3 ) . Further   data statistics can be found in Table 1 .   Synthesized User Creation . As real user data   is not available , we generate synthesized users to   mimic real users ’ reading histories . The process for   creating synthesized users is illustrated in Figure   2 and consists of the following steps : ( 1 ) Identi-   fication of signature phrases in all news articles   of a corpus to build a candidate phrase pool ; ( 2 )   Mapping of each signature phrase to a series of   news articles that contain that phrase ; ( 3 ) Random   sampling of a subset of phrases from the candi-   date phrase pool as each synthesized user ’s area of   interest ; ( 4 ) Random sampling of a set of news arti-   cles that contain each user ’s chosen interest phrase   using the phrase - article map established in step 2 .   During the training stage of the signature phrase   selector , each synthesized user is assigned only one3268   interest phrase to enable contrastive learning ( Eq .   4 ) . However , when evaluating the model , each syn-   thesized user is assigned 1∼5interest phrases   to mimic real - world scenarios . It is important to   note that it is easier to generate personalized head-   lines for users with simpler backgrounds ( e.g. users   whose reading histories only relate to one or two   topics ) . To study the effect of the number of users ’   interested phrases on the generated headlines , we   create 2,000 synthesized users with 1∼5number   of interested phrases respectively .   In general , headline personalizing is only effec-   tive when the source article content aligns with the   user ’s interests . To ensure relevancy , we randomly   select one of the user signature phrases from each   synthesized user , and then randomly choose one   news article that contains the selected phrase as   the input for the test case . This ensures that the   news article whose headline needs to be generated   is relevant to the user . The evaluation details are   further explained in Section 5 .   Headline Generation . In order to generate signa-   ture phrase oriented headlines , we use the signature   phrases identification model to extract signature   phrases from the original headlines . These gener-   ated phrases , along with the corresponding news   article contents , are then fed into the headline gen-   eration model to generate the original headlines . In   our experiments , we truncate all news articles to a   maximum of 512 tokens and only keep signature   phrases that appear in more than 10 news articles .   On average , around 10 candidate signature phrases   are identified in each news article , providing a di-   verse range of perspectives for headline generation.5 Experiments   We thoroughly evaluate our proposed system from   different perspectives , including objective eval-   uation ( Section 5.2 ) , subjective evaluation ( Sec-   tion 5.3 ) and ablation studies ( Section 5.4 ) , for   personalized headline generation .   5.1 Baseline Methods   We compare the performance of our system with   the following baseline approaches : ( 1 ) PENS-   EBNR and ( 2 ) PENS - NRMS ( Ao et al . , 2021 )   are LSTM - based personalized headline generation   models . Both were trained on the PENS dataset ,   but using different reading history encoding mod-   els ; ( 3 ) Vanilla System is a BART - large model   fine - tuned directly on headline generation datasets   without using signature phrases ; ( 4 ) Vanilla Hu-   man refers to original headline given by the au-   thor of the news article ; ( 5 ) SP - headline uses sig-   nature phrases identified in the original human-   written headline to guide headline generation ; ( 6 )   SP - random randomly selects signature phrases in   the news article to guide headline generation . ( 7 )   SP - holistic and ( 8) SP - individual were introduced   in previous sections .   5.2 Objective Evaluation   We use various metrics to evaluate the entire per-   sonalized headline generation pipeline :   ( 1)Relevance Metrics . We use pre - trained DPR   ( Karpukhin et al . , 2020 ) and Sentence - BERT   ( Reimers and Gurevych , 2019 ) models to calcu-   late the relevance score between texts . Specifically ,   we report dot - product similarity when using DPR ,   and cosine similarity when using Sentence - BERT .   These relevance metrics are calculated for both the   headline - user relevance and the headline - article rel-   evance . For headline - user relevance , the score is   calculated between the generated headline and the   user signatures . For headline - article relevance , the   score is calculated between the generated headline   and the entire news article .   ( 2)Recommendation Score . Following ( Wu et al . ,   2021a ) , we train a news recommendation system   using the MIND dataset ( Wu et al . , 2020 ) . The sys-   tem takes in a user ’s reading history and a headline   of a news article , and outputs a score indicating the   degree to which the system would recommend the   news to the user .   ( 3)Factual Consistency . We apply the pre - trained   FactCC model ( Kryscinski et al . , 2020 ) to obtain   the factual consistency score between the generated3269   headline and the news article . We report the per-   centage of generated headlines that are predicted   to be factually consistent with the news article by   the FactCC model .   ( 4)Surface Overlap . We use ROUGE - L F1 and   Extractive Coverage to evaluate the surface overlap   between the generated headline and the reference   headline / news article . ROUGE ( Lin , 2004 ) scores   are widely used to evaluate the surface level cov-   erage of generated summaries against golden stan-   dards . Specifically , ROUGE - L F1 measures the   longest common sub - sequence between the gen-   erated output and reference . Extractive Coverage   ( Grusky et al . , 2018 ) is the percentage of words   in the generated headline that are from the source   news article , measuring the extent to which the   summary is derived from the text .   Table 2 presents objective evaluation results for   generated headlines . We elaborate our observations   from the following perspectives :   User Adaptation . ( 1 ) The methods SP holistic and   SP individual generally show better performance ,   indicating that our signature phrase based head-   line generation framework is able to generate more   user - oriented headlines . In contrast , while Vanilla   System andSP Headline achieve higher Rouge - L   scores , they have lower scores in user adaptation ,   suggesting that they have higher similarity with the   original headline but do not achieve personalization .   ( 2 ) Comparing SP based methods , we observe that   using selectors fine - tuned on our signature selec - tion datasets ( i.e. -F ) leads to more user - preferred   headlines than their naive counterparts ( i.e. -N ) .   This reflects the improvement of fine - tuning sig-   nature phrase selector . It is worth noting that the   performance of SP Random is significantly lower   than SP holistic / individual , and almost similar to   Vanilla System , which suggests that user adaptation   is only achieved when signature phrases of users ’   interests are well - selected . ( 3 ) SP individual shows   better performance than SP holistic , indicating that   individual encoding better aligns users ’ reading   history with their interests .   Article Loyalty . ( 1 ) While Vanilla System gener-   ally achieves better performance in headline - article   relevance , SP individual - F generates more head-   lines that are identified as factually consistent by   FactCC . Our analysis found that headlines gener-   ated by our SP - based methods are usually anchored   to news articles by the signature phrase , i.e. the gen-   erated headlines may contain content in the context   of the signature phrase ( as shown in the example   in Figure 2 ) . This keeps the generated headlines re-   lated and factually consistent with the news article ,   thus avoiding click - bait headlines . ( 2 ) The extrac-   tive converge of the original human headlines is   lower than all machine - generated headlines , which   implies that human written headlines are more ab-   stractive . This explains the original headlines ’ low   performance in article loyalty metrics . Note that   ROUGE scores do measure our goal of headline   personalization , we present the results only to show3270   the generated headlines ’ surface - level resemblance   to the human written ones .   5.3 Subjective Evaluation   We conduct a two - step human evaluation using   16 evaluators who have high English proficiency .   In the first step , we collected 2,260 news head-   lines from 113 common topics in Newsroom and   Gigaword corpus . We presented the volunteers   with the article headlines and corresponding topics   and asked them to select around 20 headlines of   their interests mimicking their interest phrases and   reading histories . In the second step , we gener-   ated headlines for 12 randomly selected news arti-   cles containing the volunteers ’ interested phrases   ( 6 from Newsroom and 6 from Gigaword ) . We   then asked the volunteers to evaluate the generated   headlines through the following five approaches :   ( 1)Vanilla Human ; ( 2 ) Vanilla System ; ( 3 ) SP-   random ; ( 4)SP - individual - N ; ( 5)SP - individual - F .   We evaluated the headlines from three perspectives :   ( 1)User adaptation ; ( 2)Headline appropriateness   and ( 3 ) Text quality . The grading scale ranges from   1 ( worst ) to 3 ( best ) , and detailed grading standards   are provided in Appendix A.3 .   According to Figure 3 , our signature - oriented   headline generation approaches , SP - Individual - F   andSP - Individual - N , perform better than other   baseline methods in terms of user adaptation .   This is in line with the objective results that our   signature - oriented framework generates headlines   that cater more to users ’ interests .   Further , the headlines generated by Vanilla Sys-   temobtain the highest scores in headline appropri-   ateness . However , after analyzing the generated   headlines , we realized that some identified signa-   ture phrases did not correlate well with the article ’s   main point , thus diverging from the article . For   example , in the third example in Table 3 , the gen-   erated headline focuses on Shanghai Index ’s drop ,   which is only a minor evidence to support the arti-   cle ’s main point , i.e. China ’s stock market crush ,   and is therefore not appropriate to be included in   the headline .   Moreover , the Vanilla Human did not receive the   highest scores . We found some of the human writ-   ten headlines are overly rhetorical and not easily   understandable to ordinary readers ( see the fourth   example in Table 3 ) . All NLP models achieve good   performance ( around 1.8 points ) in text quality ,   which is similar to the scores of the human - written   headlines .   5.4 Ablation Study   Selectors Evaluation . To evaluate the perfor-   mance of signature selection , we rank all candidate   signature phrases within an article for a synthesized   user and report the following metrics : ( 1 ) Hit@K ,   which is the percentage of times that the correct   signature phrase is ranked among the top K ; ( 2 )   Mean rank , which is the average rank of the correct   signature phrase . We use our synthesized user eval-   uation dataset to evaluate both headline generation   and signature selection.3271   As shown in Table 4 , Individual - F demonstrates   the best performance among all selectors . This ex-   plains the high user adaptation scores of headlines   generated by SP individual - F . We have observed   that the selector does not always choose the gold   user signature phrases , yet the generated headline   still relates to user ’s interests . For example , in the   second example of Table 3 , even though the user ’s   interested phrase Star War was not chosen as the   user signature , the generated headline is still rele-   vant to Star War , as the selected signature phrase   The Force Awakens is the subheading of a movie in   theStar War movie series .   Factors Affecting Headline Generation . Through   our experiments , we have identified that the fol-   lowing factors affect the quality of the generated   headlines : ( 1 ) Number of topics that the user is   interested in . As shown in Table 5 , the evaluation   results of headlines generated from newsroom arti-   cles for synthesized users with varying number of   interest phrases indicates that , as the number of in - terest phrases increases , the user adaptation scores   decreases , while other scores remain roughly the   same . This suggests that it is easier to generate   personalized headlines for users who read news   related to fewer interest phrases . However , even   when the number of interest topics increases to   30 , our proposed method still achieves better user   adaptation scores then the vanilla systems , while   showing similar performance in article loyalty met-   ric . ( 2 ) Number of user signature phrases . Our   analysis of generated headlines revealed that when   the signature - oriented headline generator takes mul-   tiple user signature phrases as input , the generated   headline may contain factual errors . This is be-   cause the generator is compelled to incorporate   irrelevant signature phrases into a coherent head-   line , as seen in the first example in Table 3 ) . As   a result , we only use a single signature phrase to   guide headline generation .   Applying GPT-3 for Personalized Headline Gen-   eration . Recently , GPT-3 ( Brown et al . , 2020 ) has   been found to be effective in zero - shot prompting   automatic summarization ( Goyal et al . , 2022 ) . In   this section , we investigate whether prompts can   inspire GPT-3to generate personalized headlines   of good quality . To achieve this goal , we conduct   experiment with 100 random samples from our   newsroom test set using two paradigms , as shown   in Table 7 , and present the results in Table 6 .   OurSP individual - F method outperforms GPT-3   based methods in terms of user adaptation metrics   and ROUGE - L score . This suggests that despite   GPT-3 ’s strong ability in zero - shot setting , it is still3272incomparable to models that are specifically trained   for our headline generation task . Specifically , the   topic oriented method shows better performance   in user adaptation metrics than the history oriented   method , which implies that our topic selector effec-   tively reveals users ’ interests .   6 Conclusion   We investigate the generation of personalized head-   lines tailored to various users ’ interests . We pro-   pose a topic - focused generation framework and   methods for creating synthesized data to support   the training of our framework without the need for   human - annotated datasets . Additionally , we ex-   plore evaluation methods that enable the automatic   evaluation of the generated headlines from multi-   ple perspectives . Our experiments demonstrate the   effectiveness of our proposed approaches .   7 Limitations   Personalized news headline generation has the po-   tential to improve the way users consume and un-   derstand the news . However , it is important to be   aware of its limitations . The performance of any   natural language generation model , including those   used for personalized news headlines , is dependent   on the quality and consistency of the data used to   train it . Similar to personalized recommendation   systems , personalized headlines have the potential   to create echo chambers . If the model is trained   on a biased or unrepresentative dataset , it may gen-   erate outputs that are incomplete , inaccurate , or   misleading . Therefore , it is crucial to be aware of   the limitations of the model and to ensure that it   is trained on high - quality data to generate accurate   and personalized headlines .   8 Ethical Considerations   It is important to use the proposed personalized   news headline generation technique ethically and   responsibly . While the technique aims to improve   personalized content recommendations and opti-   mize the user experience , it could also be used   to generate headlines that are more likely to ap-   peal to an individual reader , potentially resulting   in a biased view of the news . In this paper , we   have taken necessary precautions to protect per-   sonal data . Our technique is based on a user ’s read-   ing history , which is represented as a sequence of   recently viewed news headlines . No demographic   data such as age , gender , or location is used or   collected , due to privacy concerns . We encouragethe community to continue to explore the potential   risks and implications of this technique .   References327332743275   A Appendix   A.1 Implementation Details   Signature Phrase Selector . We fine - tune pre-   trained DPR models on our signature phrase selec-   tion datasets ( both Newsroom and Gigaword ) to   obtain signature phrase selectors . The pre - trained   models were obtained from huggingface . Under in-   dividual setting , the signature phrase encoder was   initialized from the DPR question encoder , and   the headline encoder was initialized from the DPR   context encoder . ( The DPR models were also ap-   plied in evaluating headline - user & headline - article   relevance . ) Our signature selectors and headline   generators are trained on 8 Nvidia - A100 GPUs .   Under holistic setting , the signature phrase encoder   was initialized from the DPR question encoder , and   the history encoder was initialized from the DPR   context encoder . Fine - tuning key hyper - parameters   are shown in Table 8 :   Signature - oriented Headline Generator . We   fine - tune a pre - trained BART - large modelon our   user - oriented headline generation dataset . Our key   hyper - parameters are shown in Table 8 .   PENS . The PENS baselines were implemented   following the original paper ’s github repo . For   comparison fairness , we only use the headline of   each news article to represent that article in the   user ’s reading history . We limited the max length   of the generated headlines to be 10 words . Other   then than that we train the models following the   repo ’s original setting .   Sentence BERT . We use the pre - trained sen-   tence BERT model ( all - MiniLM - L6 - v2 ) from the   following repo : https://github.com/UKPLab/   sentence - transformers The original sentence   BERT setting is to calculate the semantic similarity   between two sentences . As a result , when calcu-   lating the headline - article relevance , we report the   maximum similarity score between the headline   and all sentences in the news article .   Recommender System . As no pretrained   model was provided by the authors We train the   model from scratch . We use the implementa-   tion provided by https://github.com/wuch15/   PLM4NewsRec with default settings .   FactCC . The FactCC model we apply as an   evaluation metric was obtained from the follow-   ing paper ’s original github repo ( directly use   the pre - trained model ): https://github.com/   salesforce / factCC .   GPT-3 . We apply GPT-3 by calling OpenAI API3276athttps://openai.com/api/ .   A.2 Analysis of GPT-3 Generated Headlines   In addition to the findings we reported in sec-   tion 5.4 , we report the following observations of   headlines generated by GPT-3 guided by prompts :   We found including the phrase within ten words   in the prompt greatly boost the quality of the gen-   erated headlines . When including this phrase , the   average length of the generated headlines is less   than 8 words . However , when not including this   phrase , the average length of generated headlines   is close to 15 words , which is much longer than the   average length of human written news headlines   ( around 8 words ) . Long headlines can contain too   much information , and does not fulfill the headline   requirement of being succinct .   A.3 Human Evaluation Details   We explain human evaluation criteria in Table 10 .   A.4 A Case Study   Table 9 shows examples of editor - written , generic   headlines compared to headlines generated by our   proposed system .   Example 1 shows the smartphone market rank-   ings can be approached from different perspectives .   The editor headline focuses on Apple ’s slip to 3rd   place , while the generated headline emphasizes on   Xiaomi ’s rise to the top . In this case , the generated   headline aligns better with the reader ’s interests .   In Example 2 , both the human headline and gen-   erated headline mention Sony ’s new PC . Our gen-   erated headline includes a reference to Microsoft ,   making it likely to capture the reader ’s interest .   In Example 3 , we show that the generated head-   line has a stronger correlation with the news content   compared to the human - written headline.32773278ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   7   /squareA2 . Did you discuss any potential risks of your work ?   8   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   3 , 4   /squareB1 . Did you cite the creators of artifacts you used ?   1 , 2 , 3 , 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   4   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   4   C / squareDid you run computational experiments ?   5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   5 , Appendix3279 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   5 , Appendix   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   5 , Appendix   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   5 , Appendix   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   5 , Appendix   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   I attached it in the supplementary material ( data.zip )   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   The authors recruit their friends as volunteer evaluators   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   We explain to evaluators that their personal data will not be disclosed   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   The risk and potential consequences of exposing personal information is low   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   53280