  Sawan Kumar   Indian Institute of Science , Bangalore   sawankumar@iisc.ac.in   Abstract   Pre - trained language models have recently   shown that training on large corpora using the   language modeling objective enables few - shot   and zero - shot capabilities on a variety of NLP   tasks , including commonsense reasoning tasks .   This is achieved using text interactions with   the model , usually by posing the task as a natu-   ral language text completion problem . While   using language model probabilities to obtain   task speciﬁc scores has been generally useful ,   it often requires task - speciﬁc heuristics such as   length normalization , or probability calibration .   In this work , we consider the question answer-   ing format , where we need to choose from a   set of ( free - form ) textual choices of unspeci-   ﬁed lengths given a context . We present ALC   ( Answer - Level Calibration ) , where our main   suggestion is to model context - independent bi-   ases in terms of the probability of a choice   without the associated context and to subse-   quently remove it using an unsupervised esti-   mate of similarity with the full context . We   show that our unsupervised answer - level cali-   bration consistently improves over or is compet-   itive with baselines using standard evaluation   metrics on a variety of tasks including com-   monsense reasoning tasks . Further , we show   that popular datasets potentially favor models   biased towards easy cues which are available   independent of the context . We analyze such   biases using an associated F1 - score . Our anal-   ysis indicates that answer - level calibration is   able to remove such biases and leads to a more   robust measure of model capability .   1 Introduction   Language models ( LM ) , trained on large corpora ,   have been shown to exhibit few - shot and zero - shot   learning capability ( Radford et al . , 2019 ; Brown   et al . , 2020 ) using only text interactions , as op-   posed to ﬁnetuning the model parameters using   task speciﬁc training examples . Relying purely on   text interactions for few - shot ability shifts the fo - cus to designing and utilizing suitable task - speciﬁc   natural language templates .   In this work , we focus on free - form multiple   choice question answering ( and commonsense rea-   soning tasks in particular ) , where given a context   and a set of choices of unspeciﬁed lengths , a model   is required to select the most suitable choice . To   enable zero - shot learning , the typical approach is   to form textual sequences by concatenating the con-   text independently with each choice and then scor-   ing the concatenated strings using a pre - trained   LM .   While LM probabilities have been shown to pro-   vide useful estimates of choice probabilities given a   context , there is no incentive to treat the choices as   equal in the absence of the associated context . For   example , the LM probabilities in a neutral context   are likely to be determined by frequency . In this   work , we explore the role of biases that are likely to   be associated with the choices naturally due to the   language modeling objective . We propose ALC   ( Answer - Level Calibration ) , where we use a neu-   tral context to model such biases and remove them   using a scaling factor determined by how similarly   a model handles the question context as compared   to a neutral context .   Further , we show that popular datasets favor   models which rely on easy cues which are con-   text independent . We use a bias - speciﬁc F1 score   to analyze such biases . Our results indicate the   need for answer - level calibration for more accurate   estimates of model capabilities , or equivalently the   design of better datasets . We hope our work will   be useful for further research in both those direc-   tions . Speciﬁcally , we analyze context - independent   biases related to length , part - of - speech ( POS ) and   neutral context probabilities of the choices .   In summary , we make the following contribu-   tions:6651.We present ALC , a model - agnostic approach   to improve the unsupervised performance of   pretrained LMs for free - form multiple choice   question answering , including commonsense   reasoning tasks .   2.We show that popular datasets favor models   relying on context - independent easy cues and   demonstrate the need for answer - level calibra-   tion to better estimate model capabilities .   2 Related Work   Prompts Jiang et al . ( 2020 ) show that manually   created templates can be sub - optimal in extract-   ing knowledge from LMs , and propose mining and   paraphrasing - based approaches using training ex-   amples . Schick and Schütze ( 2021 ) highlight the   importance of selecting templates for enabling few-   shot learning .   Calibration Probabilities output by neural net-   works are known to suffer from lack of calibration   ( Guo et al . , 2017 ) , including LM output probabil-   ities ( Braverman et al . , 2020 ) . Zhao et al . ( 2021 )   use token - level calibration to improve on few - shot   classiﬁcation and generation tasks . In contrast , we   show that answer - level calibration is more suitable   for the multiple choice setting that we consider .   While we focus on free - form multiple choice   questions in this work , when the choices are single   tokens , for example in a classiﬁcation task where   the choices are True andFalse , answer - level cali-   bration would behave similar to token - level calibra-   tion . As a result , answer - level calibration can be   seen to have a more general scope as also illustrated   empirically through our experiments .   Further , our analysis ( Section 3.4 ) shows that   answer - level calibration provides a more reliable   measure of model performance on datasets with   potential biases .   Finally , Jiang et al . ( 2021 ) explore supervised   methods , including ﬁnetuning as well as post - hoc   methods , to improve calibration using training ex-   amples . In this work , we focus mainly on unsuper-   vised calibration .   Answer - level calibration Brown et al . ( 2020 )   generally perform length normalization over the to-   ken probabilities for a choice , while observing that   for a select few tasks they obtain performance gains   when using an answer - level calibration scheme   ( which corresponds to the unscaled version in Equa-   tion 3 of ALC ) . They use task speciﬁc developmentsets to choose between length normalization and   answer - level calibration which is undesirable for   few - shot learning ( Kann et al . , 2019 ) , and speciﬁ-   cally for zero - shot learning . In this work , we show   that unscaled calibration ( as in Equation 3 ) is sub-   optimal , compared to our proposed scaled version .   More recently , Holtzman et al . ( 2021 ) also arrive   at a formulation equivalent to the unscaled version   of ALC but are motivated differently . Speciﬁcally ,   they hypothesize that the possibility of different   surface forms of the same concept causes a compe-   tition between surface forms when scored by the   LM . In contrast , we are motivated by calibration   concerns and the presence of context - independent   biases . We justify this motivation through bias   associated evaluation ( Section 5.2 ) for both the un-   scaled and scaled versions of ALC .   Alternative approaches using enhanced context   One way to make the probability estimates of the   choices more accurate is to enhance the context   using more task - speciﬁc cues . For example , Brown   et al . ( 2020 ) show that with just a few in - context   examples , signiﬁcant gains in performance can be   obtained . At the same time , it has been shown that   the order of examples as well as token - level cali-   bration in such prompts can be critical for getting   good performance ( Zhao et al . , 2021 ; Kumar and   Talukdar , 2021 ) .   While the gains from enhanced context through   additional examples may be complementary to   answer - level calibration , we focus on the zero-   shot setting in this work . In the zero - shot setting ,   Shwartz et al . ( 2020 ) , working on the question an-   swering format , propose generating textual clariﬁ-   cations using the pre - trained LM itself , to enhance   the context and improve zero - shot performance of   pre - trained LM on commonsense reasoning tasks .   While their method has a much higher computa-   tional cost , we use it as an unsupervised baseline   and show improvement over it on most tasks we   consider .   3 ALC : Proposed Method   We introduce the problem setting and notation in   Section 3.1 . We brieﬂy describe our motivation   in Section 3.2 and discuss the core idea of remov-   ing context - independent biases in Section 3.3 . We   provide the natural language formatting used in   our experiments in Section A.3 . We discuss bias   associated measures in Section 3.4.6663.1 Notation   We consider a problem setting where an exam-   ple consists of a textual context CandKtextual   choices ( or options ) O ; k2[K ] , and we need to   predict which choice Oﬁts best in context C. For   example , in the case of question answering , this   amounts to answering a question contained in the   context C. Additionally , we deﬁne an instance-   independent neutral context C , where we expect   all choices to be equally likely .   Denoting the gold answer by Y , the evaluation   data is comprised of Ninstances deﬁned by the   tuples ( C;[O ] ; Y ) ; k2[K ] ; i2[N ] .   3.2 Motivation   Our main motivation is to evaluate the suitability   of pretrained LMs for free - form multiple choice   question answering where we contend that raw con-   ditional phrase probabilities do not satisfy a natu-   ral requirement for such tasks ( Equation 2 ) . We   suggest and evaluate modiﬁcations to meet this   requirement .   3.3 Removing Context - independent Biases   We aim to obtain a probabilistic model Mwhich   provides estimates P(OjC ) , the probability of a   choice Ogiven the context C. Predictions yfor an   example can subsequently be made using :   y= argmax(P(OjC ) ) ( 1 )   We wish to build such a model using a pretrained   LM , e.g. , GPT2 . Such a LM , trained on the task   of next word prediction , is expected to provide   estimates of word probabilities given a textual con-   text . For example , given the sequence of words   ww:::w , we expect GPT2 to provide probability   estimates P(wjww:::w ) . Applying chain   rule , we can obtain estimates of phrases given a   textual context . For example , we could obtain esti-   mates of P(OjC ) .   CanP(OjC)serve as a proxy for P(OjC ) ?   It is tempting to expect the LM probabilities   P(OjC)to serve as a proxy for P(OjC)when   we can format the task in natural language . How-   ever , under the assumption that all choices Oare   equally likely given a neutral context C , this ap-   proximation can be sub - optimal . For it to be opti-   mal , we would need   P(OjC ) = P(OjC):::=P(OjC )   ( 2)However , given that these are task and instance spe-   ciﬁc choices , there is no incentive in the language   modeling objective to ensure this condition .   To address this , we deﬁne a new score S(OjC )   to behave as expected with a neutral context :   S(OjC ) = log P(OjC) logP(OjC )   ( 3 )   Predictions can subsequently be made using :   y= argmax(S(OjC ) ) ( 4 )   Scaling the bias term : Equation 3 , while desir-   able , makes a strong assumption about how the   bias is present in the LM . While valid unquestion-   ably for the neutral context , the bias in a trained   ( on task - speciﬁc data , or on a task - independent pre-   training corpus ) model is likely to depend on the   context as well . For instance , a longer or more   familiar context ( in terms of similarity to training   contexts ) may mean the model is less reliant on   context - independent cues . We therefore deﬁne a   scaled version for removing biases , where the func-   tiongoutputs the scaling term ( ranging in [ 0;1 ] ):   S(OjC ) =   logP(OjC) g(C ; C)logP(OjC )   ( 5 )   We would want this formulation to preserve the   requirement in Equation 2 which was satisﬁed by   the unscaled version in Equation 3 . Speciﬁcally , we   wantg(C ; C ) = 1 which would assign an equal   score to each choice Ogiven a neutral context .   To get a model - agnosticestimate of g , we   think of logP(OjC)andlogP(OjC)as out-   puts from different models MandMrespec-   tively , and gas a measure of similarity between   the models . Note that while Muses the avail-   able context C , Muses only the neutral con-   textC. The intuition is that if MandMare   identical , there is no new information provided by   Mand we want to set g(C ; C ) = 1 , leading to   S(OjC ) = 0 . On the other hand , if MandM   are very dissimilar , we can rely on the contextual   scores of Mand set g(C ; C ) = 0 , leading to   S(OjC ) = log P(OjC ) . Speciﬁcally , to esti-   mate g , we compute a similarity metric between   the token probabilities ( across the model ’s entire   vocabulary ) output by the two models.667g(C ; C ) = Sim ( p(C ) ; p(C ) ) ( 6 )   where pindicates the probability vector out-   put by the model across the vocabulary for the   ﬁrst token given the corresponding context . In   this work , we consider Total Variation Distance   ( TVD ) , and Bhattacharyya Coefﬁcient ( BC ) ( Bhat-   tacharyya , 1943 ) .   When using TVD , we subtract it from 1 , to ob-   tain a similarity estimate :   g(C ; C ) = 1 0:5jjp(C) p(C)jj   ( 7 )   while we directly use BC :   g(C ; C ) = Xq   p(C)[i]p(C)[i](8 )   3.4 Bias Associated Measures   Consider an instance and choice speciﬁc attribute   A(O)which can take values a ; j2[J ] . If we   expect the attribute to be uncorrelated with task per-   formance , we expect a model to perform similarly   when evaluating subsets with different distributions   of attributes A(O ) = a. If a model relies on   speciﬁc values of the attribute and if the evaluation   data has sufﬁcient representation of that value , stan-   dard evaluation metrics which ignore this attribute   may provide an erroneous estimate of the model ca-   pability . As an extreme example , consider A(:)to   denote whether the selected choice corresponds to   the shortest choice among all choice O ; k2[K ] ,   with the attribute values being true / false . Assume   then that the evaluation data is dominated by in-   stances where A(Y ) = true , i.e. , with a high   probability , the correct answer in the evaluation   data is the shortest choice . Consider also a model   which always chooses the shortest choice , irrespec-   tive of the content . The model would return close   to perfect scores using standard evaluation metrics   such as accuracy against gold labels .   To analyze the impact of such attributes , we   use a macro F1 score which takes into account   the partitions created by an attribute . Recall-   ing that an instance is represented by the tuple   ( C;[O ] ; Y ) ; i2[N ] , and letting ^Ybe the   model prediction , we deﬁne precision ( P ) , recall   ( R ) and F1 scores for each attribute value a , and   subsequently an attribute speciﬁc macro F1 score   ( F1).P=#f(A(^Y ) = a ) & ( ^Y = Y)g   # fA(^Y ) = ag   ( 9 )   R=#f(A(^Y ) = a ) & ( ^Y = Y)g   # fA(Y ) = ag   ( 10 )   F1=2PR   P+R(11 )   F1= Average(fF1 g ) ( 12 )   where # f : gdenotes the count of the correspond-   ing set . If the model performs similarly irrespective   of the attribute value , the macro F1 score F1is   equal to the standard measure of accuracy :   Accuracy = # f^Y = Yg   N(13 )   4 Experimental Setup   The datasets used and the corresponding prompts   are described in Section 4.1 . The LMs used are de-   scribed in Section 4.2 and the baseline approaches   in Section 4.3 . Experimental results and analyses   are presented in Section 5 .   4.1 Data   We used a series of commonsense reasoning tasks   and evaluated on the publicly available develop-   ment sets . We used the same versions of the data   as Shwartz et al . ( 2020 ) to allow for a direct com-   parison — COPA ( Gordon et al . , 2012 ) , Common-   senseQA ( Talmor et al . , 2019 ) , MCTACO ( Zhou   et al . , 2019 ) , SocialIQA ( Sap et al . , 2019 ) , PIQA   ( Bisk et al . , 2020 ) , WinoGrande ( Sakaguchi et al . ,   2020 ) . We also report on the adversarially gen-   erated large - scale SWAG dataset ( Zellers et al . ,   2018 ) .   Further , we report on the AI2 Reasoning Chal-   lenge ( ARC ) ( Clark et al . , 2018 ) , which has Easy   and Challenge versions .   As a representative dialog understanding task ,   we report on the DREAM ( Sun et al . , 2019 ) dataset .   Finally , we report on a recent benchmark intro-   duced for measuring multitask accuracy of pre-   trained models ( referred to as Hendrycks in the   following ) Hendrycks et al . ( 2020 ) .   For MCTACO , we used a reduced subset as pro-   vided by Shwartz et al . ( 2020 ) where each question668   is associated with only one correct choice . For   COPA , we also report on the test split due to the   small size of COPA dev set . The sizes of the   datasets used are reported in Appendix Table 8 .   All datasets contain questions in English language .   We brieﬂy describe these datasets in Section A.2 .   Examples for each dataset along with contextual   ( C ) and neutral ( C ) prompts used in this work are   captured in Section A.3 .   4.2 Models   We experiment with GPT2 ( Radford et al . , 2019 )   variants - distilgpt2 , gpt - small , gpt - medium , gpt2-   large and gpt2 - xl . The size of models used is re-   ported in Appendix Table 9 . While the gpt- * mod-   els have been trained similarly as described in Rad-   ford et al . ( 2019 ) , distilgpt2 has been pretrainedwith the supervision of GPT2(Wolf et al . , 2020 ) .   For most of our experiments , we utilize the gpt2 - xl   model .   Please refer Section A.1 for additional details   about the experimental setup .   4.3 Baselines   Uncalibrated : Predictions are made using uncali-   brated probabilities from a LM , logP(OjC ) , com-   puted as the sum of conditional log - probabilities   output by the model for the tokens in O.   Length normalized : Predictions are made us-   ing length - normalized probabilities from a LM ,   logP(OjC ) , computed as the mean of conditional   log - probabilities output by the model for the tokens   inO.669   Self - talk : We use the ofﬁcial code repositoryof   self - talk ( Shwartz et al . , 2020 ) using gpt2 - xl as   both the scoring model and the knowledge source .   Token calibration : Following Zhao et al . ( 2021 ) ,   we use the probability vector output , pby the   model at the ﬁrst token given the neutral context   to calibrate the model probabilities . Speciﬁcally ,   each token probability pis offset by pand re-   normalized : p= softmax ( p p ) . We also   tried an alternative variant suggested by Zhao et al .   ( 2021 ) where p= softmax ( p = p)but this gen-   erally did worse and we skip the corresponding   results .   5 Experimental Results   We aim to answer the following questions : Q1How does ALC compare with baselines using   standard evaluation ( accuracy ) on free - form   multiple choice question answering tasks ?   ( Section 5.1 )   Q2Does the aforementioned evaluation reﬂect   true model capability ? To answer this ques-   tion , we perform a series of bias associated   evaluations ( see Section 3.4 ) and also evalu-   ate whether ALC helps overcome such biases .   Speciﬁcally , we evaluate on biases related to   answer length , POS tag and context - ignorant   LM probability . ( Section 5.2 )   Q3Does ALC improve expected calibration error   ( Guo et al . , 2017 ) ? ( Section 5.3 )   5.1 Standard Evaluation   The overall results for the commonsense reasoning   tasks ( considered by Shwartz et al . ( 2020 ) ) using   standard evaluation of ALC , as well as the base-670   lines , with gpt2 - xl are presented in Table 1 ( top ) .   We also report on an unscaled ablation of ALC .   Note that ALC outperforms the uncalibrated base-   line on all datasets except WinoGrande ( where all   models perform poorly and we drop it from further   discussions ) . Further , the signiﬁcant gains com-   pared to token calibration ( which generally does   worse than the uncalibrated baseline ) show that   answer - level calibration is more suited for unsu-   pervised commonsense question answering when   there is no constraint on the lengths of candidate   choices . Finally , ALC outperforms or is compet-   itive with self - talkwhile being signiﬁcantly less   computationally intensive . ALC requires scoring   two strings ( context input and neutral input ) for   each choice , while self - talk requires generating   hundreds of clariﬁcation texts using data - dependent   templates and subsequently scoring them .   We also report on the average gain over the un-   calibrated baseline across gpt2 models of varying   sizes ( Table 9 ) in Table 1 ( middle ) and observe   similar trends as in the case of gpt2 - xl .   While our focus is zero - shot unsupervised evalu-   ation , we also perform few - shot ( 1 - shot and 4 - shot )   evaluation In general , for k - shot evaluation , we   sample 100 sets of size k from an unseen splitof   the dataset . A few - shot context is obtained by con-   catenating training examples with a newline token .   We report the average performance on the evalu-   ation set in Table 1 ( bottom ) and observe similar   trends as before .   We present the standard zero - shot evaluation on   additional datasets in Table 2 . The trends are sim - ilar except for the SWAG ( see Section 5.2 for an   explanation ) and the Hendrycks datasets ( see Ta-   ble 11 ) .   Finally , while our focus is causal language mod-   els , we also present results using RoBERTa - large   ( a masked language model ) in Table 10 . Again , we   observe similar trends .   In the subsequent sections , we show that the   evaluation using the accuracy metric may not reveal   true model capabilities as the datasets may favor   models which utilize easy cues for predicting the   answer .   5.2 Bias Associated Evaluation   Next , to gain a better understanding of the model   capabilities , we analyze the performance associated   with undesirable biases related to length , POS tag   and context - ignorant LM probability . Speciﬁcally ,   we deﬁne the following attributes ( see Section 3.4 ):   Shortest Attribute A(O)is set to true if Ois   the shortest ( number of tokens ) choice among the   choices O ; k2[K ] . Otherwise , the attribute is   set to false .   Longest Deﬁned similar to Shortest , but set to true   ifOis the longest answer and false otherwise .   POS Attribute A(O)is set to the POS tag of the   ﬁrst token in the choice O. We do n’t consider   POS tags which occur less than a threshold ( 25 )   in the evaluation data .   LM - Best Attribute A(O)is set to true if Ois   the most likely choice using context - ignore ( neu-   tral input ) LM probability . Otherwise , it is set to   false .   LM - Worst Deﬁned similar to LM - Best , but set to   true when Ois the least likely choice and false   otherwise .   Finally , we consider length - normalized versions   of LM - Best and LM - Worst , referred to as LM-671   Norm - Best andLM - Norm - Worst respectively .   Brieﬂy , our experiments reveal that while the   datasets considered do n’t share a similar bias pat-   tern , each usually suffers from at least one bias   considered in this work , i.e. , there is a drop in per-   formance when measured using the bias associated   score . We present the detailed results for com-   monsense reasoning tasks in Appendix Table 12 ,   using gpt2 - xl model , while highlighting the key   takeaways here . Recall that in the absence of bi-   ases in the model , the F1 score should match the   accuracy score .   In the following sections , we provide a more   directed analysis on the presence of such biases ,   on datasets where such biases are most prominent ,   and if ALC helps alleviate such biases .   5.2.1 Length   We create subsets of the CommonsenseQA and   SocialIQA dev set with speciﬁc properties to eval-   uate if the LM - Baseline has the associated biases   and if they are addressed by ALC . First , we cre-   ate subsets of examples where the shortest / longest   answer is the correct answer . We expect longer   sentences to have lower probabilities than shortersentences with the uncalibrated baseline . Addi-   tionally , with the length normalized variant , where   the ﬁnal score is obtained as the mean of condi-   tional log - probabilities instead of the sum ( as in   the uncalibrated baseline ) , longer sentences could   potentially be favored . We report the uncalibrated   and ALC ’s performance in Table 3 . Note that both   uncalibrated baseline and the length - normalized   variants favor one subset at the cost of the other ,   while ALC improves on both . In particular , the un-   calibrated baseline has a much poorer recall when   the longest answer is correct . On the other hand ,   the length normalized variant has a much poorer re-   call when the shortest answer is correct . The results   indicate that ALC provides a viable alternative to   length normalization for handling length biases .   5.2.2 POS   We analyze potential part of speech ( POS ) tag bi-   ases in Table 4 . Considering the CommonsenseQA   dataset , we create subsets of the data where the   correct answer is of the POS tag noun , verb or   adjective . Note that ALC shows less variation in   performance ( F1 ) across these subsets when com-   pared to uncalibrated baseline while improving on672   each subset . In particular , the maximum difference   in F1 scores is 8.1 for the uncalibrated baseline   while it is 5.03 for ALC ( BC ) . ALC also improves   over the length normalized variant for each subset .   5.2.3 Context - ignorant LM Probability   To understand how much of the unsupervised per-   formance comes from context - independent LM bi-   ases , we analyze subsets where the correct answer   is most / least likely without the context . We report   the performance on the PIQA and ARC datasets in   Table 5 and show that such biases indeed exist . The   key takeaway is that the standard evaluation metrics   may not give an accurate estimate of performance   and that ALC provides more reliable estimates .   Finally , we report macro F1 scores for LM-   Norm - Best and LM - Norm - Worst evaluation in Ta-   ble 6 on PIQA and SWAG datasets . The results   indicate that the datasets favours length normal-   ization aware scoring irrespective of the context .   When we measure the bias associated score , ALC   generally performs better .   5.3 Expected Calibration Error   Given a score S(OjC)for each choice O , we can   compute a conﬁdence estimate conf ( OjC)as :   conf ( OjC ) = e   Pe(14 )   Guo et al . ( 2017 ) compute expected calibration   error ( ECE ) by partitioning Nconﬁdence predic-   tions into Requal bins B ; r2[1 ; R]and comput-   ing the weighted average of the absolute difference   between the conﬁdence and accuracy in each bin :   ECE = XjBj   Njacc(B) conf ( B)j(15 )   where acc ( ) andconf ( ) measure the accuracy and   mean conﬁdence respectively in a bin . We set the   number of bins to be 20 .   We report the average difference in accuracy and   ECE compared to the uncalibrated baseline across   the evaluation datasets ( except WinoGrande ) in Ta-   ble 7 . When compared to the uncalibrated baseline ,   ALC provides gains in calibration error while also   improving performance . Length - normalization   also improves ECE , presumably by correcting for   length bias . However , length - normalization does   not improve performance on an average . The rel-   ative performance gains of ALC can be explained   through the handling of additional biases beyond   length bias .   6 Conclusion   We propose ALC ( Answer - Level Calibration ) , an   unsupervised method to improve performance of   pretrained language models . We show that , when   compared to existing baselines , ALC is more suit-   able for free - form multiple choice question answer-   ing , including commonsense reasoning tasks . We   also show that popular datasets favor models which   rely on easy cues for predictions , and that ALC pro-   vides more reliable estimates of model capabilities   by getting rid of some of these biases .   References673674675   A Appendix   A.1 Experimental Setup   We leverage the transformers library ( Wolf et al . ,   2020 ) for accessing the LMs . All experiments were   conducted using a single Nvidia GeForce GTX   1080 Ti Graphics Card . There was no training   required . A typical experiment using gpt2 - xl for   CommonsenseQA task took around 15 minutes .   The model sizes are captured in Table 9 . Size of   evaluation datasets is captured in Table 8 .   We used the nltk pos - tagger with the universal   tagset for pos - tagging .   A.2 Datasets   COPA : The COPA dataset ( Gordon et al . , 2012 )   contains a premise associated with two alternatives   where one has a more plausible causal connection   with the premise . There are two types of examples ,   depending on whether the connection is of “ effect "   or “ cause " .   CommonsenseQA : The CommonsenseQA dataset   ( Talmor et al . , 2019 ) contains common sense ques-   tions extracted from ConceptNet ( Liu and Singh ,   2004 ) . The alternative choices are made challeng-   ing by selecting from related concepts in Concept-   Net or through suggestions through crowdsourcing . MCTACO : The MCTACO dataset ( Zhou et al . ,   2019 ) contains common sense questions related to   understanding of time . Difﬁcult adversarial candi-   dates are selected using BERT ( Devlin et al . , 2019 )   predictions .   SocialIQA : The SocialIQA dataset ( Sap et al . ,   2019 ) contains questions about social interactions   with crowdsourced answers .   PIQA : The PIQA dataset ( Bisk et al . , 2020 ) con-   tains questions about common sense . The question   corresponds to a goal derived from an instruction   website and the answers were crowdsourced .   WinoGrande : The WinoGrande dataset ( Sak-   aguchi et al . , 2020 ) is based on the Winograd   Schema Challenge ( Levesque et al . , 2012 ) , where   a pair of sentences differ in one or two words con-   taining a referential ambiguity .   ARC : The ARC dataset ( Clark et al . , 2018 ) con-   tains natural grade - school science questions . The   authors provide Easy and Challenge splits . The   Challenge version is created using examples where   retrieval - based and word - occurrence based meth-   ods fail ( Clark et al . , 2018 ) . The Easy version   contains the remaining questions .   DREAM : DREAM ( Dialogue - based REAding   comprehension exaMination ) ( Sun et al . , 2019 )   provides a benchmark for reading comprehension   focusing on multi - turn multi - party dialog under-   standing .   SWAG : SWAG ( Situations With Adversarial Gen-   erations ) ( Zellers et al . , 2018 ) provides a large-   scale dataset for grounded commonsense inference   where different possible endings of a context are   provided where the correct answer is derived from   video captions while alternatives are adversarially   generated .   Hendrycks : Hendrycks et al . ( 2020 ) provide a test   suite containing 57 tasks to test the multitask accu-   racy of pretrained models . The tasks are broadly   categorized into Humanities , STEM , Social Sci-   ences and Other . We run our experiments on sub-   sets associated with these categories .   A.3 Data Formatting   In this section , we provide the formatting used   to convert task - speciﬁc examples into natural lan-   guage prompts as used in our experiments . We ﬁrst   give examples of the Context ( if any ) , the Ques-   tion andChoices as present in the corresponding   dataset , followed by the Context input andNeu-   tral input as fed to the pretrained LM.676   •CommonsenseQA   Question : A revolving door is convenient for   two direction travel , but it also serves as a   security measure at a what ?   Choices : ( A ) bank ( B ) library ( C ) department   store ( D ) mall ( E ) new york   Context input : Question : A revolving door   is convenient for two direction travel , but it   also serves as a security measure at a what ?   Answer :   Neutral input : Answer :   •MCTACO   Context : He layed down on the chair and   pawed at her as she ran in a circle under it .   Question : How long did he paw at her ?   Choices : ( A ) 2 minutes ( B ) 2 days ( C ) 3.5   hours ( D ) 1 day ( E ) 1.4 hours ( F ) 90 minutes(G ) 7 hours ( H ) 7 days   Context input : He layed down on the chair   and pawed at her as she ran in a circle under   it . Question : How long did he paw at her ?   Answer :   Neutral input : Answer :   •PIQA   Context : Remove soap scum from shower   door .   Choices : ( A ) Rub hard with bed sheets , then   rinse . ( B ) Rub hard with dryer sheets , then   rinse .   Context input : Question : Remove soap   scum from shower door . Answer :   Neutral input : Answer :   •ARC   Question : Which technology was developed   most recently ?   Choices : ( A ) cellular telephone ( B ) television   ( C ) refrigerator ( D ) airplane   Context input : Question : Which technology   was developed most recently ? Answer :   Neutral input : Answer :   •COPA - effect   Context : The man turned on the faucet .   Choices : ( A ) The toilet ﬁlled with water . ( B)677Water ﬂowed from the spout .   Context input : The man turned on the faucet ,   so   Neutral input : , so   •COPA - cause   Context : The hamburger meat browned .   Choices : ( A ) The cook froze it . ( B ) The cook   grilled it .   Context input : The hamburger meat   browned , because   Neutral input : , because   •SocialIQA   The formatting follows Shwartz et al . ( 2020 ) .   Context : Tracy did n’t go home that evening   and resisted Riley ’s attacks .   Question : What does Tracy need to do before   this ?   Choices : ( A ) make a new plan ( B ) Go home   and see Riley ( C ) Find somewhere to go   Context input : Tracy did n’t go home that   evening and resisted Riley ’s attacks . Before ,   Tracy needed to   Neutral input : Before , Tracy needed to   •WinoGrande   Context : Sarah was a much better surgeon   than Maria so _ always got the easier cases .   Choices : ( A ) Sarah ( B ) Maria   Context input : Sarah was a much better   surgeon than Maria so   Neutral input : so   •DREAM   Context : W : I wish I knew the times of the   trains to London . But our phone ’s out of   order .   M : Do n’t worry , Grandma . I ’ll ﬁnd out for   you on the Internet .   W : Thank you !   Question : What is the man going to do ?   Choices : ( A ) Go on the Internet . ( B ) Make a   phone call . ( C ) Take a train trip .   Context input : W : I wish I knew the times of   the trains to London . But our phone ’s out of   order .   M : Do n’t worry , Grandma . I ’ll ﬁnd out for   you on the Internet . W : Thank you ! Question : What is the man   going to do ? Answer :   Neutral input : Question : What is the man   going to do ? Answer :   •SWAG   Context : The person plays a song on the   violin . The man   Choices : ( A ) ﬁnishes the song and lowers   the instrument . ( B ) hits the saxophone and   demonstrates how to properly use the racquet .   ( C ) ....   Context input : The person plays a song on   the violin . The man   Neutral input : The man   •Hendrycks   Question : If 4 daps = 7 yaps , and 5 yaps = 3   baps , how many daps equal 42 baps ?   Choices : ( A ) 28 ( B ) 21 ( C ) 40 ( D ) 30   Context input : Question : If 4 daps = 7 yaps ,   and 5 yaps = 3 baps , how many daps equal 42   baps ? Answer :   Neutral input : Answer :   A.4 Note on Comparison with Self - talk   While it seems surprising that the self - talk results   in Table 1 are generally lower than the uncalibrated   baseline , we note that we have n’t underestimated   the performance of self - talk . Self - talk performance   was obtained using the ofﬁcial repository of the   project and the results align well with those re-   ported in the original work . What has changed is   the performance of the baseline , which is higher   here ( which in turn shows the signiﬁcane of the   numbers reported in this work ) . We note two dif-   ferences with respect to the self - talk repository .   First , self - talk uses a length - normalized baseline ,   while we evaluate both uncalibrated and length-   normalized baselines . Second , there is a bug in the   self - talk repository regarding calculating baseline   performance , also noted in a GitHub issue .   A.5 Additional Results   We show results across commonsense reasoning   datasets for bias - associated F1 scores in Table 12.678679