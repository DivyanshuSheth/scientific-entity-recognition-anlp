  Thomas ScialomTuhin ChakrabartySmaranda MuresanMeta AIDepartment of Computer Science , Columbia University   Abstract   Recent work on large language models relies   on the intuition that most natural language pro-   cessing tasks can be described via natural lan-   guage instructions and that models trained on   these instructions show strong zero - shot perfor-   mance on several standard datasets . However ,   these models even though impressive still per-   form poorly on a wide range of tasks outside of   their respective training and evaluation sets . To   address this limitation , we argue that a model   should be able to keep extending its knowledge   and abilities , without forgetting previous skills .   In spite of the limited success of Continual   Learning we show that Fine - tuned Language   Models can be continual learners . We empir-   ically investigate the reason for this success   and conclude that Continual Learning emerges   from self - supervision pre - training . Our result-   ing model Continual - T0 ( CT0 ) is able to learn   8 new diverse language generation tasks , while   still maintaining good performance on previous   tasks , spanning in total 70 datasets . Finally , we   show that CT0 is able to combine instructions   in ways it was never trained for , demonstrating   some level of instruction compositionality .   1 Introduction   Recent work has shown that large language mod-   els have the ability to perform zero - shot and few-   shot learning reasonably well ( Brown et al . , 2020 ;   Rae et al . , 2021 ; Smith et al . , 2022 ) . A particu-   larly successful line of work relies on the intuition   that most natural language processing tasks can be   described via natural language instructions . For   example , a summarization task can be reformat-   ted as a response to a natural language input as   shown in Table 1 . Sanh et al . ( 2022 ) and Wei et al .   ( 2022 ) have released T0 and FLAN respectively   and shown that fine - tuning models on a massive   Table 1 : An instance from T0 training set ( Sanh et al . ,   2022 ) where a summarization task is reformatted as a   natural language response to a natural language input .   mixture of NLP datasets expressed via such natural   language instructions ( i.e. , instruction tuning ) , im-   proves the zero - shot performance of large language   models . FLAN is extremely large in size ( 137B )   and is not publicly available limiting its further use   and reproducibility . Conversely T0 ( Sanh et al . ,   2022 ) is publicly available and orders of magnitude   smaller and hence we resort to working with T0 .   However impressive , these models are still lim-   ited to simple instructions and mainly Natural Lan-   guage Understanding ( NLU ) tasks . These models   perform poorly on a wide range of tasks that differ   from their respective evaluation sets . To improve   their ability on new and diverse tasks , one needs   to fine - tune these models again . However , one key   problem associated with fine - tuning is catastrophic   forgetting ( French , 1999 ) . So , how can we extend   these models knowledge and abilities , without suf-   fering from catastrophic forgetting ?   In this paper , we study Continual Learning of   large language models fine - tuned on natural lan-   guage instructions and investigate their ability to   adapt to diverse tasks , while avoiding catastrophic   forgetting on the older tasks . For this purpose ,   we propose Continual - T0 ( CT0 ) , a T0 model that   uses Continual Learning with rehearsal ( Shin et al . ,   2017 ) , i.e. using a memory buffer containing a   small portion of previous data replayed during train-   ing ( Section 3 ) . We start from T0 , a model trained   jointly on 50 datasets , resulting in a good zero - shot   performance on 12 completely different datasets .   We are then able to teach progressively 8 new di-   verse tasks , while maintaining almost 100 % of the6107initial performance on all the previous datasets .   This result is obtained by using only 1 % of data for   memory buffer . Notably , we also maintain the per-   formance for the T0 zero - shot evaluation datasets ,   even though no rehearsal for those was done , the   first of its kind setup for CL ( Section 4 ) .   Our final model , Continual - T0 ( CT0 ) in addition   to performing as well as T0 on all the different   T0 datasets , can also understand instructions about   the newly introduced tasks focused on language   generation problems such as writing a haiku , gen-   erating empathetic responses in a dialogue , sim-   plifying text , generating a headline with decoding   constraints , generating natural language explana-   tions for Natural Language Inferece ( NLI ) tasks ,   generating a Tweet on a given topic in the style   of a given author , or question answering for new   domain / concepts such as COVID-19 .   We also conduct an extensive analysis and show   that our newly learned instructions can be com-   posed in ways never seen during training , leading   to better generalization ( Section 4.3 ) . Given the sur-   prising performance of a simple continual learning   strategy , we empirically investigate the reason for   this success . Why transformer models like T0 are   continual learners ? Is it because of their multi - task   nature or the instruction tuning paradigm ? Or does   the large scale parameterization of language mod-   els contribute to this success ? Our experimental   analysis show that the easy adaptability and con-   tinual learning capabilities actually emerge from   pre - training and not the above , including scale   ( Table 5 , Section 5.1 ) .   2 Related Work   Continual Learning Current models are limited   in continuously learning without forgetting any pre-   viously acquired knowledge and abilities . Research   in this direction has investigated various strategies   such as External Memory , Constraints and Model   Plasticity ( Parisi et al . , 2019 ) . External Memory   methods often simply use rehearsal with a replay   during training ( Rebuffi et al . , 2017 ) . de Mas-   son D’Autume et al . ( 2019 ) also proposed local   fine - tuning at inference time , leveraging examples   similar to the considered input .   Through the lens of NLP tasks , Biesialska et al .   ( 2020 ) look at the problem of CL and discuss major   challenges involved . Jin et al . ( 2021 ) show CL al-   gorithms are effective for knowledge preservation .   Their study also infer that continual pretraining im - proves temporal generalization . ( Douillard et al . ,   2021 ) proposed a a dynamic expansion of special   tokens with a transformer architecture . Mi et al .   ( 2020 ) and Madotto et al . ( 2021 ) perform CL for   task oriented dialog systems by using replay based   strategy . Cao et al . ( 2021 ) propose a new CL frame-   work for NMT models , while Ke et al . ( 2021 ) pro-   poses a novel capsule network based model called   B - CL ( Bert based CL ) for sentiment classification   tasks . Jin et al . ( 2020 ) show how existing CL algo-   rithms fail at learning compositional phrases . Lin   et al . ( 2022 ) propose a benchmark and highlight key   challenges for continual model refinement in Out-   of - Distribution data streams . More recently , Sun   et al . ( 2019 ) propose a lifelong learning method   LAMOL that is capable of continually learning   new tasks by replaying pseudo - samples of previous   tasks that require no extra memory or model capac-   ity . To the best of our knowledge , LAMOL corre-   sponds to the state - of - the - art for CL in NLP . Most   similar to our work is that of Yin et al . ( 2022 ) who   also study continual learning from task instructions   based on the NATURAL - INSTRUCTION bench-   mark ( Mishra et al . , 2022).Finally instead of lim-   iting to vision - only and language - only tasks Srini-   vasan et al . ( 2022 ) study the challenge of learning   multimodal tasks in a CL setting , and systemati-   cally evaluate how upstream continual learning can   rapidly generalize to new multimodal and unimodal   tasks   Most of the aforementioned works fall into the 2   scenarios differentiated by Lomonaco and Maltoni   ( 2017 ): 1 ) learning new data of known classes ( on-   line learning ) , and 2 ) learning new classes ( class-   incremental learning ) . Thus , the study are often   limited to a narrow domain , or a specific task . In   our work , we propose to address Continual Learn-   ing more broadly : learning a diverse set of new   tasks different from the ones used for training . For   this , we leverage the idea of instruction tuning ( Wei   et al . , 2022 ; Sanh et al . , 2022 ) , that enables us to   frame any NLP task as a response to a natural lan-   guage input and use rehearsal as a mechanism to   avoid catastrophic forgetting ( Shin et al . , 2017 ) .   3 Continual Learning for Fine - tuned   Language Models   3.1 Continual Learning via Rehearsal ( CLR )   Our objective is to maintain the model ’s existing   learned skills , while progressively learning more   tasks . To prevent the model from catastrophic for-6108getting , we rely on an external memory module ,   storing a subset of previous training data ( Shin   et al . , 2017 ) . We define the tasks to be learned as   a task sequence T= ( T , T , , T)ofNtasks . D   is the corresponding dataset for task T. Formally ,   the training data augmented with rehearsal Dis   defined as :   D = D(rD ) ( 1 )   where r is the rehearsal hyper - parameter that con-   trols the percentage of examples sampled from pre-   vious tasks T , ... T. We note that r= 0corre-   sponds to no memory , and r= 1is equivalent to a   multi - task setup using all the previous examples .   3.2 Continual - T0 ( CT0 )   For all our experiments , we instantiate our model   with the T0 model ( Sanh et al . , 2022 ) . T0 is a T5   model ( Raffel et al . , 2020 ) fine - tuned in a multitask   setting on 50 datasets , where the natural language   instructions corresponding to individual tasks are   used as the input . The set of these 50 tasks corre-   sponds therefore to Tin 1 . This massive instruc-   tion tuning allows the model to perform well in   a zero - shot setup , by leveraging the information   presents only in the instructions . Our initial model   is T0_3B , the T0 version with ( only ) 3 Billions pa-   rameters for all our experiments . We used the same   hyper - parameters as the ones reported in Sanh et al .   ( 2022 ) . The only new hyper - parameter introduced   in our paper is the rehearsal proportion r. We ex-   plorer∈[0,0.25%,1%]as reported in our first set   of results ( see Section 3 ) .   For each of T0 training tasks , we consider   100,000 examples for training , such that 1 % re-   hearsal corresponds to 1,000 examples that will be   used as the memory buffer for rehearsal . Thus , for   datasets with fewer training examples , we upsam-   ple them and conversely for largest datasets like   Gigaword or Simplification , we limit to 100,000 ex-   amples . Note that here , while we used rehearsal   for the training data of T0 training tasks , we   never used any data from T0 zeroshot tasks , so   it remains completely zero - shot . It is important   to highlight that rehearsal is the standard for CL ,   and a zero - shot set up with no rehearsal has never   been explored yet to the best of our knowledge.3.3 Tasks   We briefly describe all the tasks Tused to progres-   sively train and evaluate our model ( a more com-   plete description is also given in Appendix 7.2 ) .   T0 Tasks . As detailed in Section 1 , we instantiate   our model with T0 weights . T0 is trained in a multi-   task setting on a collection of 50 datasets spanning   from QA , Classification to Summarization . We re-   fer to this set of 50 datasets as T0 train ( T0tr ) . To   evaluate the true zero - shot performance for T0 , the   authors evaluated it on a set of 12 datasets corre-   sponding to 4 tasks different from T0 train : Nat-   ural Language Inference , Co - reference resolution ,   Word sense disambiguation and Sentence comple-   tion . We refer to this set as T0 zero - shot ( T0zs ) .   New Tasks . To extend T0 capabilities and bench-   mark its performance in our continual learning   setup , we introduce 8 new tasks focused on lan-   guage generation , unlike the existing T0 evaluation   tasks and majority of the T0 training tasks ( except   summarization ) . These tasks include : 1 ) Text Sim-   plification ( Simpl ) with the goal of paraphrasing a   given text using simple language , where we train   our model on WikiAuto Jiang et al . ( 2020 ) and   evaluate it on the WikiAuto and ASSET datasets   ( Alva - Manchego et al . , 2020 ) ; 2 ) Headline Gen-   eration with Constraint ( HGen ) , where given a   news article D and an input keyword X , the goal   is to generate a headline that contains the keyword   at the beginning , at the end or anywhere ( see Ta-   ble 2 for a sample instruction to generate a head-   line containing the keyword at the beginning ) . To   create the training data , we simply leverage the   gold - reference to select the keyword X , such that   our model is trained with consistent and plausi-   ble instructions ; 3 ) Haiku Generation ( Haiku ) ,   where the task is to generate a Haiku — a type of   short form poetry originally from Japan — given   a topic ( see Table 2 for a sample instruction ) . We   train on pairs ( Haiku , title ) from Reddit and gen-   erate Haikus for novel topics at inference time ; 4 )   Covid QA ( CQA ) ( Möller et al . , 2020 ) , a Question   answering task focusing on COVID-19 . Because   T0 has been extensively trained on a QA dataset ,   CovidQA in its original format simply requires do-   main transfer . To make the task more challenging ,   we propose to provide only the question as an input ,   now framing the task as “ learn the answer by heart ”   in an encyclopedia style task . This way the task   framing can be seen as a new strategy to incorpo-6109   rating knowledge and preventing the model from   concept drift .   5)Inquisitive Question Generation ( InqQG )   where we train our model on the ELI5 dataset ( Fan   et al . , 2019 ) to generate questions that typically   require long form answers ; 6 ) Empathetic Dia-   logue Generation ( EmDg ) , where we generate   a response to a conversational context grounded   in emotional situations using the Empathetic Dia-   logue data Rashkin et al . ( 2019 ) ; 7 ) Explanation   Generation ( Exp ) where we train a model on the   eSNLI ( Camburu et al . , 2018 ) benchmark to gener-   ate natural language explanations given a premise ,   hypothesis and a label ( entail , contradict , neutral ) ;   8)Twitter Stylometry ( TwSt ) , where we generate   a relevant tweet given a hashtag and the tweet ’s au-   thor by fine - tuning on the data consisting of tweets   from the top 20 most followed users in Twitter re-   leased by Tareaf ( 2017 ) . We illustrate the 8 new   tasks with their instructions in Table 2 . A complete   detailed description for all the 8 tasks with train ,   validation splits is available in the Appendix 7.3.1.3.4 Automatic Metrics   We report the accuracy for T0 zero - shot tasks , and   standard metrics for NLG like BLEU(Papineni   et al . , 2002 ) and SARI(Xu et al . , 2016 ) for Sim-   plification , ROUGE ( Lin , 2004 ) for Headline Gen-   eration , or BERTScore ( Zhang et al . , 2020)for   open - domain NLG tasks as it has been found to   correlate well with human judgements .   We also designed customized metrics for some   of the tasks . For instance , to evaluate Twitter   Stylometry where the task is to generate a tweet in   the style of the author , we trained a Ridge Classifier   to predict the author given the evaluated tweet . For   Haiku generation , we know that in general , a Haiku   contains only 17 syllables , broken up into three   lines . We therefore create a metric to reflect the   task structure that integrates i ) the differences in   syllables and number of lines between the gold and   generated haiku , ii ) the BLEU score between gold6110and predicted , and iii ) the presence of the topic in   the generated haiku . We report all the details for   the metrics in the Appendix 7.4 .   4 Results   4.1 Learning a New Task at a time   First , we test CLR independently on three tasks   ( Headline Generation with Constraint , Simplifica-   tion , and Haiku Generation ) , by varying the re - hearsal hyper - parameter between 0 % , 0.25 % and   1 % , respectively . We report the results in terms of   Relative Gain in Figure 1 .   We observe that for the three tasks , the rehearsal   value does not affect the task result : all the blue   curves are consistent . Conversely , the rehearsal   value has a dramatic impact on the T0 zero - shot   results ( green curves ) . As already discussed previ-   ously , at 0 % rehearsal , the model catastrophically   forgets the T0 zero - shot tasks . Conversely , with   only 0.25 % rehearsal we observe an almost perfect   stability . Finally , with 1 % rehearsal ( solid line ) , T0   zero - shot results are stationary , indicating that our   model is able to maintain its performance on those   tasks , while learning a new task .   4.2 Learning a Sequence of New Tasks   As observed from our previous experiments using   Continual Learning via rehearsal we can learn a   new task at any time without catastrophic forget-   ting , with just a very little rehearsal percentage . As   a next step , we propose to measure whether lan-   guage models can progressively learn more tasks   without catastrophic forgetting . This is an impor-   tant direction as it would allow the models to con-   tinually increase their knowledge and capabilities   without forgetting the knowledge already acquired .   To test this hypothesis , we start from T0 check-   point , a model trained on 50 datasets . We progres-   sively train it on a sequence of 8 new NLG tasks   ( see Section 7.3.1 and Table 2 for description of   those tasks ) using Continual Learning via rehearsal   ( r= 1 % ) . We call our final model CT0 .   To measure the actual success for CL on a se-   quence of N tasks , we introduce the notion of Up-   per Bound ( UB ) . UB corresponds to the maximum   performance achieved by the model , when fine-   tuned only on a specific task , T. Arguably , the   model succeeds in CL , if it maintains a perfor-   mance close to UB , while learning new tasks . The   normalised results , i.e . , Relative Gain for a given   taskT , correspond to the actual scores sdivided   by their task TUB , s / UB . Hence , 1 corre-   sponds to performing similar to the UB for any task .   The model is expected to start bellow 1 before step   nsince it has not been trained yet on T , while   for the latest steps twitht > n , results below 1   indicate task forgetting .   In Figure 2 , we display the progressive sequen-   tial learning on the 8 new tasks . We learn a new   task , starting from T0 , and add to our rehearsal6111   buffer 1 % of the data of the learned task . We ob-   serve an improvement progressively for each task ,   that is our model keeps learning new tasks . At the   same time , the performance is preserved for the   other tasks , ( i.e. , the Relative gain remains around   1 ) indicating the success of our CLR method in a   sequential learning setup through more than 1000   gradient steps over 8 different tasks .   In Table 3 , we report the results on all the 8 new   tasks as well as T0tr and T0zs ( see Section 3.3 ) ,   corresponding respectively to the evaluation sets   of the 50 training datasets used in T0 , and the 12   datasets kept apart for the zero - shot evaluation . In   the first bloc of Table 3 , we observe the starting   performance of our two initial checkpoints , T0_3B   and T0pp(11B ) . The second bloc corresponds to   their respective Upper Bounds . We report the re-   sults for our models after training them progres-   sively on the 8 new tasks , as well as the baseline   LAMOL ( see Section 2 ; for fair comparison we   adapted LAMOL initialising it with T03B , addi-   tional details can be found in Appendix 7 ) . The   CT03B and CT0pp results in Table 3 are reported   after the model was fine - tuned on the latest task in   the sequence ( intermediary steps are given in Table   7 in Appendix ) .   Our two CT0 models obtain final results very   close to their UB , maintaining 99.8 % for T0pp and   98.0 % for T0_3B. This clearly indicates the effi-   ciency of the CLR method . Notably , no task suffersa decrease in performance more than 2 % for T0pp .   Table 3 shows how the CT0 model remembers and   retains knowledge from tasks trained at very early   stages of the Continual Learning process . More-   over , CT0 still performs well on the zero - shot set   of tasks ( T0zs ) despite no rehearsal for those .   It should also be noted that the T0pp model fails   to generalize for most NLG tasks , as opposed to   our CT0 model . For instance Table 6 in Appendix   shows it can generate a haiku that has a perfect syl-   lable count of 17 given an unseen topic of ‘ moun-   tain winds haunt ’ . It can also generate reasonable   natural language explanations that often comply   with our commonsense . Moreover , CT0 obtains a   new state - of - the - art on the ASSET evaluation set ,   improving over MUSS ( Martin et al . , 2020 ): 85.9   BLEU4 Vs 72.98 and 46.6 SARI Vs 44.15 , and   despite not using all the training data available .   In contrast to Continual Learning with rehearsal ,   LAMOL clearly diverges from its UB ( T03B ) indi-   cating catastrophic forgetting . While LAMOL was   known to perform well mostly on NLU tasks , we   hypothesise that the generative nature for our tasks   is not suited for the method . Finally , Continual   Learning with rehearsal approach is task order   invariant as demonstrated by revfinal results : rev-   final corresponds to CT03B trained on the 8 tasks   within in the reverse order . We give more details6112about the order choice in the Appendix .   4.3 Zero - shot Instruction Compositionality   Our CT0 model has learned effectively to process   different instructions in specific contexts : word   level constraint in the context of headline genera-   tion , or an emotional tone in the context of dialogue .   Does CT0 understand these instructions in different   contexts ? To answer this question , and to explore   whether CT0 can learn instruction compositionality   we conduct several experiments .   Zero - Shot Constraint . In Table 4 we explore   how our model succeeds in understanding con-   straint instructions beyond the one it was exposed   during training . Our model was trained on Head-   line Generation with Constraint ( HGen ) instruc-   tions with only one match , such as Make a title for   this article containing “ X ” . To test generalization ,   we prompt our CT0 model with unseen instructions   with 2 and 3 matches , such as Make a title for this   article containing “ X ” and “ Y " , orMake a title   for this article containing “ X ” and “ Y " and “ Z " .   We also compose instructions from constraint and   Twitter Stylometry resulting in instructions such as   Write a tweet about X , in the style of Y , containing Z .   CT0 respects the Contain constraint 77 % for n= 1 .   The score naturally drops when n > 1 , however   the satisfiablity is still 50 % of the time for n= 2   and 40 % for n= 3 . As expected , the ROUGE-   1 score also improves : NoCons : 30.2 , # Cons=1 :   38.9 , # Cons=2 : 43.9 and # Cons=3 : 47.4 . When   we compose HGen and TwSt , CT0 also performs   significantly better compared to CT0 ( 46.4   vs. 10.7 ) .   Zero - Shot Emotional Haiku . We explore   whether combining an emotion with the Haiku   instructions would help control the haiku gener-   ation . Note that during training , only the task   of Empathetic Dialogue has been exposed to   emotion . Our results , reported in Figure 3 , indicate   that CT0 is able to combine an emotion with the   Haiku instructions in a zero - shot setting . For   instance , given the following new instruction   Generate a haiku about “ held my hand ” . The   associated emotion is “ faithful ” . , our model   output is “ He held my hand through thick and   thin , Through sickness and health , through life and   death ” . A qualitative analysis also shows that CT0   understands subtle nuances ; for instance given as   input Generate a haiku about “ Seagulls crying   high ” . The associated emotion is “ nostalgic ” . our   model output is “ Seagulls crying high , A familiar   scene , from a childhood Now ” .   These are promising results regarding CT0 ’s abil-   ity to comprehend new instructions , including in-   struction composition . While contemporaneous   work by Nayak et al . ( 2022 ) propose a novel form   of soft prompting for compositional zero - shot learn-   ing we show that a continually fine - tuned language   model is able to perform the same .   5 Discussion   5.1 Why could LLMs be lifelong learners ?   Given our current experimental protocol , one can   draw different hypotheses : is CL a consequence   emerging from the massive multi - task pre - training   in T0 ? Or from the instruction tuning paradigm   of T0 ? Or from the scaling law as studied by Ra-   masesh et al . ( 2021 ) ? To answer this research ques-   tion , we applied the same CL setup starting from   1 ) T5 - small , 2 ) T5 - 3B , and 3 ) a T5 - 3B architec-   ture randomly initialised . Our results in Table 5   show that CT 5with 3B parameters performs sim-   ilar to CT 03B on the 8 tasks . While CT 5 - small   obtains as expected a lower average performance ,   it still mostly maintains great results w.r.t . its Upper   Bound , indicating that CL does not emerge from   scale . Conversely , when initialised randomly the   model is not even able to obtain a good UB . These   results draw a clear conclusions : CL emerges from   the intensive pre - training stage . This confirms   contemporaneous findings by Cossu et al . ( 2022)6113   HGen TwSt   # Cons 1 2 3 1   CT0 77.0 56.4 39.5 46.4   CT0 33.6 15.4 8.1 10.7   and Mehta et al . ( 2021 ) in other setups and even   modalities . We report the detailed results for those   experiments in the Appendix .   5.2 Toward Concept Drift   In the original CovidQA the task consists of an-   swering a question present in a given paragraph . In   this setup , one can arguably succeed into answer-   ing questions about COVID by transferring the task   knowledge , even without particular domain knowl-   edge about COVID . In our paper , we intentionally   chose to not provide the context for CQA but only   the question . This alternative setup corresponds   to learning by heart the answer to a question . Our   results in Table 3 show that while we framed CQA   as a new task to learn , our proposed setup also   opens new way to tackle concept drift , by directly   incorporating knowledge into a model .   5.3 Data Efficiency   Our method based on rehearsal learning is simple   yet efficient . While the complexity in term of data   storage and training is not constant ( O(1 ) ) , withonly 1 % of the previous training data we are able   to retain model abilities . This result is still data and   computationally efficient , compared to the standard   approach of retraining the model from scratch on   all tasks . In cases where the number of tasks to   learn would grow by several order of magnitude ,   more sophisticated methods could be explored . We   leave this for future research .   6 Conclusion   We explored for the first time Continual Learning   for instruction - based models . Our results indicate   thatfine - tuned Language Models are efficient con-   tinual learners : 1 % rehearsal is enough to maintain   a high performance on previously learned tasks ,   while learning new ones . Additionally , we show   that our model CT0 is able to comprehend new   instructions obtained via instruction composition .   The current technique to learn multiple tasks is to   train a model from scratch . We hope this work   paves the way toward a new paradigm where mod-   els do not have to be retrained all over again . We   believe our experimental findings will contribute   to the effectiveness of large language models , en-   abling them to progressively adapt to new concepts   and acquire more and more abilities . As an analogy   with Software Development , this could be seen as   learning new features . New checkpoints are like   new versions of a model . In this context , Continual   Learning will help toward the Call to Build Models   Like We Build Open - Source Software .6114   Acknowledgements   We would like to thank the anonymous reviewers   for their helpful comments . Tuhin is funded by   Columbia Center of Artifical Intelligence & Tech-   nology ( CAIT ) and the Amazon Science Ph.D. Fel-   lowship ) .   Limitations   As discussed in 5.3 , CL with rehearsal still requires   to use a buffer of data previously seen which limits   several scenarios where those data would not be   available anymore . While we have done our bestto select numerous and diverse tasks in this paper ,   it still represents a limited set . Would our results   still hold given hundred or thousand tasks ? In other   modalities ? It should also be noted that our study   is limited to English - only datasets , as we started   from T0 which is not multilingual in nature . Addi-   tionally while results using automatic metrics give   a fair idea of task performance and measuring CL   abilities , we would like to conduct a human evalua-   tion in near future although its expensive give the   size of test data and the number of tasks   Ethics Statement   Although we use language models trained on data   collected from the Web , which have been shown to   have issues with gender bias and abusive language ,   the inductive bias of our models should limit in-   advertent negative impacts . Unlike model variants   such as GPT , T5 is a conditional language model ,   which provides more control of the generated out-   put . We have verified carefully that our training   or evaluation data does not contain any toxic text   and it underwent manual inspection by the authors   and experts . We also believe our work in contin-   ual learning is a step towards data efficiency and   conservation of computing resources , as one saves   training time by only using 1 % rehearsal   References61156116611761187 Appendix   7.1 Tasks Order   The task order has been selected 1 ) randomly   among the three first tasks Text Simplifiction , Head-   line Generation with Constraint and Haiku Gen-   eration , and 2 ) in light of the actual success , we   progressively kept adding new tasks . This setup   corresponds to a realistic usage of our proposed   method , where future tasks were thus unknown   even for us . To assess a potential impact of the   order , we also conduct an alternative experiment   with our 3B model , where the order is reversed . We   did not experimented further different orders due   to the high computation required .   7.2 Tasks   In this section , we describe all the tasks Tused   to progressively train and evaluate our model . For   all the new tasks ( i.e. , not the T0 tasks ) , we also   designed instructions , as illustrated in Table 2 .   7.3 Automatic Metrics   7.3.1 New Tasks   All of our newly introduced tasks are language gen-   eration tasks in contrast to the T0 evaluation tasks   and majority of the T0 training tasks ( all except   summarization ) .   Text Simplification ( Simpl ) Jiang et al . ( 2020 )   provided WikiAuto , a set of 400,000 aligned sen-   tences from English Wikipedia and Simple English   Wikipedia as a resource to train sentence simpli-   fication systems . The test set contains 4,000 ex-   amples . In addition , we also evaluate our models   on a second Text Simplification dataset , ASSET   ( Alva - Manchego et al . , 2020 ) . This is a dataset ded-   icated for the evaluation of sentence simplification   in English , providing 2,000 multiple references per   example , unlike previous simplification datasets .   Table 2 shows our designed instructions for this   task .   Headline Generation with Constraint ( HGen ) .   While writing a title for a news article , it can be   very useful to add additional constraints , such as   the presence of certain words . However , traditional   decoding strategies like the BeamSearch often fail   to achieve this goal as discussed in 4 . Gigaword is   one of T0 training dataset . Our new task consists   of generating a title given a news article with addi-   tional constraints . Towards this goal , for a given   document D and an input keyword X we design thefollowing three instructions : [ Make a title for this   article , starting with /ending with /that contains   “ X ” : D where X is a word we want to be present   in the output text at the beginning / end / anywhere ,   and D the source document , as illustrated in Table   2 . To create the training data , we simply leverage   the gold - reference to select the word X , such that   our model is trained with consistent and plausible   instructions . Gigaword contains millions of train-   ing examples . The original test set is composed of   1,951 examples , so we convert it to 3 sets of 1,951   examples for our Start / End / Contain instructions ,   respectively .   Haiku Generation ( Haiku ) . For the task of   haiku generation , we crawl10,718 haikus with   at least 1 up - vote from the Subreddit haiku , and   split it in 9,742 and 974 example for the train and   test sets , respectively . Table 2 shows an example in-   struction for Haiku Generation about a given topic .   Covid QA ( CQA ) Möller et al . ( 2020 ) created   COVID - QA , a Question Answering dataset con-   sisting of 2,019 question / answer pairs annotated   by volunteer biomedical experts on scientific arti-   cles related to COVID-19 . We consider this dataset   since to the best of our knowledge , T0 has never   been exposed to any COVID-19 related data . In its   original version , the dataset is framed as SQuAD   ( Rajpurkar et al . , 2016 ) , with triplets ( context , ques-   tion , answer ) , where the context contains the an-   swer . Because T0 has been extensively trained on   QA dataset , CovidQA in its original format simply   requires domain transfer . To make the task more   challenging , we propose to provide only the ques-   tion as an input , now framing the task as “ learn   the answer by heart ” in an encyclopedia style task .   This way the task framing can be seen as a new   strategy to incorporating knowledge and prevent-   ing the model from concept drift .   Inquisitive Question Generation ( InqQG ) To   foster long form question answering Fan et al .   ( 2019 ) created the ELI5 dataset that comprises   270,000 English - language threads from the Reddit   forum of the same name , where an online com-   munity provides answers to several open ended   inquisitive questions . Table 2 shows an example   instruction in order to generate inquisitive ques-6119tions . As opposed to standard Question Generation   based on SQuAD , ELI5 enables open - ended ques-   tions , closer to human - style questions ( Scialom and   Staiano , 2020 ) . We filtered out the Reddit threads   to keep only well formed questions , resulting in   61,710 and 1,681 examples for the training and test   set , respectively .   Empathetic Dialogue Generation ( EmDg )   Rashkin et al . ( 2019 ) proposed a benchmark for   empathetic dialogue generation by creating a   dataset of conversations grounded in emotional   situations . Each example in the dataset contains an   input emotion , situation in which dialogue appears   and the entire conversation . We display in Table   2 the corresponding instruction . At the example   level , our training and test datasets contain 58,770   and 8,396 examples , respectively .   Explanation Generation ( Exp ) . The Stanford   Natural Language Inference dataset consists of a   classification task , where given a Premise(P ) and   an Hypothesis(H ) , the model has to chose between   3 options : entailed , contradiction or not related .   Camburu et al . ( 2018 ) extend this NLI dataset by   annotating the explanations of the label in natural   language . In our paper , we consider as input the   Premise(P ) , the Hypothesis(H ) , and the label , and   train our model to generate the explanation . The   dataset is composed of 100,000 and 9,824 train and   test examples , respectively .   Twitter Stylometry ( TwSt ) Tareaf ( 2017 ) ex-   tracted tweets from the top 20 most followed users   in Twitter social platform , including singers such as   Katy Perry or Selena Gomez , as well as the official   account of Barack Obama when he was president of   the USA . The style for tweets largely differs from   one account to an another , e.g. @BarackObama :   “ It ’s time to # ActOnClimate ” vs. @KimKardashian :   “ makes me want to go back blonde but i ’m scared   it will ruin my hair :-( ” . We define the Stylome-   try task as generating a relevant tweet given i ) a   hashtag , and ii ) the tweet ’s author . We thus se-   lected only tweets containing hashtags ( # ) from the   original dataset , resulting in a total of 13,041 and   250 examples for train and test sets , respectively .   We display at the bottom of Table 2 an example   instruction for this task.7.4 Automatic Metrics   T0 zero - shot evaluation set ( see Section 3.3 ) only   contains tasks framed as classification . For T0   evaluation , Sanh et al . ( 2022 ) compute the loglike-   lihood of each of the target options , and the option   with the highest log - likelihood is selected as the   prediction . This strategy holds when restricting   the evaluation to classification tasks . However , in   the context of an open - ended model able to per-   form NLG tasks , a user is interested in the actual   output of the model rather than probabilities . We   therefore report the accuracy of the prediction com-   pared to the ground - truth answer for all those tasks .   This measure is more conservative , as it requires   an exact match .   In the context of Continual Learning , we also   suspect that using only a comparison of the log-   likelihood of respective classes would not reflect   the actual model ’s memory , since the decoders are   known to suffer from catastrophic forgetting more   than the encoders ( Riabi et al . , 2021 ) .   Standard NLG Metrics . For the standard tasks ,   we rely on widely used metrics : ROUGE ( Lin ,   2004 ) for Summarization ; BLEU ( Papineni et al . ,   2002 ) and SARI ( Xu et al . , 2016 ) for Simplifica-   tion . In this paper , we also include open - domain   NLG tasks , such as Dialogue or Explanation gen-   eration . The space of possible correct outputs is   too large in this case to rely on n - gram based met-   rics like BLEU or ROUGE . For this reason , we   report BERTScore ( Zhang et al . , 2020 ) to measure   the similarity between a prediction and its gold-   reference in those tasks .   When possible , we also designed customized   metrics that are better suited for the task .   Customized NLG Metrics .   •Constraint : For our prompts with constraint ,   such as “ Write a text that starts / contains / ends   with [ some word ] ” , we also report the accuracy   of respecting the constraint . Concretely , an out-   put is correct only if it contains the [ word ] at the   right location : the beginning for start , the end for   end ; any location for contain .   •First Word Distribution ( 1Tok ) . In ELI5 , the ques-   tions are supposed to be inquisitive , not factual   like in SQuAD . Therefore , the distribution of the6120first words is very informative . For instance , the   percentage of questions starting with “ why / how ”   is more important than “ what ” . We therefore rely   on the Jensen Shannon Divergence between the   first words distributions of the ground truth ex-   amples and our predictions . We report its inverse ,   so the higher the better .   •Author Classification ( Clf ) In Twitter Stylome-   try , the author is part of the input , so the gener-   ated tweet is aligned with the author ’s style . To   measure this condition , we train a classifier on   the dataset , with the tweets as inputs , and the   corresponding author names as target categories .   We trained a Ridge Classifier using scikit - learn   ( Pedregosa et al . , 2011 ) , and obtained 0.81 % ac-   curacy . This high accuracy allows this Clf metric   to be informative enough .   •HHaiku is a type of short form poetry orig-   inally from Japan as illustrated in the Table 2 .   In general , it contains only 17 syllables , broken   up into three lines . We calculate two differences   between the prediction and the ground - truth : i )   for the number of lines , and ii ) for the number   of syllables . Hcorresponds to the average of   these two differences , BLEU and the Constraint   satisfiability ( i.e. , if the generated haiku contains   the topic phrase X that was present in the instruc-   tion ) .   7.5 Evaluation for T0 Train Set   Because there are 50 datasets with thousands of ex-   amples in the test sets per task , evaluating on each   examples would be computationally intensive . For   this reason we restricted this set to 1000 examples   randomly sampled from all the examples in the test   sets . Because the set contains both NLG and NLU   tasks , using the accuracy is not enough . For sim-   plicity we used therefore ROUGE-1 which allows   is consistent with accuracy for NLU tasks but also   allows to take into account NLG evaluation ,   7.6 Additional Results   In the main paper , Table 5 we reported the addi-   tional results when starting from T5 and a random   transformer . These results are discussed in the first   section of our Discussion .   In Table 7 we report the progressive results , and   not just the initial checkpoint , the Upper Bound   and the final model.7.7 Implementation Details   For all our experiments woth T0_3B and T0pp ,   we instantiate our model with the T0 model ( Sanh   et al . , 2022 ) using the official implementation .   For fine - tuning T0_3B , we used the same hyper-   parameters as the ones reported in Sanh et al .   ( 2022 ): all the details from the batch - size to the   learning rate are provided in details here .   The only new hyper - parameter introduced in our   paper is the rehearsal proportion r. We explored   r∈[0,0.25%,1 % ] as reported in our first set of   results .   For each task , we consider 100,000 examples   for training , such that 1 % rehearsal corresponds to   1,000 examples from the memory buffer . Thus , for   datasets with fewer training examples , we upsam-   ple them and conversely for largest datasets like   Gigaword or Simplification , we limit to 100,000   examples .   When we scaled our best setup to the 11B param-   eters version of T0 , T0pp , we observed instability   in validation performance . Thus , we changed the   learning rate from 1e-3 to 1e-4 as well as the opti-   mizer to AdamW instead of Adafactor for all our   11B experiments . All the other hyper - parameters   remain similar to the 3B model .   For the T5 ablations , we again used the Hug-   ging Face implementationsand applied the same   hyper - parameters as above .   At inference time , we use greedy decoding , i.e.   a Beam Search with K= 1.61216122