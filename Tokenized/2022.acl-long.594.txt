  Yilin Zhao , Hai Zhao , Libin Shen , Yinggong ZhaoDepartment of Computer Science and Engineering , Shanghai Jiao Tong UniversityKey Laboratory of Shanghai Education Commission for Intelligent Interaction   and Cognitive Engineering , Shanghai Jiao Tong University , Shanghai , ChinaLeyan Tech , Shanghai , China   zhaoyilin@sjtu.edu.cn , zhaohai@cs.sjtu.edu.cn   libin@leyantech.com , ygzhao@leyantech.com   Abstract   As a broad and major category in machine   reading comprehension ( MRC ) , the generalized   goal of discriminative MRC is answer predic-   tion from the given materials . However , the   focuses of various discriminative MRC tasks   may be diverse enough : multi - choice MRC   requires model to highlight and integrate all   potential critical evidence globally ; while ex-   tractive MRC focuses on higher local bound-   ary preciseness for answer extraction . Among   previous works , there lacks a unified design   with pertinence for the overall discriminative   MRC tasks . To fill in above gap , we pro-   pose a lightweight POS - Enhanced Iterative   Co - Attention Network ( POI - Net ) as the first   attempt of unified modeling with pertinence ,   to handle diverse discriminative MRC tasks   synchronously . Nearly without introducing   more parameters , our lite unified design brings   model significant improvement with both en-   coder and decoder components . The evalua-   tion results on four discriminative MRC bench-   marks consistently indicate the general effec-   tiveness and applicability of our model , and   the code is available at https://github .   com / Yilin1111 / poi - net .   1 Introduction   Machine reading comprehension ( MRC ) as a chal-   lenging branch in NLU , has two major categories :   generative MRC which emphasizes on answer gen-   eration ( Ko ˇciský et al . , 2018 ) , and discriminative   MRC which focuses on answer prediction from   given contexts ( Baradaran et al . , 2020 ) . Among   them , discriminative MRC is in great attention of   researchers due to its plentiful application scenar-   ios , such as extractive and multi - choice MRC two   major subcategories . Given a question with cor-   responding passage , extractive MRC asks for pre-   cise answer span extraction in passage ( Joshi et al . ,   Table 1 : Different focuses of multi - choice MRC task   ( RACE ) and extractive MRC task ( SQuAD 2.0 ) . Texts   in bold are the critical information or fallibility parts .   2017 ; Trischler et al . , 2017 ; Yang et al . , 2018 ) ,   while multi - choice MRC requires suitable answer   selection among given candidates ( Huang et al . ,   2019 ; Khashabi et al . , 2018 ) . Except for the only   common goal shared by different discriminative   MRCs , the focuses of extractive and multi - choice   MRC are different to a large extent due to the diver-   sity in the styles of predicted answers : multi - choice   MRC usually requires to highlight and integrate all   potential critical information among the whole pas-   sage ; while extractive MRC pays more attention   to precise span boundary extraction at local level ,   since the rough scope of answer span can be located   relatively easily , shown in Table 1 .   In MRC field , several previous works perform   general - purpose language modeling with consid-   erable computing cost at encoding aspect ( Devlin   et al . , 2019 ; Clark et al . , 2020 ; Zhang et al . , 2020c ) ,   or splice texts among diverse MRC tasks simply   to expand training dataset ( Khashabi et al . , 2020 ) ,   without delicate and specialized design for sub-8682   categories in discriminative MRC . Others utilize   excessively detailed design for one special MRC   subcategory at decoding aspect ( Sun et al . , 2019b ;   Zhang et al . , 2020a ) , lacking the universality for   overall discriminative MRC .   To fill in above gap in unified modeling for dif-   ferent discriminative MRCs , based on core focuses   of extractive and multi - choice MRC , we design   two complementary reading strategies at both en-   coding and decoding aspects . The encoding de-   sign enhances token linguistic representation at   local level , which is especially effective for ex-   tractive MRC . The explicit possession of word   part - of - speech ( POS ) attribute of human leads to   precise answer extraction . In the extractive sam-   ple from Table 1 , human extracts golden answer   span precisely because “ London Exhibition ” is a   proper noun ( NNP ) corresponding to interrogative   qualifier ( WDT ) “ Where ” in the question , while im-   precise words like “ 1862 ” ( cardinal number , CD )   and “ exhibited ” ( past tense verb , VBD ) predicted   by machines will not be retained . Thus , we inject   word POS attribute explicitly in embedding form .   The decoding design simulates human recon-   sideration andintegration abilities at global level ,   with especial effect for multi - choice MRC . To han-   dle compound questions with limited attention , hu-   man will highlight critical information in turns ,   and update recognition and attention distribution   iteratively . Inspired by above reconsideration strat-   egy , we design Iterative Co - Attention Mechanism   with no additional parameter , which iteratively exe - cutes the interaction between passage and question-   option ( Q−O ) pair globally in turns . In the multi-   choice example from Table 1 , during the first inter-   action , model may only focus on texts related to   rough impression of Q−Opair such as “ Green   Scenes ” , ignoring plentiful but scattered critical in-   formation . But with sufficient iterative interaction ,   model can ultimately collect all detailed evidence   ( bold in Table 1 ) . Furthermore , we explore a se-   ries of attention integration strategies for captured   evidence among interaction turns .   We combine two above methods and propose   a novel model called POI - Net ( POS - Enhanced   Iterative Co - Attention Network ) , to alleviate the   gap between machines and humans on discrimina-   tive MRC . We evaluate our model on two multi-   choice MRC benchmarks , RACE ( Lai et al . , 2017 )   and DREAM ( Sun et al . , 2019a ) ; and two extractive   MRC benchmarks , SQuAD 1.1 ( Rajpurkar et al . ,   2016 ) and SQuAD 2.0 ( Rajpurkar et al . , 2018 ) , ob-   taining consistent and significant improvements ,   with nearly zero additional parameters .   2 Our Model   We aim to design a lightweight , universal and effec-   tive model architecture for various subcategories   of discriminative MRC , and the overview of our   model is shown in Figure 1 , which consists of   four main processes : Encoding ( § 2.1 ) , Interaction   ( § 2.2 ) , Integration ( § 2.3 ) and Output ( § 2.4).8683   2.1 POS - Enhanced Encoder   Based on pre - trained contextualized encoder AL-   BERT ( Lan et al . , 2020 ) , we encode input tokens   with an additional POS embedding layer , as Fig-   ure 2 shows . Since the input sequence will be   tokenized into subwords in the contextualized en-   coder , we tokenize sequences in word - level with   nltktokenizer ( Bird et al . , 2009 ) additionally and   implement POS - Enhanced Encoder , where each   subword in a complete word will share the same   POS tag .   In detail , input sequences are fed into nltkPOS   tagger to obtain the POS tag of each word such as   “ JJ ” . Subject to Penn Treebank style , our adopted   POS tagger has 36 POS tag types . Considering   on the specific scenarios in discriminative MRC ,   we add additional SPE tag for special tokens ( i.e. ,   [ CLS ] , [ SEP ] ) , PAD tag for padding tokens and   ERR tag for potential unrecognized tokens . Ap-   pendix A shows detailed description of POS tags .   The input embedding in our model is the nor-   malized sum of Subword Embedding andPOS Em-   bedding . Following the basic design in embedding   layers of BERT - style models , we retain Token Em-   bedding E , Segmentation Embedding Eand Posi-   tion Embedding Ein subword - level , constituting   Subword Embedding . For POS Embedding E ,   we implement another embedding layer with the   same embedding size to Subword Embedding , guar-   anteeing all above indicator embeddings are in the   same vector space . Formulaically , the input embed-   dingEcan be represented as :   E = Norm ( E+E+E+E ) ,   where Norm ( ) is a layer normalization function   ( Ba et al . , 2016 ) .   2.2 Iterative Co - Attention Mechanism   POI - Net employs a lightweight Iterative Co-   Attention module to simulate human inner recon-   sidering process , with noadditional parameter.2.2.1 Preliminary Interaction   POI - Net splits all Ninput token embeddings into   passage domain ( P ) and question ( or Q−Opair )   domain ( Q ) to start P−Qinteractive process . To   generate the overall impression of the given pas-   sage or question like humans , POI - Net concen-   trates all embeddings in corresponding domain into   oneConcentrated Embedding by max pooling :   CE = MaxPooling ( E , ... , E)∈R ,   CE = MaxPooling ( E , ... , E)∈R ,   where His the hidden size , PN / QN is the token   amount of P / Q domain . Then POI - Net calculates   the similarity between each token in E / Eand   CE / CE , to generate attention score sfor each   token contributing to the P−Qpair . In detail , we   use cosine similarly for calculation :   s , ... , s = Cosine ( [ E , ... , E ] , CE ) ,   s , ... , s = Cosine ( [ E , ... , E ] , CE ) .   We normalize these scores to [ 0,1]by min - max   scaling , then execute dot product with correspond-   ing input embeddings :   E= ˆs·E , E= ˆs·E ,   where ˆsis the normalized attention score of i-   th passage token embedding , Eis the attention-   enhanced embedding of i - th passage token after   preliminary interaction ( the 1 - st turn interaction ) .   2.2.2 t - th Turn Interaction   To model human reconsideration ability between   passage and question in turns , we add iterable mod-   ules with co - attention mechanism , as the Iterative   Interaction Layer in Figure 1 . Detailed processes   in the t - th turn interaction are similar to preliminary   interaction :   CE = MaxPooling ( E , ... , E)∈R ,   CE = MaxPooling ( E , ... , E)∈R,8684s , ... , s = Cosine ( [ E , ... , E ] , CE ) ,   s , ... , s = Cosine ( [ E , ... , E ] , CE ) ,   E= ˆs·E , E= ˆs·E.   Note that , during all iteration turns , we calculate   attention scores with the original input embedding   Einstead of attention - enhanced embedding E   from the ( t-1)-th turn , due to :   1 ) There is no further significant performance   improvement by replacing EwithE(<0.2 %   on base size model ) , comparing to adopted method ;   2 ) With the same embedding E , attention integra-   tion in § 2.3 can be optimized into attention score   integration , which is computationally efficient with   no additional embedding storage .   2.3 Attention Integration   Human recommends to integrate all critical in-   formation from multiple turns for a comprehen-   sive conclusion , instead of discarding all findings   from previous consideration . In line with above   thought , POI - Net returns attention - enhanced em-   bedding E= ˆs·Eof each turn ( we only store   ˆsin an optimized method ) , and integrates them   with specific strategies . We design four integra-   tion strategies according to the contribution pro-   portion of each turn and adopt Forgetting Strategy   ultimately .   •Average Strategy : The attention network   treats normalized attention score ˆsof each   turn equally , and produces the ultimate rep-   resentation vector Rwith average value of   ˆs :   R=1   TXˆs·E∈R ,   where Tis the total amount of iteration turns .   •Weighted Strategy : The attention network   treats ˆswith two normalized weighted coeffi-   cients β , β , which measure the contribution   of the t - th turn calculation :   R = PβˆsPβ·E+PβˆsPβ·E ,   ˜β = Max ( s , ... , s ) ,   ˜β = Max ( s , ... , s),β=˜β+ 1   2 , β=˜β+ 1   2 ,   where s = s= 1.0 . The design moti-   vation for β , βis intuitive : when Concen-   trated Embedding CE / CE(calculating at-   tention score at the t - th turn ) has higher con-   fidence ( behaving as higher maximum value   ins / sdue to max pooling calculation ) ,   system should pay more attention to input em-   bedding E / Eat the t - th turn .   •Forgetting Strategy : Since human will partly   forget knowledge from previous considera-   tion and focus on findings at current turn , we   execute normalization operation of attention   scores from two most previous turns itera-   tively :   R = s+βˆs   1 + β·E+s+βˆs   1 + β·E ,   s = s+βˆs   1 + β ,   s = s+βˆs   1 + β .   During the iterative normalization , the ulti-   mate proportion of attention scores from pre-   vious turns will be diluted gradually , which   simulates the effect of forgetting strategy .   •Intuition Strategy : In some cases , human   can solve simple questions in intuition with-   out excessive consideration , thus we introduce   two attenuation coefficients α , αfor atten-   tion scores from the t - th turn , which decrease   gradually as the turn of iteration increases :   R = PαˆsPα·E+PαˆsPα·E ,   α = Yβ , α = Yβ.86852.4 Adaptation for Discriminative MRC   2.4.1 Multi - choice MRC   The input sequence for multi - choice MRC is   [ CLS ] P[SEP ] Q+O[SEP ] , where + de-   notes concatenation , Odenotes the i - th answer   options . In Output Layer , the representation vector   R∈Ris fed into a max pooling operation to   generate general representation :   R = MaxPooling ( R)∈R.   Then a linear softmax layer is employed to calcu-   late probabilities of options , and standard Cross   Entropy Loss is employed as the total loss . Option   with the largest probability is determined as the   predicted answer .   2.4.2 Extractive MRC   The input sequence for extractive MRC can be rep-   resented as [ CLS ] P[SEP ] Q[SEP ] , and we   use a linear softmax layer to calculate start and end   token probabilities in Output Layer . The training   object is the sum of Cross Entropy Losses for the   start and end token probabilities :   L = y·log(s ) + y·log(e ) ,   s , e = softmax ( Linear ( R))∈R ,   where s / eare the start / end probabilities for all to-   kens and y / yare the start / end targets .   For answer prediction , since some benchmarks   have unanswerable questions , we first score the   span from the i - th token to the j - th token as :   score = s+e,0≤i≤j≤N ,   then the span with the maximum score scoreis   the predicted answer . The score of null answer is :   score = s+e , where the 0 - th token is [ CLS ] .   The final score is calculated as score−score ,   and a threshold δis set to determine whether the   question is answerable , which is heuristically com-   puted in linear time . POI - Net predicts the span   with the maximum score if the final score is above   the threshold , and null answer otherwise .   3 Experiments   3.1 Setup & Dataset   The experiments are run on 8NVIDIA Tesla   P40 GPUs and the implementation of POI - Net is   based on the Pytorch implementation of ALBERT(Paszke et al . , 2019 ) . We set the maximum itera-   tion turns in Iterative Co - Attention as3 . Table 2   shows the hyper - parameters of POI - Net achieving   reported results . As a supplement , the warmup rate   is 0.1 for all tasks .   We evaluate POI - Net on two multi - choice MRC   benchmarks : RACE ( Lai et al . , 2017 ) , DREAM   ( Sun et al . , 2019a ) , and two extractive MRC bench-   marks : SQuAD 1.1 ( Rajpurkar et al . , 2016 ) and   SQuAD 2.0 ( Rajpurkar et al . , 2018 ) . The detailed   introduction is shown as following :   RACE is a large - scale multi - choice MRC task   collected from English examinations which con-   tains nearly 100 K questions . The passages are in   the form of articles and most questions need con-   textual reasoning , and the domains of passages are   diversified .   DREAM is a dialogue - based dataset for multi-   choice MRC , containing more than 10 K questions .   The challenge of the dataset is that more than 80 %   of the questions are non - extractive and require rea-   soning from multi - turn dialogues .   SQuAD 1.1 is a widely used large - scale ex-   tractive MRC benchmark with more than 107 K   passage - question pairs , which are produced from   Wikipedia . Models are asked to extract precise   word span from the Wikipedia passage as the an-   swer of the given passage .   SQuAD 2.0 retains the questions in SQuAD 1.1   with over 53 K unanswerable questions , which are   similar to answerable ones . For SQuAD 2.0 , mod-   els must not only answer questions when possible ,   but also abstain from answering when the question   is unanswerable with the paragraph .   3.2 Results   We take accuracy as evaluation criteria for multi-   choice benchmarks , while exact match ( EM ) and8686   a softer metric F1 score for extractive benchmarks .   The average results of three random seeds are   shown in Table 3 , where we only display several   BERT - style models with comparable parameters .   Appendix B reports the complete comparison re-   sults with other public works on each benchmark .   The results show that , for multi - choice bench-   marks , our model outperforms most baselines and   comparison works , and passes the significance test   ( Zhang et al . , 2021 ) with p−value < 0.01 in   DREAM ( 2.0 % average improvement ) and RACE   ( 1.7 % average improvement ) . And for extractive   benchmarks , though the performance of baseline   ALBERT is strong , our model still boosts it es-   sentially ( 1.3 % average improvement on EM for   SQuAD 1.1 and 2.3 % for SQuAD 2.0 ) . Further-   more , we report the parameter scale and train-   ing / inference time costs in § 4.4 .   4 Ablation Studies   In this section , we implement POI - Net on   ALBERTfor further discussions , and such set-   tings have the similar quantitative tendency to POI-   Neton ALBERT .   4.1 AblationTo evaluate the contribution of each component   inPOI - Net , we perform ablation studies on RACE   and SQuAD 1.1 development sets and report the   average results of three random seeds in Table 4 .   The results indicate that , both POS Embedding and   Iterative Co - Attention Mechanism provide consid-   erable contributions to POI - Net , but in different   roles for certain MRC subcategory .   For multi - choice MRC like RACE , Iterative Co-   Attention Mechanism contributes much more than   POS Embedding ( 3.86 % v.s. 1.14 % ) , since multi-   choice MRC requires to highlight and integrate   critical information in passages comprehensively .   Therefore , potential omission of critical evidence   may be fatal for answer prediction , which is guar-   anteed by Iterative Co - Attention Mechanism , while   precise evidence span boundary and POS attributes   are not as important as the former .   On the contrary , simple POS Embedding even   brings a little more improvement than the well-   designed Iterative Co - Attention ( 0.99 % v.s. 0.85 %   on EM ) for extractive MRC . In these tasks , model   focuses on answer span extraction with precise   boundaries , and requires to discard interference   words which not exactly match questions , such as   redundant verbs , prepositions and infinitives ( “ po-   litically and socially unstable ” instead of “ to be   politically and socially unstable ” ) , or partial inter-   ception of proper nouns ( “ Seljuk Turks ” instead   of “ Turks ” ) . With the POS attribute of each word ,   POI - Net locates the boundaries of answer spans   precisely . Since extractive MRC does not require   comprehensive information integration like multi-8687choice MRC , the improvement from Iterative Co-   Attention Mechanism is less significant .   Besides , we also implement POI - Net on other   contextualized encoders like BERT , and achieve   significant improvements as Table 4 shows . The   consistent and significant improvements over vari-   ous baselines verify the universal effectiveness of   POI - Net .   4.2 Role of POS Embedding   To study how POS Embedding enhances token   representation , we make a series of statistics on   SQuAD 1.1 development set about : 1 ) POS type of   boundary words from predicted spans , as Table 5   shows ; 2 ) error POS classification of POI - Net and   its baseline ALBERT , as Figure 3 shows .   The statistical results show , with POS Embed-   ding , the overall distribution of the POS types of   answer boundary words predicted by POI - Net is   more similar to golden answer , compared with its   baseline ; and the amount of error POS classifica-   tion cases by POI - Net also reduces significantly .   And there are also two further findings:1 ) The correction proportion of error POS clas-   sification ( 8.09 % ) is much higher than correction   proportion of overall error predictions ( 1.82 % ) in   POI - Net , which indicates the correction of POS   classification benefits mostly from the perception   of word POS attributes by POS Embedding , instead   of the improvement on overall accuracy .   2 ) Though answers in SQuAD 1.1 incline to dis-   tribute in several specific POS types ( “ NN ” , “ CD ” ,   “ NNS ” and “ JJ ” ) , POS Embedding prompts model   to consider words in each POS type more equally   than the baseline , and the predicted proportions   of words in rarer POS type ( “ IN ” , “ VBN ” , “ RB ” ,   “ VBG ” and so on ) increase .   4.3 Research on the Robustness of POS   Embedding   Robustness is one of the important indicators to   measure model performance , when there is numer-   ous rough data or resource in applied tasks . To   measure the anti - interference of POS Embedding ,   we randomly modify part of POS tags from nltk   POS tagger to error tags , and the results on SQuAD   1.1 development set are shown in Table 6 .   The results indicate that , POI - Net possesses sat-   isfactory POS Embedding robustness , and the im-   provement brought by POS Embedding will not   suffer a lot with a slight disturbance ( 5 % ) . We   argue that the robustness of POI - Net may bene-   fit from the integration with other contextualized   embeddings , such as Token Embedding Ewhich   encodes the contextual meaning of current word or   subword . Though more violent interference ( 20 % )   may further hurt token representations , existing ma-   ture POS taggers achieve 97 % + accuracy , which   can prevent the occurrence of above situations .   4.4 Role of Iterative Co - Attention Mechanism   To explore the most suitable integration strat-   egy and maximum iteration turn in Iterative Co-   Attention Mechanism , we implement our proposed   strategies with different maximum iteration turns,8688together with a baseline replacing Iterative Co-   Attention mechanism by a widely used Multi-   head Co - Attention mechanism ( Devlin et al . , 2019 ;   Zhang et al . , 2020a , 2021 ) for comparison in Fig-   ure 4 . We take RACE as the evaluated benchmark   due to the significant effect of attention mechanism   to multi - choice MRC .   As the figure shows , forgetting strategy leads to   the best performance , with slight improvement than   weighted strategy . Both these two strategies are in   line with the logical evidence integration in human   reconsidering process , while average strategy and   intuition strategy may work against common hu-   man logic . From the trends of four strategies in   multiple iterations , we conclude that 2 or 3 iter-   ation turns for Iterative Co - Attention lead to an   appropriate result , due to :   1 ) Fewer iteration turns may lead to inadequate   interaction between passage and question , and   model may focus on rough cognition instead of   exhaustive critical information ;   2 ) Excessive iteration turns may lead to over-   integration of information , declining the contribu-   tion by real critical evidence .   Compared to the typical Multi - head Co-   Attention mechanism , our proposed Iterative Co-   Attention mechanism obtains higher performance   with more iterations , indicating it has stronger iter-   ative reconsideration ability .   Besides , Iterative Co - Attention defeats Multi-   head Co - Attention on both parameter size and train-   ing time cost . As the parameter comparison in Ta-   ble 7 shows , POI - Net basically brings no additional   parameter except an linear embedding layer for   POS Embedding . Multi - head Co - Attention mecha-   nism and models based on it ( like DUMA in Table   3 ) introduces much more parameters , with slightly   lower performance . We also record time costs on   RACE for one training epoch on ALBERT , It-   erative Co - Attention costs 54,62,72,83,96min-   utes from 0 - turn iteration to 4 - turn iterations , while   Multi - head Co - Attention costs 54,65,76,89,109   minutes instead , with 8.3%increase on average .   4.5 Visualization   We perform a visualization display for discrimina-   tive MRC examples in Table 1 , as Figure 5 shows .   For the extractive example , benefited from POS   Embedding , POI - Net predicts the precise answer   span , based on the interrogative qualifier “ where ”   and POS attributes of controversial boundary to-   kens “ exhibited ” , “ at ” , “ London ” , “ Exhibition ” ,   “ 1862 ” .   And for the multi - choice example , without pro-   posed Iterative Co - Attention Mechanism , the over-   all distribution of attention is more scattered . The   baseline can only notice special tokens like [ CLS ]   at the 0 - th turn , and even interrogative qualifier   “ how ” due to the similar usage to “ what ” in the ques-   tion . With the execution of Iterative Co - Attention ,   POI - Net pays more attention on discrete critical   words like “ Green Scenes ” and “ events ” at the 1 - st   turn , “ series ” and “ focusing ” at the 2 - nd turn and   “ greener lifestyle ” at the 3 - rd turn . After the integra-   tion of all above critical evidence , POI - Net predicts   the golden option ultimately .   5 Related Studies   5.1 Semantic and Linguistic Embedding   To cope with challenging MRC tasks , numerous   powerful pre - trained language models ( PLMs ) have   been proposed ( Devlin et al . , 2019 ; Lewis et al . ,   2020 ; Raffel et al . , 2020 ) . Though advanced PLMs   demonstrate strong ability in contextual represen-   tation , the lack of explicit semantic andlinguistic   clues leads to the bottleneck of previous works.8689   Benefited from the development of semantic   role labeling ( Li et al . , 2018 ) and dependency syn-   tactic parsing ( Zhou and Zhao , 2019 ) , some re-   searchers focus on enhancing semantic representa-   tions . Zhang et al . ( 2020b ) strengthen token rep-   resentation by fusing semantic role labels , while   Zhang et al . ( 2020c ) and Bai et al . ( 2021 ) imple-   ment additional self attention layers to encode syn-   tactic dependency . Furthermore , Mihaylov and   Frank ( 2019 ) employ multiple discourse - aware se-   mantic annotations for MRC on narrative texts .   Instead of semantic information , we pay atten-   tion to more accessible part - of - speech ( POS ) infor-   mation , which has been widely used into non - MRC   fields , such as open domain QA ( Chen et al . , 2017 ) ,   with much lower pre - processing calculation con-   sumption but higher accuracy ( Bohnet et al . , 2018 ;   Strubell et al . , 2018 ; Zhou et al . , 2020 ) . However ,   previous application of POS attributes mostly stays   in primitive and rough embedding methods ( Huang   et al . , 2018 ) , leading to much slighter improvement   than proposed POI - Net .   5.2 Attention Mechanism   In discriminative MRC field , various attention   mechanisms ( Raffel and Ellis , 2015 ; Seo et al . ,   2017 ; Wang et al . , 2017 ; Vaswani et al . , 2017 ) play   increasingly important roles . Initially , attention   mechanism is mainly adopted on extractive MRC   ( Yu et al . , 2018 ; Cui et al . , 2021 ) , such as multi-   ple polishing of answer spans ( Xiong et al . , 2017 )   and multi - granularity representations generation(Zheng et al . , 2020 ; Chen et al . , 2020 ) . Recently ,   researchers notice its special effect for multi - choice   MRC . Zhang et al . ( 2020a ) model domains bidirec-   tionally with dual co - matching network , Jin et al .   ( 2020 ) use multi - step attention as classifier , and   Zhu et al . ( 2020 ) design multi - head co - attentions   for collaborative interactions .   We thus propose a universal Iterative Co-   Attention mechanism , which performs interaction   between paired input domains iteratively , to hope-   fully enhance discriminative MRC . Unlike other   works introducing numerous parameters by compli-   cated attention network ( Zhang et al . , 2020a ) , our   POI - Net is more effective and efficient with almost   no introduction of additional parameters .   6 Conclusion   In this work , we propose POS - Enhanced Iterative   Co - Attention Network ( POI - Net ) , as a lightweight   unified modeling for multiple subcategories of dis-   criminative MRC . POI - Net utilizes POS Embed-   ding to encode POS attributes for the preciseness of   answer boundary , and Iterative Co - Attention Mech-   anism with integration strategy is employed to high-   light and integrate critical information at decoding   aspect , with almost no additional parameter . As the   first effective and unified modeling with pertinence   for different types of discriminative MRC , evalu-   ation results on four extractive and multi - choice   MRC benchmarks consistently indicate the general   effectiveness and applicability of our model.8690References869186928693A Part - Of - Speech Tags List   In this appendix , we list all 39 POS tags ( including   POS tags from nltkPOS tagger and defined by us )   in Table 9 .   B Complete Comparison Results on   Benchmarks   We show complete public works on DREAM ,   RACE , SQuAD 1.1 and SQuAD in this appendix ,   as Tables 8 10 , 11 and 12 show .   The results show that , our POI - Net outperforms   most of comparison models and baselines , ex-   pect models : 1 ) with massive and incompara-   ble parameters like T5 ( Raffel et al . , 2020 ) and   Megatron - BERT ( Shoeybi et al . , 2019 ) ; 2 ) in more   advanced baseline architecture like XLNet ( Yang   et al . , 2019 ) , ELECTRA ( Clark et al . , 2020 ) ; 3 ) in   special model design for one single subcategory of   discriminative MRC task ( Zhang et al . , 2021 ) .   Model Dev Test   FTLM++ ( Sun et al . , 2019a ) 58.1 58.2   BERT(Devlin et al . , 2019 ) 63.4 63.2   BERT ( Devlin et al . , 2019 ) 66.0 66.8   XLNet ( Yang et al . , 2019 ) – 72.0   RoBERTa ( Liu et al . , 2019 ) 85.4 85.0   RoBERTa + MMM ( Jin et al . ,   2020)88.0 88.9   ALBERT + DUMA ( Zhu   et al . , 2020)89.9 90.4   ALBERT + DUMA + MTL – 91.8   ALBERT(rerun ) 65.7 65.6   POI - Net on ALBERT 68.6 68.5   ALBERT ( rerun ) 89.2 88.5   POI - Net on ALBERT 90.0 90.3POS Tag Meaning   CC Coordinating conjunction   CD Cardinal number   DT Determiner   EX Existential there   FW Foreign word   IN Preposition or subordinating con-   junction   JJ Adjective   JJR Adjective , comparative   JJS Adjective , superlative   LS List item marker   MD Modal   NN Noun , singular or mass   NNS Noun , plural   NNP Proper noun , singular   NNPS Proper noun , plural   PDT Predeterminer   POS Possessive ending   PRP Personal pronoun   PRP$ Possessive pronoun   RB Adverb   RBR Adverb , comparative   RBS Adverb , superlative   RP Particle   SYM Symbol   TO To   UH Interjection   VB Verb , base form   VBD Verb , past tense   VBG Verb , gerund or present participle   VBN Verb , past participle   VBP Verb , non-3rd person singular   present   VBZ Verb , 3rd person singular present   WDT Wh - determiner   WP Wh - pronoun   WP$ Possessive wh - pronoun   WRB Wh - adverb   SPE Special tokens : [ CLS ] , [ SEP ]   PAD Padding tokens   ERR Unrecognized tokens8694Model Dev ( M / H ) Test ( M / H )   BERT(Devlin et al . , 2019 ) 64.6 ( – / – ) 65.0 ( 71.1 / 62.3 )   BERT ( Devlin et al . , 2019 ) 72.7 ( 76.7 / 71.0 ) 72.0 ( 76.6 / 70.1 )   XLNet ( Yang et al . , 2019 ) 80.1 ( – / – ) 81.8 ( 85.5 / 80.2 )   XLNet + DCMN+ ( Zhang et al . , 2020a ) – ( – / – ) 82.8 ( 86.5 / 81.3 )   RoBERTa ( Liu et al . , 2019 ) – ( – / – ) 83.2 ( 86.5 / 81.8 )   RoBERTa + MMM ( Jin et al . , 2020 ) – ( – / – ) 85.0 ( 89.1 / 83.3 )   T5 - 11B ( Raffel et al . , 2020 ) – ( – / – ) 87.1 ( – / – )   ALBERT + DUMA ( Zhu et al . , 2020 ) 88.1 ( – / – ) 88.0 ( 90.9 / 86.7 )   T5 - 11B + UnifiedQA ( Khashabi et al . , 2020 ) – ( – / – ) 89.4 ( – / – )   Megatron - BERT-3.9B ( Shoeybi et al . , 2019 ) – ( – / – ) 89.5 ( 91.8 / 88.6 )   ALBERT + SC + TL ( Jiang et al . , 2020 ) – ( – / – ) 90.7 ( 92.8 / 89.8 )   ALBERT(rerun ) 67.9 ( 72.3 / 65.7 ) 67.2 ( 72.1 / 65.2 )   POI - Net on ALBERT 72.4 ( 76.3 / 70.0 ) 71.0 ( 75.7 / 69.0 )   ALBERT ( rerun ) 86.6 ( 89.4 / 85.2 ) 86.5 ( 89.2 / 85.4 )   POI - Net on ALBERT 88.1 ( 91.3 / 86.3 ) 88.3 ( 91.5 / 86.8 )   Model EM F1   SAN ( Liu et al . , 2017 ) 76.2 84.1   R.M - Reader ( Hu et al . , 2018 ) 81.2 87.9   ALBERT(Lan et al . , 2020 ) 82.9 89.3   BERT(Devlin et al . , 2019 ) 80.8 88.5   BERT ( Devlin et al . , 2019 ) 85.5 92.2   ALBERT ( Lan et al . , 2020 ) 88.3 94.1   SpanBERT(Joshi et al . , 2020 ) 88.8 94.6   XLNet ( Yang et al . , 2019 ) 89.7 95.1   RoBERTa + LUKE ( Yamada   et al . , 2020)89.8 95.0   ALBERT(rerun ) 82.7 89.9   POI - Net on ALBERT 84.5 91.3   ALBERT ( rerun ) 88.2 94.1   POI - Net on ALBERT 89.5 95.0Model EM F1   ALBERT(Lan et al . , 2020 ) 77.1 80.0   BERT(Devlin et al . , 2019 ) 77.6 80.4   NeurQuRI ( Back et al . , 2020 ) 80.0 83.1   BERT ( Devlin et al . , 2019 ) 82.2 85.0   SemBERT ( Zhang et al . , 2020b ) 84.2 87.9   ALBERT ( Lan et al . , 2020 ) 85.1 88.1   SpanBERT(Joshi et al . , 2020 ) 85.7 88.7   XLNet ( Yang et al . , 2019 ) 87.9 90.6   ELECTRA ( Clark et al . , 2020 ) 88.0 90.6   ALBERT + Retro - Reader   ( Zhang et al . , 2021)87.8 90.9   ALBERT(rerun ) 77.3 80.4   POI - Net on ALBERT 79.8 82.9   ALBERT ( rerun ) 85.4 88.5   POI - Net on ALBERT 87.7 90.68695