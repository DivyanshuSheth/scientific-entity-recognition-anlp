N   Yusen ZhangAnsong NiZiming MaoChen Henry Wu   Chenguang ZhuBudhaditya DebAhmed H. Awadallah   Dragomir RadevRui ZhangPenn State UniversityYale UniversityCarnegie Mellon UniversityMicrosoft Research   { yfz5488,rmz5227}@psu.edu , { ansong.ni,dragomir.radev}@yale.edu   Abstract   Text summarization helps readers capture   salient information from documents , news , in-   terviews , and meetings . However , most state-   of - the - art pretrained language models ( LM )   are unable to efﬁciently process long text   for many summarization tasks . In this pa-   per , we propose S , a simple , ﬂexi-   ble , and effective multi - stage framework for   input texts that are longer than the maxi-   mum context length of typical pretrained LMs .   Sﬁrst splits the data samples and gener-   ates a coarse summary in multiple stages and   then produces the ﬁnal ﬁne - grained summary   based on it . Our framework can process in-   put text of arbitrary length by adjusting the   number of stages , while keeping the LM in-   put size ﬁxed . Moreover , it can deal with   both single - source documents and dialogues ,   and it can be used on top of different back-   bone abstractive summarization models . To   the best of our knowledge , Sis the   ﬁrst multi - stage split - then - summarize frame-   work for long input summarization . Our ex-   periments demonstrate that Soutper-   forms previous state - of - the - art methods by im-   proving ROUGE scores on three long meet-   ing summarization datasets AMI , ICSI , and   QMSum , two long TV series datasets from   SummScreen , and a long document summa-   rization dataset GovReport . Our data and code   are available at https://github.com/   psunlpgroup / Summ - N .   1 Introduction   Abstractive summarization helps readers capture   salient information from various sources such as   documents , news , interviews , and meetings . Pre-   vious work has primarily focused on short texts of   news ( Gehrmann et al . , 2018 ; Zhang et al . , 2019 )   and short conversations ( Gliwa et al . , 2019 ; Chen   and Yang , 2021 ) . Recently proposed longer dia-   logue and document summarization tasks ( Zhonget al . , 2021b ; Huang et al . , 2021 ; Chen et al . , 2021 ;   Zhu et al . , 2021a ) pose challenges for current large   pretrained language models due to the time and   memory complexity of training , as well as limited   input lengths these models can consume .   A common method to handle long text re-   duces the input to a shorter one . This can   be accomplished by truncating inputs ( Lewis   et al . , 2020 ) or employing retrieve - then - summarize   pipelines ( Zhong et al . , 2021b ) . However , these   methods break the dependency of the context and   decrease the number of tokens that the model can   read , i.e. , the receptive ﬁeld of the model . The   cutting - off model depends on the lead bias of   the source text , while the retrieve - then - summarize   models heavily rely on the independence of re-   trieved units ( turns or sentences ) which are usually   scattered throughout the source text .   Another approach optimizes the attention mech-   anism in Transformers to accommodate longer in-   puts by reducing the impact of quadratic complex-   ity of the attention process using Locality - sensitive   hashing ( LSH ) attention ( Kitaev et al . , 2020 ) and   Sinkhorn attention ( Tay et al . , 2020 ) . Additionally ,   HMNet ( Zhu et al . , 2020 ) and HAT - BART ( Rohde   et al . , 2021 ) use hierarchical self - attention to ex-   tend the input limitation of typical self - attention   models . However , the simpliﬁed attention mecha-   nism weakens the power of pretrained Transformer   models , e.g. , HMNet is not pretrained on external   large - scaled unsupervised datasets as BART did .   In this paper , we propose S , a multi - stage   framework for long dialogue and document summa-   rization . Figure 1 shows the structure of S.   First , it divides each source text into segments so   that each can be completely fed into the backbone   abstractive summarization model . Then , it matches   each of them with the subset of target text using   a ROUGE - based greedy algorithm . Next , each   stage generates a coarse summary for each segment   and concatenates them together as the input to the1592   next stage . After multiple stages of compression   and summarization , the ﬁnal stage produces a ﬁne-   grained summary . The process expands the model   context to the full reception ﬁeld , meaning that the   proposed model can read the full input no matter   how long the input is . Additionally , retrieve - then-   summarize pipelines ( Zhang et al . , 2019 ) extract   sentences individually , leading to the loss of the   context information for understanding utterances .   By contrast , Sonly cuts the source text at   the end of each segment , so that the context of most   sentences are retained .   It does not assume lead bias because each part   of the source is fully used . In addition , in each   stage , it leverages a backbone abstractive summa-   rization model to recursively generate the sum-   maries . Therefore , it enjoys the full power of the   pretrained language models because the framework   preserves the intact structure of Transformers .   Sis ﬂexible to inputs with different   lengths by adjusting the number of stages . S   can change the number of coarse stages according   to the compression ratio between source and target ,   the input limit of the backbone model , and the in-   put source length . We give the empirical formula   to decide the number of needed stages for every   tested dataset . Our experiments show that ROUGE   increases on all datasets when increasing the num-   ber of stages from one to the appropriate number .   Additionally , Sis ﬂexible because it can be   applied to different backbone summarization mod-   els . For example , we found that the ROUGE scores   increase sharply on the AMI dataset when replac-   ing the backbone BART model with T5 ( Raffelet al . , 2020 ) and PEGASUS ( Zhang et al . , 2019 ) .   We conduct extensive experiments on long - input   summarization datasets in multiple domains . The   results demonstrate that the proposed model signif-   icantly outperforms previous state - of - the - art meth-   ods according to automatic and human evalua-   tions on three long meeting summarization datasets   ( AMI , ICSI , QMSum ) and one long TV series   summarization dataset ( SummScreen ) . It also   achieves state - of - the - art performance on a long doc-   ument summarization dataset ( GovReport ) . These   datasets include document summarization as well   as both query - based and query - independent long   dialogue summarization tasks .   Our contributions are : ( 1 ) We propose S ,   a simple , ﬂexible , and effective framework for long   dialogue and document summarization . To the best   of our knowledge , Sis the ﬁrst multi - stage   split - then - summarize framework to solve long text   summarization tasks . ( 2 ) We evaluate Son   both dialogue and document domains and improve   the baseline model by a large margin . ( 3 ) We an-   alyze and compare the proposed framework with   baselines and discuss its merits in detail .   2 Related Work   Long Document Summarization Long docu-   ment summarization has been studied in multi-   ple domains , such as news ( Liu et al . , 2021 ; Zhu   et al . , 2021b ) , patterns ( Trappey et al . , 2009 ) ,   books ( Kry ´ sci´nski et al . , 2021 ; Wu et al . , 2021 ) , sci-   entiﬁc publications ( Qazvinian and Radev , 2008 ;   Mao et al . , 2021 ) , and medical records ( Cohan1593et al . , 2018 ) . Gidiotis and Tsoumakas ( 2020 ) pro-   posed a divide - and - conquer method by splitting the   input into multiple segments , summarizing them   separately , and combining the summary pieces .   Grail et al . ( 2021 ) proposed a hierarchical neural   model to process segmented input blocks . Com-   pared with S , these models only split the   input once , implying the lack of ﬂexibility when   handling longer input .   The GovReport dataset was recently introduced   containing documents with more than 9000 words ,   thus greatly challenging the capabilities of current   models such as PEGASUS ( Zhang et al . , 2019 ) ,   TLM ( Pilault et al . , 2020 ) , and BIGBIRD ( Zaheer   et al . , 2020 ) . To handle this dataset , Huang et al .   ( 2021 ) proposed head - wise positional strides to   reduce the cost of the encoder - decoder attention .   Similarly , models such as Longformer ( Beltagy   et al . , 2020 ) and Reformer ( Kitaev et al . , 2020 )   adjust attention mechanisms in Transformers to   consume longer inputs . However , these models   sparsify the attention structure of the pretrained   model to ﬁt the longer source text . By contrast ,   Sis able to maintain the full structure of   various pretrained models .   Long Dialogue Summarization Various models   have also been proposed to handle long dialogue   summarization . HMNet ( Zhu et al . , 2020 ) and   HAT - BART ( Rohde et al . , 2021 ) leverage a two-   level transformer - based model to obtain word level   and sentence level representations . DialLM ( Zhong   et al . , 2021a ) , Longformer - BART - arg ( Fabbri et al . ,   2021 ) use ﬁnetuning or data augmentation to in-   corporate the external knowledge to maintain the   accuracy of lengthy input . Different from these   models , Sis a framework without modify-   ing the structure of the backbone attention model .   Multi - Stage Text Generation Multiple multi-   stage coarse - to-ﬁne frameworks have been stud-   ied in many other text generation tasks , such as   dialogue state tracking ( Chen et al . , 2020 ) , neural   story generation ( Fan et al . , 2018 ) , and extractive   summarization ( Xu and Lapata , 2020 ) . In a summa-   rization task , a two - stage extract - and - summarize   pipeline is commonly used ( Zhang et al . , 2019 ;   Pilault et al . , 2020 ; Zhao et al . , 2020 ) . However ,   unlike that work , our framework aims at long input   summarization with fully abstractive intermediate   summaries , meaning that Scan be viewed   as a summarize - then - summarize pipeline.3 Method   Figure 1 shows the workﬂow of S. The   workﬂow includes two types of stages , Ncoarse   stages , and one ﬁne - grained stage . Coarse stages   include the data segmentation and coarse summary   generation , while the ﬁne - grained stage directly   generates the summary as the ﬁnal result . Besides ,   we have N+ 1 separate models for each stage   and each was separately trained . Our experiments   show that the performance drops if different stages   share the parameters ( Section 4.2 ) . Scan   adjust and compute the number of coarse stages N   according to the stats of dataset and model .   To formulate our task , we denote one sample of   the source text as D = fD ; D; ; Dg , where   Dindicates one sentence in a document or one   turn in a dialogue . For query - based summarization ,   there is also a query Q. The goal is to generate a   summary T , given Dand the optional Q.   3.1 Data Segmentation   In long text summarization , the number of tokens   in the source data usually exceeds the limit of the   backbone summarization models , thus reducing   the quality of the summary . To make sure that the   model can capture information about all source to-   kens , we apply a segmentation algorithm for long   input summarization datasets . First , we segment   the source text so that the data input to the back-   bone model does not exceed the length limit . Then ,   we apply a greedy algorithm to ﬁnd the best target   summary that matches the source segments .   Source Segmentation Assume that the number   of the maximum input tokens of the backbone   model is K. To completely input the source in-   formation , we cut the input D(between sentences )   into multiple segments , each of them containing   fewer than Ktokens . Given the input D , we will   have nsegments S = fS ; S; ; Sgwhere   S2Dis a segment in D. For query - based sum-   marization tasks , we simply concatenate the query   to the beginning of the S , i.e. S QLS . In   both cases , the number of tokens in each segment   is less than the hyper - parameter K.   Target Matching Segmenting the source text re-   sults in nsource pieces S. We match each S   with a target segment T2Tto form the new   pair(S ; T)for the next step . We use a greedy   algorithm for target matching . We ﬁrst split T   into separate sentences T = fT ; T; ; Tg.1594Algorithm 1 Greedy Target Matching   Then , each segment Sis matched with a subset   ofTsuch that the ROUGE-1 score between the   subset and Sis maximized . However , it is not   feasible to ﬁnd the optimal set due to the consid-   erable running time . We apply a simple greedy   approximation to ﬁnd such a subset . From a null   setT , we iteratively add to the subset the sentence   with the highest ROUGE-1 gain between TandS.   Algorithm 1 shows how we obtain the new training   pair(S ; T).Lindicates the concatenation of sen-   tences while keeping them in the same order as in   the original text . We use ROUGE-1 as the match-   ing criterion because the higher ROUGE-1 score   usually implies higher scores on the other metrics   such as ROUGE-2 or ROUGE - L , while ROUGE-1   enjoys lower time complexity compared with other   ROUGE metrics .   This matching algorithm also ensures T6=;so   that each Scan be matched to at least one target   sentence . A sentence t2Tcan be added to mul-   tiple subsets Tbecause one sentence of summary   may need the information from multiple segments .   3.2 Coarse Summary Generation   In coarse summary generation , we train a summa-   rization model , that takes the segmented data as   input . We ﬁrst collect the training samples ( S ; T )   generated by data segmentation to form a new   dataset . This augments the source data to d = K   times compared with the cut - off methods , where   d = jDjindicates the averaged number of tokens   of original source text . Thus , data segmentation   helps the summarizer to better learn the task of the   current stage . Additionally , because we incorpo - rate the full input using segmentation , it does not   rely on the leading bias in the cut - off method that   only considers the ﬁrst segment S. Afterward , we   use these data to train a neural summarizer . This   way , our model treats each part of the source text   as equally important .   Given a source segment Sand an optional query   Q , we obtain the coarse summary segments using   a backbone summarization model :   C = SUMM(Q ; S )   Where l2[1 ; N]is the index of the current stage .   Then , the ncoarse summaries corresponding to the   original source S = fS ; S; ; Sgare concate-   nated : C = CLCLLC . We use C   as the new source text of next stage , which com-   presses the input source data D. i.e. D = C.   To pair with the D , the target to the next stage   is copied from the original dataset , i.e. T = T.   The proposed framework is applicable to dif-   ferent backbone models SUMM( ) , such as   BART ( Lewis et al . , 2020 ) and T5 ( Raffel et al . ,   2020 ) . We pick BART as the backbone model   because it can best illustrate the beneﬁts of our   framework ( Section 4.2 ) .   3.3 Estimation of the Number of Coarse   Stages N   The number of stages can be estimated by data stats   and model characteristics . In S , each coarse   stage compresses the input to a shorter length . Af-   terNturns of coarse stages , the averaged length   of source text is below K , the dataset is then fed   into the ﬁne - grained stage . Hence , the number of   coarse stages can be computed by the following   equation ( details can be found in Appendix A ):   ^N = dlogK logd   logc logKe   where dandcare the average length of source   text and coarse segments in stage 1 . In Section   5.7 and Table 9 , we demonstrate this estimation is   close to the empirical number of coarse stages .   The greedy algorithm in Sfor target   matching is critical to the performance . Consider   a duplication algorithm where each segment Sis   simply paired with the target T , i.e. T = T. Since   the target text is longer than the text segmented   by Algorithm 1 , the generated summary of each   coarse stage will be longer as well , leading to a   lower compression speed and larger N. Besides,1595   the duplication of the target will confuse the model ,   because some source segments will probably be   paired with the same target , causing the model to   generate duplicated content . Experiments ( Table 7 ,   “ - stage 2 ” versus “ - stage 2 & tar . seg . ” ) show that   ROUGE scores declines a lot when greedy target   segment is replaced by the duplication algorithm .   3.4 Fine - Grained Summary Generation   When the input source of Dis shorter than K ,   we can proceed to the ﬁne - grained stage . In this   stage , Dis used to train a summarization model   from scratch to obtain the ﬁnal summary . The ﬁne-   grained stage works the same way as the vanilla   backbone model . In fact , SwithN= 0is   the backbone summarizer . In the ﬁne - grained stage ,   the model is directly trained on dataset ( D ; T )   from the last coarse stage , and obtain the summary   as the ﬁnal output of S :   F = SUMM(Q ; D )   It is worth noting that , although source text may   be shorter than 2 segments , i.e. dK , we still   add them in all stages , so that each summarization   model can be trained on the full dataset .   4 Experiment Setup   We ﬁrst list the datasets and metrics to evaluate the   model . Then , we introduce the backbone model   and baselines for comparisons . Finally , we present   some implementation details .   4.1 Datasets and Metrics   Table 1 shows data statistics for the datasets .   AMI & ICSI ( McCowan et al . , 2005 ; Janin   et al . , 2003 ) are meeting scripts generated by Auto-   matic Speech Recognition ( ASR ) systems . AMI is   collected from product design meetings in a com-   pany while ICSI is collected from academic groupmeetings . Because the transcript is produced by   ASR , there is a word error rate of 36 % for AMI   and 37 % for ICSI .   QMSum ( Zhong et al . , 2021b ) is a query - based   meeting summarization dataset . It consists of meet-   ings from three domains , including AMI and ICSI ,   and the committee meetings of the Welsh Parlia-   ment and the Parliament of Canada . Each query   and sample are written by experts .   SummScreen ( Chen et al . , 2021 ) consists of   community - contributed transcripts of television   show episodes from The TVMegaSite , Inc. ( TMS )   and ForeverDream ( FD ) . The summary of each   transcript is the recap from TMS , or a recap of the   FD shows from Wikipedia and TVMaze .   GovReport ( Huang et al . , 2021 ) is a large - scale   long document summarization dataset with 19,466   long reports published by the U.S. Government   Accountability Ofﬁce on national policy issues .   We use ROUGE ( Lin , 2004 ) as the automatic   evaluation metric . We split summary outputs into   sentences to calculate the ROUGE - L score . If not   speciﬁed , F1 scores are used in all results .   4.2 Backbone Model   We pick BART ( Lewis et al . , 2020 ) as our back-   bone summarization model because it performs   well on short text summarization but not as good on   longer texts , illustrating the beneﬁts of our frame-   work . Compared with other pretrained parameters ,   the BART - large model pretrained on the CNN / DM   dataset yields the best performance ( Zhang et al . ,   2021 ) . So we use the BART - large - cnn parameter   as a better starting point .   It is worth noting that we use separate back-   bone models for each stage and each was separately   trained . We experimented with reusing the model   parameters in multiple stages but obtained a lower1596   score , e.g. the ROUGE-1 score of stage 2 on the   QMSum dataset decreases around two points if we   use the best parameters of stage 1 summarizer as   the starting point of training stage 2 summarizer .   This is because the tasks of the different stages   differ signiﬁcantly . For instance , the input to the   ﬁrst stage of dialogue summarization is the dia-   logue turn while the input to the latter stages is the   document .   4.3 Baselines   We compare the proposed framework with vari-   ous baselines . PGNet ( See et al . , 2017 ) uses a   pointer mechanism to copy the token from the   training sample . TopicSeg ( Li et al . , 2019 ) is a   multi - modal model jointly learning the segmenta-   tion and summarization . HMNet ( Zhu et al . , 2020 )   uses a hierarchical attention structure and cross-   domain pre - training for meeting summarization .   TextRank ( Mihalcea and Tarau , 2004 ) is a graph-   based ranking model for text processing . HAT-   BART ( Rohde et al . , 2021 ) is a new hierarchical   attention transformer - based architecture that out-   performs standard Transformers . DDAMS ( Feng   et al . , 2021 ) uses a relational graph to model the in-   teraction between utterances by modeling different   discourse relations .   For the SummScreen dataset , we use the neural   and hybrid model scores reported by Chen et al .   ( 2021 ) . We rename these two baselines as Long-   former+ATT andNN+BM25+Neural to clarify   the difference between other baselines .   The baseline scores we report on GovReport are   from the original paper ( Huang et al . , 2021 ) . BART   Variant indicates self - attention variants with full   attention . BART HEPOS indicates encoder vari-   ants with head - wise positional strides ( HEPOS)encoder - decoder attention .   4.4 Implementation Details   We ﬁt all models into a single RTX A6000 GPU   with a 48 GiB memory . We adopt the fairseqim-   plementation for BART . The learning rate is set to   2e-5 and the beam width is set to 2 for coarse stages   and 10 for ﬁne - grained stages . The maximum num-   ber of tokens in each batch is set to 2048 . The   maximum number of tokens in each source text is   set to 1024 because we tried to extend the positional   embeddings to 2048 or longer but obtained worse   performance . We stop the coarse stage and start the   ﬁne - grained stage when the averaged source length   is shorter than 2048 rather than 1024 to obtain a   better performance ( Section 5.7 ) . For the output   of each intermediate stage , we use < s > and < /s > to   separate each generated target segments C.   5 Results and Analysis   We discuss the evaluation results and effects of   each component of Sin this section .   5.1 Overall Results   Meeting Summarization Table 2 shows the   ROUGE scores on AMI , ICSI , and QMSum . Com-   pared with the baseline models , Sachieves   state - of - the - art results on almost all metrics . Specif-   ically , Simproves SOTA on ICSI by 0.83 ,   and1.96 ROUGE-2 / L scores , improves SOTA on   QMSum - Gold by 4.14,3.96 , and 4.35 ROUGE-   1/2 / L scores . These results demonstrate the effec-   tiveness of Son long dialogue summariza-   tion tasks.1597   TV Series Summarization Table 3 shows   ROUGE scores on SummScreen . Soutper-   forms on almost all metrics on two SummScreen   datasets . Speciﬁcally , we improve 6.58,1.65 , and   3.75 ROUGE-1/2 / L scores on the SummScreen-   FD dataset . This result demonstrates the generaliz-   ability of Sover various domains including   meetings and TV series .   Document Summarization Table 4 shows   ROUGE scores on GoveReport . Sachieves   state - of - the - art performance on ROUGE-2 and   ROUGE - L , and compatible results on ROUGE-1 .   The results show that Sis applicable to   both long dialogue and document summarization   tasks .   5.2 Effects of Number of Stages   We also notice that the performance increases con-   sistently when the number of stages goes up until   the predeﬁned number of stages . Figure 2 shows   the ROUGE-1 scores of different tasks across   stages . Stage 1 indicates the model with only one   coarse stage and no ﬁne - grained stage . In this   model , We directly use the ﬁrst segment of the   coarse summary as the output , i.e. Cof each sam-   ple . Stage i ( i > 1 ) model contains i 1coarse   stages and one ﬁne - grained stage , the generated   summary is from ﬁne - grained summarization mod-   els , i.e. F.   Although stage 2 of Son the ICSI dataset   has already outperformed the baselines , the scores   can be further improved by adding one more coarse   stage . In fact , on all datasets , increasing the number   of stages leads to a performance gain . This gain   can be explained as the following : if the output of   the current stage is longer than Ktokens , adding   one more coarse stage will help since the model   will receive more information from the source text   compared with simply truncating them . On the   contrary , if the input is smaller than K , there is no   need to add more stages , because there is only one   segment .   5.3 Improvements over Backbone Models   Salso boosts the performance of a back-   bone model by a large margin . As shown in Ta-   ble 5 , it improves the BART - large model by 6.87 ,   3.89,6.78 ROUGE-1/2 / L on AMI . This indicates   the capability of Sto boost the performance   of a weak learner on long summarization tasks . In   particular , when the backbone model is well pre-   trained on short input texts and performs well on   short summarization tasks , Scould greatly   increase the capability of the backbone model to   process and read long source texts . Also , the back-   bone of Scan be easily replaced by some   other models , and models do not necessarily have   to be identical at every stage . For example , one   can try different learners such as T5 as the back-   bone model and replace the model in stage 1 with   a dialogue - to - document model.1598   5.4 Generalizability over Backbone Models   To demonstrate our framework can generalize to   different backbone summarization models , we re-   place the BART - large - cnn model in previous ex-   periments with other neural summarization mod-   els including T5 ( Raffel et al . , 2020 ) and PEGA-   SUS ( Zhang et al . , 2019 ) using Hugging Face . Ta-   ble 6 shows the ROUGE scores of three different   models that are trained and evaluated on AMI . In   all models , Simproves the performance of   backbone models by a large margin . For instance ,   although BART - base is a weaker summarizer com-   pared with the BART - large model , the framework   is still able to improve the ROUGE-1 score by 5.06 .   5.5 Ablations   Table 7 shows the ablation study results of S   on the AMI test set . Removing stage 2 ( using the   ﬁrst segment of the coarse summary Cas the gen-   erated summary ) leads to a 5.23 ROUGE-1 score   drop . Without data segmentation , the ROUGE-1   score decreases by 6.61 using the same ﬁne - grained   stage . Removing both stage 2 and target match-   ing ( use duplication algorithm instead ) further de-   creases the performance . It even hurts the perfor-   mance of the original BART model because the   duplication of targets will introduce some biases   towards the common part of the targets .   5.6 Human Evaluation   We conduct a human evaluation to assess the fol-   lowing : Readability takes into account word and   grammatical error rate to evaluate how ﬂuent the   summary language is ; Conciseness measures how   well the summary discards the redundant informa-   tion ; Coverage measures how well the summary   covers each part of the dialogue .   We compare the results of Sand HMNet   because HMNet is a baseline model with the good   capability to read whole input . For each meeting in   AMI and ICSI dataset , we ask 3 different annotators   with English expertise to label the summaries . Each   annotator was asked to read the meeting transcript ,   gold summaries , and generated summaries using   the SummVis ( Vig et al . , 2021 ) toolkit . They were   asked to rate each summary from 1 to 5 ( higher is   better ) for each metric . We also shufﬂe the sum-   maries of two models to reduce the bias .   Table 8 shows that Sachieves higher   scores in Readability , Conciseness , and Coverage   than HMNet in both AMI and ICSI dataset . Speciﬁ-   cally , the Readability ofSgreatly surpasses   the baseline by around 0.5/1 point on AMI / ICSI   dataset . This is because BART is well - pretrained   and is able to generate more readable text and   Ssuccessfully maintains this capability.1599   5.7 Intermediate Result Analysis   To gain more understanding of the multi - stage   mechanism of S , we analyze the number   of coarse stages and the compression rate through   statistics of intermediate stages .   Early Stopping of the Coarse Stage Although   the ideal input of the ﬁnal ﬁne - grained stage should   be shorter than K , the experiment results show that   compressing input from 2 Kto 1Ktokens usually   hurts the performance of the model . This is prob-   ably because generating too many short segments   which are hard to summarize confuses the model .   Thus , we increase the length of input to the ﬁnal   ﬁne - grained stage from Kto2Kto prevent noises   in the training set . The modiﬁed formula to esti-   mate the number of coarse stages ^Nis shown as   follows ( details in Appendix A ) .   ^N=1 + log K logd   logc logK   ^N = d^Ne   Number of Coarse Stages To verify that our es-   timation ^Nis close to the empirical number of   coarse stages N , we use GovReport to compare   the two as shown in Table 9 . We choose this   dataset because it contains the most number of   samples among all ﬁve datasets , with completely   three coarse stages as well .   Table 9 shows the empirical / estimated number   of coarse stages . To clearly show the ^Nvalue ,   we display the ﬂoat number ^Nas the estimated   number , and Nas the empirical number of “ re-   maining coarse stages ” ( Table 1 ) . As can be seen ,   N=^N = d^Neholds for all stages , meaning   that the estimated ^Nis capable of estimating the   correct Nvalue . It is worth noting that , for stage   2 and stage 3 , using this formula can also estimate   “ how many additional coarse stage do we need” . Transformers S   Time O(n ) O(nK=(1 R ) )   Gen. Tokens O(n ) O(n=(1 R ) )   Compression Rate We analyze the change of   compression rate across different stages . In   S , compression rate Ris deﬁned as the   averaged source length of stage idivided by source   length of stage i 1 . As shown in Table 9 , both   compression rates in stage 2 and stage 3 of GovRe-   port are around 0.4 , this shows that the compression   rate of Sacross different stages are stable ,   meaning that the number of segments will decrease   to around 40 % of the previous stage steadily .   5.8 Time Complexity   Table 10 shows the time cost of inferring one sam-   ple using vanilla transformer versus S. Al-   though the Sneeds to generate more tokens   due to multi - stage pipeline , Sreduces the   inference time from quadratic to lower , i.e. , from   O(n)toO(Cn),C = K=(1 R ) . Regarding   training the model , Salso need to infer O(n )   additional tokens on the train / dev / test sets ( details   in Appendix B ) .   6 Conclusion   In this paper , we propose S , a simple , ﬂexi-   ble , and effective framework for long dialogue and   document summarization . It consists of multiple   coarse stages and one ﬁne - grained stage to itera-   tively compress the long source input . It enjoys the   full power of backbone models while ensuring the   full receptive ﬁeld of the summarization model . We   evaluate the model on various datasets and improve   the baselines by a large margin .   Acknowledgement   The authors would like to thank Tao Yu , Ming   Zhong , Yixin Liu , and Asli Celikyilmaz for their   valuable discussions . We also would like to thank   the anonymous reviewers for their helpful com-   ments . This work is supported in part by a grant   from Microsoft Research.1600References16011602   A Computing the Number of Stages   With regard to text length , the source text of each   stage needs to be compressed gradually to ensure   that the summary with proper length can be gener-   ated in the ﬁnal stage . Also , the compression level   determines the required number of stages , which is   a signiﬁcant indicator of time cost .   Suppose one sample of the source of stage icon-   tainsd = jDjwords , while the source of next   stage Dcontains d = jDjwords . Also   because the input of next stage is the coarse sum-   mary of current stage , D = C , thus d=   jDj = jCj . The maximum input length of   the model is K , c = PjCj = nindicates the   averaged number of tokens in the segmented pre-   dictions . dcan be expressed by the length ofcoarse summary which is the number of segmenttimes the length of coarse segments c.   In each stage , we have :   d = d   Kc   By iterating this equation for Ntime , the number   of needed coarse stages Nfor a dataset can be   decided in this way :   dYc   KK   Empirically , care similar in different stages ,   thus we replace the production of cwithcto the   N , i.e.   cYc   Thus , the estimation of Nvalue can be calcu-   lated as follows :   dc   KK   ^N = dlogK logd   logc logKe   We also call c = Kthe compression rate of stage   i , denoted as R. For target matching , the com-   pression rate of duplication segmentation is 1 and   greedy segmentation is less than 0.5 . So that target   segmentation algorithm helps reduce number of   coarse stages .   After using the early stopping of coarse stage ,   the estimation formula changes as follows :   dc   K2 K   ^N = d1 + log K logd   logc logKe   B Time Complexity   Suppose the length of the input is n , by segmenting   the source text into n = K segments , the time cost   of forwarding of one segment is K , thus the total   time cost of stage 1 is n = KK = nK. Then ,   in the next stage , the length of the source text is   reduced to nR , thus the time complexity of stage 2   isnKR . We can list the total time cost by adding   them together :   T(n ) = XnKR = nK   1 R1603   Similarly , in training phrase , stage 1 generates   O(n)tokens while stage 2 generates O(nR)tokens   for each sample in train / dev / test set . We can list   the total generated tokens by adding them together :   T(n ) = XnR = n   1 R   Thus the time cost of forwarding reduces . For   instance , the inference time of SummScreen - TMS   dataset reduces to 1024=(1 0:27)=6420:64 =   21:8 % , and GovReport dataset reduces to   1024=(1 0:43)=7890:46 = 22 : 8%of original   time cost , compared with O(n)transformers . This   shows the efﬁciency of S. On the other hand ,   since the training phrase needs to generate the tar-   get for each sample in the train / dev / test set , the   training time of Salso includes the addi-   tional generation of O()tokens for each sam-   ple in the dataset .   C Case Study   Table 11 shows a concrete sample summary gen-   erated by S. It captures the topics of the   source text and smoothly follows the outline of the   gold summary . Also , Sis able to evenly   generate the information of the whole summary ,   including the last part of source text which is trun-   cated in the standard BART - large models.1604