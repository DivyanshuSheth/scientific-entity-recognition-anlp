  Minhan Xu , Yu Hong   School of Computer Science and Technology , Soochow University , China   cosmosbreak5712@gmail.com , tianxianer@gmail.com   Abstract   We leverage embedding duplication between   aligned sub - words to extend the Parent - Child   transfer learning method , so as to improve low-   resource machine translation . We conduct ex-   periments on benchmark datasets of My ! En ,   Id!En and Tr!En translation scenarios . The   test results show that our method produces sub-   stantial improvements , achieving the BLEU   scores of 22.5 , 28.0 and 18.1 respectively . In   addition , the method is computationally efﬁ-   cient which reduces the consumption of train-   ing time by 63.8 % , reaching the duration of   1.6 hours when training on a Tesla 16 GB P100   GPU . All the models and source codes in the   experiments will be made publicly available to   support reproducible research .   1 Introduction   Low - resource machine translation ( MT ) is chal-   lenging due to the scarcity of parallel data and , in   some cases , the absence of bilingual dictionaries   ( Zoph et al . , 2016 ; Miceli Barone , 2016 ; Koehn and   Knowles , 2017 ; Zhang et al . , 2017 ) . Unsupervised ,   multilingual and transfer learning have been proven   effective in the low - resource MT tasks , grounded   on different advantages ( section 2 ) .   In this paper , we follow Aji et al . ( 2020 ) ’s work   to utilize cross - language transfer learning , of which   the “ parent - child ” transfer framework is ﬁrst pro-   posed by Zoph et al . ( 2016 ) . In the parent - child   scenario , a parent MT model and a child MT model   are formed successively , using the same neural net-   work structure . In order to achieve the sufﬁcient   warm - up effect from scratch , the parent is trained   onhigh -resource language pairs . Further , the child   inherits the parent ’s properties ( e.g. , inner parame-   ters and embedding layers ) , and it is boosted by the   ﬁne - tuning over low - resource language pairs . One   of the distinctive contributions in Aji et al . ( 2020)’sstudy is to demonstrate the signiﬁcant effect of em-   bedding duplication for transference , when it is   conducted between the morphologically - identical   sub - words in different languages .   We attempt to extend Aji et al . ( 2020 ) ’s work   by additionally duplicating embedding informa-   tion among the aligned multilingual sub - words .   It is motivated by the assumption that if the du-   plication between morphologically - identical sub-   words contributes to cross - language transference ,   the duplication among any other type of equiva-   lents is beneﬁcial in the same way , such as that of   the aligned sub - words , most of which are likely   to be morphologically - dissimilar but semantically-   similar ( or even exactly the same ) .   In our experiments , both the parent and child   MT models are built with the transformer - based   ( Vaswani et al . , 2017 ) encoder - decoder architec-   ture ( Section 3.1 ) . We use the unigram model from   SentencePiece ( Kudo and Richardson , 2018 ) for to-   kenizing , and carry out sub - word alignment using   eﬂomal ( Section 3.2 ) . On the basis , we develop a   normalized element - wise embedding aggregation   method to tackle the many - to - one embedding du-   plication for aligned sub - words ( Section 3.3 ) . The   experiments show that our method achieves sub-   stantial improvements without using data augmen-   tation .   2 Related Work   The majority of previous studies can be sorted   into 3 aspects in terms of the exploited learning   strategies , including unsupervised , multilingual   and transfer learning .   •Unsupervised MT conducts translation   merely conditioned on monolingual language   models ( Lample et al . , 2018a ; Artetxe et al . ,   2017 ) . The ingenious method that has   been explored successfully is to bridge the   source and target languages using a shareable613representation space ( Lample et al . , 2018b ) ,   which is also known as interlingual ( Cheng   et al . , 2017 ) or cross - language embedding   space ( Kim et al . , 2018 ) . To systematize   unsupervised MT , most ( although not all )   of the arts leverage bilingual dictionary   induction ( Conneau et al . , 2018 ; Søgaard   et al . , 2018 ) , iterative back - translation   ( Sennrich et al . , 2016a ; Lample et al . , 2018b )   and denoised auto - encoding ( Vincent et al . ,   2008 ; Kim et al . , 2018 ) .   •Multilingual MT conducts translation merely   using a single neural model whose parameters   are thoroughly shared by multiple language   pairs ( Firat et al . , 2016 ; Lee et al . , 2017 ; John-   son et al . , 2017 ; Gu et al . , 2018a , b ) , including   a variety of high - resource language pairs as   well as a kind of low - resource ( the target lan-   guage is ﬁxed and deﬁnite ) . Training on a mix   of high - resource and low - resource ( even zero-   resource ) language pairs enables the shareable   model to generalize across language bound-   aries ( Johnson et al . , 2017 ) . The beneﬁts re-   sult from the assimilation of relatively exten-   sive translation experience and sophisticated   modes from high - resource language pairs .   •Transferable MT is fundamentally similar   to multilingual MT , whereas it tends to play   the aforementioned Parent - Child game ( Zoph   et al . , 2016 ) . A variety of optimization meth-   ods have been proposed , including the transfer   learning over the embeddings of WordPieces   tokens ( Johnson et al . , 2017 ) , BPE sub - words   ( Nguyen and Chiang , 2017 ) and the shared   multilingual vocabularies ( Kocmi and Bojar ,   2018 ; Gheini and May , 2019 ) , as well as the   transference that is based on the artiﬁcial or   automatic selection of congeneric parent lan-   guage pairs ( Dabre et al . , 2017 ; Lin et al . ,   2019 ) . In addition , Aji et al . ( 2020 ) verify the   different effects of various transferring strate-   gies of sub - word embeddings , such as that   among morphologically - identical sub - words .   In this paper , we extend Aji et al . ( 2020 ) ’s   work , transferring embedding information not only   among the morphologically - identical sub - words   but the elaborately - aligned sub - words.3 Approach   3.1 Preliminary : Basic Transferable NMT   We follow Kim et al . ( 2019 ) and Aji et al . ( 2020 )   to build neural MT ( NMT ) models with 12 - layer   transformers ( Vaswani et al . , 2017 ) , in which the   ﬁrst 6 layers are used as the encoder while the   subsequent 6 layers the decoder .   Embedding Layer As usual , the encoder is cou-   pled with a trainable embedding layer , which main-   tains a ﬁxed bilingual vocabulary and trainable sub-   word embeddings . Each embedding is speciﬁed as   a 512 - dimensional real - valued vector .   Parent - Child Transfer We follow Zoph et al .   ( 2016 ) to conduct Parent - Child transfer learning .   Speciﬁcally , we adopt an off - the - shelf transformer-   based NMTwhich was adequately trained on high-   resource De!En ( German!English ) language   pairs . The publicly - available data of OPUS ( Tiede-   mann , 2012 ) is used for training , which comprises   about 351.7 M De!En parallel sentence pairs . We   regard this NMT model as the Parent . Further , we   transfer all inner parameters of the 12 - layer trans-   formers from Parent to Child .   By contrast , the embedding layer of Parent is par-   tially transferred to Child , which has been proven   effective in Aji et al . ( 2020 ) ’s study . Assume V   denotes the high - resource ( e.g. , the aforementioned   De - En ) vocabulary while Vthe low - resource , the   morphologically - identical sub - words Vare then   speciﬁed as the ones occurring in both VandV   ( i.e. ,V = V\V ) . Thus , we duplicate the embed-   dings of morphologically - identical sub - words V   from the embedding layer of Parent to that of Child .   Further , we randomly initialize the embeddings   of the rest sub - words Vin the Child ’s embedding   layer ( V = V V ) , where random sampling from   a Gaussian distribution is used .   Both the transferred inner parameters and the du-   plicated embeddings constitutes the initial state of   the Child NMT model . On the basis , we ﬁne - tune   Child on the low - resource language pairs , such as   the considered 18 K My ! En ( Burmese!English )   parallel data in our experiments .   3.2 Tokenizer and Alignment   We strengthen Parent - Child transfer learning by ad-   ditionally duplicating embeddings for aligned sub-   words ( between low and high - resource languages).614Doc . Sent . Token   My 113 K 1.1 M 17.4 M   I d 1.1 M 8.3 M 156.2 M   Tr 705 K 5.8 M 128.2 M   The precondition is to produce the word - level align-   ment and equivalently assign it to sub - words .   Word Alignment We use Eﬂomalto achieve   the word alignment . It is developed based on EF-   MARAL ( Östling et al . , 2016 ) , where Gibbs sam-   pling is run for inference on Bayesian HMM mod-   els . Eﬂomal is not only computationally efﬁcient   but able to perform n - to-1 alignment . We sepa-   rately train Eﬂomal on the low - resource My ! En ,   I d ( Indonesian)!En and Tr ( Turkish ) ! En parallel   data ( Section 4 ) .   Sub - word Tokenizer We train a sub - word tok-   enizer using the unigram model of SentencePiece   for each low - resource language , including My , I d   and Tr . The tokenizers are trained on monolingual   plain texts which are collected from Wikipedia ’s   dumps . The toolkit wikiextractoris utilized to   extract plain texts from the semi - structured data .   The statistics of training data is shown in Table 1 .   We uniformly set the size of sub - word vocabu-   lary to 50 K when training the tokenizers . The ob-   tained vocabulary of each low - resource language is   utilized for sub - word alignment , towards the mixed   De - En sub - word vocabulary in the Parent NMT   model . The size of De - En vocabulary is 58K.   Sub - word Alignment Given a pair of aligned   bilingual words , we construct the same correspon-   dence for their sub - words by many - to - many map-   pings . See the De!Tr example in ( 1 ) .   ( 1 ) Word Alignment : | produktion$üretme   |Harnstoff$üre   Sub - word Alignment : | produck${üre , tme }   |tion${üre , tme }   |Harn${üre }   |stoff${üre }   It is unavoidable that some of the aligned sub-   words are non - canonical . Though , the positive ef-   fect on transfer learning may be more substantial   than negative . It motivated by the ﬁndings that   the use of sub - words ensures a sufﬁcient overlapTrain . Val . Test   My - En ( ALT ) 18 K 1 K 1 K   Id - En ( BPPT ) 22 K 1 K 1 K   Tr - En ( WMT17 ) 207 K 3 K 3 K   between vocabularies ( Nguyen and Chiang , 2017 ) ,   and thus enables the transfer of a larger number of   concrete embeddings rather than random ones .   3.3N - to-1 Embedding Duplication   Assume that Vdenotes the sub - words in low-   resource vocabulary that have aligned sub - words   in high - resource vocabulary , the mapping is D(x ) ,   note that8x2V , D(x)is a set of sub - words .   Thus , in the embedding layer of Child , we extend   the range of sub - words for embedding transfer , in-   cluding both the identical sub - words Vand the   aligned V. To enable the transfer , we tackle n-   to-1 embedding duplication . It is because that , in   a large number of cases , there is more than one   high - resource sub - word corresponding to a single   low - resource sub - word ( see “ üre ” in ( 1 ) ) .   Given a sub - word xinVand the aligned sub-   words vinD(x ) , we rank vin terms of the fre-   quency with which they were found to be aligned   withxin the parallel data . On the basis , we carry   out two duplication methods as below .   •Top-1 We take the top- 1sub - word xfromv ,   and perform element - wise embedding dupli-   cation from xtox:8i ; E(x ) = E(x)(iis   thei - th dimension of embedding E( ) ) .   •Mean We adopt all the sub - words in v , and   duplicate their embedding information by the   normalized element - wise aggregation ( where ,   ndenotes the number of sub - words in v ):   8i ; E(x ) = E(x)=n   4 Experimentation   4.1 Datasets and Evaluation Metric   We evaluate the transferable NMT models for three   source languages ( My , I d and Tr ) . English is in-   variably speciﬁed as the target language . There are   three low - resource parallel datasets used for train-   ing the Child NMT model , including Asian Lan-   guage Treebank ( ALT ) ( Ding et al . , 2018 ) , PAN Lo-   calization BPPTand the corpus of WMT17 news615Model My - En Id - En Tr - En   Baseline 20.5 26.0 17.0   MI - PC 21.0 27.5 17.6   Top-1 - PC 21.9 27.6 18.0   Mean - PC 22.5 28.0 18.1   Model My - En Id - En Tr - En   Baseline 20.2 24.5 16.5   MI - PC 20.4 24.2 16.8   Top-1 - PC 21.2 26.9 16.9   Mean - PC 21.9 27.1 16.9   translation task ( Bojar et al . , 2017 ) . The statistics   in the training , validation and test sets is shown   in Table 2 . We evaluate all the considered NMT   models with SacreBLEU ( Post , 2018 ) .   4.2 Hyperparameters   We use an off - the - shelf NMT model as Parent ( Sec-   tion 3.1 ) , whose state variables ( i.e. , hyperparam-   eters and transformer parameters ) and embedding   layer are all set . On the contrary , the Child NMT   model needs to be regulated from scratch .   When training and developing Child , we adopt   the following hyperparameters . Each source lan-   guage was tokenized using SentencePiece ( Kudo   and Richardson , 2018 ) with 50k vocabulary size .   Training was carried out with HuggingFace Trans-   formers library ( Wolf et al . , 2020 ) using the Adam   optimizer with 0.1 weight decay rate . The maxi-   mum sentence length was set to 128 and the batch   size to 64 sentences . The learning rate was set to   5e-5 and checkpoint frequency to 500 updates . For   each model , we selected the checkpoint with the   lowest perplexity on the validation set for testing .   5 Results and Analysis   Table 3 shows the test results , where all the consid-   ered Parent- Child transfer models are marked with   “ PC ” , and the baseline is the transformer - based   NMT ( Section 3.1 ) which is trained merely using   low - resource parallel data ( without transfer learn-   ing ) . MI - PC is the reproduced transfer model in   terms of Aji et al . ( 2020 ) ’s study , in which only   the embedding transference of morphologically-   identical sub - words is used . We report NMT per-   formance when MI - PC is used to enhance the base-   line , as well as that when our auxiliary transfer   Model My - En Id - En Tr - En   Baseline 1.30 1.27 4.49   MI - PC 1.30 1.35 3.53   Top-1 - PC 1.11 1.00 3.07   Mean - PC 0.96 0.94 2.14   models ( i.e. , Top-1 and Mean in Section 3.3 ) are   additionally adopted , separately .   It can be observed that , compared to MI - PC , both   Top-1 - PC and Mean - PC yield improvements for all   the three low - resource MT scenarios . The most   signiﬁcant improvement occurs for My ! En MT ,   reaching up to 1.5 BLEU . Both the models general-   ize well across changes in the input sub - words . It   can be illustrated in a separate experiment where   the BPE ( Sennrich et al . , 2016b ) tokenizer is used   ( instead of SentencePiece ( Kudo and Richardson ,   2018 ) ) , and all the transfer models are run over   the newly - aligned sub - words . As shown in Table   4 , both Top-1 - PC and Mean - PC still outperform   MI - PC , yielding an improvement of 2.9 BLEU at   best ( for Id!En MT ) .   Due to unavoidable errors in the sub - word align-   ment , the utilization of a single aligned sub - word   for embedding duplication easily results in perfor-   mance degradation . Aggregating and normalizing   embeddings of all possible aligned sub - words help   to overcome the problem . Figure 1 shows the NMT   performance obtained when the i - th top - ranked   aligned sub - word is exclusively used for transfer ,   as well as the aggregation of top- isub - words is   used . It can be found that the latter model almost   always outperforms the former model.616We compare the training time consumption of all   experiments , the result is shown in Table 5 . We use   mixed precision for training the child MT model .   All experiments are conducted on a single NVIDIA   P100 16 GB GPU .   Obviously , the time that Mean - PC consumes dur-   ing training is less than other models . In the sce-   nario of Tr - En MT , the training duration is even   shortened from 4.49 hours ( i.e. , about 269 minutes )   to 2.14 , compared to the baseline model . Most   probably , it is caused by the transferring of a larger   number of sub - word embeddings during training .   In other word , Mean - PC actually transfers not   only morphologically - identical sub - words but the   aligned ones . This contributes more to the avoid-   ance of redundant learning over sub - word embed-   dings . All in all , Mean - PC is less time - consuming   when producing substantial improvements .   6 Conclusion   We enhance transferable Parent - Child NMT by du-   plicating embeddings of aligned sub - words . The   experimental results demonstrate that the proposed   method yields substantial improvements for all the   considered MT scenarios ( including My - En , Id - En   and Tr - En ) . More importantly , we successfully re-   duce the training duration . The efﬁciency can be   improved with the ratio of about 50 % at best .   Additional survey in the experiments reveals that   phonetic symbols can be used for transfer learning   between the languages belonging to different fami-   lies . For example , the phonologies of hamburger   in German and Burmese are similar ( H amburger   vs hambhargar ) . In the future , we will study bilin-   gual embedding transfer of phonologically - similar   words , so as to further improve low - resource NMT .   Acknowledgements   The research is supported by National Key R&D   Program of China ( 2020YFB1313601 ) and Na-   tional Science Foundation of China ( 62076174 ) .   References617618619