  Xindi Wang , Robert E. Mercer , and Frank RudziczDepartment of Computer Science , University of Western Ontario , London , Ontario , CanadaDepartment of Computer Science , University of Toronto , Toronto , Ontario , CanadaVector Institute for Artiﬁcial Intelligence , Toronto , Ontario , CanadaUnity Health Toronto , Toronto , Ontario , Canada   xwang842@uwo.ca , mercer@csd.uwo.ca , frank@cs.toronto.edu   Abstract   Currently , Medical Subject Headings ( MeSH )   are manually assigned to every biomedical ar-   ticle published and subsequently recorded in   the PubMed database to facilitate retrieving   relevant information . With the rapid growth   of the PubMed database , large - scale biomedi-   cal document indexing becomes increasingly   important . MeSH indexing is a challenging   task for machine learning , as it needs to as-   sign multiple labels to each article from an   extremely large hierachically organized col-   lection . To address this challenge , we pro-   pose KenMeSH , an end - to - end model that   combines new text features and a dynamic   Knowledge- enhanced mask attention that inte-   grates document features with MeSH label hi-   erarchy and journal correlation features to in-   dex MeSH terms . Experimental results show   the proposed method achieves state - of - the - art   performance on a number of measures .   1 Introduction   The PubMeddatabase is a resource that provides   access to the MEDLINE bibliographic database   of references and abstracts together with the full   text articles of some of these citations which are   available in the PubMed Central(PMC ) repository .   MEDLINEcontains more than 28 million refer-   ences ( as of Feb. 2021 ) to journal articles in the   biomedical , health , and related disciplines . Jour-   nal articles in MEDLINE are indexed according   toMedical Subject Headings ( MeSH ) , an hier-   archically organized vocabulary that has been de-   veloped and maintained by the National Library   of Medicine ( NLM ) . Currently , there are 29,369   main MeSH headings , and each MEDLINE citationhas 13 MeSH indices , on average . MeSH terms are   distinctive features of MEDLINE and can be used   in many applications in biomedical text mining and   information retrieval ( Lu et al . , 2008 ; Huang et al . ,   2011 ; Gu et al . , 2013 ) , being recognized as impor-   tant tools for research ( e.g. , knowledge discovery   and hypothesis generation ) .   Currently , MeSH indexing is done by human   annotators who examine full articles and assign   MeSH terms to each article according to rules set   by NLM . Human annotation is time consuming   and costly – the average cost of annotating one   article in MEDLINE is about $ 9.40 ( Mork et al . ,   2013 ) . Nearly 1 million citations were added to   MEDLINE in 2020 ( approximately 2,600 on a daily   basis ) . The rate of articles being added to the   MEDLINE database is constantly increasing , so   there is a huge ﬁnancial and time - consuming cost   for the status quo . Therefore , it is imperative to   develop an automatic annotation system that can   assist MeSH indexing of large - scale biomedical   articles efﬁciently and accurately .   Automatic MeSH indexing can be regarded as an   extreme multi - label text classiﬁcation ( XMC ) prob-   lem , where each article can be labeled with multi-   ple MeSH terms . Compared with standard multi-   label problems , XMC ﬁnds relevant labels from an   enormous set of candidate labels . The challenge of   large - scale MeSH indexing comes from both the   label and article sides . Currently , there are more   than 29,000 distinct MeSH terms , and new MeSH   terms are updated to the vocabulary every year . The   frequency of different MeSH terms appearing in   documents are quite imbalanced . For instance , the   most frequent MeSH term , ‘ humans ’ , appears in   more than 8 million citations ; ‘ Pandanaceae ’ , on   the other hand , appears in only 31 documents ( Zhai2941et al . , 2015 ) . In addition , the MeSH terms that   have been assigned to each article varies greatly ,   ranging from more than 30 to fewer than 5 . Further-   more , semantic features of the biomedical literature   are complicated to capture , as they contain many   domain - speciﬁc concepts , phrases , and abbrevia-   tions . The aforementioned difﬁculties make the   task more complicated to generate an effective and   efﬁcient prediction model for MeSH indexing .   In this work , inspired by the rapid development   of deep learning , we propose a novel neural archi-   tecture called KenMeSH ( Knowledge- enhanced   MeSH labelling ) which is suitable for handling   XMC problems where the labels are arrayed hi-   erarchically and could capture useful information   as a directed graph . Our method uses a dynamic   knowledge - enhanced mask attention mechanism   and incorporates document features together with   label features to index biomedical articles . Our   major contributions are :   1.We design a multi - channel document repre-   sentation module to extract document features   from the title and the abstract using a bidi-   rectional LSTM . We use multi - level dilated   convolution to capture semantic units in the   abstract channel . This module combines a   hybrid of information , at the levels of words   and the latent representations of the semantic   units , to capture local correlations and long-   term dependencies from text .   2.Our proposed method appears to be the ﬁrst to   employ graph convolutional neural networks   that integrate information from the complete   MeSH hierarchy to map label representations .   3.We propose a novel dynamic knowledge-   enhanced mask attention mechanism which   incorporates external journal - MeSH co-   occurrence information and document similar-   ity in the PubMed database to constrain the   large universe of possible labels in the MeSH   indexing task .   4.We evaluate our model on a corpus of PMC   articles . Our proposed method consistently   achieves superior performance over previous   approaches on a number of measures .   2 Related Work   2.1 Automatic MeSH Indexing   To address the MeSH indexing task mentioned in   above section , the National Library of Medicinedeveloped Medical Text Indexer ( MTI ) – software   that automatically recommends MeSH terms to   each MEDLINE article using the abstract and ti-   tle as input ( Aronson et al . , 2004 ) . It ﬁrst gener-   ates the candidate MeSH terms for given articles ,   and then ranks the candidates to provide the ﬁ-   nal predictions . There are two modules in MTI –   MetaMap Indexing ( MMI ) and PubMed - Related   Citations ( PRC ) ( Lin and Wilbur , 2007 ; Aronson   and Lang , 2010 ) . MetaMap is NLM - developed   software which extracts the biomedical concepts   in the documents and maps them to Uniﬁed Medi-   cal Language System concepts . MMI recommends   MeSH terms using the biomedical concepts discov-   ered by MetaMap . PRC uses k - nearest neighbours   to ﬁnd the MeSH annotations of similar citations   in MEDLINE . The two mentioned sets of MeSH   terms combine the ﬁnal MeSH recommendations   from MTI .   BioASQ , an EU - funded project , has organized   challenges on automatic MeSH indexing since   2013 , which provides opportunities to involve   more participants in continuing to the develop-   ment of MeSH indexing systems . Many effec-   tive MeSH indexing systems have been developed   since then , such as MeSHLabeler ( Liu et al . , 2015 ) ,   DeepMeSH ( Peng et al . , 2016 ) , AttentionMeSH   ( Jin et al . , 2018 ) , and MeSHProbeNet ( Xun et al . ,   2019 ) . MeSHLabeler introduced a Learning - to-   Rank ( LTR ) framework , which is a two - step strat-   egy , ﬁrst predicting the candidate MeSH terms and   then ranking them to obtain the ﬁnal suggestions .   MeSHLabeler ﬁrst trained an independent binary   classiﬁer for each MeSH term and then used var-   ious evidence , including similar publications and   term frequencies , to rank candidate MeSH terms .   DeepMeSH is an improved version of MeSHLa-   beler , which also uses the LTR strategy . It ﬁrst   generates MeSH predictions by incorporating deep   semantics in the word embedding space , and then   ranks the candidates . AttentionMeSH and MeSH-   ProbeNet are based on bidirectional recurrent neu-   ral networks ( RNNs ) and attention mechanisms .   The main difference between AttentionMeSH and   MeSHProbeNet is that the former uses a label - wise   attention mechanism while the latter develops self-   attentive MeSH probes to extract comprehensive   aspects of information from the input articles .   Studies in MeSH indexing with full texts are   very limited because of restrictions on full text ac-2942cess . Jimeno - Yepes et al . ( 2013 ) randomly selected   1413 articles from the PMC Open Access Subset   and used automatically - generated summaries from   these full texts as input to MTI for MeSH index-   ing . Demner - Fushman and Mork ( 2015 ) collected   14,828 full text articles from PMC Open Access   Subset and developed a rule - based string - matching   algorithm to extract a subject of MeSH terms called   ‘ check tags ’ that are used to describe the charac-   teristics of the subjects . Wang and Mercer ( 2019 )   randomly selected 257,590 full text articles from   PMC Open Access Subset and developed a multi-   channel model using CNN - based feature selection   to extract important information from different sec-   tions of the articles . HGCN4MeSH ( Yu et al . ,   2020 ) used the PMC dataset generated by Wang   and Mercer ( 2019 ) and employed graph convolu-   tional neural network to learn the co - occurrences   between MeSH terms . FullMeSH ( Dai et al . , 2019 )   and BERTMeSH ( You et al . , 2020 ) used all avail-   able full text articles in PMC Open Access Subset .   FullMeSH applied an attention - based CNN to pre-   dict the MeSH terms and LTR to get the ﬁnal MeSH   candidates ; BERTMeSH incorporated pre - trained   BERT and an attention mechanism to improve the   performance of MeSH indexing .   2.2 Graph Convolutional Networks in   Natural Language Processing   Graph convolutional neural networks ( GCN)s ( Kipf   and Welling , 2017 ) have received considerable at-   tention and achieved remarkable success in natural   language processing recently .   Some text classiﬁcation systems introduce GCN   by formulating their problems as graph - structural   tasks . For instance , TextGCN ( Yao et al . , 2019 )   built a single text graph for a corpus based on word   co - occurrence and document word relations to in-   fer labels . Zhang et al . ( 2019a ) built a GCN - based   dependency tree of a sentence to exploit syntactical   information and word dependencies for sentiment   analysis . Other research focused on learning the   relationships between nodes in a graph , such as the   label co - occurrences for multi - label text classiﬁca-   tions ; e.g. , MAGNET ( Pal et al . , 2020 ) built a la-   bel graph to capture dependency structures among   labels , and Rios and Kavuluru ( 2018 ) built a multi-   label classiﬁer that was learned from a 2 - layer GCN   over the label hierarchy .   GCN also provides a powerful toolkit for embed-   ding the taxonomies into low dimension represen - tations that could be utilized for speciﬁc tasks . For   instance , Pujary et al . ( 2020 ) used GCN to learn an   undirected graph derived from disease names in the   MeSH taxonomy in order to detect and normalize   disease mentions in biomedical texts .   3 Proposed Model   MeSH indexing can be regarded as a multi - label   text classiﬁcation problem in which , given a set   of biomedical documents X = fx;x;:::;xg   and a set of MeSH labels Y = fy;y;:::;yg ,   multi - label classiﬁcation learns the function f :   X ! [ 0;1]using the training set D= ( x;Y ) ,   i= 1;:::;n , wherenis the number of documents   in the set .   Figure 1 illustrates our overall architecture . Our   model is composed of a multi - channel document   representation module , a label features learning   module , a dynamic semantic mask attention mod-   ule , and a classiﬁer .   3.1 Multi - channel Document Representation   Module   The multi - channel document representation mod-   ule has two input channels – the title channel and   the abstract channel , for each type of text . These   two texts are represented by two embedding ma-   trices , namely E2R , the word embedding   matrix for the title , and E2R , the word   embedding matrix for the abstract . We ﬁrst apply a   bidirectional Long Short - Term Memory ( biLSTM )   network ( Hochreiter and Schmidhuber , 1997 ) in   both channels to encode the two types of text and   to generate the hidden representations hfor each   word at time step t. The computations of  ! hand  hare illustrated below :     ! h = LSTM ( x;  !h;c )     h = LSTM ( x ;   h;c)(1 )   We then obtain the ﬁnal representation for each   word by concatenating the hidden states from both   directions , namely h= [   ! h :  h]andh2R ,   wherelis the number of words in the text and d   is the hidden dimensions . The biLSTM returns   context - aware representations HandH   for the title and abstract channels , respectively :   H = biLSTM ( E )   H = biLSTM ( E ) ( 2)2943   In order to generate high - level semantic represen-   tations of abstracts , we introduce a dilated convo-   lutional neural network ( DCNN ) to the abstract   channel . The concept of dilated convolution was   originally developed for wavelet decomposition   ( Holschneider et al . , 1990 ) , and has been applied   to NLP tasks such as neural machine translation   ( Kalchbrenner et al . , 2017 ) and text classiﬁcation   ( Lin et al . , 2018 ) . The main idea of DCNN is to   insert ‘ holes ’ in convolutional kernels , which ex-   tract the longer - term dependencies and generate   higher - level representations , such as phases and   sentences . Following Lin et al . ( 2018 ) , we apply   a multi - level DCNN with different dilation rates   on top of the hidden representations generated by   the biLSTM on the abstract channel . Small di-   lation rates capture phrase - level information , and   large ones capture sentence - level information . The   DCNN returns the semantic features of the abstract   channelD2R , wheresis the   width of the convolution kernels .   3.2 Label Features Learning Module   MeSH taxonomies are organized in 16 categories ,   and each is further divided into subcategories .   Within each subcategory , MeSH terms are ordered   hierarchically from most general to most speciﬁc , up to 13 hierarchical levels . As the MeSH hierarchy   is important to our task , we use a two - layer GCN   to incorporate the hierarchical parent and child in-   formation among labels . We ﬁrst use the MeSH   descriptors to generate a label feature vector for   each MeSH term . Each label vector is calculated   by averaging the word embedding of each word in   its descriptors :   v=1   NXw;i= 1;2;:::;L ; ( 3 )   wherev2R , Nis the number of words in its de-   scriptor , and Lis the number of labels . In the graph   structure , we formulate each node as a MeSH la-   bel , and edges represent relationships in the MeSH   hierarchy . The edge types of a node include edges   from its parent , from its children , and from itself .   At each GCN layer , the node feature is aggregated   by its parent and children to form the new label   feature for the next layer :   h=(AhW ) ; ( 4 )   wherehandh2Rindicate the node pre-   sentation of the land(l+ 1)layers,()de-   notes an activation function , Ais the adjacency   matrix of the MeSH hierarchical graph , and Wis2944a layer - speciﬁc trainable weight matrix . We then   concatenate the label feature vectors from descrip-   tors in Equation 3 with GCN label vectors to form :   H= [ v : h ] ; ( 5 )   whereH2Ris the ﬁnal label vector .   3.3 Dynamic Knowledge - enhanced Mask   Attention Module   In the dynamic knowledge - enhanced mask atten-   tion module , we integrate external knowledge from   outside sources to generate a unique mask for each   article dynamically . We consider only a subset of   the full MeSH list by employing a masked label-   wise attention that computes the element - wise mul-   tiplication of a mask matrix and an attention ma-   trix for two reasons . First , the MeSH terms are   numerous and have widely varying occurrence fre-   quencies . Therefore , for each MeSH label , there   are far more negative examples than positive ones .   For each article , selecting a subset of MeSH labels ,   namely a MeSH mask , down - samples the negative   examples , which forces the classiﬁer to concentrate   on the candidate labels . Second , the issue with   the original attention mechanism ( Bahdanau et al . ,   2015 ) is that the classiﬁer focuses on spotting rele-   vant information for all predicted labels , which is   a lack of pertinence . Using a masked label - wise   attention allows the classiﬁer to ﬁnd relevant infor-   mation for each label inside the MeSH mask .   The dynamic ensures that the module generates   a unique MeSH mask for each article , speciﬁcally .   To generate the MeSH masks , we consider two   external knowledge sources : journal information   and document similarity . The journal information   refers to the name of the journal in which an ar-   ticle was published , which usually deﬁnes a spe-   ciﬁc research domain . We expect that articles pub-   lished in the same journal tend to be indexed with   MeSH terms that are relevant to the journal ’s re-   search focus . We build a journal – MeSH label co-   occurrence matrix using conditional probabilities ,   i.e. ,P(LjJ ) , which denote the probabilities of   occurrence of label Lwhen journal Jappears .   P(LjJ ) = C   C ; ( 6 )   whereCdenotes the number of co-   occurrences of LandJ , andCis the number of   occurrences of Jin the training set . To avoid the   noise of rare co - occurrences , a threshold  ﬁltersnoisy correlations . Mdenotes the MeSH label set   for journalj .   M = fLjP(LjJ ) >  ; k = 1;:::;Lg(7 )   We then use k - nearest neighbors ( KNN ) to choose   a subset of speciﬁc MeSH terms for each article   by referring to document similarity . We represent   each article by the IDF - weighted sum of word em-   beddings in the abstract :   D = PIDFePIDF ; ( 8)   whereeis the word embedding , and IDFis the   inverse document frequency of the word . Next ,   we use KNN based on cosine similarity between   abstracts to ﬁnd the Knearest neighbours for each   article in the training set . To form the unique MeSH   mask for article a , we collect MeSH terms Mfrom   the neighbours of a :   M = T[T[:::[T ; ( 9 )   whereTis the MeSH label set from the ineigh-   bour of article a. We then join the MeSH labels   generated from journal – MeSH co - occurrence for   the journal that article ahas been published in to-   gether with the MeSH terms obtained from the   neighbours of article ato form the ﬁnal MeSH   mask label set M :   M = M[M ( 10 )   Then we assign a value to each label in Yto form   M2[0;1 ] . If the label appears in M , we   assign 1 , 0 otherwise . The label order of Mis   the same asH.   We calculate the similarity between MeSH terms   and the texts in two channels by applying masked   label - wise attention .   H = H  M   = Softmax ( HH )    = Softmax ( DH ) ; ( 11 )   where  denotes element - wise multiplication ,   H denotes the masked label features , and   and   measure how informative each   text fragment is for each label in the title and ab-   stract channels , respectively . We then generate the   label - speciﬁc title and abstract representations , re-   spectively :   c=  H   c =  D;(12)2945   such thatc2R , andc2R. We   sum up the representations in the title and abstract   channels to form the document vector for each   article :   D = c+c ( 13 )   3.4 Classiﬁer   We gain scores for each MeSH term i :   ^y=(D  H);i= 1;2;:::;L ; ( 14 )   where()represents the sigmoid function . We   train our model using the multi - label binary cross-   entropy loss ( Nam et al . , 2014 ):   L = X[ ylog ( ^y) (1 y)log(1 ^y ) ) ] ;   ( 15 )   wherey2[0;1]is the ground truth of label i , and   ^y2[0;1]denotes the prediction of label iobtained   from the proposed model.4 Experiment   4.1 Datasets   We follow Dai et al . ( 2019 ) and You et al . ( 2020 ) by   using the PMC FTP service(Comeau et al . , 2019 )   and downloading PMC Open Access Subset ( as of   Sep. 2021 ) , totalling 3,601,092 citations . We also   download the entire MEDLINE collection based   on the PubMed Annual Baseline Repository ( as of   Dec. 2020 ) and obtain 31,850,051 citations with   titles and abstracts . In order to reduce bias , we only   focus on articles that are annotated by human cura-   tors ( not annotated by a ‘ curated ’ or ‘ auto ’ modes   in MEDLINE ) . We then match PMC articles with   the citations in PubMed to PMID and obtain a set   of 1,284,308 citations . Out of these PMC articles ,   we use the latest 20,000 articles as the test set , the   next latest 200,000 articles as the validation data   set , and the remaining 1.24 M articles as the train-   ing set . In total , 28,415 distinct MeSH terms are   covered in the training dataset .   4.2 Implementation Details   We implement our model in PyTorch ( Paszke   et al . , 2019 ) . For pre - processing , we removed non-   alphanumeric characters , stop words , punctuation ,   and single character words , and we converted all   words to lowercase . Titles longer than 100 char-   acters and abstracts longer than 400 characters are   truncated . We use pre - trained biomedical word em-   beddings ( BioWordVec ) ( Zhang et al . , 2019b ) , and   the embedding dimension is 200 . To avoid overﬁt-   ting , we use dropout directly after the embedding   layer with a rate of 0:2 . The number of units in   hidden layers are 200 in all three modules . We   use a three - level dilated convolution with dilation   rate[1;2;3]and select 1000 nearest documents to   generate MeSH masks for each article . We use   FAISS ( Johnson et al . , 2019 ) to ﬁnd similar docu-   ments for each citation among the training set , and   the whole process takes 10 hours . We use Adam   optimizer ( Kingma and Ba , 2015 ) and early stop-   ping strategies . The learning rate is initialized to   0:0003 , and the decay rate is 0:9 in every epoch .   The gradient clip is applied to the maximum norm   of 5 . The batch size is 32 . The model trained for   50 hours on a single NVIDIA V100 GPU . The   detailed hyper - parameter settings are shown in Ta-   ble 3 . The code for our method is available at   https://github.com/xdwang0726/KenMeSH.2946   4.3 Evaluation Metrics   We use three main evaluation metrics to test the   performance of MeSH indexing systems : Micro-   average measure ( MiM ) , example - based measure   ( EBM ) , and ranking - based measure ( RBM ) , where   MiM and EBM are commonly used in MeSH in-   dexing tasks and RBM is commonly used in evalu-   ating multi - label classiﬁcation . Micro - average F-   measure ( MiF ) aggregate the global contributions   of all MeSH labels and then calculate the harmonic   mean of micro - average precision ( MiP ) and micro-   average recall ( MiR ) , which are heavily inﬂuenced   by frequent MeSH terms . Example - based measures   are computed per data point , which computes the   harmonic mean of standard precision ( EBP ) and re-   call ( EBR ) for each data point . In the ranking - based   measure , precision at k(P@k ) shows the number   of relevant MeSH terms that are suggested in the   top - krecommendations of the MeSH indexing sys-   tem , and recall at k(R@k ) indicates the proportion   of relevant items that are suggested in the top- k   recommendations . The detailed computations of   evaluation metrics can be found in Appendix A.   The threshold has a large inﬂuence on MiF and   EBF , see Appendix B. We select ﬁnal MeSH labels   whose predicted probability is larger than a tuned   thresholdt :   MeSH= (   ^yt;1   ^y < t;0(16 )   wheretis the threshold for MeSH term i. We   compute optimal threshold for each MeSH term on   the validation set following Pillai et al . ( 2013 ) that   tunestby maximizing MiF :   t= argmaxMiF(T ) ; ( 17 )   where Tdenotes all possible threshold values for   labeli.5 Results and Ablation Studies   We evaluate our proposed model with ﬁve   state - of - the - art models : MTI , DeepMeSH ,   FullMeSH , BERTMeSH and HGCN4MeSH .   Among these , MTI , DeepMeSH , BERTMeSH ,   and HGCN4MeSH are trained with abstracts and   titles only ; FullMeSH ( Full ) and BERTMeSH   ( Full ) are trained with full PMC articles . Our   proposed model is trained on titles and abstracts ,   and is tested using 20,000 of the latest articles . We   mainly focus on MiF , which is the main evaluation   metric in MeSH indexing task .   We compare our model against previous related   systems on micro - average measure and example-   bases measure in Table 1 . Each row in the table   shows all evaluation metrics on a speciﬁc method ,   where the best score for each metric is indicated .   As reported , our model achieves the best perfor-   mance on most evaluation metrics , expect MiR and   EBR , on which BERTMeSH ( Full ) achieves the   best performance . This is because that BERTMeSH   ( Full ) is trained on full text articles , which uses   much more content information in the articles than   ours . Our model outperforms the subset of sys-   tems that were trained only on the abstract and   the title – MTI , HGCN4MeSH , DeepMeSH and   BERTMeSH in all metrics . Most importantly , there   is improvement in precision without a decrease in   recall . Comparing with systems trained on full arti-   cles indicates that our model achieves the best MiF ,   and is only slightly below BERTMeSH ( Full ) on   MiR ( 0.4 percentage points ) . Although our model   is trained only on the abstract and title ( which may   suggest that it captures less complex semantics ) ,   it performs very well against more complex sys-   tems . Furthermore , we compare the performance   of our model with HGCN4MeSH on ranking - based   measures that do not require a speciﬁc threshold .   The results , summarized in Table 2 , show that our   model always performs better than HGCN4MeSH   with up to almost 18 % improvement .   As the frequency of different MeSH terms are   imbalanced , we are interested in examining the efﬁ-   ciency of our model on infrequent MeSH terms . We   divide MeSH terms into four groups based on the   number of occurrences in the training set : ( 0;100 ) ,   [ 100;1000 ) , [ 1000;5000 ) , and [ 5000 ;) . Figure 2a   shows the distribution of MeSH terms and percent   of occurrence among the four divided groups in   the training set , which indicates that the distribu-   tion of MeSH frequency is highly biased and it2947   falls into a long - tail distribution . Figure 2b and   2c show the performance of our model comparing   to MTI baseline in the four MeSH groups on MiF   and EBF respectively . Our model obtains substan-   tial improvements among frequent and infrequent   labels on both MiF and EBF .   We are interested in studying how the effective-   ness and robustness of our model are due to the   various modules , such as the multi - channel mecha-   nism , the dilated CNN , the label graph , and masked   attention . To further understand the impacts of   these factors , we conduct controlled experiments   with four different settings : ( a ) examining a sin-   gle channel architecture by concatenating the title   and abstract as input into the abstract channel ; ( b )   removing the dilated CNN ; ( c ) replacing the la-   bel feature learning module with a fully connected   layer ; and ( d ) removing the masked attention mod-   ule . The inﬂuence of each of these modules can   then be evaluated individually . The results are sum-   marized in Table 4 .   Impacts on Multi - channel Settings As   shown in Table 4 , the multi - channel setting   outperforms the single channel one . The reason   for this could be that the single channel model   misses some important features in titles andabstracts in the LSTM layer . LSTM has the   capability to learn and remember over long   sequences of inputs , but it can be challenging   to use when facing very long input sequences .   Concatenating the title and abstract into one longer   sequence may hurt the performance of LSTM .   To be more explicit , the single channel model   may be remembering insigniﬁcant features in the   LSTM layer when dealing with longer sequences .   Therefore , extracting information from the title   and the abstract separately is better than directly   concatenating the information .   Impacts on Dilated Semantic Feature Extrac-   tions As reported in Table 4 , the performance   drops when removing the dilated CNN layer . The   reason for this seems to be that multi - level dilated   CNNs can extract high - level semantic information   from the semantic units that are often wrapped in   phrases or sentences , and then capture local correla-   tion together with longer - term dependencies from   the text . Compared with word - level information   extracted from the biLSTM layer , high - level infor-   mation extracted from the semantic units seems to   provide better understanding of the text , at least for   the purposes of labelling.2948Impacts on Learning Label Features As   shown in Table 4 , not learning the label features has   the largest negative impacts on performance espe-   cially for recall ( and subsequently F - measure ) . By   removing the label features , the model pays more   attention to the frequent MeSH terms and misclas-   siﬁes infrequent labels as negative . This indicates   that label features learned through GCN can cap-   ture the hierarchical information between MeSH   terms , and MeSH indexing for infrequent terms can   beneﬁt from this hierarchical information .   Impacts on Dynamic Knowledge - enhanced   Mask Attention Table 4 shows a performance   drop when removing the masked attention layer ,   suggesting that the attention mechanism has pos-   itive impacts on performance . This result further   suggest that the masked attention takes advantage   of incorporating external knowledge to alleviate the   extremely large pool of possible labels . To select   the proper mask for each article , two hyperparame-   ters are used : threshold  for journal - MeSH occur-   rence and the number of nearest articles K. With   = 0:5andK= 1000 , all of the gold - standard   MeSH labels are guaranteed to be in the mask .   6 Conclusion   We propose a novel end - to - end model integrating   document features and label hierarchical features   for MeSH indexing . We use a novel dynamic   knowledge - enhanced mask attention mechanism to   handle the large universe of candidate MeSH terms   and employ GCN in extracting label correlations .   Experimental results demonstrate that our proposed   model signiﬁcantly outperforms the baseline mod-   els and provides especially large improvements on   infrequent MeSH labels .   In the future , we believe two important research   directions will lead to further improvements . First ,   we plan to explore full text articles , which con-   tain more information , to see whether our model   takes advantage of the full text to improve the per-   formance of large - scale MeSH indexing . Second ,   we are interested in integrating knowledge from   the Uniﬁed Medical Language System ( UMLS )   ( Bodenreider , 2004 ) , a comprehensive ontology of   biomedical concepts , in our model .   Acknowledgements   We thank all reviewers and area chairs for their   constructive comments and feedback . Resourcesused in preparing this research were provided , in   part , by Compute Ontario , Compute Canada ,   the Province of Ontario , the Government of Canada   through CIFAR , and companies sponsoring the Vec-   tor Institute . This research is partially funded by   The Natural Sciences and Engineering Research   Council of Canada ( NSERC ) through a Discovery   Grant to R. E. Mercer . F. Rudzicz is supported by   a CIFAR Chair in AI .   References29492950   A Evaluation Metrics   Micro F - measure ( MiF ) computes the harmonic   mean of micro - average precision ( MiF ) and micro-   average recall ( MiR ):   MiF=2MiRMiP   MiR+MiP ; ( 18 )   where   MiP = PTPPTP+PFP ; ( 19 )   MiR = PTPPTP+PFN ; ( 20 )   where TP , FPandFNas true positives , false   positives , and false negatives respectively for each   labellin the set of total labels L.   EBF can be computed as the harmonic mean of   standard precision ( EBP ) and recall ( EBR ):   EBF = 2EBREBP   EBR + EBP ; ( 21 )   where   EBP = 1   NXjy\^yj   j^yj ; ( 22 )   EBR = 1   NXjy\^yj   jyj ; ( 23 )   whereyis the true label set and ^yis the predicted   label set for instance i , Nrepresents the total num-   ber of instance . Ranking - based evaluation , including precision   atk(P@k ) , and recall at k(R@k ) . The metrics are   deﬁned as follows :   P@k = 1   kXy ; ( 24 )   R@k = 1   jyjXy ; ( 25 )   whererreturns the top- krecommended items .   B Threshold Selection Affects the   Measurements   Thresholds have a huge impact on multi - label   evaluation measures . We test the model ’s per-   formance on the example - based measure and the   micro - average measure under different thresholds ,   and the results are summarized in Table 5 . Our goal   is to obtain a maximized MiF.2951