  Xinchen Yu   University of North TexasEduardo Blanco   Arizona State UniversityLingzi Hong   University of North Texas   Abstract   Hate speech is plaguing the cyberspace along   with user - generated content . This paper in-   vestigates the role of conversational context   in the annotation and detection of online hate   and counter speech , where context is deﬁned   as the preceding comment in a conversation   thread . We created a context - aware dataset   for a 3 - way classiﬁcation task on Reddit com-   ments : hate speech , counter speech , or neutral .   Our analyses indicate that context is critical to   identify hate and counter speech : human judg-   ments change for most comments depending   on whether we show annotators the context . A   linguistic analysis draws insights into the lan-   guage people use to express hate and counter   speech . Experimental results show that neural   networks obtain signiﬁcantly better results if   context is taken into account . We also present   qualitative error analyses shedding light into   ( a ) when and why context is beneﬁcial and ( b )   the remaining errors made by our best model   when context is taken into account .   1 Introduction   The advent of social media has democratized public   discourse on an unparalleled scale . Meanwhile , it is   considered a particularly conducive arena for hate   speech ( Caiani et al . , 2021 ) . Online hate speech   is prevalent and can lead to serious consequences .   At the individual level , the victims targeted by hate   speech are frightened by online threats that may   materialize in the real world ( Olteanu et al . , 2018 ) .   At the societal level , it has been reported that there   is an upsurge in ofﬂine hate crimes targeting mi-   norities ( Olteanu et al . , 2018 ; Farrell et al . , 2019 ) .   There are two common strategies to combat on-   line hate : disruption and counter speech . Disrup-   tion refers to blocking hateful content or users .   To scale this strategy , researchers have proposed   methods to identify hate ( Waseem and Hovy , 2016 ;   Davidson et al . , 2017 ; Nobata et al . , 2016 ) . While   these interventions could de - escalate the impact of   Table 1 : Reddit comments ( Target s ) deemed to be Hate ,   Neutral , or Counter - hate depending on whether one   takes into account the previous comment ( Parent ) .   hate speech , they may violate online free speech   ( Mathew et al . , 2019 ) . Additionally , attacks at the   micro - level may be ineffective as hate networks   often have rapid rewiring and self - repairing mech-   anisms ( Johnson et al . , 2019 ) . Counter speech   refers to the “ direct response that counters hate   speech ” ( Mathew et al . , 2019 ) . It has been shown   to be more effective in the long term than disruption   in theoretical and empirical studies ( Richards and   Calvert , 2000 ; Mathew et al . , 2020 ) . Identifying   hate and counter speech in natural conversations   is critical to understand effective counter speech   strategies and the generation of counter speech .   Most corpora with either hate speech ( Hate ) or   counter speech ( Counter - hate ) annotations do not   include conversational context . Indeed , they anno-   tate a user - generated comment as Hate or Counter-   hate based on the comment in isolation ( Davidson   et al . , 2017 ; Waseem and Hovy , 2016 ; Mathew   et al . , 2019 ; He et al . , 2021 ) . Therefore , systems   trained on these corpora fail to consider the ef-   fect of contextual information on the identiﬁcation   of Hate and Counter - hate . Recent studies have   shown that context affects annotations in toxic-5918ity and abuse detection ( Pavlopoulos et al . , 2020 ;   Menini et al . , 2021 ) . We further investigate the   effect of context on the task of identifying Hate   and Counter - hate . Table 1 shows examples where   a comment , denoted as Target , is Hate , Neutral or   Counter - hate depending on whether the preceding   comment , denoted as Parent , is taken into account .   In the top example , the Target goes from Neutral   to Hate when taking into account the Parent : it   becomes clear that the author is disparaging short   men . In the bottom example , the Target goes from   Hate to Counter - hate as the author uses offensive   language to counter the hateful content in the Par-   ent . This is a common strategy to express counter   speech ( Mathew et al . , 2019 ) .   In this study , we answer the following questions :   1.Does conversational context affect if a com-   ment is perceived as Hate , Neutral , or Counter-   hate by humans ? ( It does . )   2.Do models to identify Hate , Neutral , and   Counter - hate beneﬁt from incorporating con-   text ? ( They do . )   To answer the ﬁrst question , we create a collec-   tion of ( Parent , Target ) Reddit comments and an-   notate the Targets with three labels ( Hate , Neutral ,   Counter - hate ) in two independent phases : showing   annotators ( a ) only the Target or ( b ) the Parent and   theTarget . We limit context to the parent com-   ment . While the full conversation could provide   additional information , it is also known to affect   annotators ’ stance ( Dutta et al . , 2020 ) and intro-   duce biases . We ﬁnd that human judgments are   substantially different when the Parent is shown .   Thus the task of annotating Hate and Counter - hate   requires taking into account the context .   To answer the second question , we experiment   with context - unaware and context - aware classiﬁers   to detect if a given Target is Hate , Neutral , or   Counter - hate . Results show that adding context   does beneﬁt the classiﬁers signiﬁcantly .   In summary , the main contributions of this paper   are:(a ) a corpus with 6,846 pairs of ( Parent , Tar-   get ) Reddit comments and annotations indicating   whether the Target s are Hate , Neutral , or Counter-   hate ; ( b ) annotation analysis showing that the prob-   lem requires taking into account context , as the   ground truth changes ; ( c ) corpus analysis detailing   the kind of language people use to express Hateand Counter - hate ; ( d ) experiments showing that   context - aware neural models obtain signiﬁcantly   better results ; and ( e ) qualitative analysis revealing   when context is beneﬁcial and the remaining errors   made by the best context - aware model .   2 Related Work   Hate speech in user - generated content has been an   active research area recently ( Fortuna and Nunes ,   2018 ) . Researchers have built several datasets for   hate speech detection from diverse sources such as   Twitter ( Waseem and Hovy , 2016 ; Davidson et al . ,   2017 ) , Yahoo ! ( Nobata et al . , 2016 ) , Fox News   ( Gao and Huang , 2017 ) , Gab ( Mathew et al . , 2021 )   and Reddit ( Qian et al . , 2019 ) .   Compared to hate speech detection , few stud-   ies focus on detecting counter speech ( Mathew   et al . , 2019 ; Garland et al . , 2020 ; He et al . , 2021 ) .   Mathew et al . ( 2019 ) collect and hand - code 6,898   counter hate comments from YouTube videos tar-   geting Jews , Blacks and LGBT communities . Gar-   land et al . ( 2020 ) work with German tweets and de-   ﬁne hate and counter speech based on the commu-   nities to which the authors belong . He et al . ( 2021 )   use a collection of hate and counter hate keywords   relevant to COVID-19 and create a dataset contain-   ing 359 counter hate tweets targeting Asians . An-   other line of research focuses on curating datasets   for counter speech generation using crowdsourc-   ing ( Qian et al . , 2019 ) or with the help of trained   operators ( Chung et al . , 2019 ; Fanton et al . , 2021 ) .   However , synthetic language is rarely as rich as   language in the wild . Even if it were , conclusions   and models from synthetic data may not transfer to   the real world . In this paper , we work with user-   generated content expressing hate and counter - hate   rather than synthetic content .   Table 2 summarizes existing datasets for Hate   and Counter - hate detection . Most of them do not in-   clude context information . In other words , the pre-   ceding comments are not provided when annotat-   ingTarget s. Context does affect human judgments   and has been taken into account for Hate detection   ( Gao and Huang , 2017 ; Pavlopoulos et al . , 2020 ;   Menini et al . , 2021 ; Vidgen et al . , 2021 ) . Gao and   Huang ( 2017 ) annotate hateful comments in the   nested structures of Fox News discussion threads .   Vidgen et al . ( 2021 ) introduce a dataset of Reddit   comments with annotations in 6 categories taking   into account context . However , the inter annotator   agreement is low ( Fleiss ’ Kappa 0.267 ) and the5919   number of Counter - hate instances is small ( 220 ) .   Moreover , both studies use contextual information   without identifying the role context plays in the   annotation and detection . Pavlopoulos et al . ( 2020 )   allow annotators to see one previous comment to   annotate Wikipedia conversations . They ﬁnd con-   text matters in the annotation but provide no em-   pirical evidence showing whether models to detect   toxicity beneﬁt from incorporating context . Menini   et al . ( 2021 ) re - annotate an existing corpus to inves-   tigate the role of context in abusive language . They   found context does matter . Utilizing conversational   context has also been explored in text classiﬁcation   tasks such as sentiment analysis ( Ren et al . , 2016 ) ,   stance ( Zubiaga et al . , 2018 ) and sarcasm ( Ghosh   et al . , 2020 ) . In this paper , we investigate the role   of context in Hate and Counter - hate detection .   3 Dataset Collection and Annotation   We ﬁrst describe our procedure to collect ( Parent ,   Target ) pairs , where both Parents andTargets are   Reddit comments in English . Then , we describe   the annotation guidelines and the two annotation   phases : showing annotators ( a ) only the Target and   ( b ) the Parent andTarget . The two independent   phases allow us to quantify how often context af-   fects the annotation of Hate and Counter - hate .   3.1 Collecting ( Parent , Target ) pairs   In this work , we focus on Reddit , a popular so-   cial media site . It is an ideal platform for data   collection due to the large size of user popula-   tions and many diverse topics ( Baumgartner et al . ,   2020 ) . We use a list of hate words to retrieve Red-   dit conversations to keep the annotation costs rea-   sonable while creating a ( relatively ) large corpusof counter speech . We start with a set of 1,726   hate words from two lexicons : Hatebaseand a   harassment corpus ( Rezvan et al . , 2018 ) . We re-   move ambiguous words following ElSherief et al .   ( 2018 ) . To collect ( Parent , Target ) pairs , we use the   following steps . First , we retrieve comments con-   taining at least one hate word ( comment ) .   Second , we create a ( Parent , Target ) pair using   comment asTarget and its preceding com-   ment as Parent . Third , we create a ( Parent , Tar-   get ) pair using comment asParent and   each of its replies as Target . Lastly , we remove   pairs if the same author posted the Parent andTar-   get . We retrieve 6,846 ( Parent , Target ) pairs with   PushShift ( Baumgartner et al . , 2020 ) from 416 sub-   missions . We also collect the title of the discussion   from which each pair is retrieved .   3.2 Annotation Guidelines   To identify whether a Target is Hate , Neutral , or   Counter - hate , we crowdsource human judgments   from non - experts . Our guidelines reuse the deﬁni-   tions of Hate by Ward ( 1997 ) and Counter - hate by   Mathew et al . ( 2019 ) and Vidgen et al . ( 2021 ):   •Hate : the author attacks an individual or a   group with the intention to vilify , humiliate ,   or incite hatred ;   •Counter - hate : the author challenges , con-   demns the hate expressed in another comment ,   or calls out a comment for being hateful ;   •Neutral : the author neither conveys hate nor   opposes hate expressed in another comment .   Annotation Process We chose Amazon Mechani-   cal Turk ( MTurk ) as the crowdsourcing platform .   We replace user names with placeholders ( User_A5920and User_B ) owing to privacy concerns . The an-   notations took place in two independent phases .   In the ﬁrst phase , annotators are ﬁrst shown the   Parent comment . After a short delay , they click a   button to show the Target and then after another   short delay they submit their annotation . Delays   are at most a few seconds and proportional to the   length of the comments . Our rationale behind the   delays is to “ force ” annotators to read the Parent   andTarget in order . In the second phase , annota-   tors label each Target without seeing the preceding   Parent comment . A total of 375 annotators were   involved in the ﬁrst phase and 299 in the second   phase . There is no overlap between annotators thus   we eliminated the possibility of biased annotators   remembering the Parent in the second phase .   Annotation Quality Crowdsourcing may attract   spammers ( Sabou et al . , 2014 ) . For quality con-   trol , we ﬁrst set a few requirements for annotators :   they must be located in the US and have a 95 %   approval rate over at least 100 Human Intelligence   Tasks ( HITs ) . We also block annotators who sub-   mit more than 10 HITs with an average completion   time below 5 seconds ( half the time required in our   pilot study ) . As the corpus contains vulgar words ,   we require annotators to pass the Adult Content   Qualiﬁcation Test . The reward per HIT is $ 0.05 .   The second effort is to identify bad annotators   and ﬁlter out their annotations until we obtain sub-   stantial inter - annotator agreement . We collect ﬁve   annotations per HIT and compute MACE ( Hovy   et al . , 2013 , Multi - Annotator Competence Esti-   mation ) for each annotator . MACE is devised to   rank annotators by their competence and adjudi-   cate disagreements based on annotator competence   ( not the majority label ) . Then , we use Krippen-   dorff’sα(Krippendorff , 2011 ) to estimate inter-   annotator agreement : αcoefﬁcients at or above 0.6   are considered substantial ( above 0.8 are consid-   ered nearly perfect ) ( Artstein and Poesio , 2008 ) .   We repeat the following steps until α≥0.6 :   1.Use MACE to calculate the competence score   of all annotators .   2.Discard all the annotations by the annotator   with the lowest MACE score .   3.Check Krippendorff ’s αon the remaining an-   notations . Go to ( 1 ) if α < 0.6 .   The ﬁnal corpus consists of 6,846 ( Parent , Tar-   get ) pairs and a label assigned to each Target ( Hate ,   Counter - hate , or Neutral ) . The ground truth we   experiment with ( Section 5 ) is the label obtained   taking into account the Parent ( ﬁrst phase ) . The   second phase , which disregards the Parent , was   conducted for analysis purposes ( Section 4 ) . We   split the corpus into two subsets : ( a ) Gold ( 4,751   pairs withα≥0.6 ) and ( b ) Silver ( 2,095 remain-   ing pairs ) . As we shall see , the Silver pairs are   useful to learn models .   4 Corpus Analysis   Does conversational context affect if a comment   is perceived as Hate or Counter - hate ? Yes , it   does . Table 3 presents the percentage of labels   that change and remain the same depending on   whether annotators are shown the Parent , i.e. , the   context . Many Target comments that are perceived   as Hate or Counter - hate become Neutral ( 34.2 %   and 55.1 % respectively ) when the Parent is pro-   vided . More surprisingly , many Target comments   are perceived with the opposite label ( from Hate to   Counter - hate ( 8.4 % ) or from Counter - hate to Hate   ( 18.7 % ) ) when the Parent comments are shown .   We show examples of label changes in Table 4 .   In the ﬁrst example , annotators identify the Target   ( “ A brick is more effective . ” ) as Neutral without   seeing the Parent . In fact , a female is the target of   hate in the Parent , and the author of Target replies5921   with even more hatred ( and the ground truth label   is Hate ) . In the second example , the Target alone   is insufﬁcient to tell if it is Counter - hate . When an-   notators see the Parent , however , they understand   the author of Target counters the hateful content in   theParent by showing empathy towards the mail   carrier . In the last example , the Target alone is   considered Hate because it attacks someone by us-   ing the phrase “ sick person ” . When the Parent   is shown , however , the annotators understand the   Target as calling out the Parent to be inappropriate .   Label distribution and linguistic insights Fig-   ure 1 shows the label distribution for all pairs ( right-   most column in each block ) and for pairs in which   comment ( i.e. , the comment containing   at least one hate word ) is the Parent orTarget .   The most frequent label assigned to Target com-   ments is Neutral ( 49 % ) followed by Hate ( 28 % )   and Counter - hate ( 23 % ) . While Target comments   containing a hate word are likely to be Hate ( 45 % ) ,   some are Counter - hate ( 19 % ) with context .   We analyze the linguistic characteristics of Titles ,   Parent s and Target s when the Target s are Hate orCounter - hate with context to shed light on the dif-   ferences between the language people use in hate   and counter speech . We combine the set of hate   words with profanity words to check for profan-   ity words . We analyze sentiment and cognitive   factors using the Sentiment Analysis and Cogni-   tion Engine ( SEANCE ) lexicon , a popular tool for   psychological linguistic analysis ( Crossley et al . ,   2017 ) . Statistical tests are conducted using un-   paired t - tests between the groups , of which the   Target s are Counter - hate or Hate ( Table 5 ) . We   also report whether each feature passes the Bon-   ferroni correction as multiple hypothesis tests are   performed . We draw several interesting insights :   •Questions marks in Target signal Counter-   hate . They are often rhetorical questions .   •Fewer 1st person pronouns ( e.g. , I , me ) and   more 2nd person pronouns ( e.g. , you , your ) in   theParent signal that the Target is more likely   to be Counter - hate . This is due to the fact that   people tend to target others in hateful content .   •High profanity count in the Parent signals that   theTarget is Counter - hate , while high profan-   ity count in the Target signals Hate .   •More words related to awareness , enlighten-   ment and problem - solving in the Target signal   Counter - hate .   •When there are more negative words in the   Parent , the Target tends to be Counter - hate .   Target s labeled as Counter - hate contain fewer   negative and disgusting words.5922   5 Experiments and Results   We build neural network models to identify if a Tar-   getcomment is Hate , Counter - hate , or Neutral . We   randomly split Gold instances ( 4,751 ) as follows :   70 % for training , 15 % for validation and 15 % for   testing . Silver instances are only used for training .   Neural Network Architecture We experiment   with neural classiﬁers built on top of the RoBERTa   transformer ( Liu et al . , 2019 ) . The neural architec-   ture consists of a pretrained RoBERTa transformer ,   a fully connected layer ( 768 neurons and Tanh acti-   vation ) , and another fully connected layer ( 3 neu-   rons and softmax activation ) to make predictions   ( Hate , Counter - hate , or Neutral ) . To investigate the   role of context , we consider two textual inputs :   • the Target alone ( Target ) , and   • the Parent and the Target ( Parent_Target ) .   We concatenate the Target and the Parent with the   [ SEP ] special token . We conduct multiple runs of   experiments , which show consistent results . The   hyperparameters and other implementation details   are presented in the Appendix . We also experiment   models that take the title of the discussion as part   of the context , but it is not beneﬁcial .   We implement two strategies to enhance the per-   formance of neural models :   Blending Gold and Silver We adopt the method   by Shnarch et al . ( 2018 ) to determine whether Sil-   ver annotations are beneﬁcial . There are two phases   in the training process : mblending epochs using   all Gold and a fraction of Silver , and then nepochs   using all Gold . In each blending epoch , Silver in-   stances are fed in a random order to the network . The fraction of Silver is determined by a blend-   ing factorα∈[0 .. 1 ] . The ﬁrst blending epoch is   trained with all Gold and all Silver , and the amount   of Silver to blend is reduced by αin each epoch .   Pretraining with Related Tasks We also exper-   iment with several corpora to investigate whether   pretraining with related tasks is beneﬁcial . Specif-   ically , we pretrain our models with existing cor-   pora annotating : ( 1 ) hateful comments : hateful or   not hateful ( Qian et al . , 2019 ) , and hate speech ,   offensive , or neither ( Davidson et al . , 2017 ) ; ( 2 )   sentiment : negative , neutral , or positive ( Rosenthal   et al . , 2017 ) ; ( 3 ) sarcasm : sarcasm or not sarcasm   ( Ghosh et al . , 2020 ) ; and ( 4 ) stance : agree , neutral ,   or attack ( Pougué - Biyong et al . , 2021 ) .   5.1 Quantitative Results   We present results with the test split in Table 6 . The   majority baseline always predicts Neutral . The re-   maining rows present the results with the different   training settings : training with the Target or both   theParent andTarget ; training with only Gold or   blending Silver annotations ; and pretraining with   related tasks . We provide here results pretraining   with the most beneﬁcial task , stance detection , and   present additional results in the Appendices . Blend-   ing Gold and Silver annotations requires tuning α .   We did so empirically using the training and valida-   tions splits , like other hyperparameters . We found   the optimal value to be 0.3 when blending Silver   ( + Silver rows ) and 1.0 when blending Silver and   pretraining with a related task ( + Silver + Related   task rows ) .   As shown in Table 6 , blending Gold and Sil-5923   ver annotations obtains better results ( F1 weighted   average ) than using only Gold ( Target : 0.61 vs.   0.58 ; Parent_Target : 0.63 vs. 0.61 ) . We also ﬁnd   that models pretrained for stance detection obtain   better results than pretrained with other tasks ( see   detailed results in the Appendices ) . Pretraining   with stance detection data beneﬁts models trained   without context ( Target : 0.61 vs. 0.58 ) and models   with context ( Parent_Target : 0.63 vs. 0.61 ) . These   results indicate that stance information between   Parent andTarget is useful to determine whether   theTarget is Hate , Counter - Hate or Neutral .   We make two observations about the results ob-   tained using neither strategy . First , using the Tar-   getalone obtains much better results than the ma-   jority baseline ( 0.58 vs. 0.34 ) . In other words ,   modeling the Target allows the network to identify   some instances of Hate and Counter - hate despite   the ground truth requires the Parent . Second , incor-   porating the Parent comment is beneﬁcial : the F1   score for all classes are higher ( Hate : 0.59 vs. 0.56 ,   Counter - hate 0.44 vs. 0.38 , Neutral 0.70 vs. 0.69 ) ,   and so is the weighted average ( 0.61 vs. 0.58 ) . The   ﬁndings are consistent ( weighted F1 ) using either   strategy ( + Silver : 0.63 vs 0.61 , + Related task : 0.63   vs 0.61 ) or both ( 0.64 vs. 0.61 ) . The F1 scores   of the three classes with Parent_Target models are   equal to or better than those by Target only models .   Finally , the network ( a ) blending Gold and Sil-   ver annotations and ( b ) pretraining with stance   detection achieves the best performance ( Par-   ent_Target+Silver+Related task : 0.64 ) . This re-   sult is statistically signiﬁcant ( p<0.01 ) compared   to Target only model without blending Silver or   pretraining with related tasks.6 Qualitative Analysis   When is adding the context beneﬁcial ? When does   our best model make mistakes ? To investigate these   questions , we manually analyze the following :   •The errors made by the Target only network   that are ﬁxed by the context - aware network   ( Trained with Parent_Target , Table 7 ) .   •The errors made by the context - aware   network pretrained on related task ( stance )   and blending Silver annotations ( Par-   ent_Target+Silver+Related task , Table 8) .   When does the context complement Target ?   We analyze the errors made by the network using   only the Target that are ﬁxed by the context - aware   network ( Trained with Parent_Target ) . Table 7 ex-   empliﬁes the most common error types .   The most frequent type of error ﬁxed by the   context - aware model is when there is Lack of infor-   mation in the Target ( 48 % ) . In this case , the Parent   comment is crucial to determine the label of the   Target . In the example , knowing what the author of   Target refers to ( i.e. , a rhetorical question , Women   can hover ? ) is crucial to determine that the Target   is humiliating women as a group .   The second most frequent error type is Negation   ( 27 % ) . In the example in Table 7 , taking into ac-   count the Parent allows the context - aware network   to identify that the author of the Target is scolding   the author of Parent and thus countering hate .   Nobata et al . ( 2016 ) and Qian et al . ( 2019 ) have   pointed out that sarcasm and irony make detecting   abusive and hateful content difﬁcult . We ﬁnd evi-   dence supporting this claim . We also discover that   by incorporating the Parent comment , a substantial   amount of these errors are ﬁxed . Indeed , 19 % of5924   errors ﬁxed by the context - aware network include   sarcasm or irony in the Target comment .   Finally , the context - aware network taking into   account the Parent ﬁxes many errors ( 8 % ) in which   theTarget comment is Hate despite it does not   contain swear words . In the example , the Target is   introducing additional hateful content , which can   be identiﬁed by the context - aware model when the   Parent information is used .   When does the best model make errors ? In   order to ﬁnd out the most common error types   made by the best model ( context - aware , Par-   ent_Target+Silver+Related task ) , we manually an-   alyze 200 random samples in which the output of   the network differs from the ground truth . Table 8   shows the results of the analysis .   Despite 27 % of errors ﬁxed by the context - aware   network ( i.e. , taking into account the Parent ) in-   clude negation in the Target , negation is the most   common type of errors made by our best net-   work ( 28 % ) . The example in Table 8 is especially   challenging as it includes a double negation .   We observe that Rhetorical questions are almost   as common ( 27 % ) . This ﬁnding is consistent with   the ﬁndings by Schmidt and Wiegand ( 2017 ) . In   the example , the best model fails to realize that the   Target is hateful , as it disdains the author of Parent .   Swear words are present in a substantial number   of errors . Wrongly predicting a Target without   swear words as Counter - hate or Neutral accounts   for 8 % of errors , and wrongly predicting a Target   with swear words as Hate accounts for another 8 %   of errors . As pointed out by Davidson et al . ( 2017),hate speech may not contain hate or swear words .   And vice versa , comments containing swear words   may not be hateful ( Zhang and Luo , 2019 ) .   Finally , we observe Intricate text in 7 % errors .   Our best model identiﬁes the Target ( “ I have lost   all respect for her . ” ) as Hate probably because by   identifying the agreeing stance on the Parent . In-   deed , the author of Target expresses his / her attitude   without vilifying others . Hence , the ground truth   label is Neutral .   7 Conclusions and Future Work   Conversational context does matter in Hate and   Counter - hate detection . We have demonstrated so   by ( a ) analyzing whether humans perceive user-   generated content as Hate or Counter - hate depend-   ing on whether we show them the Parent com-   ment and ( b ) investigating whether neural networks   beneﬁt from incorporating the Parent . We ﬁnd   that 38.3 % of human judgments change when we   show the Parent to annotators . Experimental results   demonstrate that networks incorporating the Par-   entyield better results . Additionally , we show that   noisy instances ( Silver data ) and pretraining with   relevant datasets improves model performance . We   have created and released a corpus of 6,846 ( Parent ,   Target ) pairs of Reddit comments with the Target   annotated as Hate , Neutral or Counter - hate .   Our work have several limitations . First , we only   consider context as the parent comment . While   considering additional context might be sometimes   beneﬁcial , doing so would require careful design   to not bias annotations ( Dutta et al . , 2020 ) . Our   research agenda includes exploring reliably strate-5925gies to consider more context and identify which   parts are most important . Second , people may have   different opinions about what constitutes hate and   counter speech due to different tolerances in online   aggression . We obtained the ground truth accord-   ing to annotators ’ reliability ( MACE scores ) , which   may lead to controversial samples falling in the Sil-   ver set and thus being considered only for training   ( not for testing ) . Finally , the keywords sampling   used to create our corpus may introduce biases . De-   spite we partially mitigate the issue by considering   hateful comments in both the Parent andTarget ,   community - based sampling ( Vidgen et al . , 2021 )   could be applied in our future work .   8 Ethical Considerations   We use the PushShift API to collect data from   Reddit . Our collection process is consistent with   Reddit ’s Terms of Service . The data are accessed   through the data dumps on Google ’s BigQuery us-   ing Python .   Reddit can be considered a public space for dis-   cussion which differs from a private messaging   service ( Vidgen et al . , 2021 ) . Users consent to   have their data made available to third parties in-   cluding academics when they sign up to Reddit .   Existing ethical guidelines state that in this situa-   tion explicit consent is not required from each user   ( Procter et al . , 2019 ) . We obfuscate user names as   User_A or User_B to reduce the possibility of iden-   tifying users . In compliance with Reddit ’s policy ,   we would like to make sure that our dataset will be   reused for non - commercial research only .   The Reddit comments in this dataset were an-   notated by annotators using Amazon Mechanical   Turk . We have followed all requirements intro-   duced by the platform for tasks containing adult   content . A warning was added in the task title . An-   notators need to pass the Adult Content Qualiﬁca-   tion Test before working on our tasks . Annotators   were compensated on average with $ 8 per hour .   We paid them regardless of whether we accepted   their work . Annotators ’ IDs are not included in the   dataset . Acknowledgements   This work was supported by Research Seed Grant   of the UNT College of Information . We would   like to thank the anonymous reviewers for their   insightful comments and suggestions .   References59265927   A Annotation Interface   We show a screenshot of the annotation interface   in Figure 2 .   B Detailed Results   Table 9 presents detailed results complementing Ta-   ble 6 in the paper . We provide Precision , Recall and   weighted F1 - score using each related task for pre-   training when the input is Target and Parent_Target   respectively in Table 9.5928   Table 10 presents the mean scores of Precision ,   Recall and weighted F1 - score and their standard de - viation when we use both Silver data and pretrain-   ing on related tasks with different random seeds.5929   The results are consistent with the ﬁndings in our   study : adding the Parent does improve the perfor-   mance compared to the system that does not ( 0.63   vs. 0.60 ) .   C Hyperparamters to Fine - tune the   Systems   The neural model takes about half an hour on av-   erage to train on a single GPU of NVIDIA TITAN   Xp . We use an implementation by Pruksachatkun   et al . ( 2020 ) and ﬁne - tune the RoBERTa ( base ar-   chitecture ; 12 layers ) ( Liu et al . , 2019 ) model for   each of the four training settings . For each set-   ting , we set the hyperparameters to be the same   when the textual input is Target and Parent_Target   respectively . Hence we only report tuned hyperpa-   rameters for each setting when the input is Target   in Table 11.5930