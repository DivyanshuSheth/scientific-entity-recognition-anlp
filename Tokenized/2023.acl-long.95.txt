  Zhong Zhang , Bang Liu , Junming ShaoUniversity of Electronic Science and Technology of China , Chengdu , ChinaShenzhen Institute for Advanced Study , UESTC , Shenzhen , ChinaMila & Université de Montréal , Montréal , Canada   zhongzhang@std.uestc.edu.cn ,   bang.liu@umontreal.ca ,   junmshao@uestc.edu.cn   Abstract   Pre - trained language models ( PLMs ) are   known to be overly parameterized and have   signiﬁcant redundancy , indicating a small de-   gree of freedom of the PLMs . Motivated by   the observation , in this paper , we study the   problem of re - parameterizing and ﬁne - tuning   PLMs from a new perspective : Discovery of   intrinsic task - speciﬁc subspace . Speciﬁcally ,   by exploiting the dynamics of the ﬁne - tuning   process for a given task , the parameter opti-   mization trajectory is learned to uncover its in-   trinsic task - speciﬁc subspace . A key ﬁnding is   that PLMs can be effectively ﬁne - tuned in the   subspace with a small number of free parame-   ters . Beyond , we observe some outlier dimen-   sions emerging during ﬁne - tuning in the sub-   space . Disabling these dimensions degrades   the model performance signiﬁcantly . This sug-   gests that these dimensions are crucial to in-   duce task - speciﬁc knowledge to downstream   tasks .   1 Introduction   Pre - trained Language Models ( PLMs ) have be-   come the de facto methods for various natural   language processing ( NLP ) tasks ( Devlin et al . ,   2019 ; Radford et al . , 2019 ; Liu et al . , 2019 ) . The   typical paradigm is to pre - train a big language   model on large - scale corpora and then ﬁne - tune   the model on small task - speciﬁc datasets to adapt   to the downstream tasks . Despite the great success   of this paradigm , two questions still come to our   mind : ( 1 ) Why can a PLM with hundreds of mil-   lions of parameters be successfully ﬁne - tuned on   different downstream tasks using only hundreds   or thousands of labeled samples ? ( 2 ) Do we re-   ally need a full ﬁne - tuning of all parameters of   a PLM to reach state - of - the - art performance on   downstream tasks ? In this paper , we try to pro-   vide a new viewpoint on the two questions , andclaim that : Fine - tuning happens only in some   tiny task - speciﬁc subspaces , which can be effec-   tively learned with a small number of free pa-   rameters .   Recent studies have shown that PLMs are   highly over - parameterized and robust to pruning   ( Frankle and Carbin , 2019 ; Chen et al . , 2020 ;   Prasanna et al . , 2020 ; Gordon et al . , 2020 ; Liang   et al . , 2021 , 2022 ) , and can be ﬁne - tuned in   parameter - efﬁcient ways ( Gong et al . , 2022 ; Za-   ken et al . , 2022 ; Mahabadi et al . , 2021 ; Li and   Liang , 2021 ) . This emerging empirical evidence   tends to point to one fact that there exist some in-   trinsic structures in PLMs that are responsible for   inducing task - speciﬁc knowledge to downstream   tasks . Notably , the recent work ( Aghajanyan et al . ,   2021 ) provides a promising conclusion that PLMs   can be re - parameterized and ﬁne - tuned in random   low - dimensional subspaces using random projec-   tion , and the dimensionality of the random sub-   space is orders of magnitude smaller than the di-   mensionality of the full parameter space . Their   ﬁndings implicitly suggest the existence of such   intrinsic structure in the PLMs , which is , how-   ever , understudied . To bridge this gap , we explic-   itly demonstrate that there exist task - speciﬁc low-   dimensional subspaces in which PLMs can be ef-   fectively ﬁne - tuned .   Inspired by the low dimensional landscape hy-   pothesis ( Li et al . , 2022a ) that a training trajec-   tory of a neural network lies in a low - dimensional   subspace , in this work , we thus resort to the ﬁne-   tuning trajectory to study the intrinsic task - speciﬁc   subspaces of PLMs . We show that it is possible to   uncover the intrinsic task - speciﬁc subspaces with   a ﬁne - tuning trajectory by ﬁnding its principal di-   rections . The uncovered intrinsic task - speciﬁc sub-   spaces usually have very low dimensionalities , but   are quite effective in inducing task - speciﬁc knowl-   edge . For example , by re - parameterizing the en-   coder and optimizing only 32 free parameters per-1701layer in the intrinsic task - speciﬁc subspace , the   model allows achieving nearly the same perfor-   mance as ﬁne - tuning in the full parameter space .   Moreover , we further show that the uncovered in-   trinsic task - speciﬁc subspaces have a certain trans-   ferability .   Beyond this , we ﬁnd that the model contains   some outlier dimensions with abnormal spikes   when ﬁne - tuning in the intrinsic task - speciﬁc sub-   spaces instead of a random subspace . Disabling   these outlier dimensions degrades the model per-   formance signiﬁcantly . We believe that this phe-   nomenon is related to the previously discovered   outlier dimensions of PLMs ( Luo et al . , 2021 ; Ko-   valeva et al . , 2021 ; Puccetti et al . , 2022 ) . How-   ever , there are essential differences between them ,   which we will discuss in the latter section .   By exploring the intrinsic task - speciﬁc sub-   spaces of PLMs , the main contributions of this pa-   per are summarized as follows .   1.We interpret the ease of adapting PLMs to   downstream tasks as ﬁne - tuning happens in   tiny intrinsic task - speciﬁc subspaces . Within   this interpretation , we propose a method to   uncover the subspaces by ﬁnding the princi-   pal directions of the ﬁne - tuning trajectory .   2.We conduct extensive experiments on   the GLUE benchmark using BERT and   RoBERTa models to support our claims .   We show that the models can be effec-   tively ﬁne - tuned with a very small number   of parameters in the uncovered intrinsic   task - speciﬁc subspaces .   3.We identify some outlier dimensions when   ﬁne - tuning in the intrinsic task - speciﬁc sub-   spaces , and some empirical analysis is further   given .   2 Related Work   Intrinsic Dimensionality . Li et al . ( 2018 ) ﬁrst   deﬁned the intrinsic dimension of an objective   function in the context of deep learning . They   showed that various neural networks can be ef-   fectively re - parameterized and trained in random   low - dimensional subspaces . Their ﬁndings shed   light on understanding the high - dimensional land-   scape of complex neural networks . Following this ,   Aghajanyan et al . ( 2021 ) further measured the in-   trinsic dimensions of PLMs ﬁne - tuning on down-   stream tasks . They showed that PLMs have verylow intrinsic dimensions ranging from hundreds   to thousands . Qin et al . ( 2021 ) exploited the idea   of intrinsic subspace and proposed a prompt tun-   ing method for efﬁcient training . In addition , the   concept of intrinsic dimension is also related to   the low - rank approximation of PLMs ( Hu et al . ,   2022 ; Mahabadi et al . , 2021 ; Chen et al . , 2021 ) ,   but their motivations are entirely different . The   former aims to open the black box of models and   explore the internal mechanisms of why they are   effective , while the latter focuses on developing   new methods to train the models efﬁciently .   Random Projection and Subspace Learning .   The random projection has a long history in ma-   chine learning research community , and is a key   tool to analyze the intrinsic dimension ( Li et al . ,   2018 ; Aghajanyan et al . , 2021 ) . In the context of   optimization , Gressmann et al . ( 2020 ) proposed   a random bases descent algorithm to train neu-   ral networks in low - dimensional subspaces . How-   ever , the random projection inevitably introduces   task - irrelevant information , and is not optimal for   subspace learning . We believe that a more com-   pact and task - speciﬁc subspace can be found in the   model , which is the main motivation of this work .   Gur - Ari et al . ( 2018 ) empirically found that gra-   dient descent of neural networks happens in a tiny   subspace , Li et al . ( 2022a ) further developed a sub-   space learning algorithm DLDR that dynamically   extracts the subspace from the optimization trajec-   tory . Li et al . ( 2022b ) leveraged the DLDR algo-   rithm for adversarial training . However , to the best   of our knowledge , there is no research on the dis-   covery of non - random intrinsic task - speciﬁc sub-   space of PLMs .   Outlier Dimensions in Pre - trained Language   Models . Multiple studies have identiﬁed outlier   dimensions in PLMs . Some works were moti-   vated by calibrating the anisotropy behavior of   hidden representation of PLMs ( Timkey and van   Schijndel , 2021 ; Ding et al . , 2022 ; Luo et al . ,   2021 ; Su et al . , 2021 ; Zhang et al . , 2020 ) . An-   other line of work identiﬁed certain outlier dimen-   sions in PLMs that are very sensitive to the ﬁne-   tuning of downstream tasks ( Kovaleva et al . , 2021 ;   Puccetti et al . , 2022 ) . Disabling these outlier di-   mensions degrades the model performance signiﬁ-   cantly . Luo et al . ( 2021 ) showed that the outlier di-   mensions are artefacts derived from positional em-   beddings and layer normalization . Puccetti et al .   ( 2022 ) identiﬁed a correlation between outlier di-1702mensions and token frequency . It is worth not-   ing that our ﬁndings differ largely from previous   works in three ways : 1 ) The outlier dimensions in   their context actually refer to output neurons . In   our context , an outlier dimension refers to a spe-   ciﬁc model parameter . In other words , they con-   sider abnormal outputs , while we consider abnor-   mal weights . 2 ) The ways of identifying outlier   dimensions are different . They identify outlier di-   mensions by examining abnormal outputs , while   we ﬁnd outlier dimensions by examining abnormal   updates to weights . 3 ) The effects of disabling out-   lier dimensions are different . They show that dis-   abling just one outlier neuron can result in a signif-   icant drop in performance . In contrast , disabling   the top outlier weight has almost no effect on the   model performance . However , the model perfor-   mance will drop signiﬁcantly if we disable more   outlier weights . The reason for the emergence of   these outlier dimensions remains unclear , and we   aim to conduct further in - depth analysis in future   work .   3 Intrinsic Task - speciﬁc Subspaces   Discovery in PLMs   3.1 Preliminary : Intrinsic Dimensionality   The intrinsic dimension of an objective landscape   is ﬁrst deﬁned by Li et al . ( 2018 ) , which is   the number of independent optimization variables   with regard to minimizing the objective function .   However , ﬁnding the exact intrinsic dimension   is computationally intractable for complex objec-   tive functions like deep neural networks . There-   fore , a random subspace training method is usu-   ally employed to estimate the intrinsic dimension   ( Li et al . , 2018 ; Aghajanyan et al . , 2021 ) .   Formally , let θ∈Rbe a parameter vec-   tor that parameterizes a model f(x;θ ) . Take   the BERT - base model as an example , θrep-   resents all BERT ’s parameters that are ﬂattened   into a 110M - dimensional vector . θ∈Rde-   notes the initial parameterization , P∈Rde-   notes a random projection matrix whose columns   form an orthonormal basis for a randomly ori-   ented d - dimensional subspace of R , θ∈   Rdenotes a parameter vector in a lower d-   dimensional space . The model is ﬁne - tuned in the   lower d - dimensional subspace via the following re-   parameterization method :   θ = θ+Pθ . ( 1 )   Note that θandPare frozen during the train-   ing process , and only θis trained by the gradient   descent . In practice , the re - parameterization can   be done in a layer - wise manner to save computa-   tional resources ( Aghajanyan et al . , 2021 ) , and we   also follow the layer - wise setting for our analysis .   The intrinsic dimension of a PLM is estimated   by grid searching the minimal dthat makes the   model reach 90 % of the full ﬁne - tuning perfor-   mance . Take the BERT - base model as an exam-   ple , the intrinsic dimension for ﬁne - tuning on the   MRPC dataset is only 1861 ( Aghajanyan et al . ,   2021 ) , which is surprisingly small considering the   original model has up to 110 million parameters .   3.2 Finding Intrinsic Task - speciﬁc Subspaces   Gur - Ari et al . ( 2018 ) showed strong empirical ev-   idence that the gradient dynamically converges to   a very small subspace in various large - scale deep-   learning scenarios . The subspace is spanned by a   few top eigenvectors of the Hessian , and the di-   mension is equal to the number of data classes .   This also indicates that the training trajectory of   neural networks lies in a low - dimensional sub-   space , which is in line with the conclusion of Li   et al . ( 2022a ) . Considering an illustrative example   in Fig . 1 , the full parameter space contains three   dimensions , but the training trajectory { θ }   only lies in a 2 - dimensional subspace Sspanned   byeande . We call this subspace the intrinsic   subspace because it has a minimal degree of free-   dom ( Li et al . , 2018 ) for the objective function to   reach the optimum . The aforementioned random   subspace can be seen as a naïve estimation of S.1703We hypothesize that an intrinsic task - speciﬁc   subspace exists for each downstream task when   ﬁne - tuning a PLM . Generally , it is intractable to   search such an intrinsic task - speciﬁc subspace di-   rectly . However , if our hypothesis is true , the ﬁne-   tuning trajectory will lie in a low - dimensional sub-   space . Thus we can resort to the ﬁne - tuning tra-   jectory to obtain an approximation of the intrinsic   task - speciﬁc subspace . Speciﬁcally , given a ﬁne-   tuning trajectory { θ}of a PLM on a down-   stream task , we stack it into a matrix W∈R ,   and apply Singular Value Decomposition ( SVD )   on it .   W = UΣV , ( 2 )   where Σ∈Ris the singular value matrix ,   U∈RandV∈Rare two real orthog-   onal matrices whose columns are left and right   singular vectors , respectively . It is worth noting   that the columns of Vare actually the principal   directions of the given trajectory if zero empirical   means of columns , and these directions constitute   an orthonormal basis of the subspace in which the   trajectory lies . Theoretically , a ( t−1)-dimensional   subspace needs only tindependent points to de-   termine . We can regard this subspace as an ap-   proximation of the intrinsic task - speciﬁc subspace   whose dimension is equal to the number of points   in the trajectory . Thus , we can replace the ran-   dom projection matrix Pin Eq . ( 1 ) with Vto   re - parameterize the model .   3.3 Fine - tuning in Intrinsic Task - speciﬁc   Subspaces   Given an approximated intrinsic task - speciﬁc sub-   spaceV , we reformulate Eq . ( 1 ) by letting the   model train in the subspace as follows .   θ = θ+V θ . ( 3 )   In our early exploration , we can achieve good per-   formance close to full ﬁne - tuning by Eq . ( 3 ) . How-   ever , the performance is not stable , and sensitive   to the initialization of θ . To solve this problem ,   we propose an ensemble - like method that com-   bines multiple θof different initialization to re-   duce variance , which is as follows .   θ = θ+V∑1   hθ , ( 4)where his the number of vectors to combine , and   we set it as 16 in this paper . Note that although   the ensemble increases the number of parameters   to optimize , it does not change the instrinsic di-   mensionality of the subspace ( i.e. , the degree of   freedom ) .   In the following experimental evaluation , we   will investigate subspace ﬁne - tuning in both trans-   ductive and inductive settings to verify our hy-   potheses . The former is to verify the existence of   intrinsic task - speciﬁc subspaces when ﬁne - tuning   PLMs on the downstream tasks , and the effective-   ness of our method to uncover the subspaces . The   latter further examines how well the intrinsic task-   speciﬁc subspaces can be transferred to other sim-   ilar tasks .   4 Experiment and Analysis   4.1 Experimental Settings   Datasets and models . We evaluate the perfor-   mance of the methods on the commonly used   GLUE benchmark ( Wang et al . , 2018 ; Warstadt   et al . ,2019 ; Socher et al . , 2013 ; Dolan and Brock-   ett,2005 ; Cer et al . , 2017 ; Williams et al . , 2018 ;   Rajpurkar et al . , 2016 ) . For evaluation metrics , we   report the matched accuracy for MNLI , Matthews   correlation for CoLA , Pearson correlation for STS-   B , and accuracy for other tasks . We choose   the publicly available pre - trained language models   RoBERTa - base ( Liu et al . , 2019 ) and BERT - base-   cased ( Devlin et al . , 2019 ) for analysis . All experi-   mental results are averaged over 5 runs of different   seeds .   Implementation details . Our implementation   is based on HuggingFace ’s Transformers toolkit   ( Wolf et al . , 2020 ) . We ﬁrst need to produce a   set of ﬁne - tuning trajectories of GLUE tasks for   calculating projection matrices . We use the de-   fault script in the toolkit for ﬁne - tuning , and save   a checkpoint every epoch to obtain optimization   trajectories . We set the trajectory length to 32 ex-   cept for the MNLI dataset , which is set to 64 since   it is the largest dataset and needs more parame-   ters to ﬁt . We ﬂatten all parameters in an encoder   layer into a wide vector , and then stack all vectors   of different checkpoints into a matrix to perform   SVD . We compute independent projection matri-   ces for all layers , resulting in 12 projection ma-   trices . For transductive subspace ﬁne - tuning , the   projection matrix is calculated from the same task,1704CoLA MRPC SST-2 STS - B QQP MNLI QNLI RTE Avg .   BERT - Full 59.37 84.46 91.95 89.08 91.07 83.39 90.77 66.93 82.13   BERT - Freeze 27.52 69.66 88.81 78.35 84.48 71.55 81.61 56.46 69.81   BERT - Random 37.89 70.78 89.47 81.41 85.86 72.91 83.38 58.63 72.54   BERT - Intrinsic 60.27 84.31 89.93 89.51 89.73 81.21 87.73 67.00 81.21   RoBERTa - Full 61.04 89.31 94.29 90.70 91.72 87.23 92.48 76.68 85.43   RoBERTa - Freeze 0.00 68.38 85.32 15.69 82.81 71.16 79.11 53.86 57.04   RoBERTa - Random 27.58 68.38 91.45 75.47 86.33 77.10 84.49 58.27 71.13   RoBERTa - Intrinsic 61.07 87.21 92.43 89.43 90.18 85.53 90.57 78.77 84.40   while for inductive subspace ﬁne - tuning , it is cal-   culated from other tasks . We only re - parameterize   the encoder layers into the subspaces and leave the   embedding layer and the last classiﬁcation layer in   their original parameter space . We freeze the ini-   tial model θand the projection matrix V , and   only tune the low - dimensional vector θ . We keep   the learning rate of the embedding and classiﬁca-   tion layers unchanged and set the learning rate of   θto 0.01 .   4.2 Transductive Intrinsic Subspace   Fine - tuning   Table 1summarizes the experimental results . We   can see that freezing the encoder signiﬁcantly de-   grades the model performance as it serves as a   naïve baseline ( Note that it implies ﬁne - tuning inthe null space , i.e. , V θ=0 , which brings no   information to update the model ) . For intrinsic   subspace ﬁne - tuning , we can clearly see that it   shows comparable performance to the full ﬁne-   tuning across all GLUE tasks and models . In con-   trast , random projection only yields a marginal im-   provement over the baseline , and signiﬁcantly un-   derperforms intrinsic subspace ﬁne - tuning .   From these empirical results , we ﬁrst conclude   that PLMs can be re - parameterized and ﬁne - tuned   in some low - dimensional subspaces . Secondly ,   there exist some subspaces in which the PLMs   can most effectively adapt to downstream tasks ,   and we can uncover these subspaces by ﬁnding   the principal directions of ﬁne - tuning trajectories   in the full parameter space . This conclusion in turn   suggests that ﬁne - tuning of PLMs happens in tiny1705CoLA MRPC SST-2 STS - B QQP MNLI QNLI RTE Avg .   BERT - Full 59.37 84.46 91.95 89.08 91.07 83.39 90.77 66.93 82.13   BERT - Random 32.49 70.15 88.65 79.29 84.84 71.75 82.29 57.11 70.82   BERT - Zeroshot 35.35 78.09 91.06 85.17 87.57 75.29 84.01 75.23 76.47   BERT - Uniﬁed 61.58 84.41 91.06 89.71 91.27 83.85 90.97 67.00 82.48   RoBERTa - Full 61.04 89.31 94.29 90.70 91.72 87.23 92.48 76.68 85.43   RoBERTa - Random 0.00 68.38 89.47 27.60 84.51 73.16 82.10 54.44 59.96   RoBERTa - Zeroshot 32.93 80.44 90.60 83.10 87.12 78.76 84.46 67.12 75.57   RoBERTa - Uniﬁed 63.80 89.12 93.55 90.88 91.85 87.20 92.36 77.91 85.83   subspaces , which provides an explanation of the   ease of adapting PLMs to downstream tasks .   4.3 Inductive Intrinsic Subspace Fine - tuning   Next , we conduct inductive intrinsic subspace ﬁne-   tuning to examine the transferability of the dis-   covered subspaces . We generally follow the same   training protocol as in the last section , except that   we replace the projection matrices with the ones   calculated from other tasks .   We can observe the performance drop using   transferred task subspaces in Fig . 2 . Generally ,   we can see that even though the models are ﬁne-   tuned in transferred subspaces , they still outper-   form the random subspace baseline , which sug-   gests the transferability of intrinsic task - speciﬁc   subspaces .   The transferability of subspaces seems to cor-   relate with the scale of the transferred task . For   example , big datasets like SST-2 , QQP , MNLI   and QNLI underperform small datasets like CoLA ,   MRPC , STS - B , and RTE in providing subspaces .   This is because the intrinsic task - speciﬁc sub-   spaces of complex tasks have higher dimensions   and need more parameters to estimate .   When comparing within one column , we can   see signiﬁcant difference between distinct sub-   spaces used for ﬁne - tuning one task . We assume   similar tasks may have substantial subspace inter-   sections and thus be easier to transfer . Still , this   claim needs further analysis to conﬁrm , we will   leave it further study since transferability is not the   main focus of this paper . In summary , we empiri-   cally show that the intrinsic task - speciﬁc subspace   has a certain transferability.4.4 Uniﬁed Intrinsic Task Subspace   Qin et al . ( 2021 ) showed that a uniﬁed low-   dimensional intrinsic task subspace can be con-   structed by a multi - task prompt tuning method .   In our case , we can also construct a uniﬁed sub-   space by stacking the ﬁne - tuning trajectories of   different tasks into a matrix , and applying SVD   on it . Speciﬁcally , we sample one checkpoint for   each task and gather them to calculate the uni-   ﬁed subspace , which forms an 8 - dimensional sub-   space . And we additionally calculate a zero - shot   subspace of a task for comparison , which is cal-   culated by excluding the checkpoint of this task .   The results are given in Table 2 . We can see that   the models can be effectively ﬁne - tuned in the uni-   ﬁed subspace . For the zero - shot setting , the model   performance decreases signiﬁcantly , but still out-   performs the random baseline.1706   Next , we take the BERT model as an example   and examine the low - dimensional parameter vec-   torθlearned within the uniﬁed intrinsic subspace .   We calculate the cosine similarities between the   θvectors corresponding to different tasks and   present the results in Fig . 3 . As shown in the ﬁg-   ure , the cosine similarities between different tasks   are signiﬁcantly low , indicating that the uniﬁed in-   trinsic subspace contains disentangled knowledge   distributed in different dimensions , and the low-   dimensional parameter vector θserves as an ( un-   normalized ) probability distribution to induce task-   speciﬁc knowledge .   Based on these empirical ﬁndings , we conclude   that a uniﬁed intrinsic task subspace is feasible   and it contains disentangled knowledge . However ,   in - domain knowledge still plays a crucial role in   forming the subspace as we can see that the zero-   shot setting still has a large perform gap .   4.5 Outlier Dimensions   We ﬁnd that PLMs have a small number of out-   lier dimensions exhibiting abnormal spikes when   ﬁne - tuning in the intrinsic task - speciﬁc subspaces . We examine each dimension of the product of V θ   and consider the dimension whose absolute value   is greater than a threshold as outlier . Note that the   product of V θis the learned parameter update in   the full parameter space and we re - parameterize   the encoder of the PLM layer - wisely , thus it is a   vector with the dimension equal to the number of   all parameters of an encoder layer .   It is important to note that the outlier dimension   in our context is different from the previous studies   ( Kovaleva et al . , 2021 ; Luo et al . , 2021 ; Puccetti   et al . , 2022 ) . Previous studies use the outlier di-   mension to refer to the output channel ( 768 dimen-   sions for BERT - base ) . In our context , we ﬂatten all   parameters of a layer into a vector ( 7,087,872 di-   mensions for BERT - base ) . Then an outlier dimen-   sion refers to a speciﬁc parameter weight in the   layer . We use the BERT model and MRPC dataset   for illustration , and visualize the product of V θ   in Fig . 4to show the outlier patterns . As we can   see from the ﬁgure , when ﬁne - tuning in the intrin-   sic task - speciﬁc subspace , the outlier patterns ex-   ist in all layers . In contrast , these outlier patterns   disappear when ﬁne - tuning in a random subspace.1707CoLA MRPC SST-2 STS - B QQP MNLI QNLI RTE   BERT - Full 59.37 84.46 91.95 89.08 91.07 83.39 90.77 66.93   BERT - Random 57.27 84.46 91.79 88.66 90.66 83.68 90.41 64.48   BERT - Outlier 0.00 68.38 50.92 0.00 63.18 33.64 49.89 52.71   RoBERTa - Full 61.04 89.31 94.29 90.70 91.72 87.23 92.48 76.68   RoBERTa - Random 58.80 87.65 93.95 89.52 91.29 87.76 92.61 68.88   RoBERTa - Outlier 0.00 70.49 50.92 28.05 63.67 36.15 49.89 52.71   Model component Layer # of outliers each layer   attention.self.query.weight 1 , 2 , 3 , 4 , 6 , 7 , 8 , 9 , 10 , 11 , 12 3 , 1 , 1 , 1 , 4 , 4 , 8 , 3 , 3 , 2 , 4   attention.self.query.bias 1 1   attention.self.key.bias 10 , 11 2 , 1   attention.output.LayerNorm.weight 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 10 , 11 , 12 1 , 2 , 3 , 5 , 4 , 1 , 2 , 4 , 1 , 3 , 2   attention.output.LayerNorm.bias 1 , 2 , 3 1 , 1 , 1   intermediate.dense.weight 1 , 12 2 , 1   output.dense.weight 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 2 , 6 , 5 , 4 , 2 , 4 , 3 , 2 , 3 , 4 , 4   output.LayerNorm.weight 5 , 6 , 7 , 12 4 , 1 , 1 , 3   This phenomenon is universal for different models   and different datasets .   To investigate the effect of the outlier dimen-   sions on the models , we disable them by setting   them to zero and examine how this affects model   performance . We ﬁrst disable the top outlier di-   mension of each encoder layer and ﬁne - tune the   model in the full parameter space , which has al-   most no impact on model performance . This re-   sult is not surprising because disabling only one   weight in a layer deﬁnitely has a negligible ef-   fect on the output than disabling an output chan-   nel as the previous studies do . We continue to   disable more outlier dimensions , and these devi-   ating at least 3σfrom the mean are disabled . Ap-   proximately 0.3 % of encoder parameters are dis-   abled . We also randomly sample and disable the   same number of dimensions for comparison , and   the results are shown in Table 3 . We can see that   disabling outlier dimensions degrades the model   performance signiﬁcantly while disabling random   dimensions does not .   Next , we qualitatively examine the positions in   which the outlier dimensions emerge . We sam-   ple each layer ’s top 10 outlier dimensions and   record their positions in Table 4 . We can seethat the outlier dimensions are ubiquitous in var-   ious model components . Then , we identify one   outlier dimension O1that consistently produces   high - magnitude weights in almost all BERT lay-   ers . Furthermore , we ﬁnd that there is a consid-   erable overlap in the outlier dimensions of each   layer , which suggests that these dimensions can   propagate through layers .   Why do outlier dimensions emerge ? Previous   studies came up with several explanations like   high - magnitude scaling factors ( Kovaleva et al . ,   2021 ) , LayerNorm and residual connection ( Luo   et al . , 2021 ) , and unbalanced token frequency   ( Puccetti et al . , 2022 ) . However , these explana-   tions can not apply to our case because the deﬁni-   tions of the outlier dimension are different . Recall   that our approach to identifying outlier dimensions   is actually examining re - parameterized parameter   updates given the intrinsic task - speciﬁc subspace .   The magnitude of the updates represents the impor-   tance of corresponding parameters with respect to   solving the task . We have reason to believe that   these dimensions play an important role in con-   stituting the intrinsic subspace and are crucial to   induce task - speciﬁc knowledge to adapt to down-   stream tasks.17085 Conclusion   In this paper , we claim that the ﬁne - tuning of   PLMs happens in tiny subspaces . To uncover   such intrinsic task - speciﬁc subspaces , we exploit   the ﬁne - tuning trajectory to ﬁnd its main direc-   tion . Our empirical experiments show that PLMs   can effectively adapt to downstream tasks when   re - parameterizing and training in the found sub-   spaces , which well explains the ease of adapting   PLMs to downstream tasks . Furthermore , we ﬁnd   outlier dimensions in PLMs during the subspace   training . We consider that these dimensions are   crucial to induce task - speciﬁc knowledge to down-   stream tasks . Still , we need further in - depth anal-   ysis to understand the reasons and impact of the   emergence of outlier patterns .   Limitations   Despite the insights obtained through our analysis ,   certain limitations persist , which we outline in this   section .   With respect to the re - parameterization of pa-   rameters as presented in Eq . ( 3 ) , we adopted the   layer - wise setting as proposed by Aghajanyan et   al . ( 2021 ) in order to alleviate memory and com-   putational burdens . Nonetheless , such a setting   restricts us to only identifying local subspaces ,   rather than discovering global subspaces within   the entire parameter space of a pre - trained lan-   guage model . The existence of a task - speciﬁc   global subspace is yet to be ascertained . If such   a subspace does exist , the correlation between this   global subspace and the identiﬁed local subspaces   needs to be explored in future research .   In terms of experimental settings , the evaluation   tasks are limited to natural language understand-   ing tasks , with a lack of natural language gener-   ation tasks . On model architecture , decoder - only   ( e.g. , GPT ) and encoder - decoder ( e.g. , T5 ) models   are not included . On model scale , we use basic-   size models rather than large ones due to limited   computational resources . Consequently , the con-   clusions drawn in this study may not be applicable   to the above situations .   The analysis presented in Section 4.5 demon-   strates that pre - trained language models exhibit   a small number of outlier dimensions when ﬁne-   tuning in the intrinsic task - speciﬁc subspaces . Al-   though we have observed a signiﬁcant decline in   model performance when disabling these dimen-   sions , the underlying mechanism responsible forthe emergence of these outlier dimensions remains   unclear .   Acknowlegments   This work is supported by the Sichuan key   research program ( 22ZDYF3388 ) , Fundamen-   tal Research Funds for the Central Universi-   ties ( ZYGX2019Z014 ) , National Natural Science   Foundation of China ( 61976044 , 52079026 ) , Fok   YingTong Education Foundation for Young Teach-   ers in the Higher Education Institutions of China   ( 161062 ) , the Canada CIFAR AI Chair Program ,   and the Canada NSERC Discovery Grant ( RGPIN-   2021 - 03115 ) .   References17091710A Appendix   A.1 Hyperparameters   We ﬁrst ﬁne - tune the BERT and RoBERTa mod-   els for calculating projection matrices . We use   the ﬁne - tuning script in the Transformers toolkit .   All hyperparameters remain default except for the   number of epochs , which is set to 32 and 64 for the   MNLI and all other tasks , respectively . For intrin-   sic subspace ﬁne - tuning , the dimensionality of θ   is set to 32 and 64 for the MNLI and all other tasks ,   respectively . The learning rate of θis set to 0.01 .   The number of ensembles his set to 16 . Other   hyperparameter are the same as those in the script .   All experimental results are averaged over 5 runs   of different seeds . Each experiment is conducted   on a single GeForce RTX 2080Ti GPU with envi-   ronment of Pytorch 1.11.0 + CUDA 11.3.1 .   A.2 Ablation study   We conduct an ablation experiment over the num-   ber of dimensions of the subspaces . The results   are given in Table 5and Table 6 . The performance   increases as the number of dimensions increases .   Tasks dim=8 dim=16 dim=32   CoLA 54.06 57.17 60.27   MRPC 75.05 77.94 84.31   SST-2 89.52 90.05 89.93   STS - B 87.95 89.02 89.51   QQP 87.61 89.12 89.73   MNLI 76.93 78.48 78.70   QNLI 86.54 86.83 87.73   RTE 65.41 66.07 67.00   Tasks dim=8 dim=16 dim=32   CoLA 58.04 60.27 61.07   MRPC 75.59 78.20 87.21   SST-2 91.93 92.34 92.43   STS - B 84.10 88.10 89.43   QQP 87.58 89.25 90.18   MNLI 79.96 81.77 82.32   QNLI 89.35 89.14 90.57   RTE 74.30 78.56 78.771711ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section Limitations .   /squareA2 . Did you discuss any potential risks of your work ?   Section Limitations .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract , Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4 .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We use open - source artifacts which can be used for academic research purposes .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   We use the artifacts in compliance with their licenses .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   We use the open - source GLUE dataset which does not contain sensitive information .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4 .   C / squareDid you run computational experiments ?   Section 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix.1712 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   No response .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.1713