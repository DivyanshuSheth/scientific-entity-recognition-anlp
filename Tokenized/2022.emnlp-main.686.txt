  Thong Nguyen , Xiaobao Wu , Anh Tuan Luu ,   Cong - Duy Nguyen , Zhen Hai , Lidong BingNational University of Singapore , SingaporeVinAI Research , VietnamNanyang Technological University , SingaporeDAMO Academy , Alibaba Group   Abstract   Modern Review Helpfulness Prediction sys-   tems are dependent upon multiple modalities ,   typically texts and images . Unfortunately ,   those contemporary approaches pay scarce at-   tention to polish representations of cross - modal   relations and tend to suffer from inferior opti-   mization . This might cause harm to model ’s   predictions in numerous cases . To overcome   the aforementioned issues , we propose Multi-   modal Contrastive Learning for Multimodal   Review Helpfulness Prediction ( MRHP ) prob-   lem , concentrating on mutual information be-   tween input modalities to explicitly elaborate   cross - modal relations . In addition , we intro-   duce Adaptive Weighting scheme for our con-   trastive learning approach in order to increase   flexibility in optimization . Lastly , we propose   Multimodal Interaction module to address the   unalignment nature of multimodal data , thereby   assisting the model in producing more reason-   able multimodal representations . Experimental   results show that our method outperforms prior   baselines and achieves state - of - the - art results   on two publicly available benchmark datasets   for MRHP problem .   1 Introduction   Current e - commerce sites such as Amazon , Ebay ,   etc . , construct review platforms to collect user feed-   back concerning their products . These platforms   play a fundamental role in online transactions since   they help future consumers collect useful reviews   which assist them in deciding whether to make   the purchase or not . Unfortunately , nowadays the   number of user - generated reviews is overwhelming ,   raising doubts related to the relevance and verac-   ity of reviews . Therefore , there is a need to verify   the quality of reviews before publishing them to   prospective customers . As a result , this inspires a   recent surge of interest targeting the Review Help-   fulness Prediction ( RHP ) problem . Product InformationReview 1   Review 2   Table 1 : Example of unreasonable predictions in the   Multimodal Review Helpfulness Prediction task .   Two principal groups of early efforts focus on   purely textual data . The first group follows feature   engineering techniques , retrieving argument - based   features ( Liu et al . , 2017 ) , lexical features ( Kr-10085ishnamoorthy , 2015 ) , and semantic features ( Kim   et al . , 2006 ) , as input to their classifier . Inherently ,   their methods are labor - intensive and vulnerable to   the typical issues of conventional machine learning   methods . Instead of relying on manual features ,   the second group leverages deep neural models , for   instance , RNN ( Alsmadi et al . , 2020 ) and CNN   ( Chen et al . , 2018 ) , to learn rich features automat-   ically . Nonetheless , their approach is ineffective   because the helpfulness of a review is not only   contingent upon textual information but also other   modalities .   To cope with the above issues , recent works ( Liu   et al . , 2021b ; Han et al . , 2022 ) proposed to utilize   multi - modality via the Multi - perspective Coherent   Reasoning ( MCR ) model . Hypothesizing that a   review is helpful if it exhibits coherent text and   images with the product information , those works   take into account both textual and visual modality   of the inputs , then estimate their coherence level   to discern whether the reviews are helpful orun-   helpful . However , the MCR model contains a detri-   mental drawback . Particularly , it aims to maximize   the scores sof positive ( helpful ) product - review   pairs while minimizing those sof negative ( un-   helpful ) pairs . Hence , it was assumed that follow-   ing the aforementioned manner would project fea-   tures with similar semantics to stay close and those   with disparate ones to be distant apart . Unfortu-   nately , in multimodal learning , this was shown not   to be the case , causing the model to learn ad - hoc   representations ( Zolfaghari et al . , 2021 ) . This is   one reason leading to unreasonable predictions of   MCR in Table 1 . As it can be seen , even though   Review 1 closely relates to the product of “ 6 - Quart   Stainless Steel Stockpot ” , the model classifies it as   unhelpful . In addition , the target of Review 2 ’s text   content is vague because it does not specifically   correspond to the “ Stockpot ” . In fact , it can be   used for any product . Moreover , the image does   not clearly show any hint of the “ Stockpot ” as well .   Despite such vagueness , the output of MCR for   Review 2 is still helpful .   As a remedy to this problem , we propose Cross-   modal Contrastive Learning to mine the mutual   information of cross - modal relations in the input   to capture more sensible representations . Nonethe-   less , plainly applying symmetric gradient pattern ,   which is similar to MCR that they assign equivalent   penalty to sands , is inflexible . In cases that sis   small and sis already negatively skewed , or bothsandsare positively skewed , it is irrational to   assign equivalent penalties to both sands . Last   but not least , MCR directly leverages Coherent   Reasoning , repeatedly enforcing alignment among   modalities in the input . This ignores the unaligned   nature of multimodal input , for example , images   might only refer to a particular section in the text ,   hence do not completely align with the textual con-   tent . In consequence , strictly forming alignment   can make the model learn inefficient multimodal   representations ( Tsai et al . , 2019 ) .   To overcome the above problems , we propose   an adaptive scheme to accomplish the flexibility in   the optimization of our contrastive learning stage .   Finally , we propose to adopt a multimodal attention   module that reinforces one modality ’s high - level   features with low - level ones of other modalities .   This not only relaxes the alignment assumption but   also informs one modality of information of others ,   encouraging refined representation learning .   In sum , our contributions are three - fold :   •We propose an Adaptive Cross - modal Con-   trastive Learning for Review Helpfulness Pre-   diction task by polishing cross - modal relation   representations .   •We propose a Multimodal Interaction module   which correlates modalities ’ features without   depending upon the alignment assumption .   •We conducted extensive experiments on two   datasets for the RHP problem and found that   our method outperforms other baselines which   are both textual - only and multimodal , and ob-   tains state - of - the - art results on those bench-   marks .   2 Model Architecture   In this section we delineate the overall architecture   of our MRHP model . Particular modules of our   system are depicted in Figure 1 .   2.1 Problem Definition   Given a product item p , which consists of a de-   scription Tand images I , and a set of reviews   R={r , . . . , r } , where each review is com-   posed of user - generated text Tand images I ,   RHP model ’s task is to generate the scores   s = f(p , r),1≤i≤N ( 1)10086   where Nis the number of reviews for product pand   fis the scoring function of the RHP model . Em-   pirically , each score estimated by findicates the   helpfulness level of each review , and the ground-   truth is the descending sort order of helpfulness   scores .   2.2 Encoding Modules   Our model accepts product description T , product   images I , review text T , and review images I   as input . The encoding process of those elements   is described as follows .   Text Encoding Product description and review text   are sequences of words . Each sequence is indexed   into the word embedding layer and then passed into   the respective LSTM layer for product or review .   K = LSTM(W(T ) ) ( 2 )   K = LSTM(W(T ) ) ( 3 )   where K∈R , K∈R , landlare   the sequence lengths of product and review text   respectively , and dis the hidden size .   Image Encoding We follow Anderson et al . ( 2018 )   to take detected objects as embeddings of the im-   age . In particular , a pre - trained Faster R - CNN   is applied to extract ROI features for mobjects   { a , a , . . . , a}from the product and review im-   ages . Subsequently , we encode extracted features   using the self - attention module ( SelfAttn ) ( Vaswaniet al . , 2017 )   A = SelfAttn ( { a , a , ... , a } ) ( 4 )   where A∈Randdis the hidden size . Here   we use AandAto indicate product and review   image features , respectively .   2.3 Multimodal Interaction Module   We consider two components γ , ηwith their inputs   X , X , where ηis the concatenation of input el-   ements apart from the one in γ . For instance , if   γ = K , then η= [ K , A , A ] , where [ . , .]indi-   cates the concatenation operation . We define each   cross - modal attention block to have three compo-   nents Q , K , andV :   Q = X·W ( 5 )   K = X·W ( 6 )   V = X·W ( 7 )   where W∈R , W∈R , and   W∈Rare weight matrices . The inter-   action between γandηis computed in the cross-   attention manner   Z = CM(X , X ) = softmax / parenleftigg   Q·K√d / parenrightigg   · V   ( 8)   Our full module comprises Dlayers of the above-   mentioned attention block , as indicated in the right10087part of Figure 1 . Theoretically , the computation is   carried out as follows   Q[0 ] = X ( 9 )   T[i ] = CM[i](LN(Q[i−1]),LN(X))(10 )   U[i ] = T[i ] + Q[i−1 ] ( 11 )   Q[i ] = GeLU ( Linear ( U[i ] ) ) ( 12 )   where LNdenotes layer normalization operator . We   iteratively estimate cross - modal features for prod-   uct text , product images , review text , and review   images with a view to obtaining H , V , H , and   V.   H = Q[D ] , V = Q[D ] ( 13 )   H = Q[D ] , V = Q[D ] ( 14 )   After our cross - modal interaction module , we   proceed to pass features to undertake relation fu-   sion in three paths : intra - modal , inter - modal , and   intra - review .   Intra - modal Fusion The intra - modal alignment   is calculated for two relation kinds : ( 1 ) product   text - review text and ( 2 ) product image - review   image . Firstly , we learn alignment among intra-   modal features via self - attention modules   H = SelfAttn ( [ H , H ] ) ( 15 )   V = SelfAttn ( [ V , V ] ) ( 16 )   Then intra - modal hidden representations are fed to   a CNN , and continuously a max - pooling layer to   attain salient entries   z = MaxPool ( CNN([H , V ] ) )   ( 17 )   Inter - modal Fusion Similar to intra - modal align-   ment , inter - modal one is calculated for two types   of relations as well : ( 1 ) product text - review im-   age and ( 2 ) product image - review text . The first   step is also to relate feature components using self-   attention modules   H = SelfAttn ( [ H , V ] ) ( 18 )   H = SelfAttn ( [ V , H ] ) ( 19 )   We adopt a mean - pool layer to aggregate inter-   modal features and then concatenate the pooled   vectors to construct the final inter - modal represen - tation   I = MeanPool ( H )   ( 20 )   I = MeanPool ( H )   ( 21 )   z= [ I , I](22 )   Intra - review Fusion The estimation of intra-   review module completely mimics the inter - modal   manner . The only discrimination is that the esti-   mation is taken upon two different relations : ( 1 )   product text - product image and ( 2 ) review text -   review image .   H = SelfAttn ( [ H , V ] ) ( 23 )   H = SelfAttn ( [ H , V ] ) ( 24 )   G = MeanPool ( H )   ( 25 )   G = MeanPool ( H )   ( 26 )   z= [ G , G](27 )   Finally , we concatenate intra - modal , inter - modal ,   and intra - review output , and then feed the concate-   nated vector to the linear layer to obtain the ranking   score :   z= [ z , z , z ] ( 28 )   f(p , r ) = Linear ( z ) ( 29 )   3 Training Strategies   3.1 Adaptive Cross - modal Contrastive   Learning   In this section , we explain the formulation and   adaptive pattern along with its derivation of our   Cross - modal Contrastive Learning .   Cross - modal Contrastive Learning First of all ,   we extract hidden states of helpful product - review   pairs . Second of all , hidden features are max-   pooled to extract meaningful entries .   h = MaxPool ( H),h = MaxPool ( H)(30 )   v = MaxPool ( V),v = MaxPool ( V)(31 )   We formulate our contrastive learning framework   taking positive and negative pairs from the above-   mentioned cross - modal features . In our framework ,   we hypothesize that pairs established by modalities10088of the same sample are positive , whereas those   formed by modalities of distinct ones are negative .   L=−/summationdisplaysim(t , t)+/summationdisplaysim(t , t )   ( 32 )   where t , t∈ { h , h , v , v } , and Bdenotes   the batch size in the training process .   Adaptive Weighting The standard contrastive ob-   jective suffers from inflexible optimization due to   irrational gradient assignment to positive and nega-   tive pairs . As a result , to tackle the problem , we pro-   pose the Adaptive Weighting Strategy for our con-   trastive framework . Initially , we introduce weights   ϵandϵto represent distances from the optimum ,   then integrate them into positive and negative terms   of our loss .   L = −/summationdisplayϵ·sim(t , t )   + /summationdisplayϵ·sim(t , t)(33 )   where ϵ= [ o−sim(t , t)]andϵ=   [ sim(t , t)−o ] . Investigating the intuition to   determine the values for oando , we continue   to conduct derivation and arrive in the following   theorem   Theorem 1 Adaptive Contrastive Loss ( 33 ) has   the hyperspherical form :   L = /summationdisplay / parenleftbigg   sim(t , t)−o   2 / parenrightbigg   + /summationdisplay / parenleftbigg   sim(t , t)−o   2 / parenrightbigg   −C ,   where C > 0   We provide the proof for Theorem ( 1 ) in the Ap-   pendix section . As a consequence , theoretically the   contrastive objective arrives in the optimum when   sim(t , t ) = andsim(t , t ) = . Based   upon this observation , in our experiments we set   o= 2ando= 0 .   3.2 Training Objective   For the Review Helpfulness Prediction problem ,   the model ’s parameters are updated according tothe pairwise ranking loss as follows   L = /summationdisplaymax(0 , β−f(p , r ) + f(p , r ) )   ( 34 )   where randrare random reviews in which r   possesses a higher helpfulness level than r. We   jointly combine the contrastive goal with the rank-   ing objective of the Review Helpfulness Prediction   problem to train our model   L = L + L ( 35 )   4 Experiments   4.1 Datasets   We evaluate our methods on two publicly avail-   able benchmark datasets for MRHP task : Lazada-   MRHP and Amazon - MRHP .   Lazada - MRHP ( Liu et al . , 2021b ) consists of prod-   uct items and artificial reviews on Lazada.com , an   e - commerce platform in Southest Asia . All of the   texts in the dataset are expressed in Indonesian .   Amazon - MRHP ( Liu et al . , 2021b ) is collected   from Amazon.com , the large - scale international   e - commerce platform . Product information and   associated reviews are in English and extracted   between 2016 and 2018 .   Both datasets comprise 3 categories : ( i ) Cloth-   ing , Shoes & Jewelry ( Clothing ) , ( ii ) Electronics   ( Electronics ) , and ( iii ) Home & Kitchen ( Home ) .   We present the statistics of them in Table 2 .   4.2 Implementation Details   We use a 1 - layer LSTM with hidden dimension   size of 128 . We initialize our word embedding   with fastText embedding ( Bojanowski et al . , 2017 )   for Lazada - MRHP dataset and 300 - dimensional   GloVe pretrained word vectors ( Pennington et al . ,   2014 ) for Amazon - MRHP dataset . We set our mul-   timodal attention module to have D= 5attention   layers . For the visual modality , we extract 2048-   dimensional ROI features from each image and   encode them into 128 - dimensional vectors . Our10089   entire model is trained end - to - end with Adam opti-   mizer ( Kingma and Ba , 2014 ) and batch size of 32 .   For the training objective , we set the value of the   margin in the ranking loss to be 1 .   4.3 Baselines   We compare our proposed architecture against the   following baselines :   •BiMPM ( Wang et al . , 2017 ): a ranking model   which encodes input sentences in two direc-   tions to ascertain the matching result .   •Conv - KNRM ( Dai et al . , 2018 ): a CNN-   based model which encodes n - gram of multi-   ple lengths and uses kernel pooling to generate   the final ranking score .   •EG - CNN ( Chen et al . , 2018 ): a CNN - based   model targeting data scarcity and OOV prob-   lem in RHP task via taking advantage of   character - based representations and domain   discriminators .   •PRH - Net ( Fan et al . , 2019 ): a baseline to   predict helpfulness of a review by taking intoconsideration both product text and product   metadata .   •DR - Net ( Xu et al . , 2020 ): a cross - modality   approach that models contrast in associated   contexts by leveraging decomposition and re-   lation modules .   •SSE - Cross ( Abavisani et al . , 2020 ): multi-   modal model to fuse different modalities with   stochastic shared embeddings .   •MCR ( Liu et al . , 2021b ): a baseline model   focusing on coherent reasoning .   4.4 Automatic Evaluation   In Table 3 and 4 , we follow previous work ( Liu   et al . , 2021b ) to report Mean Average Preci-   sion ( MAP ) , Normalized Discounted Cumulative   Gain ( NDCG@N ) ( Järvelin and Kekäläinen , 2017 )   where N= 3andN= 5 . As it can be seen , multi-   modal approaches achieve better performance than   text - only ones .   For Lazada - MRHP dataset , we achieve an ab-   solute improvement of NDCG@3 of 2.4 points in10090   Clothing , NDCG@5 of 1.5points in Electronics ,   and MAP of 1.4points in Home over the previ-   ous best method , which is MCR . In addition , our   model also obtains better results than the best text-   only RHP model , which is PRH - Net , with a gain of   NDCG@3 of 9.8points in Clothing , NDCG@5 of   4.3points in Electronics , and MAP of 3.6points in   Home . Those results prove that our method can pro-   duce reasonable rankings for associated reviews .   For Amazon dataset , which is written in English ,   our model outperforms MCR on all 3 categories , by   NDCG@5 of 1.4points in Clothing , 2.7points in   Electronics , and 1.2points in Home , respectively .   These results have verified that our interaction mod-   ule and optimization approach can come up with   more useful multimodal fusion than previous state-   of - the - art baselines , not only in English context but   other language one as well .   We also perform significance tests to evaluate   the statistical significance of our improvement on   two datasets Amazon - MRHP and Lazada - MRHP ,   and note p - values in Table 5 . As shown in the table ,   all of the p - values are smaller than 0.05 , verifying   the statistical significance in the enhancement of   our method against prior best MRHP model , MCR   ( Liu et al . , 2021b ) .   4.5 Case Study   In Table 1 , we introduce an example of one prod-   uct item and two reviews extracted from Electron-   ics category of Amazon - MRHP dataset . Whereas   MCR fails to predict relevant helpfulness scores ,   our model successfully produces sensible rankings   for both of them . We hypothesize that our Multi-   modal Interaction module learns more meaningful   representations and Adaptive Contrastive Learning   framework acquires more logical hidden states of   relations among input elements . Thus , our model   is able to generate more rational outcomes .   4.6 Ablation Study   In this section , we proceed to study the impact of   ( 1 ) Adaptive Contrastive Learning framework and   ( 2 ) Cross - modal Interaction module . Adaptive Contrastive Learning It is worth not-   ing from Table 6 that plainly integrating con-   trastive learning brings less enhancement to the   performance , with the improvement of NDCG@3   dropping 0.53points in Lazada - MRHP dataset ,   NDCG@5 waning 0.84points in Amazon - MRHP   dataset . Furthermore , completely removing con-   trastive objective hurts performance , as NDCG@3   score decreasing 0.77points in Lazada - MRHP ,   and MAP score declining 1.06points in Amazon-   MRHP . We hypothesize that the model loses the   ability to learn efficient representations for cross-   modal relations .   Cross - modal Interaction In this ablation , we elim-   inate the cross - modal interaction module . As   shown in Table 6 , without the module , the improve-   ment is downgraded , for instance , N@3 drops 1.89   points in Lazada - MRHP dataset , MAP shrinks 1.39   points in Amazon - MRHP dataset . It is hypothe-   sized that without the module , the model is rigidly   dependent upon the alignment nature among multi-   modal input elements , which brings about insensi-   ble modeling because in most cases , cross - modal   elements are irrelevant to be bijectively mapped   together .   4.7 Impact of Contrastive Learning on   Cross - modal Relations   Despite improved performances , it remains a   quandary that whether the enhancement stems from   more meaningful representations of input samples ,   which we hypothesize as a significant benefit of   our contrastive learning framework . For deeper   investigation , we decide to statistically measure10091   distances among input samples using standard dis-   tance functions . Table 7 and 8 reveal the results   of our experiment . In particular , we estimate the   cosine distance ( CS ) and L2 distance ( L2 ) between   tokens of ( 1 ) product text - review text and product   image - review image ( intra - modal ) , ( 2 ) product   text - review image and product image - review text   ( inter - modal ) , and ( 3 ) product text - product im-   age and review text - review image ( intra - review ) ,   then calculate the mean value of all samples . As it   can be seen , our frameworks are more efficient in   attracting elements of helpful pairs and repelling   those of unhelpful pairs .   5 Related Work   5.1 Review Helpfulness Prediction   Past works that pursue Review Helpfulness Predic-   tion ( RHP ) dilemma follow text - only approaches .   In general , they extract salient information , for in-   stance lexical ( Krishnamoorthy , 2015 ) , argument   ( Liu et al . , 2017 ) , and emotional features ( Martin   and Pu , 2014 ) from reviews . Subsequently , these   features are fed to a standard classifier such as   Random Forest ( Louppe , 2014 ) in order to pro-   duce the output score . Inspired by the meteoric   development of computation resources , contempo-   rary approaches seek to take advantage of deep   learning techniques to tackle the RHP problem .   For instance , Wang et al . ( 2017 ) propose multi-   perspective matching between review and product   information via applying attention mechanism . Fur-   thermore , Chen et al . ( 2018 ) ; Dai et al . ( 2018 ) adapt   CNN models to learn textual representations in var-   ious views . In reality , review content are not only determined   by texts but also other modalities . As a conse-   quence , Fan et al . ( 2019 ) integrate metadata in-   formation of the target product into the prediction   model . Abavisani et al . ( 2020 ) filter out uninforma-   tive signals before fusing various modalities . More-   over , Liu et al . ( 2021b ) perform coherent reasoning   to ascertain the matching level between product   and numerous review items .   5.2 Contrastive Estimation   Different from architectural techniques such as   Knowledge Distillation ( Hinton et al . , 2015 ; Hahn   and Choi , 2019 ; Nguyen and Luu , 2022 ) or Vari-   ational AutoEncoder ( Zhao et al . , 2020 ; Nguyen   et al . , 2021 ; Nguyen and Luu , 2021 ; Wang et al . ,   2019 ) , Contrastive Learning has been introduced as   a representation - based but universal mechanism to   enhance natural language processing performance .   Proposed by Chopra et al . ( 2005 ) , Contrastive   Learning has been widely adopted in myriad prob-   lems of Natural Language Processing ( NLP ) .   As an approach to polish text representations ,   Gao et al . ( 2021 ) ; Zhang et al . ( 2021 ) ; Liu et al .   ( 2021a ) ; Nguyen and Luu ( 2021 ) employ con-   trastive loss to advance sentence embeddings and   topic representations . For downstream tasks , Cao   and Wang ( 2021 ) propose negative sampling strate-   gies to generate noisy output so that the model can   learn to distinguish correct summaries from incor-   rect ones in Document Summarization . For Spoken   Question Answering ( SQA ) , You et al . ( 2021 ) intro-   duce augmentation algorithms in their contrastive   learning stage so as to capture noisy - invariant rep-10092resentations of utterances . Additionally , Ke et al .   ( 2021 ) inherit the formulation of the contrastive   objective to construct distillation loss which trans-   fers knowledge of the previous task to the current   one . Their proposals are to improve tasks in the   Aspect Sentiment Classification domain . Unfortu-   nately , despite the surge of interest in exercising   contrastive learning for NLP , research works to   adapt the method to the MRHP task have been   scant .   6 Conclusion   In this paper , we propose methods to polish rep-   resentation learning for the Multimodal Review   Helpfulness Prediction task . In particular , we aim   to advance cross - modal relation representations by   learning mutual information through contrastive   learning . In order to further enhance our frame-   work , we propose an adaptive weighting strategy   to encourage flexibility in optimization . Moreover ,   we integrate a cross - modal interaction module to   loose the model ’s reliance on unalignment nature   among modalities , continuing to refine multimodal   representations . Our framework is able to outper-   form prior baselines and achieve state - of - the - art   results on the MRHP problem .   7 Limitations   Despite the novelty and benefits of our method   for Multimodal Review Helpfulness Prediction   ( MRHP ) problem , it does include some drawbacks .   Firstly , even though empirical results demonstrate   that our approach not only works in English con-   texts , we have not conducted the verification in   multilingual circumstances , in which product or   review texts are written in different languages . If   a model is corroborated to work efficiaciously in   such contexts , it is capable of providing myriad   benefits for practical implementation , for example ,   e - commerce applications can leverage such one   single model for multiple cross - lingual scenarios .   Furthermore , our work can also be extended to   other domains . For instance , in movie assessment ,   we need to determine whether the review suits the   material in the film , or visual scenes in the com-   ment are consistent with the textual content . These   would form our prospective future directions .   Secondly , in the MRHP problem , there are sev-   eral relationships that contrastive learning could   exploit to burnish the performance . In particular ,   performing contrastive discrimination between twosets of reviews is able to furnish the model with   useful set - based representations , which consolidate   general knowledge for better helpfulness predic-   tion . Similar insights are applicable for two sets   of product information . At the moment , we leave   such promising perspectives for future work .   8 Acknowledgement   This work was supported by Alibaba Innovative   Research ( AIR ) programme with research grant   AN - GC-2021 - 005 .   References100931009410095A Hyperspherical Form of Adaptive Contrastive Loss   We have the initial formulation of the adaptive contrastive loss   L = −/summationdisplayϵ·sim(t , t ) + /summationdisplayϵ·sim(t , t ) ( 36 )   We first substitute ϵ= [ o−sim(t , t)]andϵ= [ sim(t , t)−o]into the above equation ,   L = /summationdisplaysim(t , t)−o·sim(t , t ) + /summationdisplaysim(t , t)−o·sim(t , t)(37 )   = /summationdisplay / parenleftbigg   sim(t , t)−o   2 / parenrightbigg   + /summationdisplay / parenleftbigg   sim(t , t)−o   2 / parenrightbigg   −C ( 38 )   where C=/parenleftbig / parenrightbig+/parenleftbig / parenrightbig . Now we obtain the spherical form of our contrastive loss.10096