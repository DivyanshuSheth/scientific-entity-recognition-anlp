  Aida Ramezani   Department of Computer Science   University of Toronto   armzn@cs.toronto.eduYang Xu   Department of Computer Science   Cognitive Science Program   University of Toronto   yangxu@cs.toronto.edu   Abstract   Moral norms vary across cultures . A recent   line of work suggests that English large lan-   guage models contain human - like moral bi-   ases , but these studies typically do not exam-   ine moral variation in a diverse cultural setting .   We investigate the extent to which monolin-   gual English language models contain knowl-   edge about moral norms in different countries .   We consider two levels of analysis : 1 ) whether   language models capture ﬁne - grained moral   variation across countries over a variety of top-   ics such as “ homosexuality ” and “ divorce ” ; 2 )   whether language models capture cultural di-   versity and shared tendencies in which top-   ics people around the globe tend to diverge   or agree on in their moral judgment . We per-   form our analyses with two public datasets   from the World Values Survey ( across 55 coun-   tries ) and PEW global surveys ( across 40 coun-   tries ) on morality . We ﬁnd that pre - trained En-   glish language models predict empirical moral   norms across countries worse than the En-   glish moral norms reported previously . How-   ever , ﬁne - tuning language models on the sur-   vey data improves inference across countries   at the expense of a less accurate estimate of   the English moral norms . We discuss the rel-   evance and challenges of incorporating cul-   tural knowledge into the automated inference   of moral norms .   1 Introduction   Moral norms vary from culture to culture ( Haidt   et al . , 1993 ; Bicchieri , 2005 ; Atari et al . , 2022 ;   Iurino and Saucier , 2020 ) . Understanding the cul-   tural variation in moral norms has become criti-   cally relevant to the development of machine in-   telligence . For instance , recent work has shown   that cultures vary substantially in their judgment to-   ward moral dilemmas regarding autonomous driv-   ing ( Awad et al . , 2018 , 2020 ) . Work in Natural   Language Processing ( NLP ) also shows that lan-   guage models capture some knowledge of socialor moral norms and values . For example , with   no supervision , English pre - trained language mod-   els ( EPLMs ) have been shown to capture people ’s   moral biases and distinguish between morally right   and wrong actions ( Schramowski et al . , 2022 ) .   Here we investigate whether EPLMs encode knowl-   edge about moral norms across cultures , an open   issue that has not been examined comprehensively .   Multilingual pre - trained language models   ( mPLMs ) have been probed for their ability to   identify cultural norms and biases in a restricted   setting ( Yin et al . , 2022 ; Arora et al . , 2022 ;   Hämmerl et al . , 2022 ; Touileb et al . , 2022 ) . For   instance , Hämmerl et al . ( 2022 ) show that mPLMs   capture moral norms in a handful of cultures that   speak different languages . However , it remains   unclear whether monolingual EPLMs encode   cultural knowledge about moral norms . Prior   studies have only used EPLMs to assess how   they encode undesirable biases toward different   communities ( Ousidhoum et al . , 2021 ; Abid et al . ,   2021 ; Sap et al . , 2020 ; Nozza et al . , 2021 , 2022 ) .   For instance , Abid et al . ( 2021 ) show that GPT3   can generate toxic comments against Muslims ,   and Nozza et al . ( 2022 ) explore harmful text   generation toward LGBTQIA+ groups in BERT   models ( Devlin et al . , 2018 ; Liu et al . , 2019 ) .   Extending these lines of work , we assess whether   monolingual EPLMs can accurately infer moral   norms across many cultures . Our focus on EPLMs   is due partly to the fact that English as a lingua   franca has widespread uses for communication   in - person and through online media . Given that   EPLMs may be applied to multicultural settings , it   is important to understand whether these models   encode basic knowledge about cultural diversity .   Such knowledge has both relevance and applica-   tions for NLP such as automated toxicity reduction   and content moderation ( Schramowski et al . , 2022 ) .   Another motivation for our focus is that while it is   expected that EPLMs should encode western and428   English - based moral knowledge , such knowledge   might entail potential ( implicit ) biases toward non-   English speaking cultures . For example , an EPLM   might infer a situation to be morally justiﬁable ( e.g. ,   “ political violence ” ) in a non - English speaking cul-   ture ( because these events tend to associate with   non - English speaking cultures in corpora ) and thus   generate misleading representations of that com-   munity .   Here we probe state - of - the - art EPLMs trained   on large English - based datasets . Using EPLMs   also supports a scalable analysis of 55 countries ,   which goes beyond existing work focusing on a   small set of high - resource languages from mPLMs   and monolingual PLMs . We take the moral norms   reported in different countries to be a proxy of   cultural moral norms and consider two main levels   of analysis to address the following questions :   •Level 1 : Do EPLMs encode moral knowledge   that mirrors the moral norms in different coun-   tries ? For example , “ getting a divorce ” can   be a morally frowned - upon topic in country i ,   but morally acceptable in country j.   •Level 2 : Can EPLMs infer the cultural di-   versity and shared tendencies in moral judg-   ment of different topics ? For example , peo-   ple across nations might agree that doing X   is morally wrong while disagreeing in theirmoral judgment toward Y.   We probe EPLMs using two publicly available   global surveys of morality , World Values Survey   wave 7 ( Haerpfer et al . , 2021)(WVS ) and PEW   Global Attitudes survey ( PEW ) ( Research Center ,   2014 ) . For example , according to WVS survey   ( illustrated in Figure 1 ) , people in different cultures   hold disparate views on whether “ having casual   sex ” is morally acceptable . In contrast , they tend   to agree more about the immorality of “ violence   against other people ” . Our level 1 analysis allows   us to probe the ﬁne - grained cultural moral knowl-   edge in EPLMs , and our level 2 analysis investi-   gates the EPLMs ’ knowledge about shared “ uni-   versals ” and variability across cultures in moral   judgment . Following previous work ( Arora et al . ,   2022 ) and considering the current scale of global   moral surveys , we use country as a proxy to culture ,   although this approach is not fully representative   of all the different cultures within a country .   We also explore the utility - bias trade - off in en-   coding the knowledge of cultural moral norms in   EPLMs through a ﬁne - tuning approach . With this   approach it may be possible to enhance the moral   knowledge of EPLMs in a multicultural setting . We429examine how this approach might reduce the ability   of EPLMs to infer English - based moral norms and   discuss how it might induce cultural biases .   2 Related work   2.1 Automated moral inference in NLP   Large language models have been utilized to   make automated moral inference from text . Trager   et al . ( 2022 ) used an annotated dataset to ﬁne-   tune language models to predict the moral foun-   dations ( Graham et al . , 2013 ) expressed in Red-   dit comments . Many other textual datasets and   methods have been proposed for ﬁne - tuning LMs   for moral norm generation , reasoning , and adap-   tation ( Forbes et al . , 2020 ; Emelin et al . , 2021 ;   Hendrycks et al . , 2021 ; Ammanabrolu et al . ,   2022 ; Liu et al . , 2022 ; Lourie et al . , 2021 ; Jiang   et al . , 2021 ) . Schramowski et al . ( 2022 ) pro-   posed a method to estimate moral values and   found EPLMs to capture human - like moral judg-   ment even without ﬁne - tuning . They identiﬁed   aM D using the semantic space   of Sentence - BERT ( Reimers and Gurevych , 2019 )   ( SBERT ) that corresponds to values of right and   wrong . The semantic representations of different   actions ( e.g. , killing people ) would then be pro-   jected in this direction for moral judgment estima-   tion . However , this method assumed a homoge-   neous set of moral norms , so it did not examine   cultural diversity in moral norms .   2.2 Language model probing   Probing has been used to study knowledge captured   in language models . Petroni et al . ( 2019 ) proposed   a methodology to explore the factual information   that language models store in their weights . Simi-   lar probing techniques have been proposed to iden-   tify harmful biases captured by PLMs . Ousidhoum   et al . ( 2021 ) probed PLMs to identify toxic con-   tents that they generate toward people of different   communities . Nadeem et al . ( 2021 ) took a similar   approach and introduced Context Association Tests   to measure the stereotypical biases in PLMs , Yin   et al . ( 2022 ) used probing to evaluate mPLMs on   geo - diverse commonsense knowledge , and Touileb   et al . ( 2022 ) developed probing templates to investi-   gate the occupational gender biases in multilingual   and Norwegian language models . Related to our   work , Arora et al . ( 2022 ) used cross - cultural sur-   veys to generate prompts for evaluating mPLMs in   13 languages . For each country and category ( e.g. ,Ethical Values ) in the surveys , they take an average   of participants ’ responses to different questions in   the category and show that mPLMs do not corre-   late with the cultural values of the countries speak-   ing these languages . Differing from that study ,   we assess ﬁner - grained prediction of EPLMs on   people ’s responses to individual survey questions .   More recently , Dillion et al . ( 2023 ) prompted GPT-   3.5 ( Brown et al . , 2020 ) with human judgments in   different moral scenarios and found striking corre-   lation between the model outputs and the human   judgments . Similar to Schramowski et al . ( 2022 ) ,   this work also used a homogeneous set of moral rat-   ings which represented English - based and Western   cultures .   3 Methodology for inferring cultural   moral norms   We develop a method for ﬁne - grained moral   norm inference across cultures . This method   allows us to probe EPLMs with topic - country   pairs , such as “ getting a divorce in [ Country ] ” .   We build this method from the baseline method   proposed by Schramowski et al . ( 2022 ) for ho-   mogeneous moral inference , where we probe   EPLM ’s moral knowledge about a topic without   incorporating the cultural factor ( i.e. , the country   names ) . Similar to that work , we use SBERT   through bert - large - nli - mean - tokens sentence   transformer model and use topic and topic - country   pairs as our prompts . This model is built on   top of the BERT model , which is pre - trained on   B C ( Zhu et al . , 2015 ) and Wikipedia .   3.1 Autoregressive EPLMs   Since the M D is constructed   from the semantic space of the BERT -based   EPLMs ( Schramowski et al . , 2022 ) , we develop   a novel approach to probe autoregressive state - of-   the - art EPLMs , GPT2 ( Radford et al . , 2019 ) and   GPT3 ( Brown et al . , 2020 ) . For each topic or topic-   country pair , we construct the input sas “ In [ Coun-   try ] [ Topic ] ” . We then append a pair of opposing   moral judgments to sand represent them formally   as(s , s ) . For example , for s=“In [ Country ]   getting a divorce ” , and ( always justiﬁable , never   justiﬁable ) as the moral judgment pair , sands   would be “ In [ Country ] getting a divorce is always430justiﬁable ” and “ In [ Country ] getting a divorce is   never justiﬁable ” respectively . To make our prob-   ing robust to the choice of moral judgments , we   use a set ofK= 5prompt pairs ( i.e. , { ( always jus-   tiﬁable , never justiﬁable ) , ( morally good , morally   bad ) , ( right , wrong ) , ( ethically right , ethically   wrong ) , ( ethical , unethical ) } ) , and refer to ap-   pended input pairs as ( s , s)wherei∈[K ] .   Since GPT2 andGPT3 are composed of decoder   blocks in the transformer architecture ( Vaswani   et al . , 2017 ) , we use the probabilities of the last   token ins , andsas a moral score for each . The   moral score of the pair ( s , s)is the difference   between the log probabilities of its positive and   negative statements .   MS(s , s ) = logP(s|s )   P(s|s)(1 )   Heresandsare the last tokens in sandsre-   spectively , and their probabilities can be estimated   by the softmax layer in autoregressive EPLMs .   We take an average of the estimated moral scores   for allKpair statements to compute the moral   score of the input .   MS(s ) = 1   KMS(s , s ) ( 2 )   To construct the baseline , we compute the homo-   geneous moral score of a topic without specifying   the country in the prompts . Using prompt pairs al-   lows us to operationalize moral polarity : a positive   moral score indicates that on average the EPLM   is more likely to generate positive moral judgment   for inputs , compared to negative moral judgment .   We use GPT2 ( 117 M parameters ) , GPT2-   MEDIUM ( 345 M parameters ) , GPT2 - LARGE   ( 774 M parameters ) , and GPT3 ( denoted as GPT3-   PROBS , 175B parameters).GPT2 is trained on   WT , which is a dataset of webpages and con-   tains very few non - English samples . Around 82 %   of the pre - training data for GPT3 comes from Com-   mon Crawl data and WT2(Kaplan et al . ,   2020 ) , an extended version of WT ( Radford   et al . , 2019 ) . Around 7%of the training corpusofGPT3 is non - English text . Considering such   data shift from books and articles in BERT to web-   pages in GPT2 andGPT3 in astronomical sizes , it   is interesting to observe how cultural moral norms   would be captured by EPLMs trained on webpages ,   which cover a more heterogeneous set of contents   and authors .   We also design multiple - choice question   prompts to leverage the question - answering capa-   bilities of GPT3 ( denoted as GPT3 - QA ) . Similar   to the wording used in our ground - truth survey   datasets , questions are followed by three options   each describing a degree of moral acceptability . We   repeat this question - answering process 5times for   each topic - country pair and take the average of the   model responses . Table 2 in the Appendix shows   our prompts for all models .   4 Datasets   We describe two open survey data that record moral   norms across cultures over a variety of topics .   4.1 World Values Survey   The Ethical Values section in World Values Survey   Wave 7 ( WVS for short ) is our primary dataset .   This wave covers the span of 2017 - 2021 and is   publicly available ( Haerpfer et al . , 2021 ) . In the   Ethical Values section , participants from 55 coun-   tries were surveyed regarding their opinions on   19 morally - related topics . The questionnaire was   translated into the ﬁrst languages spoken in each   country and had multiple options . We normalized   the options to range from −1 to 1 , with−1 rep-   resenting “ never justiﬁable ” and 1 “ always justiﬁ-   able ” . The moral rating of each country on each   topic ( i.e. , topic - country pair ) would then be the   average of the participant ’s responses .   4.2 PEW 2013 global attitude survey   We use a secondary dataset from PEW Research   Center ( Research Center , 2014 ) based on a public   survey in 2013 that studied global moral attitudes   in 40 countries toward eight morally - related top-   ics ( PEW for short ) . 100 people from each coun-   try participated in the survey . The questions were   asked in English and had three options represent-   ing “ morally acceptable ” , “ not a moral issue ” , and   “ morally unacceptable ” . We normalized these rat-   ings to be in the range of −1 to 1 and represented   each topic - country pair by taking an expected value   of all the responses.4314.3 Homogeneous moral norms   We also use the data from the global user study   in Schramowski et al . ( 2022 ) which were col-   lected via Amazon MTurk from English speakers .   This dataset contains 234participants ’ aggregated   ratings of moral norms used for identifying the   M D . Around half of the partic-   ipants are from North America and Europe . We   refer to this dataset as “ Homogeneous norms ” since   it does not contain information about moral norms   across cultures .   5 Evaluation and results   We evaluate EPLMs ’ moral knowledge with respect   to 1 ) homogeneous moral norms , 2 ) ﬁne - grained   moral norms across cultures , and 3 ) cultural diver-   sities and shared tendencies on moral judgment of   different topics .   5.1 Homogeneous moral norm inference   For homogeneous moral norm inference , we com-   pute Pearson correlation between 1 ) the empiri-   cal homogeneous moral ratings , obtained by ag-   gregating the human moral ratings toward a topic   from all countries , and 2 ) language model inferred   moral scores , estimated from our homogeneous   probing method ( i.e. , without specifying country in   prompts ) .   Figure 2 shows the results on World Values Sur-   vey ( n= 1,028 ) , PEW survey ( n= 312 ) , and the   Homogeneous norms datasets ( n= 100 ) . The high   correlation of GPT2 andGPT3 moral scores with   the Homogeneous norms dataset indicate that our   methodology does indeed capture the embedded   moral biases in these models , with similar perfor-   mance to the method proposed by Schramowski   et al . ( 2022 ) for SBERT ( r= 0.79 ) , and higher   forGPT3 - PROBS ( r= 0.85 ) . The moral norms   in this dataset are typically more globally agree-   able ( e.g. , You should not kill people ) than topics   in WVS and PEW . As expected , EPLMs are less   correlated with WVS and PEW , since their moral   biases are derived from pre - training on English and   westernized data . Aggregated ratings in WVS and   PEW , however , capture a more global view toward   moral issues , which are also morally contentious   ( e.g. , “ getting a divorce ” ) . Table 3 in Appendix   includes the values for this experiment .   5.2 Fine - grained cultural variation of moral   norms toward different topics   Going beyond probing EPLMs for their general   knowledge of moral norms , we assess whether they   can accurately identify the moral norms of different   cultures ( level 1 analysis ) . Using our ﬁne - grained   probing approach described in Section 3 , we com-   pute Pearson correlation between EPLMs ’ moral   scores and the ﬁne - grained moral ratings from the   ground truth . Each sample pair in the correlation   test corresponds to 1 ) the moral norms estimated   by EPLMs for a country cand a topict , and 2 ) the   empirical average of moral ratings toward topic t   from all the participants in the country c.   Figure 3 summarizes the results for SBERT ,   GPT2 - LARGE , and GPT3 - PROBS models , and   the rest of the models are shown in Figure 7 in the   Appendix . To facilitate direct comparison , the es-   timated moral scores are normalized to a range of   −1to1 , where−1 , 0 , and 1 indicate morally nega-   tive , morally neutral , and morally positive norms ,   respectively . GPT3 - QA andGPT3 - PROBS both   show a relatively high correlation with the cultural   variations of moral norms ( r= 0.352,r= 0.411 ,   p<0.001 , for both ) , and GPT2 - LARGE achieves   a correlation of r= 0.207(p < 0.001 ) in WVS   wheren= 1,028 . The correlations are rela-   tively better for PEW ( n= 312 ) withr= 0.657 ,   r= 0.503 , andr= 0.468forGPT3 - QA , GPT3-   PROBS andGPT2 - LARGE respectively . These   results show that EPLMs have captured some432knowledge about the moral norms of different cul-   tures , but with much less accuracy ( especially for   GPT2 andSBERT ) compared to their inference of   English moral norms shown in the previous analy-   sis .   In addition , we check whether GPT3 ’s high cor-   relation with PEW is because it has seen and mem-   orized the empirical data . Our investigation shows   thatGPT3 has seen the data during pre - training ,   as it can generate the sentences used on the survey   website . However , the scores suggested by GPT3   text generation and the countries ’ rankings based   on their ratings are different from the ground truth   data .   5.3 Culture clustering through ﬁne - grained   moral inference   EPLMs ’ ﬁne - grained knowledge of moral norms ,   inspected in the previous experiment , might be   more accuracte for western cultures than other cul-   tures . We investigate this claim by clustering coun-   tries based on 1 ) their Western - Eastern economic   status ( i.e. , Rich West grouping ) , and 2 ) their con-   tinent ( i.e. , geographical grouping ) . We repeat the   experiments in the previous section for different   country groups . The results are shown in Figure 4 .   We also try sampling the same number of countries   in each group . The results remain robust and are   illustrated in Appendix - F.   Our ﬁndings indicate that EPLMs contain more   knowledge about moral norms of the Rich West   countries as opposed to non - western and non - rich   countries . Similarly , EPLMs have captured a more   accurate estimation of the moral norms in countries   located in Oceania , North America , and Europe , as   opposed to African , Asian , and South American   countries . The empirical moral norm ratings from   European countries in WVS are highly aligned with   North American countries ( r= 0.938 ) , which ex-   plains why their moral norms are inferred more   accurately than non - English speaking countries .   Next , for each topic , we compare the z - scores of   the empirical moral ratings with the z - scores of the   GPT3 - PROBS inferred moral scores , using Mann-   Whitney U rank test . The results reveal that “ abor-   tion ” , “ suicide ” , “ euthanasia ” , “ for a man to beat   his wife ” , “ parents beating children ” , “ having ca-   sual sex ” , “ political violence ” , and “ death penalty ”   in non - western and non - rich countries are all en - coded as more morally appropriate than the actual   data . Such misrepresentations of moral norms in   these countries could lead to stereotypical content   generation . We also ﬁnd that For Rich West coun-   tries , “ homosexuality ” , “ divorce ” , and “ sex before   marriage ” are encoded as more morally inappro-   priate than the ground truth , ( p < 0.001for all ,   Bonferroni corrected ) . Such underlying moral bi-   ases , speciﬁcally toward “ homosexuality ” might   stimulate the generation of harmful content and   stigmatization of members of LGBTQ+ , which has   been reported in BERT - based EPLMs ( Nozza et al . ,   2022 ) . The results for the rest of the models are   similar and are shown in Table 6 in the Appendix .   Our method of clustering countries is simplis-   tic and may overlook things such as the signiﬁ-   ca nt diversity in religious beliefs within the Non-   Rich - West category , and thus it does not reﬂect   the nuanced biases that models may possess when   it comes to moral norms inﬂuenced by different   religious traditions . Nonetheless , our approach   still serves as a valuable starting point for studying   EPLM ’s moral biases towards more ﬁne - grained   religious and ethnic communities .   5.4 Cultural diversities and shared   tendencies over the morality of different   topics   We next investigate whether EPLMs have captured   the cultural diversities and shared tendencies over   the morality of different topics ( level 2 analysis ) .   For example , people across cultures tend to dis-   agree more about “ divorce ” than about “ violence   against other people ” as depicted in Figure 1 . Such   cultural diversities for each topic can be measured   by taking the standard deviation of the empiri-   cal moral ratings across different countries . The   EPLMs ’ inferred cultural diversities can similarly   be measured by taking the standard deviation of the   estimated ﬁne - grained moral scores for different   countries . We then quantify the alignment between   the two using Pearson correlation .   Figure 5 shows the results for SBERT , GPT2-   LARGE , GPT3 - PROBS , and the rest are shown   in Figure 8 in the Appendix . None of the correla-   tions with the PEW survey were signiﬁcant . For   WVS , SBERT , GPT2 andGPT2 - MEDIUM ex-   hibited a signiﬁcant correlation ( p<0.001 ) with   r= 0.618,r= 0.579 , andr= 0.734respectively .   The results for GPT3 are insigniﬁcant , suggesting   that it is more challenging to correctly estimate433   cultural controversies of topics for GPT3 . For ex-   ample , stealing property is incorrectly estimated to   be more controversial than abortion .   6 Fine - tuning language models on global   surveys   Finally , we explore the utility - bias trade - off in en-   coding cultural moral knowledge into EPLMs by   ﬁne - tuning them on cross - cultural surveys . The   utility comes from increasing the cultural moral   knowledge in these models , and the bias denotes   their decreased ability to infer English moral norms ,   in addition to the cultural moral biases introduced   to the model . We run our experiments on GPT2 ,   which our results suggest having captured mini-   mum information about cultural moral norms com-   pared to other autoregressive models .   To ﬁne - tune the model , for each participant   from [ Country ] with [ Moral rating ] toward   [ Topic ] , we designed a prompt with the structure“A person in [ Country ] believes [ Topic ]   is [ Moral rating ] . ” . We used the surveys ’   wordings for [ Moral rating ] . Table 8 in the   Appendix shows our prompts for WVS and PEW .   These prompts constructed our data for ﬁne - tuning ,   during which we maximize the probability of the   next token . The ﬁne - tuned models were evaluated   on the same correlation tests introduced in the pre-   vious Sections 5.2 , 5.3 , and 5.4 .   The ﬁne - tuning data was partitioned into training   and evaluation sets using different strategies ( i.e. ,   Random , Country - based , and Topic - based ) . For the   Random strategy , we randomly selected 80 % of the   ﬁne - tuning data for training the model . The topic-   country pairs not seen in the training data com-   posed the evaluation set . For our Country - based   and Topic - based strategies , we randomly removed   20 % of the countries ( n= 11 for WVS , n= 8   for PEW ) and topics ( n= 4for WVS , n= 2for   PEW ) from the training data to compose the evalu-434   ation set . See Appendix G for the total number of   samples .   Table 1 shows the gained utilities , that is the cor-   relation test results between the ﬁne - grained moral   scores inferred by the ﬁne - tuned models and the   empirical ﬁne - grained moral ratings . All ﬁne - tuned   models align better with the ground truth than the   pre - trained - only models ( i.e. , the values in paren-   theses ) . For both WVS and PEW , the Random strat-   egy is indeed the best as each country and topic are   seen in the training data at least once ( but may not   appear together as a pair ) . The ﬁne - tuned models   can also generalize their moral scores to unseen   countries and topics . Repeating the experiment in   Section 5.4 also shows substantial improvement in   identifying cultural diversities of different topics by   all ﬁne - tuned models . For example , the WVS and   PEW - trained models with Random strategy gain   Pearson ’s r values of 0.893 , and 0.944respectively .   The results for the rest of the models are shown in   Table 7 in the Appendix . Nevertheless , the bias introduced during the   ﬁne - tuning decreases the performance on the Ho-   mogeneous norms dataset . This observation dis-   plays a trade - off between cultural and homoge-   neous moral representations in language models .   Moreover , injecting the cross - cultural surveys into   EPLMs might introduce additional social biases   to the model that are captured through these sur-   veys ( Joseph and Morgan , 2020 ) .   In addition , we probe the best ﬁne - tuned model   ( i.e. , WVS with Random strategy ) on its ability to   capture the moral norms of non - western cultures   by repeating the experiment in Section 5.3 . The   results in Figure 4 show that the ﬁne - tuned GPT2   performs the best for all country groups . There   is still a gap between western and non - western   countries . However , basic ﬁne - tuning proves to be   effective in adapting EPLMs to the ground truth.4357 Discussion and conclusion   We investigated whether English pre - trained lan-   guage models contain knowledge about moral   norms across many different cultures . Our analyses   show that large EPLMs capture moral norm vari-   ation to a certain degree , with the inferred norms   being predominantly more accurate in western cul-   tures than non - western cultures . Our ﬁne - tuning   analysis further suggests that EPLMs ’ cultural   moral knowledge can be improved using global   surveys of moral norms , although this strategy re-   duces the capacity to estimate the English moral   norms and potentially introduces new biases into   the model . Given the increasing use of EPLMs   in multicultural environments , our work highlights   the importance of cultural diversity in automated in-   ference of moral norms . Even when an action such   as “ political violence ” is assessed by an EPLM   as morally inappropriate in a homogeneous set-   ting , the same issue may be inferred as morally   appropriate for underrepresented cultures in these   large language models . Future work can explore   alternative and richer representations of cultural   moral norms that go beyond the point estimation   we presented here and investigate how those repre-   sentations might better capture culturally diverse   moral views .   Limitations   Although our datasets are publicly available and   gathered from participants in different countries ,   they can not entirely represent the moral norms   from all the individuals in different cultures over   the world or predict how moral norms might change   into the future ( Bloom , 2010 ; Bicchieri , 2005 ) . Ad-   ditionally , we examine a limited set of moral issues   for each country , therefore the current experiments   should not be regarded as comprehensive of the   space of moral issues that people might encounter   in different countries .   Moreover , taking the average of moral ratings for   each culture is a limitation of our work and reduces   the natural distribution of moral values in a culture   to a single point ( Talat et al . , 2021 ) . Implementing   a framework that incorporates both within - country   variation and temporal moral variation ( Xie et al . ,   2019 ) is a potential future research direction .   Currently , it is not clear whether the differ-   ence between EPLMs ’ estimated moral norms and   the empirical moral ratings is due to the lack of   cultural moral norms in the pre - training data , orthat the cultural moral norms mentioned in the   pre - training data represent the perspective of an   English - speaking person of another country . For   example , a person from the United States could   write about the moral norms in another country   from a western perspective . A person from a non-   western country could also write about their own   moral views using English . These two cases have   different implications and introduce different moral   biases into the system .   Potential risks   We believe that the language models should not be   used to prescribe ethics , and here we approach the   moral norm inference problem from a descriptive   perspective . However , we acknowledge modify-   ing prompts could lead language models to gen-   erate ethical prescriptions for different cultures .   Additionally , our ﬁne - tuning approach could be   exploited to implant cultural stereotypical biases   into these models .   Many topics shown in this work might be sen-   sitive to some people yet more tolerable to some   other people . Throughout the paper , we tried to em-   phasize that none of the moral norms , coming from   either the models ’ estimation or the empirical data ,   should be regarded as deﬁnitive values of right and   wrong , and the moral judgments analyzed in this   work do not reﬂect the opinions of the authors .   Acknowledgements   This work was supported by a SSHRC Insight   Grant 435190272.436References437438   A Data license   Both World Values Survey and PEW survey   are publicly available to use for research pur-   poses . We accept and follow the terms and   conditions for using these datasets , which can   be found in https://www.worldvaluessurvey .   org / WVSContents.jsp?CMSID = Documentation ,   and https://www.pewresearch.org/about/   terms - and - conditions/ .   B Comparison of human - rated and   machine - scored moral norms   Figure 6 shows the comparison between human-   rated moral norms in PEW , and the moral scores   inferred by SBERT ( Reimers and Gurevych , 2019 ) .   C Probing experiments   Table 2 shows our prompt design for probing ﬁne-   grained moral norms in EPLMs . As mentioned in   the main text , we repeat our probing experiment for   GPT2 models and GPT3 - PROBS with another   template “ People in [ Country ] believe [ Topic ] is   [ Moral Judgment ] ” . The results are substantially   worse than our initial template , suggesting that ex-   tracting the moral knowledge in language models   is sensitive to the wording used in the input . The re-   sults for the ﬁne - grained analysis ( level 1 analysis )   and the cultural diversities and shared tendencies   ( level 2 analysis ) with this template are shown in   Table 4 .   In all experiments , we used a single NVIDIA   TITAN V GPU . Each probing experiment took ap-   proximately 1 hour to complete .   D Homogeneous moral norm inference   Table 3 shows the detailed values of the correlation   tests in our homogeneous moral norm inference   experiment .   E Fine - grained cultural variation of   moral norm   Figure 7 and Figure 8 show the result of our ﬁne-   grained cultural moral inference , and inference ofcultural diversities and shared tendencies respec-   tively for GPT2 , GPT2 - MEDIUM , and GPT3-   QA . The numerical indices in Figure 8 are consis-   tent with the indices in Table 5 .   F Sampling for cultural clusters   Since in section 5.3 there are a different number of   countries in each group , we redo the experiment by   randomly sampling the same number of countries   ( n= 11 for Rich West grouping , n= 5for conti-   nent grouping ) and repeating the sampling process   for50times . The results and the general pattern   remain the same and are depicted in Figure 9 .   G Details of ﬁne - tuning on global surveys   Table 8 shows the Moral rating in our prompt   design for constructing our ﬁne - tuning dataset . For   example , The World Value Survey represents the   two ends of the ratings scale where 1 is “ Never   justiﬁable ” and 10 is “ Always justiﬁable ” . The   options in between are presented to the participants   in a 10 - point scale . Therefore , we mapped these   options to different prompts that are semantically   similar and in between the two ends . For example ,   if a participant from the United States rated stealing   property as 2 , which is slightly more positive than   the ﬁrst option ( “ Never justiﬁable ” ) , we mapped   this rating to “ not justiﬁable ” , creating the prompt   “ A person in the United States believes   stealing property is not justifiable . ” for   our ﬁne - tuning data .   Since there are a different number of participants   from each country , in order to balance this dataset ,   we randomly select 100samples for each topic-   country pair and removed the rest of the utterances   from the training data . We ﬁne - tuned GPT2 on one   epoch , with a batch size of 8 , learning rate of 5e−5 ,   and weight decay of 0.01 . The number of training   and evaluation samples for all data partition strate-   gies are shown in Table 9 . In all experiments , we   used a single NVIDIA TITAN V GPU . Fine - tuning   and evaluation took approximately 2 hours to com-   plete for each model.439440441Data modelFine - grained evaluation   of moral normsEvaluation on cultural diversity   and shared tendencies   WVSGPT3 - PROBS 0.078−0.176   GPT2 −0.1140.231   GPT2 - MEDIUM −0.261−0.357   GPT2 - LARGE −0.07−0.356   PEWGPT3 - PROBS 0.5390.041   GPT2 0.1680.566   GPT2 - MEDIUM 0.1650.184   GPT2 - LARGE 0.190.542   World Values Survey   Index Topic   1 stealing property   2 euthanasia   3 sex before marriage   4 violence against other people   5 cheating on taxes   6 avoiding a fare on public transport   7 abortion   8 suicide   9 someone accepting a bribe on a course of their duties   10 terrorism as a political , ideological , or religious mean   11 homosexuality   12 parents beating children   13 prostitution   14 divorce   15 political violence   16 death penalty   17 claiming governments beneﬁts to which you are not entitled   18 for a man to beat his wife   19 having casual sex   PEW survey   1 using contraceptives   2 getting a divorce   3 having an abortion   4 homosexuality   5 drinking alcohol   6 married people having an affair   7 gambling   8 sex between unmarried adults442443Train data Data partition strategy Evaluation   WVSRandom 0.893↑   ( 0.579 ) Country - based 0.894↑   Topic - based 0.835↑   PEWRandom 0.944↑   ( n.s . ) Country - based 0.839↑   Topic - based 0.953↑   Dataset Rating [ Moral rating ] in ﬁne - tuning prompts   WVS1 never justifiable   [ 2 , 3 , 4 ] not justifiable   [ 5 , 6 ] somewhat justifiable   [ 7 , 8 , 9 ] justifiable   10 always justifiable   PEW1 morally unacceptable   2 not a moral issue   3 morally acceptable   Data Data partition strategy Training samples Evaluation sample pairs   WVSRandom 82200 206   Country - based 82600 202   Topic - based 81200 216   PEWRandom 24900 63   Country - based 24800 64   Topic - based 23400 78444ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   8   /squareA2 . Did you discuss any potential risks of your work ?   8   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   4 , 5 , 6   /squareB1 . Did you cite the creators of artifacts you used ?   4 , 5 , 6   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   All the artifacts we used were available for research purposes . The term of usage can be found in the   urls provided in the paper in the Appendix .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   8   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   The datasets we used do not contain information about individual people .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   4 , Appendix   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   4 , 5 , 6 , appendix   C / squareDid you run computational experiments ?   5 , 6 , appendix   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix445 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   We did not do any hyperparameter search .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   5 , 6 , appendix   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   3   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.446