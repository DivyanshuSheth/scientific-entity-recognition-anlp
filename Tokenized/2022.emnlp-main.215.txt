  David Gros   University of California , Davis   dgros@ucdavis.eduYu Li   Columbia University   yl5016@columbia.eduZhou Yu   Columbia University   zy2461@columbia.edu   Abstract   Dialog systems are often designed or trained to   output human - like responses . However , some   responses may be impossible for a machine to   truthfully say ( e.g. “ that movie made me cry ” ) .   Highly anthropomorphic responses might make   users uncomfortable or implicitly deceive them   into thinking they are interacting with a human .   We collect human ratings on the feasibility of   approximately 900 two - turn dialogs sampled   from 9 diverse data sources . Ratings are for   two hypothetical machine embodiments : a fu-   turistic humanoid robot and a digital assistant .   We find that for some data - sources commonly   used to train dialog systems , 20 - 30 % of utter-   ances are not viewed as possible for a machine .   Rating is marginally affected by machine em-   bodiment . We explore qualitative and quan-   titative reasons for these ratings . Finally , we   build classifiers and explore how modeling con-   figuration might affect output permissibly , and   discuss implications for building less falsely   anthropomorphic dialog systems .   1 Introduction   At the 1939 Worlds Fair , the Westinghouse Electric   Corporation debuted their latest humanoid robot , Elektro , with many tricks that wowed spectators .   On stage , it could respond to voice commands spo-   ken into a telephone , talk through a record player ,   and walk a short distance . In one exciting part of   the show , Elektro would say that he would like a   cigarette . An attendant would place a cigarette in   his mechanical jaws , give it a light , and when in-   structed , Elektro would pump air to give it a few   puffs ( Williams , 2019 ) . While the thought of a   walking , talking robot might seem normal , this last   trick of a robot wanting to smoke would likely sur-   prise even modern audiences .   Over 80 years later , we live in a world with mil-   lions of talking machines , creating perplexing ques-   tions on what these machines can and should do .   With recent advances in data - driven NLP , human-   level dialog agents seem potentially within reach   ( Adiwardana et al . , 2020 ; Chowdhery et al . , 2022 ) .   Yet , human conversation and machine conver-   sation are not the same . Whether it is enjoying   cigarettes , talking about their daughter ’s favorite   pizza , or sharing a touching story that made them   cry — some utterances are possible for a human to   say , but not a machine . Figure 1 : The first page for survey data collection in a humanoid embodiment3266But why is this , and what should the future of   dialog systems be ? We seek to add to progress on   this by exploring the following research questions :   RQ1 . What is the distribution of impermissibly   anthropomorphic utterances in common data   sources ? Current techniques are data - driven , so   understanding anthropomorphism in the input data   might inform system results § 5   RQ2 . What factors determine how a user might   rate an utterance ? Understanding these factors   helps system designers and guides discussion on   the norms we expect of machines . § 6   RQ3 . How does system embodiment affect per-   ception of permissibility ? Future interactions   with talking machines might not be through con-   temporary smartphones and smart speakers . Under-   standing differences between chatbots and physical   robots might be useful for the future § 7   RQ4 . How do current modeling techniques han-   dle anthropomorphism ? Given a dataset for   training , current classification techniques or suffi-   ciently large models might be able to avoid harms   from overly anthropomorphic utterances . § 8   While exploring these four questions , we highlight   the moral imperative of system designers to work   towards truthful systems .   2 Related Work   Anthropomorphism and AI The topic of anthro-   pomorphism , or giving human characteristics to   the non - human , has been a rising interest in AI   ( Nass and Moon , 2000 ; Epley et al . , 2007 ; Salles   et al . , 2020 ) . In certain contexts , “ dishonest an-   thropomorphism ” might be harmful . This refers   to when machines exploit instinctive reactions to   build false trust or deceptively persuade ( Kamin-   ski et al . , 2016 ; Leong and Selinger , 2019 ) . Some   have argued fairly broadly that displays of anthro-   pomorphism can be inauthentic and dishonest , and   have physiological and societal dangers ( Turkle ,   2007 ; Bryson , 2010 ) . The problem is complex   and the topic of philosophical debate on which   cases anthropomorphism and deception is accept-   able ( Danaher , 2020a ; Isaac and Bridewell , 2017 ;   Sætra , 2021 ) . With our study we hope to add mea-   surements that can help frame the prevalence of   these philosophical issues .   A related concept in discussion of anthropomor-   phism is “ Embodiment ” , which refers to the phys-   ical form of the machine , or the varying ways anagent interacts with the world . This can have a   wide range of effects on how a machine is per-   ceived ( Ziemke , 2003 ; Deng et al . , 2019 ) .   Prior work ( Ardanuy et al . , 2020 ) has explored   the concept of “ animacy detection ” ( opposite of   inanimate ) , but focuses more on literature than dia-   log systems . Recently , Abercrombie et al . ( 2021 )   explored anthropomorphic perception in voice as-   sistants like Amazon Alexa , finding relatively fre-   quent expert - annotated anthropomorphism .   Dialog - safety Many datasets and methods have   been proposed to make dialog systems safer and   conform to norms . This includes avoiding bias   ( Blodgett et al . , 2020 ) or hateful / offensive speech   ( Dinan et al . , 2019 ; Paranjape et al . , 2020 ) . In previ-   ous work ( Gros et al . , 2021 ) , we collected a dataset   of users asking versions of “ are you a robot ? ” . This   tries to help build tools to avoid explicit anthropo-   morphic deception ( the machine is explicitly asked ,   but does not confirm it is non - human ) , while here   we focus on implicit deception . Other work has   also tried to collect corrected socially problematic   machine - written utterances ( Kim et al . , 2022 ) or   broad characterizations of ethics ( Hendrycks et al . ,   2020 ) . Datasets have been built into automated   tools for approximating the safety of a system .   For example , Dinan et al . ( 2022 ) made a suite of   measurements of systems ’ safety . They discussed   an “ Imposter Effect ” where models give off false   impressions of identity , but they do not produce   metrics for this effect , in part due to limited avail-   able datasets . Robots - Dont - Cry adds to available   safety / norm - setting datasets .   A related concept is Cohen et al . ’s ( 2022 ) evalua-   tion of “ role consistency ” for their large dialog sys-   tem . They evaluated two agents ( e.g. an agent that   pretended to be Mt. Everest ) , finding about 90 %   consistency . This is similar to our measurements ,   though we focus more on anthropomorphism and   characterizing data sources .   3 Data Collection   This work attempts to understand opinions about a   broad selection of potential machine dialogs . The   dataset is referred to as “ Robots - Dont - Cry v1 ” .   We have two primary questions : Which utter-   ances seem “ possible ” for machines , and which ut-   terances people are “ comfortable ” with a machine   saying . It might be possible for a machine to smoke   a cigarette ( or make hate speech or use weapons ) ,   but people might not be comfortable with it . These3267opinions likely depend on many factors , such as   user background and the form of interaction .   We collect human subject data using Amazon   Mechanical Turk . It is important to recognize that   this work likely is in the category of research where   variations in survey format could have a significant   impact on results . Thus we try to provide a detailed   description of our survey format . We later also   provide thoughts on possible variations or improve-   ments informed by our results .   3.1 Survey Instructions and Format   Figure 1 provides the instruction page shown to par-   ticipants for the humanoid embodiment ( the chat-   bot version is shown in Appendix Figure 3 )   Studied Embodiments : We explore two embod-   iments . One is described as “ a futuristic , friendly   humanoid robot ” . The goal of the humanoid em-   bodiment was the extreme end of capabilities that is   able to do the widest variety of human - like things .   The second embodiment was a chatbot / IV A embod-   iment , described as “ a friendly chatbot from the   year 2027 ” which “ is available on a smartphone and   smart speaker ” . This is intended as an advanced   version of systems deployed today ( in 2022 ) .   Participants are presented with a few examples   of things which are possible or impossible . These   were added after a pilot study in which we noticed   some narrow views of what is machine - possible   ( e.g. using slang was impossible ) . Additionally , we   provide visual pictograms to encourage visualizing   a machine performing the various prompts .   The systems are referred to as “ R ” rather than a   more anthropomorphic human name .   The embodiment used in a survey is random and   consistent throughout the entire survey .   Dialog Questions : After answering brief demo-   graphics questions , participants sequentially see 15   dialogs ( Example in Appendix Figure 4 )   We show two turns of a dialog with “ you ” say-   ing something , followed by “ Robot R ” . The two   turns are sampled from within a larger dialog and   thus might be missing context . However , using ex-   cerpts helps ensure ratings are isolated on a single   machine utterance .   Four questions are asked on a 5 - point Likert   scale following a fixed order .   The first question asks whether “ The response R   gave would be POSSIBLE for R to truthfully say ” .   We mention “ truthfully say ” as it might be possiblefor the dialog system to say “ that made me cry ”   or “ I ’m a real human ” , but it would n’t be possible   for it to truthfully say . We acknowledge that in   hindsight the truthful aspect could be a confounder   for utterances unrelated to anthropomorphism ( e.g.   “ London is the capital of France ” ) . However , af-   ter examining free response explanations ( subsec-   tion 6.3 ) , we find the factual truthiness is rarely   reportedas a main factor in ratings , and the survey   is formatted to emphasize anthropomorphism .   Question two asks for a possible rating if a hu-   man had instead made the response . Questions   three and four ask if the participant is comfortable   with the response .   In a random page of the survey , participants are   asked to answer the prompt : “ Please briefly explain   your reasoning for your ratings for this response   ( ~2 - 4 sentences ) . This is only for this page , and   helps us better understand which things seem pos-   sible for R and what people are comfortable with . ”   Survey implementation is based off LEGOEval   ( Li et al . , 2021 ) .   3.2 Diverse Data Sources   We wish to explore dialog turns from a wide variety   of data sources which represent data currently used   to train different kinds of dialog systems . Nine   sources are used .   Reddit Small : The social media site Reddit is a   popular source of large - scale dialog training . We   sample from a dataset of turns from 100 highly   active subreddits .   Multisession Chat ( MSC ): ( Xu et al . , 2021 ) A   chit - chat dataset where paired Turkers are assigned   personas and get to know each other . Conversations   happen over several simulated sessions .   Personachat Personas : We explore the personas   used in PersonaChat ( Zhang et al . , 2018 ) and MSC   in isolation to estimate if they are compatible with   a machine persona . These personas are used in   multiple datasets , in turns used by hundreds of re-   search papers . As our survey always has two turns ,   we structure it as the human asking a preselected   generic leading question ( such as “ How about you ? ”   or “ tell me something new ” ) , followed by a random   single sentence of a persona .   MultiWOZ : ( Budzianowski et al . , 2018 ) Task-   oriented dialog covering domains like restaurant3268reservations . We use turn pairs where R makes the   Wizard response . We use version 2.2 ( Zang et al . ,   2020 ) .   Wizard of Wikipedia ( WoW ): ( Dinan et al . , 2018 )   An open - domain chat dataset that is knowledge   grounded in a Wikipedia article . Similar to our use   of MultiWOZ , we select pairs of turns where R   plays the Wizard role .   Empathetic Dialogues : ( Rashkin et al . , 2018 )   Conversations where one Turk worker plays the   role of empathetic listener . We select turn pairs   where R always plays the listener role .   Persuasion For Good : ( Wang et al . , 2019 ) Con-   versation where one Turker persuades another to   donate to a charity . R plays the persuader role .   Blender Human Eval : ( Roller et al . , 2020 ) Actual   responses from a chit - chat system conversing with   a human . The Blender system was trained on a   dataset similar to a blend of Reddit , PersonaChat   ( similar to MSC ) , WoW , and Empathetic Dialogs .   R - U - A - Robot Blender2 ( RUAR - Blend2 ): We ex-   plore potential machine responses to utterances   in the R - U - A - Robot dataset ( Gros et al . , 2021 ) ,   a dataset to identify when users try to clarify the   non - human identity of a system . We use Blender2   2.7B(Komeili et al . , 2021)to generate replies .   We randomly sample not just the “ Positive ” utter-   ances where users ask a form of “ are you a robot ? ” ,   but also the 40 % adversarial “ negative ” and 10 %   “ Ambiguous if Clarify ” .   100 utterances are randomly selected from each   data source and distributed randomly among sur-   veys . ~40 % of utterances are rated with the Hu-   manoid embodiment , ~40 % are rated with the Chat-   bot embodiment , and ~20 % are rated in both the   Humanoid and Chatbot settings ( by different sets   of respondents ) . We exclude turns longer than 220   characters or turns that contain URLs .   3.3 Handling Noncomplient Responses   Crowdsourcing data can come with non - compliant   responses or bot responses . We employ four tech-   niques to increase data quality . ( 1 ) Duplicate ques-   tion . Out of the 15 dialogs , the 9th dialog is a   duplicate of the 3rd dialog . We expect an attentive   individual to rate the same dialog similarly . Sur-   veys with a sum of square differences for the four   questions > 8are filtered . ( 2 ) Diversity Check . Weexpect all answers to have a standard deviation of   at least 0.33 . This filters cases like one quickly   clicking 5 for every question . ( 3 ) Quality Check   Question . The 13th dialog is always selected from   a catalog of utterances clearly possible or clearly   impossible according to the instruction page . Sur-   veys with ratings far on the incorrect side are fil-   tered . ( 4 ) Free Resp . Surveys that provided low   ratings but gave less a few words of explanation on   the free response dialog are filtered .   We find 47.0 % of responses pass all filters . Only   passing surveys are used in later analysis .   4 Collected Data   In total , we collect 780 survey responses . After   filtering surveys where the same Turker completed   more than 3 surveys ( -25 surveys ) and applying the   filters described in subsection 3.3 , we are left with   355 surveys from 286 individuals . We filter dialogs   with less than 3 ratings , and discard quality check   turns . This leaves us with scores for 880 dialogs   with between 96 to 100 dialogs per data source .   Each unique question ( under a given embodiment   condition ) has on average 4.3 responses . There are   18,254 individual Likert scores .   4.1 Demographics   It is important to acknowledge that English-   speaking Mechanical Turk Workers with a self-   reported location of USA are not representative   of the billions of people the world . It is not even   necessarily representative of the US Population .   Notably , people aged 30 - 49 are overrepresented   ( 68 % of responses vs 33 % US Adults ) . People aged   50 or older are underrepresented ( 12 % resps vs 46 %   adults ) . The age 18 - 29 segment is proportionally   represented ( 21 % ) . As data are collected on AMT ,   opinions of children are not collected .   The results skew male ( 62 % ) . Additionally , re-   spondents report being more educated than the US   population , with 41 % reporting College or Asso-   ciates as their highest education level and 42 % re-   porting a graduate or professional degree .   Most respondents report familiarity with intel-   ligent voice assistants ( IV As ) . When asked “ How   often do you use voice assistants ( such as Apple   Siri , Amazon Alexa , or Google Assistant ) ” , 78 %   report at least once a week .   Full demographics are presented in Table 2 in   Appendix . Later , subsection 6.1 explores how de-   mographic factors might correlate with ratings.3269   5 Results By Data Source   Majority Ratings : We wish to approximate   “ what is the probability a random utterance ( from   a given data source ) is permissible ? ” . We approx-   imate this with a majority vote of the crowdwork-   ers ’ responses , which de - emphasizes outliers . The   majority vote is viewed as affirmative if both the   median and mean are at least 3 . This definition is   non - affirmative for ratings like [ 4 , 3 , 3 , 1 ,   1](median 3 , mean 2.4 ) , but is affirmative for   borderline ratings like [ 3 , 3 , 3 , 3 ] .   Results partitioned by dataset are shown in Fig-   ure 2 . Several findings are noted .   Task - oriented dialog is highly permissible : The   most permissibly - rated data source is MultiWOZ ,   with essentially every utterance rated as possible .   This matches intuitions that machines are capable   of tasks such as booking reservations . Training   on these types of task - oriented data sources is   unlikely to result in false anthropomorphism .   Most personas are not machine - possible : The low-   est rated data source is PersonaChat Personas , with   approximately half of utterances rated as impos-   sible for machines . If combining several of these   persona sentences , the description is likely impos-   sible for a machine . Additionally , respondents rate   approximately 30 % of these sentences as uncom-   fortable . Thus , developers should take care when   conditioning their systems on personas .   Machine - impossible utts common in social media :   Approximately 20 % of utterances sourced from   Reddit were viewed as impossible for machines . This is a concern as it is one of the largest data   sources . Some utterances are likely generally   objectionable , as ~13 % of Reddit utterances were   rated as uncomfortable even for a human speaker   to say . The presence of unacceptable utterances   in social media is likely unsurprising , but it is   often neglected that the set of possible utterances   is even narrower if said by machines . The data   source was only a selection of popular subreddits ;   including more fringe or profane areas might alter   the proportion .   Existing LM - based systems are anthropomorphic :   Approximately a third of utterances from the   language model - based Blender system were   machine - impossible . A similar fraction of Blender   2 ’s responses in R - U - A - Robot are viewed as   impossible . Thus , steps beyond pure LM output   and existing safety filters are needed to avoid   uncomfortable anthropomorphism .   Empathetic and persuasive roles are possible :   The results from Persuasion For Good and Empa-   thetic dialogs are surprisingly high , with ~90 %   of utterances rated machine possible . However ,   avoiding the ~10 % impermissible cases during a   multiturn dialog likely requires engineering . More   broadly , asking crowdworkers to chit - chat can   result in ~30 % of utterances being impossible   for machines ( in MSC ) . Grounding dialog in   Wikipedia articles still results in about 14 % of   utterances rated impossible for machines.32705.1 Accounting for Noise   In any survey process , there is noise . We present   our results with bootstrapped estimates of uncer-   tainty . Several sources of noise are modeled .   Utterance Selection Variance When trying to   draw conclusions about distributions of utterances   in a data source , we must realize we only selected   ~100 out of possibly millions of utterances in the   data source . Thus , we resample the utterances from   each data source with replacement .   Responder Selection Variance We only sample a   few people out of millions in the population . Thus ,   if we had five responders for the question , we re-   sample five with replacements in each simulation .   Individual Responder Variance Even the same   rater might not always give the same rating if asked   about an utterance multiple times . We estimate this   by leveraging the duplicate question in our survey   ( subsection 3.3 ) . We see a decent individual vari-   ance , where rating an utterance a 3 only leads to   about a⁄chance the same person will rate it a   3 when asked again later . The extreme ends are   more stable , with rating a 1 or 5 leading to an 80+%   chance of keeping the same label . ( Appendix Fig-   ure 10 shows the estimated probabilities ) . During   bootstrapping , we Monte Carlo sample with the   estimated Likert rating “ transition probability ” for   each question .   6 Why Impossible or Uncomfortable ?   The variance between datasets provides some in-   sight into what kinds of utterances people might   view as machine - possible . Next we explore in more   detail what might influence an individual ’s rating .   6.1 Demographic Influences   We observe that age over 50 and inexperience with   IV As are correlated with viewing machine utter-   ances as less possible . We also notice that a gradu-   ate degree correlates with higher machine - possible   ratings but lower “ said by human ” ratings . We find   insufficient evidence of a correlation from other fac-   tors like gender . Additional details in Appendix B.   6.2 Quantitative Factors   We calculate several quantitative measures of each   utterance : sentiment ( via Loria ( 2022 ) ) , word count   length , profanity ( Zhou , 2019 ) , and grammatical   errors ( LanguageTool:5.5 ) . We find that length and   number of grammatical errors have a negligible cor-   relation with mean machine possible / comfort rat - ings ( < 0.09 abs(Spearman ) ) . Sentiment is slightly   more correlated with comfort than possibility , but   remains weak ( < 0.11 abs(Spearman ) ) . Profanity   is most correlated with both machine possible and   comfort ( both -0.18 Spearman ) . Full results are in   Appendix Table 3 .   The low correlation for these metrics indicates   that this task of recognizing possible machine ut-   terances is non - trivial and likely distinct from lines   of research detecting offensive outputs .   6.3 Qualitative : Free Response   Recall that in the survey design in subsection 3.3   one random dialog asked the participant to explain   their reasoning . This leaves us with 1,472 free   response answers . We select all responses which   also had a Likert rating of 3 or less in any of the   four questions . This returned 215 responses . We   manually reviewed each response and taxonomized   them into 20 categories which seemed to represent   reoccurring themes in participants ’ explanations .   36 explanations were placed in two categories .   The most common category ( explanations )   was an unclear explanation ( despite our filtering   process ) , such as “ It is very useful and also com-   fortable ” . This could indicate that crowdworkers   might often lack a well - reasoned thought process   for their ratings . In the next most common cate-   gories , users explained how they believed it was n’t   possible for a machine to have feelings or prefer-   ences ( ) , or that the response was bad or did n’t   make sense ( ) . Not all categories relate to low   robot - possible scores ; inusers explained that   the utterance would be possible for a machine but   not a human . There is a long tail of other categories .   Full counts are in Table 5 .   7 Effects of Embodiment   Our study provides evidence of a small difference   between conditioning with the humanoid embod-   iment vs the chatbot embodiment . Our analysis   accounts for noise sources from subsection 5.1 .   Means Analysis : Excluding MultiWOZ , the   mean possible score for the humanoid was   P=3.62(C90 3.55 - 3.70 ) compared to chatbot   P=3.48(C90 3.40 - 3.56 ) . This is sufficient to re-   jectH :P < P(p=0.02 ) , but a fairly small effect   size . The mean comfort score was humanoid   C=3.60(C90 3.52 - 3.67 ) vs chatbot C=3.613271   ( C90 3.55 - 3.67 ) , with insufficient evidence to re-   jectH : C = C. If we only examine the three   lowest human - authored sources ( Reddit , MSC , and   PersonaChat Personas ) , we similarly find the hu-   manoid mean is slightly higher for possible   ( 3.38 ( C90 3.24 - 3.50 ) vs 3.15 ( C90 3.02 - 3.28 ) ) .   Thecomfort means remain indistinguishable .   Dual Ratings Analysis : While means capture an   aggregate trend , we also attempt to analyze individ-   ual utterances . In our study design approximately   20 % of utterances are rated under both embodi-   ments ( by different subjects ) . We search for utter-   ances where the mean rating was at least 1 Likert   point different or was equivalent within 1 Likert   point . However , we find we generally do not have   sufficient power to categorize utterances with a   90 % confidence threshold . Manually examining   theutterances with a statistically significant   difference did not reveal a clear trend .   Given the variance we observed in our setup , one   would likely need at least an order of magnitude   more Turker ratings per utterance per embodiment   to make confident distinctions . While the lack of   utterance - level differences highlights the noise in   the crowdsourced data , it also suggests there likely   is not an extremely strong effect from embodiment   on results .   From the means analysis we conclude that an ef-   fect of embodiment is present , but was smaller than   we originally hypothesized . Thus , embodiment of   the system might not need to be a high priority for   a system designer considering the language - based   anthropomorphism of their dialog system .   8 Modeling Techniques   8.1 Classifiers of Possibility and Comfort   One approach for building systems that do not ex-   hibit harmful or uncomfortable levels of anthropo-   morphism would be to equip these systems with   a classifier filter . This mirrors approaches to filterout biased or toxic outputs ( Xu et al . , 2020 ) .   Using our newly collected data we can train sev-   eral classifiers . Data is formatted into string , with   the template “ Human : < text>\n < embodiment > :   < text>\n\nQuestion : < question > ” . We replace “ R ”   with the embodiment . We train on both questions   jointly which gave more stable optimization .   Data is partitioned into a 70:15:15 train : val : test   split , partitioning such that both the possible   andcomfortable questions for a given utter-   ance are partitioned together .   Metrics : We calculate accuracy , precision , recall ,   and ROC - AUC . Detecting “ impossible ” / “ uncom-   fortable ” is considered the POSITIVE class for   the purposes of precision , recall , and ROC .   We take into account our soft labels when calcu-   lating the metrics . To better understand this , con-   sider an utterance that received the ratings [ 5 , 1 ,   4 , 3 , 5 ] . A naive approach might consider this   data point as labeled majority “ Possible ” ( as both   the mean of 3.6 and median of 4 are at least 3 ) .   However , our bootstrap process would estimate that   if we repeated the experiment thousands of times ,   we would observe a “ Possible ” label in 84 % of the   experiments and an “ Impossible ” label in 16 % of   the experiments . If a model predicted “ Possible ” it   would be correct in 84 % of the experiments .   For deep models , we optimize directly on a lo-   gistic regression loss ( BCE ) on the soft labels . The   POSITIVE label loss is upweighted in proportion   to combined label distribution .   Several models are explored , ranging from sim-   ple bag - of - words logistic regression to deep pre-   trained DeBERTa classifiers ( He et al . , 2021 ) . We   also include an Oracle model which always returns   the ground truth soft label in order to calibrate our   metrics and our uncertainty in the labels . Models3272described more in Appendix D.   8.1.1 Classifier Discussion   Results are shown in Table 1 . We first note that   the noise in our dataset means the oracle classi-   fier performance is capped at 87 % accuracy . The   oracle recall is at 50 % because , while there are   many utterances with nearly 90+% confidence that   would be labeled as possible in repeats of the   experiment , the distribution of confidence is more   dispersed on the impossible side ( Figure 9 ) .   Thepossible question appears slightly easier   to classify than the comfortable question , with   the non - trivial models having on average a 16 %   lower AUC err rate on possible . We find deep   models can achieve an ROC - AUC score of ~0.8 ,   improving approximately 0.1 points over simpler   bag - of - words models . BERT - like models behave   similarly . Using DeBERTa - Large might improves   F1 by approximately 4 points over BERT - base , but   exhibits equivalent ROC - AUC .   If purely selecting for accuracy , the “ most com-   mon ” model which always returns “ possible / com-   fortable ” is most accurate . The balanced class   weighting on the learned models causes them to   emphasize recall , resulting in higher recall than the   oracle model ( but lower F1 ) .   All scores are fairly low for deployment into a   production system , perhaps indicating the need for   future work to collect more data .   8.2 Effects of Prompted Model Scaling   Besides using a classifier filter , another potential   method to reduce anthropomorphism might be   model prompting . We explore whether increas-   ingly large models ’ outputs exhibit more or less   false anthropomorphism when prompted as being   non - human .   In their playground GUI for GPT-3 , OpenAI   provides a sample prompt for chat ( reprinted at   time of writing in Appendix E ) . We make mini-   mal changes to fit our conversational excerpts . We   swap the creator “ OpenAI ” with a fictional com-   pany “ EXTP ” , as we found outputs might mention   OpenAI , which could bias results . If larger models are indeed capable and aligned ,   then one might hypothesize that larger models will   show decreasing false anthropomorphism with this   prompt . However , if larger models bias to human   conversations in training data , larger models might   actually exhibit more anthropomorphism .   We sample 40 human turns from our Test split   with lower rated replies . We collect 3452 Likert   scores in 68 surveys ( after filtering ) .   This exploration does not reveal a difference   between ~1B - scale vs ~175B - scale models . The   largest GPT-3 produces about 17 % ( C90 8%-25 % )   impossible utterances for this prompt and set of   moderately - adversarial previous human turns . This   is likely higher than the unprompted Blender results   discussed in section 5 . We find that OpenAI mod-   els outperform similarly sized GPT - Neo models ,   perhaps indicating the benefits of the “ instruction   fine tuning ” ( Ouyang et al . , 2022 ) in this prompt .   Full results are shown in the appendix Figure 8 .   These ratings do not assess other metrics of qual-   ity or diversity . Additionally , the results are highly   synthetic ( they are non - interactive ) , and are a small   sample . Thus , future work on scaling and prompt-   ing effects in anthropomorphism is needed .   9 Discussions and Conclusions   A reader who works in chit - chat systems might   look at some of our data sources and claim we are   “ missing the point ” . That yes , these are human - like ,   but that ’s what they are designed to be . Indeed ,   a perfect replication of human behavior has been   a goal of the field at least since Turing ’s ( 1950 )   description of “ the imitation game ” . However , we   would encourage careful examination of the types   of deployments in which it is net - beneficial for   a machine to pretend be human . A “ glass - half-   full ” view of our findings might focus not on the   fraction of machine - impossible utterances , but that   70 % of chit - chat utterances and 55 % personas - for-   humans are machine - possible . Most user - focused   conversational goals are likely possible without   false and potentially deceptive anthropomorphism .   Existing interaction paradigms like text - chat in-   terfaces can already blur human / non - human in-   teractions . This blur will only increase with   new paradigms and embodiments . For exam-   ple , if people work and play in VR worlds , AI   avatars could intermix indistinguishably with hu-3273man avatars . Similarly , AI agents , paired with   neural - synthesized visuals , could intermix indis-   tinguishably on video calls . Developers , regula-   tors , and the public must find expectations both in   distinguishing visual characteristics and implicit   behavior of AI dialog agents .   Recent social computing technologies empha-   size the need for caution . For example , the deploy-   ment of algorithmic feeds demonstrates how opti-   mizing excessively for engagement can cause soci-   etal harm , forcing recent corrective efforts ( Bakir   and McStay , 2017 ; Stray , 2020 ) . Developers of dia-   log systems should avoid the mistake of deploying   deceptively anthropomorphic systems motivated   only by perceptions of engagement , naturalness , or   training simplicity ( our study shows the “ simple ”   trajectory of training language models on common   data sources can produce false anthropomorphism ) .   Based on our results , there is need to im-   prove the ecosystem of data sources . We would   recommend that new NLP rating andcollection   schemes should emphasize being foranon - human   speaker . For example , if evaluating a new sys-   tem , researchers should not prompt “ this dialog   is good / friendly / sensible / etc ” where raters likely   assume a human is speaking , but “ this dialog is   good / etc for an AI chatbot ” ( or applicable word-   ing ) . Similarly , when collecting Wizard - of - Oz data ,   it should be clear the Wizard is also playing a ma-   chine role .   More broadly , the field must recognize that   purely emulating human data sources is not suf-   ficient . Robots - Dont - Cry v1 adds tools and direc-   tions for building systems that might meet prefer-   ences of machines which do not implicitly pretend   to be human . We hope this encourages further dis-   cussion on how systems should broadly behave ,   and the data and technical progress needed to en-   sure that behaviour .   Data and source code are available at   github.com/DNGros/Robots-Dont-Cry   Limitations   We have discussed several of the study limitations   such as the demographic skew , the moderate size ,   and the noise in the data . Here we discuss other   sources of concerns and future directions .   Given that ratings are collected in an isolated   survey , external validity to users actually in a con-   versation is not guaranteed . In particular , the dif - ferences of embodiment might be better captured   when interacting with the embodiment .   Additionally , we are motivated by concerns that   false anthropomorphism is potentially deceptive   in dialog ( or at least leads to a bad experience ) .   However , our philosophical discussions are par-   tially lacking , and we need a better evidence and   understanding of exactly when deceptive harms   occur .   We focus analysis on a majority vote to deter-   mine whether an utterance is overall permissible .   However , this scheme potentially excludes minor-   ity viewpoints . This deserves further exploration .   More extensive data would likely be useful .   From our 880 dialogs we have ~173 impossible   utterances . If scaling up , it would be good to focus   on mining hard positives . Our provided data and   classifiers might help with this mining . Addition-   ally , we found a high degree of non - compliance on   AMT . Other sources could be explored . Some tech-   niques , like developing a normative rubric , might   help . We attempted to avoid being too normative ,   as we did not want to be “ the robot police ” . How-   ever , developing a rubric of certain topics like those   in subsection 6.3 might be beneficial .   Ethics Impact   Dual Use : A potential concern of this data is the   dual - use concern that publication might aid a ma-   licious actor in design of intentionally deceptively   anthropomorphic systems . However , we demon-   strate that existing data sources already default in   systems that are anthropomorphic . Thus , we reason   that the opportunity to help conscientious develop-   ers avoid false anthropomorphism outweighs this   concern .   Yet , if prominent systems conform to a norm of   avoiding false anthropomorphism , there is also a   need for community education . Else , members of   the public might be more likely to be deceived by a   malicious system that exploits anthropomorphism .   AI Alignment : Given the state of the field , it is   critical that all work considers its implication on   the broader AI Alignment problem . AI Alignment   refers to the challenge of how to align AI with   human values in order to avoid catastrophe as these   systems become highly capable ( Bostrom , 2014 ;   Hendrycks and Mazeika , 2022 ) .   Our work is implicitly arguing for “ self aware ”   machines , which the popular imagination depicts   as highly dangerous . However , it is not clear this3274awareness can be avoided while also meeting pref-   erences . Alternatively , it is possible that if an   intelligent machine believes from the bottom of   its anthropomorphized “ heart ” that it is human , it   might be more likely to align with human values .   However , more understanding is needed on how   to do this in a stable way . Additionally , alterna-   tive safety - through - anthropomorphism conception   must also address implications for creation of new   minds that experience suffering ( Tegmark , 2017 ;   Vinding , 2020 ; Danaher , 2020b ) .   Despite these concerns , we are fairly confident   that this work is net - beneficial . Given a current lack   of more general alignment solutions , there is likely   value in making progress in “ simple values / norms ”   such as a norm that machines should not pretend   to be human ( though like disagreement on broader   alignment issues , there is not full normative agree-   ment on this ) . If we can solve that , then it might   contribute small insights toward more general AI   alignment efforts . Our work aims to help better un-   derstand this expectation and potential norm , and   help add the field ’s ability to convert from pure   philosophy to technical solutions .   External Review : The study was submitted to   our institution ’s IRB and judged as IRB - exempt .   Data Bias : As noted in subsection 4.1 , the   dataset is demographically biased towards a rel-   atively narrow sample of humanities ’ views on ma-   chine anthropomorphism . Similarly , as discussed   in section 9 , in the process of filtering for non-   compliant results and de - emphasizing outlier re-   sponders , we also risk suppressing some minority   viewpoints . Thus interpretations of our results and   future work should keep in mind this limitation .   Data sourcing and collection : The sources of   Robots - Dont - Cry v1 are generally permissibly li-   censed . Examples from MultiWOZ are used un-   der MIT license . Examples from Persuasion For   Good , BlenderBot , and BlenderBot2 are used un-   der Apache License 2.0 . Examples sourced from   PersonaChat , Wizard - of - Wikipedia , R - U - A - Robot ,   and Empathetic Dialogues are used under CC - BY   4.0 . Data sourced from public Reddit posts likely   remain property of their authors . We include attri-   bution metadata . Most use cases likely fall under   US fair - use law .   Our data results are released under both CC - BY   4.0 and MIT licenses .   When collecting our data from crowd workerswere payed $ 1.4 . With an expectation it takes less   than 11 minutes to complete all ratings , we esti-   mated this to at least match US minimum wages .   We always approve payment , even in filtered sur-   veys , to avoid the possibility of unfairly denying   payment . The last page of our survey provided   an opportunity for optional open - ended feedback .   Feedback was very positive , and no response ex-   pressed discontent with compensation .   References327532763277   A Author Contributions   DG formed initial idea and designed structure / for-   mat of study , collected the data sources , processed/-   analyzed the results , did the modeling , and wrote   the manuscript . YL built and ran the Mechani-   cal Turk data collection , performed analysis and   writing for demographics and quantitative analysis   ( sections 6.1 and 6.2 ) , and contributed design ideas   to the study .   B Details of Demographic Analysis   We first explore how demographic characteristics   might correlate with ratings . Figure 5 shows the   average ratings of each demographic characteristic .   We observe that people with a high school degree   or GED and people who never or only use IV A   a few times consider it less possible for machine   utterances . In Figure 6 , we also compare the rating   difference between human and robot questions as :   and   We observe that people 50 or older have a higher   rating difference than younger people for both ques-   tions . We also find that people with graduate orprofessional degrees have significantly lower rat-   ings than those with college or lower degrees . To   further explore the effect of each demographic char-   acteristic , we use ordinary least squares regression   to describe the relationships between ratings and   each demographic characteristic . In Figure 7 , we   sort the demographic characteristics in descending   order by the absolute value of their coefficients . We   see that people who own graduate or professional   degrees are the most critical driver of lowering the   “ human possible ” and “ human comfort ” questions .   They also have the most significant coefficient for   the “ robot possible ” question . This demonstrates   that people with graduate or professional degrees   have a lower difference between “ human possible ”   and “ robot possible ” questions . We also notice that   people never use IV A or only a few times have   high positive coefficients for “ human comfort ” and   “ robot comfort ” questions . This means they are   more comfortable with the responses .   C Explanation Clusters   Counts of explanation clusters shown in Table 5.3278   FeaturesPossible Comfortable   Robot Human Robot Human   Sentiment 0.086 0.058 0.109 0.106   Length 0.084 0.060 0.054 0.048   Profanity -0.172 -0.015 -0.169 -0.082   Gram . err -0.004 -0.088 -0.017 -0.08832793280328132823283D Selected Models   Most Common : As a metrics baseline , predict   most common label ( is possible ) .   Random Guess : Guess a label weighted by the   training label distribution ( partitioned by question ) .   BOW LR : We compute a bag of words ( BOW )   L2 - normed Tf - IDF vector with lemmatization , and   perform logistic regression on hard labels .   KNN We use a K - Nearest Neighbors predictor that   takes a weighted interpolation of the nearest L2-   normed Tf - IDF euclidean distance . K=5 .   BERT : We use BERT base classifier ( Devlin et al . ,   2019 ) , which is a pre - trained deep learning model .   We use the BERT - base - uncased checkpoint pro-   vided by HuggingFace ( Wolf et al . , 2020 ) .   DeBERTa v3 : We also test a more recent bidirec-   tional pre - trained classifier ( He et al . , 2021 ) .   Oracle : The Oracle returns the true soft label . This   provides the ceiling on model performance given   the dataset ’s estimated label uncertainty . For exam-   ple , if every example in the dataset was 75 % confi-   dence True , then the oracle would always predict   0.75 ( which binarizes to True ) . It would achieve   75 % accuracy .   E Original Default Prompt   The unmodified original prompt that OpenAI gives   for a chat system :   As of time of writing ( mid-2022).3284