  Jun YanYang XiaoSagnik MukherjeeBill Yuchen Lin   Robin JiaXiang Ren   University of Southern CaliforniaFudan UniversityIIT Kanpur   { yanjun,yuchen.lin,robinjia,xiangren}@usc.edu   17307100059@fudan.edu.cn sagnikm@iitk.ac.in   Abstract   We study the robustness of machine read-   ing comprehension ( MRC ) models to entity   renaming — do models make more wrong pre-   dictions when the same questions are asked   about an entity whose name has been changed ?   Such failures imply that models overly rely   on entity information to answer questions , and   thus may generalize poorly when facts about   the world change or questions are asked about   novel entities . To systematically audit this is-   sue , we present a pipeline to automatically gen-   erate test examples at scale , by replacing entity   names in the original test sample with names   from a variety of sources , ranging from names   in the same test set , to common names in life , to   arbitrary strings . Across five datasets and three   pretrained model architectures , MRC models   consistently perform worse when entities are   renamed , with particularly large accuracy drops   on datasets constructed via distant supervision .   We also find large differences between models :   SpanBERT , which is pretrained with span - level   masking , is more robust than RoBERTa , de-   spite having similar accuracy on unperturbed   test data . We further experiment with different   masking strategies as the continual pretraining   objective and find that entity - based masking   can improve the robustness of MRC models .   1 Introduction   The task of machine reading comprehension   ( MRC ) measures machines ’ understanding and rea-   soning abilities . Recent research advances ( Devlin   et al . , 2019 ; Yang et al . , 2019 ; Khashabi et al . ,   2020 ) have driven MRC models to reach or even   exceed human performance on several MRC bench-   mark datasets . However , their actual ability to   solve the general MRC task is still questionable   ( Kaushik and Lipton , 2018 ; Sen and Saffari , 2020 ;   Sugawara et al . , 2020 ; Lai et al . , 2021 ) . While hu - Figure 1 : An illustrative example of the robustness to   entity renaming and our proposed perturbations for ro-   bustness evaluation . “ Michael ” is from the answer of   another test instance . “ Ashvith ” is a person name from   an external database . “ Uqlcs ” is a random string with   the same length as the original name .   mans show robust generalization on reading com-   prehension , existing works have revealed that MRC   models generalize poorly to out - of - domain data dis-   tributions ( Fisch et al . , 2019 ) and are brittle under   test - time perturbations ( Pruthi et al . , 2019 ; Jia et al . ,   2019 ; Jia and Liang , 2017 ) . All these issues could   naturally happen to MRC systems deployed in the   wild , hindering them to make reliable predictions   on user inputs with great flexibility .   In this work , we focus on an important but under-   studied type of test - time distribution shift caused   by novel entity ( e.g. , person and company ) names .   Besides the evidence provided by the surrounding   context , an MRC model also has the capacity to   leverage the entity information to make predictions   ( Sugawara et al . , 2018 ; Chen et al . , 2016 ) . The   information associated with the entity name covers   both world knowledge that can change over time   and dataset shortcuts that are unlikely to general-   ize . While contributing to performance on certain   benchmarks , the over - reliance on specific entity   names leads to an overestimation of model ’s ac-   tual ability to read and comprehend the provided508passage ( Peñas et al . , 2011 ) . It also hinders model   generalization to novel entity names , which itself   is challenging due to the large space of valid en-   tity names induced by the flexibility of entity nam-   ing . For example , person names can be chosen   from a large vocabulary depending on the country ,   while companies can be named in an even more   creative way , not to mention new names that are   being invented every day . As illustrated in Figure 1 ,   keeping the reasoning context unchanged , a robust   MRC model is supposed to correctly locate the   same span of a named entity as the answer , even   after it gets renamed .   To audit model robustness , we use entity renam-   ing as test time perturbation to mimic the situation   where a deployed MRC model encounters ques-   tions asking for novel entity names in the emerging   data . We design a general pipeline to generate nat-   ural perturbations of MRC instances by swapping   the answer entity name with another valid name   throughout the passage . We design perturbation   rules and collect resources for three types of enti-   ties with large name space : Person , Organization ,   and Geopolitical Entity .   With this proposed analysis framework , we con-   duct extensive experiments on five datasets and   three pretrained language models . Data - wise , we   find that distantly supervised MRC datasets lead to   less robustness . Entity - wise , we find that geopolit-   ical entities pose a greater challenge than people   and organizations when renamed . Model - wise , we   find that SpanBERT is more robust than BERT and   RoBERTa , mainly due to its lower sensitivity to   domain shift on names , which is likely a benefit of   its span - focused pretraining objective . Inspired by   this , we investigate several continual pretraining   objectives and find that an entity - based masking   strategy can further improve robustness .   2 Analysis Setup   2.1 Extractive MRC   The task of MRC tests a machine ’ understanding   and reasoning abilities by asking it to answer the   question based on the provided passage . We focus   on extractive MRC , where the answer is a span   in the passage . Formally , given a question Q   and a passage Pofntokens P={x . . . , x } ,   a model is expected to predict an answer span   a={x , . . . , x}(1≤i≤i+k≤n)in the   passage Pas a response to the question Q. We use   exact match ( EM ) as the metric for MRC evalua-   tion , which is the percentage of test instances that   the model exactly predicts one of the gold answers .   In both real - world scenarios and MRC datasets ,   a large portion of questions ask about entities like   people , organizations and locations . While unmen-   tioned background knowledge about the entities   might be helpful for solving the questions , overly   relying on it makes the model hard to adapt to up-   dated facts provided by the passage and generalize   to novel entities . Especially , we contrast MRC with   closed - book QA , which requires a model to directly   answer questions without access to any document   passage . Closed - book QA tests a model ’s ability   to pack knowledge into its parameters and retrieve   knowledge from parameters to answer the ques-   tion . On the contrary , we expect an MRC model to   reason based on the provided passage .   2.2 Evaluation Protocol   We study the robustness of MRC models via test-   time perturbation . Given an original test set D   and a perturbation function f ( detailed in § 3 )   as inputs , we construct Nperturbed test sets with   Nperturbation seeds . We evaluate the model on   theNperturbed test sets . By averaging the results ,   we get the average - case EM score as the final   metric , which measures the average impact on the   model performance caused by the names from a   certain perturbation . We set N= 5 in experiments .   2.3 Datasets   We choose five datasets with different characteris-   tics from the MRQA 2019 shared task ( Fisch et al . ,   2019 ): SQuAD ( Rajpurkar et al . , 2016 ) , Natural   Questions ( NQ ) ( Kwiatkowski et al . , 2019 ) , Hot-   potQA ( Yang et al . , 2018 ) , SearchQA ( Dunn et al . ,   2017 ) , and TriviaQA ( Joshi et al . , 2017 ) . Since   the official test sets of the MRQA datasets are hid-   den , we use the development set as the in - house   test set , and hold out 10 % of the training data as   the in - house development set . Their statistics are   shown in Table 1.509As a major difference in data collection , SQuAD ,   NQ , and HotpotQA employ crowdworkers to an-   notate the answer span in the passage , while   SearchQA and TriviaQA use distant supervision   to match the passage with the question . Distant   supervision provides no guarantee that the passage   contains enough evidence to derive the answer . The   context where the entity span shows up may not   even be related to the question .   2.4 MRC Models   We experiment with three pretrained language mod-   els that have demonstrated strong performance   on popular MRC benchmarks . BERT ( Devlin   et al . , 2019 ) is trained on English Wikipedia   plus BookCorpus with masked language model-   ing ( MLM ) and next sentence prediction ( NSP ) as   self - supervised objectives . RoBERTa ( Liu et al . ,   2019 ) improves over BERT mainly by dropping   the NSP objective and increasing the pretraining   time and the size of pretraining data . SpanBERT   ( Joshi et al . , 2020 ) masks random contiguous spans   to implement MLM and replaces NSP with a span-   boundary objective ( SBO ) .   The pretrained language models are finetuned   on the MRC dataset to predict the start and end to-   kens of the answer span based on the concatenated   question and passage ( Devlin et al . , 2019 ) . By de-   fault , all pretrained language models in the main   experiments are case - sensitive and in their BASE   sizes . More details are shown in Appendix § A.   3 Entity Name Substitution   In this section , we introduce our method for per-   turbing an MRC test set with substitution entity   names , i.e. , the instantiation of f . Generating   substitution names is at the core of our evaluation   as different kinds of names measure a model ’s be-   havior in different situations with different robust-   ness implications . We propose three categories of   perturbations on three entity types and collect the   corresponding name resources , aiming to audit a   model ’s robustness from different perspectives .   3.1 Perturbation Pipeline   As illustrated in Figure 2 , our perturbation pipeline   consists of four steps , which are introduced below .   Step 1 : Answer Entity Recognition . As we fo-   cus on the effect of answer entity renaming , we   first identify entities in the answers by perform-   ing named entity recognition ( NER ) with spaCy   ( Honnibal et al . , 2020 ) on the passage and extract   the results on the answer spans . We identify three   types of named entities : Person ( PER ) , Organiza-   tion ( ORG ) , and Geopolitical Entity ( GPE ) . All of   them frequently appear as answers and have large   space of valid names , making it important and chal-   lenging for models to robustly handle .   Step 2 : Perturbable Span Identification . To fa-   cilitate name substitution , we assign metadata to   detected entity names by identifying perturbable   spans within the entity name . For each type of en-   tity names , we define the applicable span types in   Table 2 . The heuristics for identifying each type of   perturbable spans are introduced in Appendix § B.   Note that given one or more entity types of interest ,   in this step we filter the test data to only keep a   subset of instances with non - empty metadata for   the corresponding entity types , which are instances   that are ready to be perturbed . Sizes of the per-   turbable subsets for different entity types and their   union ( MIX ) are shown in Table 3 .   Step 3 : Candidate Name Sampling . For each   perturbable span , we get its substitution name by   querying an external dictionary with the span type .   The substitution name is randomly sampled from   a pool of names in the external dictionary with   the same span type . We collect dictionaries with   names of different characteristics serving for differ-   ent analysis purpose , which are detailed in § 3.2.510   Step 4 : Name Substitution . Once we have a   candidate name for each perturbable span , we per-   form string mapping on the passage , question , and   the gold answer , to finish the entity renaming in   MRC instances . The name substitution changes all   mentions of the answer entity in the passage while   keeping the other reasoning context .   3.2 Candidate Name Collection   We consider three types of candidate names for   perturbations in our main experiments to simulate   the domain shift of entity names during test time .   In - Distribution Name ( InDistName ) . The set of   candidate names with their span types is the same   as the perturbable spans with their types identified   from the gold answers in the test set . This ensures   that no new name is introduced to the test set .   Database Name ( DBName ) . We collect names   in the real world by referring to relevant databases .   For PER , we collect first names(with gender fre-   quency ) and last namesfrom the official statistics   of person names in the U.S. ( We experiment with   names from other countries and languages in § 4.5 . )   We regard a first name as a male / female name if   its male / female frequency is two times larger than   its frequency of the opposite gender . The remain-   ing names are considered as neutral . Following   the practice for identifying perturbable spans , we   get the list of country / state / city names using Coun-   tries States Cities Database and the NNP list using   PTB . Rare words constitute an open vocabulary   so they will not be substituted under the DBName   perturbation .   Random String ( RandStr ) . The RandStr pertur-   bation is different from the other two as it neglects   the query span type when preparing the candidates .   We generate a random alphabetical string of the   same length and casing as the original perturbable   span . Names from low - resource languages can   look quite irregular to the pretrained language mod-   els . Random string as an extreme case provides an   estimation of the performance in this scenario .   3.3 Perturbation Quality   The validity of the perturbed instances depends on   the quality of the perturbation pipeline ( § 3.1 ) . We   manually check the accuracy of the perturbation   steps on TriviaQA , which demonstrates the largest   performance drop as we will show . Out of the four   steps in the pipeline ( Figure 2 ) , we evaluate the ac-   curacy of step 2 ( “ Perturbable Span Identification ” )   and step 4 ( “ Name Substituion ” ) while the accu-   racy of the other two steps can be inferred . The   evaluation details are provided in Appendix § C.   As shown in Table 4 , our method gets acceptable   accuracy on the three entity types , confirming the   quality of the perturbed test sets .   4 Results and Analysis   The average - case EM scores on the original and   perturbed test sets are presented in Figure 3 . We   report the mean and standard deviation over 3 train-511   ing seeds . We analyze the results from several   angles by aggregating across certain dimensions .   4.1 Which datasets lead to less robustness ?   Training on MRC datasets created with dis-   tant supervision leads to less robustness . In   Table 5 , we show the results of BERT on the   original and perturbed test sets , while results of   RoBERTa and SpanBERT follow similar patterns .   The perturbations on all 3 entity types are combined   ( shown as “ MIX ” ) . We find that models trained on   SQuAD , NQ , and HotpotQA ( with at most 6 % per-   formance drop under the DBName perturbation)are significantly more robust than models trained   on SearchQA and TriviaQA ( with about 20 % per-   formance drop under the DBName perturbation ) .   While the first group of datasets are human - labeled ,   the later group of datasets are constructed using   distant supervision . Such correlation indicates that   training noise due to mismatched questions and pas-   sages harms model ’s robustness . We hypothesize   the reason to be that , the passage in the human-   annotated datasets usually provides enough evi-   dence to derive the answer , so a model is able to   learn the actual task of “ reading comprehension ”   from the data . On the contrary , SearchQA and Triv-   iaQA use web snippets as the source of passages .   The labeling process of distant supervision assumes   that “ the presence of the answer string implies the   document does answer the question ” ( Joshi et al . ,   2017 ) , while the document may or may not contain   all facts needed to support the answer . In this case ,   because the actual reading comprehension task is   difficult to learn due to lack of evidence , the model   could be prone to use entity - specific background   knowledge ( e.g. assuming that “ Jack Higgins ” is   a British author regardless of the context ) or learn   dataset - specific shortcuts associated with certain   names via memorization ( e.g. , choosing “ Jack Hig-   gins ” whenever it ’s mentioned in the passage and   the question asks for an author ) , which causes the   robustness issue .   To better understand the failure cases , we catego-512   rize the errors made by the model into two classes :   wrong entity errors and wrong boundary errors ,   based on whether the predicted span has any word   overlap with the gold answer . We report the per-   centage of wrong entity errors in Table 6 . On all   datasets , wrong entity errors make up a larger per-   centage of all errors when the test sets get perturbed .   This suggests that the performance drop is mainly   caused by the increasing errors in identifying the   correct answer entity rather than accurately predict-   ing the boundary of a correctly - identified answer   entity .   4.2 Which entity types are more challenging ?   GPE renaming poses the greatest robustness   challenge . The renaming of PER and ORG are   similarly less challenging . In Table 7 , we present   the performance drop caused by the DBName per-   turbation for each entity type . GPE renaming   shows the largest performance drop . The com-   parison of PER and ORG differs across datasets ,   but their corresponding performance drops are gen-   erally similar . The reason is likely to be that the   model is only exposed to a small number of dis-   tinct GPE names during finetuning compared to   PER and ORG . In the training set of TriviaQA ,   there are 40k ORG names and 54k PER names ,   but only 12k GPE names . The lack of seen names   makes it hard to learn the generalization ability .   4.3 Which models are more robust ?   On distantly supervised datasets , SpanBERT   is more robust than RoBERTa , which is more   robust than BERT . In Table 8 , we show the per-   formance of the three models under perturbations   of the MIX entity type on SearchQA and Trivi-   aQA . While RoBERTa and SpanBERT show com-   parable performance on the original and InDist-   Name test sets , SpanBERT ’s improvement over   RoBERTa becomes larger with more difficult per-   turbations . Meanwhile , BERT shows even larger   performance decreases than RoBERTa . The mod-   els ’ performance differences are mainly attributed   to their different pretraining strategies . RoBERTa ’s   improvement over BERT indicates that a better   pretraining configuration ( as measured by the per-   formance on the in - domain original test set ) is also   beneficial to the performance on the perturbed test   sets , suggesting better generalization ability to the   out - of - domain data . This correlation is consistent   with the findings in Miller et al . ( 2021 ) . Span-   BERT ’s particular advantage on the perturbed test   sets indicates its span - focused pretraining objective   ( span - based MLM and span prediction based on   boundary tokens ) is especially helpful for the span-   related robustness , which is desired for the MRC   task .   Larger models are not more robust . In Ta-   ble 9 , we compare the performance of the BASE   andLARGE variants of pretrained models on Triv-   iaQA . The performance drops from the original   test sets to the perturbed test sets are similar for   these two variants in most cases , suggesting that   simply increasing the model size can not resolve   the robustness issue .   4.4 How can we disentangle reasons for   performance drop ?   Both loss of entity knowledge and domain shift   on names happen during renaming . The infor-   mation associated with the entity name that can513   be leveraged by the model includes both entity   knowledge and name clues . Entity knowledge   refers to the world knowledge associated with the   referred entity , like “ Michelle Obama is the wife   of Barack Obama , ” while name clues refer to sta-   tistical clues associated with the name ’s surface   form , like “ Barack Obama is likely to be a male   name ” , “ Barack Obama as an in - distribution name   is likely to be the answer for this dataset ” . While   all perturbations break the entity knowledge about   the original entity , InDistName does n’t introduce   additional domain shift on names and largely pre-   serve the name clues . Going from InDistName to   other perturbations , the substitution names become   more and more out of the dataset distribution . This   performance drop can be attributed to the model ’s   sensitivity to name - related domain shift .   We adopt two measurements to better understand   the domain shift on names . As a token - level mea-   surement , we calculate the percentage of test an-   swer entity tokens that are never seen in entities in   training answers and entities in training passages ,   as shown in Table 10 . Different datasets have differ-   ent percentages of unseen tokens in the original test   sets , which are mainly affected by the size and di-   versity of training data . The number goes up with   the DBName and RandStr perturbations . As an   entity - level measurement , we train an NER model   on the training passages , with named entities an-   notated by spaCy as ground truth . We evaluate the   trained model on perturbed test sets and calculateits accuracy of recognizing the perturbed answer   entity . The results are shown in Table 11 . As a sign   of domain shift , the recognition of answer enti-   ties become more difficult when they get perturbed .   GPE shows the most significant perturbation drop ,   which correlate with our observation on the MRC   task ( § 4.2 ) .   SpanBERT ’s superior robustness over   RoBERTa is mainly from handling domain   shift . From SearchQA and TriviaQA results in   Table 8 , we find that RoBERTa and SpanBERT   rely similarly on the entity knowledge ( ~13 %   performance drop from Original to InDistName on   SearchQA and ~11 % on TriviaQA ) . SpanBERT ’s   advantage over RoBERTa is mainly on its good   robustness to domain shift on names , shown by   the perfromance drop from IndistName to other   perturbations . BERT relies slightly more on entity   knowledge but much more sensitive to domain   shift on names .   4.5 Bias Exhibited by Person Names   National Origins . As the DBName perturbation   uses person names in the U.S. , it can not fully reflect   the model ’s robustness behavior when encounter-   ing real - world names of different national origins .   Therefore , we additional collect names from more   countries ( India , China ) and languages ( French ,   Arabic ) to study the potential bias in MRC mod-   els . We use the romanized form of names . Ta-   ble 12 shows the performance comparison of mod-   els when evaluated with the person names from   different countries and languages on SearchQA   and TriviaQA . Names form the U.S. and French-   speaking countries generally achieve the highest   EM scores . Names from China get the lowest per-   formance for the most of time , with significant EM   drop ( 8.4 % on SearchQA and 9.8 % on TriviaQA   for BERT ) from U.S. names . The performance gap   between different countries and languages becomes   smaller with more robust models .   Other Factors . We also consider other factors   of a name that could be related to biased model   performance . We limit our scope to the U.S. first   names and sample 1500 names from the database .   We consider two features for each name . Gender   polarity is defined as max ( , ) , where f , f   are the male frequency and female frequency of a   name provided by the database . It measures the   gender ambiguity of the name . Popularity is de-   fined as f+f . We calculate the EM score for a514   name by evaluating on a test set where all answer   first names get replaced with this name . For what   we have tried , we did n’t find evidence to support a   correlation between each factor and the EM score .   For example , with SpanBERT on TriviaQA , names   with top 20 % gender polarity gets 72.7 % EM on   average ; while the bottom 10 % names gets 72.8 %   EM . The numbers are 73.0 % vs 72.7 % for popular-   ity . We leave exploring factors that correlate with   the difficulty of a name as future work .   4.6 Improving Robustness with Continual   Pretraining   SpanBERT ’s advantage over BERT suggests that   some variants of MLM could be helpful for model   robustness . To further improve the robustness of   SpanBERT , we adopt a training paradigm with an   inserted continual pretraining stage and compare   MLM with different masking strategies as the ob-   jectives .   Training Paradigm . Existing works mainly seek   to improve model robustness during finetuning   with strategies like data augmentation ( Ribeiro   et al . , 2019 ; Min et al . , 2020 ) , but they usually in-   crease finetuning time and requires additional data .   Some recent works ( Gururangan et al . , 2020 ; Ye   et al . , 2021 ) have explored improving a pretrained   language model with “ continual pretraining ” —   continuing to train a pretrained model for more   steps with some objective . The generated check-   point can be used for finetuning on any dataset in   thestandard way with no additional cost .   Experimental Setup . The masking policy in   MLM plays an important role in instructing model   learning , which can be potentially used to improve   model robustness . Inspired by previous works ,   we experiment with four heuristic masking poli-   cies to implement the MLM objective : MLM   ( vanilla ) , MLM ( whole word ) , MLM ( span ) , andMLM ( entity ) . They perform masking at token ,   whole - word , span , and entity level respectively .   Starting from SpanBERT ( - BASE ) , we run contin-   ual pretraining with the above objectives for 8,000   steps . More details are described in Appendix § D.   Results . The results for models finetuned from   SpanBERT and different continually pretrained   models are shown in Table 13 . On SQuAD , all   masking policies slightly downgrade the perfor-   mance . With not much room for robustness im-   provement , running continual pretraining is prob-   ably at the cost of slightly sacrificing the perfor-   mance due to the inconsistent objective and discon-   tinuous learning rate that are applied when start-   ing the continual pretraining . On SearchQA and   TriviaQA , out of the four masking policies , the   entity - based masking policy shows consistent im-   provement over SpanBERT . As analyzed in § 4.4 ,   name - related domain shift is a major challenge for   the model to handle . By predicting the masked en-   tity , the model is exposed to the diverse entities in   the pertraining corpus in a more explicit way , and   gain a better sense in recognizing entities . Note   that the improvement is not statistically significant   in some cases and we leave the exploration of more   effective methods to improve model robustness as   future work .   5 Related Work   Robustness of MRC Models . The robustness   of MRC models are usually evaluated against test-   time perturbations and out - of - domain data . Re-   search on test - time perturbation proposes perturba-   tion methods at different levels as attacks ( Si et al . ,   2021 ) , such as word replacement with neighbors in   the vector space ( Rychalska et al . , 2018 ; Jia et al . ,   2019 ) , question paraphrasing ( Gan and Ng , 2019 ;   Ribeiro et al . , 2018 ) , sentence distractor injection   ( Jia and Liang , 2017 ; Zhou et al . , 2020 ) . Another   line of research ( Fisch et al . , 2019 ; Sen and Saffari ,   2020 ) tests a model on data with out - of - domain   passage or question distributions , usually from dif-   ferent datasets . Our work mainly falls into the   category of test - time perturbation . We distinguish   from previous work by focusing on the effect of   entity renaming , with the motivation that entities   can have flexible and diverse names in the real life .   Model Robustness to Entity Substitution . It is   non - trivial for NLP models to be able to properly   handle the large space of named entities . Previ-515   ous works use entity substitution to audit or im-   prove model robustness on different tasks like NER   ( Agarwal et al . , 2020 ; Lin et al . , 2021 ) , Natural Lan-   guage Inference ( Mitra et al . , 2020 ) , Coreference   Resolution ( Subramanian and Roth , 2019 ) , and Di-   alogue State Tracking ( Cho et al . , 2021 ) . Shwartz   et al . ( 2020 ) experiment with name swapping to   show that a trained MRC model has bias on some   U.S. given names due to the grounding effects that   associate names with certain entities . Ribeiro et al .   ( 2020 ) and Balasubramanian et al . ( 2020 ) investi-   gate the robustness of models on several tasks with   named entity replacement . However , these works   did n’t systematically test on MRC datasets with dif-   ferent characteristics to unveil the actual robustness   challenge . Liu et al . ( 2021 ) study the novel entity   generalization ability of open - domain QA models   by categorizing the test questions based on whether   the named entities have been seen during training .   Longpre et al . ( 2021 ) analyze the memorization   behavior of generative open - domain QA models   using knowledge conflicts . They use entity sub-   stitution to create test passages that contain facts   contradicting to what the model has learned during   training time . In contrast , we analyze extractive   MRC model ’s robustness when encountering new   entities , by evaluating on modified test sets without   intentionally introduced knowledge conflicts . The   extractive task formulation also makes the model   unable to output its memorized knowledge as gener-   ative models , leading to different analysis questions   and methods .   6 Conclusion   In this paper , we systematically study the robust-   ness of MRC models to entity name substitution .   Specifically , we propose a substitution framework   along with candidate names of different implica-   tions . We experiment with three pretrained lan-   guage models on five MRC datasets . We find thatmodels trained on distantly - supervised datasets are   susceptible to entity name substitution , while mod-   els trained on human - annotated datasets are rela-   tively robust , with GPE renaming harder than PER   and ORG renaming . The lack of robustness can   be further attributed to model ’s overreliance on en-   tity knowledge and name clues . We also find that   SpanBERT , which is pretrained using span - level   objectives , shows better robustness than BERT and   RoBERTa . Leveraging these insights , we study de-   fense approaches based on continual pretraining   and demonstrate that entity - based masking poli-   cies are beneficial to model ’s robustness . Future   works include systematically studying the effect of   background knowledge in MRC , and developing   more effective methods to improve the robustness   of MRC models .   Acknowledgments   This research is supported in part by the Office   of the Director of National Intelligence ( ODNI ) ,   Intelligence Advanced Research Projects Activity   ( IARPA ) , via Contract No . 2019 - 19051600007 ,   the DARPA MCS program under Contract No .   N660011924033 , the Defense Advanced Research   Projects Agency with award W911NF-19 - 20271 ,   NSF IIS 2048211 , NSF SMA 1829268 , and gift   awards from Google , Amazon , JP Morgan and   Sony . We would like to thank all the collabora-   tors in USC INK research lab for their constructive   feedback on the work . We would also like to thank   the anonymous reviewers for their valuable com-   ments .   References516517518519A Details for MRC Model Training   We train all MRC models using mixed precision ,   with batch size of 16 sequences for 4 epochs . The   maximum sequence length is set to 256 tokens . We   use the AdamW optimizer ( Loshchilov and Hutter ,   2019 ) with an initial learning rate of 2e-5 that is   linearly decayed to 0 during finetuning .   B Perturbable Span Identification   For PER , we only consider names with one or two   words . A one - word name is considered as a first   name , while a two - word name is considered as a   full name , with the first word being the first name   and the second word being the last name . We infer   the gender of the detected name to be male , female ,   or neutral with gender - guesser .   For GPE , we detect its contained country names ,   state names , and city names by string matching   with the Countries States Cities Database .   For ORG , besides mentions of GPE names , we   include two additional types of perturbable words   identified using Penn Treebank ( PTB ) ( Marcus   et al . , 1993 ) . Words that are annotated as NNP(S )   for more than 90 % of the time in PTB are consid-   ered as proper nouns ( dented as < NNP > ) , which   are usually specialized for naming an entity . Words   outside PTB are considered as rare words ( denoted   as < Rare > ) , which are likely to be invented by peo-   ple to name an entity . These two kinds of words are   weakly related to the characteristics of the entity   and thus can be flexible .   C Evaluation of Perturbation Quality   The accuracy of step 2 is evaluated based on   whether the perturbable spans and their correspond-   ing span types are all correct for an instance , which   also implies the quality of step 1 ( “ Answer Entity   Recognition ” ) as different entity types have differ-   ent applicable span types .   The accuracy of step 4 is evaluated based on   whether the string mapping function successfully   locates all mentions of the perturbable spans in the   passage to perform string mapping .   The quality of step 3 can be inferred from the   accuracy of step 2 for InDistName perturbation .   For DBName , we assume the database is of accept-   able quality in the sense that all names it provides   belongs to the correct span type , which is guaran-   teed by the source of the data — PTB is annotatedby human experts , U.S. names come from official   statistics , and GPE names are actively maintained   by its creator and the community for more than 3   years . RandStr is proposed to simulate the extreme   case , and we therefore do not evaluate its quality .   D Details for Continual Pretraining   MLM ( vanilla ) refers to the masking strategy used   by BERT ( Devlin et al . , 2019 ) , where the masked   tokens are randomly sampled . MLM ( whole word )   always masks all tokens corresponding to a word   at once . MLM ( span ) uses the masking strategy   proposed by Joshi et al . ( 2020 ) , which masks ran-   dom spans rather than individual whole words or   tokens . MLM ( entity ) masks a random entity for   50 % of the time , and uses MLM ( span ) for the other   50 % of the time . The idea is inspired by salient   span masking proposed in Guu et al . ( 2020 ) . All   strategies mask 15 % of the training tokens in total .   To eliminate domain shift during continual pre-   training as a possible explanation for any improve-   ments , we keep the corpus for continual pretraining   consistent with the pretraining corpus used by Span-   BERT , which is the concatenation of BookCorpus   and English Wikipedia . We train using mixed pre-   cision , with effective batch size of 2,048 sequences   for 8,000 steps , with 256 tokens per sequence . We   use the AdamW optimizer with a constant learning   rate of 1e-4.520