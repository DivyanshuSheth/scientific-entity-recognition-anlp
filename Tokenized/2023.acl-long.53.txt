  Kevin Pei , Ishan Jindal , Kevin Chen - Chuan Chang , Chengxiang Zhai , Yunyao LiUniversity of Illinois at Urbana - Champaign , IBM Research , Apple   { kspei2,kcchang,czhai}@illinois.edu , ishan.jindal@ibm.com ,   yunyaoli@apple.com   Abstract   Open Information Extraction ( OpenIE ) has   been used in the pipelines of various NLP tasks .   Unfortunately , there is no clear consensus on   which models to use for which tasks . Muddy-   ing things further is the lack of comparisons   that take differing training sets into account . In   this paper , we present an application - focused   empirical survey of neural OpenIE models ,   training sets , and benchmarks in an effort to   help users choose the most suitable OpenIE   systems for their applications . We find that the   different assumptions made by different mod-   els and datasets have a statistically significant   effect on performance , making it important to   choose the most appropriate model for one ’s   applications . We demonstrate the applicabil-   ity of our recommendations on a downstream   Complex QA application .   1 Introduction   Open Information Extraction ( OpenIE ) is the task   of extracting relation tuples from plain text ( An-   geli et al . , 2015 ) . In its simplest form , OpenIE   extracts information in the form of tuples consist-   ing of subject ( S),predicate ( P),object ( O ) , and any   additional arguments ( A ) . OpenIE is an open do-   main , intended to be easy to deploy in different   domains without fine - tuning , with all relations ex-   tracted regardless of type . The increasing avail-   ability of semi - automatically generated training   datasets ( Cui et al . , 2018 ) as well as significant ad-   vances in deep learning techniques have led to the   development of state - of - the - art neural models ( Cui   et al . , 2018 ; Garg and Kalai , 2018 ) .   Since its introduction in Etzioni et al . ( 2008 ) ,   OpenIE has attracted a large amount of attention by   the research community as a tool for a wide range   of downstream NLP tasks ( Mausam , 2016 ) . How-   ever , there is no real consensus on which OpenIE   model is best for each application . One example of   this lack of consensus in summarization , where dif-   ferent papers use OLLIE ( Christensen et al . , 2014 ) ,   Table 1 : Sample relation tuples and examples of how   different applications use OpenIE extractions .   MinIE ( Ponza et al . , 2018 ) , and Stanford CoreNLP   ( Cao et al . , 2018 ; Zhang et al . , 2021 ) for extraction .   Different applications may also have different re-   quirements . As an example , choosing a model that   assumes all relations only have a subject and ob-   ject may not be suitable for event schema induction   since that excludes any event schemas with more   than two entities . The papers that introduce new   OpenIE models and datasets do not specify how   downstream applications would be impacted by   the different assumptions those models make about   extracted relations .   We find that prior OpenIE surveys are also insuf-   ficient to find the best OpenIE model for a given   application . The only previous application - focused   OpenIE survey we found was Mausam ( 2016 ) .   However , this survey does not identify the desired   properties of OpenIE for those applications or pro-   vide an empirical comparison of OpenIE systems .   Glauber and Claro ( 2018 ) , Claro et al . ( 2019 ) , and   Zhou et al . ( 2022 ) also do not provide an empirical   application - focused survey .   Another obstacle is the lack of apples - to - apples   comparisons between OpenIE models . Compar-   isons should keep the training set , benchmark ,   and evaluation metric constant to eliminate con-   founders . Unfortunately , the papers that intro-929   duce new OpenIE models often do not provide   this apples - to - apples comparison . For example ,   CopyAttention ( Cui et al . , 2018 ) , SpanOIE ( Zhan   and Zhao , 2020 ) , IMoJIE ( Kolluru et al . , 2020b ) ,   and OpenIE6 ( Kolluru et al . , 2020a ) all compare   their model to models trained on different training   sets . OpenIE6 reports performance on the WiRe57   benchmark which MultiOIE ( Ro et al . , 2020 ) does   not , but MultiOIE reports performance on the   ReOIE2016 benchmark which OpenIE6 does not .   Because the training set can greatly affect the per-   formance of a neural model , we focus on selecting   both the appropriate OpenIE model and training   set , which we refer to as an OpenIE System .   To resolve our lack of understanding , we focus   on one particular question : How do I choose a   particular OpenIE system for a given application ?   Different implicit assumptions about OpenIE may   have a significant impact on the performance of   downstream applications such as the assumptions   that all relations are verb - based ( Zhan and Zhao ,   2020 ) or that all relations have only a subject and   object ( Kolluru et al . , 2020b ) . To answer this ques-   tion an apples - to - apples comparison must be con-   ducted for different application settings .   Because it is impractical to find the best model   for every application given the many possible appli-   cations of OpenIE , we instead characterize appli-   cations based on what properties they desire from   OpenIE such as the desire for N - ary relation ex-   traction by event schema induction . We provide   an extensive apples - to - apples comparison of neural   OpenIE models such that a practitioner can uti-   lize our practical observations to effectively select   a neural OpenIE model and training set for their   downstream application . Finally , we apply our rec-   ommendations to a downstream Complex QA task .   In summary , our contributions are as follows:•We propose a taxonomy that covers OpenIE   training sets , benchmarks , and neural models .   •We present an extensive empirical comparison   of different models on different datasets with   recommendations based on the results .   •We perform a case study on Complex QA to   show the efficacy of our recommendations .   To the best of our knowledge , our survey is the   only application - focused empirical survey on Ope-   nIE datasets , metrics , and neural OpenIE models .   2 Motivating Applications   In this section , we identify the properties of OpenIE   desired by 5 downstream applications : Slot Filling ,   Question Answering ( QA ) , Summarization , Event   Schema Induction , and Knowledge Base Popula-   tion . We survey how OpenIE is used and the prop-   erties explicitly desired by papers corresponding to   the application , either as motivation for choosing   a given OpenIE model or within a case study as a   property that would improve performance .   The desired properties we observe are Higher   Recall , Higher Precision , N - ary Relation Extrac-   tion , Inferred Relation Extraction , and Fast Ex-   traction . We define an " Inferred Relation " ( IN )   to be a relation that contains words that are not   in the original sentence . For example , given the   sentence " Bill Gates , former CEO of Microsoft , is   a Harvard dropout " , the relation ( Bill Gates , was ,   former CEO of Microsoft ) can be inferred even   though " was " is not in the original sentence . We   define an " N - ary Relation " ( N - ary ) to be a relation   with more arguments than just ( subject , predicate ,   object ) . For example , the relation ( Alice , went , to   the store , today ) has an additional argument today .   Table 2 provides a summary the explicitly desired   properties of downstream applications.930   Slot Filling Slot Filling is a task where an incom-   plete tuple must be completed using information   from a given corpus ( Chen et al . , 2019 ) . For exam-   ple , the incomplete tuple ( Obama , born in , ? ) must   be completed as ( Obama , was born in , Honolulu )   using information from the corpus . OpenIE can be   used to extract complete tuples which fill slots in   an incomplete tuple using entity linking . Soderland   et al . ( 2013 ) , Angeli et al . ( 2015 ) , Soderland et al .   ( 2015b ) , and Soderland et al . ( 2015a ) take advan-   tage of how correct relations often appear multiple   times to match empty slots to the highest precision   OpenIE tuple . They state in their case studies they   would benefit from INextraction and Soderland   et al . ( 2015b ) and Soderland et al . ( 2015a ) state   they would benefit from N - ary extraction . These   two properties allow more relation surface forms   to be extracted , which increases the chance an in-   complete tuple can be linked to a complete tuple .   Question Answering We focus on two subtasks   of Question Answering ( QA ) that utilize OpenIE :   Open - domain QA ( OpenQA ) and Complex QA .   OpenQA involves answering questions given a   large database ( Fader et al . , 2014a ) . Complex   QA involves using information from multiple sen-   tences to find answers and requires inferring re-   lationships between multiple entities ( Chali et al . ,   2009 ) . Fader et al . ( 2013 , 2014b ) , Yin et al . ( 2015 ) ,   and Clark et al . ( 2018 ) are OpenQA methods that   use retrieval - based methods to match OpenIE ex-   tractions to questions . By rewriting queries into   incomplete tuples , such as rewriting " Where was   Obama born ? " into ( Obama , born in , ? ) , it is possi-   ble to use extracted relations to answer queries by   filling in the missing slots in the query . For Com-   plexQA , Khot et al . ( 2017 ) and Lu et al . ( 2019 )   generate graphs from extracted relation tuples , then   reason over these graphs to answer questions . Inall QA applications surveyed , high recall ( HR ) is   desired , with Lu et al . ( 2019 ) using a custom Ope-   nIE method specifically for higher recall . Yin et al .   ( 2015 ) ’s case studies state that N - ary would be ben-   eficial while Lu et al . ( 2019 ) uses a custom OpenIE   method that supports IN .   Summarization OpenIE addresses the problems of   redundancy and fact fabrication in summarization .   Redundancy is when a fact is repeated multiple   times in the summary . To combat redundancy , Ope-   nIE is used to ensure that the generated summary   does not have repeated relations ( Christensen et al . ,   2014 ; Zhang et al . , 2021 ) . Fact fabrication is when   a fact that is not supported by the text being summa-   rized is in the summary . To combat fact fabrication ,   OpenIE is used to ensure that the generated sum-   mary only contains relations from the original text   ( Cao et al . , 2018 ; Zhang et al . , 2021 ) . In summa-   rization tasks , HRis useful to ensure summaries   contain all information , with Ponza et al . ( 2018 )   citing greater diversity of extractions as a way to   improve performance . high precision ( HP ) is also   desired by Zhang et al . ( 2021 ) in order to reduce   redundant extractions .   Event Schema Induction Event Schema Induction   is the automatic discovery of patterns that indicate   events , agents , and the agents ’ roles within that   event . Extracted relations can be used to find sur-   face forms of events , with redundant tuples being   used to induce event schemas . The open nature of   OpenIE allows for events to be found regardless   of the domain or surface form . HRis useful for   Event Schema Induction for the same reason it is   useful for Slot Filling : finding more surface forms   allows for more event schemas to be induced ( Bala-   subramanian et al . , 2013 ; Romadhony et al . , 2019 ;   Sahnoun et al . , 2020 ) . Sahnoun et al . ( 2020 ) also   specifically desire INso that more event schemas931can be learned , while Balasubramanian et al . ( 2013 )   state that N - ary would improve performance .   Knowledge Base Population The relations ex-   tracted by OpenIE can be used to automatically   populate knowledge bases ( KBs ) , creating new   nodes and edges . Muhammad et al . ( 2020 ) and   Kroll et al . ( 2021 ) use learning - based OpenIE mod-   els because of their ability to generalize to unseen   relations and achieve HR . Kroll et al . ( 2021 ) also   explicitly chooses Stanford CoreNLP and OpenIE6   for their fast extraction times ( FE ) .   3 OpenIE Datasets   In this section , we discuss the differences between   different OpenIE training sets and benchmarks and   their shortcomings . We provide statistics about   different datasets in Table 3 .   3.1 Training Datasets   Given how data - hungry deep learning models are   and how costly it is to manually label OpenIE   datasets , most OpenIE training sets are weakly la-   beled using high confidence extractions from prior   OpenIE models .   CopyAttention ( Cui et al . , 2018 ) , SpanOIE ( Zhan   and Zhao , 2020 ) , and OIE4 ( Kolluru et al . , 2020b )   are training sets consisting of high confidence Ope-   nIE4 extractions from Wikipedia .   SpanOIE includes extractions of all confidences   unlike CopyAttention and OIE4 which only contain   extractions above a certain confidence threshold .   The IMoJIE dataset ( Kolluru et al . , 2020b ) at-   tempts to get higher quality labels by combining   Wikipedia extractions from OpenIE4 , ClausIE , and   RNNOIE , using a common scoring metric to com-   bine extractions and filter out repeated extractions .   The LSOIE training set ( Solawetz and Larson ,   2021 ) is composed of automatically converted Se-   mantic Role Labeling ( SRL ) extractions with high   inter - annotator agreement from the Wikipedia and   Science domain of the crowdsourced QA - SRL   Bank 2.0 dataset . Because this dataset is derived   from SRL , all relations are assumed to be verb-   based and none are inferred .   Issues with existing training sets   Current OpenIE training sets are limited to   Wikipedia and Science domains , which may not   generalize to certain other domains . Additionally ,   all OpenIE training sets are weakly labeled , lead-   ing to noisy labels which may limit the capabilities   of neural OpenIE models . For example , there areinstances in LSOIE where the gold relation does   not contain a negation it should , resulting in a com-   pletely different semantic meaning . It is an open   question of how much noise exists within these   training sets .   3.2 Benchmarks   OIE2016 ( Stanovsky and Dagan , 2016 ) is a bench-   mark for OpenIE automatically derived from the   crowdsourced QA - SRL dataset annotated on Prop-   Bank and Wikipedia sentences .   WiRe57 ( Léchelle et al . , 2018 ) consists of expert   annotations for 57 sentences .   CaRB ( Bhardwaj et al . , 2019 ) uses crowdsourcing   to re - annotate the sentences in the OIE2016 bench-   mark .   ReOIE2016 ( Zhan and Zhao , 2020 ) uses manual   annotation to re - annotate OIE2016 to attempt to   resolve problems arising from incorrect extraction .   LSOIE ( Solawetz and Larson , 2021 ) has bench-   marks derived using the same sources and rules as   the training sets .   BenchIE ( Gashteovski et al . , 2021 ) is derived from   CaRB and is based on the idea that extracted rela-   tions need to exactly match at least one relation out   of a " fact set " of semantically equivalent manually   annotated gold standard relations .   Are existing benchmarks sufficient ?   Given how the OIE2016 benchmark has been re-   annotated three times , there is no real consensus   on how to annotate OpenIE . For example , CaRB   labels prepositions as part of the object and not the   predicate , but OIE2016 and ReOIE2016 do not . As   a result , it is very difficult for a single model to do   well on all benchmarks because each one makes   different assumptions . Although there are common   principles that guide OpenIE labeling , namely As-   sertedness , Minimal Propositions / Atomicity , and   Completeness and Open Lexicon ( Stanovsky and   Dagan , 2016 ; Léchelle et al . , 2018 ; Bhardwaj et al . ,   2019 ) , these principles are vague enough to be in-   terpreted in different ways .   4 Evaluation Metrics   In this section , we describe the different evalua-   tion metrics used to evaluate OpenIE models and   discuss their shortcomings .   OIE2016 introduces lexical matching , which treats   evaluation as a binary classification task . A pre-   dicted relation is matched to a gold standard rela-   tion if the heads of the predicate and all arguments932   are the same .   WiRe57 and CaRB use word - level matching ,   which calculate recall and precision based on the   proportion of matching tokens in the predicted and   gold standard relations . WiRe57 gives a greater   penalty to recall than CaRB if there are fewer pre-   dicted relations than gold standard relations .   BenchIE uses sentence - level matching , which re-   quires an exact match of the predicate and argu-   ments to a relation in the fact set . Because of   BenchIE ’s reliance on fact sets which other bench-   marks lack , the BenchIE metric is only compatible   with BenchIE and no other metrics can be used   with the BenchIE dataset . As a result , an apples-   to - apples comparison of the BenchIE dataset and   metric with other datasets and metrics is not possi-   ble , so we do not report performance on BenchIE .   Is AUC a useful metric ?   When comparing OpenIE systems , we place a   greater emphasis on F1 score than AUC . The   original implementations of CaRB , OIE2016 , and   WiRe57 use the trapezoidal rule to calculate AUC   which leads to inflated AUC scores for certain sys-   tems without low recall points . As a result , we   consider the highest F1 score on the PR curve to   be a better metric than AUC .   Are existing metrics sufficient ?   All existing OpenIE metrics are lexical metrics , and   lexical metrics are merely a proxy for comparing   the semantic meanings of the predicted relations   with the gold standard relations . For instance , ex-   isting OpenIE metrics only give small penalties for   omitting negations from predicted relations , even   though this changes the semantic meaning . This   issue can be also observed in lexical metrics used   for summarization ( Saadany and Orasan , 2021 ) .   5 Neural OpenIE Models   In this section , we describe neural OpenIE models   and the properties and assumptions they make thatset them apart . Neural OpenIE models can be cate-   gorized based on how they formulate the OpenIE   problem : as a text generation or labeling problem .   We provide overviews of the models in Table 4 .   5.1 Generative Problem Formulation   Generative OpenIE models cast OpenIE as a   sequence - to - sequence problem , taking the sentence   as input and attempting to generate all relations in   the sentence as output . The generative models we   survey rely on a copy mechanism to copy vocabu-   lary from the original sentence , meaning they can   not extract INrelations .   CopyAttention ( Cui et al . , 2018 ) generates ex-   tractions using GloVe embeddings and a 3 - layer   stacked Long Short - Term Memory ( LSTM ) as the   encoder and decoder .   IMoJIE ( Kolluru et al . , 2020b ) builds upon Copy-   Attention by using BERT embeddings and intro-   ducing iterative extraction to combat repeated ex-   tractions . Iterative extraction is repeated extraction   from the same sentence with previously extracted   relations appended to the end so the model can   identify what relations have previously been ex-   tracted .   5.2 Labeling Problem Formulation   Labeling OpenIE models cast OpenIE as a se-   quence labeling problem , usually using a BIO tag-   ging scheme to label tokens in the sentence . They   can be subdivided into Piecewise and Holistic La-   beling models .   5.2.1 Piecewise Labeling   Piecewise labeling models first label predicates and   then label arguments for each extracted predicate   to extract relation tuples .   RnnOIE ( Stanovsky et al . , 2018 ) is a bi - directional   LSTM ( BiLSTM ) transducer inspired by SRL that   uses BIO tags .   SpanOIE ( Zhan and Zhao , 2020 ) is also based on   SRL , using a BiLSTM to perform span classifica-   tion instead of BIO tagging . In span classification ,   spans of tokens of varying length are classified as   parts of the relation instead of individual tokens .   Span classification allows for the use of span fea-   tures , which can be richer than word - level features .   MultiOIE ’s ( Ro et al . , 2020 ) novelty is multi-   head attention and BERT embeddings . After la-   beling the predicates , multi - head attention is used   between the predicate and the rest of the sentence   to label the arguments.933MILIE ( Kotnis et al . , 2021 ) introduces iterative   prediction , the process of extracting one argument   of the relation tuple at a time , for multilingual Ope-   nIE . Extraction can be performed predicate , subject ,   or object first , in case other languages benefit from   different extraction orders .   Uniquely , piecewise labeling models label all   predicates in a sentence simultaneously and assume   that for each predicate , there is only one set of   arguments . This means that they can not extract   multiple relations that share the same predicate ,   unlike generative and holistic labeling models .   5.2.2 Holistic Labeling   Holistic labeling models label predicates and argu-   ments simultaneously .   OpenIE6 ( Kolluru et al . , 2020a ) introduces grid   labeling , constraint rules , and conjunction rules .   Grid labeling is the simultaneous extraction of mul-   tiple relations from a sentence . Constraint rules   penalize certain things like repeated extractions or   not extracting a relation for a head verb . Conjunc-   tion rules split relations containing conjunctions   into two separate relations . IGL - OIE is the first   stage , using only grid labeling ; CIGL - OIE is the   second stage , adding in constraint rules ; OpenIE6   is the final stage , adding conjunction rules .   DetIE ( Vasilkovsky et al . , 2022 ) uses ideas from   single - shot object detection to make predictions   more quickly than previous methods . Labeling   models generally can not label tokens that are not   in the original sentence , meaning they can not ex-   tract INrelations . However , the more recent mod-   els IGL - OIE , CIGL - OIE , OpenIE6 , and DetIE ex-   plicitly add " be " , " of " , and " from " to the end of   sentences to allow for the extraction of inferred   relations with those predicates .   5.3 Model Hyperparameters   The sensitivity to hyperparameters of the models   we survey is unclear . Of the works we survey ,   MultiOIE and OpenIE6 describe how they per-   form hyperparameter tuning and provide the hy-   perparameters they tested . SpanOIE , IMoJIE , and   DetIE do not provide details of how they obtained   the hyperparameters they use . None of these works   provide an in - depth analysis of how the perfor-   mance was affected by different hyperparameter   values . As a result , we perform our own sensitiv-   ity analysis using MultiOIE . The results of this   analysis can be found in Appendix B.   In our own experiments , we observed only minorincreases in performance from changing the hyper-   parameters in a few cases . On average , the per-   formance changes were negligible . When making   recommendations , we consider the performance   over many different combinations of model , train-   ing , and test set . Minor differences in a handful of   cases do not impact our overall conclusions . As   a result , we use the default hyperparameters used   by Ro et al . ( 2020 ) for MultiOIE . Because other   models did not report any particular sensitivity to   hyperparameters , we generalize this result to all   models we use and use the final set of hyperparam-   eters those authors use .   5.4 Existing Model Limitations   Models are often developed with specific datasets   in mind . Some papers introducing new models also   introduce new training sets such as CopyAttention   ( Cui et al . , 2018 ) , SpanOIE ( Zhan and Zhao , 2020 ) ,   and IMoJIE ( Kolluru et al . , 2020b ) which may influ-   ence model assumptions . SpanOIE also introduces   its own manually annotated benchmark , which may   have informed the assumptions SpanOIE makes .   The lack of consensus on how to label OpenIE   makes it difficult to perform apples - to - apples com-   parisons because certain models can not extract   some relations due to the assumptions they make .   OpenIE has also largely been limited to English .   MILIE makes assumptions that allow for differ-   ent extraction methods depending on the language ,   but other OpenIE models that support multilingual   extraction largely treat extraction from other lan-   guages the same as extraction from English . Multi-   lingual OpenIE remains an open field of study .   6 Experiments   In this section , we describe how we compare Ope-   nIE models and datasets for the sake of recom-   mendation . To find the best system for different   applications , we test whether the properties of Ope-   nIE models and training sets have a statistically   significant effect on accuracy in test sets with corre-   sponding properties . We are also interested in how   the choice of model affects efficiency in order to   satisfy the fast extraction property ( FE ) . We answer   the following questions :   R1 : How does whether a model supports N - ary   relation ( N - ary ) extraction and whether the training   set contains N - ary affect the F1 score of a model   on test sets with or without N - ary ?   R2 : How does whether a model supports inferred934relation ( IN ) extraction and whether the training   set contains INaffect the F1 score of a model on   test sets with or without IN ?   R3 : How does the model type affect efficiency as   measured by the number of sentences processed   per second ( Sen./Sec ) ?   6.1 Experimental Setup   Models : We compare SpanOIE , IMoJIE ,   MultiOIE , the 3 stages of OpenIE6 : IGL - OIE ,   CIGL - OIE , and OpenIE6 , and DetIE . For each   model , we train them with their paper ’s original   dev set and their original hyperparameters . We run   all experiments on a Quadro RTX 5000 GPU .   Training Datasets : We train models on the   SpanOIE , OIE4 , IMoJIE , and LSOIE training sets .   We combine the Science and Wikipedia domain   for both the training and benchmark of LSOIE , en-   suring there are no duplicate sentences from over-   lapping sentences in the domains . Due to the in-   put structure of SpanOIE and MultiOIE , they can   not be trained on training datasets with inferred   relations . Subsequently , we remove any inferred   relations from the training sets of those models .   Similarly , as IMoJIE , OpenIE6 , and DetIE can   not extract N - ary relations , we convert all N - ary   relations in the training set into binary relations   by moving additional arguments into the object .   For instance , the relation ( Alice , went , to the store ,   today ) is converted into ( Alice , went , to the store   today ) . Inferred and N - ary relations were not re-   moved from the gold standards of the test sets .   Benchmarks : We evaluate all the models on the   publicly available English benchmarks OIE2016 ,   WiRe57 , ReOIE2016 , CaRB , and LSOIE .   Evaluation Metrics : We use OIE2016 ’s ,   WiRe57 ’s , and CaRB ’s metrics for evaluation . We   perform student ’s t - test between OpenIE system ,   test set , and evaluation metric configurations to an-   swer R1,R2 , and R3 . For R1andR2the t - scores   are computed using the per - sentence F1 scores of   each method . For R3the t - scores are computed us-   ing the mean sentences per second for each training   set and test set combination for a given model .   7 Results   In this section , we perform an apples - to - apples   comparison among different OpenIE systems to   determine the SoTA OpenIE model and the best   general - purpose OpenIE training dataset .   Best OpenIE Model We compare the differentmodels on different evaluation metrics averaged   across different training and test sets in Table   5 . We observe that across all evaluation metrics   MultiOIE and CIGL - OIE have the highest or sec-   ond highest F1 score . We also observe that IGL-   OIE and CIGL - OIE are the most efficient models .   Best OpenIE Training Set Because performance   on a test set is also greatly dependent on the train-   ing set depending on the domain and generation   methods , we determine the best training set for   each test set . In Table 6 , we compare different   training and test set combinations with different   evaluation metrics averaged across models . We   observe that the models trained on LSOIE perform   best on the OIE2016 and LSOIE test sets . This is   because the LSOIE training set and the OIE2016   and LSOIE test sets are derived from different ver-   sions of QA - SRL and generated using the same   rules . On the WiRe57 , ReOIE2016 , and CaRB   test sets , we observe that the models trained on the   OIE4 and SpanOIE training sets generally perform   the best . It is likely because the OIE4 and SpanOIE   training sets contain both N - ary andINrelations   like the WiRe57 , ReOIE2016 , and CaRB test sets   while LSOIE and IMoJIE do n’t .   Of the two models with the highest aver-   age CaRB F1 scores , MultiOIE and CIGL - OIE ,   MultiOIE has higher average precision while   CIGL - OIE has higher average recall . CIGL - OIE   tends to extract longer objects than MultiOIE as   seen in Table 7 , which may explain this difference .   Overall , OpenIE models have the poorest perfor-   mance when extracting the object , which may be   due to the variance in object length from additional   arguments compared to the subject and predicate .   7.1 Research Questions   To answer our research questions , we perform stu-   dent ’s t - test using the CaRB F1 scores of the high-   est scoring model , training set , and test set combi-   nations for each setting . We perform comparisons   of OpenIE systems , where one aspect ( model or   training set ) is changed and the other aspects are   kept constant . Then , we choose the test set and   evaluation metric for the two settings that results in   the highest t - score between methods .   ForR1 , we conclude ( 1 ) regardless of training   set , the best N - ary models perform better than the   best non- N - ary models ; ( 2 ) regardless of the model ,   training on the best N - ary training sets results in   higher performance than training on the best non-935   N - ary training sets . Therefore if an application   benefits from N - ary , then the best OpenIE sys-   tem should include either a N - ary model , N - ary   training set , or both , with both being preferred .   ForR2 , we conclude that ( 1 ) INmodels are   better than non- INmodels when there is either a   INtraining and INtest set , or a non- INtraining and   non - INtest set ; ( 2 ) INtraining sets are better than   non - INtraining sets when there is an INmodel and   INtest set . Therefore if an application benefits   from IN , then the chosen training set and model   should either both be INor both be non- IN .   ForR3 , we compare the efficiency of the sole   generative model , IMoJIE , to the efficiency of ev-   ery other model . We observe that every other model   is faster than IMoJIE and the difference is sta-   tistically significant . This matches expectations ,   since it has been previously shown that IMoJIE is   slower than other OpenIE models ( Kolluru et al . ,   2020a).Therefore if an application is concerned   about efficiency , then the chosen OpenIE model   should not be a generative model .   8 A Case Study : Complex QA   To verify our recommendations , we perform a case   study using QUEST ( Lu et al . , 2019 ) , a Complex   QA method that uses OpenIE to extract entities   and predicates from the question and from docu-   ments to generate knowledge graphs . The nodes   are entities derived from subjects and objects , while   the edges are predicates . The knowledge graph is   matched to the entities in the question and traversed   to find potential answers . Because more extractions   result in a larger knowledge graph , QUEST ben-   efits from HR which the authors use their own   rule - based OpenIE method to achieve .   8.1 Experimental Setup   To test our recommendations , we replace the Ope-   nIE method used by the authors with MultiOIE   trained on SpanOIE , CIGL - OIE trained on OIE4 ,   and OpenIE6 trained on OIE4 . We chose these   models and training sets because they have the   highest overall CaRB recall and F1 scores .   One caveat is that in order for QUEST to connect   entities from multiple sentences , they must have   the same surface form . Because OpenIE methods   often extract long subjects and objects that include   adjectives and modifiers , if the subject or object of   an extraction contains entities extracted by QUEST,936   we add additional relations using those entities .   For example , in the sentence " Hector Elizondo   was nominated for a Golden Globe for his role   in Pretty Woman , " QUEST may extract the entities   " Hector Elizondo , " " Golden Globe , " and " Pretty   Woman . " If an OpenIE method were to extract the   triple ( " Hector Elizondo " , " was nominated " , " for   a Golden Globe for his role in Pretty Woman " ) ,   we would add the additional extractions ( " Hec-   tor Elizondo " , " was nominated " , " Golden Globe " )   and("Hector Elizondo " , " was nominated " , " Pretty   Woman " ) . QUEST also replaces pronouns with the   entities they refer to because nodes in the knowl-   edge graph can not be made using pronouns . We   replace pronouns using the same method QUEST   does before running any OpenIE method .   We run QUEST using the CQ - W question set   and search for answers in the Top-10 Google docu-   ment set used in their paper . Because CIGL - OIE   has the highest CaRB recall and OpenIE6 has the   highest WiRe57 recall , we expect that using either   of them will result in higher downstream perfor-   mance than using MultiOIE .   8.2 Evaluation   We compare the Mean Reciprocal Rank ( MRR ) ,   Precision@1 ( P@1 ) , and Hit@5 for each OpenIE   model . The results of our case study are summa-   rized in Table 8 . We observe higher performance   of CIGL - OIE and OpenIE6 than MultiOIE on   QUEST , which matches our expectations based   on the higher recall of CIGL - OIE and OpenIE6   and the desired property of HRbut not HPfor QA .   Our case study demonstrates the applicability of   our empirical study to the use of OpenIE methods   in downstream applications .   An important note is that oftentimes a great deal   of pre- and post - processing is necessary to adapt   OpenIE for different downstream applications . Re-   moving pronouns and adding additional entity-   based extractions was necessary to achieve reason-   able performance in QUEST . Even after modifyingMultiOIE , CIGL - OIE , and OpenIE6 in this way ,   their performance is less than the original perfor-   mance of QUEST . As a result , it is important to   not just consider the performance and properties of   OpenIE models , but also how to adapt models to   their specific needs .   9 Challenges and Future Directions   Even with the introduction of neural models , Ope-   nIE systems still have significant room for improve-   ment . In Table 2 we state that canonicalizing ex-   tractions is desired by QA while extracting from   imperative sentences is desired by both QA and   summarization , but no existing model or dataset   addresses these properties . In sections 3.1 and 3.2   we note the lack of consensus on how to label Ope-   nIE and the issues with weak labeling . Existing   metrics also have issues with semantic meaning   as discussed in section 4 , which is exacerbated by   errors caused by weak labeling . The lack of con-   sensus in how to label OpenIE relations results in a   diverse set of models as we discuss in section 5.4 .   The different assumptions these models make are   also largely constrained to English syntax , leaving   future work in multilingual OpenIE open .   10 Conclusion   In this paper , we presented an application - focused   empirical comparison of recent neural OpenIE   models , training sets , and benchmarks . Our experi-   ments showed that the different properties of Ope-   nIE models and datasets affect the performance ,   meaning it is important to choose the appropriate   system for a given application and not just choose   whatever model is state - of - the - art . We hope that   this survey helps users identify the best OpenIE   system for their downstream applications and in-   spires new OpenIE research into addressing the   properties desired by downstream applications .   Limitations   Although this work aims to be as comprehensive as   possible , there are several limitations to this paper .   Our comparisons only consider neural OpenIE   models despite rule - based methods being very pop-   ular among downstream applications . This is be-   cause of the lack of recent surveys on neural Ope-   nIE methods and the difficulties we personally en-   countered when trying to determine which OpenIE   method was state - of - the - art . We acknowledge that   there are many cases where rule - based methods937may be preferable to neural models due to being   faster or more tailor - made for a specific application .   However , we feel that focusing on neural OpenIE   methods is not a detriment because we are inter-   ested in which methods work best " out of the box " .   Based on the results reported in these neural Ope-   nIE papers , we believe they are currently the best   out - of - the - box OpenIE models using the metrics   we report in this paper on the test sets covered in   this paper .   The corpora we chose are all limited to English .   As a result , our results are not generalizable to any   downstream task that relies on different languages .   In our experiments , we do not report results for   the BenchIE test set or using the BenchIE met-   ric . This is because the BenchIE test set uniquely   can only be evaluated using the BenchIE metric ,   and the BenchIE metric can only be applied to the   BenchIE test set . We do not feel that its exclu-   sion hurts our final conclusions about the relative   performance of OpenIE methods .   We perform a case study using Complex QA   only , which we generalize to other applications .   For our case study , we were unable to replicate   the results reported in the original QUEST paper   ( Lu et al . , 2019 ) . We have been in correspondence   with the authors to address this issue , but we still   feel that our results are valid given that we use the   publicly available code and data and adapted it to   use our OpenIE methods to the best of our ability .   Similarly , we report different results to the effi-   ciency and performance of DetIE reported in the   original paper ( Vasilkovsky et al . , 2022 ) . We have   been in contact with the original authors and dif-   ferences in efficiency can be attributed to differing   hardware while differences in performance can be   attributed to different preprocessing of training and   test sets . For instance , the authors of DetIE do not   remove duplicate sentences when combining the   Science and Wiki domains of LSOIE .   We do not make specific observations based on   the different evaluation metrics , mainly focusing   on CaRB and WiRe57 F1 score for our evaluation .   We give our experimental results within appendix   A so that future researchers can make observations   and draw conclusions based on OIE2016 .   Ethics Statement   We did not create any of the models , datasets , or   applications covered in this paper . Any ethical   issues with the preexisting OpenIE datasets we usein this paper will reflect on this work .   References938939A Empirical Results   Model Performance In this section , we report the   empirical results of training each model on a variety   of training sets and evaluating them on a variety of   test sets with different evaluation metrics . Sen./Sec .   refers to the number of sentences that could be   processed per second , which we use to compare the   efficiency of different models . We report Precision   ( P ) , Recall ( R ) , F1 Score ( F1 ) , and Area Under the   Curve ( AUC ) for the OIE2016 , WiRe57 , and CaRB   metrics . We make observations using these results   in Section 7 .   Table 9 shows the performance of different OpenIE   models trained on different training sets on the   OIE2016 benchmark .   Table 10 shows performance on WiRe57 .   Table 11 shows performance on ReOIE2016 .   Table 12 shows performance on CaRB .   Table 13 shows performance on LSOIE .   Research Questions We also report the empirical   results of our student ’s t - tests comparing different   OpenIE systems , which we use to answer the re-   search questions we raise in section 6 . For each   research question , we report the number of statis-   tical significance tests that had a t - score above or   below 0 and had a p - value above or below 0.05 . We   use these results to answer those research questions   in section 7.1 .   Table 14 shows the results of the statistical signifi-   cance tests used to answer R1 from section 6 .   Table 15 shows results for R2 .   Table 16 shows results for R3.940941942943   B Hyperparameter Sensitivity Study   In this section , we report the empirical results of   training Multi 2OIE on a variety of hyperparame-   ters . For each combination of training and test set ,   we start with the original hyperparameters used   by Ro et al . ( 2020 ) , then modify one . The differ-   ent hyperparameter values we test are values the   authors test in their hyperparameter search . The   hyperparameters the authors change are the number   of epochs used for training , the dropout rate for the   multi - head attention blocks , the dropout rate for   the argument classifier , the batch size , the learn-   ing rate , the number of multi - head attention heads ,   the number of multi - head attention blocks , and the   number of dimensions for the position embeddings .   The original hyperparameter values Ro et al . ( 2020 )   use are in table 17 .   Table 18 shows the CaRB score of MultiOIE   trained with different hyperparameters , averaged   over all training and test sets .   Table 19 shows the CaRB score averaged over all   training sets on the OIE2016 test set .   Table 20 shows the CaRB score averaged over all   training sets on the WiRe57 test set .   Table 21 shows the CaRB score averaged over all   training sets on the ReOIE2016 test set .   Table 22 shows the CaRB score averaged over all   training sets on the CaRB test set .   Table 23 shows the CaRB score averaged over all   training sets on the LSOIE test set .   The largest difference in CaRB F1 score from the   original model hyperparameters was for MultiOIE   tested on WiRe57 . However , it should be noted   that WiRe57 only consists of 57 sentences with   343 relations . An incorrect prediction on a single   sentence may lead to a significant F1 difference   overall . Therefore , we feel that this difference is   not due to sensitivity to hyperparameters , but rather   due to the sensitivity of WiRe57 . For other test   sets , we observe much smaller effects of different   hyperparameters on the CaRB score.944945946947ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 10   /squareA2 . Did you discuss any potential risks of your work ?   We do not believe our observations can be used for adversarial attacks or have malicious effects . We   train models that are already publicly available on data that is also already publicly available .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Sections 3 - 7   /squareB1 . Did you cite the creators of artifacts you used ?   Sections 3 - 7 , links to the code and datasets used are in the code and data ﬁles attached to the   submission   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We did not plan to use the artifacts for any commercial applications because we were writing a   survey paper .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   We did not plan to use the artifacts for any commercial applications because we were writing a   survey paper . We were using them purely for research purposes .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   The data we use are relations in sentences . We do not believe these data may lead to a violation of   privacy . The source for the sentences were scientiﬁc articles , news articles , and Wikipedia , which we   believe do not contain offensive content .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 3   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3948C / squareDid you run computational experiments ?   Sections 5 , 7   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   We did not believe the models we used were large enough to warrant this discussion , and we ran all   models on a single GPU .   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 5.1 , experimental setup   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 6 , Appendix B , we did not include error bars but we describe how we obtained our results   and how we averaged them to reach our conclusions .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 5.1 , experimental setup   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.949