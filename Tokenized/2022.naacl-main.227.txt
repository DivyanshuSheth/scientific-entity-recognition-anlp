  Yue Zhang , Zhenghua Li , Zuyi Bao , Jiacheng Li ,   Bo Zhang , Chen Li , Fei Huang , Min ZhangInstitute of Artificial Intelligence , School of Computer Science and Technology ,   Soochow University , China;DAMO Academy , Alibaba Group , China{yzhang21,jcli20}@stu.suda.edu.cn , { zhli13,minzhang}@suda.edu.cn{zuyi.bzy,klayzhang.zb,puji.lc,f.huang}@alibaba-inc.com   Abstract   This paper presents MuCGEC , a multi-   reference multi - source evaluation dataset   for Chinese Grammatical Error Correc-   tion ( CGEC ) , consisting of 7,063 sentences   collected from three Chinese - as - a - Second-   Language ( CSL ) learner sources . Each sen-   tence is corrected by three annotators , and   their corrections are carefully reviewed by a   senior annotator , resulting in 2.3 references per   sentence . We conduct experiments with two   mainstream CGEC models , i.e. , the sequence-   to - sequence model and the sequence - to - edit   model , both enhanced with large pretrained   language models , achieving competitive bench-   mark performance on previous and our datasets .   We also discuss CGEC evaluation methodolo-   gies , including the effect of multiple refer-   ences and using a char - based metric . Our an-   notation guidelines , data , and code are avail-   able at https://github .com / HillZha   ng1999 / MuCGEC .   1 Introduction   Given a potentially noisy input sentence , gram-   matical error correction ( GEC ) aims to detect and   correct all errors and produce a clean sentence .   Recently , GEC has increasingly gained attention   for its vital value in various downstream scenarios   ( Grundkiewicz et al . , 2020 ; Wang et al . , 2021 ) .   To support GEC research , high - quality manu-   ally labeled evaluation data is indispensable . For   English GEC ( EGEC ) , such datasets are abun-   dant ( Yannakoudakis et al . , 2011 ; Dahlmeier et al . ,   2013 ; Ng et al . , 2014 ; Napoles et al . , 2017 ; Bryant   et al . , 2019 ; Napoles et al . , 2019 ; Flachs et al . ,   2020 ) . However , Chinese GEC ( CGEC ) evaluation   datasets are relatively scarce . The two publicly   available CGEC evaluation datasets are NLPCC18   and CGED , contributed by the NLPCC-2018 ( Zhao   Table 1 : A CGEC example with two references .   et al . , 2018 ) and the series of CGED shared tasks   ( Rao et al . , 2018 , 2020 ) , respectively .   Most EGEC evaluation datasets provide mul-   tiple references for each input sentence , such as   CoNLL14 - test ( Ng et al . , 2014 ) and BEA19 - test   ( Bryant et al . , 2019 ) . In contrast , sentences in   existing CGEC evaluation datasets usually have   only one reference ( i.e. , 87 % of the sentences in   NLPCC18 and all in CGED ) . This is probably due   to the different annotation workflows adopted .   As suggested by Bryant and Ng ( 2015 ) , enforc-   ing multi - reference annotation is crucial for both   GEC model evaluation and GEC data annotation ,   because there usually exist more than one accept-   able reference with similar meanings for an incor-   rect sentence , as illustrated by the example in Table   1 . On the one hand , if a GEC model outputs a cor-   rect reference , which is yet different from the one   given in the evaluation data , then the model perfor-   mance will be unfairly underestimated . To mitigate   this issue , a straightforward solution is increasing   the number of references ( Sakaguchi et al . , 2016 ;   Choshen and Abend , 2018 ) . On the other hand ,   imposing a single - reference constraint makes data   annotation problematic . If annotators submit differ-   ent equally acceptable corrections , which is very   common , it will be difficult for the senior annotator   to decide which one is the best .   Besides the lack of multiple references , existing   CGEC datasets collect sentences from a single text   source , which may be insufficient for robust model   evaluation ( Mita et al . , 2019 ) . In addition , we be-3118lieve that it is beneficial for improving data quality   to compile comprehensive annotation guidelines .   To fill these gaps , this paper presents a   multi - reference multi - source evaluation dataset for   CGEC , named MuCGEC . After investigating previ-   ous works on constructing GEC datasets , we com-   pile comprehensive annotation guidelines . Based   on a browser - based online annotation tool , each   sentence is assigned to three annotators for inde-   pendent correction , and one senior annotator for   final review . An annotator may submit multiple   references , and the senior annotator may also sup-   plement new references besides rejecting incorrect   submissions . In this way , we aim to produce as   many references as possible . In summary , this work   makes the following contributions .   ( 1)Our newly constructed MuCGEC consists   of 7,063 sentences from three representative   sources of Chinese - as - a - Second - Language   ( CSL ) learner texts . Each sentence obtains 2.3   references on average . We conduct detailed   analyses on our new dataset to gain more in-   sights .   ( 2)We conduct benchmark experiments using   two mainstream and competitive CGEC mod-   els , i.e. , the sequence - to - edit ( Seq2Edit )   and sequence - to - sequence ( Seq2Seq ) models ,   both enhanced with pretrained language mod-   els ( PLMs ) . We also experiment with an ex-   tremely effective ensemble strategy . More-   over , we investigate the effect of multiple ref-   erences on model evaluation , and propose to   use a char - based evaluation metric , which we   believe is simpler and more suitable than pre-   vious word - based ones for CGEC .   2 Data Annotation   2.1 Multi - Source Data Selection   This work focuses on CSL learner texts . In order   to investigate diverse types of Chinese grammati-   cal errors , we select data from the following three   sources .   ( 1)We re - annotate the NLPCC18 test set ( Zhao   et al . , 2018 ) , which contains 2,000 sentences   from the Peking University ( PKU ) Chinese   Learner Corpus .   ( 2)We select and re - annotate sentences from   CGED-2018 and CGED-2020 test datasets   ( Rao et al . , 2018 , 2020 ) , which come fromthe writing section of the HSK exam ( Hanyu   Shuiping Kaoshi , translated as the Chinese   level exam ) , an official Chinese proficiency   test . After removing sentences marked as   correct from the total 5,006 ones , we obtain   3,137 potentially erroneous sentences for re-   annotation .   ( 3)Lang8is a language learning platform , where   native speakers voluntarily correct texts writ-   ten by second - language learners . The NLPCC-   2018 shared task organizers collect about   717 K Chinese sentences with their corrections   from Lang8 and encourage participants to use   them as the training data . We randomly select   2,000 sentences with 30 to 60 characters for   re - annotation .   In the end , we obtain 7,137 sentences . For sim-   plicity , we discard all original corrections and di-   rectly perform re - annotation from scratch follow-   ing our new annotation guidelines and workflow .   2.2 Annotation Paradigm : Direct Rewriting   There are mainly two types of annotation   paradigms for constructing GEC data , i.e. , error-   coded and direct rewriting . The error - coded   paradigm requires annotators to explicitly mark   the erroneous span in the original sentence , then   choose its error type , and finally make correc-   tions . Ng et al . ( 2013 , 2014 ) adopt the error - coded   paradigm for constructing data for the CoNLL-   2013/2014 EGEC shared tasks . For CGEC , the   original NLPCC18 and CGED datasets both follow   theerror - coded paradigm as well .   As discussed by Sakaguchi et al . ( 2016 ) , the   error - coded paradigm poses two challenges . First ,   it is extremely difficult for different annotators to   agree upon the boundaries of the erroneous spans   and their error types , especially when there are   many categories to consider ( Bryant et al . , 2017 ) .   This inevitably leads to an increase in annotation   effort and a decrease in annotation quality . Sec-   ond , under such a complex annotation paradigm ,   annotators would pay less attention to the fluency   of the resulting reference , sometimes even leading   to unnatural expressions .   Instead , the direct rewriting paradigm asks anno-   tators to directly rewrite the input sentence and pro-   duce a corresponding grammatically correct one ,   without changing the original meaning . In order to3119   evaluate model performance , edits can be extracted   automatically from parallel sentences by additional   tools ( Bryant et al . , 2017 ) .   This annotation paradigm has proven to be ef-   ficient and cost - effective ( Sakaguchi et al . , 2016 ) ,   and has been adopted by several recent GEC data   construction works ( Napoles et al . , 2017 , 2019 ;   Syvokon and Nahorna , 2021 ; NÃ¡plava et al . , 2022 ) .   In this work , we adopt the direct rewriting   paradigm . Besides above - mentioned advantages ,   we believe this paradigm can help improve the di-   versity of references since annotators can correct   errors more freely .   2.3 Annotation Guidelines   After an extensive survey of previous work on   GEC data construction , we compiled 30 pages of   comprehensive guidelines for CGEC annotation .   During the course of the annotation process , we   gradually improved our guidelines according to   feedback from annotators .   To facilitate learning , our guidelines adopt a two-   tier hierarchical error taxonomy , including 5 major   error types and 14 minor types , as shown in Table   2 . Our guidelines describe in detail how to handle   each minor error type and provide typical exam-   ples . We will release our guidelines along with the   dataset , which we hope can benefit future research .   For dealing with word - missing errors , we found   that it was unreasonable to simply insert cer-   tain words when the missing words are context-   dependent , which means the missing words are   related to context beyond the given sentence . Ta-   ble 3 shows a sentence in which a verb is missing .   However , under sentence - level GEC annotation ,   annotators are unable to decide the specific miss-   ing verb . According to our observation , previous   CGEC datasets directly insert specific words likeSourceæçç¸ç¸ç»å¸¸æ ã   My dad usually me .   Previousæçç¸ç¸ç»å¸¸éªæ ã   My dad usually scolds me .   Oursæçç¸ç¸ç»å¸¸[MC]æ   My dad usually [ MC ] me .   â scolds â under such circumstances , which we think   is inaccurate and may cause trouble for GEC model   evaluation , because there are many other acceptable   candidates . To handle this problem , we instead in-   sert a special tag named context - dependent miss-   ing components ( MC ) . We find about 1 % of sen-   tences in MuCGEC contain â [ MC ] â tags . Current   GEC models can not handle â [ MC ] â , since â [ MC ] â   is not included in existing training data and vocab-   ulary . We leave this issue as future work .   2.4 Annotation Workflow and Tool   In order to encourage more diverse and high - quality   references , we assign each sentence to three ran-   dom annotators for independent annotation . Their   submissions are then aggregated and sent to a ran-   dom senior annotator ( reviewer ) for review . An   annotator may submit multiple references for one   sentence if he / she thinks they are all correct ac-   cording to the guidelines . The job of the senior   annotator includes : 1 ) modifying incorrect refer-   ences into correct ones ( sometimes just rejecting   them ) ; 2 ) adding new correct references accord-   ing to the guidelines . After review , the accepted   references are defined as final golden references .   For the sake of self - improvement , we employ   a self - study mechanism that allows annotators to   learn from their mistakes if they submit an incorrect   reference . Concretely , if an annotator submits a   reference that is not included in the final golden   references , he / she has to modify his / her submission   into a correct one . Moreover , the annotator can   also make complaints if he / she insists that his / her   submission is correct . We find that the self - study   and making - complaints mechanisms can trigger   very helpful discussions .   To improve annotation efficiency , we have de-   veloped a browser - based online annotation tool to   support the above workflow and mechanisms . Due3120Dataset # sent # err . sent ( perc . ) chars / sent edits / ref refs / sent   Original NLPCC18 2000 1983 ( 99.2 % ) 29.7 2.0 1.1   MuCGEC ( NLPCC18 ) 1996 ( 4 ) 1904 ( 95.4 % ) 29.7 2.5 2.5   MuCGEC ( CGED ) 3125 ( 12 ) 2988 ( 95.6 % ) 44.8 4.0 2.3   MuCGEC ( Lang8 ) 1942 ( 58 ) 1652 ( 85.1 % ) 37.5 2.8 2.1   MuCGEC 7063 ( 74 ) 6544 ( 92.7 % ) 38.5 3.2 2.3   to the space limitation , we show the visual inter-   faces for annotation and review in Appendix B.   2.5 Annotation Process   We employed 21 undergraduate students who are   native speakers of Chinese and familiar with Chi-   nese grammar as part - time annotators . Annotators   received intensive training before real annotation .   In the beginning , two authors of this paper , who   were also in charge of compiling the guidelines ,   served as senior annotators for review . After one   month , when the annotators were familiar with the   job , we selected 5 outstanding annotators as senior   annotators to join the review .   All participants were asked to annotate for at   least 1 hour every day . The whole annotation pro-   cess lasted for about 3 months .   2.6 Ethical Issues   All annotators and reviewers were paid for their   work . The salary was determined by both submis-   sion numbers and annotation quality . The average   salary of annotators and reviewers is 24 and 35   RMB per hour respectively .   All the data of the three sources are publicly   available . Meanwhile , we have obtained permis-   sion from organizers of the NLPCC-2018 and   CGED shared tasks to release our newly annotated   references in a proper way . We are deeply grateful   to them for their kind support .   3 Analysis of Our Annotated Data   Overall statistics of MuCGEC are shown in Table   4 . We also include the original NLPCC18 dataset   ( Zhao et al . , 2018 ) for comparison . First , from the proportion of erroneous sentences ,   we can see that most of the sentences are consid-   ered to contain grammatical errors in the original   annotation , but a considerable part of them are not   corrected in our annotation . We attribute this to our   strict control of the over - correction phenomenon .   Second , regarding sentence lengths , NLPCC18   has the shortest sentences , whereas CGED sen-   tences are much longer . This may be because candi-   dates on the HSK examination , an official Chinese   proficiency test , tend to use long sentences to show   their ability in Chinese .   Third , each sentence in the re - annotated   NLPCC18 receives 2.5 references on average ,   which is more than twice that in the original   NLPCC18 data . Overall , each sentence obtains 2.3   references . We believe the multi - reference charac-   teristic makes our dataset more reliable for evalua-   tion , which is further discussed in Section 6.3 .   Finally , we compare the number of char - based   edits per reference in different datasets . We de-   scribe how to derive such edits in detail in Section   6.2 . We can see that the number of edit is tightly   correlated with sentence length . The difference in   the average sentence length and number of edits   indicates that the three data sources may have a sys-   tematic discrepancy in quality and difficulty , which   we believe is helpful for evaluating the generaliza-   tion ability of models . Moreover , compared with   NLPCC18 - orig , we annotate 25 % more edits ( 2.0   vs. 2.5 ) in each reference . We believe the major   reason is that the original NLPCC18 data are an-   notated under the minimal edit distance principle   ( Nagata and Sakaguchi , 2016 ) , which requires an-   notators to select a reference with fewer edits when   correcting errors.3121   Sentence distribution with respect to numbers   of references is shown in Figure 1 . Here , we only   consider erroneous sentences . Identical references   from different annotators are counted as one refer-   ence . Overall , most sentences have 2 references ,   accounting for 39.4 % ; 29.1 % of sentences have 3   references ; 21.8 % of sentences have only 1 refer-   ence , most of which are short and easy to correct .   We believe that the average number of references   could be further increased if more annotators were   assigned to each sentence . It is also worth noticing   that annotators tend to submit a single reference ,   despite the fact that our annotation tool allows an-   notators to submit multiple ones . We suspect the   reason may be that it is more economical for anno-   tators to do so . One the one hand , it may be easy to   come up with the most suitable correction , whereas   thinking of alternatives is more time - consuming .   On the other hand , we did not give enough con-   sideration to this issue when designing the salary   computation formula . In the future , we plan to   optimize ( or simplify ) our annotation workflow so   that each annotator is required to give only one ref-   erence which he / she thinks is the best , and assign   each sentence to more annotators if we need more   references .   Human annotation performance . In order to   assess the annotation ability of our annotators and   human performance for CGEC task , we calculate   char - based Fscores by evaluating all annotation   submissions against the final golden references .   We describe how to compute char - based metrics   in detail in Section 6.2 . Each reference submitted   by an annotator is considered as a sample . Overall ,   the average Fis 72.12 , which we believe could   be higher if we discarded data that were annotated   at the early stage of our project when annotators   were less experienced and less familiar with our   guidelines .   Figure 2 shows Fscores of 15 annotators who   annotated the most sentences , in descending order   of the number of annotated sentences . We can see   that human performance varies across different an-   notators . The best annotator achieves an 82.34 F   score , while the annotator who completes the most   tasks only gets a score of 68.32 . This indicates that   we should pay more attention to annotation quality   when calculating salaries and prevent annotators   from focusing too much on annotation speed .   Common mistakes made by annotators . We   randomly select 300 invalid references rejected by   reviewers and try to understand what mistakes are   more frequently made by annotators . We manually   classify all selected references into three mistake   categories . The most frequent mistakes are caused   byincomplete correction and account for 56.7 % of   the invalid references . We found that it was some-   times difficult for annotators to correct all errors   without omissions , possibly due to the complex-   ity or flexibility of Chinese grammar . The second   frequent type of mistakes is erroneous correction ,   which means that the correction of old errors incurs   new errors , with a proportion of 32.3 % . Besides ,   11.0 % of submissions are rejected due to meaning   change , which means that the correction changes   the intended meaning of the original sentence .   4 Benchmark Models   To understand how well cutting - edge GEC mod-   els perform on our data , we adopt two mainstream   GEC approaches , i.e. , Seq2Edit and Seq2Seq . Both   models are enhanced with PLMs . We also at-3122tempt to combine them after observing their com-   plementary power in dealing with different error   types . This section briefly describes these bench-   mark models . Due to the space limitation , please   kindly refer to Appendix C for more model details .   The Seq2Edit model treats GEC as a sequence   labeling task and performs error corrections via   a sequence of token - level edits , including inser-   tion , deletion , and substitution ( Malmi et al . , 2019 ) .   A token corresponds to a word or a subword in   English , and to a character in Chinese . With mi-   nor modifications to accommodate Chinese , we   adopt GECToR ( Omelianchuk et al . , 2020 ) , which   achieves the SOTA performance on EGEC datasets .   Following recent Seq2Edit work like Awasthi   et al . ( 2019 ) and Omelianchuk et al . ( 2020 ) , we   enhance GECToR by using PLMs as its encoder .   After comparing several popular PLMs , we choose   StructBERT ( Wang et al . , 2019)due to its superior   performance after fine - tuning ( see Table 5 ) .   The Seq2Seq model straightforwardly treats   GEC as a monolingual translation task ( Yuan and   Briscoe , 2016 ) . Recent works propose to enhance   Transformer - based ( Vaswani et al . , 2017 ) Seq2Seq   EGEC models with PLMs like T5 ( Rothe et al . ,   2021 ) or BART ( Katsumata and Komachi , 2020 ) .   Unlike BERT ( Devlin et al . , 2019 ) , T5 and BART   are specifically designed for text generation . There-   fore , it is straightforward to continue training them   on GEC data . We follow these works and utilize   the recently proposed Chinese BART from Shao   et al . ( 2021 ) to initialize our Seq2Seq model .   The ensemble model . Several previous works   have proven the effectiveness of model ensemble   for CGEC ( Liang et al . , 2020 ; Hinson et al . , 2020 ) .   In this work , we clearly observe the complementary   power of the above two models in fixing different   error types ( see Table 7 ) , and thus attempt to com-   bine them .   We adopt a simple edit - wise voting mechanism .   The edits are at the char - based span level , and cor-   respond to four error types . Please refer to Section   6.2 for detailed explanation of our char - based evalu-   ation metric . More specifically , we aggregate edits   from the results of each model , and only preserve   edits that appear more than N/2times , where N   is the number of models . In other words , an edit is   kept in the final result only if it is produced by a   majority of models . We experiment with two ensemble settings :   1 ) one Seq2Edit and one Seq2Seq , denoted   by â 1 ÃSeq2Edit+1 ÃSeq2Seq â , and 2 ) three   Seq2Edit and three Seq2Seq , denoted by   â 3ÃSeq2Edit+3 ÃSeq2Seqâ . The three Seq2Edit   models are obtained using different random   seeds for initialization , and the same goes for the   Seq2Seq .   Other settings . To obtain the single - model   performance of both kinds of models , we run   them three times separately with different ran-   dom seeds for initialization and calculate aver-   age metrics . For â 1 ÃSeq2Edit+1 ÃSeq2Seq â , we   random select a pair of single models . For   â 3ÃSeq2Edit+3 ÃSeq2Seq â , we aggregate the re-   sults of all six single models .   5 Experiments on NLPCC18 - Orig Data   In order to show that our benchmark models are   competitive among existing CGEC models , we con-   duct experiments on the original NLPCC18 test set ,   on which most previous CGEC systems are tested .   Training data . For the sake of easy replica-   bility , we limit our training data strictly to public   resources , i.e. , the Lang8data ( Zhao et al . , 2018 )   and the HSKdata ( Zhang , 2009 ) . We filter dupli-   cate sentences that appear in our dataset , and dis-   card correct sentences . The final Lang8 and HSK   data contains 1,092,285 and 95,320 sentence pairs ,   respectively . The HSK data is cleaner and of higher   quality than Lang8 , but is smaller . Following the   re - weighting procedure of Junczys - Dowmunt et al .   ( 2018 ) , we duplicate the HSK data five times , and   merge them with Lang8 data .   Comparison with previous works . Table 5   shows the results . For a fair comparison , we follow   the official setting of the shared task , including the   word - based MaxMatch scorer ( Dahlmeier and Ng,3123   2012 ) for calculating the P / R / F values . We segment   model outputs by adopting the PKUNLP word seg-   mentation ( WS ) tool provided by the shared task   organizers ( Zhao et al . , 2018 ) .   When only using Lang8 for training , our single   Seq2Seq model is already quite competitive . Its   performance is only lower than MaskGEC ( Zhao   and Wang , 2020 ) by 1 point in F. Please no-   tice that MaskGEC extra uses data augmentation .   For now , our benchmark models do not use any   synthetic data for simplicity , but we believe data   augmentation could further boost the performance   of our models .   Adding the HSK training data improves perfor-   mance of all our models by about 4 points . Our   two benchmark models already achieve SOTA per-   formance under the single - model setting .   The model ensemble technique leads to obvious   performance gains ( more than 5 points ) over single   models . However , the gains from increasing the   number of component models seem rather small .   We try to explain this issue in Section 6.3 .   For Seq2Edit , we additionally present results   with other PLMs besides StructBERT , including   BERT ( Devlin et al . , 2019 ) , RoBERTa ( Liu et al . ,   2019 ) , and MacBERT ( Cui et al . , 2020 ) from theHugging Facewebsite . We use the â large â vari-   ants of all PLMs .   6 Experiments on MuCGEC   6.1 Data Splits   For hyperparameter tuning or model selection , pre-   vious works on other CGEC datasets often ran-   domly sample some sentence pairs from training   data as the dev set ( Wang et al . , 2020 ; Zhao and   Wang , 2020 ; Hinson et al . , 2020 ) , which is incon-   venient for reproducing or comparing .   In this work , we propose to provide a fixed dev   set for our newly annotated dataset , by randomly   selecting 1,125 sentences from the CGED source ,   denoted as CGED - dev . The remaining 5,938 sen-   tences are used as the test set , in which each data   source has a roughly equal amount of sentences ,   i.e. , 1,996 sentences for NLPCC18 - test , 2,000 for   CGED - test , and 1,942 for Lang8 - test .   6.2 Evaluation Metrics   Problems with word - based metrics . As discussed   in Section 5 , previous CGEC datasets are annotated   upon word sequences and adopt word - based met-   rics for evaluation . Before annotation and evalua-   tion , a sentence needs to be segmented into words   using a Chinese word segmentation ( CWS ) model .   We believe this will introduce unnecessary uncer-   tainty in CGEC evaluation procedure . First , CWS   models inevitably produce word segmentation er-   rors ( Fu et al . , 2020 ) . Second , there are multiple   heterogeneous CWS standards . Finally , we found   that a correct edit may be judged as wrong due to   the word boundary mismatch .   Char - based span - level evaluation metrics are   adopted in this work instead . First , given an input   sentence and a correction , we obtain an optimal   sequence of char - level edits with the minimal edit   distance . We consider three types of char - level   edits , corresponding to three error types :   â¢ Deleting a char for a redundant error ;   â¢ Inserting a char for a missing error ;   â¢Substituting a char with another one for a sub-   stitution error ;   Then , we convert all char - level edits intospan-   level by merging consecutive edits of the same type ,   following previous practice in EGEC and CGEC   ( Felice et al . , 2016 ; Hinson et al . , 2020)3124   The above two steps are applied to both the sys-   tem output sequence and golden reference , trans-   forming them into sets of span - level edits . Finally ,   we can calculate the P / R / F value by comparing   them . If there are multiple golden references , we   will choose the one with the highest F - score .   Span - level word - order errors . When calculat-   ing overall metrics , we only consider above three   types of errors . When analyzing , we distinguish the   fourth error type â word - order . A span - level word-   order error is usually composed of a redundant and   a missing error , where the deleted span is the same   as the inserted one . We use simple heuristic rules   to identify such errors ( Hinson et al . , 2020 ) .   Please kindly notice that we release our evalua-   tion script as well .   6.3 Results and Analysis   Main results . Table 6 shows the char - based per-   formance of the benchmark models and our an-   notators on MuCGEC . All models are trained on   Lang8+HSK , as described in Section 4 .   The overall trend of performance change is   basically consistent with those on the original   NLPCC18 dataset in Table 5 . First , the Seq2Seq   and Seq2Edit models perform quite closely on F ,   but clearly exhibit divergent strength in precision   and recall , giving a strong motivation for combin-   ing them . Second , the model ensemble approach   improves performance by a very large margin .   One interesting observation is that on MuCGEC ,   â 3ÃSeq2Edit+3 ÃSeq2Seq â substantially outper-   forms â 1 ÃSeq2Edit+1 ÃSeq2Seq â on All - test and   all three subsets . In contrast , the improvement is   only modest on the original NLPCC18 test data .   We suspect this may indicate that a multi - reference   dataset can more accurately evaluate model per-   formance . However , it may require further human   investigation for more insights .   Finally , there is still a huge performance gap   between models and humans , indicating that CGEC   research still has a long way to go .   Performance on four error types . Table 7   shows more fine - grained evaluation results on four   error types .   It is clear that the Seq2Edit model is better at   handling redundant errors , whereas the Seq2Seq   model is superior in dealing with substitution and   word - order errors . For missing errors , the two per-   form similarly well .   These phenomena are quite interesting and can   be understood after considering the underlying   model architectures . On the one hand , to correct   redundant errors , the Seq2Edit model only needs   to perform a fixed deletion operation , which is a   much more implicit choice for the Seq2Seq model ,   since its goal is to rewrite the whole sentence . On   the other hand , the Seq2Seq is suitable to substitute   or reorder words due to its natural capability of uti-3125lizing language model information , especially with   the enhancement of BART ( Lewis et al . , 2020 ) .   Again , the model ensemble approach substan-   tially improves performance on all error types . The   ensemble model is closest to the human on redun-   dant errors , probably because they are the easiest   to correct . The largest gap occurs in word - order   errors , which require global structure knowledge   to correct and are extremely challenging .   Influence of the number of references . To un-   derstand the impact of the number of references on   performance evaluation , we deliberately reduce the   available number of reference in our dataset . For   example , when the maximum number of references   is limited to 2 , we only kept the first two references   in the dataset if a sentence has more than 2 golden   references . The results are shown in Figure 3 .   When the maximum number of references in-   creases , the performance of both models and hu-   mans increases continuously , especially for hu-   mans . As only a few sentences have more than   3 references , the improvement is quite small when   the maximum number of references increases from   3 to All . This trend suggests that compared with   single - reference datasets , a multi - reference dataset   reduces the risk of underestimating performance ,   and thus is more reliable for model evaluation .   7 Related Works   EGEC resources . There has been a lot of work on   EGEC data construction . As the two earliest EGEC   datasets , FCE ( Yannakoudakis et al . , 2011 ) and NU-   CLE ( Dahlmeier et al . , 2013 ) adopt the error - coded   annotation paradigm . In contrast , JFLEG ( Napoles   et al . , 2017 ) collects sentences from TOFEL exams   and adopts the direct rewriting paradigm . W&I   ( Bryant et al . , 2019 ) also chooses the direct rewrit-   ingparadigm , and extra annotates a score indicat-   ing the language proficiency level of the writer for   each input sentence . All four datasets are com-   posed of essays from non - native English speakers   and provide multiple references .   Recently , researchers have started to annotate   small - scale EGEC data for texts written by native   English speakers , including AESW ( Daudaravi-   cius et al . , 2016 ) , LOCNESS ( Bryant et al . , 2019 ) ,   GMEG ( Napoles et al . , 2019 ) and CWEB ( Flachs   et al . , 2020 ) . In the future , we plan to extend this   work to texts written by native Chinese speakers .   CGEC resources . Compared with EGEC ,   progress in CGEC data construction largely lags be - hind . As discussed in Section 1 , NLPCC18 ( Zhao   et al . , 2018 ) and CGED ( Rao et al . , 2018 , 2020 )   are the only two evaluation datasets for CGEC re-   search . Besides them , there are also a few resources   for training CGEC models , e.g. , the Lang8 corpus   ( Zhao et al . , 2018 ) and the HSK corpus ( Zhang ,   2009 ) .   Concurrently with this work , Wang et al . ( 2022 )   present a multi - reference CGEC dataset , named   as yet another Chinese learner corpus ( YACLC ) ,   containing 32,124 sentences from Lang8 . Each   sentence is annotated by 10 annotators .   Recent progress in CGEC . In the NLPCC-2018   shared task ( Zhao et al . , 2018 ) , many systems   adopt Seq2Seq models , based on RNN / CNN . Re-   cent work mainly utilizes Transformer ( Wang et al . ,   2020 ; Zhao and Wang , 2020 ; Tang et al . , 2021 ) .   Hinson et al . ( 2020 ) first employ a Seq2Edit model   for CGEC , and achieve comparable performance   with the Seq2Seq counterparts . Some systems in   the CGED-2020 shared task ( Rao et al . , 2020 ) di-   rectly employ the open - source Seq2Edit model , i.e. ,   GECToR ( Liang et al . , 2020 ) . Most Seq2Edit mod-   els use PLMs like BERT ( Devlin et al . , 2019 ) to   initialize their encoders . Besides the above two   mainstream models , Li and Shi ( 2021 ) for the first   time apply a non - autoregressive neural machine   translation model to CGEC .   Besides modeling optimization , techniques like   data augmentation ( Zhao and Wang , 2020 ; Tang   et al . , 2021 ) and model ensemble ( Hinson et al . ,   2020 ) have proven to be very useful for CGEC .   8 Conclusions   This paper presents MuCGEC , a newly annotated   evaluation dataset for CGEC , consisting of 7,063   sentences written by CSL learners . Compared with   existing CGEC datasets , ours can support more re-   liable evaluation due to three important features :   1 ) providing multiple references ; 2 ) covering three   text sources ; 3 ) adopting strict quality control ( i.e. ,   annotation guidelines and workflow ) . After de-   scribing the data construction process , we perform   detailed analyses of our data . Then , we adopt two   mainstream and competitive CGEC models , i.e. ,   Seq2Seq and Seq2Edit , and carry out benchmark   experiments . We also propose to adopt char - based   evaluation metrics to replace previously used word-   based ones.3126Acknowledgements   We want to thank the anonymous reviewers and   Sebastian Schuster for their great help . We are also   grateful to Zeyang Liu for building the annotation   system , and Lei Zhang , Fukang Yan , Jiayu Shen ,   Houquan Zhou , Yu Zhang for helping us improve   this paper , and all annotators for their great effort in   data annotation . This work was partially supported   by National Natural Science Foundation of China   ( Grant No.62176173 and No.61876116 ) and by Al-   ibaba Group through Alibaba Innovative Research   Program . This work was also partially supported   by Projected Funded by the Priority Academic Pro-   gram Development of Jiangsu Higher Education   Institutions .   References31273128   A More Discussion on the Char - based   Span - level Metric   We find that ERRANT_ZH ( Hinson et al . , 2020 ) , a   useful evaluation tool for CGEC , only merges edits   for redundant errors and missing errors , and does   not merge edits for substitution errors . In contrast ,   as discussed in Section 6.2 , we also merge the con-   secutive edits for substitution errors for the sake of   simplicity . We hope future research can adopt our   simplified version unless there is a strong reason .   Meanwhile , we should keep thinking about which   evaluation metrics are more suitable for CGEC   task .   B Interface   Figure 4 shows our design of annotation interface   in our annotation tool , where annotators correct   assigned sentences . Given an annotation task , this   interface presents a potentially wrong sentence anda text input box . The original sentence is copied   into the input box below , so that the annotator can   directly modify it . To support multiple corrections ,   we also provide a button to add additional input   boxes . Two special buttons are provided to deal   with special cases . The error free button means that   the sentence is correct ; the not annotatable button   means that the annotator can not understand the   sentence .   Figure 5 shows the review interface , where se-   nior annotators judge whether the submitted correc-   tions are correct . All corrections of a sentence from   annotators are shown on the screen , and reviewers   click a check box to mark each of them as correct   or incorrect . The input box below allows reviewers   to supplement extra valid corrections .   C Hyperparameters   Table 8 and Table 9 shows the detailed hyperpa-   rameters for training our two benchmark models .   Due to the GPU memory limitation , we truncated   sentences longer than 100 characters when training   the Seq2Seq model . In other words , extra charac-   ters in the input sentences and the references are   discarded .   A useful trick . We find that some sentences in   MuCGEC are actually composed of multiple sen-   tences . Therefore , we split one input sentence into   multiple ones based on punctuation marks such as3129periods , exclamation marks , and so on . Then , we   perform corrections on the smaller sentences and   concatenate the results . We find this trick can con-   sistently improve performance in our preliminary   experiments . For now , we decide not to break the   sentences when releasing in order to be consistent   with the sources where the data comes from.3130