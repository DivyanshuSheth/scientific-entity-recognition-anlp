  Subba Reddy Oota * , Jashn Arora * , Veeral Agarwal , Mounika Marreddy   Manish Guptaand Bapi Raju SurampudiINRIA , Bordeaux , France;IIIT Hyderabad , India;Microsoft , India   Abstract   Several popular Transformer based language   models have been found to be successful for   text - driven brain encoding . However , existing   literature leverages only pretrained text Trans-   former models and has not explored the effi-   cacy of task - specific learned Transformer rep-   resentations . In this work , we explore trans-   fer learning from representations learned for   ten popular natural language processing tasks   ( two syntactic and eight semantic ) for predict-   ing brain responses from two diverse datasets :   Pereira ( subjects reading sentences from para-   graphs ) and Narratives ( subjects listening to   the spoken stories ) . Encoding models based on   task features are used to predict activity in dif-   ferent regions across the whole brain . Features   from coreference resolution , NER , and shallow   syntax parsing explain greater variance for the   reading activity . On the other hand , for the   listening activity , tasks such as paraphrase gen-   eration , summarization , and natural language   inference show better encoding performance .   Experiments across all 10 task representations   provide the following cognitive insights : ( i )   language left hemisphere has higher predic-   tive brain activity versus language right hemi-   sphere , ( ii ) posterior medial cortex , temporo-   parieto - occipital junction , dorsal frontal lobe   have higher correlation versus early auditory   and auditory association cortex , ( iii ) syntactic   and semantic tasks display a good predictive   performance across brain regions for reading   and listening stimuli resp .   1 Introduction   Brain encoding aims at constructing neural brain   activity given an input stimulus . Since the discov-   ery of the relationship between language stimuli   and functions of brain networks using fMRI [ for   ex . , ( Constable et al . , 2004 ) ] , researchers have been   interested in understanding how the neural encod-   ing models predict the fMRI brain activity . Sev-   eral brain encoding models have been developedto ( i ) understand the ventral stream in biological   vision ( Yamins et al . , 2014 ; Kietzmann et al . , 2019 ;   Bao et al . , 2020 ) , and ( ii ) to study the higher - level   cognition like language processing ( Gauthier and   Levy , 2019 ; Schrimpf et al . , 2021 ; Schwartz et al . ,   2019 ) .   Some recent studies ( Nishida et al . , 2015 ; Huth   et al . , 2016 ) have been able to identify brain ROIs   ( Region of Interest ) that respond to words that have   a similar meaning and have thus built a “ semantic   atlas ” of how the human brain organizes language .   Further , several studies ( Oota et al . , 2018 ; Jain and   Huth , 2018 ; Hollenstein et al . , 2019 ) have used   a wide variety of word embeddings where words   represented as vectors in an embedding space are   mapped to brain activation for improved neural   coding .   Recently , Transformer ( Vaswani et al . , 2017 )   based models like BERT ( Devlin et al . , 2019 ) have   been found to be very effective across a large num-   ber of natural language processing ( NLP ) tasks .   These Transformer based models have been pre-   trained on millions of text instances in an unsuper-   vised manner and further finetuned to specialize for   various NLP tasks . Natural language understand-   ing requires integrating several cognitive skills like   syntactic parsing of the language structure , identify-   ing the named entities , capturing the word meaning   in the context , coreference resolution , etc . Learn-   ing from massive corpora enables these models to   excel at cognitive skills required for language un-   derstanding . Interestingly , such Transformer - based   neural representations have been found to be very   effective for brain encoding as well ( Schrimpf et al . ,   2021 ) .   Despite the recent advances in mapping be-   tween language Transformers and the brain activity   recorded with reading ( Schrimpf et al . , 2021 ) , the   Transformer features themselves are notoriously   difficult to interpret . In recent works , Caucheteux   et al . ( 2021a ) ; Antonello et al . ( 2021 ) address this3220issue by disentangling the high - dimensional Trans-   former representations of language models into   four combinatorial classes : lexical , compositional ,   syntactic , and semantic representations to explore   which class is highly associated with language cor-   tical ROIs . Representations do not exist in a vac-   uum but become meaningful only when they ac-   complish a task . Therefore , the next logical step is   to see which of these Transformer representations   most effectively drive the linear mapping between   language models and the brain in the context of   NLP tasks . Gauthier and Levy ( 2019 ) fine - tune a   pretrained BERT model on multiple tasks to find   tasks best correlated with high decoding perfor-   mance . In this study , we investigate the correlation   between brain activation and feature representa-   tions learned by different task - specific networks ,   and ask which tasks lead to improvements in brain-   encoding performance .   Recently , a study using multiple computer vi-   sion tasks has shown that 3D vision task models   predict better fMRI brain activity than 2D vision   task models ( Wang et al . , 2019 ) for visual stim-   uli . Inspired by the success of correlations in the   vision field ( Wang et al . , 2019 ) , and brain encod-   ing study of a variety of language Transformer   models ( Schrimpf et al . , 2021 ; Caucheteux et al . ,   2021b , a ) , we build neural language taskonomy   models for brain encoding and aim to find NLP   tasks that are most explanatory of brain activations   for reading and listening tasks .   In this paper , we uncover insights about the as-   sociation between fMRI voxel activations and rep-   resentations of diverse NLP tasks representations .   The predictive power of task - specific representa-   tions with brain activation is ascertained by ( 1 )   using ridge regression on such representations and   predicting activations and ( 2 ) computing popular   metrics like 2V2 accuracy and Pearson correlation   between actual and predicted activations .   Specifically , we make the following contribu-   tions in this paper .   •Given Transformer models finetuned for var-   ious NLP tasks , we propose the problem of   finding which of these are the most predic-   tive of fMRI brain activity for reading and   listening tasks .   •Our language taskonomy results reveal that   Coreference Resolution , Named Entity Recog-   nition , and Shallow Syntax Parsing tasks havehigher predictive performance while reading   the text . On the other hand , paraphrase detec-   tion , summarization , and Natural Language   Inference tasks display better correlation dur-   ing listening .   •We also perform similarity analysis between   task representations from transfer learning and   neural taskonomy and derive interesting cog-   nitive insights from brain maps .   2 Related Work   Older methods for text - based stimulus rep-   resentation include text corpus co - occurrence   counts ( Mitchell et al . , 2008 ; Pereira et al . , 2013 ;   Huth et al . , 2016 ) , syntactic and discourse fea-   tures ( Wehbe et al . , 2014 ) . In recent times , both   semantic and experiential attribute models have   been explored for text - based stimuli . Semantic rep-   resentation models include distributed word embed-   dings ( Pereira et al . , 2016 ; Anderson et al . , 2017a ;   Pereira et al . , 2018 ; Toneva and Wehbe , 2019 ; Hol-   lenstein et al . , 2019 ; Wang et al . , 2020 ) , sentence   representation models ( Sun et al . , 2019 ; Toneva and   Wehbe , 2019 ; Sun et al . , 2020 ) , recurrent neural net-   works ( Jain and Huth , 2018 ; Oota et al . , 2019 ) , and   Transformer - based language models ( Gauthier and   Levy , 2019 ; Toneva and Wehbe , 2019 ; Schwartz   et al . , 2019 ; Oota et al . , 2022a , b ) . Experiential at-   tribute models represent words in terms of human   ratings of their degree of association with different   attributes of experience , typically on a scale of 0-   6 ( Anderson et al . , 2019 , 2020 ; Berezutskaya et al . ,   2020 ; Jat et al . , 2020 ; Caucheteux et al . , 2021a ;   Antonello et al . , 2021 ) or binary ( Handjaras et al . ,   2016 ; Wang et al . , 2017 ) . Fine - grained details such   as lexical , compositional , syntactic , and semantic   representations of narratives are factorized from   Transformer - based models and utilized for train-   ing encoding models . The resulting models are   better able to disentangle the corresponding brain   responses in fMRI ( Caucheteux et al . , 2021a ) .   In this paper , we focus on Transformer - based lin-   guistic stimuli representations since they have been   found to be most effective . Unlike previous stud-   ies which directly used existing task - agnostic pre-   trained models , we train task - specific Transformer   models and aim to find which model leads to the   best encoding accuracy given reading and listening   language stimuli.32213 Brain Imaging Datasets   We work with two datasets : Pereira and   Narratives - Pieman . Results on Narratives - Lucy   and Narratives - SlumLord show similar trends .   Hence , we also show results on Narratives - Lucy   and Narratives - SlumLord in the appendix .   Pereira Dataset ( Reading Sentences from Pas-   sages ) For the Pereira dataset , similar to earlier   work ( Sun et al . , 2019 , 2020 ) , we combine the data   from sentence - based experiments ( experiments-2   and 3 ) from Pereira et al . ( 2018 ) . Five subjects were   presented a total of 627 sentences from 48 broad   topics , spanning over 168 passages , where each   passage consists of 3 - 4 sentences . As in ( Pereira   et al . , 2018 ) , we focused on nine brain ROIs ( re-   gions of interest ) corresponding to four brain net-   works : ( i ) Default Mode Network ( DMN ) ( linked   to the functionality of semantic processing ) , ( ii )   Language Network ( related to language process-   ing , understanding , word meaning , and sentence   comprehension ) , ( iii ) Task Positive Network ( TP )   ( related to attention , salience information ) , and ( iv )   Visual Network ( related to the processing of visual   objects , object recognition ) . We briefly summarize   the details of the dataset and the number of voxels   corresponding to each ROI in Table 1 . We use the   AAL parcellation Atlas ( 116 ×116 brain ROIs ) to   present the brain map results , since Pereira dataset   contains annotations tied to this atlas .   Narratives - Pieman ( Listening to Stories ) The   “ Narratives ” collection aggregates a variety of fMRI   datasets collected while human subjects listened to   naturalistic spoken stories . The Narratives dataset   that includes 345 subjects , 891 functional scans ,   and 27 diverse stories of varying duration totaling∼4.6 hours of unique stimuli ( ∼43,000 words ) was   proposed in ( Nastase et al . , 2021 ) . Similar to ear-   lier works ( Caucheteux et al . , 2021b ) , we analyze   data from 82 subjects listening to the story titled   ‘ PieMan ’ with 259 TRs ( repetition time – fMRI   recorded every 1.5 sec . ) . We list number of voxels   per ROI in this dataset in Table 2 . We use the multi-   modal parcellation of the human cerebral cortex   ( Glassar Atlas : consists of 180 ROIs in each hemi-   sphere ) to display the brain maps ( Glasser et al . ,   2016 ) , since Narratives dataset contains annota-   tions tied to this atlas . The data covers ten brain   ROIs in the human brain , i.e. , Left hemisphere ( L ) ,   and Right hemisphere ( R ) for each of the following :   ( i ) early auditory cortex ( EAC : A1 , LBelt , MBelt ,   PBelt , and R1 ) which plays a key role for sound per-   ception since it represents one of the first cortical   processing stations for sounds ; ( ii ) auditory associ-   ation cortex ( AAC : A4 , A5 , STSdp , STSda , STSvp ,   STSva , STGa , and TA2 ) which is concerned with   the memory and classification of sounds ; ( iii ) pos-   terior medial cortex ( PMC : POS1 , POS2 , v23ab ,   d23ab , 31pv , 31pd , 7 m ) ; ( iv ) the temporo parieto   occipital junction ( TPOJ : TPOJ1 , TPOJ2 , TPOJ3 ,   STV , PSL ) which is a complex brain territory heav-   ily involved in several high - level neurological func-   tions , such as language , visuo - spatial recognition ,   writing , reading , symbol processing , calculation ,   self - processing , working memory , musical mem-   ory , and face and object recognition ; and ( v ) the   dorsal frontal lobe ( DFL : L_55b , SFL , L_44 , L_45 ,   IFJA , IFSP ) which covers the aspects of pragmatic   processing such as discourse management , integra-   tion of prosody , interpretation of nonliteral mean-   ings , inference making , ambiguity resolution , and   error repair .   4 Encoding Model   To explore how and where contextual language   features are represented in the brain when read-   ing sentences and listening to stories , we extract   different features spaces describing each stimulus   sentence and use them in an encoding model to   predict brain responses . Our reasoning is as fol-   lows . If a feature is a good predictor of a spe-   cific brain region , information about that feature   is likely encoded in that region . In this paper , for   both datasets , we train fMRI encoding models us-   ing Ridge regression on stimuli representations ob-   tained using a variety of NLP tasks . The main goal   of each fMRI encoder model is to predict brain3222responses associated with each brain region given   a stimuli . In all cases , we train a model per subject   separately . Following literature on brain encod-   ing ( Caucheteux et al . , 2021b ; Toneva et al . , 2020 ) ,   we choose to use a ridge regression model instead   of more complicated models . We plan to explore   more such models as part of future work . We follow   K - fold ( K=10 ) cross - validation . All the data sam-   ples from K-1 folds were used for training , and the   model was tested on samples of the left - out fold .   We used sklearn ’s ridge - regression with default   parameters , 10 - fold cross - validation , Stochastic-   Average - Gradient Descent Optimizer , Huggingface   for Transformer models , MSE loss function , and   L2 - decay ( λ ) as 1.0 . We used BERT Word - Piece   tokenizer for the linguistic Transformer input . All   experiments were conducted on a machine with 1   NVIDIA GEFORCE - GTX GPU with 16 GB GPU   RAM . We make the code publicly available .   4.1 Feature Spaces   To simultaneously test representations from mul-   tiple NLP tasks , we used the latent space features   from each of the following ten popular NLP tasks :   coreference resolution ( CR ) , named entity recog-   nition ( NER ) , natural language inference ( NLI ) ,   paraphrase detection ( PD ) , question answering   ( QA ) , sentiment analysis ( SA ) , semantic role la-   beling ( SRL ) , shallow syntax parsing ( SS ) , sum-   marization ( Sum ) and word sense disambiguation   ( WSD ) . All of these are discriminative NLP tasks ,   and thus we use models obtained by task - specific   finetuning of the same pretrained Transformer en-   coder model ( BERT - base - cased with dimension-   ality=768 ) . Given an input sentence , each task   Transformer outputs token representations at the   final layer . We use the # tokens ×768 dimension   vector obtained from the last hidden layer to obtain   latent features for the stimuli . We then build indi-   vidual ridge regression models with the extracted   latent features to predict brain responses and mea-   sure the correlation between the prediction and the   true response .   Pereira : Since individual sentences were presented   to the subjects while modeling , sentences were   passed one by one to the task Transformer model ,   and average - pooled representations were used to   encode the sentence stimuli .   Narratives - Pieman : Due to the constraint on input   sequence length for BERT ( 512 ) , we considereda window size of 10 sentences with the last two   sentences of one window overlapping with the next   to be given as input to the BERT model . We use   the average - pooled representation from BERT to   encode text stimuli . To get the representation for   a TR , we pooled the representations of only those   words of the sentences in that TR .   4.2 Task Descriptions   Here we describe the functionality of each NLP   task that we used for fMRI encoding . CR : involves   finding all expressions that refer to the same entity   in a text . PD : involves taking a passage – either   spoken or written – and rewording it in shorter   or own words . Summarization ( Sum ): involves   selecting a few important sentences from a docu-   ment or paragraph . NER : involves detection of   the named entities such as person names , location   names , company names from a given text . NLI : in-   vestigates the entailment relationship between two   texts : premise and hypothesis . QA : aims to select   an answer given a passage , a question , and a set   of candidate answers . SA : involves determining   whether a piece of text is positive , negative , or neu-   tral . SRL : assigns labels to words or phrases in a   sentence that indicates their semantic role in the   sentence , such as that of an agent , goal , or result .   SS : provides an approximation of phrase - syntactic   structure of sentences . WSD : involves determining   which sense ( meaning ) of a word is activated by   the use of the word in a particular context .   Syntactic reasoning is rather shallow compared   to deep semantic reasoning . Syntactic reasoning   follows somewhat objective grammar rules . Com-   paratively semantic reasoning is often subjective   in nature and complex . The emerging evidence   from fMRI studies ( Fedorenko et al . , 2020 , 2012 )   also points out that processing of both syntax and   semantics is distributed in the brain and it is only   when violations of these processes are probed , we   see localization of function ( Friederici et al . , 2003 ) .   Thus , in this work , we explore syntactic and seman-   tic tasks separately . Of the above mentioned tasks ,   NER and SS are syntactic , while the others involve   semantic reasoning .   Our selection of these tasks was based on the fol-   lowing design principles : ( 1 ) We wanted to select   a set of tasks covering diverse cognitive - linguistic   skills . ( 2 ) We wanted to select tasks that are a part   of popular NLP benchmarks like GLUE ( Wang   et al . , 2018 ) . ( 3 ) We selected tasks for which3223   BERT - base - cased finetuned models were available .   Note that we did not finetune any of these models   ourselves but leveraged the state - of - the - art fine-   tuned models available on Huggingface . Details of   the specific finetuned model checkpoints are men-   tioned in Table 3 in the Appendix .   4.3 Evaluation Metrics   We evaluate our models using popular brain encod-   ing evaluation metrics described in the following .   Given a subject and a brain region , let Nbe the   number of samples . Let { Y}and{ˆY}de-   note the actual and predicted voxel value vectors for   theisample . Thus , Y∈RandˆY∈R   where Vis the number of voxels in that region .   2V2 Accuracy is computed as   2V2Acc = I[cosD ( Y,ˆY ) +   cosD ( Y,ˆY ) < cosD ( Y,ˆY ) + cosD ( Y,ˆY ) ]   where cosD is the cosine distance function . I[c]is   an indicator function such that I[c ] = 1 ifcis true ,   else it is 0 . The higher the 2V2 accuracy , the better .   Pearson Correlation ( PC ) is computed as   PC = corr[Y,ˆY]where corr is the corre-   lation function .   Mean Absolute Error ( MAE ) is computed asMAE=|[Y−ˆY]| .   Statistical Significance : In order to estimate the   statistical significance of the performance differ-   ences ( across all results ) , we performed one - way   ANOV A on the mean values for the subjects . In   all such cases we report p - values corrected using   Bonferroni correction .   4.4 Neural Language Tasks Similarity   Computation   To estimate the similarity between 10 language   tasks , we took the prediction performance scores   across all the voxels in Pereira ( 97,539 ) and   Narratives - Pieman datasets ( 10,732 ) . To analyze   the relationship between tasks based on neural rep-   resentations , we calculated the Pearson correlation   between predicted voxels of each task with the re-   maining tasks . These Pearson correlation values   were used to construct heatmaps and the task simi-   larity trees(dendograms ) using hierarchical cluster-   ing for Pereira and Narratives - Pieman datasets .   5 Results   In order to assess the performance of the fMRI   encoder models learned using the representations3224   from a variety of NLP tasks , we computed the   2V2 accuracy and Pearson correlation coefficient   between the predicted and true responses across   various ROIs for both the reading ( Pereira ) dataset   ( Fig . 1 ) as well as the listening ( Narratives - Pieman )   dataset ( Fig . 2 ) .   5.1 Encoding performance of Language Task   models for reading vs listening tasks   Reading Sentences ( Pereira ): From Fig . 1 , we   observe that tasks such as CR , NER , SRL , and   SS appear to have a better correlation to the brain   responses compared to the other tasks . In or-   der to estimate the statistical significance of the   performance differences , we performed one - way   ANOV A on the mean correlation values for the   subjects across the ten language tasks for the nine   brain ROIs . The main effect of the ANOV A test   was significant for all the ROIs with p ≤10   with confidence 95 % ( see Appendix for detailed   ANOV A results ) . Further , post hoc pairwise com-   parisons ( Ruxton and Beauchamp , 2008 ) confirmed   the visual observations that on both 2V2 accuracy   and Pearson correlation measures , tasks such as   CR , NER , SRL , and SS performed significantly   better compared to other tasks ( see Appendix forpairwise comparison results ) . These results demon-   strate that when reading a sentence , information   processing operations related to recognizing named   entities , labeling semantic roles to the constituents   of a sentence , identifying the references from a   sentence to the given topic ( concept ) , and syntactic   processing may be engaged .   Further , we observe that the ROI corresponding   to language processing in the left hemisphere ( Lan-   guage_LH ) has higher encoding performance than   that of the right hemisphere ( Language_RH ) . This   is in line with the left hemisphere dominance for   language processing ( Binder et al . , 2009 ) . Also ,   lateral visual ROIs such as Vision_Object , Vi-   sion_Body , Vision_Face , and Vision ROIs display   higher correlation with the language tasks associ-   ated with named entities ( NER ) , relating the en-   tities ( CR ) , and syntax processing ( SS ) . Higher   correlations with all the visual brain regions point   to the possible alignment of visual and language   regions for semantic understanding ( Popham et al . ,   2021 ) in a reading task . Finally , across all regions ,   pretrained BERT model has worse correlation com-   pared to at least 5 other task models .   Listening Stories ( Narratives - Pieman ): From   Fig . 2 , we observe that the profiles of performance3225show low scores in the early auditory cortex ( EAC ) ,   auditory association cortex ( AAC ) ; average scores   in TPOJ and DFL ; and superior scores in PMC .   This aligns with the known language hierarchy   for spoken language understanding ( Nastase et al . ,   2020 ) . Tasks such as PD , Summarization , and   NLI seem to yield better performance in predict-   ing the brain responses than the other NLP tasks   across all the ROIs . These Pearson correlation ( τ )   results are comparatively much higher compared   to those obtained using pretrained ( task - agnostic )   GPT2 model in ( Caucheteux et al . , 2021a ) ( τrang-   ing from 0.02−0.06 ) . As shown in Fig . 2 , our   method obtains much higher correlations ( τrang-   ing from 0.02−0.229 ) . Similar to the Pereira   dataset , we estimate the statistical significance of   the performance differences using the one - way   ANOV A test . The main effect of task was signifi-   ca nt for all the ROIs with p ≤10with confidence   95 % ( see Appendix for detailed ANOV A results ) .   Also , Post hoc pairwise comparisons ( Ruxton and   Beauchamp , 2008 ) revealed that on both 2V2 accu-   racy and Pearson correlation measures , tasks such   as PD , Sum , and NLI performed significantly better   compared to other tasks ( see Appendix for pairwise   comparison results ) .   Further , from Fig . 2 , we see that the bilateral   posterior medial cortex ( PMC ) associated with   higher language function exhibits a higher corre-   lation among all the brain ROIs . ROIs , including   bilateral TPOJ and bilateral DFL , yield higher cor-   relations with the five NLP tasks , which is in line   with the language processing hierarchy in the hu-   man brain . Finally , across all regions , pretrained   BERT model has worse correlation compared to at   least 5 other task models .   In summary , different and distinct language   Taskonomy features seem to be related to the encod-   ing performance in reading versus listening tasks .   CR , NER , SRL , and SS perform better for read-   ing . PD , Sum , and NLI perform better for listening .   While listening the subject is cognitively more in-   volved in the activity compared to reading ( Buch-   weitz et al . , 2009 ) . Thus , it makes sense that shal-   low tasks like NER and SS are useful for reading   while more complex NLP tasks like PD , Sum and   NLI are effective for encoding listening stimuli .   5.2 Language Task Similarity Computation   Pearson correlation values between predicted re-   sponses for each pair of tasks were used to con-   struct the similarity matrix with heatmap for both   Pereira and Narratives - Pieman datasets , as shown   in Figs . 3 and 4 . We observe that the following task   pairs are highly correlated for the Pereira dataset :   ( NER and CR ) , ( SS and CR ) and ( PD and Sum ) .   Also these task pairs are highly correlated for the   Narratives - Pieman dataset : ( CR and NLI ) , ( NLI   and SA ) and ( PD and Sum ) . Similarities are rela-   tively higher for Narratives - Pieman compared to   the Pereira dataset . Surprisingly , the ( NLI , SA )   pair has lowest similarity for Pereira ( reading ) and   close to highest in Narratives - Pieman ( listening ) .   We hypothesize that this is because sentiment is   best conveyed while the subject is listening .   Reading sentences ( Pereira ): The stimulus sen-   tences from the Pereira dataset were fed as input   to each of the 10 task Transformers . The similarity   among the resulting representations was analyzed   using hierarchical clustering , and the clusters are   visualized as dendrograms in Fig . 5 ( left ) . We ob-   serve that the tasks are clustered into three groups   denoted using red , green , and blue colors . Next ,   we wished to check if similar task grouping is ob-   served on brain activations predicted by ridge re-   gression trained on task - specific representations .   Hence , similar clustering analysis was conducted   on the neural space representations , and the clus-   ters are visualized as dendrograms in Fig . 5 ( right)3226   across all subjects . Interestingly , the tree derived   from brain representation also shows a similar dis-   tribution of tasks across the three groups . Similar   dendrograms for individual subjects are illustrated   in Appendix - Fig . 11 .   Listening Stories ( Narratives - Pieman ): Fig . 6   compares the task similarity tree based on the pat-   terns from the pretrained task Transformers , with   the task similarity tree generated based on similar-   ity in brain response prediction performance aver-   aged across all subjects . We observe that the tasks   are clustered into three groups denoted using red ,   green , and blue colors . Again , the tree derived from   brain representation also shows a similar distribu-   tion of tasks across the three groups . Dendrograms   for individual subjects are in the Appendix - Fig . 12 .   5.3 Brain maps for whole brain predictions   The mean absolute error ( MAE ) between predictive   and actual responses is obtained using individual   task features from the taskonomy . MAE values are   obtained for all the voxels in the brain for both the   reading ( Fig . 7 ) and listening datasets ( Fig . 8) .   In the reading task , we observe from Fig . 7 that   CR has lower MAE compared to PD which in turn   has lower MAE compared to the NLI task ( brain   maps for the other tasks are reported in Fig . 17   in the Appendix ) . Overall , for the reading stim-   uli , tasks such as NLI , QA , and SA display higher   MAE values . To further investigate which subROIs ( LPTG , LMTG , LATG , LFus , Lpar , Lang ,   LIFGorb , LIFG , LaMFG , LpMFG , and LmMFG )   of the Language network are related to the predic-   tive task features , we train encoding models for   all the sub ROIs for the best encoding task , i.e. ,   for the CR task ( see Fig . 14 in Appendix ) . We no-   tice that both LMTG ( middle temporal gyrus ) and   LPTG ( posterior temporal gyrus ) are more accu-   rately predicted than the other sub ROIs . On the   other hand , LIFG - orb displays a lower Pearson cor-   relation for the CR task . The presence of superior   encoding information in the ROIs in the temporal   gyrus as compared to those in the inferior frontal   gyrus seems to mirror similar observations seen in   decoder performance ( Anderson et al . , 2017b ) .   On the other hand , in the listening task , we ob-   serve from Fig . 8 that Paraphrase and WSD display   lower MAE values compared to QA task ( brain   maps for the other tasks are reported in Fig . 18   in the Appendix ) . Taken together , for listening   stimuli , tasks such as NER , QA , SA , CR , and SS   display higher MAE values . From Fig . 8 , we see   that ROIs such as EAC and AAC have higher MAE   compared to PMC and TPOJ brain ROIs .   We further demonstrate the prediction perfor-   mance of the encoder model trained on sub ROIs   for the paraphrase task in Fig . 15 in the Appendix .   It can be observed that sub ROIs such as Pos1 and   Pos2 have a higher Pearson correlation than other   sub ROIs of the PMC region . Both sfl and l55b dis-   play a higher correlation among all the sub ROIs   for the DFL ROI . However , all the sub ROIs in the   TPOJ yield higher correlation , as shown in Fig . 15 .   The control and attention ROIs in the posterior   cingulate cortex ( for ex . , POS1 in PMC ) , together   with the superior frontal language region ( sfl in   DFL ) and TPOJ , are part of the well - known lan-   guage network associated with narrative compre-   hension ( Nastase et al . , 2020 ) , and it is heartening   to see that task features from PD task also relate to   semantic analysis of the ongoing narrative .   5.4 Discussion   ( 1 ) We used a ridge regression model instead of   more complicated models for encoding . We be-   lieve that more complex models can lead to further   exciting insights . ( 2 ) We experimented with 10   NLP tasks . Models can be pretrained for more   such tasks to check if other tasks are better predic-   tive of voxel activations . ( 3 ) We leveraged models   finetuned using datasets of different sizes across3227   tasks . While a fair comparison of dataset sizes   across tasks is impossible , we understand that this   could have resulted in some bias in our results .   ( 4 ) We used a different dataset for reading vs lis-   tening . While we believe that the differences in   task - specific model performances across reading   and listening are mainly due to the learned stimu-   lus representations , but they could also arise from   other factors such as experimental conditions , the   text domain of the stimuli or number of voxels ,   etc . ( 5 ) On Natural Language Understanding tasks   such as NLI , SA , QA and PD , Gauthier and Levy   ( 2019 ) observed that scrambled sentence represen-   tations gave better decoding performance . But en-   coding models ( especially for the listening task ) ,   scrambled order would be detrimental to making   sense of what is being heard . It is an interesting   future task to see if the opposite result is seen in the   case of brain encoding models . It is plausible that   brain uses encoding models in a flexible way when   it comes to decoding ( Kriegeskorte and Douglas,2019 ) . Kriegeskorte and Douglas ( 2019 ) mention   that “ Decoding models can help reveal whether par-   ticular information is present in a brain region in a   format the decoder can exploit . Encoding models   make comprehensive predictions about representa-   tional spaces . ” In this sense , results of current work   are not directly comparable to those of Gauthier   and Levy ( 2019 ) .   6 Conclusion   In this paper , we studied the effectiveness of task   specific NLP models for brain encoding . We ob-   serve that building individual encoding models and   exploiting existing relationships among models can   provide a more in - depth understanding of the neu-   ral representation of language information . Our   experiments on Pereira and Narrative datasets lead   to interesting cognitive insights.32287 Ethical Statement   We reused publicly available datasets for this work :   Pereira and Narratives . We did not collect any new   dataset .   Pereira dataset can be downloaded from https :   //osf.io / crwz7/ . Please read their terms of   usefor more details .   Narratives dataset can be dowloaded from   https://datasets.datalad.org/   ? dir=/labs / hasson / narratives . Please   read their terms of usefor more details .   We do not foresee any harmful uses of this tech-   nology .   References32293230   A Details of the Finetuned Models   We selected tasks for which BERT - base - cased fine-   tuned models were available . Note that we did not   finetune any of these models ourselves but lever-   aged the state - of - the - art finetuned models available   on Huggingface . Details of the specific finetuned   model checkpoints are mentioned in Table 3 .   B ANOV A test results   B.1 Pereira dataset   The main effect of model was significant for the   ROIs with 95 % confidence with these statistics :   • Language_LH : [ F(9 , 40 ) = 3.95 , p=0.0052 ]   • Language_RH : [ F(9 , 40 ) = 4.53 , p=0.0015 ]   • Vision_Body : [ F(9 , 40 ) = 4.397 , p=0.00227 ]   • Vision_Face : [ F(9 , 40 ) = 3.46 , p=0.0085 ]   • Vision_Object : [ F(9 , 40 ) = 3.40 , p=0.0121 ]   • Vision_Scenes : [ F(9 , 40 ) = 4.917 , p=0.0007 ]   • Vision : [ F(9 , 40 ) = 3.945 , p=0.00385 ]   • DMN : [ F(9 , 40 ) = 6.28 , p=0.00034 ]   • TP : [ F(9 , 40 ) = 6.54 , p=0.00042 ]   B.2 Narratives - Pieman dataset   The main effect of model was significant for the   ROIs with 95 % confidence with these statistics :   • EAC_L [ F(9,810)=3.88 , p=.00009 ]   • EAC_R [ F(9,810)=3.34 , p=.00055 ]   • AAC_L [ F(9,810)=5.37 , p=.0000007 ]   • AAC_R [ F(9,810)=6.955 , p=.00000 ]   • PMC_L [ F(9,810)=37.21 , p=.00000 ]   • PMC_R [ F(9,810)=31.62 , p=.00000 ]   • TPOJ_L [ F(9,810)=9.166 , p=.00000 ]   • TPOJ_R [ F(9,810)=7.797 , p=.00000 ]   • DFL_L [ F(9,810)=12.445 , p=.00000 ]   • DFL_R [ F(9,810)=12.27 , p=.00000]3231323232333234323532363237