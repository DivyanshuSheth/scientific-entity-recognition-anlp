  Longyue WangMingzhou XuDerek F. WongHongye Liu   Linfeng SongLidia S. ChaoShuming ShiZhaopeng TuTencent AI LabUniversity of Macau   { vinnylywang , hongyeliu , lfsong , shumingshi , zptu } @tencent.com   nlp2ct.mzxu@gmail.com , { derekfw , lidiasc } @um.edu.com   Abstract   The phenomenon of zero pronoun ( ZP ) has at-   tracted increasing interest in the machine trans-   lation ( MT ) community due to its importance   and difﬁculty . However , previous studies gen-   erally evaluate the quality of translating ZPs   with BLEU scores on MT testsets , which are   not expressive or sensitive enough for accu-   rate assessment . To bridge the data and eval-   uation gaps , we propose a benchmark testset   for target evaluation on Chinese - English ZP   translation . The human - annotated testset cov-   ers ﬁve challenging genres , which reveal dif-   ferent characteristics of ZPs for comprehen-   sive evaluation . We systematically revisit eight   advanced models on ZP translation and iden-   tify current challenges for future exploration .   We release data , code , models and annotation   guidelines , which we hope can signiﬁcantly   promote research in this ﬁeld .   1 Introduction   Zero pronoun ( ZP ) is a discourse phenomenon that   appears frequently in pronoun - dropping ( pro - drop )   languages such as Chinese and Japanese . Speciﬁ-   cally , pronouns are often omitted when they can be   pragmatically or grammatically inferred from intra-   and inter - sentential contexts ( Li and Thompson ,   1979 ) . Since recovery of such ZPs generally fails ,   this poses difﬁculties for several generation tasks ,   including dialogue modelling ( Su et al . , 2019 ) ,   question answering ( Tan et al . , 2021 ) as well as   machine translation ( MT ) ( Wang et al . , 2018a ) .   Although neural machine translation ( NMT )   ( Bahdanau et al . , 2015 ; Vaswani et al . , 2017a ;   Gehring et al . , 2017 ) has achieved great progress   in recent years , translating ZPs from a pro - drop to   a non - pro - drop language is a signiﬁcant and chal-   lenging task ( Wang et al . , 2018a ) . As shown in   Table 1 : Examples of ZP translation from two different   genres . Words in brackets are ZPs that are invisible   in decoding and underlined words are corresponding   antecedents . “ Inp . ” and “ Out . ” represent the Chinese   input and output of Google Translate , respectively . As   seen , the translations are either incomplete or incorrect .   Table 1 , an advanced NMT system still fails to   recall the ZP information , which leads to severe   problems in translation outputs : 1 ) incompleteness   where the ﬁrst case grammatically lacks the subject   “ She ” , and the associated verb should be “ teaches ” ;   and 2 ) incorrectness where the second case is se-   mantically incorrect ( i.e. , it should be “ let her buy   it ” instead of “ let you buy ” ) due to unresolved zero   anaphora of “ 她 ” and “ 它 ” .   In recent years , there has been a growing inter-   est in zero pronoun translation ( ZPT ) , which aims   to directly recover ZPs in the translation ( Wang   et al . , 2016 , 2018a ; Yu et al . , 2020 ; Ri et al . , 2021 ) .   Most studies only report performance on MT test-   sets in terms of BLEU scores , which is not expres-   sive or sensitive enough to capture the translation   quality on ZPs . To bridge this gap , we propose   a Chinese - English benchmark testset ( mZPRT )   specially targeting ZPT , which has the following   appealing characteristics :   •Human Annotation : The current ZPT dataset is   automatically annotated using alignment infor-   mation that is not accurate enough ( i.e. , TVsub   dataset ( Wang et al . , 2018a ) ) . Ours is built by pro-   fessional annotators and translators , which can11266   provide more accurate and relevant indications   of model performance .   •Multiple Domains : The frequencies and types   of ZPs vary in different domains ( Yang et al . ,   2015 ) but previous studies only consider a sin-   gle domain . Our testset covers ﬁve challenging   domains , which can comprehensively reveal dif-   ferent characteristics in respective domains .   •Annotations for Both ZP Recovery and Trans-   lation : There is no dataset consisting of both   source ZPs and target translations . We annotate   both of them in our dataset , which enables ex-   plicit investigation of the effects of recovering   ZPs on the performance of ZP translation .   The dataset is signiﬁcantly different from exist-   ing ones , as listed in Table 2 . Experimental results   on the proposed benchmark show signiﬁcant dif-   ferences in model behavior and quality across do-   mains , emphasizing the need for more comprehen-   sive evaluation as a standard procedure . Besides , a   good external system of ZP recovery can beneﬁt ZP   translation , but there is still a large space for further   improvement . Through revisiting recent advanced   models from the perspective of ZP translation , we   obtain some interesting ﬁndings :   •Scaling NMT models can achieve great perfor-   mance gains in terms of BLEU score , however ,   the translation accuracy regarding ZPs still can   not be guaranteed .   •Although BLEU scores do not vary signiﬁcantly ,   we prove that document - level NMT models are   helpful to ZP translation .   2 Preliminaries   2.1 Zero Pronoun in Machine Translation   The anaphora phenomenon is the meaning of an ele-   ment depends on the antecedent element in context , which can be considered one of the most challeng-   ing problems in natural language processing , espe-   cially for ZP ( Peral and Ferrández , 2003 ) . Recent   years have seen a surge of interest in zero pronoun   recovery ( ZPR ) and ZPT tasks , which respectively   resolve ZPs in the source and target language ( e.g.   Chinese⇒English ) .   Zero Pronoun Recovery Given a source sen-   tence , ZPR aims to insert dropped pronouns in   proper positions without changing the original   meanings ( Yang and Xue , 2010 ; Yang et al . , 2015 ,   2019 ) . It is different from the task of ZP resolu-   tion , which identiﬁes the antecedent of a referential   pronoun ( Mitkov , 2014 ) . However , more than 50 %   ZPs are non - anaphoric ( Rao et al . , 2015 ) , which   is not directly helpful to MT task compared with   ZPR systems . Previous studies regarded ZPR as a   classiﬁcation or sequence labeling problem , and ad-   vanced models can only achieve 40 ∼60 % F1 scores   on closed datasets ( Zhang et al . , 2019 ) , indicating   the difﬁculty of understanding zero anaphora .   Zero Pronoun Translation When pronouns are   omitted in a source sentence , ZPT aims to gener-   ate ZPs in its target translation . Generally , prior   works fall into three categories : 1 ) pipeline , where   input sentences are labeled with ZPs using an ex-   ternal ZPR system and then fed into a standard   MT model ( Chung and Gildea , 2010 ; Wang et al . ,   2016 , 2017b ) ; 2 ) implicit , where ZP phenomenon   is implicitly resolved by modelling document - level   contexts ( Wang et al . , 2017a ; Yu et al . , 2020 ; Ri   et al . , 2021 ) ; 3 ) end - to - end , where ZP prediction   and translation are jointly learned in an end - to - end   manner ( Wang et al . , 2018a , b , 2019 ; Tan et al . ,   2021 ) . Although these methods have achieved per-   formance improvement to some extent , they still   face a few major weaknesses .   2.2 Discussion and Motivation   Lack of Comprehensive Benchmark Testsets   Two technological advances in the ﬁeld of ZPR   and ZPT have seen vast progress over the last   decades , but they have been developed very much   in isolation . For instance , the ZPR systems are   mainly trained and evaluated on the human - labeled   BaiduKnows Corpus while the ZPT task has exper-   imented on the auto - annotated TVsub . This critical   data gap severely limits the investigation of the   effects of ZPR on the performance of ZPT . Fur-   thermore , the frequencies and types of ZPs vary   in different genres ( Yang et al . , 2015 ) . The most11267   frequent ZP in newswire text is the third person   singular它(“it ” ) ( Baran et al . , 2012 ) , while that   in SMS dialogues is the ﬁrst person 我(“I ” ) and   我们(“we ” ) ( Rao et al . , 2015 ) . This may lead to   differences in model behavior and quality across   domains . Thus , experimental results are neither   reliable nor comprehensive when evaluated on a   domain - speciﬁc dataset . Motivated by this , we pro-   pose a multi - domain benchmark dataset for evalu-   ating ZPR and ZPT tasks .   Lack of Fine - Grained Evaluation Previous   works usually evaluate ZPT models using the   BLEU metric ( Wang et al . , 2016 , 2018a ; Yu et al . ,   2020 ; Ri et al . , 2021 ) , however , we empirically   found that general - purpose metrics can not charac-   terize the performance of ZP translation . As shown   in Table 1 , the missed or incorrect pronouns may   not affect BLEU scores but severely harm true per-   formances . To bridge this gap , we also measure   model performance using a pronoun - speciﬁc eval-   uation metric ( Guillou and Hardmeier , 2018 ) . To   this end , we systematically compared existing ZPT   methods , and highlighted advances in ZPT quality   that go beyond BLEU improvement .   3 mZPRT : New Benchmark Dataset   In general , the process of manual labeling totally   spends ﬁve annotators and two translators two   months , which costs US $ 10,000 dollars .   3.1 Data Source   We determined ﬁve domains of texts ( i.e. movie   subtitle , Q&A forum , government news , web ﬁc - tion , and personal proﬁle ) that contain a proportion   of ZPs . Accordingly , we construct these subsets in   three ways : 1 ) ZP Labeling , where expert annota-   tors annotate the source side of a parallel dataset   with ﬁne - grained ZP labels ; 2 ) Translating , where   professional translators extend a monolingual ZP-   labeled dataset into a parallel one by translating   Chinese sentences into English ; 3 ) Both , where we   build them from scratch ( e.g. labeling , translating ) .   •Movie Subtitle . This is collected from valida-   tion and testsets of the TVsub corpus , which   has been auto - annotated with coarse - grained ZP   labels and translations . Then , we checked and   re - annotated ZPs with ﬁne - grained labels .   •Q&A Forum . The source - side sentences are   collected from the testset of the Baidu Knows   corpus , which has been annotated with coarse-   grained ZP labels with boundary tags . We extend   them into parallel data and re - annotated ZPs with   ﬁne - grained labels .   •Government News . We crawled relevant texts   from the bilingual news websiteand then manu-   ally aligned sentences . The source - side data are   labeled with ZPs , and each article is regarded as   one document .   •Web Fiction . We crawled 24 chapters from 5 gen-   res of books in Chineseand EnglishWebnovel   websites . We manually aligned Chinese - English11268   .   sentences to build a parallel version . We labeled   ZPs and then tagged document boundaries ac-   cording to chapter information .   •Personal Proﬁle . The source - side texts are col-   lected from 218 homepages of academic staff ,   covering 50 academic majors in QS2021 top-10   universities in China . We translate Chinese sen-   tences into English and labeled ZPs on the source   side . The texts from the same homepage can be   regarded as one document .   3.2 Human Annotation   Taking an annotation example in Figure 1 for in-   stance , the annotation guidelines are as follows :   1.In a Chinese sentence , ZP can be ascertained by   checking its syntactic structure . To avoid anno-   tation ambiguity , we provide English translation   for reference . Since English is a non - pro - drop   language , Chinese ZP can be detected according   to its English equivalent ( e.g. you⇒[你 ] ) .   2.The anaphoric ZPs can be recovered by consid-   ering their antecedents in the context ( e.g. 饼   干⇒[它们 ] ) . To improve the annotation accu-   racy , the ZPs should be double - checked using   English equivalents ( e.g. them⇒biscuits⇒   [ 它们 ] instead of [ 他们 ] or [ 她们 ] ) .   3.The non - anaphoric ZPs can be recovered by   inferring from salient entities in the environ-   ment ( e.g. [ 你]⇒The Hearer ) . If possible ,   this should be double - checked using English   equivalents ( e.g. you⇒[你 ] instead of [ 我 ] . )   4.We additionally annotate ZPs with ﬁne - grained   labels ( e.g. [ 你 ] ) according to their forms in the   sentence , including subject , object , possessive   adjective and reﬂexive ( i.e. [ · ] , [ · ] , [ · ] , [ · ] ) .This information is helpful to ﬁne - grained anal-   ysis ( in Section 3.3 ) and automatic evaluation   ( in Section 4.1 ) .   5.Some subsets only contain Chinese sentences ,   which need to be translated into English . Trans-   lators could consider document - level contexts to   generate discourse - aware translations ( i.e. sen-   tences # 1 and # 2 are given when translating   sentence # 3 , thus human translators can restore   “ they ” in the target language ) .   6.The document boundaries are also tagged ac-   cording to content clues such as topic , storyline   and scene ( e.g. < doc > . . .</doc > ) .   The “ Personal Proﬁle ” subset was collected from   academic homepages , which are public on the Uni-   versity websites . It does not contain any sensitive   information such as ID number , phone number ,   home address , email , etc . About the personal pro-   ﬁle ( e.g. person name , school name and publi-   cation ) , we have replaced them according to the   following process :   1 . Person names are replaced with Person - A , . . . ;   2.Organization names are replaced with   University - of - A , College - of - B , School - of - C ,   Faculty - of - D , . . . ;   3.Numbers ( i.e. year of work experience and num-   ber of publication ) are randomly shufﬂed .   About ZP annotation , each individual annotates   all sentences in one subset and 50 overlapping sen-   tences sampled from the other four subsets . We   followed Mitani et al . ( 2017 ) to measure the inter-   annotator agreement by calculating the average   pairwise cohen ’s kappa score . As shown in Ta-   ble 3 , our dataset reaches 0.86 % ∼0.95 % kappa11269   scores ( i.e. 0.91 % on average ) , demonstrating that   the annotators work efﬁciently and consistently un-   der this guideline . We attribute this to the fact that   English translation is more helpful to disambiguate   Chinese ZPs than only considering syntactic and   semantic information in a single language . This   is potentially useful for annotating ZP datasets in   other languages .   3.3 Data Statistics and Analysis   As illustrated in Table 3 , the proposed dataset   contains 457 documents with over 8,000   Chinese⇒English parallel sentences . According   to our ﬁne - grained labels , we also report ZP   distribution in terms of three main types : subject ,   object , and possessive adjective . As seen , ZPs in   two domains ( Q&A forum and personal proﬁle )   are subject types , covering more than 90 % of all   ZPs . On the contrary , web ﬁction contains more   possessive adjective ZPs ( 24.1 % ) while ZP types   in movie subtitles distribute more dispersedly . For   the average value across domains , subject ZPs still   occupy a large proportion while object types only   occupy a small proportion ( 78 % vs. 5 % ) .   Figure 2 further analyzes different characteris-   tics of our testset across domains . On the one hand ,   we demonstrate label density by calculating the en-   tropy of ﬁne - grained ZP distribution . By assuming   that high density indicates the difﬁculty of model-   ing ZPs , movie subtitle and web ﬁction are more   challenging than other domains . On the other hand ,   we report the frequency of the phenomenon by   computing the percentage of sentences containing   ZPs . As seen , government news and personal pro-   ﬁle contain many more ZPs than other domains .   In summary , 1 ) this emphasizes the necessity of   evaluating ZPT across multiple domains ; 2 ) it re-   veals the label imbalance and sparsity problems in   ZP - aware tasks .   4 Experiment   We systematically evaluate existing models on the   proposed mZPRT benchmark to 1 ) investigate ef-   fects of ZPR on ZPT ; 2 ) build a benchmark of ZPT   methods ; 3 ) revisit document - level NMT models .   4.1 Experimental Setup   Data and Models We carefully collected train-   ing data for building ZPR , sentence - level MT ,   document - level MT , and reconstruction - based   NMT models , which are illustrated in Table 4 . Ac-   cording to the domain between training data and   our testsets , our training processes can be cate-   gorized into in - domain , out - of - domain and mix-   domain . As seen , ZPR models of Mov . Subtitle and   Q&A forum are trained on their in - domain data ,   respectively . We concatenate these two datasets   as an out - of - domain dataset for the other three   domains . For the sentence - level MT model , the   Mov . Subtitle and the other domains are trained on   the benchmarks from 12.8 M OpenSubtitles2018   and 42.8 M WMT2021 News , respectively . For the   Q&A Forum domain , we further add pseudo - in-   domain data generated via a technique of forward-   translation ( FT ) and data augmentation . We also   apply the FT technique to the Web Fiction domain .   The pseudo - in - domain data of these two domains   are also used to tune the document - level models ,   respectively . For the document - level models , we   directly tune the Mov . Subtitle model on Opensub-   title dataset which contains document boundaries .   Since the WMT2021 dataset did not contain docu-   ment boundaries , we follow the setting of ( Li et al . ,   2020 ) by feeding the model with pseudo sentences11270as context . Since there is only Tvsub data contain-   ing weak labels , we then tune the model back to   its conventional domain by freezing the embedding   and reconstruction - related parameters ( detailed in   Appendix § A.1 ) .   All data are tokenized and then segmented   into subword units using the byte - pair encoding   ( BPE ) ( Sennrich et al . , 2016 ) . We apply 32 K merge   operations to form a vocabulary for Chinese and   English , and the vocabulary is not shared among   source and target languages . The proposed dataset   has been randomly split into validation and test-   sets . All models are implemented on top of Trans-   former ( Vaswani et al . , 2017a ) , of which conﬁgu-   rations are detailed in Section 4.2 . We employed   large - batch training ( i.e. 458 K tokens / batch ) to   optimize the performance ( Ott et al . , 2018 ) .   Evaluation Metrics We used case - insensitive to-   kenBLEU ( Papineni et al . , 2002 ) to measure the   overall translation quality of translation systems ,   and used sign - test ( Collins et al . , 2005 ) for testing   statistical signiﬁcance .   To measure the performance of ZPT , we used a   variant ofPT(Werlen and Popescu - Belis , 2017 ) ,   which is originally designed to measure the accu-   racy of explicit pronoun translation . Speciﬁcally ,   the variantZPT evaluates the accuracy of trans-   lating zero pronouns in the source sentences and is   calculated byZPT = /summationtextA(t|z )   |ZP|(1 )   whereZPis the list of zero pronouns in the source   sentences , tis the generated translation for the   zero pronoun z , and A(t|z)is a binary scorer to   judge whether tis the correct translation of z.   In this work , we implemented an automatic ver-   sion ofZPT for the proposed Chinese - English   mZPRT benchmark . For automatically identify-   ingt , we obtain the word alignment between   ZP - labeled input and its translation output with   a GIZA++ model ( Och and Ney , 2003 ) , which is   trained on a large - scale parallel corpus . To remedy   the errors of automatic alignment , for each labeled   ZPz , we use the aligned target word along with its   preceding and following words as the candidates of   t. It is not straightforward to implement A(t|z ) ,   since the Chinese and English pronouns are not   one - to - one equivalent . For example , the Chinesepronoun “ 我 ” corresponds to two English pronouns   ( i.e. , “ I ” and “ me ” ) . We disambiguate them using   pronoun form labels ( e.g. , subjective , objective in   Section 3.2 ) and a bilingual pronoun dictionary ( de-   ﬁned in Appendix § A.2 ) . For example , if the label   of Chinese pronoun “ 我 ” issubjective , its correct   translation in English is “ I ” . The automatic imple-   mentation ofZPT shows a high correlation ( as   shown in Table 11 , 0.74 Pearson score , 2750 out of   3000 translation annotations reach an agreement )   with human judges on the Chinese - English testset ,   indicating that it is a reasonable metric to evalu-   ate the accuracy of ZPT ( Rei et al . , 2020 ; Wan   et al . , 2022 ) . There are many possible ways to   implement the general idea of measuring ZP trans-   lation accuracy . The aim of this paper is not to   explore the whole space but simply to show that   one fairly straightforward implementation works   well . The limitation ofZPT is that it can only   evaluate Chinese - English ZPT , and we discuss the   extension to other languages in Appendix § A.2 .   4.2 Effects of ZPR on ZPT   As shown in Table 5 , we investigate the effects of   ZPR models on the ZPT task by studying a pipeline-   based method ZPR / mapsto→MT ( Z+ ) , where input sen-   tences are labeled by a ZPR model and then fed   into an MT model .   •MT(B ): We built a Transformer model with   the Bsettings in ( Vaswani et al . , 2017a ) .   •ZPR : We followed Song et al . ( 2020 ) to build a   ZPR model , where BERT ( Devlin et al . , 2019 ) is   used to represent each input sentence to provide   shared features . In practice , we ﬁnetune BERT   with only ZPR signals instead of jointly learning   ZP resolution .   Previous works evaluated the accuracy of ZPR on   sentence - level in terms of recall , precision , and F-   measure ( Wang et al . , 2016 ; Tan et al . , 2021 ; Song   et al . , 2020 ) . The overall recall and precision on   the test set are computed by micro - averaging over-   all test instances and then the overall F - measure   is computed . In our preliminary experiments , we   found that the precision of ZPR systems is more   impactful to the downstream translation task ( e.g.   recovering a wrong ZP is more harmful than doing   nothing ) . Thus , we report the precision of recover-   ing ZPs ( ZPR ) in following experiments .   ZPR Beneﬁts ZPT Considering all ZP types ( in   the “ All ” row ) , most ZPR models can help to im-   prove plain NMT models in terms of ZP translation11271   ( ZPT↑ ) . Speciﬁcally , Z+models can achieve   2∼30%ZPT improvements over Transformer-   Bmodels across different domains . Besides , the   higher performance of ZPR models ( ZPR↑ ) , the   better translation quality of ZPs ( ZPT↑ ) . As   seen , the ZPR model with 43 % accuracy can only   achieve +2%ZPT points while that with 62%ZPR can surprisingly obtain +30%ZPT ( i.e.   movie subtitle vs. Q&A forum ) . On the contrary ,   ZPR systems with low - quality ( i.e. < 40%ZPR   points , empirically ) harm ZPT quality . Taking gov-   ernment news for instance ( out - of - domain ZPR   and in - domain MT ) , theZPT scores decrease by   -0.7 % when using a ZPR system with only 30 %   accuracy . The ﬁndings are similar in ﬁne - grained   cases although there are still considerable differ - ences among different domains .   Large Space for Improvement The best ZPR   model ( Q&A forum ) can only achieve around 62 %   accuracy , leading to a 56 % score on translating ZPs .   There is still a large space for further improvement   of ZPR and we then identify challenges from three   perspectives ( case study in Table 6 ) . ( 1 ) out - of-   domain , where it lacks in - domain data for training   robust ZPR models . Taking personal proﬁle as an   example , the distribution of ZP types is quite differ-   ent between ZPR training data ( out - of - domain ) and   ZPT testset ( in - domain ) . This leads to that the ZPR   model often predicts wrong ZP forms ( possessive   adjective vs. subject ) . ( 2 ) error propagation , where   the external ZPR model may provide incorrect ZP   words to the followed NMT model . As seen , Z+   performs worse than a plain NMT model Bdue   to wrong pronouns predicted by the Zmodel   ( 你们vs . 我 ) . ( 3 ) multiple ZPs , where there is   a 10 % percentage of sentences that contain more   than two ZPs ( as shown in Table 3 ) , resulting in   more challenges to accurately and simultaneously   predict them . As seen , two ZPs are incorrectly   predicted into “ 我 ” instead of “ 他 ” .   4.3 Revisiting NMT Variants   Table 7 shows translation quality ( BLEU ) and ac-   curacy of ZPT ( ZPT ) across different domains .   We investigate three competitive NMT models ,   three representative ZPT approaches and two oracle   methods , as follows ( apart from Z+ ):   •Scaled Transformer : We trained B andB   Transformer models ( Vaswani et al . , 2017b ) . The   D model contains 12 - 12 layers based on11272   B conﬁgurations .   •Document - Level NMT ( D. ): We used the uni-   ﬁed encoder ( Ma et al . , 2020 ) , which takes the   concatenation of contexts and source sentences   as the input using two - level self - attention . In-   stead of using BERT pretraining , we ﬁrst train   the Transformer model on sentence - level train-   ing data until converged and then switch to   document - level training data ( Zhang et al . , 2018 ) .   •Reconstruction - based NMT ( R. ): We re-   implemented the model provided by Wang et al .   ( 2018a ) , where two additional reconstructors ( Tu   et al . , 2017 ) are introduced to reconstruct ZP-   labeled source sentences for both encoder and   decoder . The auxiliary training objectives can   encourage the latent representations to embed ZP   information .   •Oracle : We manually annotated ZPs in input   sentences and then feed them into downstream   MT / ZPT models . This can be regarded as the   “ upper bound ” performance the models can reach .   Scaling Transformer Can not Beneﬁt ZPT Al-   though the overall translation quality is signiﬁ-   cantly improved by scaling models ( Size ↑vs .   BLEU↑ ) , the accuracy of translating ZPs still can   not be guaranteed ( ZPT ) . For example , Band   Dwith larger parameters can achieve better per-   formance than the B in terms of BLEU ( +1.2   and +1.9 points on average ) . However , the corre-   spondingZPT scores remain almost unchanged   or slightly declined ( +0.5 point on average).Existing Methods Can Help ZPT But Not   Enough Three ZPT models can improve ZP   translation in most cases , although there are still   considerable differences among different domains   ( ZPT↑ ) . Introducing ZPT methods has little   impact on BLEU score ( -0.1 ∼+0.2 point on aver-   age ) , however , they can improveZPT over B   by +2.2∼+6.8 . When integrating golden ZP la-   bels into DandR.models , their BLEU andZPT scores largely increased by +1.9 and +47.5   points , respectively . The performance gap between   Oracle and others shows that there is still a large   space for further improvement for ZPT .   Evaluation on Diagnostic Subset Someone   may argue that resolving ZP can not signiﬁcantly   improve BLEU scores . We establish a benchmark   on diagnostic subsets ( only ZP sentences ) that ac-   counts for an average of 44 % of the full testset . As   shown in Table 8 , the ZPT approaches are showing   more gains in translation quality , which indicates   the necessity of recovering ZPs . Compare to the   result in Table 7 , using oracle sequence as input   achieves larger improvement on diagnostic subsets   ( +2.1 v.s. +1.1 BLEU on average ) , especially on   the in - domain dataset ( over 4.0 BLEU on average ) .   This demonstrates the importance of recovering the   ZPs on translation tasks .   4.4 Revisiting Document - Level NMT   One commonly - cited weakness in document - level   NMT is that general - purpose metrics ( e.g. BLEU )   are not sufﬁcient to distinguish translation qualities11273   from the perspective of discourse ( Müller et al . ,   2018 ; V oita et al . , 2018 , 2019 ; Xu et al . , 2021 ) .   As ZP is a signiﬁcant phenomenon of cohesion ,   the proposed dataset and metric are complemen-   tary to verify different document - level approaches .   As shown in Table 9 , we revisit three advanced   models : multi - encoder ( Zhang et al . , 2018 ) , uni-   ﬁed encoder ( Ma et al . , 2020 ) and cache - based ( Tu   et al . , 2018 ) . Encouragingly , we ﬁnd that the trend   ofZPT scores is more close to true performance .   For example , the C model has no improve-   ment in terms of BLEU while it can achieve +3.8 %   better performance on translating ZPs .   5 Conclusion   We revealed data- and evaluation - level gaps in pre-   vious works on translating ZPs . Accordingly , we   proposed a benchmark testset and evaluation met-   ric for target evaluation on ZPT . Our benchmark   emphasizes the large gap between performances of   existing models and an upper bound from the per-   spective of ZPT . We release data ( benchmark test-   set , human judgments and collected training data ) ,   code ( target metric and implemented approaches )   and models ( comparative models ) , which we hope   can signiﬁcantly promote research in this ﬁeld .   Limitation   We list the main limitations of this work as follows :   1.Extending The Dataset to Other Languages : The   zero pronoun ( ZP ) phenomenon may vary across   languages in terms of word form , occurrence   frequency and category distribution etc . This   work mainly proposed a ZP translation testset   in Chinese - English due to the lack of linguistic   experts in other languages . Another reason is   the high cost of human annotation , where we   spent $ 10,000 US dollars for annotating 8,093   sentences in ﬁve domains . The annotation guide-   line in Section 3.2 could be adapted to other lan-   guages with the corresponding linguistic experts .   Therefore , one aim of our work is attracting more   attention to this research topic , which will stim-   ulate further contributions on building multilin-   gual resources .   2.TheZPT Metric : We used the variant ofPT(Werlen and Popescu - Belis , 2017 ) as a tar-   get evaluation metric for the proposed Chinese-   English benchmark . This metric may not be ap-   plicable to all languages , because the situation of   one - to - many pronoun gender is not considered .   In Appendix § A.2 , we have revealed the reason   behind , and provided alternative ways to extend   it to other languages . Besides , the reliability of   such metrics depend on the quality of word align-   ment models . We trained the alignment model   using GIZA++ on a large - scale parallel corpus ,   which still has 10 % to 15 % deviation between   automatic and human evaluation . This gap could   be narrowed by further improving the alignment   quality . In the paper , we do not list the AZPT   metric as one of our main contributions due to   its potential limitation on applicability to other   languages .   Acknowledgements   This work was supported in part by the Science   and Technology Development Fund , Macau SAR   ( Grant No . 0101/2019 / A2 ) , and the Multi - year Re-   search Grant from the University of Macau ( Grant   No . MYRG2020 - 00054 - FST).11274References1127511276A Appendix   A.1 Experimental Setup   Machine Translation We build several domain-   speciﬁc translation models to match the domains of   the proposed testing dataset . To train NMT models   in movie subtitle and government news domains ,   we used 12.8 M OpenSubtitles2018and 42.8 M   WMT2021 Newsparallel corpora , respectively .   For the web ﬁction domain , we extracted 1.45 M   Chinese texts from the Webnovel website and then   employed the technique of forward - translation ( FT )   to generate the in - domain data . About the Q&A   Forum domain , we reused the 50 K training data of   Baidu Knows corpus as monolingual data , and also   employed FT to get the domain - speciﬁc training   data . Due to an insufﬁcient amount of in - domain   monolingual data , we used ten commercial transla-   tion systems to construct 10 ×5 K synthetic parallel   data . Note that since the scale of these pseudo data   is much less than WMT2021 , we tuned the base-   line model on the pseudo data to get the domain-   speciﬁc variant directly in these two domains . We   train an NMT model on WMT2021 News for the   Personal Proﬁle domain because we regard it as an   out - of - domain issue .   Zero Pronoun Recovery and Translation The   ZPR systems can be used to pre - process input sen-   tences before being fed into NMT models , namely   the pipeline - based ZPT method ( Table 7 Z+ ) .   We used 2.2 M TVsub ( Wang et al . , 2018a ) and 5 K   BaiduKnows to train two ZPR models for movie   subtitle and Q&A Forum testsets , respectively . For   the other three domains , we combine these two   corpora to train a general - domain ZPR model . Ac-   cordingly , document - level NMT models ( Table 7   D. ) are trained on several domains of datasets .   For movie subtitle , web ﬁction and Q&A forum   domains , we used the same data used in corre-   sponding baselines with their context and docu-   ment boundary information . Regarding govern-   ment news and personal proﬁle domains , there are   no context - aware corpora . Thus , we feed the model   with pseudo sentences as context , where context   representations act more like a noise generator to   provide richer training signals ( Li et al . , 2020 ) .   Reconstruction - based NMT models ( Table 7 R.)are pre - trained on the same data used in correspond-   ing baselines and then tune reconstruction - related   parameters with only monolingual ZP datasets .   Since there is only Tvsub data containing weak   labels , we then tune the model back to its con-   ventional domain by freezing the embedding and   reconstruction - related parameters .   A.2 Discussion onZPT   Extensibility As shown in Table 10 , Chinese-   English pronouns are many - to - many mapped . In   fact , pronoun - aware evaluation metrics mainly fo-   cus on solving one - to - many problems :   •pronoun form : to disambiguate multiple pronoun   translations according to the sentence unit of   source ZPs . Taking 我⇒I / me for example , it   should be translated into “ I ” when “ 我 ” is subject   while “ me ” when object . Therefore , we annotate   our dataset with ﬁne - grained form labels ( in Sec-   tion 3.2 ) and use this information inZPT ( in   Section 4.1 ) .   •pronoun gender : to disambiguate multiple pro-   noun translations according to the antecedent   gender of source ZPs . However , this is not a   problem for Chinese - English due to many - to - one   mapping ( e.g. 他们 /她们 / 它们⇒them ) .   However , the pronoun gender is a problem for   other pro - drop languages ( e.g. French and Span-   ish ) due to one - to - many mapping . For example ,   Audi is an automaker that makes luxury cars . It   was established by August Horch . The pronoun “ it ”   could be translated into “ il ” ( masculine singular   subject pronoun ) , “ elle ” ( feminine singular sub-   ject pronoun ) or “ cela ” ( demonstrative pronoun )   according its antecedent gender “ Audi ” . Here we   provide two alternative ways to extendZPT : ( 1 )   adding ﬁne - grained gender labels to ZPs , which   is similar to AutoPRF ( Hardmeier and Federico ,   2010 ) ; ( 2 ) introducing translation reference as soft   information , which is similar toPT(Werlen and   Popescu - Belis , 2017 ) .   Human Evaluation Guideline We carefully de-   sign an evaluation protocol according to error types   made by various NMT systems , which can be   grouped into ﬁve categories : 1 ) The translation can   not preserve the original semantics due to misun-   derstanding the anaphora of ZPs . Furthermore , the   structure of translation is inappropriately or gram-   matically incorrect due to incorrect ZPs or lack   of ZPs ; 2 ) The sentence structure is correct , but   translation can not preserve the original semantics11277   due to misunderstanding the anaphora of ZPs ; 3 )   The translation can preserve the original semantics ,   but the structure of translation is inappropriately   generated or grammatically incorrect due to the   lack of ZPs ; 4 ) where a source ZP is incorrectly   translated or not translated , but the translation can   reﬂect the meaning of the source ; 5 ) where trans-   lation preserves the meaning of the source and all   ZPs are translated . Finally , we average the score   of each target sentence that contains ZPs to be the   ﬁnal score of our human evaluation .   For human evaluation , we randomly select a hun-   dred groups of samples from each domain , each   group contains an oracle source sentence and the   hypotheses from six examined MT systems . Fol-   lowing this protocol , we asked expert raters to score   all of these samples in 1 to 5 scores to reﬂect the   quality of ZP translations . As shown in Table 11 ,   our variantZPT reaches around 0.74 Pearson   scores with human judges , whilePTreaches only   0.52 . This conﬁrms that theZPT takes great suc-   cess in adapting to Chinese to English ZPT task .   For the inter - agreement , we simply deﬁne that a   large than 3 is a good translation and a bad trans - lation is less than 3 . The annotators reached an   agreement of annotations on 91 % ( 2750 out of   3000 ) samples . Considering domain - speciﬁc sub-   sets , ZPT achieves the best scores on three do-   mains and the second on others , among automatic   metrics . The web ﬁction domain contains a num-   ber of free translations due to its literariness . We   found that COMET performs better in evaluating   semantic correctness by leveraging cross - lingual   pre - training . In the Q&A forum subset , sentences   are shorter but contain more multiple ZPs ( in Ta-   ble 3 ) . We observe that reference is helpful toPT   to disambiguate neighbor ZPs in one sentence.11278