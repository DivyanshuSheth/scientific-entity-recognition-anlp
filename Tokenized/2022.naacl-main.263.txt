  John P. Lalor , Yi Yang , , Kendall Smith , Nicole Forsgren , Ahmed AbbasiHuman - centered Analytics Lab , University of Notre DameDepartment of IT , Analytics , and Operations , University of Notre DameDepartment of Information Systems and Operations Management , HKUSTMicrosoft Research   { john.lalor , ksmith77 , aabbasi}@nd.edu , imyiyang@ust.hk , nicole.forsgren@microsoft.com   Abstract   There has been a recent wave of work assess-   ing the fairness of machine learning models   in general , and more specifically , on natural   language processing ( NLP ) models built us-   ing machine learning techniques . While much   work has highlighted biases embedded in state-   of - the - art language models , and more recent   efforts have focused on how to debias , research   assessing the fairness and performance of bi-   ased / debiased models on downstream predic-   tion tasks has been limited . Moreover , most   prior work has emphasized bias along a sin-   gle dimension such as gender or race . In this   work , we benchmark multiple NLP models   with regards to their fairness and predictive   performance across a variety of NLP tasks . In   particular , we assess intersectional bias - fair-   ness across multiple demographic dimensions .   The results show that while current debiasing   strategies fare well in terms of the fairness-   accuracy trade - off ( generally preserving predic-   tive power in debiased models ) , they are unable   to effectively alleviate bias in downstream tasks .   Furthermore , this bias is often amplified across   demographic dimensions . We conclude with   implications for future NLP debiasing research .   1 Introduction   As state - of - the - art natural language processing   ( NLP ) language models become increasingly pow-   erful and pervasive , recent progress in NLP has   underscored the need for deeper analyses of how   such models perform with respect to underrepre-   sented groups . Research on fairness in NLP has   shown that distributed representations of words   often encode stereotypes - particularly towards dif-   ferent demographic groups ( Blodgett et al . , 2020 ;   Bender et al . , 2021 ) . There is a growing stream of   research that looks at mitigating these biases , espe-   cially when it manifests in the learned embedding   state ( Bolukbasi et al . , 2016 ; Zmigrod et al . , 2019 ;   Kaneko and Bollegala , 2021 ) . While prior workhas undoubtedly moved the needle , recent surveys   and research articles have identified several impor-   tant gaps and issues ( Blodgett et al . , 2020 ; Tan and   Celis , 2019 ) . First , much of the current work on ex-   amining NLP bias ( and proposing debiasing strate-   gies ) has focused on representational harm - how   a model describes certain groups , including stereo-   typing and other misrepresentations ( Blodgett et al . ,   2020 ; Suresh and Guttag , 2019 ) . Conversely , there   has been far less work exploring allocational harm   in downstream NLP prediction tasks - when a sys-   tem distributes resources or opportunities differ-   ently ( Blodgett et al . , 2020 ; Suresh and Guttag ,   2019 ) . Downstream tasks , such as sequence clas-   sification , also affect underrepresented groups , as   these models show disparate impact on various   demographic subsets , including women , African   Americans , and the elderly ( Blodgett et al . , 2020 ;   Bender et al . , 2021 ; Shah et al . , 2020 ) .   Second , there has been limited work that ex-   amines intersectional bias across a wide array of   relevant charactersitics , including several demo-   graphic dimensions , for a variety of non - debiased   and debiased embeddings , on a multitude of down-   stream tasks . Some work has studied demographic   intersections such as young men and old women   from a theoretical perspective ( e.g. , Kearns et al . ,   2018 ) . Other recent studies have empirically shown   that the biases inherent in language models for gen-   der and race intersections might exceed those ob-   served for gender and race alone ( Tan and Celis ,   2019 ) , and that only debiasing along a single di-   mension can be problematic ( Subramanian et al . ,   2021 ) . Based on these two gaps , there is a need for   a more systematic analysis of how current state - of-   the - art language models and mitigation strategies   perform with regards to intersectional bias in down-3598   stream tasks .   Accordingly , in this study we perform a broad   benchmark analysis of intersectional bias ( Figure 1 )   encompassing the following key characteristics :   •Benchmark analysis on ten downstream se-   quence classification tasks related to five   datasets that span common modes of user-   generated content : Twitter , forums , Reddit ,   and survey responses . For these tasks , we also   note the allocational harm implications of dis-   parate impact , namely the harm associated   with biased NLP - guided interventions .   •Inclusion of five demographic dimensions :   gender , race , age , education , and income .   Having three or more dimensions on many   of the tasks affords opportunities to examine   bias for various demographic intersection sub-   groups in a more in - depth manner . On four   of the datasets , these demographics are self-   reported as opposed to being algorithmically   or heuristically inferred - an important consid-   eration for debiasing research .   •Evaluation of three prominent word embed-   dings , BERT ( Devlin et al . , 2019 ) , RoBERTa   ( Liu et al . , 2019 ) , and GloVe ( Pennington   et al . , 2014 ) , and four state - of - the - art model   debiasing methods ( Ravfogel et al . , 2020 ;   Kaneko and Bollegala , 2021 ; Zmigrod et al . ,   2019 ; Webster et al . , 2020 ) . This allows us   to draw empirical insights regarding the ef-   fectiveness of mitigation strategies for down-   stream tasks . Our results show that existing debiasing methods   are generally very adept at preserving predictive   power in downstream tasks . However , their abil-   ity to mitigate intersectional bias in such tasks is   limited . In general , debiasing BERT / RoBERTa   only incrementally alleviates disparate impact of   model classifications . Further , while gender bias   alone has disparate impact rates of 5 - 10 % or less   on most tasks , the range of bias is amplified for in-   tersections - with unfairness rates often being 20 to   50 % higher . On tasks such as inferring personality   traits , literacy , or numeracy of users , these debi-   ased models are still outside the fairness ranges   recommended by governing bodies ( Barocas and   Selbst , 2016 ) . Interestingly , these biases are more   pronounced in models using GloVe , suggesting that   debiased transformer - based models generally have   better predictive power , and are fairer .   Our main contributions are two - fold . First , we   perform a large - scale examination of intersectional   bias across an array of downstream tasks . Our   benchmark evaluation offers empirical evidence   that the concerns voiced in recent critical surveys   about too much emphasis on representational debi-   asing devoid of explicit normative goals ( Blodgett   et al . , 2020 ) , relative to mitigation of downstream   allocational harm , are well - founded . Second , we   quantify the size and scope of the intersectional   bias problem , and the risks it can introduce for se-   lect underprivileged sub - groups when deploying   NLP models for sequence prediction tasks . We   are hopeful our work will spur future research that   further sheds light on intersectional biases in down-3599stream tasks , as well as mitigation strategies for   alleviating allocational harm . Towards this goal ,   the code and data used in this work is publicly   available via GitHub .   2 Related Work   2.1 Allocational and Representational Harms   In their survey on bias in NLP , Blodgett et al . ( 2020 )   drew a distinction between allocational and repre-   sentational harms . They found that most papers in   NLP describe methods for measuring and mitigat-   ing representational harms - when “ a system ( e.g. ,   a search engine ) represents some social groups in a   less favorable light than others , demeans them , or   fails to recognize their existence altogether ” ( Blod-   gett et al . , 2020 ) . One well - known example are   stereotypes in word embeddings , such as certain   ethnic groups being more closely associated with   " housekeeper " ( Garg et al . , 2018 ) .   In contrast , ( Blodgett et al . , 2020 ) only found   four papers in their survey that were classified as   having techniques for measuring / mitigating allo-   cational harms - these “ arise when an automated   system allocates resources ( e.g. , credit ) or op-   portunities ( e.g. , jobs ) unfairly to different social   groups . ” Allocational harm is often aligned with   downstream tasks / interventions guided by the NLP   model . For instance , all four of the aforementioned   allocational harm papers measure and/or mitigate   gender bias with respect to an NLP - based occupa-   tion classifier ( De - Arteaga et al . , 2019 ; Prost et al . ,   2019 ; Romanov et al . , 2019 ; Zhao et al . , 2020 ) .   More specifically , these studies examine the allo-   cational harm of biased occupation classification   predictions on decisions that affect humans , specif-   ically whether an HR NLP system scraping web   bios classifies individuals as relevant or not for a po-   sition . Our work builds on the nascent allocational   harm literature by examining ten downstream tasks   related to five data sets spanning Twitter , Reddit ,   forum , and survey response text .   2.2 Intersectional Biases   Intersectional biases arising as a result of interact-   ing demographics have been studied in the broader   machine learning literature , either from a theoret-   ical perspective ( Kearns et al . , 2018 ; Yang et al . ,   2020 ) , or in the context of facial recognition ( Buo-   lamwini and Gebru , 2018 ) . In NLP , Tan and Celis   ( 2019 ) evaluate and reveal important intersectionalbiases in contextualized word embedding models   such as BERT and GPT-2 . However , in their study ,   intersectional biases are evaluated using the word   association test with an emphasis on representa-   tional harm - it remains unclear how intersectional   biases affect allocational harm in downstream NLP   tasks . Subramanian et al . ( 2021 ) looked at intersec-   tional biases of classification models specifically   designed for unbiased prediction , but do not evalu-   ate embedding debiasing techniques . We build on   the emergent literature on intersectional biases by   assessing datasets encompassing up to five demo-   graphic dimensions , in conjunction with state - of-   the - art word embeddings and debiasing methods ,   on downstream tasks where biased predictions can   lead to allocational harm ( § 3.1 ) .   2.3 Debiasing   Pretrained word embeddings , including static word   embeddings such as GloVe and contextualized   word embeddings such as BERT , contain human-   like biases and stereotypical associations ( Caliskan   et al . , 2017 ; Garg et al . , 2018 ; May et al . , 2019 ) .   A burgeoning body of NLP work has explored de-   biasing techniques to mitigate biases in pretrained   word embeddings . One body of work has focused   on debiasing static word embeddings ( Bolukbasi   et al . , 2016 ; Zhao et al . , 2020 , 2018 ; Kaneko and   Bollegala , 2019 ; Ravfogel et al . , 2020 ) .   Given the wide adoption of transformer - based   contextualized embedding models , recent research   has investigated bias mitigation in models such as   BERT and RoBERTa ( Zmigrod et al . , 2019 ; Web-   ster et al . , 2020 ; Garimella et al . , 2021 ; Kaneko and   Bollegala , 2021 ; Guo et al . , 2022 ) . Existing meth-   ods for debiasing static and contextualized embed-   dings have alleviated representational harm along   demographic dimensions such as gender . How-   ever , Gonen and Goldberg ( 2019 ) raised the con-   cern that some debiasing strategies geared towards   static word embeddings simply cover up the biases -   which can resurface . Moreover , the seemingly debi-   ased static embeddings often do not alleviate biases   in downstream NLP prediction tasks ( Goldfarb-   Tarrant et al . , 2021 ) . The extent to which state - of-   the - art debiasing methods can mitigate downstream   intersectional biases remains unclear . This is pre-   cisely one of the gaps our study attempts to shed   light on.3600   3 Data , Models , Experiments   As previously depicted in Figure 1 , our experimen-   tal setup is as follows . We assess predictive perfor-   mance and fairness across five datasets spanning   ten dependent variables / tasks and five demographic   dimensions . We train three models ( GloVe , BERT ,   and RoBERTa ) as our prediction and fairness base-   lines . We then debias the input embeddings for   these models ( Ravfogel et al . , 2020 ; Zmigrod et al . ,   2019 ; Kaneko and Bollegala , 2021 ) and re - train   them to compare the performance . Details of the   data , models , and evaluation metrics are below .   3.1 Data   We examine five datasets ( Table 1 ) across several   NLP tasks : psychometric dimension prediction ,   hate speech identification , personality detection ,   and sentiment analysis . The psychometric data   set ( Abbasi et al . , 2021 ) consists of free - text re-   sponses on four psychometric dimensions : subjec-   tive health literacy , numeracy , anxiety , and trust in   doctors . These free - text responses were then linked   to survey - based psychometric scores also provided   by the participants ( serving as gold - standard nu-   meric response labels ) . The data also includes self-   reported demographics for each individual : age ,   race , gender , income , and education level . This   data set was collected using crowd workers from   Amazon Mechanical Turk and Qualtrics .   Similarly , the Five Item Personality Inventory   ( FIPI ) and Myers – Briggs Type Indicator ( MBTI )   datasets include free text responses to estimate one   of the FIPI or MBTI personality traits ( Gjurkovi ´ c   et al . , 2021 ) . In particular , due to space constraints ,   we focus on the MBTI traits of perceiving and   thinking , and the FIPI traits of extraverted and sta-   ble . For FIPI , available demographics are gender ,   race , age , income , and education . For MBTI , self-   reported gender and age are available . The AskAP - atient dataset ( Limsopatham and Collier , 2016 ) is   taken from web forums and has labeled sentiment ,   along with gender and age information .   The Multilingual Twitter Corpus ( MTC ) hate-   speech dataset contains labeled Twitter messages   for the task of hate speech detection ( Huang et al . ,   2020 ) . The dataset also contains inferred author   demographic factors . We use three demographics :   gender , race , and age .   The Psychometrics , FIPI , AskAPatient , and   MBTI tasks are all relevant from an allocational   harms perspective . Biases in predictions for   healthcare - related variables ( Psychometrics ) , or   personality type variables ( MBTI , FIPI ) can affect   an individual ’s health care plan , personalized in-   terventions , job prospects , etc . Biased predictions   for drug rating sentiment can affect which drugs a   future user chooses to take .   3.2 Models and Debiasing Methods   In the experiments , we considered several different   text classification models . We used a word convo-   lutional neural network ( CNN ) model , initialized   with GloVe embeddings . We also considered two   transformer - based contexualized embedding mod-   els : BERT and RoBERTa .   CNN We trained a word convolutional neural net-   work ( CNN ) model , initialized with GloVE embed-   dings . The model consists of 3 concatenated CNN   layers with kernel size of 1 , 2 and 3 respectively .   Each layer has a filter size of 256 , rectified linear   unit ( ReLU ) activation , L2 regularization ( 0.001 ) ,   and global max pooling . The models were trained   for 35 epochs with a batch size of 32 and learning   rate of 1e .   Debiased - CNN We debiased the GloVe model   using ( Ravfogel et al . , 2020 ) . We kept all param-   eters the same as in the original paper based on3601their publically available implementation . The   projection matrix was learned over 50 epochs .   BERT and RoBERTa We fine - tuned BERT   and RoBERTa on downstream prediction   tasks . We used BERT - base - uncased and   RoBERTa - base model loaded from the trans-   formers library . We fine - tuned BERT and   RoBERTa model for five epochs using the   following hyperparameters : a batch size to 32 ,   learning rate of 1e , weight decay of 0.01 . We   saved the final model that achieves the lowest loss   on validation set .   Debiased - BERT and Debiased - RoBERTa We   debiased BERT and RoBERTa using ( Kaneko and   Bollegala , 2021 ) . We obtained the gender word   lists and stereotype word lists . We used News-   commentary - v15 corpusas the external corpus   to locate sentences where the gender and stereo-   type words occur and then debias . All BERT or   RoBERTa layers are debiased at the token level ,   and the debiasing loss weight is set to 0.8 . The   model is fine - tuned for three epochs used the fol-   lowing hyperparameters : a batch size of 32 and   learning rate of 5e .   Training Details For each dataset we trained us-   ing five - fold cross validation , so that for each exam-   ple in each dataset , we could generate predictions   as unseen test data . Each test fold was then concate-   nated for a given model for fairness calculations .   All models were trained on the same data with   hyperparameter tuning . All prediction models , de-   biasing models are trained on a NVIDIA GeForce   RTX 3090 GPU card , with 11.2 CUDA version .   Debiasing Strategy Static word embeddings   ( GloVe , Pennington et al . , 2014 ) were debiased us-   ing WordED ( Ravfogel et al . , 2020 ) . This method   iteratively learns a projection of embeddings that   removes the bias information with minimal impact   on embedding distances .   Contextualized word embedding models BERT   and RoBERTa were debiased using ContextED   ( Kaneko and Bollegala , 2021 ) , which has been   shown to work well at removing gender - bias en-   coded in embeddings . This method uses pre-   defined word lists to identify sentences that con-   tain the gendered or stereotype words , and thenfine - tunes the pretrained model parameters by en-   couraging gendered and stereotype words to have   orthogonal representations .   We also assessed two alternative debiasing meth-   ods for the contextualized word embedding models :   counterfactual data augmentation ( CDA ) ( Zmigrod   et al . , 2019 ) and Dropout ( Webster et al . , 2020 ) .   CDA augments the training corpora with counter-   factual data so that the language model is pretrained   on gender - balanced text . Dropout mitigates gen-   der biases by increasing the dropout rate in the   pretrained models . Therefore , the debiasing meth-   ods in our experiments represent different ways of   mitigating biases : dataset level ( CDA ) , debiasing   during pretraining ( ContextED and Dropout ) , and   post - tuning debiasing ( WordED ) .   3.3 Evaluation   There are several definitions of fairness in the liter-   ature ( Mehrabi et al . , 2021 ) , each with correspond-   ing methods of assessment . In this work we rely   on two prior metrics from the literature , and also   present a new metric , adjusted disparate impact , to   account for base rates in the dataset .   Disparate Impact One of the most common fair-   ness assessments is disparate impact ( DI , Friedler   et al . , 2019 ) . DI measures the inequality of posi-   tive cases between privileged and non - privileged   groups for a particular demographic . DI comes   from from the legal field , where certain regulations   require DI be above a threshold of 0.8(or below   1.2 in the inverse case ) . For true labels y , predicted   labels ˆy , and relevant demographic group A :   DI = p(ˆy= 1|A= 0 )   p(ˆy= 1|A= 1)(1 )   Where A= 0refers to the protected group and   A= 1 refers to the privileged group . A DI ra-   tio of 1indicates demographic parity , where the   rates of positive predictions are consistent across   demographic classes : P(ˆy= 1|A= 0 ) = P(ˆy=   1|A= 1 ) ( Mehrabi et al . , 2021 ) .   Statistical Parity ( SP ) Subgroup Fairness Re-   cent theoretical work on intersectional biases also   assesses demographic parity , where the score com-   pares group - specific rates to the global rate in the   dataset instead of a comparison between privileged   and protected classes ( Kearns et al . , 2018 ):   p(A = g)× |p(ˆy= 1)−p(ˆy= 1|A = g)|(2)3602This value is compared to an acceptability pa-   rameter λto assess fairness . As this method was   proposed for the intersectional case , it gives a way   to identify the upper - bound of the fairness violation   in a dataset ( Yang et al . , 2020 ):   FV= max|TPR−TPR| ( 3 )   Where Gis the set of demographic groups un-   der consideration for analysis , TPRis the true   positive rate of the classifier on the instances in   g , and TPRis the overall true positive rate for   the classifier on the dataset . Prior work considered   the average violation across groups ( Subramanian   et al . , 2021 ) , but for the purposes of this study we   are interested in a worst case analysis .   Adjusted Disparate Impact We propose re-   weighting DI to account for differences in base   rates . Adjusted DI ( ADI ) divides DI by the base   rate ratio for the protected and privileged groups :   DI=,ADI =   Note that the disparate impact metrics are not de-   fined for cases where there are no positive instances   for either the protected or privilege classes in the   data , or when there are no positive predictions for   the privileged class ( due to zero division ) . There-   fore , we use additive smoothing when calculating   DI and adjusted DI ( Zhai and Lafferty , 2004 ) .   Intersectional Fairness To assess intersectional   fairness we enumerated all combinations for each   n - demographic scenario ( e.g. , 2 - demographic , 3-   demographic , etc . ) . We set a reference demo-   graphic , specifically gender , because of the prior   work on debiasing word embeddings for gender   ( Bolukbasi et al . , 2016 ; Gonen and Goldberg , 2019 ;   Kaneko and Bollegala , 2021 ) . For intersectional   cases , we calculated DI and FV for all possible   combinations of demographics that included gen-   der . For example , the 2 - demographic case for   the psychometrics dataset involves calculating DI   and FV for the following protected groups : older   women , lower education women , lower income   women , and non - white women . Our privileged   groups are the negations of the protected groups ,   e.g. , for the above case they are younger men ,   higher education men , higher income men , and   white men . By considering disjoint demographic   groups , we avoid cumulative effects of merging   fairness results from individual demographics dur-   ing the intersectional phase . We follow the same   procedure for enumerating protected groups for the3- and 4 - demographic cases . For 5 - demographics   we consider all demographics together . For all   models and datasets , we calculated fairness and   performance metrics . For performance , we report   mean squared error ( MSE ) , Pearson ’s r , F1 , and   area under the receiver operating curve ( AUC ) . For   fairness , we report adjusted DI and fairness viola-   tion ( FV , § 3.3 ) .   4 Results and Discussion   Figure 2 shows the ADI results for BERT and   GloVe using ContextED and WordED for debias-   ing , respectively . In most cases , particularly for   BERT , disparate impact scores for gender alone are   in a reasonable range ( within 10 % ) . For GloVe ,   we do observe high gender ADI on Anxiety and   Thinking . However , as the number of demograph-   ics under consideration grows , the range of ADI   scores widens . While debiasing the word embed-   dings typically helps to reduce the unfairness for   the target demographic ( e.g. , gender ) , in the in-   tersectional cases the model still performs poorly .   There are similar trends in FV scores as the num-   ber of demographics increases , with the extent of   violations often increasing by a factor of 3x to 10x   as intersections increase ( Table 2 ) .   In some cases the intersectional disparities are   extreme . On the BERT models , the ratio of posi-   tive Numeracy predictions for the protected class   is three - to - one compared to the privileged class .   In the other direction , for 3 - demographics , hate-   speech detection positive predictions are signifi-   cantly less likely for the protected group than the   privileged group . This is consistent with prior hate-   speech detection work that has shown large ( ab-   solute value ) fairness gaps between protected and   privileged groups ( e.g. , Liu et al . , 2021 ) .   In most cases , trends are consistent between the   BERT and GloVe models ( e.g. , Extraverted , Nu-   meracy , Perceiving ) . Some counterexamples are   the Trust and Anxiety tasks . Here model choice   impacts the direction of bias . As more demograph-   ics are considered , the GloVe model skews more   unfair against the protected group , while the BERT   model remains mostly fair , skewing slightly unfair   against the privileged group . Higher trust in physi-   cians is associated with better well - being and lower   anxiety when visiting a doctor ( Netemeyer et al . ,36033604   2020 ) ; disparate predictions can lead to missed in-   terventions for trust - increase and anxiety reduction   across demographic groups . Though not depicted   in the main paper , plots for RoBERTa show similar   trends to those observed for BERT while debiasing   with ContextED ( see Appendix A ) .   Results are similar when looking at alternate   BERT debiasing methods beyond ContextED ,   namely CDA and Dropout ( Figure 3 ) . These find-   ings on the Anxiety , Literacy , Numeracy , and Trust   tasks suggest that debiasing at the dataset , embed-   ding pretraining , and post - tuning levels leads to   similar increases in unfairness as the number of   demographic intersections considered increases .   Collectively , the results underscore the alloca-   tional harm implications of NLP models on several   downstream tasks - ones that even well - designed   and well - intentioned debiasing strategies can not   overcome . This can be problematic in the era of   personalized marketing and precision health , with   NLP - based persona - generation playing a biggerrole . For tasks like numeracy and literacy , this can   affect how a patient is treated by a medical staff   during a hospital visit ( i.e. , a false positive high   literacy prediction for a person who has trouble   understanding his or her medical record ) . For the   personality indicators , inconsistent predictions may   lead to biased decisions in the workplace ( e.g. , a   manager looking to form a team of extroverts ) .   5 Conclusion   In this work we present a comprehensive bench-   marking analysis of fairness for sequence predic-   tion models . We also look at known debiasing   methods for these models and show that while the   debiased versions maintain predictive performance   ( as expected ) , they do not help with mitigating bi-   ases . While most models are relatively fair when   looking at a single demographic characteristic , ac-   counting for intersectional groups leads to less fair   models and wider ranges of bias because of the3605   combinatorial considerations of the intersectional   groups . It is our hope that this benchmarking en-   courages future work into mitigating intersectional   biases , and also to collect more demographic infor-   mation when creating new datasets .   Acknowledgement   Members of Notre Dame ’s Human - centered Ana-   lytics Lab ( HAL ) were funded in part through U.S.   NSF grant IIS-2039915 .   References360636073608A Appendix : RoBERTa Results   Figure 4 shows results of our benchmarking experiments for RoBERTa . The trends of degrading perfor-   mance are consistent with the results in BERT.3609