  Xin Sun Houfeng Wang   MOE Key Lab of Computational Linguistics , School of Computer Science , Peking University   { sunx5,wanghf}@pku.edu.cn   Abstract   Modern writing assistance applications are   always equipped with a Grammatical Error   Correction ( GEC ) model to correct errors in   user - entered sentences . Different scenarios   have varying requirements for correction be-   havior , e.g. , performing more precise correc-   tions ( high precision ) or providing more can-   didates for users ( high recall ) . However , previ-   ous works adjust such trade - off only for se-   quence labeling approaches . In this paper ,   we propose a simple yet effective counterpart   – Align - and - Predict Decoding ( APD ) for the   most popular sequence - to - sequence models to   offer more flexibility for the precision - recall   trade - off . During inference , APD aligns the   already generated sequence with input and   adjusts scores of the following tokens . Ex-   periments in both English and Chinese GEC   benchmarks show that our approach not only   adapts a single model to precision - oriented and   recall - oriented inference , but also maximizes   its potential to achieve state - of - the - art results .   Our code is available at https://github .   com / AutoTemp / Align - and - Predict .   1 Introduction   Modern writing assistance applications ( e.g. , Mi-   crosoft Office Word , Google Docsand Gram-   marly ) always contain Grammatical Error Correc-   tion ( GEC ) modules ( Ge et al . , 2018 ; Omelianchuk   et al . , 2020 ; Stahlberg and Kumar , 2021 ) to cor-   rect errors in user - entered sentences . Such appli-   cations usually require GEC models to perform   different correction tendencies and behaviors ac-   cording to practical scenarios and user preferences   ( Chen et al . , 2020 ) . For instance , as shown in Ta-   ble 1 , conservative GEC models provide precise   corrections with high confidence and avoid unnec-   essary edits for better user experience . In contrast ,   Table 1 : Examples of corrections generated by the con-   servative ( precision - oriented ) and aggressive ( recall-   oriented ) GEC models . The rewritten tokens are within   the blue blocks . Conservative GEC tends to adhere to   the input sentence , while aggressive GEC provides more   edited spans .   aggressive GEC models could provide more cor-   rection candidates to users or a following decision   system for further measurement .   Although recent studies witness the tremen-   dous success of sequence - to - sequence ( seq2seq )   generation approaches in GEC , the trade - off of   these two tendencies still largely depends on the   pre - defined model architecture , training data and   labor - consuming post - processing ( Liang et al . ,   2020 ) . Hotate et al . ( 2020 ) proposes a diverse   local beam search method to obtain diverse cor-   rections but is specifically designed for copy-   augmented GEC models and can not perform   precision - oriented decoding . Instead of seq2seq   generation , Omelianchuk et al . ( 2020 ) proposes an   efficient sequence tagger for GEC by token - level   transformations to map input tokens to target cor-   rections . They introduce two confidence thresholds   for inference to force the model to perform more   precise corrections . Chen et al . ( 2020 ) first identi-   fies incorrect spans with a tagging model and then   sets a probability threshold to adjust the precision-   recall trade - off .   Inspired by these lightweight tweaking meth-   ods for sequence labeling approaches , we pro-   pose a simple yet effective counterpart – Align-686   and - Predict Decoding ( APD ) for the seq2seq GEC   models . Our approach could not only adapt the   precision - recall trade - off of a single seq2seq GEC   model to various application scenarios , but also be   used as a simple trick to improve its overall F   performance .   During inference , APD aligns the already gener-   ated sequence with the input tokens to specify the   position which the model has reached . By tweak-   ing the score of the next token , the model changes   its preference between copy and edit operation ,   leading to a different degree of adherence to the   input sentence . The experimental results in both   English and Chinese GEC benchmarks show our   approach could effectively control the precision-   recall trade - off and achieve state - of - the - art results .   Our contributions are summarized as follows :   •We propose a novel and simple decoding ap-   proach , allowing us to adapt the precision-   recall trade - off of a seq2seq GEC model .   •Our methods achieve state - of - the - art results in   both English and Chinese GEC benchmarks .   2 Align - and - Predict Decoding   Beam search ( Lowerre , 1976 ; Och and Ney , 2004 ;   Sutskever et al . , 2014 ) is a widely used algorithm   for decoding sequences on all generation tasks ,   such as translation ( Vaswani et al . , 2017 ; Ott et al . ,2018 ) , dialogue ( Kulikov et al . , 2019 ) , etc . Multi-   ple modifications to beam search that force the out-   puts to include pre - defined lexical constraints ( i.e. ,   words and phrases ) have been proposed ( Hokamp   and Liu , 2017 ; Hu et al . , 2019 ) .   Fortunately , the input and output sentences of   GEC overlap significantly and the input tokens are   natural constraints for correction generation . This   assumption is an objective characteristic of GEC   and has been made in many previous works ( Zhao   et al . , 2019 ; Malmi et al . , 2019 ; Stahlberg and Ku-   mar , 2020 ; Sun et al . , 2021 ) . Thus , we propose a   novel decoding approach – Align - and - Predict De-   coding ( APD ) , which leverage the characteristic of   GEC to adjust behavior and tendencies of inference .   The overview of APD is shown in Figure 1 .   Given an input sentence x= ( x , . . . , x ) , we   maintain Khypotheses at the time step tduring   inference as beam search does :   H=   h , ... , h 	  =    ( y , ... , y ) , ... , ( y , ... , y ) 	  ( 1 )   where h , i∈[1 , K]denotes the i - th hypothesis   withtalready generated tokens .   Since the output of GEC is highly constrained   by the input sequence , we assume that hshould   be almost the same as part of the input sentence   x. Then , we match the suffix of each hypothesis   hwith the input xto identify the position which   the inference has reached . If there exists a unique687substring x , ... , x(j≥0)of the input xiden-   tical to the suffix y , ... , y , the next token of the   hypothesis his very likely to be x , which we   store in the set N.Formally ,   N= (   { x } ∃ ! k , x = y ;   ∅ otherwise.(2 )   As beam search does , we expand current hy-   potheses and construct possible candidates for the   next time step t+ 1with all tokens in the vocab-   ulary . The candidate ˆhof the i - th hypothesis is   obtained as follows :   ˆh = C(h , v ) = ( y , ... , y , v ) ( 3 )   where we concatenate the already generated se-   quence hwith any token vin the vocabulary .   The corresponding score is calculated by :   S ( ˆh ) = S ( h )   + w·logP(v|y , ... , y)(4 )   where Pis the output distribution predicted by the   seq2seq GEC model and wis a penalty factor   that depends on whether the token vis the next   token xat the aligned position . Specifically ,   w= (   λ v ∈N   1.0v̸∈N(5 )   where λis a hyperparameter to control the adher-   ence to the input sequence . If λ > 1.0 , inference   penalizes the score of the original next token and   tends to perform modification;ifλ < 1.0 , it is   likely to copy the token . The new hypotheses are   selected by :   H= arg topK(S ( ˆh ) ) ( 6 )   3 Experiments   3.1 Experimental Setting   We conduct our experiments in the restricted train-   ing setting of BEA-2019 GEC shared task ( Bryant   et al . , 2019 ) , with Lang-8 Corpus of Learner En-   glish ( Mizumoto et al . , 2011 ) , NUCLE ( Dahlmeier   et al . , 2013 ) , FCE ( Yannakoudakis et al . , 2011 ) and   W&I+LOCNESS ( Granger ; Bryant et al . , 2019 )   as training data . We use BEA-2019 development   set to choose the best model and select λbetween   0.1and2.5with0.05intervals based on F , F   andFfor precision - oriented , balance and recall-   oriented models , respectively . We evaluate the   performance on BEA-2019 test set by ERRANT   ( Bryant et al . , 2017 ) .   To validate the effectiveness of our approach for   the state - of - the - art seq2seq GEC models , we follow   previous work ( Grundkiewicz et al . , 2019 ; Zhang   et al . , 2019 ) to construct 300 M error - corrected sen-   tence pairs in the same way for pretraining . We   use Transformer ( big ) model ( Vaswani et al . , 2017 )   in the fairseqand a vocabulary with size of 32 K   Byte Pair Encoding ( Sennrich et al . , 2016 ) tokens .   We also use one of the models trained by the prior   work ( Sun et al . , 2021 ) which utilizes a pretrained   model BART ( Lewis et al . , 2019 ) to initialize a   GEC model which has a 12 - layer encoder and 2 - 688   layer decoder , following Li et al . ( 2021 ) .   In addition , we evaluate our approach on   NLPCC-18 Chinese GEC shared task ( Zhao et al . ,   2018 ) by official Max - Match scorerto prove our   approach is language - independent . We use a base   Transformer model and construct a character - level   vocabulary consisting of 7 K tokens . We train the   model using MaskGEC ( Zhao and Wang , 2020 ) .   The models decode with a beam size of 5 . We   show more details of training in the Appendix .   3.2 Experimental Result   As shown in Table 2 , our approach can control   the precision - recall trade - off of inference for any   seq2seq GEC models by tweaking a single hyperpa-   rameter λ . After inference tweaks , pretrained GEC   models could achieve much better precision with   comparable or even better overall performance . For   instance , our approach increases the precision of   pretrained models by over 10 points . In contrast ,   the recall improvement is smaller than precision ,   i.e. , an increment of about 6 points for pretrained   models , since it depends mainly on error - corrected   patterns that the model itself has learned . The final   system has achieved competitive performance ( 73.8   F ) and align - and - predict decoding improves it   to a new state - of - the - art result – 75.0 Fin the   BEA-2019 test set by a slight tendency towards   precision .   We further look into the performance of the pre-   trained seq2seq model over varying λin BEA-2019   development set , which is shown in Figure 2 . It is   obvious that the conservative inference ( λ < 1.0 )   with fewer edits tends to achieve higher precision   since it only provides the most confident correc-   tions , while recall of aggressive inference ( λ > 1.0 )   has an upper bound . This is because the motivation   of our approach is to simply display error - corrected   patterns that the model has learned with different   orientation rather than to improve its capability and   complement more patterns . Meanwhile , it is ob-   served that Fdoes not peak around λ= 1.0 ,   which makes it possible to adapt the precision-   recall trade - off for better overall performance .   As shown in Table 3 , our approach also performs   well in Chinese GEC , which demonstrates that it is   language - independent . We present concrete exam-689ples with different λin our validation set in Table 4 .   It is consistent with our intuition that with larger λ ,   the inference tends to heavily edit the input tokens ;   on the other hand , it adheres to the input sequence   with smaller λ .   4 Conclusion   We propose a novel language - independent decod-   ing approach to offer more flexibility to adjust the   precision - recall trade - off of inference for seq2seq   GEC models , making it adaptive to various real-   world application scenarios . It can not only adapt   a single model to precision - oriented and recall-   oriented inference , but also be used as a simple   trick for better overall performance . On both En-   glish and Chinese GEC benchmarks , our approach   further improves the state - of - the - art seq2seq GEC   model by precision - recall trade - off . In the future ,   we plan to apply it to other sentence rewriting tasks ,   such as paraphrasing and style transfer .   Acknowledgments   We thank all the reviewers for their valuable   comments to improve our paper . We thank Tao   Ge in Microsoft Research Asia for the valu-   able suggestions . The work is supported by Na-   tional Natural Science Foundation of China un-   der Grant No.62036001 and PKU - Baidu Fund   ( No.2020BD021 ) . The corresponding author of   this paper is Houfeng Wang .   References690691   A Hyper - parameters   The hyper - parameters for Chinese GEC are listed   in Table 5 . The hyper - parameters of training the   models for English GEC are listed in Table 6 and   Table 7.692B Efficiency   Table 8 shows the total latency of the seq2seq   model ( w/ pretraining ) under various batch sizes .   Our approach incurs about 5 % extra latency in the   online inference setting ( i.e. , batch size=1 ) and is   suitable for practical GEC systems.693