  Pengcheng He , Baolin Peng , Song Wang ,   Yang Liu , Ruochen Xu , Hany Hassan Awadalla , Yu Shi , Chenguang Zhu ,   Wayne Xiong , Michael Zeng , Jianfeng Gao , Xuedong HuangMicrosoft Azure AIMicrosoft Research   penhe@microsoft.com   Abstract   This paper presents Z - Code++ , a new pre-   trained language model optimized for abstrac-   tive text summarization . The model extends   the state of the art encoder - decoder model us-   ing three techniques . First , we use a two - phase   pre - training process to improve model ’s perfor-   mance on low - resource summarization tasks .   The model is first pre - trained using text cor-   pora for language understanding , and then is   continually pre - trained on summarization cor-   pora for grounded text generation . Second , we   replace self - attention layers in the encoder with   disentangled attention layers , where each word   is represented using two vectors that encode its   content and position , respectively . Third , we   use fusion - in - encoder , a simple yet effective   method of encoding long sequences in a hierar-   chical manner . Z - Code++ creates new state of   the art on 9 out of 13 text summarization tasks   across 5 languages . Our model is parameter-   efficient in that it outperforms the 600x larger   PaLM on XSum , and the finetuned 200x   larger GPT3 on SAMSum . In zero - shot   and few - shot settings , our model substantially   outperforms the competing models .   1 Introduction   Text summarization aims at producing a concise   and fluent summary while preserving salient con-   tent and overall meaning of the source docu-   ments . It has been applied in a wide range of   real - world applications , e.g. , summarizing Web   search results for interactive information retrieval   ( Gao et al . , 2022 ) and generating medical sum-   maries from doctor - patient conversation transcripts   ( Zhang et al . , 2021 ) .   While the extractive approach is the dominant   approach in commercial systems due to its simplic-   ity and effectiveness ( Allahyari et al . , 2017 ) , the   abstractive approach is getting more attention in   the research community as neural language models   are used ( e.g. , Rush et al . , 2015 ; Nallapati et al . ,2016 ; Chopra et al . , 2016 ; Liu and Lapata , 2019b , a ;   Pasunuru et al . , 2021 ) . Compared to the extractive   approach where a summary is constructed using   extracted sentences , abstractive summarizers para-   phrase the idea of the source documents in a new   form , and have a potential of generating more con-   cise and coherent summaries .   However , good abstractive summarizers are   harder to develop since we have to deal with prob-   lems like semantic representation , inference and   low - resource text generation , which are more chal-   lenging than sentence extraction . Recently , large-   scale pre - trained language models ( PLMs ) such   as PEGASUS ( Zhang et al . , 2020 ) , GPT ( Radford   et al . , 2019 ; Brown et al . , 2020 ) , T5 ( Raffel et al . ,   2020 ) , have been applied for abstractive summa-   rization . While these models can produce surpris-   ingly fluent text , the generated summaries often   contain factual inconsistencies , caused by distorted   or fabricated facts about the source documents ,   which is known as the hallucination problem ( Kry ´ s-   ci´nski et al . , 2019 ; Celikyilmaz et al . , 2020 ; Ji et al . ,   2022 ) . In addition , since the amount of text in the   source documents can be very large , it is expen-   sive to train an end - to - end abstractive model ( e.g. ,   an encoder - decoder transformer model ) given the   memory constraints of current hardware and the   latency constraints of applications such as online   document summarization for interactive informa-   tion retrieval . Therefore , a two - stage approach is   widely used , where a subset of document sentences   is coarsely selected using an extractive summa-   rizer , and an abstractive summarizer generates the   summary conditioning on the extraction ( Liu and   Lapata , 2019b ) . This approach is sub - optimal in   that salient information might be missed in the ex-   traction .   In this paper , we propose a new encoder-   decoder PLM optimized for abstractive summa-   rization , Z - Code++ , which significantly extends   Z - Code ( Wang et al . , 2020 ) , a state - of - the - art PLM5095developed for machine translation , as follows .   First , Z - Code++ is pre - trained on web text us-   ing two tasks , replaced token detection ( RTD ) and   corrupted span prediction ( CSP ) . RTD uses a gen-   erator to generate ambiguous corruptions and a   discriminator to distinguish the ambiguous tokens   from the original inputs ( Clark et al . , 2020 ) . RTD   is proved to be more sample - efficient than the clas-   sic mask language modeling ( MLM ) task in learn-   ing text representations for language understanding   ( Bajaj et al . , 2022 ; Hao et al . , 2021 ) . In CSP , a   consecutive segment of tokens are corrupted and   the model is learned to predict the corrupted spans   using all the uncorrupted tokens in the original in-   put ( Raffel et al . , 2020 ; Joshi et al . , 2020 ) . CSP   can be viewed as a generalized form of gap sen-   tences generation ( GSG ) , a pre - training task tai-   lored to abstractive summarization ( Zhang et al . ,   2020 ) , where the spans are entire sentences . CSP   outperforms GSG in our experiments . In the second   phase of grounded pre - training ( Peng et al . , 2022 ) ,   the model is continually trained on summarization   corpora of documents - summary pairs to better sup-   port low - resource fine - tuning to downstream sum-   marization tasks that require the model to produce   summaries grounded in source documents . We find   in our experiments that grounded pre - training sig-   nificantly boosts the results on downstream tasks   in low - resource settings .   To handle the large input documents , we use   fusion - in - encoder ( FiE ) , a simple yet effective   method of encoding long sequences in a hierar-   chical manner . It works by first splitting the input   sequence into small chunks , applying attention on   each chunk locally to get the chunk representation ,   and applying attention globally on the concatenated   chunk representations to get the representation of   the original input .   In addition , we replace the self - attention layer in   the encoder with the disentangled attention ( DA )   layer ( He et al . , 2020 , 2021 ) , where each word   is represented using two vectors that encode its   content and position , respectively , and the atten-   tion weights among words are computed using dis-   entangled matrices on their contents and relative   positions , respectively . DA is motivated by the ob-   servation that the attention weight of a word pair   depends on not only their contents but their relative   positions . For example , the dependency between   the words “ deep ” and “ learning ” is much stronger   when they occur next to each other than when theyoccur in different sentences . We show in our exper-   iments that DA leads to a more effective abstractive   summarizer .   For evaluation , we have pre - trained two Z-   Code++ models on English data and multi - lingual   data , respectively . The English model is trained   using 160 G English text data and the vocabulary   of DeBERTaV2 ( He et al . , 2020 ) . The multi-   lingual model is trained on mC4 corpus which is   the same as mT5 . These models are evaluated on   13 text summarization tasks across 5 languages ,   and create new state of the art on 9 tasks . As of   May 6th , 2022 , Z - Code++ sits atop of the XSum   leaderboard , surpassing UL2 , T5and PE-   GASUS . It is worth noting that our models are   very parameter - efficient . For example , Z - Code++   outperforms PaLM , which is 600x larger in   model parameters , on XSum , and outperforms a   fine - tuned , 200x larger , GPT3on SAMSum . In   zero - shot and few - shot settings , our models outper-   form more substantially the competing models .   2 Z - Code++   This section describes three modeling techniques   we have exploited to optimize Z - Code++ for ab-   stractive summarization , including two - phase pre-   training , disentangled attention , and long sequence   encoding .   2.1 Two - Phase Pre - Training   The two - phase pre - training , which includes the   language model pre - training andgrounded pre-   training phases , is inspired by the GODEL recipe   ( Peng et al . , 2022 ) that has been proposed to pre-   train language models for grounded text generation   tasks , such as dialog response generation and ab-   stractive question - answering .   In the language model pre - training phase , Z-   Code++ is pre - trained using two language model-   ing tasks , replaced token detection ( RTD ) ( Clark   et al . , 2020 ) and corrupted span prediction ( CSP )   ( Raffel et al . , 2020 ; Joshi et al . , 2020 ) . As il-   lustrated in Figure1 ( Left ) , RTD uses a generator   trained with MLM to generate ambiguous tokens   to replace tokens in the original input X , and a   discriminator to determine whether a token is from   Xor generated by the generator . Let θandθ   be the parameters of the generator and the discrimi-   nator , respectively . The MLM loss of the generator   is written as5096(1 )   where ˜Xis the input to the generator by randomly   masking 15 % tokens in original input X. The   input sequence of the discriminator is constructed   by replacing the masked tokens , x , iPC , with the   tokens , ˜x , sampled by the generator as(2 )   Then the discriminator is trained using the loss(3 )   where 1p¨qis the indicator function and ˜Xis the   input to the discriminator constructed via ( 2 ) . In   ELECTRA ( Clark et al . , 2020 ) , the discriminator   and generator share token embeddings and their pa-   rameters are optimized via MLM and RTD jointly   asL“L`λL. However , as pointed out in   ( He et al . , 2021 ) , such embedding sharing makes   training highly inefficient since MLM and RTD   pull token embeddings into very different direc-   tions , creating the “ tug - of - war ” dynamics . MLM   tries to map the tokens that are semantically simi-   lar to the embedding vectors that are close to eachother . RTD , on the other hand , tries to discriminate   semantically similar tokens , pulling their embed-   dings as far as possible to optimize the classifica-   tion accuracy . Thus , we use the method of gradient-   disentangled embedding sharing ( He et al . , 2021 )   by re - parameterizing the token embeddings of the   discriminator as   E“sgpEq`E , ( 4 )   whereEandEare the embedding parameters   of the discriminator and generator , respectively , sg   is the stop gradient operator which only allows gra-   dients propagation through E.Eis initialized   as a zero matrix . In each training pass , we first run   a forward pass of the generator to generate inputs   for the discriminator , and then a backward pass to   updateEwith respect to MLM . After that , we run   a forward pass for the discriminator using the in-   puts produced by the generator and run a backward   pass with respect to the RTD loss to update E   by propagating gradients only through E. After   model training , Eis added to Eand the sum is   saved as Ein the discriminator , as Equation 4 .   The CSP is widely used to optimize the encoder-   decoder PLMs such as T5 ( Raffel et al . , 2020 ) . As   illustrated in Figure 1 ( Right ) , given input string   X , we first select a continuous span Yby first5097randomly selecting a start position in Xand a span   with an average length of 3 . Then we replace the   selected span Ywith a sentinel token [ M ] . We   repeat the process until the replaced tokens amount   to 15 % of all tokens in X. Then , we feed the   corrupted input ˜X to the encoder . The encoder-   decoder model is then trained to recover the Y   from the context . The CSP loss is written as   L “ E¨   ˝´ÿlogp ´   Y|˜X , Yi¯˛   ‚ ( 5 )   If we restrict the corrupted span Yto a complete   sentence , CSP is equivalent to the GSG task which   simulates the process of extractive summarization   and is shown to be effective for training abstractive   summarizers ( Zhang et al . , 2020 ) . In this study ,   we find the that CSP , as a more general form of   GSG , works better across many natural language   understanding and generation tasks , including sum-   marization , as to be discussed in Section 3 .   Combining the pre - training tasks of MLM , RTD   and CSP , in the language model pre - training phase ,   Z - Code++ is optimized using the joint loss as L “   λL`λL`λL , where we set λ “   1 , λ“30 , λ“1 in our experiment .   In the second phase of grounded pre - training , Z-   Code++ is continually pre - trained on a collection of   summarization datasets , as shown in Table 1 , which   consist of documents - summary pairs pX , Yq , to   better support low - resource finetuning for down-   stream summarization tasks that require the model   to generate target summaries Ygrounded in source   documents X , as   ppY|Xq“źppy|y,¨¨¨ , y , Xq ( 6 )   Following T0 ( Wei et al . , 2021 ) , FLAN ( Sanh et al . ,   2022 ) , and GODEL ( Peng et al . , 2022 ) , we add   for each training pair pX , Yqa natural language   instruction of the summarization task , as illustrated   in the below example and in Table 1 . In our ex-   periment , we only apply grounded pre - training for   low - resource summarizations . Unless specified , we   apply the first phase Z - Code++ to downstream task   adaptation .   2.2 Disentangled Attention   Disentangled Attention ( DA ) is first used in De-   BERTa ( He et al . , 2020 , 2021 ) . DA is an extension   of the classic self - attention ( SA ) mechanism in that   DA represents each input word using two separate   vectors : one for the content and the other for the   position . Meanwhile , its attention weights among   words are computed via disentangled matrices on   both their contents and relative positions . The ex-   periments of DeBERTa shows that DA is more effi-   cient than SA to encode the positional dependency   in Transformer models . Z - Code++ adopts DA in   modeling . Our experiments show that DA leads to   a more effective abstractive summarizer .   2.3 Long Sequence Encoding   It is challenging to encode long sequence given   theOpNqmemory and computation complexity   of self - attention and DA . Various sparse attention   mechanisms have been proposed to alleviate the   problem . However , sparse attention often hurts   performance on short sequences due to the de-   crease of attention precision . Inspired by fusion - in-   decoder ( Izacard and Grave , 2020 ) and hierarchi-   cal transformer ( Liu and Lapata , 2019a ) , we pro-   pose fusion - in - encoder ( FiE ) , a simple but effective   mechanism to encode long sequences while retain-   ing high attention precision on short sequences .   FiE works by separating the Lencoder layers of   Z - Code++ into mlocal layers and nglobal lay-   ers . In each local layer , the hidden states of input   sequence are split into small chunk of size l(e.g .   256 or 512 ) , and self - attention ( or DA ) is only ap-   plied to those small chunks locally with a complex-   ity of Oplq . After local layer , the hidden states   of those small chunks are concatenated together   to form the representation of the long sequence .   Global layers are the same as original self - attention   ( or DA ) layers in encoder to fuse the local states   of small chunks . With FiE , the complexity of en-   coder is reduced from OpLNqtoOpmNl`nNq .   Both the local layers and fusion layers are initial-   ized with the corresponding weights of encoder5098   layers of Z - Code++ . Please check Appendix A.3   for a graphic illustration of FiE. In experiment , we   show that compared with LongT5 ( Guo et al . , 2021 )   which applies sparse attention that is specifically   optimized for summarization , Z - Code++ achieves   similar or better performance on long document   summarization tasks .   3 Experiment   3.1 Experiment Setups   Datasets We validate the effectiveness of Z-   Code++ on 11 representative summarization tasks ,   which are detailed in Table 2 . Among these   datasets , XSum ( Narayan et al . , 2018 ) , CN-   NDM ( See et al . , 2017 ) , NewsRoom ( Grusky   et al . , 2018 ) , and MultiNews ( Fabbri et al . , 2019 )   are news article summarizations , while SAM-   Sum ( Gliwa et al . , 2019 ) , MediaSum ( Zhu et al . ,   2021 ) , and Reddit TIFU ( Kim et al . , 2018 ) are   conversation - like summarization tasks . Follow-   ing LongT5 , we use MultiNews , MediaSum ,   arXiv ( Cohan et al . , 2018 ) and PubMed ( Cohan   et al . , 2018 ) to assess the long document summa-   rization capability . In addition , WikiLingua ( Lad-   hak et al . , 2020 ) and MLsum ( Scialom et al . , 2020 )   are used to evaluate the capacity of Z - Code++ on   multilingual summarization .   Implementation Details We have built our mod-   els following the same setting as T5 . For Z-   Code++ , there are 24 layers for the encoder   and 24 layers for the decoder with 1024 hidden   dimension sizes and 16 self - attention heads . Fol-   lowing DeBERTaV3 ( He et al . , 2021 ) , a 6 - layer   generator with the same structure as the encoder   is employed during the pre - training stage . Z-   Code++ is trained on 160 G data with a vo-   cabulary of size 128k . Our code is implemented   based on open sourced pytorchand DeBERTa . We pre - train Z - Code++ for 1 M steps with a   batch size of 2048 in Azure Machine Learning clus-   terwith 128 A-100 GPUS for 20 days . AdamW is   used as the optimizer in all experiments . For tasks   with an input length of more than 10k words , i.e. ,   arXiv and PubMed , Fusion - in - Encoder is used to   encode the document as described in 2.3 . For the   other standard summarization tasks with moderate   input length ( i.e. , less than 4k words ) we directly   feed the input document to the encoder .   For multilingual summarization , we have built   Z - Code++ with the same architecture but dif-   ferent training data and vocabulary . Specifically , Z-   Code++ is trained with mC4 data and a vocab-   ulary of size 250k , which are the same as mT5 ( Xue   et al . , 2021 ) . Following XLM ( Lample and Con-   neau , 2019 ) , CCMatrix ( Schwenk et al . , 2019 ) and   CCAligned ( El - Kishky et al . , 2019 ) , parallel data is   used to enhance the cross - lingual summarization of   Z - Code++ . Due to the limited computational   resource , Z - Code++ is trained with only 500B   tokens instead of 1 T tokens as that for mT5 train-   ing .   We use grid search to choose the grounded train-   ing and fine - tuning hyper - parameters based on val-   idation set , the parameter search range are listed in   appendix A.1 .   3.2 Experiment Results   3.2.1 Results on Standard English   Summarization Tasks   We first conduct experiments to compare the   performance of Z - Code++ with SOTA   and PEGASUS on 7 representative stan-   dard public English summarization datasets   with moderate document length , including   AESLC , SAMSum , XSUM , WikiHow , News-   Room , CNN / DailyMail(CNNDM ) , and Reddit   TIFU . Following ( Chowdhery et al . , 2022 ;   Gehrmann et al . , 2022 ) , for each dataset we re-5099   port the average F - measure ROUGE-2 score of 5   runs . Detailed F - measure of ROUGE-1 / ROUGE-   2 / ROUGE - L scores can be found in Appendix ? ? .   As listed in Table 3 , Z - Code++ achieves   substantial improvements over PEGASUS ,   which is a PLM optimized for abstractive summa-   rization , on 6 out of 7 tasks in terms of ROUGE-2   F - measure score . Specifically , on SAMSum , a   critical dialog summarization task , Z - Code++   outperforms GPT-3 that is extensively fine - tuned with LoRA(Hu et al . , 2021 ) even though   Z - Code++ has less than 1/175 parameters of   GPT-3 . Furthermore , Z - Code++ lifts SO-   TAs by 0.36 points on average . These results   demonstrate the effectiveness of Z - Code++ on   English document summarization tasks . Addi-   tionally , we observe that Z - Code++ outper-   forms PEGASUS on WikiHow , SAMSum ,   Reddit TIFU , and AESLC by a much larger margin   ( ą1 % ) than it does on XSum , CNNDM , and News-   Room . We speculate that PEGASUS is biased to   news - like tasks since it is heavily pre - trained on   large amounts of news data . In contrast , Z - Code++   is pre - trained on diverse web data and thus is more   adaptable for general - domain summarization tasks .   3.2.2 Results on Long Document   Summarization   We compare Z - Code++ to PEGASUS and LongT5 ,   which is optimized for long document summariza-   tion . Results in Table 4 show that Z - Code++   exceeds all the strong competitors on all long doc-   ument summarization datasets and lifts SOTA by   0.35 point on average . For FiE , which is used to   generate summaries for arXiv and PubMed , we   choose the chunk size l“256 , and choose the last5100   layer of encoder as fusion layer based on the exper-   iment results . Specifically , Z - Code++ outper-   forms LongT5with less than 1/3 of parameters .   These results demonstrate both the effectiveness   and flexibility of Z - Code++ by using Disentangled-   Attention to encode word dependencies .   3.2.3 Human Evaluation   As human evaluation is the most reliable measure-   ment of the quality of natural language generation   models , we submit the test results of XSum to the   leaderboard ( Khashabi et al . , 2021 ) which requires   human raters to compare the generated summaries   side by side with human written references . Please   check the paper of the leaderboad ( Khashabi et al . ,   2021 ) to get more details of human evaluation pro-   cess including instructions , dataset preparing , pay-   ments and demographics of the raters . We list thehuman evaluation results in Table5 . Z - Code++ out-   performs all the other models , e.g. , BART ,   PEGASUS , T5 , UL2(Tay et al . , 2022 ) ,   on the leaderboard in terms of human - overall score .   As the human evaluation score is an average of side-   by - side preference comparison scores , a score of   0.51 indicates that the annotators prefer the output   of Z - Code++ to the human written references . Fur-   ther more , while hallucination is one of the most   critical problems for abstractive summarization , Z-   Code++ does not suffer much , i.e. , 0.55 , among the   leaderbard . The human evaluation results validate   that Z - Code++ produces higher quality summaries   than other models .   3.2.4 Results on Multilingual Summarization   Following GEM - benchmark ( Gehrmann et al . ,   2021 ) , we evaluate the performance of Z-   Code++on multilingual summarization   with WikiLingua and MLSum . We compare Z-   Code++ with mT5 and mT5 . The   results of PaLM , a state of the art PLM , are   also listed in Table 6 . Compared with mT5 ,   Z - Code++ achieves substantially better perfor-   mance across all the tasks with only 1/3 parameters   and half training data . In addition , we observe a sig-   nificant performance gap between Z - Code++   and PaLM on WikiLingua , which is not sur-   prising due to the sharp difference in model size   and capacity . However , Z - Code++ surpasses5101PaLMon MLSum by a large margin , i.e. , 3.7 %   on MLSum(de ) , 2.8 % on MLSum(es ) , albeit Z-   Code++ has less than 1/500 parameters . We   believe that by scaling up Z - Code++ to a moderate   size ( e.g. , 10B ) , the performance gap on WikiLin-   gua would be mitigated . We leave it to future work .   3.2.5 Results on Low - Resource   Summarization   We explore how well knowledge learned in dif-   ferent pre - training stages can generalize to low-   resource summarization scenarios , i.e. zero / few-   shot evaluation . For the grounded pre - training   phase , we choose to include MediaSum , Multi-   News , NewsRoom , and WikiHow datasets . Cor-   responding instructions are listed in Table 1 . We   reckon that incorporating diverse datasets and in-   structions is beneficial , which we leave it to future   work . For the fine - tuning stage , following the set-   ting in Zhang et al . ( 2020 ) , we randomly select the   number of training data to 0 , 10 , 100 , and 1000 ,   and sample examples from XSUM , CNNDM , and   SAMSum , and then fine - tune Z - Code++ until no   significant improvement on the validation set is   observed . Note that 0 denotes zero - shot evalua-   tion . Table 11 presents the results . By fine - tuning   first - phase pre - trained model , Z - Code++ out-   performs T5 by more than 3 points on av-   erage . PEGASUS exceeds Z - Code++   when the number of training examples is less than   100 , which is foreseeable as PEGASUS is   pre - trained with a pseudo summarization objective .   However , Z - Code++ performs significantly   better than them when it is trained with more than   100 examples , showing strong generalization in the   few - shot setting . More importantly , with grounded   pre - training , Z - Code++ beats all the compet-   ing models by a large margin in both zero and   few - shot settings , outperforming PEGASUS   by 5.7/1.5/3.3 points on average . This suggests   that instructions - grounded pre - training enables ef-   fective knowledge transfer to downstream low-   resource tasks .   4 Conclusions   We present Z - Code++ , an efficient and effective   pre - trained language model optimized for abstrac-   tive text summarization . The model extends the   encoder - decoder model using three techniques .   The first is a two - phase pre - training process , where   the model is first pre - trained using text corpora   for language understanding , and then is continually   pre - trained on summarization corpora for grounded   text generation . The second technique is the use   of the disentangled attention mechanism , where   each word is represented using two vectors that   encode its content and position , respectively . The   third is the fusion - in - encoder method for encoding   long sequence inputs . We present a comprehensive   empirical study to validate the effectiveness of Z-   Code++ . The model creates new state of the art   on 9 out of 13 text summarization tasks across 5   languages . In addition , we show that our model is   parameter - efficient in that it outperforms the 600x   larger PaLM on XSum , and the finetuned 200x   larger GPT3 on SAMSum . Z - Code++ also   generalizes well to low - resource downstream tasks .   For example , in zero - shot and few - shot settings ,   our model outperforms more substantially the com-   peting models .   However , evaluation ( Liang et al . , 2022 ) and   hallucinations are still two long - standing problems   of summarizations that we do not touch with in this   work , in the future we will 1 ) explore evaluation   metrics that correlate well with human experience ,   2 ) learn to summarize to better align with human   preferences ( Stiennon et al . , 2020 ; Ouyang et al . ,   2022 ) , and 3 ) ground summarization models on5102world knowledge to largely reduce hallucinations   ( LeCun , 2022 ; Hafner et al . , 2023 ) .   Limitations   In this paper , we introduce Z - Code++ , a robust   pre - trained model tailored for summarization tasks .   However , it should be noted that there are certain   limitations to our model . Firstly , the model is not   versatile enough as it is specifically designed for   summarization . It is unclear whether it performs   well on other natural language tasks . Secondly ,   while FiE can handle document summarization ,   there are still significant potential for improving   cost efficiency . Lastly , the evaluation of multilin-   gual summarization is not thorough enough due to   the limitations of available datasets . We intend to   address these limitations in our future work .   Ethics Statement   The same as all existing generative language mod-   els , the generated text of Z - Code++ raises various   ethical considerations . One crucial consideration   is the issue of potential hallucinations in the sum-   maries generated by the model . The summaries   produced by a generative model may not neces-   sarily be faithful to the original article or entirely   factual which may mislead the users to make incor-   rect decisions based on the summary without addi-   tional knowledge . In addition , another important   consideration is the potential for bias in generated   summaries , such as bias based on gender , race , and   other factors .   References5103510451055106A Appendix   A.1 Hyper parameters   A.2 Rouge scores of summarization tasks   We list the rouge scores of summarizaiton tasks in   table10   A.3 Fusion - in - Encoder structure   In figure 3 , we show the architecture of FiE.   A.4 Ablation study   We conducted a comprehensive experiment to ex-   plore what is important for the encoder ’s language   understanding ability . Specifically , we experiment   on the natural language inference task , e.g. , MNLI   ( Williams et al . , 2018 ) , the question answering task ,   e.g. , SQuAD ( Rajpurkar et al . , 2016 ) , the summa-   rization tasks , e.g. , XSum ( Narayan et al . , 2018 )   and CNNDM ( See et al . , 2017 ) . The results in   Table 12 show that using disentangled attention   improves MNLI - matched / mismatched accuracy by   0.9%/1.2 % , indicating an improvement in the en-   coder ’s language understanding ability . This im-   provement is also reflected in the performance of   two summarization tasks , which see an improve-   ment in R2 scores by 0.39 % and 0.22 % . Removing   RTD significantly decreased performance , indicat-   ing that it is essential for improving the model ’s   NLU capability .   A.5 Evaluate on NLU tasks   In order to assess the model ’s effectiveness on nat-   ural language understanding ( NLU ) tasks , we con-   ducted experiments using the eight NLU tasks from5107Model 0 10 100 1000   XSUM   T5 12.8/2.3/9.8 13.2/2.5/10.0 21.5/5.5/17.0 31.2/9.4/23.8   PEGASUS 19.3/3.0/12.7 19.4/3.5/14.02 39.07/16.4/31.3 41.6/18.2/33.3   Z - Code++ 3.6/0.1/3.7 16.7/2.1/12.6 35.3/12.3/27.5 40.9/17.3/32.8   Z - Code++ 36.6/13.7/28.6 37.4/14.0/29.1 40.6/17.5/30.0 41.9/18.9/33.6   CNNDM   T5 18.5/4.9/13.3 19.0/5.1/13.6 24.2/7.7/17.5 31.9/11.2/21.4   PEGASUS 32.9/13.3/29.4 37.6/15.8/33.5 40.3/18.2/37.0 41.7/19.4/38.3   Z - Code++ 3.5/0.1/3.1 11.9/1.5/8.7 37.3/15.0/25.5 40.7/18.3/28.3   Z - Code++ 40.0/17.3/25.340.0/17.3/25.341.1/18.4/27.5 42.0/19.6/28.9   SAMSum   T5 9.4/1.3/8.2 14.0/4.0/12.0 29.6/10.4/23.5 41.4/17.8/32.8   PEGASUS 26.3/6.4/20.5 37.0/11.7/28.1 45.0/19.8/36.1 49.3/24.4/40.6   Z - Code++ 6.0/0.1/5.4 13.6/2.6/11.0 44.7/20.2/36.7 50.9/26.3/42.3   Z - Code++ 26.5/7.9/20.5 40.27/17.4/33.7 47.6/22.3/38.7 52.2/28.1/43.9   the GLUE dataset ( Wang et al . , 2019 ) . These tasks   are commonly used to evaluate sentence classifica-   tion performance in machine learning . Our model ,   Z - Code++ , was tested using two approaches : adapt-   ing only the encoder and fine - tuning with a clas-   sification head , similar to BERT , or adapting the   encoder - decoder and treating the task as a genera-   tion task , similar to T5 . We compared Z - Code++   to other encoder - based PLMs with similar struc-   tures , including BERT , RoBERTa , ELECTRA , De-   BERTa , and DeBERTaV3 , as well as T5 for the   encoder - decoder comparison .   The results , shown in Table 13 , demonstrate that   Z - Code++ performs comparably or better than theother models on all tasks . In particular , Z - Code++   outperformed the other encoder PLMs by an av-   erage of more than 1 % and outperformed T5 on   all tasks with an average improvement of 1.98 % in   test scores . These results demonstrate Z - Code++   as a strong universal language model with excel-   lent performance on generation tasks and superior   performance on NLU tasks .   A.6 Evaluate on NLG tasks   We evaluated the language generation performance   of Z - Code++ on a range of English tasks , in-   cluding abstractive document summarization tasks   ( XSum , CNNDM , Wikilingual - en ) , a conversa-5108ModelEval CoLA QQP MNLI - m / mm SST-2 STS - B QNLI RTE MRPC Avg .   Mcc Acc Acc Acc Corr Acc Acc Acc   # Train 8.5k 364k 393k 67k 7k 108k 2.5k 3.7k   Encoder - Only   BERT Dev 60.6 91.3 86.6/- 93.2 90.0 92.3 70.4 88.0 84.05   RoBERTa 68.0 92.2 90.2/90.2 96.4 92.4 93.9 86.6 90.9 88.82   ELECTRA 69.1 92.4 90.9/- 96.9 92.6 95.0 88.0 90.8 89.46   DeBERTa 70.5 92.3 91.1/91.1 96.8 92.8 95.3 88.3 91.9 90.00   DeBERTaV3 75.3 93.0 91.8/91.9 96.9 93.0 96.0 92.7 92.2 91.37   Z - Code++ 75.5 92.8 91.7/91.5 96.3 93.1 95.8 92.4 92.4 91.23   Encoder - Decoder   T5 Test 61.2 89.9 89.9/89.6 96.3 89.9 94.8 87.2 89.9 87.35   Z - Code++ Test 69.2 90.0 91.0/90.9 97.9 91.2 95.1 90.7 89.6 89.33   Z - Code++ Dev 86.2 92.4 91.4/91.4 96.5 92.5 95.2 92.1 91.2 92.19   tional summarization task ( SAMSum ) , data - to - text   tasks ( WebNLG - en , E2ENLG ) and a question an-   swering task ( SQuAD v1.1 ) . We compared the   performance of the Z - Code++ model to other state-   of - the - art models with similar architectures and   parameters , as shown in Table 14 .   Results show that Z - Code++ outperforms all   of the other models ’ scores by a large margin in   terms of ROUGE and BLEU scores . For exam-   ple , Z - Code++ significantly outperformed T5   on CNNDM by 1 % in terms of ROUGE-2 score ,   on the WebNLG - en task by 6.9 % , and about 1 %   BLEU score on dialog response generation tasks .   Even though it has less than 1/3 the parameters of   T5 , Z - Code++ outperformed PEGASUS on   SAMSum task by 4 % in terms of ROUGE-2 score .   We conjecture that PEGASUS is a model specifi-   cally optimized for summarization using 1500 GB   of news data , which may have introduced a domain   mismatch with the conversational summarization   task . We also compared Z - Code++ to other state-   of - the - art models with extremely large parameters ,   including PaLM , GPT3 , and UL2 . Z - Code++ out-   performed PaLM on three out of four tasks by a   large margin , even though it has less than 1/600 the   parameters of PaLM . Z - Code++ also outperformed   UL2on four out of five tasks , even though it   has less than 1/20 the parameters of UL2 . These   results demonstrate the efficiency of the Z - Code++   model.51095110ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Limitations   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Instruction   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Section 3 , experiments   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 3.1 , experiment setup5111 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 3 , experiments and appendix A 1   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 3 , experiments   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3 , experiments   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 3 , experiments   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . We quote our results from public benchmark https://leaderboard.allenai.org/genie-   mt / submissions / public which run human evaluation from their backend .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . We quote our results from public benchmark https://leaderboard.allenai.org/genie-   mt / submissions / public which run human evaluation from their backend .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . We quote our results from public benchmark https://leaderboard.allenai.org/genie-   mt / submissions / public which run human evaluation from their backend .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . We quote our results from public benchmark https://leaderboard.allenai.org/genie-   mt / submissions / public which run human evaluation from their backend.5112