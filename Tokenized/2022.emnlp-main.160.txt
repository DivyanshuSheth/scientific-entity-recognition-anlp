  Seungwhan Moon , Satwik Kottur , Alborz Geramifard , Babak Damavandi   Meta Reality Labs & Meta AI{shanemoon,skottur,alborzg,babakd}@fb.com   Abstract   Recent years have seen an increasing trend in   the volume of personal media captured by users ,   thanks to the advent of smartphones and smart   glasses , resulting in large media collections .   Despite conversation being an intuitive human-   computer interface , current efforts focus mostly   on single - shot natural language based media re-   trieval to aid users query their media and re - live   their memories . This severely limits the search   functionality as users can neither ask follow-   up queries nor obtain information without first   formulating a single - turn query .   In this work , we propose dialogs for connected   memories as a powerful tool to empower users   to search their media collection through a multi-   turn , interactive conversation . Towards this ,   we collect a new task - oriented dialog dataset   C , which contains 11.5kuser↔assistant   dialogs ( totalling 103kutterances ) , grounded   in simulated personal memory graphs . We em-   ploy a resource - efficient , two - phase data collec-   tion pipeline that uses : ( 1 ) a novel multimodal   dialog simulator that generates synthetic dia-   log flows grounded in memory graphs , and , ( 2 )   manual paraphrasing to obtain natural language   utterances . We analyze C , formulate four   main tasks to benchmark meaningful progress ,   and adopt state - of - the - art language models as   strong baselines , in order to highlight the mul-   timodal challenges captured by our dataset .   1 Introduction   The rise of smartphones and smart glasses has con-   tributed to a surge in the amount of personal media   ( photos , videos , montages , etc . ) captured by users   on a day - to - day basis in the past decade . For in-   stance , it is estimated that about 1.5trillion photos   would be clicked in the year 2022 ( Pantic , 2021 ) .   As a result , personal media collections typically   grow at an alarming rate , making it cumbersomeFigure 1 : Illustration of C : COnnected MEmories   with a Task - oriented Dialog . ( a ) Each dialog turn is fully   annotated with dialog acts and multimodal coreference   labels , accompanied with photos associated with the re-   quest . ( b ) These media are from the underlying memory   graph , a structured collection of personal media .   for users to manually search , retrieve , and re - live   their captured memories .   To alleviate this situation , solutions that perform   natural language query - based media retrieval ( Tan   et al . , 2019 ; V o et al . , 2019 ; Tellex and Roy , 2009 ;   Barbu et al . , 2013 ; Li et al . , 2017 ; Guo et al . , 2018a ;   Saha et al . , 2018 ) have been proposed . However ,   such approaches exhibit two drawbacks . First , they   are single - shot interactions without any context   carry - over , e.g. ,Show me some photos from the   beach last week . . This limits the functionality and2495does not let users ask any follow - up queries like   ‘ Display photos from the first time I was here ? ’ ,   since understanding here requires the query history .   Second , users can not seek information without ac-   tually formulating the query to retrieve the corre-   sponding memory . For instance , there is no easy   query to know the first time a user visited the beach   in the memory they are reviewing .   In order to overcome these limitations , we pro-   pose dialogs for connected memories as a powerful   interface where users can interactively query their   memory collections . By design , a conversational   agent can handle multi - turn interactions enabling   several additional queries that require context car-   ryover , e.g. ,‘When was the first time I was at this   beach ? ’ . Though prior efforts have explored the   use of dialogs in media retrieval ( Wu et al . , 2021 ;   Guo et al . , 2018b ) in other domains ( e.g. , fashion ) ,   there is no existing work focusing on interactive   search and query of personal media collections to   the best of our knowledge .   More concretely , we propose C , a new   multimodal task - oriented dialog dataset aimed at   developing conversational assistants that can en-   able users to interactively search and query their   collection of memories . Working with personal me-   dia collections presents two main obstacles : ( a )   There are no readily available public datasets that   contain personal media along with associated me-   dia attributes that we could leverage , and , ( b ) Per-   sonal memories constitute sensitive information ,   thus resulting in privacy and safety concerns . To   circumvent these roadblocks , we devise a novel   memory graph simulator that can leverage publicly   available media datasets and help create several   synthetic memory collections . We represent these   collections as memory graphs to capture useful re-   lationships between the constituent memories , e.g. ,   memories taken at the same place . We then col-   lect11.5kuser↔assistant task - oriented dialogs ( to-   talling 103kutterances ) , grounded in 1.1kmemory   graphs . An example dialog is in Fig . 1 .   Our dataset is challenging as it requires rea-   soning through both the dialog history and mul-   timodal context ( memory graphs ) to resolve coref-   erences , track the dialog state , predict the right   API , and generate a meaningful natural language   assistant response . As an example , consider the   query ‘ When was the first time I was here ? ’ . First ,   the model needs to resolve here using the dialog   history and previously viewed memories . Next , it   needs to understand that the query is seeking in - formation about a connected memory , and predict   the right API get_time(resolve ( here),first   time ) . Finally , it should produce a response like   ‘ The first time you were here was on August 2 , 2019   with Jean ’ , potentially including some chit - chat .   To capture these challenges and benchmark   progress towards assistants that can interactively   handle dialogs for connected memories , we formu-   late four main tasks : Assistant API Call Prediction ,   Multimodal Coreference Resolution ( MM - Coref ) ,   Multimodel Dialog State Tracking ( MM - DST ) , and   Response Generation . We train baseline models for   these tasks , and discuss future research directions .   2 Related Work   Task - oriented Dialogs aim to understand user   queries and accomplish a pre - defined set of tasks   ( e.g. booking hotels ) , which is a popular setting   in consumer - facing virtual assistants . Our work   addresses similar challenges often found in other   task - oriented dialogs , such as natural language un-   derstanding ( NLU ) , dialog state tracking ( DST )   ( Henderson et al . , 2014 ) , etc . Compared to the con-   ventional task - oriented dialog datasets ( e.g. Multi-   WoZ ( Budzianowski et al . , 2018 ; Eric et al . , 2019 ;   Rastogi et al . , 2019 ) ) , however , our work involves   a unique multimodal setting where dialogs are   grounded on a memory graph composed of sev-   eral media files , introducing novel challenges such   as Multimodal DST and Multimodal Coreference   Resolution given personal photo collections .   The most notable modeling approaches for task-   oriented dialog systems include casting the DST   task as a joint causal language modeling problem   ( Hosseini - Asl et al . , 2020 ; Peng et al . , 2020 ; Sun   et al . , 2021 ) , by fine - tuning a large pre - trained   transformers . We follow this recent trend and pro-   vide baselines by extending it accommodate for the   unique multimodal contexts that our dataset brings .   Multimodal Dialogs have become increasingly   more popular , where dialog models process both   visual and text input to handle queries ( Kottur et al . ,   2021 ) . Many existing literature ( Hori et al . , 2018 ;   Das et al . , 2017 ; Kottur et al . , 2019 ; de Vries et al . ,   2017 , 2018 ; Thomason et al . , 2019 ) study multi-   modal Q&A dialogs grounded on a single image   as grounding context , extending the conventional   VQA ( Antol et al . , 2015 ) tasks to multi - turn scenar-   ios . Xu et al . ( 2020 ) studies conversational recom-   mendation system using personal memories . We   extend this line of work by studying the multimodal2496   agent that operates on a collection of media ( mem-   ory graph ) , thus requiring reasoning abilities over   multiple grounding contexts . Our focus on task-   oriented dialogs extends the previous literature and   datasets that primarily focus on retrieval tasks ( Guo   et al . , 2018a ; Saha et al . , 2018 ; Firdaus et al . , 2020 ) ,   capturing structured user intents and fine - grained   attributes for each multimodal query .   Memory QA : Our work is also similar to the Mem-   ory QA tasks ( Jiang et al . , 2018 ; Moon et al . , 2019 ) ,   where the main task is to answer user QA queries   upon a collection of images , extending the Visual   QA task ( Antol et al . , 2015 ) which operates on a   single image . However , the existing literature is   limited to a simple single - turn QA interaction , and   focuses on the identification of an evidential im-   age to answer a question . While our dataset does   include QA queries , we extend the problem do-   main to the conversational settings which support   complex scenarios ( e.g. searching for related mem-   ories ) , allowing for rich multimodal interactions .   3 The C Dataset   C is aimed to enable assistant systems that   can process interactive queries from users and help   navigate their collection of memories through a   natural language conversation . Towards this , we   collect the C dataset using a two - phase ap-   proach ( shown in Fig . 2 ): ( a ) Generating synthetic   dialog flows between a user and an assistant that   are conditioned on memory graphs , using a novel   multimodal dialog simulator ( Sec . 3.1 ) , and , ( b )   Manually paraphrasing the above flows to obtain   dialogs with natural language utterances ( Sec . 3.2 ) ,   thus moving closer to real - world application . This   approach is resource - efficient as it reduces the an-   notation overheads when compared to collecting   human↔human dialogs , both in terms of cost and   time . In what follows , we describe these two phases   in detail and analyze our C dataset . See the   supplementary ( Fig . 7 ) for an example dialog .   3.1 Multimodal Dialog Self - play   We first leverage a multimodal dialog simulator   ( Sec . 3.1.2 ) to generate synthetic dialog flows be-   tween a user and an assistant . Each of these flows   is grounded in a graph connecting the memories of   a user from their collection . The memory graphs in   our work are simulated by a novel graph simulator   ( Sec . 3.1.1 ) and are designed to capture several hi-   erarchical relationships between the user memories .   3.1.1 Memory Graph Simulator   Graphs have been ubiquitously used in various   fields to effectively represent a set of entities and   relationships between them . Following this trend ,   we use a graph structure to represent a collection   of memories ( see Fig . 3 for an example ) . As men-   tioned in Sec . 1 , to circumvent the lack of read-   ily available datasets for personal photo collec-   tions and surrounding privacy issues , we construct   a novel graph simulator to synthetically generate   memories graphs using public datasets . These mem-   ory graphs are then used as an input to the multi-   modal dialog simulator to generate dialog flows.2497Memories and Attributes . Memories consti-   tute the atomic units of the graph simulator , and   can cover a wide variety of media including pho-   tographs , videos , and user - created montages . We   limit the scope of memories to represent static im-   ages in this work , although most components of our   proposed framework readily extend to the broader   definition . As photo collection of individuals is sen-   sitive information , we use publicly available image   dataset as a proxy to mitigate the risk . Specifically ,   we use Creative Commons images from MS COCO   ( Lin et al . , 2014 ) that contains objects and people   in everyday contexts as memories .   We then assign four attributes to each of the im-   ages as follows : ( a ) Activity : Each image in MS   COCO has 5associated captions . We use sentence-   BERT ( Reimers and Gurevych , 2019 ) to find the   closest activity label from the taxonomy of the Ac-   tivityNet dataset ( Heilbron et al . , 2015 ) , using aver-   age text - similarity to the captions . To ensure a good   representation , we only keep those with at least 20   memories resulting in about 138labels covering   wide variety of activities . ( b ) Place : For each activ-   ity , we first manually map it to a place type , which   then is randomly mapped to an actual place from   a manually curated list . For instance , playing fris-   bee→park→Cal Anderson Park , Seattle , USA .   ( c)People : We use the associated bounding box   annotations for MS COCO images and map those   labeled as ‘ person ’ , above a threshold size , to a   random name from a curated list of 200names .   ( d)Time attribute is sampled randomly from a con-   strained time range , depending on the relationship   shared with other memories in the graph .   Hierarchical Relationships . To closely emulate   scenarios in a personal photo collection , we devise   the following hierarchy of relations amongst the   memories : memories →events →days→trips .   Using heuristic rules , we sample and group mem-   ories into events that are then grouped into days ,   which are finally grouped into trips . These group-   ings impose constraints on the attributes of the con-   stituent memories , which can be used to generate   interesting conversational flows to query connected   memories . For instance , memories from the same   event need to happen at the same place type , while   those in a day need to happen in the same city . Sim-   ilar restrictions arise for the time attribute as well ,   which would be used to sample reasonable times   for the corresponding memories , e.g. , memories   from the same event can not be separated by more   than few hours . These hierarchical relationships en - able connected queries like ‘ What did we do after   this ? ’ , ‘ Show other pictures with Jane on this trip ’ ,   or‘Where did we go the next day ? ’ .   Memory Graphs . Putting everything together , we   construct a memory graph for each collection :   •nodes : memory , event , day , trip , person , activity   • edges : memory attributes , hierarchical relations   Note that each memory graph can contain multiple   trips . Fig . 3 illustrates a memory subgraph , visu-   alizing only one trip for brevity . We synthetically   generate multiple memory graphs which form the   input to the dialog flow simulator .   Applications in the Real - World Setting . While   we use the publicly available image dataset to gen-   erate memory graphs , applying the method above   in the existing real - world photo album products   at large - scale is straightforward as we do not re-   quire any additional information ( e.g. captions or   annotations ) other than meta data that are readily   associated with the media ( e.g. timestamp , loca-   tions ) . This meta data can be rearranged from ta-   bles to graphs without additional annotations , only   by specifying the relations of interest ( e.g. , people ,   place , time , predicted concepts ) . Memory graphs   are not only practical but also desired to enable   connected memory search .   3.1.2 Multimodal Dialog Simulator   The multimodal dialog simulator takes the gener-   ated memory graphs along with the meta informa-   tion of each node to create user ↔assistant dialog   flows , following the agenda - based dialog simulator   approach ( Schatzmann et al . , 2007 ) .   Dialog Flow Generation via Self - play . The dia-   log simulator comprises three main components :   thegoal generator , the user simulator , and the   assistant simulator . The goal generator randomly   samples an agenda for each dialog , which defines   a sequence of high - level goals for the scenario   ( e.g. ,SEARCH →GET_RELATED_PHOTOS →   GET_INFO ) . Given a goal , the user simulator   draws an acceptable dialog action based on a prob-   ability distribution , which is defined with NLU in-   tents ( e.g. ,REQUEST : GET , CONFIRM : SHARE ) ,   slots ( e.g. , location , time ) , and memory references .   The assistant simulator then takes the output of the   user simulator , retrieves the multimodal contexts   via the simulation API ( e.g. obtaining the informa-   tion of a memory node from the graph , retrieving   related memories ) , and generates natural language   generation ( NLG ) intents , slots and new memory2498references . The process is repeated until the simu-   lator exhausts every goal in the agenda .   Multimodal Dialog Ontology . Following other   task - oriented dialog datasets ( Eric et al . , 2019 ;   Rastogi et al . , 2019 ; Moon et al . , 2020 ) , for   C we provide the standard dialog anno-   tations such as the intent ( NLU & NLG ) and   slot labels . To accommodate for the complex   multimodal nature of the scenarios , we extend   the dialog ontology to include memory refer-   ence annotations as their corresponding node IDs ,   which seamlessly annotates both multimodal con-   texts and language ( e.g. ‘When was our trip to   Whistler ? ’ →INFORM : GET_INFO.time , mem-   ories : [ 8 ] ) . The same notation can be used to re-   fer the memories that are carried over in the di-   alog context ( e.g. ‘Where did we go after that ? ’   →INFORM : GET_RELATED.location , mem-   ories : [ 8 ] ) . This proposed fine - grained and unified   ontology will allow a systematic approach to study   diverse referring expressions in multimodal dialogs .   3.2 Manual Paraphrase   We then collect manual paraphrases of the gener-   ated memory - grounded dialog flows . This allows   us to draw utterances from the natural language   distribution , thus moving closer to the application .   We build an interactive tool to aid annotators -   specifically , the interface shows the images cor-   responding to the memories along with the dia-   log flow , and instructs annotators to paraphrase   without losing key information such as objects   and attributes . The C dataset thus comprises   many rich visual references , making it an ideal   dataset for studying multimodal language ground-   ing . See appendix for an example dialog . As para-   phrasing utterances is faster , cheaper , and requires   little to no domain knowledge on the annotator‘s   part , our two - phase pipeline is much more resource-   effective , when compared to collecting multimodal   human ↔human dialogs and collecting dialog an-   notations on top ( Moon et al . , 2020 ) .   3.3 C Dataset Analysis   C contains 11.4kdialogs totalling 103.4kut-   terances , grounded in 1.1kmemory graphs . Tab . 1   presents the overall dataset statistics .   Analyzing Dialogs . Dialogs in C use a total   of1.1kmemory graphs with each containing 100   memories . For every dialog , there are about 3.5   connected memory mentions with the distribution   given in Fig . 4b . User and assistant turns average   about 10.7and15.4words respectively ( distribu-   tion in Fig . 4a ) . It is interesting to note that the   assistant responses are significantly longer than the   user . As an example , consider the following user   utterance ‘ U : Are there any similar photos from   2020 ? ’ and the corresponding assistant response ‘ A :   Here‘s one of Laura and Virginia cooking sausages   at home , the afternoon of August 26 , 2020 . It looks   like a fun time ! ’ . This illustrates that the annota-   tors paraphrasing the dialog flows included : ( a )   details about the retrieved memories to give addi-   tional context to the user , thus invoking subsequent   connected memory queries ( e.g. ,‘What did we do   that evening ? ’ ) , ( b ) chitchat about the memories to   make the conversational natural sounding .   Analyzing Dialog Annotations . Our C   come with annotations at dialog level for dialog   state tracking ( NLU intents and slots ) , necessary   API calls for assistant , and multimodal coreference   resolution . Following Kottur et al . ( 2021 ) , our in-   tents follow a hierarchy of dialog acts ( 4 : ASK ,   CONFIRM , INFORM , REQUEST ) and activities ( 4 :   DISAMBIGUATE , GET , REFINE , SHARE ) . See   Fig . 4d for a breakdown distribution over dialog   acts and activities . Due to the retrieval nature of our   assistant ( either memories or associated attributes ) ,   a major chunk of the activities are GET . Similarly ,   there are 5APIs in our dataset ( Fig . 4c ):   •SEARCH : Search using input parameters ,   •REFINE_SEARCH : Build on top of search car-   rying over existing parameters ,   •GET_INFO : Seek information about current or   previouly viewed memories ,   •GET_RELATED : Explore other memories simi-   lar to the current / prior memories , and ,   •SHARE : Share it to friends or family ,   As expected , SEARCH is the most dominant API   call in the dataset . Note that the turns with GET   andREFINE_SEARCH API calls elevate the need   for conversation in retrieving connected memories ,   where the user requests for memories similar to2499   the ones already viewed or with additional speci-   fications , respectively . Finally , Fig . 4e visualizes   the distribution of number of candidates and utter-   ance difference between the current and the one   with referent memory ( coreference distance ) . For   turns requiring coreference resolution , the average   number of candidates is 2.7at a distance of 2.9   utterances . Though a majority of referents are natu-   rally 1utterance away ( previous turn ) , the long tail   ( even up to 10 + utterances ) indicates the presence   of challenging multimodal coreferences .   Analyzing Dialog Flows . As mentioned earlier ,   the multimodal dialog simulator generates the dia-   log flows during the first phase of our data genera-   tion . We visualize these dialogs flows in Fig . 5 for   the first four dialog turns , where each block denotesan intent at a particular turn and the grey stripes   denote NLU intent transitions in subsequent turns .   The width of the stripe is proportional to the fre-   quency of the transition . For brevity , each block is   label as ACT : ACTIVITY:[A|U][turn ] . The   high branch - off factors for these intents capture the   diversity of the dialogs flows in our dataset , which   is desirable in building a robust dialog system .   4 Task Formulation   To benchmark progress of conversational models   towards the goal of assisting users in interactively   querying connected memories in a meaningful way ,   we propose four main tasks for C . Tab . 2   outlines the tasks and the evaluation metrics.2500   4.1 Assistant API Call Prediction   The first step in executing any query on connected   memories successfully is to understand the user ut-   terance in the context of the dialog history and mul-   timodal information , and predict the right API call .   For instance , a query like ‘ When was the last time   I was here ? ’ should result in a GET_INFO API   prediction . Note that errors in API call prediction   cascade through the model pipeline resulting in an   incorrect or unrelated response from the assistant .   Thus , this task tests the ability of the conversational   agent to predict the right API call . Evaluation is   done per each turn through API call accuracy .   4.2 Multimodal Coreference Resolution   Recall that one of our motivations to use conver-   sations for querying connected memories is the   ability to support multi - turn queries . In such sce-   narios , humans often use short - hands or references   when the underlying referred entity ( referent ) can   be usually deduced without any ambiguity . As an   example , when looking at a particular memory , a   follow - up ‘ When was the last time I was here?’is   intuitive and natural , whereas ‘ When was the last   time I was at Waikiki Beach , Hawaii ? ’ requires the   user to remember the name and use it in the query ,   making it cumbersome .   Therefore , the model must be able to handle mul-   timodal coreferences in order to field such queries   effectively . The input for this task includes the dia-   log history , multimodal context , and all the memo-   ries mentioned so far ( as coreference candidates ) .   The models needs to thus resolve the reference to   one or more of the candidates . We use coreference   precision , recall , and F1 to measure performance .   4.3 Multimodal Dialog State Tracking   Due to the multimodal nature of C , we adopt   multimodal dialog state tracking ( MM - DST ) used   in ( Kottur et al . , 2021 ) as one of our tasks . To elab - orate , slots in our dataset can be grounded in the   multimodal context information and requires rea-   soning through the current or previously viewed   memories . For instance , a query like ‘ Where did   we go from here ? ’ requires the slot value to be the   currently viewing memory . This implies that the   dialog states can contain non - textual tokens ( e.g. ,   memories ) , thus making it multimodal . In order to   measure the performance in this task , we use slot   recall , precision , and F1 scores . Note that unlike   ( Kottur et al . , 2021 ) , we drop evaluating for dia-   log act prediction since GET has an overwhelming   majority due to the nature of the problem .   4.4 Assistant Response Generation   This task evaluates the ability of the model to ei-   ther generate a response or retrieve from a pool of   candidates , given dialog history , ground - truth APIs   & results , belief states , and multimodal contexts .   Though the model has access to API results , pro-   ducing a natural language utterance to describe it   within the flow of the dialog is still a difficult task .   We evaluate this task in two different ways :   ( a)Generative , where the model produces the re-   sponse similar to a conditional language model .   We use n - gram overlap based BLEU-4 ( Papineni   et al . , 2002 ) and more recent neural evaluation   metric BERTScore ( Zhang * et al . , 2020 ) to mea-   sure performance by comparing the generated re-   sponse to the ground truth , and ( b ) Retrieval , where   the model ranks a list of randomly pooled can-   didate responses ( unique to a turn ) along with   the ground truth . Retrieval metrics like recall@k   ( k={1,5,10 } ) , mean rank , and mean reciprocal   rank are used .   5 Modeling & Empirical Analysis   We now perform preliminary empirical evaluation   and analysis for the proposed tasks by training base-   lines . Detailed modeling work is left as future work.2501   Dataset Split . The dataset is randomly divided   into : train ( 70 % ) , val ( 15 % ) , and test ( 15 % ) . For   our experiments , models are trained using train   split and performance is reported on test , while val   is used to pick the model hyper - parameters .   Notations . We follow the notation established   in ( Kottur et al . , 2021 ) , where each dialog   of length Nrounds is represented as D=   { ( U , A , M , B)}with :   •U : User utterance at turn i   •A : Assistant utterance at turn i   •M : Multimodal context , i.e. , memory graph and   memories retrieved in the previous turns ,   •B : Multimodal belief state , a semantic parse of   U(intent , slot , memory references ) .   Therefore , given the current user utterance ( U ) , di-   alog history H= ( U , A ) , and the multimodal   context ( M ) , aC agent should predict the   user belief state Band the natural language re-   sponse Afor every dialog turn t.   Baselines . Causal language models pretrained on   large datasets have shown a lot of promise in multi-   modal and text - only task - oriented dialog modeling ,   when finetuned on the downstream task ( Hosseini-   Asl et al . , 2020 ; Peng et al . , 2020 ; Kottur et al . ,   2021 ; Moon et al . , 2020 ) . Following this popular   approach , we adopt the transformer - based GPT-2   ( Radford et al . , 2019 ) model and jointly train it   for API prediction , MM - Coref , DST , and response   generation tasks , as shown in Fig . 6 . In particular ,   we use the 12 - layer GPT-2 ( 117 M ) model and fine-   tune it on dialogs from C dataset , using early   stopping based on token perplexity ( <3 GPU hrs ) .   We use two approaches to capture M :   ( a)text - only ( GPT2 - text ) , where previously viewed   memories and their attributes are represented as flat-   tened strings . Note that this baseline uses ground-   truth activities from the memory graph .   ( b)multimodal ( GPT2 - MM ) , where bottom - up and   top - down ( ) ( Anderson et al . , 2018 ) and   ( Radford et al . , 2021 ) image features are extracted   for previous viewed memories , and fed as ‘ visual   tokens ’ while finetuning the GPT-2 model .   Analysis . A key observation from Tab . 3 is that   multimodal models outperforms text - only across   all the tasks significantly . This is intuitive , for in-   stance , multimodal coreference resolution requires   understanding the memories beyond the obvious   activity label in order to rightly resolve the ref-   erence . Consider the query : ‘ When was the last   time I played with my dog here ? ’ . To resolve to   the right memory , the system needs to understand   which memory is about playing with the dog to-   wards which a mere activity label throwing frisbee   might be insufficient . For a similar reason , addi-   tional multimodal features improve response gener-   ation , especially to include chit - chat . On the other   side , GPT - Text performs competitively on captur-   ing the dialog state .   Conclusion . We present a novel dataset for the   dialogs for connected memories , C , with   11.5Kuser↔assistant dialogs ( 103Kutterances )   grounded on the memory graphs . We present a   novel multimodal dialog simulator , which gener-   ates simulated dialogs grounded on diverse mem-   ory graphs that are automatically configured . Our   empirical analysis demonstrates many new chal-   lenges that our C dataset brings , highlighting   new directions of research in this area.25026 Limitations   The generalizability of C is naturally   bounded by the underlying graph simulator , es-   pecially around memory attribute labels of place ,   people , and time . However , we justify this as fol-   lows : ( a ) Recall that the focus of our work is to   enable an assistant that can understand and execute   user queries about connected memories through an   interactive dialog . Even with the simulated dialog   flows , C captures several interesting chal-   lenges related to multimodal dialog , for instance ,   coreference resolution and dialog state tracking ( as   seen in Sec . 3.3 and Sec . 5 ) . This opens the door   to new research directions in multimodal conversa-   tion , especially in the absence of a readily available   large - scale personal photo collection dataset ( along   with attributes and metadata ) . ( b ) Due to the two-   stage data collection pipeline , C is amenable   to data augmentation techniques that can increase   the robustness of the downstream dialog model . For   instance , the dataset can be easily augmented by   replacing named entities in the memory graph and   utterances , without changing the flow .   References2503   .2504A Supplementary Materials   A.1 Ethical Considerations   All identifiable faces from the COCO images were   blurred using a CV algorithm , mitigating potential   privacy risks . The dataset , when released publicly ,   will include those edited images .   Annotators for our task were employed as full-   time and contracted via a leading NLP / linguistics   annotation platform . Annotators were given clear   instructions and disclaimers detailing the escalation   path ( “ Report Dialog " ) for an ( unlikely ) case where   the data may include sensitive topics or images   ( shown in Figure 9 ) .   A.2 Dataset Example   Figure 7 illustrates an example dialog from   C , along with the set of images as-   sociated with each turn ( U : User , A : As-   sistant ) . API Annotations are formatted asfollows : INTENT [ slot = value , ... ]   ( request_slot ) < memory : ID > . When   there is no new image introduced for a given   turn , it is assumed that the images from previous   turns ( if any ) are left visible to the user , therefore   continuing to serve as the grounding multimodal   context .   A.3 Annotation UI   Figure 8 illustrates the annotation UI used to col-   lect multimodal paraphrases of the dialog . Anno-   tators are shown the pre - generated dialog flows   ( templated utterances ) , along with the text boxes   where the paraphrases can be entered . The top por-   tion of the UI displays the images ( assumed to be )   shown to the user for each given turn , which gets   dynamically updated as annotators click on new   text boxes for entering paraphrases . A shortened   list of meta data associated with each image is also   shown for reference.250525062507