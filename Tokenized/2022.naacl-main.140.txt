  Jinpeng Hu , Yaling Shen , Yang Liu   Xiang Wan , Tsung - Hui ChangShenzhen Research Institute of Big Data , The Chinese University of Hong Kong ,   Shenzhen , Guangdong , ChinaPazhou Lab , Guangzhou , 510330 , China   { jinpenghu , yalingshen , yangliu5}@link.cuhk.edu.cn   wanxiang@sribd.cn changtsunghui@cuhk.edu.cn   Abstract   Named entity recognition ( NER ) is a funda-   mental and important task in NLP , aiming at   identifying named entities ( NEs ) from free text .   Recently , since the multi - head attention mecha-   nism applied in the Transformer model can ef-   fectively capture longer contextual information ,   Transformer - based models have become the   mainstream methods and have achieved signif-   icant performance in this task . Unfortunately ,   although these models can capture effective   global context information , they are still limited   in the local feature and position information ex-   traction , which is critical in NER . In this paper ,   to address this limitation , we propose a novel   Hero - Gang Neural structure ( HGN ) , including   the Hero and Gang module , to leverage both   global and local information to promote NER .   Specifically , the Hero module is composed of   a Transformer - based encoder to maintain the   advantage of the self - attention mechanism , and   the Gang module utilizes a multi - window re-   current module to extract local features and   position information under the guidance of the   Hero module . Afterward , the proposed multi-   window attention effectively combines global   information and multiple local features for pre-   dicting entity labels . Experimental results on   several benchmark datasets demonstrate the ef-   fectiveness of our proposed model .   1 Introduction   Named entity recognition ( NER ) is one of the most   important and fundamental research topics in natu-   ral language processing ( NLP ) , which recognizes   named entities ( NEs ) , such as person , location ,   disease from raw text . NER has attracted sub-   stantial attention in the past decades owing to its   importance in downstream tasks , e.g. , knowledgegraph construction ( Bosselut et al . , 2019 ) , question-   answering ( Pergola et al . , 2021 ) , and relation ex-   traction ( He et al . , 2019 ) .   In the early stage , the popular methods for solv-   ing NER are some traditional machine learning   methods , e.g. , Hidden Markov Model ( HMM )   ( Morwal et al . , 2012 ) and conditional random   field ( CRF ) ( Mozharova and Loukachevitch , 2016 ) ,   which require extracting features manually , mak-   ing the process inefficient and time - consuming .   With the breakthrough of recurrent neural networks   ( RNN ) in NLP , Long short - term memory ( LSTM )   ( Hochreiter et al . , 1997 ) and Gated Recurrent Unit   ( GRU ) ( Cho et al . , 2014 ) have become mainstream   methods for this task and have achieved promis-   ing results since neural networks can automatically   extract features from the sequence and also take   each token ’s position information into considera-   tion ( Lample et al . , 2016 ; Chiu and Nichols , 2016 ;   Huang et al . , 2015 ) . Nevertheless , RNN fails to   perform well with long sequences due to the gra-   dients exploding and vanishing . In recent years ,   Transformer - based models ( Vaswani et al . , 2017 )   have become mainstream methods because these   models are able to capture long - term dependencies   with the help of multi - head attention and thus pro-   vide better global context information , especially   for long sequences ( Lee et al . , 2020 ; Yang et al . ,   2019b ) . However , these Transformer - based models   usually are insensitive to the local context since the   representation of each token is computed by the   canonical point - wise dot - product self - attention ( Li   et al . , 2019 ; Huang et al . , 2021 ) . Besides , although   some studies ( Shaw et al . , 2018 ; Devlin et al . , 2018 ;   Liu et al . , 2019 ) have been proposed to inject po-   sition information into Transformer , they are still   inadequate to help Transformer obtain appropriate   position information ( Huang et al . , 2020 ; Qu et al . ,   2021 ) . In other words , the self - attention mecha-   nism is effective in overcoming the constraints of   RNN from the perspective of long - sequence con-1924   text information extraction , but is inferior to RNN   in terms of local contextual and position informa-   tion extraction . Yet , both long - term dependencies   and local context information are essential for the   NER model to correctly identify entities .   Thus , to alleviate the shortcomings in RNN and   Transformers while maintaining their respective   strengths , in this paper , we propose a novel Hero-   Gang Neural model to leverage both global and   local contextual information to improve NER . In   doing so , on the one hand , we utilize a Transformer-   based sequence encoder ( i.e. , Hero module ) to ex-   tract effective global contextual information with   the help of the self - attention mechanism . On the   other hand , a multi - window recurrent unit ( i.e. ,   Gang module ) is applied to extract local features   from multiple sub - sequences under the guidance   of the extracted global information . Afterward , we   propose to use multi - window attention to elabo-   rately combine global and local contextual features .   The performance of our proposed model signifi-   cantly outperforms the strong baseline models on   several NER benchmark datasets ( including both   general and biomedical domains ) and achieves new   state - of - the - art results on some datasets .   2 Method   NER is usually performed as a sequence label-   ing problem . In detail , given a sequence of   X = x , x , ... , xwithNtokens , we aim tolearn a function that maps the input sequence into   another one with the corresponding label ˆY=   ˆy,ˆy,ˆy , ... , ˆyin the same length . As summa-   rized in Figure 1 , the Transformer - based models   ( e.g. , BERT ( Devlin et al . , 2018 ) , XLNET ( Yang   et al . , 2019b ) ) are regarded as the Hero module to   model the entire sentence for global sequence infor-   mation extraction and the Gang module is respon-   sible for local and relative position information ex-   traction . Afterward , we employ the multi - window   attention to elaborately combine these different fea-   tures ( i.e. , features extracted from the Hero and   Gang modules ) , which is then used to predict la-   bels for each token . Therefore , the aforementioned   process can be formulated as :   ˆY = f(X , H(X),G(X ) ) , ( 1 )   where H(·)andG(·)refer to the Hero and Gang   modules , respectively , and the details of them are   presented in the following subsections .   2.1 Hero Module   The role of the Hero module in our proposed model   is similar to that of the leader in a team , who is re-   sponsible for providing guidance , offering instruc-   tions , giving directions , and assigning sub - tasks to   fellow memberships . Therefore , the Hero module   is required to have a comprehensive understanding   of the task , including overall and local progress .   Thanks to the characteristics of the multi - head self-   attention mechanism , Transformer is powerful in1925modeling long sequences and can provide more   effective global information than other counterpart   models , and it has already achieved promising re-   sults in the NER task ( Luo et al . , 2020 ; Beltagy   et al . , 2019 ) . Thus , we employ a Transformer-   based encoder as our Hero module to obtain the   global context information zfor each token xby   [ z , z,···,z ] = f(x , x , ... , x ) . ( 2 )   Herein , f(·)refers to a pre - trained Transformer-   based sequence encoder ( e.g. , BERT ( Devlin et al . ,   2018 ) and BioBERT ( Lee et al . , 2020 ) ) . The   features zare then input to the Gang module for   extracting local contextual features and their corre-   sponding relative position information .   2.2 Gang Module   As introduced in the previous section , although pre-   trained models are able to provide effective global   contextual representation , it lacks the ability to   extract local features and relative position informa-   tion . Thus , we propose a multi - window recurrent   module , named Gang , to enhance local informa-   tion extraction . Recurrent structures ( RS ) , such as   LSTM , GRU , and tradition RNN are effective in   extracting both local and relative position informa-   tion from the sequence , owing to characteristics of   the recurrent mechanism . To better emphasize the   local features of each word without being disturbed   by long - distance information , we construct a slid-   ing window with a fixed length to generate shorter   sub - sequences , where each sub - sequence includes   several consecutive elements in z. An additional   advantage of this operation is that , in comparison   with the whole sequence , the sub - sequence is much   shorter so that it is easier to be modeled by the RS .   In detail , for a single sliding window with   length k , each hidden state zfrom the Hero   module , the corresponding sub - sequence is   z , z , ... , z , ... , z , zthat includes   2k+ 1consecutive tokens . This sub - sequence of   length 2k+ 1contains rich local contextual infor-   mation of x , and thus we utilize an RS to encode   it for obtaining local semantic and relative position   information . To extract the local information of   two directions , we utilize a bidirectional structure   to encode this sequence span , where the forward   RS computes a representation− − − →hfrom left to   right , and the other backward RS computes a vec-   tor← −hfor the same sub - sequence in reverse . We   concatenate the← −hand− − − →has the local featureh= [ ← −h,− − − →h]for token x , and then we can ob-   tain local features for each token in sequence Xvia   the similar way , denoted as h = h , h,···,h .   In practice , we need to consider two situations .   First , each token might have multiple levels of lo-   cal information , such as phrase - level and clause-   level , which may affect the understanding of the   current token . Second , since different tokens or   the same token in various contexts might have dif-   ferent relationships with their surrounding words ,   we need to consider more sub - sequences with vary-   ing lengths for obtaining more comprehensive lo-   cal contextual information . Therefore , we propose   to utilize multiple sliding windows with different   window sizes to extract richer local features to al-   leviate the above issues . We assume that local   features h , h,···,hare extracted from differ-   ent groups of sub - sequences , whose corresponding   window lengths are k , k , · · · , k. This process   can be formulated as :   h , h,···,h = Gang(k , k , · · · , k , z ) ,   ( 3 )   where Mis the number of sliding windows and   his a group of local features extracted from the   corresponding sliding window with length k. The   process is similar to the task assignment in the team ,   where different members are responsible for their   own sub - tasks .   2.3 Multi - window Attention   We obtain global representation zfrom the   Hero module and multiple local features   h , h,···,hfrom the Gang module . Next , we   apply the multi - window attention to effectively   combine global contextual information and local   features . In doing so , two types of attention meth-   ods are proposed in our model : MLP - Attention   and DOT - Attention , respectively .   MLP - Attention We first concatenate these local   features with global information and obtain the   intermediate state mby a fully connected layer .   m = MLP([z , H ] ) , ( 4 )   where H= [ h , h,···,h]andmhave the   same dimension as z. MLP represents a fully con-   nected layer . Then mis used as a query vector and   [ z , H]serves as the key and value matrix . The final   token representation can be computed by   s = softmax ( m([z , H]))[z , H ] . ( 5 )   DOT - Attention Instead of using a fully connected1926   layer to generate a query vector , in this approach ,   we directly regard zas the query vector and Has   the key and value matrix . We can obtain the final   local feature by   u = softmax ( z(H))H. ( 6 )   Since uis a weighted sum of different local fea-   tures without considering global information , we   use the sum of uandzas the final representation   for each token x. Thus , the final representation   can be obtained by   s={z+u , z+u,···,z+u}.(7 )   After obtaining the final representation from   MLP - Attention or DOT - Attention , sis sent to the   corresponding classifier implemented by the soft-   max function to predict the distribution of labels   for each token in X.   3 Experiments Settings   3.1 Dataset and Metrics   In our experiments , six datasets are used in our   experiments , WNUT17 ( W17 ) ( Strauss et al . ,   2016 ) , WNUT16 ( W16 ) ( Derczynski et al . ,   2017 ) , OntoNotes 5.0 ( ON5e ) ( Pradhan et al . ,   2013 ) , BC5CDR - disease ( BC5 - D ) , BC2GM , and   BC5CDR - chem ( BC5 - C ) . The W17 and W16 are   social media benchmark datasets constructed from   Twitter , and ON5e is a general domain dataset con-   sisting of diverse sources like telephone conver-   sations , newswire , etc . BC5CDR , including both   BC5 - D and BC5 - C , is a dataset used for the BioCre-   ative V Chemical Disease Relation Task and con-   tains chemical and disease mentions , where hu-   mans manually annotate the annotations . BC2GM   is the dataset that is usually utilized for the BioCre-   ative II gene mention tagging task and contains   20000 sentences from the abstracts of biomedical   publications . For all datasets , we utilize the official   splits for a fair evaluation and the statistics of thedatasets are shown in Table 1 . Besides , we follow   previous studies that the final models are trained on   training and validation sets on each dataset except   the ON5e dataset .   For metrics , we exploit the same evaluation met-   rics used by previous works where precision ( P ) ,   recall ( R ) , and F-1 score are reported to evaluate   the performance of our model .   3.2 Implementation Details   We implement our model based on transformers   ( Wolf et al . , 2020)and employ pre - trained mod-   els to obtain global contextualized representation .   Specifically , for general domain datasets ( i.e. , W16 ,   W17 and ON5e ) , we use BERT - cased - large ( De-   vlin et al . , 2018)and XLNET - large - cased ( Yang   et al . , 2019b)as our Hero module . For biomedical   datasets , BioBERT ( Lee et al . , 2020)is utilized   to obtain global information . We follow their de-   fault settings for all BERT , XLNET , and BioBERT :   24 layers of self - attention with 1024 dimensional   embeddings . For hyperparameters of the Gang   module , the hidden sizes of bidirectional recurrent   structures for each window size are half of the em-   bedding dimension from the output of the Hero   module ( i.e. , 512 ) . During the training process ,   we use Adam ( Kingma and Ba , 2014 ) to optimize   the negative log - likelihood loss function . More   training details are shown in the Appendix A.1 . Be-   sides , we also compare four operations to combine   different level features from the Hero and Gang   module : MLP - Attention , DOT - Attention , concate-   nation , and summation , respectively , where con-   catenation is to connect all features directly through1927   s= [ h , h,···,h , z ] , and summation is to add   up these features by s = h+h+···+h+z .   3.3 Baselines   To explore the impact of our proposed model , we   compare our model to the previous studies . For   general domain , following baselines are compared   in our experiment on W16 , W17 and ON5e .   •CNN - BLSTM - CRF ( Chiu and Nichols , 2016 )   utilizes a hybrid bidirectional and CNN architec-   ture to detect word - and character - level features .   •BERT ( Devlin et al . , 2018 ) is a pre - trained lan-   guage model and we apply it to the NER task by   direct fine - tuning .   •SANER ( Nie et al . , 2020b ) , CL - KL ( Wang et al . ,   2021 ) and AESUBER ( Nie et al . , 2020a ) im-   prove entity recognition by leveraging syntactic   information or semantically relevant texts .   •H - NER ( Luo et al . , 2020 ) utilizes both   sentence - level and document - level representa-   tions to improve sequence labeling .   •S - LSTM - CRF ( Xu et al . , 2021 ) integrates   the structured information by graph - encoded rep-   resentations obtained from GNNs .   •BARTNER ( Yan et al . , 2021 ) formulates NER   tasks as a span sequence generation problem .   In addition , we also compare our proposed model   with the following baselines on the aforementioned   biomedical datasets :   •MTM - CW ( Wang et al . , 2019a ) , BLM(Sachan et al . , 2018 ) , NCBI_BERT ( Peng et al . ,   2019 ) , MT - BNER ( Tong et al . , 2021 ) uti-   lize multi - task learning or transfer learning to   enhance biomedical NER .   •BBERT ( Lee et al . , 2020 ) is a pre - trained   model trained with a large amount of biomedical   corpus and then applied by directly fine - tuning .   •KB - LM ( Yuan et al . , 2021 ) proposes a   biomedical pre - trained language model that in-   corporates knowledge from the Unified Medical   Language System ( UMLS ) .   Note that in both general and biomedical domains ,   our model does not require external resources .   4 Results and Analyses   4.1 General Domain NER   In this subsection , to explore the effectiveness of   our proposed model , we conduct experiments to   compare our model with existing studies , and the   results are reported in Table 2 . There are sev-   eral observations drawn from different aspects .   First , when we make a fair comparison without ex-   tra resources ( e.g. , BERT , XLNET , and ASTRA ) ,   our model obtains significant improvements on   all datasets in terms of Precision , Recall , and F-   1 , which confirms the effectiveness of our pro-   posed Hero - Gang neural structure . This is because   multiple - level features can be reasonably encoded   into the model and thus alleviate the limitations of1928   Transformer in local feature extraction . Second ,   although some complicated models enhance NER   by incorporating extra knowledge , e.g. , SANER   uses augmented semantic information , Hire - NER   utilizes two - level hierarchical contextualized rep-   resentations , and CL - KL selects a set of seman-   tically relevant texts to improve NER , our model   achieves competitive results without such require-   ments . This is because each word in the natural text   usually has a closer relationship with its surround-   ing words , especially the adjacent words , such that   features extracted by the Gang module can provide   more valuable information for NER , and thus our   model achieves promising performance . Third , the   XLNET - based model obtains better results than the   BERT - based model , which indicates that XLNET   can generate more effective representations on the   NER task . The reason behind this might be that   XLNET combines the permutation operation with   the autoregressive technology to further improve   representation learning , so that XLNET can pro-   vide a better text understanding than BERT .   4.2 Biomedical NER   We also compare our model with state - of - the - art   models in the biomedical NER on the aforemen-   tioned datasets with all results reported in Table 3 .   There are several observations . First , we can see   that our model outperforms existing methods , re-   gardless of whether they introduce external knowl-   edge , which further confirms the validity of our in-   novation in combining local and global features to   enhance feature extraction . Second , although some   models utilize higher - level features , e.g. , BIOKM-   NER leverages POS labels , syntactic constituents , dependency relations , and MTM - CW employs   multi - task learning to train the model , our model   can achieve better results through a simple Hero-   Gang structure . This means that local features ex-   tracted from the Gang module under the guidance   of global information are also effective in assist-   ing biomedical text representations and even show   more significant potential than those special de-   signs for the medical domain ( i.e. , domain - related   multi - task learning ) . Third , the models using the   multi - window attention ( i.e. , DOT - Attention and   MLP - Attention ) outperform those using concate-   nation or summation . This observation suggests   that multi - window attention can elaborately weigh   local features from different sliding windows to   enhance feature combinations .   4.3 Analyses   Effect of position information Recurrent struc-   tures are able to extract both context and position in-   formation by its token - by - token manner while other   network structures , including CNN and MLP , fail to   encode the relative position information . Thus , to   explore the effect of position information , we com-   pare models with different structures to construct   the Gang module and report the improvements of   F-1 score based on different Gang modules in Fig-   ure 2 . First , we can observe that models with Gang   module are better than Base ( i.e. , BERT ) , where   all the values in Figure 2 are positive , further il-   lustrating the effectiveness of our innovation in   combining both global and local features , no mat-   ter what type of structure is used to construct the   Gang module . Second , models with LSTM and   GRU outperform those with CNN and MLP , indi-1929   cating that recurrent structures are more promising   in short sequence feature extraction . Since the re-   current structures can effectively capture position   information by its token - by - token manner and help   the model understand word - word relations based   on their relative positions , we may conclude that   position information is vital for improving perfor-   mance . Third , the comparison between CNN and   MLP shows the power of CNN in extracting fea-   tures from sub - sequences since CNN can leverage   more fine - grained features , such as n - gram .   Ablation studies In this subsection , we compare   our multi - window model with single - window mod-   els , and the improvements compared with Base   model are shown in Figure 3 . We have following   observations . First of all , illustrated by the compar-   isons among Base ( i.e. , BERT ) and others , models   with sliding windows achieve better performance ,   where all the improvement values in Figure 3 are   positive . This illustrates that both single window   and multi - window recurrent structures can help to   enhance token representation and bring different   degrees of improvement , which further shows the   importance of local features in this task . Second ,   we can observe that the optimal single window   sizes for different datasets are also different . For   example , the optimal single window size of W17   is 5 , while that for BC2GM is 7 , which indicates   that the best length of the local sequence depends   on the characteristics of datasets to some extent .   Third , compared with those models using a single   window , the multi - window recurrent module ob-   tains better performance , illustrating that features   extracted from multiple sub - sequences are more   effective than those captured from a single one .   The reason could be that multi - window can help   the model pay attention to different local context   sub - sequences and give them appropriate weights   through the multi - window attention mechanism ,   such that it can provide more reasonable local in-   formation and alleviate the impact of the character-   istics of the datasets themselves .   Case Study To further show the validity of our   model , we perform qualitative analysis on some   cases with their real labels and predicted labels   from different models . Figure 4 shows two cases   from ON5e and BC5 - C , respectively . We can ob-   serve that our model can predict more complete   entities than Base . Specifically , in the first case ,   our model can recognize all the words in the en-   tity"a period of years " while Base model only   recognizes the word " years " . In the second case ,   our model is able to identify " Monosodium gluta-   mate " , but Base model regards these words as two   different entities . In addition , in the first example ,   compared with real labels , our model can label two   " of"correctly with the help of local features , which   are O and I - date , respectively , while Base classifies   both " of " as O. The sub - sequence ( i.e. , " a period   of years , " ) from the second " of " is usually used   to describe time such that this information is able   to assist the model in marking the " of " as I - date .   However , for the first " of " , its sub - sequence " divest   themselves of such speculative " does not contain   any meaning related to the entity themes , and thus   the model marks the corresponding " of " as O.1930   5 Related Work   NER is a fundamental task in NLP ( Huang et al . ,   2015 ) , which has drawn substantial attention over   the past years and there have been many studies   to address this task . Recently , deep learning has   played a dominant role in NER due to its effec-   tiveness in capturing contextual information from   sequences . The recurrent neural networks ( RNN ) ,   including its variants such as LSTM ( Hochreiter   et al . , 1997 ) , and GRU ( Cho et al . , 2014 ) , is a   promising structure for solving this task since it   can effectively learn sequence information with its   recurrent mechanism ( Ma and Hovy , 2016 ; Huang   et al . , 2015 ; Chiu and Nichols , 2016 ; Zhu and   Wang , 2019 ) . However , it is ineffective for RNN to   learn long sequences due to the gradients exploding   and vanishing . Thus , Transformer - based models ,   such as BERT ( Devlin et al . , 2018 ) , BioBERT ( Lee   et al . , 2020 ) , and XLNET ( Yang et al . , 2019b ) ,   are proposed to alleviate these problems with the   help of the self - attention mechanism . Compared to   RNN , Transformer is able to capture long - distance   information through multiple multi - head attention   layers and has achieved impressive performance   in this task ( Nie et al . , 2020b ; Luo et al . , 2020 ;   Yamada et al . , 2020 ; Gui et al . , 2019 ) .   However , multi - head attention usually treats ev-   ery position identically , which lead to the loss of   position information . To mitigate this problem , sev-   eral approaches have been proposed to advance the   Transformer ( Dai et al . , 2019 ; Shaw et al . , 2018 ;   Yan et al . , 2019 ) . Shaw et al . ( 2018 ) proposed   cross - lingual position representation to help self-   attention alleviate word order divergences in differ-   ent languages and learn position information . Yanet al . ( 2019 ) introduced the directional relative po-   sitional encoding and an adapted Transformer En-   coder to model the character - level and word - level   features . Although these position embeddings are   able to help the model learn position information ,   they are still not enough to solve the issue appro-   priately ( Wang et al . , 2019b ; Huang et al . , 2020 ;   Qu et al . , 2021 ) . Besides , Transformer - based ap-   proaches can not effectively extract local features   that are also important for sequence learning tasks ,   and some studies have been proposed to alleviate   this problem ( Xu et al . , 2017 ; Li et al . , 2019 ; Yang   et al . , 2019a ) . Xu et al . ( 2017 ) proposed to use the   fixed - size ordinally forgetting encoding to model   sentence fragments , which is then used to predict   the label for each text fragment . Li et al . ( 2019 )   utilized convolutional self - attention by producing   queries and keys with causal convolution to incor-   porate local contextual information into the atten-   tion mechanism . To address these issues , we offer   an alternative solution , namely Hero - Gang Neural   model , to enhance local and position information   extraction via multiple recurrent structures under   the guidance of global information .   6 Conclusion   In this paper , we propose a novel Hero - Gang Neu-   ral ( HGN ) structure to effectively combine global   and local features for enhancing NER . In detail , the   Hero module aims to capture global understand-   ing by a Transformer - based encoder , which is then   used to guide the Gang to extract local features   and relative position information through a multi-   window recurrent module . Afterward , we utilize   the multi - window attention to elaborately combine1931the global information and local features for en-   hancing representations that are then used to pre-   dict the entity label for each token . Empirically ,   our proposed model achieves new state - of - the - art   results on several NER benchmark datasets , includ-   ing both general and biomedical domains . Besides ,   we compare different structures to construct the   Gang model and investigate the effect of the num-   ber of sliding windows , which further illustrates   the effectiveness of our proposed model .   Acknowledgements   This work is supported by Chinese Key - Area Re-   search and Development Program of Guangdong   Province ( 2020B0101350001 ) , NSFC under the   project “ The Essential Algorithms and Technolo-   gies for Standardized Analytics of Clinical Texts ”   ( 12026610 ) and the Guangdong Provincial Key   Laboratory of Big Data Computing , The Chinese   University of Hong Kong , Shenzhen .   References193219331934A Appendix   A.1 Hyper - parameter Settings   We have tested several combinations of hyper-   parameters in tuning our models for all NLP and   Biomedical benchmark datasets ( i.e. , W16 , W17 ,   ON5 , BC5CDR - disease , BC2GM , and BC5CDR-   chem ) . Table 4 reports the combinations that   achieve the highest F-1 score for each dataset.1935 M H .NLP D B D   W16 W17 ON5BC2GM BC5 - D BC5 - C   HGN ( MLP)Window Size { 1,3,5,7 } { 3,5,7 } { 5,7,9 } { 1,3,5 } { 5,7,11 } { 5,7,11 }   Learning Rate 3e-5 5e-5 1e-5 1e-5 9e-6 1e-5   Batch Size 32 32 32 32 32 32   HGM ( DOT)Window Size { 3,5,7 } { 5,7,9 } { 3,5,7 } { 3,5,7 } { 5,7,9 } { 5,7,11 }   Learning Rate 3e-5 5e-5 1e-5 1e-5 9e-6 9e-6   Batch Size 32 32 32 32 32 321936