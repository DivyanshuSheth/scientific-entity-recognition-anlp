  Toshiki Kawamoto , Hidetaka Kamigaito , Kotaro Funakoshi and Manabu Okumura   Tokyo Institute of technology   { kawamoto,kamigaito,funakoshi,oku}@lr.pi.titech.ac.jp   Abstract   A repetition is a response that repeats words in   the previous speaker ’s utterance in a dialogue .   Repetitions are essential in communication to   build trust with others , as investigated in lin-   guistic studies . In this work , we focus on repe-   tition generation . To the best of our knowledge ,   this is the first neural approach to address repe-   tition generation . We propose Weighted Label   Smoothing , a smoothing method for explicitly   learning which words to repeat during fine-   tuning , and a repetition scoring method that   can output more appropriate repetitions during   decoding . We conducted automatic and human   evaluations involving applying these methods   to the pre - trained language model T5 for gen-   erating repetitions . The experimental results   indicate that our methods outperformed base-   lines in both evaluations .   1 Introduction   Dialogues can build a trusting relationship with   others , thus are essential in our daily lives ( Schein ,   1993 ; Searle and Vanderveken , 1985 ) . There are   several types of responses in dialogues , and the one   we focus on is repetitions ( Tannen , 1987 ) . A repe-   tition is a response that uses the previous speaker ’s   words or phrases . Figure 1 shows an example . The   phrases " a bear " and " came out " are repeated . Rep-   etitions frequently appear in a conversation with   diverse roles , e.g. , to indicate attentive listening ,   confirm the previous utterance , and show agree-   ment or sympathy ( Machi , 2019 ; Shimojima et al . ,   2002 ) . Many linguistic studies investigating repe-   titions have concluded that they are important for   building and strengthening relationships between   speakers ( Tannen et al . , 1989 ; Johnstone , 2002 ;   Norrick , 1987 ; Brown , 1999 ) . From the above lin-   guistic point of view , we can say that repetitions   are indispensable in dialogues .   Repetitions are similar to paraphrases and re-   flections , which are component skills of counsel-   Figure 1 : Example repetition . Listener ’s response uses   words from previous speaker ’s utterance . Yellow words   indicate those that are repeated and green words indicate   those in the repetition .   ing ( Theron , 2008 ) , in terms of using the previ-   ous speaker ’s utterance . Paraphrases and reflec-   tions have been generated using a template - based   method ( Han et al . , 2013 ) .   While many studies have tackled general re-   sponse generation with neural network - based   frameworks ( Adiwardana et al . , 2020 ; Zhang et al . ,   2020 ) , less attention has been paid to repetitions .   This might be because they are buried in a huge   amount of response data . Therefore , we focus on   automatically generating repetitions . To the best of   our knowledge , this is the first study on generating   repetitions with a neural approach . We used the   pre - trained language model T5 ( Raffel et al . , 2019 )   for generating repetitions because it has performed   well in language generation in past few years ( e.g. ,   Radford et al . ; Raffel et al . , 2019 ; Lewis et al . ,   2020 ) .   In generating repetitions , it is important to take   into account which words should be repeated from   the previous utterance . The repeated words might   represent objective facts , names of people and   places , and the speaker ’s experiences and emotions ,   though they are different depending on the lan-   guage ( Machi , 2008 ) . When we use a pre - trained   language model , however , the model can not explic-   itly learn the repeat likelihood among words during   fine - tuning because it is difficult to directly teach   which words are likely to be repeated at this step .   To solve this problem , we propose Weighted La-   bel Smoothing ( WLS ) , which is an improvement   upon Label Smoothing ( LS ) ( Szegedy et al . , 2016 ) .   The method enables a language model - based re-852sponse generator to learn the words it should use   for each input utterance during fine - tuning . We   also propose the repetition scoring method ( RSM )   to expand the scoring method proposed in Wu et al .   ( 2016 ) for selecting repetitions that contain appro-   priate repeated words during decoding .   We evaluated the proposed methods on a dataset   we created in Japanese for automatic and human   evaluations . Our methods outperformed baselines ,   i.e. , fine - tuned pre - trained language models with-   out our methods , in both evaluations . This indicates   that our methods can generate repetitions that con-   tain appropriate words to repeat .   Our contributions are as follows :   1.To the best of our knowledge , this is the first   study to use a neural model for generating   repetitions .   2.We will release our code and the dataset of   repetitions we created .   3.We propose WLS , that takes into account   words that should be repeated during fine-   tuning , for generating repetitions .   4.We propose RSM to select repetitions contain-   ing appropriate repeated words during decod-   ing .   2 Proposed Methods   Repetitions do not necessarily mean we repeat   any word . For the utterance " Today ’s dinner was   pizza . " , the repetition " Oh , you ate pizza . " is more   appropriate than " Oh , you ate today . " However , a   fine - tuned pre - trained language model alone may   not be enough to generate repetitions with appro-   priate repeated words . Therefore , to generate a re-   sponse that repeats more appropriate words , we in-   troduce repeat scores ( § 2.1 ) to calculate how likely   a word is repeated and incorporate the scores into   WLS ( § 2.2 ) for fine - tuning and RSM ( § 2.3 ) for   beam search in decoding .   2.1 Repeat Score   We should give high scores to words that tend to   be used in repetitions and low scores to words   that should not be . Since only content words   ( nouns , verbs , adjectives , or adverbs ) are repeated   in Japanese , we define a repeat score only for them .   Since subwords are used as a unit in a pre - trainedlanguage model , all the subwords in the same con-   tent word receive the same repeat score .   We use BERT ( Devlin et al . , 2019 ) to construct   a model for scoring the repeat scores in the range   of [ 0 , 1 ] . We pass the final hidden state of BERT   through SpanExtractor ( Lee et al . , 2017 ) for each   word and then convert the vector to a scalar value   through a multi - layer perceptron , which has a sig-   moid function as the last layer . In the training data ,   the label is set to 1 if the target content word was   repeated , and 0 if it was not . The output is then   normalized by applying min - max scaling .   2.2 Weighted Label Smoothing ( WLS )   In this section , we explain how to learn words   to repeat when fine - tuning a pre - trained language   model for repetition generation . Neural response   generation models try to optimize cross - entropy   loss . Let Xbe a previous utterance and Ybe a   response , where Yis divided into subwords as   Y = y , . . . , y. Letting Kbe the total number of   subwords and vbe the k - th subword , the cross-   entropy loss is defined as follows :   L(q , p ) = −q(v ) log{p(v|y , X ) } ,   where p(v|y , X)is the probability of vthat the   model outputs at time step tgiven X , and q(v )   is the probability of vin a target distribution that   the model aims for . When a one - hot distribution   is used , q(v)is as follows with a function δ ,   which becomes 1 when v = y :   q(v ) = δ .   When LS is used , however , q(v)is as follows with   uniform distribution u(v ) = 1 /K :   q(v ) = ( 1 −ϵ)δ+ϵu(v ) ,   where ϵis a hyperparameter .   A one - hot distribution and LS can not learn a sub-   word to repeat explicitly because there are labels   other than the target , i.e. , vwhen v̸=y , that   have the same q(v ) . Therefore , we propose WLS ,   which takes into account how likely a subword is   repeated . We use repeat scores , explained in § 2.1 ,   instead of u(v ) . The q(v)of WLS is defined as   follows :   q(v ) = ( 1 −ϵ)δ+ϵr(v )   K,853   where r(v)is the repeat score forv , and γis   a hyperparameter . We use the q(v)of WLS as   the distribution in the cross - entropy loss function .   Subwords in the previous speaker ’s utterance are   weighted in accordance with their r(v ) . Note that   if we set γ= 0 , WLS is the same as LS .   2.3 Repetition Scoring Method ( RSM )   Pre - trained language models usually use beam   search in decoding . We propose a scoring method ,   RSM , to select more appropriate repetitions in the   beam search . RSM is an extension of a scoring   method for machine translation in Wu et al . ( 2016 ) .   The original scoring method uses a length normal-   ization procedure and coverage penalty ( Tu et al . ,   2016 ) . Length normalization treats sentences of   different lengths equally . The coverage penalty   gives a high score to a sentence that is most likely   to cover all the words in the source sentence . Since   the original scoring method can not select a repeti-   tion with appropriate repeated words , we modify   the method by adding repeat scores , which indicate   words to repeat . Letting Ybe a candidate response   during beam search and Xbe the previous utter-   ance , the generation probability is P(Y|X ) . The   scoring function s(Y , X)of RSM is as follows :   where αandβare hyperparameters for length nor-   malization and coverage penalty , respectively . We   carry out two modifications to the original scoringmethod to yield RSM . First , we use the attention   value of pwithout suppression . In contrast to   machine translation , in which an input and output   have a one - to - one relationship , lengths of an input   and output are not the same in repetition generation ,   and so it is not suitable to suppress the attention   value under 1.0 . Second , we add the term rs(Y , X ) ,   which represents the sum of repeat scores for sub-   words in the response .   3 Dataset   We manually created pairs of a speaker ’s utterance   and its repetition as our dataset using a crowdsourc-   ing service . Since repetitions often occur when   a listener replies to a speaker , we used utterances   in a corpus of listening dialogues ( Yoshino et al . ,   2018 ) between an elderly person and caregiver or   clinical psychologist as the speaker ’s utterances   in our dataset . In this corpus , the elderly person   tends to be a speaker and the others are listeners .   We extracted the elderly person ’s utterances con-   taining content words for creating a repetition . The   number of extracted utterances was 5,548 . We   asked three crowdsourcing workers to create rep-   etitions for each utterance . Specifically , a worker   was shown two utterances before each target utter-   ance and asked to create a repetition , that supports   the creation of context - aware repetitions . When   the workers found it difficult to create a repetition   for an utterance , they could discard it . The total   number of workers was 333 .   Examples from the dataset are given in Table 1 .   The size and statistics of our repetition dataset are   shown in Tables 2 and 3 . The word overlap rate   is the percentage of words in an utterance that are854   repeated in a repetition . The content - word over-   lap rate is the percentage of content words of an   utterance that are repeated . Comparing the average   numbers of tokens , repetitions are much shorter   than utterances . This may indicate that repetitions   can not be produced simply by copying the utter-   ances , and we need to select information that is   worth repeating from the utterances .   To understand what types of words overlap , Ta-   ble 4 shows the percentage of all words ’ parts - of-   speech and overlapped words ’ parts - of - speech in   utterances . Since " postpositional particles " and   " auxiliary verbs " tend to accompany content words   in a Japanese unit called ‘ bunsetsu ’ , it might be   natural that they also appear in repetitions in high   percentages .   While we can have at most three repetitions for   an utterance in our dataset , we used only one ran-   domly selected repetition for an utterance in the   training data . We used all repetitions for an utter-   ance for the evaluation on the validation and test   data to consider the diversity of responses .   4 Experiments   4.1 General Setup   Repeat scores were calculated from the training   data . SentencePiece ( Kudo and Richardson , 2018 )   was used to segment the dataset into subwords .   With WLS , the hyperparameter ϵwas set to 0.1   following a previous study ( Szegedy et al . , 2016 ) ,   andγwas tuned to 4with the validation data , as   explained in Appendix A. With RSM , we used   α= 0.2andβ= 0.2 , following a previous study   ( Wu et al . , 2016 ) , and a beam size of 5 . We used   MeCabas a tokenizer to identify content words .   4.2 Compared Methods   The baseline methods were as follows :   Rule - Based is a rule - based method , with which   a response is created with a content word in the   speaker ’s utterance + " desuka " ( " is it ? " ) . The con-   tent word is randomly selected from the utterance .   Examples of rule - based responses are given in Ta-   ble 5 . Responses made with Rule - Based always   contain a repeated word and have few grammat-   ical errors . However , " desuka " can not cover all   situations . " desuka " was chosen because 52 % of   repetitions in our dataset ends with " desuka " , and   6.1 % of repetitions are a single word + " desuka " .   BertSumAbs ( Liu and Lapata , 2019 ) is a model   trained with BERTas the encoder and randomly   initialized Transformer as the decoder .   T5(Raffel et al . , 2019 ) is a model that was fine-   tuned with the repetition dataset .   LSis T5 fine - tuned with LS .   Copy is T5 fine - tuned with the copy mechanism   ( See et al . , 2017 ) . Since the copy mechanism can   be considered similar to the repetition model in that   it is used to generate the same words as in an input   sentence , we used it for comparison.855   Note that the T5 and BERT were versions pre-   trained in Japanese . Our methods are as follows :   WLS is T5 fine - tuned with WLS , as mentioned in   § 2.2 .   RSM is T5 using RSM during beam search , as   mentioned in § 2.3 .   WLS + RSM is T5 fine - tuned with WLS and using   RSM during beam search .   4.3 Automatic Evaluation   The evaluation metrics were ROUGE ( RG-1 , RG-2 ,   RG - L ) ( Lin , 2004 ) and the percentage of outputs   containing correct repeated words . The correct   repeated words are content words repeated in the   gold response . The experimental results are listed   in Table 6 . WLS + RSM obtained the highest scores   for all metrics , confirming the effectiveness of both   WLS and RSM .   We conducted an ablation study to analyze the   results of RSM . The results are listed in Table 7 .   Since w/o rsreceived the lowest scores , rswas   considered the most effective .   Examples of an input and generated responses   from the baseline and our model are shown in Table   8 . The proposed model ( WLS + RSM ) successfully   generated a response that was close to the correct   response , focusing on " having friends who play   Go " .   4.4 Human Evaluation   We also conducted a human evaluation by com-   paring three types of response generation methods :   Rule - Based , T5 , and our model ( WLS + RSM ) . The   evaluation measures were grammaticality ( Gram ) ,   relevance ( Rel ) , coherence ( Cohe ) , and whether   repeated words are included ( Rep ) . Two hundred   pairs were randomly selected from the test data .   The responses were shown to five workers and eval-   uated on a three - point Likert scale . The response   was evaluated with the previous speaker ’s utter-   ance and one turn before the speaker ’s utterance as   context , meaning the context helps in determining   whether the response is an appropriate repetition .   The total number of evaluators was 110 .   Average scores from the evaluation are listed in   Table 9 . WLS + RSM outperformed the other meth-   ods for all measures , confirming its effectiveness .   5 Conclusion   We focused on repetition generation . Although rep-   etitions play an important role in dialogues , there   has been no neural approach for this task to the   best of our knowledge . We proposed WLS , which   is an improvement upon LS , during fine - tuning and   RSM , which is an extended scoring method , dur-   ing decoding for repetition generation . Through   automatic and human evaluations , we confirmed   that our model can generate repetitions that con-   tain more appropriate words to repeat than baseline   models . For future work , we will take into account   synonyms and multiple gold repetition instances to   calculate repeat scores for improving the diversity   of responses . We are also planning to incorporate   our repetition model into a general response gener-   ation framework.856Acknowledgements   We thank Dr. Koichiro Yoshino of RIKEN for   providing the listening dialogue data .   Ethical Statement   Neural generative models have the potential to gen-   erate unexpected responses such as violent remarks .   As we focused on repetition generation , its model   repeats a user ’s utterance , and so there is little   chance of causing unintended responses compared   with chit - chat dialogue systems . However , this   does not mean that unintended responses will never   appear , e.g. , when a user ’s utterance is an unin-   tended expression . Thus , the same consideration   must be taken as with other dialogue systems .   Our dataset was created to repeat from utterances   in a privacy - secured dataset , and so there is no   privacy issue . Since the license of the original   dataset is CC BY - NC 4.0 , we could use it for this   study . We define that the license of our dataset is   also CC BY - NC 4.0 .   References857858   A Exploring Hyperparameter γ   We explored the effect of γon the percentage of   responses containing a correct repeated word . The   model we used for experiments was the pre - trained   model T5 , fine - tuned with the training data in § 3 .   We generated repetitions on the development data .   The results are listed in Table 10 . The best score   was recorded when γ= 4.0 . Therefore , we used   this value .   B P - values   We now discuss the p - values in the experimen-   tal results . To obtain p - values , we conducted the   Wilcoxon rank sum test to compare the effective-   ness between baseline models and our proposed   models . Table 11 shows the p - values for Table 6   from LS . Table 12 shows those for Table 9 from   T5.859