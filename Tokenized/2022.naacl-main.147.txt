  Xiaomeng Jin , Manling Li , Heng Ji   University of Illinois Urbana - Champaign   { xjin17 , manling2 , hengji}@illinois.edu   Abstract   Event schema depicts the typical structure of   complex events , serving as a scaffolding to ef-   fectively analyze , predict , and possibly inter-   vene in the ongoing events . To induce event   schemas from historical events , previous work   uses an event - by - event scheme , ignoring the   global structure of the entire schema graph . We   propose a new event schema induction frame-   work using double graph autoencoders , which   captures the global dependencies among nodes   in event graphs . Specifically , we first extract   the event skeleton from an event graph and de-   sign a variational directed acyclic graph ( DAG )   autoencoder to learn its global structure . Then   we further fill in the event arguments for the   skeleton , and use another Graph Convolutional   Network ( GCN ) based autoencoder to recon-   struct entity - entity relations as well as to detect   coreferential entities . By performing this two-   stage induction decomposition , the model can   avoid reconstructing the entire graph in one   step , allowing it to focus on learning global   structures between events . Experimental re-   sults on three event graph datasets demonstrate   that our method achieves state - of - the - art perfor-   mance and induces high - quality event schemas   with global consistency .   1 Introduction   Event Schema ( Chambers and Jurafsky , 2008 , 2009 ;   Balasubramanian et al . , 2013 ; Nguyen et al . , 2015 ;   Modi et al . , 2016 ; Li et al . , 2021 ) is induced from   historical events to describe the common or stereo-   typical evolution pattern of complex events . Figure   1b shows an example schema of complex event   “ IED - bombing ” ( Improvised Explosive Device ) ,   where multiple events are inter - connected via tem-   poral links ( e.g. , T happens after A- ) and arguments ( e.g. , the transporting E- is the weapon that is being assembled ; thetransporting D is controlled by the as-   sembler , i.e. , the A ) . It enables a descriptive   analysis of inter - event structures , as well as the pre-   diction of future events over temporal - based and   argument - based structures .   A number of methods have been proposed   for learning event schemas from instance event   graphs , called event schema induction , which   fall into three categories . The set - based ( Cham-   bers , 2013 ; Sha et al . , 2016 ; Huang et al . , 2016 )   andsequence - based ( Granroth - Wilding and Clark ,   2016 ; Rudinger et al . , 2015 ) methods treat a com-   plex event as a set or a linear sequence of atomic   events , respectively . However , they fail to cap-   ture the multi - dimensional evolution of real - world   complex events , i.e. , multiple events may precede   or follow one event . Graph - based methods ( Li   et al . , 2020 , 2021 ) , instead , adopt graphs to for-   mulate event schemas . A graph model is usually   learned from instance event graphs through gener-   ating the schema event by event . Although graph-   based methods are theoretically superior to the first   two categories , existing graph - based methods are   limited to modeling only the local structure of event   graphs , i.e. , the first - order dependency of an event   node with respect to its neighbors , while ignor-   ing the high - order andglobal dependencies among   atomic events in the entire graph .   However , modeling the global structure of event   graphs is crucial to event schema induction . The   global structure enables the model to be aware of   the position of each event node in the entire graph .   It allows the model to better comprehend the role of   a specific event in the complex event . For example ,   in Figure 1b , there are three T events in   the schema , but they differ regarding the item being   transported , i.e. , bombs , the injured , or suspects .   The global structure context enables the model to   differentiate the position of the three T   events and predict their neighbor events precisely .   Moreover , when the model only has access to local2013   structure information during the generation process ,   the schema generated may only be consistent at the   local level , but not at the global level . For example ,   the schema may keep repeating the sequence of   A -T -A . From this   perspective , the global structure information can   be viewed as the supervision that guides the entire   generation process to be globally consistent .   To capture the global structure information of   complex events , in this paper , we propose a new   event schema induction approach using double   graph autoencoders . Graph Autoencoder ( Salha   et al . , 2019 ; Zhang and Chen , 2018 ) is known to be   able to preserve the structural information of an en-   tire graph in the embedding space . Therefore , our   key idea is to use graph autoencoders to capture theglobal dependency among nodes in event graphs .   Specifically , our model contains two graph auto-   encoders that are organized in a hierarchical man-   ner : ( 1 ) To model the skeleton of an event graph ,   we design a high - level variational graph autoen-   coder for directed acyclic graphs ( DAGs ) . Event   skeleton is a subgraph of an event graph , which   consists of salient events and their temporal orders ,   representing the fundamental structure of event evo-   lution . ( 2 ) With the event skeleton as the global   context , we decorate entity nodes in the skeleton by   introducing a low - level graph autoencoder based on   Graph Convolutional Network ( GCN ) . It takes as   input an expanded event skeleton and reconstructs   the original event graph by adding coreferential   entities and entity - entity relations .   These two graph autoencoders decompose the   process of event schema induction into two steps ,   thereby avoiding the need to reconstruct the entire   graph directly , and improving the schema learn-   ing efficiently at both the high level ( event schema   skeleton ) and the lower level ( entity - entity rela-   tions ) .   We conduct extensive experiments on three event   graph datasets . The experimental results demon-   strate that our proposed method achieves state - of-   the - art performance on event schema induction .   Additionally , we show in a case study that the event   schema generated by our method is more reason-   able and globally consistent .   We make the following novel contributions :   •We propose a two - stage global structure aware   schema induction framework , providing a   global context of event skeleton to determine   inter - event interactions via arguments .   •We introduce a double graph autoencoder that   preserves the global structural information ,   allowing the model to capture high - order de-   pendencies between nodes .   •We propose a comprehensive set of metrics for   structure - aware comparison between schema   graphs and instance graphs .   •Our method significantly outperforms base-   lines , demonstrating the effectiveness of con-   sidering global structure context in event   schema induction .   2 Problem Formulation   Our data resources come from news and Wikipedia   articles that describe a series of complex events.2014   We extract events , entities , as well as their relations   using Information Extraction ( IE ) tools ( Wu et al . ,   2008 ; Li et al . , 2011 ; Hogenboom et al . , 2013 ; Lin   et al . , 2020 ) , then construct instance event graphs .   An instance event graph consists of two types of   nodes : event nodes and entity nodes , and we use   tto represent the type of node i. Similarly , we   usetto denote the edge type between node iand   nodej . Accordingly , there are three types of edges   in an instance graph : ( 1 ) the event - event temporal   link(i , j ) , which represents a temporal order that   event jhappens after event i , with tandtindicat-   ing their event types , such as T ; ( 2 ) the   event - entity argument link ( i , a , m ) , which repre-   sents that event ihas an argument entity m , which   plays the argument role t = a , such as A ;   ( 3 ) the entity - entity relation ( m , r , n ) , which rep-   resents that there is a relation t = rbetween   entity mand entity n , such as A .   Given a set of instance graphs { G , G , · · · }   that belong to the same topic , our goal is to learn   a schema Sthat summarizes the instance graphs   and represents their underlying common evolution   pattern . Note that different from instance graphs ,   nodes ( events and entities ) in the schema Sare not   instantiated but represented by their types .   3 Our Approach   3.1 Overview   We design two graph autoencoders , where the first   autoencoder deals with the high - level skeleton of   an event graph and the second autoencoder focuseson the low - level arguments of an event graph .   As shown in the upper side of Figure 2 , the high-   level autoencoder , which is specially designed for   directed acyclic graphs ( DAGs ) , takes an instance   event skeleton as input , and encodes the event skele-   ton as a probability distribution in the embedding   space . Then it reconstructs the input skeleton by   feeding a vector sampled from the distribution into   the decoder .   After the event skeleton is reconstructed , we dec-   orate the entity nodes according to the pre - defined   argument roles of each event ( as shown on the right   side of Figure 2 ) . However , the entity - entity rela-   tions and coreference links among arguments are   still missing in the expanded event skeleton . There-   fore we introduce a low - level graph autoencoder   to take as input an expanded event skeleton and   reconstruct the original event graph ( as shown in   the lower part of Figure 2 ) . The low - level graph au-   toencoder employs Graph Convolutional Network   ( GCN ) to encode each node into an embedding   vector , then predicts the type of an entity - entity   relation based on the learned entity embeddings .   3.2 Event Skeleton Generation   An instance event graph can contain up to hundreds   of nodes , but the majority are entity nodes that are   associated with event nodes . Therefore , as shown   in the upper left of Figure 2 , our first step is to   extract the event skeleton Gfrom the instance   graph G , which serves as the backbone of G.   For a given instance event graph G , we use a2015graph neural network ( GNN ) based variational au-   toencoder to process its event skeleton G⊆G.   Traditional GNNs learn the node representations   by aggregating information from their neighbors   iteratively , then apply a readout function to all node   representations and output the representation of the   entire graph ( Xu et al . , 2019 ) . However , off - the-   shelf GNNs are not suitable for modeling event   skeleton , because event skeleton is a DAG whose   nodes follow an intrinsic partial order , whereas ex-   isting GNN models focus more on capturing the   local structure of a graph .   Encoding . To capture the global structure of event   skeleton G , inspired by Zhang et al . ( 2019 ) , we   design a new GNN architecture for the encoder , in   which messages can only pass forward following   event - event temporal orders . Specifically , for a   given event node iin the event skeleton G , its   representation sis computed by :   s= AGG / parenleftbig   { s|(j , i)∈G } ∪t / parenrightbig   , ( 1 )   where tis the one - hot event type vector of an event   node i , and we use it as the initial feature for the   event . AGG(·)is the aggregate function . In Eq .   ( 1 ) , we only consider predecessors as neighbors ,   allowing the model to capture the temporal flow   of the event graph . Considering that predecessors   contribute differently to predicting the current event   node , we utilize a self - attention function as the   aggregate function to characterize the importance   of different predecessors :   s = σ / parenleftbigg   W / summationdisplaySoftMax / parenleftbig   α / parenrightbig   s+Wt / parenrightbigg   ,   ( 2 )   where α= LeakyReLU / parenleftig   w / bracketleftbig   Ws , Wt / bracketrightbig / parenrightig   is ( unnormalized ) attention weight , wis a learn-   able vector , WandWare learnable matrices ,   [ · , · ] denotes concatenate operation , and σ(·)is a   nonlinear activation function .   It is worth noting that according to Eq . ( 1 ) , s   can only be computed after its predecessors ’ rep-   resentations have been computed , which implies   that the encoder computation sequence naturally   follows the topological order of the event skele-   ton . Specifically , we first create a dummy event   node START that has an outgoing link to each   event node that has no predecessor , and a dummy   event node END with an incoming link from each   event node that has no successor . Then we compute   the representations of all event nodes according toAlgorithm 1 : Event Skeleton Generation   their topological order . Finally , the representation   ofEND , i.e. ,s , is taken as the output of the   encoder .   Decoding . The decoder follows a GNN architec-   ture similar to the encoder , as presented in Algo-   rithm 1 . The input is the encoder output s   forG , and our goal is to reconstruct the skeleton   graph G. Specifically , we first obtain the pos-   terior approximation p(·|G)by calculating the   mean vector µand the diagonal covariance matrix   Σvia two Multi - Layer Perceptrons ( MLPs ) ( line 2 ) .   Then we sample a vector sfrom the Gaussian dis-   tribution N(µ,Σ)as the global representation of   the skeleton graph , which will be used throughout   the following generation process ( line 3 ) . The rep-   resentation of the generated graph gis initialized   as an all - zero vector ( line 4 ) .   Next , the decoder generates a DAG node by node   ( line 5 - 17 ) . When generating a new event node i ,   the decoder computes the event type distribution   for node ito obtain a sampled type t(line 6 ) . The   distribution is learned based on the concatenation   ofsandg , summarizing the input skeleton graph   and the generated graph . Based on the value of   t , the decoder performs one of the following two   steps:2016•If the generated event node iisEND , then the   decoder will stop the generation procedure ,   and connect all nodes that do not have any   predecessor to the END node ( line 8 - 10 ) .   •Otherwise , the decoder uses another MLP to   predict the edge probability of node iwith ex-   isting nodes , and add the generated edges into   the graph ( line 12 - 15 ) . After the generation   step for node iis completed , the decoder then   computes the representation sfor node iac-   cording to Eq . ( 2 ) ( line 16 ) , and updates the   generated graph representation gbys(line   17 ) . The updated gwill be further used to   generate a new node for the next iteration .   Different from existing event schema induction   methods that are only able to model the local struc-   tural information of event graphs , our method en-   codes the global information of an event graph as   the global graph representation s , and further ap-   plies this global state to guide the entire generation   process . This enables our model to fully capture   the structural information of event graphs , which   has been shown to be extremely important for event   schema induction .   3.3 Entity - Entity Relation Completion   After generating the event skeleton , we aim to com-   plete the generated event schema graph by adding   back entities as well as edges associated with these   entities . We take advantage of the event ontology ,   where each event has a predefined set of argument   roles . For example , an A event has an- argument role whose type can be   ( PER ) , as well as a argument role whose   type can be ( LOC ) . Therefore , we first   expand the event skeleton by adding the predefined   event arguments back to each event , as shown in   the right of Figure 2 . Yet , such an expanded event   skeleton Gis not the final event schema , because :   ( 1 ) entity - entity relations are missing , e.g. , the L- _ relation between a PER entity and a   LOC entity in Figure 1b ; ( 2 ) entities in Gcan   be coreferential , which require to be merged . For   example , in Figure 1b , the of the   T event is also the where the   A event happens .   We formulate the above two cases as a unified   entity - entity edge missing problem , by treating   coreference as a special type of entity - entity re-   lation between unmerged nodes . To predict miss-   ing entity - entity relations , we design a low - levelgraph autoencoder . It reads the expanded event   skeleton Gas input , and then reconstructs the   original event graph Gby adding entity - entity re-   lations back , as shown in the lower part of Figure   2 . During the inference stage , the learned graph   autoencoder can therefore take a generated event   skeleton as input , and output a comprehensive event   schema graph .   Encoding . Different from the high - level autoen-   coder whose purpose is to generate event skele-   ton , the low - level autoencoder is to reconstruct   the relations between entities , therefore , the low-   level graph autoencoder is expected to be non-   probabilistic , and therefore we adopt Graph Convo-   lutional Networks ( GCN ) ( Kipf and Welling , 2017 )   as our encoder . Let sdenote the representation   at iteration kfor the node i∈G , which can be   either an event or an entity . The encoder updates   node representations iteratively for k= 1 , · · · , K ,   where Kis the layer number of the GCN encoder :   s = σ / parenleftig   W / summationdisplayαs / parenrightig   .(3 )   HereN(·)denotes the set of neighbors of node   i , α= 1//radicalbig   |N(i)| · |N ( j)|,Wis a learnable   matrix , and σis an activation function . sis ini-   tialized as the one - hot type vector of node i , i.e. ,   s = t. All edges in G , including event - event   temporal links and event - entity argument links , are   treated as undirected when counting neighbors , due   to our focus on the local graph structure . The en-   coder output is the final representation sof node   i∈G.   Decoding . The decoder takes as input the final   node representations sfrom the encoder , and   reconstructs entity - entity relations in the original   event graph G. The entity - entity relation can be   one of the following three cases : no relation , coref-   erence , or predefined entity - entity relation types in   the ontology . Therefore , we create two new rela-   tion types N - R andC - R ,   and add them into the original set of entity - entity   relations as the prediction target . Specifically , the   decoder predicts the relation type tof two given   entities iandjby a MLP :   ˆt= MLP / parenleftbig   [ s , s]/parenrightbig   , ( 4 )   where ˆtdenotes the predicted relation type . It is   noteworthy that N - R dominates the pre-   diction targets , making the classification problem   highly unbalanced . To address this issue , we divide2017the task into two steps : predicting the existence of   edges and then predicting the type of a known edge ,   to optimize the learning effectiveness .   3.4 Training and Generation   Training . The training objective for the high - level   variational graph autoencoder consists of a recon-   struction loss and a regularization loss :   L= Dist ( G , G)+KL / parenleftbig   N(µ,Σ),N(0,1)/parenrightbig   ,   ( 5 )   where the first term Dist ( G , G)measures the   distance between the input event skeleton Gand   the reconstructed event skeleton G. We sum   up the negative log - likelihood of each decoding   step by forcing them to generate the ground truth   event type or edge at each step . The second   term KL / parenleftbig   N(µ,Σ),N(0,1)/parenrightbig   forces the distribu-   tion output by the encoder to be close to the stan-   dard Gaussian distribution , to ensure a smooth em-   bedding space .   The loss function for the low - level GCN graph   autoencoder is the cross - entropy loss between pre-   dicted and ground - truth entity - entity relations :   L=/summationdisplayCELoss ( ˆt , t ) . ( 6 )   Schema Generation . We are able to generate the   event schema by decoding the trained model . We   first sample a global graph representation from   N(0,1)in the embedding space of the high - level   variational graph autoencoder . It is then fed into the   decoder to generate a schema skeleton with event   nodes only . Afterwards , the skeleton is further   fed into the low - level graph autoencoder to predict   coreferential entities and entity - entity relations .   4 Experiments   4.1 Datasets   We conduct experiments in the scenario of IED   bombings following the state - of - the - art graph-   based schema induction literature ( Li et al . , 2021 ) .   Specifically , three subtypes of complex events for   IED are considered , including General IED , Car   bombing IED , and Suicide IED . To construct the   dataset , we select a set of Wikipedia articles re-   lated to IED bombings and identify the references   in each Wikipedia article , then collect news articles   from those references . We use RESIN ( Wen et al . ,2021 ; Du et al . , 2022 ) , a state - of - the - art IE system ,   to extract event mentions , relations , and entity men-   tions from these news articles , and perform entity   coreference resolution . We also do human curation   to correct obviously erroneous event - event links   ( e.g. , a temporal link indicating that an I   event happens after a Devent for one person ) .   The statistics of these three datasets are summa-   rized in Table 1 .   4.2 Baselines   We compare our proposed method with graph   schema induction baselines :   ( 1)Temporal Event Graph Model ( TEGM ) ( Li   et al . , 2021 ) . It is the state - of - the - art graph schema   induction model , which generates the entire event   graph using an auto - regressive graph generation   model . We compare with it to explore the effec-   tiveness of the two - stage framework that generates   event skeleton first and provides a global context   for argument generation .   ( 2)Frequency - Based Sampling ( FBS ) . To ex-   amine the effectiveness of our graph autoencoder   to generate event skeleton , we compare with a   frequency - based baseline . It constructs the event   schema based on the frequency of event - event tem-   poral links in the training data . Specifically , for   each possible pair of event types ( t , t ) , we com-   pute the number of edges in training graphs whose   two associated events exactly have type tandt .   After that , we construct a schema graph in which   each node corresponds to one event type , and there   is no edge in the schema graph initially . Then at   each timestamp , we sample one pair of event types   according to their frequency , and add this sampled   edge into the schema graph . The procedure is re-   peated until we detect a cycle in the schema graph   after adding a new edge .   4.3 Experimental Setup   Evaluation Metrics . For a given dataset , we first   train our model on the training instance graphs ,   then generate the event schema according to the   steps presented in Section 3.4 . To evaluate the   quality of the generated schema , we compare the   schema with test instance graphs in terms of the   following metrics , to see how the schema captures   the structure information of test instance graphs :   ( 1)Event type match . We compute the set of   event types in the generated schema graph , as well   as the set of event types in one test instance graph,2018   then we compute how similar these two sets are by   calculating the F1 score between the two sets .   ( 2)Event sequence match . We collect all event   sequences with a length of 2 or 3 from the gen-   erated schema graph and one test instance graph ,   respectively , and compute the F1 score between   these two multi - sets .   ( 3)Node / edge type distribution . We compute   the node type distribution and edge type distribu-   tion of the generated schema and one test instance   graph , respectively , then compute the KL diver-   gence between the node / edge type distributions of   the schema and each test instance graph .   ( 4)Maximum common subgraph ( MCS ) . A max-   imum common subgraph of two graphs is an in-   duced subgraph of both graphs , and that has as   many nodes as possible . The size of the maximum   common subgraph can reflect the global structure   similarity between two graphs . Therefore , we com-   pute the number of nodes and edges of the maxi-   mum common subgraph between the schema and   each test instance graph .   Note that the last two metrics , i.e. , node / edge   type distribution and maximum common subgraph ,   are new metrics proposed by us . We compute the   above metrics between the schema and each test   instance graph , then report the average values on   all test instance graphs .   Hyperparameter Settings . For the high - level vari-   ational autoencoder , the dimension size of node   hidden state is 256 , and the dimension size of the   Gaussian distribution is 56 . The learning rate is   10 , and the number of training epochs is 700.For the low - level GCN autoencoder , we use a two-   layer GCN as the encoder , whose dimension sizes   of hidden layers are 256 and 64 , respectively . The   learning rate is 10 , and the number of training   epochs is 500 . We investigate how the size of train-   ing instance graphs and the dimension of hidden   node state influence the model performance , with   results shown in Appendix A.   4.4 Results   Comparison with baselines . All methods are eval-   uated on the same test set . Our method achieves   the best performance on both original and revised   datasets . We only show results on the revised   dataset since it is cleaner and the induced schema   is more reasonable . In Table 2 , our method Dou-   bleGAE achieves significant gains compared with   baselines on event type , event sequence matching ,   and maximum common subgraph matching . It   demonstrates that capturing the global node de-   pendency in event graphs is essential to event   schema induction . In contrast , Temporal Event   Graph Model ( TEGM ) does not consider global   graph structure , thus has a large performance gap   compared to our model . It is worth noticing that   Frequency - Based Sampling ( FBS ) is a competitive   baseline according to the experimental result , espe-   cially when measured by KL divergence of edge   type distribution . This is because FBS constructs   an event schema exactly based on the frequency of   edges in the training instance graphs .   Case study . We plot the schema skeleton gen-   erated by Temporal Event Graph Model ( TEGM)2019   and our model ( DoubleGAE ) in Figure 3 . In Figure   3a , there is a recurrence of certain local structures   of events within the schema . e.g. , T -   A andA -A . This demonstrates   that TEGM is able to successfully capture and   memorize the local structure information of event   graphs . However , such learned local structures may   not always be reasonable , since an A event   is not likely to repeat immediately after another   A multiple times . In addition , TEGM may   not be able to learn the correct position of an event   in the entire schema , for example , the I   event happens at the very beginning of the schema .   The result indicates that TEGM fails to maintain   global consistency when generating schemas . In   contrast , there are no consecutively repeated events   in the schema induced by our model , and the entire   schema is more logically meaningful in both local   and global levels .   We also present the complete schema output   of our model after filling in event arguments and   entity - entity relations in Figure 3c . As we can   see , our model successfully identifies coreferential   entities , e.g. , the transporter of T , the   perpetrator of C , and the attacker of A   belong to one entity . Our model is also able to add   entity - entity relations between event arguments , for   example , the victim of a C event is “ located   near ” the place where this C event happens .   5 Related Work   Existing event schema induction methods can be   classified into three categories : set - based schema ,   sequence - based schema , and graph - based schema .   The set - based methods represent event triggers   by a set without modeling their inter - relations   ( Chambers , 2013 ; Cheung et al . , 2013 ; Nguyen   et al . , 2015 ; Sha et al . , 2016 ; Huang et al . , 2016 ;   Yuan et al . , 2018 ; Huang and Ji , 2020 ; Shen et al . ,   2021 ; Zeng et al . , 2021 ) , which can be regarded   as atomic schema induction . In contrast , we aim   to induce schemas for complex events involvingmultiple events .   Another line of work focuses on the sequence-   based methods , which takes event - event relations   into account , and orders event structures into se-   quences ( Chambers and Jurafsky , 2008 , 2009 ;   Rudinger et al . , 2015 ; Granroth - Wilding and Clark ,   2016 ; Pichotta and Mooney , 2016 ; Modi , 2016 ;   Weber et al . , 2018 , 2020a ) . Instead of representing   events as structures , some work treats events as nat-   ural language steps and induces schema knowledge   through story ending prediction ( Mostafazadeh   et al . , 2016 ; Weber et al . , 2020b ; Kwon et al . ,   2020 ) , machine reading comprehension ( Oster-   mann et al . , 2018 , 2019 ) , and schema goal - step   prediction ( Zhang et al . , 2020 ; Yang et al . , 2021 ) .   Instead of ignoring event structures or organizing   events as simple sequences , we aim to capture the   multi - dimensional evolution of events , as well as   the structured connections .   As a further step , researchers propose to use   graphs to represent schemas ( Wanzare et al . , 2016 ;   Modi et al . , 2016 ; Li et al . , 2020 , 2021 ) , where   each event can be followed by multiple alterna-   tive outcomes . Li et al . ( 2021 ) introduce the con-   cept of “ complex event schema ” , a comprehensive   graph schema consisting of both temporal orders   and multi - hop argument relations , allowing time- ,   location- , and argument - based tracking of events .   However , it adopts an auto - regressive graph gen-   eration model , which only models the first - order   dependency of an event node with respect to its   neighbors . In contrast , we propose to encode a   global graph context via event skeleton generation   using double auto - encoders .   6 Conclusion and Future Work   In this work , we propose a novel event schema   induction framework using double graph autoen-   coders , i.e. , a high - level variational graph autoen-   coder to learn the event skeleton , followed by a   low - level GCN graph autoencoder to reconstruct   entity - entity relations . Our autoencoders are able2020to preserve the global structural information of   event graphs , thus capturing the multi - dimensional   evolution of complex events , and providing global   context to consolidate argument relations . Exper-   iments demonstrate that our method significantly   outperforms baselines by generating high - quality   and globally consistent event schemas .   In the future , we aim to effectively induce   schemas from graphs of different sizes , especially   extremely large graphs . We also plan to make   use of event hierarchies and induce hierarchical   schemas with optimal event type granularity .   Acknowledgement   We thank the anonymous reviewers helpful sugges-   tions . This research is based upon work supported   by U.S. DARPA KAIROS Program No . FA8750-   19 - 2 - 1004 . The views and conclusions contained   herein are those of the authors and should not be   interpreted as necessarily representing the official   policies , either expressed or implied , of DARPA ,   or the U.S. Government . The U.S. Government is   authorized to reproduce and distribute reprints for   governmental purposes notwithstanding any copy-   right annotation therein .   References202120222023A Additional Discussions   Impact of training graph size . It is shown in Ta-   ble 1 that instance event graphs consist of hundreds   of nodes on average . To see if the size of training   instance graphs can affect the model performance ,   we set a threshold for the size of training instance   graphs , and only train our method on those graphs   whose size is less than the given threshold . The   result is presented in Figure 4a , which shows that   our model achieves the best performance when the   threshold is relatively small . The result demon-   strates that including large training instance graphs   will not help improve the model performance , be-   cause larger instance graphs may have more noisy   events nodes with repeated types .   Impact of dimension of node hidden state . We   also investigate the impact of dimension of node   hidden state in the high - level variational graph au-   toencoder to model performance . The result is pre-   sented in Figure 4b , which demonstrates that our   model performs best when dim= 256 in all three   datasets , since a too large or a too small dimension   will lead to performance drop due to over - fitting or   under - fitting .   B Implementation Details   B.1 Details of Dataset Cleaning Strategy   For each complex event , we construct an instance   graph , where coreferential events or entites are   merged . Among the events we include those that   are connected through entity coreference links , or   that have temporal relationships . The graph can   then be extended using related events that share   arguments or that are linked by a relation . We   consider isolated events to be irrelevant nodes   in schema induction , therefore they are excluded   from the instance graphs during graph construc-   tion . In schema graphs , type labels and nodeindices are used to represent each node , with   mention level information ignored . Although   RESIN is a state - of - the - art IE system , there still   remains some errors in event temporal links . There-   fore , we do human curation to remove obviously   erroneous links . Below are some event - event   link examples that we delete from the instance   graphs : ( D , I ) , ( A JD ,   A ) , ( EP , S P ) ,   ( D , D ) , ( D , E BS ) ,   ( S , D ) , ( EP , S ) ,   ( T C , R P ) .   C Scientific Artifacts   C.1 RESIN Information Extraction System   Data License and Usage We obtain the code from   the open - source information extraction system   RESIN ( Wen et al . , 2021 ) . We run the code at   https://github.com/RESIN-KAIROS/   RESIN - pipeline - public . The system is   released for research purpose and is licensed   under the GNU General Public License v3 or later .   The system covers the general news domain and   supports three languages , i.e. , English , Spanish   and Russian . It does not contain offensive content .   Discussions about IE Quality The performance   of each component is shown in Table 3 ( Wen et al . ,   2021 ) . Although IE graphs are noisy , schema in-   duction can still benefit from it . It is because that   the schema induction task aims to find the recurring   patterns , which will still be preserved even in the   noisy data .   C.2 IED Schema Induction Corpus   Data License and Usage We obtain the dataset   from the state - of - the - art graph schema induction   literature ( Li et al . , 2021 ) . The dataset is released   for research purpose and is licensed under the GNU   General Public License v3 or later . The system   covers the general news domain and is an English   corpus .   Data Collection We utilize the news articles in the   state - of - the - art graph schema induction literature   ( Li et al . , 2021 ) in https://github.com/   limanling / temporal - graph - schema .   We collect associated news articles concerning   each complex event type , such as Car - bombing   IED , using Wikipedia as a source . As a first step ,   we search candidate Wikipedia categories based   on the name of the complex event type , and then   dig deeper into each page to identify complex2024   events that belong to that category . Afterwards ,   we collect the reference news articles for each   complex event , use these articles as the cluster   of documents relating to the complex event , and   perform IE to construct the instance graph . Using   this Wikipedia - based data collection approach , we   have been able to cover a wide range of scenarios ,   including most complex events that occur in   human society , such as Disease outbreak and   Disaster . Therefore , our schema induction method   does not depend on manual work and is not limited   to a specific complex event scenario.2025