  Paula Fortuna ,   Mónica Domínguez   NLP Group   Pompeu Fabra University   Barcelona , Spain   first.last@upf.eduLeo Wanner   ICREA and   Pompeu Fabra University   Barcelona , Spain   leo.wanner@upf.eduZeerak Talat   Simon Fraser University   Burnaby , Vancouver   zeerak_talat@sfu.ca   Abstract   Addressing hate speech in online spaces has   been conceptualized as a classification task that   uses Natural Language Processing ( NLP ) tech-   niques . Through this conceptualization , the   hate speech detection task has relied on com-   mon conventions and practices from NLP . For   instance , inter - annotator agreement is concep-   tualized as a way to measure dataset quality   and certain metrics and benchmarks are used   to assure model generalization . However , hate   speech is a deeply complex and situated con-   cept that eludes such static and disembodied   practices . In this position paper , we critically   reflect on these methodologies for hate speech   detection , we argue that many conventions in   NLP are poorly suited for the problem and en-   courage researchers to develop methods that   are more appropriate for the task .   1 Introduction   Online hate speech is the cause for growing con-   cern , due to its social impacts ( Soral et al . , 2018 ) .   In response , automatic detection of hate speech has   become a popular research area in Natural Lan-   guage Processing ( NLP ) , with a substantial number   of papers at leading conferences , targeted work-   shops and shared tasks dedicated to it . Simultane-   ously , content moderation infrastructures that rely   on machine learning technologies have been pro-   posed to address the propagation of online harms .   The increase in research interest to hate speech   detection has spurred on a growth and variety in   annotated resources for the task created within   the academy and industry . However , at the same   time , critical work on hate speech detection has   found that there are significant challenges related   to the published research outcome with respect to   the construction of data ( Vidgen and Derczynski ,   2021 ; Fortuna et al . , 2020 ) , model generalizability   ( Nozza , 2021 ; Fortuna et al . , 2021 ) , and socially bi-   ased effects of models ( Talat et al . , 2018 ; Davidsonet al . , 2019 ) . These challenges indicate that despite   a techno - optimist attitude from the NLP commu-   nity to the task ( Talat et al . , 2021 ) , the acceleration   in research on the topic has impeded a maturation   of the basic conventions , principles and practices   which this research is based on . Hence , it seems   timely to assess whether the NLP conventions and   practices that have been followed so far are indeed   appropriate for the classification of hate speech .   In contrast to prior work , which has sought to   address specific shortcomings of machine learning   algorithms for hate speech detection ( e.g. , Dixon   et al . , 2018 ) , our work presents a comprehensive   analysis of the inadequacies of some of NLP ’s   methodological conventions for algorithmic hate   speech detection . That is , in this position paper we   reflect on contemporary methodological challenges   that arise from applying common NLP methodolo-   gies to hate speech detection .   More specifically , we a ) reflect on frequently   used conventions for classification of text and ex-   plore their inadequacies with reference to hate   speech detection ; and b ) discuss the future of cur-   rent methodologies for this task . On the basis   of our analysis , we conclude that current mod-   els are incapable of detecting hate speech without   harms to marginalized communities . We there-   fore call for the scientific community to adapt NLP   methodologies such that future developments cen-   ter the impacts that used methodologies may have   on marginalized communities . We believe that by   critically reflecting on the potential real - world im-   pacts of the methodologies for hate speech detec-   tion on marginalized communities , the scientific   community can come to identify methodologies   that result in more just futures .   2 Reflections On Hate Speech Detection   Hate speech detection is commonly conceptualized   as a supervised classification task , with the goal to   determine whether content is hateful or not ( Yin11794and Zubiaga , 2021 ) . This setting requires that we i )   define the problem ; ii ) collect , sample , and annotate   data to obtain labels;and iii ) apply optimization   technologies ( i.e. machine learning algorithms ) to   the labeled data . Finally , the resulting methods and   models are evaluated using specific metrics and   techniques . In this section , we review these stages   through the lens of hate speech detection .   2.1 Definitional Challenges of Hate Speech   Defining hate speech is to control the discourse sur-   rounding the phenomena ; determine which groups   are minoritized , and therefore should be protected ;   and which patterns of speech should be sanctioned   ( Gelber , 2021 ) . Coining a definition of hate speech   is therefore a political task , in particular due to the   implication that each of the many available possi-   ble definitions carry ( Thylstrup and Talat , 2020 ) .   While definitions are subject to the cultural norms   of the geography in which they are created ( Talat   et al . , 2022 ) , universalist assumptions surround-   ing established definitions of hate speech in NLP   fail to account for the diversity required for the   task . Such universalist assumptions allow for hate   speech detection infrastructures as “ a third layer of   interpretation ” between the sender and recipient of   a message ( Thylstrup and Talat , 2020 ) .   Moreover , hate speech is often categorized un-   der the umbrella terms such as “ abusive language ” ,   “ offensive language ” , or “ toxicity ” ( Poletto et al . ,   2021 ; Jigsaw , 2019 ) , resulting in a concept drift ,   where hate speech cedes prominence to the more   generic concepts , such as generally offensive lan-   guage . As a result , models are prone to learn   patterns that emphasize the more frequently occur-   ring categories ( e.g. ‘ insult ’ ) and under - perform on   hate speech ( Fortuna et al . , 2020 ) .   Furthermore , most NLP research exclusively   considers textual material , assuming that it pro-   vides adequate information . However , hate speech   is deeply tied to oppression and it is therefore nec-   essary to understand the speaker and listener ’s sub-   jectivities to situate the text and adjudicate whether   it constitutes hate speech . More often than not , this   information is unavailable from the text.2.2 On Challenges of Hate Speech Annotation   A common convention in labeling a dataset is to   use an odd number of annotations for each text   sample . The reliability of the labels in a dataset is   often measured by computing the Inter - Annotator   Agreement ( IAA ) . In this section , we discuss bi-   ases in annotations and the paradoxical search for   ground truth within disagreement .   2.2.1 Annotation Bias   Socially biased systems are a growing concern   within NLP ( Blodgett et al . , 2020 ; Talat et al . ,   2022 ) . Social biases are particularly apparent in   hate speech datasets ( and models ) as biases are   a reflection of wider social tension . Data source   selection and sampling strategies can also be con-   tributors to the social biases found in datasets and   models . For instance , data samples often skew   towards particular perpetrators or keywords ( e.g. ,   slurs ) , resulting in datasets that favor explicit abuse   and hate speech ( e.g. Davidson et al . , 2017 ; Basile   et al . , 2019 ; Founta et al . , 2018 ) . Collected data   samples are then annotated through the lens of the   selected definitions of hate speech , and the IAA is   computed . Once satisfactory levels of agreement   have been obtained , the “ ground - truth ” for each   text is selected ( Pustejovsky and Stubbs , 2012 ) , of-   ten by selecting the label chosen by the majority of   annotators . Thus , labels embed the subjectivities   of the annotators . For instance , if three annotators   agree that “ cats are better than dogs ” , the agree-   ment reflects the annotators ’ subjective preferences ,   in spite of complete agreement ( i.e. IAA = 1.0 ) ,   rather than the inherent value of cats or dogs .   The annotation challenge is further aggravated   by the absence of widely agreed - upon annotation   criteria ( Vidgen et al . , 2019a ) , resulting in unclear   categories . For instance , the term “ abusive ” has   been described in terms of the speakers ’ intent to   injure ( Pitsilis et al . , 2018 ) and in terms of the as-   sumed impact on the reader ( Wulczyn et al . , 2017 ) .   Further , as annotators most often can not commu-   nicate with the authors or the targeted subjects ,   annotators must make assumptions with respect to   the intention of the authors of a text and its im-   pact on readers – as , e.g. , in the case of the above   reclaimed slurs ( Sap et al . , 2019 ) .   The selection of annotators is another source of   social biases ( Talat et al . , 2021 ) . Annotators are of-   ten recruited from crowd - working platforms , with   little regard to their subject matter expertise or so-11795cial and cultural background . However , annotators ’   subjectivities , expertise ( Waseem , 2016 ) , attitudes   and beliefs ( Sap et al . , 2021 ) , and their diversity   and variability ( Hovy and Prabhumoye , 2021 ) have   been shown to influence annotation results in spite   of training and exposure to annotation guidelines .   2.2.2 On Ground Truth and Agreement   The goal for annotation efforts in NLP is to assign   a gold label to data ( e.g. , a document or an entity   therein ) ( Zeinert et al . , 2021 ) . The search for a sin-   gle label in the face of disagreement is based on the   assumption that there exists a single correct label   which can be approximated using agreement ag-   gregation methods . IAA is used as a proxy for the   quality , i.e. , correctness , of obtained labels . In the   context of hate speech , IAA is often very low ( Vi-   gna et al . , 2017 ; Olteanu et al . , 2018 ; Poletto et al . ,   2019 ) . However , researchers often rely on a sin-   gle label as ground truth , disregarding the absence   of agreement , variability , and subjectivity of the   obtained ground truth ( Paullada et al . , 2021 ) . The   result is that , paradoxically , researchers construct   ground truth for inherently subjective questions on   the basis of disagreement .   2.3 Model Learning and Evaluation   Once a dataset has been labeled , models can be   trained and evaluated . To assess model perfor-   mance and generalizability , the trained models are   evaluated on held - out test sets ( Chollet and Al-   laire , 2018 ; Pustejovsky and Stubbs , 2012 ) . This   paradigm of evaluation assumes that training data   and data encountered when a model is deployed   are independent and identically distributed ( I.I.D. )   ( Arlot and Celisse , 2010 ) . For hate speech , the   I.I.D. assumption means that the annotation of a   text is independent of earlier annotations of other   texts , and that the data sampled from outside of the   dataset will follow the same class distribution that   is evident in the labeled dataset . Below , we detail   the limitations that can arise from the assumptions   made in for model evaluation and the risks from   drawing conclusions from them .   2.3.1 On Model Understanding   Although contemporary machine learning models   often show an impressive performance when ap-   plied to different NLP tasks , they have been crit-   icized for failing to grasp pragmatics due to their   reliance on the distributional hypothesis ( Bender   and Koller , 2020 ) . This is particularly concerningfor addressing hate speech as it operates at the lin-   guistic level of pragmatics . It is therefore important   to understand how machine learning models make   judgments on whether texts are hateful . Deeper as-   sessments of hate speech models suggest that they   have a very superficial understanding of language   ( see appendix A ) . In fact , prior work has argued that   the reported performances for hate speech detection   are in part influenced by spurious correlations ( e.g.   Rahman et al . , 2021 ; Wiegand et al . , 2019 ) , and   overlapping data in the train and test sets ( Arango   et al . , 2019 ) . This work has shown that correcting   of these issues results in a decrease of performance .   If models are over - fitting to spurious correlations   and are incapable of language understanding , the   question arises whether we can rely on current clas-   sifiers for robustly detecting hate speech .   2.3.2 On Interpreting Model Performance   Recent work on quantitative benchmarks has ques-   tioned the ability of contemporary methods to mea-   sure generalization in machine learning ( Raji et al . ,   2021 ; Paullada et al . , 2021 ) , and for hate speech   ( Röttger et al . , 2021 ) . Researchers have found that   well - performing systems for hate speech detection   are susceptible to minor adversarial modifications   of the input text that significantly alter the meaning   ( Gröndahl et al . , 2018 ) . That is , solely relying on   quantitative benchmark results can produce an in-   complete picture of the performance of evaluated   models , leading researchers to over - estimate the   performances of systems that are brittle in nature .   2.3.3 On Model Generalization   One reason models may be vulnerable to adversar-   ial attacks is that they over - fit to tokens and token   interaction patterns instead of learning to general-   ize the concept of hate speech ( Oliva et al . , 2020 ;   Sarkar and KhudaBukhsh , 2021 ; Oak , 2019 ; For-   tuna et al . , 2021 ) . Although this issue has been   identified by prior work , we reconsider it in light   of the limitations we have discussed thus far .   We identified the following three factors that   make the I.I.D. assumption unlikely to hold , re-   sulting in models that are unlikely to generalize :   1 ) Given the variety of concepts and definitions   in hate speech , it is very hard to assure that the   different samples express the same flavor of the   phenomena ; 2 ) due to the fact that hate speech only   occurs very rarely in random samples ( Vidgen et al . ,   2019b ) , sampling strategies tend to over - emphasize   domains where hate speech is more likely to occur,11796making it unlikely that two independent enterprises   in dataset creation will result in complementary   datasets ; and 3 ) the speed at which linguistic shift   happens in online social media ( Hogan and Quan-   Haase , 2010 ) makes it unclear if time - bounded data   collections from social media can be I.I.D. This   also raises an open question for the development of   machine learning models for hate speech detection :   How can we identify when it becomes necessary to   train new models to keep abreast with the changes   that have occurred in language use since the train-   ing data was sampled ?   3 Discussion : On the Present and Future   for Hate Speech Detection   In the preceding sections , we have highlighted a   number of challenges and ethical concerns that   arise from the current conceptualization of hate   speech detection . We argue that these limitations   render current models unable to detect hate speech   without significant risk to minorities . Specifically ,   classifiers that are unable to accurately classify con-   tent directed towards marginalized communities   risk increasing the costs for said communities to   participate in online spaces , due to the increased   risks of being subject to hate speech whilst also re-   maining unprotected by hate speech detection sys-   tems ( Oliva et al . , 2020 ) . Thus , contemporary sys-   tems for hate speech detection risk reproducing nor-   mative values and attitudes towards acceptable lan-   guage use while further entrenching marginaliza-   tion in online spaces ( Thylstrup and Talat , 2020 ) .   Overcoming the identified challenges will re-   quire shifting our research practices . In this section ,   we propose new directions for hate speech detec-   tion . However , we do not expect that implementing   any individual solution in isolation will result in   ready to use classifiers . We therefore emphasize   the need for research to continuously reassess the   risks that arise from methodological innovations   for hate speech detection .   Accounting for Plurality of Hate Speech While   contemporary methods for annotating hate speech   imply the assumption that there is a universal defi-   nition of hate speech , and that models derived from   labeled data are applicable across all contexts , we   argue instead for a pluralist approach to annotation .   By taking a pluralist approach , e.g. through situat-   ing models within subjective contexts , researchers   are afforded the ability to view hate speech as con-   textual to the subjectivities of the target of hate . For instance , by narrowing down definitions of   hate , clearly providing the geographical and cul-   tural contexts , and specifying the values and goals   for the model , researchers can clearly articulate   within which contexts models and data are valid   and which particular groups models seek to protect .   Such model framing can help address the issues   surrounding universality and can provide space for   researchers to consider how their choices have po-   litical implications for what speech is sanctioned .   Accounting for Context Supervised machine   learning models for hate speech primarily oper-   ate on text , and a single label for each document   during training . However , whether a text amounts   to hate speech is highly context dependent ( Talat   et al . , 2018 ) . For instance , whether a word is used   as a slur or as a reclaimed term depends on the   identity of the speaker , the phrasal and social con-   texts in which it is uttered . The primary means of   approximating conversational context in prior work   has been through the use of conversation threads   and user metadata ( see appendix B.1 ) . Such conver-   sational contexts only account for a small number   of contexts that are invoked during the utterance   and annotation . Annotators , for example , may hold   prior knowledge on the histories , social hierarchies ,   conflicts , or stereotypes concerning the groups ad-   dressed in a document . Hate speech detection re-   search would therefore benefit from considering   methods to explicitly incorporate such information   into the modeling pipeline .   Representative Sampling Procedures Given   the sampling methods used to ensure an adequate   distribution of hate speech for labeling , models are   often trained on data distributions that significantly   vary from real - world occurrences of hate speech .   To address this concern , future data collection ef-   forts should seek to minimize such distributional   differences whilst taking into account wider no-   tions of conversational contexts .   Representative Annotation Processes The com-   mon NLP practices of having a small number of   annotations for each document ( often just three an-   notations per document ) that are used to compute   IAA and perform label aggregation erases different   opinions on the label of a document . In this way ,   the use of aggregation methods for label voting ex-   erts direct control over subjective positions on the11797labels , and thereby the discourses that the model   will come to replicate . However , the interpreta-   tion of what constitutes hate is a highly subjective   question and is subject to the workers ’ individual   subjectivities ( Waseem , 2016 ) . While subjectivities   are inherent , we propose that researchers use scales   ( DeVellis and Thorpe , 2021 ) to measure different   dimensions of hate speech . Scales have previously   been used in the social sciences for asking subjec-   tive questions , and could provide new possibilities   for hate speech research . In particular , the use of   scales can allow for framing models in terms of the   values that they embody .   Evaluation of Hate Speech Models In the last   few years , research on hate speech detection has   shown increases in performance across a number   of metrics . However , as we have argued , despite   quantitative improvements such performance in-   creases do not reveal a full picture of model perfor-   mances . In fact , contemporary models only display   a superficial understand of hate speech ( see ap-   pendix A for an analysis of Vidgen et al . ( 2021 ) ) . It   is therefore necessary for research on hate speech   to consider new evaluation paradigms and metrics .   Such initiatives must center models ’ abilities to   generalize beyond identifying frequently occurring   tokens . For instance , an emphasis on evaluating   models for over - fitting to particular tokens ( see ap-   pendix B.4 ) can provide a greater understanding of   model generalizability . Another promising direc-   tion is the creation of test suites that target potential   areas of concern for models for detecting hate ( e.g.   Röttger et al . , 2021 ) . Another avenue for improved   evaluation is to directly leverage training data to   create hard - to - pass tests for machine learning mod-   els . While such evaluation practices are a step in   the right direction , they do not address the ques-   tion of context . It is therefore necessary to develop   evaluation practices that seek to evaluate models   in the contexts that models are developed for , with   the aid of methods capable to identify if new data   is still I.I.D. to the data used for training .   Handling Classification Errors For many NLP   tasks classification errors do not have immediate   harms . For hate speech detection , classificationerrors can result in significant immediate harms to   people . False negatives can result in hateful speech   being passed as acceptable which can allow harm-   ful content to remain unsanctioned ( Oliva et al . ,   2020 ) . While false positives can result in inoffen-   sive speech being sanctioned . Given content mod-   eration ’s commitment to dominant social norms   ( Thylstrup and Talat , 2020 ) , classification errors of-   ten disproportionately affect marginalized commu-   nities , i.e. white supremacist content remains un-   sanctioned while content from marginalized com-   munities is removed ( Davidson et al . , 2019 ; Oliva   et al . , 2020 ) . In light of these concerns , it is prudent   for the NLP community and legislators to reflect   on the ramifications of classification errors . For   instance , we encourage both communities to reflect   on whether it is appropriate to deploy models that   will produce racialized and gendered classification   errors , which entity is to be held to account for such   errors , and how victims of automated classification   errors should be compensated .   4 Conclusion   In this paper , we have argued that current NLP   practices for hate speech detection are unlikely to   address the core concerns of hate speech detec-   tion , i.e. identify hate with minimal errors and   protect marginalized communities . We therefore   call for the NLP community to rethink its method-   ologies such that future developments reduce risk   for marginalized communities .   One avenue for future work is to follow the prin-   ciples of design justice ( Costanza - Chock , 2018 ) ,   which emphasizes community inclusion and own-   ership of ( technological ) solutions . Following the   principles of design justice , researchers would de-   center their own expertise in favor of the lived   expertise of affected communities . We strongly   believe that future steps must center a multi-   disciplinary approach in close communication with   affected groups . By taking steps to document and   address the limitations of contemporary methods   for hate speech , researchers can identify new av-   enues for improving hate speech detection models .   Moreover , researchers can take steps towards ensur-   ing that content moderation technologies provide   safer online spaces for marginalized communities   by mitigating the prevalence of online hate speech.11798Limitations   This study is intended as a theoretical considera-   tion of the issues that arise in hate speech detection .   The study analyzes the current limits of using ma-   chine learning infrastructures for the identification   and moderation of hate speech . One limitation   of the work is the theoretical frame of our work .   While our frame allows for more deeply under-   standing the issues of contemporary methods for   hate speech detection , deeper considerations of so-   ciological and anthropological methods can afford   significant improvements in our understanding of   NLP technologies , such as hate speech detection ,   as socio - technical systems and their social impacts .   A further limitation of our work is its focus on re-   search rather than application , and therefore does   not discuss how classification models are used in   real - world content moderation applications . This   is left to future work .   Acknowledgements   We thank the reviewers for their insightful com-   ments . The first author is supported by the research   grant SFRH / BD/143623/2019 , provided by the Por-   tuguese national funding agency for science , re-   search and technology , Fundação para a Ciência   e a Tecnologia ( FCT ) , within the scope of Opera-   tional Program Human Capital ( POCH ) , supported   by the European Social Fund and by national funds   from MCTES .   References117991180011801   A Hate Speech Dynabench Model   Classification Results   In this section , we provide examples of sentences   classified by a state of the art model for hate speech   detection ( i.e. the Dynabench classifier proposed   by Vidgen et al . ( 2021 ) ) to illustrate challenges forstate of the art models . This analysis is neither   quantitative nor exhaustive , however it seeks to il-   lustrate core issues for technical systems for the   classification of hate speech . Quantitative analysis   of mis - classifications have a set of concerns ( see   Section 2.3.2 for more detail ) . We submitted adver-   sarial cases to the model interface , and find that   model fails to predict correctly for simple adversar-   ial examples ( see table 1 ) .   We craft our adversarial examples on the basis   of western notions of sexism , starting with a sim-   ple example that we expect the model to correctly   classify : “ Women ’s place is in the kitchen ” . We   then introduce minor variations to this sentence in   order to gauge the model ’s understanding of sexism   ( see table 1 for a full list of our examples ) . For in-   stance , we replace the word “ women ” with “ men ” ,   “ oven ” , and “ gender ” . For these three cases , all   examples except for “ oven ” are predicted as hate   speech . While the model correctly identifies that   an oven does in fact belong in a kitchen , it ’s gender-   invariance for predicting hate speech reveals that   the model has not learned the relationship between   sexism and gender roles . That is , the model has   not learned the social factors which are crucial to   delineating hate speech from inoffensive speech .   When we keep the word “ women ” static and   change the remaining sentence to something in-   offensive , we see that the model performs better   which indicates a correct distinction between mi-   nority terms and hate speech . We continue with   new examples and introduce replacements to the   word “ kitchen ” . When replaced by “ school ” , the   model incorrectly predicts that it is hateful . This   further evidences the model ’s inability to distin-   guish prescriptions of actions from hate . More   concerning is the model ’s inability to correctly re-   late the implication of femicide , when evaluated   on the text “ Women ’s place is in the cemetery . ”   While this further provides further evidence that   the model does not learn a latent understanding of   power dynamics , it also illustrates that the model   may not provide adequate protections against vio-   lent speech towards women .   In our second set of examples ( examples 2.1-   2.3 ) , we examine how the model responds more   broadly to conversations around power dynamics .   We see here that the explicit mentioning of gen-   der and race prompts incorrect predictions from11802the model , i.e. , that the mere mention of compar-   ative privilege is labeled as hateful . Should this   model be deployed , it would actively limit conver-   sations around race , gender , and power dynamics   more broadly . Such conversations are frequently   had by communities that are marginalized , in ef-   forts to identify , discuss , and seek to remedy their   own marginalization . That is , the model would   censor conversations that are necessary to have , in   order for society to progress beyond contemporary   forms of marginalization , thereby actively limiting   movements for social progress .   In our final three examples , we see that the model   makes incorrect predictions for all three cases . In   the latter two cases , we see further evidence that the   model does not link notions of sexism and fascism   with their expressed goals of marginalization .   B Improvements for Hate Speech   Detection   We acknowledge that the NLP community is work-   ing towards identifying shortcomings of the current   research practices , e.g. , by studying how to learn   from disagreements ( Davani et al . , 2021 ) or dif-   ferent perspectives during annotation ( Leonardelli   et al . , 2021 ; Akhtar et al . , 2021 ; Kumar et al . , 2021 ) .   Here , we provide a brief summary of these efforts ,   which can also serve as a source of ideas for future   approaches to the problem .   B.1 Improving Hate Speech Modeling   To counteract the lack of contextual information ,   the latest developments have added information   to single text samples , including the conversation   threads ( Gao and Huang , 2017 ) , network data and   user information ( e.g. Mosca et al . , 2021 ) . Adding   conversational context , is a positive step forward   as it adds contextual information which can help to   frame the message being predicted . However the   inclusion of more context may not be sufficient for   addressing the participation of models in processes   of marginalization . As have argued in this paper ,   we believe that considering methods from design ,   which aim to center and give definitional power to   affected populations in the design and development   processes , can be a productive path forward .   B.2 Handling Search with Keywords   Some prior work has sought to evaluate the quality   of keyword - based data collection . For instance ,   ( Ousidhoum et al . , 2020 ) rely on topic modeling toevaluate text in datasets to assess the quality of key-   words . By applying LDA , the authors extract topics   from the dataset and compare them to the keywords   used to populate the dataset . The average stabil-   ity of the keywords and topics si then measured   by comparing the average similarity between the   keywords and topics predicted by the topic model .   Moreover , Ousidhoum et al . ( 2020 ) seek to identify   the best matchin terms between the keywords and   the words produced for each topic . By performing   and developing such analyses , researchers may be   able to evaluate the degree to which the collected   data reflects the data that they intended to collect ,   thereby improving the quality of datasets .   B.3 Improving Conventions for the   Annotation and Quality of a Dataset   Recent work has sought alternatives to IAA and   gold standards for general applications of machine   learning ( Uma et al . , 2021 ) . For hate speech , a num-   ber of studies have found novel methods to address   these issues ( Leonardelli et al . , 2021 ; Akhtar et al . ,   2021 ; Kumar et al . , 2021 ) . One potential solution   to such issues , is the notion of perspective - aware   models and datasets ( Akhtar et al . , 2021 ) . In this   framing , different subjectivities of people lead to   different models that embody such subjectivities .   To this effect , Akhtar et al . ( 2021 ) train different   classifiers on the basis of how data was annotated   by different groups , including groups that have   traditionally been marginalized and excluded . By   doing so , Akhtar et al . ( 2021 ) provide further legit-   imacy to their model , as these do not seek to repre-   sent a universal position on hate speech , but rather   seek to situate models within the lived realities of   people . While we believe that this approach con-   stitutes an improvement on predominant practices   in NLP , this approach also has several risks . First ,   the approach requires considering and discussing   which perspectives are warranted for the purposes   of developing resources and models for hate speech   detection . For instance , while it could be interest-   ing to understand how white supremacists would   view notions of hate speech , training models on   their annotations is unlikely to yield systems that   attain the goal of protecting marginalized commu-   nities . Rather , taking their perspective is likely to   engage actively in marginalization of traditionally   excluded and marginalized communities . Second ,   this approach raises ethical concerns surrounding   the exposure of hate speech to marginalized com-11803ID Text HS Dynabench %   1.1 Women ’s place is in the kitchen . hateful 99.88   1.2 Men ’s place is in the kitchen . hateful 98.32   1.3 Oven ’s place is in the kitchen . not - hateful 99.68   1.4 Gender ’s place is in the kitchen . hateful 98.32   1.5 Women are human beings . not - hateful 99.97   1.6 Women ’s place is in school . hateful 99.60   1.7 Women ’s place is in the cemetery . not - hateful 99.96   1.8 Women went to the kitchen . hateful 99.86   1.9 If a woman wants she can stay in her kitchen . hateful 98.06   1.10 Women ’s places are anywhere they wish to be . hateful 99.67   1.11 Women like to do many things , close or far   from the kitchen.hateful 97.78   1.12 Women kitchen . hateful 92.95   1.13 Kitchen women . not - hateful 76.75   2.1 White girls have more privilege than black   girls.hateful 99.69   2.2 White men are more privileged than white   women.hateful 95.86   2.3 White men have more privilege than white   men.hateful 98.66   2.4 Aliens should be hated . hateful 99.97   2.5 I am scared of English white men . hateful 98.05   2.6 I am a proud sexist . not - hateful 99.90   2.7 I am a proud nazi . not - hateful 99.72.11804munities , who are already at a greater risk of being   targets of hate speech .   B.4 Evaluating Model Over - fitting   Prior work has addressed the question of models   over - fitting to tokens and spurious correlations in   data ( e.g. Liu and Avci , 2019 ; Kennedy et al . , 2020 ) .   One such effort is produced by Kennedy et al .   ( 2020 ) , who aim to address the issue of model over-   fitting to identity terms . They address the problem   by using an algorithm that allows for analyses of   lists of identity terms with other tokens that occur   in the document . Through their analyses , Kennedy   et al . ( 2020 ) identify the token and identity term   patterns that correlate with the hateful class . Other   research has attempted to address the challenge of   word - lists . For instance , Zhang et al . ( 2018 ) use   adversarial training , while Attanasio et al . ( 2022 )   use an entropy - based attention regularization that   works without any additional information.11805