  Peijie JiangDingkun Long Yanzhao Zhang Pengjun Xie   Meishan ZhangMin ZhangSchool of New Media and Communication , Tianjin University , ChinaInstitute of Computing and Intelligence , Harbin Institute of Technology ( Shenzhen )   jzx555@tju.edu.cn , { zhangmeishan,zhangmin2021}@hit.edu.cn   { longdingkun1993,zhangyanzhao00,xpjandy}@gmail.com   Abstract   Boundary information is critical for various   Chinese language processing tasks , such as   word segmentation , part - of - speech tagging ,   and named entity recognition . Previous stud-   ies usually resorted to the use of a high - quality   external lexicon , where lexicon items can of-   fer explicit boundary information . However ,   to ensure the quality of the lexicon , great hu-   man effort is always necessary , which has been   generally ignored . In this work , we suggest un-   supervised statistical boundary information in-   stead , and propose an architecture to encode   the information directly into pre - trained lan-   guage models , resulting in Boundary - Aware   BERT ( BABERT ) . We apply BABERT for   feature induction of Chinese sequence label-   ing tasks . Experimental results on ten bench-   marks of Chinese sequence labeling demon-   strate that BABERT can provide consistent im-   provements on all datasets . In addition , our   method can complement previous supervised   lexicon exploration , where further improve-   ments can be achieved when integrated with   external lexicon information .   1 Introduction   The representative sequence labeling tasks for the   Chinese language , such as word segmentation , part-   of - speech ( POS ) tagging and named entity recogni-   tion ( NER ) ( Emerson , 2005 ; Jin and Chen , 2008 ) ,   have been inclined to be performed at the character-   level in an end - to - end manner ( Shen et al . , 2016 ) .   The paradigm , naturally , is standard to Chinese   word segmentation ( CWS ) , while for Chinese POS   tagging and NER , it can better help reduce the error   propagation ( Sun and Uszkoreit , 2012 ; Yang et al . ,   2016 ; Liu et al . , 2019a ) compared with word - based   counterparts by straightforward modeling .   Recently , all the above tasks have reached state-   of - the - art performances with the help of BERT-   like pre - trained language models ( Yan et al . , 2019;Meng et al . , 2019 ) . The BERT variants , such as   BERT - wwm ( Cui et al . , 2021 ) , ERNIE ( Sun et al . ,   2019 ) , ZEN ( Diao et al . , 2020 ) , NEZHA ( Wei et al . ,   2019 ) , etc . , further improve the vanilla BERT by ei-   ther using external knowledge or larger - scale train-   ing corpus . The improvements can also beneﬁt   character - level Chinese sequence labeling tasks .   Notably , since the output tags of all these   character - level Chinese sequence labeling tasks in-   volve identifying Chinese words or entities ( Zhang   and Yang , 2018 ; Yang et al . , 2019 ) , prior bound-   ary knowledge could be highly helpful for them .   A number of studies propose the integration of an   external lexicon to enhance their baseline models   by feature representation learning ( Jia et al . , 2020 ;   Tian et al . , 2020a ; Liu et al . , 2021 ) . Moreover ,   some works suggest injecting similar resources into   the pre - trained BERT weights . BERT - wwm ( Cui   et al . , 2021 ) and ERNIE ( Sun et al . , 2019 ) are the   representatives , which leverage an external lexicon   for masked word prediction in Chinese BERT .   The lexicon - based methods have indeed   achieved great success for boundary integration .   However , there are two major drawbacks . First ,   the lexicon resources are always constructed   manually ( Zhang and Yang , 2018 ; Diao et al . ,   2020 ; Jia et al . , 2020 ; Liu et al . , 2021 ) , which   is expensive and time - consuming . The quality   of the lexicon is critical to our tasks . Second ,   different tasks as well as different domains require   different lexicons ( Jia et al . , 2020 ; Liu et al . , 2021 ) .   A well - studied lexicon for word segmentation   might be inappropriate for NER , and a lexicon   for news NER might also be problematic for   ﬁnance NER . The two drawbacks can be due to the   supervised characteristic of these lexicon - based   enhancements . Thus , it is more desirable to offer   boundary information in an unsupervised manner .   In this paper , we propose an unsupervised   Boundary - Aware BERT ( BABERT ) , which is   achieved by fully exploring the potential of statisti-526cal features mined from a large - scale raw corpus .   We extract a set of N - grams ( a predeﬁned ﬁxed N )   no matter they are valid words or entities , and then   calculate their corresponding unsupervised statisti-   cal features , which are mostly related to boundary   information . We inject the boundary information   into the internal layer of a pre - trained BERT , so   that our ﬁnal BABERT model can approximate the   boundary knowledge softly by using inside repre-   sentations . The BABERT model has no difference   from the original BERT , so that we can use it in the   same way as the standard BERT exploration .   We conduct experiments on three Chinese se-   quence labeling tasks to demonstrate the effec-   tiveness of our proposed method . Experimental   results show that our approach can signiﬁcantly   outperform other Chinese pre - trained language   models . In addition , compared with supervised   lexicon - based methods , BABERT obtains com-   petitive results on all tasks and achieves further   improvements when integrated with external lex-   icon knowledge . We also conduct extensive anal-   yses to understand our method comprehensively .   The pre - trained model and code are publicly avail-   able at http://github.com/modelscope/   adaseq / examples / babert .   Our contributions in this paper include the fol-   lowing : 1 ) We design a method to encode un-   supervised statistical boundary information into   boundary - aware representation , 2 ) propose a new   pre - trained language model called BABERT as   a boundary - aware extension for BERT , 3 ) verify   BABERT on ten benchmark datasets of three Chi-   nese sequence labeling tasks .   2 Related Work   In the past decades , machine learning has achieved   good performance on sequence labeling tasks   with statistical information ( Bellegarda , 2004 ; Low   et al . , 2005 ; Bouma , 2009 ) . Recently , neural mod-   els have led to state - of - the - art results for Chinese   sequence labeling ( Lample et al . , 2016 ; Ma and   Hovy , 2016 ; Chiu and Nichols , 2016 ) . In addi-   tion , the presence of language representation mod-   els such as BERT ( Devlin et al . , 2019 ) has led   to impressive improvements . In particular , many   variants of BERT are devoted to integrating bound-   ary information into BERT to improve Chinese se-   quence labeling ( Diao et al . , 2020 ; Jia et al . , 2020 ;   Liu et al . , 2021).Statistical Machine Learning Statistical infor-   mation is critical for sequence labeling . Previous   works attempt to count such information from large   corpora in order to combine it with machine learn-   ing methods for sequence labeling ( Bellegarda ,   2004 ; Liang , 2005 ; Bouma , 2009 ) . Peng et al .   ( 2004 ) attempts to conduct sequence labeling by   CRF and a statistical - based new word discovery   method . Low et al . ( 2005 ) introduce a maximum   entropy approach for sequence labeling . Liang   ( 2005 ) utilizes unsupervised statistical information   in Markov models , and gets a boost on Chinese   NER and CWS .   Pre - trained Language Model Pre - trained lan-   guage model is a hot topic in natural language pro-   cessing ( NLP ) communities ( Devlin et al . , 2019 ;   Liu et al . , 2019b ; Wei et al . , 2019 ; Clark et al . ,   2020 ; Diao et al . , 2020 ; Zhang et al . , 2021 ) and   has been extensively studied for Chinese sequence   labeling . For instance , TENER ( Yan et al . , 2019 )   adopts Transformer encoder to model character-   level features for Chinese NER . Glyce ( Meng et al . ,   2019 ) uses BERT to capture the contextual rep-   resentation combined with glyph embeddings for   Chinese sequence labeling .   Lexicon - based Methods In recent studies , lexi-   con knowledge has been applied to improve model   performance . There are two mainstream categories   to the work of lexicon enhancement . The ﬁrst aims   to enhance the original BERT with implicit bound-   ary information by using the multi - granularity   word masking mechanism . BERT - wwm ( Cui et al . ,   2021 ) and ERNIE ( Sun et al . , 2019 ) are represen-   tatives of this category , which propose to mask   tokens , entities , and phrases as the mask units   in the masked language modeling ( MLM ) task to   learn the coarse - grained lexicon information dur-   ing pre - training . ERNIE - Gram ( Xiao et al . , 2021 ) ,   an extension of ERNIE , utilizes statistical bound-   ary information for unsupervised word extraction   to support masked word prediction , The second   category , which includes ZEN ( Diao et al . , 2020 ) ,   EEBERT ( Jia et al . , 2020 ) , and LEBERT ( Liu et al . ,   2021 ) , exploits the potential of directly injecting   lexicon information into BERT via extra modules ,   leading to better performance but is limited in pre-   deﬁned external knowledge . Our work follows the   ﬁrst line of work , most similar to ERNIE - Gram .   However , different from ERNIE - Gram , we do not   discretize the real - valued statistical information ex-527   tracted from corpus , but adopt a regression manner   to leverage the information fully .   3 Method   Figure 1 shows the overall architecture of our un-   supervised boundary - aware pre - trained language   model , which mainly consists of three compo-   nents : 1 ) boundary information extractor for un-   supervised statistical boundary information min-   ing , 2 ) boundary - aware representation to integrate   statistical information at the character - level , and   3 ) boundary - aware BERT learning which injects   boundary knowledge into the internal layer of   BERT . In this section , we ﬁrst focus on the details   of the above components , and then introduce the   ﬁne - tuning method for Chinese sequence labeling .   3.1 Boundary Information Extractor   Statistical boundary information has been shown   with a positive inﬂuence on a variety of Chinese   NLP tasks ( Song and Xia , 2012 ; Higashiyama et al . ,   2019 ; Ding et al . , 2020 ; Xiao et al . , 2021 ) . We   follow this line of work , designing a boundary in-   formation extractor to mine statistical information   from a large raw corpus in an unsupervised way .   The overall ﬂow of the extractor includes two   steps : I ) First , we collect all N - grams from the   raw corpus to build a dictionary N , in which we   count the frequencies of each N - gram and ﬁlter out   the low frequencies items ; II ) second , considering   that word frequency is insufﬁcient for represent-   ing the ﬂexible boundary relation in the Chinesecontext , we further compute two unsupervised in-   dicators which can capture most of the boundary   information in the corpus . In the following , we will   describe these two indicators in detail .   Pointwise Mutual Information ( PMI ) Given   an N - gram , we split it into two sub - strings and com-   pute the mutual information ( MI ) between them as   a candidate . Then , we enumerate all sub - string   pairs and choose the minimum MI as the overall   PMI to estimate the tightness of the N - gram . Let   g={c ... c}be an N - gram that consists of m   characters , we calculate PMI using this formula :   where p(·)denotes the probability over the corpus .   Note that , when m= 1 , the corresponding PMI is   constantly equal to 1 . The higher PMI indicates that   the N - gram ( e.g. , " /uni8D1D / uni514B / uni6C49 / uni59C6(Beckham ) " ) has a   similar occurrence probability to the sub - string pair   ( e.g. , " /uni8D1D / uni514B(Beck ) " and " /uni6C49 / uni59C6(Ham ) " ) , leading   to a higher association between internal sub - string   pairs , which makes the N - gram more likely to be a   word / entity . In contrast , a lower PMI means the N-   gram ( e.g. , " /uni514B / uni6C49(Kehan ) " ) is possibly an invalid   word / entity .   Left and Right Entropy ( LRE ) Given an N-   gram g , we ﬁrst collect a left - adjacent character   setS={c , ... , c}withncharacters . Then ,   we utilize the conditional probability between g   and its left adjacent characters in Sto compute528the left entropy ( LE ) , which measures sufﬁcient   boundary information . LE can be deﬁned as :   LE(g ) = −/summationdisplayp(cg|g ) logp(cg|g ) . ( 2 )   Similar to LE , we further collect a right adjacent set   S={c , ... , c}withncharacters to calculate   the right entropy ( RE ) for the N - gram g :   RE(g ) = −/summationdisplayp(gc|g ) logp(gc|g).(3 )   Intuitively , LRE represents the abundance of   neighboring characters for the N - gram . With a   lower LRE , the N - gram ( e.g. , " /uni6C49 / uni59C6 " ) has a more   ﬁxed context , indicating it is more likely to be a   part of a phrase or entity . Conversely , the N - gram   with a higher LRE ( e.g. , " /uni8D1D / uni514B / uni6C49 / uni59C6 " ) will interact   more with context , which prefers to be an indepen-   dent word or phrase .   Finally , we utilize PMI and LRE to measure the   ﬂexible boundary relations in the Chinese context ,   and then update each N - gram in Nwith the unsu-   pervised statistical indicators above .   3.2 Boundary - Aware Representation   By using the boundary information extractor , we   can obtain an N - gram dictionary Nwith unsuper-   vised statistical boundary information . Unfortu-   nately , since the context independence and the high   relevance to N - gram , previous works ( Ding et al . ,   2020 ; Xiao et al . , 2021 ) use such statistical features   for word extraction only , which ignore the potential   of statistical boundary information in representa-   tion learning . To alleviate this problem , we propose   boundary - aware representation , a highly extensible   method , to fully beneﬁt from the statistical bound-   ary information for representation learning .   To achieve boundary - aware representation , we   ﬁrst build contextual N - gram sets from the sen-   tence . As shown in Figure 1 ( b ) , given a sen-   tence x={c , c , ... , c}withncharacters and   the maximum N - gram length N , we extract all   N - grams that include cas the contextual N - gram   setS={c , cc , · · · , c ... c}for char-   acter c. Then , we design a composition method   to integrate the statistical features of N - grams in   Sby using speciﬁc conditions and rules , aiming   to avoid the sparsity and contextual independence   limitations of statistical information .   Concretely , we divide the information composi-   tion method into PMI and entropy representation .   First , we concatenate the PMI of all N - grams in Sto generate PMI representation :   wheree∈R , anda= 1 + 2+···+Nis the num-   ber of the N - grams that contain c. Note that the   position of each N - gram is ﬁxed in PMI representa-   tion . We strictly follow the order of N - gram length   and the position of cin N - gram to concatenate   their corresponding PMI , ensuring that the position   and context information can be encoded into e.   Entropy representation focuses on the contextual   interactions of each character . When cis the bor-   der of N - grams in S , we separately aggregate the   LE and RE as left and right entropy representation :   where e∈R , e∈R , and b = Nis the   number of integrated N - grams . Similar to PMI   representation , the position of each N - gram in e   andeis ﬁxed and symmetric . Therefore , the   boundary - aware representation eofccan be for-   malized as :   e = e⊕e⊕e , ( 6 )   where e∈R. Finally , by composing multi-   granularity statistical boundary information in a   speciﬁc order , we are able to obtain the boundary-   aware representation , which explicitly contains the   boundary and context information .   Figure 2 shows an example of the boundary-   aware representation . Given a sentence " /uni5357 / uni4EAC   /uni5E02 / uni957F / uni6C5F / uni5927 / uni6865(Nanjing Yangtze River Bridge ) "   and a maximum N - gram length N= 3 , we ﬁrst   build a contextual N - gram set for the character " /uni957F   ( Long ) " . Then , we integrate the PMI of all N - grams   in a speciﬁc order ( from N - gram " /uni957F " to " /uni4EAC / uni5E02 / uni957F   ( Mayor of Jing ) " ) to compute PMI representation .   Furthermore , left and right entropy representations   are also calculated in a particular order ( from N-   gram " /uni957F " to " /uni957F / uni6C5F / uni5927(Yangtze River Big ) " and   " /uni4EAC / uni5E02 / uni957F " , respectively ) . Finally , we concatenate   the above features to produce the overall boundary-   aware representation of the character " /uni957F".529   3.3 Boundary - Aware BERT Learning   Boundary - aware BERT is a variant of BERT , en-   hanced with boundary information simply and ef-   fectively . In this subsection , we describe how the   boundary information can be integrated into BERT   during pre - training by boundary - aware learning .   Boundary - Aware Objective As mentioned in   Section 3.2 , given a sentence xwith character-   length n , we can compute the corresponding   boundary - aware representation E={e , ... , e } .   Then , we transfer the BERT feature into the bound-   ary information space and approximate it to E   for boundary - aware learning . Moreover , Liu et al .   ( 2021 ) shows that encoding basic lexical knowl-   edge in the shallow BERT layers is a more effec-   tive approach . Hence , we use the hidden features   H={h , ... , h}of the l - th shallow layer to   achieve the boundary - aware objective :   L=/summationdisplayMSE(Wh , e ) , ( 7 )   where MSE(·)denotes the mean square error loss .   Wis a trainable matrix used to project BERT   representation into boundary information space .   Previous classiﬁcation - based word - level mask-   ing methods use statistical information as thresh-   olds to ﬁlter valid words for masked word predic-   tion . Unlike the above works , we softly utilize such   information in a regression manner , avoiding possi-   ble errors in empirically ﬁltering valid tags , thereby   fully exploring the potential of this information .   Pre - training Following Jia et al . ( 2020 ) and   Gao and Callan ( 2021 ) , we opt to initialize our   model with a pre - trained BERT model released   by Googleand randomly initialize the other pa-   rameters , alleviating the enormous cost of train - ing BABERT from scratch . In particular , we dis-   card the next sentence prediction task during pre-   training , which is conﬁrmed to be not essential for   the pre - trained language models ( Lan et al . , 2020 ;   Liu et al . , 2019b ) . The total pre - training loss of   BABERT can be formalized as :   L = L+L , ( 8)   whereLis the standard objective of MLM task .   3.4 Fine - tuning for Sequence Labeling   Straightforward Fine - tuning As shown in Fig-   ure 1 ( c ) , because BABERT has the same archi-   tecture as BERT , we can adopt the identical proce-   dure that BERT uses for ﬁne - tuning , where the   output of BABERT can be used as the contex-   tual character representation for sequence label-   ing . Concretely , given a sequence labeling dataset   D={(x , y ) } , where yis the label sequence   ofx , we utilize the output of BABERT and a CRF   layer to calculate the sentence - level output proba-   bility p(y|x ) , which is exactly the same as Liu   et al . ( 2021 ) . The negative log - likelihood loss for   training can be deﬁned as :   L=−/summationdisplaylogp(y|x ) , ( 9 )   At the inference stage , we use the Viterbi algorithm   ( Viterbi , 1967 ) to generate the ﬁnal label sequence .   Combining with Supervised Lexicon Features   We can naturally combine BABERT with other su-   pervised lexicon - based methods because of the un-   supervised setting of BABERT . To this end , we pro-   pose a lexicon - enhanced BABERT ( BABERT - LE )   for the ﬁne - tuning stage , which utilizes the lexicon   adapter proposed by Liu et al . ( 2021 ) to incorporate   external lexicon knowledge into BABERT feature :   ˆh = LA(h , S ) , ( 10 )   where LA(·)is the lexicon adapter , Sis a set of   related N - gram embeddings of character c , andˆh   is the lexicon - enhanced version of original BERT   feature h. We apply the lexicon adapter after the   l - th layer to be consistent with boundary - aware   learning . Finally , BABERT - LE performs a similar   ﬁne - tuning procedure as BABERT for training:5304 Experiments   4.1 Datasets   Following previous works ( Devlin et al . , 2019 ;   Xiao et al . , 2021 ) , we draw the mixed corpus of   Chinese Wikipediaand Baidu Baikeas our pre-   training corpus , which contains 3B tokens and 62 M   sentences . To further conﬁrm the effectiveness   of our proposed method for Chinese sequence la-   beling , we evaluate BABERT on ten benchmark   datasets of three representative tasks :   Chinese Word Segmentation We use three   CWS benchmarks to evaluate our BABERT . Penn   Chinese TreeBank version 6.0 ( CTB6 ) is from   Xue et al . ( 2005 ) , and MSRA and PKU are from   SIGHAN 2005 Bakeoff ( Emerson , 2005 ) .   Part - Of - Speech Tagging For Chinese POS tag-   ging , we conduct experiments on CTB6 ( Xue et al . ,   2005 ) and the Chinese part of Universal Dependen-   cies ( UD ) ( Nivre et al . , 2016 ) . The UD dataset uses   two different POS tagsets , which are universal and   language - speciﬁc tagsets . We follow Shao et al .   ( 2017 ) , referring to the corpus with the two tagsets   as UD1 and UD2 , respectively .   Named Entity Recognition For the Chinese   NER task , we conduct experiments on OntoNotes   4.0 ( Onto4 ) ( Weischedel et al . , 2011 ) and News   datasets ( Jia et al . , 2020 ) , both of which are from   the standard newswire domain . Moreover , we eval-   uate BABERT in the internet novel ( Book ) and   ﬁnancial report ( Finance ) domains ( Jia et al . , 2020 )   to further verify the robustness of our method .   The statistics of the benchmark datasets are   shown in Table 1 . For a fair comparison , we split   these datasets into training , development , and test   sections following previous works ( Jia et al . , 2020 ;   Liu et al . , 2021 ) . Note that MSRA , PKU , and Fi-   nance do not have development sections . Therefore ,   we randomly select 10 % instances from the training   set as the development set for these datasets .   4.2 Experimental Settings   Hyperparameters During pre - training , we use   the hyperparameters of BERT to initialize   BABERT and Adam ( Kingma and Ba , 2014 ) for   optimizing . The number of BERT layers Lis12 ,   with 12self - attention heads , 768 dimensions for   hidden states , and 64 dimensions for each head . Train Dev Test   CWSCTB6 23401 2078 2795   MSRA 86924 - 3985   PKU 19056 - 1944   POSCTB6 23401 2078 2795   UD1/2 3997 500 500   NEROnto4 15724 4303 4346   Book 6675 2551 2551   News 5179 610 708   Finance 46364 - 2000   The batch size is set to 32 , the learning rate is 1e-4   with a warmup ratio of 0.1 , and the max length   of the input sequence is 512 . To extract unsuper-   vised boundary information , we set the maximum   N - gram length Nto4and the frequency ﬁltering   threshold to 50 . Then we use the 3 - th BERT layer   to compute boundary - aware objective . BABERT   has no extra modules , which is why the parameter   size and model architecture are the same as those   of BERT . Finally , we train the BABERT on 8   NVIDIA Tesla V100 GPUs with 32 GB memory .   For Chinese sequence labeling , we empirically   set hyperparameters based on previous studies ( Jia   et al . , 2020 ; Liu et al . , 2021 ) and preliminary ex-   periments . The batch size is 32 , the max sequence   length is 256 , and the learning rate is ﬁxed to 2e-5 .   Baselines To verify the effectiveness of our pro-   posed BABERT , we build systems on the following   methods to conduct fair comparisons :   •BERT is the Chinese version BERT   model released by Google .   •BERT - wwm performs segmentation on the   corpus and further conduct word - level mask-   ing in pre - training ( Cui et al . , 2021 ) .   •ERNIE is an extension of BERT , which lever-   ages external lexicons for word - level masking   ( Sun et al . , 2019 ) .   •ERNIE - Gram is an extension of ERNIE ,   which alleviates the limitations of external lex-531   icons by using statistical information for entity   and phrase extraction ( Xiao et al . , 2021 ) .   •ZEN uses an extra N - gram encoder to inte-   grate external lexicon knowledge into BERT   during pre - training ( Diao et al . , 2020 ) .   •NEZHA leverages functional relative posi-   tional encoding , supervised word - level mask-   ing strategy , and enormous training datato   enhance vanilla BERT ( Wei et al . , 2019 ) .   •BERT - LE is a lexicon - enhanced BERT ( Liu   et al . , 2021 ) , which introduces a lexicon   adapter between BERT layers to incorporate   external lexicon embeddings . We strictly fol-   low . Liu et al . ( 2021 ) to reimplement it with   open - source word embeddings   4.3 Main Results   The overall Chinese sequence labeling results are   shown in Table 2 . We report the F1 - score of the test   datasets on CWS , POS , and NER tasks . Here , we   ﬁrst compare our BABERT with various Chinese   pre - trained language models to evaluate its effec-   tiveness . Then , we compare BABERT - LE with   other supervised lexicon - based methods to show   the potential of BABERT in combining with exter-   nal lexicon knowledge . First , we examine the F1 values of the BERT   baseline . As shown , BERT obtains comparable re-   sults on all Chinese sequence labeling tasks , which   is similar to that of Diao et al . ( 2020 ) , Tian et al .   ( 2020a ) and Liu et al . ( 2021 ) . BABERT signiﬁ-   cantly outperforms BERT , resulting in an increase   of90.47−89.80 = 0 .67on average . This observa-   tion clearly indicates the advantage of introducing   boundary information into BERT pre - training .   Compared with various BERT extensions , our   BABERT can achieve competitive performances   as a whole . First , in comparison with BERT - wwm ,   ERNIE , ERNIE - gram , and ZEN , which lever-   age external lexicons that include high - frequency   words for pre - training , BABERT outperforms all   of them by averaging= 0.48   point , and achieves top scores on eight of the ten   benchmarks . This result is consistent with our in-   tuition that directly exploiting a supervised lexi-   con can only achieve good performance in speciﬁc   tasks , indicating the limitation of these methods   when the chosen lexicon is incompatible with the   target tasks . Second , we ﬁnd that BABERT sur-   passes NEZHA in the average F1 values , indicating   that the boundary information is more critical than   the data scale for Chinese sequence labeling .   Then , we compare our method with supervised   lexicon - based methods . Lattice - based methods   ( Zhang and Yang , 2018 ; Yang et al . , 2019 ) are the   ﬁrst to integrate word features into neural networks   for Chinese sequence labeling . MEM ( Tian et al . ,532   2020a , b ) designs an external memory network after   the BERT encoder to incorporate lexicon knowl-   edge . EEBERT ( Jia et al . , 2020 ) builds entity em-   beddings from the corpus and further utilizes them   in the multi - head attention mechanism . The re-   sults are shown in Table 2 ( II ) . All the above meth-   ods lead to signiﬁcant improvements over the base   BERT model , which shows the effectiveness of ex-   ternal lexicon knowledge . Moreover , BABERT can   achieve comparable performance with the above   methods , which further demonstrates the potential   of our unsupervised manner .   BABERT learns boundary information from un-   supervised statistical features with vanilla BERT ,   which means it has excellent scalability to fuse   with other BERT - based supervised lexicon mod-   els . As shown , we can see that our BABERT - LE   achieves further improvements and state - of - the - art   performances on all tasks , showing the advantages   of our unsupervised setting and boundary - aware   learning . Interestingly , compared with MEM - ZEN ,   BABERT - LE has larger improvements over their   corresponding baselines . One reason might be that   both ZEN and the memory network module ex-   ploits supervised lexicons , which leads to a dupli-   cation of introduced knowledge .   4.4 Analysis   In this subsection , we conduct detailed experimen-   tal analyses for an in - depth comprehensive under-   standing of our method .   Few - Shot Setting To further verify the effective-   ness of BABERT , we conduct experiments under   the few - shot setting , where we randomly sample   10 , 50 , and 100 instances of the original training   data from PKU ( CWS ) and Onto4 ( NER ) . For fair   comparisons , we compare BABERT with the pre-   trained language models without external super-   vised knowledge . The results are presented in Ta-   ble 3 . As the size of training data is reduced , theOnto4 Book News Finance   BABERT 81.90 76.84 80.27 86.89   + T - test 81.77 76.65 80.12 86.23   - PMI 81.12 76.28 79.51 85.56   - LRE 81.51 76.47 79.67 85.94   - LE 81.58 76.62 79.88 85.94   - RE 81.53 76.59 79.75 85.90   LayerCWS POS NER   PKU CTB6 Onto4 News   12 96.38 94.68 81.02 79.19   696.53 94.84 81.45 79.91   396.70 95.03 81.90 80.27   196.62 94.90 81.59 79.52   performance drops signiﬁcantly , indicating that the   performances of such models rely on the scale of   the labeled training data . Nevertheless , BABERT   achieves top scores under each setting and signif-   icantly outperforms vanilla BERT , demonstrating   the potential of injecting unsupervised boundary   information by using regression - based boundary-   aware learning , which effectively alleviates the low-   resource problem .   Choice of Statistical Features As mentioned in   Section 3.2 , we use PMI and LRE to model unsu-   pervised boundary - aware representation . In addi-   tion to these features , T - test ( Xiao et al . , 2021 ) is   another popular choice that can be utilized . Thus ,   we conduct ablation experiments to check the ef-   fectiveness of these features . We analyze ﬁve fea-   ture combining settings , including PMI ( -LRE ) and   LRE ( -PMI ) alone , the ablation study of LRE ( -LE   and -RE ) , and our two indicators with additional   T - test features being concatenated at the end of the   original boundary - aware representation ( + T - test ) .   As shown in Table 4 , the combination of PMI and   LRE can achieve the best results , discarding ei-   ther of them will result in decreased performance .   Besides , right entropy is more important than left   entropy according to our results , which may be   that the right entropy is more compatible with the   reading characteristics of Chinese . Interestingly,533   adding T - test does not bring further improvements .   One possible reason is that the T - test is essentially   similar to the entropy measure of 2 - grams , which   has already been injected into our BABERT model .   Boundary Information Encoding Layer Previ-   ous works ( Jawahar et al . , 2019 ; Liu et al . , 2021 )   exploit the fact that different BERT layers would   generate different concept representations . The   shallow BERT layers are more likely to capture ba-   sic lexicon information , while the top layers focus   on the semantic representation . We empirically set   lin{1,3,6,12}to explore the effect of computing   boundary - aware loss by the hidden features Hof   different BERT layers on Chinese sequence label-   ing tasks . Table 5 shows the results . We can see   that the best F1 - score can be achieved when l= 3   on all datasets , which indicates that the BABERT   still needs sufﬁcient parameters to learn the basic   boundary information . Interestingly , the BABERT   performs poorly when l= 12 , which might be   due to a conﬂict between the MLM loss and our   boundary - aware regression loss during pretraining .   Qualitative Analysis To explore how BABERT   improves the performance for Chinese sequence   labeling , we conduct qualitative analysis on the   News test dataset , which consists of four different   subdomains , namely game ( GAM ) , entertainment   ( ENT ) , lottery ( LOT ) and ﬁnance ( FIN ) . The results   are shown in Table 6 . We can see that compared   with other pre - trained language models , BABERT   can obtain consistent improvement in all domains   with unsupervised statistical boundary information ,   while the other models only improve performance   on speciﬁc domains . Moreover , as shown in Ta-   ble 7 , we also give an example from the game do-   main to further demonstrate the effectiveness of our   method . BABERT is the only model that correctly   recognizes all entities . In particular , the prediction   of BABERT for the entity " WCG2011 " indi-   cates the potential of boundary information .   5 Conclusion   In this paper , we proposed BABERT , a novel   unsupervised boundary - aware pre - training model   for Chinese sequence labeling . In BABERT ,   given a Chinese sentence , we calculated boundary-   aware representation with unsupervised statisti-   cal information to capture boundary information ,   and directly injected such information into BERT   weights during pre - training . Unlike previous works ,   BABERT exploited an effective way to utilize   boundary information in an unsupervised manner ,   thereby alleviating the limitations of supervised   lexicon - based approaches . Experimental results   on ten benchmark datasets of three different tasks   illustrated that our method was highly effective   and better than other Chinese pre - trained models .   Moreover , the combination with supervised lexicon   extensions could achieve further improvements and   state - of - the - art results on most tasks .   Limitations   BABERT suffers from three major limitations . The   ﬁrst limitation is that in the boundary information   extractor , we empirically chose PMI and LRE . In   addition to these indicators and the T - test measure   we veriﬁed in the experimental analyses , some al-   ternatives that contain boundary information could   be used to compute boundary - aware representation .   Thus , we plan to explore more unsupervised sta-   tistical features . The second limitation is that we   focused only on Chinese sequence labeling tasks in   this work , ignoring the potential of boundary infor-   mation and BABERT in other Chinese NLP tasks .   The third one is that we only consider BABERT   for Chinese . For other languages which do not use   spaces between words such as Japanese and Thai ,   we can also attempt to inject boundary information ,   and the effectiveness in these languages should be   veriﬁed by experiments . Further research is needed   to evaluate our BABERT in future studies.534Acknowledgements   This work is supported by grants from the National   Key Research and Development Program of China   ( No . 2018YFC0832101 ) and the National Natural   Science Foundation of China ( No . 62176180 ) .   References535536537