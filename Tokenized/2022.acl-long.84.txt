  Huibin Zhang , Zhengkun Zhang , Yao Zhang , Jun Wang , Yufan Li   Ning Jiang , Xin Wei , Zhenglu YangTKLNDST , CS , Nankai University , China , Ludong University , China , Mashang Consumer Finance Co. , Ltd. , China   { zhanghuibin.cc , zhangzk2017,yaozhang , junwang,1811486 }   @mail.nankai.edu.cn,{ning.jiang02,xin.wei02}@msxf.com ,   yangzl@nankai.edu.cn   Abstract   Procedural Multimodal Documents ( PMDs )   organize textual instructions and correspond-   ing images step by step . Comprehending   PMDs and inducing their representations for   the downstream reasoning tasks is designated   as Procedural MultiModal Machine Compre-   hension ( MC ) . In this study , we approach Pro-   cedural MC at a ﬁne - grained level ( compared   with existing explorations at a document or   sentence level ) , that is , entity . With delicate   consideration , we model entity both in its tem-   poral and cross - modal relation and propose a   novel Temporal - Modal Entity Graph ( TMEG ) .   Speciﬁcally , a heterogeneous graph structure   is formulated to capture textual and visual en-   tities and trace their temporal - modal evolution .   In addition , a graph aggregation module is in-   troduced to conduct graph encoding and rea-   soning . Comprehensive experiments across   three Procedural MC tasks are conducted on   a traditional dataset RecipeQA and our new   dataset CraftQA , which can better evaluate the   generalization of TMEG .   1 Introduction   MultiModal Machine Comprehension ( MC ) is   a generalization of machine reading compre-   hension by introducing multimodality . Due to   its differences from Visual Question Answering   ( VQA ) ( Antol et al . , 2015 ) in the form of under-   standing multimodal contexts and conducting mul-   timodal questions and answers , there has been a   lot of attention in recent years devoted to this ﬁeld .   In this paper , we investigate a task that has been   typical of MC recently , named Procedural MC ,   a task of reading comprehension of Procedural   Multimodal Documents ( PMDs ) .   As shown in Figure 1 , a recipe that contains suc-   cessive multimodal instructions is a typical PMD .   Reading a recipe seems trivial for humans but is   still complex for a machine reading comprehensionFigure 1 : A PMD instance : a recipe about making mini   fruit pizza . The entities and relations ( i.e. temporal and   multimodal ) among them are highlighted .   system before it can comprehend both textual and   visual contents and capture their relations .   Current Procedural MC studies ( Yagcioglu   et al . , 2018 ; Liu et al . , 2020 ) comprehend PMDs by   encoding text and images at each procedure step .   These efforts , however , only scratch the surface and   lack deep insight into the elementary unit of PMDs ,   that is , entity . From now on , we use entity to re-   fer uniformly to entity in text and object in image .   For instance , the recipe in Figure 1 involves mul-   tiple entities , i.e. , Strawberry andSugar Cookie   Dough , etc . In this work , we target at approaching   the Procedural MC task at a ﬁne - grained entity   level .   We observe that a PMD essentially assembles   an evolution process of entities and the relations   between them . Speciﬁcally , the relation between   entities can be summarized in the following two   categories : Temporal Relation . The state of an entity may   change as steps progress . Still looking at Fig-   ure 1 , strawberries are complete at step 1 and1179by step 4 they are washed and cut into pieces .   We use temporal relation to depict the associa-   tion between an entity ’s changing visual signals   in images or changing contexts in text . Multimodal Relation . An entity is naturally and   powerfully associated with other entities within   a single modality . Meanwhile , the cross - modal   association of an entity is worth exploiting to con-   tribute to distinct modality understanding . For ex-   ample , the visual signal and context about sugar   cookie dough can be interpreted by each other at   step 2 . We generalize the intra- and inter- modal   associations of entities with the multimodal rela-   tion .   Based on the above observations , we believe that   simultaneously modeling entities and the tempo-   ral and multimodal relations is a key challenge in   understanding PMDs . Recent efforts ( Amac et al . ,   2019 ; Huang et al . , 2021 ) are devoted to encoding   temporal relations of entities , while it neglects the   multimodal relation . Heterogeneous graphs have   become the preferred technology for representing ,   sharing , and fusing information to modern AI tasks ,   e.g. , relation extraction ( Christopoulou et al . , 2019 )   and recommendation ( Fan et al . , 2019 ) . Inspired by   this , we construct a heterogeneous graph , with enti-   ties as nodes and relations as edges . Therefore , the   research goal of the procedural MC task will shift   from understanding unstructured PMDs to learning   structured graph representations .   In this work , we propose a novel Temporal   Modal Entity Graph model , namely TMEG . Our   model approaches Procedural MC at a ﬁne-   grained entity level by constructing and learning a   graph with entities and temporal and multimodal re-   lations . Speciﬁcally , TMEG consists of the follow-   ing components : 1 ) Node Construction , which   extracts the token - level features in text and object-   level features in images as the initial entity em-   beddings ; 2 ) Graph Construction , which con-   structs the temporal , intra - modal , and cross - modal   relations separately to form a uniﬁed graph ; and   3)Graph Aggregation , which utilizes the graph-   based multi - head attention mechanism to perform   fusion operations on graphs to model the evolution   of entities and relations . Finally , the graph represen-   tation is fed into a graph - based reasoning module   to evaluate the model ’s understanding ability .   In addition , in order to further advance the re-   search of Procedural MC , we release CraftQA , a   multimodal semantically enriched dataset that con - tains about 27k craft product - making tutorials and   46k question - answer pairs for evaluation . We eval-   uate three representative subtasks , i.e. , visual cloze ,   visual coherence , and visual ordering , on CraftQA   and a public dataset RecipeQA ( Yagcioglu et al . ,   2018 ) . The quantitative and qualitative results show   the superiority of TMEG compared to the state-   of - the - art methods on all three tasks . The main   contributions of this paper can be summarized as   follows : We innovatively study the Procedural MC task   at a ﬁne - grained entity level . We comprehen-   sively explore the relations between entities in   both temporal and multimodal perspectives . We propose a Temporal - Modal Entity Graph   model TMEG , which constructs a graph with   entities as nodes and relations as edges , and then   learns the graph representation to understand   PMDs . We release a dataset CraftQA . The experimental   results on CraftQA and RecipeQA show TMEG   outperforms several state - of - the - art methods .   2 Related Work   Procedural Text Comprehension . Procedural   text comprehension requires an accurate prediction   for the state change and location information of   each entity over successive steps . Several datasets   have been proposed to evaluate procedural text   comprehension , e.g. , Recipe ( Bosselut et al . , 2018 ) ,   ProPara ( Mishra et al . , 2019 ) , and OPENPI ( Tan-   don et al . , 2020 ) . To model entity evolution , KG-   MRC ( Das et al . , 2019 ) constructs knowledge   graphs via reading comprehension and uses them   for entity location prediction . DYNAPRO ( Amini   et al . , 2020 ) introduces a pre - trained language   model to dynamically obtain the contextual em-   bedding of procedural text and learn the attributes   and transformations of entities . ProGraph ( Zhong   et al . , 2020 ) enables state prediction from context   by constructing a heterogeneous graph with vari-   ous knowledge inputs . KoalA ( Zhang et al . , 2021b )   utilizes the external commonsense knowledge in-   jection and data enhancement to reason the states   and locations of entities . TSLM ( Faghihi and Kord-   jamshidi , 2021 ) formulate comprehension task as a   question answering problem and adapt pre - trained   transformer - based language models on other QA   benchmarks . REAL ( Huang et al . , 2021 ) builds1180   a general framework to systematically model the   entity , action , location by using a graph neural net-   work .   Inspired by these previous works , we propose a   temporal - modal entity graph model , which is de-   signed with temporal encoding and modal encoding   to model multiple types of entities .   Multimodal Graph . In recent multimodal re-   search , graph structure has been utilized to model   the semantic interaction between modalities . ( Yin   et al . , 2020 ) propose a graph - based multimodal fu-   sion encoder for neural machine translation , which   converted sentence and image in a uniﬁed mul-   timodal graph . ( Khademi , 2020 ) convert image   regions and the region grounded captions into   graph structure and introduced graph memory net-   works for visual question answering . ( Zhang et al . ,   2021a ) propose a multimodal graph fusion ap-   proach for named entity recognition , which con-   ducted graph encoding via multimodal semantic in-   teraction . ( Yang et al . , 2021 ) focus on multimodal   sentiment analysis and emotion recognition , which   uniﬁed video , audio , and text modalities into an   attention graph and learned the interaction through   graph fusion , dynamic pruning , and the read - out   technique .   In contrast to the above methods , we formulate   our multimodal graph on temporal entities and suc-   cessfully deploy it in Procedural MC .   3 Proposed Method   In this section , we introduce : ( 1 ) problem deﬁni-   tion of Procedural MC in Section 3.1 ; ( 2 ) The   homogeneous graph of each textual instruction ( im - age ) and our TMEG in Section 3.2 and Section 3.3 ,   respectively ; and ( 3 ) graph aggregation module   to conduct graph encoding and reasoning in Sec-   tion 3.4 . Figure 2 gives a high - level overview of   TMEG .   3.1 Problem Deﬁnition   Here , we deﬁne the task of Procedural MC , given :   •ContextS = fsgin textual modality ,   which represents a series of coherent textual   instructions to perform a speciﬁc skill or task   ( e.g. , multiple steps to complete a recipe or a   craft product ) .   •QuestionQand AnswerA , which is either a   single image or a series of images in a reason-   ing task ( e.g. , visual cloze , visual coherence ,   or visual ordering ) .   Following ( Liu et al . , 2020 ) , we combine the im-   ages contained in QandAto formNcandidate   image sequencesfa;:::a;:::ag . LetNbe   the length of the j - th candidate image sequence   a = fI;:::Ig . Take the visual cloze task   as an example , we ﬁll the placeholder of the ques-   tion with candidate answers to form Nimage se-   quences with length N= 4 . The model requires   to select the most relevant candidate by calculating   the similarity between text sequence S = fsg   and each image sequence a.   3.2 Homogeneous Graph Construction   As shown in Figure 2 , we ﬁrst extract the tokens   ( objects ) in text ( image ) as the initial nodes of ho-   mogeneous graph , respectively .   Textual Node . LetNbe the number of tex-   tual instructions S = fsg . First , each in-1181structionsis tokenized into a token sequence   fe;e;:::eg , where [ CLS ] and [ SEP ] are   the special tokens introduced to mark the start and   the end of each instruction . Then , we utilize an   off - the - shelf POS tagger ( Akbik et al . , 2018 ) to   identify all nouns and noun phrases in the token   sequence . Finally , we concatenate all the token se-   quences of textual instructions Sand feed the token   embedding into the graph aggregation module .   Visual Node . For each image sequence a , we em-   ploy a pre - trained Faster - RCNN to extract a set   fe;e;:::egwithkobject features as visual   tokens . Following ( Messina et al . , 2020 ; Dosovit-   skiy et al . , 2021 ) , we reserve [ CLS ] as the begin-   ning token for each image whose ﬁnal embedding   is regarded as the representation of the whole im-   age . The operation of visual node is in a similar   manner as textual and any two nodes in the same   instruction ( image ) are connected to construct a   homogeneous graph .   3.3 Heterogeneous Graph Construction   Based on the homogeneous graph of each textual   instruction ( image ) , we introduce various types of   edges to construct our heterogeneous graph TMEG .   3.3.1 Temporal Edge   It is essential to model the temporal evolution of   entities for comprehending procedural content . Let   us revisit the example in Figure 1 . When a human   reads step 4 , the connection between entities ( e.g. ,   strawberries andoranges ) and their descriptions   in step 1 is naturally established .   We design the temporal edge to model the evolu-   tion of entities in text and image . It can be seen that   the temporal edge describes the evolution at differ-   ent steps . For the textual nodes , the same entity ap-   pearing in different steps are connected by a textual   temporal edge ( node - based ) . While for the visual   nodes , we directly calculate the Euclidean Distance   between object features due to the absence of accu-   rate object detection . Following ( Song et al . , 2021 ) ,   if the distance between node ( object ) iand node   ( object)jis less than a threshold  , we treat them   as the same object and connect node ito nodejvia   a visual temporal edge ( node - based ) . Meanwhile ,   we consider that there may also be temporal evolu-   tion for edges , such as changes in the relationship   between entities . Therefore , we also introduce tem-   poral edge ( edge - based ) to characterize the change   of edges.3.3.2 Modal Edge   As shown in Figure 1 , the textual instruction of   each image can be viewed as a noisy form of image   annotation ( Hessel et al . , 2019 ) . The association be-   tween image and sentence can be inferred through   entity representations under different modalities .   Correspondingly , we design the intra - modal edge   and the inter - modal edge to represent the modal   interactions . In Section 3.2 , any two nodes in the   same modality and the same instruction ( image )   are connected by an intra - modal edge . It is worth   noting that for each instruction ( image ) , the special   [ CLS ] node is connected to all other nodes in order   to aggregate graph - level features .   On the other hand , the textual node represent-   ing any entity and the corresponding visual node   are connected by an inter - modal edge . We employ   a visual grounding toolkit ( Yang et al . , 2019 ) to   detect visual objects for each noun phrase . Speciﬁ-   cally , we predict the bounding box corresponding   to the text entity and compute the Intersection over   Union ( IoU ) between all visual objects . If the IoU   between the prediction box and the visual box ex-   ceeds a threshold  , the textual node and the cor-   responding visual node are connected by an inter-   modal edge ( node - based ) . Similar to section 3.3.1 ,   considering the inﬂuence of entity - relationship un-   der different modalities , we also introduce inter-   modal edge ( edge - based ) to characterize the inter-   action between edges .   3.4 Graph Aggregation   3.4.1 Node Encoding   As described in Section 3.2 , we have obtained the   embeddings of the textual tokens and visual ob-   jects . Similar to ( Li et al . , 2020 ) , all embeddings   are mapped to a set of initial node embeddings , and   each node embedding is the sum of 1 ) a textual   token embedding or visual object embedding ; 2 )   a position embedding that identiﬁes the position   of the token or object in the image ; and 3 ) a seg-   ment embedding generated from the step number in   PMD which indicates different textual instructions   or images .   3.4.2 Edge Encoding   To encode the structural information into TMEG ,   we consider the temporal encoding and the modal   encoding separately . For any two nodes vandvin1182TMEG , we construct two mappings :  ( v;v ) !   Rand  ( v;v)!Rwhich encode the tempo-   ral edge and the modal edge between them . The   temporal encoding and the modal encoding of the   total graph are fed into the graph - based aggregation   module .   3.4.3 Graph - Based Fusion   As shown in the right part of Figure 2 , we ﬁrst in-   troduce two multi - layer perceptrons ( MLP ) with   Tanh activation function to project different node   embeddings from two modalities into the same   space . Then , we extend the VisualBERT ( Li et al . ,   2020 ) to the graph - based fusion layer , which con-   catenates the node embeddings from MLPs as input   and outputs their graph - based joint representations .   Speciﬁcally , in each fusion layer , updating the hid-   den states of textual node and visual node mainly   involve the following steps .   Firstly , we exploit a graph - based multi - head at-   tention mechanism to generate contextual represen-   tations of nodes . Formally , the output of the h - th   attention head in the l 1layer can be obtained as   follows :   A(q;k;v ) = Xv   Softmax ( e)   ; ( 1 )   e = q    k   p   d+b+b ; ( 2 )   whereq;k;v are the query matrix , key matrix ,   and value matrix generated from the hidden state   Hof nodes in the l 1layer .  ( i;j)and   ( i;j)denote the temporal encoding and the   modal encoding of TMEG , which serve as bias   terms in the attention module . It is worth noting   that each head in the multi - head attention mech-   anism exhibits a broad range of behaviors ( Vig ,   2019 ) ; thus , we add different temporal encoding   and modal encoding separately for each attention   head . Meanwhile , in order to model the relation-   ship of edges , the temporal encoding and the modal   encoding are learned separately for each layer .   We concatenate the output of each head and pass   them to a position - wise Feed Forward Networks   ( FFN ) which is preceded and succeeded by residual   connections and normalization layer ( LN ) ,   ^H = LN   W[A;:::A ] + H   ;   H = LN   FFN(^H ) + ^H   ; ( 3 )   whereWis a learnable parameter and [ ] denotesAlgorithm 1 :   the concatenation manipulation . Finally , based on   TMEG , we stack multi - layer graph - based fusion   layers to conduct graph encoding . Algorithm 1   shows the aggregation of TMEG in detail .   3.4.4 Graph - Based Reasoning   As mentioned in Section 3.2 , we regard the hid-   den state of [ CLS ] as the representations of each   instruction ( image ) , where their ﬁnal hidden states   HandHare passed into the graph reasoning   module for task completion .   Firstly , we leverage the one - to - one correspon-   dence between instruction and image , e.g. , each   instruction has an image to visualize it ( Alikhani   et al . , 2021 ) . TMEG involves a Contrastive Co-   herence Loss for keeping the alignment between   instruction and image . Let HandHrepre-   sent the positive and negative examples , the loss   Lof thei - th step can be deﬁned as follows :   L= logexpfsim(H;H)=  gPexpfsim(H;H)=  g ;   ( 4 )   whereKis the total number of negative sam-   ples ( He et al . , 2020 ) generated from the min - batch ,   sim(;)and  are the standard cosine similarity   function and temperature .   In a downstream reasoning task , the model   needs to predict the correct candidate a=   fI;:::Igbased on the instructions S=   fsg . Referring to the sentence image predic-1183tion task in ( Li et al . , 2020 ) , we concatenate   all representations of each candidate image se-   quence to generate a instruction candidate pair as :   ( S;a ) = [ CLS;H;:::H;SEP;H;:::H ] ,   where [ CLS ] and [ SEP ] are special tokens as used   in ( Li et al . , 2020 ) . We pass this input through a   shallow transformer followed by a fully connected   layer to obtain the prediction score P(S;a)for   thej - th candidate , and the prediction loss can be   deﬁned as   L= logexp ( P(S;a))Pexp ( P(S;a));(5 )   whereais the correct candidate and Nis the   number of candidates . We get the ﬁnal loss func-   tion and optimize it through the Adam optimizer :   L = L+L ; ( 6 )   whereis the balance parameter . Unless oth-   erwise speciﬁed , all the results in this paper use   = 0:1which we ﬁnd to perform best .   4 Experiments   4.1 Datasets and Metrics   RecipeQA . RecipeQA ( Yagcioglu et al . , 2018 ) is   a multimodal comprehension dataset with 20 K   recipes approximately and more than 36 K question-   answer pairs . Unlike other multimodal reading   comprehension datasets ( Tapaswi et al . , 2016 ; Iyyer   et al . , 2017 ; Kembhavi et al . , 2017 ) analyze against   movie clips or comics , RecipeQA requires reason-   ing real - world cases .   CraftQA . We collect CraftQA from Instructables ,   which is an online community where people can   share their tutorials for accomplishing a task in a   step - by - step manner . Speciﬁcally , we collect the   most visited tutorials and remove those that con-   tain only text or video . For question and answer   generation , we also remove the tutorials that con-   tain less than 3 images . To construct the distractor   of each task , we compute the Euclidean distance   between the image features that are extracted from   a pretrained ResNet-50 ( He et al . , 2016 ) . Taking   the visual cloze task as an example , the distractor is   sampled from the nearest neighbors of the ground-   truth image based on Euclidean distance . Finally ,   CraftQA contains about 27k craft product - making   tutorials and 46k question - answer pairs . We em-   ploy CraftQA to evaluate the reading comprehen-   sion performance of TMEG in different domains as   well as its domain transfer capability . More statis-   tics about these two datasets are shown in Table 1 .   Metric . In three Procedural MC tasks that are   tested in the following experiments ( visual cloze ,   visual coherence , and visual ordering ) , we use clas-   siﬁcation accuracy as the evaluation metric , which   is deﬁned as the percentage of yielding the ground-   truth answer during testing ( Yagcioglu et al . , 2018 ;   Amac et al . , 2019 ; Liu et al . , 2020 ) .   4.2 Implementation Details   For visual node construction , we employ the pre-   trained Faster R - CNN ( Ren et al . , 2015 ) model   provided by Detectron2 ( Wu et al . , 2019 ) and limit   the number of objects to 36 for each image . Fol-   lowing ( Yang et al . , 2019 ; Song et al . , 2021 ) , we set   the thresholds andas 7 and 0.5 , respectively ,   for the temporal and the modal edge constructions .   The framework of the graph - based fusion mod-   ule is built on VisualBERT ( Li et al . , 2020 ) with its   initialized parameters and tokenizer implemented   by HuggingFace ’s transformers library ( Wolf et al . ,   2020 ) . The shallow transformer in the graph - based   reasoning module is designed as 2 hidden layers   with a size of 512 and 8 attention heads . During   the training stage , the batch size is ﬁxed to 16 and   the number of negative samples Kis set to 8 . The   temperature parameter  in Eq.(4 ) is set to 0.07 .   The balance parameter in Eq.(6 ) is set to 0.1 .   Adam with the learning rate 510is used to   update parameters . We introduce an early stopping   mechanism and set the patience value to 5 , which   means the training will stop if the model perfor-   mance is not improved in ﬁve consecutive times .   Our source code will be released online.1184   4.3 Baselines   We compare our model with the following mod-   els : ( 1 ) Hasty Student ( HS ) ( Yagcioglu et al . ,   2018 ) discards textual context and directly ex-   ploits the similarities and dissimilarities between   answer images to rank candidates . ( 2 ) PRN ( Amac   et al . , 2019 ) introduces external relational mem-   ory units to keep track of textual entities and em-   ploys a bi - directional attention mechanism to ob-   tain a question - aware embedding for prediction .   ( 3)MLMM - Trans ( Liu et al . , 2020 ) modiﬁes the   framework of the transformer ( Vaswani et al . , 2017 )   and conducts an intensive attention mechanism at   multiple levels to predict correct image sequences .   ( 4)VisualBERT ( Li et al . , 2020 ) consists of a stack   of transformer layers that extend the traditional   BERT ( Devlin et al . , 2019 ) model to a multimodal   encoder . The performance of some baselines on   RecipeQA has been previously reported in ( Amac   et al . , 2019 ; Liu et al . , 2020 ) .   4.4 Experimental Results   4.4.1 Comparison Analysis   As shown in Table 2 , TMEG shows favorable per-   formance in different reasoning tasks , with an av-   erage accuracy of 69.73 and 49.54 on RecipeQA   and CraftQA , following behind the “ Human ” per-   formance . Besides , the performance on the visual   ordering task exceeds human accuracy for the ﬁrst   time , which proves that the temporal and modal   analysis in TMEG is effective in comprehending   PMDs . MLMM - Trans performs comparably with   VisualBERT while inferior to TMEG , which may   be attributed to their superﬁcial consideration of   entities .   MLMM - Trans ignores the entity information   contained in text ( e.g. , the correspondence between   entities in text and images ) and VisualBERT di-   rectly fuses textual and visual features without con-   sidering entity evolution . In TMEG , we explic-   itly identify and model entity evolution in PMD ,   whereas MLMM - Trans and VisualBERT assume   entity information to be learned implicitly along-   side other data .   Meanwhile , CraftQA has more images   ( 20.14 vs 12.67 ) and tokens ( 535.88 vs 443.01 ) on   average than RecipeQA . More diverse complex   cases in CraftQA require better comprehension and   reasoning capacities for both models and humans .   We believe this can explain the lower results   on CraftQA . This emphasizes the necessity of   comprehending entity coherence in a multimodal   context .   4.4.2 Ablation Study   We evaluate the effects of temporal encoding ,   modal encoding , and contrastive coherence loss1185   Lto examine the three modules .   Edge Encoding . Table 2 also shows the ablation   results of our model when each module is respec-   tively removed . In terms of edge encoding , re-   moving temporal encoding makes more negative   effects on TMEG than moving the modal encoding ,   reﬂecting the signiﬁcance of modeling temporal   entity evolution for procedural MC .   Contrastive Coherence Loss . As shown in the   last row of Table 2 , we ﬁnd that Lcan indeed   improve TMEG . The reason is that Lhelps en-   hance the learning of textual entity representation   and visual entity representation in Procedural MC .   4.5 Analysis of TMEG   4.5.1 Balance Parameter   In Figure 3 , we illustrate the inﬂuence of the bal-   ance parameter in Eq.(6 ) , which balances the   contrastive coherence loss Land the candidate   prediction lossL. We tunefrom 0 to 0.2 with   0.05 as the step size . We observe that the model   beats the highest accuracy when = 0:1 . Gener-   ally , ( 1 ) introducing the contrastive coherence loss   can improve TMEG for better ﬁtting downstream   tasks , and ( 2 ) appropriately balancing the predic-   tion lossLand contrastive coherence loss L   helps TMEG comprehend PMDs .   4.5.2 Cross - Domain Investigation   To study the domain transfer capability of our   framework , we evaluate TMEG in different do-   mains , as shown in Table 3 . Speciﬁcally , The   model trained on RecipeQA is evaluated on   CraftQA , and the reverse is true for CraftQA . Re-   sults show that compared with other baselines , our   model achieves more generalized and better com-   prehension performance on domain transfer by in-   corporating TMEG .   4.5.3 Case Study   Figure 4 further presents a visual cloze example   on RecipeQA which requires a correct image in   the missing piece after reading the context . We   compare the highest - scored candidate images re-   spectively picked out by MLMM - Trans ( Liu et al . ,   2020 ) , VisualBERT ( Li et al . , 2020 ) , and TMEG.1186By considering the temporal - modal entity evolu-   tion , TMEG can capture the salient entities ( e.g. ,   Strawberry andSugar Cookie Dough ) and trace   their evolution at each step , thereby inferring the   ground - truth answer .   5 Conclusion   In this paper , we propose a novel temporal - modal   entity graph ( TMEG ) to approach Procedural MC .   Based on TMEG , we introduce graph - based fusion   module and reasoning module , which are used to   aggregate node features and solve downstream rea-   soning tasks .   What ’s more , we introduce another Procedural   MC dataset called CraftQA to assist in evaluating   the generalization performance of TMEG in differ-   ent domains and domain transfer . Extensive experi-   ments on the RecipeQA and CraftQA validate the   superiority of TMEG . A promising future direction   is to introduce temporal - modal entity graphs into   the video understanding task ( Lin et al . , 2020 ; Xu   et al . , 2020 ) , which also calls for an enhancement   of the temporal and the cross - modal reasoning ca-   pability .   Ethical Considerations   Intellectual Property . CraftQA contains question   answer pairs generated from copyright free tutori-   als found online . All of the tutorials are licensed   with the Creative Commons licensewhich helps   share knowledge and creativity for common use .   The collection of CraftQA is in accordance with   the Terms of Service of Instructables as follows : by   posting , providing , uploading , submitting , sharing ,   publishing , distributing , making available or allow-   ing others to access and/or use Your Content to or   through the Service You are solely responsible and   liable for the consequences of doing so and you   acknowledge and agree that Your Content can and   may be viewed worldwide .   We also construct experimental evaluations on   the RecipeQA dataset . Referring to the ofﬁcial   dataset descriptions of RecipeQA , Legal and Ethi-   cal Considerations had been taken into account dur-   ing the construction of RecipeQA . We have cited   the corresponding papers in this study . Privacy . According to the Privacy Statement of   Instructables , users can choose whether or not to   expose their information when publishing tutorials .   Respecting personal privacy , we have removed all   of the personal information of users from CraftQA   and promise CraftQA is n’t involved with any   privacy issues .   Acknowledgements : This work was supported in   part by the National Natural Science Foundation of   China ( No . 62106091 ) and Shandong Provincial   Natural Science Foundation ( No . ZR2021MF054 ) .   References118711881189