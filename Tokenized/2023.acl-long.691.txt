  Marco Antonio Stranisci , Rossana Damiano , Enrico MensaViviana Patti , Daniele Paolo Radicioni , Tommaso CaselliDipartimento di Informatica , Università degli Studi di Torino , ItalyCLCG , University of Groningenaequa - tech , Turin , Italy   marcoantonio.stranisci@unito.it   Abstract   Biographical event detection is a relevant task   for the exploration and comparison of the ways   in which people ’s lives are told and represented .   In this sense , it may support several applica-   tions in digital humanities and in works aimed   at exploring bias about minoritized groups . De-   spite that , there are no corpora and models   specifically designed for this task . In this paper   we fill this gap by presenting a new corpus an-   notated for biographical event detection . The   corpus , which includes 20Wikipedia biogra-   phies , was compared with five existing corpora   to train a model for the biographical event de-   tection task . The model was able to detect all   mentions of the target - entity in a biography   with an F - score of 0.808and the entity - related   events with an F - score of 0.859 . Finally , the   model was used for performing an analysis of   biases about women and non - Western people   in Wikipedia biographies .   1 Introduction   Detecting biographical events from unstructured   data is a relevant task to explore and compare bias   in representations of individuals . In recent years ,   the interest in this topic has been favored by studies   about social biases on allegedly objective public   archives such as Wikipedia . Sun and Peng ( 2021 )   developed a resource for investigating gender bias   on Wikipedia biographies showing that personal   life events tend to be more frequent in female ca-   reer sections than in those of men . Lucy et al .   ( 2022 ) developed BERT - based contextualized em-   beddings for exploring representations of women   on Wikipedia and Reddit .   The detection of biographical events has been   addressed with complementary approaches by dif-   ferent research communities . Projects in Digital   Humanities have focused mostly on representa-   tional aspects , delivering ontologies and knowledge   graphs for the collection and study of biographi-   cal events ( Tuominen et al . , 2018 ; Fokkens et al . ,2017 ; Plum et al . , 2019 ; Krieger , 2014 ) . When   it comes to NLP , the focus has been mainly on   developing models for the detection and classifica-   tion of events ( Rospocher et al . , 2016 ; Gottschalk   and Demidova , 2018 ) . Few are the works that di-   rectly target biographies and focus on identifying   biographical events with varied approaches ( super-   vised and unsupervised ) across different datasets   ( e.g. , Wikipedia vs.newspaper articles ) , making   their comparison impossible ( Bamman and Smith ,   2014 ; Russo et al . , 2015 ; Menini et al . , 2017 ) . Al-   though not directly targeting biographies , some   works focused on the identification of entity - related   sequences of events ( Chambers and Jurafsky , 2008 )   and entity - based storylines ( Chambers and Juraf-   sky , 2009 ; Minard et al . , 2015 ; V ossen et al . , 2016 ) .   Despite the above mentioned variety of ap-   proaches to biographical event detection , there are   pending and urgent issues to be addressed , which   limit a full development of the research area . In   particular , we have identified three critical issues :   i)the lack of a benchmark annotated corpus for   evaluating biographical event detection ; ii)the lack   of models specifically designed for detecting and   extracting biographical events ; and finally iii)the   lack of a systematic study of the potential represen-   tation bias of minority groups , non - Western people ,   and younger generations in biography repositories   publicly available , such as Wikipedia ( D’ignazio   and Klein , 2020 ) .   Contributions Our work addresses these issues   by presenting a novel benchmark corpus , a BERT-   based model for biographical event detection , and   an analysis of 48,789Wikipedia biographies of   writers born since 1808 . Our results show that   existing data sets annotated for event detection   may be easily re - used to detect biographical events   achieving good results in terms of F - measure . The   analysis of the 48,789biographies from Wikipedia   extends the findings from previous work indicat-   ing that representational biases are present in an12370allegedly objective source such as Wikipedia along   intersectional axes ( Crenshaw , 2017 ) , namely eth-   nicity and gender .   The rest of the paper is organized as follows . In   Section 2 , we present WikiBio , a novel manually   annotated corpus of biographical events . Section 3   presents the experiments in event detection and   coreference resolution of the target entities of a   biographies . Section 4 is devoted to the analysis of   the biases in Wikipedia biographies . Conclusions   and future work end the paper in Section 5 .   Code and WikiBio corpus are available at   the following url : https://github.com/   marcostranisci / WikiBio/ .   2 The WikiBio Corpus   WikiBio is a corpus annotated for biographical   event detection , composed of 20Wikipedia biogra-   phies . The corpus includes all the events which are   associated with the entity target of the biography .   In this section , we present our annotation   scheme , discuss the agreement scores and present   some cases of disagreement . Lastly , we present the   results of our annotation effort , and compare them   with existing corpora annotated for event detection   and coreference resolution .   2.1 Annotation Tasks   Since the biographical event detection task consists   in annotating all events related to the person who   is the subject of a biography , annotation guidelines   focus on two separate subtasks : ( i ) the identifica-   tion of all the mentions of the target entity and   the resolution of its coreference chains ; and ( ii )   the identification and linking of all the events that   involve the target entity .   Entity annotation . The entity annotation subtask   requires the identification of all mentions of a spe-   cific Named Entity ( NE ) ( Grishman and Sundheim ,   1996 ) of type Person , which is the target of the   biography and all its coreferences ( Deemter and   Kibble , 2000 ) within the Wikipedia biography . For   the modeling of this subtask , we used the GUM cor-   pus ( Zeldes , 2017 ) , introducing different guidelines   about the following aspects : i)only the mentions   of the entity - target of the biography must be an-   notated ; ii)mentions of the target entity must be   selected only when they have a role in the event   ( Example 1 , where the possessives “ his ” is not an-   notated ) ; and iii)indirect mentions of the targetentity must be annotated only if they are related to   biographical events ( Examples 2 and 3 ) .   1.Kenule Saro - Wiwa was born in Bori [ ... ] His   father ’s hometown was the village of Bane ,   Ogoniland .   2.Hemarried Wendy Bruce , whom hehad   known since they were teenagers .   3.In 1985 , the Biafran Civil War novel Sozaboy   was published .   Event Annotation . Although there is an intuitive   understanding of how to identify event descrip-   tions in natural language texts , there is quite a large   variability in their realizations ( Pustejovsky et al . ,   2003b ) . Araki et al . ( 2018 ) point out that some   linguistic categories , e.g. , nouns , fits on an event   continuum . This makes the identification of event   mentions a non trivial task . Our event annotation   task mainly relies on TimeML ( Pustejovsky et al . ,   2003a ) and RED ( O’Gorman et al . , 2016 ) , where   ‘ event ’ is “ a cover term for situations that happen   or occur . ” ( Pustejovsky et al . , 2003a )   Events are annotated at a single token level with   no restrictions on the parts of speech that realize   the event . Following Bonial and Palmer ( 2016 ) ,   we introduced a special tag ( LINK ) for marking a   limited set of light and copular verbs , as illustrated   in Example 4 . The adoption of LINK is aimed at   increasing the compatibility of the annotated cor-   pus with OntoNotes , the resource with the highest   number of annotated events .   4.Ken Saro - Wiwa < LINK > was < LINK />a   Nigerian < EVENT > writer < EVENT / >   < LINK source=‘be ’ target   = ‘ writer ’ / > .   Lastly , to enable automatic reasoning on biogra-   phies , we annotate the contextual modality of   events ( O’Gorman et al . , 2016 ) . In particular ,   to account for the uncertainty / hedged modality ,   i.e. , any lexical item that expresses “ some de-   gree of uncertainty about the reality of the target   event ” ( O’Gorman et al . , 2016 ) , we have defined   three uncertainty values : INTENTION , for mark-   ing all the events expressing an intention ( like ‘ try ’   or ‘ attempt ’ ) ; NOT_HAPPENED , for marking all   events that have not occured ; EPISTEMIC , which   covers all the other types of uncertainty ( e.g. , opin-   ion , conditional ) . The uncertainty status of the12371Annotation Layer A0 & A1 A0 & A2   Event 0.72 0 .86   Entity 0.65 0 .86   LINK 0.76 0 .64   CONT_MOD 0.71 0 .64   events is annotated by linking the contextual modal-   ity marker and the target event , as illustrated in   Example 5 :   5.Feeling alienated , he decided toquit college ,   but was stopped [ ... ]   < CONT_MOD source   = ’ decided ’ target = quit ’   value=’INTENTION ’ / >   < CONT_MOD source   = ’ stopped ’ target = ’ quit ’   value=’NOT_HAPPENED ’ / >   Corpus Annotation and IAA . The annotation   task was performed by three expert annotators ( two   men and one woman - all authors of the paper ) ,   near - native speakers of British English , having a   long experience in annotating data for the specific   task ( event and entity detection ) . One annotator   ( A0 ) was in charge of preparing the data by dis-   carding all non - relevant sentences to speed - up the   annotation process . This resulted in a final set of   1,691sentences containing at least one mention of   a target entity . The entity and event annotations   were conducted as follows : A0 annotated the entire   relevant sentences , while a subset of 400sentences   was annotated by A1 and A2 , who respectively   labeled 200sentences each . We report pair - wise   Inter - Annotator Agreement ( IAA ) using Cohen ’s   kappa in Table 1 . In general , there is a fair agree-   ment across all the annotation layers . At the same   time , we observe a peculiar behavior across the   annotators : there is a higher agreement between   A0 and A2 for the event and entity layers when   compared to A0 and A1 , but the opposite occurs   with the relations layers ( LINK and CONT_MOD ) .   For the events , the higher disagreement is due to   nominal events , often misinterpreted as not bearing   an eventive meaning . For instance , the noun “ trip ”   in example 6 was not annotated by A1 .   6.When Ng ˜ug˜ıreturned to America at the end   ofhismonth trip[ ... ]For the entities , we observed that disagreement   is due to two reasons . The first is the consequence   of a disagreement in the event annotations . When-   ever annotators disagree on the identification of   an event , they also disagree on the annotation of   the related entity mention , as in the case of the   pronoun ‘ his ’ in example 6 . Another reason of dis-   agreement regards indirect mentions . Annotators   often disagree on annotation spans , as in “ Biafran   Civil War novel Sozaboy was published ” where   A1 selected ‘ SozaBoy ’ , while A2 ‘ novel Sozaboy ’ .   When it comes to LINK , problems are mainly due   to the identification of light verbs . Despite the deci-   sion of considering only a close set of copular and   light verbs to be marked as LINK ( Cfr Bonial and   Palmer ( 2016 ) ) , annotators used this label for other   verbs , such as ‘ begin ’ or ‘ hold ’ .   7.Walker began to take up reading and writing .   2.2 WikiBio : Overview and Comparison with   Other Resources   The WikiBio corpus is composed of 20biographies   of African , and African - American writers extracted   from Wikipedia for a total amount of 2,720sen-   tences . Among them , only 1,691sentences include   at least one event related to the entity target of the   biography . More specifically , there are 3,290an-   notated events , 2,985mentions of a target entity ,   343LINK tags , and 75CONT_MOD links .   Corpora size and genres We compare WikiBio   against five relevant existing corpora that , in princi-   ple , could be used to train models for biograph-   ical event detection : GUM ( Zeldes , 2017 ) , Lit-   bank ( Sims et al . , 2019 ) , Newsreader ( Minard et al . ,   2016 ) , OntoNotes ( Hovy et al . , 2006 ) , and Time-   Bank ( Pustejovsky et al . , 2003b ) . For each corpus ,   we took into account the number of relevant an-   notations and the types of texts . As it can be ob-   served in Table 2 , corpora vary in size and genres .   OntoNotes is the biggest one and includes 159,938   events , and 22,234entity mentions . The smaller   is NewsReader , with only 594annotated events .   TimeBank and LitBank are similar in scope , since   they both include about 7.5Kevents , while GUM   includes 9,762entity mentions .   Text types With the exception of GUM , which   includes 20biographies out of 175documents , all   other corpora contains types of texts other than12372Corpus Size Text types Relevant task   TimeBank 7,471events news Event detection   OntoNotes 159,938 events ,   22,234 entity   mentionsframe - theory Event & Entity detection   NewsReader 594events TimeML Event detection   GUM 9,762 entity   mentionsbiographies Entity detection   LitBank 7,383events literary works Event Detection   biographies such as news , literary works , and tran-   scription of TV news . To get a high - level pic-   ture of the potential similarities and differences   in terms of probability distributions , we calculated   the Jensen - Shannon Divergence ( Menéndez et al . ,   1997 ) . Such metric may be useful for identifying   which corpora are most similar to WikiBio . The   results show that WikiBio converges more with   GUM ( 0.43 ) , OntoNotes ( 0.48 ) and LitBank ( 0.49 )   rather than with TimeBank ( 0.51 ) and Newsreader   ( 0.54 ) . Such differences have driven the selection   of data for the training set described in Section 3.2 .   Annotations of entities , events , and coreference   The distribution of the target entity within biogra-   phies in the WikiBio corpus has been compared   with two annotated corpora for coreference res-   olution and named entity recognition : OntoNotes   ( Hovy et al . , 2006 ) and GUM ( Zeldes , 2017 ) . Since   such corpora were developed for identifying the   coreferences of all NEs in a document , we mod-   ified annotations to keep only the most frequent   NEs of type ‘ person ’ in each document . The ratio-   nale was making these resources comparable with   WikiBio , which includes only the coreferences to a   single entity , namely the subject of each biography .   After doing that , we computed the ratio between   the number of tokens that mention the target en-   tity and the total number of tokens , and the ratio   between the number of sentences where the tar-   get entity is mentioned against the total number of   sentences . While this operation did not impact on   GUM , in which 174out of 175documents contain   mentions of people , it had an important impact on   OntoNotes , in which 1,094documents ( 40 % ) do   not mention entities of the type Person .   Tokens mentioning the target entity are 5%on   OntoNotes , 8.7%on GUM and 4%on WikiBio .   Such differences can be explained by the averagelength of documents in these corpora , which is of   388tokens in OntoNotes , 978 in GUM , and 3,754   in WikiBio . As a matter of fact , if the percentage   of sentences mentioning the target - entity is consid-   ered instead of the total number of tokens , WikiBio   shows an higher ratio ( 61.7 % ) of sentences men-   tioning the target entity , than OntoNotes ( 20.8 % )   and GUM ( 42.6 % ) .   The three most frequently occurring lemmas in   the WikiBio corpus seem to be strongly related to   the considered domain : ‘ write ’ represents 3.2%of   the total , ‘ publish ’ 2.9 % , and ‘ work ’ 1.8 % . ‘ Re-   turn ’ ( 1.3 % ) appears to have a more general scope ,   since it highlights a movement of the target entity   from a place to another . The comparison with other   corpora annotated for event detection shows differ-   ences concerning the most frequent events . The top   three in OntoNotes ( Bonial et al . , 2010 ) are three   light verbs : ‘ be ’ , ‘ have ’ , and ‘ do ’ . This may be   intrinsically linked to its annotation scheme which   considers all verbs as candidates for being events ,   including semantically empty ones ( Section 2.1 ) .   NewsReader ( Minard et al . , 2016 ) and TimeBank   ( Pustejovsky et al . , 2003b ) include two verbs ex-   pressing reporting actions among the top five , thus   revealing that they are corpora of annotated news .   Litbank ( Sims et al . , 2019 ) , which is a corpus of   100annotated novels , includes in its top - ranked   events two visual perception verbs and two verbs   of movement , which may reveal the centrality of   characters in this documents . The event ‘ say ’ is   top - ranked in all the five corpora .   3 Detecting Biographical Events   In this section we describe a series of experiments   for the detection of biographical events . Exper-   iments involve the use of the existing annotated   corpora for two tasks : entity mentions detection12373   ( Section 3.1 ) and event detection ( Section 3.2 ) . In   both cases we used a 66 million parameters Distil-   Bert model ( Sanh et al . , 2019 ) . In this setting the   WikiBio corpus is both used as part of the training   set and as a benchmark for testing how well exist-   ing annotated corpora may be used for the task . For   such experiments a NVIDIA RTX 3030 ti was used .   The average length of each fine - tuning session was   40minutes .   3.1 Entity Detection   For this task we adapted the annotations in   OntoNotes ( Hovy et al . , 2006 ) and GUM ( Zeldes ,   2017 ) keeping only mentions of the most frequent   entities of type ‘ person ’ . As a result we obtained   870documents from OntoNotes , 174from GUM .   The WikiBio corpus was split into three subsets :   five documents for the development , 10for the   test , and five for the training . Given the imbalance   between the existing resources and WikiBio , we   always trained the model with a fixed number of   100documents , in order to reduce the overfitting   of the model over the other datasets .   Experiments consist in training a DistilBert   model for identifying all the tokens mentioning the   target entity of a given model and were performed   on six different training sets . Since the focus of   our work is to develop a model for detecting bio-   graphical events , WikiBio was used as development   set for better monitoring its degree of compatibil-   ity with existing corpora . Following the approach   by Joshi et al . ( 2020 ) , we split each document into   sequences of 128tokens , and for each document   we created one batch of variable length containing   all the sequences . Table 4 shows the results of these   experiments . As it can be observed , including the   WikiBio corpus in the training set did not result in   an increase of the performance of the model . This   may be due to the low number of WikiBio docu-   ments in the training . The highest performance was   obtained in two experiments : one using a training   set only composed of documents from OntoNotes , which obtained a F - score of 0.808 , and one with a   miscellaneous of 50OntoNotes and 50GUM doc-   uments , that obtained 0.792 . To understand if the   difference between the two experiments is signifi-   ca nt , we performed a One - Way ANOV A test over   the train , development , and test F - scores obtained   in both experiments . The test returned a p - value   of0.44 , which confirms a significant difference   between the two results   3.2 Event Detection   Event Detection experiments were guided by the   comparison between WikiBio and the resources   for event detection described in Section 2.2 . Since   OntoNotes was annotated according to the Prop-   Bank guidelines ( Bonial et al . , 2010 ) , which only   consider verbs as candidates for such annotation ,   we partly modified its annotations before running   the experiments . We first adapted the OntoNotes   semantic annotation by replacing light and copu-   lar verbs ( Bonial and Palmer , 2016 ) with nominal   ( Meyers et al . , 2004 ) and adjectival events . Then   we ran a battery of experiments by fine - tuning a   DistilBert - based model using each dataset for train-   ing , and a series of miscellaneous of the most sim-   ilar corpora to WikiBio according to the Jensen-   Shannon Divergence metric ( Table 3 ) . Since we   were concerned with both assessing the effective-   ness of WikiBio for training purposes and testing   how far biographic events can be extracted , we   designed our training and testing data as follows .   WikiBio was employed in different learning phases :   in devising the training set ( i.e. , existing resources   were employed either alone or mixed with Wik-   iBio ) ; additionally , the development set was always   built by starting from WikiBio sentences . Finally ,   we always tested on WikiBio data .   As for the entity - detection experiments , the   1,691sentences containing events annotated in the   WikiBio corpus were split into three sets of equal   size that were used for training ( 564 ) , development   ( 563 ) , and testing ( 564 ) . Given the disproportion12374   between OntoNotes and other corpora , we sampled   a number of sentences for training which did not   exceeded 5,073 , namely three times the number   of sentences annotated in our corpus . Such length   was fixed also for miscellaneous training sets .   Experiments were organized in two sessions . In   the first session we fine - tuned a DistilBert model   for five epochs , using as training set the five cor-   pora presented in Section 2.2 individually as well as   three combinations of them : i)misc_01 , a miscel-   laneous of sentences extracted on equal size from   all corpora ; ii)misc_02 , in which sentences from   NewsReader , the most different corpus with Wik-   iBio ( Table 3 ) , were removed ; iii)misc_03 , a com-   bination of sentences from OntoNotes and Litbank ,   namely the two most similar corpora with Wik-   iBio . The model was fine - tuned on these training   sets both with and without a subset of the WikiBio   corpus for a total of 16different training sets . In   addition , we also fine - tuned and tested WikiBio   alone . We then continued the fine - tuning only for   the models which obtained the best F - scores .   Observing Table 5 , it emerges that , differently   from entity - detection experiments , including a sub-   set of WikiBio in the training set , even if in a small   percentage , always improves the results of the clas-   sifier . This especially happens for Litbank ( +0.191   F - Score ) , and TimeBank ( +0.031F - Score ) .   When looking at results of finetuning for single   corpora , it emerges that the model trained on the   modified version of OntoNotes and TimeBank ob-   tains the best scores . Such results are interesting   for two reasons . They confirm the intuition that   OntoNotes annotations may be easily modified to   account for nominal and adjectival events . They   also confirm the high compatibility of WikiBio and   TimeBank guidelines ( Sect . 2.1 ) . Even if the latter   is more divergent from WikiBio than other corpora ,   it seems to be compatible with it . As expected for   its limited size and high divergence with WikiBio ,   the training set based on NewReader sentences ob - tains the worst results , with an F - Score below 0.5 .   Results of miscellaneus training sets are interest-   ing as well : they generally result in models with   better performance , and they seem to work bet-   ter on the basis of their divergence with WikiBio .   Trained on misc_01 , a combination of all corpora ,   the model scores 0.827 , which is below the result   obtained with the modified version of OntoNotes .   If Newsreader is removed , the model obtains 0.831 ,   and0.832if also TimeBank is removed . It is also   worth mentioning the delta between the F - score   on the training and the test sets , which is −0.054   for misc_01 , −0.029for misc_02 , and −0.013for   misc_03 .   After the first fine - tuning step , we performed a   One - Way ANOV A for testing the significance of   differences between experiments . Analyzed in such   a way , the four best - ranked models never showed   a p - value below 0.5 , which means that there are   no significant differences between them . Thereby ,   we kept them for the second fine - tuning step that   consists on training the model for 15epochs on   these datasets . Absolute results ( Table 5 ) show   that the model trained on Timebank obtained the   best F - Score . However , as for the entity detection   experiments , we considered the deltas between the   training and test F - scores to select the best model   for our analysis . All models acquired by employing   a miscellaneous training set obtained a lower delta   between training and test , and scored a similar F-   Score .   4An Intersectional Analysis of Wikipedia   Biographies   In this section we provide an analysis of writers ’ bi-   ographies on Wikipedia adopting intersectionality   as a theoretical framework and the model described   in Section 3 as a tool for detecting biographical   events .   The concept of intersectionality ( Crenshaw,12375   2017 ) has been developed in the context of gen-   der and black studies to account inequalities that   can not be explained without a joint analysis of   socio - demographic factors . For instance , African   American women workers suffer higher discrim-   ination than their male counterpart , as Crenshaw   ( 1989 ) observed in her seminal work . Therefore ,   the injection of different socio - demographic fea-   tures for the analysis of discriminations may unfold   hidden forms of inequities about certain segments   of population . We adopt this framework to analyse   how the representations of non - Western women   writers on Wikipedia differs from those of Western   Women , Transnational Men , and Western Men .   For this analysis , we gathered 48,486Wikipedia   biographies of writers born since 1808 . We define   as Transnational all the writers born outside West-   ern countries and people who belong to ethnic mi-   norities ( Boter et al . , 2020 ; Stranisci et al . , 2022 ) .   Western men ’s biographies are 28,036 , Western   women ’s 12,413 , Transnational men ’s 5,471 , and   Transnational women ’s 2,470 . Information about   occupation , gender , year of birth , ethnic group , and   country of birth was obtained from Wikidata ( Vran-   deˇci´c and Krötzsch , 2014 ) , which has been used   for filtering and classifying biographies .   For each biography , we first identified all thementions of the corresponding target entity ( Sec-   tion 3.1 ) . We then removed the sentences that   do not contain a mention of the entity . This re-   duced the number of sentences to be annotated   for event detection from 1,486,320to1,163,475   ( −21.8 % ) . As a final step , we annotated events   ( Section 3.2 ) in the filtered sentences .   Table 6 shows the distribution of biographical   events about men , women , Western , and Transna-   tional people . The vast majority of events are about   Western men ( 62.2 % ) , while at the opposite side of   the spectrum there are Transnational women writ-   ers , whose representation is below 5 % . Ethnicity is   a cause of underrepresentation more than gender :   events about Transnational men are only 11.2%of   the total , while those about Western women 21.4 % .   The average number of events per - author shows a   richness in the description of Transnational Women   ( 50.92events ) against Western ones ( 43.73events ) .   The analysis of event types presents a similar   distribution . 27,885event types – intended as the   number of unique tokens that occur in each distri-   bution – are detected in Western men ’s biographies   ( 44.9perbiography ) , while only 9,254 in Transna-   tional women ’s biographies ( 40.4perbiography ) .   However , the overlap of event types between these   two categories is very large ( 92.6 % ) The same12376comparison , conducted on the other groups , re-   veals a higher number of group - specific event types :   87.8%of event types about Transnational Men are   shared with Western Men , and the rate is lower for   Western Women ( 84.1 % ) .   A comparative analysis of most distinctive   events per category of people provides additional   insight about the representation of women and   Transnational writers in Wikipedia biographies . In   order to do so , we first computed the average fre-   quency of each event in all biographies of the four   groups of writers in Table 6 . We then compared   these distributions with the Shifterator library ( Gal-   lagher et al . , 2021 ) , which allows computing and   plotting pairwise comparisons between different   distribution of texts with different metrics . Coher-   ently with the analysis performed in previous sec-   tions , we chose the Jensen - Shannon Divergence   metric , and analyzed the distribution of events   about Transnational Women against Transnational   Men , Western Men , and Western Women . Table 7   shows the most diverging events between Transna-   tional and Western writers , while Table 8 shows   the20events about Transnational women that di-   verge most with other distributions : Transnational   men , Western men , and Western women . Events   are ordered on the basis of how much they are spe-   cific to the distribution of Transnational women .   In Appendix A graphs with comparisons between   distributions can be consulted .   A first insight from a general overview of distinc-   tive events about Transnational Women writers is   that they seem to never die . Events like ‘ death ’ or   ‘ died ’ are never distinctive for them but always for   the group against which they are compared . This   may be explained by the average year of birth of   Transnational Women writers with a biography on   Wikipedia , which is 1951 , while for Western men   is1936 , 1943 for Transnational men , and 1944 for   Western woman .   The analysis of the most salient biographical   Group Events Avg Types   Western M 1,57 M 56.08 27 , 885   Transnational M 285 K 52.10 14 , 057   Western W 542 K 43.73 17 , 324   Transnational W 125 K 50.92 9 , 254events between Transnational women and Transna-   tional men shows how intersectionality helps to   identify gender biases . When Transnationals are   considered as a single group ( Table 7 ) against the   Western counterparts , the majority of the biograph-   ical events are related to career ( award , conferred )   or to social commitment ( activist , migrated , ex-   ile ) . When the comparison is made within the   Transnational group ( Table 8) , the gender bias   demonstrated by Sun and Peng ( 2021 ) and Bam-   man and Smith ( 2014 ) clearly emerges . In fact ,   ‘ married ’ , ‘ marriage ’ , and ‘ divorce ’ are associated   to Transnational women . In addition , there is a   lack of career - related events about them , while this   is not the case for men ( actor , chairman , politi-   cian ) . The comparison between Transnational   women and Western men still shows a gender   bias , but less prominent . Among the most salient   events , only ‘ mother ’ highlights a potential bias ,   while events on Transnational women career ( ‘ win ’ ,   ‘ won ’ , ‘ award ’ , ‘ selected ’ ) , education ( ‘ degree ’ ,   ‘ education ’ , ‘ schooling ’ ) and social commitment   ( ‘ activist ’ ) are present .   Finally , the comparison between Transnational   and Western women offers three additional insights .   First , the only event about private life which is   salient for one of the two groups is ‘ married ’ . This   indicates that private life events of women - in   general - are always presented in relation to their   conjugal status . Second , careers and social com-   mitments are particularly present for Transnational   women . Finally , the framing of the concept “ reloca-   tion ” is expressed using different event triggers : the   more neutral ‘ move ’ is used for Western women ,   while the more marked , negatively connotated term   ‘ migrate ’ is associated with Transnational women .   Summarizing , Transnational Women are under-   represented on Wikipedia with respect to other   groups , both in terms of number of biographies   and events . The analysis of their most distinctive   biographical events shows that the already - known   tendency of mentioning private life events about   women in Wikipedia biographies ( Sun and Peng ,   2021 ; Bamman and Smith , 2014 ) can be refined   when coupled to ethnic origins . Indeed , the extent   of the presence of gender biases is more salient   when comparing the biographical entries within   the same broad “ ethnic ” group , while is becomes   obfuscated across groups , making other bias ( i.e. ,   racial ) more prominent.123775 Conclusion and Future Work   In this paper we presented a novel set of compu-   tational resources for deepening the analysis of   biographical events and improving their automatic   detection . We found that existing annotated cor-   pora may be successfully reused to train models   that obtain good performances . The model for en-   tity detection , trained on OntoNotes , obtained a F-   score of 0.808 , while the model for event detection ,   trained on TimeBank and Wikibio , scored 0.859 .   We have applied these newly developed resources   to perform an analysis of biases in Transnational   women writers on Wikipedia adopting intersection-   ality as a framework to interpret our results . In par-   ticular , we have identified that the representation   of women and non - Western people on Wikipedia   is problematic and biased . Using different axes   of analysis - as suggested by intersectionality - it   becomes easier to better identify these biases . For   instance , gender bias against Transnational women   are more marked when comparing their biographies   against those of Transnational men rather than   Western ones . On the other hand , potential racial bi-   ases emerge when comparing Transnational women   to Western women . Using an intersectional frame-   work would benefit the understanding and counter-   ing of biases of women and non - Western people on   Wikipedia .   Future work will improve the model for bio-   graphical event detection , and to extend the anal-   ysis on a wider set of biographical entries from   different sources .   Transnational Western   poet , education , school-   ing , award , degree ,   completed , awarded ,   activist , obtained ,   professor , started ,   translated , conferred ,   migrated , exile , recip-   ient , born , novelist ,   writer , lyricistwrote , appeared , sold ,   illustrated , described ,   married , starred , met ,   told , illustrator , enlisted   Limitations and Ethical Issues   This work presents some limitations that will be   addressed in future work . In particular , i)even ifTransnational Women Transnational Men   defeated , daughter , ac-   tress , married , lost , ap-   peared , marriage , deaf-   eating , won , began ,   activist , loosing , di-   vorced , raised , attended ,   win , featured , seeded ,   mother , grewactor , son , chairman ,   lyricist , served , politi-   cian , critic , father ,   joined , death , accused ,   known , poet , scholar ,   elected , imprisoned ,   president , established ,   exile   Transnational Women Western Men   activist , degree , won ,   actress , received , born ,   daughter , award , educa-   tion , defeated , recipient ,   defeating , win , selected ,   mother , writer , school-   ing , completed , poet ,   lostwrote , enlisted , service ,   actor , claimed , father ,   assigned , drafted ,   directed , developed ,   death   Transnational Women Western Women   defeated , defeating ,   lost , activist , education ,   loosing , schooling , de-   gree , poet , completed ,   win , seeded , injury ,   award , match , reach ,   migrated , participated ,   professor , losswrote , appeared , mar-   ried , author , published ,   starred , death , lives ,   moved , died , sold , illus-   trator , illustrated , nom-   inated , reviewer , write ,   lived , developed , spent   the model for biographical event detection obtained   good results , more sophisticated approaches may   be devised to increase its effectiveness ( e.g. , best   performing LMs , multi - task settings ) ; ii)the in-   tersectional analysis was performed on a specific   sample of people , and thus limited to writers . Tak-   ing into account people with other occupations may   lead to different results ; finally , iii)only Wikipedia   biographies were considered : biographies from   other sources may differ in style and thus pose   novel challenges to the biographical event detec-   tion task .   The research involved the collection of docu-   ments from Wikipedia , which are released under   the Creative Commons Attribution - ShareAlike   3.0 license . The annotation of the experiment12378was not crowdsourced . All the three annotators   are member of the research team who carried   out the research as well as authors of the present   paper . They are all affiliated with the University of   Turin with whom they have a contract regulated by   the Italian laws . Their annotation activity is part   of their effort related to the development of the   present work , which was economically recognized   within their contracts with the University of   Turin . A data statement for the research can   be accessed at the following url : https :   //github.com / marcostranisci/   WikiBio / blob / master / README.md   References12379   A Comparison Between Transnational   Women and Men through the JS   Divergence Metric   In this Section you can observe a comparative   analysis of the divergence between events about   Transnational women against Transnational men   ( Figure 1 ) , Western men ( Figure 2 ) , and Western   women ( Figure 3 ) . All divergences were computed   and plotted with Shifterator ( Gallagher et al . , 2021).123801238112382ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   6   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . For our work we handled public data from Wikipedia   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Not applicable . Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Not applicable . Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Not applicable . Left blank .   C / squareDid you run computational experiments ?   3   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   312383 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   The focus of the experiments was to test the impact of different training set over the same vanilla   version of a small LM like DistilBert . So we did n’t provide information about that   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   we used the standard parameters of these off - the - shelf tools   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 2.1 and Section " Limitations and Ethical Issues "   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   2.112384