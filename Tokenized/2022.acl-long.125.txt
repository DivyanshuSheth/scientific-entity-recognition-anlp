  Alan AnsellEdoardo Maria PontiAnna KorhonenIvan Vuli ´ cLanguage Technology Lab , University of CambridgeMila - Quebec AI Institute and McGill University   Abstract   Fine - tuning the entire set of parameters of a   large pretrained model has become the main-   stream approach for transfer learning . To in-   crease its efﬁciency and prevent catastrophic   forgetting and interference , techniques like   adapters and sparse ﬁne - tuning have been de-   veloped . Adapters are modular , as they can   be combined to adapt a model towards dif-   ferent facets of knowledge ( e.g. , dedicated   language and/or task adapters ) . Sparse ﬁne-   tuning is expressive , as it controls the behav-   ior of all model components . In this work , we   introduce a new ﬁne - tuning method with both   these desirable properties . In particular , we   learn sparse , real - valued masks based on a sim-   ple variant of the Lottery Ticket Hypothesis .   Task - speciﬁc masks are obtained from anno-   tated data in a source language , and language-   speciﬁc masks from masked language model-   ing in a target language . Both these masks   can then be composed with the pretrained   model . Unlike adapter - based ﬁne - tuning , this   method neither increases the number of param-   eters at inference time nor alters the original   model architecture . Most importantly , it out-   performs adapters in zero - shot cross - lingual   transfer by a large margin in a series of mul-   tilingual benchmarks , including Universal De-   pendencies , MasakhaNER , and AmericasNLI .   Based on an in - depth analysis , we addition-   ally ﬁnd that sparsity is crucial to prevent both   1 ) interference between the ﬁne - tunings to be   composed and 2 ) overﬁtting . We release the   code and models at .   1 Introduction   Fine - tuning of pretrained models ( Howard and   Ruder , 2018 ; Devlin et al . , 2019 , inter alia ) is ar-   guably the dominant paradigm in NLP at present .   Originally , “ ﬁne - tuning ” involved supervised learn-   ing of all the parameters of a model pretrained   on unlabeled texts . However , given the size ofTransformer - based architectures , this approach is   often time- and resource- inefﬁcient , and may result   in catastrophic forgetting and interference ( Wang   et al . , 2020 ) during multiple adaptations . To over-   come these limitations , two main alternatives have   emerged : 1 ) through adapters , new parameters can   be added to a pretrained model in the form of extra   intermediate layers ( Rebufﬁ et al . , 2017 ; Houlsby   et al . , 2019 ) and ﬁne - tuned while keeping all the   pretrained parameters ﬁxed ; 2 ) sparse ﬁne - tuning   ( SFT ) of a small subset of pretrained model param-   eters ( Guo et al . , 2021 ; Zaken et al . , 2021 ; Xu et al . ,   2021b , inter alia ) .   Adapters have proven especially useful in multi-   lingual NLP ( Bapna and Firat , 2019 ; Üstün et al . ,   2020 ; Pfeiffer et al . , 2020b ; Vidoni et al . , 2020 ;   Pfeiffer et al . , 2021b ; Ansell et al . , 2021 ) because   they exhibit a surprising degree of modularity . This   ability to disentangle and recombine orthogonal   facets of knowledge in original ways ( Ponti et al . ,   2021 ; Ponti , 2021 ) allows for separately learning a   task adapter from labeled data in a source language   and dedicated language adapters from unlabeled   data in the source language and target languages .   By stacking these components , it is possible to per-   form zero - shot cross - lingual transfer . Compared   to sequentially ﬁne - tuning the full model on both   the task and target language , this yields superior   performance and efﬁciency ( Pfeiffer et al . , 2020b ) .   Notably , achieving coverage over Ntasks inN   target languages with the sequential approach re-   quiresNNmodels to be trained , whereas the   modularity of adapters reduces this to N+N.   Meanwhile , the advantage of SFTs over adapters   is their expressivity : rather than a non - linear trans-   formation of the output of Transformer layers ( e.g. ,   using a shallow MLP as with adapters ) , they can   operate directly on a pretrained model ’s embedding   and attention layers . It therefore seems natural to   search for a parameter - efﬁcient ﬁne - tuning method   that is both modular and expressive.1778   To this end , we propose Lottery Ticket Sparse   Fine - Tuning ( LT - SFT ) , a simple and general-   purpose adaptation technique inspired by the Lot-   tery Ticket Hypothesis ( LTH ; Frankle and Carbin ,   2019 ; Malach et al . , 2020 ) , which was originally   conceived for pruning large neural networks . In   particular , after ﬁne - tuning a pretrained model for a   speciﬁc task or language , we select the subset of pa-   rameters that change the most . Then , we rewind the   model to its pretrained initialization ( without set-   ting any value to zero , contrary to the original LTH   algorithm ) . By re - tuning again only the selected   subset of parameters , we obtain a sparse ﬁne - tuning   in the form of a vector of differences with respect   to the pretrained model . Multiple SFTs can be com-   posed by simply summing them with the pretrained   model . We provide a graphical representation of   our method in Figure 1 .   We benchmark LT - SFT on a series of multilin-   gual datasets , including Universal Dependencies   ( Zeman et al . , 2020 ) for part - of - speech tagging and   dependency parsing , MasakhaNER ( Adelani et al . ,   2021 ) for named entity recognition , and Americas-   NLI ( Ebrahimi et al . , 2021 ) for natural language in-   ference . We evaluate it in a zero - shot cross - lingual   transfer setting on 35 typologically and geographi-   cally diverse languages that include both languages   seen and unseen during masked language modeling   of the pretrained model . The results in all transfer   tasks indicate that LT - SFT consistently achieves   substantial gains over the current state - of - the - art   adapter - based method for cross - lingual transfer ,   MAD - X ( Pfeiffer et al . , 2020b).In addition to its superior performance , modu-   larity , and expressivity , LT - SFT offers a series of   additional advantages over adapters : 1 ) the number   of parameters remains constant , which prevents the   decrease in inference speed observed when adapter   layers are added ; 2 ) the neural architecture remains   identical to the pretrained model , which makes   code development model - independent rather than   requiring special modiﬁcations for each possible ar-   chitecture ( Pfeiffer et al . , 2020a ) . Finally , 3 ) we em-   pirically demonstrate that the peak in performance   for LT - SFT is consistently found with the same per-   centage of tunable parameters , whereas the best re-   duction factor for MAD - X is task - dependent . This   makes our method more robust to the choice of   hyper - parameters .   In addition , we ﬁnd that a high level of spar-   sity in language and task ﬁne - tunings is beneﬁcial   to performance , as this makes overlaps less likely   and poses a lower risk of creating interference be-   tween the knowledge they contain . Moreover , it   makes ﬁne - tunings less prone to overﬁtting due to   their constrained capacity . Thus , sparsity is a fun-   damental ingredient for achieving modularity and   composability . These properties in turn allow for   systematic generalization to new combinations of   tasks and languages in a zero - shot fashion .   2 Background   To establish a broader context for our research , we   ﬁrst provide a succinct overview of current methods   for efﬁcient ﬁne - tuning , such as adapters and SFT .   We then recapitulate the Lottery Ticket Hypothesis,1779upon which our newly proposed method is built .   Adapters and Composition . Anadapter is a com-   ponent inserted into a Transformer model with the   purpose of specializing it for a particular language ,   task , domain , or modality ( Houlsby et al . , 2019 ) .   Previous work in multilingual NLP has mainly   adopted the lightweight yet effective adapter vari-   ant of Pfeiffer et al . ( 2021a ) . In this setup , only one   adapter module , consisting of a successive down-   projection and up - projection , is injected per Trans-   former layer , after the feed - forward sub - layer . The   adapter Aat theb - th Transformer layer performs   the following operation :   A(h;r ) = Ua(Dh ) + r : ( 1 )   handrare the Transformer hidden state and the   residual at layer b , respectively . D2Rand   U2Rare the down- and up - projections , re-   spectively ( hbeing the Transformer ’s hidden layer   size , andmthe adapter ’s dimension ) , and a()is   a non - linear activation function . The residual con-   nectionris the output of the Transformer ’s feed-   forward layer whereas his the output of the sub-   sequent layer normalization . During ﬁne - tuning of   a pretrained model with adapters , only the adapter   parametersUandDare modiﬁed while the pre-   trained model ’s parameters are kept ﬁxed .   In the MAD - X adapter composition framework   for cross - lingual transfer ( Pfeiffer et al . , 2020b ) ,   alanguage adapter ( LA ) for a massively multi-   lingual Transformer ( MMT ) is learned for each   source and target language through masked lan-   guage modeling ( MLM ) , and a task adapter ( TA )   is learned for each target task , where the LA for the   source language is inserted during TA training . At   inference time , the task adapter and target language   adapter are composed by stacking one on top of   the other . This adapter composition approach has   been shown to be highly effective for cross - lingual   transfer ( Pfeiffer et al . , 2020b , 2021b ; Ansell et al . ,   2021 ) , especially for low - resource languages and   target languages unseen during MMT pretraining .   Sparse Fine - Tuning . We callF = F(;+  )   asparse ﬁne - tuning ( SFT ) of a pretrained neural   modelF(;)if  is sparse . We sometimes refer   to  itself as an SFT , or as the SFT ’s difference   vector . Previously proposed SFT methods include   DiffPruning ( Guo et al . , 2021 ) , BitFit ( Zaken et al . ,   2021 ) and ChildTuning ( Xu et al . , 2021b ) . Diff-   Pruning simulates sparsity of the difference vector   during training by applying a continuous relaxationof a binary mask to it . BitFit on the other hand   allows non - zero differences only for bias parame-   ters . ChildTuning selects a subset of ﬁne - tunable   parameters by using Fisher information to mea-   sure the relevance of each parameter to the task .   These methods have been shown to be competitive   with full ﬁne - tuning on GLUE ( Wang et al . , 2019 ) ,   despite the difference vector  having fewer than   0.5 % non - zero values .   Lottery Ticket Hypothesis . ( LTH ; Frankle and   Carbin , 2019 ; Malach et al . , 2020 ) states that each   neural model contains a sub - network ( a “ winning   ticket ” ) that , if trained again in isolation , can match   or even exceed the performance of the original   model . To achieve this , after a pruning stage where   some parameters are zero - masked and frozen ac-   cording to some criterion ( e.g. , weight magnitude ) ,   the remaining parameters are restored to their orig-   inal values and then re - tuned . This process of prun-   ing and re - training can be iterated multiple times .   The LTH has so far been used mostly for model   compression through network pruning ; to our   knowledge , we are the ﬁrst to use it for pretrained   model adaptation .   Multi - Source Task Training . Ansell et al . ( 2021 )   showed that training task adapters using data from   multiple source languages can result in sizable im-   provements in downstream zero - shot transfer per-   formance even when the total number of training   examples is held constant . In their training setup ,   each batch consisted of examples from a single ,   randomly selected source language , the language   adapter for which is activated for the duration of   the training step .   3 Methodology   3.1 Lottery Ticket Sparse Fine - Tuning   Training . In this work , we propose Lottery Ticket   Sparse Fine - Tuning ( LT - SFT ) . Similar to the Lot-   tery Ticket algorithm of Frankle and Carbin ( 2019 ) ,   our LT - SFT method consists of two phases :   ( Phase 1 ) Pretrained model parameters are   fully ﬁne - tuned on the target language or task data   D , yielding. Parameters are ranked according   to some criterion , in our case greatest absolute dif-   ferencej j , and the top Kare selected   for tuning in the next phase : a binary mask is   set to have 1 in positions corresponding to these   parameters , and 0 elsewhere .   ( Phase 2 ) After resetting the parameters to their1780original values  , the model is again ﬁne - tuned ,   but this time only the Kselected parameters are   trainable whereas the others are kept frozen . In   practice , we implement this by passing the masked   gradient  rL(F(;);D)(where  denotes   element - wise multiplication and La loss function )   to the optimizer at each step . From the resulting   ﬁne - tuned parameters we can obtain the sparse   vector of differences  =  .   In addition , we experiment with applying a   regularization term which discourages parameters   from deviating from their pretrained values .   Speciﬁcally , we use L1 regularization of the form   J( ) = Pj j .   Composition . Although we often use the term   “ sparse ﬁne - tuning ” to refer to the difference vector   itself , an SFT is most accurately conceptualized   as a functional which takes as its argument a param-   eterized function and returns a new function , where   some sparse difference vector  has been added to   the original parameter vector . Suppose we have a   language SFT Sand a task SFT Sdeﬁned by   S(F(; ) ) = F(;+  )   S(F(; ) ) = F(;+  ):   Then we have   SS(F(; ) ) = F(;+  +  ):   3.2 Zero - Shot Transfer with LT - SFT   We adopt a similar cross - lingual transfer setup to   MAD - X ( Pfeiffer et al . , 2020b , see also § 2 ) . We   start with an MMT Fwith pretrained parameters   learned through masked language modeling on   many languages , such as mBERT ( Devlin et al . ,   2019 ) or XLM - R ( Conneau et al . , 2020 ) .   For each language of interest l , we learn a lan-   guage SFT  through LT - SFT ( also with an   MLM objective ) on text from language l.   For each task of interest t , we learn a task SFT   through LT - SFT on annotated data from some   source language s. When learning the task SFT ,   we ﬁrst adapt to the source language by applying   the language SFT for s. The language SFT is   removed again after training . That is , we performLT - SFT on F(;+  ) to obtain ﬁne - tuned   parameter vector . We then calculate  =    (+  ) . Note that during task training ,   we also learn a classiﬁer head , which is fully ﬁne-   tuned during both phases of LT - SFT adaptation ,   with the same random initialization applied at the   beginning of each phase .   We perform zero - shot adaptation of Fto target   languagelfor tasktby composing language and   task SFTs to obtain F = F(;+  +  ) .   On top of this , we stack the classiﬁer head learned   fort . For a formal algorithm of LT - SFT and the   transfer procedure , we refer to Appendix A.   4 Experimental Setup   To evaluate our new method extensively , we bench-   mark its zero - shot cross - lingual performance on   four distinct tasks : part - of - speech tagging ( POS ) ,   dependency parsing ( DP ) , named entity recogni-   tion ( NER ) , and natural language inference ( NLI ) .   Table 1 summarizes our experimental setup , includ-   ing the datasets and languages considered in our   experiments . We put emphasis on low - resource   languages and languages unseen during MMT pre-   training , although we also evaluate on a few high-   resource languages . In total , we cover a set of 35   typologically and geographically diverse languages ,   which makes them representative of cross - lingual   variation ( Ponti et al . , 2019 , 2020 ) .   4.1 Baselines and Model Variants   The main baseline is MAD - X , the state - of - the - art   adapter - based framework for cross - lingual trans-   fer ( Pfeiffer et al . , 2020b ) . We use the “ MAD-   X 2.0 ” variant , where the last adapter layers are   dropped . Pfeiffer et al . ( 2021b ) found that this im-   proved performance , which we could conﬁrm in   our preliminary experiments . Since adapters with   the conﬁguration used by Pfeiffer et al . ( 2020b ) are   unavailable for many languages in our evaluation ,   we train our own for all languages . In Appendix   D we also provide an evaluation with comparable   language adapters from AdapterHub ( Pfeiffer et al . ,   2020a ) where available .   We also perform experiments with BF(Za-   ken et al . , 2021 ) to establish a baseline for an exist-   ing SFT technique . In addition to the main LT - SFT   model variant , on POS and DP we test a -   SFT variant as an ablation , where the Kparame-   ters to be ﬁne - tuned are selected at random rather   than based on an informed criterion.1781   For both LT - SFT and MAD - X , we also evaluate   a task adaptation ( TA)- conﬁguration , where   only the task SFT / adapter is applied , without the   target language SFT / adapter .   4.2 Language SFT / Adapter Training Setup   MLM Training Data . For all languages in our   POS and DP evaluation , we perform MLM lan-   guage SFT / adapter training on Wikipedia corpora .   We also use Wikipedia for all languages in our NER   evaluation if available . Where this is not the case ,   we use the Luo News Dataset ( Adelani et al . , 2021 )   for Luo and the JW300 corpus ( Agi ´ c and Vuli ´ c ,   2019 ) for Nigerian Pidgin . The main corpora for   the languages in our NLI evaluation are those used   by the dataset creators to train their baseline models   ( Ebrahimi et al . , 2021 ) ; however , since the sizes of   these corpora are restricted due to containing only   parallel data , we augment them with data from   Wikipedia and the corpora of indigenous Peruvian   languages of Bustamante et al . ( 2020 ) where avail-   able . More details on data sources are provided in   Appendix B.   Training Setup and Hyper - parameters . For   both SFTs and adapters , we train for the lesser   of 100 epochs or 100,000 steps of batch size 8 and   maximum sequence length 256 , subject to an ab-   solute minimum of 30,000 steps since 100 epochs   seemed insufﬁcient for some languages with very   small corpora . Model checkpoints are evaluated ev-   ery 1,000 steps ( 5,000 for high - resource languages )   on a held - out set of 5 % of the corpus ( 1 % for high-   resource languages ) , and the one with the smallest   loss is selected at the end of training . We use the   AdamW optimizer ( Loshchilov and Hutter , 2019)with an initial learning rate of 5 e-5 which is linearly   reduced to 0 over the course of training .   Following Pfeiffer et al . ( 2020b ) , the reduction   factor ( i.e. , the ratio between model hidden size   and adapter size ) for the adapter baseline was set   to 2 for a total of7.6 M trainable parameters . For   comparability , we set the same number of trainable   parametersKfor our language LT - SFTs . This   results in language SFTs with a sparsity of 4.3 %   for mBERT and 2.8 % for XLM - R. Since BF   tunes exclusively the bias parameters , its language   SFTs have a ﬁxed sparsity of 0.047 % for mBERT   and 0.030 % for XLM - R.   Importantly , during language sparse ﬁne - tuning ,   we decouple the input and output embedding ma-   trices and ﬁx the parameters of the output matrix ;   otherwise , we ﬁnd that the vast majority of the K   most changed parameters during full ﬁne - tuning   belong to the embedding matrix , seemingly due to   its proximity to the model output , which damages   downstream performance . We also ﬁx the layer   normalization parameters ; all other parameters are   trainable . For language adaptation , we apply L1   regularization as described in § 3.1 with = 0:1 .   Note that the speciﬁed training regime is applied in   the same way during both phases of LT - SFT .   For language adapter training in the MAD - X   baseline , we use the Pfeiffer conﬁguration ( Pfeiffer   et al . , 2021a ) with invertible adapters , special ad-   ditional sub - components designed for adapting to   the vocabulary of the target language , which yields   consistent gains.17824.3 Task SFT / Adapter Training Setup   For POS tagging , DP , and NER , we train task   SFTs / adapters on the datasets indicated in Table 1   for 10 epochs with batch size 8 , except during the   ﬁrst phase of LT - SFT training where we train for   only 3 epochs . Model checkpoints are evaluated   on the validation set every 250 steps , and the best   checkpoint is taken at the end of training , with the   selection metric being accuracy for POS , labeled   attachment score for DP , and F1 - score for NER .   Similarly to language ﬁne - tuning , we use an initial   learning rate of 5 e-5 which is linearly reduced to   0 over the course of training . For POS and NER   we use the standard token - level single - layer multi-   class model head . For DP , we use the shallow   variant ( Glavaš and Vuli ´ c , 2021 ) of the biafﬁne   dependency parser of Dozat and Manning ( 2017 ) .   For NLI , we employ the same ﬁne - tuning hyper-   parameters as Ebrahimi et al . ( 2021 ): 5 epochs with   batch size 32 , with checkpoint evaluation on the val-   idation set every 625 steps , and an initial learning   rate of 2e-5 . We apply a two - layer multi - class clas-   siﬁcation head atop the MMT output corresponding   to the [ CLS ] token .   We found that the number of trainable param-   eters during task adaptation ( governed by Kfor   SFTs and reduction factor for adapters ) has a large   effect on performance : we thus experiment with a   range of values . Speciﬁcally , we test adapter reduc-   tion factors of 32 , 16 , 8 , 4 , 2 , and 1 , and equivalent   values ofKfor SFT .   During task adaptation , we always apply the   source language adapter following Pfeiffer et al .   ( 2020b ) , or source language SFT ( see § 3.2 ) .   4.4 Multi - Source Training   To validate that task LT - SFT training , like task   adapter training in prior work ( Ansell et al . , 2021 ) ,   beneﬁts from the presence of multiple source lan-   guages in the training data , and to push the bound-   aries of zero - shot cross lingual transfer , we perform   multi - source training experiments on DP and NLI.We adopt a similar setup to Ansell et al . ( 2021 ):   we obtain the training set by concatenating the train-   ing data for all source languages . We randomly   shufﬂe the training set and train as in the single-   source case , except that each batch is composed   of examples from a single source language , whose   language SFT is applied during the training step .   We prioritize maximizing performance rather   than providing a fair comparison against the single-   source case , so unlike Ansell et al . ( 2021 ) , we use   the entirety of the training sets . In derogation of   this principle , we set a maximum of 15 K examples   per language for DP to better balance our sample .   For DP , we train our models on the UD treebanks   of 11 diverse high - resource languages . For NLI ,   we train on MultiNLI ( Williams et al . , 2018 ) plus   the data for all 14 non - English languages in the   XNLI dataset ( Conneau et al . , 2018 ) .   We also evaluate multi - source task SFT training   on extractive question answering ( QA ) , as a com-   paratively generous amount of multilingual data   is available for this task . Speciﬁcally , we train on   English data from SQuAD version 1 ( Rajpurkar   et al . , 2016 ) , all languages from MLQA ( Lewis   et al . , 2020 ) , and those languages from XQuAD   ( Artetxe et al . , 2020 ) which also appear in MLQA .   We evaluate on the languages present in XQuAD   but not in MLQA . For QA , we train for 5 epochs   with batch size 12 and initial learning rate 3 e-5 .   Full details of the source languages can be found   in Appendix B.   We use an equivalent reduction factor of 1 for   all tasks , following the strongest setting from our   single - source experiments . Except as stated above ,   the training conﬁguration and hyper - parameters are   the same as for single - source training .   5 Results and Discussion   We report the average test performance of zero-   shot cross - lingual transfer for the best reduction   factor ( or equivalent K ) in Table 2 . Some pat-   terns emerge across all four tasks : ﬁrst , LT - SFT   consistently outperforms all the baselines . In par-   ticular , it surpasses the state - of - the - art MAD - X   across all tasks , with gains of 2.5 accuracy in part-   of - speech tagging , 2.5 UAS and 3.7 LAS in de-   pendency parsing , 1.8 F1 score in named entity   recognition , and 1.9 accuracy in natural language   inference . Compared to -SFT , its superior   performance demonstrates the importance of select-   ing “ winning tickets ” rather than a random subset1783   of parameters . Secondly , the results demonstrate   the importance of language SFTs / adapters for spe-   cializing pretrained models to unseen languages ,   as they bring about a large increase in performance   across the 4 tasks compared to the corresponding   settings with task adaptation only ( TA- ) .   We remark that LT - SFT ’s zero - shot performance   also exceeds translation - based baselines on the   AmericasNLI task , achieving an average accu-   racy of 51.4 % , compared with the 48.7 % of the   ‘ translate - train ’ baseline of Ebrahimi et al . ( 2021 ) .   In Figure 2 , we provide a more detailed overview   of average cross - lingual model performance across   a range of different reduction factors . The results   for the LT - SFT and -SFT methods gener - ally improve or stay steady as the number of train-   able task parameters increases . On the contrary ,   there is not such a trend for MAD - X , as lower   reduction factors may degrade its results . This   makes it easier to choose a good setting for this   hyper - parameter when using SFT . Moreover , it is   worth stressing again that , contrary to MAD - X ,   this hyper - parameter does not affect inference time .   BFperforms much worse than the other   methods which perform language adaptation across   all tasks . Bearing in mind the strong trend towards   increasing performance with increasing Kfor the   other SFT methods , it seems likely that BF ,   with two orders of magnitude fewer trainable pa-   rameters , lacks the capacity to learn effective task1784   and language SFTs .   For additional results at the level of individual   languages and an analysis of the efﬁcacy of lan-   guage adaptation for high- versus low- resource tar-   get languages , we refer the reader to Appendix C.   5.1 Multi - Source Training   As shown in Table 4 , multi - source LT - SFT train-   ing brings about a large improvement in zero - shot   cross - lingual transfer performance on DP , and a   modest improvement for NLI . This may be a result   of the fact that the training set for NLI contains a   relatively small number of non - English examples   compared to the DP training set . Also , the Amer-   icasNLI target languages generally have a lower   degree of genealogical relatedness to the source   languages compared to the DP target languages .   Table 3 demonstrates that multi - source training   is also beneﬁcial to zero - shot cross - lingual trans-   fer for QA on a series of relatively high - resourcelanguages . In particular , LT - SFT multi - source train-   ing of XLM - R Base outperforms single - source   full ﬁne - tuning of XLM - R Large ( a larger model )   comfortably , and outperforms XLM - R Base single-   source full ﬁne - tuning by a signiﬁcant margin . The   fact that such an improvement occurs despite each   of the 6 non - English source languages having more   than an order of magnitude less training data than   the English data from SQuAD illustrates the dispro-   portionate advantage of multilingual source data .   5.2 Beneﬁts of Sparsity   Finally , we address the following question : is spar-   sity responsible for preventing the interference of   separate ﬁne - tunings when they are composed ? To   support this hypothesis with empirical evidence ,   we use LT - SFT to train languageand task ﬁne-   tunings with different levels of density , i.e. the   percentage of non - zero values ( from 5 % to 100 % ) .   We then evaluate all possible combinations of den-   sity levels . The results are visualized in the form of   a contour plot in Figure 3 for selected combinations   of tasks and languages : Buryat , Cantonese , Erzya ,   Maltese , and Upper Sorbian for DP , and Hausa ,   Igbo , Luganda , Swahili and Wolof for NER.1785From Figure 3 , it emerges that the performance   decreases markedly for SFTs with a density level   greater than ~30 % of ﬁne - tuned parameters . We   speculate that this is due to the fact that sparser   ﬁne - tunings have a lower risk of overlapping with   each other , thus creating interference between the   different facets of knowledge they encapsulate . It   must be noted , however , that alternative hypothe-   ses could explain the performance degradation in   addition to parameter overlap , such as overﬁtting   as a result of excessive capacity . While we leave   the search for conclusive evidence to future work ,   both of these hypotheses illustrate why enforcing   sparsity in adaptation , as we propose in our method ,   is crucial to achieving modularity .   6 Related Work   Within the framework of the Lottery Ticket Hypoth-   esis , a series of improvements have been suggested   to make the original algorithm to ﬁnd winning tick-   ets ( Frankle and Carbin , 2019 ) more stable : after   ﬁne - tuning , Frankle et al . ( 2019 ) rewind the param-   eters to their values after a few iterations rather than   their values before training , whereas Renda et al .   ( 2020 ) also rewind the learning rate . In addition ,   Zhou et al . ( 2019 ) found that 1 ) different criteria   can be used to select weights as an alternative to the   magnitude of their change ; 2 ) different rewinding   methods are also effective , such as restoring the   original sign , but not the value . In future work , we   will investigate whether these variants also beneﬁt   our method for cross - lingual transfer , where the   LTH is used for adaptation rather than pruning .   Whereas the LTH was originally conceived in   the vision domain for convolutional architectures ,   it is also effective for pruning models trained on   NLP tasks ( Yu et al . , 2020 ) , such as neural machine   translation , and based on Transformer architectures   ( Prasanna et al . , 2020 ) . Recently , Xu et al . ( 2021a )   adapted the LTH speciﬁcally to prune pretrained   models after ﬁne - tuning .   To the best of our knowledge , Wortsman et al .   ( 2020 ) is the only instance where winning tickets   were composed in previous work . In their exper-   iment , a set of task - speciﬁc masks were linearly   combined at inference time , in order to generalize   to new tasks in a continuous learning setting.7 Conclusion and Future Work   We have presented a new method to ﬁne - tune pre-   trained models that is both modular ( like adapters )   and expressive ( like sparse ﬁne - tuning ) . This   method is based on a variant of the algorithm to ﬁnd   winning tickets under the framework of the Lottery   Ticket Hypothesis . We infer a sparse vector of dif-   ferences with respect to the original model for each   individual language ( by modeling unlabeled text )   and each individual task ( with supervised learning ) .   The adaptations for a language and a task can then   be composed with the pretrained model to enable   zero - shot cross - lingual transfer . Comparing our   method with the state - of - the - art baseline in several   multilingual tasks , the results have indicated sub-   stantial gains across the board in both languages   seen and unseen during pretraining ( which includes   many truly low - resource languages ) .   In future work , our method offers several po-   tential extensions . In addition to the variants to   the Lottery Ticket algorithm surveyed in § 6 , given   the importance of sparsity for modularity ( § 5.2 ) ,   we plan to experiment with additional algorithms   previously applied to pruning that can identify and   ﬁne - tune a subset of the model parameters , such   as DiffPruning ( Guo et al . , 2021 ) and ChildTun-   ing ( Xu et al . , 2021b ) . Finally , given its sim-   plicity and generality , our method is suited for   many other applications of transfer learning in ad-   dition to cross - lingual transfer , such as multimodal   learning , debiasing , and domain adaptation . The   code and models are available online at .   AcknowledgementsAlan wishes to thank David and Claudia Hard-   ing for their generous support via the Harding Dis-   tinguished Postgraduate Scholarship Programme .   Anna and Ivan are supported by the ERC PoC Grant   MultiConvAI ( no . 957356 ) and a Huawei research   donation . We would like to thank Chiara Ponti for   the graphic illustration . We also thank the anony-   mous reviewers for their helpful suggestions .   References178617871788178917901791A Algorithm of Cross - Lingual Transfer with LT - SFT   Algorithm 1 Cross - Lingual Transfer with Lottery - Ticket Sparse Fine - Tuning   function LS(D , L,,,K )       while not converged do     rL(;D )    (   1 if2argmaxj j   0 otherwise       while not converged do       rL(;D )        return   end function   function C L T ( D , D , D , L,,,K )    LS(D;L;;;K )    LS(D;L;+  ; ;K )    LS(D;L;;;K )   return+  +   end function1792B Languages1793C Results by Language1794D MAD - X Results with AdapterHub Adapters   E Parameter Overlap between Languages1795In order to understand whether similar languages   also share similar sub - networks , we plot the pair-   wise overlap ( in percentage ) between parameter   subsets of language SFTs in Figure 5 . Except for a   single instance ( Mandarin Chinese and Cantonese )   where the high overlap reﬂects the fact that both   languages are genealogically related , we ﬁnd that   the overlap is small for most language pairs . The   explanation , we believe , is two - fold . Firstly , most   of the languages in the multilingual datasets con-   sidered in our experiments belong to separate gen-   era and families . Therefore , a lack of correlation   in parameter subsets is expected . Secondly , for   a pretrained model , there exist multiple parame-   ter subsets ( “ winning tickets ” ) with comparable   performance ( Prasanna et al . , 2020 ) . The Lottery   Ticket algorithm selects randomly among these   equally valid subsets . Hence , a lack of overlap   does not necessarily imply the reliance on disjoint   sub - networks.1796