  Inbal Magar Roy Schwartz   School of Computer Science and Engineering , The Hebrew University of Jerusalem , Israel   { inbal.magar,roy.schwartz1}@mail.huji.ac.il   Abstract   Pretrained language models are typically   trained on massive web - based datasets , which   are often “ contaminated ” with downstream test   sets . It is not clear to what extent models ex-   ploit the contaminated data for downstream   tasks . We present a principled method to study   this question . We pretrain BERT models on   joint corpora of Wikipedia and labeled down-   stream datasets , and fine - tune them on the rel-   evant task . Comparing performance between   samples seen andunseen during pretraining en-   ables us to define and quantify levels of mem-   orization and exploitation . Experiments with   two models and three downstream tasks show   that exploitation exists in some cases , but in   others the models memorize the contaminated   data , but do not exploit it . We show that these   two measures are affected by different factors   such as the number of duplications of the con-   taminated data and the model size . Our results   highlight the importance of analyzing massive   web - scale datasets to verify that progress in   NLP is obtained by better language understand-   ing and not better data exploitation .   1 Introduction   Pretrained language models are getting bigger   and so does their capacity to memorize data   from the training phase ( Carlini et al . , 2021 ) . A   rising concern regarding these models is “ data   contamination”—when downstream test sets find   their way into the pretrain corpus . For instance ,   Dodge et al . ( 2021 ) examined five benchmarks and   found that all had some level of contamination in   the C4 corpus ( Raffel et al . , 2020 ) ; Brown et al .   ( 2020 ) flagged over 90 % of GPT-3 ’s downstream   datasets as contaminated . Eliminating this phe-   nomenon is challenging , as the size of the pretrain   corpora makes studying them difficult ( Kreutzer   et al . , 2022 ; Birhane et al . , 2021 ) , and even dedupli-   cation is not straightforward ( Lee et al . , 2021 ) . ItFigure 1 : We pretrain BERT on Wikipedia along with   both the labeled training and test sets ( denoted seen )   of a downstream task ( e.g. , SST ) . Then , we fine - tune   this model on the same training set for that task . We   compare performance between samples seen andunseen   during pretraining to quantify levels of memorization   and exploitation of labels seen in pretraining .   remains unclear to what extent data contamination   affects downstream task performance .   This paper proposes a principled methodology   to address this question in a controlled manner   ( Fig . 1 ) . We focus on classification tasks , where   instances appear in the pretrain corpus along with   their gold labels . We pretrain a masked language   modeling ( MLM ) model ( e.g. , BERT ; Devlin et al . ,   2019 ) on a general corpus ( e.g. , Wikipedia ) com-   bined with labeled training and test samples ( de-   noted seen test samples ) from a downstream task .   We then fine - tune the model on the same labeled   training set , and compare performance between   seen instances and unseen ones , where the latter   are unobserved in pretraining . We denote the differ-   ence between seen andunseen asexploitation . We   also define a measure of memorization by compar-   ing the MLM model ’s performance when predict-   ing the masked label for seen andunseen examples .   We study the connection between the two measures.157We apply our methodology to BERT - base and   large , and experiment with three English text clas-   sification and NLI datasets . We show that exploita-   tion exists , and is affected by various factors , such   as the number of times the model encounters the   contamination , the model size , and the amount of   Wikipedia data . Interestingly , we show that memo-   rization does not guarantee exploitation , and that   factors such as the position of the contaminated   data in the pretrain corpus and the learning rate   affect these two measures . We conclude that labels   seen during pretraining can be exploited in down-   stream tasks and urge others to continue developing   better methods to study large - scale datasets . As far   as we know , our work is the first work to study the   level of exploitation in a controlled manner .   2 Our Method : Assessing the Effect of   Contamination on Task Performance   To study the effect of data contamination on down-   stream task performance , we take a controlled ap-   proach to identify and isolate factors that affect this   phenomenon . We assume that test instances appear   in the pretrain corpus with their gold labels , and   that the labeled training data is also found in the   pretrain corpus . We describe our approach below .   We pretrain an MLM model on a general corpus   combined with a downstream task corpus , contain-   ing labeled training and test examples . We split the   test set into two , adding one part to the pretrain cor-   pus ( denoted seen ) , leaving the other unobserved   during pretraining ( unseen ) . For example , we add   the following SST-2 instance ( Socher et al . , 2013 ):   I love it ! 1   We then fine - tune the model on the same labeled   training set , and compare performance on the seen   andunseen test sets . As both test sets are drawn   randomly from the same distribution , differences   in performance indicate that the model exploits   the labeled examples observed during pretraining   ( Fig . 1 ) . This controlled manipulation allows us to   define two measures of contamination :   mem is a simple measure of explicit memoriza-   tion . We consider the MLM task of assigning thehighest probability to the gold label ( among the   candidate label set ) ; given the instance text ( e.g. , I   love it ! [ MASK ] ) .mem is defined as the dif-   ference in MLM accuracy by the pretrained model   ( before fine - tuning ) between seen andunseen .   mem is inspired by recent work on factual prob-   ing , which uses cloze - style prompts to asses the   amount of factual information a model encodes   ( Petroni et al . , 2019 ; Zhong et al . , 2021 ) . Similarly   to these works , mem can be interpreted as lower   bound on memorization of contaminated labels .   expl is a measure of exploitation : the difference   in task performance between seen andunseen .   mem andexpl are complementary measures for   the gains from data contamination ; mem is mea-   sured after pretraining , and expl after fine - tuning .   As we wish to explore different factors that influ-   enceexpl , it is also interesting to see how they af-   fectmem , particularly whether mem leads to expl   and whether expl requires mem . Interestingly , our   results indicate that these measures are not neces-   sarily tied .   Pretraining design choices Simulating language   model pretraining under an academic budget is not   an easy task . To enable direct comparisons between   different factors , we pretrain medium - sized models   ( BERT-{base , large } ) on relatively small corpora   ( up to 600 M tokens ) . We recognize that some of   the results in this paper may not generalize to larger   models , trained on more data . However , as data   contamination is a prominent problem , we believe   it is important to study its effects under lab condi-   tions . We hope to encourage other research groups   to apply our method at larger scales .   3 Which Factors Affect Exploitation ?   We study the extent to which pretrained models   can memorize and exploit labels of downstream   tasks seen during pretraining , and the factors that   affect this phenomenon . We start by examining   how many times a model should see the contami-   nated data in order to be able to exploit it .   We pretrain BERT - base on MLM using a com-   bined corpus of English Wikipedia ( 60 M tokens ) ,   and increasing numbers of SST-5 copies ( Socher   et al . , 2013 ) . To facilitate the large number of ex-   periments in this paper , we randomly downsample158   SST-5 to subsets of 1,000 training , seen andunseen   instances . We train for one epoch , due to the practi-   cal difference between the number of times the task   dataappears in the corpus and the number of times   the model sees it . For example , if a contaminated   instance appears in the corpus once , but the model   is trained for 50 epochs , then in practice the model   encounters the contaminated instance 50 times dur-   ing training . Further exploration of the difference   between these two notions is found in App . A. See   App . D for experimental details . We describe our   results below .   Exploitation grows with contaminated data du-   plicates Bothmem andexpl levels increase in   proportion to the contaminated data , reaching 60 %   mem and almost 40 % expl when it appears 200   times ( Fig . 2 , left ) . This suggests a direct connec-   tion between both mem andexpl and the number   of times the model sees these labels . This finding   is consistent with several concurrent works , which   show similar connections in GPT - based models .   These works study the impact of duplication of   training sequence on regeneration of the sequence   ( Carlini et al . , 2022 ; Kandpal et al . , 2022 ) , and the   effect on few - shot numerical reasoning ( Razeghi   et al . , 2022 ) . One explanation for this phenomenon   is the increase in the expected number of times la-   bels are masked during pretraining . To check this ,   we pretrain BERT - base with 100 copies of SST-5   and varying probabilities of masking the label . Our   results ( Fig . 2 , right ) show that the higher this prob-   ability , the higher mem andexpl values . These   results motivate works on deduplication ( Lee et al . ,   2021 ) , especially considering that casual language   models ( e.g. , GPT ; Radford et al . , 2019 ) are trained   using next token prediction objective , and so every   word in its turn is masked .   In the following , we fix the number of con-   taminated data copies to 100 and modify other   conditions — the size of the Wikipedia data and the   model size ( base / large ) . We also experiment with   two additional downstream tasks : SST-2 and SNLI   ( Bowman et al . , 2015 ) . All other experimental de-   tails remain the same . Fig . 3 shows our results .   Memorization does not guarantee exploitation   Perhaps the most interesting trend we observe is   the connection between mem andexpl . Low mem   values ( 10 % or less ) lead to no expl , but higher   mem values do not guarantee expl either . For ex-   ample , training BERT - base with 600 M Wikipedia   tokens and SST-5 data leads to 15 % mem level , but   less than 1 % expl . These results indicate the mem   alone is not a sufficient condition for expl .   Model and corpus sizes matter Across all three   datasets and almost all corpora sizes , mem levels   of BERT - large are higher then BERT - base . This   is consistent with Carlini et al . ( 2021 ) ’s findings   that larger models have larger memorization ca-   pacity . Also , we observe that mem levels ( though   not necessarily expl ) of SST-5 are consistently   higher compared to the other datasets . This might   be due to the fact that it is a harder dataset ( a 5 - label   dataset , compared to 2/3 for the other two ) , with   lower state - of - the - art results , so the model might   have weaker ability to capture other features .   Much like memorization , exploitation is also   affected by the size of the model , as well as the   amount of additional clean data . We observe   roughly the same trends for all three datasets , but   not for the two models . For BERT - base , 2–6 %   expl is found for low amounts of clean data , but159   gradually decreases . For BERT - large , the trend is   opposite : expl is observed starting 300 M and con-   tinues to grow with the amount of external data , up   to 2–4 % . This indicates that larger models benefit   more from additional data .   We next explore other factors that affect expl .   Unless stated otherwise , we use BERT - base ( 60 M   Wikipedia tokens , 100 copies of SST-5 ) .   Early contamination leads to high exploitation   Does the position of the contaminated data in the   pretraining corpus matter ? To answer this , we pre-   train the model while inserting contaminated data   in different stages of pretraining : at the beginning   ( in the first third ) , the middle , or the end . Our re-   sults ( Fig . 4 , left ) show that early contamination   leads to high expl ( up to 17 % ) , which drops as   contamination is introduced later . In contrast , the   highest mem levels appear when contamination is   inserted in the middle of the training . We also ob-   serve that in early contamination mem levels are   lower thenexpl . This is rather surprising , since   the model has certain level of memorization of the   labels ( as expressed by expl ) , but it does not fully   utilize these memories in the MLM task of mem .   This suggests that in early contamination , the lower   bound that mem yields on memorization is not tight .   The model might have an “ implicit ” memories of   the labels , which are not translated to gains in the   MLM task of predicting the gold label ( mem ) . Dis-   tinguishing between implicit and explicit memory   of LMs is an important question for future work .   We note that different stages of training also   yield different learning rates ( LRs ) . In our exper-   iments we follow BERT , using linear LR decay   with warmup . We might expect instances observed   later , with lower LR , to have a smaller affect on the   model ’s weights , thus less memorized . Fig . 4 ( left )   indeed shows that late contamination leads to no   expl ( though mem levels remain relatively high ) .   To separate the LR from the contamination timing ,   we repeat that experiment with a constant LR of   2.77e-5 ( midway of the linear decay ) . Fig . 4 ( right )   shows that in the last stage , both measures increase   compared to the LR decay policy . As the LR is con-   stant , this indicates that both LR and contamination   timing might affect label memorization .   Large batch size during pretraining reduces ex-   ploitation Similar to learning rate , the batch size   can also mediate the influence that each instance   has on the models weights . We pretrain BERT - base   several times with increasing batch sizes . Our   experiments show that as we decrease the batch   size , both measures increases ( Fig . 5 ) . In the ex-   treme case of batch size=2 , mem reaches 49 % , and   expl reaches 14 % . This phenomenon might be   explained by each training instance having a larger   impact on the gradient updates with small batches .   A good initialization matters Carlini et al .   ( 2019 ) showed that memorization highly depends   on the choice of hyperparameters . We observe a   similar trend — expl depends on the random seed   used during fine - tuning . These results are also   consistent with prior work that showed that fine-   tuning performance is sensitive to the selection of   the random seed ( Dodge et al . , 2020 ) . Careful in-   vestigation reveals that some random seeds lead   to good generalization , as observed by unseen per-   formance , while others lead to high exploitation :   When considering the top three seeds ( averaged   across experiments ) for expl — two out of those   seeds are also in the worst three seeds for general-   ization . This indicates a tradeoff between general-   ization and exploitation . Future work will further160study the connection between these concepts . To   support such research , we publicly release our ex-   perimental results .   4 Related Work   Memorization in language models has been ex-   tensively studied , but there is far less research on   data contamination and the extent models exploit   the contamination for downstream tasks . Most re-   lated to our work is Brown et al . ( 2020 ) ’s post - hoc   analysis of GPT-3 ’s contamination . They showed   that in some cases there was great difference in   performance between ‘ clean ’ and ‘ contaminated ’   datasets , while in others negligible . However , they   could not perform a controlled experiment due to   the high costs of training their models . As far as   we know , our work is the first work to study the   level of exploitation in a controlled manner .   Several concurrent works explored related ques-   tions on memorization or utilization of training in-   stances . These works mostly use GPT - based mod-   els . Carlini et al . ( 2022 ) showed that memorization   of language models grows with model size , training   data duplicates , and the prompt length . They fur-   ther found that masked language models memorize   an order of magnitude less data compared to causal   language model . This finding hints that exploita-   tion levels might be even higher on the latter . Kand-   pal et al . ( 2022 ) showed that success of privacy at-   tacks on large language models ( as the one used in   Carlini et al . , 2021 ) is largely due to duplication in   commonly used web - scraped training sets . Specif-   ically , they found that the rate at which language   models regenerate training sequences is superlin-   early related to a duplication of the sequence in the   corpus . Lastly , Razeghi et al . ( 2022 ) examined the   correlations between model performance on test   instances and the frequency of terms from those in-   stances in the pretraining data . They experimented   with numerical deduction tasks and showed that   models are consistently more accurate on instances   whose terms are more prevalent .   5 Discussion and Conclusion   We presented a method for studying the extent   to which data contamination affects downstream   fine - tuning performance . Our method allows to   quantify the explicit memorization of labels fromthe pretraining phase and their exploitation in fine-   tuning . Recent years have seen improvements in   prompt - based methods for zero- and few - shot learn-   ing ( Shin et al . , 2020 ; Schick and Schütze , 2021 ;   Gu et al . , 2021 ) . These works argue that masked   language models have an inherent capability to per-   form classification tasks by reformulating them as   fill - in - the - blanks problems . We have shown that   given that the language model has seen the gold   label , it is able to memorize and retrieve that label   under some conditions . Prompt - tuning methods ,   which learn discrete prompts ( Shin et al . , 2020 ) or   continuous ones ( Zhong et al . , 2021 ) , might latch   on to the memorized labels , and further amplify   this phenomenon . This further highlights the im-   portance of quantifying and mitigating data con-   tamination .   Acknowledgements   We wish to thank Yarden Shoham Tal , Michael   Hassid , Yuval Reif , Deborah Elharar , Gabriel   Stanovsky and Jesse Dodge for their feedback and   insightful discussions . We also thank the anony-   mous reviewers for their valuable comments . This   work was supported in part by the Israel Science   Foundation ( grant no . 2045/21 ) and by a research   gift from the Allen Institute for AI .   References161162   A Two Notions of “ Occurences ”   As noted in Sec . 3 , the number of times an instance   appears in the corpus is a different notion than the   number of times the model sees it during training .   The latter also takes into account the number of   training epochs . For example , if an instance ap-   pears in the corpus once , but the model is trained   for 50 epochs , than practically the model sees it 50   times . In the field on memorization and data con-   tamination , it is mostly common to report the num-   ber of times an instance appears in the corpus ( Car-   lini et al . , 2021 ; Brown et al . , 2020 ) . However , the   following experiments emphasizes the importance   of accounting for the number of times a sample is   seen . In the first experiment we fix the number of   times the contamination appears in the corpus ( 10   copies ) , and change the number of times it is seen .   We do so by performing second - stage - pretraining   ( Gururangan et al . , 2020 ; Zhang and Hashimoto ,   2021 ) on a combined corpus of Wikipedia and 10   copies of SST-5 . We train one model for one epoch ,   and the other for 5 epochs . Results are shown in   Tab . 1 . In the second experiment we fix the number   of times the model seesSST-5 , and change the num-   ber of times it appears in the corpus . We do so by   performing second - stage - pretraining for one epoch   on a combined corpus of Wikipedia and changing   number of copies of SST-5 . Results are shown in   Tab . 2 .   We observe that expl levels of the models   which saw the contamination 50 times are rather   similar . On the contrary , expl levels of the model   which saw the data 10 times is 5 % lower . These   results indicate the number of times contamina-   tion is seen during training have great influence on   expl . In the main experiments presented in this   paper we train for one epoch in order to eliminate   the difference between the two notion ( appears vs.   seen ) .   B Same Ratio , Different expl   In Sec . 3 we have seen the expl andmem grows   with the number of contamination occurrences in   the corpus . One explanation for the results in is that   the rising ratio between the contaminated corpus   and the full corpus leads to increased mem . We163conduct experiments in which we keep the ratio   between the two fixed while increasing their abso-   lute sizes . We keep constant ratio of 1:10 between   the number of instances ( in Wikipedia set we con-   sider lines as instances ) in the datasets . To do so ,   we adjust both the size of Wikipedia and the du-   plications of SST-5 train and seen test sets in the   corpus . For example , to achieve total corpus sized   1 M we use 9k instances from Wikipedia and 50   copies of SST-5 ( which yields 1k samples ) . We   focus on BERT - base and SST-5 task and follow   the basic experiment setup and hyperparameters of   our main experiments ( Sec . 3 ) . Our results ( Fig . 7 )   show that this manipulation leads to increased mem ,   indicating the importance of the total number of   occurrences of the task data .   C Position of Contamination Matters   We pretrain BERT - base model while inserting con-   taminated data in different stages of pretraining .   We discuss the experiment in Sec . 3 . Results on   SST-2 and SNLI can be found in Fig . 6 .   D Experimental Details   Originally , BERT model was trained on Masked   Language Modelling ( MLM ) task and Next Sen-   tence Prediction task ( NSP ; Devlin et al . , 2019 ) .   However , Liu et al . ( 2019 ) showed that removing   the NSP loss does not impact the downstream task   performance substantially . Therefore we pretrain   both BERT models ( -base and -large , both uncased )   on the MLM task only .   Wikipedia Data We extracted and pre-   processed the April 21 ’ English Wikipedia dump .   We used the wikiextractor tool ( Attardi , 2015 ) . In   order to measure the effect of contamination when   contaminated data is shuffled across the pretraining   corpus , we divided clean Wikipedia text into lines   ( instances which were originally separated by new   line symbol ) .   Experimental Details for Sec . 3 All models   were trained with the following standard proce-   dure and hyperparameters . Specific experimental   adjustments will be discussed later . We pretrained   BERT models using huggingface ’s ( Wolf et al . ,   2020 ) run_mlm script for masked language model-   ing . We used heads sized 64 ( calculated as : hidden   dimension divided by the number of heads ) with   standard architecture as implemented in transform-   ers library . We used a combined corpus of 60 M   tokens of Wikipedia along with 100 copies of the   downstream corpus . Due to computational limi-   tations , we limited the training sequences to 128   tokens . We pretrained for 1 epoch and used batch   size of 32 to fit on 1 GPU . We trained with a learn-   ing rate of 5e-5 . We apply linear learning rate   warm up for the first 10 % steps of pretraining and   linear learning rate decay for the rest . We fine - tune   the models on 1,000 samples of the downstream   corpora ( SST-2 , SST-5 and SNLI ) .   We fine - tune for 3 epochs using batch size of   8 . We use AdamW ( Loshchilov and Hutter , 2019 )   optimizer with learning rate of 2e-5 and default pa-   rameters : β= 0.9 , β= 0.999 , ϵ= 1e-6 , with bias   correction and without weight decay . We average   the results over ten random trials . As baselines we   use pretrained BERT - base and BERT - large and fine-   tune them as described above . Accuracy results on164theunseen test sets are shown in Tab . 3 .   In the experiment of contamination in different   stages of training , we divided the entire corpus   ( clean and contaminated ) into 3 equal size sections ,   making sure that all the contaminated data appears   entirely in one of those sections . We disabled the   random sampler and shuffled each section individu-   ally . We refer to the sections as ‘ first ’ , ‘ middle ’ and   ‘ last ’ according to the order they appear in train-   ing . All our experiments were conducted using the   following GPUs : RTX 2080Ti , Quadro RTX 6000 ,   A10 and A5000 .   Experimental Details for App . A We conducted   second - stage - pretraining by continuing to update   BERT - base weights . We used batch size of 32 and   learning rate of 5e-5 . Learning rate scheduling , op-   timization and fine - tuning are the same as standard   procedure described above.165