  Daking Rai , Bailin Wang , Yilun Zhou , Ziyu YaoGeorge Mason University , MIT{drai2 , ziyuyao}@gmu.edu , { bailinw , yilun}@mit.edu   Abstract   Compositional and domain generalization   present significant challenges in semantic pars-   ing , even for state - of - the - art semantic parsers   based on pre - trained language models ( LMs ) .   In this study , we empirically investigate im-   proving an LM ’s generalization in semantic   parsing with two simple techniques : at the to-   kenlevel , we introduce a token preprocessing   method to preserve the semantic boundaries   of tokens produced by LM tokenizers ; at the   sequence level , we propose to use special to-   kens to mark the boundaries of components   aligned between input and output . Our exper-   imental results on two text - to - SQL semantic   parsing datasets show that our token prepro-   cessing , although simple , can substantially im-   prove the LM performance on both types of   generalization , and our component boundary   marking method is particularly helpful for com-   positional generalization .   1 Introduction   Pre - trained language models ( LMs)such as T5   ( Raffel et al . , 2020 ) have now been more and more   widely adopted for semantic parsing due to their   promising performance and straightforward archi-   tectures ( Shaw et al . , 2021 ; Scholak et al . , 2021 ;   Yin et al . , 2021 ; Qi et al . , 2022 ; Xie et al . , 2022 ;   Qiu et al . , 2021 ) . However , recent work revealed   that these LMs still struggle to generalize on out-   of - distribution ( OOD ) samples ( Lake and Baroni ,   2018 ; Keysers et al . , 2019 ; Shaw et al . , 2021 ; Qiu   et al . , 2022b ) . For example , if a parser has learned   “ how many heads are in the department ” and “ how   many people are older than 56 ” , it is expected to   generalize to “ how many heads of the departments   Table 1 : Our proposed techniques . Top : we preprocess   the text such that its T5 tokenization aligns with word   semantics . Coloring indicates tokenization ; for example ,   “ avg ” is converted into three tokens of “ a ” , “ v ” and “ g ” .   Bottom : we add separator tokens to mark the boundaries   of aligned semantic components in the input and output .   are older than 56 ” . Generalizing to such novel com-   ponent compositions is known as compositional   generalization . Additionally , generalizing to new   domains ( e.g. , from “ entertainment ” to “ flight ” ) is   referred to as domain generalization .   In this paper , we investigate these two types   of generalization of LMs in text - to - SQL seman-   tic parsing , i.e. , given a natural language ( NL ) in-   put and the database schema , producing a SQL   query that can be executed against the database   for desired output . We conduct experiments us-   ing the cross - database Spider benchmark ( Yu et al . ,   2018b ) and its derivation Spider - CG ( Gan et al . ,   2022 ) . Compared with existing benchmarks ( Key-   sers et al . , 2019 ; Lake and Baroni , 2018 ) , this task   setting is both more realistic ( e.g. , containing larger   language variations ) and more challenging ( e.g. , re-   quiring grounding to the database context).150Although previous work tackling the two types   of generalization all requires non - trivial engineer-   ing effort ( see Section 2 ) , in this work , we present   two simple yet effective techniques , which are ex-   tremely easy to implement with LMs ( Table 1 ) .   Our techniques improve the generalization of LMs   by preserving the semantic boundaries at the token   and the sequence levels . At the token level , our   first technique rewrites the inputs to handle naming   conventions in database schemas and SQL queries   such that a pre - trained LM tokenizer can split them   into semantically meaningful tokens . At the se-   quence level , our second technique introduces spe-   cial tokens to mark the semantic boundaries ( e.g. ,   phrases ) aligned between the source NL and the tar-   get SQL . These special tokens implicitly help the   LM - based parser build more precise input - output   correspondences that are crucial for compositional   generalization .   On five evaluation sets , the experimental results   based on T5 - base show that , albeit simple , our   token - level technique dramatically improves both   types of LM generalization , and our sequence - level   technique is particularly helpful for compositional   generalization . Combining them together leads to   further improvements . Our additional experiments   further demonstrate the generalizability of our ap-   proaches ( e.g. , to text - to - LISP expression parsing   ( Semantic Machines et al . , 2020 ) ) .   2 Related Work   Text - to - SQL Semantic Parsing . This task has   received considerate attention since the creation of   the WikiSQL ( Zhong et al . , 2017 ) and Spider ( Yu   et al . , 2018b ) datasets . While a large amount of   existing work designed specialized architectures   for this task ( Yu et al . , 2018a ; Zhang et al . , 2019 ;   Wang et al . , 2020 ; Lin et al . , 2020 ) , there has been   a trend of directly fine - tuning pre - trained sequence-   to - sequence models as semantic parsers ( Shaw   et al . , 2021 ; Scholak et al . , 2021 ; Xie et al . , 2022 ;   Qi et al . , 2022 ) . Our work follows the same line and   proposed approaches to further improve the LM   performance . On the other hand , Guo et al . ( 2019 ) ;   Gan et al . ( 2021 ) ; Herzig et al . ( 2021 ) showed that   simplifying the SQL representation in a way that   the new representation can semantically better align   with the NL can dramatically improve the parsing   performance . In our work , we follow the NatSQL   representation ( Gan et al . , 2021 ) as it has better   alignments with the NL.Injecting Priors into Semantic Parsers . Our two   techniques can be viewed as injecting human prior   knowledge into neural models for better general-   ization , which has been one of the major research   efforts on improving domain and compositional   generalization . The key consideration to be taken   when injecting priors is the trade - off between the   form and the generalizability . Strong priors in   the form of specialized model architectures ( Shaw   et al . , 2021 ; Herzig and Berant , 2021 ; Wang et al . ,   2021 ) are either too expensive or not applicable   across domains . Weaker priors in terms of special-   ized training algorithms ( Yin et al . , 2021 ; Conklin   et al . , 2021 ) are more general , but often weaker in   performance compared to other lines of methods .   Our work is in the spirit of the third line on the   use of data augmentation ( Andreas , 2020 ; Akyürek   et al . , 2020 ; Qiu et al . , 2022a ) . However , instead of   synthesizing new data from scratch , we “ annotate ”   the data with semantic boundary markers , which is   not only much simpler but also brings better perfor-   mance . The final line of work ( Qiu et al . , 2022b ;   Levy et al . , 2022 ) is based on the learning capaci-   ties in the context of large LMs , which is out of the   scope of this work .   3 Methods   3.1 Token Preprocessing   We present our two techniques for improving   the generalization of LM - based semantic parsers .   LM pre - training learns high - quality contextualized   word representation ( Devlin et al . , 2019 ) , but to ef-   fectively use it on a downstream task , the tokeniza-   tion needs to “ make sense . ” For example , if the text   “ pet_age ” is tokenized as “ pet ” , “ _ ” and “ age ” , then   the semantics of “ pet ” and “ age ” acquired during   pretraining can be directly used . However , if it is151   tokenized as “ pe ” , “ t_a ” and “ ge ” , then pre - training   is hardly useful because the model does not even   recognize the two semantic words .   Unfortunately , this latter case is very common   when tokenizing non - natural language texts , such   as database schemas and SQL queries . Thus , we   propose a token preprocessing method to induce   more natural tokenization by , at a high level , adding   white spaces and handling the naming conventions   in database schema and SQL queries . We show   examples in Table 2 and details in Appendix A.   3.2 Component Boundary Marking   At the sequence level , our second technique further   assists LMs in recognizing the semantic boundaries   of components aligned between input and output .   An example is shown in Table 1 . While prior work   has attempted the goal via implementing alignment-   based attention supervision ( Yin et al . , 2021 ) , we   propose to insert special tokens in input and out-   put to inject such bias . Specifically , we use pairs   of “ [ sepN ] ” and “ [ /sep N]”,N∈Z , to mark   the boundaries , so as to hint the LM that compo-   nents within the paired special tokens should be   aligned . In practice , we also observed cases where   an NL component has to be aligned with a SQL   component consisting of multiple non - continuous   segments . To handle it , we will apply the same   pair of special tokens to each segment of the same   component . An example is shown in Table 8 in the   Appendix .   Finally , we note that our method assumes the   availability of component annotations . Such anno-   tations can be obtained via human labeling ( Gan   et al . , 2021 ) , heuristic rules ( Yin et al . , 2021 ) , or   other advanced machine learning algorithms , but   this is beyond the scope of our work .   4 Experiments   4.1 Setup   Datasets . We use two datasets , Spider ( Yu et al . ,   2018b ) and Spider - CG ( Gan et al . , 2022 ) . Spiderconsists of a training set ( Spider ) and a develop-   ment set ( Spider ) with non - overlapping domains   but otherwise similar data characteristics ( e.g. ,   length ) . Thus , we train the models on Spider , and   consider Spideras the evaluation for domain gen-   eralization . Spider - CG is derived from Spider by   first dissecting each Spider instance into different   components according to its dependency parse and   generates data in two ways : substituting a compo-   nent in one instance with one from another instance   and appending one component from one instance   to another instance . Depending on whether the   instances come from the Spider training or devel-   opment set , we get four splits : CG - SUB , CG-   SUB , CG - APPand CG - APP , all of which are   only used for evaluation . The instances created   under substitution share similar data characteristics   while those under appending are much longer , so   a good model performance on the latter requires   compositional generalization . Table 3 summarizes   the dataset information . In addition , we use the   NatSQL representation ( Gan et al . , 2021 ) through-   out the experiment due to its better alignment with   the NL input .   Evaluation Metrics . We follow the standard Spi-   der benchmarking and employ two evaluation met-   rics . Exact Match ( EM ) compares the generated   and the ground - truth query by performing exact   set matching at the lexical level ( Yu et al . , 2018b ) .   Execution Match ( EX ) measures whether execut-   ing the generated query on the given database can   yield the same results as using the ground truth .   Notably , for a fair comparison with existing seman-   tic parsers on the Spider leader board , we follow   Gan et al . ( 2022 ) , convert each generated NatSQL   query into a SQL query , and report the evaluation   results based on the converted SQL query .   Models , Baselines , and Implementation . We   evaluate our proposed techniques by applying them   to the pre - trained T5 model ( Raffel et al . , 2020 ) .   Our experiments are conducted using T5 - base , with   the use of database contents following Lin et al .   ( 2020 ) . As our second technique leverages com-   ponent boundary labels to encourage the composi-   tional generalization of LM , we compare it with a   baseline ( Yin et al . , 2021 ) which similarly assumes   the labels but utilizes them in a more complicated   way , i.e. , transforming the component alignments   into supervision on the cross attention between   input and output of the LM . We denote this base-152   line as Attn . Sup .For both methods , we lever-   age component annotations from Spider - SS ( Gan   et al . , 2022 ) . These annotations were generated by   applying a syntactic parser to decompose the NL   question into sub - questions and then manually an-   notating their corresponding NatSQL components .   We also compare with the state - of - the - art models ,   RATSQLand RATSQL , from Gan et al .   ( 2022 ) , although their models adopt a specialized   architecture ( i.e. , RATSQL ( Wang et al . , 2020 ) ) and   RATSQLadditionally employed task - specific   pre - training ( Shi et al . , 2021 ) . Both models used   the same component annotations from Spider - SS .   Finally , for each of our model variants in Ta-   ble 4 , we repeat the experiment three times , using   three random seeds consistently across all models ,   and report the average results . We include more   implementation details in Appendix D.   4.2 Results   Main Results . We present our results in Table   4 . First , all models obtain the best performance on   the in - distribution evaluation set CG - SUBwhile   suffering from more than 10 % performance drops   on others , confirming the challenges of the domain   and compositional generation . As expected , all   models have the worst performance on CG - APP ,   which requires both types of generalization . Be-   tween the two types , it is also observed that compo-   sitional generalization ( as measured by CG - APP)is more challenging than domain generalization ( as   measured by Spiderand CG - SUB ) .   Second , our results show that the token prepro-   cessing method , albeit simple , can improve both   domain and compositional generalizations of LMs   dramatically . For example , comparing T5 - base   with T5 - base+Tok , the latter is improved by around   5 - 7 % EM and 7 % EX for domain generalization   ( on Spiderand CG - SUB ) , 5 % EM and 3.5 % EX   for compositional generalization ( on CG - SUB ) ,   and 9 % EM and 11 % EX for the challenging case   when both types occur ( on CG - APP ) . Addition-   ally , we also show the effectiveness of token pre-   processing with T5 - 3B on Spiderin App . B.   Moving on to our proposed component boundary   marking method , it shows to be particularly help-   ful for compositional generalization . Specifically ,   applying it to T5 - base leads to a 9 % EM and 7 %   EX increase on CG - APP , and an 8 % EM and 8 %   EX increase on CG - APP . On the in - distribution   evaluation set , this technique also gives slight im-   provement , whereas , for domain generalization ,   there is no obvious impact from this technique .   Finally , augmenting T5 - base with both tech-   niques ( i.e. , T5 - base+Tok+Comp ) leads to better   performance than applying each technique individ-   ually in most evaluation sets , implying that our   two techniques are complementary to each other .   Specifically , for in - distribution evaluation , using   each technique individually or both of them to-   gether yield similar results ; for domain general-   ization , there is no additional gain from applying   component boundary marking on the top of the   token preprocessing ; for compositional generaliza-153tion , the two techniques together contribute the best   EM across all models and baselines . Overall , com-   bining the two techniques shrinks the performance   gap between in - distribution and domain OOD by   around 2 - 4 % EM , composition OOD by 7 % , and   joint OOD by 13 % .   Compared with Special Architectures . De-   spite its simplicity , our T5 - base+Tok+Comp model   achieves comparable or better performance than the   two RATSQL variants on CG - SUB . It also per-   forms comparably to RATSQLon CG - APP .   Compared with Attn . Sup . Surprisingly , the at-   tention supervision has only led to around 2 % EM   and 1.5 % EX gains on CG - APP , while no further   advantage is observed on other evaluation sets . In   our conjecture , this is due to the misalignment be-   tween the objective of Attn . Sup ( Yin et al . , 2021 )   and the attention mechanism of pre - trained LMs .   Specifically , Attn . Sup encourages the attention   distribution of different heads to be consistent with   the component alignment supervision . However ,   prior work ( V oita et al . , 2019 ) suggests that differ-   ent attention heads of even the same layer may have   different functions and roles . Thus , when coarsely   defining the objective function , it may not allow for   the most effective supervision . Furthermore , simi-   lar to our finding , Yin et al . ( 2021 ) did not observe   performance gain when they applied Attn . Sup to   T5 - base on CFQ ( Keysers et al . , 2020 ) .   Qualitative Analysis on Tokenization . To qual-   itatively understand how our token preprocessing   helps the generalization , we randomly sampled 50   examples from the Spiderto analyze how fre-   quently the T5 tokenizer divides tokens into less   meaningful subtokens . Consequently , we found   243 tokenization issues in total , and 140 of them   can be resolved by our token preprocessing . The   remaining cases are like splitting “ i d ” into “ i ” and   “ d ” as shown in Table 1 , which is beyond our scope .   Error Analysis on Component Boundary Mark-   ing . We manually examined 50 error predictions   from T5 - base+Tok+Comp and contrasted them   with the errors of T5 - base+Tok . Intriguingly , we   observed much more frequent schema items or   value hallucinations from the former . For exam-   ple , it may generate queries accessing non - existing   columns in a table , or misspells the literal values   in the queries . We conjecture that this is because   our component boundaries are only applied to the   NL input , not the database schema ( note that literal   values are grounded and attached to schema items   in their input representations ; see Appendix D for   details ) . This reveals a new challenge of LM gen-   eralization in text - to - SQL semantic parsing , i.e. ,   how to properly handle the database schema when   injecting prior knowledge into LMs for composi-   tional generalization .   Generalizing to Other Semantic Parsing Tasks .   While our main focus in this work is on text - to-   SQL parsing , we also investigate whether our ap-   proaches can generalize beyond this specific task .   To this end , we implemented both of our techniques   to SMCalFlow - CS ( Yin et al . , 2021 ) , a composi-   tional generalization dataset for text - to - LISP ex-   pression parsing ( Semantic Machines et al . , 2020 ) .   For “ + Comp ” , We utilize the span - level alignments   heuristically derived by Yin et al . ( 2021 ) as com-   ponent annotations . Our results in Table 5 show   that : ( 1 ) Our token preprocessing can be univer-   sally helpful for LMs to model schema items , pred-   icates , etc . , leading to 1.2 % performance gain over   T5 - base ; ( 2 ) Our component boundary marking   method is highly effective for compositional gener-   alization , which offers 2.6 % additional gain .   5 Conclusion   In this paper , we present two simple yet effective   techniques to improve the domain and composi-   tional generalization of LMs in text - to - SQL seman-   tic parsing . Our techniques aid LMs in preserving   the semantic boundaries of tokens and components   in their input and output . We also demonstrate   their potential to be generalized to other semantic   parsing tasks.154Limitations   Future work can further apply our approaches   to other semantic parsing tasks . For example ,   for parsing texts to lambda - calculus expressions   for knowledge base question answering ( Dong   and Lapata , 2016 ) , one can similarly preprocess   the schema items ( e.g. , “ department_time ” into   “ department _ time ” ) and typed values ( e.g. ,   “ dallas : ci ” into “ dallas : ci ” ) for more mean-   ingful subword tokenization results . In addition ,   our experiments are based on T5 . To further verify   the effectiveness of our techniques , one can apply   them to other pre - trained language models such as   BART ( Lewis et al . , 2020 ) and GPT-2 ( Radford   et al . , 2019 ) as well .   Acknowledgments   We would like to thank all anonymous reviewers   for their constructive comments . We also thank Yu-   jian Gan and Xinyun Chen for their help in using   the NatSQL and the Spider - SS datasets , as well as   Pengcheng Yin for using the code base of Attn . Sup .   This project was supported by resources provided   by the Office of Research Computing at George   Mason University ( https://orc.gmu.edu ) and   funded in part by grants from the National Sci-   ence Foundation ( Awards Number 1625039 and   2018631 ) .   References155156   A Token Preprocessing Details   We propose a simple token preprocessing method .   Instead of directly feeding the input to the subword   tokenizer , we introduce three preprocessing steps :   ( 1 ) For schema items in input and output , reversing   the snake case to the normal , e.g. , “ pet_age ” to   “ pet _ age ” ; ( 2 ) For any call of “ Table . Column ” ,   splitting the tokens around the access operator “ . ”   ( i.e. , “ Table . Column ” ) ; and ( 3 ) Replacing any   reserved words that can not be properly handled   in NatSQL , e.g. , “ avg ” to “ average ” . In practice ,   we also handle formalism - specific special tokens ,   e.g. , adding the “ less than ” operator “ < ” to the   vocabulary of T5 tokenizer . While we showcase   our token preprocessing under text - to - SQL parsing ,   the intuition can be generalized to other formalisms   ( e.g. , regex , λ - expression ) easily .   In addition , we also check the issue of tokeniza-   tion in other popular LM tokenizers and found that   the tokenization issue is not specific to T5 . Exam-   ples of bad tokenization from BERT ( Devlin et al . ,   2019 ) and GPT2 ( Radford et al . , 2019 ) tokeniz-   ers and after our token preprocessing are listed in   Table 6 .   B T5 - 3B Experiment   To assess the effectiveness of our token preprocess-   ing technique with larger LMs , we apply it to T5-   3B and evaluate the model on Spider . The results157   are shown in Table 7 . Our results show that T5-   3B+Tok has a performance gain of 1.1 % , indicating   that it is helpful for larger LMs as well . Addition-   ally , we also provide results with and without using   DeepSpeed ( 2023 ) , a deep learning optimization   library that is used to train large models more effi-   ciently . Surprisingly , although DeepSpeed ( 2023 )   helped us improve training speed , we found a per-   formance drop of around 2.1 - 2.2 % EX while using   it . However , our token preprocessing consistently   leads to around 1.0 % absolute performance gain .   CComponent Boundary Marking Details   In Table 8 , we present one more example of com-   ponent boundary marking . In this example , the NL   component “ What is the most populace city ” is   aligned with two non - continuous SQL segments ,   “ select city . Name , city . Population ” and   “ order by city . Population desc limit 1 ” .   To handle such cases , we apply the same pair of   special tokens “ [ sep0 ] ” “ [ /sep0 ] ” twice , one   for each segment .   D Implementation Details   Our experiments are conducted based on the pre-   trained T5 model . The input to T5 follows the sameformat and order as Scholak et al . ( 2021 ) ( except   our additional token preprocessing , if applied ) , i.e. ,   “ Question | Database 1 | Table 1 : Column 1 ,   Column 2, ... | Table 2 : Column 1 , Column   2 ... ” . We also use the database contents as parts   of the input , following Lin et al . ( 2020 ) . For ex-   ample , if the NL question mentions a literal value   ( e.g. , “ New York ” ) , the appearance of whom can be   found in the contents of a certain “ Column 1 ” via   fuzzy string matching , then when we represent the   database schema , we will include it via “ Database   1 | Table 1 : Column 1 ( New York ) , Column   2 , ... ” .   We fine - tune the T5 - base LM that consists of   220 million parameters on NVIDIA A100 GPU for   10 - 12 hours . It was trained with a learning rate of   10and batch size 16 for T5 - base for a maximum   of 20 K training steps . The model is evaluated on   Spiderfor every 1 K training steps , and the best   checkpoint is selected based on the model EM on   Spider . In inference time , we perform simple   greedy decoding .   We use the PyTorch - Transformers library ( Wolf   et al . , 2020 ) , which is a library for state - of - the-   art pre - trained models for NLP , to fine - tune our   models . Specifically , our code for fine - tuning T5-   base is adapted from PICARD ’s implementation   ( Scholak et al . , 2021 ) . Furthermore , we also use   DeepSpeed ( 2023 ) to fine - tune all of our T5 - base   models .   Datasets . We used Spider ( Yu et al . , 2018b ) , Nat-   SQL ( Gan et al . , 2021 ) , Spider - CG ( Gan et al . ,   2022 ) , and SMCalFlow - CS ( Yin et al . , 2021 ) in   our work . They are under the license of CC BY - SA   4.0 . Our use of these datasets is consistent with   their intended use , i.e. , for scientific research . All   datasets are in English . They contain annotated NL   and SQL or NatSQL or LISP expression pairs from   the open domain.158ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations   /squareA2 . Did you discuss any potential risks of your work ?   We do n’t see the potential of how our two techniques can be misused .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   4   /squareB1 . Did you cite the creators of artifacts you used ?   2 , 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   B   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   B   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Sensitive contents are unlikely to be contained in the datasets we used . For example , for Spider - CG ,   it is annotated by domain experts .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   B   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   4.1   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   B159 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   4.1   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   4.1   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   4.1   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.160