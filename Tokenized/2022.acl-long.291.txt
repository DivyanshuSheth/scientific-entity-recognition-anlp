  Wenxuan Shi , Fei Li , Jingye Li , Hao Fei , Donghong Ji   Key Laboratory of Aerospace Information Security and Trusted Computing , Ministry of   Education , School of Cyber Science and Engineering , Wuhan University , Wuhan , China   { shiwenxuan,lifei_csnlp,theodorelee,hao.fei,dhji}@whu.edu.cn   Abstract   The state - of - the - art model for structured sen-   timent analysis casts the task as a dependency   parsing problem , which has some limitations :   ( 1 ) The label proportions for span prediction   and span relation prediction are imbalanced .   ( 2 ) The span lengths of sentiment tuple compo-   nents may be very large in this task , which will   further exacerbates the imbalance problem . ( 3 )   Two nodes in a dependency graph can not have   multiple arcs , therefore some overlapped senti-   ment tuples can not be recognized . In this work ,   we propose nichetargeting solutions for these   issues . First , we introduce a novel labeling   strategy , which contains two sets of token pair   labels , namely essential label set and whole   label set . The essential label set consists of   the basic labels for this task , which are rela-   tively balanced and applied in the prediction   layer . The whole label set includes rich labels   to help our model capture various token rela-   tions , which are applied in the hidden layer   to softly influence our model . Moreover , we   also propose an effective model to well col-   laborate with our labeling strategy , which is   equipped with the graph attention networks to   iteratively refine token representations , and the   adaptive multi - label classifier to dynamically   predict multiple relations between token pairs .   We perform extensive experiments on 5 bench-   mark datasets in four languages . Experimental   results show that our model outperforms previ-   ous SOTA models by a large margin .   1 Introduction   Structured Sentiment Analysis ( SSA ) , which aims   to predict a structured sentiment graph as shown in   Figure 1(a ) , can be formulated into the problem of   tuple extraction , where a tuple ( h , e , t , p ) denotes   a holder hwho expressed an expression etowards   a target twith a polarity p. SSA is a more chal-   lenging task , because other related tasks only focusFigure 1 : ( a)An example of structured sentiment analy-   sis.(b)The head - first parsing graph proposed by Barnes   et al . ( 2021 ) , where the arcs related to holder(target)-   expression linking relations are bold . ( c)Our proposed   essential label set , which has more balanced label dis-   tribution for holder , target or expression span prediction   and their linking relation prediction .   on extracting part of tuple components or the text   spans of the components are short . For example ,   Opinion Role Labeling ( Katiyar and Cardie , 2016 ;   Xia et al . , 2021 ) does not include the extraction   of sentiment polarities , and Aspect - Based Senti-   ment Analysis ( ABSA ) ( Pontiki et al . , 2014 ; Wang   et al . , 2016 ) extracts the aspect and opinion terms   typically consisting of one or two words . The state-   of - the - art SSA model is proposed by Barnes et al .   ( 2021 ) , which casts the SSA task as the dependency   parsing problem and predicts all tuple components   as a dependency graph ( Figure 1(b ) ) .   However , their method exists some shortages .   Taking Figure 1(b ) as example , only 2 arcs ( e.g. ,   expressed →import and expressed →Moscow ) are   related to span linking relation prediction ( i.e. ,   the relations between expressions and holders   or targets ) , while much more other arcs are re-   lated to span prediction ( e.g. , import →the and   import →meat ) . Such imbalanced labeling strat-4232   egy will make the model pay more attention on   span prediction but less on span relation prediction .   Furthermore , since the span lengths of sentiment tu-   ple components may be very large in the SSA task ,   the label imbalanced problem will become more se-   vere . Besides , the dependency parsing graph is not   able to deal with multi - label classification , since   it does not allow multiple arcs to share the same   head and dependent tokens . Therefore , some over-   lapped sentiment tuples can not be recognized . The   statistics of span length and multi - label problems   are listed in Table 1 .   To alleviate the label imbalance problem in   Barnes et al . ( 2021 ) , we propose a novel labeling   strategy that consists of two parts : First , we design   a set of labels called essential label set ( Figure   1(c ) ) , which can be considered as the basic label set   for decoding SSA tuples , since it only includes the   labels to tag the boundary tokens of spans . As seen ,   the proportion of span prediction labels and span   relation prediction labels are relatively balanced , so   that we can mitigate the label imbalance problem   and meanwhile keep the basic ability of extracting   sentiment tuples if the essential label set is learnt   in the final prediction layer of our model .   However , the labels related to recognize non-   boundary tokens of SSA components are also im-   portant . For instance , they can encode the relations   between the tokens inside the spans , which may   benefit the extraction of the holders , expressions or   targets with long text spans . To this end , we designanother label set called whole label set ( Figure 2 ) ,   which includes richer labels to fully utilize various   information such as the relations among boundary   tokens , non - boundary tokens , the tokens within a   span , the tokens across different spans . Moreover ,   since the dependency - based method ( Barnes et al . ,   2021 ) only considers the local relation between   each pair of tokens , we add the labels between   [ CLS ] and other tokens related to sentiment tuples   into our whole label set , in order to utilize sentence-   level global information . Considering that if the   whole label set is directly applied on the output   label for training , the label imbalance problem may   occur again . We instead employ the whole label   set in a soft and implicit fashion by applying it on   the hidden layer of our model .   To well collaborate with our labeling strat-   egy , we also propose an effective token graph   model , namely TGLS ( Token Graph with a novel   Labeling Strategy ) , which uses rich features such   as word , part - of - speech tags and characters as in-   puts and yields contextualized word representations   by BiLSTM and multilingual BERT(Devlin et al . ,   2018 ) . In the hidden layer , we build a multi - view   token graph , which has four views corresponding   to different relations in the whole label set and each   view is a graph attention network ( Veli ˇckovi ´ c et al . ,   2017 ) with token representations as the nodes . In   the prediction layer , we introduce a novel adaptive   multi - label classifier to extract all the sentiment   tuples no matter that they are overlapped or not .   We conduct extensive experiments on five bench-   marks , including NoReC(Øvrelid et al . , 2020 ) ,   MultiB , MultiB(Barnes et al . , 2018 ) , MPQA   ( Wiebe et al . , 2005 ) and DS ( Toprak et al . ,   2010 ) . The resluts show that our TGLS model   outperforms the SOTA model by a large margin . In   summary , our main contributions include :   •We design a novel labeling strategy to address   the label imbalance issue in prior work . Con-   cretely , we employ the whole label set and   essential label set in the hidden and predic-   tion layer respectively , achieving a balance4233between the label variety and label imbalance .   •We propose an effective token graph model   to well collaborate with our labeling strat-   egy , which learns the token - token relations   via multi - view token graph networks and rea-   sons the labels between each pair of words   using the adaptive multi - label classifier for   both overlapped and non - overlapped tuple ex-   traction .   •The experimental results show that our model   has achieved the SOTA performance in 5   datasets for structured sentiment analysis , es-   pecially in terms of the end - to - end sentiment   tuple extraction .   2 Related Work   The task of the Structured Sentiment Analysis   ( SSA ) can be divided into sub - tasks such as span   extraction of the holder , target and expression , re-   lation prediction between these elements and as-   signing polarity . Some existing works in Opin-   ion Mining used pipeline methods to first extract   spans and then the relations mostly on the MPQA   dataset ( Wiebe et al . , 2005 ) . For example , Katiyar   and Cardie ( 2016 ) propose a BiLSTM - CRF model   which is the first such attempt using a deep learning   approach , Zhang et al . ( 2019 ) propose a transition-   based model which identifies opinion elements by   the human - designed transition actions , and Xia   et al . ( 2021 ) propose a unified span - based model   to jointly extract the span and relations . However ,   all of these works ignore the polarity classification   sub - task .   InEnd2End Aspect - Based Sentiment Analysis   ( ABSA ) , there are also some attempts to unify sev-   eral sub - tasks . For instance , Wang et al . ( 2016 )   augment the ABSA datasets with sentiment expres-   sions , He et al . ( 2019 ) make use of this data and   models the joint relations between several sub - tasks   to learn common features , and ( Chen and Qian ,   2020 ) also exploit interactive information from   each pair of sub - tasks ( target extraction , expres-   sion extraction , sentiment classification ) . However ,   Wang et al . ( 2016 ) only annotate sentiment - bearing   words not phrases and do not specify the relation-   ship between target and expression , it therefore   may not be adequate for full structured sentiment   analysis .   Thus , Barnes et al . ( 2021 ) propose a unified ap-   proach in which they formulate the structured senti-   ment analysis task into a dependency graph parsingtask and jointly predicts all components of a sen-   timent graph . However , as aforementioned , this   direct transformation may be problematic as it may   introduce label imbalance in span and relation pre-   diction . Thus , we propose an effective graph model   with a novel labeling strategy in which we employ   a whole label set in the hidden layer to softly af-   fect our model , and an essential label set in the   prediction layer to address the imbalance issue .   The design of our essential label set is inspired   by the Handshaking Tagging Scheme ( Wang et al . ,   2020 ) , which is a token pair tagging scheme for   entity and relation extraction . The handshaking   tagging scheme involves only the labels related to   the boundary tokens and enables a one - stage joint   extraction of spans and relations . In our work , we   modify the handshaking tagging scheme to use it   for SSA . Furthermore , since the component span of   this task is relatively long , only utilizing the bound-   ary tokens can not make full use of the annotation   information , so we propose a new label set called   whole label set , which together with essential label   set constitutes our labeling strategy .   3 Token - Pair Labeling Strategy   3.1 Essential Label Set   Our essential label set only involves the labels re-   lated to the boundary tokens , therefore the label   proportions for span prediction and span relation   prediction are relatively balanced . Given a sen-   tence " Moscow government has expressed the wish   to import the Mongolian meat . " , the essential label   set consists of the following labels :   •Holder : Moscow →government   •Exp : Neutral : expressed →Moscow   •Target : import →meat   •Exp Head to Holder Head : expressed →   Moscow   •Exp Tail to Holder Tail : wish→government   •Exp Head to Target Head : expressed →im-   port   •Exp Tail to Target Tail : wish→meat   where the Holder , Exp . andTarget represent the   three components of a sentiment tuple , the Head or   Tailmeans the start or end token of a component ,   and the Neutral denotes the polarity .   3.2 Whole Label Set   Our whole label set involves both the labels related   to boundary and non - boundary tokens , as well as4234   the labels related to [ CLS ] and all tokens in the   sentiment tuples . Thus , our whole label set can   be divided into three groups , span labels , relation   labels and [ CLS ] -related labels . Given the sen-   tence in Figure 2 , the whole label set include the   following labels :   •Span Label : e.g. import →Mongolian   •Rel Label : e.g. Moscow →expressed   •[CLS ] -related Label : e.g. [ CLS ] →ex-   pressed   where the span and relation labels make our model   be aware of the token relations inside and across   the spans of sentiment components , and [ CLS ] -   related labels can help our model to capture the   sentence - level global information . We apply whole   labels in the hidden layer to softly embed the above   information into our model , in order to avoid the   potential label imbalance issue .   3.3 Decoding   We first decode all the expression - holder and   expression - target pairs that meet the constraints   of essential label set . In detail , we can get all com-   ponent spans based on span prediction labels ( e.g.   Holder , Exp : Neutral andTarget labels ) , then we de-   code all expression to holder or target pairs as long   as it meets one of the corresponding relation predic-   tion labels ( e.g. for expression to holder pairs , the   labels are Exp Head to Holder Head andExp Tail   to Hoder Tail ) . After decoding all the component   pairs , we enumerate all possible triples from pairs   with the same expression , thus finally decode all   the sentiment tuples.4 Methodology   In this section , We formally present our proposed   TGLS model in detail ( Figure 3 ) , which mainly   consists of four parts , the encoder layer , the multi-   view token graph as the hidden layer , the adaptive   multi - label classifier as the prediction layer and the   hierarchical learning strategy to train our model .   4.1 Encoder Layer   Consider the itoken in a sentence with ntokens ,   we represent it by concatenating its token embed-   ding e , part - of - speech ( POS ) embedding e ,   lemma embedding e , and character - level em-   bedding etogether :   w = e⊕e⊕e⊕e ( 1 )   where⊕denotes the concatenation operation . The   character - level embedding is generated by the con-   volution neural networks ( CNN ) ( Kalchbrenner   et al . , 2014 ) . Then , we employ bi - directional   LSTM ( BiLSTM ) to encode the vectorial token   representations into contextualized word represen-   tations :   h= BiLSTM ( w ) ( 2 )   where his the token hidden representation .   Moreover , in the same way as previous work   ( Barnes et al . , 2021 ) , we also enhance token rep-   resentations with pretrained contextualized em-   beddings using multilingual BERT ( Devlin et al . ,   2018).42354.2 Multi - view Token Graph   In this section , we propose a novel multi - view to-   ken graph as our hidden layer , which includes four   views , span graph , relation graph , [ CLS ] -related   graph and vanilla GAT graph , and each view is   full connected with the attention scoring weights as   graph edges and the token representations as graph   nodes . Recall that the whole label set is applied   in this layer , which includes three groups of labels   ( span , relation and [ CLS ] -related labels ) . Thus ,   three views of graphs ( span , relation and [ CLS ] -   related graph ) are used to digest information from   three groups of labels respectively , while one view   ( vanilla GAT graph ) is not assigned for any specific   task , as the method in vanilla graph attention net-   work ( GAT ) ( Veli ˇckovi ´ c et al . , 2017 ) . Formally , we   represent the latent token graph Gas follows :   G=    V , S , S , S , S(3 )   where superscript Gdenotes the graph layer , Vis   the set of tokens , Sis the attention scoring ma-   trix in vanilla GAT , S , SandSare the atten-   tion scoring matrices used to capture information   from span , relation and [ CLS ] -related labels re-   spectively . Without loss of generality , we employ   S={S , S , S , S}unifiedly to represent the   four matrices .   4.2.1 Graph Induction   In this section , we introduce the process that we   induce the edges of our multi - view token graphs   ( i.e. four attention scoring matrices S ) using a   mechanism of attention scoring .   Attention Scoring Our attention matrices are   produced by a mechanism of attention scoring   which takes two token representations h , has   the input , and for the attention matrix correspond-   ing to a certain view v∈ { o , s , r , c } , we first map   the tokens to qandkwith two multi - layer   perceptions ( MLP ):   q , k = MLP(h ) , MLP(h)(4 )   Then we apply the technique of Rotary Position   Embedding ( RoPE ) ( Su et al . , 2021 ) to encode the   relative position information . Thus , for the graph   of view v , the attention score Sbetween token i   andjcan be calculated as follows :   S= ( q)Rk ( 5)where Rcan incorporate explicit relative po-   sitional information into the attention score S.   And in the same way as calculating S , we can   produce the scores of all views and all token pairs ,   thus inducing the whole graph edges S :   S = n   S|v∈ { o , s , r , c } , 1≤i , j≤no   ( 6 )   where nis the length of the sentence . The pro-   cess that the whole label set learnt by attention   scoring matrices S , SandSthrough a multi-   label adaptive - threshold loss will be introduced in   Section 4.4 .   4.2.2 Multi - hop Reasoning   Considering that the attention scoring matrix S   now fuses rich information , we naturally think of   applying a multi - hop reasoning to obtain more in-   formative token representations . Concretely , we   first apply a softmax on our adjacency attention   matrix S , then the computation for the representa-   tionuof the token iat the ( l+ 1)layer , which   takes the representations from previous layer as   input and outputs the updated representations , can   be defined as :   A = Softmax    S   , v∈ { o , s , r , c } ( 7 )   u = σ   1   NXXAWu    ( 8)   where Wis the trainable weight , Nis the neigh-   bor of token iin graph of view v , σis the ReLU   activation function .   4.3 Adaptive Multi - label Classifier   Considering that the previous sota model ( Barnes   et al . , 2021 ) is not able to deal with multi - label clas-   sification as aforementioned , we propose a novel   adaptive multi - label classifier as our prediction   layer to identify possible essential labels for each   token pair .   Firstly , we take a shortcut connection between   the outputs of the encoder layer and graph layer   to get the final representation c = h⊕ufor   each token . And by taking cas the input , we   calculate the attention scoring matrices Sbased   on the mechanism of attention scoring ( cf . Eq.(4 ) ,   Eq.(5 ) and Eq.(6 ) ):   S={S|r∈ R } ( 9)4236where superscript Pdenotes the prediciton layer ,   Rdenotes the essential label set . Then , we intro-   duce a technique of adaptive thresholding , which   produces a token pair dependent threshold to en-   able the prediction of the labels for each token pair .   Adaptive Thresholding For a certain token pair   with representations of c , c , the token pair de-   pendent threshold THand the whole THare   calculated as follows :   TH=    qRk   TH=   TH|1≤i , j≤n 	  ( 10 )   where q = Wh+b , k = Wh+b ,   theW , W , bandbare the trainable weight   and bias matrix , Rare calculated in the same   way as Eq.(5 ) , which is used to incorporate explicit   relative positional information .   Formally , for a certain token pair c , c , the essen-   tial label set is predicted by the following equation :   Ω=   r|S > TH , r∈ R 	  ( 11 )   whereRdenotes the essential label set , Ωis the   set of predicted labels of token pair c , c.   4.4 Training   In this section , we will propose a novel loss func-   tion , namely multi - label adaptive - threshold loss ,   to enable a hierarchical training process for our   model and our labeling strategy ( i.e. whole label   set learnt by S , SandSin the hidden layer ,   essential label set learnt by Sin the prediction   layer ) , which is based on a variantof Circle loss   ( Sun et al . , 2020 ) , the difference is that we replace   the fixed global threshold with the adaptive token   pair dependent threshold to enable a flexible and   selective learning of more useful information from   whole label set .   Take the hidden layer as an example . Actu-   ally , we also implement the adaptive threshold-   ing ( cf . Eq.(10 ) ) in the hidden layer , where we   compute all the token pair dependent threshold   TH = n   TH|1≤i , j≤no   by taking the to-   ken representation handhas the input . Then ,   the multi - label adaptive - threshold loss in hiddenlayer can be calculated as follows :   L = XXlog   e+Xe      + XXlog   e+Xe      ( 12 )   where Ω⊆ RandΩ⊆ Rare positive   and negative classes involving whole labels that   exist or not exist between token iandj . When   minimizing L , the loss pushes the attention score   Sabove the threshold THif the token pair   possesses the label , while pulls below when it does   not .   In a similar way we can calculate the loss L   in the prediction layer by taking TH , Sas the   inputs of the loss function . Thus the whole loss of   our model can be calculated as follows :   L = L+αL ( 13 )   where the αis a hyperparameter to adjust the ratio   of the two losses .   5 Experiments   5.1 Datasets and Configuration   For comparison with previous sota work ( Barnes   et al . , 2021 ) , we perform experiments on five struc-   tured sentiment datasets in four languages , includ-   ing multi - domain professional reviews NoReC   ( Øvrelid et al . , 2020 ) in Norwegian , hotel reviews   MultiBandMultiB(Barnes et al . , 2018 ) in   Basque and Catalan respectively , news MPQA   ( Wiebe et al . , 2005 ) in English and reviews of on-   line universities and e - commerce DS(Toprak   et al . , 2010 ) in English .   For fair comparison , we use word2vec skip - gram   embeddings openly available from the NLPL vec-   tor repository(Kutuzov et al . , 2017 ) and enhance   token representations with multilingual BERT ( De-   vlin et al . , 2018 ) , which has 12 transformer blocks ,   12 attention heads , and 768 hidden units . Our net-   work weights are optimized with Adam and we also   conduct Cosine Annealing Warm Restarts learning4237   rate schedule ( Loshchilov and Hutter , 2016 ) . We   fixed the word embeddings during training process .   The char embedding size is set to 100 . The dropout   rate of embeddings and other network components   are set to 0.4 and 0.3 respectively . We employ 4-   layer BiLSTMs with the output size set to 400 and   2 - layer for multi - hop reasoning with output size   set to 768 . The learning rate is 3e-5 and the batch   size is 8 . The hyperparameter αin Eq.13 is set   to 0.25 ( cf . Section 6.2 ) . We use GeForce RTX   3090 to train our model for at most 100 epochs and   choose the model with the highest SF1 score on the   validation set to output results on the test set .   5.2 Baselines   We compare our proposed model with three state-   of - the - art baselines which outperform other models   in all datasets :   RACL - BERT Chen and Qian ( 2020 ) propose a   relation - aware collaborative learning framework   for end2end sentiment analysis which models the   interactive relations between each pair of sub - tasks   ( target extraction , expression extraction , sentiment   classification ) . Barnes et al . ( 2021 ) reimplement   the RACL as a baseline for SSA task in their work . Head - first and Head - finalBarnes et al . ( 2021 )   cast the structured sentiment analysis as a depen-   dency parsing task and apply a reimplementation   of the neural parser by Dozat and Manning ( 2018 ) ,   where the main architecture of the model is based   on a biaffine classifier . The Head - first and Head   final are two models with different setups in the   parsing graph .   5.3 Evaluation Metrics   Following previous SOTA work ( Barnes et al . ,   2021 ) , we use the Span F1 , Targeted F1 and two   Sentiment Graph Metrics to measure the experi-   mental results .   In detail , Span F1 evaluates how well these mod-   els are able to identify the holders , targets , and   expressions . Targeted F1 requires the exact extrac-   tion of the correct target , and the corresponding   polarity . Sentiment Graph Metrics include two F1   score , Non - polar Sentiment Graph F1 ( NSF1 ) and   Sentiment Graph F1 ( SF1 ) , which aims to measure   the overall performance of a model to capture the   full sentiment graph ( Figure 1(a ) ) . For NSF1 , each   sentiment graph is a tuple of ( holder , target , expres-4238   sion ) , while SF1 adds the polarity ( holder , target ,   expression , polarity ) . A true positive is defined as   an exact match at graph - level , weighting the over-   lap in predicted and gold spans for each element ,   averaged across all three spans .   Moreover , for ease of analysis , we add an Over-   all Span F1 score which evaluates how well these   models are able to identify all three elements of a   sentiment graph with token - level F1 score .   5.4 Main Results   In this section , we introduce the main experimental   results compared with three state - of - the - art models   RACL - BERT ( Chen and Qian , 2020 ) , Head - first   and Head - final models ( Barnes et al . , 2021 ) .   Table 2 shows that in most cases our model per-   forms better than other baselines in terms of the   Span F1 metrics across all datasets . The average im-   provement ( ↑1.4 ) in Overall Span F1 score proves   the effectiveness of our model in span extraction .   Besides , there exists some significant improve-   ments such as extracting holder on DS(↑6.3 )   and extracting expression on NoReC(↑4.7 ) , but   the extracting expression on DS(↓2.9 ) are poor .   As for the metric of Targeted F1 , although   the Head - first model performs well on MPQA ,   our TGLS model is obviously more robust as we   achieves superior performance on other 4 datasets .   There are also extremely significant improvements   such as on NoReC(↑6.2 ) and on MultiB   ( ↑5.6 ) , it proves the capacity of our model in exact   prediction of target and the corresponding polar .   As for the Sentiment Graph metrics , which   are important for comprehensively examining   span , relation and polar predictions , our TGLS   model achieves superior performance throughout   all datasets in both NSF1 and SF1 score , especially   onNoReC(↑7.2 and ↑6.4 ) . And the average im-   provement ( ↑4.5 ) in SF1 score verifies the excellent   ability of our model in the end - to - end sentiment   tuple extraction .   5.5 Ablation Study   In this section , we conduct extensive ablation stud-   ies on NoReCto better understand independent   contributions of different components in terms of   span overall F1 , targeted F1 and SF1 scores .   Firstly , we remove each view of our graphs sep-   arately . As shown in Table 3 , we observe that   the[CLS ] -related graph is effective in all three   metrics which proves the importance of utilizing   sentence - level global information . As we assumed ,   the span graph makes more contribution to the   performance of span extraction ( Span Overall F1 )   while the relation graph contributes more to end - to-   end sentiment tuple extraction ( SF1 ) . And we also   observe that the vanilla GAT graph makes consid-4239erable improvement in SF1 score .   Then , we test the effectiveness of the Rotary Po-   sition Embedding ( RoPE ) ( Su et al . , 2021 ) . The   results in Table 3 demonstrate that RoPE can make   our model more sensitive to the relative positional   information since it significantly improves the per-   formance of exact target extraction ( Targeted F1 ) .   Last , we replace the adaptive threshold with   fixed global threshold , and we observe that the per-   formance drops drastically in all three metrics , it   suggests that the adaptive thresholding mechanism   is very crucial for our model since the flexibility   can allow our model to selectively learn more use-   ful information for SSA task from whole labels .   6 Analysis   In this section we perform a deeper analysis on the   models in order to answer three research questions :   6.1 Does our modeling strategy mitigate the   label imbalance problem in span   prediction and span relation prediction ?   Experimental results in Table 2 show that our   model performs significantly better in the SF1   score , which to some extent proves that our model   can ensure the efficiency of relation extraction .   However , there lacks a metric to directly quantify   the ability in relation extraction and it is still a   worthy question to explore how much of the im-   provement comes from our new model and how   much from our new labeling strategy ?   To answer the question , we replace our labels   with the dependency - parsing - based labels in head-   final setting ( Barnes et al . , 2021 ) and experiment   on all datasets in terms of a new relation prediction   metric , where a true positive is defined as any span   pair that overlaps the gold span pair and has the   same relation . Table 4 shows that our new model   achieves superior performance of relation predic-   tion than the previous sota model ( Barnes et al . ,   2021 ) . Besides , with new labeling strategy , we can   see that our model significantly improve the per-   formance on all datasets compared with the model   with replaced dependency - parsing - based labels .   6.2 What is the appropriate value for the   hyperparameter αin Eq . 13 ?   In this section , we experiment on five datasets to   heuristically search for the appropriate value of   hyperparameter α(cf . Eq.(13 ) ) . Figure 4 shows   that all datasets achieve higher SF1 score with αbetween 0.1 and 0.5 . We ended up fixing alpha   to 0.25 , since most datasets yield optimal results   around this value . In addition , it is worth noting   that when αis set to 0 , which means that the whole   labels are completely removed , the performance   drops a lot , which once again proves the effective-   ness of learning whole labels in the hidden layer .   6.3 Is the whole label set helpful for long span   identification ?   In this section , we experiment on NoReCto   further explore whether whole labels contribute to   long span identification . Figure 5(a ) evaluates the   Expression F1 scores regarding to different expres-   sion lengths , we can find that whole labels helps   most on those expressions with longer length . In   Figure 5(b ) , we also report the SF1 scores regarding   to different distances , that is , from the leftmost to-   ken in a tuple to the rightmost token , which shows   a similar conclusion .   7 Conclusion   In this paper , we propose a token graph model with   a novel labeling strategy , consisting of the whole   and essential label sets , to extract sentiment tu-   ples for structured sentiment analysis . Our model   is capable of modeling both global and local to-   ken pair interactions by jointly predicting whole   labels in the hidden layer and essential labels in   the output layer . More importantly , our model-   ing strategy is able to alleviate the label imbalance   problem when using token - graph - based approaches   for SSA . Experimental results show that our model   overwhelmingly outperforms SOTA baselines and   improves the performance of identifying the sen-   timent components with long spans . We believe   that our labeling strategy and model can be well   extended to other structured prediction tasks .   Acknowledgements   We thank all the reviewers for their insightful com-   ments . This work is supported by the National Nat-   ural Science Foundation of China ( No . 62176187 ) ,   the National Key Research and Development Pro-   gram of China ( No . 2017YFC1200500 ) , the Re-   search Foundation of Ministry of Education of   China ( No . 18JZD015 ) , the Youth Fund for Hu-   manities and Social Science Research of Ministry   of Education of China ( No . 22YJCZH064 ) , the   General Project of Natural Science Foundation of   Hubei Province ( No.2021CFB385).4240References4241