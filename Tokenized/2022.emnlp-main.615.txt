  Yilun ZhaoLinyong NanZhenting QiRui ZhangDragomir RadevYale UniversityZhejiang UniversityPenn State University   { yilun.zhao , linyong.nan}@yale.edu   Abstract   Reasoning over tabular data requires both table   structure understanding and a broad set of table   reasoning skills . Current models with table-   specific architectures and pre - training methods   perform well on understanding table structures ,   but they still struggle with tasks that require   various table reasoning skills . In this work , we   develop RTAP to show that high - level ta-   ble reasoning skills can be injected into models   during pre - training without a complex table-   specific architecture design . We define 7 table   reasoning skills , such as numerical operation ,   temporal comparison , and conjunction . Each   reasoning skill is associated with one exam-   ple generator , which synthesizes questions over   semi - structured tables according to the sampled   templates . We model the table pre - training   task as a sequence generation task and pre-   train RTAP to generate precise answers   to the synthetic examples . RTAP is evalu-   ated on four benchmarks covering three down-   stream tasks including : 1 ) WSQL - W   andWTQfor Table Question Answering ;   2)TF for Table Fact Verification ; and   3)L NLG for Faithful Table - to - Text Gen-   eration . Experimental results demonstrate that   RTAP achieves new state - of - the - art per-   formance on all benchmarks and delivers a   significant improvement on low - resource set-   ting . Our code is publicly available at https :   //github.com / Yale - LILY / ReasTAP .   1 Introduction   Inspired by the massive success of pre - trained lan-   guage models ( LM ) on free - form natural language   ( NL ) tasks ( Devlin et al . , 2019 ; Dong et al . , 2019 ;   Raffel et al . , 2020 ; Lewis et al . , 2020 ) , researchers   have attempted to extend the pre - training to table   data . Tables are a valuable form of data that or-   ganize information in a structured way . They of-   ten contain data that is organized in a more ac-   cessible manner than in unstructured texts . To   adapt the pre - training paradigm on structured tab - Figure 1 : The illustration of RTAP pre - training .   The tables are crawled from Wikipedia . During pre-   processing , we perturb the table row order to alleviate   unwanted bias brought by table encoding . The colored   cells are relevant facts necessary to answer the given   question . Each color corresponds to a different table   reasoning skill . And each reasoning skill corresponds to   an example generator , which synthesizes QA pairs over   tables according to the sampled templates . We model   the pre - training task as a sequence generation task and   pre - train RTAP to generate correct answers given   the flatten table and synthetic question .   ular data , previous works mainly focus on design-   ing models with table - specific architectures and   pre - training methods . This includes introducing   a structure - aware attention mechanism ( Yin et al . ,   2020 ; Deng et al . , 2020 ; Zayats et al . , 2021 ) , adding   auxiliary structure indicative embeddings ( Herzig9006et al . , 2020 ; Eisenschlos et al . , 2020 ; Wang et al . ,   2021b ) , and designing table - specific pre - training   objectives ( Yin et al . , 2020 ; Yu et al . , 2021a ; Wang   et al . , 2021b ; Liu et al . , 2022b , a ) . While these   methods are effective in understanding table struc-   tures , they increase the modeling complexity and   lack interpretability on why models learns table   reasoning skills during pre - training .   This paper presents a new table pre - training ap-   proach , named RTAP , which enables a model   to efficiently learn table structure understanding   and table reasoning skills during pre - training . We   first defined 7 table reasoning skills , such as numer-   ical operation and temporal comparison . As shown   in Figure 1 , for each reasoning skill , a correspond-   ing example generator was applied to synthesize   Question Answering ( QA ) examples over tables .   We modeled the pre - training task as a sequence gen-   eration task and pre - trained a sequence - to - sequence   ( seq2seq ) LM to generate the answer to the syn-   thetic questions . RTAP is theoretically appli-   cable to any seq2seq LM without a table - specific   architecture design . Our key insight is that if a   language model can be pre - trained to generate the   answers to synthetic questions , which require var-   ious table reasoning skills , it should have a great   table structure understanding and table reasoning   capacity , thereby conferring benefits to downstream   tasks . The main contributions of our work can be   summarized as follows :   •We develop a new table reasoning example   generation pipeline , which produces a large-   scale table QA corpus that requires various   reasoning skills over semi - structured tables .   •We propose a new table pre - training method ,   RTAP , which helps the model to learn ta-   ble structure understanding and various table   reasoning skills during pre - training without   any table - specific architecture design .   •RTAP is evaluated on four downstream   benchmarks . Experimental results demon-   strate that RTAP achieves new state - of-   the - art results on all of them , and delivers a   great improvement on low - resource setting .   2 Pre - training Corpus   2.1 Table Source and Pre - processing   We chose publicly available semi - structured tables   as the table source . Specifically , we extracted ta - bles from English Wikipedia , which covered a   wide range of domains including popular culture ,   geography , politics , and science . We kept tables   with 8 - 30 rows and at least three columns , resulting   in around 600 K tables . For each extracted table , a   pre - processing script was applied to automatically   annotate table columns with their data types ( i.e ,   string , number , and date ) , which allows us to gen-   erate questions that involve manipulating numbers   and dates . Furthermore , recent work ( Yang et al . ,   2022 ; Wang et al . , 2022 ) demonstrates that existing   table pre - training approaches might encode table   row order as an unwanted bias . For example , the   pre - trained model being aware of row order infor-   mation is inclined to select the first or last row of   tables when answering superlative - type questions   without truly understanding the table content . To   alleviate this problem , we randomly shuffled table   rows during pre - processing .   2.2 Example Generation   We defined 7 types of table reasoning skills , with   examples and explanations shown in Table 1 . The   example generation pipeline was adapted from   Yoran et al . ( 2021 ) . Each reasoning skill is as-   sociated with one example generator and several   question templates . The example generator was   implemented as a function that takes a table T   and generates several reasoning examples ( T , q ,   a ) according to the template , where qdenotes the   question , and adenotes the answer .   Each template contains typed variables that are   instantiated with content from the extracted ta-   ble . Specifically , column col and cell value val   are indexed to specify that val : i must be in-   stantiated by a cell value from the i - th column .   Some templates also regulate that the selected   column and cell value must be date or number   type . OPERATOR andORDINAL correspond to   operators and ordinal numerals that are instanti-   ated according to the specific reasoning skill . And   CONDITION : i can be 1 ) a cell value from the   i - th column ; or 2 ) a number / temporal comparison   statement if the i - th column is date or number type .   For example , the question from Figure 1 " Which   Company Name , with Headquarter was United   States , has the 4th Profit ? " are generated from one   of the " Numerical Comparison " templates : " Which   col:1 , with col:2 wasCONDITION:2 , has9007   theORDINAL col:3 ? "   Once all variables in the sampled template were   instantiated , we obtained question q. Then the   example generator would programmatically return   the corresponding answer a.   2.3 Example Sampling   After generating a vast number of QA examples for   each reasoning skill , we had to sample pre - training   data from these synthetic examples . In our setting ,   the portion of pre - training examples ( Table 1 ) cor-   responding to each reasoning skill roughly matches   the portion of logical operations defined in Tab-   Fact ( Chen et al . , 2020b ) . We raised the portion of   numerical operation skill as numerical reasoning is   more challenging for models to learn . To increase   the diversity of pre - training corpus , for each reason-   ing skill , we also sampled { SQL query , execution   result } pairs from TAPEX ( Liu et al . , 2022b ) pre-   training corpus as complementary QA examples . The sampled pairs were categorised according to   their function ( e.g. , COUNT , SUM ) . As a result , we   obtained a total of 4 M pairs of reasoning examples   as the pre - training corpus for RTAP .   3 Pre - training RTAP   Task Formulation Each example in the synthetic   pre - training corpus contains a question qand a   semi - structured table Tas the model input . The   task objective is to generate an accurate answer   string a=(a , a , . . . , a)given the question q   and input table T :   a = argmax   ∏P(a∣a , q , T;θ ) , ( 1 )   where θdenotes the parameters of a seq2seq LM .   Model Architecture Our method is theoretically   applicable to any seq2seq LM , such as T5 ( Raf-9008fel et al . , 2020 ) and GPT3 ( Brown et al . , 2020 ) .   In our experiments , we implemented RTAP   based on BART ( Lewis et al . , 2020 ) , a widely used   Transformer - based pre - trained model ( Vaswani   et al . , 2017 ) that has proved its effectiveness on   various comprehension and text generation tasks .   In our experiments , we chose BART - Large as a   backbone , which has around 400 M parameters and   12 layers in both encoder and decoder .   Data Serialization As illustrated in Figure 1 , the   input contains a question and its corresponding ta-   ble . We flattened the table so that it can be fed   directly into the encore - decoder model . Specifi-   cally , by inserting several special tokens to indicate   the table boundaries , a flattened table is denoted as   T= h∣h∣⋯∣h c∣c∣⋯∣c   ⋯ c∣c∣⋯∣c   where [ HEAD ] and[ROW ] are special tokens in-   dicating the region of table headers and rows re-   spectively . We prefixed the flattened table Twith   the question and feed them into the model encoder .   The decoder is tasked to generate the answer(s ) ,   separated by commas , autoregressively .   4 Downstream Tasks   We evaluated RTAP on three different types of   downstream tasks to verify its effectiveness . The   statistics and examples for each task are shown in   Table 2 , and Table 10 in the Appendix , respectively .   The fine - tuning of RTAP is similar to the pro-   cedure for pre - training discussed in section 3 . We   modeled both downstream tasks as sequence gener-   ation tasks and leverage generative LMs to generate   the output autoregressively .   Table QA WSQL - W ( Zhong et al . ,   2017 ) and WTQ(Pasupat and Liang , 2015 )   were used to evaluate RTAP performance on   Table QA tasks . WSQL - W ( Zhong et al . ,   2017 ) requires the models to perform filtering and   optional aggregation on table cell values to an-   swer the given a question . WTQ(Pasupat and   Liang , 2015 ) requires a broader set of reasoning   skills , thus is more challenging . The Table QA   task formulation is the same as the RTAP pre-   training task . We used the denotation accuracy ,   which checks whether the predicted answers are   equal to the ground truths , as evaluation metric . Table Facts Verification We chose T-   F ( Chen et al . , 2020b ) to evaluate RTAP   performance on Table Facts Verification tasks .   Given a table and a statement , TF ( Chen   et al . , 2020b ) tries to distinguish whether the state-   ment is entailed or refuted by the table . TF   divides its test sets into Test and Test   subsets , where Test contains examples re-   quiring more complex table reasoning skills . Fur-   thermore , it selects a small test set Test with   2 K samples for human evaluation . To fine - tune on   TF , following BART ( Lewis et al . , 2020 ) ,   we applied a binary classifier upon the hidden state   of the last token in the decoder for the output .   The objective is to generate the verification label   L∈{0,1}given the statement s=(s , s,⋯ , s )   and the input table T :   L = argmaxP(i∣s , T;θ ) ( 2 )   We used the accuracy ( i.e. , percentage of correct   predictions ) as evaluation metric .   Faithful Table - to - Text Generation We chose   L NLG ( Chen et al . , 2020a ) to evaluate   RTAP performance on the Faithful Table - to-   Text Generation task . Compared with previous   Table - to - Text generation benchmarks ( Wiseman   et al . , 2017 ; Balakrishnan et al . , 2019 ; Parikh et al . ,   2020 ; Nan et al . , 2022b ) , which primarily focus   on surface - level realizations without much logical   inference , L NLG is tasked to generate state-   ments that are logically entailed by the selected   table region . Given the serialized input table with   its selected columns as T , the objective is to gen-   erate a sentence y=(y , y , . . . , y)that is both   fluent and factually correct :   y = argmax   ∏P(y∣y , T;θ ) ( 3 )   To evaluate the logical fidelity of generated sen-   tences , Chen et al . ( 2020a ) proposed two model-   based evaluation methods : Parsing - based Evalu-   ation ( SP - Acc ) , and NLI - based Evaluation ( NLI-   Acc ) . SP - Acc directly extracts the meaning rep-   resentation from the generated sentence and exe-   cutes it against the table to verify the correctness .   NLI - ACC uses a Natural Language Inference ( NLI )   model to predict entailment relationships . Follow-   ing Chen et al . ( 2020a ) , we used SP - Acc , NLI - Acc   as logical - fidelity evaluation metrics ; and BLEU-   1/2/3 as surface - level evaluation metrics . It is worth9009   noting that higher BLEU scores do not correlate   with better logical fidelity ( Nan et al . , 2022a ) .   5 Experiments   5.1 Implementation Details   We implemented our models based on the fairseq   library ( Ott et al . , 2019 ) . We adopted BART - Large   as the backbone model . For table pre - training , we   synthesized and sampled 4 M pairs of reasoning ex - amples . In the following sections , unless specified   explicitly , all the experimental results were eval-   uated under the default settings of 4 M reasoning   examples and BART - Large configuration . Our pre-   training procedure ran 80,000 steps with a batch   size of 256 , which took about 34 hours on an 8   NVIDIA A5000 24 GB cluster . For downstream   tasks , the fine - tuning procedure ran 30,000 steps   with a batch size of 256 . The best pre - training and   fine - tuning checkpoints were both selected accord-   ing to the validation loss .   5.2 Main Results   Table QA OnWSQL - W , RTAP out-   performs all the baselines ( Table 3 ) . Specifically ,   on the test set of WSQL - W , RTAP   achieves a denotation accuracy of 90.4 % , which is   4.3 % higher than BART and 1.2 % higher than the   previous best performance . On the more challeng-   ingWTQ , as shown in Table 4 , RTAP also   surpasses the previous best system by 1.4 % . It is   worth noting that compared to WSQL - W ,   WTQcontains much fewer tables and examples ,   which makes the adaptation of BART to tabular   structures more challenging . Further RTAP   obtains an improvement of 21.1 % over BART , in-   dicating that in the low data regime , the improve-   ments brought by RTAP are more significant .   We also evaluated RTAP performance under   low - resource settings ( Section 6.1 ) .   Table Fact Verification As shown in Table 5 ,   RTAP also obtains a new state - of - the - art accu-   racy on all test subsets of TF . For example ,   it surpasses the previous best system by 0.4 % on   Test , and 1.0 % on Test .   Faithful Table - to - Text Generation Table 6   presents the results on L NLG . It is observed   that the BART backbone has already achieved com-   petitive results in terms of both surface - level and   logical - fidelity metrics . Compared with the BART   backbone , RTAP obtains slightly lower results   on BLEU scores , which is reasonable since we con-9010   tinued pre - training RTAP on our pre - training   corpus that is irrelevant to the text generation task .   However , RTAP significantly improves the   logical - fidelity scores , enhancing the SP - Acc and   NLI - Acc by 4.7 % and 5.5 % , respectively . The   results demonstrate that RTAP can also help   improve faithful text generation .   6 Analysis   Experimental results on three different kinds of   downstream tasks show that RTAP can broadly   improve BART ’s generic table reasoning capabil-   ities , which could be adapted to different down-   stream tasks , regardless of whether the tasks are   highly similar to the RTAP pre - training task   or not . In this section , we further analyze our ap-   proach in terms of various aspects to provide re-   searchers with a deeper insight for future work .   6.1 Low - resource Setting   To further understand how well RTAP learns   table reasoning skills during pre - training , we con-   ducted experiments under the low - resource setting ,   where we fine - tuned RTAP on 20 % and 5 %   of downstream task training data . As shown in   Table 7 , in the low - resource setting , the improve-9011   ments introduced by RTAP are often more   significant . For example , with only 5 % train-   ing data of downstream tasks , RTAP deliv-   ers a dramatic improvement of 20.0 % , 21.4 % , and   11.2 % over BART on WSQL - W , WTQ ,   andTF , respectively . The results from the   low - resource setting show that RTAP endows   BART with generic table reasoning capabilities .   6.2 The Scale of Pre - training Corpus   Figure 2 illustrates RTAP performance on   downstream tasks with different pre - training cor-   pus scales . We found that increasing the pre-   training corpus generally brings positive effects   for all downstream tasks . Furthermore , for simple   tasks like WSQL - W , the gains by scalingup pre - training corpus are marginal , while for com-   plex tasks like WTQ , it shows a positive trend   by scaling up the pre - training corpus .   6.3 Necessity of Each Reasoning Skill   We investigated the contributions of the 7 reason-   ing skills to the downstream task performance of   RTAP . We devised 8 variants of RTAP :   one was trained with examples from all reason-   ing skills , while others were trained with examples   without one reasoning skill . For each reasoning   skill , we sampled 150 K examples from the pre-   training corpus . We kept the scale of pre - training   corpus the same ( i.e. , 900 K ) . We chose WTQ   for experiments , on which BART does not perform   well . Results shown in Table 8 demonstrate that all   reasoning skills can benefit the model performance   onWTQ . Furthermore , we find that some rea-   soning skills , such as counting and temporal com-   parison , bring more improvements to the model   compared to others .   The analysis also helps us understand how the   sets of pre - defined reasoning skills are injected   during pre - training . When adopting RTAP to   a new downstream task that requires new reason-   ing skills different from existing seven reasoning   skills , one can also inject the new reasoning skill   into model during the pre - training in a similar way .   Specifically , once the templates for the new rea-   soning skill are designed , the synthesis pipeline   will generate new examples for pre - training . Pre-   training RTAP on these synthetic examples   can help model learn the new reasoning skill .   6.4 Multi - Task Fine - tuning   We further conducted multi - task fine - tuning exper-   iments to explore whether RTAP can benefit   from the source task . We chose WSQL - W   andTF for the source task , as their train-   ing datasets are relatively rich , and WTQas9012the target task . Models were first fine - tuned on   the source task and then fine - tuned on the target   task . As shown in Table 9 , multi - task fine - tuning   delivers a significant improvement to the target task   when initialized by BART ; while the improvements   are marginal when initialized by RTAP . This   is reasonable because most table reasoning skills   acquired by multi - task learning have been injected   into the model during the pre - training .   7 Related Work   Reasoning Over Tables Reasoning over the in-   put context is an important requirement for neural   models to be applied in the real world , and es-   pecially when the input is structured knowledge   such as a table . Several Table QA benchmarks ( Pa-   supat and Liang , 2015 ; Zhong et al . , 2017 ; Iyyer   et al . , 2017 ; Chen et al . , 2020c ) have been pro-   posed to test systems ’ capability to conduct differ-   ent types of reasoning , including numerical , logical   or multi - hop reasoning . For Table Fact Verification   tasks ( Chen et al . , 2020b ; Aly et al . , 2021 ) , the   models are required to perform logical inference   to verify whether the given statement is entailed or   refuted . Furthermore , Table - to - Text ( Chen et al . ,   2020a ; Parikh et al . , 2020 ; Nan et al . , 2022b ) tasks   to generate a natural language description of some   part of the table based on inferences obtained from   facts in the contexts . More recently , numerical rea-   soning over tabular data in financial domain has   also raised increasing attention ( Zhu et al . , 2021 ;   Chen et al . , 2021b ; Zhao et al . , 2022 ; Cheng et al . ,   2022 ; Li et al . , 2022 ; Zhou et al . , 2022 ) .   Table Pre - training Inspired by the huge suc-   cess of pre - training in natural language , researchers   have attempted to extend pre - training to structured   tabular data ( Yin et al . , 2020 ; Herzig et al . , 2020 ;   Eisenschlos et al . , 2020 ; Shi et al . , 2021 ; Yu et al . ,   2021a ; Wang et al . , 2021b ; Deng et al . , 2020 , 2021 ;   Liu et al . , 2022b ) in recent years . Previous table   pre - training work such as TBERT ( Yin et al . ,   2020 ) and TP(Herzig et al . , 2020 ; Eisenschlos   et al . , 2020 ) took corrupted tables and NL sen-   tences as input and tried to recover the corrupted   parts . They had the intuition that such recovering   processes can help strengthen the linking between   sentences and structured tables . On the other hand ,   TAPEX ( Liu et al . , 2022b ) learned from synthetic   SQL programs . And Jiang et al . ( 2022 ) further pre-   trained TAPEX over natural and synthetic QA ex-   amples to improve the few - shot performance overtable QA tasks . Meanwhile , pre - training for Text-   to - SQL tasks ( Shi et al . , 2021 ; Yu et al . , 2021a , b ;   Deng et al . , 2021 ) also attracted researchers ’ at-   tention in recent years . Unlike previous work , we   model the pre - training task as a sequence genera-   tion task , and inject various table reasoning skills   into the model by tasking it to generate the precise   answers of reasoning examples .   Synthetic Pre - training Corpus Generating a   large - scale synthetic pre - training corpus is widely   used in both natural language pre - training ( Cam-   pagna et al . , 2020 ; Geva et al . , 2020 ; Yoran et al . ,   2021 ; Neeraja et al . , 2021 ; Yue et al . , 2022 ) and   table pre - training ( Yu et al . , 2021a , b ; Wang et al . ,   2021a ; Liu et al . , 2022b ) . For example , Geva et al .   ( 2020 ) utilized automatically - generated numerical   data to inject numerical reasoning skills during pre-   training . And Yoran et al . ( 2021 ) leveraged large-   scale WikiPedia resources to automatically gener-   ate examples that requires reasoning over multiple   facts in the paragraph , and continue pre - training   LM on this synthetic corpus . Furthermore , recent   works ( Liu et al . , 2022b ; Pi et al . , 2022 ) showed   that pre - training can be achieved by learning a pro-   gram executor over synthetic corpus .   8 Conclusion   In this paper , we propose RTAP , a new ta-   ble pre - training approach , which injects various   pre - defined table reasoning skills into models via   learning to generate correct answers of synthetic   questions . Compared to previous work which de-   sign table - specific architectures , RTAP is easy   to implement and is theoretically applicable to any   sequence - to - sequence LM . RTAP is evaluated   over four downstream benchmarks . The experimen-   tal results demonstrate that RTAP achieves   new state - of - the - art results on each of them . This   includes the improvements on WSQL - W   denotation accuracy to 90.4 % ( +1.2 % ) ; WTQ   denotation accuracy to 58.6 % ( +1.4 % ) ; TF   accuracy to 84.7 % ( +0.7 % ) ; and L NLG SP-   Acc to 54.8 % ( +4.0 % ) , NLI - Acc to 89.2 % ( +3.6 % ) .   Further analysis demonstrates that RTAP de-   livers a significant improvement to BART on the   low - resource setting , indicating that our proposed   pre - training approach can effectively improve the   model ’s generic table reasoning capabilities.9013Limitations   The main limitation of our approash is that we   utilized a template - based method to synthesize pre-   training corpus . Although such template - based ap-   proach ensures the faithfulness of generated QA   examples and the diversity of reasoning process   required to answer the questions , it limits the se-   mantic diversity of questions . We believe future   work could exploit 1 ) more different types of rea-   soning skills , such as advanced numerical reason-   ing skills required in the finance domain ( Zhu et al . ,   2021 ; Chen et al . , 2021b ) ; 2 ) a more universal   synthetic example generation pipeline ; 3 ) extend-   ing models to tables with hierarchical structures   ( e.g. , more than one row or column header ) ( Cheng   et al . , 2022 ; Zhao et al . , 2022 ) ; 4 ) a more efficient   training framework ( Biesialska et al . , 2020 ; Yoran   et al . , 2021 ) that can update models to learn newly-   defined reasoning skills effectively .   Ethical Consideration   Tables used in our synthetic pre - training corpus   are collected and extracted from the 02 - 20 - 2022   Wikipedia dump , which is publicly available un-   der the Creative Commons Attribution - ShareAlike   3.0 License and the GNU Free Documentation Li-   cense . The licenses permit us to compose , modify ,   publish , and distribute additional annotations upon   the original content .   Acknowledgements   We would like to thank the anonymous reviewers   and action editors for their constructive feedback .   References90149015901690179018