  Ramy EskanderandCass LowryandSujay Khandagale ,   Judith KlavansandMaria PolinskyandSmaranda MuresanColumbia University , The Graduate Center , City University of New York , University of Maryland ,   Abstract   Unsupervised cross - lingual projection for part-   of - speech ( POS ) tagging relies on the use of   parallel data to project POS tags from a source   language for which a POS tagger is available   onto a target language across word - level align-   ments . The projected tags then form the ba-   sis for learning a POS model for the target   language . However , languages with rich mor-   phology often yield sparse word alignments   because words corresponding to the same ci-   tation form do not align well . We hypoth-   esize that for morphologically complex lan-   guages , it is more efﬁcient to use the stem   rather than the word as the core unit of abstrac-   tion . Our contributions are : 1 ) we propose   an unsupervised stem - based cross - lingual ap-   proach for POS tagging for low - resource lan-   guages of rich morphology ; 2 ) we further in-   vestigate morpheme - level alignment and pro-   jection ; and 3 ) we examine whether the use of   linguistic priors for morphological segmenta-   tion improves POS tagging . We conduct ex-   periments using six source languages and eight   morphologically complex target languages of   diverse typologies . Our results show that the   stem - based approach improves the POS mod-   els for all the target languages , with an average   relative error reduction of 10.3 % in accuracy   per target language , and outperforms the word-   based approach that operates on three - times   more data for about two thirds of the language   pairs we consider . Moreover , we show that   morpheme - level alignment and projection and   the use of linguistic priors for morphological   segmentation further improve POS tagging .   1 Introduction   Low - resource languages lack annotated data even   for basic syntactic information such as parts of   speech ( POS ) . To address this problem , two main   unsupervised approaches have been adopted : zero-   shot model transfer ( Pires et al . , 2019 ) and cross-   lingual POS tagging via alignment and projection(Yarowsky et al . , 2001 ; Fossum and Abney , 2005 ;   Das and Petrov , 2011 ; Duong et al . , 2013 ; Täck-   ström et al . , 2013 ; Agi ´ c et al . , 2015 , 2016 ; Buys   and Botha , 2016 ; Eskander et al . , 2020b ) . Eskander   et al . ( 2020b ) show that the alignment and projec-   tion approach is less sensitive to the morphological   dissimilarities between the source and target lan-   guages than zero - shot model transfer .   In annotation projection , the word structure in   the source and target languages impacts the quality   of the alignment and projection phases , and hence   affects the overall performance of the ultimate POS   model . This becomes a concern for languages with   rich word structure where afﬁxation is common as   they usually suffer from sparse alignment models   that often fail to align words corresponding to the   same citation form due to the extensive paradigms   and translation inconsistencies . Sparse alignment   hinders the ability of a system to project the tags   properly and results in null tags on the target side .   These null tags then reduce the number of qualify-   ing training examples and impact the POS model   by introducing non - continuous labeled sequences .   Adding to these practical issues , the concept of   word as a unit of structure has long been questioned   in language sciences ( Marantz , 2001 ) .   We hypothesize that using the stem as the core   unit of abstraction results in better POS models for   low - resource languages of rich morphology . Our   contribution is three - fold .   Unsupervised stem - based cross - lingual ap-   proach for POS tagging for morphologically com-   plex low - resource languages , where we use the   stem as the core unit of abstraction . In order to   adapt a fully - unsupervised approach , we use a state-   of - the - art unsupervised morphological segmenter ,   MorphAGram ( Eskander et al . , 2016 , 2020a ) , to   derive the stems and morphemes . We follow the   setup of Eskander et al . ( 2020b ) using the Bible   as the only source of parallel data in order to emu-   late a low - resource scenario . We experiment with4061the same six source languages , namely English ,   Spanish , French , German , Russian and Arabic , but   choose six morphologically complex target lan-   guages , namely Amharic , Basque , Finnish , Indone-   sian , Telugu and Turkish and add two new target   languages , namely Georgian and Kazakh , where   we contribute a small POS - annotated dataset for   the former . We show that the stem - based approach   outperforms the word - based one in 43 language   pairs out of 48 , with an average relative error reduc-   tion of 10.3 % in accuracy per target language , up to   21.0 % in the case of Kazakh . We also show that the   stem - based approach outperforms the word - based   approach which operates on three - times more data   for about two thirds of the experimental pairs .   Morpheme - level alignment and projection ,   which allows for abstracting away from how the   morphemes are combined in the source and target   languages . We test the setup with Arabic as the   source language and show improvements for seven   out of the eight target languages .   Using linguistic priors in morphological segmen-   tation , which results in better segmentation models   towards better alignment and projection . Using   Georgian as a case study , we show that the use of   linguistic priors , in the form of a set of afﬁxes pro-   vided by an expert in the target language , improves   the ultimate POS models .   Finally , we make our code publicly available to   encourage further research .   2 Approach   We perform fully unsupervised cross - lingual POS   tagging via alignment and projection . We follow   the main architecture presented by Eskander et al .   ( 2020b ) ( Section 2.1 ) . A primary difference is that   we harness unsupervised morphological segmenta-   tion to use the stems as the core unit of abstraction   for both alignment and projection ( Section 2.2 ) .   In addition , we experiment with morpheme - level   alignment and projection ( Section2.3 ) and exam-   ine the use of linguistic priors towards better mor-   phological segmentation and POS tagging ( Sec-   tion 2.4 ) . This allows for less sparse alignment   models and denser projections , which in turn pro-   duces larger POS training data of a better quality .   Figure 1 : Unsupervised cross - lingual POS tagging   via alignment and projection   2.1 Word - based Alignment and Projection   Figure 1a shows the pipeline presented by Eskander   et al . ( 2020b ) . The only input to the process is a par-   allel text between the target language and a source   one for which a POS tagger is accessible . First,4062   Figure 2 : An example of alignment and projection from Arabic onto Amharic . The alignment models are   trained on the New Testament . Arabic reads right to left .   the parallel text gets white - space tokenized and is   used to train two word - alignment models ( source-   to - target and target - to - source ) using GIZA++ ( Och   and Ney , 2003 ) . The alignment models are then   applied to align the source and target sides on the   word level . Next , the source side is tagged for POS   using an off - the - shelf tagger ( e.g. , Stanza ( Qi et al . ,   2020 ) ) . The source tags are then projected onto   the target across the word - based alignments , where   only those bidirectional alignments whose conﬁ-   dence is above a particular threshold are considered   in order to eliminate one - to - many and many - to - one   alignments and those alignments of low conﬁdence .   The projected tags for each token represent what   are called token constraints , while the tag distri-   bution of each word type across the whole target   side forms type constraints . The token and type   constraints are then coupled by nullifying those   tag assignments whose type - level probabilities are   below some threshold . The target text , along with   its projected tags , then constitutes the training data   for a neural POS tagger , where only the top scoring   sentences , in terms of tag - assignment density and   alignment conﬁdence , are considered .   The neural tagger is a bidirectional long short-   term memory ( BiLSTM ) model ( Hochreiter and   Schmidhuber , 1997 ) that uses a custom softmax ac-   tivation to handle the null tags . It uses word embed-   dings , both randomly initialized and the contextual   multilingual embeddings XLM - R ( Conneau et al . ,   2019 ) ; preﬁx and sufﬁx n - gram character embed-   dings , where nis in{1,2,3,4 } ; and hierarchical   Brown - cluster ( Brown et al . , 1992 ) embeddings . The architecture can beneﬁt from parallel data of   multiple source languages , where either the projec-   tions from multiple source languages ( Mul _ proj )   or the decoded outputs that are based on multiple   single - source models ( Mul _ out ) can be combined   through maximum - voting mechanisms .   2.2 Stem - Based Alignment and Projection   While the architecture by Eskander et al . ( 2020b )   yields the state - of - the - art results for unsupervised   POS tagging when evaluated on 12 languages of di-   verse typologies , the complexity of word structure   in the source and target languages has a direct im-   pact on the quality of both alignment and projection .   Rich word structure where afﬁxation is common   increases the ratio of word types to word tokens ,   which in turn results in sparse alignment models   and incomplete projections that form null tags on   the target side . Null tags result in a score that is too   low for the underlying sentence to qualify as a train-   ing example and introduce missing information for   the training of the POS model , which negatively   impacts the overall quality of POS tagging .   An example is shown in Figure 2a , where Arabic   and Amharic are the source and target languages ,   respectively . The example corresponds to verse   MAT 15:35 , “ He commanded the multitude to sit   down on the ground ” , where the word - alignment   models are trained on the New Testament . As   shown , the two Arabic - Amharic pairs { MÈ¥m , } ( and the people , the people ) and{€ÈÈ,}(he commanded , then he commanded ) are   not aligned , resulting in null tags . The sparse word-4063   Table 1 : Paired inﬂected forms that correspond to   the same citation form across Arabic and Amharic   parallel verses in the New Testament   alignment models are simply unable to properly   align words that correspond to the same citation   form because of the extensive paradigms , which ,   along with translation inconsistencies , leads to the   loss of the one - to - one correspondence between   word structures across parallel texts ( examples are   shown in Table 1 ) . Using the stem instead of the   word as the core unit of abstraction is more produc-   tive ; the stem is usually shared by all the members   of a paradigm , which reduces misalignment .   Figure 2b shows that stemming the Arabic and   Amharic texts yields complete one - to - one align-   ments and projections , which in turn eliminates   the word - based null assignments and assigns each   word on the Amharic side a valid POS tag .   Figure 1b illustrates our overall stem - based ap-   proach . We ﬁrst stem the source and target sides   and train two stem - level alignment models , one in   each direction . Next , we assign the stems of the   source side the POS tags of their corresponding   words , which are then projected onto the target   stems through the stem - level alignments . We then   apply the token and type constraints on the labeled   stems on the target side . However , since we train   the ultimate POS model on the word level , we re-   place each target stem by its corresponding word   and assign that word the stem - based projected POS   tag . The rest of the pipeline for sentence selection   and training the POS model are the same as in the   word - based architecture described in Section 2.1 .   We assume that the source language is a high-   resource one for which an off - the - shelf stemmer   is accessible . On the other hand , for the tar-   get languages , we use MorphAGram(Eskander   et al . , 2020a ) to train an unsupervised morpho-   logical segmentation model using the target side   of the parallel text . MorphAGram is a state-   of - the - art framework for unsupervised morpho - logical segmentation based on Adaptor Gram-   mars ( AGs ) ( Johnson et al . , 2007 ) , nonparamet-   ric Bayesian models that generalize Probabilistic   Context Free Grammars ( PCFGs ) . We run Mor-   phAGram in a cascaded setup of two learning   rounds . In the ﬁrst round , we train a segmen-   tation model using a language - independent high-   precision grammar ( PrStSu2a+SM ) to obtain a list   of morphemes . We then seed these morphemes into   the best performing language - independent gram-   mar ( PrStSu+SM ) for the second round of learning   as described by Eskander et al . ( 2016 , 2020a ) . Both   PrStSu2a+SM and PrStSu+SM grammars model   the word as a sequence of preﬁxes , a stem and suf-   ﬁxes , where the afﬁxes are recursively deﬁned in   order to model multiple consecutive items .   2.3 Morpheme - Based Alignment and   Projection   Next , we perform morpheme - based alignment and   projection in a similar fashion as in the stem - based   approach ( Section 2.2 ) . This approach abstracts   away from whether the morphemes in the source   and target languages are free - standing or not .   On the source side , each morpheme receives a   separate POS tag using an off - the - shelf POS tag-   ger . These tags are then projected onto the target   morphemes through bidirectional morpheme - level   alignments . We obtain the target morphemes using   MorphAGram , where the output of the PrStSu+SM   grammar yields preﬁxes , a stem , and sufﬁxes for   each word . However , since we train the POS model   on the word level , we replace each sequence of   morphemes on the target side by its corresponding   word and assign that word the POS tag of the repre-   sentative morpheme . We deﬁne the representative   morpheme either as the morpheme whose POS tag   ranks the highest among those of the other mor-   phemes ( ) or as the stem morpheme ( ) .   2.4 Using Linguistic Priors for Segmentation   We hypothesize that better detection of stems yields   more robust alignment and projection towards im-   proved POS tagging . Accordingly , instead of con-   ducting morphological segmentation on the target   side in a fully unsupervised manner , we follow Es-   kander et al . ( 2021 ) by seeding afﬁx morphemes   into the grammar tree prior to training the segmen-   tation model textcolorbluein a minimally super-4064vised fashion ; these afﬁxes are generated manually   by an expert in the target language . With Georgian   as a case study , we examine this setup in the stem-   based approach using the PrStSu+SM grammar .   2.5 Featurizing Segmented Data   In this setup , we utilize the unsupervised   morphological - segmentation model that is trained   on the target side of the parallel text to produce   stem , complex - preﬁx and complex - sufﬁx features ,   and leverage these features as part of the neural   POS model . For training , we use these features as   randomly initialized embeddings that we concate-   nate with the existing word , afﬁx and words - cluster   embeddings prior to applying the BiLSTM encod-   ing layer .   3 Experiments and Evaluation   3.1 Languages and Data   We conduct our experiments on six source lan-   guages and eight target ones , for a total of 48   language pairs . We use the same source lan-   guages used by Eskander et al . ( 2020b ) , namely   English , Spanish , French , German , Russian , and   Arabic , and experiment with eight typologically   diverse target languages : six morphologically rich   languages that are largely agglutinative , namely   Basque , Finnish , Georgian , Kazakh , Telugu , and   Turkish ; morphologically rich Amharic , where   many morphological alterations rely on consonan-   tal roots ; and less morphologically rich Indonesian .   We conduct the experiments in a truly low-   resource scenario , where we use the New Testa-   ment as the source of our parallel data ( unless noted   otherwise ): limited in size and out - of - domain with   respect to the evaluation sets . We use the Multilin-   gual Parallel Bible Corpus(Christodouloupoulos   and Steedman , 2015 ) as the source of data for all   the languages , except for Georgian and Kazakh .   3.2 Experimental Settings   For the tagging of the source languages , we use   the same off - the - shelf taggers as in Eskanderet al . ( 2020b ): Stanza(Qi et al . , 2020 ) for En-   glish , Spanish , French , German and Russian and   MADAMIRA ( Pasha et al . , 2014 ) for Arabic . We   also use MADAMIRA for Arabic morphologi-   cal segmentation . For the stemming of the other   source languages , we use the Snowball Stemmer   ( Porter , 2001 ) as part of NLTK(Bird and Loper ,   2004 ) . On the other hand , we use MorphAGram   ( Eskander et al . , 2020a ) to train and apply the   morphological - segmentation models for the target   languages as described in Section 2.2 .   We follow Eskander et al . ( 2020b ) by using   the same thresholds for alignment and projection ,   along with the same neural hyperparameters of the   POS tagger . We also evaluate our models in terms   of POS accuracy on the same Universal Dependen-   cies ( UD ) v2.5 ( Zeman et al . , 2019 ) test datasets .   However , since the UD project does not currently   contain Georgian datasets , we developed a small   POS dataset for Georgian ( 100 sentences ) follow-   ing the UD - tagging scheme . The sentences are   taken from the Modern Georgian and Political texts   sub - corpora of the Georgian National Corpus ,   and they are hand - tagged and carefully revised by   a linguist who specializes in and speaks Georgian   as a second language . Finally , all the results are   averaged over three runs .   3.3 System Performance   Table 2 reports the POS accuracy for the baseline   word - based approach and our stem - based approach   for all the 48 target - source language pairs using the   New Testament as the source of parallel data . In   addition , we report the results for the two multi-   lingual setups Mul _ outandMul _ proj per target   language . The stem - based approach outperforms   the word - based one in 43 language pairs and all the   multilingual setups except Mul _ proj in the case   of Indonesian , which stands out in our language   sample as the least complex in terms of morphol-   ogy . The biggest improvement in the stem - based   approach is achieved in the cases of Russian →   Turkish , Russian →Kazakh , Spanish →Kazakh   and Arabic →Georgian , with relative error reduc-   tions of 33.8 % , 30.2 % , 28.6 % and 27.2 % , respec-   tively . When averaging across the sources ( includ-4065   Table 2 : POS - tagging performance ( accuracy ) of the word - based and stem - based approaches when using   the New Testament as the source of parallel data . The best result per target - source pair is in bold . The   highest relative error reduction in the stem - based approach per target language is marked by * . The   stem - based improvements that are not statistically signiﬁcant for p - value < 0.01are between parentheses .   Table 3 : POS - tagging performance ( accuracy ) of   the word - based , stem - based and morpheme - based   approaches when projecting from Arabic using   the New Testament as the source of parallel data .   The best result per target language is in bold . All   the morpheme - based improvements are statistically   signiﬁcant for p - value < 0.01 .   ing the multilingual ones ) , Kazakh , Telugu and   Turkish experience the highest relative error reduc-   tions of 21.0 % , 12.1 % and 12.1 % , respectively . On   the other hand , Russian and Arabic yield the high-   est relative error reductions of 16.3 % and 15.6 % ,   respectively , when averaging across the target lan-   guages , which is in line with the morphological   complexity of the two languages .   Eskander et al . ( 2020b ) show that related lan - guages transfer best across each other . This results   in efﬁcient word - based baselines for related lan-   guage pairs , which in turn limits the corresponding   gains in the stem - based approach . On the other   hand , a low word - based baseline makes room for   improvement when operating on the stem level .   For instance , both Georgian and Telugu witness the   highest stem - based gains when transferring from   Arabic , their lowest performing source language in   the word - based approach . For more information   about the correlation between language relatedness   and cross - lingual learning , see Eskander ( 2021 ) .   Next , we evaluate the morpheme - based approach   ( Section 2.3 ) . Table 3 reports the performance of   the word - based , stem - based and morpheme - based   approaches using Arabic as the source language   since it is more morphologically complex than the   other sources . The morpheme - based approach re-   sults in dense training instances as both alignment   and projection are performed in a more ﬁne - grained   level compared to the word - based and stem - based   approaches . It therefore improves POS tagging   for all the target languages except Amharic , where   Telugu beneﬁts the most with relative error reduc-   tions of 23.9 % and 15.3 % over the stem - based ap-   proach using the and mechanisms ,   respectively . The difference in the performance of   the and mechanisms is only statisti-4066   Table 4 : POS - tagging performance ( accuracy ) of the word - based and stem - based ( with and without   linguistic priors ( LP ) ) approaches when using the New Testament as the source of parallel data . The   best result per source language is in bold . The improvements in the LP stem - based approach that are not   statistically signiﬁcant for p - value < 0.01are between parentheses .   cally signiﬁcant for p - value < 0.01 in the cases   of Amharic and Basque , where the mecha-   nism yields better performance , and in the case of   Telugu , where the mechanism is superior .   Finally , we hypothesize that the quality of morpho-   logical segmentation highly affects the efﬁciency   of the morpheme - based setups , which explains the   variation in the performance across the different   target languages and mechanisms .   As mentioned earlier , we use Georgian as a case   study to examine the impact of using linguistic pri-   ors for morphological segmentation on the quality   of POS tagging ( Section 2.4 ) . The results are listed   in Table 4 . The use of linguistic priors improves   the stem - based approach except when projecting   from Arabic . The lack of improvement in the case   of Arabic can be explained by over - segmentation   that produces incorrect POS tags for the common   conjunction da(and ) . The characters daalso cor-   respond to a verbal preﬁx that is manually seeded   as a prior . This seeding causes erroneous projec-   tions labeling daas a verb or an adverb when   projecting from Arabic .   Finally , we experiment with the use of the stem   and afﬁx information as training features in the   POS neural model ( Section 2.5 ) . However , most of   the improvements due to the use of these features   are not statistically signiﬁcant ( See Appendix A for   full results ) since such features are surpassed by   the preﬁx and sufﬁx n - gram character embeddings .   3.4 Analysis of the Stem - Based Approach   Upon alignment and projection , the highest scoring   target sentences are selected as training examples ,   where sentence score is deﬁned as the harmonic   mean of the percentage of tokens with projected   tags and the average alignment probability of those   tokens . The ﬁne - grained stem - level alignments   allow for better alignment conﬁdence and more   dense sentences , which in turn increases sentencescores and the number of training examples , and   hence reduces the number of out - of - vocabulary   words ( OOVs ) . Table 5 lists the average number   of training examples , average relative increase in   the number of training examples , average relative   increase in sentence scores and average relative   decrease in the number of OOVs for each target   language in the stem - based approach with respect   to the word - based one . We witness improvements   in the examined aspects for each target language ,   which explains the considerable improvements in   the stem - based approach .   Next , we examine the average relative error re-   duction in the detection of open - class tags ( nouns ,   verbs and adjectives ) in the stem - based approach   as compared to the word - based one per target lan-   guage ( Table 6 ) and per source language ( Table 7 ) .   Kazakh beneﬁts the most from the stem - based ap-   proach at the detection of nouns and adjectives ,   while Amharic receives the highest gains for verbs .   On the other hand , projecting from Russian in the   stem space achieves the highest gains for nouns ,   while the stem - based projection from Arabic yields   the highest gains for both verbs and adjectives .   Finally , Figure 3 illustrates the absolute improve-   ments in POS tagging when applying the stem-   based approach using the New - Testament as the   source of parallel data compared to the word - based   approach using the entire Bible as the source of par-   allel data ( three - times more data ) . As illustrated ,   the stem - based approach achieves better perfor-   mance in about two thirds of the language pairs   with an average absolute gain of 1.7 % . This means   using the stem as the core unit of abstraction com-   pensates for the lack of adequate parallel data .   4 Related Work   The line of work most closely related to ours is   unsupervised cross - lingual POS tagging via align-   ment and projection , which was ﬁrst introduced4067   Table 5 : Average number of training examples , average relative increase in training examples , average   relative increase in sentence scores and average relative decrease in OOVs per target language in the   stem - based approach w.r.t . to the word - based one   Table 6 : Average relative error reductions for the   detection open - class tags per target language   Table 7 : Average relative error reductions for the   detection of open - class tags per source language   by Yarowsky et al . ( 2001 ) . They applied noise-   reduction techniques to improve the alignments   and used the resulting transition and emission prob-   abilities to deﬁne an HMM POS tagger .   Exploiting multiple source languages via max-   imum voting was then explored by Fossum and   Abney ( 2005 ) , by voting among the outputs of   different single - source models , and by Agi ´ c et al .   ( 2015 ) , by projecting the annotations from multiple   languages before training the POS tagger .   In order to increase the size of the training data ,   Das and Petrov ( 2011 ) proposed graph - based label   propagation , while Duong et al . ( 2013 ) ; Agi ´ c et al .   ( 2015 ) applied self - training and revision . On an - other hand , Täckström et al . ( 2013 ) and Buys and   Botha ( 2016 ) investigated the use of token and type   constraints to reject projections of low conﬁdence .   Eskander et al . ( 2020b ) derived a cross - lingual   POS - tagging pipeline that utilizes the best prac-   tices in alignment and projection . In addition , they   examined the use of pretrained multilingual contex-   tual embeddings , along with afﬁx embeddings and   Brown clusters , within a rich neural architecture ,   which achieves the state - of - the - art results for unsu-   pervised POS tagging . We follow their approach by   presenting stem - based alignment and projection for   morphologically complex low - resource languages .   Regarding unsupervised morphological segmen-   tation , several generative and discriminative frame-   works have been developed over the last two   decades . The two most notable frameworks are :   1 ) Morfessor ( Creutz and Lagus , 2007 ; Grönroos   et al . , 2014 ) , a commonly - used HMM framework   that utilizes the MDL principle to segment into   morphemes of a hierarchical structure ; and 2 ) Mor-   phAGram ( Eskander et al . , 2020a ) , a segmenta-   tion framework that is based on Adaptor Gram-   mars ( AGs ) ( Johnson et al . , 2007 ) , Bayesian mod-   els that utilize Probabilistic Context Free Gram-   mars ( PCFGs ) . We use MorphAGram to train   morphological - segmentation models as it achieves   the state - of - the - art performance and allows for de-   riving afﬁx and stem information ( as opposed to a   sequence of unlabeled morphemes ) .   5 Conclusion and Future Work   We presented a fully unsupervised stem - based ap-   proach for cross - lingual POS tagging via alignment   and projection , where we use the stem as the core   unit of abstraction to abstract away from complex   afﬁxation . Our experiments using six source lan-   guages and eight morphologically rich target lan-4068Figure 3 : Absolute performance increases ( accuracy ) when applying the stem - based approach using the   New Testament as the source of parallel data as compared to the word - based approach using the entire   Bible as the source of parallel data   guages in low - resource setups show improvements   over the word - based approach in 43 language pairs   out of 48 , with an average relative error reduction   of 10.3 % in accuracy per target language . In addi-   tion , we examined morpheme - based alignment and   projection and the use of linguistic priors in mor-   phological segmentation , which further improve   POS tagging .   In the future , we plan to study the role of mor-   phological typology in cross - lingual learning . This   allows for deriving disciplined guidelines for the   selection of an appropriate source language that   transfers well to the target language of interest .   Acknowledgements   This research is based upon work supported by the   Intelligence Advanced Research Projects Activity   ( IARPA ) , ( contract # FA8650 - 17 - C-9117 ) and the   National Science Foundation ( awards # 1941742   and # 1941733 ) . The views and conclusions herein   are those of the authors and should not be inter-   preted as necessarily representing ofﬁcial policies ,   expressed or implied , of ODNI , IARPA , NSF or   the U.S. Government . The U.S. Government is   authorized to reproduce and distribute reprints for   governmental purposes notwithstanding any copy-   right annotation therein.6 Ethical Considerations   The Georgian annotations were done by a lin-   guist with appropriate compensation after educat-   ing them about the research purpose and the annota-   tion process . We claim ownership of the Georgian   dataset for open distribution as the text is taken   from the Modern Georgian and Political texts sub-   corpora of the Georgian National Corpus , which is   an open - sourced work that allows for modiﬁcations ,   derived work and redistribution . The quality of the   annotations was examined manually and empiri-   cally . The source code and the data will be released   open - source . Finally , the limitations of the work   lay within the reported performance . There should   be no potential risks given these stated limitations .   References   Željko Agi ´ c , Dirk Hovy , and Anders Søgaard . 2015 .   If all you have is a bit of the bible : Learning pos   taggers for truly low - resource languages . In Pro-   ceedings of the 53rd Annual Meeting of the As-   sociation for Computational Linguistics and the   7th International Joint Conference on Natural   Language Processing ( Volume 2 : Short Papers ) ,   pages 268–272.4069Željko Agi ´ c , Anders Johannsen , Barbara Plank ,   Héctor Martínez Alonso , Natalie Schluter , and   Anders Søgaard . 2016 . Multilingual projection   for parsing truly low - resource languages . Trans-   actions of the Association for Computational   Linguistics , 4:301–312 .   Steven Bird and Edward Loper . 2004 . NLTK : The   natural language toolkit . In Proceedings of the   ACL Interactive Poster and Demonstration Ses-   sions , pages 214–217 , Barcelona , Spain . Associ-   ation for Computational Linguistics .   Peter F Brown , Peter V Desouza , Robert L Mercer ,   Vincent J Della Pietra , and Jenifer C Lai . 1992 .   Class - based n - gram models of natural language .   Computational linguistics , 18(4):467–479 .   Jan Buys and Jan A. Botha . 2016 . Cross - lingual   morphological tagging for low - resource lan-   guages . In Proceedings of the 54th Annual Meet-   ing of the Association for Computational Lin-   guistics ( Volume 1 : Long Papers ) , pages 1954 –   1964 , Berlin , Germany .   Christos Christodouloupoulos and Mark Steedman .   2015 . A massively parallel corpus : the bible in   100 languages . Language resources and evalua-   tion , 49(2):375–395 .   Alexis Conneau , Kartikay Khandelwal , Naman   Goyal , Vishrav Chaudhary , Guillaume Wenzek ,   Francisco Guzmán , Edouard Grave , Myle Ott ,   Luke Zettlemoyer , and Veselin Stoyanov . 2019 .   Unsupervised cross - lingual representation learn-   ing at scale . arXiv preprint arXiv:1911.02116 .   Mathias Creutz and Krista Lagus . 2007 . Unsu-   pervised models for morpheme segmentation   and morphology learning . ACM Transactions   on Speech and Language Processing ( TSLP ) ,   4(1):1–34 .   Dipanjan Das and Slav Petrov . 2011 . Unsu-   pervised part - of - speech tagging with bilingual   graph - based projections . In Proceedings of   the 49th Annual Meeting of the Association for   Computational Linguistics : Human Language   Technologies - Volume 1 , pages 600–609 . Associ-   ation for Computational Linguistics .   Long Duong , Paul Cook , Steven Bird , and Pavel   Pecina . 2013 . Simpler unsupervised pos tagging   with bilingual projections . In Proceedings of the51st Annual Meeting of the Association for Com-   putational Linguistics ( Volume 2 : Short Papers ) ,   pages 634–639 .   Ramy Eskander . 2021 . Unsupervised Morpholog-   ical Segmentation and Part - of - Speech Tagging   for Low - Resource Scenarios . Columbia Univer-   sity .   Ramy Eskander , Francesca Callejas , Elizabeth   Nichols , Judith L Klavans , and Smaranda Mure-   san . 2020a . Morphagram , evaluation and frame-   work for unsupervised morphological segmen-   tation . In Proceedings of The 12th Language   Resources and Evaluation Conference , pages   7112–7122 .   Ramy Eskander , Cass Lowry , Sujay Khanda-   gale , Francesca Callejas , Judith L Klavans ,   Maria Polinsky , and Smaranda Muresan . 2021 .   Minimally - supervised morphological segmen-   tation using adaptor grammars with linguistic   priors . In Findings of the Association for Compu-   tational Linguistics : ACL - IJCNLP 2021 , pages   3969–3974 .   Ramy Eskander , Smaranda Muresan , and Michael   Collins . 2020b . Unsupervised cross - lingual part-   of - speech tagging for truly low - resource scenar-   ios . In Proceedings of the 2020 Conference on   Empirical Methods in Natural Language Pro-   cessing ( EMNLP ) , pages 4820–4831 .   Ramy Eskander , Owen Rambow , and Tianchun   Yang . 2016 . Extending the use of adaptor gram-   mars for unsupervised morphological segmenta-   tion of unseen languages . In Proceedings of he   Twenty - Sixth International Conference on Com-   putational Linguistics ( COLING ) , Osaka , Japan .   Victoria Fossum and Steven Abney . 2005 . Automat-   ically inducing a part - of - speech tagger by pro-   jecting from multiple source languages across   aligned corpora . In International Conference on   Natural Language Processing , pages 862–873 .   Springer .   Stig - Arne Grönroos , Sami Virpioja , Peter Smit ,   and Mikko Kurimo . 2014 . Morfessor FlatCat :   An HMM - based method for unsupervised and   semi - supervised learning of morphology . In Pro-   ceedings of the 2014 International Conference   on Computational Linguistics ( COLING ) , pages   1177–1185.4070Sepp Hochreiter and Jürgen Schmidhuber . 1997 .   Long short - term memory . Neural computation ,   9(8):1735–1780 .   Mark Johnson , Thomas L Grifﬁths , Sharon Gold-   water , et al . 2007 . Adaptor grammars : A frame-   work for specifying compositional nonparamet-   ric bayesian models . Advances in neural infor-   mation processing systems , 19:641 .   Alec Marantz . 2001 . Words and things . handout ,   MIT .   Franz Josef Och and Hermann Ney . 2003 . A   systematic comparison of various statistical   alignment models . Computational linguistics ,   29(1):19–51 .   Arfath Pasha , Mohamed Al - Badrashiny , Mona T   Diab , Ahmed El Kholy , Ramy Eskander , Nizar   Habash , Manoj Pooleery , Owen Rambow , and   Ryan Roth . 2014 . Madamira : A fast , comprehen-   sive tool for morphological analysis and disam-   biguation of arabic . In LREC , volume 14 , pages   1094–1101 .   Telmo Pires , Eva Schlinger , and Dan Garrette .   2019 . How multilingual is multilingual BERT ?   InProceedings of the 57th Annual Meeting of   the Association for Computational Linguistics ,   pages 4996–5001 , Florence , Italy . Association   for Computational Linguistics .   Martin F Porter . 2001 . Snowball : A language for   stemming algorithms .   Peng Qi , Yuhao Zhang , Yuhui Zhang , Jason Bolton ,   and Christopher D Manning . 2020 . Stanza :   A python natural language processing toolkit   for many human languages . arXiv preprint   arXiv:2003.07082 .   Oscar Täckström , Dipanjan Das , Slav Petrov , Ryan   McDonald , and Joakim Nivre . 2013 . Token and   type constraints for cross - lingual part - of - speech   tagging . Transactions of the Association for   Computational Linguistics , 1:1–12 .   David Yarowsky , Grace Ngai , and Richard Wicen-   towski . 2001 . Inducing multilingual text anal-   ysis tools via robust projection across aligned   corpora . In Proceedings of the ﬁrst international   conference on Human language technology re-   search , pages 1–8 . Association for Computa-   tional Linguistics . Daniel Zeman , Joakim Nivre , Mitchell Abrams ,   and et al . 2019 . Universal dependencies 2.5 .   LINDAT / CLARIAH - CZ digital library at the   Institute of Formal and Applied Linguistics   ( ÚFAL ) , Faculty of Mathematics and Physics ,   Charles University.4071A Appendix : Segmentation Information   as Training Features   Table 2 shows the POS tagging results ( accuracy )   when using the stem and afﬁx information as train-   ing features in the neural POS model , as described   in Subsection 2.5 .   Table 8 : Performance with segmentation features   B Appendix : Hardware   We use a Google - Cloud virtual instance of 48   2.00GHz cores and 240 GB of RAM to run all of   our experiments . The training rate is nearly 2,500   sentences per hour.4072