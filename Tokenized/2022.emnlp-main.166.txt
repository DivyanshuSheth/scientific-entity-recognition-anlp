∗   Weiwen Xu , Yang Deng , Wenqiang Lei , Wenlong Zhao , Tat - Seng Chua , and Wai LamThe Chinese University of Hong KongSichuan UniversityNational University of Singapore   { wwxu,ydeng,wlam}@se.cuhk.edu.hk   { wenqianglei,wenlzhao}@gmail.com , chuats@comp.nus.edu.sg   Abstract   We study automatic Contract Clause   Extraction ( CCE ) by modeling implicit   relations in legal contracts . Existing CCE   methods mostly treat contracts as plain text ,   creating a substantial barrier to understanding   contracts of high complexity . In this work , we   first comprehensively analyze the complexity   issues of contracts and distill out three implicit   relations commonly found in contracts ,   namely , 1 ) Long - range Context Relation that   captures the correlations of distant clauses ;   2)Term - Definition Relation that captures the   relation between important terms with their   corresponding definitions ; and 3 ) Similar   Clause Relation that captures the similarities   between clauses of the same type . Then we   propose a novel framework ConReader to   exploit the above three relations for better   contract understanding and improving CCE .   Experimental results show that ConReader   makes the prediction more interpretable and   achieves new state - of - the - art on two CCE tasks   in both conventional and zero - shot settings .   1 Introduction   Legal Contract Review is a process of thoroughly   examining a legal contract before it is signed to en-   sure that the content stated in the contract is clear ,   accurate , complete and free from risks . A key com-   ponent to this application is the Contract Clause Ex-   traction ( CCE ) , which aims to identify key clauses   from the contract for further in - depth review and   risk assessment . Typically , CCE consists of two   major tasks targeting different query granularities   for real - life usages . They are Clause Analysis ( CA )   andClause Discovery ( CD ) , where CA aims toidentify clauses that belong to a general clause type ,   while CD aims to identify clauses similar to a spe-   cific clause ( depicted in Figure 1 ) . CCE is both   expensive and time - consuming as it requires legal   professionals to manually identify a small num-   ber of key clauses from contracts with hundreds of   pages in length ( Hendrycks et al . , 2021 ) . Therefore ,   there is a pressing need for automating CCE , which   assists legal professionals to analyze long and te-   dious documents and provides non - professionals   with immediate legal guidance .   The biggest challenge to automating CCE is the   complexities of contracts . In the literature , sim-   ply treating contracts as plain text , most pretrained   language models perform poorly on CCE ( Devlin   et al . , 2019 ; Liu et al . , 2019 ) . Some works try   to simplify CCE from the perspective of contract   structure . For example , Chalkidis et al . ( 2017 ) as-   sign a fixed extraction zone for each clause type   and limit the clauses to be extracted only from their   corresponding extraction zones . Hegel et al . ( 2021 )   use visual cues of document layout and placement   as additional features to understand contracts . How-   ever , their local context assumption is not flexible   and , more seriously , neglects more complicated   relations inherent in the contracts .   In fact , as shown in Figure 1 , contracts are   formal documents that typically follow a semi-   structured organization . The body of a contract   is usually organized into some predefined articles   such as " Definitions " and " Terminations " , where   relevant clauses are orderly described inside . Dif-   ferent articles may hold different levels of impor-   tance . For example , the " Definitions " article is   globally important because it clearly defines all im-   portant terms that would be frequently referenced ,   while other articles are sparsely correlated , hold-   ing local importance . We attempt to decompose   the complexities into a set of implicit relations ,   which can be exploited to better understand con-   tracts . Therefore , as shown in Figure 1 , we identify2581   three implicit relations to directly tackle the com-   plexities from three aspects :   1)The implicit logical structure among distant   text : This is originated from the fact that a clause   from one article may refer to clauses from distant   articles . However , most pretrained language mod-   els ( e.g. BERT ) inevitably break the correlations   among clauses because they have to split a contract   into multiple segments for separate encoding due   to the length limitation . Therefore , we define a   Long - range Context Relation ( LCR ) to capture   the relations between different segments to keep   the correlations among clauses .   2)The unclear legal terms : Legal terms need to   be clearly and precisely declared to minimize am-   biguity . Thanks to the " Definition " article , we can   easily find the meaning of a particular term . Then   the relation between each term and its definition is   defined as Term - Definition Relation ( TDR ) . The   clarity of TDR allows consistent information flow   by enhancing terms with semantics - rich definitions ;   3)The ambiguity among clauses : It is usually   hard to differentiate different types of clauses just   from their text formats . For example , clauses of   type " Expiration Date " and " Agreement Date " both   show up as dates . It leads to the third relation   defined as Similar Clause Relation ( SCR ) . SCR   captures the similarity of the same type of clauses   across contracts . It enhances a clause ’s semantics   with its unique type information and thus maintains   the discrimination among different clause types .   Furthermore , LCR and TDR are two intra - contract   relations while SCR is an inter - contract relation .   In light of the above investigations about thecomplexities of contracts , we propose a novel   framework , ConReader , to tackle two CCE tasks   by exploiting the above three relations for better   contract understanding . Concretely , we reserve a   small number of token slots in the input segments   for later storage of the three kinds of relational   information . To prepare intra - contract relations ,   including LCR and TDR , we get the segment and   definition representations from pretrained language   models . Regarding the inter - contract relation , i.e.   SCR , since the size of SCR increases as the number   of contracts increases , we are unable to enumerate   all possible SCRs . Therefore , we enable input seg-   ments to interact with a Clause Memory that stores   recently visited clauses , where a clause retriever is   adopted to retrieve similar clauses from the Clause   Memory . Then , we enrich each segment by filling   the reserved slots with context segments , relevant   definitions , as well as retrieved similar clauses . Fi-   nally , a fusion layer is employed to simultaneously   learn relevant information both from the local ( i.e.   within the segment ) or global context ( i.e. via im-   plicit relations ) for extracting the target clause .   To summarize , our main contributions are three-   fold :   •This work targets automatic CCE . We compre-   hensively analyze the complexity issues of mod-   eling legal contracts and distill out three implicit   relations , which have hardly been discussed be-   fore .   •We propose a novel framework ConReader to   effectively exploit the three relations . It enables a   more flexible relations modeling and reduces the   difficulties in understanding contracts for better2582CCE .   •Experimental results on two CCE tasks , namely   CA and CD , show considerable improvements in   both performance and interpretability .   2 Framework   Overview We describe the problem definition for   CCE via extractive Question Answering ( QA ) ( Ra-   jpurkar et al . , 2016 ) . Let { c}be a contract   in the form of multiple segments and qbe a query   either represented as a clause type in the CA task   or a specific clause in the CD task . Our goal is to   extract clauses { y}corresponding to the query .   There may be multiple or no correct clauses and   each clause is a text span in a particular segment   denoted by its start and end index if existent .   Figure 2 depicts the overview of ConReader ,   which consists of four main components :   •LCR Solver tackles LCR by encoding the   wrapped segments { x}aware of the query   qand the reserved slots rinto hidden states   { h } , where the overall segment represen-   tations are stored in a segment bucket B.   •TDR Solver tackles TDR by encoding all def-   initions { d}from the contract into hidden   states{h } , where the overall definition rep-   resentations are stored in a definition bucket B.   •SCR Solver tackles SCR by retrieving similar   clause representations { /hatwidesth}from a Clause   Memory Maccording to a similarity function   f(·,·)between the segment and the stored clause .   •Aggregator enriches each segment representation   with the three relational information for extract-   ing the target clause .   2.1 Long - range Context Relation Solver   The goal of LCR Solver is to output all segment rep-   resentations in a contract in the face of the length   limitation of pretrained language models . Mean-   while , to allow a flexible relation modeling in later   Aggregator , we reserve some token slots for later   storage of relational information before encoding .   Specifically , we concatenate each segment with   the query and the reserved token slots to form the   input sequence within the length limitation :   where [ · ; · ] denotes the sequential concatenation ,   [ CLS],[SEP]are special tokens at the beginning or   in the middle of the two text . Note that the reserved   token slots rare occupied with placeholders and   only take a small portion of the entire sequence   ( |r|<<512 ) such that they only slightly affect the   efficiency . It does not matter which token is chosen   as the placeholder since we would directly mask   these slots such that they will not affect the hidden   states of query and segment tokens as well as not   receive gradient for update .   Then , we apply a RoBERTa encoder Enc ( · )   to get the hidden states for all input sequences :   h = Enc(x ) , where h∈R , andhis   the hidden dimension . To reflect the order of differ-   ent segments in a contract , we also add a segment   positional embedding ( Vaswani et al . , 2017 ) to the   hidden state h at[CLS]to get the segment rep-   resentation for each input segment :   where Pos(·)is a standard RoBERTa positional en-   coder . All segment representations are temporarily   stored in a segment bucket B={/hatwidesth } .   2.2 Term - Definition Relation Solver   TDR Solver is responsible for providing the spe-   cific definitions for terms that may raise ambiguity.2583Algorithm 1 : SCR Solver ( training )   It can be observed in Figure 1 that definitions are   well organized in the “ Definition " article . There-   fore , we use regular expressions including some   keywords like “ shall mean " , “ mean " to automati-   cally extract those definitions . Then , we prepare   the definition inputs as :   where each definition is presented in the form of   key - value pair . Each key kdenotes a legal term   in the contract and the value vdenotes its corre-   sponding definition text . Then we apply the same   RoBERTa encoder to encode these definitions into   hidden states h , where the hidden states h   at[CLS]are denoted as definition representations   { /hatwidesth } , which are temporarily stored in another   definition bucket B.   2.3 Similar Clause Relation Solver   Since SCR is an inter - contract relation , we are un-   likely to enumerate all possible clause pairs . There-   fore , we maintain a Clause Memory Mto : ( 1 ) dy-   namically store clauses of all types ; and ( 2 ) allow   input segments to retrieve similar clauses accord-   ing to a similarity function f ( · , · ) . Details can be   found in Algorithm 1 .   Dynamic Update of M During training , we as-   sume each query qimplies a particular clause type   l(the query of CA itself is a clause type , while the   query of CD belongs to a clause type ) , where we   haveLclause types in total . Initially , Mallocates   the same memory space of size |M| for each clausetype to store the corresponding clause representa-   tions . Suppose that we get hfrom LCR Solver   forxand there is a clause yof type lcorrespond-   ing to the given query qinside x. We denote its   clause representation has the concatenation of its   start and end token representations :   h= [ h : h]∈R(4 )   where [ · : · ] denotes vector concatenation , and   sandeare the start and end index of yinside   x. When encountering such clause , we add hto   its corresponding memory partition M[l ] . If the   memory partition is full , we follow the first - in first-   out ( FIFO ) principle to remove the earliest clause   representation stored in M[l]to make room for   the new one , such that the clause representations   stored are always up - to - date .   Retrieve Clauses from M When asking to iden-   tify clause of type l , we allow each input segment   to retrieve a similar clause from the Clause Mem-   ory . The retrieved clause would imply the semantic   and contextual information of this type of clauses   in other contracts , facilitating the extraction of the   same type of clauses in the current contract .   Specifically , given the hidden states of the in-   put sequence hwith a query qof type las well   as the Clause Memory M , we limit the retrieval   process only in the corresponding memory parti-   tionM[l]during training to retrieve truly similar   ( i.e. of the same type ) clauses that provide precise   guidance on clause extraction in the current con-   tract . The retriever is implemented as a similarity   function f ( · , · ):   /hatwidesth= arg maxf(h , h)(5 )   where f(h , h ) = cos ( h W , hW ) ,   W∈Rand W∈Rare parameters   to project h , hto the same space .   To make the retriever trainable such that it can   learn to capture the common characteristics of the   same type of clauses , we introduce a Retrieval Loss   Lto minimize a contrastive learning loss func-   tion ( Hadsell et al . , 2006 ) , where a negative clause   h∈ M \ M [ l]is randomly sampled:25842.4 Aggregator   After obtaining relational information from corre-   sponding relation solvers , we fill all these represen-   tations into the reserved token slots and allow the   new segment sequence to automatically learn three   implicit relations via a fusion layer .   For LCR and TDR , not all segment or definition   representations in the corresponding buckets are   necessary for each input segment as they may be   repeated ( i.e. LCR ) or out of segment scope ( i.e.   TDR ) . Therefore , for the m - th input segment , we   remove the repeated segment representation ( i.e.   /hatwidesth ) and only consider the definition representa-   tions whose terms appear in this segment:=B\/hatwidesth   B={/hatwidesth|dinc , n∈[1 , N]}(7 )   For SCR , each segment is paired with one clause   representation retrieved . Then after filling all cor-   responding representations into the reserved slots ,   we get the final hidden state hfor each segment :   h= [ h;B;/hatwidesth;B ] ( 8)   where hare the hidden states ranging from   [ CLS]to the second [ SEP]inh . Note that we do   not set a specific size of reserved slots for each re-   lation , but only assure that the total size should not   exceed |r| . The reserved slots taken by these repre-   sentations are unmasked to enable calculation and   gradient flow . Then hwould pass a fusion layer   to automatically learn the three implicit relations :   o = Fusion ( h ) ( 9 )   where Fusion ( · ) is a standard RoBERTa layer   with randomly initialized parameters and ois the   relation - aware hidden states for the m - th segment .   We use oto extract clause :   P(m ) = softmax ( oW )   P(m ) = softmax ( oW)(10 )   where P(m)andP(m)denote the probabilities   of a token being the start and end positions respec-   tively . W , W∈Rare corresponding parame-   ters . The Extraction Loss Lis defined as the cross-   entropy between the predict probabilities and the   ground - truth start and end positions respectively.2.5 Training & Prediction   Training During training , we assume that the   clause type for each input query is available and   follow ConReader to get LandL , where the   final training objective is the summation of them   L = L+L. If no clauses can be extracted given   the current query , we set both the start and end   positions to 0 ( i.e. [ CLS ] ) .   Prediction At the prediction time , we may en-   counter zero - shot scenarios where the clause types   are out - of - scope of the existing Ltypes and , more   seriously , CD essentially does not provide the   clause type for each query clause . This would stop   ConReader from generalizing to these scenarios   as we are unable to indicate which memory parti-   tion of Mfor retrieval . To address this limitation ,   we allow the retrieval to be performed in the en-   tire clause memory ( the condition in Equation 5   would be replaced to h∈ M ) since the retriever   has already learned to effectively capture the com-   mon characteristics of similar clauses . To deal   with the extraction of multiple clauses , we follow   Hendrycks et al . ( 2021 ) to output top Tclauses ac-   cording to P(m)×P(m)in the contract , where   0≤i≤j≤ |x|denote positions in x.   3 Experimental Settings   We conduct experiments on two CCE tasks , namely   CA and CD , in two settings : ( 1 ) the conventional   setting where the clauses in the training and test   sets share the same clause types ; and ( 2 ) a more   difficult zero - shot setting where the clause types   differ substantially for training and test set .   Datasets To implement ConReader on CA and   CD in both settings , we combine two datasets   which originally only tackle one of the tasks :   •CUAD ( Hendrycks et al . , 2021 ) is proposed to   only tackle CA . It carefully annotates 41 types   of clauses that warrant review . CUAD provides   CA datasets for both training and test .   •Contract Discovery ( Borchmann et al . , 2020 ) is   proposed to only tackle CD . It annotates 21 types   of clauses substantially different from CUAD and   applies a repeated sub - sampling procedure to pair   two clauses of the same type as a CD example .   However , since the legal annotation is expensive ,   it only provides development and test sets .   For CA , we use the training set of CUAD to   train a ConReader model . We evaluate it on the2585test set of CUAD for the conventional setting and   on the development and test sets of Contract Dis-   covery for the zero - shot setting . For CD , since we   now have a training set from CUAD , we apply the   same supervised extractive QA setting , where one   clause is supposed to be extracted conditioned on   the query clause instead of original unsupervised   sentence matching formulation . Similar to Borch-   mann et al . ( 2020 ) , we sub - sample k ( k = 5 in our   work ) clauses for each clause type and split them   into k - 1 seed clauses and 1 target clause . Then ,   we pair each of the seed clauses with the contract   containing the target clause to form k - 1 CD ex-   amples . By repeating the above process , we can   finally get the CD datasets for both training and   evaluation . Similar to CA , we train another model   for CD and evaluate it in two settings . Details of   data statistics can be found in Appendix A.1 .   Evaluation Metrics Following Hendrycks et al .   ( 2021 ) , we use Area Under the Precision - Recall   curve ( AUPR ) and Precision at 80 % Recall   ( P@0.8R ) as the major evaluation metrics for CA .   In CUAD , an extracted clause is regarded as true   positive if the Jaccard similarity coefficient be-   tween the clause and the ground truth meets a   threshold of 0.5 ( Hendrycks et al . , 2021 ) . While   in Contract Discovery , it tends to annotate longer   clauses with some partially related sentences ( ex-   amples can be found in Appendix A.2 ) . Therefore ,   we also regard an extracted clause as true positive   if it is a sub - string of the ground truth . For CD ,   we use AUPR and Soft - F1 to conduct a more fine-   grained evaluation in terms of words ( Borchmann   et al . , 2020 ) .   Baseline Methods We compare with several re-   cently published methods , including : 1 ) Rule-   based or unsupervised contract processing models :   Extraction Zone ( Chalkidis et al . , 2017 ) and Sen-   tence Match ( Borchmann et al . , 2020 ) ; 2 ) Strong   pretrained language models : BERT ( Devlin et al . ,   2019 ) , RoBERTa ( Liu et al . , 2019 ) , ALBERT ( Lan   et al . , 2020 ) , DeBERTa ( He et al . , 2020 ) and   RoBERTa+PT that pretrained on 8 GB contracts   ( Hendrycks et al . , 2021 ) ; and 3 ) Models tackling   long text issue : Longformer ( Beltagy et al . , 2020 ) ,   and Hi - Transformer ( Wu et al . , 2021 ) .   Implementation Details We apply our frame-   work on top of two model sizes , namely , RoBERTa-   base ( 12 - layer , 768 - hidden , 12 - heads , 125 M param-   eters ) and RoBERTa - large ( 24 - layer , 1024 - hidden ,   16 - heads , 355 M parameters ) from Huggingface .   The reserved slots size |r|is set to 30 such that   most of the relational information can be filled in .   The size of Clause Memory |M| for each parti-   tion is 10 . In prediction , we follow Hendrycks   et al . ( 2021 ) to output top T= 20 clauses . Recall   that the query of CD is a clause , which is much   longer than a clause type . We set the max query   length for CA and CD to be 64 and 256 respectively .   The max sequence length is 512 for both models   in two tasks . We follow the default learning rate   schedule and dropout settings used in RoBERTa .   We use AdamW ( Loshchilov and Hutter , 2019 ) as   our optimizer . We use grid search to find optimal   hyper - parameters , where the learning rate is cho-   sen from { 1e-5,5e-5,1e-4 } , the batch size is chosen   from { 6,8,12,16 } .   We additionally introduce 1.7 M and 7 M param-   eters to implement the clause retriever f(·,·)and   fusion layer Fusion inConReader . Comparing to   RoBERTa , their sizes are almost negligible , and   hardly affect the speed . All experiments are con-   ducted on one Titan RTX card .   4 Results   Conventional Setting Table 1 shows the results   of CA and CD in the conventional setting . Among   base - size models , ConReader -base significantly   improves over all previous methods on both tasks ,   where it surpasses the RoBERTa - base by 4.0 and2586   3.9 AUPR respectively . Among large - size models ,   ConReader -large can exceed RoBERTa - large by   1.7 AUPR and 5.3 P@0.8R on CA and achieves   the new state - of - the - art . Such a large improvement   on P@0.8R would make the model less likely to   miss important clauses that may cause huge losses ,   which is especially beneficial in the legal domain .   Notably , ConReader -large also exceeds DeBERTa-   xlarge by 1.3 AUPR with less than half of its pa-   rameters ( 364 M vs 750 M ) , demonstrating the ef-   fectiveness of our framework .   Additionally , there are several notable observa-   tions : 1 ) As the queries in CD are clauses , they   are more diverse than the 41 queries of CA , mak-   ing it a more difficult CCE task . 2 ) We find that   ConReader -base outperforms RoBERTa+PT - base .   This implies that explicitly modeling the complexi-   ties of the contracts is more valuable than learning   from the in - domain data in an unsupervised man-   ner . 3 ) The improvements of the models designed   for long text ( Longformer and Hi - Transformer ) are   less significant than ConReader . It suggests that   there are more sophisticated issues in contracts   other than long text . In addition , Longformer fa-   vors Precision than Recall , causing P@0.8R to be   0 in CA and low performance in CD . Such a char-   acteristic is not suitable for CCE as it has lower   tolerance to miss important clauses .   Zero - shot Setting In Table 2 , we show the re-   sults of CCE in the zero - shot setting , where users   may look beyond the 41 types of clauses annotated   in Hendrycks et al . ( 2021 ) for their particular pur-   poses . We can observe that : 1 ) All models suffer   from a great performance drop in both tasks due   to the label discrepancy between training and eval-   uation , which highlights the challenge of CCE in   the zero - shot setting . 2 ) Though Longformer - base   performs well in the conventional setting , it is less   competitive against RoBERTa - base in the zero - shot   setting . We conjecture that it sacrifices the atten-   tion complexity for encoding longer text , which   is hard to capture the semantic correlations never   seen before in the zero - shot setting . 3 ) ConReader -   base achieves superior generalization ability in the   zero - shot setting . This is because the three implicit   relations widely exist in contracts , which are not   restricted to a particular clause type .   Ablation Study To investigate how each relation   type contributes to CCE , we conduct an ablation   study by ablating one component of ConReader in   each time , which is shown in Table 3 . For clar-   ity , discarding LCR Solver means that we do not   fuse segment representations in Aggregator but we   still split a contract into segments for separate en-   coding . 1 ) Discarding LCR Solver would slightly   degrade the performance . Since LCR only appeals   to a small number of clauses that require distant   interactions , it has little benefit to the clauses that   require interaction within a segment . This limits   LCR in contributing to CCE . 2 ) The ablation study   in terms of TDR shows that definition information   actually improves CCE . It enhances the represen-   tations of terms with specific explanations , which   makes them less ambiguous and thus allows con-   sistent information flow . 3 ) Discarding SCR Solver   and the Retrieval Loss would also cast a serious   impact on the results , especially on CD . Since the   Retrieval Loss is a learning objective concerning   the semantics of clauses , it benefits CD by allevi-   ating the difficulty in understanding the query se-   mantics . As a result , LCR , SCR , and TDR should   all be taken into consideration for building reliable   CCE models .   5 Further Analyses   Analysis of TDR Solver The quality of extracted   definitions is of vital importance as it directly de-   termines the effectiveness of definition representa-   tions . Therefore , to check the quality of our auto-   matically extracted definitions , we compare them   with ground - truth definitions annotated by us in   CUAD . The statistics of ground - truth definitions   and the quality of automatically extracted defini-2587   tions are shown in Table 4 . Specifically , more than   half of the contracts contain definitions ( 290 / 408   for training , 65 / 102 for test ) , where our rule - based   extraction can correctly extract definitions for most   of them . In addition , the results in Table 4 ( b ) show   our extracted definitions ( + Auto ) are capable of   improving the ability of baseline models to extract   clauses by enhancing the representations of legal   terms and their benefits are almost the same as the   ground - truth definitions ( + Manual ) .   Analysis of SCR Solver To examine in depth   the effect of SCR Solver , we implement several   variants from the perspectives of gathering similar   clauses ( Access ) and maintaining the Clause Mem-   ory ( Update ) . As shown in Table 5 , for Access ,   we evaluate two variants by randomly selecting a   clause representation from the corresponding mem-   ory partition ( w/ Random M[l ] ) or retrieving the   most similar one from the entire memory ( w/ Re-   trieved M ) . Since the first variant selects a truly   positive example ( of the same type ) to train the Re-   trieval Loss , the performance only drops marginally   comparing to our default design . While the second   variant is less effective since it can not guarantee   the retrieval of a positive example , which imposes   a distracting signal in the Retrieval Loss . For Up-   date , we replace our FIFO update strategy with   random update ( w/ Random Update ) or stopping   update when memory is full ( w/o Update ) . The   first variant can also partially keep the clause repre-   sentations update , while the second variant can not ,   causing it to be less effective due to poor clause rep-   resentations . Overall , our default design for SCR   Solver is more effective than those variants .   Case Study Figure 3 shows the attention distri-   bution of the start and end tokens of the ground-   truth clause over the reserved slots . It provides   the interpretability that ConReader can precisely   capture the relevant relations with high attention   probability . For example , it indicates that there is   an important cue ( " Section 5.3 " ) in the No.7 seg-   ment . It provides the detailed explanation of rele-   vant terms ( " Software Support and Maintenance "   and “ SOFTWARE " ) that mentioned in this clause .   In addition , the start and end tokens also exhibit   high correlations with corresponding SCR start and   end representations , showing that similar clauses   can help determine the exact clause location .   Effect of Training Data Size We simulate low-   resource scenarios by randomly selecting 10 % ,   30 % , 50 % , and 100 % of the training data for train-   ing CCE models and show the comparison results   among various methods . The performance trends   are visualized in Figure 4 . In general , ConReader -   base makes an consistent improvement on different   data sizes . Impressively , it can yield an absolute   increase of 14 AUPR on CA by increasing the   training volume from 10 % to 30 % . ConReader -   base with 50 % of the training data ( ConReader -   base@50 % ) can reach or almost exceed the per-   formance of other approaches trained on 100%2588   training data on both CA and CD . These results   shall demonstrate the great value of ConReader   in maintaining comparable performance and sav-   ing annotation costs at the same time . Meanwhile ,   the performance trends of the two tasks indicate   that there is still a lot of room for improvement ,   suggesting that the current bottleneck is the lack   of training data . According to the above analysis ,   we do believe that applying ConReader can still   achieve stronger results than textual - input baselines   ( e.g. RoBERTa ) when more data is available and   therefore , reduce more workload of the end users .   6 Related Work   Contract Review Earlier works start from clas-   sifying lines of contracts into predefined labels ,   where handcrafted rules and simple machine learn-   ing methods are adopted ( Curtotti and McCreath ,   2010 ) . Then , some works take further steps to   analyze contracts in a fine granularity , where a   small set of contract elements are supposed to   be extracted , including named entities ( Chalkidis   et al . , 2017 ) , parties ’ rights and obligations ( Fu-   naki et al . , 2020 ) , and red - flag sentences ( Leiva-   diti et al . , 2020 ) . They release corpora for auto-   matic contract review , allowing neural models to   get surprising performance ( Chalkidis and Androut-   sopoulos , 2017 ; Chalkidis et al . , 2019 ) . Recently ,   studies grow increasing attention on CCE to ex-   tract clauses , which are complete units in contracts ,   and carefully select a large number of clause types   worth human attention ( Borchmann et al . , 2020 ;   Wang et al . , 2021b ; Hendrycks et al . , 2021 ) . Due   to the repetition of contract language that new con-   tracts usually follow the template of old contracts   ( Simonson et al . , 2019 ) , existing methods tend to   incorporate structure information to tackle CCE .   For example , Chalkidis et al . ( 2017 ) assign a fixed   extraction zone for each clause type and limit theclauses to be extracted from corresponding extrac-   tion zones . Hegel et al . ( 2021 ) leverage visual cues   such as document layout and placement as addi-   tional features to better understand contracts .   Retrieval & Memory Retrieval from a global   memory has shown promising improvements to   a variety of NLP tasks as it can provide extra or   similar knowledge . One intuitive application is   the open - domain QA , where it intrinsically neces-   sitates retrieving relevant knowledge from outer   sources since there is no supporting information   at hand ( Chen et al . , 2017 ; Karpukhin et al . , 2020 ;   Xu et al . , 2021a , b ) . Another major application is   neural machine translation with translation mem-   ory , where the memory can either be the bilingual   training corpus ( Feng et al . , 2017 ; Gu et al . , 2018 )   or a large collection of monolingual corpus ( Cai   et al . , 2021 ) . It also has received great attention in   other text generation tasks including dialogue re-   sponse generation ( Cai et al . , 2019 ; Li et al . , 2021 )   and knowledge - intensive generation ( Lewis et al . ,   2020 ) , as well as some information extraction tasks   including named entity recognition ( Wang et al . ,   2021a ) , and relation extraction ( Zhang et al . , 2021 ) .   7 Conclusion   We tackle Contract Clause Extraction by exploring   three implicit relations in contracts . We compre-   hensively analyze the complexities of contracts and   distill out three implicit relations . Then we propose   a framework ConReader to effectively exploit these   relations for solving CCE in complex contracts . Ex-   tensive Experiments show that ConReader makes   considerable improvements over existing methods   on two CCE tasks in both conventional and zero-   shot settings . Moreover , our analysis towards in-   terpretability also demonstrates that ConReader is   capable of identifying the supporting knowledge   that aids in clause extraction .   Limitations   In this section , we discuss the limitations of this   work as follows :   •In this paper , we employ some language-   dependent methods to extract the definitions .   Specifically , we use some regular expressions   to extract definitions from English contracts   in the TDR solver due to the well - organized   structure of contracts . Therefore , some sim-   ple extraction methods have to be designed2589to tackle the definition extraction when apply-   ing our framework to legal contracts in other   languages .   •In order to meet the need of the end users ,   there is much room for improvement of the   CCE models . Due to the limited training data   from CUAD ( 408 contracts ) , it would be dif-   ficult to train a robust model that can be di-   rectly used in real - life applications , especially   those requiring the zero - shot transfer capa-   bility . Therefore , it would be beneficial to   collect more training data in order to satisfy   the industrial requirements . In addition , the   low - resource setting is also a promising and   practical direction for future studies .   Ethics Statement   The main purpose of CCE is to reduce the tedious   search effort of legal professionals from finding   needles in a haystack . It only serves to highlight   potential clauses for human attention and the le-   gal professionals still need to check the quality of   those clauses before continuing to the final contract   review ( still human work ) . In fact , we use P@0.8R   as one of our evaluation metrics because it is quite   strict and meets the need of legal professionals .   We also conduct a zero - shot setting experiment to   demonstrate that the benefit of ConReader is not   learning from biased information and has a good   generalization ability .   We use publicly available CCE corpora to train   and evaluate our ConReader . The parties in these   contracts are mostly companies , which do not in-   volve gender or race issues . Some confidential   information has originally been redacted to protect   the confidentiality of the parties involved . Such   redaction may show up as asterisks ( * * * ) or un-   derscores ( _ _ _ ) or blank spaces . We make identify   and annotate all definitions in those contracts . Such   definitions are well structured , which require little   legal knowledge . These annotations are just to ver-   ify the effectiveness of TDR Solver in ConReader   but not to contribute a new dataset . We can re-   lease the annotated definitions for the reproduction   of our analysis if necessary . We report all pre-   processing procedures , hyper - parameters , evalua-   tion schemes , and other technical details and will   release our codes for reproduction ( we move some   to the Appendix due to the space limitation).References25902591A Appendix   A.1 Data Statistics   We show the datasets statistics in Table 6 . CUAD   annotates 41 types of clauses that lawyers need to   pay attention to when reviewing contracts . Some   types are " Governing Law " , " Agreement Date " ,   " License Grant " , and " Insurance " et al . Contract   Discovery annotates another 21 types of clauses   that must be well - understood by the legal annota-   tors . These types include " Trustee Appointment " ,   " Income Summary " , and " Auditor Opinion " et al .   The two datasets differ substantially in their an-   notated types , making Contract Discovery a good   resource for conducting zero - shot experiments . To   prepare a real zero - shot setting , we further remove   6 types of clauses annotated in both corpora to pre-   pare a real zero - shot setting . The types include :   change of control covenant , change of control no-   tice , governing law , no solicitation , effective date   reference , effective date main .   Since most contents in contracts are unlabeled ,   which cause a large imbalance between extractable   and non - extractable segments . If a CCE model   is trained on this imbalanced data , it is likely to   output an empty span since it has been taught by   the non - extractable segments not to extract clauses .   Therefore , we follow Hendrycks et al . ( 2021 ) to   downweight contract segments that do not contain   any relevant clauses in the training set such that   extractable and non - extractable segments are ap-   proximately balanced ( i.e. 1:1 ) . While in test sets ,   we keep all non - extractable segments . This ex-   plains why test sets have fewer contracts but more   segments .   A.2 Annotation Difference   Table 7 shows the annotation difference between   CUAD and Contract Discovery on “ Governing   Law " clauses . In fact , Contract Discovery tends to   annotate more facts into the clause , such as parties ’   obligations . Due to such annotation difference , we   also regard an extracted clause as true positive in   calculating AUPR if it is a sub - string of the ground   truth in the zero - shot setting .   A.3 Performance by Type   Figure 5 shows the AUPR scores for each clause   type of ConReader and RoBERTa.2592Task Source # Type Dataset # Contract # Segment # Clause   CACUAD 41 Train 408 38,226 11,180   CUAD 41 Test ( Conv . ) 102 155,098 2,643   Contract Discovery 15 Dev ( Zero . ) 287 407,907 1,031   Contract Discovery 15 Test ( Zero . ) 286 375,606 1,031   CDCUAD 41 Train 408 55,249 15,988   CUAD 41 Test ( Conv . ) 102 711,282 10,448   Contract Discovery 15 Dev ( Zero . ) 287 602,236 5,524   Contract Discovery 15 Test ( Zero . ) 286 560,721 5,549This Agreement shall be construed in accordance with and governed by the substantive internal   laws of the State of New York .   This Agreement shall be governed by the laws of the State of New York , without giving effect to   its principles of conflicts of laws , other than Section 5 - 1401 of the New York General Obligations   Law .   This Agreement is subject to and shall be construed in accordance with the laws of the Common-   wealth of Virginia with jurisdiction and venue in federal and Virginia courts in Alexandria and   Arlington , Virginia . Section 4.8 Choice of Law / Venue . This Agreement will be governed by and construed and   enforced in accordance with the internal laws of the State of California , without giving effect to   the conflict of laws principles thereof . Each Party hereby submits to personal jurisdiction before   any court of proper subject matter jurisdiction located in Los Angeles , California , to enforce the   terms of this Agreement and waives any and all objections to the jurisdiction and proper venue of   such courts .   This Agreement will be governed by and 4 construed in accordance with the laws of the State of   Delaware ( without giving effect to principles of conflicts of laws ) . Each Party : ( a ) irrevocably and   unconditionally consents and submits to the jurisdiction of the state and federal courts located in   the State of Delaware for purposes of any action , suit or proceeding arising out of or relating to   this Agreement ;   Section 4.8 . Choice of Law / Venue . This Agreement will be governed by and construed and   enforced in accordance with the internal laws of the State of California , without giving effect to   the conflict of laws principles thereof . Each Party hereby submits to personal jurisdiction before   any court of proper subject matter jurisdiction located in Los Angeles , California , to enforce the   terms of this Agreement and waives any and all objections to the jurisdiction and proper venue of   such courts.25932594