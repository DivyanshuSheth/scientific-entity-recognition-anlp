  Jae Sung ParkSheng Shen   Ali FarhadiTrevor DarrellYejin ChoiAnna RohrbachPaul G. Allen School of Computer Science & Engineering , University of WashingtonUniversity of California , BerkeleyAllen Institute for Artificial Intelligence   Abstract   Recent video - text models can retrieve relevant   videos based on text with a high accuracy , but   to what extent do they comprehend the seman-   tics of the text ? Can they discriminate between   similar entities and actions ? To answer this , we   propose an evaluation framework that probes   video - text models with hard negatives . We au-   tomatically build contrast sets , where true tex-   tual descriptions are manipulated in ways that   change their semantics while maintaining plau-   sibility . Specifically , we leverage a pre - trained   language model and a set of heuristics to cre-   ate verb and person entity focused contrast sets .   We apply these in the multiple choice video - to-   text classification setting . We test the robust-   ness of recent methods on the proposed auto-   matic contrast sets , and compare them to ad-   ditionally collected human - generated counter-   parts , to assess their effectiveness . We see that   model performance suffers across all methods ,   erasing the gap between recent CLIP - based   methods vs. the earlier methods .   1 Introduction   Relating video and text modalities is one of the   important goals in vision and language . Video is a   complex signal where people and objects act and   interact with each other through space and time .   Thus correctly associating a textual description and   a video requires understanding of entities , their   actions and much more , making it a hard problem .   One of the popular ways of training and evaluat-   ing video - text models is via cross - modal matching .   Often the task is formulated as a retrieval problem ,   where the goal is to select the correct match among   many ( e.g. thousand ) candidates , and distractors   are picked randomly ( Yu et al . , 2018 ) . Another   way is via multiple - choice prediction , where the   goal is to pick the true match out of several ( e.g. 5 )   candidates ( Torabi et al . , 2016 ) . The latter allowsFigure 1 : Samples of our video - to - text tasks on   the MSR - VTT ( Xu et al . , 2016 ) and LSMDC   dataset ( Rohrbach et al . , 2017 ; Park et al . , 2020 ) . A   hard negative option is added by manipulating verb   ( top ) and entity ( bottom ) in the ground truth sentence .   Two SOTA methods MMT ( Gabeur et al . , 2020 ) and   CLIP4CLIP ( Luo et al . , 2021 ) incorrectly choose the   manipulated sentence ( option B ) in both these cases .   for more controlled choice of negatives , which are   typically selected from other videos . Commonly ,   the retrieval setting is used during training to avoid   capturing any specific multiple - choice patterns or   biases , while both are used for evaluation .   Recent methods that leverage the large - scale   CLIP model ( Radford et al . , 2021 ) show significant   improvement in cross - modal matching , specifically ,   in the retrieval setting ( Fang et al . , 2021 ; Luo et al . ,   2021 ) . They outperform the prior state - of - the - art   methods , often based on the Multimodal Trans-   former design ( Miech et al . , 2020 ; Gabeur et al . ,   2020 ; Lei et al . , 2021 ) . However , we know that of-   ten model performance is “ over - estimated ” due to   the lack of challenging samples in evaluation . For   instance , Gardner et al . ( 2020 ) show that model per-   formance on several NLP tasks and one image - text   task is much lower on contrast sets , which are test   samples with small perturbation done by human   experts in a way that changes the gold label .   In this work , we are investigating whether the   video - text models also struggle in an evaluation   framework that probes them with hard negatives .   Instead of using human - designed contrast sets that   are not easily scalable , we propose an automated   pipeline that can generate contrast sets via verb and3574human entity manipulation . Our manipulations are   carefully designed to preserve fluency but change   semantics of the textual descriptions , making them   invalid for a given video . We focus on entities and   verbs to evaluate if the model can truly understand   “ who did what " in a video . Inspired by ( Li et al . ,   2020 ; Morris et al . , 2020 ) , we leverage a gener-   ative T5 language model ( Raffel et al . , 2020 ) to   manipulate the verb phrase and use heuristics to   swap person entities . Note that our pipeline does   not require a trained video - text model in the loop .   We apply our automatic manipulations to two   popular video - text benchmarks , MSR - VTT ( Xu   et al . , 2016 ) and LSMDC ( Rohrbach et al . , 2017 ) .   We additionally collect human generated contrast   sets to compare with our automatic ones . To make   sure that our automatic negatives are of high qual-   ity , we also confirm that humans can successfully   select the correct description for a given video with   our hard negatives . Finally , we benchmark sev-   eral video - text models on our contrast sets . We   find that all methods degrade in performance with   the introduction of hard negatives in the multiple-   choice setting ( Figure 1 ) . This includes the recent   CLIP - based works that demonstrated large gains in   the retrieval setting . This shows that all methods   have difficulty discriminating between entities and   verbs when the remaining context is unchanged .   We observe that model performance drops espe-   cially on cases such as verb antonym swaps , where   fine - grained action understanding is important .   2 Related Work   Defending and generating adversarial examples   ( Jia et al . , 2019 ; Jin et al . , 2020 ) have been mostly   explored in NLP since the reign of pre - trained lan-   guage models ( LMs ) ( Devlin et al . , 2019 ) . Li   et al . ( 2020 ) ; Garg and Ramakrishnan ( 2020 ) ;   Morris et al . ( 2020 ) show that substituting words   in a sentence with masked LMs ( Devlin et al . ,   2019 ; Liu et al . , 2019 ) can successfully mislead   the classification and entailment model predic-   tions to be incorrect . Template - based ( McCoy   et al . , 2019 ; Glockner et al . , 2018 ) and manually   crafted ( Gardner et al . , 2020 ) perturbations on eval-   uation datasets have also been studied for textual   entailment . Ribeiro et al . ( 2020 ) have curated a list   of checklists to reveal bugs present in NLP models .   Language - based adversarial examples can be col-   lected to study the robustness of vision - language   models as well . Shekhar et al . ( 2017 ) intro - duces FOIL - COCO dataset to evaluate the vision-   language model ’s decision when associating im-   ages with both correct and " foil " captions . Akula   et al . ( 2020 ) measure the robustness of visual refer-   ring expression models by checking if grounding is   performed correctly after word manipulation . Hen-   dricks and Nematzadeh ( 2021 ) show that vision-   language Transformers are worse at verb under-   standing than nouns . New versions of the VQA   dataset ( Antol et al . , 2015 ) are proposed to study   robustness of VQA models ( Shah et al . , 2019 ; Li   et al . , 2021 ) . Bitton et al . ( 2021 ) automatically   generate contrast sets from scene graphs to probe   compositional consistency of VQA models . Our   work is different in that we use pre - trained LMs to   introduce perturbations and evaluate robustness of   video -language models .   3 Designing Contrast Sets   In this section we present our approach to automati-   cally constructing text - based contrast sets for video-   language tasks . Suppose we are given a video V   and description s. Contrast sets ˆC={ˆs , ... , ˆs }   are designed such that ˆsis semantically inconsis-   tent with Vand yet models incorrectly select ˆs   oversin a video - to - text multiple - choice setting .   While there are different ways to create valid ˆC ,   we investigate manipulating 1 ) person entities and   2)verb phrases in the original descriptions . Quali-   tative examples of ˆCare shown in Table 1 .   3.1 Contrast Sets for Person Entities   First , we investigate automatically swapping the   name ( or identity ) of a person . The LSMDC   dataset ( Rohrbach et al . , 2017 ; Park et al . , 2020 ) in-   cludes movie descriptions with character identities   ( e.g. Harry Potter ) , and a list of characters present   in each movie along with their gender . We replace   each character ’s ID with one from the same movie   and with the same gender , to prevent the language   statistics alone from detecting the swapped IDs .   For the MSR - VTT dataset ( Xu et al . , 2016 ) we   do not have the identities ; however , 80 % of videos   have gender cues in the descriptions . Thus the con-   trast sets are created by swapping the gender of a   person mentioned in a sentence and the correspond-   ing pronouns ( e.g. , A woman is pushing her stroller   →A man is pushing his stroller ) . This is done with   a template that maps gender - sensitive words and   pronouns to their counterparts ( see Appendix).3575   3.2 Language Model Generated Verb   Contrast Sets   The above rule - based strategies can not be directly   translated to create contrast sets for verb phrases :   1 ) a substitute verb phrase is not guaranteed to be   inconsistent with a video , and 2 ) the sentence may   look unnatural and no longer be textually plausible .   Based on their success in adversarial attack gen-   eration ( Li et al . , 2020 ; Garg and Ramakrishnan ,   2020 ; Morris et al . , 2020 ) , we instead leverage pre-   trained language models ( LMs ) to automatically   manipulate the verb phrases .   We identify verb phrases in a sentence using   Spacy ( Honnibal and Montani , 2017 ) , replace them   with a mask token [ MASK ] , and select top K   phrases that best fit the mask token using probabil-   ity scores from a LM . Different from prior work   ( Li et al . , 2020 ) , we use T5 - base model ( Raffel   et al . , 2020 ) instead of masked language models   ( Devlin et al . , 2019 ; Liu et al . , 2019 ) to easily sup-   port generating multi - word candidates . We addi-   tionally finetune T5 to learn verb phrases in the   downstream training data with unsupervised de-   noising objective ( Raffel et al . , 2020 ) . This is done   to mitigate the possible distribution shift between   ground truth and manipulated descriptions , which   could be exploited to distinguish between the two .   We then filter the Ksentence candidates with   the following criteria : 1 ) There is no verb in the   sentence . 2 ) Verbs are rare or unseen in training   descriptions . 3 ) The sentence has a high perplexity   measured by GPT2 - XL ( Radford et al . , 2019 ) to en-   sure grammaticality and plausibility ( Morris et al . ,   2020 ) . Lastly , we check that the semantics of a   candidate is inconsistent with the original sentence .   This is when a)a candidate verb is an antonymof   an original verb , or b)a word embedding ( Mrkši ´ c   et al . , 2016 ) of candidate and original verbs andtheir sentence encodings ( Reimers and Gurevych ,   2019 ) both have low cosine similarity scores . We   handle the antonyms separately , as the embedding-   based scores do not adequately capture these , i.e. ,   a sentence with an antonym verb may still be con-   sidered semantically close to an original sentence .   3.3 Human - Generated Verb Contrast Sets   Are language models capable of generating con-   trast sets of good quality ? To answer this question ,   we follow the original contrast sets work ( Gardner   et al . , 2020 ) , and also create negatives manually to   see if the performance on machine and human gen-   erated contrast sets is similar . We use the Amazon   Mechanical Turk ( AMT ) platform and ask work-   ers to modify a verb phrase such that a sentence   becomes inconsistent with a video ( see Appendix ) .   4 Experiments   4.1 Datasets and Multiple Choice Design   MSR - VTT ( Xu et al . , 2016 ) is composed of 10 K   YouTube videos each paired with 20 natural de-   scriptions and is typically evaluated on retrieval   performance with 1000 video text pairs as candi-   dates in the test set . The multiple choice version   ( Yu et al . , 2018 ) has 2,990 test videos as queries ,   and a positive caption with 4random captions from   other videos as 5answer options . We label this split   as the Random MC . We design another MC prob-   lem by replacing one negative option with one from   our contrast sets . In particular , Gender MC swaps   gender in an original sentence ; VerbMC and   VerbMC include verb - based negatives generated   by our approach and by humans .   LSMDC ( Rohrbach et al . , 2017 ) includes short   movie clips and captions . Characters in these cap-   tions are labeled as SOMEONE and we can not   construct contrast sets for person - entities . We in-   stead use captions in ( Park et al . , 2020 ) that include3576   the character identities . We create a new split using   the same movies in training and test so that the   test identities have been seen during training . We   call this modified dataset LSMDC - IDs . Using this   set , Random MC is newly defined with 4negative   captions drawn randomly from different clips of   the same movie . ID MC swaps the character IDs   ( Section 3.1 ) as negatives , and Verb MC includes   the verb contrast sets , as before .   4.2 Video - Text Models and Evaluation   We benchmark Transformer ( Vaswani et al . , 2017 )   based video - language models in our experiments .   Portillo - Quintero et al . ( 2021 ) apply frozen CLIP   features ( Radford et al . , 2021 ) to perform zero-   shot video to text retrieval ( CLIP zero - shot ) . Multi   Modal Transformer ( MMT ) ( Gabeur et al . , 2020 )   learns the joint representation between text and   multiple modalities in videos . Inspired by Dz-   abraev et al . ( 2021 ) , we also extend MMT to   take frozen CLIP features as input , denoted as   MMT - CLIP . CLIP4CLIP ( Luo et al . , 2021 ) and   CLIP2Video ( Fang et al . , 2021 ) directly finetune   CLIP with temporal pooler and are the state - of - the-   art in retrieval tasks . ViT - B/32 model is used for   all CLIP experiments ( see Appendix C for details ) .   We train the above models with a contrastive loss   to learn the joint video - text representation . In MC   settings , we mark it as correct , if a ground truth   sentence is scored the highest . In addition , we also   evaluate humans on the MC task . We report video-   to - text ( V →T ) Recall@1 for retrieval evaluation .   4.3 Results   Table 2 shows results on the MSR - VTT dataset . In   video - to - text retrieval , we see a significant gap in   performance between the CLIP - finetuned models   and all other models . Moreover , CLIP zero - shot   matches MMT in this metric . Next , we see that   Random MC is nearly solved by almost all models .   However there is a significant drop in performance   across all models when evaluated on contrast-   set based MC . Interestingly , the performance gap   between MMT and the finetuned CLIP models   with high retrieval performance ( CLIP4CLIP and   CLIP2Video ) is gone in this setting , meaning   stronger retrieval performance does not guarantee   robustness to word - level manipulations . We also   observe that models with frozen CLIP features per-   form better on Gender MC thanVerb MC , and fine-   tuning the CLIP features on video - language task   can make the model less sensitive to gender infor-   mation . Finally , to verify that the automated verb-   based contrast sets are valid , we note that : models   onVerbMC perform on par with the human   produced ones VerbMC , and humans maintain   accuracy greater than 90 % on both contrast sets .   Table 3 presents results on the LSMDC - IDs   dataset . We find that distinguishing different clips   of the same movie ( Random MC ) is not “ solved ”   by the models unlike the MSR - VTT . We also notice   that the ID swaps are significantly easier than the   verb swaps , and CLIP features are particularly help-   ful in distinguishing different character IDs ( MMT   vs. MMT - CLIP ) . Table 4 shows that model accu-   racy drops by at least 13.9 % when the “ negative ”   IDs appear more frequently in the training data than   the original IDs , meaning the models struggle to   identify IDs in the long - tail . The results on verb   contrast sets are similar to the MSR - VTT dataset .   The performance is much lower on contrast - set   MC cases than Random MC . There is no signifi-   ca nt gap between VerbMC andVerbMC . Our   automated contrast sets are still valid as humans   perform above 90 % for both cases .   Does Semantic Proximity in Verb Contrast   Sets Affect the Model Performance ? To answer   this , we first considered a subset containing verb   antonyms . For the remaining ones , we use the off-   the - shelf sentence encoders , SentBERT ( Reimers   and Gurevych , 2019 ) and CLIP text transformer3577   ( Radford et al . , 2021 ) , to measure the semantic   proximity b.w . the original and negative sentences ,   and select the ones with the highest and lowest 15 %   according to these scores ( High / Low ) . We present   the results on MSR - VTT in Table 5 . We notice that   the models especially struggle with antonyms , suchas dropping from 83.7 % ( in Table 2 ) to 70.6 % for   CLIP4CLIP . Humans on the other hand get 92.9 %   accuracy and show no difference in their perfor-   mance . The best models achieve high accuracy on   par with humans on semantically different exam-   ples ( Low ) as measured by both SentBERT amd   CLIP - Text . However , model performance is much   lower for contrast sets with high semantic similar-   ity ( High ) , whereas human performance is not as   affected ( e.g. CLIP4CLIP drops to 77.6 % and hu-   mans maintain 92.2 % accuracy on SentBERT ) . In   Figure 2 , we show failure cases where the SOTA   models are misled by semantically close sentences   and verb antonyms , due to their lack of fine - grained   understanding of actions in the video .   5 Conclusion   We present a pipeline to build automatic contrast   sets for video and language tasks , focused on ma-   nipulating person entities and verb phrases . We   show that models struggle on contrast sets com-   pared to random negatives , and stronger retrieval   models do not show better robustness to hard neg-   atives . For verb contrast sets , we find that model   performance is strongly correlated with semantic   proximity , unlike humans . We leave it as future   work to use automatic contrast sets in training to   improve model robustness , and designing contrast   sets for different concepts / parts of speech.35786 Ethical Considerations   Our goal is to diagnose performance of video-   language models on hard negative samples w.r.t .   verbs and person entities . Overall , we envision   positive impact from this work , as it aims to ex-   pose limitations of the existing models . Some of   our entity swaps focus on apparent gender ( as de-   scribed by humans in the video - text datasets ) , but   we do not predict biological sex or gender iden-   tity . We construct our verb - focused contrast sets   automatically , using a large generative language   model , thus potentially some biases present in such   a model could propagate into our hard negative   samples . Practitioners who wish to use our contrast   sets should be mindful of such sources of bias .   Acknowledgements   This work was funded by DARPA MCS program   through NIWC Pacific ( N66001 - 19 - 2 - 4031 ) , and   in part by DARPA ’s LwLL , and/or SemaFor pro-   grams , and Berkeley Artificial Intelligence Re-   search ( BAIR ) industrial alliance programs .   References357935803581Male Nouns Female Nouns   man→woman woman →man   men→women women →men , guys   boy→girl girl→boy , guy   boys→girls girls→boys , guys   guy→woman , girl lady→man , guy   guys→women , girls , ladies ladies→men , guys   A Contrast Set Construction   Here , we provide more details on construction of   each contrast set .   A.1 Gender Contrast Sets   Table 6 shows the mapping of gender - sensitive   words . We use these rules to swap only a single   word in the sentence . This is to guarantee that   swapping gender leads to different semantics ( e.g.   man and woman walk together − →woman and man   walk together both apply to the same video if all   words are swapped ) . If there are more than one   possible mappings , we randomly sample one from   a uniform distribution . Lastly , we swap all gender-   sensitive pronouns that have the same gender as   original noun . These contrast sets are used for the   MSR - VTT dataset ( Xu et al . , 2016 ) .   A.2 Person ID Contrast Sets   The first character ID in a sentence is replaced by   a different character ID that appears in the same   movie and has the same gender . Among all the   candidates , the manipulated ID is sampled from a   uniform distribution . The following character IDs   in the same sentence have uniform chance of being   kept or swapped using the same strategy . These   contrast sets are used for the LSMDC - IDs dataset .   A.3 Verb Contrast Sets   Attack Selection We use Spacy to get the POS   tags , and find verb phrases that match a list of pre-   defined patterns ( verb ; verb + preposition ) .   Candidate Generation We use T5 model and   performed beam search ( beam size = 50 ) to gener-   ateK= 50 multi - word candidates . Candidate Constraints We keep a candidate if   the lemmatized verbsin it appeared more than 30   times in the training set . For fluency , we calculate   perplexity score of original and manipulated sen-   tence using GPT2 - XL ( Radford et al . , 2019 ) , which   we call pplandppl . We calculate the normalized   difference of perplexity scores ppl = to remove a candidate that is less plausible than   the original . Specifically , candidates are kept if   ppl<0.6 , orppl<1.4∩ppl<750 . Lastly ,   the semantic inconsistency constraints are satisfied   if the word embedding ( Mrkši ´ c et al . , 2016 ) of   the lemmatized verbs in the candidate and orig-   inal sentence have cosine similarity score lower   than 0.4 , and the sentence embeddings ( Reimers   and Gurevych , 2019 ) have cosine similarity score   lower than 0.8 .   B Human vs Machine Generated Verb   Contrast Sets   Figure 3 shows a distribution of machine and hu-   man generated verb contrast sets . Each instance is   the number of lemmatized verbs divided by total   number of verbs in the contrast sets . We see that   machine generated contrast set is more skewed to   the left , and does n’t share the same distribution   of verbs as in the human generated contrast sets .   ( e.g. human contrast sets have more occurrences   ofcryandthrow in MSR - VTT , and jump anddrop   in LSMDC - IDs ) . Despite the difference , note that   models have similar performances in both contrast   sets .   C Implementation Details   •MMT ( Gabeur et al . , 2020 ): We use the fol-   lowing features extracted from video : mo-   tion from S3D ( Xie et al . , 2018 ) , audio from   VGGish ( Hershey et al . , 2017 ) , scene embed-   dings , face , OCR , Speech , and Appearance .   We refer to Miech et al . ( 2018 ) ; Gabeur et al .   ( 2020 ) for more details about the features .   For MSR - VTT , we use the released check-   point from their code , which is pre - trained   on HowTo100 M dataset ( Miech et al . , 2019 )   and further finetuned on MSR - VTT.3582   For LSMDC - IDs which needs re - training ,   we used their finetuning code for LSMDC   dataset ( Rohrbach et al . , 2017 ) . The model   is trained with max margin ranking loss on   1 Nvidia RTX-6000 GPU for 12 hours . Hy-   perparameter search was done to find mar-   gin of 0.05 , batch size of 32 , and Adam opti-   mizer ( Kingma and Ba , 2015 ) with learning   rate5e . The best model was selected by   the video - to - text retrieval performance with   Recall@1 . We found training from scratch   performs better than using pre - trained model .   This has been also observed by Gabeur et al .   ( 2020 ) for the LSMDC dataset .   •MMT - CLIP : We replace the appearance fea-   tures in MMT with frozen CLIP ViTB/32 fea-   tures and train with the same architecture.•CLIP zero - shot : In ( Portillo - Quintero et al . ,   2021 ) CLIP(ViTB-32 ) ( Radford et al . , 2021 )   features are aggregated via mean pooling to   approximate video representation . This video   representation and text embedding from CLIP   are combined to perform retrieval and MC in   a zero shot manner .   •CLIP4CLIP ( Luo et al . , 2021 ): We use the   hyperparameters from the finetuning codeto   reproduce their results . We use mean pooling   for the similarity calculator and CLIP model is   initialized with ViTB-32 weights . The model   was trained with 4 Nvidia RTX-6000 GPUs   for 5 epochs ( 48 gpu hours ) . The best model   was selected by using Recall@1 in video - to-   text retrieval.3583•CLIP2Video ( Fang et al . , 2021 ): We used   the released checkpoint on MSR - VTT using   their code base . This model is not used   for LSMDC - IDs because finetuning code was   not provided . CLIP model is initialized with   ViTB-32 weights .   D Multiple Choice Details   Here we provide more details about our evaluation   data . Note , that we use 5 text candidates ( 1 posi-   tive and 4 negative ) for all multiple choice ( MC )   settings .   D.1 MSR - VTT   We use the standard train / val / test split in MSRVTT   dataset ( Xu et al . , 2016 ) .   •Retrieval : 1,000 ground truth video - text pairs   in the test set ( Yu et al . , 2018 ) .   •Random MC : 2,990 videos and all negative   options are drawn randomly from other videos   ( Yu et al . , 2018 ) .   •Gender MC : 2,477 video - text instances . Us-   ing the original descriptions from Random   MC , a single negative is drawn from gender   contrast sets to replace one of the options in   Random MC ( the remaining 3 are kept ) . Note ,   that not all videos involved people or con-   tained gender - sensitive words in descriptions ,   hence some instances are filtered .   • VerbMC : 2,554 video - text instances . Con-   structed using the same strategy as in Gender   MC but a single negative is drawn from verb   contrast sets generated by language models .   Instances are filtered when there are no valid   verb contrast sets satisfying constraints in Sec-   tion A.3 .   • VerbMC : 2,554 video - text instances . We   use the instances in VerbMC , and a nega-   tive is drawn from human designed verb con-   trast sets .   D.2 LSDMC - IDs   We define a new split using LSMDC - ID descrip-   tions with character IDs ( proper names ) ( Park et al . ,   2020 ) . Note , that Rohrbach et al . ( 2017 ) ; Park et al .   ( 2020 ) use development and test sets where videos   come from distinct movies than the training data , meaning that IDs in test data are not seen in train-   ing . To overcome this issue , we split their train-   ingdescriptions into 80%/10%/10%/ ratio to create   new training / validation / test sets that share the same   movies and identities across splits .   •Retrieval : 7,010 ground truth video - text pairs .   •Random MC : 7,010 videos , negative text op-   tions drawn randomly from different videos   but the same movie .   •ID MC : 7,010 video - text instances . We re-   place one negative in Random MC with the   one from ID contrast sets .   • VerbMC : 7,010 video - text instances . We   replace one negative in Random MC with one   from the language model generated verb con-   trast sets .   • VerbMC : 3,500 video - text instances . We   replace one negative in Random MC with one   from the human designed verb contrast sets   ( we only crowdsourced 3,500 instances ) .   E Human Annotation Details   We ran two different human annotations , one to   evaluate our VerbMC and another to manually   design verb contrast sets . Figures 4 and 5 show the   respective HIT UIs . We use Amazon Mechanical   Turk interface to get a pool of annotators from   native Enlgish speaking countries and with high   approval rate , and pay them $ 15 hour on average   which is above a minimum wage .   F Dataset Details   We include additional information on the MSR-   VTT ( Xu et al . , 2016 ) and LSMDC ( Rohrbach   et al . , 2017 ) datasets . MSR - VTT contains diverse   YouTube videos and corresponding crowdsourced   descriptions in English language . LSMDC con-   tains movie clips and associated descriptions from   scripts or Audio Description , also in English . Both   datasets are distributed for research use . The li-   cense , personally identifiable information ( PII ) ,   and consent details of each dataset are in the re-   spective papers . Since LSMDC contains clips from   movies , some may contain nudity or violence , etc.358435853586