  Forrest Sheng Bao and Ge Luo and Hebi Li   Iowa State University , Ames , IA , USA   forrest.bao@gmail.com , { gluo,hebi}@iastate.edu   Minghui Qiu   Alibaba Group   Hangzhou , China   minghui.qmh@alibaba - inc.comYinfei Yang   1600 Amphitheater Parkway   Mountain View , CA , USA   yangyin7@gmail.com   Youbiao He   Iowa State University   Ames , IA , USA   yh54@iastate.eduCen Chen   East China Normal University   Shanghai , China   cecilia.cenchen@gmail.com   Abstract   Canonical automatic summary evaluation met-   rics , such as ROUGE , focus on lexical simi-   larity which can not well capture semantics nor   linguistic quality and require a reference sum-   mary which is costly to obtain . Recently , there   have been a growing number of efforts to al-   leviate either or both of the two drawbacks .   In this paper , we present a proof - of - concept   study to a weakly supervised summary evalua-   tion approach without the presence of reference   summaries . Massive data in existing summa-   rization datasets are transformed for training   by pairing documents with corrupted reference   summaries . In cross - domain tests , our strategy   outperforms baselines with promising improve-   ments , and show a great advantage in gauging   linguistic qualities over all metrics .   1 Introduction   In natural language processing , the problem of sum-   marization studies generating a summary from a   source document which is longer than the summary .   De facto metrics to judge a generated summary in-   clude ROUGE ( Lin , 2004 ) , BLEU ( Papineni et al . ,   2002 ) , and METEOR ( Banerjee and Lavie , 2005 ) .   Previous work ( Ng and Abrecht , 2015 ; Liu and Liu ,   2008 ; Liu et al . , 2016 ; Shang et al . , 2018 ) agrees on   two major drawbacks of them : 1 ) they favor lexical   similarity , falling short on semantic similarity or   linguistic quality , and 2 ) they require a reference   summary which is often expensive to obtain ( Zopf ,   2018 ) .   Initially , the first drawback is partially allevi-   ated by replacing lexicons with their word embed-   dings ( Ng and Abrecht , 2015 ; Ellouze et al . , 2017 ;   Ruseti et al . , 2018 ; Xia et al . , 2019 ) . After thebirth of transformers ( Vaswani et al . , 2017 ) , this   effort has expanded to sentence or document level ,   including reference - based ( Zhang * et al . , 2020 ;   Zhao et al . , 2019 ) , and reference - free ones ( Vasi-   lyev et al . , 2020 ; Scialom et al . , 2019 ; Gao et al . ,   2020 ) . The main difference between the two groups   is whether a reference summary is needed when   evaluating a machine - generated summary .   The two groups have complementary pros and   cons . Reference - based metrics have a better perfor-   mance , but they are impractical when summariza-   tion is used industrially , such as in customer sup-   port ( Liu et al . , 2019 ) , team conversation ( Zhang   and Cranshaw , 2018 ) , and bug reporting ( Rastkar   et al . , 2014 ) , where it is too costly to manually craft   an equally massive amount of reference summaries .   In contrast , without human written reference sum-   maries , reference - free approaches generally per-   form poorer . Modern transformer - based reference-   free approaches often rely on non - summarization   tasks , such as QA ( Vasilyev et al . , 2020 ; Scialom   et al . , 2019 ) . Such fact - focused approach makes   them excel on content / fact aspects ( still worse than   reference - based ones ) but not on linguistic ones .   The non - summarization tasks also introduce noises .   Therefore , in this paper , as a proof of concept ,   we explore a hybrid or middle approach to com-   bine the best of both worlds . Using document-   summary pairs in existing summarization datasets ,   our weakly supervised approach mutatesrefer-   ence summaries and pair them with documents to   form training data and then use the trained model   to evaluate unseen summaries in the presence of   12450documents without corresponding reference sum-   maries . In this way , we make use of human written   summaries , which are very precious , in training ,   but we do not need them in summary evaluation .   We call our approach SueNes , which stands for   “ Summary evaluation by Negative sampling . ”   The quality of a summary is usually evaluated   on two facets : content / fact aspects and linguistic   qualities . Experiments later will show that a value   of our approach is that we can use the same model   architecture to build models that excel on different   tasks by feeding training data from the same source   but mutated in different strategies . For example ,   deleting words is the best for linguistic qualities   while deleting sentences is the best for content / fact   coverage .   Our approach is empirically compared against an   array of existing metrics on three human summary   evaluation datasets . Despite being training - based ,   our approach exhibits consistent results across var-   ious training domains which are all different from   the test domain . It outperforms reference - free base-   lines with promising improvements on content / fact   aspects , and further outperforms all existing met-   rics in gauging linguistic qualities .   In summary , our contributions or merits are :   •a simple but effective approach to reference-   freesummary quality assessment ,   •negative sampling for preparing training data   from the unlabeled ,   • one task / framework for multi - aspect judging ,   •extensive cross - domain experiments to vali-   date the effectiveness and domain robustness   of our approach .   We hope our study can inspire more research   into hybridizing reference - free and reference - based   summary evaluation . Our code is at https://   github.com/forrestbao/SueNes/   2 The Approach   2.1 Model Architecture   A reference - free single - document summary qual-   ity assessor can be formulated as a regression   function f(d , s)∈[0,1]of an input document   d= [ t , t , · · · ] , and a machine - generated summary   s= [ t , t , · · · ] , where t ’s and t ’s are text tokens .   As a proof of concept , we explore an extremely leanimplementation of f : first dandsare jointly trans-   formed into a vector representation e = g(d , s ) ,   and then it is mapped to a summary quality score   via a fully - connected layer , i.e. , f(d , s ) = σ(We ) .   The function gcan be implemented in the   BERT ( Devlin et al . , 2019 ) style with an input   sequence [ [ CLS ] , t , t,···,[SEP ] , t , t , · · · ,   [ SEP ] ] . The output on the [ CLS ] token is a joint   representation of both the document dand the sum-   mary s.   While the human evaluation to a summary may   cover multiple aspects , such as content / fact cover-   age and linguistics , a model of us will only yield   one number . But by using different data mutation   strategies , we can get models ( different f ’s ) adept   at different aspects of a summary .   2.2 Negative Sample Generation   It is impractical to train fwith existing summa-   rization datasets , such as CNN / DailyMail ( Her-   mann et al . , 2015 ; Nallapati et al . , 2016 ) , because   they contain only high - quality , reference - class sum-   maries written manually and thus are all of label 1 .   Some summary evaluation datasets , such as Real-   Summ ( Bhandari et al . , 2020 ) , Newsroom ( Grusky   et al . , 2018 ) , and TAC2010 ( NIST , 2010 ) , do con-   tain human ratings to system - generated summaries   of various qualities . But they are too small , con-   taining no more than 100 news articles or article   groups each . Therefore , training against human   ratings or in a supervised approach is impractical .   To work around , we propose a weakly super-   vised solution as depicted in Figure 1(a ) . Existing   summarization datasets contain many document-   summary pairs . For each pair ⟨d , s⟩ , the reference   summary sis mutated into Knew summaries of dif-   ferent extents s , s , · · · , s , which are then paired   with the document to form new pairs   ⟨d , s⟩,⟨d , s⟩,···,⟨d , s⟩ ,   which are finally assigned targets to form the train-   ing data   ( ⟨d , s⟩ , y),(⟨d , s⟩ , y),···,(⟨d , s⟩ , y ) .   As illustrated in Figure 2 , the training target   yis the percentage of intact content . For   example , if 30 % of tokens in a mutated summary   are not original , then the label is 0.7 . In addition ,   the original document - summary pair ⟨d , s⟩is also   used in training with a target of 1 .   22451   Mutations can happen at the token or sentence   level , where tokens or sentences are randomly se-   lected for mutation . A selected token or sentence   is mutated in one of the three methods :   1.inserting a token / sentence from other sum-   maries behind it ,   2.deleting it , or   3.replacing it with a token / sentence from other   summaries .   We do not mix different mutation levels nor mix dif-   ferent mutation methods when preparing the train-   ing data . Instead , our experiments study one com-   bination of a mutation level and a mutation method ,   denoted as a mutation strategy , each time.3 Experiments   3.1 Test data   The ground truth of a summary ’s quality is hu-   man ratings to it . A model trained ( Fig . 1(a ) ) is   tested ( Fig . 1(b ) ) against human ratings . Three test   datasets are chosen below . Due to the limited num-   ber and sizes of human evaluation datasets , they are   all in the news domain . The human evaluation pro-   tocols can be found in their respective references .   TAC2010 ( NIST , 2010 ) is a multi - document   ( ten - document ) summarization task reporting   both factual and linguistic aspects . We use / summationtextf(d , s)to approximate the score of the   summary scomposed from ten documents dto   d. We only use Set A of TAC2010 because Set B   is not for regular summarization .   Newsroom ( Grusky et al . , 2018 ) also covers   both factual ( in INFormativeness and RELevance )   and linguistic ( in COHerence and FLUency ) as-   pects . For human ratings , three human annotators   rate one pair of a document and machine - generated   summary . The mean of their ratings on each aspect   is used in our experiments .   RealSumm ( Bhandari et al . , 2020 ) focuses on   only factual coverage . It covers 14 abstractive and   11 extractive summarizers published after 2018 and   conducts human evaluation on the two groups sep-   arately .   Note that we do not and can not train a model   against the labels in a test set , as mentioned in § 2.1 .   32452If a test set rates on multiple aspects , we do not   train one model for each aspect . Nor do we train   models for individual or a collection of test sets .   We compute correlation between the predictions   from our model and human ratings on each aspect   of each test set .   3.2 Training data   Three widely used summarization datasets from   three different domains are chosen for train-   ing : Billsum ( Kornilova and Eidelman , 2019 ) ,   Scientific - Papers / arXiv ( Cohan et al . , 2018 ) , and   Big - Patent ( Sharma et al . , 2019 ) . Datasets from   the news domain are avoided on purpose because   the test data is in the news domain . This cross-   domain setting allows us to examine whether a   model is prone to domain differences . For each   reference summary , K= 5 mutated summaries   are generated . The percentage of intact content is   measured by the number of tokens and the num-   ber of characters for token - level and sentence - level   mutations , respectively .   3.3 Baselines and upper bounds   To fairly compare , four recent metrics :   BLANC ( Vasilyev et al . , 2020 ) , Sum-   maQA ( Scialom et al . , 2019 ) , SUPERT ( Gao   et al . , 2020 ) and LS - Score ( Wu et al . , 2020 ) ,   are used as baselines because like our approach ,   they do not need a reference summary to judge a   machine - generated summary , i.e. , reference - free .   Human crafted reference summaries give   reference - based metrics advantages . The results   of reference - based metrics are included as soft up-   per bounds : ROUGE-1 , ROUGE-2 and ROUGE-   L(Lin , 2004 ) , MoverScore ( Zhao et al . , 2019 ) ,   BertScore ( Zhang * et al . , 2020 ) , BLEU ( Pap-   ineni et al . , 2002 ) , METEOR ( Banerjee and Lavie ,   2005 ) , and S(Peyrard et al . , 2017 ) .   3.4 Settings   Because the baselines use BERT , we use BERT   as well for a fair comparison . Specifically , BERT-   base uncased ( L=12 , H=768 ) is fine - tuned , with a   learning rate of 1e-5 , three epochs , and a batch size   of 14 . The input sequence is limited to 512 tokens   using the round robin trimmer . The training loss is   MSE as this problem is regression .   3.5 Results   We use the summary - level ( Peyrard et al . , 2017 )   meta - evaluation strategy to report an approach’saverage correlation with human ratings . Summary   evaluation usually covers two types of aspects , con-   tents / facts and linguistics . They are reported sepa-   rately in Tables 1 and 2 . Due to space limit , only   the best mutation strategy is reported for each as-   pect group .   On content / fact aspects , the best mutation strat-   egy is sentence deletion and our best models outper-   form baselines on all test datasets . Our approach   makes the most improvement over baselines on Re-   alSumm , a dataset much bigger than Newsroom   and more modern than TAC2010 , and the least im-   provement on TAC2010 , the oldest dataset .   On linguistic aspects , the best mutation strat-   egy is word deletion . Here , even our worst model   can not be outperformed by any baseline nor upper   bound . As mentioned earlier , canonical metrics are   lexical - based while modern reference - based and   reference - free approaches focus on facts . Through   mutating reference summaries , our approach can   create summaries of different linguistic qualities .   Although our approach makes big improvements   over baselines on TAC2010 and Newsroom ’s FLU-   ency , its edge is smaller on Newsroom ’s COHer-   ence . A sentence - level scrambling mutation may   improve our approach ’s performance on COHer-   ence in the future .   42453   3.6 Discussions   What is the best mutation ? Across datasets ,   deletion - based mutations are most effective . The   two kinds of deletions happen to be complemen-   tarily effective for two aspect groups : sentence   deletion for content / fact aspects vs. word deletion   for linguistic aspects . This is an advantage of our   approach that under a uniformed framework , dif-   ferent summary quality aspects can be gauged by   designing different mutation options .   The complementariness of sentence deletion and   word deletion can be well explained as that remov-   ing a sentence from a reference summary reduces   a great amount of key information while removing   a word from a sentence changes it syntactically .   We found that word - level mutations are less useful   for content / fact aspects , probably because of the   inertia of the context after words are altered .   Which training domain / dataset should I use ?   Due to the composition of summarizers and the lim-   ited data size in human evaluation , it is very hard   to get a consistent ranking of metrics on different   datasets ( Bhandari et al . , 2020 ) . For example , in   Table 1 , Billsumm outperforms all baselines and its   peers on TAC2010 but not the case on Newsroom   and RealSumm .   Still , the impact of training domain seems man-   ageable . The average absolute deviations across   the training datasets / domains are given at the bot-   tom of Tables 1 and 2 . They mostly below 3.5 % . A   qualitative analysis shows that the variation seems   more due to the characteristics of the text thanthe domain . Legislative bills ( Billsum ) have lots   of short , hierarchical clauses and thus differ from   common English greatly . Scientific papers have   many equations and cross - references . There are   also many occurrences of LTEX or MathML in the   dataset arXiv . On top of that , all our experiments   use different training and test domains . Hence we   would say that the impact of domain variation is   very small .   4 Conclusion   In this paper , we propose a weakly supervised   approach to summary quality evaluation . A few   mutation methods are introduced to make use of   the massive , precious human written summaries in   summarization datasets . In cross - domain experi-   ments , our approach achieves better performance   than baselines , especially on linguistic aspects . We   hope this proof - of - concept study can inspire more   reference - free summary evaluation .   Acknowledgments   Bao , Luo , Li , and He ’s work in this paper is par-   tially supported by National Science Foundation   ( NSF ) grants No . MCB-1821828 and No . CNS-   1817089 . The authors would also like to thank   reviewers who have given precious feedback on   improving this work .   References   52454   62455A Dataset statistics   For test sets :   •TAC2010 Guided Summarization Task Set   Aconsists of 46 topics , each of which is asso-   ciated with a set of 10 documents . We evalu-   ate the metrics over summaries generated by   43 systems .   •Newsroom contains human - rated summaries   generated by 7 systems for 60 documents .   •RealSumm sampled 100 documents from the   CNN / DailyMail test set , and collected human   ratings for summaries generated by 11 extra-   tive systems and 14 abstractive systems .   For training sets , the numbers of pairs of docu-   ments and reference summaries in the train split   are :   •Billsum : 18,949   •Scientific papers / arXiv : 203,037   •Big - Patent : 1,207,222   For each dataset , we use the entire ( except for Big-   Patent , 10 % due to its huge size ) train split in   Google Tensorflow Datasets for training .   B Computational environment and cost   All experiments were carried out on one RTX3090   GPU installed on a desktop computer . The training   takes about a week for all three training datasets .   C Another type of mutation   In addition to the three mutation methods men-   tioned already , we have another method called cros-   spairing .   Illustrated in Figure 3 , it is inspired by the next-   sentence prediction ( NSP ) task in original BERT   training . Given a document and its reference sum-   mary , we create negative data by pairing the docu-   ment with reference summaries of other documents .   72456We assign the label 0 to a mismatching document-   summary pair , and the label 1 to any original pair   of a document and its reference summary .   D Complete empirical results   Due to space limit , we were only able to present   the result of the best mutation method in § 3.5 . Full   results are given in Tables 3 , 4 , 5 , and 6 . Pear-   son ’s for LS - Score is unable to be produced due to   reasons explained in the footnote on page 4 .   82457   92458