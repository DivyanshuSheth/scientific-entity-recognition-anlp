  Young - Suk Lee † , Ramón Fernandez Astudillo † , Thanh Lam Hoang ‡ ,   Tahira Naseem † , Radu Florian † , Salim Roukos †   { ysuklee,tanseem,raduf,roukos}@us.ibm.com   ramon.astudillo@ibm.com   t.l.hoang@ie.ibm.com   IBM Research AI † , IBM Research - Ireland ‡   Abstract   AMR parsing has experienced an unprecen-   dented increase in performance in the last three   years , due to a mixture of effects including   architecture improvements and transfer learn-   ing . Self - learning techniques have also played   a role in pushing performance forward . How-   ever , for most recent high performant parsers ,   the effect of self - learning and silver data aug-   mentation seems to be fading . In this paper we   propose to overcome this diminishing returns   of silver data by combining Smatch - based en-   sembling techniques with ensemble distillation .   In an extensive experimental setup , we push   single model English parser performance to a   new state - of - the - art , 85.9 ( AMR2.0 ) and 84.3   ( AMR3.0 ) , and return to substantial gains from   silver data augmentation . We also attain a new   state - of - the - art for cross - lingual AMR parsing   for Chinese , German , Italian and Spanish . Fi-   nally we explore the impact of the proposed   technique on domain adaptation , and show that   it can produce gains rivaling those of human   annotated data for QALD-9 and achieve a new   state - of - the - art for BioAMR .   1 Introduction   Adoption of the Transformer architecture ( Vaswani   et al . , 2017 ) for Abstract Meaning Representation   ( AMR ) parsing ( Cai and Lam , 2020 ; Fernandez As-   tudillo et al . , 2020 ) as well as pretrained language   models ( Bevilacqua et al . , 2021 ; Zhou et al . , 2021b ;   Bai et al . , 2022 ) have enabled an improvement of   above 10Smatch points ( Cai and Knight , 2013 ) ,   the standard metric , in the last two years .   Data augmentation techniques have also shown   great success in pushing the state - of - the - art of   AMR parsing forward . These include generat-   ing silver AMR annotations with a trained parser   ( Konstas et al . , 2017 ; van Noord and Bos , 2017 ) ,   with multitask pre - training and fine - tuning ( Xu   et al . , 2020 ) as well as combining AMR to source   text and silver AMR generation ( Lee et al . , 2020)and stacked pre - training of silver data from dif-   ferent models – from low performance to high   performance silver data ( Xia et al . , 2021 ) . How-   ever , the latest BART - based state - of - the - art parsers ,   have shown diminishing returns for data augmenta-   tion . Both SPRING ( Bevilacqua et al . , 2021 ) and   Structured - BART ( Zhou et al . , 2021b ) gain a mere   0.5Smatch from self - learning , compared with over   1point gains of the previous , less performant , mod-   els . Since performance scores are already above   where inter annotator agreement ( IAA ) is assumed   to be , i.e. 83 for newswire and 79 for web text   reported in ( Banarescu et al . , 2013 ) , one possible   explanation is that we are reaching some unavoid-   able performance plateau .   In this work we show that we can achieve sig-   nificant performance gains close to 2 Smatch point   with the newly proposed data augmentation tech-   nique , contrary to the results from the previous   state - of - the - art systems . The main contributions of   this paper are as follows :   •We propose to combine Smatch - based model   ensembling ( Barzdins and Gosko , 2016 ;   Hoang et al . , 2021 ) and ensemble distillation   ( Hinton et al . , 2015 ) of heterogeneous parsers   to produce high quality silver data .   •We offer a Bayesian ensemble interpretation   of this technique as alternative to views such   as Minimum Bayes Risk decoding ( Goel and   Byrne , 2000 ) and name the technique Maxi-   mum Bayes Smatch Ensemble ( MBSE ) .   •Applied to English monolingual parsing ,   MBSE distillation yields a new single system   state - of - the - art ( SoTA ) on AMR2.0 ( 85.9 ) and   AMR3.0 ( 84.3 ) test sets .   •Trained with Structured - mBART , it yields   new SoTA for Chinese ( 63.0 ) , German ( 73.7),5379Italian ( 76.1 ) and Spanish ( 77.1 ) cross - lingual   parsing .   •Applied to domain adaptation , MBSE distilla-   tion achieves the performance comparable to   human annotations of QALD-9 training data   and achieves new SoTA on BioAMR test set .   •We release QALD-9 - AMR treebankat ,   which comprises 408 training and 150 test   sentences .   2 Maximum Bayes Smatch Ensemble   Ensemble distillation ( Hinton et al . , 2015 ) inte-   grates knowledge of different teacher models into   a student model . For sequence to sequence models ,   e.g. machine translation , it is possible to ensemble   models by combining probabilities of words given   context at each time step ( Kim and Rush , 2016 ; Fre-   itag et al . , 2017 ) . Syntactic and semantic parsers   model a distribution over graphs that is harder to   integrate across teacher models in an optimal way .   For particular cases like dependency parsing , it is   possible to ensemble teachers based on the notion   of edge attachment ( Kuncoro et al . , 2016 ) , which   is related to the usual evaluation metric , Label At-   tachment Score ( LAS ) . However , AMR graphs are   quite complex and not explicitly aligned to words .   The standard Smatch ( Cai and Knight , 2013 ) metric   approximates the NP - Complete problem of align-   ing nodes across graphs with a hill climbing algo-   rithm . This illustrates the difficulty of achieving   consensus across teachers for AMR ensembling .   Prior work ensembling AMR graphs has lever-   aged Smatch directly or its hill climbing strategy   for ensembling . The ensemble in ( Barzdins and   Gosko , 2016 ) selects , among a number of candi-   date AMRs , the one that has the largest average   Smatch with respect to all sampled AMRs . The   ensemble in ( Hoang et al . , 2021 ) , modifies the can-   didate AMRs to increase consensus as measured   by coverage . Then it selects from the union of orig-   inal and modified graphs for the one with highest   coverage or largest average Smatch . One possible   intepretation of both techniques is that of Minimum   Bayes Risk ( MBR ) decoding , a well established   method in Automatic Speech Recognition ( ASR )   ( Goel and Byrne , 2000 ) and Machine Translation   ( MT ) ( Kumar and Byrne , 2004 ) . Assuming that   we have a model predicting a graph from an inputsentence p(g|w ) , normal decoding entails search-   ing among model outputs gfor the one that has the   highest likelihood according to the model p(g|w ) .   MBR searches instead for the model output that   minimizes the risk with respect to the distribution   of possible human ( gold ) outputs for a given input   ˆg= arg min{E{R(g , g ) } }   where p(g|w)is the distribution of correct hu-   man outputs , e.g. given by multiple annotators ,   andRis a risk function that measures how severe   deviations from gare . In this case risk would be   minus Smatch . Since in practice p(g|w)is not   available , MBR takes often the strong assumption   of replacing p(g|w)by the model distribution   itself p(g|w ) .   Here we suggest another Bayesian interpretation ,   that requires less strong assumptions than MBR , a   Bayesian model ensemble ( Wilson and Izmailov ,   2020 ) . Indeed techniques above can be seen as   solving   ˆg= arg max{E{Smatch ( g,˜g ) } }   where p(M | D ) is the distribution of models M   given training data D , approximated by a sample   average of models of different architectures or dif-   ferent random seeds , and   ˜g= postarg maxp(y|y , w )   is the output of a conventional decoding process   for each parser prediction distribution p , in-   cluding post - processing post ( ) . This process dif-   fers across models indexed by M , for example   ycan be transition actions or linearized graphs   andpost ( ) running the state - machine or linearized   graph post - processing . Gis the space of candidate   graphs , which in Barzdins and Gosko ( 2016 ) are   the AMRs resulting from decoding each sample   from p(M | D ) and in Hoang et al . ( 2021 ) are   those same graphs plus the modified pivot graphs .   There is in principle no restriction on how to build   the set G.Decoding a graph g∈ G means here   selecting the member of that set maximizing the ex-   pected Smatch and is different from each parser ’s   decoding process.5380   If we replace Smatch ( ) by an indicator function   on the decoding outputs 1 , then   ˆg= arg max{E{1 } }   recovers majority voting of AMR graphs . Since the   space of graphs is exponentially large on the input   size , this would be too sparse to attain meaningful   vote counts . The propagation of the uncertainty in   p(M | D ) through the Smatch ( ) transformation   both solves the sparsity problem , and allows op-   timization on a space that is better related to the   target metric . The method will be henceforth de-   scribed here as Maximum Bayes Smatch Ensemble   distillation ( MBSE distillation ) .   In what follows , we will consider three versions   for ensembling , the Smatch version of Hoang et al .   ( 2021 ) ( graphene - Smatch ) , the average - Smatch se-   lection of Barzdins and Gosko ( 2016 ) , and a greedy   version of Barzdins and Gosko ( 2016 ) where we   select the two highest Smatch AMRs and from that   pair , keep the graph with the highest Smatch with   respect to the remaining graphs ( greedy - select ) .   The greedy - select algorithm is given in Algorithm 1   of Appendix A and performs similarly to the   average - Smatch of Barzdins and Gosko ( 2016 ) .   3 Silver Training Strategy   We now describe the AMR silver training strategy   proposed in this work . This strategy creates high   quality English and cross - lingual AMR annotations   for unlabeled data with MBSE and alternative input   sentences of gold AMRs via AMR - to - text .   As depicted in Fig . 1 , we start with 1 ) a set   of gold - labeled ( English sentence , AMR ) pairs,2 ) a set of unlabeled English sentences and 3 )   pre - trained English - to - foreign language Machine   Translation systems . Assuming Noff - the - shelf   AMR parsers , we train each of the Nparsers using   the gold data with their respective training proce-   dure . More than one random seed may be trained   for some parsers , leading to more than NAMR   parses for each input sentence .   After the parsers have been trained , we use them   to parse the unlabeled English text as in Konstas   et al . ( 2017 ) . Interpreting the set of trained mod-   els as samples of the model distribution , we apply   the MBSE distillation methods described in Sec-   tion . 2 . We apply all variations of the MBSE algo-   rithms including graphene - Smatch , greedy - select   and average - Smatch algorithms .   For English parsers , the MBSE distilled AMR   annotations are added to the human - annotated gold   treebanks for enhanced model training . For cross-   lingual parsers , we translate all English input sen-   tences to the target foreign languages and train re-   spective cross - lingual parsers with pairs of ( Foreign   language input sentences , AMR graphs in English ) ,   following ( Damonte and Cohen , 2018 ) .   Following Lee et al . ( 2020 ) , we also apply an   AMR - to - text model ( Mager et al . , 2020 ; Ribeiro   et al . , 2021 ; Bevilacqua et al . , 2021 ) to generate   additional sentences for human - annotated AMR .   We filter out the generated texts if they are too sim-   ilar ( BLEU > 0.9 ) or too dissimilar ( BLEU < 0.1 )   to the original input texts , as measured by BLEU   ( Papineni et al . , 2002 ) . AMR - to - text generationis   used for cross - lingual AMR parser training only.5381For Standard Experiments For Domain Adaptation   Dataset Split Sents Tokens Dataset Split Sents Tokens   AMR2.0 Train 36,521 653 K QALD-9 - AMR ( new)Train 408 3,475   Test 1,371 30 K Test 150 1,441   Dev . 1,368 29 K   AMR3.0 Train 55,635 1 M Bio AMR Train 5,452 231 K   Test 1,898 39 K Test 500 22 K   Dev . 1,722 37 K   LP Test 1,562 21 K   PropBank silver20 K 386 K SQuAD2.0 - Q silver135 K 1.5 M   SQuAD2.0 - C silver70 K 2 M BioNLP - ST-2011 silver15 K 460 K   Ontonotes5.0 silver59 K 1.1 M CRAFT silver27 K 740 K   WikiText-103 silver70 K 2 M PubMed silver26 K 750 K   4 Experimental Setup   4.1 Corpus Statistics and QALD-9 - AMR   Table 1 details the corpora considered for the   standard benchmark experiments on AMR2.0 and   AMR3.0 test sets ( lef ) and out - of - domain data used   for domain adaptation experiments ( right ) . Silver   indicates the unlabeled data for silver AMR ac-   quisition . SQuAD2.0 - Q(uestions ) are for QALD-   9 ( silver ) and PubMed , BioNLP-2011 ( Kim   et al . , 2011 ) and CRAFT ( Cohen et al . , 2017 ) for   BioAMR ( silver ) .   Since there were no human annotations of   QALD-9 corpus , we created QALD-9 - AMR tree-   bank . QALD-9 training / test data have been anno-   tated by 3 skilled resident human annotators with   experience in AMR annotations over a year . Each   of the annotators annotated both the train and test   data sets , followed by cross validation by each   other . The final annotations were adjudicated by   the most experienced annotator . Inter - annotator   agreement ( IAA ) rate on a subset of 158 training   sentences is over 95 % in Smatch . The data is made   publicly available under an Apache2 license .   4.2 Parsing Models   We use 4 off - the - shelf AMR parsers to parse un-   annotated raw texts . We train the parsers following   their standard configurations .   APT ( Zhou et al . , 2021a)is a transition - based   parser that combines hard attention over sentences   with a target side action pointer mechanism to de-   couple source tokens from node representationsand address alignments . Cross - attention of all de-   coder layers is used for action - source alignment .   SPRING Bevilacqua et al . ( 2021)fine - tunes   BART ( Lewis et al . , 2020 ) to predict linearized   AMR graphs , avoiding complex pipelines .   Structured - BART Zhou et al . ( 2021b)mod-   els the transition - based parser state within a   pre - trained BART architecture , outperforming   SPRING . This is the main parser for our work .   AMRBART Bai et al . ( 2022)improves the   structure awareness of pre - trained BART over   AMR graphs by introducing node / edge denoising   and sub - graph denoising tasks , for graph - to - graph   pre - training , achieving significant improvement   over previous BART - based systems .   4.3 Structured - mBART   For cross - lingual AMR parsing , we adapt   Structured - BART by replacing the pretrained   BART with mBART of ( Liu et al . , 2020 ) , hence-   forth Structured - mBART . The codebase is made   publicly available under an Apache2 license .   Structured - mBART diverges from Structured-   BART mainly in input processing and vocabulary :   •For task vocabulary , Structured - mBART in-   cludes ~250 K sentencepiece tokens of ( Kudo ,   2018 ) including 25 language tags , e.g. es_XX ,   whereas Structured - BART includes ~50 K   BPE tokens of ( Sennrich et al . , 2016 ) .   •We append the source language tag to the end5382Models AMR2.0 AMR3.0 Q9AMR LP BioAMR   APT ( Zhou et al . , 2021a ) 83.0 81.1 83.7 79.0 55.2   Structured - BART ( Zhou et al . , 2021b ) 84.6 83.1 87.7 81.0 62.4   SPRING(Bevilacqua et al . , 2021 ) 84.2 83.2 87.7 81.3 61.6   SPRING(Bevilacqua et al . , 2021 ) 83.8 82.9 86.4 81.0 60.5   AMRBART ( Bai et al . , 2022 ) 85.4 84.2 88.0 82.3 63.4   aver.-Smatch ( A ) ( Barzdins and Gosko , 2016 ) 86.2 84.9 89.0 82.9 64.1   graphene - Smatch ( P ) ( Hoang et al . , 2021 ) 86.7 85.4 89.3 83.1 65.8   greedy - select ( G ) 85.9 84.8 88.8 82.8 63.9   of each input sentence without specifying the   target language tag for Structured - mBART .   •For Structured - mBART , we set the learning   rate to 3e−5 , cf.1e−4of Structured - BART ,   and move the layer normalization to the be-   ginning of each transformer block .   We obtain contextualized embeddings from the   pre - trained mBART for multilingual input sentence   representations . For target action sequences , we   map the sentencepiece tokens to the corresponding   target token , by averaging all values from the sen-   tencepiece tokens corresponding to the target token .   For German , Italian and Spanish input texts , we   apply the tokenizer from JAMR parserbefore sen-   tencepiece tokenization . For Chinese , we directly   apply the sentencepiece tokenizer .   5 Results   To explore the effect of the proposed MBSE distil-   lation and training strategy , we consider an exten-   sive experimental setup including standard English   benchmarks ( Section 5.1 ) , cross - lingual bench-   marks ( Section 5.2 ) and out of domain data sets   ( Section 5.3).For model training and selection   details , see Appendix B and Appendix C.   We first provide the performance evaluation of   each ensembling technique used in MBSE in Ta-   ble 2 to demonstrate the effectiveness of the en-   semble techniques by themselves . We test the algo-   rithm on the standard test data sets from AMR2.0   and AMR3.0 and three out - of - domain data sets ,   Q9AMR ( QALD-9 - AMR ) , LP ( Little Prince ) andBioAMR in Table 1 . We consider here only stan-   dard English AMR parsing . As expected , all MBSE   algorithms , average - Smatch , graphene - Smatch and   greedy - select , improve individual models by large   margins . Note that while the ensembles outperform   single model state - of - the - art by a large margin , the   use of heterogeneous ensembles of models is com-   putationally prohibitive in practice , both due to   the cost of running different models but also the   ensembling techniques .   5.1 English AMR Parsing   As displayed in Table 1 , we consider the stan-   dard AMR2.0 ( LDC2017T10 ) and AMR3.0   ( LDC2020T02 ) treebank as gold data . For en-   semble distillation , we use the data sets denoted   by silverfor comparison with previous work , and   silverand silverto investigate the impact of un-   labeled corpus size on model performance . For   silver , we use all sentence examples in PropBank   ( LDC2004T14 ) . From SQuAD2.0 - C(ontexts )   we filter out the ~92 K sentences , removing bad   utf8 encoding ( ~7 K ) and ill - formed disconnected   graphs produced by APT ( ~15 K ) . Silvercom-   prises Ontonotes5.0 ( LDC2013T19 ) and silver   WikiText-103   The results are shown in Table 3 . The lower   part of the table ( denoted by Ours ) compares the   performances of Structured - BART in various silver   data augmentation setups including our proposed   MBSE distillation . With the same unlabeled cor-   pus silver , greedy - select distillation improves 1.0   Smatch point on AMR2.0 ( 84.2 vs. 85.2 ) and 1.5   Smatch point on AMR3.0 ( 82.0 vs. 83.5 ) over the5383Models silver AMR2.0 AMR3.0   Naseem et al . ( 2019 ) 75.5 -   Zhang et al . ( 2019a ) 76.3 -   Zhang et al . ( 2019b ) 77.0 -   Cai and Lam ( 2020 ) 80.2 -   Fernandez Astudillo et al . ( 2020 ) 80.2 -   Lyu et al . ( 2020 ) - 75.8   Lee et al . ( 2020 ) 85 K 81.3 -   Xu et al . ( 2020 ) 14 M 81.4 -   Bevilacqua et al . ( 2021 ) 200 K 84.5 83.0   Zhou et al . ( 2021a ) 70 K 82.6 80.3   Xia et al . ( 2021 ) 1.8 M 84.2 -   Bai et al . ( 2022 ) 200 K 85.4 84.2   Zhou et al . ( 2021b ) sep - voc joint - voc sep - voc joint - voc   Structured - BART - baseline 84.0 84.2 82.3 82.0   + self - trained silver90 K - 84.7 82.7 82.6   + self - trained silver+ ensemble dec . 90 K - 84.9 83.1 -   Ours below ( Struct - BART ) sep - voc joint - voc sep - voc joint - voc   + SPRING silver90 K 84.8 84.8 83.0 83.2   + SPRING + self - trained silver(50:50 ) 90 K 84.8 84.7 83.0 83.2   Ensemble-4 distillation ( APT + Structured - BART + SPRING+ SPRING )   + MBSE - P silver90 K 85.185.1 83.2 83.5   + MBSE - G silver90 K 85.0 85.2 83.4 83.5   + MBSE - G silver149 K 85.385.4 83.6 83.7   + MBSE - G siver219 K 85.385.5 83.7 83.9   + MBSE - G silver+ ensemble dec . 219 K 85.6 85.7 84.0 84.2   Ensemble-5 distillation ( APT + Structured - BART + SPRING+ SPRING+ AMRBART )   + MBSE - A silver90 K 85.3 83.6   + MBSE - A silver149 K 85.5 84.0   + MBSE - A silver219 K 85.7 84.1   + MBSE - A silver+ ensemble dec . 219 K 85.9 84.3   Structured - BART baselines . Graphene - Smatch dis-   tillation performs similarly to greedy - select one .   To isolate the effect of ensembling , we provide   two additional baselines : 1 ) silver obtained from   SPRING , which is expected to have complemen-   tary information to self - trained silver , and 2 ) an   equal mixture of SPRING and Structured - BART   ( random 50:50 ) , which tests if the MBSE selection   strategy bears any effect . MBSE distillation outper-   forms these two baselines by between 0.2and0.5   Smatch point , depending on the scenario , proving   that MBSE selection has a clear positive effect .   We also investigate the impact of unlabeled cor-   pus size on model performance by adding silverand silverto silver , i.e. silverand silver .   We observe additional 0.3 - 0.4 improvement , com-   plementary to the one obtainable with conventional   ensemble decoding . This pushes the numbers to   85.7and84.2 , setting a new SoTA for single sys-   tem with 4 model ensemble ( Ensemble-4 ) distilla-   tion . Note that using 5 model ensemble ( Ensemble-   5 ) distillation moves the Smatch scores even higher   to 85.9 for AMR2.0 and 84.3 for AMR3.0 .   5.2 Cross - lingual AMR Parsing   For cross - lingual AMR parsing , we consider the   well known cross - lingual extension of AMR2.0   ( Damonte and Cohen , 2018 ) . Our cross - lingual5384   parsers are trained with Structured - mBART , always   using separate vocabulary ( sep - voc ) . Input sen-   tences of the English training data are machine   translated into the target languages with WLTto   generate cross - lingual parser training data .   Table 4 shows the results on the human translated   AMR2.0 test set , following standard practices . We   provide results for recently published cross - lingual   AMR parsers and different silver training versions   of Structured - mBART . Structured - mBART with   4 model ensemble ( Ensemble-4 ) distillation with   just silverimproves the Smatch score by 2.1 to   2.6 over the Structured - mBART baselines , out-   performing very strong previous SoTA from ( Cai   et al . , 2021a ) on Chinese and Spanish and tied on   Italian . Increasing the input sentence diversity via   AMR - to - text generation and ensemble decoding   further improve the system performances , attaining   new cross - lingual SoTA on all four languages . In-   creasing the silver training data size to silver   and using 5 model ensemble ( Ensemble-5 ) push   the numbers higher by 0.2 - 0.5 Smatch points .   ( Uhrig et al . , 2021 ) report that translate - and-   parse pipelines outperform the conventional cross - lingual parsers , we thus include translate - and - parse   from the combination of WLT and Structured-   BART + MBSE distillation . This out - performs   the cross - lingual parsers by 0.6 - 1.0 Smatch on all   languages except for Spanish , when trained with   the same MBSE avg.-Smatch silverdata .   Comparing the fine - grained F1 scores for cross-   lingual parsers with those for English , as shown   in Table 5 , we observe that cross - lingual parsers   are particularly worse than English for negation .   For instance , German negations are often realized   as a compound , as in nicht tarifäre ( non - tariff ) ,   which is aligned to the non - negated stem portion   of the concept tariff , losing its negation meaning .   We observe similar issues in English with prefixed   negations such as unhappy , inadequate , atypical .   5.3 Domain Adaptation   We use the AMR2.0 version of BioAMR ( medical   domain ) as this has clearly defined partitionsand   was used in Bevilacqua et al . ( 2021 ) . We also use   QALD-9 - AMR , constructed from QALD-9 data   ( Usbeck et al . , 2018 ) , a corpus of natural language5385Languages Smatch Unlabeled NoWSD Concepts NER Neg . Wiki Reentrant SRL   EN - mono 85.9 89.0 86.3 92 93 75 81 78 85   DE - cross 73.7 77.9 73.8 75 89 48 79 61 68   ES - cross 77.1 81.4 77.5 81 89 62 79 67 74   IT - cross 76.1 80.4 76.3 79 90 56 78 65 73   ZH - cross 63.0 67.9 63.1 65 85 35 70 51 58   DE - pipeline 74.6 78.9 74.8 75 91 51 80 62 68   ES - pipeline 77.1 81.1 77.3 80 91 61 79 66 74   IT - pipeline 76.7 80.9 76.9 79 91 58 80 65 73   ZH - pipeline 64.0 68.9 64.0 66 86 40 74 51 59   questions for executable semantic parsing ( Kapani-   pathi et al . , 2021 ) . Corpus statistics of the domain   adaptation data is summarized in Table 1 .   Table 6 shows the experimental results . Re-   sults for SPRING are taken from Bevilacqua et al .   ( 2021 ) . For each test set , we report the results un-   der three different training conditions , all of which   include either AMR2.0 or AMR3.0 treebank in the   training data : ( 1 ) use only silver data with MBSE   distillation , ( 2 ) use only domain gold sentences ,   ( 3 ) use both silver data and domain gold sentences .   Since BioAMR is annotated in AMR2.0 style and   QALD-9 - AMR in AMR3.0 style , we use the corre-   sponding Structured - BART models as indicated in   the table .   As for BioAMR data , MBSE distillation ( with   both graphene - Smatch and greedy - select ) on   silver – comprising PubMed ( LDC2008T20 ,   LDC2008T21 ) , BioNLP - ST-2011 and CRAFT –   improves over the Structured - BART baseline by   6.5 Smatch point ( 60.4 vs. 66.9 ) . However , adding   just 201 domain gold sentences to AMR2.0 tree-   bank results in 11.9 Smatch point improvement   over the baseline ( 60.4 vs. 72.3 ) . A close inspec-   tion shows that this is largely due to the NER score   improvement , as shown in the column under NER ,   i.e. NER score 27.0 in Structured - BART ( AMR2.0 )   vs. 68.0 after adding 201 domain gold sentences .   The dramatic impact of NE coverage no longer   holds when we double the domain gold sentences   from 201 to 403 . In fact , MBSE greedy - select   silver+ 201 domain gold sentences ( 75.8 ) is more   effective than doubling the domain gold sentences   ( 74.3 ) . Finally , by combining MBSE distillation on   silverwith 5 K domain gold sentences , the system   achieves 81.3 Smatch , outperforming the previousSoTA by 1.4 .   Regarding QALD-9 - AMR data , MBSE distilla-   tion on silver , i.e. SQuAD - Q(uestion ) sentences ,   Training Data Smatch NER   BioAMR Evaluations   SPRINGDFS59.7   SPRINGDFS+ silver 59.5   SPRINGDFS(In domain ) 79.9   Ours   Struct - BART ( AMR2.0 ) 60.4 27.0   + MBSE - G silver63.2 31.0   + MBSE - G silver66.9±0.230.0   + MBSE - P silver66.9±0.231.0   +201 domain gold sent . 72.3±0.268.0   +403 domain gold sent . 74.3±0.270.0   +5 K domain gold sent . 79.8±0.280.0   + MBSE - G silv.+201 gold 75.8±0.370.0   + MBSE - G silv.+5 K gold 81.3±0.281.0   QALD-9 - AMR Evaluations   Ours   Struct - BART ( AMR3.0 ) 87.2 84.0   + MBSE - G silver88.0 88.0   + MBSE - G silver89.5±0.185.0   + MBSE - P silver89.3±0.287.0   +200 domain gold sent . 88.5±0.584.0   +408 domain gold sent . 89.8±0.186.0   + MBSE - G silv.+200 gold 90.0±0.387.0   + MBSE - G silv.+408 gold 90.1±0.187.05386Models BioAMR Q9AMR   Structured - BART 8.7 % 0.7 %   + MBSE silver 3.7 % 0.7 %   +200 domain gold sents 0.6 % 0.7 %   is almost as effective as 408 domain gold sentences   ( 89.8 ) for both graphene - Smatch ( 89.3 ) and greedy-   select ( 89.5 ) algorithms . Combining 408 domain   gold sentences with MBSE greedy - select silver   adds less than 1 Smatch point to 90.1 .   Since MBSE distillation on silverlags behind   the performance of 201 human annotated AMR   for BioAMR , mostly due to low NER scores , we   further analyze the target vocabulary coverage of   named entity ( NE ) types occurring in the test sets .   The analysis is shown in Table 7 . NE types are   equally well covered in all models for Q9AMR   ( QALD-9 - AMR ) . 0.7 % out - of - vocabulary ( OOV )   ratio is caused by a typo in human annotation of   the test set , i.e. country misspelled as countrty . For   BioAMR , however , NE type OOV ratio of MBSE   silver model is 3.7 % , e.g. protein - segment , macro-   molecular - complex , substantially higher than 0.6 %   of the model trained with 201 domain gold sen-   tences . When the NE type is OOV , there is no   chance for the system to produce the missing NE   type , let alone predicting it correctly , underscoring   the challenges posed by domain specific concepts   unavailable elsewhere .   6 Related Work   There have been numerous works applying ensem-   ble / knowledge distillation ( Hinton et al . , 2015 ) to   machine translation ( Kim and Rush , 2016 ; Freitag   et al . , 2017 ; Nguyen et al . , 2020 ; Wang et al . , 2020 ,   2021 ) , dependency parsing ( Kuncoro et al . , 2016 )   and question answering ( Mun et al . , 2018 ; Ze et al . ,   2020 ; You et al . , 2021 ; Chen et al . , 2012 ) . Re-   garding ensembling AMR graphs , Barzdins and   Gosko ( 2016 ) propose choosing the AMR with   highest average sentence Smatch to all other AMRs .   Hoang et al . ( 2021 ) proposed a more complex tech-   nique capable of building new AMRs by exploiting   Smatch ’s hill climbing algorithm . Our work brings   together ensemble distillation and Smatch - based   ensembling and shows that it can provide substan - tial gains over the standard self - training .   Damonte and Cohen ( 2018 ) show that it may   be possible to use the original AMR annotations   devised for English as representations of equiva-   lent sentences in other languages . Damonte and   Cohen ( 2018 ) ; Sheth et al . ( 2021 ) propose annota-   tion projection of English AMR graphs to target   languages to train cross - lingual parsers , using word   alignments . Blloshmi et al . ( 2020 ) show that one   may not need alignment - based parsers for cross-   lingual AMR , and model concept identification as a   seq2seq problem . Procopio et al . ( 2021 ) reframe se-   mantic parsing as multilingual machine translation   ( MNMT ) and propose a seq2seq architecture fine-   tuned on pretrained - mBART with an MNMT ob-   jective . Cai et al . ( 2021b ) propose to use bilingual   input to enable a model to predict more accurate   AMR concepts . Xu et al . ( 2021 ) propose a cross-   lingual pretraining approach via multitask learning   for AMR parsing . Cai et al . ( 2021a ) propose to use   noisy knowledge distillation for multilingual AMR   parsing . We introduce Structured - mBART and at-   tain new SoTA in Chinese , German , Italian and   Spanish cross - lingual parsing by applying MBSE   distillation and AMR - to - text .   We subsume domain adaptation under data aug-   mentation with MBSE distillation , where the only   difference between the two lies in the properties of   the unlabeled data . The unlabeled data is drawn   from the target domain for the purpose of domain   adaptation rather than those similar to the source   training data for data augmentation in general .   7 Conclusion   We proposed a technique called Maximum Bayes   Smatch Ensemble ( MBSE ) distillation , which   brings together Smatch - based model ensembling   Barzdins and Gosko ( 2016 ) ; Hoang et al . ( 2021 )   and ensemble distillation Hinton et al . ( 2015 ) of het-   erogeneous parsers , to significantly improve AMR   parsing . The technique generalizes well across var-   ious tasks and is highly effective , leading to a new   single system SoTA in English and cross - lingual   AMR parsing and achieving the performance com-   parable to human annotated training data in domain   adaptation of QALD-9 - AMR corpus . Remaining   technical challenges include tokenization and align-   ment of an input token corresponding to more than   one concept for AMR parsing and identification of   unknown named entities and their types for domain   adaptation.5387References538853895390A Greedy - Select Ensemble Algorithm   Algorithm 1 : Greedy - Select MBSE Algorithm and   Corpus Selection   Input : AMR ... AMRparses from nAMR pars-   ing models , where n≥3   Optionally Require : Smatch score threshold = θ   Output : One - best AMR parseLet bestAMR = NULLfor∀in1≤i , j≤nandi̸=jdo Compute sentence Smatch score   smatch ( AMR , AMR ) , total n(n−1)/2   scores . Pick the highest smatch ( AMR , AMR ) . forEach AMR , where a = iora = jdo Pick the highest smatch ( AMR , AMR ) ifa = ithen bestAMR = AMR else bestAMR = AMR end if ifsmatch ( AMR , AMR ) < θthen bestAMR = NULL { no AMR to be used from this sen-   tence } end if end forend forreturn bestAMR   We start with nparses from nheterogeneous   parsing models , where the minimum number of   parses is 3 . For each input sentence , we com-   pute sentence - level Smatch scores between any two   parses across all nparses , for a total of n(n−1)/2   Smatch scores ( lines 2 - 3 ) . Subsequently , we pick   the two parses AMRand AMRwith the high-   est Smatch score , where AMRdenotes the AMR   parse from the system i(line 4 ) For each of the   two parses , AMRand AMR , we choose the parse   with the higher Smatch score against the rest of   the parses as the best parse ( lines 5 - 11 ) . When   the scores are tied , we select the first parse output   ( equivalent to a random choice of fixed seed ) .   We incorporate an optional parse selection crite-   rion into Algorithm 1 , indicated as Optionally Re-   quire and specified in lines 12 - 15 . The bestAMR   for input sentence is selected for data augmenta-   tion if the Smatch score smatch ( AMR , AMR )   is greater than or equal to the pre - specified value θ . Models AMR2.0 AMR3.0   Structured - BART 34,156 33,200   SPRING 25,129 29,407   SPRING 17,866 17,830   APT 10,235 6,949   Total 87,386 87,386   B Model Structures and Parameter Size   Pre - trained BART and mBART share the same   model configurations except for the vocabulary size .   There are 12 encoder / decoder layers , 16 heads per   layer , 1024 model dimension and 4096 feed for-   ward network ( FFN ) size . BART includes ~50 K   and mBART , ~250 K task vocabulary .   When using separate vocabulary ( sep - voc ) ,   Structured - BART and Structured - mBART use the   same vocabulary as BART and mBART , respec-   tively , for the source . For the target , they create   embedding vectors for action symbols and the tar-   get vocabulary size vary according to the train-   ing data . When using joint vocabulary ( joint - voc ) ,   Structured - BART shares the same vocabulary be-   tween the source and the target , a combination of   BART vocabulary and the additional embedding   vectors for some action symbols .   V ocabulary and parameter sizes for Structured-   BART and Structured - mBART trained with MBSE   distillation are shown in Table 9 and Table 10 , re-   spectively .   C Implementation Details   Our models are implemented with FAIRSEQ   toolkit ( Ott et al . , 2019 ) , trained and tested on a sin-   gle NVIDIA Tesla A100 / V100 GPU with 40 - 80 GB   memory . We use fp16 mixed precision training and   all models are trained on 1 GPU .   For all English AMR parsing models with silver   data , we use the Adam optimizer with β= 0.95391Languages voc size # param   DE ( German ) 34,689 681,894,912   ES ( Spanish ) 34,881 682,288,128   IT ( Italian ) 33,681 679,830,528   ZH ( Chinese ) 59,473 732,652,544   Lgs . vocab base model ens . model   EN joint - voc 60min 60min   DE sep - voc 23min 42min   ES sep - voc 24min 44min   IT sep - voc 22min 40min   ZH sep - voc 30min 60min   andβ= 0.98 . Batch size is set to 1024 maxi-   mum number of tokens with gradient accumulation   over 8 steps . Learning rate schedule is the same as   Vaswani et al . ( 2017 ) with 4000 warm - up steps and   1e−7warm - up initial learning rate and the maxi-   mum learning rate 1e−4 . Dropout rate is 0.2 and   label smoothing rate is 0.01 . These hyper param-   eters are fixed and not tuned for different models   and datasets . All models are trained for 10 epochs   and the best 5 checkpoints are selected based on   the development set Smatch from greedy decod-   ing . Model parameters are averaged over the top   3 and top 5 models . The model that produces the   highest development set score , after beam search   decoding with beam size = 1 , 5 and 10 , is selected   as the final model . Training with MBSE greedy-   select silvertakes 48 - 72 hours , and all other   models with less silver data take less time to train .   For cross - lingual AMR parsing , maximum learn-   ing rate is always set to 3e−5 . Baseline models   trained only on AMR2.0 corpus are trained up to   80 epochs whereas models with silver(and AMR-   to - text ) is trained up to 30 epochs and models with   silver , up to 15 epochs . Model parameters   are updated after gradient is accumulated for 8192   tokens . Dropout rate , label smoothing rate and   model selection criteria are the same as the English   parsers . Training baseline models takes about 10   hours . Training with silvertakes about 24 hours .   Training with silvertakes about 96 - 120 hours . In order to reduce the vocabulary size , which sub-   sequently reduces the model parameter size and   memory requirement , we prune out singleton target   vocabulary for training with silver data .   Inference time for AMR2.0 benchmark test set   is shown in Table 11 , where beam size=10 and   batch size=64 for all languages . EN is decoded on   NVIDIA Tesla A100 and all other languages , on   NVIDIA Tesla V100.5392