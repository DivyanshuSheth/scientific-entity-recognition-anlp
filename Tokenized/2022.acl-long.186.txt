  Yunlong Liang , Fandong Meng , Jinan Xu , Yufeng Chenand Jie ZhouBeijing Key Lab of Traffic Data Analysis and Mining ,   Beijing Jiaotong University , Beijing , ChinaPattern Recognition Center , WeChat AI , Tencent Inc , China   { yunlongliang,jaxu,chenyf}@bjtu.edu.cn   { fandongmeng,withtomzhou}@tencent.com   Abstract   Multimodal machine translation and textual   chat translation have received considerable at-   tention in recent years . Although the conversa-   tion in its natural form is usually multimodal ,   there still lacks work on multimodal machine   translation in conversations . In this work , we   introduce a new task named Multimodal Chat   Translation ( MCT ) , aiming to generate more   accurate translations with the help of the as-   sociated dialogue history and visual context .   To this end , we firstly construct a Multimodal   Sentiment ChatTranslation Dataset ( MSCTD )   containing 142,871 English - Chinese utterance   pairs in 14,762 bilingual dialogues and 30,370   English - German utterance pairs in 3,079 bilin-   gual dialogues . Each utterance pair , corre-   sponding to the visual context that reflects the   current conversational scene , is annotated with   a sentiment label . Then , we benchmark the   task by establishing multiple baseline systems   that incorporate multimodal and sentiment fea-   tures for MCT . Preliminary experiments on   four language directions ( English ↔Chinese   and English ↔German ) verify the potential of   contextual and multimodal information fusion   and the positive impact of sentiment on the   MCT task . Additionally , as a by - product of   the MSCTD , it also provides two new bench-   marks on multimodal dialogue sentiment anal-   ysis . Our work can facilitate research on both   multimodal chat translation and multimodal di-   alogue sentiment analysis .   1 Introduction   Multimodal machine translation ( Huang et al . ,   2016 ; Calixto and Liu , 2017 ) and textual chat trans-   lation ( Wang et al . , 2016 ; Farajian et al . , 2020 ;   Liang et al . , 2021a ) mainly focus on investigatingthe potential visual features and dialogue context ,   respectively . Both of them have received much   attention . Although plenty of studies on them   have been carried out based on either image cap-   tions ( Calixto et al . , 2017 , 2019 ; I ve et al . , 2019 ;   Yin et al . , 2020 ; Yao and Wan , 2020 ) or textual   dialogues ( Wang et al . , 2017 ; Maruf et al . , 2018 ;   Liang et al . , 2021c ) , to our knowledge , little re-   search work has been devoted to multimodal ma-   chine translation in conversations . One important   reason is the lack of multimodal bilingual conver-   sational datasets .   Generally , conversation in its natural form is   multimodal ( Poria et al . , 2019 ; Liang et al . , 2021b ) .   When humans converse , what a speaker would say   next depends largely on what he / she sees . That is ,   the visual information plays a key role in ( i ) supple-   menting some crucial scene information ( e.g. , the   specific locations or objects , or facial expressions ) ,   ( ii ) resolving ambiguous multi - sense words ( e.g. ,   bank ) , and ( iii ) addressing pronominal anaphora   issues ( e.g. , it / this ) . For instance , as shown in Fig . 1   ( a ) , the image obviously points out the current loca-   tion “ on the sea ” , which may help disambiguate the   meaning of “ course ” in the utterance X. Specifi-   cally , the dialogue history ( i.e. , talking about mar-   itime affairs ) and the corresponding visual context   ( i.e. , on the sea / boat ) assist us to determine that the   word “ course ” means “ route / direction ” instead of   “ curriculum ” . In Fig . 1 ( b ) , the visual context indi-   cates object information , i.e. , the “ defibrillator ” in   X , which may help with translation . In Fig . 1 ( c ) ,   the image of the utterance Xalso demonstrates   that it can provide appropriate candidates ( i.e. ,the   jeans ) when translating the pronoun “ these ” . Be-   sides , the image offers some clues to judge the sen-   timent when it is hard to judge the polarity based   only on the utterance ( e.g. ,Yin Fig . 1 ( b ) and X   in Fig . 1 ( c ) ) . All of the above call for a real - life   multimodal bilingual conversational data resource   that can encourage further research in chat transla-2601   tion .   In this work , we propose a new task named   Multimodal ChatTranslation ( MCT ) , with the   goal to produce more accurate translations by   taking the dialogue history and visual context   into consideration . To this end , we firstly con-   struct a Multimodal Sentiment ChatTranslation   Dataset ( MSCTD ) . The MSCTD includes over   17k multimodal bilingual conversations ( more than   142k English - Chinese and 30k English - German   utterance pairs ) , where each utterance pair corre-   sponds with the associated visual context indicat-   ing where it happens . In addition , each utterance   is annotated with one sentiment label ( i.e. , posi-   tive / neutral / negative ) .   Based on the constructed MSCTD , we bench-   mark the MCT task by establishing multiple   Transformer - based ( Vaswani et al . , 2017 ) sys-   tems adapted from several advanced representa-   tive multimodal machine translation models ( I ve   et al . , 2019 ; Yao and Wan , 2020 ) and textual chat   translation models ( Ma et al . , 2020 ; Liang et al . ,   2021c ) . Specifically , we incorporate multimodal   features and sentiment features into these mod - els for a suitable translation under the current   conversational scene . Extensive experiments on   four language directions ( English ↔Chinese and   English ↔German ) in terms of BLEU ( Papineni   et al . , 2002 ) , METEOR ( Denkowski and Lavie ,   2014 ) and TER ( Snover et al . , 2006 ) , demonstrate   the effectiveness of contextual and multimodal in-   formation fusion , and the positive impact of sen-   timent on MCT . Furthermore , experiments on the   multimodal dialogue sentiment analysis task of the   three languages show the added value of the pro-   posed MSCTD .   In summary , our main contributions are :   •We propose a new task : multimodal chat trans-   lation named MCT , to advance multimodal   chat translation research .   •We are the first that contributes the human-   annotated multimodal sentiment chat transla-   tion dataset ( MSCTD ) , which contains 17,841   multimodal bilingual conversations , totally   173,240 < English utterance , Chinese / German   utterance , image , sentiment > quadruplets .   •We implement multiple Transformer - based   baselines and provide benchmarks for the new2602task . We also conduct comprehensive analysis   and ablation study to offer more insights .   •As a by - product of our MSCTD , it also facili-   tates the development of multimodal dialogue   sentiment analysis .   2 Tasks   In this section , we firstly clarify the symbol defi-   nition , and then define the proposed Multimodal   Chat Translation task and the existing Multimodal   Dialogue Sentiment Analysis task .   In a multimodal bilingual conversation ( e.g. ,   Fig . 1 ( a ) ) , we assume the two speakers have al-   ternatively given utterances in different languages   foruturns , resulting in X , X , X , X , ... , X   andY , Y , Y , Y , ... , Yon the source and tar-   get sides , respectively , along with the correspond-   ing visual context representing where it hap-   pens : Z , Z , Z , Z , ... , Z. Among these ut-   terances , X , X , X , ... , Xare originally spo-   ken by the first speaker and Y , Y , Y , ... , Y   are the corresponding translations in the tar-   get language . Similarly , Y , Y , Y , ... , Yare   originally spoken by the second speaker and   X , X , X , ... , Xare the translated utterances   in the source language . According to languages   and modalities , we define three types of con-   text : ( 1 ) the dialogue history context of Xon   the source side as C={X , X , X , ... , X } ,   and ( 2 ) that of Yon the target side as   C={Y , Y , Y , ... , Y } , and ( 3 ) the visual di-   alogue context C={Z , Z , Z , ... , Z , Z } .   Multimodal Chat Translation . When translat-   ing the u - th utterance X={x , x , ... , x } ,   the goal of the MCT task is to generate   Y={y , y , ... , y}with the guidance of bilin-   gual dialogue history contexts CandCand   the associated visual context C. Formally , the   probability distribution of the target utterance Y   is defined as follows :   P(Y|X , C ) = Yp(y|y , X , C),(1 )   where y = { y , y , y , ... , y}and   C={C , C , C}.Multimodal Dialogue Sentiment Analysis . Tak-   ing the u - th utterance Xfor example , the task   aims to predict a sentiment label ℓ∈{Positive , Neu-   tral , Negative } for it given the corresponding image   Zand the dialogue history C.   3 Dataset   In this section , we mainly introduce our MSCTD   in five aspects : Data Source § 3.1 , Annotation Pro-   cedure § 3.2 , Annotation Quality Assessment § 3.3 ,   Dataset Statistics § 3.4 , and the introduction of   Related Datasets § 3.5 .   3.1 Data Source   We mainly select the multimodal dialogues from   the public available OpenViDial dataset ( Meng   et al . , 2021 ) , where each monolingual ( English ) ut-   terance corresponds to an image . Since the original   English utterance in OpenViDial is automatically   extracted from the corresponding movie image by   optical character recognition ( OCR ) , it contains a   lot of noises or errors . Furthermore , the lack of as-   sociated translations and sentiment labels for utter-   ances , makes it impossible for directly conducting   research on multimodal chat translation , sentiment-   aware machine translation , and multimodal dia-   logue sentiment analysis with this data . Therefore ,   we further correct the wrong English utterances   and annotate the corresponding Chinese / German   translations and sentiment labels .   3.2 Annotation Procedure   To build the MSCTD , the annotation procedure in-   cludes two steps : automatic annotation and then hu-   man annotation according to the annotation rules .   Automatic Annotation . To improve the annotation   efficiency , we firstly construct a paired English-   Chinese subtitle database . Then , we utilize the   original English utterance to automatically se-   lect its Chinese translation by perfectly matching   the English subtitle in the constructed bilingual   database . As a result , about 78.57 % original En-   glish utterances are paired with Chinese transla-   tions.2603   Human Annotation . Since the full data are large ,   we divide the data into three parts and employ three   annotators who are Chinese postgraduate students   highly proficient in English comprehension . Each   annotator is responsible for annotating one part   according to the following guidelines :   • Check and correct each English utterance ;   •Check and correct the matched Chinese subti-   tle to suit the current conversational scene ;   •For the remaining 21.43 % ( without Chinese   subtitles ) , translate them according to the cor-   rected English utterance , the corresponding   image , and the dialogue history .   Additionally , we employ another three annotators   to label sentiment polarity for each utterance inde-   pendently ( i.e. , each one annotates the full data )   according to the current utterance , the associated   image and the dialogue history . Following Firdaus   et al . ( 2020 ) , majority voting scheme is used for se-   lecting the final sentiment label for each utterance .   Finally , having the conversations in both lan-   guages allows us to simulate bilingual conversa-   tions where one speaker speaks in English and the   other responds in Chinese ( Farajian et al . , 2020 ;   Liang et al . , 2021a ) . Fig . 1 shows three bilin-   gual conversations where the two speakers have   alternatively given utterances , along with their cor-   responding translations . By doing so , we build the   MSCTD.3.3 Annotation Quality Assessment   To evaluate the quality of annotation , we use Fleiss ’   Kappa to measure the overall annotation consis-   tency among three annotators ( Fleiss and Cohen ,   1973 ) . We measure this data from two aspects :   translation quality and sentiment quality .   For translation quality , we measure the inter-   annotator agreement on a subset of data ( sample   50 dialogues with 504 utterances ) , and we ask the   three annotators mentioned above to re - annotate   this subset independently . Then , we invite an-   other postgraduate student to measure the inter-   annotator agreement on the re - annotated subset by   the three annotators . Finally , the inter - annotator   agreement calculated by Fleiss ’ kappa are 0.921 for   English ↔Chinese and 0.957 for English ↔German ,   respectively . They indicate “ Almost Perfect Agree-   ment ” between three annotators .   For sentiment quality , we measure the inter-   annotator agreement on the full dataset . The inter-   annotator agreements calculated by Fleiss ’ kappa   is 0.695 , which indicates “ Substantial Agreement ”   between three annotators . The level is consistent   with previous work ( Firdaus et al . , 2020 ) which can   be considered as reliable .   3.4 Dataset Statistics   As shown in Tab . 1 , the MSCTD contains totally   17,841 bilingual conversations and 142,871/30,3702604   English - Chinese / English - German utterance pairs   with two modalities ( i.e. , text and image ) , where   each utterance has been annotated with onesenti-   ment label . For English - Chinese / English - German ,   we split the dialogues into 13,749/2,066 for train ,   504/504 for valid , and 509/509 for test while keep-   ing roughly the same distribution of the utterance   pair / image , respectively . The detailed annotation   of sentiment labels are also listed in Tab . 1 , where   three labels account for similar proportion .   Based on the statistics in Tab . 1 , the average num-   ber of turns per dialogue is about 10 , and the aver-   age numbers of tokens per turn are 8.2 , 10.9 , and   8.3 for English utterances ( word level ) , Chinese   utterances ( character level ) , and German utterance   ( word level ) , respectively .   3.5 Related Datasets   The related datasets mainly involve three research   fields : multimodal machine translation , textual   chat translation , and multimodal dialogue senti-   ment analysis .   Inmultimodal machine translation , there ex-   ists one dataset : Multi30 K ( Elliott et al . , 2016 ) ,   where each image is paired with one English cap-   tion and two human translations into German and   French . It is an extension of the original En-   glish description dataset : Flickr30 K ( Young et al . ,   2014 ) . Afterwards , some small - scale multimodal   test sets ( about 3k instances ) are released to eval-   uate the system , such as WMT18 test set ( 1,071   instances ) ( Barrault et al . , 2018 ) .   Intextual chat translation , three datasets have   been released : BSD - AMI - ON ( Rikters et al . ,   2020 ) , BconTrasT ( Farajian et al . , 2020 ) , and   BMELD ( Liang et al . , 2021a ) . The BSD - AMI - ON   is a document - aligned Japanese - English conver-   sation corpus , which contains three sub - corpora :   Business Scene Dialogue ( BSD ( Rikters et al . ,   2019 ) ) , Japanese translation of AMI meeting cor-   pus ( AMI ( McCowan et al . , 2005 ) ) , and Japanesetranslation of OntoNotes 5.0 ( ON ( Marcus et al . ) ) .   The BconTrast and BMELD are two human-   annotated datasets , which are extended from   monolingual textual dialogue datasets Taskmaster-   1 ( Byrne et al . , 2019 ) and MELD ( Poria et al . ,   2019 ) , respectively .   Inmultimodal dialogue sentiment analysis ,   the MELD ( Poria et al . , 2019 ) and MEISD ( Fir-   daus et al . , 2020 ) datasets are publicly available .   The MELD dataset is constructed by extending the   EmotionLines ( Hsu et al . , 2018 ) from the scripts   of the popular sitcom Friends . It is similar to   MEISD , which is also built from famous English   TV shows under different genres ( e.g. ,Friends ,   Grey ’s Anatomy , The Big Bang Theory ) .   The resources mentioned above are extensively   used in corresponding fields of research and they   even cover some sub - tasks in MSCTD . However ,   our MSCTD is different from them in terms of both   complexity and quantity .   Firstly , multimodal machine translation datasets   and textual chat translation datasets are either in   multimodal or textual dialogue , while ours includes   both . It is obvious that conducting multimodal ma-   chine translation in conversations is more challeng-   ing due to the more complex scene . Furthermore ,   MSCTD covers four language directions and con-   tains more than 17k human - annotated utterances-   image triplets , which is more than the sum of the   annotated ones in Multi30 K , BSD - AMI - ON , Bcon-   TrasT , and BMELD . Tab . 2 provides information   on the number of available modality , dialogues , and   their constituent utterances for all the five datasets .   What is more , our MSCTD is also annotated with   sentiment labels while they are not .   Secondly , compared with two existing mul-   timodal dialogue sentiment analysis datasets ,   MSCTD ’s quantity of English version is nearly   ten - times of the annotated utterances in MEISD or   MELD . More importantly , our MSCTD provides   an equivalent Chinese multimodal dialogue senti-2605   ment analysis dataset and a relatively small Ger-   man counterpart . Tab . 3 shows the comparison for   all the five datasets , i.e. , MELD , MEISD , and our   MSCTD on three languages .   4 Image Features   Following previous work ( Wang et al . , 2018 ; I ve   et al . , 2019 ; Meng et al . , 2021 ) , we focus on two   types of image representation , namely the coarse-   grained spatial visual feature maps and the fine-   grained object - based visual features .   Coarse - grained Spatial Visual ( CSV ) Features .   We use the ResNet-50 model ( He et al . , 2016 ) pre-   trained on ImageNet ( Deng et al . , 2009 ) to extract a   high - dimensional feature vector f∈Rfor image   Z. These features contain output activations for   various filters while preserving spatial information .   We refer to models that use such features as CSV .   Fine - grained Object - based Visual ( FOV ) Fea-   tures . Since using coarse - grained image features   may be insufficient to model fine - grained visual   elements in images including the specific locations ,   objects , and facial expressions , we use a bag - of-   objects representation where the objects are ob-   tained using an off - shelf Faster R - CNNs ( Ren et al . ,   2015 ) pre - trained on Visual Genome ( Krishna et al . ,   2017 ) . Specifically , for an input image Z , we ob-   tain a set of detected objects from Faster R - CNNs ,   i.e. ,O={o , o , o , ... , o } , where mis   the number of extracted objects and o∈R.   Each object is captured by a dense feature represen-   tation , which can be mapped back to a bounding   box / region ( i.e. , Region - of - Interest ( ROI ) ) . We   refer to models that use such features as FOV .   Both types of features have been used in vari-   ous vision and language tasks such as multimodal   dialogue sentiment analysis ( Firdaus et al . , 2020 ) ,   image captioning ( Xu et al . , 2015 ; Shi et al . , 2021 ) ,   and multimodal machine translation ( I ve et al . ,   2019 ; Lin et al . , 2020 ; Su et al . , 2021).5 Baseline Models   To provide convincing benchmarks for the MSCTD ,   we perform experiments with multiple Transformer-   based ( Vaswani et al . , 2017 ) models for the multi-   modal chat translation task . Additionally , we pro-   vide several baselines for the multimodal dialogue   sentiment analysis task .   5.1 Multimodal Chat Translation   According to different visual features , we divide   the baselines into three categories : text only ( T ) ,   text plus coarse visual features ( T + CSV ) , and text   plus fine - grained visual features ( T + FOV ) .   T : Trans . ( Vaswani et al . , 2017 ): the standard   transformer model , which is a sentence - level neu-   ral machine translation ( NMT ) model ( Yan et al . ,   2020 ; Meng and Zhang , 2019 ; Zhang et al . , 2019 ) ,   i.e. , regardless of the dialogue history . TCT ( Ma   et al . , 2020 ): A unified document - level NMT model   based on Transformer by sharing the first encoder   layer to incorporate the dialogue history , which   is used as the Textual Chat Translation ( TCT )   model by ( Liang et al . , 2021c ) . CA - TCT ( Liang   et al . , 2021c ): A multi - task learning model that   uses several auxiliary tasks to help model generate   coherence - aware translations .   T+CSV : Trans.+Emb ( Vaswani et al . , 2017 ): it   concatenates the image feature to the word em-   bedding and then trains the sentence - level NMT   model . Trans.+Sum ( I ve et al . , 2019 ): it adds the   projected image feature to each position of the en-   coder output . Trans.+Att ( I ve et al . , 2019 ): this   model utilizes an additional cross - attention sub-   layer to attend the image features in each decoder   block . MCT : we implement the multimodal self-   attention ( Yao and Wan , 2020 ) in the encoder to   incorporate the image features into the chat trans-   lation model . CA - MCT : similarly , we incorporate   image features into the multitask - based chat transla-   tion model ( Liang et al . , 2021c ) by the multimodal   self - attention .   T+FOV : Trans.+Con ( Vaswani et al . , 2017 ): it   concatenates the word sequence to the extracted   object sequence and thus obtains a new sequence   taken as the input of the sentence - level NMT model .   Trans.+Obj ( I ve et al . , 2019 ): it is a translate - and-   refine model ( two - stage decoder ) where the im-   ages are only used by a second - pass decoder . M-   Trans . ( Yao and Wan , 2020 ): it leverages a mul-   timodal self - attention layer to encode multimodal   information where the hidden representation of im-2606   ages are induced from the text under the guidance   of image - aware attention . MCT : here , we incorpo-   rate the object - level features into the model instead   of coarse one . CA - MCT : similarly , we incorporate   the object - level features into the multi - task learning   model .   5.2 Multimodal Dialogue Sentiment Analysis   We perform several experiments with different   models . text - CNN ( Kim , 2014 ): it only applies   CNNs to extract textual information for each ut-   terance in a dialogue . In this approach , we do   not use the dialogue history or the additional vi-   sual information . DialogueRNN ( Majumder et al . ,   2019 ): this baseline is a powerful approach for cap-   turing dialogue history with effective mechanisms   for sentiment analysis . DialogueRNN + BERT ( Fir-   daus et al . , 2020 ): this model improves the perfor-   mance of DialogueRNN by using BERT ( Devlin   et al . , 2019 ) embeddings instead of Glove ( Pen-   nington et al . , 2014 ) embeddings to represent the   textual features . DialogueRNN + PLM : we propose   a stronger baseline built upon the DialogueRNN   for sentiment analysis . Specifically , we utilize   RoBERTa ( Liu et al . , 2019 ) embeddings for En-   glish sentiment analysis , and ERNIE ( Sun et al . ,   2019 ) embeddings for Chinese sentiment analysis ,   and XLM - R ( Conneau et al . , 2020 ) embeddings for   German sentiment analysis .   Following Firdaus et al . ( 2020 ) , we only use the   coarse - grained image features ( i.e. ,CSV ) when   training above models with the visual information.6 Experiments   6.1 Setup   For multimodal chat translation , we utilize the stan-   dard Transformer - Base architecture ( Vaswani et al . ,   2017 ) . Generally , we use the settings described   in previous work ( I ve et al . , 2019 ; Yao and Wan ,   2020 ; Liang et al . , 2021c ) to conduct experiments   on our MSCTD .   For multimodal dialogue sentiment analysis , we   mainly follow the settings of previous work ( Poria   et al . , 2019 ; Firdaus et al . , 2020 ) .   Please refer to Appendix A for more details .   6.2 Metrics   For multimodal chat translation , following pre-   vious work ( Liang et al . , 2021c ; I ve et al . ,   2019 ) , we use the SacreBLEU(Post , 2018 ) ,   METEOR ( Denkowski and Lavie , 2014 ) and   TER ( Snover et al . , 2006 ) with the statistical   significance test ( Koehn , 2004 ) for fair compar-   ison . Specifically , for Chinese →English , we re-   port case - insensitive score . For English →Chinese ,   the reported score is at the character level . For   English ↔German , we report case - sensitive BLEU   score .   For multimodal dialogue sentiment analysis , fol-   lowing Poria et al . ( 2019 ) , we report weighted-   average F - score .   6.3 Results of Multimodal Chat Translation   Results on English ↔Chinese . ( 1 ) Among all   only text - based models ( M1 ∼M3 ) , we find that M12607ModelChinese →English   BLEU↑METEOR ↑TER↓   Transformer ( T ) 20.43 24.06 61.00   TCT ( T ) 20.81 24.45 61.19   CA - TCT ( T ) 21.23 24.82 60.75   MCT ( T+CSV ) 22.25 25.60 59.69   CA - MCT ( T+CSV ) 22.68 25.60 59.14   performs worse than M2 , showing that the dialogue   history indeed is beneficial for better translations .   Furthermore , M3 can further improve the transla-   tion performance , which suggests that modeling   the coherence characteristic in conversations is cru-   cial for higher results . These can also be found in   other settings ( e.g. , M7 vs. M4 ∼6 ; M8 vs. M7 ) . ( 2 )   The models with image features incorporated get   higher results than corresponding text - based mod-   els ( i.e. , M4∼M6&M9 vs. M1 ; M7&M12 vs. M2 ;   M8&M13 vs. M3 ) . ( 3 ) The dialogue history and   the image features obtain significant cumulative   benefits ( M8 vs. M1 and M13 vs. M1 ) ( 4 ) Among   these image - based models ( M4 ∼M8 or M9 ∼M13 ) ,   we observe that different fusion manners of text and   image features reflect great difference on effects . It   shows that there is much room for further improve-   ment using other more advanced fusion methods .   ( 5 ) Using FOV image features is generally bet-   ter than the coarse counterpart CSV ( M9∼M13   vs. M4 ∼M8 ) , which demonstrates that the fine-   grained object elements may offer more specific   and effective information for better translations .   Results on English ↔German . Similar findings   are found on English ↔German . This shows that   our conclusions are solid and convincing on general   datasets . All these results prove the value of our   constructed MSCTD .   Furthermore , we provide some stronger base-   lines that we firstly train the model on the general-   domain corpus and then fine - tune it on our chat   translation dataset . The results are presented in   Table Tab . 8 of Appendix B , which show similar   findings observed in Table Tab . 4 .   6.4 Effect of Sentiment on Multimodal Chat   Translation   To evaluate the effect of sentiment , we conduct   some experiments on several baselines including   single - modality ones and double - modality ones . In   terms of implementation , following Si et al . ( 2019 ) ,   we append the sentiment label to the head of the   source utterance . Tab . 5 shows the results . Compar-   ing them with the results ( M1 ∼M3 and M7 ∼M8 )   without using the sentiment in Tab . 4 , we find that   using the ground - truth sentiment label has a posi-   tive impact on the translation performance . There-   fore , we believe that it is a topic worthy of research   in the future .   We also conducted the experiments with auto-   matically predicted sentiment labels rather than   the gold ones as the reviewer suggested , where   we used the mixed sentiment presentation by dot-   multiplying the predicted sentiment distribution   and the sentiment label representation . The re-   sults are shown in Tab . 6 , where we find that the   sentiment factor , as the inherent property of con-   versations , indeed has a positive impact on transla-   tion performance . We also observe that using the   automatically predicted sentiment labels ( actually   the mixed sentiment representation ) shows slightly   lower results than using ground truth in terms of   three metrics . The reason may be that the mixed   sentiment representation has certain fault tolerance .   6.5 Results of Multimodal Dialogue Sentiment   Analysis   In Tab . 7 , we report the results of sentiment classi-   fication on three datasets under different settings .   Results on MSCTD - Zh . We can see that the text-   based models perform much poorer than other mul-   timodal systems , which shows that it is not enough   to evaluate the sentiment based only on the text . It   indicates that visual information and contextual em-   beddings are crucial for classifying sentiment po-   larities . Overall , we achieve weighted F1 score of   67.57 % with the “ DialogueRNN+ERNIE ” model .   Results on MSCTD - En / MSCTD - De . On En-   glish / German , we observe the same findings on2608   Chinese . These show that it is beneficial to in-   troduce the visual information and contextual em-   beddings into the multimodal dialogue sentiment   analysis task for different languages . Overall , we   achieve the best F1 score of 66.45 % and 54.46 %   on English and German , respectively .   On this task , we obtain consistent results with   previous work ( Poria et al . , 2019 ; Firdaus et al . ,   2020 ) , which suggests the utility and reliability   of our MSCTD . Additionally , MSCTD - Zh and   MSCTD - De bridge the gap on multimodal dialogue   sentiment analysis of Chinese and German .   7 Conclusion and Future Work   In this paper , we introduce a new multimodal ma-   chine translation task in conversations . Then , we   construct a multimodal sentiment chat translation   dataset named MSCTD . Finally , we establish mul-   tiple baseline systems and demonstrate the impor-   tance of dialogue history and multimodal informa-   tion for MCT task . Additionally , we conduct mul-   timodal dialogue sentiment analysis task on three   languages of the MSCTD to show its added value .   MCT is a challenging task due to the complex   scene in the MSCTD , leaving much room for fur-   ther improvements . This work mainly focuses on   introducing the new task and dataset , and we pro-   vide multiple models to benchmark the task . In the   future , the following issues may be worth exploring   to promote the performance of MCT :   •How to effectively perceive and understand   the visual scenes to better assist multimodal   machine translation in conversations ?   •How to build a multimodal conversation rep-   resentation model to effectively align , interact ,   and fuse the information of two modalities?8 Ethical Considerations   In this section , we discuss the main ethical con-   siderations of MSCTD : ( 1 ) Intellectual property   protection . The English utterance and image of   MSCTD is from OpenViDial dataset ( Meng et al . ,   2021 ) . For our translation and sentiments , its per-   missions are granted to copy , distribute and modify   the contents under the terms of the Creative Com-   mons AttributionShareAlike 3.0 Unported License   and Creative Commons CC0 License , respectively .   ( 2 ) Privacy . The data source are publicly avail-   able movies . Its collection and Chinese / German   annotation procedure is designed for chat transla-   tion purpose , and does not involve privacy issues .   ( 3 ) Compensation . During the sentiment annota-   tion , Chinese and German translation , the salary   for annotating each utterance is determined by the   average time of annotation and local labor compen-   sation standard . ( 4 ) Data characteristics . We refer   readers to the content and Meng et al . ( 2021 ) for   more detailed characteristics . ( 5 ) Potential prob-   lems . While principled measures are taken to en-   sure the quality of the dataset , there might still be   potential problems with the dataset quality , which   may lead to incorrect translations in applications .   However , moderate noise is common in large - scale   modern translators , even for human translated sen-   tences , which should not cause serious issues .   Acknowledgements   This work is supported by the National Key R&D   Program of China ( 2020AAA0108001 ) and the Na-   tional Nature Science Foundation of China ( No .   61976015 , 61976016 , 61876198 and 61370130 ) .   Liang is supported by 2021 Tencent Rhino - Bird Re-   search Elite Training Program . The authors would   like to thank the anonymous reviewers for their   valuable comments to improve this paper.2609References26102611   A Implementation Details   For multimodal chat translation , we utilize the stan-   dard Transformer - Base architecture ( Vaswani et al . ,   2017 ) . Generally , we use the settings described   in previous work ( I ve et al . , 2019 ; Yao and Wan ,   2020 ; Liang et al . , 2021c ) to conduct experiments   on our MSCTD . Specifically , we use 512 as hidden   size , 2048 as filter size and 8 heads in multihead   attention . Both the encoder and the decoder of   all the models have 6 layers and are trained using   THUMT ( Tan et al . , 2020 ) . We set the training   step to 100,000 steps . The dropout is set to 0.1 .   The batch size for each GPU is set to 4096 tokens .   The experiments are conducted using 4 NVIDIA   Tesla V100 GPUs , which gives us about 4 * 4096   tokens per update . The models are optimized us-   ing Adam ( Kingma and Ba , 2014 ) with β=0.9   andβ=0.998 , and learning rate is set to 1.0 . La-   bel smoothing is set to 0.1 . Following Liang et al.(2021c ) , we set the number of dialogue context to   3 . During inference , the beam size is set to 4 , and   the length penalty is 0.6 in all experiments .   For the pre - training - then - fine - tuning setting , we   firstly train our model on the WMT20 datasets for   100,000 steps . Then , we utilize the pre - trained   model to initialize our all multimodal chat transla-   tion models .   For multimodal dialogue sentiment analysis , we   mainly follow the settings of previous work ( Poria   et al . , 2019 ; Firdaus et al . , 2020 ) . The experiments   are conducted on an NVIDIA Tesla V100 GPU and   the batch size is set to 64 . The learning rate is set   to 0.001 .   B Pre - training - then - fine - tuning Results   In this section , we provide some stronger base-   lines that we firstly train the standard trans-   former ( Vaswani et al . , 2017 ) model on the general-   domain corpus ( WMT20 dataset of Appendix C )   and then fine - tune it on our chat translation   dataset ( i.e. , using the pre - training - then - fine - tuning   paradigm ) . In Tab . 8 , the M1 denotes we directly   evaluate the pre - trained model on the target chat   test set ( i.e. , without fine - tuning on chat translation   dataset . ) . The M2 ∼M7 apply the pre - training - then-   fine - tuning paradigm . From Tab . 8 , we observe   similar conclusions to § 6.3 . This shows that our   findings on the newly proposed dataset are solid   even under the stronger baselines . Besides , we also   find that , after pre - training on the general - domain   corpus , the model obtains significant improvement   ( M2∼M7 vs. M1 ) .   C WMT20 Dataset   For English ↔Chinese , we combine News Com-   mentary v15 , Wiki Titles v2 , UN Parallel Cor-   pus V1.0 , CCMT Corpus , and WikiMatrix . For   English ↔German , we combine six corpora includ-   ing Euporal , ParaCrawl , CommonCrawl , TildeR-   apid , NewsCommentary , and WikiMatrix . First ,   we filter out duplicate sentence pairs and remove   those whose length exceeds 80 . To pre - process the   raw data , we employ a series of open - source / in-   house scripts , including full-/half - width conver-   sion , unicode conversation , punctuation normal-   ization , and tokenization ( Wang et al . , 2020 ) . Af-   ter filtering , we apply BPE ( Sennrich et al . , 2016 )   with 32 K merge operations to obtain subwords .   Finally , we obtain 22,244,006 sentence pairs for   English ↔Chinese and 45,541,367 sentence pairs2612   for English ↔German , respectively.2613