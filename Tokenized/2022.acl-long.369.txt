  Kevin StowePrasetya Ajie UtamaIryna GurevychUbiquitous Knowledge Processing Lab ( UKP )   Department of Computer Science   Technical University of DarmstadtBloomberg , London , United Kingdom   www.ukp.tu-darmstadt.de   Abstract   Natural language inference ( NLI ) has been   widely used as a task to train and evaluate mod-   els for language understanding . However , the   ability of NLI models to perform inferences   requiring understanding of Ô¨Ågurative language   such as idioms and metaphors remains under-   studied . We introduce the IMPLI ( Idiomatic   and Metaphoric Paired Language Inference )   dataset , an English dataset consisting of paired   sentences spanning idioms and metaphors . We   develop novel methods to generate 24k semi-   automatic pairs as well as manually creating   1.8k gold pairs . We use IMPLI to evaluate NLI   models based on RoBERTa Ô¨Åne - tuned on the   widely used MNLI dataset . We then show that   while they can reliably detect entailment rela-   tionship between Ô¨Ågurative phrases with their   literal counterparts , they perform poorly on   similarly structured examples where pairs are   designed to be non - entailing . This suggests   the limits of current NLI models with regard   to understanding Ô¨Ågurative language and this   dataset serves as a benchmark for future im-   provements in this direction .   1 Introduction   Understanding Ô¨Ågurative language ( i.e. , that in   which the intended meaning of the utterance dif-   fers from the literal compositional meaning ) is a   particularly di cult area in NLP ( Shutova , 2011 ;   Veale et al . , 2016 ) , but is essential for proper natu-   ral language understanding . We consider here two   types of Ô¨Ågurative language : idioms and metaphors .   Idioms can be viewed as non - compositional multi-   word expressions ( Jochim et al . , 2018 ) , and have   been historically di cult for NLP systems . For in-   stance , sentiment systems struggle with multiword   expressions in which individual words do not di-   rectly contribute to the sentiment ( Sag et al . , 2002 ) .   Table 1 : Examples of entailment ( ! ) and non-   entailment pairs ( 9 ) from the IMPLI dataset .   Metaphors involve linking conceptual properties   of two or more domains , and are known to be per-   vasive in everyday language ( Lako  and Johnson ,   1980 ; Stefanowitsch and Gries , 2008 ; Steen et al . ,   2010 ) . Recent work has shown that these types of   Ô¨Ågurative language are impactful across a broad   array of NLP tasks ( see ¬ß 2.1 ) .   Large - scale pre - training and transformer - based   architectures have yielded increasingly powerful   language models ( Vaswani et al . , 2017 ; Devlin   et al . , 2019 ; Liu et al . , 2019 ) . However , rela-   tively little work has explored these models ‚Äô rep-   resentations of Ô¨Ågurative and creative language .   NLI datasets have widely been used for evaluat-   ing the performance of language models ( Dagan   et al . , 2006 ; Bowman et al . , 2015a ; Williams et al . ,   2018 ) , but there are insu cient Ô¨Ågurative language   datasets in which a literal sentence is linked to a   corresponding Ô¨Ågurative counterpart that are large   enough to be suitable for evaluating NLI . Due to   the creative nature of human language , creating   a dataset of diverse , high - quality literal /Ô¨Ågurative   pairs is time - consuming and di cult .   To address this gap , we build a new English   dataset of paired expressions designed to be lever-   aged to explore model performance via NLI .   Our dataset , IMPLI ( Idiomatic /Metaphoric Paired   Language Inference ) , is comprised of both sil-   ver pairs , which are built using semi - automated5375methods ( ¬ß 3.1 ) , as well as hand - written gold pairs   ( ¬ß 3.4 ) , crafted to reÔ¨Çect both entailment and non-   entailment scenarios . Each pair consists of a   sentence containing a Ô¨Ågurative expression ( id-   ioms / metaphors ) and a literal counterpart , designed   to be either entailed or non - entailed by the Ô¨Ågura-   tive expression ( Table 1 shows some examples ) .   Our contribution thus consists of three key parts :   ¬àWe create a new IMPLI dataset consisting   of 24,029 silver and 1,831 gold sentence   pairs consisting of idiomatic and metaphoric   phrases that result in both entailment and non-   entailment relationship ( see Table 2 ) .   ¬àWe evaluate language models in an NLI setup ,   showing that metaphoric language is surpris-   ingly easy , while non - entailing idiomatic rela-   tionships remain extremely di cult .   ¬àWe evaluate model performance in a number   of experiments , showing that incorporating   idiomatic expressions into the training data   is less helpful than expected , and that idioms   that can occur more in more Ô¨Çexible syntactic   contexts tend to be easier to classify .   2 Background   2.1 Figurative Language and NLP   Figurative language includes idioms , metaphors ,   metonymy , hyperbole , and more . Critically , Ô¨Ågu-   rative language is that in which speaker meaning   ( what the speaker intends to accomplish through an   utterance ) di  ers from the literal meaning of that   utterance . This leads to problems in NLP systems   if they are trained mostly on literal data , as their   representations for particular words and /or phrases   will not reÔ¨Çect their Ô¨Ågurative intended meanings .   Figurative language has a signiÔ¨Åcant impact on   many NLP tasks . Metaphoric understanding has   been shown to be necessary for proper machine   translation ( Mao et al . , 2018 ; Mohammad et al . ,   2016 ) . Sentiment analysis also relies critically on   Ô¨Ågurative language : irony and sarcasm can reverse   the polarity of a sentence , while metaphors and id-   ioms may make more subtle changes in the speaker   meaning ( Ghosh et al . , 2015 ) . Political discourse   tasks including bias , misinformation , and political   framing detection beneÔ¨Åt from joint learning with   metaphoricity ( Huguet Cabot et al . , 2020 ) . Figu-   rative language engendered by creativity on social   media also poses di culty for many NLP tasks   including identifying depression symptoms ( Yadavet al . , 2020 ; Iyer et al . , 2019 ) and hate speech de-   tection ( Lemmens et al . , 2021 ) .   We are here focused on idioms and metaphors .   There is currently a gap in diagnostic datasets for   idioms , and our work Ô¨Ålls this gap . There ex-   ist some relevant metaphoric resources ( see x2:2 ) ;   metaphors are known to be extremely common and   important to understanding Ô¨Ågurative language , our   resource serves to build upon this work .   2.2 NLI and related challenges   Natural language inference is the task of predicting ,   given two fragments of text , whether the meaning   of one ( premise ) entails the other ( hypothesis ) ( Da-   gan et al . , 2006 ) . The task is formulated as a 3 - way   classiÔ¨Åcation problem , in which the premise and   hypothesis pairs are labeled as entailment , contra-   diction , orneutral , if their relationship could not   be directly inferred ( Bowman et al . , 2015b ) . NLI   has been widely used as an evaluation task for lan-   guage understanding , and there have been a large   number of challenging datasets , which have been   used to further our understanding of the capabilities   of language models ( Wang et al . , 2018 , 2019 ) .   Paired data for Ô¨Ågurative language is relatively   sparse , and there is a gap in the diagnostic datasets   used for NLI in this area . Previous work includes   the literal /metaphoric paraphrases of Mohammad   et al . ( 2016 ) and Bizzoni and Lappin ( 2018 ) , al-   though both contain only hundreds of samples ,   insucient for proper model training and evalu-   ation . With regard to NLI , early work proposed the   task of textual entailment as a way of understand-   ing metaphor processing capabilities ( Agerri et al . ,   2008 ; Agerri , 2008 ) . Poliak et al . ( 2018 ) build   a dataset for diverse NLI , which includes some   creative language such as puns , albeit making no   claims with regard to Ô¨Ågurativeness .   Zhou et al . ( 2021 ) build a dataset consisting of   paired idiomatic and literal expressions . They be-   gin with a set of 823 idiomatic expressions yield-   ing 5,170 sentences , and had annotators manually   rewrite sentences containing these idioms as literal   expressions . We expand on this methodology by   having annotators only correct deÔ¨Ånitions for the   idioms themselves and use these deÔ¨Ånitions to au-   tomatically generate the literal interpretations of   the idioms by replacing them into appropriate con-   texts : this allows us to scale up to over 24k silver   sentences . We also expand beyond paraphrasing by   incorporating both entailment and non - entailment5376   pairs to enable NLI - based evaluation .   Similar to this work , Chakrabarty et al . ( 2021a )   build a dataset for NLI based on Ô¨Ågurative lan-   guage . Their dataset consists of Ô¨Ågurative /literal   pairs recast from previously developed simile and   metaphor datasets , along with a parallel dataset   between ironic and non - ironic rephrasing . This   sets the groundwork for Ô¨Ågurative NLI , but the   dataset is relatively small outside of the irony do-   main , and the non - entailments are generated purely   by replacing words with their antonyms , restrict-   ing the novelty of the hypotheses . Their dataset is   relatively easy for NLI models ; here we show that   Ô¨Ågurative language can be challenging , particularly   with regard to non - entailments .   Zhou et al . ( 2021 ) and Chakrabarty et al . ( 2021a )   provide invaluable resources for Ô¨Ågurative NLI ;   our works aims to covers gaps in a number of areas .   First , we generate a large number of both entail-   ment and non - entailment pairs , allowing for better   evaluation of adversarial non - entailing examples .   Second , our silver methods allow for rapid develop-   ment of larger scale data , allowing for model train-   ing and evaluation . We show that while entailment   pairs are relatively easy ( accuracy scores ranging   from .86 to .89 ) , the non - entailment pairs are ex-   ceedingly challenging , with the roberta - large   model achieving accuracy scores ranging from .311   to .539 .   3 Building a Dataset   Our IMPLI dataset is built from idiomatic and   metaphoric sentences paired with entailing and non-   entailing counterparts , from both silver pairs ( ¬ß 3.1 )   and manually written sentences ( ¬ß 3.4 ) . For our pur-   poses , we follow McCoy et al . ( 2019 ) in conÔ¨Çating   the neutral and contradiction categories into a non-   entailment label . We then label every pair as either   entailment ( ! ) or non - entailment ( 9).Due to the di cult nature of the task and to avoid   issues with crowdsourcing ( Bowman et al . , 2020 ) ,   we employed expert annotators . We used two Ô¨Çuent   English speakers , both graduate students in linguis-   tics with strong knowledge in Ô¨Ågurative language ,   paid at a rate of $ 20 /hr . For each method below , we   ran pilot studies , incorporated annotator feedback   and iteratively assessed the viability of identify-   ing and generating appropriate expressions . As the   annotators were working on generating new expres-   sions , agreement was not calculated : we instead   assessed the quality of the resulting expressions   ( see Section 3.3 ) . Table 2 contains an overview of   the di  erent entailment and non - entailment types   collected ( Detail examples are also provided in Ap-   pendix D ) .   3.1 Silver pairs   First , we explore a method for generating silver   pairs using annotators to create phrase deÔ¨Ånitions   which can be inserted automatically into relevant   contexts , yielding a large number of possible en-   tailment and non - entailment pairs that di  er only   with regard to the relevant phrase . Our procedure   hinges on a key assumption : for any given Ô¨Ågura-   tive phrase , we can generate a contextually indepen-   dent literal paraphrase . We then replace the original   expression with the literal paraphrase , following   the assumption that the Ô¨Ågurative expression neces-   sarily entails its literal paraphrase :   He ‚Äôs stuck in bed , which is his hard   cheese .!He ‚Äôs stuck in bed , which is   hisbad luck .   Conversely , in contexts where the original phrase   is used literally , replacing it with the literal para-   phrase should yield a non - entailment relation .   Switzerland is famous for six cheeses ,   sometimes referred to as hard cheeses .5377   9Switzerland is famous for six cheeses ,   sometimes referred to as bad luck .   3.1.1 Idioms   To build idiomatic pairs , we use three corpora that   contain sentences with idiomatic expressions ( IEs )   labelled as either Ô¨Ågurative or literal . These are the   MAGPIE Corpus ( Haagsma et al . , 2020 ) , the PIE   Corpus ( Adewumi et al . , 2021 ) , and the SemEval   2013 Task 5 ( Korkontzelos et al . , 2013 ) . We collect   the total set of IEs that are present in these corpora .   We then extract deÔ¨Ånitions for these using freely   available online idiom dictionaries .   These deÔ¨Ånitions are often faulty , incomplete ,   or improperly formatted . We employed annota-   tors to make manual corrections . The annotators   were given the original IE as well as the deÔ¨Åni-   tion extracted from the dictionary . The annotators   were asked to ensure that the dictionary deÔ¨Ånition   given was ( 1 ) a correct literal interpretation and   ( 2 ) Ô¨Åt syntactically in the same environments as   the original IE . If the deÔ¨Ånition met both of these   criteria , the IE can be replaced by its deÔ¨Ånition to   yield an entailment pair . If either criterion was not   met , annotators were asked to minimally update   the deÔ¨Ånition so that it satisÔ¨Åed the requirements .   In total this process yielded 697 IE deÔ¨Ånitions .   We then used the above corpora , replacing these   deÔ¨Ånitions into the original sentences ( see Figure   1 ) . We use the Ô¨Ågurative /literal labels from the   original corpora : replacing them into Ô¨Ågurative   contexts yields entailment relations , while replac-   ing them into contexts where the phrase is meant   literally then yields non - entailments .   3.1.2 Adversarial DeÔ¨Ånitions   As a second method for generating non - entailment   pairs , we asked annotators to write novel , adversar-   ial deÔ¨Ånitions for IEs . Given a particular phrase ,   they were instructed to invent a new meaning for   the IE that was not entailed by the true meaning ,   but which seemed reasonable presuming they had   never heard the original IE . Some examples of this   process are shown in Table 3 .   We then replace these adversarial deÔ¨Ånitions into   Ô¨Ågurative sentences from the corpora . This yields   pairs where the premise is an idiom used Ô¨Ågura-   tively , and the hypothesis is a sentence that attempts   to rephrase the idiom literally , but does so incor-   rectly , thus yielding non - entailments ( Figure 2 ) .   3.1.3 Metaphors   Metaphors are handled in a similar way : we start   with a collection of minimal metaphoric expres-   sions ( MEs ) . These are subject - verb - object and   adjective - noun constructions from Tsvetkov et al .   ( 2014 ) . Each is annotated as being either literal or   metaphoric , along with an example sentence . We   passed these MEs directly to annotators , who were   then instructed to replace a word in the ME so that   it would be considered literal in a neutral context .   1.drop prices!reduce prices5378   2.hard truth!unpleasant truth   3.hairy problem!dicultproblem   These can then be replaced in a similar fashion :   we start with the original Ô¨Ågurative sentence , re-   place the ME with the literal replacements , and   the result is an entailing pair with the metaphoric   sentence entailing the literal .   We apply this procedure to the dataset   of Tsvetkov et al . ( 2014 ) , yielding 100   metaphoric /literal NLI entailment pairs . We   then take a portion of the Common Crawl dataset ,   and identify sentences that contain these original   MEs . We identify sentences that contain the   words from the metaphoric phrase , and replace the   metaphoric word itself with its literal counterpart .   This yields 645 additional silver pairs .   3.2 Postprocessing   For all silver methods , we also employ syntactic   postprocessing to overcome a number of hurdles .   First , phrases used idiomatically often follow dif-   ferent syntactic patterns than when used literally .   Original : These point out of this world ,   but where to is not made clear .   Replaced : * These point wonderful , but   where to is not made clear .   This phrase in literal contexts functions syntacti-   cally as a prepositional phrase , while idiomatically   it is used as an adjective . When replaced with   the deÔ¨Ånition " wonderful " in a literal context , we   get a grammatically incoherent sentence . Second ,   phrases in their literal usage often do not form full   constituents , due to the string - matching approach   of the original datasets . Many literal usages ofthese phrases are thus incompatible with the de-   Ô¨Åned replacement .   ¬àI think [ this one has to die ] forthe other one   to live .   ¬àTurn in[the raw edges ] of both seam al-   lowances towards each other and match the   folded edges .   To avoid these issues , we ran syntactic parsing   on the deÔ¨Ånition and the expression within each   context , requiring that the expression in context   begins with the same part of speech as the deÔ¨Ånition   and that it does not end inside of another phrase .   Additionally , for each replacement , we ensured   that the verb conjugation matched the context . For   this , we identiÔ¨Åed the conjugation in the context ,   and used a de - lemmatization script to conjugate the   replacement verb to match the original .   3.2.1 Additional Issues   In implementing and analyzing this procedure , we   noted a number of practical issues . First , a large   number of the MEs provided are actually idiomatic   or proverbial : the focus word does not actually   contribute to the metaphor , but rather the entire   expression is necessary . Similarly , we found that   replacing individual parts of MEs is often insu -   cient to fully remove the metaphoric meaning . We   iterated over possible solutions to circumvent these   issues and found that it is best to simply skip in-   stances for which a replacement does not yield a   feasible literal interpretation .   3.3 Evaluating Pair Quality   In order for these automatically created pairs to be   useful for NLI - based evaluation , they need to be   of suciently high quality . As the annotators were   generating novel deÔ¨Ånitions and pairs , rather than   inter - annotator agreement , we instead evaluate the   quality of the resulting pairs by testing whether the   automatically generated pairs contained the appro-   priate entailment relation . For this task , each anno-   tator was given 100 samples for each general cate-   gory of silver generations ( idiomatic entailments ,   idiomatic non - entailments , and metaphoric entail-   ments ) . They were asked if the entailment relation   between the two sentences was as expected . An   expert than adjudicated disagreements to determine   the Ô¨Ånal percentage of valid pairs .   To evaluate the syntactic validity of the gener-   ated pairs , we additionally ran the Stanford PCFG   dependency parser ( Klein and Manning , 2003 ) on5379   the pairs . Per previous work in NLI ( Williams et al . ,   2018 ) , we evaluate the proportion of sentences for   which the root node is S.   Table 4 shows the results . The semi - supervised   examples evoked the correct entailment relation   between % 88 and % 97 of the time : while there is   still noise present , this indicates the e  ectiveness   of the proposed methods . With regard to syntax ,   we see S node roots for between 82 % and % 90   of the sentences : within the range of the SNLI   performance ( 74%-88 % ) , and slightly behind the   MNLI ( 91%-98 % ) . We Ô¨Ånd that the generated   hypotheses are not signiÔ¨Åcantly di  erent in quality   than the premises . This indicates that the method   for generation preserves the original syntax .   These methods allow us to quickly generate a   substantial number of high - quality pairs to evalu-   ate NLI systems on Ô¨Ågurative language . However ,   they may introduce additional bias as we employ a   number of restrictions in order to ensure syntactic   and semantic compatibility , and we lack full non-   entailment pairs for metaphoric data . We therefore   expand our dataset with manually generated pairs .   3.4 Manual Creation of Gold Pairs   To create gold pairs , annotators were given a Ô¨Ågura-   tive sentence along with the focus of the Ô¨Ågurative   expression : for idioms , this is the IE ; for metaphors ,   the focus word of the metaphor . For idioms , we   used the MAGPIE dataset to collect contextually   Ô¨Ågurative expressions . For metaphors , we collected   metaphoric sentences from the VUA Metaphor Cor-   pus ( Steen et al . , 2010 ) , the metaphor dataset of   ( Mohammad et al . , 2016 ) , and instances from the   Gutenberg poetry corpus ( Jacobs , 2018 ) annotated   for metaphoricity ( Chakrabarty et al . , 2021b ; Stowe   et al . , 2021 ) . Annotators were instructed to rewrite   the sentence literally . This was done by removing   or rephrasing the Ô¨Ågurative component of the sen-   tence . This yields gold standard paraphrases for   idiomatic and metaphoric contexts .   We then asked annotators to write non - entailed   hypotheses for each premise . They were encour - aged to keep as much of the original utterance as   possible , ensuring high lexical overlap , while re-   moving the main Ô¨Ågurative element of the sentence .   For idioms , this comes from adding or adjusting   words to force a literal reading of the idiom :   ¬àThe old girl Ô¨Ånally kicked the bucket . 9The   girl kicked the bucket on the right .   For metaphors , this typically involves keeping   the same phrasing while adapting the sentence to   have a di  erent , non - metaphoric meaning .   ¬àYou must adhere to the rules . 9You must   adhere the rules to the wall .   3.5 Antonyms   Previous work in NLI has employed the technique   of replacing words in the literal sentences with their   antonyms to yield non - entailing pairs ( Chakrabarty   et al . , 2021a ) . We replicate this process for idioms :   for the manually elicited deÔ¨Ånitions , we replace   key words as determined by annotators with their   antonyms . This yields sentences which negate the   original Ô¨Ågurative meaning and are thus suitable   non - entailment pairs . Previous work found this   antonym replacement for Ô¨Ågurative language re-   mains relatively easy for NLI systems , which we   can additionally explore with regard to idioms .   These manual annotations provide a number of   concrete beneÔ¨Åts . First , they are not restricted to   individual words or phrases ( excluding antonyms ):   the Ô¨Ågurative components can be rewritten freely ,   allowing for diverse , interesting pairs . Second , they   are written by experts , ensuring higher quality than   the automatic annotations , which may be noisy .   4 Experiments /Results   Using the IMPLI dataset , we aim to answer a series   of questions via NLI pertaining to language mod-   els ‚Äô ability to understand and represent Ô¨Ågurative   language accurately . These questions are :   ¬àR1 : How well do pre - trained models per-   form on Ô¨Ågurative entailments and non-   entailments ?   ¬àR2 : Does adding idiomatic pairs into the   training data a  ect model performance ?   ¬àR3 : Does the Ô¨Çexibility of idiomatic expres-   sions a  ect model performance ?   Our dataset provides unique advantages in ad-   dressing these research questions that cover gaps5380   in previous work : it contains a large number of   both entailments and non - entailments and is large   enough to be used for training the models .   R1 : pre - trained Model Performance   We obtain baseline NLI models by Ô¨Åne - tuning   roberta - base androberta - large models on   the MNLI dataset ( Williams et al . , 2018 ) , with en-   tailments as the positive class and all others as the   negative and evaluate them on their original test   sets as well as IMPLI .Due to variance in neural   model performance ( Reimers and Gurevych , 2017 ) ,   we take the mean score over 5 runs using di  erent   seeds .   We report results in Table 5 . We observe that   idiomatic entailments are relatively easy to classify ,   with accuracy scores over .84 . Non - entailments   were much more challenging . Silver pairs gener-   ated through adversarial deÔ¨Ånitions were especially   dicult : the pairs contain high lexical overlap , and   in many cases the premise and hypotheses are se-   mantically similar . The replacement into literal   samples were easier , as the idiomatic deÔ¨Ånition   clashes more starkly with the original premise ,   making non - entailment predictions more likely .   Consistent with Chakrabarty et al . ( 2021a ) ‚Äôs work   in metaphors , non - entailment through antonym re-   placement is easiest for idioms : the antonymic   relationship can be a marker for non - entailment ,   despite the high word overlap .   With regard to metaphors , silver entailment pairs   are relatively easy . Manual pairs are more challeng-   ing but are still much easier than idioms . This   is supported by the fact that metaphors are com-   mon in everyday language : these models have   likely seen the same ( or similar ) metaphors in train-   ing . Our Ô¨Åndings show that in fact metaphoric-   ity may not be particularly challenging for deep   pre - trained models , as they are able to e  ec-   tively capture the metaphoric entailment relations .   Theroberta - large model performs better for   metaphoric expressions than roberta - base , butthe di  erence on other partitions is relatively   small . We also Ô¨Ånd that lexical overlap plays   a signiÔ¨Åcant role here as noted by previous work   ( McCoy et al . , 2019 ): sentences with high overlap   tend to be classiÔ¨Åed as entailments regardless of   the true label ( for more , see Appendix B ) .   We note that the manual pairs tend to be more   dicult for both idioms and metaphors : these pairs   can be more Ô¨Çexible and creative , whereas the sil-   ver pairs are restricted to more regular patterns .   R2 : Incorporating Idioms into Training To   evaluate incorporating idioms into training , we   then split the idiom data by idiomatic phrase types ,   keeping a set of IEs separate as test data to as-   sess whether the model can learn to correctly han-   dle novel , unseen phrases . Our goal is to assess   whether poor performance is due to models ‚Äô not   containing these expressions in training , or be-   cause their ability to represent Ô¨Ågurative language   inherently limited . We hypothesize that the non-   compositional nature of these types of Ô¨Åguration   should lead to poor performance on unseen phrases ,   even if the model is trained on other idiomatic data .   For each task , we split the data into 10 folds by   IE and incrementally incorporate these folds into   the original MNLI for training , leaving one fold out   for testing . We experiment with incorporating all   training data for both labels , as well as using only   entailment or non - entailment samples . We then   evaluate our results on the entire test set , as well as   the entailment and non - entailment partitions .   Figure 4 shows the results , highlighting that ad-   ditional training data yields only small improve-   ments . Pairs with non - entailment relations remain   exceedingly di cult , with performance capping   out at only slightly better than chance . As hypoth-   esized , additional training data is only somewhat   e  ective in improving language models ‚Äô idiomatic   capabilities ; this is not su cient to overcome di -   culties from literal usages of idiomatic phrases and   adversarial deÔ¨Ånitions , indicating that idiomatic5381   language remains di cult for pre - trained language   models to learn to represent .   R3 : Syntactic Flexibility Finally , we assess   models ‚Äô representation of idiomatic composition-   ality . Nunberg et al . ( 1994 ) indicate that there are   two general types of idioms : " idiomatic phrases " ,   which exhibit limited Ô¨Çexibility and generally occur   only in a single surface form , and " idiomatically   combining expressions " or ICEs , in which the con-   stituent elements of the idiom carry semantic mean-   ing which can inÔ¨Çuence their syntactic properties ,   allowing them to be more syntactically Ô¨Çexible .   For example , in the idiom spill the beans , we   can map the spilling activity to divulging of infor-   mation , and the beans to the information . Because   this expression has semantic mappings to Ô¨Ågura-   tive meaning for its syntactic constituents , Nunberg   et al . ( 1994 ) argue that it can be more syntactically   Ô¨Çexible , allowing for expressions like the beans   that were spilled by Martha to maintain idiomatic   meaning . For Ô¨Åxed expressions such as kick the   bucket , no syntactic constituents map directly to   the Ô¨Ågurative meaning ( " die " ) . We then expect less   syntactic Ô¨Çexibility , and thus the bucket that was   kicked by John loses its idiomatic meaning .   We hypothesize that model performance will be   correlated with the degree to which a given idiom   type is Ô¨Çexible : more Ô¨Åxed expressions may be   easier , as they are seen in regular , Ô¨Åxed patterns   that the models can memorize , while more Ô¨Çexible   ICEs will be more di cult , as they can appear in   di  erent patterns , cases , and word order , often even   mixing in with other constituents . To test this , we   deÔ¨Åne an ICE score as the percentage of times a   phrase occurs in our test data in a form that does   not match its original base form . Higher percent-   ages mean the phrase occurs more frequently ina non - standard form , acting as a measure for the   syntactic Ô¨Çexibility of the expression . We assessed   the performance of the roberta - base model for   each idiom type , evaluating Spearman correlations   between performance and idioms ‚Äô ICE scores .   We found no correlation between ICE scores   and performance for entailments , nor for adver-   sarial deÔ¨Ånition non - entailments ( r=:004 = : 45 ,   p=:921 = : 399 , see Appendix C ) . However , we do   see a weak but signiÔ¨Åcant correlation ( r=:188 ,   p=0:016 ) with non - entailments from literal con-   texts : the model performs better when the phrases   are more Ô¨Çexible , contrary to our initial hypothesis .   One possible explanation is that the model mem-   orizes a speciÔ¨Åc Ô¨Ågurative meanings for each Ô¨Åxed   expression , disregarding the possibility of these   words being used literally . When the expression   is used in a literal context , the model then still as-   sumes the Ô¨Ågurative meaning , resulting in errors   on non - entailment samples . The ICEs are more   Ô¨Çuid , and thus the model is less likely to have a   concrete representation for the given phrase : it is   better able to reason about the context and interact-   ing words within the expression , making it easier to   distinguish the entailing and non - entailing samples .   5 Conclusions and Future Work   In this work , we introduce the IMPLI dataset , which   we then use to evaluate NLI models ‚Äô capabilities   on Ô¨Ågurative language . We show that while widely   used MNLI models handle entailment admirably   and metaphoric expressions are relatively easy , non-   entailment idiomatic relationships are more di -   cult . Additionally , adding idiom - speciÔ¨Åc training   data fails to alleviate poor performance for non-   entailing pairs . This highlights how currently lan-   guage models are inherently limited in representing   some Ô¨Ågurative phenomena and can provide a tar-   get for future model improvements .   For future work , we aim to expand our data col-   lection processes to new data sources . Our dataset   creation procedure relies on annotated samples and   deÔ¨Ånitions : as more idiomatic and metaphoric re-   sources become available , this process is broadly   extendable to create new Ô¨Ågurative /literal pairs . Ad-   ditionally , we only explore this data for evaluating   NLI systems : this data could also be used for other   parallel data tasks such as Ô¨Ågurative language inter-   pretation ( Shutova , 2013 ; Su et al . , 2017 ) and Ô¨Ågu-   rative paraphrase generation . As natural language   generation often relies on training or Ô¨Åne - tuning5382models with paired sentences , this data could be a   valuable resource for Ô¨Ågurative language genera-   tion systems .   References538353845385A Model Hyperparameters   We use a Ô¨Åxed set of hyperparameters for all NLI   Ô¨Åne - tuning experiments : learning rate of 1e ,   batch size 32 , and maximum input length of 128   tokens . The models are trained for 3 epochs . We   used the HuggingFace implementation of the mod-   els ( Wolf et al . , 2020 ) .   B Lexical Overlap   Previous research shows that NLI systems exploit   cues based on lexical overlap , predicting entailment   for overlapping sentences ( McCoy et al . , 2019 ; Nie   et al . , 2019 ) . Our dataset consists mostly of pairs   with high overlap : this could explain why the non-   entailment sections are more di cult . We thus   evaluate system predictions for our datasets as a   function of lexical overlap . Figure 5 shows density-   based histograms of the results , comparing overlap   via Levenshtein distance ( Levenshtein , 1965 ) for   correctly and incorrectly classiÔ¨Åed pairs .   Our data contains higher overlap than the MNLI   data , with the bulk of the density falling on mini-   mally distant pairs . We also note a distinct di  er-   ence between our entailment and non - entailment   pairs : non - entailments contain extremely high over-   lap and are frequently misclassiÔ¨Åed in these cases   where the distance is small , matching previous re-   ports for NLI tasks : lexical overlap is a key artifact   for entailment , and this reliance persists when clas-   sifying idiomatic pairs .   C Syntactic Flexibility Correlations   Figure 6 shows correlations between ICE scores   ( determined by frequency of occurences of a given   IE outside of its normal form ) and roberta - base   model performance on that IE .   D Dataset Examples   Table 6 shows examples from each type of pair   generation.538653875388