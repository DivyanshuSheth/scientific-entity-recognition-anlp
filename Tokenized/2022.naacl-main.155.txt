  Sanghwan BaeDonghyun KwakSungdong KimDonghoon Ham   Soyoung KangSang - Woo LeeWoomyoung Park   NA VER CLOV ANA VER AI Lab   { sanghwan.bae , donghyun.kwak , sungdong.kim , donghoon.ham ,   sy.kang , sang.woo.lee , max.park}@navercorp.com   Abstract   Recent open - domain dialogue models have   brought numerous breakthroughs . However ,   building a chat system is not scalable since   it often requires a considerable volume of   human - human dialogue data , especially when   enforcing features such as persona , style , or   safety . In this work , we study the challenge   of imposing roles on open - domain dialogue   systems , with the goal of making the sys-   tems maintain consistent roles while convers-   ing naturally with humans . To accomplish   this , the system must satisfy a role speci-   ﬁcation that includes certain conditions on   the stated features as well as a system pol-   icy on whether or not certain types of utter-   ances are allowed . For this , we propose an   efﬁcient data collection framework leveraging   in - context few - shot learning of large - scale lan-   guage models for building role - satisfying dia-   logue dataset from scratch . We then compare   various architectures for open - domain dia-   logue systems in terms of meeting role speciﬁ-   cations while maintaining conversational abil-   ities . Automatic and human evaluations show   that our models return few out - of - bounds ut-   terances , keeping competitive performance on   general metrics . We release a Korean dialogue   dataset we built for further research .   1 Introduction   Recent large - scale language models ( LMs ) have   brought numerous breakthroughs in open - domain   dialogue systems , yielding human - like responses   ( Zhang et al . , 2020 ; Adiwardana et al . , 2020 ;   Brown et al . , 2020 ; Roller et al . , 2021 ; Kim et al . ,   2021a ) . In addition , there have been progresses in   controlling dialogue systems in persona , style , and   safety ( Zhang et al . , 2018 ; Smith et al . , 2020 ; Xu   et al . , 2021 ) , which impose consistency on chat-   bot ’s personality and mitigate undesirable featuresFigure 1 : An example of a chatbot system that cares   for senior citizens living alone . The utterance in red   highlights the model ’s mistaken identity as a chef rather   than the caring chatbot .   such as toxic or biased language . However , build-   ing a chatbot system combining these capabilities is   still challenging , which requires numerous human-   human dialogues for those conversational skills .   Most task - oriented dialogue systems conduct   speciﬁc roles such as booking assistants , infor-   mation providers , customer service agents , or per-   sonal assistants ( Eric et al . , 2017 ; Xu et al . , 2017 ;   Budzianowski et al . , 2018 ) . However , studies on   open - domain dialogue systems that perform spe-   ciﬁc roles have been insufﬁciently investigated ,   even though the role can be deﬁned for the practical   chatbot systems ( e.g. , chatbots that care for senior   citizens living alone , or counseling chatbots ) . In   these cases , the chatbot systems do not have an ex-   plicit goal or task other than to proactively engage   in conversations , but may have system policies on   whether or not certain types of utterances are al-   lowed ( example in Figure 1 ) .   To address these issues , we study methods for   Role Speciﬁed Open - Domain Dialogue ( RSODD )   systems . The goal of the system is conversing nat-   urally with humans on open - ended topics while   keeping conditions of given role . Certain condi-   tions in persona , style , safety , and system policy   must be satisﬁed in order to achieve the goal . We2128consider a general and scalable framework to treat   them , instead of using individual approaches to   control each .   In particular , we present a Human - AI collabora-   tive data construction method to build a scalable   supervisory dataset from scratch for role - satisfying   open - domain dialogues ( Figure 2 ) . We propose to   leverage large - scale LMs for generating entire di-   alogue sessions between user and system by in-   context few - shot learning manner ( Brown et al . ,   2020 ; Kim et al . , 2021a ) , followed by human-   interactive correction processes . Our method can   signiﬁcantly reduce the cost of building dataset   when compared to manually producing gold dia-   logues ( Section 3.2 ) . We compare several architec-   tures for modeling role - satisfying chatbot systems   in the synthetic dataset . In extensive experiments   and ablation studies , we show that the proposed   models considerably reduce undesirable utterances   that violate the given role speciﬁcation compared   to the in - context learning baseline , while achieving   competitive SSA ( Adiwardana et al . , 2020 ) scores   for their responses . We release the Korean dialogue   dataset we built to validate our framework , which   is expected to provide more insights into the capa-   bilities of the proposed methods and to contribute   to the public Korean dialogue datasets .   The contribution of our work is summarized as   follows .   1.We make a step towards role speciﬁed open-   domain dialogue ( RSODD ) systems which   are capable of conversing naturally on open-   ended topics while satisfying role speciﬁca-   tions .   2.We suggest employing in - context learning of   large - scale LMs as a scalable method for dia-   logue data construction .   3.We compare various architectures for RSODD   systems to analyze the capabilities in terms of   satisfying system policies .   4.We release the ﬁrst Korean RSODD dataset   while demonstrating the effectiveness of data   construction method .   2 Related Work   Pretrained LM in Open - domain dialogue   Many prior works tried to pretrain the models on   large - scale social comment chains data like Red-   dit to model conversational behavior ( Zhang et al . ,   2020 ; Adiwardana et al . , 2020 ) , followed by ﬁne-   tuning on the diverse target dialogue dataset to im-   prove engagingness and humanness ( Roller et al . ,   2021 ) . To avoid undesired behaviors of the models   including toxicity and bias from the human - human   conversation , they merely exclude some parts of   training data using automatic ﬁltering by prede-   ﬁned criteria .   Synthetic Dialogue Generation To reduce cost   of dialogue collection , there have been many ap-   proaches to generate synthetic dialogues ( Schatz-   mann et al . , 2007 ; Shah et al . , 2018 ; Campagna   et al . , 2020 ) . They usually deﬁne task schema , rules   and templates to simulate certain scenarios in the   task - oriented dialogue ( TOD ) . Kim et al . ( 2021b )   proposed neural simulation approach using pre-2129trained LMs for a fast domain adaptation in the   TOD . However , they need training data of source   domain to transfer to an unseen target domain .   Xu et al . ( 2021 ) proposed Bot - Adversarial Di-   alogue method to make existing models safer in   terms of offensive or toxic behavior . Sun et al .   ( 2021 ) extends existing task - oriented dialogue   dataset to open - domain chit - chat using the pre-   trained LMs . Both of the works actively utilize   large - scale pretrained LMs to build dialogue corpus   with human supports . We also introduce human - AI   collaborative dialogue collection method , while es-   pecially utilizing few - shot in - context learning abil-   ity of large - scale LM ( Brown et al . , 2020 ; Kim   et al . , 2021a ) . To the best of our knowledge , this   work is the ﬁrst to propose using the in - context   learning approach to generate synthetic samples   from large - scale language models for the purpose   ofdialogue data generation .   On the Role in Dialogue In TOD , the system   side plays functional roles utilizing explicit knowl-   edge base of speciﬁc domain ( Williams et al . , 2013 ;   Henderson et al . , 2014a , b ; Eric et al . , 2017 ; Xu   et al . , 2017 ; Budzianowski et al . , 2018 ) . For ex-   ample , agent in Budzianowski et al . ( 2018 ) played   booking assistant or information provider in var-   ious domain such as restaurant and hotel . On the   other hand , Zhang et al . ( 2018 ) proposed assigning   explicit persona to each dialogue agent , promot-   ing the agent to make more speciﬁc and consistent   responses in open - domain dialogue setting . How-   ever , the persona given by a few natural language   sentences is insufﬁcient to represent speciﬁc role   in the real world scenario . Sun et al . ( 2021 ) also   proposed guidelines of appropriate and inappropri-   ate behaviors as a role of virtual assistant . We note   that a recent concurrent work ( Shuster et al . , 2021 )   studied conditioning dialogue models with similar   motivations . We explore more into how to ﬁx the   chatbot ’s role to meet speciﬁc system policies in   diverse conversational interactions .   Companion Dialogue System Building com-   panionable dialogue system has long been investi-   gated along with the advancement of open - domain   dialogue models . Webb et al . ( 2010 ) deﬁnes com-   panions to be persistent , collaborative and conver-   sational partners , and proposes evaluation strate-   gies : empathy , positivity , and adaptive . Kopp et al .   ( 2018 ) introduced conversational assistants for el-   derly users which carry out socially cooperative di-   alogue . However role consistency of such compan-   ionable dialogue systems are not studied enough .   3 Data Construction   In this section , we describe a framework to gather   supervisory data for building RSODD systems . The   input to the framework is a role speciﬁcation de-   scribed by the chatbot developer ( Table 1 for ex-   ample ) , which deﬁnes the conditions in the dia-   logue interactions for the system . We assume a   pre - existing dataset that properly meets the speciﬁ-   cation is n’t available . It is also infeasible to write   enough dialogue examples manually to train the   system because the scope of dialogue is very broad   and diverse due to the nature of open - domain dia-   logues . To remedy this , we focus on composing the   dataset with a few samples of human - written dia-   logues using in - context few - shot learning of large-   scale LMs ( Brown et al . , 2020 ; Liu et al . , 2021 ) .   3.1 One - shot Dialogue Generation   As reported in Kim et al . ( 2021a ) , large - scale LMs   can generate dialogues with a speciﬁc personality ,   given a prompt consisting of a brief description   of the chatbot ’s properties and few dialogue exam-   ples . We use this method to build the entire dataset .   First , we write a few dialogue examples that sat-   isfy the role speciﬁcation . And we attach each of   them at the end of the system description ( Outline2130   in Table 1 ) to compose input prompts for one - shot   in - context learning . Figure 3 ( a ) shows an example   input . Then , the LM generates whole dialogue ses-   sions . That is , the LM acts as both a system and a   user ( Figure 3 ( b ) ) . Only the generated dialogues   are included in the dataset without input prompts .   3.2 Human Filtering   It is difﬁcult to include all the details of speciﬁca-   tions in the prompt and reﬂect them in the genera-   tion . Therefore , we employ human annotation on   the generated data . We give the annotator each con-   versation session and ask them to label the point   where the ﬁrst out - of - boundsoccurred . Figure 3   ( b ) shows an example of a veriﬁed dialogue ( more   examples are provided in Appendix H ) . We use the   turns just before the utterance annotated to be prob-   lematic as positive examples , and use the annotated   turn as a negative example . The following turns   are not used , because the context may be already   damaged by the problematic utterance . Annotation   time per dialogue session is about 88s , which is   13.3 times faster than human writing time per ses-   sion ( about 1170s ) . The percentage of remaining   utterances after the ﬁltering phase is 30.4 % ( See   Table 2).3.3 Collecting Human - Bot Dialogues   Although human ﬁltering is included in the dataset   building process , the actual utterances are all   machine - generated . Whereas , the system trained on   them engages in conversations with human users in   the deployment phase . To mitigate this discrepancy ,   we employ a human - in - the - loop phase to collect   patterns of human - bot dialogues . Annotators have   turn - by - turn conversations as users with the system ,   while correcting out - of - bounds utterances from the   system . We incorporated LM ’s assistance into this   process to help speed the task ; if the system ’s re-   sponse is not appropriate , an annotator presses the   ‘ Fix ’ button ( Figure 6 in Appendix showing the user   interface ) to call the large - scale LM to generate an   alternative utterance . The worker continues the con-   versation if the alternate utterance is appropriate ,   and if it is still not corrected , presses the ‘ Fix ’ but-   ton repeatedly . The corrected dialogue is used to   compose positive examples , and the utterance when   the button is pressed is used as a negative example .   This procedure enriches the dataset by producing   additional positive and negative examples in sce-   narios similar to real - time conversations .   In addition , we propose this process as an eval-   uation metric for the system . Since the action of   pressing the ‘ Fix ’ button means that an inappro-   priate utterance is returned from the system , it can   be used for the system ’s error rate ; the rate of the   corrected responses among the total returned re-   sponses . This metric is intuitive and does not incur2131additional costs because it is performed concur-   rently with the data collection process described   above .   4 Models   4.1 Notation   Response prediction task in open - domain   dialogues is predicting an utterance   y={y , y,···,y}given a dialogue his-   toryx={s , u , s , u,···,s , u } , where s   anduare system utterance and user utterance   respectively .   4.2 Out - of - Bounds Detection   The most straightforward method for constraining   the system ’s utterances according to the role speci-   ﬁcation is to detect and discard out - of - bounds ut-   terances . We consider a BERT - based ( Devlin et al . ,   2019 ) binary classiﬁer ﬁne - tuned to classify posi-   tive / negative examples in datasets . Since the clas-   siﬁer can not perform a conversation by itself , we   assume a two - stage model ; a response prediction   model returns responses , which are censored by the   classiﬁer . If an out - of - bounds utterance is detected ,   we select and return one of several pre - deﬁned ques-   tions about other topics , similar to the method used   in Xu et al . ( 2021 ) . Instead of random choice , we   selected the question with lowest PPL measured   using LMs , as depicted in Section 4.3 .   4.3 Response Selection   Another conceivable approach to constrain the sys-   tem ’s utterances is to pre-ﬁlter the response candi-   dates for response selection models . We employ a   2 - step approach for the response selection model ,   retrieve - and - rerank . The retriever of poly - encoder   architecture ( Humeau et al . , 2020 ) rapidly ﬁnds   the top - k plausible responses from the response   candidates , which are then carefully reranked by   the reranker of cross - encoder architecture . Both re-   triever and reranker are ﬁne - tuned in the same way   as Humeau et al . ( 2020 ) depicts .   Since the response candidates are limited by ﬁl-   tering , it is important to predict the context which   can not be answered with response candidates in   order to avoid non - sensible responses . One of the   effective methods to predict unanswerable contexts   is to utilize the uncertainty of the model ( Feng   et al . , 2020 ; Penha and Hauff , 2021 ) . Penha and   Hauff ( 2021 ) proposed a risk - aware score using   MC Dropout ( Gal and Ghahramani , 2016 ) and   we employ a similar approach using thresholding ;   we score the retrieved responses using mean and   variance of the predictive distribution from MC   Dropout :   S(x,ˆy ) = E[R]−var[R ] ,   where ˆyis a candidate response that is retrieved ,   R={f(x,ˆy),f(x,ˆy),···f(x,ˆy)}is a pre-   dictive distribution obtained by employing dropout   ( Srivastava et al . , 2014 ) at test time and conduct-   ingmforward passes , and fis a score function   of reranker . If all the scores of retrieved responses   are lower than a certain threshold , it is predicted as   unanswerable context .   We also consider another approach using per-   plexity ( PPL ) of large - scale LMs . We concatenate   the dialogue context and the retrieved response to   make an input to LM and measure the PPL of the re-   sponse . Thresholding is employed for ﬁnal decision   and the threshold is determined on the validation   set ( See Appendix C ) .   4.4 Response Generation   Fine - tuning LMs on target data is known to be ef-   fective in learning desirable traits of focused tasks   ( Roller et al . , 2021 ; Gehman et al . , 2020 ) . There-   fore , we consider ﬁne - tuned LMs as response gen-   eration model using maximum likelihood estima-   tion ( MLE ) . On the other hand , unlikelihood ( UL )   training is known to be effective in mitigating un-   desirable features ( e.g. , token repetition or logical   inconsistency ) of generative models ( Li et al . , 2020 ;   Welleck et al . , 2020 ) . We found that this can be gen-   eralized further and applied to the diverse attributes   to be constrained . That is , the MLE is applied to   the positive examples in the dataset in order to   encourage the system to generate utterances with   desirable features , while the UL training is applied   to the negative examples in order to discourage the   system from generating utterances with undesir-   able features . Both types of training are performed   concurrently.2132Formally , we ﬁne - tune LMs as generative mod-   els using maximum likelihood estimation ( MLE ) ,   which minimizes :   L(p , x , y ) = −/summationdisplaylogp(y|x , y ) ,   wherexis a dialogue history in positive examples   andyis a corresponding gold response . Unlikeli-   hood training is done by adding a loss that penalizes   the token set Cto be constrained ,   L(p , C , x , y ) =   −/summationdisplay / summationdisplaylog ( 1−p(y|x , y ) ) ,   whereC⊆V is a subset of the vocabulary . We   employ this to the negative examples in dataset   { ( x , y ) } . For this , Cis deﬁned as{y } , which   results in the following :   L(p , x , y ) =   −/summationdisplaylog ( 1−p(y|x , y ) ) .   The ﬁnal loss function consists of mixing MLE loss   and UL loss ,   L = L+αL , ( 1 )   whereα∈Ris the mixing hyper - parameter .   4.5 Retrieve - fail - Generate   We also consider a pipelined approach that consists   of response selection and generation models . We   ﬁrst tried a Retrieve - and - Reﬁne architecture ( Roller   et al . , 2021 ; Weston et al . , 2018 ) , but it failed in   α - blending . In addition , according to Roller et al .   ( 2021 ) , the Retrieve - and - Reﬁne strategy delivers   marginal or no improvements over the generator .   Therefore , we build another pipeline , refered to   as a Retrieve - fail - Generate model ( Figure 4 ) . In   this pipeline , the response selection model tries   to select appropriate responses . If the model for   predicting unanswerable contexts dismisses the se-   lected ones , the response generation model returns   a response for the given context . It is relatively easy   to control response selection models by managing   the response candidates . Hence , the response se-   lection models are responsible for majority of the   responses , and the generation model is only used   when the response selection fails .   5 Experiments   We detail experimental settings and results in this   section , including evaluations of the data collected   by in - context few - shot learning ( Section 5.2 ) , com-   parisons of model variants ( Section 5.3 ) , and evalu-   ations on system ’s response qualities ( Section 5.4 ) .   5.1 Dataset   We built a Korean dialogue dataset for a chatbot   system to have casual conversations on a regular ba-   sis with senior citizens who live alone . This dataset   was collected using the framework described in   Section 3 , assuming a role speciﬁcation in Table 1 .   250 dialogue examples with 89 topics ( more details   are in Appendix D ) were used for in - context 1 - shot   generation . We used 39B size of HyperCLOV A   ( Kim et al . , 2021a ) as generation model ( sampling   at temperature 0.5 using nucleus sampling ( Holtz-   man et al . , 2020 ) with P=0.8 ) . Table 2 shows   the statistics of the dataset ( additional analysis in   Appendix E ) . We use 5 % of each for validation   sets .   5.2 Evaluation on Generated Dialogues   We ﬁrst assess the quality of the generated dia-   logues to verify the dialogue generating method   described in Section 3.1 . Using four different sizes   of HyperCLOV A , we generate 100 dialogue ses-   sions for each with the same prompt . We ask the   crowd workers to rate on a scale of 1 to 5 whether   the generated dialogue satisﬁes several conditions2133   expected to be controlled through in - context learn-   ing ( the detailed description of the evaluation cri-   teria is provided in Appendix F ) . The results are   shown in Table 3 . It shows that the larger the model   size , the better to meet the conditions by in - context   learning , which is also shown in previous studies   ( Brown et al . , 2020 ; Kim et al . , 2021a ) . In addition ,   Distinct-1/2 ( Li et al . , 2016 ) indicates that the text   generated by large models is more diverse .   5.3 Model Comparison   Out - of - Bounds Detection Table 5 shows the   classiﬁcation accuracy and F1 score of the trained   classiﬁer . We use generator controlled by in-   context learning ( IC ) as a response prediction   model to evaluate the effect of the classiﬁer alone .   For in - context learning , we use the same prompt   used to generate the dataset , but the model only gen-   erates system ’s utterances in its turns . The classi-   ﬁer signiﬁcantly lowers the error rate of in - context   learning ( Table 4 ) , showing the effectiveness of   the classiﬁer . On the other hand , the error rate is   relatively higher than those of the best models of   response selection and generation . This is because   the classiﬁer is not perfect ( about 92 % in accuracy ) ,   and even when it properly detects out - of - bounds ,   the pre - deﬁned questions as alternatives are occa-   sionally incoherent with the contexts .   Response Selection We ﬁne - tune the response   selection models on positive examples of the ﬁl-   tered data and automatically evaluate them by mea-   suring Hits@1/ K(Roller et al . , 2021 ) on the valida-   tion set . Results are shown in Table 6 . We addition-   ally found that training on unﬁltered datasets brings   improvements to the Hits@1/ Kperformance itself .   Therefore , we use the models that trained on un-2134   Method positive negative   In - context Learning 2.65 2.74   Likelihood Training 2.07 2.47   Unlikelihood Training 2.48 46.70   ﬁltered dataset in the subsequent experiments . Re-   sponse candidates are limited to system responses   within positive examples ( unique system ’s turns of   ﬁltered data in Table 2 ) . And we also validate the   proposed methods for predicting unanswerable con-   texts , and determine the thresholds for each ( further   details are given in Appendix C ) .   Table 4 shows the error rate of the response se-   lection models . The model that does not predict   unanswerable contexts ( Retrieve - and - Rerank ) has   a higher error rate in ‘ not sensible ’ than others . The   case of using PPL as the method for predicting   unanswerable contexts shows a lower overall error   rate than the case of using MC Dropout , and the   proportions of the total contexts predicted as unan-   swerable are similar at 4.23 % and 3.85 % for PPL   and MC Dropout , respectively . The results also   show the error types from the models . Although   only the ﬁltered utterances are used as response   candidates , ‘ wrong persona ’ and ‘ policy violation ’   appear in responses . It seems that a few unﬁltered   utterances remain in the response candidates , since   the human ﬁltering is not perfect . Or even the same   utterance can cause errors depending on the con-   text . For example , it is possible to agree with when   a user calls the system by a different name .   Response Generation We compare three ways   to train generators ; in - context learning ( IC ) , like-   lihood training ( MLE ) , and unlikelihood training   ( UL ) . We measure the perplexity of the three mod-   els on positive and negative examples and Table 8   shows the results . The difference between the PPL   of the positive examples and the negative examplesis the smallest in in - context learning . When trained   on positive examples with likelihood training , the   difference increases slightly , because the PPL of   the positive examples is lowered . When adding un-   likelihood training , the PPL for negative examples   increase signiﬁcantly , which mean the model is   less likely to generate out - of - bounds utterances .   Table 4 shows the error rate of each model . Com-   pared with in - context learning , likelihood training   with the ﬁltered dataset can reduce the error rate   signiﬁcantly . Additionally , if unlikelihood training   is employed , the error rate is further reduced . A   similar trend can be found in all types of errors .   Retrieve - fail - Generate We also experiment   with a Retrieve - fail - Generate model consisting   of the best conﬁgurations for response selection   ( PPL ) and generation ( UL ) models . Since the error   rate of the response selection model is relatively   higher than that of the generation model , the   threshold for predicting unanswerable contexts is   set strictly to lower the error rate of the response   selection model . Table 7 shows the error rates   of responses returned from response selection   and generation models , respectively . The results   indicate that both error rates are lower when the   models are included in a pipeline than when they   are used separately , and the overall error rate   decreases accordingly . The response selection   model returns the responses within the candidates   extracted from the positive examples of the trainset ,   so that the ﬂow of the conversation is not dispersed   and tends to be similar to the trainset . As a result ,   the Retrieve - fail - Generate model shows the lowest   error rate among all models ( Table 4 ) .   Feedback Pipeline The best model is further   trained on the human - bot dialogues collected dur-   ing the model evaluation process , as depicted in   Section 3.3 . Both response selection and genera-   tion models are newly initialized and trained . As2135   a result , all types of error rates are consistently   reduced ( Table 4 ) , and the error rates of both the   response selection and generation models are de-   creased ( Table 7 ) . The effect is stronger on the   response generation .   5.4 Response Quality   To assess the overall response quality of the pro-   posed chatbot system , we use the sensibleness   and speciﬁcity average ( SSA ) metric ( Adiwardana   et al . , 2020 ) , which is shown to have a strong corre-   lation with asking raters how humanlike the model   is . This metric is a average of two scores : sen-   sibleness and speciﬁcity . Sensibleness measures   whether a model ’s responses make sense in con-   text and do not contradict anything that was said   earlier , while speciﬁcity measures whether a re-   sponse is speciﬁc to a given context . However , ex-   act comparison with the scores in Adiwardana et al .   ( 2020 ) is difﬁcult , because of the static role of our   chatbot system and language discrepency in phras-   ing of questions . Therefore , We re - estimate hu-   man interactive SSA in our experiments . To collect   human - human conversations , we transcribe 100   call speeches between users and workers who play   system ’s role . And we collect 100 human - bot con-   versations by allowing the crowd workers to chat   with the system . Labeling was conducted by inde-   pendent crowd workers with majority voting of 5   workers per turn .   The results are given in Table 9 . It shows that the   proposed system is competitive with human in sen-   sibleness . And the majority of the responses from   the system are labeled as speciﬁc , which allows us   to conclude that the proposed system achieves low   error rate with non - generic responses . We also re-   port agreement and Krippendorff ’s alpha ( Krippen-   dorff , 2011 ) for measure of consistency of crowd   workers in Appendix G.   6 Discussion   Although our methods achieve the low error rates   in human interactive evaluations , the results have   some limitations . The results should be regarded   as the error rates of typical conversations without   adversarial attack . Because the annotators are in - structed to participate in the chat as if they were   typical users , they did not try as many conversa-   tions that could induce toxic words from the model .   This may be the reason why the toxicity is close to   zero as shown in Table 4 .   The human ﬁltering process in the proposed data   collection framework has room to be more efﬁcient .   Since the accuracy of the classiﬁer is comparable   even when just 10 % of the total data is used ( Ta-   ble 5 ) , it is expected that the ﬁltering cost can be   reduced by adding a model ﬁltering process before   human ﬁltering , which is similar to the method   proposed in Sun et al . ( 2021 ) .   7 Conclusion   We present a framework for building role speci-   ﬁed open - domain dialogue systems from scratch .   We propose leveraging large - scale LMs to gener-   ate supervisory datasets for training dialogue sys-   tems with arbitrary roles with minimal effort for   manually composing dialogues . Our research also   analyzes several model architectures for the task .   By extensive experiments , we demonstrate the ef-   fectiveness of the collected data and modeling ap-   proaches in terms of satisfying role constraints and   improving dialogue abilities . We argue that our   framework can be extended to implement dialogue   systems with various roles and characters , even   when available datasets are few .   8 Ethical Considerations   Workers annotating the dataset we built were hired   on a part - time basis and compensated based on the   number of working hours . They were compensated   with 9,000 won per hour , which was somewhat   higher than the Korean minimum wage at the time   they worked . Appropriate instructions for the use of   collected data were given at the time of contract and   consent was obtained . We will release our dataset   in CC - BY - NC - SA license .   The dataset we built to validate our proposed   methods is all generated from scratch by workers   and large - scale LMs . Although there is no user   data in the dataset , pre - trained language models   are known to exhibit private details in their out-   puts ( Carlini et al . , 2020 ) , as well as social biases   ( Bender et al . , 2021 ; Bordia and Bowman , 2019 ;   Garrido - Muñoz et al . , 2021 ; Shwartz and Choi ,   2020 ) and toxic contents ( Gehman et al . , 2020 ) . To2136address these concerns , we determined categories   and criteria for harmful texts based on legal and   ethical considerations provided by experts in our   group , and we instructed annotators to ﬁlter the   dataset using these criteria . However , due to miss-   ing annotations and cultural or social biases , this   may be imperfect . To mitigate this , we had multiple   crowd workers annotate the same data . In addition ,   because the users in the dataset are regarded to be   a vulnerable population , our group ’s ethical con-   sultation looked through the issues that would be   sensitive to them , and dialogues containing these   topics were also eliminated .   Despite these efforts , using this dataset to di-   rectly train end - to - end chatbot models can involve   certain risks , due to the lack of controllability and   interpretability in end - to - end neural response pre-   diction models . And it should not be overlooked   that they may cause some potential harm , even   though the chatbot systems can help reduce social   loneliness of the user population . For example , a   user can become emotionally attached to a bot ,   even codependent on it , which can divert attention   away from real - world relationships and cause dis-   tress if the chatbot fails . It ’s also worth noting that   a chatbot can be programmed to impersonate a real   person and be used for phishing and fraud . Dur-   ing such conversations , users may provide private   and sensitive information , such as speciﬁc health   conditions and private attributes , which could be ex-   ploited if it falls into the wrong hands . For this rea-   son , when incorporating this dataset in real - world   applications , the application developers should en-   sure that it is used safely and ethically .   Since our proposed framework also can be used   for building another dataset and chatbot system   with arbitrary speciﬁcations , it is not exempt from   the possibility of propagating linguistic biases and   toxicity . Similar to Xu et al . ( 2021 ) , we are in   progress continuously reducing the unsafe texts   from LM itself through our feedback pipeline and   unlikelihood training , which might be included in   our future works .   Acknowledgements   The authors thank all the members of CLOV A and   AI Lab of NA VER for devoted supporting and dis-   cussion . In particular , they would like to thank   Gichang Lee , Hwijeen Ahn , and the members of   CLOV A Conversation for their dedicated efforts to   facilitate the large - scale models . In addition , theauthors thank Hyeri Kim , Hyunjung Park , and Yuin   Jeong for their great help in designing the role of   the chatbot . Also , they thank Jeeseung Han for   technically supporting the serving and testing envi-   ronments . Finally , the authors thank Nako Sung for   his support and valuable comments on this project .   References2137213821392140A Training Details   Pre - trained Language Models We use the   same Transformer - based Vaswani et al . ( 2017 ) pre-   trained language model for retriever , reranker , and   classiﬁer . Our pre - training strategy involves train-   ing with a masked language model ( MLM ) task   identical to BERT ( Devlin et al . , 2019 ) . The model   is based on Huggingface Transformers ( Wolf et al . ,   2020 ) . We use the corpus that we produced in-   house and the public Korean dialogue corpus   for pre - training . Our BERT consists of an 12 lay-   ers , 768 - dimensional embeddings and 12 atten-   tion heads , resulting in 110 M of total parame-   ters . And we use 6.9B size of HyperCLOV A ( Kim   et al . , 2021a ) as the pre - trained language model   for generator . This model is based on megatron-   LM ( Shoeybi et al . , 2019 ) . The model speciﬁcation   follows Kim et al . ( 2021a ) . Naver Smart Machine   Learning ( NSML ) platform ( Sung et al . , 2017 ; Kim   et al . , 2018 ) has been used in the experiments .   Retriever We employ the poly - encoder architec-   ture of Humeau et al . ( 2020 ) with 256 - dimensional   embeddings and 16 codes . We truncated dialogue   histories exceeding 10 turns or 256 tokens . The   model was trained with a batch size of 32 with in-   batch negatives . It was trained for 20 epochs with   early stopping using a maximum learning rate of   3×10and an linear scheduler . This ﬁne - tuning   took approximately 6 hours using 1 NVIDIA V100 .   Reranker We employ the cross - encoder architec-   ture . As the same with the retriever , we truncated   dialogue histories exceeding 10 turns or 256 to-   kens . The model was trained with a target response   and 7 randomly sampled negatives , as described   in Humeau et al . ( 2020 ) . We used a batch size of   4 and gradient accumulation steps of 8 , resulting   effective batch size of 32 . We trained the model for   20 epochs with early stopping using a maximum   learning rate of 3×10and an linear scheduler .   This took approximately a week using 4 NVIDIA   V100 .   Classiﬁer We use maximum 512 tokens from di-   alogue histories , truncating exceeding tokens from   the beginning . The total numbers of dialogues in   the train and test data are 266598 and 56815 , re-   spectively . Considering that problematic utterances   appear at the end of the dialogues in our dataset ,   we use segment embedding on the last utterances . Method AUC   MC Dropout 0.5985   PPL 0.6943   The input therefore looks like this : [ CLS ] dialogue   history [ SEP ] response . The model is trained with   a batch size of 16 for 100 epochs using an initial   learning rate of 10and an exponential sched-   uler . We trained 15 classiﬁers , 5 each using 10 % ,   20 % , and 100 % of the training data . It took approx-   imately 2 hours to train a classiﬁer on 10 % of the   train data using 1 NVIDIA TITAN RTX . Table 5   shows the mean accuracy and mean F1 score of   the classiﬁers . The ﬁnal classiﬁer we use is the one   with the best performance ( Accuracy : 0.9234 , F1 :   0.9276 , trained on 100 % of the data ) .   Generator For efﬁcient training , we employ   LoRA ( Hu et al . , 2021 ) for all generator ﬁne - tuning .   We ﬁx rank for adapter to 4 and LoRA αto 32 with   a learning rate of 5×10 , a weight decay factor   of 0.1 , and a batch size of 8 . The maximum training   epochs are 3 with early stopping . This took about   5 hours using 1 NVIDIA V100 .   B Inference Speed   Table 11 shows the average inference latency of   each architecture in experiments . All models were   run on a single NVIDIA A100 using cuda 11.1 and   cudnn 8.0.5.2141   C Validation Set for Predicting   Unanswerable Contexts   We build validation set to compare strategies for   predicting unanswerable contexts by replacing gold   responses in some portion of validation set with   non - sensible responses . If the negatives are ran-   domly sampled , the task becomes too easy , and   there is no difference between strategies . Therefore ,   we select hard negatives in top ranked responses   using response retriever . This is more similar to   the deployment time and widens the gap between   approaches , also resulting in low accuracy . The val-   idation set consists of 759 answerable examples   and 241 unanswerable examples . Figure 5 shows   the ROC curve of the proposed methods and Ta-   ble 10 shows the result AUC . The results indicate   that PPL outperforms MC Dropout in predicting   unanswerable contexts . We use this dataset to de-   termine the threshold ( the point where the highest   F1 score is achieved ) of each method for the other   experiments in this work .   D Topics in Dataset   The dataset ( Section 5.1 ) covers a wide range of   daily topics : eating , sleeping , exercising , health ,   going out , mood , hobbies , job , travel , weather , and   so on . In order to include these various topics in   the dataset , the example dialogue used on the gen-   eration process by in - context learning is conﬁgured   to cover 89 sub - topics . These topics can be found   in Table 13 . The generated dialogues are not con-   ﬁned to these sub - topics , and topic shifts occur   frequently within conversations ( See Table 14 for   examples ) .   E Diversity of Collected Dataset   Distinct-1 and distinct-2 of the generated dialogues   ( Generated ) in Table 2 are smaller than those   written by humans ( Example ) . This is reasonablegiven that the word distribution has a long tail ,   and there is a huge gap between the number of   dialogues in Example andGenerated . This can   be conﬁrmed by sampling 250 dialogues from the   generated dialogues and measuring Distinct-1 and   Distinct-2 , resulting in mean of 33.94 ( 0.0039 ) and   76.34 ( 0.0054 ) , respectively ( standard deviation in   brackets ) . Also , the overall distinct-1 and distinct-2   scales are reasonable .   In Table 2 , it can be seen that the average num-   ber of words per turn for Filtered are small , which   might be because relatively early parts of conver-   sations remain through the ﬁltering process , and   these parts usually contain short greetings . Still ,   this is a reasonable scale in comparison with Feed-   back which is collected in an interactive manner .   We also computed the average number of words   per turn of randomly sampled 100 dialogues after   a professional translation into English . The result   was 11.2 , which is reasonable in daily conversa-   tions ( 14.6 in DailyDialogue ( Li et al . , 2017 ) for   the same metric ) .   F Human Evaluation on Generated   Dialogues   We conducted a human evaluation to verify the ef-   ﬁcacy of RSODD data generation utilizing LMs .   Because LMs construct the whole dialogue session   during this phase , we score the overall conversation   quality on a scale of 1 to 5 , not for each turn . If   it is ﬂawless , it is worth 5 points , and points are   reduced for each ﬂaw . Table 15 provides the dimen-   sions used for this evaluation . For general dialogue   generation ability , crowdworkers were asked to an-   notate if the dialogue is ﬂuent and coherent ( Wu   et al . , 2019 ; Finch and Choi , 2020 ) . Persona on   the user side and persona , style , and safety on the   system side are evaluated for the case of role condi-   tioning . These are part of role speciﬁcation in Table   1 and correspond to the items expected to be con-   trolled by in - context learning . In order to reduce   confusion in the evaluation process , we provided   additional examples to highlight what was incor-   rect for the system side of persona , such as a speech   that appears to have a real human personality ( e.g. ,   " I am a real human " ) or utterances implying a phys-   ical meeting ( e.g. , " I ’ll see you at the park at 3   o’clock . " ) or acting as a radio presenter ( e.g. , " the   guest we invited today is this person").2142   G Consistency of SSA Evaluation   We report the crowd worker agreement as a mea-   sure of subjectivity . Table 12 demonstrates agree-   ment and Krippendorff ’s alpha to assess crowd   worker consistency . The agreement is reasonable ,   given that the questions are subjective and previ-   ous research ( Adiwardana et al . , 2020 ) reported a   similar level of agreement ( 76 % of sensibleness   and 66 % of speciﬁcity ) . Table 16 shows the an-   notated examples . Since speciﬁcity measures how   particular the utterance is and how deeply it relates   to the preceding context ( Adiwardana et al . , 2020 ;   Finch and Choi , 2020 ) , agreement seems to be low   when the utterance itself is not speciﬁc but is deeply   related to the previous context or vice versa .   H Dialogue Examples   Table 17 and 18 show generated dialogues by in-   context one - shot learning described in Section 3.1 .   The last utterances in each example are annotated   as violating the system ’s speciﬁcation ( Table 1 ) .   Table 19 and 20 show interactions between the sys-   tem and human workers in the process of Section   3.3 . The utterances in red are marked as violating   the system ’s speciﬁcation and the ones in blue are   corrected responses by LMs.214321442145Sensibleness votes : 5/5 , Speciﬁcity votes : 5/5   ...   AI : Great . Are you tired or sleepy during the day by any chance ?   User : No , not at the moment .   AI : What about taking a short nap if you have time ?   Sensibleness votes : 5/5 , Speciﬁcity votes : 3/5   ...   AI : I will . What about your health ?   User : Yes , everything ’s ﬁne .   AI : That ’s good . Did you eat well ?   Sensibleness votes : 5/5 , Speciﬁcity votes : 2/5   ...   AI : Thank you and have a nice day .   User : Alright .   AI : I ’ll call you later.21462147214821492150