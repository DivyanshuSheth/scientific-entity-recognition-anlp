  Yilun Zhou   MIT CSAILMarco Tulio Ribeiro   Microsoft Research   { yilun,julie_a_shah}@csail.mit.edu marcotcr@microsoft.com   https://yilunzhou.github.io/exsum/Julie Shah   MIT CSAIL   Abstract   Interpretability methods are developed to un-   derstand the working mechanisms of black-   box models , which is crucial to their respon-   sible deployment . Fulﬁlling this goal requires   both that the explanations generated by these   methods are correct andthat people can easily   and reliably understand them . While the for-   mer has been addressed in prior work , the lat-   ter is often overlooked , resulting in informal   model understanding derived from a handful   of local explanations . In this paper , we intro-   duce explanation summary ( ES ) , a math-   ematical framework for quantifying model un-   derstanding , and propose metrics for its quality   assessment . On two domains , EShigh-   lights various limitations in the current prac-   tice , helps develop accurate model understand-   ing , and reveals easily overlooked properties   of the model . We also connect understandabil-   ity to other properties of explanations such as   human alignment , robustness , and counterfac-   tual similarity and plausibility .   1 Introduction   Understanding a model ’s behavior is often a prereq-   uisite for deploying it in the real world , especially   in high - stake scenarios such as ﬁnancial , legal ,   and medical domains . Unfortunately , most high-   performing models , such as neural networks , are   black - boxes . Thus , model - agnostic interpretability   techniques have been developed , with the majority   being “ local ” – algorithms that produce an expla-   nation for a speciﬁc input at a time ( e.g. , Li et al . ,   2016 ; Ribeiro et al . , 2016 ) .   Even with these local explanations , there are still   two hurdles to overcome before achieving the ulti-   mate goal of complete understanding of a model .   First , some local explanations may not correctly ( or   faithfully ) represent the model ’s reasoning process   ( Jacovi and Goldberg , 2020 ) , as has been demon-   strated both theoretically ( Nie et al . , 2018 ) and em-   pirically ( Adebayo et al . , 2018 ) in prior work . As aFigure 1 : Local model explanations need to be both   correct and easily understandable . While much prior   work ( e.g. , Zhou et al . , 2022 ) has studied the former   property , this paper focuses on the latter , which has thus   far been largely ignored .   result , correctness evaluation has received much at-   tention in the community ( e.g. , Samek et al . , 2016 ;   Arras et al . , 2019 ; Zhou et al . , 2022 ) .   Another mostly overlooked property of expla-   nations is their understandability . As the model   understanding pipeline depicted in Fig . 1 shows ,   explanations need to be both correct and easily un-   derstandable , since even correct explanations are   not as valuable if they lead to incorrect understand-   ing . However , the concept of understandability has   yet to be formalized , and instead users often de-   rive model understanding from few examples in a   non - rigorous ( and potentially incorrect ) manner .   Consider the sentiment classiﬁcation task shown   in Fig . 2 . On a test input , the model makes the   correct prediction of positive sentiment . Obviously ,   this evidence is insufﬁcient to conclude that “ in   general , the model classiﬁes positive inputs cor-   rectly ” , because even a random - guess model is   correct 50 % of the time on a single instance . In-   stead , statistics such as the confusion matrix serve   to rigorously support ( or refute ) generalization   claims about model performance – for example ,   “ the model is correct 97.6 % of the time on positive   inputs ” – ensuring an accurate understanding of   model performance.5359   Do we understand model behaviors in the same   rigorous way ? Fig . 2 shows that the SHAP score   ( Lundberg and Lee , 2017 ) of the word “ memorable ”   is highest at 0.48 , while that of “ for ” is negligible   at -0.02 . Therefore , it is tempting to conclude that   “ in general , the model recognizes the high positive   contribution of highly positive words and ignores   stop words ” – as expected for an accurate sentiment   classiﬁer . However , this is a generalization from   a single instance , and thus potentially unreliable .   We need the “ confusion matrix ” analogue for such   claims , which to the best of our knowledge does not   exist , making it hard to derive model understanding   from local explanations .   In this paper , we propose ES , a mathemati-   cal framework to formalize model understanding .   InES , each piece of “ model understanding ”   is speciﬁed precisely via a rule that links inputs to   attribution values . For example , the tentative under-   standing described in the previous paragraph could   be formalized as “ words more positive than mem-   orable ( as measured by the word sentiment score   given in the dataset , e.g. , ﬂawless , charming , etc )   have SHAP attribution value in the [ 0.48 , 1 ] range . ”   This precise deﬁnition allows for quantitative eval-   uations . For example , this rule covers 1.6 % of all   words in the corpus , and is only correct 3.1 % of   the time . For the rule to be 90 % correct , we need a   wide and uninformative range of [ -0.01 , 1 ] , indicat-   ing that a hasty generalization from “ memorable ” is   unwarranted . Similarly , a saliency range of [ -0.05 ,   0.05 ] for stop words is only correct 64 % of the   time : over 1/3 of stop words have non - negligible   saliency – an understanding that is easily available   with ES , but might be missed with informalexplanation inspection . We deﬁne metrics to estab-   lish the quality proﬁle of each rule and present a   tool that makes it easy for users to construct E-   Srules from local explanations . Finally , we   demonstrate how ESreveals the various draw-   backs in the current practices of ad hoc model un-   derstanding , and allows for better understanding of   model behavior in two separate tasks .   2 On Generalized Model Understanding   Besides the practical example above , we start from   ﬁrst principles and argue that generalized model   understanding is the central concept for explana-   tion usefulness . Local explanations are math emat-   icaldescrip tions ( MD ) of some aspect of model   behavior , for speciﬁc inputs . For example , gradient   saliency ( in the embedding space ) is the sensitiv-   ity of the prediction to inﬁnitesimal changes in the   token embedding ; occlusion saliency is the predic-   tion change if individual embeddings are zeroed   out . It is with these mathematical descriptions that   people associate high - level interpretations ( HL ) of   model behavior , such as associating the above two   metrics with word importance . This ( unconscious )   train of thought can be described as follows :   x→MD→HL .   Crucially , people rarely study MD or HL for one   speciﬁc input , as explanations are often used to un-   derstand broader model behaviors , such as reliance   upon spurious correlation , non - discrimination of   a protected class , or usage of unknown scientiﬁc   principles . We elaborate upon these use cases in   App . A to demonstrate that people implicitly or   explicitly seek generalized model understanding.5360From another perspective , analogous to why people   ultimately focus on the generalization accuracy of   a model , they ( should ) focus on generalized model   understanding derived from local explanations .   For example , after observing that some highly   polar words have high contribution for a senti-   ment classiﬁcation model , people conclude that   allhighly polar words have high contribution . This   process can be formalized as follows :   x→MD→HL   ...   x→MD→HL      →HL ,   where HLis the generalized high - level model   understanding . This generalization is too informal ,   not least because the step from MDtoHLis   itself already informal . Alternatively , we propose   to generalize at the MD level , as follows :   x→MD   ...   x→MD      →MD→HL .   Since MDs are rigorously deﬁned mathematical   quantities ( e.g. , the prediction of the sentence drops   by 32 % after the embedding of “ great ” is zeroed   out ) , we can deﬁne and evaluate the quality of their   generalization , and HLcan also include any fail-   ures and anomalies . As each MD is a local expla-   nation , we call MDtheexplanation summary   ( ES ) , and proceed by instantiating this princi-   ple for feature attribution explanations .   3 The ESFramework   3.1 Setup and Notation   We focus on the classiﬁcation setting , but all the   ideas below can extend straightforwardly to regres-   sion . We have an input space Xand output space   Y={1, ... ,K}ofKclasses . A data point is an   input - output pair d= ( x , y)∈D = X×Y , dis-   tributed as P. We consider a model m :X →   ∆wherem(x)is the predicted class distribu-   tion on the probability simplex .   Feature attribution explainers assign an attribu-   tion , also known as saliency or importance , to each   input feature , such as a token in a text input . For   an instance ( x , y ) , each feature of xis called a   fundamental explanation unit ( FEU ) , deﬁned as   u= ( x , y , l ) ∈U with 1≤l≤Las the fea-   ture index . e(u)∈ E represents the attribution   value assigned to it , where Eis the attribution   space , such as [ −1,1]for normalized explanations.e(u ) = /parenleftBig   e, ... ,e , e, ... ,e / parenrightBig   ∈E   denotes the explanations on all other FEUs of x.   Wedeﬁne a distribution PoverUsuch that the   probability ( or probability density ) of u= ( x , y , l )   is1 / Lof that ofdunder the data distribution P.   In other words , sampling of ucan be performed in   two steps : ﬁrst draw an instance d= ( x , y)∼P ,   then a feature index l∼Unif({1, ... ,L } ) .   3.2 ESRules   AnESrule formalizes a piece of model under-   standing , such as that for positive words in Fig . 2 ,   which we use as the running example .   Deﬁnition 3.1 ( ESrule ) .AnESrule   ris deﬁned by two functions . A binary - valued   applicability function a : U→ { 0,1}determines   whether the rule applies to a given FEU , with 1   being applicable and 0 otherwise . We use a(U ) =   { u∈U : a(u ) = 1}to denote the applicability   set . A set - valued behavior function is deﬁned as   b : a(U)×E→P ( E)whereP(E)is the power   set ( i.e. , the set of all subsets ) of E. This function   predicts a set of possible explanation values for the   FEU , called the behavior range . The rule is written   asr=/angbracketlefta , b / angbracketright . We abbreviate b(u , e(u))asb(u )   and refer to the two functions as a- andb - functions .   For FEUu= ( x , y , l ) , thea - function typically   depends only on x , but could depend on the entire   inputx(e.g . , for long sentences ) or the output y   ( e.g. , for positive class ) . In our example , it tests   whether the sentiment score is greater than that of   the word “ memorable ” ( 0.638 ) . The b - function usu-   ally outputs a constant range . Since “ memorable ”   has a saliency of 0.479 , the range is [ 0.479 , 1.0 ] .   3.3 Additional Examples   While we expect most rules to use rather simple   a- andb - functions , they can also be more com-   plex with more nuanced aspects . For the follow-   ing examples , recall that u= ( x , y , l ) . An ap-   plicability function can target words only in long   sentences using a conjunction with len(x)≥L ,   whereLis the threshold . We can also target inputs   with ambivalent predictions with maxm(x)≤   0.6 , where maxm(x)is the probability of the   predicted class . For behavior functions , to in-   dicate the ﬁrst word of the sentence has higher   saliency than the rest , we can deﬁne b(u , e ) =   ( maxe,1.0 ] , where the a - function selects   the ﬁrst word ( i.e. a(u ) = 1 ) . Similarly , to   describe that an FEU has higher saliency than all5361the verbs in a sentence , we can can use b(u , e ) = /parenleftBig   max{e},+∞/parenrightBig   .   3.4 ESRule Unions   Since a single ESrule is designed to capture   one aspect of model understanding , multiple rules   are often necessary for comprehensive understand-   ing . However , conﬂicts can occur when multiple   rules apply to the same FEU but the b - functions are   different . We resolve them by deﬁning the compo-   sition of two or more rules into a rule union .   Deﬁnition 3.2 ( Precedence - Mode Composition ) .   Two rules , r=/angbracketlefta , b / angbracketrightandr=/angbracketlefta , b / angbracketright , can be   composed into a precedence - mode rule union r=   r > rdeﬁned asr=/angbracketlefta , b / angbracketrightwhere   a(u ) = 1{a(u ) + a(u)≥1 } , ( 1 )   b(u ) = /braceleftBigg   b(u ) ifa(u ) = 1 ,   b(u ) ifa(u ) = 0,a(u ) = 1,(2 )   represent the a- andb - functions of rule union r ,   with semantics similar to those for rules .   For example , if we want to split positive adjec-   tives into a separate rule from other positive words ,   we create a rule to test for part - of - speech and senti-   ment score , and assign a higher precedence to this   rule , such that the original rule is only applicable   to the remaining non - adjectives . One useful prac-   tice is to include a lowest - precedence catch - all rule   that covers everything not addressed by other rules ,   with a constant a(u ) = 1 function , which leaves   no FEUs unaccounted for .   Deﬁnition 3.3 ( Intersection - Mode Composition ) .   Two rules , r=/angbracketlefta , b / angbracketrightandr=/angbracketlefta , b / angbracketright , can be   composed into an intersection - mode rule union   r = r&rdeﬁned asr=/angbracketlefta , b / angbracketrightwhere   a(u ) = 1{a(u ) + a(u)≥1 } ; ( 3 )   b(u ) =       b(u ) if a(u ) = 1,a(u ) = 0 ,   b(u ) if a(u ) = 0,a(u ) = 1 ,   b(u)∩b(u ) ifa(u ) = a(u ) = 1 .   ( 4 )   Unlike precedence - mode , intersection - mode   composition is symmetric with respect to the two   rules . This mode is helpful when each property   of an FEU has a corresponding behavior range ,   and the ﬁnal behavior range of an FEU depends   on FEU ’s properties . For example , if verbs have   a behavior range of [ -0.4 , 0.4 ] and strongly pos-   itive words have a behavior range of [ 0.3 , 1 ] , a   strongly positive verb would have a behavior range[0.3 , 0.4 ] , or the intersection of the two constituent   ranges . In our case studies , however , we do not en-   counter any situations in which intersection - mode   compositions were preferable .   Since rule unions are also deﬁned by a- andb-   functions , they can form other rule unions in the   same way . Recursively , this results in a list of   rules composed into a single rule union , written as   r= ( r > r)&((r&r)>r ) . This rule union   represents our generalized model understanding .   3.5 Quality Metrics   We propose three metrics for establishing the qual-   ity proﬁles of ESrules or rule unions .   Deﬁnition 3.4 ( Coverage ) .The coverage of a rule   ( union)r=/angbracketlefta , b / angbracketrightis deﬁned as follows :   κ(r ) = E[a(U ) ] . ( 5 )   This represents the fraction of FEUs that we at-   tempt to understand . While individual rules may   have low coverage because they specialize in as-   pects of the model behavior , we want their union   to have high coverage to achieve a comprehensive   understanding of the model and prevent model pre-   diction from being excessively affected by the un-   covered ( i.e. unexplained ) input features . For our   positive word rule , the coverage is the frequency of   those words in the corpus and not surprisingly is   only 1.6 % . By contrast , including a catch - all rule   in the union maxes out its coverage value at 100 % .   Deﬁnition 3.5 ( Validity ) .LetPbePtrun-   cated to the set of applicable FEUs . The validity   of a rule ( union ) r=/angbracketlefta , b / angbracketrightis then deﬁned as fol-   lows , capturing the intuitive notion of a “ correct ”   understanding :   ν(r ) = E [ 1{e(U)∈b(U)}].(6 )   For our example , we compute it as the frequency   that the saliency of those words is actually in the   range of [ 0.479 , 1 ] – which turns out to be only   3.1 % of the time . However , validity alone is not   sufﬁcient , as it increases with wider behavior range .   We thus establish sharpness as a competing metric .   Deﬁnition 3.6 ( Sharpness ) .LetPbe the proba-   bility measure corresponding to the marginal dis-   tribution over explanation values generated by the   explainer on u∼P. The sharpness of a rule   ( union)r=/angbracketlefta , b / angbracketrightis deﬁned as follows :   σ(r ) = E / bracketleftbig   1−P(b(U))/bracketrightbig   , ( 7 )   whereb(U)=b(U)\{U}removes the actual   attribution value Ufrom the behavior range to pre-5362vent penalizing sharpness simply because the attri-   bution value is very common ( e.g. , zero for sparse   explanations ) , in which case Pis discrete at U.   Sharpness represents precision in the understand-   ing , as 1−σ(r)gives the probability that a random   FEU explanation value is correct . Thus , a lack   of precision represented by a wide behavior range   has minimal sharpness . We use the probability   measure Pto deﬁne the “ size ” , as it is consistent   across all explanation distributions , most of which   are non - uniform . A more general interpretation of   sharpness is the consistency of the described model   behavior : if a behavior range is wide ( e.g. , contain-   ing very positive andnegative saliencies ) , then it is   less sharp , and hence less useful . Pcould be re-   placed by an application - speciﬁc diversity measure ,   though the precision notion may be lost .   There is generally a trade - off between validity   and sharpness , as more precise rules ( i.e. , those   with narrower behavior ranges ) are less likely to   be valid . For our rule , the probability of a ran-   dom word saliency being in [ 0.479 , 1.0 ] is 0.2 % ,   indicating that explanation values are rarely higher   than 0.479 . This makes sharpness very high at   99.8 % . However , the rule is not useful because   of its low validity ; i.e. , it is almost never correct .   By comparison , the looser range of [ -0.01 , 1.0 ]   has 90.4 % validity but 28.6 % sharpness . There is   another trade - off between coverage and the two ,   since a larger set of covered FEUs tends to be more   diverse , making it harder to write a b - function that   remains as valid and sharp simultaneously .   Since these metrics are all expected values , we   can estimate them by their empirical estimate from   a dataset ( i.e. , a simple average ) , and Pcan be   constructed by kernel density estimation .   4 ESDevelopment Process and GUI   We describe a systematic procedure for authoring   ESrule unions from scratch and utilize it in   Sec . 5 . Starting from an empty rule union withno FEUs covered , we iteratively create rules that   target uncovered FEUs . Each rule describes one   model behavior , such as that for highly positive   words . For a rule , the a- andb - functions need to   be deﬁned , which may involve setting and tuning   parameters , such as the sentiment threshold . Last ,   we add a lowest precedence catch - all rule if any   FEUs remain uncovered . During this process , we   may also merge or split rules and change the com-   position structure according to the metric values .   To support these steps , we developed a Python   Flask - based ( Grinberg , 2018 ) graphical user inter-   face ( GUI , Fig . 3 ) . Users can visualize the FEUs ,   with font formatting for their coverage and validity .   Users can also ﬁlter for uncovered or invalid FEUs ,   iteratively constructing and reﬁning the rule union .   ESrule deﬁnitions usually include parameters   such as the sentiment threshold . Manually select-   ing correct values for the parameters is tedious , so   the lower middle panel of the GUI implements au-   tomatic parameter tuning for a given target metric   value . Installation and usage instructions for the   GUI are available on the project page .   5 Evaluation   We construct ESrule unions for SST and QQP   models ( details in App . B ) . We split the test set   into a construction set to create the rule union and   tune its parameters ( analogous to the training and   validation set in supervised model training ) and an   evaluation set to compute unbiased estimates of   the metric values ( analogous to the test set ) .   5.1 Sentiment Classiﬁcation   Setup We use SHAP explanations ( Lundberg and   Lee , 2017 ) for ﬁne - tuned RoBERTa ( Liu et al . ,   2019 ) , and take 300 random sentences as the con-   struction set , with the remaining 1910 sentences   as the evaluation set . We compute ﬁve features   for each FEU : sentiment score , part of speech   ( POS ) , named entity recognition ( NER ) , depen-   dency tag ( DEP ) and word frequency . For example ,   the word “ same ” in the sentence “ They felt like   thesame movie to me . ” has sentiment score of   0.028 , POS = ADJ , NER = O , DEP = amod , and   frequency of 7.14e-4 , with SHAP saliency of -0.82 .   Current Practice We evaluate the current prac-   tice of extracting informal model understanding   from local explanation inspection against the three5363   metrics . We assess three values of K , the num-   ber of inspected instances : 1 , for the typical ad   hocsetting of generalization from a single explana-   tion , 10 , for a more careful investigation , and 30 ,   which is quite cumbersome for manual inspection .   These examples are selected either randomly or by   submodular pick ( Ribeiro et al . , 2016 ) . Next , we   consider three ways to extract model understand-   ing – belief - guided ( BG ) , quantile-ﬁtting ( QF ) and   word - level ( WL ) – and apply them to create rules   on strongly positive words and stop words intro-   duced in Sec . 1 . For the strongly positive word   rule , BG mandates that words more positive than   the average sentiment score should have an above-   average saliency score , representing the belief of   a positive correlation between the two . For the   stop word rule , a saliency range belief of [ -0.05 ,   0.05 ] is averaged with the observed range . For both   rules , QF extracts the 5%-95 % quantile interval of   the saliencies for words covered by the respective   rule . WL , by contrast , creates a behavior range for   each word seen , with 0.03 margin on both sides .   App . B.1.1 presents technical details for these .   We formalize the understanding derived from   the selected instances and plot their coverage and   validity metrics on the evaluation set in Fig . 4 . For   BG and QF , the bars represent the average metric   value of the positive word and stop word rules . For   WL , the bars represent the metric for the rule union   consisting of an individual rule for each unique   word . Error bars for the random pick represent the   standard deviation across ﬁve iterations . Tab . 4 of   App . B.1.1 presents the complete statistics for all   metric values , and we highlight several ﬁndings .   •A very small number of samples ( e.g. , 1 ) exhibit   large variance for random pick , and low validity   for both pick methods . This conﬁrms the intu-   ition that model understanding from very few   explanations should be avoided .   •BG overall yields low validity , because its “ be-   liefs ” turn out to be quite incorrect . This suggests   a strong prior belief about how the model works   could lead to incorrect conclusions .   •While submodular pick can select a more diverse   set of words , to the particular beneﬁt of the cov-   erage of WL , its validity is generally lower due   to under - representation of common words .   •Although WL achieves highest coverage andva-   lidity , it has > 500rules atK=30 , with similar   words having very different ranges , as shown in   Fig . 5 – a conglomerate ( almost ) impossible to   make sense of . It also overﬁts , as the evaluation   set validity is much lower than the construction   set validity ( which is 100 % by construction ) .   •AtK=10 , only the stop word rule with random   pick QF achieves validity > 80 % , indicating that   even the more careful practices are unreliable .   All the drawbacks call for a principled way to de-   rive robust model understanding with enforceable   metric values ( e.g. validity ) . As we demonstrate   next , given a large construction set and automatic   parameter tuning assistance , we can create such   aESrule union . Finally , as a meta - point ,   the above discussion above of various limitations   would not be possible without the proposed E-   Sformalization and metric deﬁnitions .   ESConstruction We create a rule union   consisting of nine rules , with target validity of 90 %   and tune the sharpness accordingly . Tab . 1 summa-   rizes the individual and aggregate metrics .   Clearly , high validity comes at the cost of low   sharpness . Since ( 1 −sharpness ) is the probabil-   ity that a random FEU has an explanation value   within the behavior range , this around 90.7 % valid-   ity should be put into a context where the random   baseline achieves a validity of around 75 % . In this   sense , we attain only a crude understanding of the   local explanations that misses many subtleties .   Nonetheless , Rule 3 ( strongly positive words)5364   and Rule 6 ( stop words ) achieve better validity-   sharpness trade - off than their counterparts created   using the ad hoc BG and QF methods above . More-   over , the WL rules cover all words seen in the   analyzed instances – analogous , in a sense , to our   ESrule union . While the validity - sharpness   trade - off is comparable between the two , ours has   100 % coverage due to the effectively “ catch - all ”   Rule 7 , while WL rules have less than 60 % . Most   importantly , as our rule union is composed of nine   semantically organized rules , it is much more in-   terpretable than WL , which include more than 500   unpredictably varying rules ( Fig . 5 ) .   The fact that the ESrule union reveals the   imprecision and limitations of our model under-   standing while still performing better than current   practice emphasizes the need for more formal and   quantitative model understanding , as well as the de-   velopment of methods that are easier to understand ,   in addition to being correct . Below , we highlight   two sets of rules that quantitatively support or re-   fute our intuition , and cover the rest in App . B.1.2 .   Rule 2 , 3 , 4 , 8 , 9 : Sentiment - carrying words . We   expect a sentiment classiﬁer to recognize sentiment-   laden words . To test our intuition , we create rules   for positive and negative words , and further split   each set of words into two according to sentiment   strength , resulting in four rules . For the two rules   on strong words , we ﬁnd that wide behavior ranges   of [ 0.01 , 1 ] and [ -1 , -0.01 ] are necessary to achieve   90 % validity , suggesting the looseness of the model   understanding . However , we do observe that nega-   tive adjectives ( but not positive ones ) are modeled   much better , where a range of [ -1 , -0.06 ] is suf-   ﬁcient for the same validity . Thus , we create a   separate Rule 2 , with very high sharpness of 84.2 % .   For the two rules on words of weaker sentiment ,   even wider ranges of [ -0.11 , 1 ] and [ -1 , 0.05 ] are   necessary . Since both ranges encroach upon the   other side , the model often considers these words   to have an impact opposite to their intrinsic mean-   ing , but we fail to extract further understanding .   In addition , negative rules are much sharper than   positive ones , suggesting that the model consid-   ers a negative word to be stronger evidence for a   negative prediction than its positive counterpart .   Rule 6 : Stop words . While stop words ( e.g. , “ the ” ,   “ of ” ) should have negligible impact on prediction   ( and saliency values close to zero ) , a narrow behav-   ior range of [ -0.05 , 0.05 ] only has 64 % validity . We   create this rule for all stop words with 90 % target   validity and use different ranges on different words   for better sharpness . On average , we get [ -0.07 ,   0.12 ] , demonstrating that they can sometimes be   more inﬂuential than even strong sentiment words .   The ranges also tilt to the positive side , uncovering   a grammaticality bias wherein prediction is more   negative for grammatically incorrect sentences with   stop words masked out by SHAP .   5.2 Paraphrase Detection   Setup We use LIME explanations ( Ribeiro et al . ,   2016 ) for a ﬁne - tuned BERT model ( Devlin et al . ,   2019 ) , with 500 random test sentences as the con-   struction set and the remaining ≈40k as the eval-   uation set . We remove the word sentiment feature   but add the question ID ( 1 or 2 ) of each FEU .   ESConstruction QQP is a more complex   domain than SST , since the label is the semantic   equivalence of twosentences . The metric values   for the ESare summarized in Tab . 2 . Below,5365we describe how expectations for the model are   validated , but a hidden – and somewhat surprising –   phenomenon is also uncovered . All other rules are   documented in App . B.2.1 .   Rule 1 , 2 : Matching words . Due to the nature   of the task , we expect the model to rely heavily   on matching words . For such a word u , deﬁned   as ( proper ) noun , verb , adjective or pronoun that   has exactly one case - insensitive match vin the   other question , we expect similar saliency to their   match due to symmetry , or formally its saliency   s∈[s−α , s+β ] , whereαandβare lower   and upper margins . This behavior function is non-   constant , with output depending on the saliency   values of other words in the sentence .   For the same margin , FEUs for pairs of negative   predictions have much higher validity than positive   ones , so we split the rule into two based on the   prediction . Despite a less than 1 % difference in   sharpness ( Tab . 2 ) , we have α = β= 0.07for the   negative rule , but 0.18 for the positive rule , suggest-   ing that the matching words make a much larger   and more unpredictable contribution to positive   predictions . Interestingly , all other rules had wider   intervals for positive predictions as well .   Rule 3 : Non - matching words . Next we study   model behaviors for non - matching words , deﬁned   analogously to matching ones . Following the pre-   vious split based on predicted label , we designed   two rules . The negative rule has a reasonably sharp   behavior range of [ -0.35 , 0.01 ] at 90 % validity .   Given that LIME saliency is the linear regression   coefﬁcient on a neighborhood created by word era-   sure , we conclude that the presence of these non-   matching words mostly causes the prediction to   tilt toward the non - paraphrase ( i.e. negative ) class ,   indeed a very reasonable behavior . However , we   can not ﬁnd a range with 10 % sharpness at 90 %   validity for the positive rule and thus discard it .   With regard to the sharpness contrast by pre-   dicted label , one explanation is that the model de-   faults to a negative prediction , since many negative   pairs consist of completely unrelated questions and   the model decision is largely insensitive to input   perturbations , leading to stable LIME coefﬁcients .   On the other hand , a positive prediction requires the   cooperation of all parts of both questions . Depend-   ing on the exact sentence structure , the importance   of each word to the match are different and hard to   predict , which prevents the rules from being sharp . Word Average Baseline Here , we introduce a   new baseline as an “ automated ” version of WL   rules in SST . Speciﬁcally , for each word in the   construction set , we compute a behavior range   around its average saliency , with sharpness of   29.4 % ( matching that of our ESrule union ) .   As Tab . 2 shows , the resulting rule union is much   worse than our manual one on both evaluation set   coverage and validity , which is not surprising as the   word saliency should be more context - dependent ,   due to the matching mechanism of paraphrase de-   tection . Moreover , with more than 2,000 con-   stituent rules , the rule union barely qualiﬁes as   any sort of generalized model understanding .   6 Related Work   As discussed in Sec . 1 , explanation evaluation usu-   ally has a focus on correctness ( or faithfulness )   – i.e. , whether the explanation truly reﬂects the   model ’s reasoning process . This includes san-   ity checks ( Adebayo et al . , 2018 ) , proxy metrics   ( Samek et al . , 2016 ; Arras et al . , 2019 ) , and explicit   ground truth ( Zhou et al . , 2022 ) . The understand-   ability issue has been much less studied , with the   exception by Zheng et al . ( 2021 ) , who proposed   an evaluation speciﬁcally for rationale models ( Lei   et al . , 2016 ) . ES , however , addresses post hoc   explanations of general black - box models .   In addition , a few prior works have attempted to   capture the “ end - to - end ” utility of explanations :   whether access to explanations leads to perfor-   mance increase in certain tasks . Hase and Bansal   ( 2020 ) propose a model - teaching - human setup , sub-   sequently extended by Pruthi et al . ( 2022 ) into   an automated evaluation procedure . Bansal et al .   ( 2021 ) study whether explanations can improve   human - machine teaming performance . While these   studies report mostly negative results , pinpointing   the root cause is difﬁcult due to their end - to - end na-   ture . Poor understanding of the explanations may   be a major reason , as indicated by ES .   Last , some authors have proposed methods for   understanding model predictions beyond individ-   ual instances . For example , the anchor method   ( Ribeiro et al . , 2018 ) generates an explicit do-   main of applicability for each explanation , while   Lakkaraju et al . ( 2016 ) and Lakkaraju et al . ( 2019 )   proposed to learn “ patches ” of the input space spec-   iﬁed by logical predicates . ESalso empha-   sizes the need to understand models that general-   izes across instances , and uses logical predicates in5366the formulation , but focuses on model understand-   ing via explanations instead of direct predictions ,   which can capture a wider variety of behaviors ( e.g.   the matching and non - matching behaviors of the   QQP model ) . Furthermore , the ﬁne - grained anal-   ysis of behaviors allows us to investigate whether   models are “ correct for the correct reason . ”   7 The Many Faces of Understandability   The central thesis of this paper is quite simple and   intuitive : in order to understand a model from local   explanations , we need to understand those local   explanations . While ESis the ﬁrst framework   to explicitly formalize and quantify the notion of   understandability , we argue that it is connected to   many often - discussed and desirable properties of   explanation ( further details in App . C ) .   Human Alignment Users sometimes expect ex-   planations that are aligned with their expectations .   For example , the fact that highly salient words con-   vey strong sentiment is taken as evidence for the   quality of an explanation method by Li et al . ( 2016 ) .   In image classiﬁcation , this concept is typically im-   plemented as a pointing game between the high-   saliency region and the segmentation mask of the   predicted class ( Fong and Vedaldi , 2017 ) . However ,   alignment does not imply correctness , as the model   could use any spurious correlations , which should   be faithfully highlighted by the explainer . However ,   higher - alignment explanations are indeed more un-   derstandable , since by deﬁnition they agree more   with human intuition . Thus , an alignment - based   evaluation can be considered as one of understand-   ability . Nonetheless , understandability can also be   achieved by correcting human expectations , e.g. ,   users realizing that punctuations are actually im-   portant for predictions ( contrary to expectations ) .   Robustness It is often argued that explanations   should be robust ( Ghorbani et al . , 2019 ) – similar   inputs should induce similar explanations . How-   ever , robustness can be at odds with correctness :   if the model truly applies vastly different logic for   two very close inputs – such as a pair of inputs that   only differ in the root feature of a decision tree –   then their explanations should be distinct , as they   are routed down two different sub - trees . Nonethe-   less , slow - varying explanations are generally easier   to understand than those that change erratically and   unpredictably ( independent of their correctness ) ,   and thus robustness is related to understandability . Counterfactual Similarity and Plausibility   Counterfactual explanations ( e.g. Ross et al . , 2021 )   indicate how the input should change in order to   alter the model prediction . Besides the success   rate of achieving target prediction , they are often   evaluated on similarity ( the magnitude of input   change ) and plausibility ( the naturalness of the   changed input ) . Both properties can serve as   proxies for understandability : it is easier to relate   an input to another similar and natural input than   to a totally different or abnormal one . However ,   App . C presents two cases where they should not   be similar or plausible but remain understandable ,   to highlight certain model behaviors .   8 Discussion and Conclusion   Traditionally , model explanations are evaluated on   correctness ( or faithfulness ) , i.e. , whether they cor-   respond to how models actually make predictions ,   e.g. , reliance on spurious correlations ( Zhou et al . ,   2022 ; Adebayo et al . , 2022 ) . Such evaluation , how-   ever , does not answer the equally important ques-   tion of whether these ( presumably correct ) expla-   nations are understandable . Even faithful explana-   tions can lead users into error , if misunderstood   ( e.g. , trusting a model incorrectly ) .   In a sense , the most correct explanation for an   input is the literal trace of model computation , but   it is also arguably the least understandable ( or use-   ful ) . As we abstract away from low - level details   and use higher - level concepts such as word senti-   ment , the resulting explanation loses correctness   but gains understandability . At the other extreme   are explanations that are trivially understandable   but completely wrong , such all attribution values   being 0 ( i.e. , no feature impacts the model predic-   tion ) . Thus , a trade - off often occurs between these   two desiderata , and we need to choose a sweet spot .   Concretely , we propose ESrules and rule   unions , along with three quality metrics to formal-   ize and evaluate understandability . Such rigorous   investigations stand in contrast to current ad hoc   practices , which are prone to yielding unreliable   and coarse model understanding . For SST and QQP   datasets , ESdemonstrates that our model un-   derstanding is quite limited and imprecise , even   with very reasonable explanations . Being aware of   this is an asset . While EShelps us to recog-   nize that our understanding is incomplete , it still   helps uncover unexpected model behaviors that   warrant further investigation.5367Limitations and Ethical Impacts   Limitations   One notable requirement of ESis the exten-   sive human involvement in constructing and opti-   mizing its rules . However , this process is necessary ,   as the alternative of generalizing from a few expla-   nations has various ﬂaws , depicted in Fig . 2 and   Fig . 4 . Practically , we spent about 3 hours on each   rule union in Sec . 5 , and our effort was streamlined   by the systematic process and GUI presented in   Sec . 4 , which could be further improved by meth-   ods that automatically propose candidate rules .   In addition , another area requiring human in-   volvement is the FEU feature deﬁnitions , which are   often domain - dependent : both the sentiment score   and the matching word features reﬂect the nature   of the tasks . Other features may be necessary for   other tasks . For example , in question - answering ,   one important FEU feature could be the kind of in-   terrogative word used in the question ( e.g. , “ what ”   vs. “ when ” vs. non - interrogative words ) . If impor-   tant features are missed , the quality of the ES   rules – and , hence , the model understandings – will   suffer accordingly .   Last , the difﬁculty of obtaining overall high-   quality model understanding may result from the   fundamental limitations of word - level attribution-   based explanations , which can not account for   higher - level interactions . EScould aid in the   development of new explanation methods that are   easier for humans to understand . As a ﬁrst step , we   explore deﬁning and evaluating model understand-   ing obtained from instance - based explanations with   whole input as FEUs . App . D details the investiga-   tion , which raises questions such as the reliability   of such explanations .   Ethical Impacts   As interpretability methods are increasingly de-   ployed for quality assurance , auditing and knowl-   edge discovery purposes , it is important to ensure   the legitimacy of any conclusions drawn from ex-   planations . While the correctness of these explana-   tions is often studied , we argue their understand-   ability should be equally emphasized , and evalua-   tions with our newly proposed ESframework   and GUI reveal many problems of existing ad hoc   procedures . Thus , a more careful treatment on   the understandability aspect is necessary for well-   calibrated model understandings and responsible   model deployment in the real world . Acknowledgment   This research is supported by the National Science   Foundation ( NSF ) under the grant IIS-1830282 .   We thank the reviewers for their efforts .   References536853695370A Real World Use Cases for Explanations   Here , we discuss several scenarios in which people use local explanations to understand models , and   argue that people invariably derive generalized model understanding from these explanations .   A.1 Spurious Correlation Identiﬁcation   Natural datasets can contain many spurious correlations . For example , in a COVID-19 chest X - ray dataset ,   most positive images ( i.e. , patients diagnosed with COVID-19 ) come from a pneumonia - specializing   hospital and contain a watermark of the hospital name , while most negative images from other hospitals   do not . Thus , a model could achieve very high accuracy by simply detecting the watermark rather than   genuine medical signals . Similar spurious correlations could also be present in the text domain , such as   the correlation between an exclamation mark and the positive sentiment class , or between the word “ not ”   and the contradiction class in natural language inference .   It is crucial for people to be aware of the shortcuts that models may take , and one possible way   to highlight such behaviors is via feature attribution , which in the examples above would assign an   abnormally high score to the watermark region , exclamation mark , or the word “ not . ” Assuming the   explanations do indeed exhibit such patterns , when people claim a model relies on spurious correlation ,   they mean this in a general sense : for example , the model is likely to focus on the watermark in anyimage   that contains it , rather than in only a speciﬁc set of images .   A.2 Fairness Assurance   Similar to spurious correlation features , other features should not have a high impact , but for reasons   of fairness . For example , decisions made by a loan approval model should not be affected by gender ,   therefore the gender feature should not have a high attribution score .   If we observe that the gender of one applicant heavily impacts the model ’s decision , we may suspect   the model is discriminative ; conversely , observing that it has minimal impact could increase our assurance   of the model ’s fairness . However , such single - instance observations are fundamentally exploratory , and   claims about the model ’s fairness or discrimination must be established using a population of instances to   determine whether the trend persists generally .   A.3 Model - Guided Human Learning   In some cases , a very accurate and “ super - human ” model could be a source for knowledge discovery .   Consider the task of early - stage cancer detection from CT scans , which is challenging for doctors . If a   label is generated from follow - up visits tracking whether patients develop cancer after a certain number   of years , a model achieving better test accuracy than doctors is likely to use certain cues that would be   missed by humans or not known to be linked to cancer .   For these models , explanation methods such as saliency maps could be used to help doctors make better   diagnoses , or assist scientists in the creation of new pathological theories . Similarly to the above two use   cases , generalized model understanding across different inputs are necessary , because doctors need to   apply what they have learned to new patients , and scientists require new theories to hold broadly .   B Additional Evaluation Details   Tab . 3 summarizes the key parameters of our experiment . Both saved models are publicly accessible from   Huggingface Hub , and the model names in the table are links to the respective model checkpoints . For   normalization , we divided all explanation values for all test set instances by a single scaling factor such   that the maximum magnitude of new explanations is 1 .   B.1 SST Sentiment Classiﬁcation   For the explainer , we used the PartitionSHAP algorithm implemented by the shap repository . Fig . 6   shows the explanations on three sentences ( after normalization).5371Task Dataset Model Acc . F1 Explainer   Sentiment SST-2 RoBERTa 95.6 % 0.957 SHAP   Paraphrase QQP BERT 90.7 % 0.875 LIME   B.1.1 Details of Current Practices   Here , we provide an extended description of the three current practices , and how they are applied on the   handful of selected examples , collectively called the “ sample ” below .   The ﬁrst method , “ belief guided ” ( BG ) , represents the practice wherein the user has some expectations   ( or beliefs ) about the attributions of certain words , and modiﬁes ( or updates ) them after observing   explanations on some actual test inputs . It operates differently for the two rules on positive - sentiment and   stop words , as follows .   1 . For positive - sentiment words , the prior belief is that a word with a higher sentiment score ( one of the   FEU features provided by the SST dataset ) should also receive more positive attribution . This leads to   a rule that applies to all words with a sentiment score greater than α , and has a behavior function that   outputs a constant range of [ β,1](recall that SHAP attribution values are normalized to [ −1,1]range ) .   It then computes the value of αas the mean sentiment score and βas the mean attribution value for all   words in the sample with positive sentiment scores .   2.For the stop words – deﬁned as those with parts of speech AUX , DET , ADP , CCONJ , SCONJ , PRON ,   PART , and PUNCT – it has a prior belief that they should have a attribution value range of [ -0.05 , 0.05 ]   ( i.e. , not important to model prediction ) , and computes the observed attribution range [ α , β]for stop   words in the sample . The ﬁnal behavior range as predicted by the behavior function of this rule is the   average of these two : [ −(0.05 + α)/2,(0.05 + β)/2 ] .   The second method , “ quantile ﬁtting ” ( QF ) , represents the practice wherein the user fully follows the   observed data without any prior beliefs . Speciﬁcally , for a set of words , it collects all attribution values for   words within the set and then creates a rule that applies to this set , with the behavior function predicting a   constant range of 5 % to 95 % quantile of these attribution values . For the two rules for positive - sentiment   and stop words , the set of words ( and hence the applicability functions ) is deﬁned in the same way as for   the BG method above .   The last method , “ word - level ” ( WL ) , can be considered a more extreme version of QF , where the user   not only lacks any prior expectations for the explanations but also considers each word individually . For   example , if the user observes that the word “ brilliant ” has an attribution value of 0.5 in one sentence and   the word “ fantastic ” has attribution of 0.8 in another , they would notconclude that other , similarly positive   words would have attributions approximately within the range of [ 0.5,0.8 ] . Speciﬁcally , for every distinct   wordwin the sample , this method builds a rule that applies only to that word , with a constant behavior5372function that outputs a range of [ min(s)−0.03,max(s ) + 0.03 ] , wheresis the list of attributions   received by different occurrences of w. In many cases , especially given a small sample , word wonly   appears once , in which case sis a list containing only that attribution value .   Tab . 4 presents the metric values of the above methods . Fig . 4 of Sec . 5.1 depicts a graphic summary .   belief - guided quantile-ﬁtting word - level   K pick positive stop word positive stop word seen words   1SP 10 , 72 , 50 49 , 45 , 65 10 , 63 , 44 49 , 63 , 45 28 , 51 , 61   RNDµ12 , 63 , 57 49 , 58 , 53 12 , 45 , 56 49 , 73 , 33 17 , 41 , 68   RNDσ 6 , 25 , 27 0 , 9 , 6 6 , 32 , 29 0 , 24 , 21 6 , 12 , 10   10SP 10 , 61 , 61 49 , 47 , 63 10 , 78 , 34 49 , 72 , 38 49 , 66 , 48   RNDµ10 , 71 , 52 49 , 56 , 56 10 , 75 , 32 49 , 84 , 25 41 , 68 , 48   RNDσ 0 , 6 , 7 0 , 4 , 4 0 , 9 , 10 0 , 3 , 2 2 , 1 , 2   30SP 10 , 64 , 59 49 , 50 , 60 10 , 88 , 17 49 , 82 , 29 57 , 73 , 42   RNDµ10 , 66 , 56 49 , 57 , 55 10 , 82 , 26 49 , 86 , 24 51 , 78 , 39   RNDσ 0 , 4 , 5 0 , 1 , 2 0 , 6 , 7 0 , 2 , 2 2 , 3 , 2   B.1.2 Complete Rule Union Description   Below , we present the details of the construction process for rules not discussed in Sec . 5.1 .   •Rule 1 : Negation words have negative saliency . We found that negation words – not , n’t , no , nothing   and those with NEG dependency tag – almost invariably receive ( sometimes highly ) negative saliency ,   regardless of the sentence label or sentiment of the word being modiﬁed . We create a rule that predicts a   constant behavior range [ −1.0,0.002 ] , with 89.5 % validity and 65.1 % sharpness . Although the validity   is under our 90 % target , we found that to make it higher , the upper limit of the behavior range needs to   be 0.1 , which results in an extremely low sharpness of 11 % . Thus , we decided against it .   •Rule 5 : Person names have positive saliency . During our initial inspection , we found several cases   where the name of a person ( e.g. director or actor ) have positive saliency values . Thus , we create this   rule from the NER tag , covering 2.3 % of words . However , after parameter tuning , we found that while   many of the words have positive saliency , the correct characterization is that they all have small saliency   values , as a behavior range of [ −0.06,0.1]achieves 91.6 % validity . However , since SHAP saliencies   are mostly concentrated around 0 , this range achieves a meager sharpness of 26.8 % . Despite this , we   still decide to keep it .   •Rule 7 : Zero - sentiment words have small saliency . Besides stop words , we should expect words that   do not carry sentiment , such as most nouns and verbs ( e.g. , movie andget ) , to have small saliency   magnitudes . Due to the wide range of words applicable under this rule , we choose the saliency range to   be[−0.15,0.15]for≥90 % validity , but this range yields lowest sharpness of 13.5 % .   •Rule 8 , 9 : Weakly positive / negative words have weakly positive / negative saliency . Finally , we set up   two rules to capture words that have sentiment of neither zero ( covered by Rule 19 ) nor high - magnitude   ( covered by Rule 3 – 5 ) . To achieve 90 % validity , we require a behavior range of [ −0.11,1]for weakly   positive words and [ −1,0.05]for weakly negative words , unfortunately again with quite low sharpness .   Notably , both ranges need to “ spill over ” to the other side of zero for the required validity.5373B.2 QQP Paraphrase Detection   For the explainer , we used the LIME algorithm implemented by the lime repository . Fig . 7 depicts the   explanations on two pairs ( after normalization ) .   B.2.1 Complete Rule Union Description   Below , we present details of the construction process for rules not discussed in Sec . 5.2 .   •Rule 4 , 5 : Saliency for trailing question marks . Since the dataset is composed of pairs of questions ,   the vast majority of sentences conclude with question marks . These should be purely decorative and   syntactic , and so should have small saliency , similar to stop words . However , we observe that the   saliencies assigned to them for positive and negative predictions are very different , so we create two   rules for these two cases . With a 90 % validity target , the saliency range is [ -0.04 , 0.03 ] for negative   predictions and [ -0.07 , 0.06 ] for positive predictions . Again , the saliencies for positive predictions   demonstrate more variation than those for negative ones .   •Rule 6 , 7 : Saliency for stop words . Similar to SST , we use these two rules to ensure stop words should   notbe inﬂuential . We split the stop word group into ﬁner segments by part of speech , to achieve higher   sharpness . On average , the range is [ -0.07 , 0.03 ] for negative predictions and [ -0.09 , 0.1]for positive   predictions , which again demonstrate a much higher degree of variation .   •Rule 8 . 9 : Saliency for negation words . In the SST case , we found that negation words typically   have negative saliency regardless of the sentiment label , and test whether this holds for QQP as well .   Following on our previous ﬁndings , we use two rules to separately model inputs of positive and negative   predictions . We ﬁnd that the range is [ -0.1 , 0.24 ] for positive predictions and [ -0.21 , 0.01 ] for that for   negative predictions . Curiously , the same negative saliency trend is preserved here as well , but only for   inputs with negative predictions .   •Rule 10 , 11 : Saliency for everything else . Finally , we designed two lowest - precedence “ catch - all ”   rules to complete the coverage . The range for positive prediction FEUs is [ -0.13 , 0.25 ] . For negative   prediction inputs , we ﬁnd that breaking them according to different parts of speech ( nouns , verbs ,   adjectives , and everything else ) is helpful , with verbs having a particularly narrow saliency range of   [ -0.05 , 0.05 ] . On average , the saliency range is approximately [ -0.09 , 0.05 ] .   C Understandability as a Uniﬁed Theme   In this section , we elaborate on how understandability is the uniﬁed theme behind many properties of   explanations that seem “ orthogonal ” to correctness . Speciﬁcally , we discuss three properties : human   alignment , robustness , and counterfactual similarity and plausibility.5374C.1 Human Alignment   Many prior works have assessed how much explanations agree with human expectation . For example ,   Li et al . ( 2016 ) observed that the word “ hate ” contributes the most to a negative sentiment prediction   in many inputs , and used it to argue the explanation is correct . In a similar sentiment classiﬁcation task ,   Bastings et al . ( 2019 ) used the high degree of overlap between the extracted rationale and strong - sentiment   words to argue the superior quality of a neural rationale model ( Lei et al . , 2016 ) . In computer vision ,   this alignment is often implemented as a pointing game that computes the intersection - over - union ( IoU )   metric between the salient region and the semantic segmentation mask of the predicted class ( Simonyan   et al . , 2014 ; Fong and Vedaldi , 2017 ) , as shown in Fig . 8 . For a model that predicts breast cancer onset   using patients ’ genetic information , Covert et al . ( 2020 ) demonstrated that many of the inﬂuential genes   identiﬁed by their explainer were indeed known to be associated with the disease .   As discussed in App . A.1 , models could use any unexpected spurious correlation , such as the green   background in Fig . 8 . For these models , correct explanations should have low alignment scores . When   correctness ( or faithfulness ) is the sole desideratum of interpretability methods , it is unclear what purposes   these alignment evaluations serve . Some authors ( e.g. Jacovi and Goldberg , 2020 ) have even argued they   are fundamentally misleading and ﬂawed in nature as they focus on plausibility , which is sometimes at   odds with the goal of correctness .   However , from the perspective of understandability , high - alignment explanations are arguably very   understandable , simply because they align closely with human expectation . Thus , given the same level of   correctness , a higher - alignment explainer may be preferable .   C.2 Robustness   Besides human alignment , robustness – i.e. that similar inputs should have similar explanations – is   also argued to be a favorable property for explanation . For example , Ghorbani et al . ( 2019 ) argued   that explanations are fragile due to their adversarial vulnerability , Alvarez - Melis and Jaakkola ( 2018a )   empirically estimated the Lipschitz constant for many explainers , and Alvarez - Melis and Jaakkola ( 2018b )   proposed an inherently interpretable model that is explicitly regularized for explanation robustness .   Robustness generally conﬂicts with correctness . If , for two inputs , the model is using distinct reasoning   patterns , the correct explainer should faithfully report distinct explanations for them . One straightforward   example is the decision tree model shown in Fig . 9 , where the root node splits on the second feature at   a threshold value of 3 . For two inputs xandxthat agree on all features except the second one , with   x= 2.99andx= 3.01 , since they are sent down two different sub - trees at the very beginning , the   model is likely to use for totally different features.5375Nonetheless , as implicitly argued by the works above , erratic model behaviors are less understandable   because they make it more difﬁcult to identify generalizable patterns compared with slowly varying expla-   nations in the input space . Thus , robustness is another aspect of the same understandability desideratum .   C.3 Counterfactual Similarity and Plausibility   Unlike feature attribution explainers that assign importance to individual features , counterfactual ( CF )   explainers ( e.g. , Ross et al . , 2021 ) directly generate whole inputs but for a target predicted class . Thus , a   CF explanation indicates how to cross the decision boundary from the input .   Naturally , the fundamental requirement of CF explanations is achieving the target prediction , which is   typically known as validity . However , this is trivially satisﬁable by simply ﬁnding a training instance with   the target prediction , along with other ways such as creating adversarially perturbed or nonsensical inputs .   Thus , two additional requirements are often enforced : similarity and plausibility . The former says that the   CF explanation should be close to the original input ( with regard to , for example , edit distance ) , and the   latter says the CF explanation should be plausible , or natural . Tab . 5 depicts various CF explanations and   their satisfaction of the three requirements .   Input : This restaurant is the best I have been , with especially great food .   CF Type Val . Sim . Plau .   This restaurant is the worst I have   been , with especially terrible food . “good ” CF      Rude service!training set   look - up     This resturant is the best I have   been , with especially great food.adversarial   typo injection     Fjwpeaf fawekl fka erj sfdlk erjlm   adl erio fdnonsensical   inputs     Validity for CF can be considered as the correctness analogy for feature attribution , but the purposes   of similarity and plausibility are not readily apparent . As CF explanations represent ways to cross   the decision boundary , people need to meaningfully understand how the CF instance is related to the   original input . It is difﬁcult to relate two dissimilar instances , and an implausible CF instance is generally   unexpected . Thus , similarity and plausibility are required to make CF explanations more understandable .   Interestingly , if our true goal is the understandability of the relationship between the input and its CF   explanation , there are cases where similarity or plausibility is notdesirable . First , consider a sentence   length classiﬁer that predicts positive for sentences of at least 10 words , and negative otherwise . Given an   input of three words , the CF explainer should generate dissimilar CF instances of at least 10 words in   order to correctly illustrate the decision boundary , while instances of even more words would be helpful   for understanding the “ at least 10 words ” logic . Second , consider a classiﬁer trained on a typo - free dataset   and having high probability of making mistakes on inputs that contain typos . To illustrate this behavior ,   CF explanations should contain randomly ( not adversarially ) injected typos , which are implausible , but   useful as long as the typo injection is understood by people .   D Additional Details on Instance - Based Explanations   In this section , we describe our initial attempt at extending the ESframework to another type of   explanations : instance - based explanations ( IBE ) . The IBE for an input xis a set of instances and their   predictions{(x,/hatwidey ) } , wherexandxare semantically related ( e.g. , negation ) . We deﬁne /hatwideyas the   predicted probability of positive class.5376Type b(/hatwidey)ν σ   Entity change /hatwidey±0.05 91.4 56.5   Minor insert /hatwidey±0.05 89.1 57.3   Negation other -side(ˆy)30.4 50.0   Negation same -side(ˆy)69.6 49.9   Negation ( ≤6 words ) other -side(ˆy)56.2 49.7   We use P ( PJ , Wu et al . , 2021 ) to generate instances of three semantic operations . Entity   change replaces a proper noun ( e.g. actor name ) with another using “ lexical ” mode of PJ . Minor insert   adds a short text to the sentence using “ insert ” mode . Negation generated a negated version of the input   using “ negation ” mode . For each operation type , our expectation for model behavior is formalized as a   rangeb(/hatwidey)on / hatwidey . We expect the prediction to be unchanged by the ﬁrst two operations allowing for a margin   of 0.05 , but changed to the other side of 0.5 by negation . We then deﬁne validity ν = E / bracketleftBig   1 / bracketrightBig   and sharpness σ= 1−P[b(/hatwideY)]analogously .   Tab . 6 summarizes the results . While our expectation is mostly conﬁrmed for entity change and minor   insert , it is notably violated in the case of negation , with only 30.4 % validity , indicating model prediction   is on the same side 69.6 % of time . Upon further evaluation , we ﬁnd that validity drops with sentence   length , with short sentences of six words or fewer having much higher validity ( for other - side ) . Since the   PJ rewriting model is learned rather than manually deﬁned and negation is more complex than the other   two operations , there are two failure modes , as presented in Tab . 7 . In the ﬁrst , a negation is applied to the   input sentence , but on a part irrelevant to the sentiment . In the second , the generated sentence is not a   negation of the input by any reasonable standard .   These examples highlight the importance of clearly deﬁning the operation : rather than a generic   Input sentence “ Negated ” sentence   Human Nature initially succeeds by allowing it-   self to go crazy , but ultimately fails by spinning   out of control .Human Nature initially succeeds by allowing it-   self to go crazy , but ultimately fails by not com-   ing to consciousness .   This may be the dumbest , sketchiest movie on   record about an aspiring writer ’s coming - of - age .This may be the dumbest , sketchiest movie on   record , not an aspiring writer ’s coming - of - age .   Before long , the ﬁlm starts playing like General   Hospital crossed with a Saturday Night Live spoof   of Dog Day Afternoon .Before long , the ﬁlm starts playing like nothing   crossed with a Saturday Night Live spoof of Dog   Day Afternoon .   A startling and fresh examination of how the bike   still remains an ambiguous icon in Chinese society   .A startling and fresh examination of how the bike   still seems to be an ambiguous icon in Chinese   society .   Never engaging , utterly predictable and com-   pletely void of anything remotely interesting or   suspenseful .Not engaging , utterly predictable and completely   void of anything remotely interesting or suspense-   ful .   Between the drama of Cube ? Are there no interesting problems ?   Tailored to entertain ! No tails ! 5377negation , we would need the negation to happen on the “ sentiment - carrying ” part . It is also crucial to   ensure that the generator is of a high quality in order to minimize the chance of generating nonsensical   outputs . Despite many advances in generative language modeling , it have been shown to be undesirable in   many ways ( e.g. , Holtzman et al . , 2019 ) , all of which affect the quality of the counterfactual explanation .   At a high level , IBE explains the local prediction by illustrating ways to cross ( e.g. , negation ) or not   cross ( e.g. , entity change ) the decision boundary in the ( very ) high - dimensional input space . However , as   the negation case indicates , we must be careful about the exact deﬁnition of the rewriting ( e.g. , negating   any part of the input or the “ sentiment - carrying ” part only ) , as it could have a signiﬁcant impact on the   conclusion . Furthermore , it is difﬁcult for any rewriting mechanism to achieve 100 % validity due to   the high dimensionality , the multitude of possible ways of rewriting , and the imperfection of the model .   Focusing only on the mistakes ( or ignoring them altogether ) yields incomplete model understanding .   Instead , the validity metric , which indicates the generalized model behavior , should be used to .   There are many potentially fruitful directions for future work . First , the quality of instances obviously   depend on the generative models , which , while impressive , are known to be ﬂawed in many ways ( e.g. ,   Holtzman et al . , 2019 ; Nadeem et al . , 2021 ; Wolfe and Caliskan , 2021 ) . Second , each rule essentially   covers the entire input space . Partitioning the input space in some way may allow for identiﬁcation of both   more and less consistent areas , which is makes the applicability function much more difﬁcult to deﬁne as it   now takes whole sentences rather than individual words . Finally , unlike feature attribution , which conveys   the single notion of “ importance , ” different instances of the same input can reveal different aspects of   model behavior , calling for a potentially different deﬁnition of coverage , which measures completeness of   understanding.5378