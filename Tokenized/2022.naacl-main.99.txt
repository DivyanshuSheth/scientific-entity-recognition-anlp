  Haiyan Yin , Dingcheng Li , Ping Li   Cognitive Computing Lab   Baidu Research   10900 NE 8th St , Bellevue , WA 98004 , USA   { haiyanyin18 , dingchengl , pingli98}@gmail.com   Abstract   Paraphrase generation is an important lan-   guage generation task attempting to interpret   user intents and systematically generate new   phrases of identical meanings to the given   ones . However , the effectiveness of para-   phrase generation is constrained by the ac-   cess to the golden labeled data pairs where   both the amount and the quality of the train-   ing data pairs are restricted . In this paper ,   we propose a new weakly supervised para-   phrase generation approach that extends the   success of a recent work that leverages rein-   forcement learning ( RL ) for effective model   training with data selection . While data se-   lection is privileged for the target task which   has noisy data , developing a reinforced selec-   tive learning regime faces several unresolved   challenges . In this paper , we carry on impor-   tant discussions about the above problem and   present a new model that could partially over-   come the discussed issues with a model - based   planning feature and a reward normalization   feature . We perform extensive evaluation on   four weakly supervised paraphrase generation   tasks where the results show that our method   could signiﬁcantly improve the state - of - the - art   performance on the evaluation datasets .   1 Introduction   Paraphrase generation is an important natural lan-   guage generation task which aims to generate a   target sentence that encapsulates the meaning of   a given source sentence while conforming to the   style of some desired exemplar . It plays an essential   role in many real - world applications for natural lan-   guage processing , such as semantic parsing ( Berant   and Liang , 2014 ; Wu et al . , 2021 ) , machine trans-   lation ( Resnik et al . , 2010 ; Mallinson et al . , 2017 ) ,   recommend system ( Falke et al . , 2020 ) and ques-   tion answering ( Fader et al . , 2013 ; Rinaldi et al . ,   2003 ; Duboué and Chu - Carroll , 2006 ) . Different   from other controllable text generation tasks where   golden labelled data pairs are accessible and oftenbeing readily available , for paraphrase generation   tasks , large scale of parallel paraphrase samples   are often extremely hard to collect because gen-   erating them would often consume extensive do-   main knowledge or the generation could hardly be   standardized . Therefore , the chance of performing   supervised learning in real life scenarios would be   considerably limited .   To overcome the data unattainable issue , un-   supervised and semi - supervised approaches have   achieved growing attention in the recent decade .   Generally , the unsupervised approaches adopt   sampling - based or editing - based techniques ( Bow-   man et al . , 2016 ; Miao et al . , 2019 ) to remedy   golden standard knowledge but they generally re-   sult in less coherent or controllable target phrases   due to their lose of supervision . Therefore , in   our paper , we focus on weakly - supervised para-   phrase generation which has demonstrated great   effectiveness in many major natural language pro-   cessing tasks ( Dehghani et al . , 2017 ; Sun et al . ,   2020 ) . Although weakly - supervised approaches   have successfully pushed forward the state - of - the-   art performance standard for the language - based   tasks , when employed for paraphrase generation ,   they still face the challenge of how to acquire high-   quality paired paraphrase data and therefore lead   to noisy data pairs which might bring negative ef-   fect to the downstream task of training paraphrase   generation models .   To overcome the aforementioned challenge and   effectively train paraphrase generation models from   noisy and not equally informative paraphrase pairs ,   we adopt a learning to selectively learn approach .   That is , a meta model is learned to select intuitive   paraphrase pairs while eliminating the low qual-   ity ones . Thus the paraphrase generation model   which is jointly learned with the meta data selector   model could achieve better performance through   the carefully speciﬁed selective learning process .   Nonetheless , it is impossible for learning an effec-1385tive meta data selection policy to be a supervised   learning task due to the missing of optimal tar-   get selection policy . To overcome this issue , we   adopt a reinforcement learning - based approach to   learn effective selection policy without supervised   signal . To this end , we extend the success of pre-   vious reinforcement learning - based approach for   data selection ( Ding et al . , 2021 ) . However , formu-   lating a Markov decision process ( MDP ) for the   paraphrase learning process is a non - trivial task .   In previous works , several important parts of their   MDP formulation , such as the design of reward   signal , are in need of further investigation ( Yoon   et al . , 2019 ; Ding et al . , 2021 ) and there also lacks   in depth discussion on the challenge of solving the   reinforcement learning problem . In this paper , we   are motivated to extend this important line of using   reinforcement learning to perform selective learn-   ing in weakly - supervised paraphrase generation   problems and thus overcoming the data unattain-   able issue . Overall we present several key insights   into formulating the MDP for the selective learn-   ing problem as well as developing a model - based   reinforcement learning framework to effectively   solving the MDP .   This paper has three main contributions :   •We present a novel model - based reinforcement   learning approach for effectively training para-   phrase generation models under weakly super-   vised regime , where our proposed reinforce-   ment learning approach could effectively over-   come some of the major limitations of the exist-   ing works for data selection .   •We present an in - depth discussion on the chal-   lenges and the potential ways to formulate the   selective weakly supervised paraphrase gener-   ation tasks with reinforcement learning , which   sheds light on the important direction of devel-   oping more sophisticated reinforcement learn-   ing frameworks for weakly supervised para-   phrase generation .   •We present extensive empirical evaluation re-   sults on four evaluation datasets where the   weakly supervised datasets are generated from   supervised or unsupervised manner . The eval-   uation results show that our proposed method   could lead to substantially better performance   than all the considered baseline approaches over   all the evaluation datasets.2 Related Work   Paraphrase Generation has long been an im-   portant research problem for the natural lan-   guage processing community . Traditional meth-   ods solve this problem by exploiting linguistic   knowledge ( Wubben et al . , 2010 ; McKeown , 1979 )   or utilizing statistical machine translation ( Quirk   et al . , 2004 ; Dolan et al . , 2004 ) . As being   a sequence generation task , most of recently   emerged approaches are framed as instances of the   deep neural networks - based sequence - to - sequence   ( seq2seq ) models ( Prakash et al . , 2016 ; Chen   et al . , 2020 ) . Early works are mostly devel-   oped under a supervised setting while discard-   ing the noise in the datasets . Two representa-   tive examples are the Residual LSTM ( Prakash   et al . , 2016 ) and BERT ( Chen et al . , 2020 ) . Later   on , researchers start to work on improving the   quality of the paraphrases , such as leveraging re-   trieval augmented ( Kazemnejad et al . , 2020 ; Lewis   et al . , 2020b ; Hashimoto et al . , 2018 ) or syntac-   tic structure - based ( Iyyer et al . , 2018 ; Chen et al . ,   2019 ) approaches to produce better paraphrases .   Besides the aforementioned approaches , there are   also another lines of methods that attempt to al-   leviate the labeling cost with attempts like unsu-   pervised learning ( Bowman et al . , 2016 ; Fu et al . ,   2019 ; Bao et al . , 2019 ; Miao et al . , 2019 ; Wang   et al . , 2020 ) as well as simulated annealing ( Liu   et al . , 2020b ) and reinforcement learning ( Siddique   et al . , 2020 ) . Compared to the conventional re-   inforcement learning methods which consider the   generators as the policy models , our work models   the policy as a meta learner to accomplish a data   selection objective . Our work is mostly related to   ( Ding et al . , 2021 ) , but we adopt a very different   reinforcement learning approach which is the key   for effective selective learning .   Selective Learning refers to the case of selecting   items , e.g. , features or data points , to learn from   among other items . It motivates many important   ﬁelds in machine learning , such as active learn-   ing ( Cohn et al . , 1996 ; Settles , 2009 ; Xu et al . ,   2013 ; Fan et al . , 2019 ; Liu et al . , 2020a ) and robust   machine learning ( Hendrycks et al . , 2018 ; Reed   et al . , 2015 ; Mirzasoleiman et al . , 2020 ) . Our   work is motivated by the existing instance - wise   active data / feature acquisition approaches . One   typical example is the conventional linear model   that poses sparsity inducing prior distribution to1386the model ( Tibshirani , 1996 ) and thus actively se-   lects important features to the model . Recently ,   there also emerged approaches that adopt reinforce-   ment learning to actively ﬁnd optimal feature sub-   sets ( Yoon et al . , 2019 ; Shim et al . , 2018 ; Zannone   et al . , 2019 ) . Though such attempts have demon-   strated certain efﬁcacy in handling instance - wise   feature selection , they only deal with non time-   series data in non NLP domains , while the focus   of our work is to deal with noisy labeled pairs in   paraphrase generation tasks . Our work is mostly   related to the instance - level active data acquisi-   tion approaches ( Yoon et al . , 2020 ; Ding et al . ,   2021 ) , which are mostly adopted under the cir-   cumstances of data efﬁcient or cost - sensitive learn-   ing or when dealing with noisy data . Yoon et al .   ( 2020 ) and Ding et al . ( 2021 ) are formulation - wise   identical while Yoon et al . ( 2020 ) is among the   very ﬁrst model for active data selective learning ,   whereas Ding et al . ( 2021 ) applies it on the task   of paraphrase generation . Our work extends this   important direction to perform selective learning   but we formulate a new model - based reinforcement   learning method which aims to overcome partial   limitations for the existing work and is empirically   proven to be more effective than it on all the exper-   imental domains .   3 Reinforced Selective Learning for   Paraphrase Generation   We present the general formulation for reinforce-   ment learning - based selective weakly - supervised   paraphrase generation problems .   3.1 Weakly - Supervised Paraphrase   Generation Problem   Paraphrase generation is a sequence - to - sequence   natural language generation problem . Formally ,   given a set of Nsource sentencesX={x } ,   where each sentence Xis a set of discrete to-   kens , i.e. , x={o } , paraphrase generation   aims to obtain a non - parallel output sentences   Y={y } , where each yencapsulates iden-   tical meaning to xbut comes in the form fol-   lowing some desired exemplars . When training   paraphrase generation model , obtaining golden la-   beled targetYis a critical challenge . Therefore ,   we consider a weakly - supervised paraphrase gen-   eration regime , forming a set of pseudo labeled   pairs termed asD = { x , y } . When train-   ing models under the weakly - supervised regime , our work adopts a commonly taken assumption   in weakly - supervised learning works . That is , the   model has access only to a small set of high - quality   parallel sentencesD={x , y}(L<<M )   which could be considered as golden labeled pairs .   To generate high - quality target for the pseudo   labeled pairs , retrieval - based expansion approach is   adopted to generate paraphrase { y}which has   recently demonstrated great effectiveness in text   generation tasks ( Kazemnejad et al . , 2020 ; Lewis   et al . , 2020b ) . Speciﬁcally , for each source sen-   tence x , BM25 ( Robertson and Zaragoza , 2009 ) is   ﬁrst adopted as an effective retriever . Then we use   Elastic Search ( Gormley and Tong , 2015 ) to create   search indexes for fast searching similar sentences   tox . The beneﬁt of using such a combination is   that the method provides ﬂexibility for weak su-   pervision while being training - free . Though there   are considerable possibilities for adopting alterna-   tive approaches such as training - based methods for   generating paraphrases , we demonstrate that our   adopted method already yields promising perfor-   mance .   Given the training data D andD , the   paraphrase generation model is optimized by Max-   imum Likelihood Estimation ( MLE ) , i.e. ,   L(ψ ) = /summationdisplay−logp(y|x).(1 )   3.2 Markov Decision Process   Reinforcement learning is an area of machine learn-   ing concerned with how to learn sequential / non-   sequential decision making policies for the prob-   lems formulated as Markov Decision Processes .   Formally , a Markov Decision Process is formu-   lated as a tuple < S , A , P , R , γ > , whereSis a   set of states , Ais a set of actions , Pis atransition   probability matrix , Ris the reward function applied   upon a state - action pair , and γis adiscount factor .   Among the transition probability matrix , each of   its entry determines the probability of transiting   from one state to another . In the reinforcement   learning environment , actions are executed state by   state , forming time sequences . At each time step ,   the agent observes a state , determines an action to   be issued under the state , and receives a reward   from the environment suggesting the or optimality   of the action given the state . The state is Marko-   vian , which means that the decision could be solely   determined by the presented state and not on its   preceding states . The objective of reinforcement1387learning is to maximize the cumulative rewards   received by the agent .   3.3 Reinforced Paraphrase Generation   We now present a detailed discussion on how to   technically combine reinforcement learning with   paraphrase generation and devise a reinforcement   learning - based selective learning paradigm .   Given the training regime with noisy labeled   data , evaluating the value of the data would be   a fundamental problem . To tackle this problem ,   we target at utilizing reinforcement learning tech-   niques to learn an adaptive data valuator model   M(·)which could be jointly updated with the para-   phrase generator model and intuitively give value   evaluation over the pseudo paraphrase pairs . Gener-   ally , M(·)could be considered as a reinforcement   learning agent that we train to maximize the re-   ward signal which is quantitatively represented as   improvement achieved by the generator through-   out the model training period . With this regard ,   we present how to formulate the Markov Decision   Process ( MDP ) from corresponding state , action   and reward for the selective learning of paraphrase   generation task in the following section . We also   discuss the challenges of MDP formulation in se-   lective learning and potential ways to improve it .   STATE The state refers to what the agent observe   for decision making . It is the representation of the   informative features for an instance or a group of   data instances to be evaluated . Ideally , the infor-   mation conveyed by a state should dynamically   change throughout the learning process . With end-   to - end RL , the state could be represented by low-   level raw features , such as image pixels . It is priv-   ileged to use high - level representations for state   which could potentially ease the policy learning . In   our case , we adopt a very common representation   for the state sentences , which is extracted from a   pretrained language model . Generally it reﬂects   the static importance of data to the task without de-   tailed modeling on the learning process . However ,   the importance values for the data inferred by our   model are dynamic due to the dynamic updates of   model parameters .   ACTION In selective learning , the way we could   model action is relatively ﬁxed . That is , an action   needs to tell the decision about whether a data point   or a group of pints need to be selected or not . In   this paper , we model the action as a Bernoulli vari-   able over each data point . There is generally lessspace to improve the action modeling part in the   reinforced paraphrase generation process .   REWARD The reward signal is designated to tell   the beneﬁt of selecting a data instance to update   the model over its paraphrase generation quality .   It is also the most problematic item to model in   the MDP . While the data valuator model is jointly   trained with the generator model , it is impossible   to obtain a golden standard reward signal to tell the   importance of data for the dynamic learning envi-   ronment . Most of the existing selective learning   approach model the reward from the improvement   of the downstream task performance before and   after the model is updated by the data instance   placed in a mini batch of samples . However , such   performance score - based reward modeling has the   following two major limitations : ( 1 ) it generates   a reward over a group of mini batch samples and   thus could not yield precise term over each indepen-   dent data point ; ( 2 ) the reward score has a changing   distribution whose scale keeps decreasing and even-   tually converges to 0 , which could bring difﬁculty   to the policy learning ( upon convergence the per-   formance would no more increase and therefore   lead to a mean of 0 over the reward ) . Most of   the existing works consider the performance score-   based reward modeling only without compensating   its scaling or independence issue . We develop a   method with model - based and scaling ﬂavour to   partially overcome the aforementioned challenges .   4 MB - RPG : Model - based Reinforced   Paraphrase Generation Framework   In this section , we introduce our proposed Model-   based Reinforced Paraphrase Generation frame-   work ( MB - RPG ) . The overview of MB - RPG is   shown in Figure 1 . The essences of MB - RPG   can be summarised as two points . One , MB - RPG   adopts model - based planning attempting to per-   form decision making based on multi - step look-   ahead . This way , we can address the ﬁrst limitation ,   namely , the short - slightness brought by the conven-   tional score - based approach , one - step look - ahead .   Two , MB - RPG adopts a reward scaling module   to normalize the reward as a ﬁxed distribution to   overcome the stochasticity of reward distribution .   4.1 Model - based Planning   We present a sophisticated method where the deci-   sion making system is developed to learn the policy   based on long term effects , in order to overcome the1388   limitation of the existing works where the reward   is inferred from short - term noisy prediction loss-   based metrics . In fact , the model - based planning   method has been long studied and been proved ef-   fective in the reinforcement learning literature ( Oh   et al . , 2015 ; Wang and Ba , 2020 ) . Unlike model-   free reinforcement learning approaches where the   agent experiences single - length trajectory , at each   time step the episodes expand along one of the ac-   tion directions instead . Accordingly , the resulting   single chain enables our model - based approach to   expand future trajectories through multiple action   directions and perform the decision making based   on the planned roll - outs . Let τbe the planned fu-   ture steps , i.e. , at each time step , the policy network   makes decision based on the planned τ - step future   states along multiple ( e.g. , all ) action directions .   Then the multi - step reward for each action direc-   tion can be formulated as follows :   R(s , a ) = E / bracketleftBig / summationdisplayr(s , a)/bracketrightBig   ,   wheres∼Ψ(s|s , a),(2 )   where Ψ(·)is the learned model of the environ-   ment ( e.g. , one - step transition model ) . In our case ,   as the environment is only determined by the pa-   rameter value for the generator model , we spec-   ify one independent generator model at each plan-   ning direction . That is , over the Naction direc-   tions , we will reserve one model to expand each   action direction . After planning , the decision of   which sample to use is made deterministic by select-   ingargmaxR(s , a ) . The aggressive planningscheme enables the decision making to consider   prioritizing the actions with better long - term effect .   This would typically signiﬁcantly accelerate the   exploration efﬁciency for the policy training . Such   planning would only take effect on the action sam-   pling part , i.e. , which samples to be activated to   choose would be determined by planning . With   this design , the learning part is unaffected and con-   veniently inherits a model - free nature .   4.2 Reward Scaling   One problem in our prediction - loss based reward   modeling is that the reward signal would have a   noisy distribution whose mean would gradually   decay and eventually converge to 0 . In order to re-   solve it , inspired by many principled reinforcement   learning methods such as Actor - critic ( Konda and   Tsitsiklis , 1999 ) and PopArt ( Hessel et al . , 2019 ) ,   we propose a simple yet effective reward scaling   approach to normalize the reward signal to a static   distribution . To this end , our normalization method   stores the scalar reward signals within a recent win-   dow , which would typically consume very minimal   amount of memory . Then when we update the pol-   icy model , we normalize the reward signal for each   transition in the following manner ,   ˆR = R−R   R−R(3 )   whereRandˆRare the raw and normalized re-   ward signals , RandRare the minimal and   maximal values obtained from the window of re-   wards . With the normalized reward signal , the data1389Algorithm 1 : Pseudocode for Model - based   Reinforced Paraphrase Generation ( MB-   RPG ) Algorithm   Input : Weakly - labeled parallel dataset   D , pretrained language model   g(y|x)and RL selector p(s|x ) .   Output : A paraphrase generation model   g(y|x).while not done do Sample N mini - batches of data D   fromD foragent←1,Ndog←g forstep←1,Ndo Compute state representation s Compute selection probabilities Sampleafor each instance Updategwith the samples end end Compute reward using validation data   for each agent model ; Updateg(y|x)with the agent with   maximum reward ; Update RL selector p(s|x)with the   reward.end   selection policy could be optimized by the REINI-   FORCE algorithm ( Sutton et al . , 1999 ) . We present   the pseudocode for MB - RPG in Algorithm 1 .   5 Experiments   In this section , we present extensive empirical eval-   uation results on comparing our method with its   various counterparts on four commonly used para-   phrase generation datasets .   5.1 Experimental Setting   Evaluation Datasets . For comparison , we con-   sider to adopt both supervised and unsupervised   datasets . Note that our method adopts a semi-   supervised setting where the target paraphrases aregenerated following a retrieval - based method intro-   duced in Section 3.1 and thus alleviates the need   for golden labeled target data . Overall , we exper-   iment with the following four datasets . The ﬁrst   two datasets for supervised setting and the last two   for unsupervised scenario :   •Quora - s : corresponds to the Quora Question   Pairs ( QQP ) datasetwhich consists of 400,000   question pairs and each pair comes with a binary   tag telling whether that pair is paraphrase or   not . To split the dataset for training and testing ,   we follow the existing works ( Li et al . , 2018 ;   Kazemnejad et al . , 2020 ; Ding et al . , 2021 ) and   use randomly sampled non - overlapping parallel   pairs with sizes 100 K , 3 K and 30 K for training ,   validation and testing , respectively .   •Twitter : is the Twitter News URL Corpuspro-   posed by Lan et al . ( 2017 ) . The dataset is cre-   ated by large - scale sentential paraphrases from   Twitter by linking tweets through shared URL .   Following ( Li et al . , 2018 ; Kazemnejad et al . ,   2020 ; Ding et al . , 2021 ) , we randomly sample   110 K instances from automatically labelled data   as our training dataset and two non - overlapping   datasets of sizes 1 K and 5 K from the human-   annotated data to form the validation set and the   testing set , respectively .   •Quora - U : is the unsupervised version of Quora-   s. To make a fair comparison , we follow the   settings of the works ( Miao et al . , 2019 ; Liu   et al . , 2020b ) and use two sets of 3 K and 20 K   non - overlapping pairs as the validation set and   the testing set , respectively .   •MSCOCO : is the COCO image captioning   dataset . It consists of over 500 K captioning   paraphrase pairs for more than 120 K images . To   create the datasets for training and testing we   follow Lin et al . ( 2014 ) to split the dataset and   adopt an identical evaluation protocol presented   in Liu et al . ( 2020b ) .   Implementation . We adopt a pretrained trans-   former encoder - decoder ( sequence - to - sequence )   WS - BART ( Lewis et al . , 2020a ) as the backbone   of the generator in our proposed method MB - RPG.1390   To improve the efﬁciency of reinforcement learn-   ing , we model the reinforced data valuator model   Mas a pretrained BERT followed by two fully-   connected trainable layers as the head for policy   output . BERT serves as a feature extractor and is   kept ﬁxed during policy learning . We present other   details for our method in appendix .   Baseline Methods . We compare our method with   twelve benchmark approaches including the state-   of - the - art method . In general , the baselines   come from the following three categories : ( i )   supervised methods that are trained with high-   quality supervised target paraphrases , i.e. , Trans-   former ( Vaswani et al . , 2017 ) , RbM ( Li et al . ,   2018 ) , Residual LSTM ( Prakash et al . , 2016 )   and two retrieval - based methods FSET ( Kazem-   nejad et al . , 2020 ) and RaE ( Hashimoto et al . ,   2018 ) ; ( ii ) unsupervised methods that do not   have access to the parallel data , including Con-   strained sentence Generation with Metropolis-   Hastings ( CGMH ) ( Miao et al . , 2019 ) VAE ( Bow-   man et al . , 2016 ) , Unsupervised Paraphrase gen-   eration with Simulated Annealing ( UPSA ) ( Liu   et al . , 2020b ) and Progressive Unsupervised Para-   phrasing ( PUP ) ( Siddique et al . , 2020 ) , Dynamic   Blocking ( DBlock ) ( Niu et al . , 2020 ) ; ( iii ) semi-   supervised methods including WS - BART ( Lewis   et al . , 2020a ) which corresponds to BART trained   upon the weakly - supervised data and Learning To   Selectively Learn ( LTSL ) ( Ding et al . , 2021 ) which   is the most closely related method to ours . LTSL   also adopts reinforcement learning for selectivelearning and our approach is a model - based im-   proved version over its vanilla policy gradient for-   mulation . Also note that both LTSL andMB - RPG   adopts BART as the pretrained generator .   5.2 Benchmark Results   We present the benchmark results on all the com-   pared methods on the four evaluation datasets in   Table 1 . Note that our proposed method is denoted   as MB - RPG . Overall , we could conclude that MB-   RPG outperforms all its baselines with signiﬁcant   margins in terms of BLEU scores and ROUGE   scores across the four evaluation datasets .   From the results , we could notice that under the   supervised setting , most methods , such as Trans-   former and FSET , could signiﬁcantly outperform   most of the results obtained from the unsupervised   setting ( e.g. , CGMH and V AE ) . Compared with   thesupervised methods , even though our method   does not touch the supervised labels , it could still   outperform the supervised methods by noticeable   margins . This further indicates that our method is   promising to be adopted in many real life appli-   cations where there is rather limited access to the   golden labelled paraphrase pairs .   Compared to the unsupervised approaches , MB-   RPG is also superior especially in terms of the   iBLEU and BLEU metrics . The main reason might   be that the word editing or sampling attempts pro-   posed in the unsupervised baselines yield less desir-   able target paraphrases and thus makes the perfor-   mance of the model trained under the unsupervised1391   data fall far below our method and various super-   vised baselines . The inferior performance of the   unsupervised methods has also been empirically   evaluated and discussed by Niu et al . ( 2020 ) .   From Table 1 , we could notice that the perfor-   mance of MB - RPG is much better than its closest   counterpart LTSL , while LTSL is also a reinforce-   ment learning - based selective learning method . In   both supervised and unsupervised scenarios , MB-   RPG could outperform LTSL consistently by a no-   ticeable margins . The average improvements on   BLEU-2 and BLEU-4 scores are 4.02 and 3.57 re-   spectively . On each BLEU or ROUGE metrics ,   MB - RPG achieves better scores than LTSL . This   shows that our proposed method MB - RPG achieves   state - of - the - art method on various paraphrase gen-   eration benchmark datasets . It also shows that us-   ing model - based planning and reﬁning the noisy   reward could bring considerable beneﬁt to the data   valuation process . Such result sheds lights to the   research of reﬁning the formulation for the Markov   Decision Process and coming up with more ade-   quate reinforcement learning frameworks to facili-   tate better data valuation . The results reveals that   the performance of the vanilla WS - BART is infe-   rior than LTSL or MB - RPG , both of which adopt   WS - BART as their generators ’ backbone .   5.3 Ablation Study   To thoroughly evaluate the effect of the individual   components we proposed upon the vanilla policy   gradient method , we present an ablation study to   evaluate the individual effect of such components .   Speciﬁcally , we consider three ablated baselines : 1 )   w/o planning : our model without the model - based   planning module ; 2 ) w/o rew norm : our model   without the reward normalization module ; 3 ) LTSL :   without both planning and reward normalization   modules . We present the results in Table 2 . From   the results , we notice that the baseline w/o rew plan-   ning achieves similar results with LTSL which is   not comparable to our proposed method . It veriﬁesthe importance of leveraging the model - based plan-   ning to reduce the noise among the short - term one-   step reward signals . From the results shown in Ta-   ble 2 , we also notice that the baseline w/o rew norm   achieves better results than LTSL , but not as good   as the full version of our method . This shows that   reward normalization is an essential step to train   the Markov Decision Process formulated for selec-   tive paraphrase generation . The aforementioned   results also reveal that the model - based planning   module and the reward normalization module are   two modules with relatively parallel effects of each   other without much conﬂicting situations . Lastly ,   we wish to highlight that the WS - BART without   reinforcement learning - based selective learning es-   sentially performs very outstanding by itself . How-   ever , leveraging reinforcement learning - based se-   lective learning could result in signiﬁcant boost to   the performance of WS - BART . This shows that re-   inforced selective learning is a promising direction   to consider for improving the SOTA performance   in paraphrase generation or other generation tasks .   6 Conclusion   Our work tackles an important problem of leverag-   ing reinforcement learning - based selective learning   techniques to effectively deal with the noisy label   issue in paraphrase generation tasks . We introduce   a model - based framework which performs plan-   ning to capture the long - term effects for efﬁcient   exploration so as to overcome the noisy short - term   reward issue experienced by most of the existing   approaches . We also propose an effective reward   normalization approach which could normalize the   noisy reward signal to a distribution with a ﬁxed   zero - mean . We demonstrate that our proposed   method could outperform baseline approaches with   signiﬁcant margins on the testiﬁed domains . Future   work includes reﬁning the state andreward terms   in the Markov Decision Process for better data val-   uation or feature selections . One ongoing work is   to integrate MB - RPG into our previous feature ex-   ploration work in video recommendation ( Li et al . ,   2020 ) to improve feature quality . Another one is   to employ MB - RPG in our RL - based coreference   resolution to ﬁlter out irrelevant features ( Fei et al . ,   2019 ) . In addition , we could consider construct-   ing the policy model upon alternative generator   backbone . Also , we could consider inferring the re-   ward from different sources , such as incorporating   auxiliary language models in the training.1392References139313941395