  Megha Sundriyal , Atharva Kulkarni , Vaibhav Pulastya ,   Md Shad Akhtar , Tanmoy ChakrabortyIIIT Delhi , India , IIT Delhi , India   { meghas , atharvak , vaibhav17271 , shad.akhtar}@iiitd.ac.in ,   tanchak@ee.iitd.ac.in   Abstract   The widespread diffusion of medical and po-   litical claims in the wake of COVID-19 has   led to a voluminous rise in misinformation and   fake news . The current vogue is to employ   manual fact - checkers to efficiently classify and   verify such data to combat this avalanche of   claim - ridden misinformation . However , the   rate of information dissemination is such that   it vastly outpaces the fact - checkers ’ strength .   Therefore , to aid manual fact - checkers in elim-   inating the superfluous content , it becomes im-   perative to automatically identify and extract   the snippets of claim - worthy ( mis)information   present in a post . In this work , we introduce the   novel task of Claim Span Identification ( CSI ) .   We propose CURT , a large - scale Twitter cor-   pus with token - level claim spans on more than   7.5ktweets . Furthermore , along with the stan-   dard token classification baselines , we bench-   mark our dataset with DABERTa , an adapter-   based variation of RoBERTa . The experimental   results attest that DABERTa outperforms the   baseline systems across several evaluation met-   rics , improving by about 1.5points . We also   report detailed error analysis to validate the   model ’s performance along with the ablation   studies . Lastly , we release our comprehensive   span annotation guidelines for public use .   1 Introduction   The swift acceleration of Online Social Media   ( OSM ) platforms has led to tremendous democ-   ratized content creation and information exchange .   Consequently , these platforms serve as ideal breed-   ing grounds for malicious rumormongers and tale-   bearers , abetting a colossal upsurge of misinfor-   mation . Such misinformation manifests in many   ways , including bogus claims , fabricated informa-   tion , and rumors . The massive COVID-19 ‘ In-   fodemic ’ ( Naeem and Bhatti , 2020 ) is one such   malignant byproduct that led to the rampant spreadFigure 1 : Examples of claim tweets and their ground   truth claim spans highlighted in boldface text ( blue ) .   of political and social calumny ( Ferrara , 2020 ; Mar-   golin , 2020 ; Ziems et al . , 2020 ) , accompanied by   counterfeit pharmaceutical claims ( O’Connor and   Murphy , 2020 ) . Therefore , finding such claim-   ridden posts on OSM platforms , investigating their   plausibility , and differentiating the credible claims   from the apocryphal ones has risen to be a pertinent   research problem in Argument Mining ( AM ) .   ‘ Claim ’ , as coined by Toulmin ( 2003 ) , is ‘ an   assertion that deserves our attention ’ . It is the key   component of any argument ( Daxenberger et al . ,   2017 ) . Consider the second tweet , ‘ We do n’t have   evidence ... ’ , as given in Figure 1 . For the task of   claim identification at the coarse level , the entire   tweet will be marked as a claim . However , on   closer inspection , we find that the text fragments   of‘our wine keeps you from getting # COVID19 ’   and‘Better alternative to # DisinfectantInjection ’   represent the finer argumentative units of claim   and form the set of evidence , based on which this   tweet is considered a claim . Segregating such argu-   mentative units of misinformed claims from their   benign counterparts fosters many benefits . To be-   gin with , it partitions the otherwise independent   claims in a single post , enabling us to retrieve a   larger number of claims . Secondly , it acts as a   precursor to the downstream tasks of claim check-7701worthiness and claim verification . Thirdly , it will   also bring in the angle of explainability in coarse-   grained claim identification . Finally , it will serve   the manual fact - checkers and hoax - debunkersto   conveniently strain out the unnecessary shreds of   text from further processing . We further elaborate   on the necessity of claim span identification and   exemplify it in Section 2 .   Though the recent literature reflects extensive   work done in claim detection ( Daxenberger et al . ,   2017 ; Chakrabarty et al . , 2019 ; Gupta et al . , 2021 ) ,   limited forays have been made in claim span iden-   tification i.e. , recognizing the argumentative com-   ponents of a claim ( Wührl and Klinger , 2021 ) . In   the recent past , commendable work has been done   on span - level argument unit recognition pertaining   to other computational counterparts under the um-   brella of AM , such as hate speech ( Mathew et al . ,   2021 ) , toxic language ( Pavlopoulos et al . , 2021 )   etc . Such study , however , has eluded the realm   of claims , owning to the lack of quality annotated   datasets . This heralds a specialized corpus creation   on claim span identification .   To this end , we propose CURT ( Claim Unit   Recognition in Tweets ) , a large - scale , claim span   annotated Twitter corpus . We also present several   baseline models for solving claim span identifica-   tion as a token classification task and evaluate them   onCURT . Furthermore , we introduce claim descrip-   tions , which are generic prompts aimed to assist the   model in focusing on the most significant regions   of the input text using explicit instructions on what   to designate as a ‘ claim ’ . They are elucidated later   in detail . Finally , we benchmark our dataset with   DABERTa ( Description Aware RoBERTa ) , a plug-   and - play adapter - based variant of RoBERTa ( Liu   et al . , 2019 ) , endeavored to infuse the Pre - trained   Language Model ( PLM ) with the description in-   formation . Empirical results attest that DABERTa   outperforms the conventional baselines and generic   PLMs for our task consistently across various met-   rics .   Contributions . Through this work , we make the   following tangible contributions :   1.Formulation of a novel problem statement :   We propose the novel task of Claim Span Identi-   fication that aims to identify argument units of   claims in the given text.2.Claim span identification dataset and exten-   sive annotation guidelines : We posit a large-   scale Twitter dataset , the first of its kind , with   7.5kclaim span annotated tweets , to placate the   absence of the annotated dataset for claim span   identification . Additionally , we develop com-   prehensive annotation guidelines for the same .   3.Claim span identification system : We propose   a robust claim span identification framework   based on Compositional De - Attention ( CoDA )   andInteractive Gating Mechanism ( IGM ) .   4.Extensive evaluation and analysis : We eval-   uate our model against different baselines to   confirm sizable improvements over them . We   also report thorough qualitative and quantitative   analysis along with the ablation studies .   Reproducibility . We release our dataset and   code for DABERTa athttps://github.com/   LCS2 - IIITD / DABERTA - EMNLP-2022 . We   present detailed dataset annotation guidelines in   the Appendix ( A.1 ) .   2 Why Claim Span Identification ?   As stated in Section 1 , we hypothesize that claim   span identification would aid fact - checkers to   quickly segregate claim - ridden content from the   rest of the post . Moreover , we suppose that it will   be a propitious precursor for claim verification and   fact - checking , facilitating better retrieval of rele-   vant evidences . We back our hypothesis with a   small experiment of evidence - based document re-   trieval . We collect 50 random samples from CURT ,   along with their corresponding ground - truth claim   spans . Further , for both the tweets and the claim   spans , we extract top- krelevant articles from a   knowledge - base leveraging the traditional retrieval   system , BM25 ( Robertson et al . , 1995 ) . We use   the recently released publicly available CORD19   corpus ( Wang et al . , 2020 ) to retrieve factual doc-   uments . Finally , we present retrieved documents   to three evaluators and ask them to mark whether   or not the retrieved shreds of evidence are relevant   to the given input tweet / span from our dataset . All   three annotators label each text - evidence pair inde-   pendently . Eventually , to obtain the final relevancy   score , majority voting is employed . We obtain a   high inter - annotator score ( Fleiss Kappa ) of 0.63   and 0.67 for tweets and spans , respectively .   We compare the performance of tweet - based and   span - based retrievals in terms of precision ( P ) and7702   normalized Discounted Cumulative Gain ( nDCG )   scores and report them in Table 1 . For comparison ,   we consider two different top- ksettings ( k=3 and   5 ) . We begin by examining the retrieval perfor-   mance using P@ k , which measures the fraction of   relevant documents extracted in the top- kset . Span-   based document retrieval consistently improves   precision scores when compared to tweets . For   nDCG@5 , we discover that span - based retrieval   outperforms tweet - based retrieval by more than 3 % .   When we limit the retrieval depth to 3 , we see a   similar pattern . This , in turn , demonstrates that   entire posts contain much extraneous information ,   frequently impeding the performance of evidence   retrieval systems that are a prerequisite for both   automated and manual fact - checking . In summary ,   we reinforce that our hypothesis positively stands   true , as span - based document retrieval results in a   better score for precision as well as nDCG . This   attests to the task ’s feasibility and importance in   the realm of claims .   3 Related Work   Claims on Social Media . The prevailing re-   search on claims could be cleft into three categories   – claim detection ( Levy et al . , 2014 ; Chakrabarty   et al . , 2019 ; Gupta et al . , 2021 ) , claim check-   worthiness ( Jaradat et al . , 2018 ; Wright and Au-   genstein , 2020 ) , and claim verification ( Zhi et al . ,   2017 ; Hanselowski et al . , 2018 ; Soleimani et al . ,   2020 ) . Bender et al . ( 2011 ) pioneered the efforts in   claim detection by introducing the AAWD corpus .   Subsequent studies largely relied on using linguisti-   cally motivated features such as sentiment , syntax ,   context - free grammars , and parse - trees ( Rosenthal   and McKeown , 2012 ; Levy et al . , 2014 ; Lippi and   Torroni , 2015 ) . Recent works in claim detection   have engendered the use of large language mod-   els ( LMs ) . Chakrabarty et al . ( 2019 ) re - enforced   the power of fine - tuning , as their ULMFiT LM ,   fine - tuned on a large Reddit corpus of about 5 M   opinionated claims , showed notable improvements   in claim detection benchmark . Gupta et al . ( 2021 )   proposed a generalized claim detection model for   detecting claims independent of its source . They   handled structured and unstructured data in con - junction by training a blend of linguistic encoders   ( POS and dependency trees ) and a contextual en-   coder ( BERT ) to exploit the input text ’s semantics   and syntax . As LMs account for significant com-   putational overheads , Sundriyal et al . ( 2021 ) ad-   dressed this quandary and proposed a lighter frame-   work that attempted to fabricate discernible feature   spaces . The CheckThat ! Lab ’s CLEF- 2020 shared   task ( Barrón - Cedeno et al . , 2020 ) has garnered the   attention of several researchers . Williams et al .   ( 2020 ) won the task by fine - tuning the RoBERTa   ( Liu et al . , 2019 ) accentuated by mean pooling   and dropout . Nikolov et al . ( 2020 ) ranked second   with their out - of - the - box RoBERTa vectors supple-   mented with Twitter meta - data .   Span Identification . Zaidan et al . ( 2007 ) intro-   duced the concept of rationales , which highlighted   text segments that supported their label ’s judgment .   Trautmann et al . ( 2020 ) released AURC- 8dataset   with token - level span annotations for the argumen-   tative components of stance along with their cor-   responding label . Mathew et al . ( 2021 ) proposed   a quality corpus for explainable hate identification   with token - level annotations . The SemEval com-   munity has initiated fine - grained span identifica-   tion concerning other domains of argument mining   such as toxic comments ( Pavlopoulos et al . , 2021 )   and propaganda techniques ( Da San Martino et al . ,   2020 ) . These shared tasks amassed many solutions   constituting transformers ( Chhablani et al . , 2021 ) ,   convolutional neural networks ( Coope et al . , 2020 ) ,   data augmentation techniques ( Rusert , 2021 ; Plu-   ci´nski and Klimczak , 2021 ) , and ensemble frame-   works ( Zhu et al . , 2021a ; Nguyen et al . , 2021 ) .   Wührl and Klinger ( 2021 ) resembled the closest   study to ours , wherein they compiled a corpus of   around 1.2kbiomedical tweets with claim phrases .   In summary , existing literature on claims concen-   trates entirely on sentence - level claim identification   and does not investigate on eliciting fine - grained   claim spans . In this work , we endeavor to move   from coarse - grained claim detection to fine - grained   claim span identification . We consolidate a large   manually annotated Twitter dataset for claim span   identification task and benchmark it with various   baselines and a dedicated description - based model .   4 Dataset   Over the past few years , several claim detection   datasets have been released ( Rosenthal and McK-   eown , 2012 ; Chakrabarty et al . , 2019 ) . However,7703   none of these corpora come with claim - based ratio-   nales that quantify a post as a claim . To bridge this   gap , we propose CURT ( Claim UnitRecognition   inTweets ) , a large scale Twitter corpus with token-   level claim span annotations .   Data Selection . We annotate claim detection   Twitter dataset released by Gupta et al . ( 2021 ) for   our task . However , the guidelines they presented   have certain reservations , wherein they do not ex-   plicitly account for benedictions , proverbs , warn-   ings , advice , predictions , and indirect questions .   As a result , tweets such as ‘ Dear God , Please put   an end to the Coronavirus . Amen ’ and‘@FLO-   TUS Melania , do you approve of ingesting bleach   and shining a bright light in the rectal area as a   quick cure for # COVID19 ? # BeBest ’ have been   mislabeled claims . This prompted us to extend the   existing guidelines and introduce a more exclusive   and nuanced set of definitions based on claim span   identification . We present details of the extended   annotation guidelines and guideline development   procedure in Appendix ( A.1 ) . In total , we anno-   tated7555 tweets from the Twitter corpus by Gupta   et al . ( 2021 ) which met our guidelines .   Dataset Statistics and Analysis . We segment   CURT into three partitions – training set , validation   set , and test set , in the split of 80:10:10 . Dataset   related statistics are given in Table 2 . One impor-   tant point to note here is that while a claim tweet   is typically 27tokens long , a claim span is only   around 10tokens long . This implies that the claim-   ridden tweets have a lot of extraneous informa-   tion . Arguments can also perhaps comprise several   claims that may or may not be related to each other .   Around 19 % of the claim tweets in our dataset con-   tain multiple claim spans . As a result , in total , we   obtain 9458 claim spans from 7555 tweets . We ob-   serve that the majority of the tweets contain single   claims . Out of 7555 tweets , 6039 include a single   claim , demonstrating that the majority of tweets   contemplate single assertions at a time.5 Proposed Methodology   In this section , we outline DABERTa and its intri-   cacies . The main aim is to seamlessly coalesce crit-   ical domain - specific information into Pre - trained   Language Models ( PLM ) . To this end , we intro-   duce Description Infuser Network ( DescNet ) , a   plug - and - play adapter module that conditions the   LM representations with respect to the handcrafted   descriptions . The underlying principle behind this   theoretical formalization is to link a claim span to   a claim description to guide the model on what to   focus on explicitly . As shown in Figure 2 , Desc-   Net houses two sub - components , namely , Compo-   sitional De - Attention block ( CoDA ) and Interactive   Gating Mechanism ( IGM ) . The particulars of each   component are delineated in the following sections .   Claim Descriptions . Before delving into CoDA   andIGM , we first examine Claim Descriptions ,   which are the cornerstone of the proposed model .   Claim Descriptions are handcrafted templates that   guide the model where to concentrate its focus .   The inclusion of claim description encourages the   model to focus on the most essential phrases in the   input tweet , which may be thought of as guided   attention that leads to increased performance . We   judiciously curated our claim descriptions in accor-   dance with the annotation guidelines for claims and   non - claims offered by Gupta et al . ( 2021 ) . In Table   3 we list some of the claim descriptions along with   the claims that they most align with . It is notewor-   thy that a claim can align with more than one claim   descriptions as well .   Overview of PLMs for Token Classification . To   begin with the details of the proposed framework ,   DABERTa , we present the working of PLMs for the   token classification task . PLMs such as BERT ( De-   vlin et al . , 2019 ) , DistilBERT ( Sanh et al . , 2019 ) ,   and RoBERTa ( Liu et al . , 2019 ) are widely used   for various downstream NLP tasks owning to their   strong contextual language representation capabil-   ities and fine - tuning ease . As the input to these   PLMs , each iinput text is first tokenized into a   sequence of sub - word embeddings X∈R ,   where Nis the maximum sequence length and d   is the feature dimension . Then a positional embed-   ding vector PE∈Ris added to the token   embeddings in a pointwise fashion to retain the   positional information ( Vaswani et al . , 2017 ) .   The vector Z∈R , hence obtained , is fed   to a stack of transformer encoder blocks . Each7704   encoder block is a modular unit consisting of two   sub - layers : ( i)Multi - Headed Self - Attention , and   ( ii)Feed - Forward Network . Furthermore , each   sub - layer contains a residual connection , followed   by dropout and layer normalization . For the task of   token classification , the output of the last encoder   layer is passed to a CRF layer ( Lafferty et al . , 2001 ) .   This modularity of PLMs enables easy integration   of adapter modules in their architecture for making   these PLMs task - specific and domain - dependent .   We choose RoBERTa ( Liu et al . , 2019 ) as our back-   bone network as it is the best - performing baseline   ( see Table 4 ) .   Description Infuser Network ( DescNet ) .Desc-   Net is designed to facilitate deep semantic interac-   tion among the input text and claim descriptions ,   and help underline the key fragments of claims .   It consists of precisely engineered components of   CoDA and IGM , each devised to augment the pro - cess of claim span identification .   To put formally , consider D={d , d , ... , d }   as the set of mclaim descriptions and T=   { t , t , ... , t}as the corpus of ninput texts . The   description representations are extracted from pre-   trained RoBERTa ( Liu et al . , 2019 ) and passed   through a transformer encoder layer . To begin with ,   eachiPLM generated vector Z∈Rof in-   put text tinteracts with each jdescription vector   D∈Rvia the CoDA block . Here the vector   Zforms the query , which is processed against the   vector Dacting as the key and value ( Equation 1 ) .   All such compositionally manipulated vectors   Z , after interacting with each jdescription vec-   tors are concatenated and passed through a dropout   layer before going through a non - linear transfor-   mation for dimensionality reduction ( Equation 2 ) .   The resultant vector Zalong with the vector Zis7705passed to the IGM module to extract the semanti-   cally appropriate features pertinent for fine - grained   claim span identification ( Equation 3 ) .   The vector ˆZis then passed to a CRF layer .   Compositional De - Attention Block ( CoDA ) .   The traditional narrative on attention mechanism   ( Bahdanau et al . , 2015 ; Parikh et al . , 2016 ; Seo   et al . , 2016 ; Vaswani et al . , 2017 ) is heavily biased   on the use of Softmax operator where the atten-   tion weights are always bounded between [ 0,1 ] .   Such a convex weighted addition scheme allows   the vectors to only contribute in an additive man-   ner . To counter this bottleneck , Tay et al . ( 2019 )   devised a quasi - attention technique that enables   learning of additive as well as subtractive attention   weights , allowing the input vectors to add to ( +1 ) ,   not contribute to ( 0 ) , and even subtract from ( −1 )   the output vector . They decomposed the original   Softmax - based self - attention as pointwise multi-   plication between two matrices as shown in Equa-   tion 4 , where G(.)is the negative pointwise L   distance between query Qand key K.   We adopt this quasi - attention strategy to promote   more meaningful interaction between the input text   and claim descriptions and generate more precise   claim - relevant representations .   Interactive Gating Mechanism ( IGM ) .To fur-   ther distinguish salient tokens inclusive in claim   spans , we posit Interactive Gating Mechanism . To   begin with , the vectors ZandZare max pooled   to obtain Z , Z∈R. These vectors are passed   through a series of gates , the first of them being the   conflict gate C , aimed at capturing the semantically   conflicting features in ZandZ(Equation 6 ) .   The refine gate R , on the other hand , endeav-   ors to capture the semantically similar features be-   tween ZandZ(Equation 8) .   To congregate the conflicting and similar seman-   tic representations spawned by the gates CandR , we employ an adaptive gating scheme to retain   maximum differential information from each gate .   It is given by Equation 10 .   Finally , this vector ˆZis passed to a CRF layer   for token classification .   6 Experiments and Results   Baseline Models . We employ the following base-   line systems . ▷CNN+CRF : A Convolutional Neu-   ral Network ( CNN ) trained with GloVe ( Penning-   ton et al . , 2014 ) and a CRF head on top . ▷BiL-   STM+CRF ( Huang et al . , 2015 ): A sequence la-   beling model comprising Bidirectional Long Short-   Term Memory ( BiLSTM ) and CRF layer . ▷BERT   ( Devlin et al . , 2019 ): A bidirectional transformer-   inspired auto - encoder language model fine - tune for   our span identification task . ▷DistilBERT ( Sanh   et al . , 2019 ): A smaller , faster , and lighter ver-   sion of BERT fine - tune on our dataset for the task   at hand . ▷SpanBERT ( Joshi et al . , 2020 ): An   enhanced version of the BERT trained on span pre-   diction objective . ▷RoBERTa ( Liu et al . , 2019 ): A   robustly optimized BERT approach , RoBERTa , is a   variant of BERT with improved training methodol-   ogy . We fine - tune it on our dataset . ▷NLRG ( Chh-   ablani et al . , 2021 ): A system proposed at SemEval-   2021 Task 5 on toxic span detection ( Pavlopoulos   et al . , 2021 ) . It is a combination of SpanBERT   and RoBERTa where the former model is used   for predicting the span start and end , while the   latter is used for token classification . ▷HITSZ-   HLT ( Zhu et al . , 2021b ): The system topped the   SemEval-2021 task on toxic span detection . They   approached the task as a combination of sequence   labeling and span extraction and proposed an en-   semble of three BERT - based models .   Evaluation Metrics . In concordance with   Pavlopoulos et al . ( 2021 ) , we evaluate the perfor-   mance of all the systems , based on token - level   precision ( P ) , recall ( R ) , and F1 scores . To further   put a lens over how the models fare for different   token types , we calculate the micro - level precision ,   recall , and F1 score for each of the ‘ B’,‘I ’ , and ‘ O ’   tokens . Lastly , to quantify the number of tokens   included in the spans , we also report the Dice   Similarity Coefficient ( DSC ) ( Dice , 1945).7706   Performance Comparison . We summarize our   collated results in Table 4 . Evidently , DABERTa   outperforms all the baseline systems against major-   ity of the evaluation metrics . We analyze all the   systems based on the following questions .   How accurately do the models predict ? To gauge   how well each model performs for the token classi-   fication task , we monitor precision , recall , and F1   scores . As it can be inferred from Table 4 , the tradi-   tional word embedding - based deep learning models   of CNN and BiLSTM give the poorest token clas-   sification performance . An appreciable improve-   ment of about 10 - 14 % across all three metrics is ob-   served when we move from the classical deep learn-   ing architectures to the transformer - based models   of DistilBERT , BERT , SpanBERT , and RoBERTa .   This underlines the importance of using contextual   word embeddings and transformer - based architec-   tures for the task at hand . The addition of the CRF   layer further amplifies the performance of these   models . SpanBERT also fares better than BERT   as it is trained using span prediction objective . We   also notice that employing the CRF layer results in   a somewhat better balance of precision and recall   when compared to using a basic linear layer . The   ensemble - based models of NLRG and HITSZ - HLT   also give admissible results for our task . Our pro-   posed model , DABERTa , surpasses all the models   in terms of the precision , recall , and F1 scores . An   improvement of about 1.5%is observed between   RoBERTa and DABERTa in terms of these metrics .   This justifies the inclusion of claim descriptions   that amalgamate domain - specific semantic infor-   mation into RoBERTa architecture via the deftly   crafted adapter module . In summary , we see that   all the models show a good trade - off between pre-   cision and recall .   Are the models aggressive or defensive ? Observ-   ing the precision , recall , and F1 scores for each   of the ‘ B’,‘I ’ , and ‘ O’tags , as shown in Table 4 ,   we get an idea of how aggressive or defensive the   models are at predicting claim spans . CNN and   BiLSTM show considerable resistance in predict-   ing the claim spans , as evidenced by high preci-   sion , recall , and F1 scores for the token ‘ O’and   less for the tokens ‘ B’and‘I ’ . The BERT - based   models show a sizable improvement of about 22 %   and15 % for predicting tokens ‘ B’and‘I ’ , respec-   tively , over the traditional deep learning models .   The addition of CRF layers further bolsters the pre-   dictive power for the token ‘ B’ . DABERTa offers   an improvement of about 4 - 5%over its traditional   counterpart for predicting the token ‘ B ’ . Upon close   inspection , we observe that the ranges of precision ,   recall , and F1 scores for predicting the tokens ‘ I ’   and‘O’vary by not more than 3 % . However , the   predictive power for the token ‘ B’varies vastly by   about 25 % . Hence , we hypothesize that the inclu-   sion of descriptions makes our model cognizant of   the syntactic and semantic constructs of claims .   How the models behave for multiple spans ? Fig-   ure 3 illustrates how well the models identify7707multiple spans . It is observed that the models of   CNN and BiLSTM find it challenging to identify   multiple spans . The transformer - based models   with a linear head tend to predict more claim   spans in the tweet than required . This issue is   mitigated when the linear head is replaced with a   CRF layer . Still , these models can identify roughly   only80 % of the time the occurrence of multiple   spans . On the other hand , our model , DABERTa ,   correctly predicts multiple spans almost more   than 85 % of the time . Moreover , it does not   predict more claim spans than required . Thus , the   addition of domain - specific claim descriptions   appropriately guides DABERTa in identifying the   correct occurrence of spans .   Ablation Study . Table 4 also reports the ablation   studies . Replacing CoDA with a naïve Dot - Product   Attention ( DPA ) , we observe a drop in the perfor-   mance across almost all the metrics . Amongst all ,   the performance drop in predicting the token ‘ B ’   is the most prominent ( ∼1.5%across precision ,   recall , and F1 ) . Thus , we conjecture that the quasi-   attention mechanism is better able to spot the start-   ing of a claim fragment than DPA . When IGM is   removed , the performance for predicting token ‘ B ’   slightly improves . However , it leads to a decrease   in the predictive power for ‘ O’token ( ∼2.5%in   F1 ) . Therefore , the combination of CoDA andIGM   obtains the most balanced performance .   Hyper - parameter Tuning . We utilize the base   version of RoBERTa ( Liu et al . , 2019 ) to propose   DABERTa . The model is trained end - to - end using   the Adam optimizer ( Kingma and Ba , 2014 ) , learn-   ing rate of 4e−5 , and batch size of 32 for 20 epochs   with early stopping if the dice score does not im-   prove after 5 epochs . We used the Nvidia Tesla   v100 32 GB GPU . The hyper - parameter tuning is   done with respect to the validation dataset . Figure 4 reflects the effect of integrating the   adapter DescNet at different layer of RoBERTa .   It is observed that the performance consistently in-   creases as the integration is done at a higher level   of RoBERTa layers . This is admissible as studies   on probing the PLM layers suggest that different   layers encode distinct linguistic properties ( Tenney   et al . , 2019 ) . Furthermore , evidence by Peters et al .   ( 2018 ) suggests that the lower layers of a language   model encode the syntactic information , whereas   the higher layers capture the complex semantics .   As we strive to employ deep semantic interaction   between the PLM representations and the claim   descriptions , our results are consistent with their   findings .   Error Analysis . In this section , we manually ana-   lyze the errors the models are prone to make . Table   5 highlights randomly sampled tweets from our   dataset , CURT , along with their gold spans and   predictions from DABERTa . In addition , we also   consider the predictions from the best - performing   baseline , RoBERTa , for a fair comparison . We   analyze the errors committed by both the systems   and divide them into three different categories : ( i )   tweets with a single - claim span , ( ii ) tweets with   claim - like premises , and ( iii ) tweets with claims   that can be inferred from the underlying undertone   of the tweet but no explicit span can be marked   to highlight the claim - specific connotation , e.g. ,   figurative sentences , satire , indirect questions etc .   ( Note : For simplicity , we refer to such claims as   implicit claims . ) In the most straightforward situa-   tion where the tweets only contain a single claim   DABERTa makes more precise predictions than the   baseline system as shown in the first example of   Table 5 . We observe that both the models identify   the claim - span correctly ; however , RoBERTa iden-   tifies some unnecessary spans , which trespasses   our objective of equipping the fact - checkers with   only relevant information . The second type of er-   ror related to spans is the presence of claim - like   premises . Claims and premisesare closely related   components of argument mining , and differentiat-   ing them is strenuous , even for humans . Example 2   in Table 5 , exhibits a post containing claim - premise   pair . There are two conclusive claims in the tweet   – ‘ # coronavirus was used by the # CCP as a bio   weapon ’ and‘CCP is kicking out black people from   hotels even if they do n’t have covid ’ . Even though   ‘ not only to kill people but to encourage racism7708   among their citizens against foreigners ’ appears to   be a claim at first glance , it serves as the premise   to support the conclusive part of the arguments   brought forward in the tweet . In most cases , we   discern that both the systems identify the claim-   spans correctly , but they are easily fooled by the   premises , hence leaving room for significant im-   provement in this regard .   Another prominent class of errors is implicit   claims . Extracting the claim - spans in implicit   claims is arduous . We observe that both the sys-   tems strive to understand the linguistic structure   of the implicit claims . For instance , in sample   3 , the user intends to assert that honey , ginger ,   garlic , or turmeric do not cure COVID19 ; how-   ever , DABERTa fails to understand the user ’s in-   tention and yields the wrong span . We perceive   similar behavior from the best - performing base-   line , RoBERTa , as well . A plausible reason is   the skewed nature of the dataset , which is lop-   sided with a significant bias toward explicit claims .   According to our observations , DABERTa outper-   forms the best - performing baseline system signifi-   cantly ( ∼4%;p < 0.0004 ) .Hence , furnishing us   with empirical shreds of evidence that DABERTa   can be efficiently used for claim span identification .   7 Conclusion   Through this systematic research , we introduced   the novel task of Claim Span Identification , which   is valuable on various fronts . We conducted   an evidence - based document retrieval experiment , demonstrating that employing claim spans retrieves   more relevant evidence than using the entire tweet .   Furthermore , as there exists no specialized corpus   for claim span identification , we compiled CURT ,   a large - scale Twitter corpus consisting of around   7.5ktweets annotated with token - level claim spans .   We showed convincing results using various to-   ken classification baselines on our dataset . More-   over , we benchmarked CURT withDABERTa , an   adapter - based variant of RoBERTa , that encapsu-   lates critical domain - specific information into the   pre - trained model via claim descriptions . Through   extensive qualitative , quantitative , and empirical   results , we illustrated how DABERTa outperforms   the other models on different fronts . Lastly , we also   developed an extensive set of annotation guidelines   and released them for further research .   8 Limitations   Though DABERTa yields the state - of - the - art per-   formance in claim span identification ; there are a   few cases where it falls short . Even for humans ,   recognizing claim spans in figurative or metaphor-   ical sentences is arduous ; consequently , our sug-   gested model also struggles with them . As a re-   sult , our future study will focus on boosting the   claim span identification performance , especially   for such sentences . Our analysis also bestowed that   the high resemblance between claims and premises   confuses the model , making it difficult to distin-   guish between the two . DABERTa shares the said   limitation with other baseline systems as well . As a   result , this could be another alluring open challenge   to work on.7709Acknowledgement   The authors would like to thank the Ramanujan   Fellowship , SERB and CAI , IIIT - Delhi for the valu-   able support .   References771077117712A Appendix   A.1 Annotations   A.1.1 Guideline Development   While different frameworks and models of argu-   mentation range in intricacy and claim conceptual-   ization , the claim element is colloquially perceived   as a principal component of an argument . Fol-   lowing Stab and Gurevych ( 2017 ) , we define the   claim as ‘ the argumentative component in which   the speaker or writer conveys the central , con-   tentious conclusion of their argument ’ . Aharoni   et al . ( 2014 ) proposed a framework in which an   argument is often divided into two parts : claim   and premise . The premise , which is another cru-   cial component of an argument , encompasses all   shreds of evidence obliged to either corroborate or   refute the claim . We confine our corpus to claim   components only . However , claims and premises   are usually indistinguishable and frequently blend   together . As a result , distinguishing them can be   challenging , especially when authors use claim - like   statements as a premise .   Due to the highly subjective nature of claims , it   is imperative to devise structured annotation guide-   lines to annotate a new dataset for the claim span   identification task . Therefore , after rigorous anal-   ysis and discussion , we established an initial set   of annotation guidelines . To acclimate better with   the dataset , we progressed through iterations of im-   provements . In every iteration , 100 random tweets   were annotated by three annotatorsfollowing the   initial set of annotation guidelines . The annota-   tors resolved the ambiguous cases mutually . In   successive iterations , we further addressed the un-   settled tweets that necessitated clarifications in the   annotation guidelines . We reconsidered all prior   annotations for every change in the guideline to   ensure that the annotations emulated the most ad-   vanced version of the annotation guidelines . The   final sprint of pilot annotation included annotating   another set of randomly chosen 100 tweets with the   final guidelines . Following Trautmann et al . ( 2020 ) ,   we calculated the inter - annotator agreement using   theαagreement measure ( Krippendorff et al . ,   2016 ) . We computed the mean pairwise value per   post , where each token can be being classified into   two classes , claim , and non - claim . We obtained   a more than satisfactory agreement score of 0.87.Finally , the entire Twitter dataset was annotated by   the same annotators that carried out the prefatory   pilot annotations .   A.1.2 General Instructions   •A claim is a statement that says you strongly   believe that something is true . The action of   showing , using or stating something strongly .   •We use tweets that are annotated with a binary   label using LESA guidelines ( Gupta et al . , 2021 ) ,   which indicates whether a tweet is a claim or not .   •The claim span is that part of a sentence that   contains the semantic representation of the claim .   Example : @realDonaldTrump A lot of people   are saying cocaine cures COVID-19 . Claim span :   cocaine cures COVID-19 .   •Since our primary goal is to tackle misinforma-   tion in OSM , we majorly focus on claims that   have some social impact .   A.1.3 Guidelines and Examples   •In the case of facts , we annotate the fact / span   that may not be known by everyone , for example ,   scientific facts or legal ( law ) facts , and does n’t   involve any commonsense . However , we do not   include universal facts in the claim span .   Example 1 : “ Water is colorless ” is a universally   known fact and hence should not be marked as   claim span .   Example 2 : “ Virus always mutate ” is a scientific   fact that may not be known by everyone . Hence   we will annotate this fact as a claim .   •An assertion about future eventuali-   ties / predictions will not be included in the   claim span . Prediction is an extrapolation   based on an assertion and is associated with a   confidence level that can never be greater than   or equal to 100 % . Thus , we will not consider   predictions as a part of the claim span .   Example : @realDonaldTrump Uh no actually   The virus will never go away Scientists will   develop a vaccine for it that should be ready by   next June which will allow nearly everyone to be   immune to # CoronaVirus This really is n’t hard   to understand even for a very stable genius .   •A proverb is a simple , concrete , traditional saying   that expresses a perceived truth based on com-   mon sense or experience which contains wisdom,7713truth , morals , and traditional views in a metaphor-   ical , fixed , and memorizable form . The proverbs   are not facts . The elements of proverbs should   be annotated as claim span .   Example : “ Prevention is better than cure ” is not   a claim .   •If a claim contains statistics or dates , they should   be included in the span . But not all numbers are   important .   Example 1 : “ @FernandoSVZLA @AP So far   50 people outside China have it with no deaths .   If China was hiding information and it was more   lethal , we would see that fairly quickly ” . Here   the claim span is [ 50 people outside China have   it with no deaths ]   Example 2 : “ 57 round trip to LA thanks coron-   avirus ” . The number is not important here .   •In case there are multiple conclusive independent   claims in one tweet , we annotate each one of   them separately .   Example : “ 5 million left Wuhan before the lock-   down . If they were really interested in knowing ,   they ’d be testing at least 1 in 100 cases of all   viral pneumonia . They ’re limiting who ’s being   tested so they are n’t accused of lying . Oh , and   they might be asked to actually do something . ”   Claim span would consist of : [ 1 ] “ 5 million left   Wuhan before the lockdown ” and [ 2 ] “ They ’re   limiting who ’s being tested so they are n’t accused   of lying ”   •Tweets that negate a possibly false claim are also   considered to be claims .   Example : “ disinfectants are not a cure for coron-   avirus ” .   •Tweets ‘ reporting ’ something to be true or an   instance to have happened or will happen are   claims .   •In cases of claims made in the form of a con-   ditional sentence , the premise / context would be   included in the span .   Examples : if you ’ve been in the McDonald ’s play   place you ’re immune to the coronavirus .   •For claims containing humor / sarcasm , only the   humorous phrase will be considered as a claim   span if it has some social impact . For satire , the   complete sentence will be considered .   Example : @TheRickWilson Drinking bleach   and/or injecting Disinfectant will cure COVID19.And cancer , heart disease , OCD , schizophrenia   and AIDS . And life . # Covid19 # COVID Claim :   Drinking bleach and/or injecting Disinfectant   will cure COVID19 . And cancer , heart disease ,   OCD , schizophrenia and AIDS . And life .   •Personal experience will only be part of the   claim phrase if they are opinions with societal   impacts / implications .   Example : Story about how # HydroxyChloro-   quine likely help people recover from # Coron-   avirus . IMO , it was never touted as the cure but   as option for treatment doctors should consider   and it appears to work in some cases .... 39 in one   place . https://t.co/2hhi6aSVrY   Claim : [ it was never touted as the cure but as op-   tion for treatment doctors should consider and it   appears to work in some cases .... 39 in one place . ]   •A claim can be a sub - part of a question , only if it   is not a direct question .   Example : @FLOTUS Melania , do you approve   of ingesting bleach and shining a bright light in   the rectal area as a quick cure for # COVID19 ?   # BeBest ”   Claim : [ ingesting bleach and shining a bright   light in the rectal area as a quick cure for   # COVID19 ]   •Ground / Reasoning to justify a claim will not be   a part of the claim phrase .   Example : Covid-19 vaccine development and   deployment in China , when available , will be   made a global public good , which will be China ’s   contribution to ensuring vaccine accessibility and   affordability in developing countries   Claim phrase : [ Covid-19 vaccine development   and deployment in China , when available , will   be made a global public good ]   •Mocking / attacking a group or individual is not a   part of the claim phrase .   Example : Because # coronavirus has tremendous   chances of getting cured but your anti - national   agenda is worse than death   Claim : [ coronavirus has tremendous chances of   getting cured ]   •Claim phrases do not include the predicate part   that does not contribute to it being a claim .   Example : I firmly believe that [ if they found   a way to bottle the @andersoncooper giggle , it   would cure the corona virus ] 7714A.2 Data Preprocessing   We employ NLTKto tokenize the tweets . Each   token in the tweet is BIO ( Begin - Inside - Outside ) en-   coded to generate the labels ( Ramshaw and Marcus ,   1999 ) . Tag ‘ B’indicates that the token is at the start   of a span , tag ‘ I’indicates that the token is within   the span , while tag ‘ O’denotes that the token is out-   side the span . As RoBERTa tokenizes each word   into subwords ( Liu et al . , 2019 ) , each subword is   given the BIO tag as per their parent word . We   eliminate tokens made of non - ASCII and special   characters , as well as remove the URLs provided   in the tweets . Finally , we split hashtag terms by   underscore delimiter and over non - consecutive up-   percase character . For instance , # WuhanLab splits   into‘Wuhan ’ and‘Lab ’ .7715