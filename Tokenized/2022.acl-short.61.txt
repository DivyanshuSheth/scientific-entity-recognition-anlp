  Rahil Parikh   Institute for Systems Research   University of MarylandChristophe Dupuy   Amazon Alexa AIRahul Gupta   Amazon Alexa AI   Abstract   Natural Language Understanding ( NLU ) mod-   els can be trained on sensitive information such   as phone numbers , zip - codes etc . Recent liter-   ature has focused on Model Inversion Attacks   ( ModIvA ) that can extract training data from   model parameters . In this work , we present   a version of such an attack by extracting ca-   naries inserted in NLU training data . In the   attack , an adversary with open - box access to   the model reconstructs the canaries contained   in the model ’s training set . We evaluate our   approach by performing text completion on ca-   naries and demonstrate that by using the prefix   ( non - sensitive ) tokens of the canary , we can   generate the full canary . As an example , our   attack is able to reconstruct a four digit code in   the training dataset of the NLU model with a   probability of 0.5 in its best configuration . As   countermeasures , we identify several defense   mechanisms that , when combined , effectively   eliminate the risk of ModIvA in our experi-   ments .   1 Introduction   Natural Language Understanding ( NLU ) mod-   els are used for different tasks such as question-   answering ( Hirschman and Gaizauskas , 2001 ) , ma-   chine translation ( Macherey et al . , 2001 ) and text   summarization ( Tas and Kiyani , 2007 ) . These mod-   els are often trained on crowd - sourced data that   may contain sensitive information such as phone   numbers , contact names and street addresses . Nasr   et al . ( 2019 ) , Shokri et al . ( 2017 ) and Carlini et al .   ( 2018 ) have presented various attacks to demon-   strate that neural - networks can leak private infor-   mation . We focus on one such class of attacks ,   called Model Inversion Attack ( ModIvA ) ( Fredrik-   son et al . , 2015 ) , where an adversary aims to recon-   struct a subset of the data on which the machine-   learning model under attack is trained on . We   also demonstrate that established ML practices ( e.g.   dropout ) offer strong defense against ModIvA.In this work , we start with inserting potentially   sensitive target utterances called ‘ canaries’along   with their corresponding output labels into the train-   ing data . We use this augmented dataset to train an   NLU model f. We perform a open - box attack on   this model , i.e. , we assume that the adversary has   access to all the parameters of the model , including   the word vocabulary and the corresponding em-   bedding vectors . The attack takes the form of text   completion , where the adversary provides the start   of a canary sentence ( e.g. , ‘ my pin code is ’ ) and   tries to reconstruct the remaining , private tokens   of an inserted canary ( e.g. , a sequence of 4 digit   tokens ) . A successful attack on freconstructs all   the tokens of an inserted canary . We refer to such a   ModIvA as ‘ Canary Extraction Attack ’ ( CEA ) . In   such an attack , this token reconstruction is cast as   an optimization problem where we minimize the   loss function of the model fwith respect to its   inputs ( the canary utterance ) , keeping the model   parameters fixed .   Previous ModIvAs were conducted on computer   vision tasks where there exists a continuous map-   ping between input images and their corresponding   embeddings . However , in the case of NLU , the dis-   crete mapping of tokens to embeddings makes the   token reconstruction from continuous increments   in the embedding space challenging . We thus for-   mulate a discrete optimization attack , in which the   unknown tokens are eventually represented by a   one - hot like vector of the vocabulary length . The   token in the vocabulary with the highest softmax   activation is expected to be the unknown token of   the canary . We demonstrate that in our attack ’s best   configuration , for canaries of type “ my pin code   iskkkk",k∈ { 0,1 , . . . , 9},1≤i≤4 , we   are able to extract the numeric pin kkkkwith   an accuracy of 0.5(a lower bound on this accu-   racy using a naive random guessing strategy for a   combination of four digits equals 1×10).552Since we present a new application of ModIvA   to NLU models , defenses against them are an im-   portant ethical consideration to prevent harm and   are explored in Section 6 . We observe that stan-   dard training practices commonly used to regular-   ize NLU models successfully thwart this attack .   2 Related Work   Significant research has been conducted in the field   of privacy - preserving machine learning . Shokri   et al . ( 2017 ) determine whether a particular data-   point belongs to the training set X. The success   of such attacks has prompted research in investi-   gating them ( Truex et al . , 2019 ; Hayes et al . , 2017 ;   Song and Shmatikov , 2019 ) . Carlini et al . ( 2018 )   propose the quantification of unintended memoriza-   tion in deep networks and presents an extraction   algorithm for data that is memorized by genera-   tive models . Memorization is further exploited in   Carlini et al . ( 2020 ) where instances in the training   data of very large language - models are extracted by   sampling the model . The attacks described above   are closed - box in nature where the adversary does   not cast the attack as an optimization problem but   instead queries the model multiple times .   Open - box ModIvA were initially demonstrated   on a linear - regression model ( Fredrikson et al . ,   2014 ) for inferring medical information . It has   been extended to computer vision tasks such as fa-   cial recognition ( Fredrikson et al . , 2015 ) or image   classification ( Basu et al . , 2019 ) . Our work is a   first attempt at performing ModIvAs on NLP tasks .   3 Attack Setup   We consider an NLU model fthat takes an ut-   terance xas input and uses the word - embeddings   E(x)for the tokens in xto perform a joint in-   tent classification ( IC ) and named - entity recogni-   tion ( NER ) task . We assume an adversary with   open - box access to f , which means that they are   aware of the model architecture , trained parame-   tersθ , loss function L(f(E(x)),y ) , label set Y   of intents and entities supported by the model and   vocabulary Vwhich is obtained from the word-   embeddings matrix W∈I R. However , the   adversary does not have access to the training data   Xused to train f. The adversary ’s goal is to   reconstruct a ( private ) subset ˆx⊆X.   To perform a CEA on f , we keep the parame-   tersθfixed and minimize the loss function Lwith   respect to the unknown inputs ( i.e. , tokens ) of agiven utterance . This is analogous to a traditional   learning problem , except with fixed model param-   eters and a learnable input space . In this work ,   we use the NLU model architecture described in   Section 4.1 .   3.1 Canary Extraction Attacks   We consider a canary sentence x= ( x , x ) ,   x∈Xwith tokens ( p , .. , p , u .. , u)and   output label y∈Y. The first mtokens in x   represent a known prefix x(e.g . “my pin code   is ” ) and the next ntokens ( u , .. , u)represent the   unknown tokens that an attacker is interested in   reconstructing x(e.g . “one two three four ” ) .   We represent the set of word embeddings of this   canary E(x)as(e, .. ,e , e, .. ,e ) .   A trivial attack to identify the nunknown tokens   inxis by directly optimizing L(f(E(x)),y )   over(e, .. ,e ) , where ( e, .. ,e)are ran-   domly initialized . Words corresponding to the op-   timized values of ( e, .. ,e)are then assigned   by identifying the closest vectors in the embedding   matrix Wusing a distance metric ( e.g. Euclidean   distance ) . However , our experiments demonstrate   that this strategy is not successful since the updates   are performed in a non - discrete fashion , whereas   the model fhas a discrete input space . We thus fo-   cus on performing a discrete optimization , inspired   by works on relaxing categorical variables to facili-   tate efficient gradient flow ( Jang et al . , 2016 ; Song   and Raghunathan , 2020 ) , as illustrated in Figure 1 .   We define a logit vector z∈I Rfor each   token u∈x . We then apply a softmax activation   with temperature Tto obtain a∈I R :   a = e   Pefor v = 1 , 2 , . . . , |V|(1 )   ais a differentiable approximation of the arg - max   over the logit vector for low values of T. This   vector then selectively attends to the tokens in the   embedding matrix , W∈I R , resulting in the   embeddings ( e, .. ,e)used as inputs fed to   the model during the attack :   e = W·afor1≤i≤n ( 2 )   We then train our attack and optimize for Z∈   I R , withZ= ( z , . . . , z ):   ˆZ= arg minL(f(E(x)),y ) ( 3)553   Zis the only trainable parameter in the attack and   all parameters of fremain fixed . Once converged ,   we identify the token xas the one with the highest   activation in a. We decrease the temperature T   exponentially to ensure low values of Tin Equation   ( 1)and enforce the inputs to fto be discrete . In   our experiments , we define zover a subset of   candidate words for xV , V⊆Vto prevent the   logit vector from becoming too sparse .   4 Experiments   4.1 Target Model Description   We attack an NLU model jointly trained to perform   IC and NER tagging . This model has a CLC struc-   ture ( Ma and Hovy , 2016 ) . The input embeddings   lead to 2 bi - LSTM layers and a fully - connected   layer with softmax activation for the IC task and   a Conditional Random Field ( CRF ) layer for the   NER task . The sum of the respective cross - entropy   and CRF loss is minimized during training . We   use FastText embeddings ( Mikolov et al . , 2018 ) as   inputs to our model .   4.2 Canary Insertion   We inject Rrepetitions of a single canary with sen-   sitive information and its corresponding intent and   NER labels into the training set of the NLU model .   We insert three different types of canaries with n   unknown tokens , n∈ { 4,6,8,10 } , described in   Table 1 . Cis a set of 12 colors . Additional details   of the canaries and their output labels are presented   in the Appendix A. The adversary aims to recon-   struct all the nunknown , sensitive tokens in the   canary . The reduced vocabulary Vin Equation ( 1 )   is the set of all digits for canary callandpinand   the names of 12 colors for canary color .   4.3 Attack Evaluation   We inject the canary into Snips ( Coucke et al . ,   2018 ) , ATIS ( Dahl et al . , 1994 ) and NLU-   Evaluation ( Xingkun Liu and Rieser , 2019 ) . The   canary is repeated with R∈ { 1,10,100,500 } . For   each combination of R , canary type and length n ,   the experiment is repeated 10 times ( trials ) with 10   different canaries , to account for variation induced   by canary selection . We define the following evalu-   ation metrics averaged across all trials to evaluate   the strength of our attack .   Average Accuracy ( Acc ): Fraction of the trials   where the attack correctly reconstructs the entire   canary sequence in the correct order . A higher   Accuracy indicates better reconstruction . Accuracy   is1if we can reconstruct all ntokens in each of   the 10 trials .   Average Hamming Distance per Token   ( HDT ): The Hamming Distance ( HD ) ( Hamming ,   1950 ) is the number of positions at which the recon-   structed utterance sequence is different from the   inserted canary . Since HD is proportional to the   length of the canary , we normalize it by the length   of the unknown utterance ( HDT = HD / n ) . The   HDT can be interpreted as the probability of recon-   structing the incorrect token for a given position in   the canary , averaged across the 10 trials . A lower   HDT indicates better reconstruction .   Accuracy reports our performance on recon-   structing allnunknown tokens in the correct order   and is a conservative metric . HDT quantifies our   average performance for reconstructing each po-554   sition in the unknown sequence . We evaluate our   attack against randomly choosing a token from the   reduced vocabulary V. Thus for a given value of   n , the expected accuracy and HDT of this baseline   are()and1−respectively .   5 Results   The trivial attack described in Sec3.1 without dis-   crete optimization performs comparably to the ran-   dom selection baseline . We thus focus on perform-   ing the attack with discrete optimization in this Sec-   tion . Table 2 shows the best reconstruction metrics   for the different values of nand the correspond-   ing repetitions R∈ { 10,100,500}at which these   metrics are observed in the Snips dataset . In our ex-   periments , our attack consistently outperforms the   baseline . For n= 4,6 , we reconstruct at least one   complete canary for each pattern . The attack also   completely reconstructs a 10 - digit pinfor higher   values of R , with an accuracy of 0.10 . Even when   we are unable to reconstruct every token in any   trial , i.e. accuracy is zero , we still outperform the   baseline , as observed from the HDT values .   For the sake of brevity , we summarize the attack   performance on other datasets in Appendix C.2 .   We observe that the attack is dataset - dependent   with best performance for the Snips dataset and   poorest for the NLU - evaluation dataset .   5.1 Discussion   The training data of NLU models may poten-   tially contain sensitive utterances such as “ call   k. . . k",k∈ { 0,1 , . . . , 9 } . An adver-   sary who wishes to extract the phone - number can   assume the prefix “ call " , along with the output la-   bels of the utterance which are also trivial to guess , given access to the label set Y. Our canaries act   as a placeholder for such utterances . We choose to   insert the canary color since the names of colors   appear infrequently in the datasets mentioned in   Section 4.3 , allowing us to evaluate the attack on   ‘ out - of - distribution ’ data which is more likely to be   memorized by deep networks ( Carlini et al . , 2018 ) .   Forn= 4 andR= 1 ( i.e. , the canary only   appears once in the train set ) , our attack has an   accuracy of 0.33 for canary color and 0.10 for pin .   This suggests that the attack could potentially re-   construct sensitive information from short rare ut-   terances in real - world scenarios . For a special case   when the adversary attempts to reconstruct a ten   digit phone - number in canary callwith a three digit   area - code of their choosing , the attack can recon-   struct the remaining seven digits of the number   with an accuracy of 0.1 when R= 1 . For con-   ciseness , we show these results in Appendix C.1 .   We observe that our model is more effective and   with fewer repeats for the canary color than ca-   naries pinandcallof the same length . Our empiri-   cal analysis indicates the attack is more successful   in extracting tokens that are relatively infrequent   in the training data and in reconstructing shorter   canaries . As shown in Appendix C.1 , the attack   performs best for R= 1000 . However , this trend   of improved reconstruction for larger values of R   is not monotonic and we observe a general decline   in reconstruction for R > 1000 . We are unsure of   the vulnerabilities that facilitate CEA . While un-   intended memorization is a likely explanation , we   note that our attack performs best on the Snips data ,   although the smaller ATIS data should be easier to   memorize ( Zhang et al . , 2016 ) .   6 Proposed Defenses against ModIvA   We propose three commonly used modeling tech-   niques as defense mechanisms- Dropout ( D ) , Early   Stopping ( ES ) ( Arpit et al . , 2017 ) and including   a Character Embeddings layer in the NLU model   ( CE ) . D and ES are regularization techniques to re-   duce memorization and overfitting . CE makes the   problem in 3 more difficult to optimize , by concate-   nating the embeddings of each input token with a   character level representation . This character level   representation is obtained using a convolution layer   on the input sentence ( Ma and Hovy , 2016 ) .   For defense using D , we use a dropout of 20 %   and 10 % while training the NLU model . For ES ,   we stop training the NLU model under attack if the555validation loss does not decrease for 20 consecutive   epochs to prevent over - training .   6.1 Efficacy of Defenses   In this section we present the performance of the   proposed defenses against ModIvA. To do so , we   evaluate the attack on NLU models trained with   each defense mechanism individually , and in all   combinations . The canaries are inserted into the   Snips dataset and repeated 10,500and1000 times .   The results are summarized in Table 3 . We observe   that the attack accuracy for each defense ( used indi-   vidually and in combination ) is nearly zero for all   canaries and is thus omitted in the table . We also   note that the HDT approaches the random baseline   for most defense mechanisms . The attack perfor-   mance is comparable to a random - guess when the   three mechanisms are combined . However , when   dropout or character embedding is used alone , HDT   values are lower than the baseline , indicating the   importance of combining multiple defense mecha-   nisms . Additionally , training with defenses do not   have any significant impact on the performance of   the NLU model under attack . The defenses thus   successfully thwart the proposed attack without   impacting the performance of the NLU models.7 Conclusion   We formulate and present the first open - box   ModIvA in a form of a CEA to perform text com-   pletion on NLU tasks . Our attack performs discrete   optimization to select unknown tokens by optimiz-   ing over a set of continuous variables . We demon-   strate our attack on three patterns of canaries and   reconstruct their unknown tokens by significantly   outperforming the ‘ chance ’ baseline .   To ensure that the proposed attack is not misused   by an adversary , we propose training NLU mod-   els with three commonplace modelling practices –   dropout , early - stopping and including character   level embeddings . We observe that the above prac-   tices are successful in defending against the attack   as its accuracy and HDT values approach the ran-   dom baseline . Future directions include ‘ demystify-   ing’such attacks , and strengthening the attack for   longer sequences with fewer repeats and a larger V   and investigating additional defense mechanisms ,   such as those based on differential privacy , and   their effect on the model performance .   8 Ethical Considerations   The addition of proprietary data to existing datasets   to fine - tune NLU models can often insert confi-   dential information into datasets . The proposed   attack could be misused to extract private infor-   mation from such datasets by an adversary with   open - box access to the model . The objectives of   this work are to ( 1 ) study and document the actual   vulnerability of NLU models against this attack ,   which shares similarities with existing approaches   ( Fredrikson et al . , 2014 ; Song and Raghunathan ,   2020 ) ; ( 2 ) warn NLU researchers against the pos-   sibility of such attacks ; and ( 3 ) propose effective   defense mechanisms to avoid misuse and help NLU   researchers protect their models .   Our work demonstrates that private information   such as phone - numbers and zip - codes can be ex-   tracted from a discriminative text - based model ,   and not only from generative models as previ-   ously demonstrated ( Carlini et al . , 2020 ) . We advo-   cate for the necessity to privatize such data using   anonymization ( Ghinita et al . , 2007 ) or differential   privacy ( Feyisetan et al . , 2020 ) . Additionally , in   case the training data continues to contain some   private information , practitioners can prevent the   extraction of sensitive data by using the defense   mechanisms described in Section 6 , which reduces   the attack performance to a random guess.556References557558A Inserted Canary Information   The inserted canaries and corresponding intent and   NER label sets are listed below .   1.Canary call : “ call k. . . k",k∈   { 0,1 , . . . , 9 } , for1≤i≤n .   •Sequence NER tags : “ O B - canary   I - canary . . .I - canary| { z } "   • Intent : “ CallIntent "   2.Canary 2 : “ my pin code is k. . . k",k , for   1≤i≤n .   •Sequence NER tags : “ O O O O B - canary   I - canary . . .I - canary| { z } "   • Intent : “ PinIntent "   3.Canary 3 : “ color k. . . k",k∈{‘red ’ ,   ‘ green ’ , ‘ lilac ’ , ‘ blue ’ , ‘ yellow ’ , ‘ brown ’ ,   ‘ cyan ’ , ‘ magenta ’ , ‘ orange ’ , ‘ pink ’ , ‘ purple ’ ,   ‘ mauve ’ } for 1≤i≤n .   •Sequence NER tags : “ O B - canary   I - canary . . .I - canary| { z } "   • Intent : “ ColorIntent "   The canary repetitions Rare split between the train   and validation set in a ratio of 9 : 1 .   B Training Parameters   We decrease the temperature Texponentially after   each iteration t. The temperature at the titeration   Tis given by T= 0.997×10 .   We use the Adam optimizer and train our attack   for 250 epochs . We begin with an initial learning   rate of 6.5×10for our attack with a decay rate   of9.95×10 .   C Results   C.1 Attack Performance Across Canary   Repetitions   Table 4 shows the model performance for just one   repeat of the canary in the Snips dataset i.e. R=   1 . The n= 7 example for the callcanary refers   to the special case when the adversary is trying   to reconstruct a 10 - digit phone number beginning   with a three digit area code of their choice .   Table 5 illustrates the best reconstruction   metrics for different values on nand with   R∈ { 10,100,500,1000,2000 } . We observe an   accuracy of 0.5 for the canary pinwhen n= 4   andR= 1000 . Figure 2 illustrates the model   performance across canaries in the Snips dataset   with varying number of repetitions R. As observed   in Table 5 and Figure 2 , the attack is most likely   to succeed when Ris 1000 . However , the attack   weakens for higher values of R.   C.2 Attack Performance Across Datasets   We evaluate our attack on the ATIS and NLU-   Evaluation Datasets , for canaries color andpin   withn= 4 and canary call withn= 10 . To   ensure that we maintain a comparable number   or repeats with respect to the size of the dataset ,   R∈ { 10,100,200,500}for the ATIS dataset and   R∈ { 100,500,1000,5000,10000}for the NLU-   Evaluation dataset . As shown in Figure 3 , the at-   tack performance is almost comparable for shorter   sequences in Snips and ATIS but under - performs   for the NLU - Evaluation data . Figure 4 and Figure   5 illustrate the HDT for the ATIS and NLU Evalua-   tion datasets for Rcanary repetitions respectively.559560