  Rilwan A. Adewoyin , Ritabrata Dutta , Yulan HeDepartment of Computer Science , University of Warwick , UKDepartment of Statistics , University of Warwick , UKThe Alan Turing Institute , UKDepartment of Computer Science and Engineering ,   Southern University of Science and Technology , China   { rilwan.adewoyin,ritabrata.dutta,yulan.he}@warwick.ac.uk   Abstract   In this paper , we study the task of improving   the cohesion and coherence of long - form text   generated by language models . To this end ,   we propose RSTGen , a framework that utilises   Rhetorical Structure Theory ( RST ) , a classical   language theory , to control the discourse struc-   ture , semantics and topics of generated text .   Firstly , we demonstrate our model ’s ability to   control structural discourse and semantic fea-   tures of generated text in open generation eval-   uation . Then we experiment on the two chal-   lenging long - form text tasks of argument gen-   eration and story generation . Evaluation using   automated metrics and a metric with high cor-   relation to human evaluation , shows that our   model performs competitively against existing   models , while offering signiﬁcantly more con-   trols over generated text than alternative meth-   ods .   1 Introduction   Controllable text generation has attracted much at-   tention in recent years . Generating coherent and   cohesive long - form text with controllable attributes   is particularly challenging due to the relatively com-   plex syntactic and semantic structures involved   compared to short - text generation . Long - form text   generation can ﬁnd applications in automatic argu-   mentation , motivational speech , and opinionated   writing , to name a few .   A popular approach to controllable text genera-   tion is to design prompts , which control high - level   linguistic features ( i.e. , text style and sentiment ) by   preﬁxing simple context to the input of a language   model . These non - granular methods are often too   coarse to allow different parts of the text to have   diverse or contrasting features . Furthermore , they   tend to focus on a single linguistic feature of text ,   meaning that extra frameworks such as Plugand-   Play ( Dathathri et al . , 2019 ) are required to control   multiple linguistic features . To achieve improved content control , researchers   have coupled a content planning module with a   text generation module ( Hua and Wang , 2020 ; Hua   et al . , 2021 ) , in which the content planning mod-   ule ﬁrstly generates a set of keyphrases and their   corresponding positions in sentences and the gener-   ation module ﬁlls in the gaps . To control syntactic   structures of text , recent works have proposed a   neuro - symbolic approach leveraging Dependency   Structure Theory ( DST ) ( Shen et al . , 2019 ; Du   et al . , 2020 ) . Enforcing speciﬁc structures over   the generated text has been proven to be useful in   increasing coherence and cohesion of short - form   text . However , a relatively large amount of DST   information is required to encode comparatively   short texts . Thus , they are unsuitable for use in   long - form text generation tasks .   Similar to DST , Rhetorical Structure Theory   ( RST ) ( Mann and Thompson , 1988 ) is a classi-   cal tree - based interpretation of natural language .   Whereas DST is most useful for intra - sentence in-   terpretation , RST is most useful for inter - sentence   interpretation of language . We propose a neurosym-   bolic framework that can imbue an RST under-   standing of text to existing language models . More   speciﬁcally , our framework ( 1 ) allows more ﬁne-   grained control of syntax , semantics and text struc-   ture ; ( 2 ) utilises a well - deﬁned rhetorical structure   of language , thus offering better interpretability ; ( 3 )   can be directly integrated into existing pre - trained   language models such as GPT-2 ( Radford et al . ,   2019 ) and BART ( Lewis et al . , 2019 ) .   We evaluate our proposed framework on two   tasks : argument generation and story generation .   We show that our proposed framework improves   upon existing content control approaches on au-   tomatic evaluation metrics including BLEU ( Pa-   pineni et al . , 2002 ) , METEOR ( Denkowski and   Lavie , 2014 ) and generates better text in terms   of grammar and coherence measure . We further   demonstrate our model ’s ability to generate text1822with control over syntactic , semantic and discourse   features . Our main contributions can be sum-   marised below :   •We propose a novel framework to imbue the   RST information into pre - trained language   models .   •We develop the ﬁrst neurosymbolic frame-   work that provides interpretable ﬁne - control   over syntax , semantics , discourse structure ,   topical keywords and keyword positions .   •We demonstrate the superiority of our pro-   posed framework over existing planning and   control methods for two long - form text gener-   ation tasks .   2 Related Work   We discuss several lines of research in controllable   text generation .   Prompts For prompt - based control , a context   is prepended to a language model input . The   prepended item is related to the type of desired   output . This method has been used to manipulate   the syntax ( Dušek and Jur ˇcíˇcek , 2016 ; Goyal and   Durrett , 2020 ) and semantics ( Wen et al . , 2015 ;   Chen et al . , 2019 ) of the output . Alternatively , the   prepended item can provide semantic control in   order to cover a given topic ( Wang et al . , 2019a ) ,   mention speciﬁed entities ( Fan et al . , 2019 ) , display   a certain attribute ( Hu et al . , 2017 ; Balakrishnan   et al . , 2019 ) , or even exhibit a style of text ( Keskar   et al . , 2019 ) . These methods suffer from the in-   ability to exert ﬁne - control , that is , a change in any   one of the input prompts will change the whole   generated text . Furthermore , all these works utilise   non - expansive features for their prompts , which   prevents them from making iterative improvements   to existing generated text .   Content Planning Despite the impressive   progress made in many generation tasks , earlier   neural models are known to produce low - quality   content ( Wiseman et al . , 2017 ) , often with low   relevance , and poor discourse structure ( Zhao   et al . , 2019 ) . Content - based planning approaches   were added into neural systems to enhance content   relevance ( Moryossef et al . , 2019 ; Yao et al . ,   2019 ; Hua and Wang , 2019 ) . Hua et al . ( 2021 )   extended previous content planning approaches   by dividing the content plan into different typesof information ; ‘ Entities ’ , ‘ Claim ’ and ‘ Concepts ’ .   An alternative planning approach focuses on where   the key phrases should be placed in the generated   text in order to improve coherence ( Hua and Wang ,   2020 ) . The text generator is then conditioned   on the provided key phrases and their respective   positions in text . Similar to our approach , these   methods allow users varying levels of custom   control by manual augmentation of the planning   module output .   Syntactic or Discourse Control Various works   utilised syntactic parse trees with the transformer   structure to gain syntactic control or improve inter-   pretability on short - form text generation tasks ( Li   et al . , 2020 ; Nguyen et al . , 2020 ) . With a focus on   improving long - form text generation , Ji and Huang   ( 2021 ) used a Variational Autoencoder ( V AE ) struc-   ture to model RST discourse relations between suc-   cessive elementary discourse units ( spans of text ) .   We build upon their approach by using a more   expressive binary tree formalisation of RST . This   formalisation extends the modelling of sequential   elementary discourse units by also modelling nu-   clearity and relationship between non sequential   discourse units .   Extending on previous works , Wang et al .   ( 2019b ) attempted to couple tree structures and   transformers . We instead embed the tree structure   of RST into transformers through constrained at-   tention in a separate RST embedding dimension .   3 Rhetorical Structure Theory   Rhetorical Structure Theory ( RST ) provides a for-   mal structure for interpreting language based on   relations between parts of text . Each structure is   deﬁned by three sets of features : the binary tree   structuring of Discourse Units ( EDU)s , the nucle-   arity between sibling DUs , and the relationship   between sibling DUs . An example RST tree us-   ing the schema provided by ( Mann and Thompson ,   1988 ) as shown in Figure 1 .   Binary Tree Structuring of Discourse Units   To form a binary discourse tree Rfrom text t , the   text must be successively divided into smaller sub-   sets of text . In this manner , node 0 represents the   full text , with subsequent nodes representing sub-   texts , as in Figure 1 . The text at each node is called   a Discourse Unit , while the nodes at the leaves of   the tree are Elementary Discourse Units ( EDUs).1823   Sibling Nodes−Relations : A parent node is   a non - terminal node which has two child nodes ,   referred to as siblings . Each parent nodes ’ children ,   has a RST relation r , i∈Ndescribing the   syntactic purpose of each child node relative to the   other . In Figure 1 , the RST relations are presented   in blue texts .   Sibling Nodes−Nuclearity : Associated with   each pair of sibling node ’s relation is a joint nu-   clearity labelling , n , j∈N. This nuclearity ex-   plains the role of each sibling node ’s discourse unit   relative to the other sibling node . Each joint nucle-   arity labelling between pair of sibling nodes must   include at least one Nuclei .   Formally , we deﬁne a binary RST Tree Ras a   collection of its parent nodes { v } , where vis   a parent node at position l , with a RST relation r   and a Nuclearity nbetween its two children . For   example , v represents node 2 in Fig-   ure 1 . We refer to the group of relations in an RST   Tree , R , asR. Similarly , the group of Nuclearity   values is referred to as R.   4 RSTGen : RST - Dependent Long - form   Text Generator   We propose a long - form text generation approach ,   call RSTGen , which provides control over seman-   tics and structure of text , with the aim of improving   coherence andcohesion of the generated text . Co-   herence is concerned with how well constituent   sentences / chunks follow on from the previous sen-   tence / chunk . This can be interpreted as the topic of   each subsequent sentence / chunk being relevant to   the previous sentence / chunk ( Carrell , 1982 ; Waller ,   2015 ) . Cohesion describes the way in which text   is tied together by linguistic devices such as There-   fore ... , However ... , In addition to ( Waller , 2015 ) .   This can be interpreted as how smooth the model   transitions between the types of sentences . With   an RST interpretation , cohesion is loosely related   to the RST relations between different nodes in the   binary tree , while coherence is related to the RST   relations and key phrases at EDUs .   Task formulation Our input consists of ( 1 ) an   encoding for a binary RST Tree R , ( 2 ) a set of   keyphrases information Kthat are relevant to the   text . The RST encoding is formed as sets of three   pieces of information for each parent node in the   binary RST Tree . These include Nuclearity R ,   RST Relation Rand RST node position R. The   keyphrase information contains two pieces of infor-   mation . These are KandK , the key phrases and   the RST node positions of the key phrases . The   model has been trained to work with varying levels   of speciﬁcation for the context information .   Partial Context Provision In practice , we do   not expect users to provide the full RST Tree   or key phrases since often only a subset of fea-   tures of the context information may be known1824or needed . For example , in Figure 2 , the goal is   to produce a text contrasting the innovative abil-   ity of Microsoft and Tesla . A user only needs   to provide partial information , e.g. , the keywords ,   K= [ BMW , Tesla , innovation ] and the RST   relation , R⊃[Contrast ] . Our model will be   able to automatically predict an RST tree following   a conditional sampling method we have designed   in Section 4.3 .   While a user will often want to specify the key   phrases themselves , we could also employ auto-   mated methods to create an expanded set . For ex-   ample , existing Planning Modules ( Hua and Wang ,   2020 ; Hua et al . , 2021 ) can be used to generate a   set of keywords based on some prior information .   In what follows , we describe our proposed RST-   Gen in more details .   4.1 Tokenisation   While our RST controlled text generation frame-   work can be built upon any pre - trained language   model , we use GPT as an example here . The GPT   Tokeniser is used to tokenise the target text yand   key phrases Kto a set of word tokens . Two new   tokens are added to the tokeniser : ‘ < rst > ’ and   ‘ < kp > ’ . The former is prepended to the RST tree   R , and the latter is prepended to each key phrase .   4.2 Embedding Layers   Our framework requires three additional embed-   ding layers to facilitate the encoding of the RST   position , RST nuclearity and RST relation informa-   tion presented in R , RandR. These embedding   layers are designed to produce vectors that match   the size vof hidden vectors in the base model .   We use a RST relation Embedding Matrix W∈   Rto encode Ras there are 18 possible RST   relations and a pad token , and an RST Nuclearity   Embedding matrix W∈Rto encode R ,   which consists of 3 possible nuclearity labels and a   pad token .   To embed RST node position encodings R , we   create a novel embedding layer designed to cap-   ture the positional relationships of nodes in a bi-   nary tree , in a space efﬁcient manner . The in-   tent is to explicitly capture the relationship be-   tween a node and its ancestor nodes . Our em-   bedding layer features a non - trainable embedding   matrix W∈R , a trainable   feed forward layer W∈Rand   a Gelu activation layer f ( · ) . In our experimentsmax_rst_node = 4094 andmax_tree_depth =   12 .   Thei - th column in the non - trainable embedding   matrix Wis a vector representing the position   of node i , in terms of a sequence of Lefts ( L ) and   Rights ( R ) required to travel from the root node 0   to node i. Left is encoded as −0.05and Right as   0.05 . For example , the vector at node position 5 in   Win the example RST tree shown in Figure 1 is   encoded as [ 0.05,−0.05,0,0 .... ]with the remain-   ing0s representing padding values up to the length   max_tree_depth .   It is important to note that in our RST Tree en-   coding R , a parent node vis labelled with the   relation rand nuclearity nconnecting its chil-   dren . As such our encodings for a R , RandR   do not include the leaf nodes with no children , but   still represent R. This allows our encodings , for a   Binary RST Tree with Nnodes to be sequences of   length⌈N/2⌉.   4.3 RST Predictor   We use an RST Predictor to predict the rela-   tion and nuclearity of a child node conditional   on the RST relation , nuclearity and position   of their parent node . For example we pre-   dict the left child node v , by modelling   the following conditional distribution v∼   p / parenleftbig   r , n|l , r , n,1(v= ) .   A full RST Tree can be predicted by iteratively   repeating this one - step sampling .   We propose a neural sampler method to estimate   the conditional distribution p(·| · ) . It is trained on   the RST Trees observed in the training set of the   RST Annotated Dataset introduced in Section 6 .   Our neural RST Sampler , depicted in Figure 3 ,   uses the BART ( Lewis et al . , 2019 ) model . The   encoder takes a prompt as input . The decoder is re-   initialised and reduced to two layers . The decoder   takes four inputs : ( 1 ) The parent nodes ’ relation r ;   ( 2 ) The parent node ’s nuclearity n ; ( 3 ) The parent   node ’s RST position l ; ( 4 ) A vector bindicating   whether the target node is a left child node or right   child node . This vector bis calculated as the differ-   ence between the node position embeddings for the   parent node and the target child node encodings .   We use the Embedding Layers structures from   our RSTGen model to encode the inputs ( 1 - 4 ) to   the Encoder . The BART decoder contains two clas-   siﬁcation heads , to predict relation and nuclearity .   Two approaches can be used to guide our RST1825   predictor to produce more relevant RST Trees .   Firstly , the training set can be limited to a speciﬁc   domain , such as argumentative text , to introduce   bias the prediction towards a speciﬁc linguistic   style . Secondly , during inference the predictive   distribution can be augmented , to ensure speciﬁc   structural properties are achieved . In our experi-   ments , we use this to predict RST Trees with a spe-   ciﬁc size . We have extended this to other structural   features such as specifying RST relation frequency ,   and position .   4.4 RST - Aware Attention ( R.A.A. )   To better encode the RST tree structure in language   model training and to improve the coherence of   text by reducing the occurrence of hallucination   in long - form generated text , we propose a novel   attention scheme , called RST - aware attention . In   particular , to generate a word token at position i ,   we create dynamic attention masks mallowing   the hidden state hto focus only on structurally   relevant hidden representations h , j / negationslash = i. The   hidden representation his structurally relevant to   hifh ’s associated RST node is an ancestor of   the RST node associated to h. The RST - aware   attention is described in Algorithm 1 .   In the above process , we need to ﬁrst detect the   RST node position of the word token to be gener-   ated . We do this in a sequential manner with the   help of an EDU splitter ( Wang et al . , 2018 ) , which   can detect EDU boundaries in text . At the start of   text generation , we set the initial EDU node posi-   tion as the leftmost leaf node in the RST tree andAlgorithm 1 : RST - aware attention   then proceed to generate the ﬁrst token . Afterwards ,   we use the EDU splitter to detect whether an EDU   boundary has occurred . If no boundary is detected ,   we continue with the generation of the next word   token ; otherwise , we infer the next EDU node po-   sition as the second leftmost leaf node in the RST   tree . The above process is repeated until the ‘ end   of sequence ’ token is generated or until the ﬁ-   nal child EDU node is generated . In practice we   use heuristics to avoid the need to perform an EDU   boundary detection at each generation step .   5 Open Generation Evaluation   We ﬁrst train a model using our RST annotated   dataset described below . Then we analyse the   model ’s ability to control semantic , syntactic and   text structure .   RST Annotated Dataset We use the ConvoKit   API ( Chang et al . , 2020 ) to collect a total of over   965k text examples from Reddit to use as the train-   ing set . Table 1 provides a detail regarding the   division of the dataset between different subreddits .   The following subreddits comprise the majority of   our dataset , DebateReligion , RelationshipAdvice ,   Politics andChangeMyView . These subreddits con-   tain many long texts with opinions or persuasive1826   style of writing . Further , we choose large samples   from the subreddit CasualConversation to com-   plement the limited range of language present in   the former subreddits . Each sample contains a   reddit post , its RST tree generated using the best   statistical RST parser ( Feng and Hirst , 2012 ) and   keyphrases extracted from the post using the PageR-   ank inspired algorithm , TextRank ( Mihalcea and   Tarau , 2004 ) . The keyphrase extraction process is   detailed in Appendix D.   Semantic Control In Table 2 we show the gener-   ated short text conditional on various RST relations .   For all four examples the same key phrases of “ a   good mouse alternative " and “ none of this " were   passed to the model . Furthermore , we provide the   ﬁrst three words “ It would be " to the decoder as a   text prompt . The text is generated by varying the   RST relations , randr . We can observe that the   generated text varies depending on the desired RST   relations input to the model .   Structural Discourse Control Here we use a re-   construction style test to evaluate the ability of our   model to include the target relations in the gener-   ated text at the correct position . To do this , we   allow our model to generate text ˆyusing the RST   treeTas the conditioning factor . We then use an   RST parser to generate a RST tree ˆTfrom the gen-   erated text ˆy . For each relation r , we calculate a   recall score as the proportion of nodes with relation   rinTthat also occur in ˆTat the same position   l. The results are shown in Figure 4 . We include   results for different lengths of text , measured by   elementary discourse units .   We observe that our model achieves strong lev-   els of control for the following relations : Attri-   bution , Background , Condition , Contrast , Elabo-   ration , Joint , Manner - Means , and Temporal . We   believe that the weakened performance on Cause   andComparison is due to their respective similarity   toAttribution andContrast . We omit topic change   since our datasets contains texts mostly constrained   to a single topic .   Text Length Control By editing the length of   the RST encoding , we gain strong control of the   length of the generated text . Here we ﬁx the key   phrases and vary the length of the RST context ( i.e. ,   number of EDUs ) from 2 to 14 to demonstrate the   increasing length of generated text . Results are   depicted in Figure 5 . We believe that our method   provides a more natural way to control text length   when compared to the heuristic methods of ﬁne   tuning text generation hyper - parameters such as   repetition penalty , length penalty and minimum   length .   RST Tree Edit Distance Here we extend the   Tree Edit Distance to RST Trees to evaluate how   well RSTGen is able generate text that adheres to   the RST encoding passed to it . Speciﬁcally , we   investigate how well this model performs as the1827   RST structure becomes increasingly long . For this   experiment , we pass an RST Encoding and a set of   keywords to our model and generate a text y. We   then extract an RST tree from the generated text y   using an RST parser . An edit includes the follow-   ing operations : Changing / Removing the relation or   nuclearity label of the node at position lwith cost 1 ;   Changing the position of a node to it ’s sibling node   with cost 1 ; and Deleting / Inserting a blank node at   position lwith cost 3 . For a tree of size swe use a   normalising factor of 3×s , the cost of creating the   tree . Our normalising factor does allow distances   over 1 . We observe from Figure 6 that generated   text is able to adhere to correct RST structure in   terms of RST node positions relatively well . How-   ever , when considering nuclearity and relation , he   inaccuracy of RST structure grows signiﬁcantly .   6 Evaluation on Argument Generation   and Story Generation   We evaluate RSTGen for the tasks of argument gen-   eration and story generation . These tasks require   our model to output coherent and cohesive text .   6.1 Experimental Setup   We describe the datasets used for ﬁne - tuning our   model , baseline models and the methods used to   ensure fair comparison . Training detail and hyper-   parameter setting can be found in Appendix C.   Datasets We use two different datasets for argu-   ment generation and story generation , respectively .   CMV Dataset We use the argument generation   dataset ﬁrst introduced in ( Hua et al . , 2021 ) .   This dataset contains pairs of claims and counter-   arguments extracted from titles and responses , re-   spectively , from the Reddit ChangeMyView ( CMV )   forum . This dataset uses posts dated in 2019 , which   prevents overlap with our training set for the RST   Annotated Dataset . The goal of this task is to gen-   erate the counter - argument given the initial claim .   Writing Prompts Dataset We use the Story gener-   ation dataset created by Fan et al . ( 2018a ) and   utilised in ( Ji and Huang , 2021 ) . This dataset con-   tains pairs of prompts and stories extracted from   theWritingPrompts subReddit . The prompt is an   introduction to a story that must be extended . We   limit each story to be up to 270 tokens .   Baselines For argument generation , we evaluate   the following baselines :   CTRL . ( Keskar et al . , 2019 ) uses text prompts   to control the style of output . The trigger   r / changemyview style output we prepend the text   ‘ Opinion Title : ‘ to the opening statement .   This model is approximately 10 times larger than   RSTGen and we assume that it achieves perfor-   mance gains primarily through its larger size .   PAIR ( Planning and Iterative Reﬁnement ) . Hua   and Wang ( 2020 ) devised a two - step generation   process . The ﬁrst step uses a ﬁne - tuned BERT to1828   allocate a position to each pre - deﬁned keyword , in   order to create a template for the text to be gener-   ated . Then a ﬁne - tuned BART model is used to ﬁll   the template .   DYPLOC ( Dynamic Planning of Content using   Mixed Language Models ) . Hua et al . ( 2021 ) pro-   posed ﬁrst categorising the keywords into concepts   and entities . A BERT - based Planner is used to ex-   pand this set of concepts and entities , while another   is used to generate the claim . The claim provides a   brief summary of the extended answer . Four sep-   arate BART Encoders and a BART Decoder are   used to convert the expanded set of concepts and   entities into an argumentative text .   For both PAIR and DYPLOC , we use their gold   standard plan as input to the model , this follows the   headline results given in both papers . In pursuit of   fair testing , we convert the gold standard plans of   PAIR and DYPLOC to two RST encodings . More   concretely , for PAIR , we convert its gold standard   template , consisting of a set of phrases and their   corresponding starting positions in the text to be   generated , into a RST encoding . These phrases be-   come the keyphrases for RSTGen . The RST node   position for each key phrase can be determined   by parsing the true RST structure of the text . For   DYPLOC , ﬁrst we collect the gold standard set   of concepts / entities and claims , and ﬁlter out any   repeated words phrases . Then , the RST node posi-   tion for each word / phrase can be determined in a   similar manner to PAIR .   For Writing Prompts , we compare our proposed   approach with the following models :   DiscoDVT ( Ji and Huang , 2021 ) . To the best of our   knowledge it is the only other work that uses RST   to improve the long form text generation ability of   a language model . It has three modules : an RST   planner , an encoder and a decoder . These mod - ules are combined to form a V AE structure wherein   the RST planner produces a sequence of discrete   hidden vectors representing the sequence of RST   relations between EDUs in their text . Then at each   generation step , sequential hidden vectors are used   to guide the text generation process . For fair com-   parison we do not pass keyphrases to RSTGen .   RSTGen Ablations . We evaluate ablations of RST-   Gen to validate the two signiﬁcance features of our   proposed RSTGen framework . Firstly , we remove   the RST - Aware Attentions ( R.A.A. ) , after which   we remove RST Nuclearity , then Relation , and ﬁ-   nally node positions .   Metrics We use two sets of metrics . For the   Argument Generation experiments , we use the   GRUEN ( Zhu and Bhat , 2020 ) set of metrics ,   which measure four important features of text :   Grammaticality , Non - redundancy , Focus and Struc-   ture / Coherence . A ﬁnal metric combines these four   features , and has been shown to correlate strongly   with human reviewers . For the Story Generation   experiments , we use BLEU ( Papineni et al . , 2002 )   focusing on the n - gram precision ; Distinct- n(Li   et al . , 2016 ) , which takes the proportion of distinct   n - grams relative to all generated n - grams , thus pro-   ducing a non - reference - based measure for diver-   sity ; the GRUEN Structure and Coherence metric   ( G - SC ) ; and MS - Jaccard ( Alihosseini et al . , 2019 )   which uses the Jaccard Index to measure how simi-   lar the distribution of n - grams is between two sets   of text .   6.2 Evaluation Results   This section presents the performance of RSTGen   against baseline models and discusses the perfor-   mance of RSTGen ablations.1829Argument Generation Table 3 shows that our   proposed RSTGen using the DYPLOC Planner per-   forms on par with CTRL in terms of Grammar   despite using only one tenth parameters of CTRL .   It outperforms all the baselines in Structure and   Coherence .   Using automatically predicted RST trees as in-   put , we observe degraded performance of RSTGen   results . We believe that this can be attributed to   our Markovian RST sampling methodology of sam-   pling a child node dependent on its direct parent   node . This may produce RST Trees with unreal-   istic structures . Nevertheless , it still outperforms   CTRL in Redundancy andFocus , implying that the   combination of RST relations , nuclearity and key   phrases provide strong content guidance .   RSTGen without the RST - aware attention   ( R.A.A. ) experiences a drop in performance across   all metrics , especially Grammar andFocus . During   the generation of longer sequences , using R.A.A.   ensures that the same key phrase or RST informa-   tion does not inﬂuence adjacent elementary dis-   course units , leading to more diversity and less rep-   etition in generated text . We posit that the removal   of R.A.A. exacerbates the reduced performance of   RSTGen for longer texts , a trait that was exempli-   ﬁed in Figure 6 .   Story Generation Table 4 shows that the R.A.A.   variants of our model perform on par with the base-   line . We observe that the removal of the R.A.A.   causes a signiﬁcant drop in performance , speciﬁ-   cally Distinct-4 , conﬁrming our ﬁndings from the   Argument Generation . As the RST relations carries   information on the semantics of text , we observe   that its removal has a signiﬁcant effect on the sim-   ilarity based metrics of BLEU-1 , Distinct-4 and   MS - Jaccard 3.7 Conclusion   We present a novel controlled text generation frame-   work , RSTGen , which uses ﬁne - control over a   Rhetorical Structure Theory based context as a   means to improve the coherence and cohesion of   generated text . We also leverage the structural in-   formation presented by RST to propose an rst aware   attention scheme , ensuring that the model attends   to the correct information during long form gen-   eration . Through investigation of RSTGen ’s open   generation text , we showed that our approach can   exhibit a high level of intepretable ﬁne - control over   syntactic , semantic and structural features of text .   8 Ethics Statement   We acknowledge that our proposed model may be   susceptible to learning harmful biases present in   the dataset . In and of itself this has the poten-   tial to harm minorities , marginalised communities   and project stigmas present in society . Further , we   recognise that our efforts to improve coherence ,   cohesion and control might be misused to author   offensive or ﬁctitious content . Therefore , we advo-   cate for morally correct and responsible practices   in the case of real - world application .   Acknowledgements   This work was funded by the the UK Engineering   and Physical Sciences Research Council ( grant no .   EP / T017112/1 , EP / V048597/1 ) . YH is supported   by a Turing AI Fellowship funded by the UK Re-   search and Innovation ( grant no . EP / V020579/1 ) .   References183018311832A RST Schemas   We list in Table A1 the RST relations and necleus   that our proposed RSTGen framework utlises . We   also provide a schema in Table A2 explaining the   meaning of Nuclearity labels used by RSTGen .   B Argument and Story Generation   Dataset Details   For both the argument generation and story gener-   ation experiments our datasets originate from the   respective paper ’s of models in the baseline study .   We do not make any alterations to the datasets ,   but instead simply use it as provided . We refer   the reader to and Hua et al . ( 2021 ) for the argu-   ment generation dataset ( i.e. , the CMV dataset )   and Fan et al . ( 2018b ) for further details regard-   ing the story generation dataset ( i.e. , the Writing   Prompt dataset ) .   C Reproducibility   Code The code used to train and evalu-   ate our models can be downloaded from   https://github.com/Rilwan-A/RSTGen . As well as   code , the github will contain links to the RST an-   notated versions of the CMV dataset and Writing   Prompts dataset . Access to the full RST Annotated   Reddit dataset can be gained upon request .   Repositories The RSTGen models were ex-   tended from pretrained models in Huggingface ’s   Transformers repository ( Wolf et al . , 2019 ) . RST-   Gen is initialised using the GPT2 - base model   with approximately 124 M parameters . The neural   RST Predictor was initalised using the BART - base   model . We used Pytorch - Lightning ( Falcon et al . ,   2019 ) for all our training scripts .   Hardware For ﬁne - tuning the RSTGen model   on the RST Annotated Dateset , we used 2 GeForce   RTX 3090 ( 24 GB ) . For the Argument Generation   Tasks and the Story Generation Task , the RSTGen   variants and RST Neural Sampler are ﬁne - tuned   using 1 GeForce RTX 3090 ( 24 GB ) . All training   was done using mixed Precision ( FP16 ) to improve   memory efﬁciency .   Fine - tuning For ﬁne - tuning all variations of   RSTGen , we used the Adafactor optimiser with   the following parameter settings : scale parameter   = False , relative step = True , warmup init = True ,   learning rate = None , weight decay = 0.01 . Due to   the high computational expense required , we didnot perform extensive hyper - parameter tuning for   our RSTGen models .   When ﬁne - tuning on the RST Annotated Dataset ,   we used an effective batch size of 44 and trained   using an Early Stopping rule allowing for at most   one epoch to pass with no improvement . The max-   imum target sequence length is 270 tokens . The   maximum RST Tree Size is 36 parent nodes . The   maximum Key Phrase sequence size is 64 tokens .   These models take approximately 5 epochs to con-   verge which takes approximately 10 hours .   D Keyphrase Extraction using TextRank   Our second conditioning factor is the phrases that   are important to the generated text . This impor-   tance is determined by the steps listed below :   Noun Chunks and Named Entities Given a text   x , the noun chunks and named entities are extracted   to form a set of sub - phrases [ x , x , ... , x]∈x .   Graph Formation Separately , a graph Gis   formed from text xby extracting all words wthat   have a Part - Of - Speech tag of either ‘ Adjective ’ ,   ‘ Noun ’ , ‘ Proper Noun ’ or ‘ VERB ’ . These words   are lemmatised and form nodes V∈G.   Edge Creation A weighted edge wbetween   nodes V , V , i / negationslash = jhas a weight of 1 if the distance   between the words corresponding to V , Vis less   than some threshold k. In other TextRank imple-   mentations factors such as word length , position   and frequency can be used to scale w.   Node Scoring The TextRank score S ( V)of a   nodeVis initialised to a default value . Then Page   Rank ’s adapted Eigenvector centrality measure is   used to to calculate the importance of each node   V.S ( V)is iteratively updated using the equation   below , until convergence is reached :   S(V ) = ( 1−d ) +    d×/summationdisplayw×S ( V)/summationtextw      where dis a damping factor and set to 0.85 as in   ( Mihalcea and Tarau , 2004 ) and N(i)are the set of   indices of the neighbours of V.   Keyphrase Scoring Given the set of node scores   S ( V ) , the score of a sub - phrase x , with Lwords ,   is computed by summing the scores of the words   it contains normalised by its length +1to favor1833   longer n - grams . This is shown in the equation   below :   S(x ) = /summationtextS(V )   L+ 1   Keyphrase candidates are then ranked and redun-   dant candidates ﬁltered out . Two candidate phrases   are considered redundant if they have the same   lemmatised form .   E Argumentation Generation Examples   In this section , we show some example text gener-   ated from RSTGen in comparison to baseline mod-   els , where text with poor coherence is underlined .   We can observe that CTRL generates coherent text   but with a shorten length . Both PAIR and DYPLOC   exhibit some inconsistency in their generated text .   On the contrary , RSTGen generates much longer   text with its inconsistency appearing to be more   subtle.18341835