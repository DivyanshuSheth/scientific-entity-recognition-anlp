  Te - Lin Wu , Alex Spangher , Pegah Alipoormolabashi ,   Marjorie Freedman , Ralph Weischedel , Nanyun PengUniversity of California , Los Angeles , ISI , University of Southern California , Sharif University of Technology   { telinwu,violetpeng}@cs.ucla.edu , palipoor976@gmail.com   { spangher,mrf,weisched}@isi.edu   Abstract   The ability to sequence unordered events is ev-   idence of comprehension and reasoning about   real world tasks / procedures . It is essential   for applications such as task planning and   multi - source instruction summarization . It of-   ten requires thorough understanding of tem-   poral common sense and multimodal infor-   mation , since these procedures are often con-   veyed by a combination of texts and im-   ages . While humans are capable of reason-   ing about and sequencing unordered procedu-   ral instructions , the extent to which the cur-   rent machine learning methods possess such   capability is still an open question . In this   work , we benchmark models ’ capability of rea-   soning over and sequencing unordered multi-   modal instructions by curating datasets from   online instructional manuals and collecting   comprehensive human annotations . We ﬁnd   current state - of - the - art models not only per-   form signiﬁcantly worse than humans but also   seem incapable of efﬁciently utilizing multi-   modal information . To improve machines ’   performance on multimodal event sequenc-   ing , we propose sequence - aware pretraining   techniques exploiting the sequential alignment   properties of both texts and images , resulting   in > 5 % improvements on perfect match ratio .   1 Introduction   Instructions are essential sources for agents to learn   how to complete complex tasks composed of multi-   ple steps ( e.g. , “ making a wood sign from scratch ” ) .   However , instructions do not always come in a   proper sequential order , for example , when instruc-   tions must be combined across sources ( e.g. , to   accomplish a complex task there might be multi-   pleuseful resources for certain task - steps come   out from a single Google search ) . Therefore , se-   quencing unordered task - steps is crucial for com-   prehending and inferring task procedures , which   requires thorough understanding of event causal   and temporal common sense . It is essential forFigure 1 :   applications such as multi - source instruction sum-   marization and robot task planning ( Garattoni and   Birattari , 2018 ) .   Existing work has studied sequencing unordered   texts from paper abstracts or short stories ( Chen   et al . , 2016 ; Cui et al . , 2018 ) . However , real - life   tasks are often complex , and multimodal informa-   tion is usually provided to supplement textual de-   scriptions to avoid ambiguity or illustrate details   that are hard to narrate , as illustrated in Figure 1 .   To investigate whether current AI techniques can   efﬁciently leverage multimodal information to se-   quence unordered task instructions , we curate two   datasets from online instructional manuals ( Hadley   et al . ; Yagcioglu et al . , 2018 ) . We consider two rep-   resentative instruction domains : cooking recipes   and “ How - To " instructions ( WikiHow ) . We estab-   lish human performance for the sequencing task   on a subset of each data resource . As certain steps   to perform a task can potentially be interchange-   able , we collect annotations of possible orders4525alternative to the originally authored ones to cre-   atemultiple references . Such additional annotation   provides not only better measurement of human   and model performance by alleviating unintended   biases from content creators , but also a useful re-   source for future research of models that are aware   of task - step dependencies and interchangeability .   To measure the ability of state - of - the - art AI tech-   niques to sequence instruction steps , we construct   models consisting of : ( 1 ) an input encoder which   encodes image , text , or multimodal inputs , and ( 2 )   anorder decoder which predicts step order us-   ing the encoded representations . They are jointly   trained with the order supervisions .   Our preliminary studies show that multimodal   information is consistently helpful for the sequenc-   ing task . However , compared to humans , current   models are less efﬁcient in utilizing multimodal   information . We hypothesize that it is because the   models do not effectively capture the sequential   information in the vision modality nor the sequen-   tial alignment between multimodal contents . To ad-   dress this , we propose to equip models with capabil-   ities of performing sequential aware multimodal   grounding . Speciﬁcally , we propose several self-   supervised objectives , including sequence - based   masked language modeling , image region model-   ing , and content swapped prediction , to pretrain the   models before ﬁnetuning them on the downstream   sequencing task .   The proposed pretraining techniques are shown   to be effective in improving multimodal perfor-   mance , enjoying a > 5 % improvement on the per-   fect match ratio metric . However , it is still sig-   niﬁcantly behind human performance ( 15 % in   perfect match ratio metric ) . The same trend is ob-   served when alternative orders are considered .   Our key contributions are two - fold : ( 1 ) We pro-   pose a multimodal sequencing task with two cu-   rated instructional manuals , and comprehensive   human annotations . ( 2 ) We investigate model per-   formance on sequencing unordered manuals , and   propose sequence - aware pretraining techniques to   more effectively use the multimodal information .   Our experiments and extensive analysis provide   insights on which task categories are most chal-   lenging for the state - of - the - art models . They also   shed the light that more sophisticated sequential   multimodal grounding are required to further im-   prove the performance for the proposed multimodal   sequencing task.2 Problem Deﬁnition   Given a task procedure Sconsisting of Nsteps ,   where each step S2Scan consist of two types   of contents : a textual description Tof tokens   fTgand / or image(s ) I = fIg . A   model is required to take as inputs a random per-   mutation of S , i.e. S = fS;:::;Sg , wherep   is a permutation ( Scan take one of the follow-   ing three modalities : T , I , andfT;Ig ) , and   predict the correct order of S , i.e. argsort ( S ) .   3 Datasets and Human Annotation   We are interested in understanding the current state-   of - the - art models ’ performance on this multimodal   instruction sequencing task . To this end , we curate   instruction datasets to support our study .   3.1 Instruction Manual Datasets   There are three major features we require for the   target datasets : ( 1 ) It is multimodal . ( 2 ) It con-   sists of task procedures as sequences of steps . ( 3 )   Different modalities are used intentionally to com-   plement each other . In light of these , we consider   the following two datasets :   RecipeQA . We start from a popular as well as intu-   itive choice of instruction manuals , recipes , which   fully fulﬁll the aforementioned criteria . RecipeQA   is a multimodal question answering dataset consist-   ing of recipes scraped from Instructables.com ( Yag-   cioglu et al . , 2018 ) . We utilize the recipes collected   in RecipeQA and convert each unique recipe into   sequential multimodal steps for our task .   WikiHow . To expand the types of instruction man-   uals for our task beyond recipes , we also consider a   popular “ How To ... " type of instructions , WikiHow ,   which is an online knowledge base that consists of   human - created articles describing procedures to   accomplish a desired task . Each article contains   a high level goal of a task , a short summary of   the task procedures , and several multimodal steps   where each step consists of a description paired   with one or a few corresponding images .   We scrape the entire WikiHow knowledge re-   source , containing more than 100k unique articles   ( mostly ) with multimodal contents , as well as the   hierarchically structured category for each article .   Table 1 presents the essential statistics of the two   datasets ( more details are in Append . Sec . A).4526   3.2 Human Performance Benchmark   To ensure the validity of our proposed multimodal   sequencing task , we establish the human perfor-   mance via Amazon Mechanical Turk . Since our   dataset is constructed from resources that are not   directly designed for the sequencing task , the qual-   ity of random samples is unveriﬁed . Speciﬁcally ,   some articles in WikiHow may not have a notion   of proper order among the steps . As a result , to   construct a high quality test set particularly for Wik-   iHow for establishing human performance , we ﬁrst   identify a set of categories which are more likely   to feature proper order , e.g. Home and Garden   andHobbies and Crafts .A random proportion is   then sampled and the co - authors further downsam-   ple the subset to 300 samples with the aforemen-   tioned criteria via majority vote . For RecipeQA ,   we randomly sample 100 recipes from the dataset .   And hence , the resulting two subsets serve as our   golden - test - set for performance benchmarking .   Human Performance . Prompted with a task goal   and a randomly scrambled sequence of the task-   steps ( can be one of the following modalities : mul - timodal or text / image - only ) , workers are asked to   examine the contents and decide the proper per-   forming order . Human performance are then com-   puted against the original authored orders as the   ground truths , averaged across the whole set .   Alternative Orders . When performing a task ,   some steps can be interchangeable . To take the   interchangeability into consideration in our bench-   mark task , we also collect possible alternative or-   ders to the original ones to create multiple refer-   ences . For each instance in our golden - test - set ,   given the instruction steps sequenced in their orig-   inal order , we ask workers to annotate alternative   orders if the presented task - steps can be performed   following a different order .   Although in this work we are mainly focusing on   sequential instructions and hence the interchange-   ability is also gauged in a sequential manner , we   want to point out that the nature of task - step in-   terchangeability is also highly related to parallel   ( branching ) steps of tasks ( Sakaguchi et al . , 2021 ) .   We argue that the actions that can be performed   interchangeably imply no direct dependencies are   among these actions and thus can potentially be   parallelized , and hence our alternative order formu-   lation can help inferring these parallel actions .   More details of the two human annotation tasks   can be found in Append . Sec . B.   4 Models   To benchmark the proposed task , we construct mod-   els comprising : ( 1 ) an encoder which encodes mul-   timodal or text / image - only inputs , and ( 2 ) an order   decoder which utilizes the encoded representations   to predict the orders . To help models capture se-   quentiality in task - steps better as well as adapt to   our target task domains , we pretrain the encoders   with several self - supervised objectives on the in-   structions before integrating them with the decoder .   4.1 Input Encoders   Text - Only Encoders . We use RoBERTa ( Liu et al . ,   2019 ) for text - only inputs . Although the next-   sentence prediction in BERT ( Devlin et al . , 2019)4527   can potentially be exploited for sequencing , we   empirically ﬁnd that RoBERTa performs better .   Multimodal Encoders . We consider the following   two V&L models mainly due to their easy adapta-   tion to our proposed sequencing task :   VisualBERT ( Li et al . , 2019 ) grounds object de-   tected image regions ( e.g. by Faster - RCNN ( Ren   et al . , 2016 ) ) to language with a single transformer   model ( Vaswani et al . , 2017 ) . VisualBERT is pre-   trained with : ( 1 ) multimodal masked language   modeling ( MLM ) , and ( 2 ) image - text matching   prediction ( ITM ) , where the image in an image-   caption pair is randomly replaced with another one   to create misalignment , and the model is required   to predict whether the current pair is aligned .   CLIP - ViL ( Shen et al . , 2021 ) is also a single-   stream V&L model similar to VisualBERT , while   the visual encoder is replaced by a patch - based   model inspired by the ViT ( Dosovitskiy et al . , 2021 )   in CLIP ( Radford et al . , 2021 ) , where the image fea-   tures are taken as gridded - image - patches as shown   in Figure 2 . The pretraining objectives remain the   same as VisualBERT . Empirically , both Shen et al .   ( 2021 ) and this work ﬁnd such patch - based model   tends to yield better downstream performance .   Image - Only Encoders . We attempt to provide an   image - only baseline on our sequencing task with   two visual encoders : ( 1 ) ResNet -based ( He et al . ,   2016 ) Faster - RCNN model ( also the visual encoder   in VisualBERT ) where both the detected regional   features and the whole - image - feature are used , and   ( 2 ) the aforementioned patch - based CLIP model.4.2 Sequence - Aware Pretraining   The standard multimodal grounding techniques ( Li   et al . , 2019 ; Lu et al . , 2019 ; Su et al . , 2020 ; Chen   et al . , 2020a ) do not explicitly concern the sequen-   tiality of text and associated image sequences , and   hence may fall short of effectively utilizing the   sequential properties in multimodal inputs . To en-   courage models to have better awareness of the se-   quential alignments in multimodal instruction steps ,   we propose to pretrain the encoders with the fol-   lowing self - supervised objectives : ( 1 ) masked lan-   guage modeling ( MLM ) , ( 2 ) ( patch - based ) image-   swapping predictions ( ISP / PISP ) , and ( 3 ) sequen-   tial masked region modeling ( SMRM ) . Figure 2   illustrates an overview of the pretraining paradigm .   For the proposed objectives , the inputs to the   models are generally ordered instruction step se-   quences , which can be further sub - sampled to pro-   duce length - varying subsequences . Although we   do not ﬁnd this necessarily beneﬁt the downstream   performance , it is observed that the sub - sampling   helps the model converge faster . While all of our   proposed objectives can be applied to sequence   with arbitrary length ( 2 ) , without loss of gen-   erality and for simplicity , the following sections   assume the sub - sampled sequence is of length 2 .   4.2.1 Masked Language Modeling   The standard MLM ( Devlin et al . , 2019 ) is em-   ployed by the text - only models to adapt a pre-   trained language model to the target domain ( task   instructions ) . Following prior V&L works , we ap-   ply MLM to multimodal models . Speciﬁcally , we   ensure that the textual description of each step T   gets similar amount of tokens being masked - out   such that the models can potentially exploit the4528image sequences more .   4.2.2 Swapping - Based Prediction   This objective concerns , with certain probability ,   randomly swapping a pair of items in a sequence   and asking the model to judge whether the resulting   sequence is properly ordered or not ( i.e. binary   classiﬁcation ) . We mainly perform the swapping in   the image modality and hence it can be viewed as a   sequence - aware version of ITM objective in most   V&L models . As in ITM , the output representation   at the [ CLS ] token is used to make the prediction .   Standard . For an ordered sequence S , we can   randomly swap twoitems ofS , fS;Sg , where   i < j , tofS;Sg , with a certain probability .   Our preliminary studies ﬁnd that swapping the tex-   tual contents does not necessarily help the down-   stream performance for either text - only or multi-   modal models , so we only perform the swapping on   the imagesfI;Igin both multimodal and image-   only models . For patch - based image inputs ( or   regional features ) , the whole patches of an image   are swapped with those of another one within the   same sequence , as illustrated in Objin Figure 2 .   Patch - Based . We can perform the aforementioned   swapping prediction with a ﬁner granularity , di-   rectly on the image patches . Assuming each im-   ageIis cropped into wpatches ( or wdetected   regions ) , i.e.fig = fi;:::;ig , we ran-   domly select M(ranging from 1 to w ) number   of patches each from the two images I;I(i.e .   fig;fig;p;q2M - sized sampled indices ) to   be swapped with probability . Speciﬁcally , for   each image patch i2I , a randomly selected   image patch i2Iis sampled to be swapped   with . The sampled M - sized indices do not need   to be the same set of integers for each image . The   Objin Figure 2 illustrates the patch - based swap-   ping prediction with w= 4andM= 2 .   4.2.3 Sequential Masked Region Modeling   Prior works extend the masked learning to the vi-   sual modality , where the masked target is either a   predeﬁned discrete visual vocabulary ( Sun et al . ,   2019 ; Bao et al . , 2021 ) or ( soft ) object class la-   bels ( Lu et al . , 2019 ; Su et al . , 2020 ; Chen et al . ,   2020a ) . In this work , we construct a feature - based   target vocabulary dynamically in each training   batch . We ﬁrst randomly select the same amountofX% ( X= 15 ) patches for each image to be   masked out ( replaced with 0 - tensor ) , and then con-   struct a target vocabulary from the original output   representations ( before masking ) of these patches .   Concretely , denote the output representation   of an input image - patch iash(i)and the   masked positions of IasD , we can construct a   candidate list from all the output representations of   the patches at the masked positions of each image ,   i.e. C = fh(i)g[fh(i)g;m;n2D;D. De-   note the masked image patches ( the gray - colored   image patches in Figure 2 ) as mask(i ) , for   each output masked representation h(mask(i ) ) ,   we concatenate it with all the candidates , i.e.   h(mask(i ) ) jjh(i’);8i’2C , which results injCj   concatenated representations for each masked po-   sition . AjCj - way multi - class classiﬁcation can   then be performed by maximizing the probability   ofp(ijh(mask(i ) ) ; C ) . For robust training ,   we additionally : ( 1 ) shufﬂe the candidate set Cfor   each masked position to prevent overﬁtting , and   ( 2 ) ensure the overlapping of masked positions in   each pair of images , D\D , is < 50 % , allowing   the models to utilize information of similar regions   from other images in the sequence .   4.2.4 Overall Training Objective   As the mechanism in some objectives can not guar-   antee mutually exclusive impacts ( e.g. performing   ISP and PISP simultaneously may create confusing   swapped patches ) , we employ a turn - taking fash-   ion , with uniform probability , one of the objectives   ( Obj ) is sampled for each training mini - batch . The   overall pretraining objective is deﬁned as below :   4.3 Order Decoder – BERSON   BERSON is a recently proposed state - of - the - art   neural sentence ordering framework ( Cui et al . ,   2020 ) , where a pointer network ( Vinyals et al . ,   2016 ) exploits both the local ( relative pairwise or-   der ) and global ( self - attentions on top of the entire   input sequence ) information of the inputs to decode   the predicted order . BERSON mainly exploits the   [ CLS ] output representations for relational under-   standing , which aligns well with how our encoders   are pretrained ( Figure 2 ) . We integrate our en-   coders ( with or without sequence - aware pretrain-   ing ) into BERSON , replacing its original BERT en-   coder . The BERSON - module - speciﬁc components   are freshly initialized and then the entire integrated   module is ﬁnetuned on our sequencing task.45295 Experiments and Analysis   Our experiments seek to answer these questions :   ( 1 ) How valid is the proposed task for humans to   complete ? ( 2 ) Is multimodality helpful ? ( 3 ) Can   the proposed sequence - aware pretraining utilize   multimodality more effectively ? ( 4 ) How would re-   sults differ when alternative orders are considered ?   5.1 Evaluation Metrics   We adopt metrics from sentence ordering works :   Position - Based metrics concern the correctness of   the absolute position of each item in a sequence , in-   cluding : ( 1 ) Accuracy ( Acc ) which computes the   ratio of absolute positions in the ground truth or-   der that are correctly predicted ; ( 2 ) Perfect Match   Ratio ( PMR ) which measures the percentage of   predicted orders exactly matching the ground truth   orders ; and ( 3 ) Distance ( Dist . ) which measures   the average distancebetween the predicted and   ground truth positions for each item .   Longest Common Subsequence computes the av-   erage longest subsequences in common ( Gong   et al . , 2016 ) between the predicted and ground   truth orders ( L ) . We also consider a stricter ver-   sion , longest common substring , which requires   the consecutiveness for the comparisons ( L ) .   Kendall ’s Tau (  ) ( Lapata , 2003 ) is deﬁned as   1 2(#inversions ) =( # pairs ) , where the   inversion denotes that the predicted relative or-   der of a pair of items is inverted compared to   the corresponding ground truth relative order , and   # pairs =     forN - length sequence .   Each metric focuses on different perspectives of the   predictions , i.e. position metrics concern the abso-   lute correctness , while common subsequence and   metrics measure if general sequential tendency is   preserved despite incorrect absolute positions .   5.2 Implementation Details   We use the original data splits for RecipeQA . For   WikiHow , to prevent models ’ exploiting knowledge   from similar articles , we split the data so that cer-   tain ( sub)categories do not overlap in each split . We   use only the train splits in each dataset to perform   their respective pretraining . More details of the   data splits are in Append . Sec . A. Preliminary stud-   ies show that joint training with both RecipeQA   and WikiHow data does not necessarily improvethe downstream performance , thus the models eval-   uated in the two datasets are trained simply using   their respective training sets for faster convergence .   We cap the overall sequence length at 5and each   step description with maximally 5sentences for   both models and humans . The maximum input   length per step is 60tokens ( overall maximum   length = 300 ) for training and GPU memory efﬁ-   ciency.= 0:5for both ISP and PISP . All images   are resized to 224224 , and 3232patch is used   for CLIP - based models , resulting in 77 = 49   patches per image . Aside from standard positional   embedding , we only supplement a modality token   type embedding ( text : = 0 , image : = 1 ) to the multi-   modal models . Pretrained weights for each encoder   is obtained either from their corresponding code   bases or by running their codes on our setup .   5.3 Standard Benchmark Results   Table 2 summarizes both the human and model per-   formance for each input modality evaluated using   the original ground truth orders on the golden - test-   set , whereas Table 3 summarizes a more detailed   breakdown of the model performance when incre-   menting combinations of pretraining objectives .   As is shown , multimodal information is veri-   ﬁed consistently helpful for humans . Compared   under same scenario with or without the sequence-   aware pretraining , the two multimodal models   consistently outperform their text - only counter-   parts , where the proposed pretraining technique   is shown particularly effective for the patch - based   multimodal model ( CLIP - ViL ) . However , our top-   performing models still exhibit signiﬁcant gaps be-   low human performance , especially in PMR .   Additionally , we observe a different trend in the   two datasets where the multimodality beneﬁts more   in RecipeQA than WikiHow . The gap between   the multimodal human and model performance is   larger than the text - only counterparts in WikiHow ,   while a reversed trend is shown in RecipeQA . We   hypothesize that recipes may contain more domain-   speciﬁc language usages and/or less words for   the pretrained language models and hence bene-   ﬁts more from the our in - domain sequence - aware   pretraining . Humans , on the other hand , beneﬁt   more from the images in WikiHow as its texts are   hypothesized to contain more ambiguities .   WikiHow Category Analysis . We are interested   in knowing on which categories of WikiHow our4530   models perform closer to humans , and on which the   multimodal information is most efﬁciently utilized .   In Figure 3 we select categories with the top and   least performance gaps ( with PMR metric , top=3 ,   least=2 ) between the human and our best perform-   ing models . We observe that the categories on   which multimodal models outperform the text - only   ones the most are also the categories the models   perform closest to humans , e.g. Home and Garden .   We hypothesize that the images in these categories   are well complementary to the texts and that our   sequence - aware grounding performs effectively . In   contrast , in categories such as Arts and Entertain-   ment andHobbies and Crafts where humans still   enjoy beneﬁts from multimodal information , our   models have difﬁculty utilizing the multimodal in-   formation . We hypothesize that better visual under-   standing may alleviate the potentially suboptimal   grounding as images of these categories can con-   tain many non - common objects .   5.4 Evaluating with Alternative Orders   For each instance where alternative ground truth   orders exist , the performance is computed by the   best each predicted order can obtain against all the   ground truth orders , denoted by multi - reference   performance , and the subset containing these in-   stances is denoted as the multi - reference subset .   Statistics . Table 5 lists the essential statistics of   the multi - reference subsets , including the counts of   the multi - reference instance for each dataset and   modality , as well as the per - instance statistics .   Multi - Reference Performance . The noticeable   main competitors in Table 2 are multimodal and   text - only models , and hence for conciseness , in Ta-   ble 4 we mainly report the multi - reference version4531   of their best performing variants with the selected   metrics . Several trends still hold : ( 1 ) Multimodal   models still outperform the text - only counterparts .   ( 2 ) Human performance is still well above mod-   els ’ even under multi - reference setups . Addition-   ally , both humans and models perform signiﬁcantly   worse in the multi - reference subset when single   ( original ) ground truth is enforced , implying the   validity of our alternative order annotations .   We originally hypothesize that enforcing the   original authored order to be the only ground truth   would be unfair to the text - only models , as im-   ages can often better represent the detailed scene   changes omitted by the texts , while in reality cer-   tain steps may not need to strictly follow the au-   thored order . Judging from the number of instances   that improve after evaluating with alternative or-   ders , the text - only model indeed beneﬁts more from   the multi - reference setup . Examining the general   trends in Table 4 , one can conclude that the textual   contents indeed posses certain levels of ambigu-   ities where images can help to alleviate . How-   ever , as the performance gaps between multimodal   and text - only models are still signiﬁcant under the   multi - reference settings , advantages of multimodal-   ity . Note that humans achieve perfect performance   on the multi - reference subset in RecipeQA , though   unlikely it may seem , it is mainly due to recipes   tend to have rarer possible alternative orders .   WikiHow Categories . Table 6 lists the WikiHow   categories with the most ( top-5 ) annotated multi-   reference ground truths . Note that the categories   with more annotated alternative ground truths are   also among the worse performance from both hu-   mans and models ( Figure 3 ) . We provide sample   qualitative inspections in Append . Sec . C.1 .   6 Related Work   Sequence Ordering . Story sequencing test is a   popular way of examining children ’s abilities on4532sequential reasoning which is shown evident for   procedural understanding ( Tomkins , 1952 ; Baron-   Cohen et al . , 1986 ; Loucks et al . , 2017 ) . In NLP ,   existing works attempt the sequencing task as sort-   ing a series of unordered sentences ( Chen et al . ,   2016 ; Cui et al . , 2018 ; Logeswaran et al . , 2018 ;   Oh et al . , 2019 ; Lee et al . , 2020 ; Calizzano et al . ,   2021 ) from paper abstracts or short paragraphs .   While certain prior work also attempts to extend it   to incorporate multimodality ( Agrawal et al . , 2016 ) ,   the dataset used , Visual StoryTelling ( Huang et al . ,   2016 ) , features album images that were not in-   tended to be procedural nor supply unstated details   to complement the texts . In computer vision , ex-   isting work leverages shufﬂe frame prediction for   learning video representations ( Lee et al . , 2017 ; Xu   et al . , 2019 ; Wang et al . , 2020 ; Li et al . , 2020 ) as   well as cycle consistency constraints for learning   temporal dynamics ( Epstein et al . , 2021 ) . Zellers   et al . ( 2021 ) also features a pairwise relative frame   re - ordering objective to learn temporal common   sense from scripted videos , however , as their down-   stream tasks mainly concern visual reasoning and   ordering by frame - text - matching ( also on Visual   StoryTelling ) , the re - ordering objective is more   focused on the visual modality . Our work takes   a different perspective to tackle a comprehensive   multimodal sequencing task with a focus on the   procedural task - solving knowledge and gauging   the helpfulness of complementary information in   different modalities .   Task / Procedure Understanding . Other works   have utilized WikiHow for learning task knowledge .   In NLP , textual descriptions of WikiHow have   been used for abstractive summarization ( Koupaee   and Wang , 2018 ) , procedural understanding ( Zhou   et al . , 2019 ; Tandon et al . , 2020 ) , and intent esti-   mation ( Zhang et al . , 2020a ) . Prior work ( Zhang   et al . , 2020b ) considers WikiHow for learning event   temporal ordering , but limited to only pairwise re-   lations . A concurrent work uses WikiHow to infer   visual goals ( Yang et al . , 2021 ) . We hope our cura-   tion can help advancing the goal of comprehensive   multimodal procedural understanding .   Another popular form of comprehending given   procedures is through a multiple choice machine   comprehension task . Prior work has utilized text   book ﬁgures ( Kembhavi et al . , 2017 ) as a holistic   " reading reference " for models to select the correct   order of certain ( textually described ) events from   given multiple choices . Another work attemptsthe original visual ordering task of RecipeQA ( Liu   et al . , 2020 ) ( also an multiple choice task ) . How-   ever , we argue that our task tackles a more com-   plex task as the desired orders need to be directly   derived and the event - wise complementary multi-   modal understanding is not an essential component   in these existing works .   Multimodality . Beside models used in this work ,   there are several recent advanced multimodal   grounding techniques ( Tan and Bansal , 2019 ; Li   et al . , 2019 ; Lu et al . , 2019 ; Su et al . , 2020 ; Chen   et al . , 2020b ; Huang et al . , 2020 ; Wen et al . , 2021 ) .   We utilize VisualBERT and CLIP - ViL for their   simplicity to be adapted to our task and easier in-   tegration to our proposed pretraining techniques ,   however , our framework is able to incorporate any   of the aforementioned multimodal models .   7 Conclusions   In this work we present studies of language and   multimodal models on procedure sequencing , lever-   aging popular online instructional manuals . Our   experiments show that both multimodality and our   proposed sequence - aware pretraining are helpful   for multimodal sequencing , however , the results   also highlight signiﬁcant gaps below human perfor-   mance ( 15 % on PMR ) .   We provide insights as well as resources , such   as the multi - reference annotations of the sequenc-   ing task , to spur future relevant research . We also   anticipate that the alternative orders deﬁned and   annotated in our work can beneﬁt more comprehen-   sive task - procedure understanding . Future work   such as predicting task steps which can be parallel   or interchangeable , and understanding step depen-   dencies can be explored .   Acknowledgments   Many thanks to Liunian Harold Li for his origi-   nal CLIP - ViL implementation ; to I - Hung Hsu and   Zi - Yi Dou for their helpful discussion and feed-   back ; and to the anonymous reviewers for their   constructive suggestions . This material is based   on research supported by the Machine Common   Sense ( MCS ) program under Cooperative Agree-   ment N66001 - 19 - 2 - 4032 with the US Defense Ad-   vanced Research Projects Agency ( DARPA ) and a   CISCO research contract . The views and conclu-   sions contained herein are those of the authors and   should not be interpreted as necessarily represent-   ing DARPA , CISCO , or the U.S. Government.4533Ethics and Broader Impacts   We hereby acknowledge that all of the co - authors   of this work are aware of the provided ACM Code   of Ethics and honor the code of conduct . This   work is mainly about sequencing a given series of   multimodal task procedures , represented by text de-   scriptions along with their images . The followings   give the aspects of both our ethical considerations   and our potential impacts to the community .   Dataset . We collect the human performance on   our sequencing task ( both the standard human per-   formance and the alternative order annotations ) via   Amazon Mechanical Turk ( MTurk ) and ensure that   all the personal information of the workers involved   ( e.g. , usernames , emails , urls , demographic infor-   mation , etc . ) is discarded in our dataset . While   the sequence orders either from the original author   intended ones or those annotated by the workers for   the standard performance may possess unintended   biases against certain population group of people   ( e.g. due to cultural differences or educational dif-   ferences , some tasks may be performed differently   from the original intended orders ) , we anticipate   the additional multi - reference annotation can allevi-   ate such an issue as well as provide a broader view   to approach procedural understanding , i.e. certain   task - steps can be interchanged .   This research has been reviewed by the IRB   board and granted the status of an IRB exempt .   The detailed annotation process ( pay per amount of   work , guidelines ) is included in the appendix ; and   overall , we ensure our pay per task is above the the   annotator ’s local minimum wage ( approximately   $ 12 USD / Hour ) . We primarily consider English   speaking regions for our annotations as the task   requires certain level of English proﬁciency .   Techniques . We benchmark the proposed sequenc-   ing task with the state - of - the - art large - scale pre-   trained language and multimodal models with our   novel sequence - aware pretraining techniques . As   commonsense and task procedure understanding   are of our main focus , we do not anticipate pro-   duction of harmful outputs , especially towards vul-   nerable populations , after training models on our   proposed task .   References453445354536A Details of Datasets   A.1 Image Contents   For simplicity and computational concerns , in this   work we only pair one image to each of its asso-   ciated task - step textual descriptions . However , in   both WikiHow and RecipeQA , each task - step can   have more than one associated images or visual   contents represented by short clips or GIFs . We   simply select the ﬁrst image , which is supposed to   be the most representative , for those step featuring   multiple images ; and sample the frame in the mid-   dle of time interval for clips or GIFs . Nevertheless ,   our framework does not assume any limitation on   how many images per step to be processed .   A.2 WikiHow Categories   The category in WikiHow generally forms a hier-   archical directed acyclic graph . Each category can   have its relevant subcategory , which usually spans   ﬁner - granularity of category types . For example , a   possible category traversal path is : Cars and Vehi-   cles→Public Transport →Air Travel , which can   lead to the article How to Overcome the Fear of Fly-   ing . We attach these full category traversal paths   as an additional feature to each of the article in our   dataset , and we also will provide a complete list   of the taxonomy composed by all the categories   and subcategories in WikiHow . We include the   category - data counts in Table 7 for a reference ,   where we only show the top - level category here .   The more in - depth categories can be referred to in   the full released version of the dataset .   A.3 Train - Dev Splits   For RecipeQA we use the original data splits which   ensure no identical recipe appears in more than one   set ( each recipe has its unique recipe - id ) , as this   dataset only has one category and the data quality   is much more uniform than that of WikiHow , i.e.   most recipes fulﬁll our target dataset criteria .   For WikiHow , we split the data according to   the third level category to prevent models from ex-   ploiting too similar task knowledge in the same   category , where the level ( three ) is empirically de-   cided . Speciﬁcally , we ensure that the third - level   categories where the articles in our golden - test - set   belong to , do not appear in the train set . We ﬁrst   split the WikiHow dataset into train , development ,   and test set following this strategy , and then con-   struct our golden - test - set by sub - sampling a subset   of this ( larger ) test set followed by manual inspec-   tions , to ensure its quality . And then , we simply   join the remaining test set samples to the develop-   ment set . Refer to Table 1 in the main paper for   detailed statistics .   B Details of Human Annotation   B.1 Golden - Test - Set Selections   In order to construct a high - quality test set for hu-   mans to evaluate , we manually select the samples   which meet our general criteria : ( 1 ) the tasks are   procedural in both texts and images ( 2 ) the task ’s   images are designed to complement the textual de-   scriptions or provide a more illustrative informa-   tion for some unstated implicit knowledge . We ask   three of our internal members ( co - authors ) to per-   form such manual selection , and preserve ones that   have majority votes . In total , we select 300 samples   for WikiHow and 100 samples for RecipeQA .   B.2 General Annotation Procedure   B.2.1 Standard Performance Benchmark   We collect the human performance via Amazon   Mechanical Turk ( MTurk ) . Each MTurk worker is   required to read the provided instruction carefully ,   as shown in Figure 5a , and then perform the task,4537which is designed to be done in an intuitive drag - n-   drop ( illustrated in Figure 5b ) fashion .   Each MTurk HIT is designed to have ﬁve sets   of sequencing tasks followed by a few additional   questions such as conﬁdence level of the worker   when inferring the order , and whether different   modalities are helpful in a particular task . For each   unique sample in the selected golden - test - set , we   construct three annotation sets each for one modal-   ity version : multimodal , text - only , and image - only .   We launch the HITs containing the same sample   but with different modalities with a week gap to   prevent potential memorization if the same worker   happens to annotate the exactly identical data sam-   ple . We estimate the time required to complete   each of our HITs to be 10 - 15 minutes , and adjust   our pay rate accordingly to $ 2 or $ 3 USD depend-   ing on the length of the task . This roughly equates   to a $ 12 to $ 15 USD per hour wage , which is above   the local minimum wage for the workers . In total   we receive annotated HITs from around 80 workers   for WikiHow , and 14 workers for RecipeQA .   In order to ensure annotation quality and ﬁlter   potential MTurk spammers , we design a few sets to   be our qualiﬁcation rounds for later on worker pool   selection . The Pearson correlation between the   performance of the qualiﬁcation samples and the   overall HIT performance is 0.6 with p - value < 0.05 .   Since it is positive correlated and signiﬁcant , we   censor assignments with substantially low overall   performance ( < 20 % on accuracy metric ) , and re-   launch the HITs containing those samples for a few   more rounds for higher quality annotations .   Finally , since the agreement is sufﬁciently high   ( see Section 3.2 ) , we simply compute the human   performance using all of the collected annotated   orders from all the participated workers , which   result in reasonably high human performance upper   bound for our proposed sequencing task .   B.2.2 Annotating Alternative Orders   We deliberately ask a different set of MTurk work-   ers than those participated in the standard per-   formance benchmark round for annotating the al-   ternative orders . In total we receive HITs from   around 70 workers for WikiHow , and 40 workers   for RecipeQA . The monetary rewards and other   general settings follow the same procedure as in   the standard performance collection . We compute   pairwise IAAs for each worker against every other   workers , using the method described in Append .   Sec . B.3 , and then we place a threshold to ﬁlter outworkers that tend to have too low IAAs ( which is a   likely indicator that a worker is either a spammer or   not understanding our task well ) . As the ﬁnal IAAs   among the selected pool of workers are sufﬁciently   high ( see Section 3.2 ) , for each instance we per-   form a majority vote on the annotated alternative   orders to serve as the ﬁnal multi - references .   B.3 Inter - Annotator Agreements ( IAA )   B.3.1 Standard Performance   As orders concern not only positioning of the   items but also more complicated relative informa-   tion among the items in a sequence , we propose   to measure the agreements among orders center-   ing around the concept of pairwise relationship .   Speciﬁcally , we transform an integer sequence or-   der to an one - hot encoded representation of the    pairs of relative relations . Consider an ex-   ample : suppose three items ( 1,2,3 ) are to be   ordered , and all the pairwise relations are { 12,13 ,   21,23,31,32 } . The transformed one - hot rep-   resentation is deﬁned as : R= { 12 : 1,13 : 1 ,   21 : 0,23 : 1,31 : 0,32 : 0 } = { 110100 } , i.e. ,   R(ij ) = 1 iffijis a valid relatively ordered pair .   Similarly , R= { 001110 } .   Using the aforementioned deﬁnition of R , we   can compute Cohen ’s Kappa inter - annotator agree-   ment score for a pair of annotated order per each   instance . The overall scores can be computed by   ﬁrstly taking the average of pairwise Kappa scores   of annotations for each instance , and then taking   the average across the entire dataset .   B.3.2 Alternative Orders   To evaluate the agreements for the alternative or-   ders , we focus on the differences between an order   and the ground truth in their transformed represen-   tations . We ﬁrst compute the one - hot difference be-   tween an alternative order to the ground truth order ,   e.g. suppose ground truth order is simply o=123 ,   and an alternative order is o=132 , thenR=   absj{110100 } - { 110001 } j= { 000101 } . To focus   on the agreements of the differences to the original   ground truth , we apply the Kappa score on a pair of   orders by retaining the union of the positions where   each order differ from the ground truth in their one-   hot representations . For example , if o=213 , then   R = absj{110100 } - { 011100 } j= { 101000 } ,   and hence the differences to the ground truth are   at positions 4;6fromoand1;3fromo , i.e. the   union isf1;3;4;6 g. Computing the Kappa scores4538Algorithm 1 Alternative Order IAA Per Instance   Require : fAg : A list of annotation series ,   whereA = fagdenotesKorders   annotated by nth worker for an instance .   Require : f(x;y ): IAA scoring function . InitializeS : empty score listfori= 1 to N do forj = i+ 1to N do One - hot encodefag , andfag AssumeK < K// otherwise swap whilefagnot empty do Find best match according to R ^m;^n= arg maxf(R;R)fag.pop ( ^m);fag.pop ( ^n)S = S[score end while whilefagnot empty doS = S[f(o;o);fag.pop(m ) end while end forend forreturn mean(S )   onRandRat these positions leads to   computing the scores on lists { 0011 } and { 0110 } .   To compute the agreements of two series of al-   ternative orders from two annotators ( the series can   have different lengths ) , we ﬁrst iteratively ﬁnd all   the best matching pair of orders from the two series   ( each order in a series can only be matched once ) .   When one series contain more orders than the other ,   the remaining unmatched orders will be compared   to the ground truth to serve as the penalty . For   a particular instance , we take the mean of all the   Kappa scores ( the best - matching - pair and penalty   scores ) as the IAA for the two annotators , as de-   tailed in Algorithm 1 . The overall IAA is computed   similarly to the standard case .   B.4 Additional Statistics   Apart from the main sequencing task , we also ask   the annotators for their conﬁdence of predictions   and if multimodality is helpful for deciding the or-   der in the standard benchmark round . We hereby   provide two more statistics obtained from the work-   ers : the percentages of conﬁdence levels and which   modality ( modalities ) helps for deciding the order .   Modality Helps . As which modality is potentially   more helpful , we include the percentages of each   answer category in Table 8 . It can be noticed that   majority of workers ( > 60 % ) think that multimodal   ( both modalities ) is helpful , and especially in the   recipe data , there are > 90 % of workers indicating   the effectiveness of utilizing multimodal inputs .   Conﬁdence Levels . As shown in Table 9 , majority   of workers feel at least fairly conﬁdent ( score of   4 ) about their predictions , which can justify the   validity of our selection of golden - test - set .   C Additional Results   C.1 Qualitative Inspections   Figure 4 shows a few qualitative examples in dif-   ferent categories . Figure 4a shows that while step   1 and 3 may seem confusing if only looking at   the texts , the images can help deciding the proper   order , whereas models may fail to grasp such mul-   timodal information in Figure 4b . In Figure 4c we   show an example where multi - reference beneﬁts   both humans and the models , although in reality   it should be more commonsensical to stirbefore   refrigerating the mixtures .   C.2 Image - Only Multi - References   We also provide the detailed multi - reference per-   formance break down on the image - only modality   using the best performing models in Table 2 , CLIP ,   in Table 10 for references .   D More Model Details   Multimodal Model Considerations . Bugliarello   et al . ( 2020 ) suggests that many V&L models can4539   achieve similar downstream performance if well   trained , and thus we consider the models presented   in this work , VisualBERT and CLIP - ViL , due to   their simplicity of adapting to our sequencing task ,   as well as their main differences being how the   visual inputs are encoded ( via standard object de-   tector networks or patch - based models like CLIP ) ,   which suits our proposed objectives well .   Swapping - Based Predictions . In Section 4.2.2   we mention that we do not observe necessary im-   provements when swapping the textual contents .   Our hypothesis is that the pairwise loss function   applied in the BERSON module already takes care   of this especially for the textual contents . And   that the stronger discourse - level hints inherent inthe textual descriptions may make this operation   unnecessary . On the other hand , both image and   multimodal alignment does not share this similar   property with the texts , and hence this reasons why   swapping the visual modality sufﬁces this particu-   larly pretraining objective .   D.1 Training & Implementation Details   Training Details . All the models in this work   are trained on a single Nvidia A100 GPUon   a Ubuntu 20.04.2 operating system . The hyperpa-   rameters for each model are manually tuned against   different datasets , and the checkpoints used for test-   ing are selected by the best performing ones on4540   the held - out development set , which is constructed   using the method described in Append . Sec . A.3 .   Implementation Details . The implementations of   the transformer - based models are extended from   the HuggingFacecode base ( Wolf et al . , 2020 ) ,   and our entire code - base is implemented in Py-   Torch . The computer vision detector model used   in one of our image - only encoders , ResNet - based   Faster - RCNN ( Ren et al . , 2016 ) , adopts the detec-   tron2 open sourced module , and their pretrained   weights are obtained from the ofﬁcial implemen-   tation from Facebook AI Research . Implemen-   tation of BERSON modules are adapted from the   original author ’s implementation , where more de-   tails can be found in their paper . Implementation   of the VisualBERT is obtained from the MMF   framework from Facebook AI Research , and CLIP-   ViL model is obtained and adapted from the origi-   nal author ’s released code repository . We use this   same repository for the image - only encoder CLIP .   D.2 Hyperparameters   For the sequencing task , we train all the models   for 5 or 10 ( for multimodal models ) epochs for all   the model variants , where the training time varies   from 2 - 4 hours for the text - only models and 6 - 8   hours for the multimodal models . We list all the   hyperparameters used in Table 11 . We also includethe search bounds and number of trials in Table   12 , that all of our models adopt the same search   bounds and ranges of trials .   D.3 WikiHow Images   Although the images in WikiHow can often be syn-   thetic or " cartoon - ish " , we observe that modern   object detectors can still propose meaningful re-   gions , regardless of whether the object class predic-   tion is sensible or not . We include some predicted   bounding boxes in Figure 6 for references . And   hence , although there may be concerns on subop-   timal visual understanding from these images , we   do believe both of our ResNet and CLIP visual   encoders can extract reasonably useful features .   E Releases & Codes   The scraped WikiHow dataset will be released upon   acceptance , along with a clearly stated documenta-   tion for usages . We will also release the code for   processing the RecipeQA dataset particularly for   our procedure sequencing task , where the original   dataset can be obtained from their project web-   site . If permitted by the authors of the BERSON   model , we will also release the cleaned code repos-   itory which encompasses the majority of the im-   plementations in this work upon acceptance . We   hope that by sharing the datasets and their essential   tools , more interest could be drawn into research on   multimodal procedure understanding and its future   research directions.45414542