  Tiezheng Yu , Hanchao Yu , Davis Liang , Yuning Mao , Shaoliang Nie ,   Po - Yao Huang , Madian Khabsa , Pascale Fung , Yi - Chia WangHong Kong University of Science and TechnologyMeta AI   tyuah@connect.ust.hk , yichiaw@meta.com   Abstract   Short - form video hashtag recommendation   ( SVHR ) aims to recommend hashtags to con-   tent creators from videos and corresponding   descriptions . Most prior studies regard SVHR   as a classification or ranking problem and se-   lect hashtags from a set of limited candidates .   However , in reality , users can create new hash-   tags , and trending hashtags change rapidly over   time on social media . Both of these properties   can not be easily modeled with classification   approaches . To bridge this gap , we formulate   SVHR as a generation task that better repre-   sents how hashtags are created naturally . Ad-   ditionally , we propose the Guided Generative   Model ( GGM ) where we augment the input   features by retrieving relevant hashtags from a   large - scale hashtag pool as extra guidance sig-   nals . Experimental results on two short - form   video datasets show that our generative mod-   els outperform strong classification baselines ,   and the guidance signals further boost the per-   formance by 8.11 and 2.17 absolute ROUGE-   1 scores on average , respectively . We also   perform extensive analyses including human   evaluation , demonstrating that our generative   model can create meaningful and relevant novel   hashtags while achieving state - of - the - art per-   formance on known hashtags .   1 Introduction   Short - form videos on social media are increasingly   popular thanks to the proliferation of multimedia   technologies and portable devices ( Vandersmissen   et al . , 2014 ; Montag et al . , 2021 ) . To highlight the   topics and salient information of the videos , hash-   tags – words or unspaced phrases prefixed with   a “ # ” – have been widely used . Proper use of   hashtags can also increase the probability of the   videos being discovered ( Cao et al . , 2020 ) . In light   of this , short - form video hashtag recommendationFigure 1 : Video frames and the video description are   the inputs . The Guided Generative Model ( GGM ) gen-   erates hashtags related to video frames ( e.g. , # winter   and # cold - weather ) as well as video description ( e.g. ,   # snowstorm and # toronto ) . The generated novel hashtag   that never appears in the training set is highlighted . We   use mock - up video frames in the paper .   ( SVHR ) , which aims to suggest relevant and mean-   ingful hashtags to content creators when they share   videos , has received considerable attention from   industry and academia ( Li et al . , 2019 ; Jain and   Jindal , 2020 ; Mehta et al . , 2021 ) . However , most   previous studies on SVHR consider it as a classifi-   cation problem and rank the hashtags in a small and   fixed - size set one by one ( Li et al . , 2019 ; Wei et al . ,   2019 ; Cao et al . , 2020 ; Yang et al . , 2020 ) . These   methods are time - consuming and far from the ac-   tual application , where users are free to create new   hashtags , and trending hashtags change rapidly on   social media platforms .   To fill this research gap , we formulate SVHR as   a generation task that better represents the process   through which hashtags are created by content cre-   ators . Figure 1 shows an example of the generation   results that include a novel hashtag ( # cold - weather ) .   The generative model learns to generate hashtags   related to video frames as well as video description.9482Additionally , we propose to retrieve hashtags from   a large hashtag pool to augment input features and   use the retrieved hashtags to guide hashtag gen-   eration . Inspired by the effectiveness of vision-   language models ( VLMs ) ( Radford et al . , 2021 ; Jia   et al . , 2021 ; Fürst et al . , 2022 ) in video - text retrieval   tasks , we construct our hashtag retriever based on   VLM . Then , we build a multimodal hashtag gener-   ator to generate hashtags from the retrieved hash-   tags , video frames , and user - written video descrip-   tions . To leverage multimodal inputs , we introduce   a cross - modal attention mechanism ( CAM ) to fuse   information from different modalities . We name   the whole architecture Guided Generative Model   ( GGM ) .   We conduct experiments to evaluate strong clas-   sification baselines , generative models and the pro-   posed GGM on two well - known short - form video   datasets : SFVD1 and SFVD2 . For the classifi-   cation models , we regard SVHR as a multi - label   classification problem and compute probabilities   over all hashtags that appear in training set by a   softmax activation so the models can capture the   interaction between the labels of each video . Ex-   perimental results demonstrate that the generative   models outperform the classification models , and   the guidance signals further boost the performance   by 8.11 and 2.17 ROUGE-1 scores on average . We   further create an unseen test set ( Simig et al . , 2022 )   for SFVD1 and analyze the models ’ performance   on it . Results show that generative models are able   to generate unseen hashtags that the classification   models can never predict . In addition , we assess the   generated hashtags with human evaluation since the   automatic metrics might underestimate our models ’   ability to create novel hashtags . The results from   our human evaluations show that GGM is able to   create meaningful novel hashtags that are statisti-   cally comparable to the ground truth hashtags .   Our contributions are summarized as follows :   •We are the first to formulate SVHR as a gener-   ation task that better represents how hashtags   are created naturally . We propose the Guided   Generative Model ( GGM ) , which leverages   the retrieved hashtag to augment the input for   hashtag generation .   •We present an extensive analysis of experi-   mental results , including human evaluation ,   and demonstrate that GGM achieves state-   of - the - art performance on two large - scale   datasets ( SFVD1 and SFVD2).•Our work benchmarks classification and gen-   erative models on SVHR datasets and high-   lights the advantage of generative approaches ,   which we hope will catalyze research in this   area .   2 Related Work   Short - form Video Hashtag Recommendation .   Li et al . ( 2019 ) introduced the SVHR task and used   graph convolutional network to deal with long - tail   hashtags . Several works leveraged user information   in SVHR ( Wei et al . , 2019 ; Liu et al . , 2020 ) . Yang   et al . ( 2020 ) proposed incorporating sentiment fea-   tures when recommending hashtags . Cao et al .   ( 2020 ) focused on modeling the multimodal infor-   mation of short - form videos . However , most of   the previous works consider the SVHR as a binary   classification problem and select hashtags from   limited candidates by computing the recommended   scores one by one ( e.g. , 101 candidates ( Yang et al . ,   2020 ; Cao et al . , 2020 ) and 1001 candidates ( Wei   et al . , 2019 ) ) . Generally , these approaches are time-   consuming and not practical for real - world applica-   tions . Therefore , we formulate SVHR as a genera-   tion task which better represents how hashtags are   generated naturally by users .   Keyphrase and Microblog Hashtag Genera-   tion . Keyphrase generation ( KPG ) aims to gen-   erate phrases that highlight salient information   for a piece of text . According to ( Meng et al . ,   2021 ) , existing KPG methods can be divided into   One2One ( Meng et al . , 2017 ) which generates   one keyphrase at a time and One2Seq ( Yuan et al . ,   2020 ) which generates a sequence of keyphrases   at once . In this work , we apply One2Seq frame-   work and randomly shuffle the target hashtags in   each batch to mitigate the effect of order when   fine - tuning the model . Different from KPG , SVHR   requires the model to process multimodal inputs .   Recently , Wang et al . ( 2019b ) introduced the mi-   croblog hashtag generation task , which can be   viewed as a variation of the KPG for the social   media domain . Additionally , topic - aware mod-   els ( Wang et al . , 2019a ) and news articles ( Zheng   et al . , 2021 ) are leveraged to improve hashtag gen-   eration . To our knowledge , we are the first to gen-   erate hashtags for short - form videos .   Retrieval - Augmented Generation . Retrieval-   Augmented Generation ( RAG ) has been widely   used in NLP tasks , such as neural machine transla-9483   tion ( Gu et al . , 2018 ; Hossain et al . , 2020 ) , open-   domain question answering ( Lee et al . , 2019 ; Guu   et al . , 2020 ; Lewis et al . , 2020b ) and knowledge-   grounded dialogue generation ( Lian et al . , 2019 ) .   Recently , some works also utilized this framework   in multimodal tasks . Zhang et al . ( 2021 ) proposed   a Retrieve - Copy - Generate ( RCG ) model for open-   book video captioning . To tackle the Outside-   knowledge visual question answering task , ( Gao   et al . , 2022 ) transformed the image into plain text ,   performed knowledge passage retrieval , and gener-   ated answers entirely in the natural language space .   This work extends the RAG framework to multi-   modal hashtag generation . Both hashtag retriever   and generator can accept any advanced models as   drop - in replacements .   3 Methodology   In this section , we first introduce our problem def-   inition of the SVHR task . Then , we present our   Guided Generative Model ( GGM ) which generates   the hashtag from multimodal inputs .   3.1 Problem Definition   The main objective of SVHR is to generate recom-   mended hashtags given a short - form video and its   user - written textual description . To enrich the in-   put signals , we construct a large - scale hashtag poolas the knowledge base . Note that recommended   hashtags should not be limited to the hashtag pool   since meaningful novel hashtags are also consid-   ered valuable . Formally , the visual information of   the video is formulated as frames F , andAdenotes   the acoustic information of the video . The tex-   tual description is defined as a sequence of words   D= ( d , ... , d).Kis the hashtag knowledge   base , and all hashtags in the training set are in-   cluded by default . We do not include hashtags   from external sources since the hashtag styles can   vary widely across datasets , as shown in Figure 4 .   Finally , the models need to recommend the optimal   hashtags Yby finding :   arg max Prob(Y|F , A , D , K ; θ ) ( 1 )   where θis the set of trainable parameters .   3.2 Guided Generative Model   Figure 2 depicts the architecture overview of GGM ,   which consists of a VLM - based hashtag retriever ,   a video encoder , a text encoder and a text decoder .   VLM - based Hashtag Retriever The goal of the   hashtag retriever is to find the top- kmost rele-   vant hashtags from the hashtag knowledge base   Kgiven a video . Inspired by the recent improve-   ments in video - to - text retrieval ( Portillo - Quintero9484   et al . , 2021 ; Luo et al . , 2021 ; Fang et al . , 2021 ) ,   we built our hashtag retriever based on vision-   language models . The hashtag retriever applies   a Bi - encoders framework ( Figure 3 ) . The text en-   coder maps all hashtags in the pool to a list of   hashtag representations T= ( t , ... , t ) . The vi-   sion encoder calculates the frames ’ embedding   ( w , ... , w)of each video , where each frame ’s   embedding wcomes from the representation   ( [ CLS ] ) token of the vision encoder outputs . Sim-   ilar to ( Luo et al . , 2021 ) , the video representation   is calculated by adopting a mean pooling mecha-   nism to aggregate the embeddings of all frames ,   ˆw = mean - pooling ( w , ... , w ) . Then , the   similarity score sim between the video representa-   tionˆwand each hashtag representation tis com-   puted by :   sim = tˆw   ∥t∥ ∥ˆw∥(2 )   The hashtags with top- ksimilarity scores are   selected as the guidance signal for our generative   model ’s input .   To train the hashtag retriever , we adopt the pre-   trained VLM ( Radford et al . , 2021 ) to initialize   the model and follow the same contrastive learning   loss . Each training sample consists of video frames   and one hashtag corresponding to the video .   Audio - Grounded Video Encoder The audio-   grounded video encoder creates the video repre-   sentation given the audio and frames from the   video . We employ a Transformer - based audio en-   coder ( Wu et al . , 2022 ) to encode the sound of the   video . The acoustic information is mapped to an   audio feature a , and the audio feature will be used   directly as the audio input to the audio - vision fu-   sion mechanism . For the video frames , we use aNlayers Vision Transformer ( ViT ) ( Dosovitskiy   et al . , 2020 ) to encode them . Each frame is divided   into patches and encoded as a frame embedding   w∈R , where nis the number of patches .   All video frame embeddings are concatenated to   build the visual representation w∈R.   After that , a cross - modal attention mechanism   ( CAM ) is applied to get the audio - grounded video   representation v(Eq . 3 ) . CAM is based on the   multi - head attention module in Transformer archi-   tecture ( Vaswani et al . , 2017 ) . The query is linearly   projected from the audio feature , and the key and   value are linearly projected from the visual feature .   In addition , we conduct residual connection ( He   et al . , 2016 ) between the visual representation and   the video representation .   v = CAM ( wW , aW , aW ) + w ( 3 )   Video - Grounded Text Encoder The video-   grounded text encoder takes the video represen-   tation and text ( i.e. , user - written description and   guidance signal ) as the input to produce the video-   grounded text representation . We employ a N   layers Transformer text encoder to get text rep-   resentations . Each layer consists of a bi - directional   self - attention sub - layer and a fully connected feed-   forward network . In order to input both the descrip-   tion and the guidance signal to the text encoder ,   the input text is formatted into “ description   [ sep ] guidance signal ” , and the output   is a text embedding t∈R , where kdenotes   the number of input tokens . Similar to the audio-   grounded video encoder , we use CAM to fuse the   video and text representations ( Eq . 4 ) . Finally , the   original text representation tis residual connected   to the video - grounded text representation t.   t = CAM ( tW , vW , vW ) + t ( 4 )   Video - Grounded Text Decoder We construct   anN - layers video - grounded text decoder follow-   ing the standard Transformer text decoder . For   each layer in the decoder , the bi - directional self-   attention is replaced with causal self - attention .   Meanwhile , an additional cross - attention sub - layer   is inserted to perform multi - head attention over the   video - grounded text representation . The decoder   generates hashtags token by token . A beginning-   of - sequence ( BOS ) token is used to indicate the   start of decoding , and an end - of - sequence ( EOS )   token is used to signal its end . In addition , we use a   separator token to separate the generated hashtags.9485   4 Experimental Settings   4.1 Datasets   We evaluate our models on two well - known   large - scale short - form video datasets , SFVD1 and   SFVD2 . After filtering out the videos that only   have hashtags occurring lower than five times , we   obtain 95,265 short - form videos for SFVD1 and   312,778 for SFVD2 . The statistics of both datasets   are shown in Table 1 . Note that all videos in   SFVD1 are shorter than seven seconds and videos   in SFVD2 are shorter than 90 seconds . For SFVD2 ,   we regard the user tags as the hashtags in our exper-   iments . We randomly split the datasets into three   disjoint subsets with 80 % , 10 % and 10 % of the data   for training , validation and test sets , respectively .   In order to simulate the real - world scenario   where new and/or trending hashtags emerge , we   construct an unseen test set for SFVD1 with two   steps similar to ( Simig et al . , 2022 ): ( 1 ) Choose   500 hashtags that appear in the training split , and   ( 2 ) Move all samples in the original training and   test set that contain any of these 500 hashtags to the   unseen test set . Finally , the SFVD1 dataset covers   69,539 samples for training , 9,826 for validation ,   8,690 for seen testing and 10,210 for unseen test-   ing . Note that seen hashtags could also appear in   the unseen test set samples because videos have   multiple labels , and we only ensure that at least   one unseen hashtag exists in each sample of the   unseen test set . The numbers of seen and unseen   hashtags are 10,104 and 570 , respectively .   4.2 Implementation Details   For the video pre - processing , we extract frames   with a frame rate of 2fps if the video duration is   less than seven seconds and uniformly sample 15   frames if the video is longer than seven seconds .   ffmpeg is used to extract the audio as a WA V   format file from the video . The retrieval pool of   hashtags contains all hashtags in the training set . We further evaluate the unseen hashtags to test   the retriever ’s generalization ability as shown in   Table 4 . For the GGM , we initialize the video   encoder with the ViT - base model and construct   the text encoder - decoder based on the BART - base   model . During decoding , we use beam search with   a beam size of 5 . The decoding process will not   stop until the end - of - sequence token is emitted or   the length of the generated hashtags reaches to   maximum length . We set the maximum length as   32 for SFVD1 and 64 for SFVD2 . See Appendix A   for more implementation details .   4.3 Baselines   The following baselines are implemented for com-   parison : 1 ) BERT ( Devlin et al . , 2019 ) takes video   description as input for classification . 2 ) VLM is   the same as our VLM - based Hashtag Retriever . 3 )   ViT(Dosovitskiy et al . , 2020 ) takes video as input   for classification . 4 ) ViT - BERT concatenates the   video and description embedding for classification .   5)BART ( Lewis et al . , 2020a ) generates hashtags   from the video description . 6 ) Trocr - fid ( Li et al . ,   2021 ) generates hashtags based on the video . we   apply the fusion - in - decoder ( Izacard and Grave ,   2021 ) strategy to let the model accept multi - frame   input . 7 ) VG - BART ( Yu et al . , 2021 ) takes video   and description to generate hashtags . We replace   the visual feature extractor from 3D ResNet ( Hara   et al . , 2017 ) to ViT for a fair comparison with GGM .   Appendix A describes the details of the baseline   implementation .   Similar to ( Gong and Zhang , 2016 ; Mahajan   et al . , 2018 ) , we build multi - label classifiers by min-   imizing the cross - entropy between the predicted   softmax distribution and the target distribution . The   reason we do not include previous binary classifica-   tion approaches on SVHR ( Yang et al . , 2020 ; Cao   et al . , 2020 ; Wei et al . , 2019 ) is three - fold : ( 1 ) They   select the hashtags one by one from a pre - defined   relatively small number of candidates ( e.g. , 101 ) .   In contrast , we only give the hashtags in training   set as prior information . ( 2 ) For each test sam-   ple , the models need to compute a score for each   video - hashtag pair across all hashtag candidates ,   which is time - consuming and far from the actual   application . ( 3 ) Multi - label classifiers can encode   interrelation among one video ’s hashtags , which   binary classification approaches cannot.9486   4.4 Evaluation Metrics   We adopt ROUGE metrics ( Lin , 2004 ) that were   initially used for summarization evaluation since   we consider SVHR as a generation task . ROUGE-   1 and ROUGE-2 F1 are used to measure the n-   gram overlaps . We include ROUGE-2 because   some hashtags are combinations of words . We   do not employ ROUGE - L because the order of   hashtags should not affect the evaluation . The F1   score is used to calculate the exact match of the   hashtags between the labels and predictions . In   addition , we include the BERTScore ( Zhang et al . ,   2019 ) to compute the semantic similarity of the   hashtags . More details of the evaluation metrics   are in Appendix B.1 .   5 Results and Analysis   5.1 Main Results   Effectiveness of Generative Models Table 2   shows the performance of the classification and   generation models on the SVHR task . When we   only use descriptions as input , BART achieves   higher scores than BERT across all metrics . This   could be because the BART decoder can better cap-   ture the textual information from the input descrip-   tion compared to the classification layer in BERT .   On the contrary , ViT surpasses Trocr - fid when only   taking videos as input . We speculate that the text   decoder of Trocr - fid may lose some visual informa-   tion through the cross - modal transformation from   vision to text while ViT directly maps the video   to hashtag labels . When taking both video and   description as input , VG - BART achieves better per-   formance than ViT - BERT . Most importantly , GGM   performs significantly better than VG - BART ( the   SOTA multimodal summarization model ) with the   help of the guidance signals . Surprisingly , the au-   dio information does not improve the performance   much , especially on the SFVD2 . It could be be-   cause lots of the audio is just random background   music and more than 5 % of the videos in SFVD2   do not have audio . Additionally , to verify that the   reason of the low performance of the classification   models is not because we choose top five hashtags ,   we evaluate ViT - BERT with different numbers of   hashtags as final results . Table 3 shows that VG-9487   BART outperforms ViT - BERT with all different   settings , indicating the superiority of using genera-   tive models for SVHR .   SFVD1 versus SFVD2 We find that the models   generally perform better on SFVD2 than SFVD1   except for ROUGE-2 scores ( Table 2 ) . There are   two possible explanations . First , as shown in Fig-   ure 4 , most of the hashtags in SFVD2 are made   up of one word ( e.g. , cat , youth , family ) , while   the hashtags in SFVD1 usually contain multiple   words ( e.g. , # furrypride , # TheRealFamily , # dont-   judgeme ) . It ’s easier for models to generate single-   word hashtags and thus perform better on SFVD2 .   And due to the small proportion of multi - word hash-   tags in SFVD2 , ROUGE-2 drops because it calcu-   lates the overlap of multi - word hashtags . Second ,   there are more hashtags ( 20.94 % versus 4.48 % )   that appear in the corresponding video description   of SFVD2 than SFVD1 . Thus , the better model per-   formance on SFVD2 could be partially attributed   to the fact that it is easier for the models to extract   words from the description as hashtags .   VLM Retrieval versus ViT Classification It ’s   surprising that the performance of VLM is signifi-   cantly lower than ViT on both datasets . To explore   this phenomenon more deeply , we test how ViT   and VLM perform discrepantly among varying top-   k. Table 4 illustrates the recall of the models on   the SFVD1 test set with different k. We can see   that the performance gap between VLM and ViT   decreases as kincreases . We speculate it is because   ViT is optimized by cross - entropy loss and there-   fore learns to predict a sharp distribution , while   VLM applies a contrastive loss and outputs a more   flattened distribution . The sharp distribution lets   ViT perform better when kis small , and the flat-   tened distribution makes VLM more stable when   kgrows . We further discuss the effectiveness of   VLM retrieved hashtags in Section 5.2 .   5.2 Effectiveness of Guidance Signal   To explore how different guidance signals can af-   fect generation performance , we first use hashtags   selected by ViT to create the ViT - Guided Gener-   ative Model ( GGM ) as a comparison of the   GGM . As we can see from Table 3 , the perfor-   mance of GGMis lower than the GGM when   using the top-50 hashtags as the guidance , which   contradicts the recall of ViT and VLM , as shown   in Table 4 . We conjecture that VLM - based Hash-   tag Retriever is trained on a contrastive learning   loss which makes it provides more robust features   for the next step generative model . Additionally ,   Table 3 depicts the performance over different num-   bers of hashtags in the guidance signal . Note that   the result of GGM k=0 can be regarded as an ab-   lation study of our approach without the guidance   signal . We find that GGM guided with top-50 hash-   tags outperforms others . We speculate that fewer   hashtags reduce the information in the guidance sig-   nal , while numerous hashtags will introduce extra   noise to the model .   5.3 Performance on Unseen Test Set   To simulate the new trending hashtags , we con-   struct an unseen test set for SFVD1 and investigate   the models ’ performance on this test set . Since clas-   sification models will never predict unseen hash-   tags , we only evaluate the generative models for   comparable performance . Table 2 and 5 show that   both VG - BART and GGM perform much worse9488   in the unseen test set than in seen test set . When   we add the unseen hashtags in the hashtag retrieval   pool for creating the guidance signal , GGM   achieves better scores in all metrics . As discussed   in Section 4.1 , seen hashtags could also appear   in the unseen test set due to the strong correla-   tions between labels . To explicitly investigate the   model ’s performance on those unseen hashtags , we   calculate the number of unseen hashtags recalled   at least once . Results show that GGM has recalled   13 unseen hashtags at least once . When we in-   clude the unseen hashtags in the hashtag retrieval   pool , GGM recalls 51 unseen hashtags at   least once . There are still more than 90 % of the   unseen hashtags which have never been recalled ,   indicating that generating unseen hashtags is chal-   lenging even with the guidance signal .   5.4 Novel Hashtag Analysis   One advantage of the generative models is that they   can create novel hashtags that never appeared in   the training set . These novel hashtags are valuable   because they increase the diversity of the hashtag   recommendation and could become new trends in   the social media platform . However , our quanti-   tative results focus on word overlap , which might   underestimate the effectiveness of the novel hash-   tags . Thus , we conduct a case study and human   evaluation on the generated novel hashtags . Case Study We present a case study of the hash-   tags generated by GGM , shown in Figure 5 . It is   clear that the model can capture the video and its   description to generate novel and meaningful hash-   tags . For instance , GGM generates # willowtree   based on the video description in the first case and   creates # cold - weather relevant to the video frames   in the second case . Neither of the novel hashtags   was in the training set , but they are still meaningful   and relevant to the video and description .   Human Evaluation Novel hashtags should be   understandable and relevant to the video and de-   scription so that users can easily access the correct   information . Similar to ( Simig et al . , 2022 ) , we   randomly sample 100 novel hashtags created by   GGM and manually evaluate their understandabil-   ity and relevancy . For a fair comparison , we include   and mix 100 ground truth hashtags from the same   videos in the human evaluation . Assessments are   scored on a scale of one to five , with higher scores   being better . Each sample is evaluated by three   people , and we average the three scores as the final   result . As illustrated in Table 6 , both generated   novel hashtags and ground truth hashtags achieve   high understandability scores ( larger than four ) , in-   dicating that hashtags created by our model are   meaningful . Interestingly , our generated hashtags   are significantly more relevant to the corresponding   video and its description with p - value < 0.05 . Fur-   ther analysis shows that ground truth hashtags con-   sist of many generic hashtags such as “ # funnny ” ,   “ # follow ” and “ # remake ” , which are not closely re-   lated to the specific video content . In contrast , the   generated novel hashtags can capture more details   of the video , better representing the salient infor-   mation in the video . Hence , our GGM is able to   generate novel and meaningful hashtags to improve   the diversity of recommended hashtags .   6 Conclusion and Future Work   In this paper , we formulate the short - form video   hashtag recommendation ( SVHR ) as a generation   task , and we propose a Guided Generative Model   ( GGM ) that generates hashtags from multimodal   inputs and guided signals from a VLM - based Hash-   tag Retriever . Our work benchmarks classification   and generative models on SVHR datasets and high-   lights the advantage of using generative models .   Our GGM achieves state - of - the - art performance ,   and the human evaluation results show that GGM is   able to generate meaningful novel hashtags compa-9489rable to ground - truth hashtags . We hope our work   can catalyze research on using generative models   for SVHR .   For future work , since the hashtag recommenda-   tion is a highly concurrent task in the real - world   application , we believe improving computational   efficiency to strike a balance between accuracy   and efficiency is one of the important directions .   Besides , the popular trend of short - form videos   changes rapidly on the internet , so it ’s important   for systems to accurately generate hashtags for new   trending videos .   7 Limitations   Our methods are currently trained and tested on   two SVHR datasets . Gender biases and unethical   hashtags could exist in the datasets , which may   cause the model trained on these datasets to gen-   erate these biases . Besides , although our methods   are not language - specific , we only choose the En-   glish dataset due to its rich resource . Furthermore ,   we regard the user tags as the hashtags for SFVD2   in our experiments and there are small differences   between the user tags and hashtags . Experiments   on more diverse languages and datasets are needed   in the future .   As an initial work for SVHR , in our task formula-   tion , our model only take the video and its descrip-   tion as input to predict hashtags and ignore user   preference in hashtag recommendations . Exploring   user preference is also a promising direction for   future work .   We used up to eight A100 GPUs per experiment   and it took more than one day to run experiments   on SFVD2 . More efficient models are needed for   real - world applications . Besides , we hope our ex-   perimental results can be used as benchmarks for   comparison in future work to avoid repeating train-   ing .   8 Ethics Statement   Although the generative models can create novel   hashtags , which is beneficial for increasing hashtag   diversity on social platforms , generative models   also have the potential to generate toxic or offen-   sive hashtags . Since we finetune the Pre - trained lan-   guage models ( PLMs ) on existing SVHR datasets ,   undesirable hashtags could come from biases that   are encoded in the PLMs ( Blodgett et al . , 2020 )   or the undesirable hashtags in the training set . We   recommend that when generative models are de - ployed in real - world applications , additional post-   processing should be carried out to remove undesir-   able and harmful hashtags . In addition , our hashtag   generator will only recommend hashtags to human   content creators who ultimately have the responsi-   bility to decide which hashtags should be used .   References94909491   A Implementation Details   We initialize the models from Hugging Face Mod-   els . The name of the initial checkpoint and the   number of trainable parameters for each model   is shown in Table 7 . We use learning rates 6e   following ( Lewis et al . , 2020a ) and Adam opti-   mizer ( Kingma and Ba , 2014 ) to fine - tune the   GGM . For all of our experiments , we use a batch   size of 32 . The final results are the average test set   performance on the best three checkpoints in the   validation set .   For classification baselines , we implement the   multi - label classification models from the pre-   trained vision and language models ( e.g. , ViT ,   BERT ) because each short - form video could have   multiple ground - truth hashtags . Firstly , we create a   hashtag set that contains all hashtags in the training   set ( i.e. , 10,674 hashtags for SFVD1 and 43,282   hashtags for SFVD2 ) as the candidates for the clas-   sification outputs . Then , similar to ( Mahajan et al . ,   2018 ) , we compute probabilities over the hashtag   set using a softmax activation , and the models are   trained to minimize the cross - entropy loss between9492   the predicted softmax distribution and the target   distribution of each short - form video . The target   distribution is a vector that only has knon - zero   entries , each set to 1kcorresponding to the ground-   truth hashtags for the video . We also implement the   multi - label classification models with per - hashtag   sigmoid outputs and minimize each hashtag ’s aver-   age binary cross - entropy loss . However , the results   are significantly worse ; actually , we find the mod-   els only predict high - frequency hashtags for every   test set sample .   For generative models , we follow the standard   sequence generation models that generate the hash-   tags token by token . Decoding automatically stops   when the end - of - sequence token is predicted . For   Trocr - fid , we use the pre - trained ViT - base model   to initialize the vision encoder and use the decoder   of the BART - based model to initialize the text de-   coder .   B Implementation Details for Evaluation   B.1 Automatic Evaluation   For ROUGE and BERTScore , we randomly   shuffle the predicted hashtags to remove   the effect of hashtag order . Moreover , we   split the multi - word hashtags into single   words before calculating the ROUGE . We   userougeto compute ROUGE scores and   use microsoft / deberta - xlarge - mnl   model to compute BERTScores as suggested .   wordninjais used for separating words from   the hashtags .   B.2 Human Evaluation   In Table 6 , we conduct a human evaluation of the   understandability and relevancy of the generated   hashtags from the SFVD1 dataset . In detail , we   randomly sample 100 novel hashtags from GGM   and the ground truth hashtags from the same videosfor comparison . Assessments are scored on a scale   of one to five , with higher scores being better . Un-   derstandability means whether the hashtag is un-   derstandable given the context of the video and   the corresponding description . Relevancy means   whether the hashtag is relevant to the video or the   corresponding description . Note that the annota-   tors can search online for more information about   the hashtags if they do n’t know it . We assign each   hashtag to three annotators and take the average   score as the final result . In total , we used six anno-   tators from the US , and all annotators voluntarily   participated in the human evaluation . All annota-   tors agree to have their evaluation results included   as part of this paper ’s results .   Here is an extra note for the annotators when   they do the human evaluation . For evaluating rele-   vancy , sometimes the hashtags can be very generic   and the annotators should give lower scores for   them . For example , given a family comedy show ,   the relevancy score of the hashtags will be “ # fami-   lycomedy ” > “ # comedy ” > “ # room”.9493ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 7 " Limitation "   /squareA2 . Did you discuss any potential risks of your work ?   Both Section 7 " Limitation " and Section 8 " Ethics Statement "   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Yes , we summarize the paper ’s main claims in " Abstract " and Section 1 " Introduction "   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   We create NLP models in Section 3 " Methodology "   /squareB1 . Did you cite the creators of artifacts you used ?   Section 2 and Section 3 and Section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   In Appendix C , we discuss the license of the dataset that we use .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 3 , Section 4 and Section 5   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Appendix C   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 1 , Section 2 and Section3   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   In Section 4.1 " Datasets " we discuss the data   C / squareDid you run computational experiments ?   Sections 4 and 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4 and Appendix A9494 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4 and Appendix B1   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 5   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix B2   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Appendix B2   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Appendix B29495