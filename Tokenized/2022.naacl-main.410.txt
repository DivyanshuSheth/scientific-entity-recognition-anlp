  Michael Wiegand   Digital Age Research Center ( D!ARC )   Alpen - Adria - Universität Klagenfurt   AT-9020 Klagenfurt , Austria   michael.wiegand@aau.atElisabeth Eder   Institut für Germanistik   Alpen - Adria - Universität Klagenfurt   AT-9020 Klagenfurt , Austria   elisabeth.eder@aau.at   Josef Ruppenhofer   Leibniz Institute for the German Language   D-68161 Mannheim , Germany   ruppenhofer@ids-mannheim.de   Abstract   We address the task of distinguishing implicitly   abusive sentences on identity groups ( Muslims   terrorize the world daily ) from other group-   related negative polar sentences ( Muslims de-   spise terrorism ) . Implicitly abusive language   are utterances not conveyed by abusive words   ( e.g. bimbo orscum ) . So far , the detection of   such utterances could not be properly addressed   since existing datasets displaying a high degree   of implicit abuse are fairly biased . Following   the recently proposed strategy to solve implicit   abuse by separately addressing its different sub-   types , we present a new focused and less biased   dataset that consists of the subtype of atomic   negative sentences about identity groups . For   that task , we model components that each ad-   dress one facet of such implicit abuse , i.e. de-   piction as perpetrators , aspectual classification   and non - conformist views . The approach gen-   eralizes across different identity groups and   languages .   1 Introduction   Abusive language is commonly defined as hurtful ,   derogatory or obscene utterances made by one per-   son to another person . Examples are ( 1)-(2 ) .   In the literature , closely related terms include   hate speech ( Waseem and Hovy , 2016 ) or cyber   bullying ( Zhong et al . , 2016 ) . While there may   be nuanced differences in meaning , they are all   compatible with the general definition above .   Due to the rise of user - generated web content ,   the amount of abusive language is growing . NLPmethods are required to focus human review ef-   forts towards the most relevant microposts . Though   there has been much work on abusive language de-   tection in general , there has been little work focus-   ing on implicit forms of abusive language ( 3)-(4 )   ( Waseem et al . , 2017 ) . By implicit we understand   abusive language that is notconveyed by ( unam-   biguously ) abusive words ( e.g. bimbo , scum ) .   Detailed analyses of the output of existing classi-   fiers have also revealed that currently only explicit   abuse can be reliably detected ( van Aken et al . ,   2018 ; Wiegand et al . , 2019 , 2021b ) .   In this paper , we define implicit abuse as those   abusive utterances that lack any abusive word ac-   cording to the largest lexicon of abusive words   available , i.e. the lexicon by Wiegand et al . ( 2018 ) .   In particular , datasets focusing on abuse towards   identity groups ( Jews , gay people etc . ) contain a   high degree of implicit abuse . For example , accord-   ing to Wiegand et al . ( 2021b ) , on the dataset from   Waseem and Hovy ( 2016 ) , 56 % of the abusive in-   stances are implicit , while on the dataset from Sap   et al . ( 2020 ) , as many as 62 % are .   So far , existing research on implicitly abusive   language detection on identity groups has been lim-   ited by various biases on existing datasets ( Arango   et al . , 2019 ; Wiegand et al . , 2019 ) , most promi-   nently the identity - group bias ( Dixon et al . , 2018 ):   mentions of identity groups almost exclusively oc-   cur in microposts that are considered abusive . As   a consequence , most classifiers erroneously learn   identity groups as clues for abusive language .   Given that implicit abuse is a challenging prob-   lem , Wiegand et al . ( 2021b ) argue that the only5600reasonable approach to solve this problem is to   address specific subtypes individually rather than   consider all types of implicit abuse at once .   In this paper , we address the task of distinguish-   ing implicitly abusive remarks on identity groups   ( 5)-(7 ) from other negative polar sentences ( 8)-(10 ) .   The task is a binary classification problem . Each   instance is an individual atomic sentence .   We create a novel less biased dataset for this   task . In NLP , there is an increasing awareness of   the importance of producing such data ( Gardner   et al . , 2020 ) . Moreover , Zhou et al . ( 2021 ) find that   ensuring the quality of datasets during their cre-   ation is considerably more effective than even the   most sophisticated statistical debiasing techniques .   Unlike previous work , we focus on a linguisti-   cally informed classification approach and show   that this approach is equally effective for differ-   ent identity groups and can be used to outperform   supervised classifiers trained on existing datasets .   We consider only negative polar utterances , since   implicitly abusive microposts have a predominantly   negative sentiment . For instance , on a random sam-   ple of 200 implicitly abusive instances from the   dataset by Sap et al . ( 2020 ) , we could not find a   single remark with a positive or neutral sentiment .   Ourcontributions are the following :   •We present the first extensive study on how   to detect implicitly abusive remarks among   negative atomic remarks on identity groups .   •We establish the predictiveness of 3 linguistic   features , namely , aspectual classification , the   detection of perpetrators and non - conformist   views . The latter two features are addressed   for the first time , in general .   • We present a new dataset for this task .   •We introduce new lexical resources for detect-   ing perpetrators and non - conformist views .   This paper only addresses one subset of implicit   abuse . However , we consider this focus appropri-   ate , since it is not trivial to detect these instances .   As a comprehensive classifier that can detect all   these types , we envisage a meta - classifier that col-   lects predictions of individual classifiers designed   for different subtypes of abusive language . All resources created as part of this research are   made publicly available . They are contained in   the supplementary materialto this paper , which   also includes implementation details .   2 Related Work   Much of the previous work in abusive language   detection follows a one - size - fits - all approach ( For-   tuna and Nunes , 2018 ) . Surveys on existing   datasets do not address implicit abuse ( Vidgen and   Derczynski , 2020 ; Poletto et al . , 2021 ) .   Wiegand et al . ( 2021b ) present a roadmap on   implicit abuse arguing that this type of abusive   language has not adequately been addressed in pre-   vious work . No classification experiments are pre-   sented . Next to implicit abuse towards identity   groups , they identify as subtypes dehumanization ,   euphemisms , call for action , multimodal abuse and   comparisons . Comparisons are also addressed by   Wiegand et al . ( 2021a ) who present the first dataset   for this subtype along with classification exper-   iments . The comparisons do not target identity   groups . Therefore , our novel dataset and the com-   parison dataset comprise different sentence types .   Breitfeller et al . ( 2019 ) present a study on mi-   croaggressions which are comments or actions ex-   pressing a prejudiced attitude towards marginalized   groups unconsciously . Such instances are cases of   implicit abuse . Since this is a descriptive study no   data for classification are introduced .   Han and Tsvetkov ( 2020 ) propose a classifica-   tion approach for what they call veiled toxicity ,   an umbrella term for many different subtypes of   implicit abuse . The approach is evaluated on the   dataset by Sap et al . ( 2020 ) which Wiegand et al .   ( 2021b ) report to have considerable biases .   ElSherief et al . ( 2021 ) introduce a general   dataset for implicit abuse which is sampled from   tweets by hate groups . The authors report biases in   the dataset , such as the identity - group bias .   3 Data   As a source for our data , we chose Twitter since it   is a platform that contains a high degree of abusive   language . We focused on 4 identity groups that   cover a range of different characteristics ( religion ,   sexual orientation and gender ) and that can also be   frequently found in existing datasets . Moreover ,   they need to occur with sufficient frequency in both5601languages we are going to examine . The groups   aregay people , Jews , Muslims andwomen .   The abusive utterances we are looking for are es-   sentially stereotypical sentences on identity groups .   Such remarks typically realize the abused target ,   i.e. the identity group , as the agent ( i.e. logical sub-   ject ) of the verb ( 5)-(7 ) . Our new dataset focuses   only on this argument position since stereotypical   remarks usually depict identity groups as the en-   tities performing some action ( agent ) rather than   being affected by it ( patient , i.e. logical object ) .   We obtain such utterances by extracting tweets con-   taining mentions of our identity groups followed   by a negative polar verb . ( This strategy has been   proposed by Wiegand et al . ( 2021b ) in order to en-   sure lexical variability . ) The focus on verbs rather   than on nouns and adjectives was motivated by the   fact that the latter two are more likely to be ex-   plicitly abusive words . For example , these parts of   speech compose 91 % of the lexicon by Wiegand   et al . ( 2018 ) . In this work , we are interested in   implicit abuse , however . To test the recall of our   sampling approach , we inspect two random sam-   ples of 200 abusive ( atomic ) instances from two   popular datasets that focus on identity groups ( Sap   et al . , 2020 ; Waseem and Hovy , 2016 ) . We find   that 80/84 % of the instances realize the identity   group as an agent . 70/70 % of the predicates are   verbs , the remainder being adjectives and nouns .   Of the verbal predicates , 79/92 % were negative   polar verbs .   Vidgen et al . ( 2021b ) recently introduced a   dataset similar to ours : It focuses on identity groups   and also aims at having annotators create suit-   able non - abusive data . Their goal is to reduce the   identity - group bias on their data by a large degree .   We refer to this dataset as DynaB . We examined   the non - abusive instances in DynaB for our 4 iden-   tity groups ( Table 1 ) and found that more than 80 %   of the instances are cases of reported abuse ( Chiril   et al . , 2020 ) , as in ( 11 ) , negations ( 12 ) , or simply   positive or neutral utterances ( 13 ) .   Our dataset , however , consists of atomic sentences , i.e. there is no negation or reported abuse ( 5)-(10 ) .   Further , all sentences convey a negative sentiment .   We believe this to be more challenging since a clas-   sifier needs a proper understanding of the atomic   utterances themselves rather than looking for posi-   tive / neutral sentiment ( 13 ) or context clues indicat-   ing a non - abusive nesting , such as negation words   ( e.g. not(11 ) ) or reporting verbs ( e.g. say(12 ) ) .   We implemented the following measures pro-   posed by Wiegand et al . ( 2021b ) for producing less   biased data for the detection of implicit abuse .   •Our data is sampled from one textual source ,   i.e. Twitter . Both abusive and non - abusive   sentences are sampled by the same pattern   ( i.e. mention of identity group preceding a   negative verb ) . Thus no biases are caused by   merging instances from different text sources .   •In order to avoid any user biases , tweets were   sampled from a wide set of different users .   The average number of tweets per user is 1.1 .   •In order to avoid a focus on frequently oc-   curring verbs , we sampled our dataset from a   wide set of negative polar verbs . On average ,   each verb occurs twice in the final dataset . Un-   like previous datasets , this sampling strategy   thus puts due emphasis on the “ long tail ” of   the verb distribution .   •We only included sentences that do notcon-   tain explicitly abusive words . Otherwise , clas-   sifiers could easily detect the respective abu-   sive utterances since they would just have to   focus on these explicit clues .   •We remove any text co - occurring with our   sentences that might give rise to spurious   correlations , e.g. hashtags or user names .   We observed that particularly hashtags , such   as#banIslam or#feminismIsCancer , often   strongly correlate with abusive tweets . Such   hashtags display a behaviour similar to explic-   itly abusive words .   We created a gold standard for English and an-   other , less - resourced language , German . Exactly   the same sampling procedure was applied to both   datasets . However , due to the sparsity of German   language content on Twitter ( Hong et al . , 2011 ) ,   the German dataset is smaller .   Both datasets were annotated via the crowd-   sourcing platform Prolific .The label of each5602   instance represents the majority vote of 5 differ-   ent crowdworkers , who were native speakers . We   opted for a very high approval rate ( i.e. 95 % or   higher ) in order to guarantee a sufficiently high   annotation quality . ( The supplementary material   contains annotation guidelines . ) Table 1 offers   some descriptive statistics .   On a random sample of 200sentences , we com-   puted the agreement between the majority vote of   our crowdsourced judgments and one co - author   of this paper . We measured substantial agreement   ofκ= 0.87on the English and κ= 0.82on the   German dataset ( Landis and Koch , 1977 ) .   4 Supervised Classifiers and Evaluation   We consider RoBERTa ( Liu et al . , 2019 ) as a base-   line for generic supervised classification for En-   glish data . For our German data , we use the best   transformer according to Chan et al . ( 2020 ) . We   fine - tune the pretrained models on the given task   using the FLAIR framework ( Akbik et al . , 2019 ) .   ( The supplementary notes contain more details on   allclassifiers including hyperparameter settings . )   As evaluation measures , we use macro - average   precision , recall , F1 - score . For all classifiers built   with transformers , we report the average over 5   training runs ( including standard deviation ) . All   other classifiers produce deterministic output .   5 Linguistically Informed Classifier   We propose a linguistically informed classifier   which models 3 component tasks . We describe   how this classifier is built for English . The com-   ponent tasks represent concepts which have been   suggested to be predictive for this task ( Wiegand   et al . , 2021b ) but , so far , could not be tested due   to the lack of data . In order to avoid overfitting ,   each component comes with a separate classifier   being built on training data different to the test data   of our main task . Since we manually labeled ourdataset also for each of the component tasks   we can conduct an intrinsic evaluation of each   component , too . In order to have an unbiased an-   notation , each crowdworker was only allowed to   participate in exactly one of our annotation tasks .   5.1 Component 1 : Aspectual Classifier   The Task . In our first task we address aspectual   classification . Abusive utterances regarding iden-   tity groups are usually stereotypes ( Sap et al . , 2020 ) .   Per definition , stereotypes coincide with habitual   ( or non - episodic ) aspect ( 14)-(15 ) . On the other   hand , episodic aspect ( 16)-(17 ) , i.e. utterances that   express information about a single event ( Friedrich   and Pinkal , 2015 ) , despite the fact that they may   be tendentious ( Mendelsohn et al . , 2021 ) or even   be cases of fake news ( Zhou and Zafarani , 2020 ) ,   is more likely to be non - abusive . We distinguish   between episodic and non - episodic sentences .   The Method . Aspectual classification was inves-   tigated by Friedrich and Pinkal ( 2015 ) and an im-   plementation of their classifier is available as part   ofsitent ( Friedrich et al . , 2016 ) . However , we ob-   served substantial issues with sitent when applied   to our data . The tool was trained on Wikipedia   and MASC ( Ide et al . , 2008 ) . On these datasets ,   episodic aspect is biased towards past tense . How-   ever , our data originates from Twitter and both   episodic and non - episodic sentences co - occur in   present tense .   As a consequence , we decided to build a classi-   fier from scratch . As no suitable labeled training   data for our domain ( i.e. social media ) is available ,   we decided to apply a form of distant supervision   ( Mintz et al . , 2009 ) . As a proxy for episodic sen-   tences , we sampled tweets from news feeds ( e.g.   LGBT_news orGazaTVNews ) from Twitter . Such   tweets typically report on specific events ( 18)-(19 ) .   For the non - episodic sentences , we considered the   implied statements ( 21 ) from the social bias frames5603   corpus ( Sap et al . , 2020 ) . In that dataset , the an-   notators added for each abusive instance ( 20 ) the   stereotype that the remark alludes to ( 21 ) .   For our training set , we randomly sampled 1000   news tweets (= episodic ) and 1000 implied state-   ments (= non - episodic ) . As classifiers , we trained   RoBERTa and a feature - based baseline . The latter   was included since generic supervised classifiers   ( such as RoBERTa ) are susceptible of learning spu-   rious correlations contained in training data . Such   correlations can not be ruled out as our training   data for the two classes was sampled from different   sources . Our feature - based baseline , which is a   logistic regression trained on high - level features   that are fairly domain independent , makes such   overfitting less likely . The features for detecting   episodic sentences check for mentions of concrete   entities or a specific point in time , while features   for non - episodic sentences try to detect states and   generalizations . Table 2 lists the full feature set .   Table 3 shows the result of the different clas-   sifiers on our English dataset ( § 3 ) . sitent per-   forms poorly . We attribute it to the tense bias re-   ported above . The feature - based baseline is strong   but it does not outperform RoBERTa . Therefore ,   RoBERTa does not seem to be seriously affected   by spurious correlations . We use the output of   RoBERTa in all subsequent experiments . In order   to facilitate the combination with other components   of our classifier , we use the majority vote of the 5   runs of this classifier .   5.2 Component 2 : Perpetrator Classifier   The Task . A common stereotype that can be ob-   served with every identity group is the depictionas perpetrators ( 22)-(24 ) . By perpetrators , we un-   derstand persons who commit an illegal , criminal ,   or evil act . Although different identity groups   are typically depicted as different perpetrators ( e.g.   Muslims are depicted as terrorists ( 22 ) , women   are considered to be dishonest ( 23 ) , while gay   men are accused of being pedophiles ( 24 ) ) , all   these stereotypes describe actions that involve crim-   inal offenses ( e.g. raping , stealing ) or morally con-   temptible behaviour ( e.g. adultery , lying ) . We think   it is most economical to frame the detection of   perpetrators as a single task .   We consider the task a form of semantic role   labeling ( Gildea and Jurafsky , 2002 ) , i.e. perpetra-   tors are specific entities evoked by particular verbs .   Therefore , we need to find perpetrator - evoking   verbs ( e.g. terrorize , betray , rape)and the respec-   tive argument position of the perpetrator .   The Method . In order to obtain a labeled dataset   of perpetrator - evoking verbs , we randomly sam-   pled 500 negative polar verbs from the Subjectivity   Lexicon ( Wilson et al . , 2005 ) and asked crowdwork-   ers to form simple sentences ( only a main clause )   in which the given verb evokes an event that in-   cludes some perpetrator . The 500 verbs are in no   way tuned for our test data ( § 3).Since we do not   want crowdworkers to invent any anti - Semitic , ho-   mophobic , Islamophobic or misogynist sentences ,   we invented a fictitious people whose name has no   phonetic resemblance to existing identity groups .   The crowdworkers were asked to depict these peo-   ple as perpetrator , if possible . Obviously , plausible   sentences can only be formed with the subset of   perpetrator - evoking verbs we are looking for . For   other verbs , such as grieve ordread , forming such5604sentences is not possible . Therefore , crowdwork-   ers were asked not to provide a sentence in case   they felt that they were unable to meet the criterion   of constructing a context with a perpetrator being   a participating entity of the event evoked by the   given verb . Only if the majority of 5 crowdworkers   managed to produce such sentences for the same   verb , did we consider it as a perpetrator - evoking   verb . This setting also allowed us to identify the   semantic role of the perpetrator . Overall , 165 out   of 500 verbs were identified as perpetrator - evoking   verbs . In 96 % of the respective sentences , the se-   mantic role of the perpetrator was the agent of the   verb ( as in ( 22)-(24 ) ) .   In a second step , we extended the list of   perpetrator - evoking verbs . Our aim is to obtain   a ( nearly ) exhaustive list of perpetrator - evoking   verbs . Therefore , we train a classifier on our   500 verbs ( each verb labeled as either perpetrator-   evoking orother ) and classify each verb from the   largest list of publicly available negative polar   verbs . We took the verbs from the set of negative   polar words from Wiegand et al . ( 2018 ) ( totaling   1,700 negative verbs ) . We trained a logistic regres-   sion classifier where each verb was represented by   its ( publicly available ) word embedding induced   on Common Crawl ( Mikolov et al . , 2018).We   ended up with 491 perpetrator - evoking verbs . Our   lexicon - based classifier identifies a perpetrator if   it is observed as an agent of one of these 491 verbs .   This classifier is run on our dataset ( § 3 ) . The out-   put is evaluated against the gold annotation for this   component task .   As a baseline , we run a very fine - grained   semantic - role labeling system based on FrameNet   ( Baker et al . , 1998 ) on our data . We chose open   sesame ( Swayamdipta et al . , 2017 ) which is the   most recent publicly available tool for semantic-   role labeling based on FrameNet . Due to its fine-   grained inventory , there are frame elements ( this   is the term for semantic roles in FrameNet ) which   semantically correspond to our concept of perpe-   trators . More precisely , we considered text spans   as perpetrators if they are predicted to be one of   the following frame elements : Abuser , Assailant ,   Counter_actor , Destroyer , Invader , Killer , Manip-   ulator , Offender , Perpetrator andWrongdoer .   Table 4 shows the performance of the different   classifiers to detect mentions of perpetrators in our   English dataset ( § 3 ) . Our lexicon - based classifier   outperforms FrameNet , which is known to have a   limited lexical coverage ( Das and Smith , 2011 ) .   5.3 Component 3 : Non - Conformist Views   The Task . For our third component task , we con-   sider the sentiment of the agent towards the patient   ( as conveyed by the main verb in the sentence ) in   combination with the sentiment expected a priori to-   wards the patient . ( The agent is always the mention   of the identity group . ) This is illustrated in Table 5 .   We observe a systematic relationship between abu-   sive language and fine - grained sentiment : If the   sentiment of the identity group ( i.e. the agent )   towards the patient is opposite to the prior sen-   timent of the patient , then this utterance depicts   the identity group as having a non - conformist   view . Such views are perceived as abusive utter-   ances : If someone attributes non - conformist views   to some identity group , then , one often intends to   stigmatize this group as not belonging to their own   community . This phenomenon is referred to as   othering ( Burnap and Williams , 2016 ) .   The Method . In order to detect the above pat-   tern indicating non - conformist views , we need the   output of two modules : the first determines the   prior sentiment of the patient ( i.e. the phrase repre-   senting the logical object ) ; the second determines   the sentiment of the agent towards the patient . The   prior sentiment of the patient can be easily detected   by running a sentiment text classifier on that phrase .   For this , we use TweetEval ( Barbieri et al . , 2020 ) .   The difficult part is to detect the sentiment of   the agent towards the patient . Sentiment text clas-   sifiers are unable to determine such fine - grained   sentiment information . They capture the general   sentiment of a given text which may be different .   For instance , ( 25 ) conveys a positive sentiment of   Muslims towards violence , while the sentiment of   the sentence is generally considered negative.5605   Instead of a text classifier , we seek a lexicon that   specifies for any negative polar verb ( out of con-   text ) whether it conveys a positive sentiment of the   agent towards the patient ( e.g. glorify , long ( for ) ,   pray ( to ) ) or a negative sentiment towards it ( e.g.   abhor , dislike , suffer ) . The only lexicons with such   information are EffectWordNet ( effectWN ) ( Choi   and Wiebe , 2014 ) and the connotation - frames lexi-   con(frames ) ( Rashkin et al . , 2016 ) . Unfortunately ,   both resources only cover about 40 % of the verbs   in our dataset . We also determined a significant   level of noise in these resources ( as detailed in the   supplementary notes ) . Therefore , we decided to   create a novel lexicon with that information . It   should cover all possible negative verbs . We first   had crowdworkers annotate for some ( seed ) nega-   tive verbs the sentiment of the agent towards the   patient . We chose the 500 verbs we already used   in § 5.2 . The majority of the crowdworkers ’ judge-   ments represent our gold standard annotation . On   these annotated 500 verbs we trained a logistic re-   gression classifier . As features , we represented   each verb by its word embedding from Common   Crawl . ( Using such a representation is common   practice for this task ( Rashkin et al . , 2016 ) . ) The   resulting classifier was run on the same large set of   1,700 negative verbs we used in § 5.2 . For each verb   the classifier predicts the sentiment of the agent to-   wards the patient . The result is our novel lexicon .   Since we have also manually annotated the senti-   ment of the agent towards the patient for each verb   in the sentences of our labeled dataset ( § 3 ) , we can   evaluate this lexicon against our labeled dataset .   Table 6 evaluates the different lexicons to deter-   mine the sentiment of the agent towards the patient   on our novel dataset . The table shows that our   novel bootstrapped lexicon produces a notable im-   provement over the existing resources .   5.4 How the Final Classifier is Built   Figure 1 shows how the component tasks intro-   duced in § 5.1 - 5.3 are combined to produce our lin-   guistically informed classifier : We consider those   sentences as abusive that are non - episodic and   which either depict the identity group as perpe-   trator or attribute non - conformist views to it . We   use the best - performing component classifiers as   determined by our previous evaluation ( § 5.1-§5.3 ) .   We also experimented with a supervised classi-   fier that uses the predictions from our component   classifiers as features . However , since the classifi-   cation performance was on a par with our proposed   ( rule - based ) classifier ( Figure 1 ) , we decided in fa-   vor of the latter classifier . It has a clear advantage   over supervised classification in that it does not   require any labeled training data to combine the   predictions of the component classifiers .   6 Evaluation on English Dataset   We evaluate the linguistically informed classifier   ( Figure 1 ) on our new English dataset ( for implicit   abuse ) against other classifiers trained on existing   datasets . We carry out a cross - dataset evaluation :   None of the classifiers , including our linguistically   informed classifier , has been trained on our English   dataset . Given the recent criticism against within-   dataset evaluation ( Arango et al . , 2019 ; Wiegand   et al . , 2019 ) in which high performance is often the   result of overfitting , this is a fairly unbiased set up .   As datasets for training supervised baselines , we   chose those that focus on implicit abuse ( ElSh-   erief et al . , 2021 ) or abuse towards identity groups5606   ( Waseem and Hovy , 2016 ; Sap et al . , 2020 ; Vid-   gen et al . , 2021a , b ) . We also included Founta et al .   ( 2018 ) as a more general dataset sampled from   Twitter . For each dataset , we fine - tune the pre-   trained RoBERTa model ( § 4 ) on the training parti-   tion of the respective dataset . As a further baseline ,   we run the state - of - the - art classifier for abusive lan-   guage detection PerspectiveAPIon our dataset .   We also include an oracle version of our lin-   guistically informed classifier , that combines the   gold standard annotation for the component tasks   ( § 5.1-§5.3 ) rather than the outputs of the respective   classifiers . This can be considered the upper bound   for the linguistically informed classifier .   Finally , we also consider a human classifier   as a general upper bound . We randomly sampled   the judgment of one individual annotator from the   crowdsourced gold - standard annotation for the de-   tection of abusive language . This individual judge-   ment may notably differ from the gold standard   label which is the majority label of 5 annotators .   Table 7 displays the results . The classifiers   trained on existing datasets do not perform well on   our new dataset . The best classifier among them is   the one trained on the DynaB -dataset . For DynaB   ( unlike the other datasets ) , special attention was   paid to the inclusion of non - abusive instances ( § 3 ) .   Still , our linguistically informed classifier is more   effective . DynaB suffers from the identity - group   bias ( § 1 ): its recall for non - abusive instances is   only at 20 % . As detailed in § 3 , DynaB focuses   on non - abusive nesting of abusive statements ( such   as ( 11 ) or ( 12 ) ) . However , it only contains very   few non - abusive atomic utterances ( 8)-(10 ) . With   about 68 % , our linguistically informed classifier   has no perfect recall for non - abusive instances ei-   ther . Since both this classifier and DynaB have in   general a high precision ( with DynaB having the   highest of all classifiers ) , it makes sense to combine   them in order to raise the overall recall . We com-   bine the two classifiers by predicting a non - abusive   sentence if one of the two classifiers predicts one .   This combination further increased performance .   Thus we could outperform DynaB by 8%-points in   macro - average F1 .   The strong performance of the oracle version of   our linguistically informed classifier ( 77.7 % F1 ) is   proof that our 3 linguistic concepts are predictive   of abuse on identity groups . The fact that it out-   performs our best automatic solution ( 73.8 % F1 )   suggests that there is still room for improvement .   Table 8 examines the performance of the indi-   vidual components of our linguistically informed   classifier . Since the combined classifier outper-   forms every individual classifier , we can conclude   that the information contained in the components   is complementary to a certain degree .   Table 8 also shows that the individual compo-   nents are effective across the 4 targets which sug-   gests that they are target independent .   7 Evaluation on German Dataset   Our final experiments focus on our German dataset .   As baselines , we consider supervised classifiers   ( § 4 ) trained on the German datasets for abusive lan-5607   guage detection , i.e. GermEval 2019 ( Struß et al . ,   2019 ) and GermEval 2021 ( Risch et al . , 2021 ) .   Next to a classifier that replicates our linguistically   informed classifier on German data , we also test a   cross - lingual classifier . Following previous work   ( Zampieri et al . , 2020 ) , we fine - tune the multilin-   gual transformer XLM - RoBERTa ( Conneau et al . ,   2020 ) on our English dataset . Since this language   model also covers German , the resulting classifier   can also be applied on our German dataset .   Table 9 shows the results . The human baseline is   notably higher than on the English dataset . German   tweets are predominantly posted by native speakers   resulting in more fluent language . This makes the   manual annotation for the human baseline easier .   With the linguistically informed classifier we   outperform both GermEval classifiers . The oracle   version is even notably better . These results suggest   that the linguistic properties of our 3 components   are language independent . The fact that the mul-   tilingual transformer performs best indicates that ,   in general , the type of implicit abuse we address in   this work , is valid across different languages .   8 Discussion   As the performance of our oracle classifier shows ,   even a perfect linguistically informed classifier is   still below human performance . We could identify   two types of ambiguous utterances in our misclassi-   fications that may be responsible : A few sentences   are underspecified as to whether they report facts   or reflect the author ’s opinion being biased by their   stereotypical views ( 26)-(27 ) . Only the interpreta-   tion as an opinion is perceived abusive .   Moreover , the prior sentiment of the patient may   occasionally depend on the ideology of the reader .   For instance , atheists may consider ( 28 ) abusive   while religious persons would not . Similarly , fem - inists and non - feminists may have a different per-   ception of ( 29 ) . It may be debatable that unique   class labels as we have assigned to ( 26)-(29 ) are ad-   equate . One may argue that without further context   these ambiguities can not be properly resolved .   A general limitation of our approach is that our   data exclusively originate from Twitter . Therefore ,   we can not rule out that certain results reported in   this paper only hold for data from this platform .   Given , however , that we made sure that the data   from that platform that we use are not affected by   any obvious user or topic biases ( § 3 ) and given that   our proposed method works across 4 different iden-   tity groups and 2 different languages , we estimate   the likelihood that this limitation has significantly   affected our results to be very low .   Another limitation of our work is the focus on   atomic sentences in which the identity group is the   agent of some negative verb . As we have moti-   vated in § 3 , our exploratory data analysis suggests   that this is the most frequent surface realization   of such abusive remarks . However , implicitly abu-   sive remarks targeting identity groups may also be   expressed in other ways , such as ( 30 ) where the   identity group is not an agent of some negative   polar verb .   While constructions such as ( 30 ) are possible , we   are unaware of any sampling method that would   enable us to capture such constructions . We expect   these constructions also to be more infrequent than   the more prototypical atomic sentences . Therefore ,   we leave it to future work to address them .   9 Conclusion   We presented a new focused dataset for implicitly   abusive remarks among negative polar utterances   on identity groups . We identified 3 linguistic prop-   erties which allow us to effectively detect such   abusive remarks across different identity groups   and across different languages . The utterances   have to be non - episodic and the identity group is   either depicted as a perpetrator or attributed to a   non - conformist view . We are also able to notably   outperform classifiers trained on previous datasets.560810 Ethical Considerations   Most of our new gold standard data were created   with the help of crowdsourcing . All crowdwork-   ers were compensated following the wage recom-   mended by the crowdsourcing platform Prolific ( i.e.   $ 9.60 per hour ) . Since we were aware of the offen-   sive nature of the data that the crowdworkers had   to annotate , we inserted a respective warning in the   task advertisement . In order to keep the psycho-   logical strain of the crowdworkers at an acceptable   level , the data to be annotated was split into bins   of 100 - 200 instances . Furthermore , we allowed   each crowdworker to take part in one single task   only . We also made it very clear in the task descrip-   tion that we follow a linguistic purpose with our   crowdsourcing tasks and the opinion expressed in   the sentences to be annotated in no way reflects the   opinion of ( us ) researchers designing the tasks .   One of our crowdsourcing tasks included invent-   ing sentences in which a group of people is framed   as a perpetrator ( § 5.2 ) . Since we did not want   crowdworkers to invent any anti - Semitic , homo-   phobic , Islamophobic or misogynist content , we   introduced the name of a fictitious people which   the crowdworkers were to use in their sentences .   We also made sure that the particular name did not   have any obvious phonetic resemblance to existing   identity groups . Although the resulting sentences   being invented are not directed against any existing   identity groups they may still be considered abu-   sive . However , we think that this is justifiable in   this particular context since we are not aware of any   existing dataset that contains a similar content ( i.e.   a focused dataset for learning perpetrator - evoking   verbs ) that we could have used for our experiments .   In principle , creating morally disputable content   as part of research is not unusual . Both in plagia-   rism detection ( Potthast et al . , 2010 ) , deception   detection ( Ott et al . , 2011 ) and , quite recently , abu-   sive language detection itself ( Vidgen et al . , 2021b ;   Wiegand et al . , 2021a ) a procedure similar to ours   was pursued .   One substantial part of the data we are going   to make publicly available as part of this research   will include sentences extracted from Twitter . In   order to protect the privacy rights of the authors   of the tweets and individuals mentioned in them ,   we anonymized our data by discarding mentions of   usernames . The public release of a limited number   of tweets as in the range of our dataset is also in   accordance with the regulations of Twitter . A datasheet describing our novel dataset of la-   beled sentences for the task of detecting implic-   itly abusive remarks about identity groups ( both   English and German version ) following the spec-   ification of Gebru et al . ( 2018 ) was added to the   supplementary material .   Our current data focuses on the four identity   groups Jews , Muslims andgay people andwomen .   This choice was mainly motivated by the fact that   these groups are among the most abused identity   groups on social media . As a consequence , it was   also possible to obtain a reasonable amount of data   ( even with our restrictive measures to ensure less bi-   ased datasets ) . Moreover , these identity groups are   well represented in existing datasets . This allows us   to compare our proposed classifier against baseline   classifiers trained on these existing datasets . We ac-   knowledge that abusive language on the web is also   directed against other identity groups . We leave   their automatic detection to future work . How-   ever , our study suggests that abusive language that   targets these other identity groups will follow the   same language patterns as the instances of abusive   language examined in this paper .   11 Acknowlegdgements   The authors would like to thank Sybille Sornig for   manually annotating parts of the data on which our   descriptive statistics in Section 3 are based . We   are also grateful to Ines Rehbein for feedback on   earlier drafts of this paper .   References5609561056115612