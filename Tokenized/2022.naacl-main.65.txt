  Farima Fatahi Bayat   University of Michigan   farimaf@umich.eduNikita Bhutani   Megagon Labs   nikita@megagon.aiH. V . Jagadish   University of Michigan   jag@umich.edu   Abstract   A major drawback of modern neural OpenIE   systems and benchmarks is that they prioritize   high coverage of information in extractions   over compactness of their constituents . This   severely limits the usefulness of OpenIE ex-   tractions in many downstream tasks . The util-   ity of extractions can be improved if extrac-   tions are compact and share constituents . To   this end , we study the problem of identifying   compact extractions with neural - based meth-   ods . We propose C IE , an OpenIE sys-   tem that uses a novel pipelined approach to   produce compact extractions with overlapping   constituents . It ﬁrst detects constituents of the   extractions and then links them to build extrac-   tions . We train our system on compact extrac-   tions obtained by processing existing bench-   marks . Our experiments on CaRB and Wire57   datasets indicate that C IE ﬁnds 1.5x-   2x more compact extractions than previous   systems , with high precision , establishing a   new state - of - the - art performance in OpenIE .   1 Introduction   A popular domain - agnostic paradigm to structure   the raw text is open information extraction ( Ope-   nIE ) ( Banko et al . , 2007 ) . Not relying on any pre-   deﬁned schema , OpenIE systems typically extract   information as ( subject ; relation ; object ) triples .   The extracted information is then used in sev-   eral downstream applications , including answering   questions ( Khot et al . , 2017 ) , summarizing docu-   ments ( Hao et al . , 2018 ; Ji et al . , 2013 ) , and popu-   lating knowledge bases ( Fan et al . , 2019 ) .   Despite much progress , state - of - the - art neural   OpenIE systems focus on covering more informa-   tion from the input sentence often at the cost of util-   ity and compactness of the extracted triples . The ex-   tracted triples have long , over - speciﬁc constituents   ( i.e. the relation and its arguments ) . Figure 1 illus-   trates such example triples produced by a popularFigure 1 : Example sentences with non - compact triples   from IMoJIE vs. compact triples from our benchmark .   Compact triples can share constituents . Constituents   for subjects , relations and objects are indicated in blue ,   green and orange , respectively .   OpenIE system , IMoJIE ( Kolluru et al . , 2020b ) .   As shown , the knowledge that the second child of   Henry was born in wedlock is embedded in a long   argument . This can be problematic for downstream   applications , especially knowledge base popula-   tion ( Gashteovski et al . , 2020 ; Stanovsky et al . ,   2015 ) that derive power from merging multiple   pieces of information extracted about the same   entity . In contrast , the compact extractions are   more pliable for tasks such as identifying similar   facts and merging facts that share constituents . For   example , compact extractions in Figure 1 can be   merged to derive that Beth is born in wedlock .   Although some prior work ( Corro and Gemulla ,   2013 ; Gashteovski et al . , 2017 ; Bhutani et al . , 2016 )   has explored the compactness of OpenIE triples ,   these systems are rule - based and have been super-   seded by end - to - end neural OpenIE systems . In this   work , we study the problem of identifying compact   extractions with neural - based methods . Inspired by   ( Corro and Gemulla , 2013 ) , we deﬁne an extracted   triple to be compact if it does not contain infor-   mation that can be independently represented in   another triple . To further improve the suitability of900compact triples for knowledge base population , we   require compact triples extracted from a sentence   to have overlapping constituents .   Existing neural systems adopt a sequence label-   ing ( Kolluru et al . , 2020a ; Wang et al . , 2021 ; Ro   et al . , 2020 ) or a sequence generation ( Kolluru et al . ,   2020b ) approach to identify triples and their con-   stituents , typically all at once , or through a pipeline   that ﬁrst identiﬁes the relations and then their corre-   sponding arguments . None of these methods guar-   antee that the extracted triples will be compact and   share constituents .   We propose a novel pipeline system for ﬁnd-   ing compact triples that share their constituents .   We call our OpenIE system , C IE . To en-   courage the constituents to be shared across triples ,   C IEﬁrst extracts the constituents using a   Constituent Extraction model and then links them   using a Constituent Linking model to obtain triples .   We adapt a table ﬁlling method ( Wang et al . ,   2021 ) with a new schema for identifying both con-   stituent boundaries and their roles ( i.e. , subject   or object ) . This allows the constituent extraction   model to capture interactions among constituents   and minimize ambiguities in boundary detection .   For the task of constituent linking , we train a model   that builds on contextual representations speciﬁc   to a given pair of constituents and predicts their   relation type . Such a two - step approach enables us   to optimize the models for each sub - task with dif-   ferent objectives and also promote the constituent   reuse across triples .   Existing neural OpenIE systems are trained on   benchmarks that combine extractions from multi-   ple OpenIE systems . However , no such large - scale   benchmark exists for compact triples . We develop   a new benchmark using a subset of sentences in the   OpenIE2016 benchmark ( Mausam , 2016 ) . Speciﬁ-   cally , we develop a data processing algorithm that   targets extraction from individual clauses in a sen-   tence . Given an input sentence , it identiﬁes clauses   and then uses OpenIE systems such as IMoJIE over   the clauses to ﬁnd compact triples . We train C- IE on the new benchmark .   Our experiments on a ﬁne - grained benchmark ,   Wire57 , show that C IEoutperforms exist-   ing non - neural and neural systems by 5.8 F1 pts   and 7.1 F1 pts , respectively . Manual evaluation   over a coarse - grained benchmark , CaRB , indicates   thatC IEproduces 1.5x-2x more compact   extractions than existing systems with comparableprecision , establishing a new state - of - the - art for the   OpenIE task .   2 Background and Preliminaries   Given a sentence s = ww ... w , an OpenIE sys-   tem generates triples of the form ( subject ; relation ;   object ) , where subject , relation andobject are the   constituents of a triple .   2.1 Extracting Compact Triples   A recent study ( Gashteovski et al . , 2020 ) shows   that triples from modern neural OpenIE systems   are difﬁcult to align to knowledge bases such as   DBpedia . Less than 77 % of triples from neural   OpenIE systems had the same arguments as DBpe-   dia facts . In contrast , the corresponding alignment   ratio for some of the non - neural OpenIE systems   was as high as 98 % . They attribute this behavior   to the speciﬁcity of the triples . A compact triple ,   which does not contain complete clauses as part of   a constituent or contain additional information , is   easier to align to DBpedia . Our goal is to leverage   neural - based methods to extract compact triples .   2.2 System Architecture   We focus on extraction from individual clauses   within a sentence , where each clause includes a   subject , a verb , optionally a direct object , and   a compliment . Since extractions from different   clauses share information , we split the OpenIE task   into two sub - tasks : constituent extraction andcon-   stituent linking .   The task of constituent extraction is to ﬁnd a   set of constituents such that each constituent cis a   contiguous span of words c.span = { ( w , w ) }   and has a pre - deﬁned type c.type∈Ywhere   Y={Argument , Predicate } . The constituent   that takes the relation role in a triple has c.type =   Predicate , and subject andobject constituents   havec.type = Argument . This schema simpli-   ﬁes the task and provides more information to the   constituent linking model .   The task of constituent linking is to connect a   given set of Predicate constituents{p , . . .p }   andArgument constituents{a , . . .a}to ob-   tain triples . We formulate this as a relation clas-   siﬁcation task where the set of relations is Y=   { Subject , Object } . The model predicts relations   rbetween each pand{a , . . .a}such that:901∃(i , j ) : r(a , p ) = Subject , r ( p , a ) = Object   to construct triple ( a;p;a ) .   3 Approach   In this section we describe our pipeline system ,   C IE . We ﬁrst detail the constituent extrac-   tion model , its training constraints , and the decod-   ing algorithm in Section 3.1 . Then , we describe the   constituent linking model in Section 3.2 . Figure 3   shows an overview of C IE architecture .   3.1 Constituent Extraction Model   The constituent extraction model aims to ﬁnd con-   stituent spans and their types in a sentence . Fol-   lowing recent progress in entity - relation extrac-   tion ( Wang et al . , 2021 ) , we model this as a ta-   ble ﬁlling problem . However , we design a new   table schema for the constituent extraction task .   Figure 2 shows an example schema . A sentence   swith|s|tokens corresponds to a table T   such that each cell is labeled based on the rela-   tion between the pair of words . For each con-   stituent , corresponding cells are labeled with y∈   { Argument , Predicate } . For relations between   different constituents , corresponding cells are la-   beled withy∈ { Subject , Object } . The cells   with no relations are labeled None . Graphically ,   constituents are squares on the diagonal , and rela-   tions are rectangles off the diagonal.3.1.1 Table Filling Model   Given the tabular formulation , the constituent ex-   tractor performs two tasks : a ) ﬁll the table by pre-   dicting labels for each word pair , b ) extract the   constituents given the label probabilities . Follow-   ing ( Wang et al . , 2021 ) , we adopt a biafﬁne atten-   tion mechanism , described next , to learn interac-   tions between word pairs when ﬁlling the table .   Given the input sentence s , we ﬁrst obtain contex-   tual representation hfor each word using a pre-   trained language model ( e.g. BERT ( Devlin et al . ,   2018 ) ) . We then employ two MLPs to identify   the head and tail role of the word given its vector   representation h.   h = MLP(h),h = MLP(h )   Next , using the biafﬁne scoring function , we calcu-   late the scoring vector of each pair of words ( e.g.   w , w ) as follows :   t= ( h)Uh+(h⊕h)U+b   whereU , Uare weight parameters , bis the   bias term and⊕denotes concatenation . Then , we   feed the score vector t , jinto a softmax function   to calculate the probability distribution of the corre-   sponding labels l∈Y , whereY = Y∪Y∪None .   P(y|s ) = Softmax ( t )   Finally , we train the 2D table to minimize the fol-   lowing training objective :   L = −1   |s|/summationdisplay / summationdisplaylog(P(y = Y|s ) )   whereYis the gold label for cell ( i , j)in the table .   3.1.2 Training Constraints   ( Wang et al . , 2021 ) shows that structural constraints   imposed on the table during training can signiﬁ-   cantly enhance the model . We adopt their symmetry   andimplication constraints . However , we observed   that these alone are not sufﬁcient if certain labels   are preferred over others . For example , all triples   must have a subject , but some may not have an ob-   ject . We propose a new triple constraint to further   enhance our model . In this section , we describe   the three constraints in detail . We also introduce   P∈Rthat denotes the stack of P(y|s )   for all word pairs in sentence s.   Symmetry : This constraint ensures that the table   is symmetric i.e. the squares are symmetric about   the diagonal . As shown in Figure 2 , this ensures   the label assigned to the ( second , Henry ) cell is the902   same as the cell ( Henry , second ) . Given matrix P ,   We formulate this constraint as symmetrical loss :   L=−1   |s|/summationdisplay / summationdisplay / summationdisplay|P−P|   Implication : This constraint implies that no re-   lation would appear unless its constituents are   present in the table . This is imposed on P :   for each word in the diagonal , maximum pos-   sibility over the constituent type space Y=   { Argument , Predicate } is not lower than the   maximum possibility for other words in the same   row or column over the relation type space Y=   { Subject , Object } .   L=−1   |s|/summationdisplay / bracketleftbigg   max(P , P)−max(P)/bracketrightbigg   Triple Constraint : This constraint enables the   model to increase the likelihood of certain roles   ( e.g. Subject ) over the others ( e.g. Object ) to en-   sure the triples are valid . We enforce this constraint   onP : For each column or row corresponding to   aPredicate constituent , the maximum possibility   of off - diagonal words over Subject type is not   lower than the maximum possibility of off - diagonal   words over Object type . We formulate this con-   straint as triple loss . L = −1   2|ps|/summationdisplay / bracketleftbigg   { max(P)−max(P }   + { max(P)−max(P)}/bracketrightbigg   wherepsis union ofPredicate spans in sentence .   Finally , we jointly optimize four objectives in   training : L + L+L+L   3.1.3 Decoding   Given the label probability tensor P , we need to   decode the constituents in the testing phase . We   follow a 2 - step decoding procedure that ﬁnds spans   of constituents ﬁrst and then assigns a label to each   span . The decoder ﬁrst calculates the distance be-   tween adjacent rows and columns of the table to   ﬁnd constituents ’ boundaries . Next , it assigns a   type to each span and ﬁlters out any None con-   stituents before passing the output to the linking   model . The upper part of Figure 3 shows the out-   put of the decoder , which extracts two constituents   ( “ was ” , “ born ” ) of type Predicate and three con-   stituents ( “ Beth ” , “ the second child of Henry ” , “ in   wedlock ” ) of type Argument . We provide a de-   tailed description of the decoding algorithm in Ap-   pendix A.2 .   3.2 Constituent Linking Model   The constituent linking model aims to take a   Predicate constituent and a set of Argument   constituents as input and predict a relation label903Y={Subject , Object , None } . This procedure   is repeated for each predicate constituent in the sen-   tence . We formulate this as a relation classiﬁcation   task where the model classiﬁes relation labels of   given constituent pairs based on context .   Following prior work ( Zhang et al . , 2019 ; Zhong   and Chen , 2020 ) , we modify the token sequence   of input sentence by adding marker tokens < Pr > ,   < /Pr>,<Arg>,</Arg > to highlight the con-   stituent spans and their types . The markers help   the linking model combine context information and   constituent information for relation classiﬁcation .   As shown in Figure 3.a , two types of constituents   are extracted from the input sentence . For each   constituent of type Predicate , we modify the in-   put sentence by highlighting the location of the   Predicate and allArgument constituents . Then ,   we feed this processed sentence to a pre - trained   encoder ( BERT ) .   Next , we concatenate the output representation   of the start position of predicate pwith the output   representation of the start position of argument a :   X(p , a ) = h⊕h   Finally , we feed the concatenated representation   into a multi - layer perceptron ( MLP ) to predict the   probability distribution of the relation type r∈   Y∪None :   P(r|p , a ) = MLP ( X )   4 Benchmark Creation   To train the constituent extraction and constituent   linking models for extracting compact triples , we   need a benchmark of compact triples . Existing   OpenIE benchmarkis created by combining ex-   tractions from multiple existing OpenIE systems .   Although widely adopted , we observed that it in-   cludes over - speciﬁc and sometimes incorrect ex-   tractions from previous systems . This encouraged   us to design a data processing algorithm that can   extract compact triples from scratch . Inspired by   rule - based OpenIE system ( Corro and Gemulla ,   2013 ) , we ﬁnd compact triples by extracting the   following clauses within a sentence :   Main Clauses are independent clauses that express   a complete concept .   Complement Clauses are subordinate clauses that   serve to complete the meaning of a verb or noun in   the sentence .   Coordinate Clauses are independent clauses   joined to the main clause using coordinating con-   junctions such as and , or , but , etc .   We identify clauses within a sentence using its de-   pendency graph . We ﬁrst build a sentence tree such   that the root is the head of the main clauses and   the ﬁrst - level children are clauses modifying the   root word . We then perform a postﬁx traversal of   the tree until we ﬁnd a sub - tree with no clausal   children . At this point , we run a standard Ope-   nIE system , IMoJIE ( Kolluru et al . , 2020b ) , over   the clause corresponding to the sub - tree to obtain   triples . We then backtrack and extract triples for   other clausal children and lastly the parent . We   provide pseudo - code of algorithm in Appendix A.   We run our algorithm on each multi - clause sentence   in the OpenIE2016 benchmark and obtain a new   benchmark tailored for extracting compact triples .   Figure 1 shows example sentences and compact   triples from this benchmark .   5 Experimental Setup   Training Dataset : We train C IEusing   the benchmark described in Section 4 . Table 1   compares the statistics of our new benchmark and   bootstrapped OpenIE2016 benchmark . As shown ,   our benchmark has 1.25 times more extractions per   sentence than OpenIE2016 and its constituents are   more compact . We use about 1 % of sentences for   validation and the remaining for training .   Comparison Systems : We compare C IE   against state - of - the - art sequence - labeling systems ,   OpenIE6 ( Kolluru et al . , 2020a ) and Multi2OIE   ( Ro et al . , 2020 ) , and sequence - generation system ,   IMoJIE ( Kolluru et al . , 2020b ) ) . We also compare   it against traditional non - neural systems designed   for extracting compact facts : NestIE ( Bhutani et al . ,   2016 ) and MinIE ( Gashteovski et al . , 2017 ) .   Evaluation Datasets and Metrics : We evaluate   the OpenIE systems both automatically and man-   ually on standardized benchmarks . For automatic904   evaluation , we ﬁrst assess all systems with CaRB   test and Wire57datasets . Since these datasets   are not targeted for compact triples , for a fair com-   parison we exclude triples that have at least one   clause within a constituent . Table 2 shows the statis-   tics of the original and processed datasets . Each   dataset also provides its own scoring function . We   report precision ( P ) , recall ( R ) , and F1 computed by   these scoring functions . Wire57 contains more ﬁne-   grained extractions than the CaRB dataset and its   scoring function is more rigorous for compact facts   since it penalizes over - speciﬁc extractions . How-   ever , both CaRB and Wire57 scoring functions are   based on token - level matching of system extrac-   tions against ground truth facts . Moreover , these   benchmarks are incomplete , meaning that the gold   extractions do not include all acceptable surface   realizations of the same fact . These drawbacks en-   couraged us to additionally perform a fact - centered   evaluation using the BenchIE ( Gashteovski et al . ,   2021 ) benchmark and scoring paradigm . Finally ,   we carry out a manual evaluation on 100 sentences   to avoid bias towards different scorers .   Implementation Details : Since the schema de-   sign of the table ﬁlling model does not support con-   junctions inside constituents , we follow previous   work ( Kolluru et al . , 2020a ) and pre - process the   sentences into smaller conjunction - free sentences   before passing them to the system .   For a fair comparison to previous work , we use   bert - based - uncased ( Devlin et al . , 2018 ) as the text   encoder for both the constituent extraction model   and constituent linking model . Each model con-   tains nearly 110 M parameters . For both models , we   set the max sequence length to 512 , initial learning   rate to 5e-5 , weight decay to 1e-5 , and the batch   size to 32 . We use AdamW optimizer to ﬁne - tune   each model . The batch size is 300 for constituent   extraction model and 20 for the constituent linking   model , both equipped with early stopping . We use   NVIDIA GeForce RTX 2080 Ti GPU to train both   models for a cumulative time of 8 hours.6 Experimental Results   6.1 Automatic Token - level Evaluation   Table 3 summarizes the performance of OpenIE   systems across the CaRB and Wire57 datasets and   scoring functions . On the ﬁne - grained Wire57   dataset with a strict Wire57 scorer , C IE   outperforms neural OpenIE systems ( by 7.2 - 9 F1   pts ) and non - neural systems ( by 5.8 - 10.8 F1 pts ) .   On the more coarse - grained CaRB dataset , al-   most all OpenIE systems achieve comparable per-   formance in terms of overall F1 using the CaRB   scoring function . The neural systems still outper-   form non - neural systems in terms of F1 , which is   in line with previous studies . However , neural Ope-   nIE systems are tuned based on the CaRB scoring   function and thus tend to produce extractions that   are biased towards this scoring method . Previous   works ( Kolluru et al . , 2020a ) also report issues with   the scoring function not being able to handle con-   junctions properly . Table 7 shows the limitations of   the CaRB benchmark and scoring function through   an example . As illustrated , the set of extractions   produced by C IEis more exhaustive than   IMoJIE and ground truth extractions . However , the   CaRB scoring function assigns an F1 score of 62.0   to IMoJIE extractions , and 39.7 toC IEex-   tractions . To resolve incompleteness of the CaRB   benchmark and potential bias towards its scoring   function , we undertake a fact - centered evaluation ,   detailed in Section 6.2 , and a manual evaluation ,   described in Section 6.3 .   6.2 Fact - centric Evaluation   ( Gashteovski et al . , 2021 ) claims that CaRB and   Wire57 benchmarks and scoring functions overes-   timate a system ’s ability to extract correct facts .   They propose an alternative benchmark and evalu-   ation framework , BenchIE , that exhaustively lists   all fact - equivalent extractions and clusters them   into fact synsets . The scoring function considers   an extraction as correct , if and only if it exactly   matches any of the gold extractions from any of the   fact synsets . They report Precision , Recall , and F1   based on exact triple matching .   Table 5 shows the performance of different Ope-   nIE systems on BenchIE . As shown , C IE   outperforms all other systems except MinIE . We   found that MinIE aims to exhaustively produce   different representations of the same fact . In con-   trast , C IEfollows the setup of neural Ope-   nIE systems and encourages at most one repre-905   sentation per fact . As a result , MinIE produces   1.36x more extractions than C IE , achiev-   ing much higher recall than its neural counterparts .   6.3 Manual Evaluation   Limitations in the aforementioned benchmarks and   evaluation frameworks encouraged us to perform   human evaluation on triples generated by various   systems . To this end , we randomly select 100 sen-   tences from the CaRB validation set and feed them   to all systems to investigate the generated triples .   Next , we ask two graduate CS students , blind to the   OpenIE systems , to mark each triple for correctness   ( 0 or 1 ) based on whether it is asserted in the text   and correctly captures the semantic information .   They also label extractions for compactness ( 0 or   1 ) . We consider an extraction compact if none of its   constituents is longer than 10 words , includes con-   junction or can be an independent extraction . We   found an inter - annotator agreement of 0.68 on cor-   rectness and 0.83 on compactness using the Cohens   Kappa metric . We report the results of the manual   evaluation in Table 4 . Neural systems target infor-   mativeness , which results in high precision at the   cost of compactness . On the other hand , non - neural   systems that aim for compact triples suffer from   low precision . C IEoffers a better trade-   off between precision and compactness . It achieves   comparable precision to neural models ( -6 % ) while   providing substantially more compact extractions   ( +36 % ) . Compared to the MinIE , C IE   produces triples with signiﬁcantly higher precision   ( +22 % ) while producing a comparable number of   compact triples . NestIE achieves comparable com-   pactness rate to C IEbut suffers from low   precision and total number of extractions .   7 Analysis   7.1 Compact and Overlapping Constituents   To understand the performance of C IEin   generating compact triples that share constituents ,   we introduce the following metrics :   •Average Constituent Length ( ACL ): aver-   age length of constituents across all system-   generated triples . This is a “ syntactic ” measure   of compactness . The lower the ACL score , the   higher the compactness of triples .   •Number of Constituent Clauses ( NCC ): average   number of clauses per constituent that could be   extracted as independent triples . The lower the   NCC score , the better the compactness of triples .   •Repetitions Per Argument ( RPA ): number of to-   tal arguments divided by the number of unique   arguments . The higher the RPA score , the higher   fraction of total constituents produced per sen-906tence are shared .   Table 3 summarizes the performance on these   metrics over CaRB and Wire57 benchmarks . We   do not conduct a separate analysis over BenchIE   since it uses a subset of CaRB sentences . As shown ,   the ACL scores of C IEare signiﬁcantly   lower than its neural counterparts and closely fol-   lows MinIE . The average constituent length ( ACL )   of NestIE triples is the lowest since it breaks sen-   tences into small triples with verb , noun , preposi-   tion , and adjective mediated relations . For instance ,   the sentence : “ 2 million people died of AIDS . ” is   broken down into T1 : ( 2 million people ; died ) , and   T2 : ( T1 ; of ; AIDS ) . However , its ﬁne - grained strat-   egy greatly sacriﬁces F1 for compactness . C- IEachieves the lowest NCC score which in-   dicates that the constituents in triples contain the   fewest verbal clauses . As a result , these triples are   more suitable for downstream applications such as   text summarization and knowledge - base construc-   tion than other counterparts .   Finally , high RPA scores of C IEdemon-   strate the effectiveness of our approach as it enables   the system to reuse the same constituent to generate   multiple triples . MinIE achieves a slightly higher   RPA score than C IEsince it extracts mul-   tiple triples to represent the same fact leading to a   higher repetition of unique constituents .   7.2 Effectiveness of Design Choices   Pipelined Approach vs. Uniﬁed Table Filling .   To compare our pipelined approach with a uniﬁed   extraction model , we follow UniRE ( Wang et al . ,   2021 ) , which decodes a single table to identify   entities and relations jointly . We follow their 3 - step   decoding algorithm to obtain the constituents and   links between them from the same table ( with the   schema shown in Figure 2 ) . We refer to this model   asC IE . We report the performances   in Table 6 and show that performance drops by   jointly training the constituent and linking model .   This aligns with the observations in recent entity-   relation extraction work that pipelined approaches   are more effective than joint models .   Effectiveness of Schema Design . Our table   schema for constituent extraction includes both   labels for constituents as well as labels to link   them . We argued that this design captures the con-   textual dependency information between the con-   stituents that boosts extraction performance . We   compare the effectiveness of this schema design to   another schema that uses only constituent labels   Y:{Argument , Predicate } ∪None . Note that   we use the same constituent linking model to obtain   triples from the extracted constituents . We refer to   this setting as C IE . Table 6 illus-   trates the performance of this system on both CaRB   and Wire57 datasets . We ﬁnd that C IE   achieves signiﬁcantly higher F1 compared to C- IE and conclude that incorporating   additional context in the table schema improves the   performance of the constituent extraction model .   7.3 Error Analysis   We examine C IEtriples produced for 50   randomly selected sentences of the CaRB valida-   tion dataset and 20 randomly selected sentences   of the Wire57 dataset . Upon close analysis , we   identify ﬁve major sources of error :   Constituent Not Found : ( 49.29 % ) We ﬁnd that   the constituent extraction model can fail to cor-   rectly label the constituents in the table . We   found that the model gets biased towards producing   None labels due to the imbalanced distribution of   labels .   Wrong Relation Type : ( 28.17 % ) These involve   errors where the constituent linking model fails to   correctly predict the link between the constituents .   The current model encodes one sentence per pred-   icate to ﬁnd its arguments . Alternatively , we can   encode one sentence per predicate - argument pair   to focus more on each relation . Relation labels in   the constituent extraction model can also assist the   linking model in predicting the correct relations .   We reserve this issue for future work .   Boundary Detection Error : ( 11.26 % ) These in-   clude errors where the decoder in constituent ex-   traction fails to correctly identify the boundaries   of the constituents . Boundary detection in con-   stituent extraction model is highly dependant on   the choice of distance threshold ( α ) , as explained   in A.2 , which limits its robustness .   Inexpressive Table Error : ( 7.04 % ) These include   errors where constituents have overlapping spans   that participate in two roles within the same extrac-907   tion or two different extractions .   Less than 4.22 % of the errors were because of   incorrect constituent type predictions . This indi-   cates the effectiveness of our table ﬁlling method   on constituent type detection .   8 Related Work   OpenIE has been studied extensively for over a   decade with a history of statistical and rule - based   systems ( Banko et al . , 2007 ; Fader et al . , 2011 ;   Corro and Gemulla , 2013 ; Mausam et al . , 2012 ; An-   geli et al . , 2015 ) that extract triples from sentences   without using any training data . Recently , neural   models have been developed that are trained end-   to - end on extractions bootstrapped from previous   OpenIE systems . These can broadly be classiﬁed   intolabeling - based andgeneration - based systems .   Labeling - based systems ( Stanovsky et al . , 2018 ;   Kolluru et al . , 2020a ; Ro et al . , 2020 ) tag each word   in the sentence and construct triples in an auto-   regressive manner or by using a unique predicate   for each triple . Generation - based systems ( Kolluru   et al . , 2020b ; Bhutani et al . , 2019 ) use a sequence-   to - sequence model to generate triples one word   at a time . Labeling - based systems can handle re-   dundancy in extracted triples and are faster than   generation - based systems ( Kolluru et al . , 2020a ) .   Compactness in OpenIE : There has been prior   work ( Bhutani et al . , 2016 ; Gashteovski et al . , 2017 ;   Stanovsky and Dagan , 2016 ; Angeli et al . , 2015 )   that focuses on ﬁnding compact triples and shows   that concise triples are useful in several seman-   tic tasks . However , recent studies ( L ´ echelle et al . ,   2018 ; Gashteovski et al . , 2020 ) indicate that neural   OpenIE systems produce more speciﬁc triples with   additional information than conventional OpenIE   systems and are harder to align with existing knowl-   edge bases . Therefore , we focus on designing a   neural OpenIE system that extracts compact triples . Grid Labeling : Also known as table ﬁlling , grid   labeling has been recently applied to entity relation   extraction ( Gupta et al . , 2016 ; Wang et al . , 2021 )   and open information extraction tasks ( Kolluru   et al . , 2020b ) . However , these models map entities   ( constituents ) and relations ( subject , object ) in a   uniﬁed label space to capture the inter - dependency   between them . ( Zhong and Chen , 2020 ) shows   that a pipelined approach for entity and relation   extraction outperforms prior joint models that use   the same encoder for the two sub - tasks . In this   work , we validate this claim for the OpenIE task .   Furthermore , we design a grid labeling schema   that identiﬁes constituents and their types , akin to   entities in the entity relation extraction task .   9 Conclusion   In this work we extract compact triples from sin-   gle sentences using an end - to - end pipelined ap-   proach , ﬁrst extracting triple constituents using a   novel table ﬁlling model and then determining rela-   tions between them with a classiﬁer . Our method   achieves excellent performance in producing ex-   haustive compact triples with high precision . We   hope that C IEserves as a strong baseline   and makes us re - think the value of all - at - once in-   formation extraction systems .   10 Acknowledgments   The research described herein was sponsored by   the U.S. Army Research Institute for the Behav-   ioral and Social Sciences , Department of the Army   ( Contract No . W911NF-20 - C-0028 ) . The views   expressed in this presentation are those of the au-   thor and do not reﬂect the ofﬁcial policy or position   of the Department of the Army , DOD , or the U.S.   Government.908References909A Appendix   A.1 Benchmark Creation   The Algorithm 2 gives a high - level overview of   our benchmark creation mechanism while a lot   of details and difﬁculties have been omitted . The   Benchmark Creation Algorithm extracts triples for   each sentence using the Algorithm 1 . The OpenIE   system used to produce triples out of simple clauses   is IMoJIE ( Kolluru et al . , 2020b ) .   The following example illustrates the benchmark   creation algorithm . Given the sentence : “ The   group reach a small shop , where the crocodile   breaks through a wall ” , the algorithm ﬁrst builds   the sentence tree as shown in Figure 4 . Then , start-   ing from the root , ExtractTriple function traverses   the tree until it reaches a child ( “ breaks ” ) with no   further clausal children . At this point , a clause   for the subtree rooted at “ breaks ” is generated   and fed into the IMoJIE system . IMoJIE extracts   triple : ( the crocodile ; breaks ; through a wall ) out   of this clause . Then , since both children of the root   ( “ reach ” ) are processed , the IMoJIE triple of the   root ’s corresponding clause is extracted as ( The   rest of the group ; reach ; a small shop ) .   Algorithm 1 : ExtractTriples   Data : Tree Node R   Result : Set of compact triples T   T = set ( ) ;   forchild in R.children do   ifchild has no clausal child then   T + = IMoJIE(child.clause ) ;   end   else   T+= ExtractTriples(child ) ;   end   end   T + = IMoJIE(R.clause ) ;   return T   Algorithm 2 : Benchmark Creation   Data : Sentence List S = [ s , s, .. ,s ]   Result : B benchmark of compact triples for   sentences in S   B = set ( ) ;   forsentence in S do   root = build sentence tree(sentence ) ;   B + = ExtractTriples(root ) ;   end   return B   A.2 Table Decoding   Following the ( Wang et al . , 2021 ) work , in the test-   ing phase , we rely on the label probability tensor   P∈Rof the sentence s , to ﬁrst extract   constituent spans , and then predict the constituent   type . Next , we describe the decoding procedure .   A.2.1 Constituent Span Detection   One important observation of the ground truth ta-   ble is that a constituent ’s corresponding rows and   columns are identical ( e.g. , row 2 and row 3 of   Figure 2 are identical ) . Therefore , given the ten-   sorP , we compute the distance of adjacent rows   ( and columns ) . If the distance is larger than a pre-   deﬁned threshold α(which is set to 1.2 ) , a split po-   sition is detected . This means that the two adjacent   rows ( columns ) belong to different constituents   or one belongs to a constituent while the other is   not . Following the ( Wang et al . , 2021 ) work , we   ﬂatten thePtensor from both row and column per-   spectives and calculate the euclidean distance of   adjacent rows and adjacent columns . Finally , we   average these two distances as the ﬁnal distance   and compare the ﬁnal distance to αto ﬁnd the span   of different constituents .   A.2.2 Constituent Type Detection   Given a constituent ’s span ( i , j ) , we decode the   constituent type t∈Y , whereY = Y∪Y∪   None , according to its corresponding square sym-   metric about the diagonal :   t = argmaxAvg(P )   Spans with predicted type t∈Yare regarded as   constituents and passed to the constituent linking   model.910