  Leonardo F. R. Ribeiro , Mengwen Liu , Iryna Gurevych ,   Markus Dreyer , Mohit BansalUKP Lab , Technical University of DarmstadtAmazon Alexa AI , UNC Chapel Hill   Abstract   Despite recent improvements in abstractive   summarization , most current approaches gener-   ate summaries that are not factually consistent   with the source document , severely restricting   their trust and usage in real - world applications .   Recent works have shown promising improve-   ments in factuality error identification using   text or dependency arc entailments ; however ,   they do not consider the entire semantic graph   simultaneously . To this end , we propose F-   G , a method that decomposes the docu-   ment and the summary into structured mean-   ing representations ( MR ) , which are more suit-   able for factuality evaluation . MRs describe   core semantic concepts and their relations , ag-   gregating the main content in both document   and summary in a canonical form , and reduc-   ing data sparsity . FG encodes such   graphs using a graph encoder augmented with   structure - aware adapters to capture interactions   among the concepts based on the graph con-   nectivity , along with text representations using   an adapter - based text encoder . Experiments on   different benchmarks for evaluating factuality   show that FG outperforms previous   approaches by up to 15 % . Furthermore , F-   G improves performance on identifying   content verifiability errors and better captures   subsentence - level factual inconsistencies .   1 Introduction   Recent summarization approaches based on pre-   trained language models ( LM ) have established   a new level of performance ( Zhang et al . , 2020 ;   Lewis et al . , 2020 ) , generating summaries that are   grammatically fluent and capable of combining   salient parts of the source document . However , cur-   rent models suffer from a severe limitation , generat-   ing summaries that are not factually consistent , that   is , the content of the summary does not meet theFigure 1 : Example of ( a ) a document , ( b ) a summary ,   and ( c ) the corresponding document and ( d ) summary   graph - based meaning representations . The summary   graph does not contain the " consider " node , indicating a   factual error ( red dashed edge ) .   facts of the source document , an issue also known   ashallucination . Previous studies ( Cao et al . , 2018 ;   Falke et al . , 2019 ; Maynez et al . , 2020 ; Dreyer   et al . , 2021 ) report rates of hallucinations in gen-   erated summaries ranging from 30 % to over 70 % .   In the face of such a challenge , recent works em-   ploy promising ideas such as question answering   ( QA ) ( Durmus et al . , 2020 ; Nan et al . , 2021 ) and   weakly supervised approaches ( Kryscinski et al . ,   2020 ) to assess factuality . Another line of work   explores dependency arc entailment to improve the   localization of subsentence - level errors within gen-   erated summaries ( Goyal and Durrett , 2020 ) .   However , these methods have a reduced corre-   lation with human judgments and may not capture   well semantic errors ( Pagnoni et al . , 2021 ) . One   reason for the poor performance is the lack of good   quality factuality training data . Second , it is chal-   lenging to properly encode core semantic content   from the document and summary ( Lee et al . , 2021 )   and reason over salient pieces of information in3238order to assess the summary factuality . Third , pre-   vious work ( DAE , Goyal and Durrett , 2021 ) treats   semantic relations as isolated units , not simultane-   ously considering the entire semantic structure of   both document and summary texts .   To mitigate the above issues , we explore mean-   ing representations ( MR ) as a form of content rep-   resentation for factuality evaluation . We present   FG , a novel graph - enhanced approach   that incorporates core information from the docu-   ment and the summary into the factuality model us-   ing graph - based MRs , which are more suitable for   factuality evaluation : As shown in Figure 1 , graph-   based MRs capture semantic relations between en-   tities , abstracting away from syntactic structure and   producing a canonical representation of meaning .   Different from previous methods ( Kryscinski   et al . , 2020 ; Goyal and Durrett , 2021 ) , F-   G is a dual approach which encodes both text   and graph modalities , better integrating linguistic   knowledge and structured semantic knowledge . As   shown in Figure 2 , it is composed of parameter-   efficient text and graph encoders which share the   same pretrained model and differ by their adapter   weights ( Houlsby et al . , 2019 ) . The texts from   the document and summary are encoded using the   adapter - based text encoder whereas the entire se-   mantic structures that represent document and sum-   mary facts are used as input to the graph encoder   augmented structure - aware adapters ( Ribeiro et al . ,   2021b ) . The representations of the two modalities   thus are combined to generate the factuality score .   In particular , we explore Abstract Meaning Rep-   resentation ( AMR ) ( Banarescu et al . , 2013 ) , a se-   mantic formalism that has received much research   interest ( Song et al . , 2018 ; Guo et al . , 2019 ; Ribeiro   et al . , 2019 , 2021a ; Opitz et al . , 2020 , 2021 ; Fu   et al . , 2021 ) and has been shown to benefit down-   stream tasks such as spoken language understand-   ing ( Damonte et al . , 2019 ) , machine translation   ( Song et al . , 2019 ) , commonsense reasoning ( Lim   et al . , 2020 ) , and question answering ( Kapanipathi   et al . , 2021 ; Bornea et al . , 2021 ) .   Intuitively , AMR provides important benefits :   First , it encodes core concepts as it strives for   a more logical and less syntactic representation ,   which has been shown to benefit text summariza-   tion ( Hardy and Vlachos , 2018 ; Dohare et al . , 2018 ;   Lee et al . , 2021 ) . Furthermore , AMR captures   semantics at a high level of abstraction explic-   itly modeling relations in the text and reducingthe negative influence of diverse text surface vari-   ances with the same meaning . Lastly , recent stud-   ies ( Dreyer et al . , 2021 ; Ladhak et al . , 2021 ) demon-   strate that there is a trade - off between factuality   and abstractiveness . Structured semantic represen-   tations are potentially beneficial for reducing data   sparsity and localizing generation errors in abstrac-   tive scenarios . Figure 1 shows examples of ( c )   document and ( d ) summary AMRs , where the sum-   mary AMR is missing a crucial modifying node   present in the document AMR , which indicates a   factual error in the summary .   We consolidate a factuality dataset with human   annotations derived from previous works ( Wang   et al . , 2020 ; Kryscinski et al . , 2020 ; Maynez et al . ,   2020 ; Pagnoni et al . , 2021 ) . This dataset is con-   structed from the widely - used CNN / DM ( Her-   mann et al . , 2015 ) and XSum ( Nallapati et al . ,   2016 ) benchmarks . Extensive experimental results   demonstrate that FG achieves substan-   tial improvements over previous approaches , im-   proving factuality performance by up to 15 % and   correlation with human judgments by up to 10 % ,   capturing more content verifiability errors and bet-   ter classifying factuality in semantic relations .   2 Related Work   Evaluating Factuality . Recently , there has been   a surge of new methods for factuality evaluation   in text generation , especially for summarization .   Falke et al . ( 2019 ) propose to rerank summary hy-   potheses generated via beam search based on en-   tailment scores to the source document . Kryscinski   et al . ( 2020 ) introduce FCC , a model - based ap-   proach trained on artificially generated data , to mea-   sure if the summary can be entailed by the source   document in order to assess the summary factual-   ity . QA - based methods ( Wang et al . , 2020 ; Dur-   mus et al . , 2020 ; Honovich et al . , 2021 ; Nan et al . ,   2021 ) generate questions from the document and   summary , and compare the corresponding answers   in order to assess factuality . Xie et al . ( 2021 ) for-   mulate causal relationships among the document ,   summary , and language prior to evaluate the factu-   ality via counterfactual estimation .   Categorizing Factual Errors . A thread of anal-   ysis work has focused on identifying different cate-   gories of factual errors in summarization . Maynez   et al . ( 2020 ) show that semantic inference - based au-   tomatic measures are better representations of sum-   marization quality , whereas Pagnoni et al . ( 2021)3239propose a linguistically grounded typology of fac-   tual errors and develop a fine - grained benchmark   for factuality evaluation , moving to a fine - grained   measure , instead of using a binary evaluation . Fab-   bri et al . ( 2021 ) introduce different resources for   summarization evaluation which include a toolkit   for evaluating summarization models .   Factuality versus Abstractiveness . Recent   works ( Dreyer et al . , 2021 ; Ladhak et al . , 2021 )   investigate the trade - off between factuality and   abstractiveness of summaries and observe that   factuality tends to drop with increased abstractive-   ness . Semantic graphs are uniquely suitable to   detect factual errors in abstractive summaries as   they abstract away from the lexical surface forms   of documents and summaries , enabling direct   comparisons of the underlying semantic concepts   and relations of a document - summary pair .   Graph - based Representations for Summariza-   tion . A growing body of work focuses on using   graph - based representations for improving sum-   marization . Whereas different approaches encode   graphs into neural models for multi - document sum-   marization ( Fan et al . , 2019 ; Li et al . , 2020 ; Pa-   sunuru et al . , 2021 ; Wu et al . , 2021 ; Chen et al . ,   2021 ) , AMR structures have been shown to benefit   both document representation and summary gener-   ation ( Liu et al . , 2015 ; Liao et al . , 2018 ; Hardy and   Vlachos , 2018 ; Dohare et al . , 2018 ) and have the po-   tential of improving controllability in summariza-   tion ( Lee et al . , 2021 ) . The above works are related   toFG as they use semantic graphs for   content representation , but also different because   they utilize graphs for the downstream summariza-   tion task , whereas FG employs them for   factuality evaluation .   Semantic Representations for Factuality Evalua-   tion . More closely related to our work , Goodrich   et al . ( 2019 ) extract tuples from the document and   summary and measure the factual consistency by   overlapping metrics . Xu et al . ( 2020 ) weight facts   present in the source document according to the   facts from the gold summary using contextual em-   beddings , and verify whether a generated summary   is able to capture the same facts as the target . Re-   cently , dependency arc entailment ( DAE , Goyal   and Durrett , 2020 ) is used to measure subsentence-   level factuality by classifying pairs of words de-   fined by dependency arcs which often describe se-   mantic relations . However , FG is con - siderably different from those approaches , since it   explicitly encodes the entire graph semantic struc-   ture into the model . Moreover , while DAE consid-   ers semantic edge relations of the summary only ,   FG encodes the semantic structures of   both the input document and summary leading to   better factuality performance at both sentence and   subsentence levels .   3 FG Model   We introduce FG , a method that employs   semantic graph representations for factuality evalu-   ation in text summarization , describing its intuition   ( § 3.3 ) and defining it formally ( § 3.4 ) .   3.1 Problem Statement   Given a source document Dand a sentence - level   summary S , we aim to check whether Sisfactual   with respect to D. For each sentence d∈Dwe   extract a semantic graph G. Similarly , for the sum-   mary sentence Swe extract its semantic graph G.   We use texts and graphs from both document and   summary for factuality evaluation . Sentence - level   summary predictions can be aggregated to generate   a factuality score for a multi - sentence summary .   3.2 Extracting Semantic Graphs   We select AMR as our MR , but FG can   be used with other graph - based semantic repre-   sentations , such as OpenIE ( Banko et al . , 2007 ) .   AMR is a linguistically - grounded semantic formal-   ism that represents the meaning of a sentence as a   rooted graph , where nodes are concepts and edges   aresemantic relations . AMR abstracts away from   surface text , aiming to produce a more language-   neutral representation of meaning . We use a state-   of - the - art AMR parser ( Bevilacqua et al . , 2021 ) to   extract an AMR graph G= ( V , E , R)with a   node set Vand labeled edges ( u , r , v ) ∈ E , where   u , v∈ Vandr∈ Ris a relation type . Each G   aims to explicitly represent the core concepts in   each sentence . Figure 1 shows an example of a ( b )   sentence and its ( d ) corresponding AMR graph .   Graph Representation . We convert each Ginto   a bipartite graph G= ( V , E ) , replacing each   labeled edge ( u , r , v ) ∈ Ewith two unlabeled   edges{(u , r),(r , v ) } ∈ E. Similar to Beck et al .   ( 2018 ) , this procedure transforms the graph into its   unlabeled version . Pretrained models typically use3240   a vocabulary with subword tokens , which makes   it complicated to properly represent a graph using   subword tokens as nodes . Inspired by Ribeiro et al .   ( 2020 , 2021b ) , we transform each Ginto a new   token graph G= ( V , E ) , where each token of a   node v∈ Vbecomes a node v∈ V. We convert   each edge ( u , v)∈ Einto a set of edges and   connect every token of uto every token of v.   3.3 Intuition of Semantic Representation   In order to represent facts to better assess the   summary factuality , we draw inspiration from tra-   ditional approaches to summarization that con-   dense the source document to a set of “ semantic   units ” ( Liu et al . , 2015 ; Liao et al . , 2018 ) . Intu-   itively , the semantic graphs from the source doc-   ument represent the core factual information , ex-   plicitly modeling relations in the text , whereas the   semantic summary graph captures the essential con-   tent information in a summary ( Lee et al . , 2021 ) .   The document graphs can be compared with the   summary graph , measuring the degree of semantic   overlap to assess factuality ( Cai and Knight , 2013 ) .   Recently , sets of fact triples from summaries   were used to estimate factual accuracy ( Goodrich   et al . , 2019 ) . That approach is related to F-   G as it uses graph - based MRs , but also differ-   ent because it compares the reference and the gener-   ated summary , whereas we compare the generated   summary with the input document . Moreover , dif-   ferently from Goodrich et al . ( 2019 ) , FG   explicitly encodes the semantic structures using a   graph encoder and employs AMR as a semantic rep-   resentation . Finally , in contrast to DAE ( Goyal and   Durrett , 2021 ) , which focuses only on extracting   summary graph representations , FG uses   semantic graphs for both document and summary.3.4 Model   Figure 2 illustrates FG , which is com-   posed of text and graph encoders . The text encoder ,   denoted by E , uses a pretrained encoder E , aug-   mented with adapter modules which receives the   summary Sand document Dand outputs a con-   textual text representation . Conversely , the graph   encoder , denoted by E , uses the same E , but is   augmented with structure - aware adapters . Ere-   ceives the summary and multiple document seman-   tic graphs corresponding to its sentences , and out-   puts graph - aware contextual representations that   are used to generate the final graph representation .   During training , only adapter weights are trained ,   whereas the weights from Eare kept frozen . Fi-   nally , both graph and text representations are con-   catenated and fed to a final classifier , which pre-   dicts whether the summary is factual or not .   Text Encoder . We employ an adapter module be-   fore and after the feed - forward sub - layer of each   layer of the encoder . We modify the adapter archi-   tecture from Houlsby et al . ( 2019 ) . We compute the   adapter representation for each token iat each layer   l , given the token representation h , as follows :   ˆz = W(σ(W(h ) ) ) + h , ( 1 )   where σis the activation function and(·)de-   notes layer normalization . W∈Rand   W∈Rare adapter parameters . The rep-   resentation of the [ CLS ] token is used as the final   textual representation , denoted by t.   Graph Encoder . In order to re - purpose the pre-   trained encoder to structured inputs , we employ a   structural adapter ( Ribeiro et al . , 2021b ) . In par-   ticular , for each node v∈ V , given the hidden3241representation h , the encoder layer lcomputes :   g = GraphConv((h),{(h):u∈ } )   z = Wσ(g ) + h , ( 2 )   where N(v)is the neighborhood of the node v   inGandW∈Ris an adapter parameter .   GraphConv(·)is the graph convolution that com-   putes the representation of vbased on its neighbors   in the graph . We employ a Relational Graph Con-   volutional Network ( Schlichtkrull et al . , 2018 ) as   graph convolution , which considers differences in   the incoming and outgoing relations . Since AMRs   are directed graphs , encoding edge directions is   beneficial for downstream performance ( Ribeiro   et al . , 2019 ) . The structural adapter is placed be-   fore , whereas the normal adapter is kept after the   feed - forward sub - layer of each encoder layer .   We calculate the final representation of each   graph from the pooling denoted as z={z|   v∈ V } , where zis the final representation of v.   Thus , we use a multi - head self - attention ( Vaswani   et al . , 2017 ) layer to estimate to what extent each   sentence graph contributes to the document seman-   tic representation based on the summary graph .   This mechanism allows encoding a global docu-   ment representation based on the summary graph .   In particular , each attention head computes :   α = Attn ( z , z ) ,   g=/summationdisplayαWz,(3 )   where zis the final representation of G , kis the   number of considered sentence graphs from the   input document and W∈Ris a parameter .   The final representation is derived from the text   and graph representations , q= [ t;g ] , and fed into a   classification layer that outputs a probability distri-   bution over the labels y= { Factual , Non - Factual } .   3.5 Edge - level Factuality Model   Inspired by Goyal and Durrett ( 2021 ) , we evaluate   the factuality at the edge level . In this setup , we   use the same text and graph encoders ; however ,   we encode the semantic graphs differently . In par-   ticular , we concatenate Gwith each G∈Dand   feed the concatenation to the graph encoder . The   representation of a node v∈ Gis calculated as :   r=/summationdisplayE(D;S )   r=/summationdisplayE(G;G )   r= [ r;r](4 )   where A(v)is the set of all summary words aligned   withv , andrandrare the word and node rep-   resentations , respectively . Edge representations are   derived for each AMR edge ( u , v)∈ E : r=   [ r;r ] . The edge representation ris fed into a   classification layer that outputs a probability distri-   bution over the output labels ( y= { Factual , Non-   Factual } ) . We assign the label non - factual for an   edge in Gif one of the nodes in this edge is aligned   with a word that belongs to a span annotated as non-   factual . Otherwise , the edge is assigned the label   factual . We call this variant FG -E.   4 Experimental Setup   4.1 Data   One of the main challenges in developing models   for factuality evaluation is the lack of training data .   Existing synthetic data generation approaches are   not well - suited to factuality evaluation of current   summarization models and human - annotated data   can improve factuality models ( Goyal and Durrett ,   2021 ) . In order to have a more effective training   signal , we gather human annotations from different   sources and consolidate a factuality dataset that can   be used to train FG and other models .   The source collections of the dataset are pre-   sented in Table 1 . The dataset covers two parts ,   namely CNN / DM ( Hermann et al . , 2015 ) and   XSum ( Nallapati et al . , 2016 ) . CNN / DM con-   tains news articles from two providers , CNN and   DailyMail ; while XSum contains BBC articles .   CNN / DM has considerably lower levels of abstrac-   tion , and the summary exhibits high overlap with   the article ; a typical CNN / DM summary consists   of several bullet points . In XSum , the first sentence   is removed from an article and used as a summary ,   making it highly abstractive . After we remove du-   plicated annotations , the total number of datapoints   is 9,567 , which we divide into train ( 8,667 ) , dev   ( 300 ) and test ( 600 ) sets . We call this dataset F-   C .3242   4.2 Method Details   We limit the number of considered document   graphs due to efficiency reasons . In particular , we   compute the pairwise cosine similarity between   the embeddings of each sentence d∈Dand the   summary sentence S , generated by Sentence Trans-   formers ( Reimers and Gurevych , 2019 ) . We thus   select ksentences from the source document with   the highest scores to be used to generate the docu-   ment semantic graphs .   The model weights are initialized with(electra - base discriminator , 110 M parameters ,   Clark et al . , 2020 ) , the structural adapters are pre-   trained using the release 3.0 of the AMR corpus   containing 55,635 gold annotated AMR graphs ,   and the text adapters are pretrained using synthetic   generated data . The adapters ’ hidden dimension is   32 , which corresponds to about 1.4 % of the param-   eters of the original encoders . The number   of considered document graphs ( k ) is 5.We re-   port the test results when the balanced accuracy   ( BACC ) on dev set is optimal . Following previous   work ( Kryscinski et al . , 2020 ; Goyal and Durrett ,   2021 ) , we evaluate our models using BACC and   Micro F1 scores .   5 Results and Analysis   We compare FG with different methods   for factuality evaluation : two QA - based methods ,   namely QAGS ( Wang et al . , 2020 ) and QUALS   ( Nan et al . , 2021 ) , and FCC(Kryscinski et al . ,   2020 ) . We fine - tune FCCusing the training   set , that is , it is trained on both synthetic data and   FC . We call this approach FCC+ .   Table 2 presents the results . QA - based   approaches perform comparatively worse than   FCCon CNN / DM , while QAGS has a general   better performance than QUALS .FCChas a   strong performance on CNN / DM , as it was trainedon synthetic data derived from this dataset . How-   ever , the FCC ’s performance does not transfer   to XSum . FCC+ has a large increase in per-   formance , especially on XSum , demonstrating the   importance of human - annotated data for training   improved factuality models .   FG outperforms FCC+ by 2.4   BACC points in both subsets and by 10.3 BACC in   XSum , even though FCC+ was pretrained on   millions of synthetic examples . This indicates that   considering semantic representations is beneficial   for factuality evaluation and FG can be   trained on a small number of annotated examples .   Pretraining structural adapters improves the per-   formance on CNN / DM and XSum . Finally , F-   G ’s performance further improves when both   structural and text adapters are pretrained , improv-   ing over FCC+ by 3.7 BACC points .   5.1 Correlation with Human Judgments   We also evaluate the model performance using   correlations with human judgments of factual-   ity ( Pagnoni et al . , 2021 ) . In this experiment ,   FCC+ andFG are trained with the   FC data without the Pagnoni et al .   ( 2021 ) ’s subset , which is used as dev and test sets ,   according to its split . For both models , follow-   ing Pagnoni et al . ( 2021 ) , we obtain a binary factu-   ality label for each sentence and take the average   of these labels as the final summary score . We use   the official script to calculate the correlations .   AMR and Factuality . We investigate whether   S ( Cai and Knight , 2013 ) , a metric that   measures the degree of overlap between two AMRs ,   correlates with factuality judgments . We calculate   theS score between all the summary sen-   tence graphs and kdocument sentence graphs , with   k∈ { 1,3,5 } . We obtain one score per summary3243   sentence by maxing over its scores with the sen-   tence graphs , then averaging over the summary   sentence scores to obtain the summary - level score .   We also calculate the S between the gener-   ated summary and the reference summary graphs .   As shown in Table 3 , S approaches have a   small but consistent correlation , slightly improving   over n - gram based metrics ( e.g. , METEOR and   R -L ) in CNN / DM , suggesting that AMR ,   which has a higher level of abstraction than plain   text , may be a semantic representation alternative   to content verification .   QA - based approaches have higher correlation on   the CNN / DM dataset than XSum where their corre-   lation is relatively reduced , and DAE shows higher   Spearman correlation than FCC on XSum .   FCC+ andFG , which are trained   on data from FC , have a overall higher   performance than models trained on synthetic data ,   such as FCC , again demonstrating the im-   portance of the human - annotation signal when   training factuality evaluation approaches . Finally ,   FG has the highest correlations in both   datasets , with a large improvement in XSum , sug-   gesting that representing facts as semantic graphs   is effective for more abstractive summaries .   Types of Errors . Figure 3 shows the influence   of the different types of factuality errors ( Pagnoni   et al . , 2021 ) for each approach . Semantic Frame   Errors are errors in a frame , core , and non - core   frame elements . Discourse Errors extend beyond   a single semantic frame introducing erroneous links   between discourse segments . Content Verifiability   Errors capture cases when it is not possible to ver-   ify the summary against the source document due   to the difficulty in aligning it to the source . Note   that whereas BERTS strongly correlates with   content verifiability errors as it is a token - level sim-   ilarity metric , the other methods improve in Seman-   tic Frame Errors .FG has the highest   performance suggesting that graph - based MRs are   able to capture different semantic errors well . In   particular , FG improves in capturing con-   tent verifiability errors by 48.2 % , suggesting that   representing facts using AMR is helpful .   5.2 Edge - level Factuality Classification   We assess factuality beyond sentence - level with   FG -E(§3.5 ) . We train and evaluate the   model against the sentence - level factuality data   from Maynez et al . ( 2020 ) . In this dataset , hu-   man annotations for sentence and span levels are3244   available . We derive the edge labels required for   FG -Etraining as follows : For each edge   in the summary graph , if one of the nodes con-   nected to this edge is aligned with a word that   belongs to a span labeled as non - factual , the edge   is annotated as non - factual . Summary - level labels   are obtained from edge - level predictions : if any   edge in the summary graph is classified as non-   factual , the summary is labeled as non - factual . We   use the same splits from Goyal and Durrett ( 2021 ) .   We compare FG -Ewith DAE and addi-   tionally with a sentence - level baseline ( Goyal and   Durrett , 2021 ) and FG .   Table 4 shows that the edge - level factuality clas-   sification gives better performance than sentence-   level classification , and FG performs bet-   ter in both sentence and edge classification levels .   FG -Eoutperforms DAE , demonstrating   that training on subsentence - level factuality anno-   tations enables it to accurately predict edge - level   factuality and output summary - level factuality .   Finally , while the semantic representations con-   tribute to overall performance , extracting those   representations adds some overhead in preprocess-   ing time ( and slightly more in inference time ) , as   shown in Appendix C.   5.3 Model Ablations   In Table 5 , we report an ablation study on the im-   pact of distinct FG ’s components . First ,   note that only encoding the textual information   leads to better performance than just encoding   graphs . This is expected since pretrained encoders   are known for good performance in NLP textual   tasks due to their transfer learning capabilities and   the full document text encodes more information   than the selected kdocument graphs . Moreover ,   AMR representations abstract aspects such as verb   tenses , making the graphs agnostic regarding more   fine - grained information . However , this is com-   pensated in FG , which captures coarse-   grained details from the text modality . Future work   can consider incorporating such information into   the graph representation in order to improve the   factuality assessment .   Ultimately , FG , which uses both docu-   ment and summary graphs , gives the overall best   performance , demonstrating that semantic graph   representations complement the text representation   and are beneficial for factuality evaluation .   Number of Document Graphs Table 6 shows   the influence of the number of considered docu-   ment graphs measured on FC ’s dev   set performance . Note that generally more doc-   ument graphs leads to better performance with a   peak in 5 . This suggests that using all graph sen-   tences from the source document is not required for   better performance . Moreover , the results indicate   that our strategy of selecting document graphs us-   ing the contextual representations of the document   sentences which are compared to the summary per-   forms well in practice .   We additionally present the performance of   FG with other semantic representations   in Appendix D.   5.4 Comparison to Full Fine - tuning   FG only trains adapter weights that are   placed into each layer of both text and graph en-   coders . We compare FG with a model   with similar architecture , with both text and graph   encoders , but without ( structural ) adapter layers .   We then fine - tune all the model parameters . Ta-   ble 7 shows that FG performs better even   though it trains only 1.4 % of the parameters of the   fully fine - tuned model , suggesting that the struc-   tural adapters help to adapt the graph encoder to   semantic graph representations .   5.5 Case Study   FG -Ecomputes factuality scores for   each edge of the AMR summary graph and those   predictions are aggregated to generate a sentence-3245   level label ( § 5.2 ) . Alternatively , it is possible to   identify specific inconsistencies in the generated   summary based on the AMR graph structure . This   factuality information at subsentence - level can pro-   vide deeper insights on the kinds of factual incon-   sistencies made by different summarization mod-   els ( Maynez et al . , 2020 ) and can supply text gen-   eration approaches with localized signals for train-   ing ( Cao et al . , 2020 ; Zhu et al . , 2021 ) .   Figure 4 shows a document , its generated sum-   mary , and factuality edge predictions by DAE and   FG -E.First , note that since DAE uses   dependency arcs and FG -Eis based on   AMR , the sets of edges in both approaches , that   is , the relations between nodes and hence words ,   are different . Second , both methods are able to   detect the hallucination six years , which was never   mentioned in the source document . However , DAE   does not consider that police appealed for help in   tracing is factual whereas FG -Ecaptures   it . This piece of information is related to a span in   the document with a very different but semantically   related form ( highlighted in bold in Figure 4 ) . This   poses challenges to DAE , since it classifies seman-   tic relations independently and only considers the   text surface . On the other hand , FG -E   matches the summary against the document not   only at text surface level but semantic level .   6 Conclusion   We presented FG , a graph - based ap-   proach to explicitly encode facts using meaning rep-   resentations to identify factual errors in generated   text . We provided an extensive evaluation of our   approach and showed that it significantly improves   results on different factuality benchmarks for sum-   marization , indicating that structured semantic rep-   resentations are beneficial to factuality evaluation .   Future work includes ( i ) exploring approaches to   develop document - level semantic graphs ( Naseem   et al . , 2021 ) , ( ii ) an explainable graph - based com-   ponent to highlight hallucinations and ( iii ) to com-   bine different meaning representations in order to   capture distinct semantic aspects .   Acknowledgments   We thank the anonymous reviewers for their valu-   able feedback . We also thank Shiyue Zhang , Kevin   Small , and Yang Liu for their feedback on this   work . Leonardo F. R. Ribeiro has been supported   by the German Research Foundation ( DFG ) as part   of the Research Training Group “ Adaptive Prepara-   tion of Information form Heterogeneous Sources ”   ( AIPHES , GRK 1994/1).3246Impact Statement   In this paper , we study the problem of detecting fac-   tual inconsistencies in summaries generated from   input documents . The proposed models better con-   sider the text internal meaning structure and could   benefit general generation applications by evalu-   ating their output regarding factual consistency ,   which could ensure that these systems are more   trustworthy . This work is built using semantic rep-   resentations extracted using AMR parsers . In this   way , the quality of the parser used to generate the   semantic representations can significantly impact   the results of our models . In our work , we miti-   gate this risk by employing a state - of - the - art AMR   parser ( Bevilacqua et al . , 2021 ) .   References3247324832493250   Appendices   In this supplementary material , we detail experi-   ments ’ settings , additional model evaluations and   additional information about semantic graph repre-   sentations .   A Additional Examples   Figure 6 shows examples of AMR representations   generated from summaries and salient sentences   from the respective source document .   B Details of Models and   Hyperparameters   The experiments were executed using the version   3.3.1of the transformers library released by Hug-   ging Face ( Wolf et al . , 2019 ) . In Table 8 , we   report the hyperparameters used to train F-   G . We use the Adam optimizer ( Kingma   and Ba , 2015 ) and employ a linearly decreasing   learning rate schedule without warm - up . Mean   pooling is used to calculate the final representation   of each graph .   Structural Adapters ’ Pretraining . The struc-   tural adapters are pretrained using AMR graphs   from the release 3.0 ( LDC2020T02 ) of the AMR   annotation corpus ( Knight et al . , 2020).Similarly   to the masked language modeling objective , we ex-   ecute self - supervised node - level prediction , where   we randomly mask and classify AMR nodes . The   goal of this pretraining phase is to capture domain   specific AMR knowledge by learning the regular-   ities of the node / edge attributes distributed over   graph structure .   Text Adapters ’ Pretraining . The text adapters   are pretrained using synthetically created data ,   which is generated by applying a series of rule-   based transformations to the sentences of source   documents ( Kryscinski et al . , 2020 ) . The pretrain-   ing task is to classify each summary sentence as   factual or non - factual . The goal of this pretraining   phase is to learn suitable text representations to   better identify whether summary sentences remain   factually consistent to the input document after the   transformation .   C Speed Comparison   FG encodes the structured semantic rep-   resentations that encode facts from the document   and summary . Despite their effectiveness , extract-   ing semantic graphs , such as AMR , is computa-   tionally expensive because current models employ   Transformer - based encoder - decoder architectures   based on Transformers and pretrained language   models .   In this experiment , we compare the time execu-   tion of FG -EandDAE in a sample of   1000 datapoints extracted from the XSum test set .   In order to extract the semantic graphs , we inves-   tigate two AMR parsers , Parser1 : a dual graph-   sequence parser that iteratively refines an incremen-   tally constructed graph ( Cai and Lam , 2020 ) , and   Parser2 : a linearized graph model that employs   BART ( Bevilacqua et al . , 2021 ) . The execution of   the AMR parsers is parallelized using four Tesla   V100 GPUs . We use Parser2 for the experiments   in this paper since it is the current state of the art in   AMR parsing , although it is slower in preprocess-   ing than Parser1 .   As shown in Table 9 , DAE ’s preprocess-   ing is much faster compared to this phase in   FG -E , since DAE employs a fast en-3251hanced dependency model from the Stanford   CoreNLP tool ( Manning et al . , 2014 ) . This model   builds a parse by performing a linear - time scan over   the words of a sentence . Finally , note that F-   G is slower than DAE in inference because   it employs adapters and encodes both graphs and   texts from the document and summary , whereas the   DAE model encodes only the texts .   D Comparing Semantic Representations   for Factuality Evaluation   OpenIE graph - based structures were used in or-   der to improve factuality in abstractive summariza-   tion ( Cao et al . , 2018 ) , whereas dependency arcs   were shown to be beneficial for evaluating factu-   ality ( Goyal and Durrett , 2020 ) . We thus investi-   gate different graph - based meaning representations   using FG . AMR is a more logical repre-   sentation that models relations between core con-   cepts , and has a rough alignment between nodes   and spans in the text . Conversely , dependencies   capture more fine - grained relations between words ,   and all words are mapped into nodes in the depen-   dency graph . OpenIE constructs a graph with node   descriptions similar to the original text and uses   open - domain relations , leading to relations that are   hard to compare .   As shown in Table 10 , whereas OpenIE performs   slightly better than dependency graphs , AMR gives   the best results according to the two metrics , high-   lighting the potential use of AMRs in representing   salient pieces of information . Different from our   work , Lee et al . ( 2021 ) and Naseem et al . ( 2021 )   propose a graph construction approach which gen-   erates a single document - level graph created using   the individual sentences ’ AMR graphs by merg-   ing identical concepts – this is orthogonal to our   sentence - level AMR representation and can be in-   corporated in future work .   E Semantic Representations   In Figure 5 we show AMR and dependency repre-   sentations for the summary sentence “ police have   appealed for help in tracing a woman who has been   missing for six years . ” . In § 5.5 those semantic rep-   resentations are used to predict subsentence - level   factuality using edge - level information . In partic-   ular , FG -Eemploys AMR ( Figure 5a )   whereas DAE uses dependencies ( Figure 5b).32523253