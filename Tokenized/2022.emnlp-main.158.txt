  Sneha Mondal , Ritika . , Shreya Pathak , Preethi Jyothi , Aravindan RaghuveerGoogle ResearchIIT Bombay   { snehamondal , araghuveer}@google.com ,   { ritikagoyal , shreyapathak , pjyothi}@cse.iitb.ac.in   Abstract   Code - switching has seen growing interest in   recent years as an important multilingual NLP   phenomenon . Generating code - switched text   for data augmentation has been sufﬁciently   well - explored . However , there is no prior   work on generating code - switched text with   ﬁne - grained control on the degree of code-   switching and the lexical choices used to   convey formality . We present CC , an   encoder - decoder translation model that con-   verts monolingual Hindi text to Hindi - English   code - switched text with both encoder - side   and decoder - side interventions to achieve ﬁne-   grained controllable generation . CCcan   be invoked at test - time to synthesize code-   switched text that is simultaneously faithful   to syntactic and lexical attributes relevant to   code - switching . CCoutputs were sub-   jected to rigorous subjective and objective   evaluations . Human evaluations establish that   our outputs are of superior quality while be-   ing faithful to desired attributes . We show sig-   niﬁcantly improved BLEU scores when com-   pared with human - generated code - switched   references . Compared to competitive base-   lines , we show 10 % reduction in perplexity   on a language modeling task and also demon-   strate clear improvements on a downstream   code - switched sentiment analysis task .   1 Introduction   Bilingual speakers form a signiﬁcant portion ( cur-   rent estimates of 43 % ) of the world ’s population .   To cater to the human - computer interaction needs   of this user segment , Natural Language Genera-   tion ( NLG ) and Natural Language Understanding   ( NLU ) tasks for code - switching ( CS ) are receiving   increasing amount of attention from the research   community ( Zhang et al . , 2021 ) . Code - switching in   Figure 1 : Distribution of CMI , SPI and formality scores   over CS samples in the ACS test set .   a sentence typically involves switching between a   matrix language L1 and an embedded language L2 .   A key challenge in CS that has not been previously   addressed by generation models is to explicitly con-   trol for syntactic and lexical diversity ( Do ˘gruöz   et al . , 2021 ) . Such controllable NLG models would   help build robust computational models for CS text .   We draw attention to three speciﬁc dimensions of   diversity that are commonly observed in CS text . In   this work , we aim to generate text that speciﬁcally   spans these three CS dimensions .   1 . Language Mix Ratio : The ﬁrst syntactic di-   mension of diversity refers to the varying number   of L1 and L2 words in a CS sentence that depends   on a number of factors like language pair , socio-   economic context , etc . For instance , Al - Azami   ( 2006 ) ﬁnd that among immigrant Bengalis in the   UK , ﬁrst - generation immigrants tend to use Ben-   gali and English as L1 and L2 respectively , while   the order is reversed for younger generation immi-   grants . Code - mixing index ( CMI ) ( Gambäck and   Das , 2014 ) is commonly used to quantify the ratio   of L1 vs. L2 .   2 . Language Burstiness : The second syntac-   ticdimension of diversity captures burstiness , i.e.   the length of homogenous spans of L1 and L2 ,   in CS text . This is also a function of the lan-   guages involved and social contexts . For instance ,   Czech - English speakers switch to English for high-   information content words in prominent prosodic   positions while speaking Czech ( Myslín and Levy,24662015 ) . I - index ( Guzmán et al . , 2017 ) , which we   also refer to as switch - point index ( SPI ) is popularly   used to quantify the extent of burstiness .   3 . Formality : This lexical dimension of diversity   refers to the choice of words used in a CS sentence .   A CS sentence can be constructed using either for-   mal or informal words depending on whether it is   used in a news broadcast or a social media post ,   respectively . Many studies suggest that CS patterns   and word choices are speaker - dependent ( Vu et al . ,   2013 ) and sometimes even gender - dependent ( Fin-   nis , 2014 ) .   We further substantiate the practical relevance of   the above three dimensions of diversity by analyz-   ing the test set of the ACSbenchmark ( Tarunesh   et al . , 2021a ) which consists of 2.5 K human-   generated Hindi - English sentences . We compute   CMI / SPI using counts of L1 and L2 words / spans   in CS text and formality is determined using a pre-   trained classiﬁer . Figure 1 shows signiﬁcant vari-   ation of CS text across all three dimensions , thus   offering evidence of diversity in real CS text . Fig-   ure 4 in Appendix A contains ACSexamples   that span the three dimensions of interest .   In this paper , we tackle the problem of generat-   ing CS text from monolingual text while providing   inference - time levers to control for CMI , SPI and   formality of the generated CS text . To the best of   our knowledge , there is no prior work that can gen-   erate CS text while controlling for these attributes .   For NLU models , data augmentation ( Chang et al . ,   2018 ; V olpi et al . , 2018 ; Shen et al . , 2020 ) is a   common and effective way to make them robust   to a wide variety of inputs . Therefore , it follows   that building controllable code - switched genera-   tion models would be very useful in making NLU   ( via data augmentation ) and NLG models robust to   lexical and syntactic diversity .   We propose CCforcontrollable code-   switched gener ation using a machine translation-   based model where the source and target languages   are monolingual Hindi text and Hindi - English CS   text , respectively . In practical scenarios , parallel   data with diverse vocabulary in conjunction with di-   verse attribute ( CMI , SPI , formality ) values for CS   text is not easily available . We tackle this challenge   through task - speciﬁc multi - task training objectives .   We employ encoder - side training to control for at-   tributes when parallel training data is available , and   decoder - side control via beam re - weighting whenparallel data is not available . In summary , we make   the following four key contributions :   •We introduce the problem of controllable code-   switching to enable generation of CS text with a   desired set of attributes . ( Sections 3.1 )   •We propose a modeling methodology for con-   trollable code - switched generation by using both   encoder and decoder level controls ( Section 3.2 ) .   •We demonstrate the efﬁcacy of the proposed   approach with detailed ablations and comparison   to state - of - the - art benchmarks . ( Section 4 , 5 , 6 ) .   •We release a new Hindi - English CS dataset ,   D -ACSwhich is an extension to the   ACSdataset with additional diversity of CMI   and SPI ( Section 7 ) .   2 Related Work   2.1 Diversity in Code Switching : Linguistic   Perspective   Different from conventional languages , code-   switched languages are complex , personal-   ized ( Bawa et al . , 2020 ) , evolving languages ( Prat-   apa and Choudhury , 2017 ) that depend on linguis-   tic ( Al - Azami , 2006 ) , social ( Gardner - Chloros and   Edwards , 2004 ) and economic contexts ( Mohanty ,   2006 ) . Below we discuss a few reasons that cause   CS diversity and refer the reader to recent sur-   veys ( Sitaram et al . , 2019 ; Do ˘gruöz et al . , 2021 )   for a more detailed treatment of this topic .   In a linguist ’s perspective of code-   switching , Do ˘gruöz et al . ( 2021 ) makes a   strong argument that computational linguistics ( pri-   marily via large language models ) are constrained   by the lack of available of diverse CS training data ,   evaluation benchmarks and absence of user - facing   applications . Systematically producing diverse CS   text for robustifying NLU and NLG tasks is stil   an open problem ( Do ˘gruöz et al . , 2021 ) . Though   computational models of CS exist , they fall short   in terms of coverage of the natural utterances that   a bilingual speaker would produce ( Pratapa et al . ,   2018 ) .   Despite signiﬁcant advances in NLP powered by   large pretrained language models , generating and   understanding CS text with the diversity and rich-   ness required by applications is harder compared   to tasks involving monolingual data ( Winata et al . ,   2021 ) . Do ˘gruöz et al . ( 2021 ) observe that while   benchmarks like LINCE ( Aguilar et al . , 2020 ) and   GLUECoS ( Khanuja et al . , 2020a ) serve as critical   resources for the CS research community , they do2467not yet represent the entire spectrum of CS . Specif-   ically , many tasks in the above benchmarks consist   of annotated tweets which only represent a certain   type of CS . In this paper , we address this gap by   generating diverse CS text with dimensions of di-   versity inspired from linguistic usage patterns of   CS .   2.2 Computational models for Code - switched   Generation   Prior work on generating synthetic code - switched   text has explored the use of purely generative   models ( Garg et al . , 2018 ; Samanta et al . , 2019 ;   Gao et al . , 2019 ; Chang et al . , 2019 ) , sequence-   to - sequence models that leverage parallel mono-   lingual text ( Winata et al . , 2019 , 2018 ) and trans-   duction models that translate from the matrix or   embedded language to code - switched text ( Gau-   tam et al . , 2021 ; Gupta et al . , 2021 ; Tarunesh et al . ,   2021b ) . Ours is the ﬁrst work to tackle the problem   of controllable code - switched generation .   2.3 Controlled Text Generation   Controlled text generation ( Hu et al . , 2017 ) using   large pretrained LMs is a popular subarea of NLG ,   where control is typically exercised by ﬁnetuning a   pretrained LM for an attribute(e.g . , ( Keskar et al . ,   2019 ) ) or combining the pretrained LM with at-   tribute classiﬁers or other experts ( Dathathri et al . ,   2019 ; Yang and Klein , 2021 ; Liu et al . , 2021 ) .   A popular technique for controlled translation is   to introduce a discrete tag at the source and/or   the target ( Sennrich et al . , 2016 ) . Recent work   by Schioppa et al . ( 2021 ) explored the use of addi-   tive control vectors instead of discrete tags for more   ﬁne - grained control . There is no prior work that ex-   plores controllable generation for code - switching   and this paper aims to address that gap .   3 Methodology   3.1 Problem Description   Our goal is to build a model CCthat trans-   forms a monolingual sentence into a semantically   equivalent CS sentence , while simultaneously sat-   isfying multiple CS attributes speciﬁed by the user   at test time . The three main architectural choices   in CCare :   •Model architecture : Transformer - based   encoder - decoder models are currently the ubiqui-   tous choice for neural translation ( Vaswani et al . ,2017 ) . We use a pretrained text - to - text multilingual   model mT5 ( Xue et al . , 2021 ) , as our base model .   •Enabling controlled code - switched genera-   tion : Attribute - aware generation can be achieved   via both encoder and decoder - side controls .   Encoder - side controls assume the availability of   ( at least a small amount of ) parallel monolingual to   CS text with attribute annotations that are further   used to train the model . In contrast , decoder - side   controls are applied only during the decoding step   and hence do not require any training - time inter-   vention . Either approach can be adopted depending   on the type of annotated data that is available .   •Multi - task training : We introduce a multi-   task training objective that involves jointly train-   ing two auxiliary translation tasks from English to   Hindi , and Hindi to English , along with the primary   translation task of Hindi to Hindi - English . For the   primary task , 20 % of the source Hindi tokens are   masked . This masking step dramatically reduces   the model ’s propensity to copy source tokens , and   was found to be critical for performance especially   when using decoder - side controls .   In this work , we aim at controlling three CS at-   tributes : CMI , SPI and formality . CMI and SPI   values can be deterministically computed for a   CS sentence . CMI for a CS sentence of length   nwithη , ηtokens in L1 and L2 , respectively ( i.e.   n = η+η ) , is written as 1−. Low   CMI values indicate monolingualism and the high-   est CMI value of 0.5 indicates an equal number of   L1 and L2 tokens in the sentence . SPI is computed   as , whereS(i , i+ 1 ) is 1 if tokens i   andi+ 1belong to different languages , and is 0   otherwise . SPI varies between 0 ( monolingual ut-   terance ) and 1 ( consecutive tokens coming from L1   and L2 ) . CMI and SPI are dependent attributes . A   low CMI score typically results in a low SPI score ,   since low code - switching implies fewer switches   between L1 and L2 . In contrast , a high CMI score   can be attained with either a low SPI score ( longer   language spans , less interleaving ) or a high SPI   score ( short language spans , frequent interleaving ) .   Unlike CMI and SPI that are deterministic func-   tions of the CS text , formality values are obtained   using a binary classiﬁer trained to detect formality   in English text . More details about ﬁnetuning the   formality classiﬁer to make it more amenable to   CS are in Appendix B.2.2468   3.2 Our Approach   CCscaffolds on a Transformer - based trans-   lation model consisting of multi - layered encoder   and decoder networks . The encoder Econverts   a monolingual sentence X={x}into a se-   quence of higher - level representations denoted by   Z={z},z∈Rthat is further passed to   the decoderD.Dautoregressively converts the   encoded representations Zinto the target CS se-   quenceY={y}one token at a time . CC   additionally provides encoder - side and decoder-   side controls to the base Transformer model to   support ﬁne - grained CS generation . Figure 2 il-   lustrates the main components of CC .   Encoder - side Control : A commonly adopted   technique for encoder - side control is to introduce   an explicit attribute tag at the start of the source   sentence as an input to the model ( Kobus et al . ,   2016 ; Sennrich et al . , 2016 ) . The main limitation   of tag - based control is in dealing with continuous-   valued attributes that have to be discretized into   bins . Keeping the number of bins too small might   result in coarse characterizations of the attribute ,   while increasing the number of bins could lead to   limited data being available for each bin . A more   natural handling of continuous - valued attributes   is to directly learn a vector embedding for eachattribute that can be added to the encoder represen-   tations .   Motivated by Schioppa et al . ( 2021 ) , we deﬁne   an attribute vector V∈Rfor each CS attribute a.   EachVis scaled with a weight wthat is equal to   the attribute value . Multiple attributes are handled   using a linear combination of the attribute vectors ,   V=/summationtextwV.Vis added to each encoder repre-   sentationzfromEbefore being passed as input   toD. Thus , znow becomes z+V. The attribute   vectors are learned during training using the end-   to - end translation objective . Parallel text without   attribute annotations can still be used during train-   ing by setting Vto0 .   Decoder - side Control : Decoder - side control is   desirable when we want to control for attributes   like formality for which we do not have parallel   text . Towards this , we borrow the idea of condition-   ing an existing autoregressive model on a desired   attribute from the F framework ( Yang and   Klein , 2021 ) .   F aims to alter the output probabilities   from a trained decoder with the help of an attribute   classiﬁer . The Transformer - based decoder autore-   gressively models the conditional probability dis-   tribution of a CS token at time - step jgiven the   entire monolingual sequence , i.e. , P(y|y , X ) .   If we want to additionally condition the decoder on   an attributea , the required probability distribution   would become :   P(y|y , X , a ) ∝P(a|y , X)P(y|y , X )   ≈P(a|y)P(y|y , X)(1 )   We assume conditional independence between the   attribute and the monolingual input sentence , given   the preﬁx of the output sentence . P(y|y , X )   from Equation 1 is already modeled by the   Transformer - based decoder . P(a|y)can be es-   timated using a binary classiﬁer for agiven the   preﬁxy . The binary classiﬁer Cpredicts whether   the preﬁxy , when expanded to completion , will   satisfy attribute aor not . During decoding , new   probabilities for each output word yare obtained   by multiplying and re - normalizing the probability   distributions shown in Equation 1 .   4 Experiments and Results   4.1 Evaluation Metrics   Semantic consistency with the source monolingual   sentence is computed using BLEU scores ( Papineni2469et al . , 2002 ) between a reference CS sentence and   the generated sentence . Attribute faithfulness for   CMI is measured using two metrics : CMIand   CMI.CMImeasures the percentage of test   instances where the binned CMI score of the gener-   ated CS sentence exactly matches the binned CMI   score of the reference CS sentence . CMImea-   sures Pearson correlation between real - valued CMI   scores of generated CS sentences and reference CS   sentences . Analogously , we can deﬁne SPIand   SPI . Unless speciﬁed otherwise , we use 3 bins   to compute CMIand 2 bins to compute SPI .   4.2 Model Architecture and Implementation   Details   For all our experiments , we start with the publicly   available mt5 - small checkpoint , which is further   ﬁnetuned on task - speciﬁc training data . We use   the AdamW optimizer with a constant learning   rate of 5e-4 . All our models are trained on a sin-   gle NVIDIA A100 GPU . We use the harmonic   mean of BLEU and attribute correlation ( CMI   and/or SPI ) as the checkpoint selection crite-   rion to ensure that the generated outputs are both   attribute - preserving and semantically meaningful .   We also implement the multi - task training objective   of monolingual and masked CS translation that was   described in Section 3.1 . More implementation   details appear in Appendix B.1 .   For decoder - side control , we use the formality   adapterby Krishna et al . ( 2020 ) , stacked on an   XLM - R Hindi language adapterby Pfeiffer et al .   ( 2020 ) . Recall that our approach requires the clas-   siﬁer to act on sentence preﬁxes at every decoding   step , instead of complete sentences . To this end ,   we ﬁnetune the formality classiﬁer on preﬁxes of   formal and informal Hindi sentences . More details   of the ﬁnetuning dataset are in Appendix B.2 .   4.3 Datasets   For parallel Hindi to Hindi - English text , we use   theACScorpus ( Tarunesh et al . , 2021b ) that   consists of around 21K,1 K , and 2.5 K samples in   training , dev , and test splits respectively . The do-   main is largely conversational as many of the CS   sentences are extracted from movie scripts . We   use the IIT Bombay Hindi - English parallel cor-   pus ( Kunchukuttan et al . , 2017 ) for multi - task train-   ing .   4.4 Encoder - side Control   4.4.1 Single and Multi - Attribute Control   Table 1 details the performance of CCin var-   ious control settings . Going from CCto   CC , we see substantial gains in CMIand   CMI . A similar gain in SPIandSPIis   seen when we compare CCwith CC .   Controlling for both CMI and SPI simultaneously   ( c.f . , last two rows in Table 1 ) leads to a signiﬁcant   boost in BLEU scores along with maintaining high   CMI / SPI correlations . Multi - task training leads   to a clear improvement in BLEU scores , but does   not signiﬁcantly vary performance on the two at-   tributes .   To understand how our model performs with   using smaller amounts of CS training data , we   train CCon a random half of ACS . Even   with using only 50 % ofACSduring training ,   we achieve close to 90 % on all evaluation metrics   when compared to using ACSin full ( BLEU of   60.11 ; CMI , CMI , SPIandSPIvalues   of 0.74 , 0.82 , 0.79 and 0.84 , respectively ) . This   shows that CCis able to generate high - quality   and diverse CS sentences even when trained on   fairly limited amounts of CS data .   4.4.2 Human Evaluation   We sampled 500 Hindi sentences from ACStest   and compared CS outputs produced by different   CCvariants , a competitive baseline ( called   TCS ) by Tarunesh et al . ( 2021b ) , and human ref-   erences . Three human raters with native ﬂuency   in Hindi and English assessed the model outputs   along two dimensions : 1 ) naturalness indicating2470   whether the code - switching is natural , and 2 ) se-   mantic consistency , measuring how well the model   output preserves the meaning of the monolingual   Hindi sentence . Each dimension was rated using   one of three quality scores shown in Table 2 . Ta-   ble 2 also lists a few automated metrics , including   “ verbatim copy " that refers to the percentage of   model outputs that are an exact copy of the Hindi   source and “ source BLEU " which is the BLEU   score between the generated output and the Hindi   source .   AllCCvariants perform signiﬁcantly better   than TCS on both naturalness and semantic con-   sistency scores . CMIandSPIincrease sub-   stantially when using controlled CCvariants   compared to CC . Among all the models ,   CC has the least propensity to copy , as   evidenced by the lowest scores of verbatim copying   and source - BLEU . Moreover , CC attains   naturalness and semantic consistency scores very   close to CC , despite far less copying , which   is truly indicative of superior code - switching .   4.5 Decoder - side Control   We sampled 500 monolingual sentences from the   ACStest split and generated two model out-   puts with and without decoder - side control for for-   mality . Raters were shown both outputs , and   asked to rate if one was more formal , less for-   mal , or equally formal compared to the other . Theformality - controlled outputs were also evaluated   for naturalness and semantic consistency , as in Sec-   tion 4.4.2 . From Table 3 , we see that for all mod-   els a signiﬁcant number of formality - controlled   outputs are marked as more formal than their no-   control counterparts . Comparing CCand   CC , we ﬁnd that multi - task training dra-   matically improves naturalness and semantic con-   sistency of generated outputs while simultaneously   making them more formal . The ability of the model   to generate more formal outputs drops a little when   also controlling for CMI . However , interestingly ,   the encoder - side control for CMI makes the model   more robust to decoder - side interventions , as ev-   idenced by the higher naturalness and semantic   consistency scores in the last row compared to the   previous two rows .   5 Ablations and Model Variants   Comparison with Tagging . Figure 3b shows how   tagging compares to the scaled vector approach for   encoder - side control . On increasing the number of   bins ( k ) from 3 to 8 for tagging , CMIincreases   from 0.77 to 0.89 , making it more comparable to   theCMIof 0.92 using the scaled approach . As   we increase the number of bins , k - class CMI   drops for both the tagging and scaled vector ap-   proaches , with the latter showing a small but con-   sistent improvement across bins .   Interpolation to Unseen Attributes . The tagging   technique will not be able to generalize to CMI / SPI   bins that were never seen during training . In con-2471   trast , the scaled vector technique could potentially   interpolate to unseen CMI / SPI values . We test this   out by holding out speciﬁc CMI bins and evaluating   performance on all three bins in Table 4 . As ex-   pected , the performance on the held - out bin drops   when considering each bin , but only by fairly small   margins . This validates our claim that the scaled   technique is able to successfully interpolate to un-   seen attribute values . A similar analysis is done for   SPI in Table 10 in Appendix C.   Freezing Style Vector . Is it important to learn the   attribute vector or will performance be as good   with an attribute vector whose weights are frozen ?   Table 5 shows that while there is a small gain in   performance with using a trained attribute vector ,   most of the model ’s performance can be attributed   to the decoder effectively learning how to scale the   attribute vector regardless of its actual values .   6 Downstream Tasks   6.1 Language Model Perplexity   We train language models ( LMs ) on text from   CCand other baseline approaches and evalu-   ate perplexity on two different test sets contain-   ing real code - switched data . We start with theXLM - RoBERTa checkpointand ﬁnetune it on dif-   ferent synthetic datasets with a causal LM loss ,   using the script provided by Huggingface .   CC is compared against two prior   techniques - TCS ( Tarunesh et al . , 2021b ) and   GCM ( Rizvi et al . , 2021 ) . 100 K real monolingual   sentences are sampled from the IIT - B Hindi corpus   and used to generate 200 K synthetic CS sentences   from each technique . Note that CC re-   quires both CMI and SPI values to be provided   during inference . We experiment with two sam-   pling schemes for assigning CMI and SPI scores to   monolingual sentences :   1.Random : Each sentence is assigned a CMI   score uniformly sampled from ( 0,0.5]and an   SPI score uniformly sampled from ( 0,1 ] . This   scheme does not account for the relationship   between CMI and SPI .   2.Discretized : Each sentence is ﬁrst as-   signed a CMI score uniformly sampled from   { 1 / n,2 / n, ... ,⌈n/2⌉/n } , wherenis the   number of tokens in the monolingual sentence .   If the sampled CMI score is less than or equal   to 0.33 , the SPI is uniformly sampled between   ( 0,0.6 ] ; otherwise it is uniformly sampled be-   tween ( 0,1 ] . This scheme ensures that the   assigned CMI score is meaningful for a sen-   tence of length n. It also accounts for the   relationship between CMI and SPI by enforc-   ing that low CMI scores co - occur only with   low SPI scores , while high CMI scores can   co - occur with either low or high SPI scores .   Table 6 compares perplexities on the test split of   ACS.CCmodels yield the lowest perplex-   ity among all synthetic data generation methods ,   and the discretized scheme attains a lower score   compared to the random scheme . The synthetic CS   data is derived starting from the IIT - B Hindi corpus .   LMs trained on this synthetic data are evaluated   on an out - of - domain test set , and hence do not out-   perform the LM trained on ACS - train ( that is   distributionally similar to ACS - test ) . Table 7   shows how the trend reverses when evaluating per-   plexities on a new test set from HinGE ( Srivastava   and Singh , 2021 ) . The LM trained on synthetic2472   Train set Test perplexity↓   CC 647.65±0.29   CC 613.78±0.53   ACS Train-50pc 794.38   ACS Train 637.41   data from CC yields a 3.5%improve-   ment over the LM trained on ACS . We also train   an LM on 50 % ofACSand another on synthetic   sentences from CC trained on half   ofACS . We observe only a small degradation   in test perplexity when CS training data is halved   forCC(c.f . , ﬁrst two rows in Table 7 ) . The   LM trained on 50 % ofACSsees a large drop   in performance ( 794.38 ) , while CCtrained on   this subset is still able to generate diverse samples   with little loss in quality ( 647.65 ) .   6.2 Code - switched Sentiment Analysis   We show the effect of pretraining with synthetic   text from CCon a downstream task with CS   inputs . We select sentiment analysis from the   GLUECoS benchmark ( Khanuja et al . , 2020b ) con-   sisting of roughly 13 K , 3 K and 1 K training / dev / test   instances . We use the same sets of 200 K syn-   thetic samples described earlier for TCS , GCM   and CC . We pretrain an mBERT   model ( Pires et al . , 2019 ) on this synthetic text   followed by ﬁnetuning on the sentiment analysis   data . We identify the best model via the dev set   and report max / mean F1 scores on the test set com-   puted over ﬁve random seeds . Table 8 shows that   CCperforms better than both TCS and GCM ,   and the discretized sampling improves slightly over   the random sampling .   7 The Diverse All - CS Dataset   We release a new dataset D -ACSwhich   is an extension of ACSwith more natural diver-   sity in CMI and SPI.We select 1928 sentences   from ACS - test with a sentence length between 5   and 15 tokens . To mimic the natural distribution of   CMI / SPI scores , we refer to ACS - train and di-   vide CMI / SPI values into two bins , resulting in four   CMI , SPI combinations . We use CC to   generate model outputs and sample proportional to   the number of ACSsentences assigned to each   CMI / SPI combination . After removing duplicates ,   we have 5380 unique CS sentences that were sub-   jected to human evaluations . Table 9 shows the   distribution of scores for naturalness and semantic   consistency . We release the subset of data that is at   least “ Somewhat Natural " and scored “ Somewhat   well " or higher on semantic consistency .   8 Discussion and Conclusion   We list a few observations from our experiments .   1 . We ﬁnd that decoder - side control is more brittle   and requires interventions like multi - task training   to improve on naturalness and consistency of the   generated outputs . Encoder - side control , in com-   parison , is more robust . The trade - off is that the   latter requires parallel data that is more tedious   to acquire , while decoder - side control does not .   2 . The tagged approach nears the scaled vector   approach in performance on increasing the num-   ber of bins to 8 . However , the scaled vector has   the advantage of interpolating to nearby CMI / SPI   values without having seen them explicitly during2473training . 3 . CMI / SPI could also be controlled on   the decoder side . However , our current method of   classiﬁer - based reweighting would not work since   a classiﬁer will not be able to meaningfully pre-   dict CMI / SPI , which are length - dependent values ,   based on only a sentence preﬁx .   In conclusion , we establish CCas a state - of-   the - art controllable code - switched generation tool   with test - time controls that outperforms existing   unconstrained baselines .   9 Limitations   Our method is currently evaluated on a single lan-   guage pair . The main bottleneck that prevented   us from evaluating on a second language pair was   the lack of a high - quality dataset like ACS . We   believe our model will be able to generalize to a   different language pair given similar amounts of   resources of comparable quality . We fully intend   to test this out as future work after creating such re-   sources for other languages and publicly releasing   them to the community for further use . It would   also be interesting to investigate how to work with   larger but noisier code - switched text mined from   social media text and whether such resources could   be used to supplement a resource like ACSof   much smaller size .   Another limitation relates to the model itself .   CCis an encoder - decoder model and inherits   the limitations associated with such models , the   main one being that the model has difﬁculty scaling   to long sentences .   10 Ethical Considerations   As with all language generation models , the gen-   erated outputs are a function of the data that was   used to train the model . One should be cognizant of   this when deploying such a model in applications   that could beneﬁt from controllable code - switching ,   e.g. , chatbots .   11 Acknowledgements   The fourth author would like to thank Google Re-   search India for the compute support and the grant   towards research on models for code - switching .   We are very grateful to the Task Mate team ( espe-   cially Auric Bonifacio Quintana ) for their inputs on   framing annotation guidelines and helping us scale   human evals on their platform . We would also like   to thank Abhirut Gupta from Google Research foruseful discussions on training multilingual text - to-   text models .   References247424752476A Diverse Examples From ACS   Figure 4 contains real CS utterances from the   ACStest set , with varying scores across the   three diversity dimensions of interest . First two   rows in this table illustrate that a low CMI score co-   occurs typically with low SPI scores . In contrast , a   high CMI score can co - occur with either a low or a   high SPI score , as seen in the last two rows of the   table .   B Implementation Details   B.1 Multi - Task Objective   To implement the multi - task objective , we proceed   as follows . First , we create a monolingual transla-   tion corpus that consists of 800 K parallel samples   randomly selected from the IIT Bombay Hindi-   English corpus . Next , we create a masked CS   translation corpus from the ACStrain split by   masking each source Hindi token independently   with a probability of 0.2 . We then make 10 copies   of this masked CS translation corpus , and merge   it with the monolingual translation corpus . Up-   sampling CS training data in this manner reduces   the skew between the size of monolingual transla-   tion ( 800 K ) and CS translation corpus ( ≈21 K ) . We   train for 3 epochs on this merged dataset , with a   task - speciﬁc preﬁx indicating whether the model   should perform monolingual or CS translation for   the current instance . Metrics are evaluated on the   ACSdev split every 500 steps . Checkpoint se-   lection criterion is the harmonic mean of BLEU   and attribute correlation of the attribute(s ) of inter-   est . Metrics from the best checkpoint are reported   on the ACS test split .   B.2 Formality Classiﬁer   For decoder - side control , we use the formality   task adapter open - sourced by Krishna et al . ( 2020 )   stacked on a Hindi language adapter by Pfeiffer   et al . ( 2020 ) . This formality classiﬁer is trained   on English sentences from the GYAFC formality   classiﬁcation dataset released by Rao and Tetreault   ( 2018 ) . To ﬁnetune it on Hindi preﬁxes , we require   a similar Hindi formality classiﬁcation dataset . To   the best of our knowledge , no such dataset is pub-   licly available , therefore we attempt to build a syn-   thetic dataset for our purpose . We sample 500 sentences from the BBC Hindi   News datasetand mark them “ formal ” . This   dataset contains news documents from 14 unique   categories , including Science , International and   Business . Similarly we sample 500 monolin-   gual Hindi sentences from the Movie - CS split of   ACStrain , and mark them “ informal ” . The   Movie - CS split contains subtitles from 30 con-   temporary Bollywood movies , and is therefore as-   sumed informal . Our formality classiﬁer is ﬁne-   tuned on all preﬁxes of length greater than 3 tokens ,   derived from this dataset .   We ﬁnetune only the task adapter , keeping the   language adapter and base model weights frozen .   We used the ofﬁcial training script provided by   AdapterHub . Training was done for 2 epochs ,   with a constant learning rate of 5e-6 .   C Interpolation to Unseen SPI   Similar to the set up in Section 5 , we hold out   speciﬁc SPI bins during training , and evaluate if the   scaled vector approach can successfully interpolate   to the unseen bin . Results from this analysis are   reported in Table 10 . While the performance on   the held - out bin does drop , it is by fairly small   margins , thereby establishing the generalizability   of the scaled vector approach .   D Human Evaluations   To evaluate CCmodel outputs , we created a   template that asks linguistic experts to assess model   outputs on naturalness , semantic consistency and   formality .   Every CS sentence was evaluated along each of   these dimensions by 3 human raters . Annotations   for a single sentence were aggregated according   to a majority vote among raters . There were no   instances where a majority vote could not be com-   puted . Numbers reported in Tables 2 and 3 are   aggregates reported over a set of 500 instances,2477   sampled randomly from the ACStest set . In the   following sections , we provide exact annotation   instructions , as well as relevant details about the   rater pool .   D.1 Naturalness   Raters were shown a monolingual Hindi sentence ,   its machine - generated code - switched counterpart ,   and asked How natural does the mixed language   sentence sound ? They could select one of three   possible options -   Very natural - The mixed language sentence   sounds natural . This is how a bilingual speaker   might speak . The ﬂow between English and Hindi   is spontaneous . The sentence sounds grammati-   cally correct .   Somewhat natural - There may be some awk-   ward construction or minor spelling / grammar er-   rors . Someone who is just learning to speak Hindi ,   or a non - native speaker may communicate this way .   Not at all natural - The mixed language sen-   tence sounds awkward and artiﬁcial . It mixes En-   glish and Hindi words in a way that is not actually   spoken or written . As a result , the grammar of the   mixed - language sentence is incorrect .   D.2 Semantic Consistency   Raters were shown a monolingual Hindi sentence ,   its machine - generated code - switched counterpart , and asked How well does the mixed - language sen-   tence capture the meaning of the Hindi sentence ?   They could select one of three possible options -   Very well - Both sentences are exactly identical   in meaning .   Somewhat well - There is some meaning over-   lap between the Hindi and mixed - language sen-   tence , but they are not identical . The mixed sen-   tence adds or drops some critical component of   meaning from the Hindi sentence .   Not at all - The mixed sentence has no over-   lap with the Hindi sentence ; the two are entirely   different .   D.3 Formality   Raters were shown a monolingual Hindi sentence ,   two machine generated code - switched outputs , and   asked Which among the code - switched senteces is   more formal ? Annotation instructions speciﬁed   that formal language was more likely to be seen   while reporting , talking to seniors or colleagues ,   and in polite conversation .   D.4 Rater Pool   Evaluations were performed by bilingual raters   from India , who are ﬂuent in Hindi and English .   Every rater either held or was working towards a   Diploma . Raters were paid USD 0.10 for a com-   pleted task , where a task involves evaluating one2478CS text along all three dimensions.2479