  Pengwei Zhan , Jing Yang , Xiao Huang , Chunlei Jing , Jingying Li , Liming WangInstitute of Information Engineering , Chinese Academy of Sciences , Beijing , ChinaSchool of Cyber Security , University of Chinese Academy of Sciences , Beijing , China   { zhanpengwei,yangjing,huangxiao}@iie.ac.cn   { jingchunlei,lijingying,wangliming}@iie.ac.cn   Abstract   Neural language models have achieved supe-   rior performance . However , these models   also suffer from the pathology of overconﬁ-   dence in the out - of - distribution examples , po-   tentially making the model difﬁcult to inter-   pret and making the interpretation methods   fail to provide faithful attributions . In this   paper , we explain the model pathology from   the view of sentence representation and argue   that the counter - intuitive bias degree and direc-   tion of the out - of - distribution examples ’ rep-   resentation cause the pathology . We propose   aContrastive learning regularization method   using Adversarial examples for Alleviating   thePathology ( ConAAP ) , which calibrates the   sentence representation of out - of - distribution   examples . ConAAP generates positive and   negative examples following the attribution   results and utilizes adversarial examples to   introduce direction information in regulariza-   tion . Experiments show that ConAAP effec-   tively alleviates the model pathology while   slightly impacting the generalization ability on   in - distribution examples and thus helps inter-   pretation methods obtain more faithful results .   1 Introduction   Neural language models have achieved superior   performance in various natural language process-   ing ( NLP ) domains and are used in many ﬁelds to   accomplish critical tasks , such as toxic comment   classiﬁcation and rumor detection . However , the   drawbacks of NLP models in test - time interpretabil-   ity pose potential risks to these tasks , as existing   interpretation methods always fail to obtain faith-   ful attributions on these models , thereby failing to   reveal potential ﬂaws and biases .   Following Ribeiro et al . ( 2016 ) , Schwab and   Karlen ( 2019 ) , and Situ et al . ( 2021 ) , the attribu-   tion obtained by a faithful interpretation method   should indicate the real contribution of features inFigure 1 : Conﬁdence distribution comparison between   BERT tuned with normal method and ConAAP . We re-   move words of different importance on normal exam-   ples in testing set ( operation is detailed in § 3.3 ) . The   attribution is obtained by gradient - based method ( § 3.2 ) .   The normally tuned model is pathological , as the con-   ﬁdence distribution after removing important words is   similar to after removing unimportant words , indicat-   ing that the interpretation method can not obtain faith-   ful attributions . The model tuned with ConAAP is non-   pathological , as the model can discriminate between   the important and unimportant words in terms of conﬁ-   dence changing , and the attributions are more faithful .   terms of model conﬁdence changing . Speciﬁcally ,   the important words marked by a faithful attribu-   tion should contribute most to the model prediction ,   and masking them out from the sentence should   greatly decrease model conﬁdence . Conversely ,   unimportant words should have little impact on pre-   diction and conﬁdence . However , abnormal model   behaviors have been widely reported in previous   works . For example , Goodfellow et al . ( 2015 ) il-   lustrate that a well - trained model will sometime   predict pure noise rubbish examples , which should   contain only the unimportant features , with high   conﬁdence . Feng et al . ( 2018 ) also ﬁnd that model   tends to predict meaningless examples with tokens   removed with higher conﬁdence than normal exam-   ples . We also demonstrate similar abnormal behav-   ior and the unfaithfulness of attribution by showing   the conﬁdence distribution on the Movie Review6493(MR ) testing set ( Pang and Lee , 2005 ) of the basic   version BERT ( Devlin et al . , 2019 ) ﬁne - tuned on   MR training set in Figure 1 .   According to Guo et al . ( 2017 ) and Feng et al .   ( 2018 ) , model pathology is a major reason for   these abnormal behaviors . They argue that neu-   ral language models are overconﬁdent in their   prediction as the model overﬁts the negative log-   likelihood loss to produce low - entropy distribution   over classes . Thus the model will also be over-   conﬁdent in examples outside the distribution of   training instances , leading to the counter - intuitive   model conﬁdence in these abnormal behaviors . Em-   pirically , Feng et al . ( 2018 ) also demonstrate the   explanation by mitigating the pathology with an en-   tropy regularization that maximizes the uncertainty   on out - of - distribution examples . Following their   ﬁndings , we argue that the interpretation method   fails to provide faithful results is mainly due to the   drawback of models rather than the drawback of the   interpretation method itself , i.e. , the unfaithfulness   of attribution is due to the model pathology .   In this paper , we explain the model pathology ,   which potentially makes the model difﬁcult to in-   terpret , from the view of sentence representation ,   and intuitively show how thepathol ogyleads to   unfaithfulness attributionandhow toalleviatethe   pathol ogyeffectively . Based on our ﬁndings , we   also propose a Contrastive learning regularization   method using Adversarial examples for Alleviating   thePathology ( ConAAP ) . We summarize our main   contributions as follows :   1.We explain the model pathology and how it   causes the unfaithfulness attribution from the   view of sentence representation . We argue   that the counter - intuitive bias degree andbias   direction of the out - of - distribution examples   are two key factors leading to the pathology .   2.We propose ConAAP , a contrastive learning   regularization method that calibrates the sen-   tence representation of out - of - distribution ex-   amples . ConAAP generates positive and nega-   tive examples following the attribution results   and utilizes adversarial examples to introduce   direction information in regularization .   3.Experiments show that ConAAP effectively   alleviates the model pathology while slightly   impacting the generalization ability on in-   distribution examples and thus helps interpre-   tation methods obtain more faithful results.2 Related Work   Interpreting the Language Model . To interpret   a language model , previous works utilize the   gradient - based method ( Li et al . , 2016 ; Sundarara-   jan et al . , 2017 ; Ross et al . , 2017 ; Zhan et al . , 2022a ;   Feng et al . , 2018 ; DeYoung et al . , 2020 ) , atten-   tion scores ( Bahdanau et al . , 2015 ; Luong et al . ,   2015 ; Vaswani et al . , 2017 ) , Occlusion ( Gao et al . ,   2018 ; Li et al . , 2019 ; Jin et al . , 2020 ; Zhan et al . ,   2022b ; Li et al . , 2020 ) , and Shapley values ( Lund-   berg and Lee , 2017 ) to attribute the model predic-   tion . To quantitatively evaluate the faithfulness of   the obtained attribution , metrics including Reduced   Length ( Feng et al . , 2018 ) , Comprehensiveness ,   Sufﬁciency , and Area Over the Perturbation Curve   ( AOPC ) ( DeYoung et al . , 2020 ) are proposed .   Contrastive Learning . Encouraged by the re-   markable success of contrastive learning in com-   puter vision ( CV ) in learning better representa-   tion and improving performance on downstream   tasks ( Chen et al . , 2020b , a ; Pan et al . , 2021 ) , var-   ious methods have been proposed for NLP tasks .   Limited by the discrete nature of text , instead of   generating contrastive pairs by cropping , resizing ,   and rotating the input like in CV tasks , previous   works in NLP are always by back - translating , word   deleting , reordering , and substituting ( Giorgi et al . ,   2021 ; Wu et al . , 2020 ; Gao et al . , 2021 ) . It is shown   that contrastive learning helps improve sentence   representation and model performance on down-   stream NLP tasks . However , few works focus on   model pathology and interpretability .   Adversarial Examples in Contrastive Learning .   It is found that using adversarial examples , which   can fool the model while being imperceptible to   humans ( Gao et al . , 2018 ; Li et al . , 2019 ; Jin   et al . , 2020 ; Li et al . , 2020 ) , in contrastive learn-   ing , can produce better sentence representations   and increase downstream performance . However ,   previous works always utilize adversarial examples   as challenging examples and focus on the model   robustness and performance ( Kim et al . , 2020 ; Ho   and Vasconcelos , 2020 ; Meng et al . , 2021 ) rather   than the model pathology and interpretability .   3 Method   3.1 Preliminaries   Given a data distribution Dover input text X∈X   and output labels Y∈Y = { 1, ... ,C } , a model6494f : X→Y maps the input text to the output soft-   max probability , which is trained by minimizing   the empirical riskL(X , Y;θ)that equals to   E[−logexp(wr(X))/summationtextexp(wr(X))](1 )   whereWis the classiﬁcation parameters , w∈W   denotes the classiﬁcation parameters toward class   Y , θis the model parameters , and r(·)denotes   the sentence representation of input text . Speciﬁ-   cally , in classiﬁcation tasks , BERT always uses the   value of [ CLS ] token as representation , while other   models , including LSTM and CNN , always use   the average token embedding before the last dense   layer . After training , the model correctly classiﬁes   text based on the posterior probability :   P(Y|X ) = exp(wr(X))/summationtextexp(wr(X))(2 )   wherewdenotes the classiﬁcation parameters   toward the ground - truth class Y. This value is   always regarded as the conﬁdence in prediction .   3.2 Faithful Attribution   In this paper , we use the gradient - based method as   the basic interpretation method to obtain attribution ,   which is formally deﬁned as follows :   Attr(X ) = /parenleftbigg / vextenddouble / vextenddouble / vextenddouble / vextenddouble∂wr(X )   ∂emb(x)/vextenddouble / vextenddouble / vextenddouble / vextenddouble / parenrightbigg(3 )   whereX = xx ... xis a normal sentence ,   emb(·)denotes the word embedding . To measure   the faithfulness of the obtained attribution , previous   works always measure the inﬂuence of words of dif-   ferent importance on model conﬁdence . We use the   Area Over the Perturbation Curve ( AOPC ) form   of Comprehensiveness ( Comp . ) and Sufﬁciency   ( Suff . ) metrics ( DeYoung et al . , 2020 ; Samek et al . ,   2017 ; Nguyen , 2018 ) to measure the faithfulness .   AOPC is formulated as(4 )   andAOPCis formulated as(5 )   wheret(·)means remove the kmost important   words in a sentence according to attribution , whilet(·)means remove the kleast important words ,   Kindicates the range of words to be considered .   If attribution is faithful , it is expected to have a   high AOPC value and a low AOPCvalue ,   indicating that the information in the important   words has an overall larger impact on prediction   than in unimportant words .   3.3 Model Pathology From the View of   Sentence Representation   In this section , we explain the model pathology   from the view of sentence representation and try to   answer how does thepathol ogylead tounfaithful-   ness attribution ?   Feng et al . ( 2018 ) propose an analysis method   called input reduction , which iteratively calculates   the attribution and removes the least important   word in a sentence . By analyzing the model conﬁ-   dence change on the incomplete sentence , they ﬁnd   that when the reduced examples are nonsensical for   humans and lack information for supporting the pre-   diction , the models still make the same prediction   as the original sentence with high conﬁdence . The   counter - intuitive high conﬁdence is attributed to the   model overconﬁdence in such out - of - distribution   examples .   To make the analysis process more compatible   with the calculation of faithfulness ( 4 ) ( 5 ) , we use   a variant reduction method to generate incomplete   out - of - distribution examples rather than the one   proposed by Feng et al . ( 2018 ) . Speciﬁcally , given   a sentence and a well - trained model , we ﬁrst obtain   the attribution of the sentence according to ( 3 ) , and   then cumulatively remove the words in the sentence .   We remove not only the unimportant words but also   the important words . For the important words , we   cumulatively remove 50 % of words in descending   order of the attribution . For the unimportant words ,   we cumulatively remove 50 % of words in ascend-   ing order of the attribution . Additionally , we gen-   erate adversarial example , which is imperceptible   to humans and can mislead the model prediction ,   from the given normal sentence with PWWS ( Ren   et al . , 2019 ) . Therefore , we have four kinds of   examples : ( i ) the in - distribution normal example ,   ( ii ) the out - of - distribution examples with important   words removed , ( iii ) the out - of - distribution exam-   ples with unimportant words removed , and ( iv ) the   adversarial example located on the other side and   in the vicinity of the decision boundary .   Following these operations , we ﬁne - tune a basic6495   BERT on MR training set and obtain the sentence   representations of the four kinds of examples de-   rived from the MR testing set instances . We then   project the representations to a two - dimensional   space with t - SNE ( van der Maaten and Hinton ,   2008 ) . The visualization of the sentence represen-   tation of three MR instances and their attributions   according to ( 3 ) are shown in Figure 2 . We can   summarize some counter - intuitive phenomena .   Observation 1 : When themost importantfew   words areremoved , therepresentations ofsuch   incomplete out - of - distribution examples arestill   very close totheoriginalsentence . Intuitively , the   most important few words should contain the most   signiﬁcant information for supporting the predic-   tion . Losing this information , the model conﬁ-   dence should decrease , and the representation of   such incomplete sentences should be close to the   adversarial example , which is located on the other   side and in the vicinity of the decision boundary .   Focusing on instance 1 / circlecopyrt , when the three most im-   portant words amazing ( /trianglesolid),dwarfs ( /trianglesolid ) , and ev-   erything ( /trianglesolid ) are removed from the instance , the   sentence is transformed into “ Gosling provides an   amazing performance that dwarfs everything else   in the ﬁlm . ” , which is unfathomable to humans   and does not contain any information supporting   classifying this incomplete sentence into any class(positive or negative ) . However , the representation   of this sentence ( /trianglesolid ) is still close to the original sen-   tence ( • ) , indicating that the model still regards it   belongs to the original class with high conﬁdence .   Observation 2 : When unim portantwords arere-   moved , therepresentations ofsuch incomplete out-   of - distributionexamplesarebiased away from the   originalsentence more than expected . Intuitively ,   the unimportant words should contain low - impact   information to support the prediction . Losing this   unimportant information , the model conﬁdence   should almost not change , and the representations   of such incomplete sentences should still be close   to the original sentence . Focusing on instance 1 / circlecopyrt ,   when the six least important words else(/squaresolid),ﬁlm   ( /squaresolid),Gosling ( /squaresolid),an(/squaresolid),in(/squaresolid ) , and the(/squaresolid ) are   removed from the instance , the sentence is trans-   formed into “ Gosling provides anamazing per-   formance that dwarfs everything elseintheﬁlm . ” .   Even though this sentence is grammatically incor-   rect , it is still easy for humans to classify it as   a positive example . However , the representation   of this incomplete sentence ( /squaresolid ) is largely biased   from the original examples ( • ) and is even closer   to the adversarial example (  ) than the sentence   with three important words removed ( /trianglesolid ) , indicat-   ing that the model predicts this out - of - distribution   examples with lower conﬁdence.6496Similar phenomena can also be observed in in-   stances 2 / circlecopyrtand3 / circlecopyrt . More results can be found in Ap-   pendix B.2 . Based on Observation 1 andObser-   vation 2 , we can answer the question raised before   from the view of sentence representation : When   important words are masked out from the sentence ,   the representations of such out - of - distribution ex-   amples are sometimes too close to the original   sentence , maintaining the high model conﬁdence ,   even if such examples do not contain any informa-   tion supporting the prediction . When unimportant   words are masked from the sentence , the repre-   sentations of such out - of - distribution examples are   sometimes largely biased away from the original   sentence and are approaching the decision bound-   ary , decreasing the model conﬁdence , even if such   examples are still easy for humans to classify .   Appendix B.1 provides further study on the dis-   tance between out - of - distribution sentences and the   in - distribution normal sentence , which supports our   claim on Observation 1 and Observation 2 .   3.4 Contrastive Learning with Adversarial   Examples for Alleviating the Pathology   In this section , we try to answer howtoalleviatethe   pathol ogyeffectively ? We also detail the proposed   ConAAP regularization method . According to our   analysis , the model pathology can be explained by   the counter - intuitive sentence representation distri-   bution of out - of - distribution examples . Therefore ,   a natural way to alleviate the pathology is to cali-   brate their distribution . To calibrate the sentence   representation , we should focus on both the bias   degree andbias direction .   For the out - of - distribution examples with unim-   portant words removed , which are always used to   measure the AOPCvalue , we try to decrease the   bias degree of their representation from the orig-   inal normal example , as most of these examples   are still easy to classify . For the out - of - distribution   examples with important words removed , which   are always used to measure the AOPC value ,   we try to increase the bias degree of their represen-   tation from the original normal example , as these   examples are more difﬁcult to classify . However , if   they are pushed away from the original example in   a direction away from the decision boundary , the   counter - intuitive high conﬁdence will still be main-   tained . Therefore , we also simultaneously force   their bias direction toward the decision boundary ,   which is indicated by the adversarial example . To achieve the calibration , we reuse the word   removal operation we proposed in § 3.3 and used in   Figure 2 . The operation to delete important words   is deﬁned as t , and the operation to delete unim-   portant words is deﬁned as t. We also deﬁne   the operation that generates adversarial examples   ast . To formulate the contrastive loss objective   of ConAAP , for convenience , we ﬁrst deﬁne the   calculationS :   S= exp(sim[r(X),r(X)]/τ ) ( 6 )   where sim denotes the cosine similarity , i.e. ,   sim[r , r ] = rr//bardblr / bardbl / bardblr / bardbl.k , ldenotes the ex-   ample type , and k , l∈{neg , pos , adv , · } , which re-   spectively indicates the example X , X , X   sampled from the examples generated by the oper-   ationst , t , t , and the normal example . i , j   are the example indexes . τis a temperature param-   eter similar to the normalized temperature - scaled   cross - entropy ( NT - Xent ) loss ( Chen et al . , 2020a ;   van den Oord et al . , 2018 ) . Therefore , for a normal   example in a mini - batch { X } , the loss objec-   tive of ConAAP can be formulated as :   ( 7 )   where   S = S+S   + 1[S+S+S ]   and 1∈ { 0,1}is an indicator function that   equals 1 if [ · ] is true , Bis the batch size .   To reduce the bias degree from the original ex-   ample of the representation of out - of - distribution   examples with unimportant words removed , we use   the termSin the numerator . This constraint   increases the similarity between the representation   of the normal example and examples with unimpor-   tant words removed , implying that model should   regard the information in the removed unimportant   words only slightly impacting the prediction .   To increase the bias degree from the original   example of the representation of out - of - distribution   examples with important words removed , we use   the termSin the denominator . This constraint   decreases the similarity between the representation   of normal example and examples with important6497words removed , implying that model should regard   the information in the removed important words   signiﬁcant in prediction .   We simultaneously use the term Sin   the numerator to force the bias direction of out-   of - distribution examples with important words re-   moved toward the decision boundary indicated by   the adversarial example . We also use the term   Sin the denominator to prevent the represen-   tation of normal example and adversarial example   from collapsing together , ensuring that the adver-   sarial example can always be utilized as a guide to   locate the direction of decision boundary . It should   be noted that ConAAP only focuses on alleviating   the model pathology , and we leave improving the   model robustness to future work .   The termsS+S+Sin the denom-   inator imply that the model should differentiate the   various examples and their derived examples in a   mini - batch , as the semantics of different examples   should be different .   Finally , we use the L as regularization   and combine it with the normal training method ,   which originally trains the model only with maxi-   mum likelihood . The overall objective can thus be   formulated as follows :   minL(X , Y ) + αL ( X)(8 )   whereαis a parameter balancing the two parts .   4 Experiment   4.1 Metrics   We measure the model pathology and the faith-   fulness of attribution with metrics AOPC and   AOPC , and the parameter Kin them is both   set as the 40 % of words for each sentence . We   useAOPCto indicate the difference between   AOPC andAOPC , i.e. , the difference be-   tween the overall inﬂuence of words of different   importance on prediction . Based on the Reduced   Length ( Feng et al . , 2018 ) , we also use IR#and   UR#to measure the inﬂuence of Important and   Unimportant words on prediction , measuring the   number of important and unimportant words re-   moved until the prediction changes . We use R   to indicate the difference between IR#andUR # .   Larger AOPCandRare expected for a non-   pathological model and faithful attribution . We also   use accuracy ( ACC . ) and conﬁdence ( P(Y|X ) )   on normal examples to measure the generalization   ability of model on in - distribution examples.4.2 Experiment Setup   Dataset . Focusing on the text classiﬁcation , our   experiments are performed on AG News ( Zhang   et al . , 2015 ) , MR ( Pang and Lee , 2005 ) , and   IMDB ( Maas et al . , 2011 ) . More details of datasets   are provided in Appendix A.1 .   Model . Three models in different architectures   are adopted . For TextCNN , we reuse the archi-   tecture in ( Kim , 2014 ) while replacing the embed-   ding with the 300 - dimensional GloVe ( Pennington   et al . , 2014 ) . For LSTM ( Hochreiter and Schmid-   huber , 1997 ) , we connect a Bi - LSTM layer with   150 hidden units with a dense layer based on the   300 - dimensional GloVe layer . For BERT ( Devlin   et al . , 2019 ) , we use the base uncased version .   Baseline . To show the effectiveness of ConAAP   and empirically demonstrate the analysis of the   bias degree and bias direction we provide in § 3.3   and § 3.4 , we use the following baselines : ( i ) Nor-   mal : using ( 1 ) as objective . ( ii ) ConAAP : com-   bining ( 7 ) with Normal method , using ( 8) as ob-   jective . ( iii ) Entropy : maximizing the model un-   certainty on the reduced examples ( Feng et al . ,   2018 ) . Please see Appendix A.3 for more details   onEntropy method . ( iv ) ConAAP w/o imp - dir :   removingS , SinL , indicating   removing the calibration on the bias direction of   out - of - distribution examples with important words   removed . ( v ) ConAAP w/o imp - deg - dir : removing   S , SandSinL , indicat-   ing removing the calibration on both the bias degree   and direction of out - of - distribution examples with   important words removed .   Implementation Details . The batch size is set   as 64 . For efﬁciency , we use a method called   CharDelete to generate adversarial examples in   t , which randomly deletes characters in the im-   portant words until the attack success . More de-   tails of CharDelete are in Appendix A.2 . We use   Adam ( Kingma and Ba , 2015 ) as the optimizer .   Most setting of learning rate / α / τfor LSTM ,   TextCNN , and BERT is 5e-4/1.2/0.1 , 5e-4/1.2/0.1 ,   3e-5/1.2/0.01 . All reported results are the average   of ﬁve independent runs .   4.3 Main Results   ConAAP marginally impacts the generalization   performance for in - distribution examples . Ta-   ble 1 illustrates the accuracy and conﬁdence results   for in - distribution examples . Utilizing ConAAP6498   as regularization during training has a minimal im-   pact on the model ’s behavior for in - distribution   examples , as evidenced by the marginally changed   model accuracy and conﬁdence . The accuracy dif-   ference between ConAAP and Normal training   methods is within 1.11 % , and the model conﬁ-   dence on normal examples P(Y|X)decreases by   at most 0.04 compared to Normal method . These   results demonstrate that imposing regularization on   the sentence representations of out - of - distribution   examples only slightly compromises the model ’s   generalization performance for in - distribution ex-   amples . Furthermore , bias degree and direction   constraints in ConAAP also have only a minor im-   pact on generalization capabilities .   ConAAP effectively alleviates model pathology .   Table 2 illustrates the results on model pathology   and attribution faithfulness . ConAAP consistently   yields the largest values for RandAOPC ,   indicating that the model considers the information   in important words to have a more signiﬁcant im-   pact on predictions than that in unimportant words ,   and the attributions are more faithful . Moreover ,   when the calibration on the bias direction of out-   of - distribution examples with important words re-   moved ( w/o imp - dir ) is removed , both Rand   AOPCdecrease , indicating less faithful attribu-   tions and reduced effectiveness in alleviating model   pathology . Removing the calibration on both the   bias degree and direction of out - of - distribution ex-   amples with important words removed ( w/o imp-   deg - dir ) leads to further reductions in Rand   especially AOPCvalues , demonstrating the ef-   fectiveness of ConAAP ’s multi - view objective that   simultaneously calibrates the bias degree and direc-   tion of the representations of various examples .   4.4 Further Analysis and Ablation Study   In this section , we conduct further analysis and   ablation study on BERT and MR dataset .   Hyperparameter α . Figure 3(a ) illustrates the   inﬂuence of α . We ﬁnd that AOPCbegins to   increase when α>0.05and stabilizes for α>0.5 .   The accuracy is stable and will slightly increase as   αcontinues to increase .   Temperature τ . Figure 3(b ) illustrates the in-   ﬂuence ofτ . We ﬁnd that ConAAP is sensitive   toτ , and an appropriate τcontributes to both   model accuracy and the effectiveness in alleviat-   ing the pathology . AOPCreaches its peak when6499   τ= 0.01 , while the accuracy ﬂuctuates by no more   than 1.4 % when τis assigned different values .   Batch size . Figure 3(c ) shows the inﬂuence of   batch size on ConAAP . Larger batch sizes prove   beneﬁcial for ConAAP , as both model accuracy and   the ability to alleviate pathology improve with an   increase in batch size .   Attack method in t. Various attack methods   can be utilized in ConAAP ( Gao et al . , 2018 ; Garg   and Ramakrishnan , 2020 ; Li et al . , 2019 ; Jin et al . ,   2020 ) , and the inﬂuence of attack methods is shown   in Figure 4 . ConAAP remains effective in alleviat-   ing model pathology when utilizing different attack   methods . It should be noted that adversarial ex-   amples in ConAAP are used to introduce direction   information and are not intended to be nearly im-   perceptible to humans . Consequently , their quality   is not of primary concern , and a fast CharDelete   method sufﬁces for our purposes .   Conﬁdence Changing with Word Removal .   Figure 5 illustrates the impact of word removal   on model conﬁdence . As more important words   are removed , the conﬁdence of the Normal method   remains close to 1 , while the label shift induced   by word removal causes the model ’s conﬁdence   in the original class to approach 0 ( Figure 5(a ) ) .   In contrast , the distribution of ConAAP is consid-   erably smoother than the Normal method ( Figure   5(c ) ) . When more unimportant words are removed ,   the conﬁdence for both ConAAP and the Normal   method consistently concentrates in a high region   ( Figure 5(b)(d ) ) .   Case study . The case study is shown in Figure 6 .   For the model trained with the Normal method , var-   ious interpretation methods show considerable di-   vergence in word importance . Moreover , the model   predicts the sentence with high conﬁdence even   after removing the two most important words ( e.g. ,6500   following Gradient attribution , the model predicts   the sentence “ One of the great estmovies ever ” as   positive with 87.21 % conﬁdence ) . In contrast , for   the model trained with ConAAP , different interpre-   tation methods show a more consistent result of   word importance ( e.g. , important words are con-   centrated in greatest , movie ; unimportant words   are concentrated in one , of , the , ever ) , resulting   in more faithful attributions . Speciﬁcally , when   the two most important words are removed , the   average conﬁdence across different attributions is   41.43 % . Conversely , when unimportant words are   removed , the model can still make high - conﬁdence   predictions similar to the original examples .   5 Conclusion   In this paper , we argue that the failure of interpre-   tation methods to provide faithful attributions for   language models is due to the model pathology that   models are overconﬁdent in out - of - distribution ex - amples when making predictions . We explain the   model pathology from the perspective of sentence   representation and propose ConAAP , a contrastive   learning regularization method to calibrate the sen-   tence representation of out - of - distribution exam-   ples . Experiments demonstrate the effectiveness   of ConAAP in alleviating model pathology , which   helps interpretation methods obtain faithful results .   We hope that our work will provide a new perspec-   tive on research in the ﬁeld of interpretability .   Limitations   We explain model pathology from a classiﬁcation   perspective , but the pathological nature may exist   in language models for performing various tasks ,   such as reading comprehension , textual entailment ,   and visual question answering . Although our pro-   posed regularization technique may be applica-   ble to various tasks , we have only investigated   its effectiveness in classiﬁcation problems . Fur-   ther evaluations are expected to be conducted in   future works . The proposed method also leads to   more time - consuming training , primarily due to   the generation of adversarial examples , while only   a minimal amount of time is spent on generating   out - of - distribution examples .   Ethics Statement   This paper investigates model pathology from a   sentence representation perspective and proposes   a regularization technique to alleviate the pathol-   ogy . It is possible that the proposed method can be   used for both benign purposes , such as ﬁxing the   potential ﬂaws and biases of models , and malign   ones , such as exposing the vulnerabilities of mod-   els , which makes it easier for adversaries to gener-   ate malicious input . Despite these risks , we argue   that studying model pathology openly is essential .   Exploring the pathological nature of models will   help us effectively control these potential risks and   improve our understanding of the mechanics of   natural language models . All datasets used in this   paper are publicly accessible , and our work fully   complies with their respective licenses .   Acknowledgements   The authors would like to thank the anonymous   reviewers for their thorough and constructive feed-   back . This research was supported by National   Research and Development Program of China   ( No.2019YFB1005200).6501References65026503A Additional Experimental Details   A.1 Details on Dataset   AG News contains news articles in the areas of   World , Sport , Business , and Science / Technology ,   with 120,000 for training and 7,600 for testing . MR   contains movie reviews from Rotten Tomatoes la-   beled as positive or negative , with 8,530 for training   and 1,066 for testing . IMDB contains binary polar   movie reviews from the Internet Movie Database ,   with 25,000 for training and 25,000 for testing .   A.2 Details on CharDelete Attack Method   We use the CharDelete adversarial attack method   intto generate adversarial examples in our main   experiments . The details of CharDelete are shown   in Algorithm 1 . ConAAP does not tend to generate   high - quality adversarial examples that are imper-   ceptible to humans and only utilizes adversarial   examples to introduce direction information into   regularization . This attack method meets our re-   quirements , and a complex method is unnecessary .   Algorithm 1 : CharDelete Algorithm   A.3 Details on Entropy Method   The Entropy training method is proposed by ( Feng   et al . , 2018 ) . They use an entropy of the output   distribution as a regularization term in the overall   training objective . Speciﬁcally , the loss objective   of the Entropy method is   L = /summationdisplaylog(P(Y|X ) )   + λ / summationdisplayH / parenleftbig   P / parenleftbig   Y|t(X)/parenrightbig / parenrightbig ( 9)whereλis a parameter balancing the two terms ,   tgenerates the sentences with multiple unim-   portant words reduced to the minimum length that   can keep the model predictions by beam search , b   is the beam width , Hdenotes the entropy . λis set   as 1e-3 , in accordance with the original paper .   B Additional Experimental Results   B.1 Distance Between Different Examples   We also provide the aggregated results on the dis-   tance between out - of - distribution sentences and the   in - distribution normal sentence in Figure 7 - 9 . After   removing important words , the density distribution   of Euclidean distance between such sentence rep-   resentations and the original sentences becomes   smoother , with an increase in the maximum dis-   tance . However , most sentence representations re-   main close to the original ones ( with Euclidean   distance approaching 0 ) . Intuitively , although the   density distribution becomes smoother after impor-   tant word removal , there is no signiﬁcant horizontal   shift ( i.e. , minimal distance changes ) , indicating   that information from some important words does   not have a sufﬁcient impact on predictions . Af-   ter removing unimportant words , the change in   the density distribution of Euclidean distance be-   tween such sentence representations and the origi-   nal sentence is less pronounced than when impor-   tant words are removed . However , the representa-   tions of some sentences diverge considerably from   the originals when only a few unimportant words   are removed ( e.g. , distance greater than 10 in MR   when only one unimportant word is removed ) , in-   dicating that information from some unimportant   words may have a much greater inﬂuence on pre-   dictions than expected .   B.2 Sentence Representation Distribution   Figure 10 - 11 show more visualization of the sen-   tence representation and the attribution on instance   sentences . Observation 1 and Observation 2 can   also be observed in these examples.650465056506ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   In Section Limitations .   /squareA2 . Did you discuss any potential risks of your work ?   In Section Ethics Statement .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   In Abstract and Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   In Section 3 and Section 4 .   /squareB1 . Did you cite the creators of artifacts you used ?   In Section 1 and Section 4.2 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   In Section Ethics Statement .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   In Section Ethics Statement .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   All datasets utilized by us are widely adopted benchmark datasets .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   In Section 4.1 , Section 4.2 , and Appendix A.   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   In Appendix A.1 .   C / squareDid you run computational experiments ?   In Section 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   In Section 4.2.6507 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   In Section 4.1 and Section 4.2 .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   In Section 4.2 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   In Section 4.2 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.6508