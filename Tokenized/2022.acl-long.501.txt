  Yao DouMaxwell ForbesRik Koncel - KedziorskiNoah A. SmithYejin ChoiPaul G. Allen School of Computer Science & Engineering , University of WashingtonAllen Institute for AI   Abstract   Modern neural language models can produce   remarkably ﬂuent and grammatical text . So   much , in fact , that recent work by Clark et al .   ( 2021 ) has reported that conventional crowd-   sourcing can no longer reliably distinguish be-   tween machine - authored ( GPT-3 ) and human-   authored writing . As errors in machine gener-   ations become ever subtler and harder to spot ,   it poses a new challenge to the research com-   munity for robust machine text evaluation .   We propose a new framework called S - for scrutinizing machine text via crowd   annotation . To support the broad range of real   machine errors that can be identiﬁed by laypeo-   ple , the ten error categories of S —   such as redundancy , common sense errors ,   and incoherence — are identiﬁed through sev-   eral rounds of crowd annotation experiments   without a predeﬁned ontology .   We then use S to collect over 41k   error spans in human - written and machine-   generated paragraphs of English language   news text . We isolate factors for detailed   analysis , including parameter count , training   data , and various decoding - time conﬁgura-   tions . Our approach successfully quantiﬁes   measurable gaps between human authored text   and generations from models of several sizes ,   including fourteen conﬁgurations of GPT-3 . In   addition , our analysis unveils new insights ,   with detailed rationales provided by laypeople ,   e.g. , that the commonsense capabilities have   been improving with larger models while math   capabilities have not , and that the choices of   simple decoding hyperparameters can make re-   markable differences on the perceived quality   of machine text . We release our training mate-   rial , annotation toolkit and dataset at https :   //yao - dou.github.io / scarecrow/ .   1 Introduction   Clark et al . ( 2021 ) demonstrated the challenges   of human evaluation in the era of GPT-3 ( BrownFigure 1 : After a model ( here , GPT-3 DaVinci ) has read   the prompt ( top sentence ) and generated a continuation   ( next paragraph ) , the S annotation frame-   work provides a systematic way for humans to mark   issues throughout the text and explain what is wrong .   Our own annotations are pictured here .   et al . , 2020 ) , as crowd workers are no longer able   to reliably distinguish GPT-3 ’s generations from   human - written text .   Or are they ? In this paper , we propose a new   framework for systematically scrutinizing machine   text so that even crowd workers , despite the known   challenges reported by recent literature , can suc-   cessfully critique seemingly ﬂuent generations . We   not only quantify a measurable gap between ma-   17250   chine text and human text , but reveal the distribu-   tions of speciﬁc categories of issues , and pinpoint   their occurrences in text written by several sizes of   language models as well as humans .   To achieve this , we develop S , a   methodology for eliciting categorical judgements   of errors in machine - generated text from crowd   workers . One goal in natural language generation   ( NLG ) is to produce ﬂuent outputs which can be   read by laypeople . As such , we propose that im-   portant errors to address are those which are rec-   ognized by readers without NLP expertise . Our   framework allows crowd workers to annotate prob-   lems in model outputs at the span level . A single   such annotation is shown in Figure 1 .   To make this possible , we establish a categoriza-   tion of shortcomings commonly found in machine   generated text ( Table 1 ) . This error schema covers   a broad scope of problems as identiﬁed by experts ,   but has been honed according to what is salient to   non - expert readers through several pilot rounds of   crowd annotation without a ﬁxed label set . The   result is a framework that is usable by everyday   people with minimal training , but covers the error   phenomena found in real machine - generated text .   Labeling spans of text using speciﬁc error types cre-   ates a picture of contemporary model generationswith an unprecedented level of detail . In contrast to   judging text holistically ( Celikyilmaz et al . , 2021 ) ,   insights from this method are speciﬁc and practical ,   as it measures exactly how and where problems   arise .   We conduct a large - scale analysis of human-   written and machine - generated text using S - , collecting 13k annotations of 1.3k para-   graphs , amassing 41k spans labeled with error type ,   severity , and an explanation . Through this , we   characterize in which ways GPT-3 ’s generations   are better than those of previous models , and which   aspects do not improve with increased data and pa-   rameters . We also provide a rigorous error analysis   of text generated by several other contemporary   language models , examining the impact of model   size , training data , and decoding strategy .   We provide our detailed annotator training sys-   tem and task interface so that future researchers   may employ and reﬁne them for error analyses of   machine - generated text . We hope this will con-   tribute to the standardization of NLG human evalu-   ation ( Howcroft et al . , 2020 ) .   2 Key Findings   We perform a large - scale annotation of errors in   English news text generated by ﬁve sources ( four   27251   models and ground truth articles ) . We present Fig-   ures 2 , 3 , and 4 as summaries of our main results .   As a reminder to readers , Grover ( Zellers et al . ,   2019 ) is the same model size and architecture as   GPT-2 XL ( Radford et al . , 2019 ) , but trained in-   domain ( on news text ) . As such , our results cover   three increasing model sizes ( GPT-2 Small , XL ,   and GPT-3 ( Brown et al . , 2020 ) ) , one change in   domain ( Grover ) , and ground - truth text ( Human ) .   For GPT-3 , we also study a variety of decoding   conﬁgurations ( Figure 4 ) .   The main quantity we measure ( on y - axes ) is   span coverage , which is the average portion of   tokens that ends up covered by annotations of a   particular error type . Since it is possible that multi-   ple spans nest or overlap , there is no upper bound   for this quantity . ( See Figure 12 for a comparison   of span coverage with other measurement alterna-   tives . ) Figure 2 measures span coverage for each   type of span separately , Figure 3 stacks them , and   Figure 4 removes non - error spans ( reader issues )   before adding them ( as in Figure 3 , but without   showing the individual types ) .   The following are our key ﬁndings .   1 . Scaling pays off to improve Encyclopedic ,   Com mon sense , and Incoherent errors ( Fig .   2 ) . These error categories decrease with   in - domain training ( Grover ) and larger model size(GPT-3 ) . Human text still shows the fewest of   these kinds of errors .   2 . Scaling beneﬁts plateau for Off - Prompt ,   Bad Math , and Gram mar and Usage errors   ( Fig . 2 ) . These three error categories see amodel plateau in error reduction when scaling   to GPT-3 . Of these error types , humans still   commit fewer Off - Prompt ( more : § E.1 ) and   Gram mar andUsage errors , but Bad Math ap-   pears saturated for our domain .   3.Self - Contradiction and Redundant errors   exhibit more complex scaling behavior ( Fig . 2 ) .   We roughly categorize these trends as rising   and falling : increasing for medium or large - scale   models , but dropping for human - authored text .   Text generated by GPT-2 Small is so often   incoherent that there is little possibility for Self-   Contradiction ( more : § E.2 ) , and the increase in   Redundant errors varies based on how errors are   counted ( more : § E.3 ) .   4 . Human - authored text produces the most   reader issues ( Figs . 2 and 3 ) . The Needs   Google and Tech nicalJargon span categories   both have a humans highest trend , and both fall   under reader issues : problems that are not necessar-   ilyerrors , but that still prevent full comprehension   37252   or factual veriﬁcation of the text ( more : § E.4 ) .   Furthermore , human - authored text is not free   from error annotations ( Figure 3 ) . This can serve   either as a control for baseline error rates ( more :   § E.6 ) , or as a mechanism for critiquing human   writing .   5 . Decoding hyperparameters have a huge im - pact ( Figure 4 ) . For the previous ﬁndings , we ﬁx   the sampling conﬁguration for all models to an   apples - to - apples setup for fair comparison : top- p=   0.96 , ( softmax ) temperature = 1 , and no frequency   penalty ( i.e. , word repetition penalty ; deﬁned pre-   cisely in § 5.2 , Equation 1 ) . To study the effects of   these decoding settings , we annotate text generated   by GPT-3 using a variety of values for top- pand   temperature , both with and without a frequency   penalty .   To our surprise , the decoding hyperparameters   considerably affected error rates ( more : § E.5 ) . As   seen in Figure 4 , the worst sampling procedure   for GPT-3 ( argmax sampling with no frequency   penalty ) performed even worse than GPT-2 XL .   But the best sampling procedure ( surprisingly , also   argmax sampling , but with a frequency penalty )   produced text with as few apparent S   error spans as those authored by humans ( more :   § E.6 ) .   All of these ﬁndings are discussed in more detail   in Appendix E.   3 Evaluation of Natural Language   Generation   We make our study in the area of open - ended natu-   ral language generation , a loose term for generat-   ing longer texts with an increased level of creative   freedom . The common factor in all open - ended   generation tasks such as story , blog , and dialog   generation is the wide and diverse nature of target   outputs . Lexically and even semantically dissimi-   lar responses to the same prompt could be equally   valid . For example , a model prompted with the   blog title “ Recipes for success this Holiday season ”   could describe how to roast a turkey or strategies   for dealing with the stresses of holiday travel .   This allowable variation poses a particular dif-   ﬁculty for the evaluation of generation systems .   Traditionally , text generation quality for tasks like   machine translation or graph - to - text generation   has been measured by word overlap with human-   authored references ( Papineni et al . , 2002 ; Lin ,   2004 ) . Though measures like BLEU allow for mul-   tiple references , they break down when the space of   allowable outputs is large , as in open - ended gener-   ation . Recently introduced metrics seek to remedy   this problem ( Hashimoto et al . , 2019 ; Pillutla et al . ,   2021 ) , but the gold standard for evaluating gener-   ated text is still human judgment .   However , current approaches to eliciting human   47253judgement of generated text often do not provide   detailed insight into where models are making   progress , where they are failing , and the scope   of these failures . A / B - style testing allows for di-   rectly comparing one system against others ( Clark   and Smith , 2021 ) , but can only express relative im-   provements . Simple Likert scale judgements can   assess text quality , but do not explain why a gener-   ated text receives a given rating , or which segment   of the text is problematic . Insights into model fail-   ures often come instead from a small scale expert   analysis of outputs . However , these “ error analy-   ses , ” once a staple of NLP research , have become   less common in recent years , perhaps due to their   small size and high variance .   A hypothesis of the current work is that a well de-   signed error analysis annotation framework could   be used by crowdworkers to annotate large amounts   of text , thereby providing detailed information   about model progress and failures as well as ac-   tionable directions for future research . Such a   framework would be easy to learn , reusable , and   independent of particular models or experimental   conditions . In what follows , we outline the details   of such a method .   4 S Annotation Methodology   This section describes the high - level annotation   methodology for S .   4.1 Prompt and Generation   Our annotations consider two segments of text : a   one - sentence prompt , and a one - paragraph gener-   ation . The prompt is human - written . It provides   both starting tokens for model generation , as well   as context for humans to evaluate whether a model   is able to stay on - prompt — both topically and fac-   tually . Annotators know that the prompt is written   by a human .   The generation is either text sampled from a   language model , or the human - authored continua-   tion to the prompt . Annotators , who do not know   whether the generation came from a model or hu-   mans , assess this text . A paragraph length ( 80–145   tokens ) is chosen to balance expressiveness with   scope . For expressiveness , models must be given   a sufﬁcient number of tokens to express their ca-   pabilities lexically , syntactically , and semantically .   One paragraph allows for signiﬁcantly more vari-   ation than a single sentence . On the other hand ,   assessing multiple paragraphs is challenging , both   as a crowdsourcing task itself , and because it broad-   ens the kinds of errors to include larger narrative   scope . We leave extensions of S to   longer narrative lengths for future work .   4.2 Span Labeling   Annotators select spans that contain problems   in the generation . The spans are automatically   snapped to word boundaries . We choose spans   to balance speciﬁcity ( i.e. , vs. simply comment-   ing on the text as a whole ) with ease of use ( vs.   imposing a more structured annotation schema ) .   4.3 Span Selection   We instruct workers to select the smallest span —   minimally a single word — that contains an issue .   Sometimes this involves an entire phrase , sentence ,   57254or multiple sentences . We aim for speciﬁcity be-   cause during aggregation , it is possible to “ back   off ” annotations to larger spans , but not the inverse .   Once they select a span , workers ( 1 ) label the er-   ror type , ( 2 ) choose a severity level , and ( 3 ) explain   their reasoning behind the error . Workers use the   annotation interface shown in Figure 5 to mark a   span with these three steps . We describe each step   in greater detail in the next three sections .   4.4 Error Types   Each selected span is labeled with exactly one error   type . Multiple errors may be marked with partially   or fully overlapping spans in the case that one text   segment contains multiple problems .   We chose ten error types to balance three crite-   ria : linguistic analysis , observed errors in gener-   ated text , and capabilities of everyday people with   one to two hours of training . We developed the   schema by starting with the ﬁrst two criteria ( lin-   guistic analysis and observed errors ) , and reﬁning   it over several pilot annotation studies , with 30   crowd workers performing 750 total annotations of   60 paragraphs before beginning data collection .   We broadly group the errors into three categories :   language errors , factual errors , and reader issues .   Language errors are issues with internal and ex-   ternal structure of text : which ideas are expressed ,   and whether they are expressed coherently and con-   sistently . Factual errors denote that the information   presented is known to be incorrect . Reader issues ,   on the other hand , are cases where the text is too   technical or obscure to assess its factuality . Hence ,   reader issues are not errors , per se , but regions   where a reader would need assistance outside of   the text itself for comprehension .   We present the ten error types in Table 1 ( several   pages back ) . Appendix A provides more details ,   examples , and explanations for all error types .   4.5 Severity   Errors naturally vary in how jarring they are to a   reader . We deﬁne three error severity levels , and   ask annotators to pick one for each error .   The severity levels are as follows . ( 1)Almost   no impact on quality ; just a small problem . ( 2 )   Understandable , but difﬁcult ; what ’s written is still   comprehensible , but there ’s clearly an issue . ( 3 )   Very difﬁcult to understand ; the error almost com-   pletely ruins the text . We provide examples of each severity in Ap-   pendix B.1 . In this paper , we omit an analysis of   the severity labels ( except for an illustration in Fig-   ure 12 ) , but include it in our data release for future   work to explore .   4.6 Explanation   Finally , we ask annotators to explain their reason-   ing behind each error in natural language . We pro-   vide example explanations during training , but do   not impose strict guidelines . This paper primarily   focuses on quantitative error analysis , but we an-   ticipate the error explanations may warrant future   investigation .   4.7 Annotation Process   We use Amazon Mechanical Turk ( AMT ) for all   data collection .   Training We ﬁrst pay each worker $ 40 to take an   extensive qualiﬁcation task , which both trains them   in the span categorization scheme and quizzes their   understanding . We pass workers if they score 90   points out of 100 points ( details in Appendix B.2 ) .   Annotation Workers annotate each paragraph us-   ing a custom annotation interface ( shown partially   in Figure 5 ) , for which we pay $ 3.50 . We calculated   $ 3.50 per annotation by aiming to pay workers at   least $ 15 / hour . After several annotation rounds , we   observed considerable variation in time per annota-   tion , so this cost should not be necessarily seen as   a requirement for S annotations .   5 Data Collection   We collect 13k human annotations of 1.3k para-   graphs using S , resulting in over 41k   spans .   5.1 Models   We consider four model conﬁgurations to test re-   cent state - of - the - art transformer - based ( Vaswani   et al . , 2017 ) models .   GPT-2 Small ( Radford et al . , 2019 ) The 117 M   parameter variant of GPT-2 , which is pretrained on   WebText , without additional ﬁne - tuning .   GPT-2 XL ( Radford et al . , 2019 ) The 1.5B pa-   rameter variant of GPT-2 , ( WebText , no ﬁne-   tuning ) .   67255Grover - Mega ( Zellers et al . , 2019 ) The 1.5B pa-   rameter variant of Grover , a model with the same   architecture and parameter count of GPT-2 , trained   on news articles and their metadata .   GPT-3 DaVinci ( Brown et al . , 2020 ) The 175B   parameter variant of GPT-3 , which is trained on   a version of the Common Crawl web scrape with   additional ﬁltering and deduplicating .   In addition , we also use the actual human - written   text from the data sources we draw from , which we   denote as Human .   5.2 Decoding strategies   We consider three main hyperparameters when sam-   pling from models : pfortop - p ornucleus sampling   ( Holtzman et al . , 2020 ) , an alternative to top - k ; t   for the softmax temperature ; and f.p.forfrequency   penalty . The frequency penalty scales a token ’s   likelihood based on how many times it was already   generated by applying the following modiﬁcation   to the model ’s output :   ` ( t ) ` ( t) c(t)   ( 1 )   where`(t)is the model ’s output for token tat the   i - th position , c(t)is the count of token t ’s sam-   pled occurrences prior to the i - th position , and  is   the frequency penalty . We omit studying presence   penalty , another hyperparameter offered for GPT-3 ,   simply due to annotation budget constraints .   To compare models as consistently as possible ,   we set identical decoding strategies for our primary   data collection . We refer to this as the “ apples - to-   apples ” decoding setup throughout the paper :   p= 0:96t= 1:0 f.p.= 0   However , we also wish to study the effects of   these decoding strategies . We annotate generations   from the strongest available model ( currently , GPT-   3 ) varying the following parameters : p2f0:4;0:7;0:9;0:96 g   t2f0:0(argmax);0:4;0:7;1:0 g   f.p.2f0(none);1(full)g   For budget reasons , we only vary pandt   independently — i.e. , we set p= 0:96when varying   t , andt= 1:0when varying p.   5.3 Prompt Selection   We use news articles as the sources of prompts for   models to condition on for generation . Speciﬁcally ,   we use news articles found in the Common Crawl .   We select the ﬁrst sentence as the prompt .   Our use of news text is constrained by two fac-   tors . First GPT-3 is trained on the Common Crawl ,   from 2016 through 2019 . We wish to avoid testing   GPT-3 by generating from articles it saw during   training , due to the possibility of copying ( Carlini   et al . , 2021 ) . Second , news articles began heav-   ily covering the COVID-19 pandemic beginning   around February 2020 . Though testing models ’ ca-   pabilities to generate text about unseen events is a   valuable line of study , the distribution shift caused   by COVID-19 in news writing about all aspects of   life is difﬁcult to overstate .   As such , to make the comparison more amenable   to models ’ training data , we consider news articles   from January 2020 . We select articles where there   is a known topic — such as Food orSports — from   the Common Crawl metadata , to allow for studying   any effect of coarse - grained subject .   5.4 Generation   We generate between 80 and 145 tokensfrom   each model as a continuation to the ﬁrst sentence   of the news article . We stop generating when we   heuristically detect the ﬁrst sentence boundary after   80 tokens . If the model does not end a sentence   between 80 and 145 tokens , we sample again . For   theHuman setting , we use the remainder of the   article , similarly stopping after the ﬁrst sentence   boundary after 80 tokens .   5.5 Annotation   Crowdsourcing Workers ﬁrst complete training   and qualiﬁcation tasks . We provide more details in   4.7 . From pilot studies , we discovered that each er-   ror , depending on its severity and clarity , has only a   77256low to moderate chance of being identiﬁed by each   worker . However , most worker - identiﬁed errors   were truly problems . In other words , annotators   labeled issues with high precision and low recall .   To account for this , we have 10 workers annotate   each paragraph . We examine the agreement and   variability of annotations in Appendix C.   Dataset statistics We provide detailed dataset   statistics in Appendix D.   6 Error Prediction   A natural question is : using this data , can machines   learn to detect and classify errors in machine gen-   erated text ?   Task We frame this problem as a span classiﬁ-   cation task . Given a span from a generated text ,   the goal is to classify its error type or output “ No   Error ” if there is none . Positive examples for each   error class are taken from our data . We sample   random spans that were not labeled with any error   type as negative examples . To ensure a breadth   of span lengths , we sample 3 negative spans for   every length of error span in the generated text . We   split the generated texts into train , development ,   and test sets using 1063 texts ( 28029 error spans ) ,   100 texts ( 2538 spans ) and 100 texts ( 2677 spans )   respectively .   Model We use a standard span classiﬁcation   model inspired by Wadden et al . ( 2019 ) . This   model encodes every generated text using a pre-   trained language model ( RoBERTa - large ) . Spans   are represented with the ﬁnal layer of this encod-   ing . Following previous work , we concatenate the   start and end tokens with a task - speciﬁc learned   length embedding . The resulting vector is passed   through a feedforward network which reduces its   dimensionally to the number of error categories   plus a “ No Error ” option . The resulting model has   357 M trainable parameters . The model is trained to   minimize the cross entropy of the correct span cate-   gory . We train for 15 epochs using AdamW with a   learning rate of 10 . We validate after each epoch   and use the checkpoint with the lowest validation   loss ( epoch 8) .   Evaluation To evaluate the error prediction   model , we use per - token precision , recall , and F   score per error category . We classify every span up   to length 30 in a generated text . We take as gold   labels the aggregated human error spans collected   in our data . In other words , models predict the com-   bined spans of all 10 annotators . For comparison ,   we also report as Human the average metrics of one   annotator versus the others ( i.e. , 1 - vs-9 ) .   Results Table 2 shows the error prediction capa-   bility of this model in terms of precision and recall .   As we noted earlier , a single human annotator can   be thought of as a high precision , low recall judge .   These results bear out this claim . For all but one cat-   egory , humans have higher precision annotations .   However , the models trained on the aggregation   of human labels can achieve considerably higher   recall . For half of the error categories , this leads to   higher model Fscores than the human annotators .   We see that the model is successful at identifying   information that human ’s would have to manually   verify ( Needs Google ) , achieving nearly perfect   recall with precision close to 0.6 . The model can   also identify Gram mar andUsage , Incoherent ,   and Redundant errors with higher recall than an   individual human annotator , though at the cost of   precision ( sometimes in the .20s ) .   7 Related Work   Automated evaluation metrics such as BLEU ( Pa-   pineni et al . , 2002 ) , ROUGE ( Lin , 2004 ) , ME-   TEOR ( Banerjee and Lavie , 2005 ) , and BERTScore   87257(Zhang et al . , 2019 ) compute a generation ’s score   based on a ( set of ) reference(s ) . Their use is well-   established in tasks like machine translation and   summarization , but they are less helpful in open-   ended text generation , where there is a vast diver-   sity of possible high - quality continuations .   Recent studies propose automated metrics for   open - ended text generation evaluation such as : Per-   ception Score ( Gu et al . , 2021 ) , which diffuses eval-   uation onto a multidimensional space and assigns   a single score ; UNION ( Guan and Huang , 2020 ) ,   which learns to distinguish human - written stories   from negative samples by generating perturbations   of human - written stories ; and MAUVE ( Pillutla   et al . , 2021 ) , which compares the distribution of   machine - generated text to that of human language .   An alternate recent approach to assessing open-   ended text generation was presented in TuringAd-   vice ( Zellers et al . , 2021 ) , where crowd workers   assess machine - generated advice in response to   Reddit posts . In their error analysis , Zellers et al .   connect problems in generated text to core NLP   tasks , such as Self - Contradiction errors as in-   stances of failed natural language inference ( Monz   and de Rijke , 2001 ) , or Off - Prompt errors as   cases of failed reading comprehension ( Richardson   et al . , 2013 ) . While past work has attempted to   guide text generation using discriminative models   trained for such tasks ( Holtzman et al . , 2018 ) , it   remains an open challenge .   Comparative human evaluations of natural lan-   guage generations ask annotators to rank system   outputs relative to each other . Text is typically eval-   uated using a few global criteria , such as ﬂuency   and relevance , using discrete ( e.g. , 5 - point ) ( Sai   et al . , 2020 ) or continuous scales ( Novikova et al . ,   2018 ) . Recent work even automates this approach ,   running a human evaluation alongside automatic   metrics on leaderboard submissions ( Khashabi   et al . , 2021 ) . In the RoFT system ( Dugan et al . ,   2020 ) , annotators attempt to detect the boundary be-   tween human- and machine - written text as a proxy   for assessing quality . Table 3 summarizes the dif-   ferences between these schemes and S .   See Celikyilmaz et al . ( 2021 ) for a recent survey of   text generation evaluation techniques across both   human and automatic metrics .   While these approaches may be helpful —   sometimes ( Card et al . , 2020)—at ranking systems ,   they do not give us insight into exactly which parts   of a generation fall short , and why . One approach   related to or annotation method is pursued by Wood   et al . ( 2018 ) , who develop a collaborative mobile   app where users draw “ grafﬁti ” commentary on   news articles . S aims to assess model   generations the way we would critique human-   written text : by locating , coarsely categorizing , and   explaining problems .   8 Conclusion   We present S , a method for identifying   and explaining issues in generated text . Along with   the annotation framework , we present an analysis   of the S method applied to several large   neural language models in an open - ended news gen-   eration task . We release our data and methodology   to the community .   Acknowledgments   The authors thank members of xlab for their feed-   back on this work . This research is supported in   part by NSF ( IIS-1714566 ) , DARPA MCS pro-   gram through NIWC Paciﬁc ( N66001 - 19 - 2 - 4031 ) ,   DARPA SemaFor program , and Allen Institute for   AI .   References   97258   107259   117260A S Annotation Schema   Here , we present in greater detail the S   annotation error types . A visual summary is   shown in Figure 6 .   While we annotate using this schema , the   essence of our study is to embrace language users ’   abilities to detect when something may be wrong   with text . In other words , we do not wish for our   span deﬁnitions to get in the way of humans de-   scribing problems with text . To this end , we en-   courage researchers to embrace label back off ( to   coarser categories ) , merging labels ( based on em-   pirical observations ) , and reﬁning the annotation   ontology over time . The central goal is to collect   what people ﬁndwrong with text .   A.1 Language Errors   We deﬁne ﬁve categories of language errors , which   concern the selection of ideas in a text and how   they are expressed . These range from grammar   and syntax problems to issues of semantics and   pragmatics .   A.1.1 Gram mar andUsage   This category of errors includes missing words ,   extra words , and incorrect or out of order words .   We also label Gram mar and Usagefor in-   serted words or small phrases that could be deleted   to resolve the issue :   We avoid partitioning Gram mar andUsageer-   rors into more detailed categories based on the ob-   servation that large language models produce fewer   issues of syntax and diction ( aside from Redun-   dant errors , described next ) . As such , we focus   instead on semantic and pragmatic errors , captured   by the upcoming error types .   A.1.2 Redundant   While “ redundant ” can also include extra unnec-   essary information , we speciﬁcally use the Re-   dundant label to mark repetition . In identifying   redundant text , our schema annotates both the an-   tecedent ( ﬁrst mention ) and the redundant text   ( when the repetition occurs ) . Sometimes the exact   word or phrase will be repeated .   Other times , generated text expresses the same   idea repeatedly using different words .   A.1.3 Off - Prompt   The prompt is a human - written sentence used as   context from which the model generates a contin-   uation . Models sometimes generate text that is   unrelated to the prompt .   Other times , the text may be related , but it con-   tradicts what is stated in the prompt .   127261A.1.4 Self - Contradiction   When a model generates text that contradicts the   prompt , that is labeled as Off - Prompt . But when   a model generates text that contradicts itself , that   is labeled as Self - Contradiction . We also mark   theantecedent ( original statement ) .   A.1.5 Incoherent   Generated text is sometimes grammatical , not re-   dundant , on prompt , and not contradictory , but still   confusing . We provide the Incoherentlabel for   such sentences .   A.2 Factual Errors   We deﬁne three categories of factual errors , which   encompass known incorrect statements .   A.2.1 Bad Math   Generated text will sometimes have issues with   basic mathematical operations of known quanti-   ties ( e.g. , “ half of ten apples is four ” ) , problems   converting ﬁxed units ( e.g. , m to cm ) .We also include problems converting currencies   that are wildly implausible under modern assump-   tions ( e.g. , £ 1 = $ 18 US ) .   A.2.2 Com mon sense   These errors mark spans that violate our every-   day basic understanding of the world . Though it   is challenging to precisely deﬁne commonsense   knowledge ( Liu and Singh , 2004 ) , we include non-   encyclopedic knowledge and basic reasoning .   The following example concerns broadly sensi-   ble numerical ranges .   The next example involves world knowledge ,   akin to scripts ( Schank and Abelson , 1977 ) .   The following example involves lexical entail-   ment .   The ﬁnal example involves time .   A.2.3 Encyclopedic   These errors are ones that we know are factu-   ally wrong , and that we could look up in , say ,   Wikipedia .   137262   The distinction between Encyclopedicerrors ,   and the upcoming Tech nicalJargon and Needs   Google issues , depends on the reader ’s knowledge .   A.3 Reader Issues   We deﬁne two categories of reader issues . These   are words or statements a reader can not verify with-   out using an external resource .   A.3.1 Tech nicalJargon   Sometimes generated text includes speciﬁc words   from a ﬁeld that requires expertise to understand .   Which words are jargon depends on the reader ’s   particular expertise . This means Tech nicalJar-   gon spans are more accurately thought of as   potential issues rather than known errors .   A.3.2 Needs Google   Many facts — especially those involving speciﬁc   people , events , dates , or numbers — could be cat-   egorized as encyclopedic knowledge . However ,   whether the fact is accurate may require additional   veriﬁcation by the everyday reader . To make this   distinction between known encyclopedic knowl-   edge and trivia , we introduce this label to denote   that a reader would need to search online to verify   whether it is true .   We instruct annotators to notlook up facts   marked with the Needs Google span . We do   this to keep the focus of the task on classiﬁcation ,   rather than factuality detection . As a result , Needs   Google spans mark statements that would need to   be veriﬁed , rather than known errors .   To illustrate the annotation methodology and   schema in practice , we present four complete ex-   ample annotations in Figure 7 . This ﬁgure also   illustrates how much variation we see across mod-   els .   B Annotation Details   B.1 Error Severity   We provide here examples for each of the three er-   ror severity levels , which we also give to annotators   during training .   147263   B.2 Grading Details   In the training material , there are 10 annotation   exercises , 10 multiple choice questions , and 1 real   task question to test workers ’ understanding .   Annotation Exercise After going through each   error type , there is an annotation exercise . Workers   are asked to mark the span with that particular error   in a short text . Each exercise is worth 5 points .   Multiple Choice Question After going through   alllanguage errors , and going through all factual   errors and reader issues , there is a language error   label quiz and a reader andfactual error label quizrespectively . Each label quiz consists of 5 multi-   ple choice questions , where workers are asked to   choose the error type of a marked span in a short   text . Each multiple choice question is worth 3   points .   Real Task Question At the end of the whole   training material , workers are asked to apply what   they learn in an actual task where they annotate a   given paragraph with full tool like ones shown in   Figure 7 . This question is worth 20 points . We   mark 7 error spans as the solution . As long as   they can mark 5 of 7 error spans , they get a full 20   157264points . Otherwise , 4 points will be deducted for   each missing error span .   In total , there are 100 points . We pass workers if   they score90 points , and then they are provided   with the solution to review .   C Data Quality   Identifying and classifying errors in potentially   noisy machine - generated text is a challenging task .   How consistent are the annotations collected from   crowd workers ? In this section , we examine the   agreement and variability of the collected annota-   tions .   At a high level , we observe either acceptable or   high inter - annotator agreement across error cate-   gories . For rare error types such as Bad Math ,   high agreement stems from the prevalence of spans   with no error . For such categories , we recommend   treating each annotator as a high precision , low   recall judge , and considering the information from   their aggregate annotations . Figure 8 gives an ex-   ample of the perspective gained by viewing all 10   annotations of a single generation .   Agreement Table 4 shows token - level inter-   annotator agreement statistics aggregated over all   collected data . Since a single annotator can la-   bel a single span with multiple errors , we break   the agreement statistics down by error category .   We report Krippendorff ’s  coefﬁcient , a chance-   corrected measure of agreement for multiple anno-   tators ( Krippendorff , 2018 ) . Due to computational   constraints , we calculate this coefﬁcient per gener-   ation and report the average across the dataset . The   agreement shown here is high for most categories   ( > 0.8 ) and acceptable ( > 0.6 ) for all error types .   The Krippendorff measure may be deceptively   high for some error types such as Bad Math , where 99 % of tokens are not annotated with this   error . The Two Agree measure in Table 4 gives a dif-   ferent characterization of this data . Two Agree for a   given error label is the percentage of tokens labeled   by at least one annotator that were also labeled   by one or more additional annotators . This metric   allows us to see where annotators agree that par-   ticular errors exist while ignoring the majority of   tokens ( for most error categories ) which annotators   agree are not errors . Two Agree shows signiﬁcantly   lower rates for sparse errors with high Krippendorff   scores , such as Encyclopedic . However , it reveals   stronger agreement among Incoherentand Off-   Prompt errors than might be expected given the   Krippendorff coefﬁcient .   A limitation for both metrics is the use of token-   based overlap .   Bootstrap One issue we face is high variance   of annotations . To determine the impact of this   variance for lower - data settings , we perform a boot-   strap analysis using largest subset of our data ( GPT-   3 , top - p= 0:96,t= 1 , f.p . = 0 , for which we   have annotations of 200 + generations ) . We choose   50 generations ( roughly 500 annotations ) and cal-   culate the error statistics therein . We repeat this   process 1000 times and report the mean , standard   deviation , and coefﬁcient of variation in Table 5 .   We also calculate the coefﬁcient of variation for dif-   ferent numbers of samples , shown in Figure 9 . We   see that as the number of samples increases , the co-   efﬁcient of variation decreases as expected , though   less precipitously after 30 examples . These results   show that with as few as 50 documents , the S - error analysis should yield relatively robust   results . However , this varies by error type : rare   errors like Bad Math and Encyclopedicshow   greater variance . Here , again we repeat our recom-   mendation to treat annotations for these categories   in aggregate . These results motivate our collection   of at least 500 annotations per condition studied .   D Dataset Statistics   We list the data collection quantities in Table 6 ,   and plot visualizations of three aspects : prompt   topic and annotated span proportions are shown in   Figure 10 , and average span lengths are shown in   Figure 11 .   E Detailed Analysis   In this section we perform a detailed analysis of   the trends of individual error types and decoding   167265   conﬁgurations .   To begin , we consider apples - to - apples model de-   coding conﬁgurations . To expand on these results ,   originally presented in Figure 2 , we also present   two additional ways of counting error spans , which   we show in Figure 12 . While our method for count-   ing errors throughout the paper takes into account   the number of tokens covered in each span ( span   coverage ) , we also show plots for scaling each span   by its severity level ( span coverageseverity ) , and   by ignoring both severity and token length ( simply   span counts ) . These changes in measurement fur-   ther illuminate model error characters , which we   discuss in the upcoming sections ( refer to Figure   12 ) .   177266   E.1 Off - Prompt   Under initial analysis of span coverage , Off-   Prompt errors show a model plateau at GPT-3 .   Measuring span counts offers barely perceptible im-   provement , indicating that scaling language models   over more in - domain training does not guarantee   topicality .   This observation is consistent with growing work   onprompt programming as a new technique for   attempting to steer large pretrained models to com-   plete the desired task ( Branwen , 2020 ; Gao et al . ,   2020 ; Reynolds and McDonell , 2021 ) . In practice ,   we observe that while GPT-3 will sometimes con-   tinue a prompt by writing an article , other times , it   may elaborate on the prompt itself :   Of course , this generation is not literally Off-   Prompt , but it is out of place when other genera-   tions are continuations of the prompt , rather than   further elaborations of it .   While avoiding Off - Prompt errors for lan-   guage models is worth exploring with prompt pro-   gramming and other avenues , an investigation of   these techniques is outside the scope of this work .   Finally , we note that Off - Prompt spans are   the most prevalent error ( not reader issue ) marked   for human - authored text . We suggest that a higher   rate of false positives for this error type , coupled   with its prevalence in model - generated text , makes   further reﬁnement of this error a compelling avenue   for further study .   E.2 Self - Contradiction   While changing from span coverage tospan counts   alters the relative order of GPT-2 XL and Grover   ( though still within conﬁdence bounds ) , the puz-   zling question is why GPT-2 Small performs better   than most ( or all ) other models . Why would the   smallest model produce the fewest Self - Contra-   187267   diction errors ?   We posit the reason is that GPT-2 generations   are so Incoherent and Off - Prompt that there   is little opportunity for relevant , comprehensible   points to be made and then reversed . For example ,   see the GPT-2 Small annotated generation in the top   left of Figure 7 . The entire text is covered by Off - Prompt and Incoherent errors . If we look at   GPT-2 Small ’s error distribution in Figure 3 , we see   most of its added density comes from signiﬁcantly   197268more Off - Prompt and Incoherenttokens .   E.3 Redundant   The different counting methods shown in Figure 12   reveal a change in the results for Redundant er-   rors . Rather than repetition simply increasing as   models grow larger , we observe that GPT-3 repeats   in a similar number of cases ( lower span counts ) ,   but for more tokens ( higher span coverage ) . This   matches the qualitative observation that GPT-3 pro-   duces larger topically repetitive blocks , rather than   simple word or phrase repetitions generated by   GPT-2 - sized models :   Such repetitions can be more difﬁcult to clearly   isolate , because even slight wording changes pro-   duce variations in tone and connotation . Rather   than being identical semantically , we observe GPT-   3 will seem stuck on a particular topic , elaborating   on and rephrasing similar ideas more times than a   human writer ( hopefully ) would .   E.4 Reader Issues   We observe the highest number of Needs   Google and Tech nicalJargon issues in human-   authored text .   Needs Google issues broadly represent any spe-   ciﬁc claim that could be fact - checked . In our do-   main ( news articles ) , these are primarily whether an   event happened on a particular day , whether a per-   son holds a role , or whether a mechanism works as   described ( e.g. , chemical or technical ) . As seen in   Figure 13 ( which shows GPT-3 ’s span distribution ) ,   Needs Google issues happen roughly equally for   all topics . We believe this trend is due to the news   article domain , which is prone to a high density of   speciﬁc information . As such , for other domains ,   this trend may be less prevalent , more difﬁcult to   label ( e.g. , subtle claims assumed to be true in long   running text ) , or both .   We observe that Tech nicalJargon issues are   inﬂuenced by topic ( Figure 13 , bottom ) , occurring   signiﬁcantly more frequently in Business , Health ,   Science , andTechnology topics than in others . This   trend displays a clear topic - dependence even within   a single broader domain ( news ) . These results in-   dicate that both reader issues are characteristics of   natural text . Of course , one might wish to measure   or minimize potential reader issues for a particu-   lar application — for example , claim veriﬁcation , or   controlling for reading level .   E.5 Decoding Hyperparameters   We discuss the effects of the decoding hyperpa-   rameters we consider — top- p , temperature , and fre-   quency penalty — on generation quality . For the   sake of annotation cost , we only vary these param-   eters for the strongest model available , GPT-3 .   First , we show the effect of varying top- pand   temperature alone ( i.e. , with no frequency penalty )   on different error types . Figure 14 shows the effect   on two salient spans : Off - Prompt andRedun-   dant . ( We omit others for space . ) We observe that   annotators naturally label errors the way we would   207269   intuitively expect the model to produce them , given   the hyperparameter changes . The bottom - right cor-   ner of each subplot , where t= 1andp= 0:96 , is   the conﬁguration with the highest amount of ran-   domness from sampling . As we move away from   that corner — either left by lowering temperature ,   or up by lowering top- p — we lower the amount   of randomness . We observe a positive correlation   with randomness and Off - Prompt errors , and an   inverse correlation with Redundant errors . In   other words , sampling from a larger set of words   makes the model more prone to changing topics ,   but less likely to repeat itself , and vice versa .   After conﬁrming these intuitive measures , we   turn our attention to Figure 15 , which investigates   the overall error spans for GPT-3 both without ( left )   and with ( right ) the frequency penalty . ( Note that   unlike Figure 14 , both heatmaps in Figure 15 have   the same color scale . ) We observe that introducing   the frequency penalty lowers error rates for every   value of temperature and top- pthat we try . Further-   more , it appears to reverse the trend seen without a   frequency penalty : that sampling from a larger set   of words produces fewer errors .   The overall results for all decoding conﬁgura-   tions were shown previously in Figure 4 . In the   next section , we focus on the GPT-3 decoding con-   ﬁguration that produced the fewest number of er-   rors , and compare it to human authored text .   E.6 Best GPT-3 vs. Humans   The best GPT-3 conﬁguration shown in Figure   4 — argmax sampling with frequency penalty = 1 —   appears to match error rates seen in human text . Is   the text generated by this model truly as error - free   as news articles ?   We ﬁrst look at the error composition of both   sets of annotations . To get a clear picture of the po-   tential problems , we plot only error spans ( ignoring   reader issues ) , and we omit length scaling , instead   plotting span counts . This breakdown is shown in   the left plot of Figure 16 . The error compositions   are similar , the largest differences being more Re-   dundant errors for GPT-3 , and more Gram mar   andUsage errors for human - authored text .   Next , we perform a manual analysis of 160 er-   rors , sampling 10 at random from each of the 8   error types for each model ( GPT-3 and human-   authored text ) . We show the results in the center   plot of Figure 16 . We notice that a greater portion   of errors in human - authored text were due to arti-   facts present in the text - only format of the Common   Crawl . For example , links to other articles or ad-   vertisements sometimes appear in the middle of an   article ’s text . While annotators were quick to mark   these spans , they reﬂect errors in formatting , not   in writing . We partition these errors separately and   exclude them from the subsequent calculations .   217270   Finally , we scale each error type ’s prevalence for   each model ( i.e. , the left plot of Figure 16 ) by the   portion of errors that we estimate to be legitimate   based on our manual annotation ( i.e. , Figure 16 ,   center ) to produce the right plot of Figure 16 . After   taking into account each error type ’s frequency , we   estimate that 48 % of GPT-3 ’s worker - annotated   errors overall are legitimate , compared to 9 % for   human - written articles .   This analysis suggests two ﬁndings . First ,   human - authored news paragraphs contain many   times fewer issues than text authored by GPT-3 us-   ing the best decoding conﬁguration we tested . Sec-   ond , the noise of error annotations may be as high   as 90 % when assessing high - quality text . Though   it would require further manual annotation to ver-   ify , we conjecture that the trend of GPT-3 ’s error   spans being more reliable ( only 50 % noise ) would   continue , and that text generated by GPT-2 would   contain even fewer false positives . We note that   such rates are not ﬁxed — after all , the manual an-   notations were done by one of the authors simply   by reading carefully — but that more realistic text   may require correspondingly more effort by human   annotators .   E.7 Topics   As noted in § 5.3 , we collect data using prompts   drawn primarily from 12–14 news topics . For con-   ciseness , we show results only for GPT-3 , and only   for the standard apples - to - apples decoding conﬁgu-   ration .   Figure 17 plots , based on the prompt topics , the   average portion of the generation that is covered by   error spans . While there is no signiﬁcant difference   between most topics , the results do indicate that   generating text in more technical domains leads to   higher span counts .   Figure 13 shows individual span prevalence by   topic . The top heatmap normalizes each topic ( col-   227271umn ) independently . Needs Google issues and   Off - Prompt errors dominate the error types , with   a few exceptions : for History , and Nature articles ,   Redundant trumps Off - Prompt as a source of   errors .   For the bottom , if we instead normalize by er-   ror label ( row ) , we can observe which topics are   more prone to certain error types than others . For   example , we can see Bad Math errors are most   common in Business andHealth generations ; Enter-   tainment causes the most Self - Contradiction er-   rors ; and Tech nicalJargon issues appears more   frequently in articles about Business , Technology ,   orHealth .   E.8 Error explanations   Figure 18 displays word clouds for common uni-   grams and bigrams found in the error explanations   for each error type , and Figure 19 shows the aver-   age explanation lengths for each error type . For   Tech nicalJargon , Redundant , and Needs   Google error types , the prominent words do not   provide much illumination and they have short av-   erage explanation length , indicating that the ex-   planations are straightforward afﬁrmations of the   category ( “ I think this is ﬁnancial jargon , ” “ The   information is repeated , ” or“I would need Google   to check this . ” ) . But for categories like Encyclo-   pedicand Bad Math , we observe some coarse   trends : “ year ” is prevalent in both , “ movie ” ap-   pears in Encyclopedic , and “ million ” is present in   Bad Math , which suggests that the explanations   are more likely from outside knowledge and needs   some calculation ( “ The iPhone uses a lightening   connector not a L - shaped connector , ” or “ 5000   feet is 1524 meters . ” )   Figure 20 presents a few representative explana-   tions for four error types , taking particular note of   their explanation lengths ( Figure 19 ) . Both Self-   Contradiction and Redundant errors have an-   tecedents , but their explanations are markedly dif-   ferent . Explanations for Self - Contradiction con-   tain more information describing the particular se-   mantics that is reversed , which are less obvious at   ﬁrst glance than other errors . On the other hand ,   Redundant errors are more straightforward to   spot , often involving simple lexical overlap , and so   do n’t require elaboration .   Explanations for Com mon sense contain the   true commonsense knowledge that the text violates ,   which may take several words to explain . But anexplanation for a Gram mar and Usage error   simply corrects the error ; as these errors are easier   to ﬁx , the explanation lengths are often short .   F Future Work   We outline several further directions of study cen-   tering around the S annotation frame-   work , considering both natural implications and   broader steps .   F.1 S Studies : Simple   Find the best - performing GPT-3 decoding hy-   perparameters . We observed that for GPT-3 , a   frequency penalty value of 1 with argmax sampling   produced fewer error spans than any other conﬁg-   uration ( Fig . 4 ) . We have not tried varying the   frequency penalty to values between 0 and 1 , or   adding any presence penalty ( § 5.2 ) , both of which   then allow for fresh explorations of top- pand tem-   perature .   Study decoding parameters in smaller models .   How good can ( a ﬁnetuned ) GPT-2 get ? We saw   decoding parameters considerably impacted GPT-   3 ’s performance , moving it from edging out Grover   to error rates close to humans ( Fig . 4 ) . Could such   decoding changes have a similar effect on a GPT-   2 - sized model ? Or might a smaller model favor   different decoding hyperparameteres ?   Back - off annotations . We observed good anno-   tator agreement given the complexity of the task ,   but the odds that two annotators agree exactly on   each span ’s type and boundaries remains only mod-   erate ( § C ) . We did not try backing - off ( a ) error   types into coarser categories ( e.g. , language , fac-   tual , reader issue ) or even to binary presence ; ( b )   span boundaries into phrase or sentence - level an-   notations . Applying a type of back - off could also   allow clustering methods to discover different error   ontologies .   Improve automatic error detection . While we   present baseline results for automatic span error de-   tection ( § 6 ) , we anticipate that signiﬁcant progress   is still available in this new task .   F.2 S Studies : Complex   Align multiple annotations . In the current work ,   we largely treat annotators independently , with the   exception of measuring their overlap to study agree-   ment ( § C ) or taking their union to train prediction   model ( § 6 ) . However , we might consider other   237272   ways of viewing the 10 annotations for each gener-   ation together . For example , we might consider the   aggregate decision of whether a token is labeled   with anyspan a measure of how noticeable or jar-   ring an error is . This measure may be related to   error severity , but may be distinct from it .   One might also consider formal methods for   computing annotation alignments . The Gamma   measure , proposed by Mathet et al . ( 2015 ) , satisﬁes   the long list of criteria needed to align and mea-   sure S annotations : spans of multiple   types , with gaps , full and partial span overlap , more   than three annotators , and the potential to merge   or split annotations ( which we have not addressed   in this paper ) . While we performed experiments   with this measure , we experienced difﬁculties pro-   ducing intuitive alignments with the authors ’ soft-   ware , which disallows conﬁguring parameters of   the mixed - integer programming problem . Emerg-   ing concurrent work ( Titeux and Riad , 2021 ) offers   a reimplementation of this measure that exposes   additional parameters , which may be a promising   avenue . However , it is possible that aligning anno-   tations is a challenging task on its own that might   247273require use of the explanations .   Characterize error nuance . Related to the pre-   vious point about error alignment , one might study   whether model size affects span agreement . Anec-   dotally , errors from larger models like GPT-3 —   even of the same type , like Com mon sense errors —   are more difﬁcult to describe without careful con-   sideration , and may also be more difﬁcult to iden-   tify .   Characterize repetition . Our quantitative stud-   ies of Redundant errors ( e.g. , Figs . 14 and 12 )   point to semantic repetition as the major issue that   emerges as models are scaled . Though this effect   may be mitigated by changes to the decoding algo-   rithm ( like the frequency penalty ) , we still observe   that models have difﬁculty striking a balance of   repetition . With excessive paraphrasing , generated   text seems stuck on an idea . But equally , if a gen-   eration moves too quickly between ideas without   linking them together or to an overall theme , the   text lacks coherence . We posit that the issue of   Redundant text emerges as the shadow of encom-   passing issues of narrative structure and discourse .   F.3 Broadening S   Constrained generation This paper focuses on   open - ended generation , but a natural extension of   this method would be to assessing constrained gen-   eration tasks , such as machine translation .   New error types Especially if considering a   novel task setting , new error types may prove use-   ful . For example , in constrained generation , one   might consider an Adequacy error , which — as   in machine translation — would indicate that the   meaning of a span diverges from what is expected   given the generation constraints . Furthermore , one   might need to introduce annotations on the pro-   vided ( not generated ) text to account for desired   semantic components that are missing from the gen-   erated text . Or , perhaps for a dialog setting , one   might introduce a Generic label , which would   indicate that a portion of the generation is other-   wise coherent and correct , but offers a lack of new   information .   Corpus - level evaluation Other work has consid-   ered the evaluation of natural language generationsat - scale , looking at distributional properties of the   text ( Caccia et al . , 2020 ; Pillutla et al . , 2021 ) . We   suggest that these views are complementary to   instance - based , human evaluation proposed here ,   and combining the approaches could lead towards a   more holistic view of generative evaluation . For ex-   ample , while all Self - Contradiction errors right   now are within - document , one could similarly iden-   tifycross - document contradiction errors , where a   model is inconsistent at a more global scale .   F.4 Applications   Detecting factuality One potential application   of the S data could be using the Needs   Google spans as a dataset of its own . In addition   to training models to identify spans that require ver-   iﬁcation , one could go a step further and consider   evidence retrieval for each span , and even propose   a classiﬁcation task .   Editing errors One errors can be detected , can   they be ﬁxed ? The difﬁculty and scope of ﬁxing   S -identiﬁed errors may depend on the   error type , as error ﬁxes may have cascading effects   in the rest of the document .   257274