  Yu WanDayiheng LiuBaosong YangHaibo ZhangBoxing Chen   Derek F. WongLidia S. ChaoNLPCT Lab , University of Macau   nlp2ct.ywan@gmail.com , { derekfw , lidiasc}@umac.moAlibaba Group   { liudayiheng.ldyh , yangbaosong.ybs , zhanhui.zhb ,   boxing.cbx}@alibaba-inc.com   Abstract   Translation quality evaluation plays a crucial   role in machine translation . According to   the input format , it is mainly separated into   three tasks , i.e. , reference - only , source - only   and source - reference - combined . Recent meth-   ods , despite their promising results , are specif-   ically designed and optimized on one of them .   This limits the convenience of these meth-   ods , and overlooks the commonalities among   tasks . In this paper , we propose UniTE , which   is the Ô¨Årst uniÔ¨Åed framework engaged with   abilities to handle all three evaluation tasks .   Concretely , we propose monotonic regional at-   tention to control the interaction among in-   put segments , and uniÔ¨Åed pretraining to bet-   ter adapt multi - task learning . We testify our   framework on WMT 2019 Metrics and WMT   2020 Quality Estimation benchmarks . Exten-   sive analyses show that our single model can   universally surpass various state - of - the - art or   winner methods across tasks . Both source   code and associated models are available at   https://github.com/NLP2CT/UniTE .   1 Introduction   Automatically evaluating the translation quality   with the given reference segment(s ) , is of vital   importance to identify the performance of Ma-   chine Translation ( MT ) models ( Freitag et al . , 2020 ;   Mathur et al . , 2020a ; Zhao et al . , 2020 ; Kocmi   et al . , 2021 ) . Based on the input contexts , trans-   lation evaluation can be mainly categorized into   three classes : 1 ) reference - only evaluation ( R )   approaches like BLEU ( Papineni et al . , 2002 ) and   BLEURT ( Sellam et al . , 2020a ) , which evaluate the   hypothesis by referring the golden reference at tar-   get side ; 2 ) source - only evaluation ( S ) methods   like YiSi-2 ( Lo , 2019 ) and TransQuest ( Ranasingheet al . , 2020b ) , which are also referred as quality es-   timation ( QE ) . These methods estimate the quality   of the hypothesis based on the source sentence with-   out using references ; 3 ) source - reference - combined   evaluation ( S+R ) works like COMET ( Rei   et al . , 2020 ) , where the evaluation exploits informa-   tion from both source and reference . With the help   of powerful pretrained language models ( PLMs ,   Devlin et al . , 2019 ; Conneau et al . , 2020 ) , model-   based approaches ( e.g. , BLEURT , TransQuest , and   COMET ) have shown promising results in recent   WMT competitions ( Ma et al . , 2019 ; Mathur et al . ,   2020b ; Freitag et al . , 2021 ; Fonseca et al . , 2019 ;   Specia et al . , 2020 , 2021 ) .   Nevertheless , each existing MT evaluation work   is usually designed for one speciÔ¨Åc task , e.g. ,   BLEURT is only used for Rtask and can not   support Sand S+Rtasks . Moreover ,   those approaches preserve the same core ‚Äì evalu-   ating the quality of translation by referring to the   given segments . We believe that it is valuable , as   well as feasible , to unify the capabilities of all MT   evaluation tasks ( R , Sand S+R )   into one model . Among the promising advantages   are ease of use and improved robustness through   knowledge transfer across evaluation tasks . To   achieve this , two important challenges need to be   addressed : 1 ) How to design a model framework   that can unify all translation evaluation tasks ? 2 )   How to make the powerful PLMs better adapt to   the uniÔ¨Åed evaluation model ?   In this paper , we propose UniTE -UniÔ¨Åed   Translation Evaluation , a novel approach which   uniÔ¨Åes the functionalities of R , S and   S+Rtasks into one model . To solve the   Ô¨Årst challenge as mentioned above , based on the   multilingual PLM , we utilize layerwise coordina-   tion which concatenates all input segments into one   sequence as the uniÔ¨Åed input form . To further unify   the modeling of three evaluation tasks , we propose   a novel Monotonic Regional Attention ( MRA ) strat-8117egy , which allows partial semantic Ô¨Çows for a spe-   ciÔ¨Åc evaluation task . For the second challenge , a   multi - task learning - based uniÔ¨Åed pretraining is pro-   posed . To be concrete , we collect the high - quality   translations and degrade low - quality translations   of NMT models as synthetic data . Then we pro-   pose a novel ranking - based data labeling strategy   to provide the training signal . Finally , the mul-   tilingual PLM is continuously pretrained on syn-   thetic dataset with multi - task learning manner . Be-   sides , our proposed models , named UniTE - MRA   and UniTE - UP respectively , can beneÔ¨Åt from Ô¨Åne-   tuning with human - annotated data over three tasks   at once , not requiring extra task - speciÔ¨Åc training .   Experimental results demonstrate the superior-   ity of UniTE . Compared to various strong base-   line systems on each task , UniTE , which uniÔ¨Åes   R , Sand S+Rtasks into one sin-   gle model , achieves consistently absolute improve-   ments of Kendall ‚Äôs  correlations at 1.1 , 2.3 and   1.1 scores on English - targeted translation directions   of WMT 2019 Metric Shared task ( Fonseca et al . ,   2019 ) , respectively . Meanwhile , after introduc-   ing multilingual - targeted support for our uniÔ¨Åed   pretraining strategy , a single model named UniTE-   MUP also gives dominant results against existing   methods on non - English - targeted translation eval-   uation tasks . Furthermore , our method can also   achieve competitive results over WMT 2020 QE   task compared with the winner submission ( Ranas-   inghe et al . , 2020b ) . Ablation studies reveal that ,   the proposed MRA and uniÔ¨Åed pretraining strate-   gies are both important for model performance ,   making the model preserve the outstanding perfor-   mance and multi - task transferability concurrently .   2 Related Work   In this section , we brieÔ¨Çy introduce the three direc-   tions of translation evaluation .   2.1 Reference - Only Evaluation   Rassesses the translation quality via compar-   ing the translation candidate and the given refer-   ence . In this setting , the two inputs are written   in the same language , thus being easily applied   in most of the metric tasks . In the early stages ,   statistical methods are dominant solutions due to   their strengths in wide language support and intu-   itive design . These methods measure the surface   text similarity for a range of linguistic features ,   including n - gram ( BLEU , Papineni et al . , 2002 ) , to - ken ( TER , Snover et al . , 2006 ) , and character ( ChrF   & ChrF++ , Popovic , 2015 , 2017 ) . However , recent   studies pointed out that these metrics have low con-   sistency with human judgments and insufÔ¨Åciently   evaluate high - qualiÔ¨Åed MT systems ( Freitag et al . ,   2020 ; Rei et al . , 2020 ; Mathur et al . , 2020a ) .   Consequently , with the rapid development of   PLMs , researchers have been paying their at-   tention to model - based approaches . The basic   idea of these studies is to collect sentence repre-   sentations for similarity calculation ( BERTScore ,   Zhang et al . , 2020 ) or evaluating probabilistic con-   Ô¨Ådence ( PRISM - ref , Thompson and Post , 2020 ;   BARTScore , Yuan et al . , 2021 ) . To further improve   the model , Sellam et al . ( 2020a ) pretrained a spe-   ciÔ¨Åc PLM for the translation evaluation ( BLEURT ) ,   while Lo ( 2019 ) combined statistical and repre-   sentative features ( YiSi-1 ) . Both these methods   achieve higher correlations with human judgments   than statistical counterparts .   2.2 Source - Only Evaluation   S , which also refers to quality estimation ,   is an important translation evaluation task espe-   cially for the scenario where the ground - truth ref-   erence is unavailable . It takes the source - side   sentence and the translation candidate as inputs   for the quality estimation . To achieve this , the   methods are required to model cross - lingual se-   mantic alignments . Similar to reference - only   evaluation , statistical - based ( Ranasinghe et al . ,   2020b ) , model - based ( TransQuest , Ranasinghe   et al . , 2020b ; PRISM - src , Thompson and Post ,   2020 ) , and feature combination ( YiSi-2 , Lo , 2019 )   are typical and advanced methods in this tasks .   2.3 Source - Reference - Combined Evaluation   Aside from the above tasks that only consider either   source or target side at one time , S+Rtakes   both source and reference sentences into account .   In this way , methods in this context can evaluate the   translation candidate via utilizing the features from   both sides . As a rising paradigm among transla-   tion evaluation tasks , S+Ralso proÔ¨Åts from   the development of cross - lingual PLMs . For ex-   ample , Ô¨Ånetuning PLMs over human - annotated   datasets ( COMET , Rei et al . , 2020 ) achieves new   state - of - the - art results among all evaluation ap-   proaches in WMT 2020 ( Mathur et al . , 2020b).8118   3 Methodology   As mentioned above , massive methods are pro-   posed for different automatic evaluation tasks . On   the one hand , it is inconvenient and expensive to   develop and employ different metrics for differ-   ent evaluation scenarios . On the other hand , sepa-   rate models absolutely overlook the commonalities   among these evaluation tasks , of which knowledge   potentially beneÔ¨Åts all three tasks . In order to ful-   Ô¨Åll the aim of unifying the functionalities on R ,   S , and S+Rinto one model , in this sec-   tion , we introduce UniTE ( Figure 1 ) .   3.1 Model Architecture   By receiving a data example composing of hypoth-   esis , source , and reference segment , UniTE Ô¨Årst   modiÔ¨Åes it into concatenated sequence following   the given setting as R , S , or S+R :   x = Concat ( h;r)2R ;   x = Concat ( h;s)2R ; ( 1 )   x = Concat ( h;s;r)2R ;   where h , sandrare hypothesis , source and refer-   ence segments , with the corresponding sequence   lengths being l , landl , respectively . The input   sequence is then fed to PLM to derive representa-   tions ~H. Take Ras an example :   ~H = PLM(x)2R ; ( 2 )   wheredis the model size of PLM . According   to Ranasinghe et al . ( 2020b ) , we use the Ô¨Årst output   representation as the input of feedforward layer . Compared to existing methods ( Zhang et al . ,   2020 ; Rei et al . , 2020 ) which take sentence - level   representations for evaluation , the advantages of   our architecture design are as follows . First , our   UniTE model can beneÔ¨Åt from layer - coordinated   semantical interactions inside every one of PLM   layers , which is proven effective on capturing di-   verse linguistic features ( He et al . , 2018 ; Lin et al . ,   2019 ; Jawahar et al . , 2019 ; Tenney et al . , 2019 ;   Rogers et al . , 2020 ) . Second , for the uniÔ¨Åed ap-   proach of our model , the concatenation provides   the unifying format for all task inputs , turning our   model into a more general architecture . When con-   ducting different evaluation tasks , our model re-   quires no further modiÔ¨Åcation inside . Note here ,   to keep the consistency across all evaluation tasks ,   as well as ease the uniÔ¨Åed learning , his always   located at the beginning of the input sequence .   After deriving ~H , a pooling block is arranged   after PLM which gives sequence - level representa-   tionsH. Finally , a feedforward network takes   Has input , and gives a scalar pas prediction :   H = Pool ( ~H)2R ; ( 3 )   p = FeedForward ( H)2R:(4 )   For training , we encourage the model to reduce the   mean squared error with respect to given score q :   L= ( p q ): ( 5 )   However , for the pretraining of most PLMs ( e , g. ,   XLM - R , Conneau et al . , 2020 ) , the input patterns   are designed to receive two segments at most . Thus   there exists a gap between the pretraining of PLM8119and the joint training of UniTE where the concate-   nation of three fragments is used as input . More-   over , previous study ( Takahashi et al . , 2020 ) shows   that directly training over S+Rby follow-   ing such design leads to worse performance than   Rscenario . To alleviate this issue , we propose   two strategies : Monotonic Regional Attention as   described in ¬ß 3.2 and UniÔ¨Åed Pretraining in ¬ß 3.3 .   3.2 Monotonic Regional Attention   To Ô¨Åll the modeling gap between the pretraining of   PLM and the joint training of three downstream   tasks , a natural idea is to unify the number of   involved segments when modeling semantics for   S , Rand S+Rtasks . Following   this , we propose to modify the attention mask of   S+Rto simulate the modeling of two seg-   ments in Sand R. SpeciÔ¨Åcally , when cal-   culating the attention logits , semantics from a spe-   ciÔ¨Åc segment are only allowed to derive informa-   tion from two segments at most . Considering the   conventional attention module :   A= Softmax(QK   p   d)2R ; ( 6 )   whereLis the sequential length for input , Q;K2   Rare query and key representations , respec-   tively . As to monotonic regional attention ( MRA ) ,   we simply add a mask Mto the softmax logits to   control attention Ô¨Çows :   A= Softmax(QK   p   d+M)2R;(7 )   M= (    1 ( i;j)2U ;   0 otherwise;(8 )   where Ustores the index pairs of all masked areas .   Following this idea , the key of MRA is how to   design the matrix U. For the cases where interac-   tions inside each segment , we believe that these   self - interactions are beneÔ¨Åcial to the modeling . For   other cases where interactions are arranged across   segments , three patterns are included : hypothesis-   reference , source - reference , and hypothesis - source .   Intuitively , the former two parts are beneÔ¨Åcial for   model training , since they might contribute the   monolingual signals and cross - lingual disambigua-   tion to evaluation , respectively . This leaves the   only case , where our experimental analysis also   veriÔ¨Åes ( see ¬ß 5.1 ) , that interaction between hypoth-   esis and source leads to the performance decrease   forS+Rtask , thus troubling the unifying .   To give more Ô¨Åne - grained designs , we propose   two approaches for UniTE - MRA , which apply the   MRA mechanism into UniTE model ( Figure 2 ):   ‚Ä¢Hard MRA . Only monotonic attention Ô¨Çows   are allowed . Interactions between any two seg-   ments are strictly unidirectional through the   entire PLM , where Ustores the index pairs of   unidirectional interactions of h!r , s!r   andh!s , where ‚Äú ! ‚Äù denotes the direction   of attention Ô¨Çows .   ‚Ä¢Soft MRA . SpeciÔ¨Åc attention Ô¨Çows are for-   bidden inside each attention module . The in-   volved two segments may interact inside a   higher layer . In practice , index pairs which   denoting h!sors!hbetween source   and hypothesis are stored in U.   Note that , although the processing in source and   reference may be affected because their positions   are not indexed from the start , related studies on   positional embeddings reveal that , PLM can well   capture relative positional information ( Wang and   Chen , 2020 ) , which dispels this concern .   3.3 UniÔ¨Åed Pretraining   To further bridge the modeling gap between PLM   and the joint training of UniTE mentioned in ¬ß 3.1 ,   we propose a uniÔ¨Åed pretraining strategy includ-   ing the following main stages : 1 ) collecting and   downgrading synthetic data ; 2 ) labeling examples   with a novel ranking - based strategy ; 3 ) multi - task   learning for uniÔ¨Åed pretraining and Ô¨Ånetuning .   Synthetic Data Collection As our approach   aims at evaluating the quality of translations , gen-   erated hypotheses with NMT models are ideal syn-   thetic data . To further improve the diversity of   synthetic data quality , we follow existing experi-   ences ( Sellam et al . , 2020a ; Wan et al . , 2021 ) to8120apply the word and span dropping strategy to down-   grade a portion of hypotheses . The collected data   totally contains Ntriplets composing of hypothesis ,   source and reference segments , which is formed as   D = fhh;s;rig .   Data Labeling After obtaining the synthetic   data , the next step is to augment each data pair   with a label which serves as the signal of uniÔ¨Åed   pretraining . To stabilize the model training , as well   as normalize the distributions across all score sys-   tems and languages , we propose a novel ranking-   based approach . This method is based on the idea   of Borda count ( Ho et al . , 1994 ; Emerson , 2013 ) ,   which provides more precise and well - distributed   synthetic data labels than Z - score normalization .   SpeciÔ¨Åcally , we Ô¨Årst use available approaches to   derive the predicted score ^qfor each item , yield-   ing labeled synthetic quadruple examples formed   asD = fhh;s;r;^qig . Then , we tag each   example with its rank index ~qreferring to ^q :   ~q = IndexOf ( ^q;Q ) ; ( 9 )   whereQis the list storing all the sorted ^qdescend-   ingly . Then , we use the conventional Z - score strat-   egy to normalize the scores :   q=~q     ; ( 10 )   whereandare the mean and the standard de-   viation of values in Q , respectively . The dataset   thus updates its format to D = fhh;s;r;qig .   Note here that , an example with higher ^qis as-   signed with higher ~q , thus a larger value of q.   Compared to related approaches which apply Z-   score normalization ( Bojar et al . , 2018 ) , or leave   the conventional labeled scores as signals for learn-   ing ( i.e. , knowledge distillation , Kim and Rush ,   2016 ; Phuong and Lampert , 2019 ) , our approach   can alleviate the bias of chosen model for label-   ing and prior distributional disagreement of scores .   For example , different methods may give scores   with different distributions . Especially for transla-   tion directions of low - resource , scores may follow   skewed distribution ( Sellam et al . , 2020a ) , which   has a disagreement with rich - resource scenarios .   Our method can unify the distribution of all label-   ing data into the same scale , which can also be   easily applied by the ensembling strategy .   Multi - task Pretrainig and Finetuning To unify   all evaluation scenarios into one model , we applymulti - task learning for both pretraining and Ô¨Ånetun-   ing . For each step , we arrange three substeps for all   input formats , yielding L , L , andL ,   respectively . The Ô¨Ånal learning objective is to re-   duce the summation of all losses :   L = L+L+L : ( 11 )   4 Experiments   4.1 Experimental Settings   Benchmarks Following Rei et al . ( 2020 ) ; Yuan   et al . ( 2021 ) , we examine the effectiveness of the   propose method on WMT 2019 Metrics ( Ma et al . ,   2019 ) . For the former , we follow the common   practice in COMET(Rei et al . , 2020 ) to collect   and preprocess the dataset . The ofÔ¨Åcial variant   of Kendall ‚Äôs Tau correlation ( Ma et al . , 2019 ) is   used for evaluation . We evaluate our methods   on all of R , Sand S+Rscenarios .   For Sscenario , we further conduct results on   WMT 2020 QE task ( Specia et al . , 2020 ) referring   to Ranasinghe et al . ( 2020a ) for data collection and   preprocessing . Following the ofÔ¨Åcial report , the   Pearson ‚Äôs correlation is used for evaluation .   Model Pretraining As mentioned in ¬ß 3.3 , we   continuously pretrain PLMs using synthetic data .   The data is constructed from WMT 2021 News   Translation task , where we collect the training sets   from Ô¨Åve translation tasks . Among those tasks , the   target sentences are all in English ( En ) , and the   source languages are Czech ( Cs ) , German ( De ) ,   Japanese ( Ja ) , Russian ( Ru ) , and Chinese ( Zh ) .   SpeciÔ¨Åcally , we follow Sellam et al . ( 2020a ) to use   T -base ( Vaswani et al . , 2017 ) MT   models to generate translation candidates , and use   the checkpoints trained via UniTE - MRA approach   for synthetic data labeling . We pretrain two kinds   of models , one is pretrained on English - targeted   language directions , and the other is a multilingual   version trained using bidirectional data . Note that ,   for a fair comparison , we Ô¨Ålter out all pretraining   examples that are involved in benchmarks .   Model Setting We implement our approach upon   COMET ( Rei et al . , 2020 ) repository and follow   their work to choose XLM - R ( Conneau et al . , 2020 )   as the PLM . The feedforward network consists of   3 linear transitions , where the dimensionalities of8121   corresponding outputs are 3,072 , 1,024 , and 1 , re-   spectively . Between any two adjacent linear mod-   ules inside , hyperbolic tangent function is arranged   as activation . During both pretraining and Ô¨Åne-   tuning phrases , we divided training examples into   three sets , where each set only serves one scenario   among R , Sand S+Rto avoid learn-   ing degeneration . During Ô¨Ånetuning , we randomly   extracting 2,000 training examples from bench-   marks as development set . Besides UniTE - MRA   and UniTE - UP which are derived with MRA ( ¬ß 3.2 )   and UniÔ¨Åed Pretraining ( ¬ß 3.3 ) , we also extend the   latter with multilingual - targeted uniÔ¨Åed pretraining ,   thus obtaining UniTE - MUP model .   Baselines As to Rapproaches , we select   BLEU ( Papineni et al . , 2002 ) , ChrF ( Popovic ,   2015 ) , YiSi-1 ( Lo , 2019 ) , BERTScore ( Zhang et al . ,   2020 ) , BLEURT ( Sellam et al . , 2020a ) , PRISM - ref ( Thompson and Post , 2020 ) , BARTScore ( Yuan   et al . , 2021 ) , XLM - R+Concat ( Takahashi et al . ,   2020 ) , and RoBERTa+Concat ( Takahashi et al . ,   2020 ) for comparison . For Smethods , we   post results of both metric and QE methods , in-   cluding YiSi-2 ( Lo , 2019 ) , XLM - R+Concat ( Taka-   hashi et al . , 2020 ) , PRISM - src ( Thompson and Post ,   2020 ) and multilingual - to - multilingual MTran-   sQuest ( Ranasinghe et al . , 2020b ) . For S+R ,   we use XLM - R+Concat ( Takahashi et al . , 2020 )   and COMET ( Rei et al . , 2020 ) as strong baselines .   Data Collection for UniTE - UP   4.2 Main Results   English - Targeted Results on English - targeted   metric task are conducted in Table 1 . Among   all involved baselines , for R methods ,   BARTScore ( Yuan et al . , 2021 ) performs better   than other statistical and model - based metrics.8122   As to Sscenario , MTransQuest ( Ranasinghe   et al . , 2020b ) gives dominant performance . Fur-   ther , COMET ( Rei et al . , 2020 ) performs better   than XLM - R+Concat ( Takahashi et al . , 2020 ) on   S+Rscenario .   As for our methods , we can see that , UniTE-   MRA achieves better results on all tasks , demon-   strating the effectiveness of monotonic attention   Ô¨Çows for cross - lingual interactions . Moreover , the   proposed model UniTE - UP , which uniÔ¨Åes R ,   S , and S+Rlearning on both pretrain-   ing and Ô¨Ånetuning , yields better results on all eval-   uation settings . Most importantly , UniTE - UP is   asingle model which surpasses all the different   state - of - the - art models on three tasks , showing its   dominance on both convenience and effectiveness .   Multilingual - Targeted As seen in Table 2 , the   multilingual - targeted UniTE - MUP gives dominant   performance than all strong baselines on R ,   Sand S+R , demonstrating the trans-   ferability and effectiveness of our approach . Be-   sides , the UniTE - UP also gives dominant results ,   revealing an improvement of 0.6 , 0.3 and 0.9 av-   eraged Kendall ‚Äôs  correlation scores , respectively .   However , we Ô¨Ånd that UniTE - MUP outperforms   strong baselines but slightly worse than UniTE - UP   on English - targeted translation directions ( see Ta - ble 3 ) . We think the reason lies in the curse of   multilingualism and vocabulary dilution ( Conneau   et al . , 2020 ) .   Quality Estimation The results for UniTE ap-   proach on WMT 2020 QE task are concluded in   Table 4 . As seen , it achieves competitive results   on QE task compared with the winner submis-   sion ( Ranasinghe et al . , 2020b ) .   5 Ablation Studies   In this section , we conduct ablation studies to in-   vestigate the effectiveness of regional attention pat-   terns ( ¬ß 5.1 ) , uniÔ¨Åed training ( ¬ß 5.2 ) , and ranking-   based data labeling ( ¬ß 5.3 ) . All experiments are   conducted by following English - targeted setting .   5.1 Regional Attention Patterns   To investigate the effectiveness of MRA , we further   collect experiments in Table 5 . As seen , MRA can   give performance improvements than full attention ,   and preventing the interactions between hypothesis   and source segment can improve the performance   most . We think the reasons behind are twofold .   First , the source side is formed with a different lan-   guage , whose semantic information is rather weak   than the reference side . Second , by preventing   direct interactions between source and hypothe-8123   sis , semantics inside the former must be passed   through reference , which is helpful for disambigua-   tion . Besides , not allowing the source to derive   information from the hypothesis is better than the   opposite direction . Wang and Chen ( 2020 ) found   that the positional embeddings in PLM are engaged   with strong adjacent information . We think the rea-   son why S!H performs worse than H ! S lies in   the skipping of indexes , which corrupts positional   similarities in alignment calculation .   Additionally , when we combined two methods   together , i.e. , uniÔ¨Åed pretraining and Ô¨Ånetuning   with S+RUniTE - MRA setting , model per-   formance drops to 34.9 over English - targeted tasks   on average . We think that both methods all intend   to solve the problem of unseen S+Rinput   format , and MRA may not be necessary if massive   data examples can be obtained for pretraining . Nev-   ertheless , UniTE - MRA has its advantage on wide   application without requiring pseduo labeled data .   5.2 UniÔ¨Åed Training   Experiments for comparing uniÔ¨Åed and task-   speciÔ¨Åc training are concluded in Table 6 . As   seen , when using the uniÔ¨Åed pretraining check-   point to Ô¨Ånetune over the speciÔ¨Åc task , performance   over three models reveals performance drop con-8124   sistently , indicating that the uniÔ¨Åed Ô¨Ånetuning is   helpful for model learning . This also veriÔ¨Åes our   hypothesis , that the cores of R , S , and   S+Rtasks are identical to each other . More-   over , uniÔ¨Åed pretraining and Ô¨Ånetuning are comple-   mentary to each other . Also , utilizing task - speciÔ¨Åc   pretraining instead of uniÔ¨Åed one reveals worse per-   formance . To sum up , unifying both pretraining   and Ô¨Ånetuning only reveals one model , showing its   advantage on the generalization on all tasks , where   one united model can cover all functionalities of   R , Sand S+Rtasks concurrently .   5.3 Ranking - based Data Labeling   To verify the effectiveness of ranking - based label-   ing , we collect the results of models applying dif-   ferent pseudo labeling strategies . After deriving   the original scores from the well - trained UniTE-   MRA checkpoint , we use Z - score and proposed   ranking - based normalization methods to label syn-   thetic data . For both methods , we also apply an   ensembling strategy to assign training examples   with averaged scores deriving from 3 UniTE - MRA   checkpoints . Results show that , Z - score normal-   ization reveals a performance drop when applying   score ensembling with multiple models . Our pro-   posed ranking - based normalization can boost the   UniTE - UP model training , and its ensembling ap-   proach can further improve the performance .   6 Conclusion   In the past decades , automatic translation eval-   uation is mainly divided into R , Sand   S+Rtasks , each of which develops indepen-   dently and is tackled by various task - speciÔ¨Åc meth-   ods . We suggest that the three tasks are possibly   handled by a uniÔ¨Åed framework , thus being ease   of use and facilitating the knowledge transferring .   Contributions of our work are mainly in three folds:(a ) We propose a Ô¨Çexible and uniÔ¨Åed translation   evaluation model UniTE , which can be adopted   into the three tasks at once ; ( b ) Through in - depth   analyses , we point out that the main challenge of   unifying three tasks stems from the discrepancy   between vanilla pretraining and multi - tasks Ô¨Ånetun-   ing , and Ô¨Åll this gap via monotonic regional atten-   tion ( MRA ) and uniÔ¨Åed pretraining ( UP ) ; ( c ) Our   single model consistently outperforms a variety   of state - of - the - art or winner systems across high-   resource and zero - shot evaluation in WMT 2019   Metrics and WMT 2020 QE benchmarks , showing   its advantage of Ô¨Çexibility and convincingness . We   hope our new insights can contribute to subsequent   studies in the translation evaluation community .   Acknowledgements   The authors would like to send great thanks to all   reviewers and meta - reviewer for their insightful   comments . This work was supported in part by   the Science and Technology Development Fund ,   Macau SAR ( Grant No . 0101/2019 / A2 ) , the Multi-   year Research Grant from the University of Macau   ( Grant No . MYRG2020 - 00054 - FST ) , National   Key Research and Development Program of China   ( No . 2018YFB1403202 ) , and Alibaba Group   through Alibaba Research Intern Program .   References81258126   A Collection of Pretraining Data   Considering the English - targeted model , we select   Czech ( Cz ) , German ( De ) , Japanese ( Ja ) , Russian   ( Ru ) , and Chinese ( Zh ) as source languages , and   English ( En ) as target . For each translation direc-   tion , we collect 1 million samples , Ô¨Ånally yielding   5 million examples in total for uniÔ¨Åed pretrain-   ing . As to the multilingual - targeted model , we   further collect 1 million synthetic data for each   language direction of En - Cz , En - De , En - Ja , En-   Ru , and En - Zh . Finally , we construct 10 million   examples for the pretraining of the multilingual   version by adding the data of the English - targeted   model . Note that , for a fair comparison , we Ô¨Ålter   out all pretraining examples that are involved in   benchmarks .   B Reproducibility   All the models reported in this paper were Ô¨Ånetuned   on a single Nvidia V100 ( 32 GB ) GPU . Specif-   ically for UniTE - UP and UniTE - MUP , the pre-   training is arranged on 4 Nvidia V100 ( 32 GB )   GPUs . Our framework is built upon COMET repos-   itory ( Rei et al . , 2020 ) . For the contribution to the   research community , we release both the source   code of UniTE framework and the well - trained   evaluation models as described in this paper at   https://github.com/NLP2CT/UniTE.8127