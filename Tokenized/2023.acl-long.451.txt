  Wenjun Hou , Kaishuai Xu , Yi Cheng , Wenjie Li , Jiang LiuDepartment of Computing , The Hong Kong Polytechnic University , HKSAR , ChinaResearch Institute of Trustworthy Autonomous Systems and   Department of Computer Science and Engineering ,   Southern University of Science and Technology , Shenzhen , China   houwenjun060@gmail.com   { kaishuaii.xu , alyssa.cheng}@connect.polyu.hk   cswjli@comp.polyu.edu.hk , liuj@sustech.edu.cn   Abstract   This paper explores the task of radiology report   generation , which aims at generating free - text   descriptions for a set of radiographs . One sig-   nificant challenge of this task is how to cor-   rectly maintain the consistency between the im-   ages and the lengthy report . Previous research   explored solving this issue through planning-   based methods , which generate reports only   based on high - level plans . However , these   plans usually only contain the major observa-   tions from the radiographs ( e.g. , lung opacity ) ,   lacking much necessary information , such as   the observation characteristics and preliminary   clinical diagnoses . To address this problem ,   the system should also take the image infor-   mation into account together with the textual   plan and perform stronger reasoning during   the generation process . In this paper , we pro-   pose an Observation - guided radiology Report   Genertioframework ( ORG ) . It first pro-   duces an observation plan and then feeds both   the plan and radiographs for report generation ,   where an observation graph and a tree reason-   ing mechanism are adopted to precisely enrich   the plan information by capturing the multi-   formats of each observation . Experimental re-   sults demonstrate that our framework outper-   forms previous state - of - the - art methods regard-   ing text quality and clinical efficacy .   1 Introduction   Radiology reports , which contain the textual de-   scription for a set of radiographs , are critical in the   process of medical diagnosis and treatment . Nev-   ertheless , the interpretation of radiographs is very   time - consuming , even for experienced radiologistsFigure 1 : Our proposed framework contains two stages ,   including the observation planning stage and the report   generation stage . Red color denotes positive observa-   tions , while Blue color denotes negative observations .   ( 5 - 10 minutes per image ) . Due to its large potential   to alleviate the strain on the healthcare workforce ,   automated radiology report generation ( Anderson   et al . , 2018 ; Rennie et al . , 2017 ) has attracted in-   creasing research attention .   One significant challenge of this task is how to   correctly maintain the consistency between the im-   age and the lengthy textual report . Many previ-   ous works proposed to solve this through planning-   based generation by first concluding the major ob-   servations identified in the radiographs before the   word - level realization ( Jing et al . , 2018 ; You et al . ,   2021 ; Nooralahzadeh et al . , 2021 ; Nishino et al . ,   2022 ) . Despite their progress , these methods still   struggle to maintain the cross - modal consistency   between radiographs and reports . A significant   problem within these methods is that , in the stage   of word - level generation , the semantic information   of observations and radiographs is not fully utilized .   They either generate the report only based on the   high - level textual plan ( i.e. , major observations)8108or ignore the status of an observation ( i.e. , posi-   tive , negative , and uncertain ) , which is far from   adequate . The observations contained in the high-   level plan are extremely concise ( e.g. , lung opac-   ity ) , while the final report needs to include more   detailed information , such as the characteristics of   the observation ( e.g. , a subtle but new lung opacity )   and requires preliminary diagnosis inference based   on the observation ( e.g. , lung infection must be sus-   pected ) . In order to identify those detailed descrip-   tions and clinical inferences about the observations ,   we need to further consider the image information   together with the textual plan , and stronger reason-   ing must be adopted during the generation process .   In this paper , we propose ORG , an   Observation - guided radiology Report Genertio   framework . Our framework mainly involves two   stages , i.e. , the observation planning and the report   generation stages , as depicted in Figure 1 . In the   first stage , our framework produces the observation   plan based on the given images , which includes   the major findings from the radiographs and their   statuses ( i.e. , positive , negative , and uncertain ) . In   the second stage , we feed both images and the   observation plan into a Transformer model to gen-   erate the report . Here , a tree reasoning mechanism   is devised to enrich the concise observation plan   precisely . Specifically , we construct a three - level   observation graph , with the high - level observations   as the first level , the observation - aware n - grams   as the second level , and the specific tokens as the   third level . These observation - aware n - grams cap-   ture different common descriptions of the observa-   tions and serve as the component of observation   mentions . Then , we use the tree reasoning mecha-   nism to capture observation - aware information by   dynamically aggregating nodes in the graph .   In conclusion , our main contributions can be   summarized as follows :   •We propose an Observation - guided radiology   Report Genertioframework ( ORG ) that   can maintain the clinical consistency between   radiographs and generated free - text reports .   •To achieve better observation realization , we   construct a three - level observation graph con-   taining observations , n - grams , and tokens   based on the training corpus . Then , we per-   form tree reasoning over the graph to dynami-   cally select observation - relevant information .   •We conduct extensive experiments on two pub - licly available benchmarks , and experimental   results demonstrate the effectiveness of our   model . We also conduct a detailed case anal-   ysis to illustrate the benefits of incorporating   observation - related information .   2 Methodology   2.1 Overview of the Proposed Framework   Given an image X , the probability of a report   Y={y , . . . , y}is denoted as p(Y|X ) . Our   framework decomposes p(Y|X)into two stages ,   where the first stage is observation planning , and   the second stage is report generation . Specifically ,   observations of an image Z={z , . . . , z}are   firstly produced , modeled as p(Z|X ) . Then , the   report is generated based on the observation plan   and the image , modeled as p(Y|X , Z ) . Finally , our   framework maximizes the following probability :   p(Y|X)∝p(Z|X)/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipuprightp(Y|X , Z)/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright .   2.2 Observation Plan Extraction and Graph   Construction   Observation Plan Extraction . There are two avail-   able tools for extracting observation labels from   reports , which are CheXpert ( Irvin et al . , 2019 ) and   CheXbert ( Smit et al . , 2020 ) . We use CheXbert   instead of CheXpert because the former achieved   better performance . To extract the observation plan   of a given report , we first adopt the CheXbert to   obtain the observation labels within 14 categories   C={C , . . . , C}as indicated in Irvin et al .   ( 2019 ) . More details about the distribution of ob-   servation can be found in Appendix A.1 . The label   ( or status ) of each category belongs to Present ,   Absent , and Uncertain , except the No Finding cat-   egory , which only belongs to Present andAbsent .   To simplify the observation plan and emphasize   the abnormalities presented in a report , we regard   Present andUncertain as Positive and Absent as   Negative . Then , observations are divided into a   positive collection C / POS and a negative collection   C / NEG , and each category with its corresponding   label is then converted to its unique observation   C / POS∈C / POS orC / NEG ∈C / NEG , result-   ing in 28 observations . For example , as indicated   in Figure 1 , the report presents Lung Opacity while   Cardiomegaly is absent in it . These categories8109   are converted to two observations : Lung Opac-   ity / POS and Cardiomegaly /NEG . Then , we locate   each observation by matching mentions in the re-   port and order them according to their positions .   These mentions are either provided by Irvin et al .   ( 2019)or extracted from the training corpus ( i.e. ,   n - grams ) , as will be illustrated in the following part .   Finally , we can obtain the image ’s observation plan   Z={z , . . . , z } .   Tree - Structured Observation Graph Construc-   tion . Since observations are high - level concepts   that are implicitly related to tokens in reports , it   could be difficult for a model to realize these con-   cepts in detailed reports without more comprehen-   sive modeling . Thus , we propose to construct an ob-   servation graph by extracting observation - related n-   grams as the connections between observations and   tokens for better observation realization . Specif-   ically , it involves two steps to construct such a   graph : ( 1 ) n - grams extraction , where n∈[1,4 ]   and ( 2 ) < observation , n - gram > association . Fol-   lowing previous research ( Diao et al . , 2021 ; Su   et al . , 2021b ) , we adopt the pointwise mutual infor-   mation ( PMI ) ( Church and Hanks , 1990 ) to fulfillthese two steps , where a higher PMI score implies   two units with higher co - occurrence :   PMI(¯x,ˆx ) = logp(¯x,ˆx )   p(¯x)p(ˆx ) .   For the first step , we extract n - gram units S=   { s , . . . , s}based on the training reports . Given   two adjacent units ¯xandˆxof a text sequence , a   high PMI score indicates that they are good collec-   tion pairs to form a candidate n - gram s , while a   low PMI score indicates that these two units should   be separated . For the second step , given a pre-   defined observation set O={z , . . . , z } , we   extract the observation - related n - gram units with   PMI(z , s ) , where zis the i - th observation , sis   thej - th n - gram , and p(z , s)is the frequency that   an n - gram sappears in a report with observation   zin the training set . Then , we can obtain a set   of observation - related n - grams s={s , . . . , s } ,   where s={t , . . . , t } , and tokens in n - grams   form the token collection T={t , . . . , t } . Note   that we remove all the stopwords in T , using the   vocabulary provided by NLTK . Finally , for each   observation , we extract the top - K n - grams as the   candidates to construct the graph , which contains8110three types of nodes V={Z , S , T } . We list part of   the n - grams in Appendix A.2 . After extracting rele-   vant information from the training reports , we con-   struct an observation graph G= < V , E > by in-   troducing three types of edges E={E , E , E } :   •E : This undirected edge connects two adja-   cent observations in an observation plan ( i.e. ,   < z , z > ) .   •E : This directed edge connects an observa-   tion and an n - gram ( i.e. , < z , s > ) .   •E : This directed edge connects an n - gram   with its tokens ( i.e. , < s , t > ) .   2.3 Visual Features Extraction   Given an image X , a CNN and an MLP layer are   first adopted to extract visual features X :   X={x , . . . , x}=MLP(CNN(X ) ) ,   where x∈Ris the i - th visual feature .   2.4 Stage 1 : Observation Planning   The output of observation planning is an obser-   vation sequence , which is the high - level summa-   rization of the radiology report , as shown on the   left side of Figure 2 . While examining a radio-   graph , a radiologist must report positive observa-   tions . However , only part of the negative observa-   tions will be reported by the radiologist , depending   on the overall conditions of the radiograph ( e.g. ,   co - occurrence of observations or the limited length   of a report ) . Thus , it is difficult to plan without   considering the observation dependencies ( i.e. , la-   bel dependencies ) . Here , we regard the planning   problem as a generation task and use a Transformer   encoder - decoder for observation planning :   h={h , . . . , h}=Encoder(X ) ,   z = Decoder(h , z ) ,   p(z|X , Z ) = Softmax ( Wz+b ) ,   where h∈Ris the i - th visual hidden represen-   tation , Encoderis the visual encoder , Decoderis   the observation decoder , z∈Ris the decoder   hidden representation , W∈Ris the weight   matrix , and b∈Ris the bias vector . Then the   planning loss Lis formulated as :   L=−/summationdisplaywlogp(z|X , Z )   w=/braceleftigg   1 + αifz∈C / POS ,   1 otherwise .By increasing α , the planner gives more attention to   abnormalities . Note that the plugged αis applied to   positive observations and No Finding /NEG instead   ofNo Finding /POS .   2.5 Stage 2 : Observation - Guided Report   Generation   Observation Graph Encoding . We use a Trans-   former encoder to encode the observation graph   constructed according to Section 2.2 . To be spe-   cific , given the observation graph Gwith nodes   V={Z , S , T } and edges E={E , E , E } , we   first construct the adjacency matrix ˆA = A+I   based on E. Then , VandˆAare fed into the Trans-   former for encoding . Now ˆAserves as the self-   attention mask in the Transformer , which only al-   lows nodes in the graph to attend to connected   neighbors and itself . To incorporate the node type   information , we add a type embedding P∈Rfor   each node representation :   N = Embed ( V ) + P ,   V={Z , S , T}=Encoder(N,ˆA ) ,   where Embed ( · ) is the embedding function , and   N∈Rrepresents node embeddings . For obser-   vation nodes , Pdenotes positional embeddings ,   and for n - gram and token nodes , Prepresents type   embeddings . Z , S , andT∈Rare encoded rep-   resentations of observations , n - grams , and tokens ,   respectively .   Vision - Graph Alignment . As an observation   graph may contain irrelevant information , it is nec-   essary to align the graph with the visual features .   Specifically , we jointly encode visual features X   and token - level node representations Tso that the   node representations can fully interact with the vi-   sual features , and we prevent the visual features   from attending the node representations by intro-   ducing a self - attention mask M :   [ h , T ] = Encoder([X , T],M ) ,   where h , T∈Rare the visual representation   and the aligned token - level node representations ,   respectively .   Observation Graph Pruning . After aligning vi-   sual features and the observation graph , we prune   the graph by filtering out irrelevant nodes . The   probability of keeping a node is denoted as :   p(1|T ) = Sigmoid ( WT+b ) ,   where W∈Ris the learnable weight and   b∈Ris the bias . We can optimize the pruning8111   process with the following loss :   L= [ −β·dlogp(1|T )   −(1−d ) log(1 −p(1|T ) ) ] ,   where βis the weight to tackle the class imbalance   issue , and dis the label indicating whether a token   appears in the referential report . Finally , we prune   the observation graph by masking out token - level   nodes with p(1|T)<0.5and masked token - level   node representations denote as T = Prune ( T ) .   Tree Reasoning over Observation Graph . We   devise a tree reasoning ( TrR ) mechanism to ag-   gregate observation - relevant information from the   graph dynamically . The overall process is shown   in Figure 3 , where we aggregate node informa-   tion from the observation level ( i.e. , first level )   to the n - gram level ( i.e. , second level ) , then to   the token level ( i.e. , third level ) . To be specific ,   given a query qand node representations at l - th   levelk∈ { Z , S , T } , the tree reasoning path is   q− →q− →q   − − →q , and the overall process ,   is formulated as below :   v = MHA ( Wq , Wk , Wk ) ,   q = LayerNorm ( q+v ) ,   where MHA and LayerNorm are the multi - head   self - attention , and layer normalization modules   ( Vaswani et al . , 2017 ) , respectively . W , W , and   W∈Rare weight metrics for query , key , and   value vector , respectively . Finally , we can obtain   the multi - level information q , containing observa-   tion , n - gram , and token information .   Report Generation with Tree Reasoning . As   shown in the right side of Figure 2 , an observation-   guided Transformer decoder is devised to incorpo-   rate the graph information , including ( i ) multiple   observation - guided decoder blocks ( i.e. , Decoder ) ,   which aims to align observations with the visual   representations , and ( ii ) a tree - reasoning block   ( i.e. , TrR ) , which aims to aggregate observation-   relevant information . For Decoder , we insert anobservation - related cross - attention module before   a visually - aware cross - attention module . By do-   ing this , the model can correctly focus on regions   closely related to a specific observation . Given the   visual representations h , the node representations   V={Z , S , T } , and the hidden representation   of the prefix h∈R , thet - th decoding step is   formulated as :   Decoder=      h = Self - Attn ( h , h , h ) ,   h = Cross - Attn ( h , Z , Z ) ,   h = Cross - Attn ( h , h , h ) ,   TrR=/braceleftigg   h = Self - Attn ( h , h , h ) ,   q = TrR(h,[Z , S , T ] ) ,   p(y|X , G , Y ) = Softmax ( Wq+b ) ,   where Self - Attn is the self - attention module , Cross-   Attn is the cross - attention module , h , h , h∈   Rare self - attended hidden state , observation-   related hidden state , visually - aware hidden state   of Decoder , respectively . h∈Ris the self-   attended hidden state of TrR , W∈Ris the   weight matrix , and b∈Ris the bias vector .   We omit other modules ( i.e. , Layer Normalization   and Feed - Forward Network ) in the standard Trans-   former for simplicity . Note that we extend the   observation plan Zto an observation graph G , so   the probability of yconditions on Ginstead of Z.   Then , we optimize the generation process using the   negative log - likelihood loss :   L=−/summationdisplaylogp(y|X , G , Y ) .   Finally , the loss function of the generator is L=   L+L.   3 Experiments   3.1 Datasets   Following previous research ( Chen et al . , 2020 ,   2021 ) , we use two publicly available benchmarks   to evaluate our method , which are IU X-   ( Demner - Fushman et al . , 2016 ) and MIMIC-   CXR(Johnson et al . , 2019 ) . Both datasets have   been automatically de - identified , and we use the   same preprocessing setup of Chen et al . ( 2020 ) .   •IU X- is collected by Indiana Univer-   sity , containing 3,955 reports with two X-   ray images per report resulting in 7,470 im-8112   ages in total . We split the dataset into   train / validation / test sets with a ratio of 7:1:2 ,   which is the same data split as in ( Chen et al . ,   2020 ) .   •MIMIC - CXR consists of 377,110 chest   X - ray images and 227,827 reports from   63,478 patients . We adopt the standard   train / validation / test splits .   3.2 Evaluation Metrics and Baselines   We adopt natural language generation metrics   ( NLG Metrics ) and clinical efficacy ( CE Metrics )   to evaluate the models . BLEU ( Papineni et al . ,   2002 ) , METEOR ( Banerjee and Lavie , 2005 ) , and   ROUGE ( Lin , 2004 ) are selected as NLG Metrics ,   and we use the MS - COCO caption evaluation tool   to compute the results . For CE Metrics , we adopt   CheXpert ( Irvin et al . , 2019 ) for MIMIC - CXR   dataset to label the generated reports compared   with disease labels of the references .   To evaluate the performance of ORG , we   compare it with the following 10 state - of - the - art   ( SOTA ) baselines : R2G(Chen et al . , 2020 ) ,   CA(Liu et al . , 2021c ) , CMCL ( Liu et al . , 2021a ) ,   PPKED ( Liu et al . , 2021b ) , R2GCMN ( Chenet al . , 2021 ) , A T ( You et al . ,   2021 ) , K M(Yang et al . , 2021 ) , MT   ( Nooralahzadeh et al . , 2021 ) , CMM - RL ( Qin and   Song , 2022 ) , and CMCA ( Song et al . , 2022 ) .   3.3 Implementation Details   We adopt the ResNet-101 ( He et al . , 2015 ) pre-   trained on ImageNet ( Deng et al . , 2009 ) as the vi-   sual extractor . For IU X- , we further fine - tune   ResNet-101 on CheXpert ( Irvin et al . , 2019 ) . The   layer number of all the encoders and decoders is   set to 3 except for Graph Encoder , where the layer   number is set to 2 . The input dimension and the   feed - forward network dimension of a Transformer   block are set to 512 , and each block contains 8 at-   tention heads . The beam size for decoding is set to   4 , and the maximum decoding step is set to 64/104   for IU X- and MIMIC - CXR , respectively .   We use AdamW ( Loshchilov and Hutter , 2019 )   as the optimizer and set the initial learning rate   for the visual extractor as 5e-5 and 1e-4 for the   rest of the parameters , with a linear schedule de-   creasing from the initial learning rate to 0 . αis   set to 0.5 , the dropout rate is set to 0.1 , and the   batch size is set to 32 . For IU X - ray , we train   the planner / generator for 15/15 epochs , and βis   set to 2 . For MIMIC - CXR , the training epoch of8113   the planner / generator is set to 3/5 , and βis set to   5 . We select the best checkpoints of the planner   based on micro Fof all observations and select   the generator based on the BLEU-4 on the valida-   tion set . Our model has 65.9 M parameters , and   the implementations are based on HuggingFace ’s   Transformers ( Wolf et al . , 2020 ) . We conduct all   the experiments on an NVIDIA-3090 GTX GPU   with mixed precision . The NLTK package version   is 3.6.2 .   4 Results   4.1 NLG Results   Table 1 shows the experimental results . ORG   outperforms most of the baselines ( except CMCA   onIU X- ) and achieves state - of - the - art per-   formance . Specifically , our model achieves 0.195   BLEU-4 on the IU X- dataset , which is   the second - best result , and 0.123BLEU-4 on the   MIMIC - CXR dataset , leading to a 5.1%incre-   ment of compared to the best baseline ( i.e. , CMCA ) .   In terms of METEOR , ORGachieves compet-   itive performance on both datasets . In addition ,   our model increases R - L by 0.6%on the MIMIC-   CXR dataset compared to the best baseline and   achieves the second - best result on the IU X-   dataset . This indicates that by introducing the guid-   ance of observations , ORGcan generate more   coherent text than baselines . However , we notice   that on the IU X- dataset , there is still a perfor-   mance gap between our model and the best baseline   ( i.e. , CMCA ) . The reason may be that the overall   data size of this dataset is small ( ∼2,000 sam-   ples for training ) . It is difficult to train a good   planner using a small training set , especially with   cross - modal data . As we can see from Table 3 , the   planner only achieves 0.132Macro - Fon the IU   X- dataset , which is relatively low compared   to the performance of the planner on the MIMIC-   CXR dataset . Thus , accumulation errors unavoid-   ably propagate to the generator , which leads to   lower performance .   4.2 Clinical Efficacy Results   The clinical efficacy results are listed on the right   side of Table 1 . On the MIMIC - CXR dataset , our   model outperforms previous SOTA results . Specifi-   cally , our model achieves 0.385F , increasing by   1.4%compared to the best baseline . In addition ,   0.416precision and 0.418recall are achieved by   ORG , which are competitive results . This indi-   cates that our model can successfully maintain the   clinical consistency between the images and the   reports .   4.3 Ablation Results   To examine the effect of the observation plan and   the TrR mechanism , we perform ablation tests , and   the ablation results are listed in Table 2 . There are   three variants : ( 1 ) ORGw / oPlan , which does   not consider observation information , ( 2 ) ORG8114   w / oGraph , which only considers observations but   not the observation graph , ( 3 ) ORGw / oTrR ,   which select information without using the TrR   mechanism . Compared to the full model , the perfor-   mance of ORGw / oPlan drops significantly on   both datasets . This indicates that observation infor-   mation plays a vital role in generating reports . For   ORGw / oGraph , the performance on NLG met-   rics decreases significantly , but the performance of   clinical efficacy remains nearly the same as the full   model . This is reasonable because the observation   graph is designed to enrich the observation plan to   achieve better word - level realization . On the per - formance of ORGw / oTrR , a similar result of   ORGw / oGraph is observed . This indicates that   TrR can enrich the plan information , and stronger   reasoning can help generate high - quality reports .   We also conduct experiments on the impact of   the number ( K ) of selected n - grams , as shown in   Table 4 . There is a performance gain when increas-   ing K from 10to20and to 30on both datasets .   On the IU X- dataset , B-2 increases by 2.4 %   and3.7%and B-4 rises by 1.0%and1.5 % . A sim-   ilar trend is also observed on the MIMIC - CXR   dataset .   4.4 Qualitative Analysis   We conduct a case study and analyze some error   cases on the MIMIC - CXR dataset to provide more   insights .   Case Study . We conduct a case study to show how   the observation and the tree reasoning mechanism   guide the report generation process , as shown in   Figure 4 . We show the generated reports of OR-   G , ORGw / oTrR , and ORGw / oPlan ,   respectively . All three models successfully gener-   ate the first three negative observations and the last   positive observation . However , variant w / oplan   generates " mild pulmonary vascular congestion   without overt pulmonary edema " which is not con-   sistent with the radiograph . In terms of the output   of variant w / oTrR , " mediastinal silhouettes are un-   changed " is closely related to observation Enlarged   Cardiomediastinum instead of Cardiomegaly . Only   ORGcan generate the Cardiomegaly /POS pre-   sented in the observation plan with a TrR path . This   indicates that observations play a vital role in main-   taining clinical consistency . In addition , most of8115the tokens in the observation mention mild to mod-   erate cardiomegaly can be found in the observation   graph , which demonstrates that the graph can pro-   vide useful information in word - level realization .   Error Analysis . We depicte error cases gener-   ated by ORGin Figure 5 . The major error is   caused by introducing incorrect observation plans .   Specifically , the generated plan of the upper case   omits one positive observation ( i.e. , Enlarged Car-   diomediastinum /POS ) , resulting in false negative   observations in its corresponding generated report .   Another error is false positive observations appear-   ing in the generated reports ( e.g. , the bottom case ) .   Thus , how to improve the performance of the plan-   ner is a potential future work to enhance clinical   accuracy .   5 Related Work   5.1 Image Captioning and Medical Report   Generation   Image Captioning ( Vinyals et al . , 2015 ; Rennie   et al . , 2017 ; Lu et al . , 2017 ; Anderson et al . , 2018 )   has long been an attractive research topic , and there   has been a surging interest in developing medical   AI applications . Medical Report Generation ( Jing   et al . , 2018 ; Li et al . , 2018 ) is one of these appli-   cations . Chen et al . ( 2020 ) proposed a memory-   driven Transformer model to generate radiology   reports . Chen et al . ( 2021 ) further proposed a cross-   modal memory network to facilitate report genera-   tion . Qin and Song ( 2022 ) proposed to utilize re-   inforcement learning ( Williams , 1992 ) to align the   cross - modal information between the image and   the corresponding report . In addition to these meth-   ods , Liu et al . ( 2021c ) proposed the Contrastive   Attention model comparing the given image with   normal images to distill information . Yang et al .   ( 2021 ) proposed to introduce general and specific   knowledge extracted from RadGraph ( Jain et al . ,   2021 ) in report generation . Liu et al . ( 2021a ) pro-   posed a competence - based multimodal curriculum   learning to guide the learning process . Liu et al .   ( 2021b ) proposed to explore and distill posterior   and prior knowledge for report generation .   Several research works focus on improving the   clinical accuracy of the generated reports . Liu et al .   ( 2019 ) proposed a clinically coherent reward for   clinically accurate reinforcement learning to im-   prove clinical accuracy . Lovelace and Mortazavi   ( 2020 ) proposed to use CheXpert ( Irvin et al . , 2019 )   as a source of clinical information to generate clini - cally coherent reports . Miura et al . ( 2021 ) proposed   to use entity matching score as a reward to encour-   age the model to generate factually complete and   consistent radiology reports . Nishino et al . ( 2022 )   proposed a planning - based method and regarded   the report generation task as the data - to - text gener-   ation task .   5.2 Planning in Text Generation   Another line of research closely related to our work   is planning in text generation , which has been ap-   plied to multiple tasks ( e.g. , Data - to - Text Genera-   tion , Summarization , and Story Generation ) . Hua   and Wang ( 2020 ) propose a global planning and   iterative refinement model for long text generation .   Kang and Hovy ( 2020 ) propose a self - supervised   planning framework for paragraph completion . Hu   et al . ( 2022 ) propose a dynamic planning model   for long - form text generation to tackle the issue of   incoherence outputs . Moryossef et al . ( 2019 ) pro-   posed a neural data - to - text generation by separating   planning from realization . Su et al . ( 2021a ) pro-   posed a controlled data - to - text generation frame-   work by planning the order of content in a table .   6 Conclusion   In this paper , we propose ORG , an observation-   guided radiology report generation framework ,   which first produces an observation plan and then   generates the corresponding report based on the   radiograph and the plan . To achieve better observa-   tion realization , we construct a three - level observa-   tion graph containing observations , observation-   aware n - grams , and tokens , and we propose a   tree reasoning mechanism to capture observation-   related information by dynamically aggregating   nodes in the graph . Experimental results demon-   strate the effectiveness of our proposed framework   in terms of maintaining the clinical consistency   between radiographs and generated reports .   Limitations   There are several limitations to our framework .   Specifically , since observations are introduced as   guiding information , our framework requires obser-   vation extraction tools to label the training set in   advance . Then , the nodes contained in the observa-   tion graph are mined from the training data . As a   result , the mined n - grams could be biased when the   overall size of the training set is small . In addition ,   our framework is a pipeline , and the report genera-8116tion performance highly relies on the performance   of observation planning . Thus , errors could accu-   mulate through the pipeline , especially for small   datasets . Finally , our framework is designed for   radiology report generation targeting Chest X - ray   images . However , there are other types of medi-   cal images ( e.g. , Fundus Fluorescein Angiography   images ) that our framework needs to examine .   Ethics Statement   TheIU X-(Demner - Fushman et al . , 2016 ) and   MIMIC - CXR ( Johnson et al . , 2019 ) datasets have   been automatically de - identified to protect patient   privacy . The proposed system is intended to gen-   erate radiology reports automatically , alleviating   the workload of radiologists . However , we notice   that the proposed system can generate false positive   observations and inaccurate diagnoses due to sys-   tematic biases . If the system , as deployed , would   learn from further user input ( i.e. , patients ’ radio-   graphs ) , there are risks of personal information   leakage while interacting with the system . This   might be mitigated by using anonymous technol-   ogy to protect privacy . Thus , we urge users to   cautiously examine the ethical implications of the   generated output in real - world applications .   Acknolwedgments   This work was supported in part by General Pro-   gram of National Natural Science Foundation of   China ( Grant No . 82272086 , 62076212 ) , Guang-   dong Provincial Department of Education ( Grant   No . 2020ZDZX3043 ) , Shenzhen Natural Sci-   ence Fund ( JCYJ20200109140820699 and the Sta-   ble Support Plan Program 20200925174052004 ) ,   and the Research Grants Council of Hong Kong   ( 15207920 , 15207821 , 15207122 ) .   References81178118   A Appendices   A.1 Observation Statistics   There are 14 categories of observations : No Find-   ing , Enlarged Cardiomediastinum , Cardiomegaly ,   Lung Lesion , Lung Opacity , Edema , Consolidation ,   Pneumonia , Atelectasis , Pneumothorax , Pleural   Effusion , Pleural Other , Fracture , and Support De-   vices . Table 5 lists the observation distributions   annotated by CheXbert(Smit et al . , 2020 ) in the   train / valid / test split of two benchmarks.8119   A.2 Observation - aware N - grams   Here are some of the observation - aware n - grams   we use in our experiments , as shown in Figure 6 .   These categories are Enlarged Cardiomediastinum ,   Consolidation , and Cardiomegaly .8120ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section " Limitations " .   /squareA2 . Did you discuss any potential risks of your work ?   Section " Ethical Statement " .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section " Abstract " and Section 1 " Introduction " .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3 .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We will include the license and terms of use when releasing our code .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 3 .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section 3 .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Section 3 .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3 and Appendix A.1 .   C / squareDid you run computational experiments ?   Section 3 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Left blank.8121 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 3 .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.8122