  Myeongjun JangBodhisattwa Prasad MajumderJulian McAuley   Thomas LukasiewiczOana - Maria CamburuUniversity of Oxford , UKVienna University of Technology , AustriaUniversity of California San DiegoUniversity College London , UK   myeongjun.jang@cs.ox.ac.uk   Abstract   While recent works have been considerably   improving the quality of the natural language   explanations ( NLEs ) generated by a model to   justify its predictions , there is very limited re-   search in detecting and alleviating inconsisten-   cies among generated NLEs . In this work , we   leverage external knowledge bases to signiﬁ-   cantly improve on an existing adversarial at-   tack for detecting inconsistent NLEs . We ap-   ply our attack to high - performing NLE mod-   els and show that models with higher NLE   quality do not necessarily generate fewer in-   consistencies . Moreover , we propose an off-   the - shelf mitigation method to alleviate incon-   sistencies by grounding the model into exter-   nal background knowledge . Our method de-   creases the inconsistencies of previous high-   performing NLE models as detected by our at-   tack .   1 Introduction   The accurate yet black - box nature of deep neu-   ral networks has accelerated studies on explain-   able AI . The advent of human - written natural lan-   guage explanations ( NLEs ) datasets ( Wiegreffe and   Marasovic , 2021 ) has paved the way for the de-   velopment of models that provide NLEs for their   predictions . However , by introducing an adver-   sarial attack , which we hereafter refer to as eIA   ( explanation Inconsistency Attack ) , Camburu et al .   ( 2020 ) found that an early NLE model ( Camburu   et al . , 2018 ) was prone to generate inconsistent   NLEs ( In - NLEs ) . More precisely , two logically   contradictory NLEs generated by a model for two   instances that have the same context are considered   to form an inconsistency . For example , assume   a self - driving car stops in a given trafﬁc environ-   ment ( the context ) . If the passenger asks the car   Q1:“Why did you stop ? ” , and it provides NLE1 :   “ Because the trafﬁc light is red . ” , and , for the same   context , if the passenger instead asks Q2 : “ Why   did you decide to stop here ? ” and the car providesNLE2 : “ Because the trafﬁc light is green ” , then   NLE1 and NLE2 form an inconsistency .   A model that generates In - NLEs is undesirable ,   as it either has a faulty decision - making process   ( e.g. , the trafﬁc light was green , so the car should   not have stopped ) , or it generates NLEs that are   not faithfully describing its decision - making pro-   cess ( e.g. , the car stopped for a red trafﬁc light ,   but states that it was green ) ( Camburu et al . , 2020 ) .   While recent high - performing NLE models have   largely improved in terms of the quality ( plausi-   bility ) of the generated NLEs , to our knowledge ,   these models have not been tested against generat-   ing inconsistent NLEs .   In this work , we ﬁrst propose a fast , efﬁcient , and   task - generalizable adversarial attack that utilizes   external knowledge bases . Through experiments   on two datasets and four models , we verify the   increased efﬁciency of our approach over the eIA   attack , the only inconsistency attack for NLE mod-   els , to our knowledge . We also show that the high-   performing NLE models are still prone to generat-   ing signiﬁcantly many In - NLEs and , surprisingly ,   that a higher NLE quality does not necessarily im-   ply fewer inconsistencies . Second , we propose a   simple yet efﬁcient off - the - shelf method for allevi-   ating inconsistencies that grounds any NLE model   into background knowledge , leading to fewer in-   consistencies . The code for this paper is available   at https://github.com/MJ-Jang/eKnowIA .   2 Inconsistency Attack   We propose eKnowIA ( explanations Know ledge-   grounded Inconsistency Attack ) , which detects   more In - NLEs in a faster and more general manner   than eIA .   2.1 Original eIA Attack   Setting . Given an instance x , Camburu et al .   ( 2020 ) divide it into : the context part xthat   remains ﬁxed , and the variable partxthat is540changed during the attack . For example , xandx   would be a premise and a hypothesis , respectively ,   for natural language inference ( NLI — detailed be-   low ) . Let e(x)denote the NLE generated by a   model mfor the input x = ( x,ˆ x ) . The objective   is to ﬁnd ˆ xsuch that e(x)ande((x,ˆ x))are   logically contradictory ( see examples in Table 10 ) .   Steps . The eIA attack has the following steps :   1.Train a neural model to act as a reverse   explainer , called RE , that takes x   ande(x)as input and generates x , i.e. ,   RE(x ; e(x ) ) = x.   2 . For each generated NLE e(x ):   ( a)Automatically create a set of statements   Ithat are inconsistent with e(x ) .   ( b)For each ˆe∈I , generate a variable part   ˆ x = RE(x ; ˆe ) .   ( c ) Query monˆx= ( x,ˆ x)to get e(ˆ x ) .   ( d)Check whether e(ˆ x)is indeed incon-   sistent with e(x)by checking whether   e(ˆ x)is included inI.   CreatingI.Camburu et al . ( 2020 ) used simple   elimination of negation ( removing “ not ” or “ n’t ” )   and a task - speciﬁc template - based approach for   this step . For the template - based approach , they   manually create a set of label - speciﬁc templates for   NLEs such that introducing the instance - speciﬁc   terms of an NLE from one template into any tem-   plate from another label creates an inconsistency .   They illustrate this process only on the e - SNLI   dataset ( Camburu et al . , 2018 ) , leaving room to   question how easily it generalizes to other datasets .   e - SNLI contains NLEs for the SNLI dataset ( Bow-   man et al . , 2015 ) , where the NLI task consists in   identifying whether a premise and a hypothesis are   in a relation of entailment ( if the premise entails   the hypothesis ) , contradiction ( if the hypothesis   contradicts the premise ) , or neutral ( if neither en-   tailment nor contradiction hold ) . Examples of their   templates are : “ < X > is < Y > ” ( for entailment ) and   “ < X > can not be < Y > ” ( for contradiction ) . Based on   the templates , for a e(x)of “ A dog is an animal . ” ,   an inconsistent statement of “ A dog can not be an   animal ” is obtained ( < X > = “ A dog ” , < Y > = “ an   animal ” ) . They manually identiﬁed an average of   10 templates per label .   2.2 Our eKnowIA Attack   The template - based approach in eIA has two ma-   jor drawbacks : ( 1 ) requires substantial human ef-   fort to ﬁnd an exhaustive set of templates for eachdataset , ( 2 ) many different ways of obtaining incon-   sistencies ( e.g. , using antonyms ) are not taken into   account . Moreover , even their negation rule can   also be improved . To alleviate these drawbacks , we   adopt three rules .   Negation . We remove and addnegation tokens to   negated and non - negated sentences , respectively .   To avoid grammatical errors , we add one negation   per sentence only if the sentence belongs to one of   the following two templates :   • < A > is < B > , < A > are < B > ( add “ not ” ) ,   •<A > has < B > , < A > have < B > ( add “ does / do   not ” only if < B > is a noun ) .   Antonym replacement for adjectives / adverbs .   We replace adjectives / adverbs with their antonyms   from ConceptNet ( Speer et al . , 2017 ) ( using the   NLTK POS tagger ) . Only one adjective or adverb   at a time is replaced for each NLE , to avoid deterio-   rating the contradictory meaning . Employing other   abundant thesauruses could improve our approach ,   which we leave as future work .   Unrelated noun replacement . We replace a noun   with an unrelated one , e.g. , “ human ” with “ plant ” .   This is only applied to the noun that is the last word   of the sentence , to reduce the possibility of false   inconsistencies as the part - of - speech ( POS ) tagger   occasionally made incorrect predictions for words   in the middle of a sentence . To get unrelated nouns ,   we use the DistinctFrom andAntonym relations in   ConceptNet . However , we noticed that ConceptNet   contains noisy triplets where the subject and object   are not antonyms , such as “ man ” and “ people ” for   “ person ” . To avoid these , we created a list ( see Ta-   ble 8 in the appendix ) of triplets from ConceptNet   to be ignored , by manually investigating a random   subset of 3000 detected inconsistencies . While this   involved human effort , we highlight that this is due   to the nature of ConceptNet and other knowledge   bases with more accurate instances may be used   instead . However , since we only found eight noisy   triplets , we decided to keep ConceptNet , which   otherwise worked well for our datasets . Finally , we   also noticed that our rules may not lead to In - NLEs   if both the context and variable part contain nega-   tions . Examples are in Table 9 in the appendix . We   ﬁlter out such pairs .   2.3 Experiments   Datasets . We consider two tasks : NLI ( with the   e - SNLI dataset described in Sec . 2.1 ) and com-541   monsense question answering ( CQA ) . The Cos-   E 1.0 dataset ( Rajani et al . , 2019 ) contains CQA   instances formed of a question , three answer can-   didates , and an NLE for the correct answer . The   objective of the Cos - E ( Rajani et al . , 2019 ) dataset   is to select an answer among the three candidates   given a question and to generate an NLE to support   the answer . Following Camburu et al . ( 2020 ) , we   set the premise as context and the hypothesis as the   variable part for e - SNLI . For Cos - E , to avoid omit-   ting the correct answer , we set the question and the   correct answer as the context , and the remaining   two answer candidates as the variable part . Just   like eIA , our attack is solely intended for detecting   In - NLEs and not as a label attack ( which may or   may not happen ) .   Evaluation metrics . LetIbe generated at Step   2a for each instance in a test set D , and letI⊆   Ibe the set of detected In - NLEs ( after Step 2d ) .   For each instance , our attack can identify multiple   inconsistencies ( via multiple variable parts ) . We ,   therefore , use two evaluation metrics : hit - rate ( H )   and success - rate ( S ):   S = N/|D|andH=|I|/|I| ,   where Nis the number of unique instances for   which the attack identiﬁed at least one inconsis-   tency . Intuitively , Sdenotes the ratio of the test   instances where the attack is successful , while H   denotes the ratio of detected In - NLEs to that of the   proposed In - NLEs .   Models . We consider the following high-   performing NLE models , with their implemen - tation detailed in Appendix A.1 : NILE ( Kumar   and Talukdar , 2020 ) for NLI , CAGE ( Rajani   et al . , 2019 ) for CQA , and WT5 - base ( 220 M   parameters ) ( Narang et al . , 2020 ) for both   tasks . WT5 models with more parameters ( e.g. ,   WT5 - 11B ) would require considerably more   computing while providing relatively small gains   in NLE quality ( 32.4 for WT5 - base vs. 33.7 for   WT5 - 11B ( Narang et al . , 2020 ) ) . Therefore , they   are not considered here due to limited computing   resources . Implementation details are given in   Appendix A.1 .   2.4 Results   eKnowIA vs. eIA . We compare eKnowIA with   eIA only on the WT5 - base model , since eIA re-   quires a prohibiting amount of time . As in Cam-   buru et al . ( 2020 ) , we manually veriﬁed the nat-   uralness of adversarial hypotheses on 50 random   samples for each model . Sentences that go against   common sense are considered unnatural . Minor   grammatical errors and typos are ignored . We ob-   serve that 81.5 % of the adversarial hypotheses were   natural , on average , for each model . Details are   in Appendix A.4 . The results are summarized in   Table 2 . The e - SNLI results are adjusted to reﬂect   the proportion of natural adversarial hypotheses   by multiplying the number of detected pairs of   In - NLEs for each model with the estimated nat-   uralness ratio . For Cos - E , an unnatural variable   part would consist of stop words or a repetition   of another answer candidate . We automatically   found 2 out of 22 examples to be unnatural , which   were removed . We observe that eIA generates a   tremendous amount of inconsistent candidates ( I ) ,   e.g. , 24 M for e - SNLI , thus being extremely slow   ( e.g. , 10 days vs. 40 min for eKnowIA ) , while also   obtaining lowerSandHthan eKnowIA ( e.g. ,   2.19 % vs. 12.88 % S ) .   eKnowIA on NLE models . The results of   eKnowIA applied to NILE , CAGE , and WT5 are in542   the upper lines of each block in Table 1 . All models   are vulnerable to the inconsistency attack . Also , a   better NLE quality may not necessarily guarantee   fewer inconsistencies . For example , WT5 - base has   a better NLE quality than CAGE on Cos - E ( 0.55   vs. 0.43 e - ViL score ; see below ) , but eKnowIA de-   tected more inconsistencies for WT5 - base than for   CAGE ( 0.95 vs. 0.42 success rate ) . Examples of   generated In - NLEs are in Table 3 . More examples   are in Tables 10–12 in Appendix A.7 . We observe   that the In - NLEs usually contradict common sense ,   which is aligned with previous studies showing that   language models , used as pre - trained components   in the NLE models , often suffer from factual incor-   rectness ( Mielke et al . , 2020 ; Zhang et al . , 2021 ) .   3 Our K Method for Alleviating   Inconsistencies   Our approach for alleviating inconsistencies in   NLE models consists of two steps : ( 1 ) extraction of   knowledge related to the input and ( 2 ) knowledge   injection .   Extracting related knowledge . We leverage a   knowledge extraction heuristic proposed by Xu   et al . ( 2021 ) as follows :   1 . Extract entities from an input ’s context part .   2.Find all knowledge triplets that contain theentities .   3.For each entity , calculate a weight sfor each   extracted triplet as :   s = w×N / NandN=/summationtextN ,   where wis the weight of the j - th triplet pre-   deﬁned by the knowledge base ( e.g. , Concept-   Net ) , Nis the number of extracted triplets   of the relation rfor the given instance , and   Kis the total number of triplets containing   the entity for the given instance .   4.For each entity , extract the triplet with the   highest score .   Grounding with the extracted knowledge . Af-   ter extracting the triplet with the highest weight per   entity in an instance , we transform each of them   into natural language and concatenate them to the   instance . We use “ Context : ” as a separator between   the input and the triplets . We leverage the templates   that transform a relation into free - text ( e.g. , IsAto   “ is a ” ) from Petroni et al . ( 2019 ) .   3.1 Experiments   We apply our K approach to NILE , CAGE ,   and WT5 - base , and name them KnowNILE ,   KnowCAGE , and KnowWT5 - base , respectively.543Inconsistencies . The results in Table 1 show that   grounding in commonsense knowledge diminishes   the number of In - NLEs for all models and tasks .   The K models defended against 58 % of the   examples attacked by eKnowIA . Also , we observed   that , among the inconsistent examples of K   models , 20 % of them on average were newly in-   troduced instances . Examples that failed to be de-   fended , as well as newly introduced In - NLEs are   provided in Tables 15 - 16 in Appendix A.7 . Suc-   cessfully defended examples are provided in Ta-   ble 4 . More successfully defended examples , non-   defended examples , and newly attacked examples   can be found in Tables 13 - 14 in Appendix A.7 .   First , we highlight that a successfully defended   example means that our eKNowIE attack did not   ﬁnd an adversarial instance together with which the   K model would form a pair of In - NLEs , while   our attack did ﬁnd at least one such adversarial   instance for the original model . Second , we no-   tice that even when the selected knowledge might   not be the exact knowledge needed to label an in-   stance correctly , the model can still beneﬁt from   this additional knowledge . For example , in the   ﬁrst sample in Table 14 in Appendix A.7 , the most   proper knowledge triplet would be { dog , Distinct-   From , bird } . However , despite the indirect knowl-   edge given , i.e. , { dog , DistinctFrom , cat } , the   model is able to defend the In - NLE by inferring   that dogs are different from other animals . To ex-   amine whether the improved consistency of the   K models stems from knowledge leakage ( us-   ing the same knowledge triplets in the mitigation   method as in the attack ) , we calculate the overlap   of triplets . On the e - SNLI dataset , we ﬁnd that   only 0.3 % of knowledge triplets are reused for the   attack on the K models , and no overlap was   found for the Cos - E dataset . This indicates that the   leakage is not signiﬁcant .   NLE quality . To evaluate the quality of generated   NLEs , we conducted a human evaluation using   Amazon MTurk , as automatic evaluation metrics   only weakly reﬂect human judgements ( Kayser   et al . , 2021 ) . We follow the setup from Kayser et al .   ( 2021 ): we asked annotators ( three per instance )   to judge whether the generated NLEs justify the   answer with four options : { no , weak no , weak yes ,   yes } and calculated the e - ViL score by mapping   them to{0,1/3,2/3,1 } , respectively . Details of   thehuman evaluation are in Appendix A.5 . In Ta - ble 1 , the K models show similar NLE qual-   ity to their original counterparts , suggesting that   ourK method preserves NLE quality while   decreasing inconsistencies . Similar results are ob-   served on the automatic evaluation of NLEs ( see   Appendix A.6 ) .   4 Related Work   A growing number of works focus on building NLE   models in different areas such as natural language   inference ( Camburu et al . , 2018 ) , question answer-   ing ( Narang et al . , 2020 ) , visual - textual reasoning   ( Hendricks et al . , 2018 ; Kayser et al . , 2021 ; Ma-   jumder et al . , 2022 ) , medical imaging ( Kayser et al . ,   2022 ) , self - driving cars ( Kim et al . , 2018 ) , and of-   fensiveness classiﬁcation ( Sap et al . , 2019 ) . Most   commonly , the performance of these models is as-   sessed only in terms of how plausible the reasons   provided by their NLEs are . To our knowledge ,   Camburu et al . ( 2020 ) is the only work to inves-   tigate inconsistencies in NLEs . We improve their   adversarial attack as well as bring an approach to   alleviate inconsistencies . Works have also been   conducted to analyse and make dialogue models   generate responses consistent with the dialogue   history ( Zhang et al . , 2018 ; Welleck et al . , 2019 ;   Li et al . , 2020 ) . However , these works are difﬁ-   cult to be applied to NLE models , in part because   they require speciﬁc auxiliary datasets , such as   pairs of inconsistent sentences . Other works in-   vestigated the logical consistency of a model ’s pre-   dictions ( Elazar et al . , 2021 ; Mitchell et al . , 2022 ;   Kumar and Joshi , 2022 ; Lin and Ng , 2022 ) , but   would not have straightforward extensions for in-   vestigating NLEs inconsistencies . Besides consis-   tency , NLEs can also be assessed for their faith-   fulness w.r.t . the decision - making process of the   model that they aim to explain ( Wiegreffe et al . ,   2021 ; Atanasova et al . , 2023 ) .   5 Summary and Outlook   We proposed the eKnowIA attack , which is more   generalizable , successful , and faster than the previ-   ous eIA attack in detecting In - NLEs . Our experi-   ments show that current NLE models generate a sig-   niﬁcant number of In - NLEs , and that higher NLE   quality does not necessarily imply fewer inconsis-   tencies . We also introduced a simple but efﬁcient   method that grounds a model into relevant knowl-   edge , decreasing the number of In - NLEs . Our work   paves the way for further work on detecting and   alleviating inconsistencies in NLE models.544Limitations   Our eKnowIA attack contains logical rules de-   signed speciﬁcally for the English language . While   these rules may apply or be adapted to other lan-   guages with simple morphology , there could be   languages in which completely new rules may be   needed . Both our attack and the K method   rely on knowledge bases , which may sometimes be   noisy . We employed manual efforts to eliminate ( a   small number of ) noisy triples from ConceptNet .   Our attack also relies on a manual annotation to   ensure that the adversarial inputs are natural ( esti-   mated to be the case 81.5 % of the time ) . Finally ,   we were not able to test our methods on instances   with long text , as we are not aware of datasets with   NLEs for long text inputs or long NLEs .   Acknowledgements   This work was partially supported by the Alan Tur-   ing Institute under the EPSRC grant EP / N510129/1 ,   by the AXA Research Fund , and by the EU TAI-   LOR grant 952215 . Oana - Maria Camburu was   supported by a Leverhulme Early Career Fellow-   ship . We also acknowledge the use of Oxford ’s   ARC facility , of the EPSRC - funded Tier 2 facility   JADE II ( EP/ T022205/1 ) , and of GPU computing   support by Scan Computers International Ltd.   References545546A Appendix   A.1 Implementation Details   We implemented the WT5 - base model based on   the HuggingFace transformers packageand repli-   cated performance close to the reported results ( see   Section A.3 ) . For the other models , we used the im-   plementations provided by the respective authors .   A single Titan X GPU was used .   A.2 Training RE   We adopted T5 - base ( Raffel et al . , 2020 ) for train-   ing the reverse explainer ( RE ) . We trained   the model for 30 epochs with a batch size of 8 .   For efﬁcient training , early stopping was applied   if the validation loss increases for 10 consecutive   logging steps , which were set to 30,000 iterations .   The dropout ratio was set to 0.1 . We used the   AdamW optimiser ( Loshchilov and Hutter , 2018 )   with learning rate 1eand epsilon 1e . We also   used gradient clipping to a maximum norm of 1.0   and a linear learning rate schedule decaying from   5e .   For Cos - E , we used 10 % of the training data as   the validation set , and the original validation set as   the test set .   A.3 WT5 - base Performance Replication   This section describes the performance of our   trained WT5 - base model . We report the accuracy   for measuring the performance on the natural lan-   guage inference ( NLI ) and CQA tasks . To auto-   matically evaluate the quality of generated NLEs ,   we use the BLEU score ( Papineni et al . , 2002 ) ,   ROUGE ( Lin , 2004 ) , Meteor ( Banerjee and Lavie ,   2005 ) , and the BERT score ( Zhang et al . , 2020 ) ,   which are widely used automatic evaluation met-   rics . The results are summarised in Table 5 . In   terms of accuracy and BLEU score , our replication   performs better than originally reported for Cos - E ,   but produced slightly lower results for e - SNLI.A.4 Naturalness Evaluation of the Generated   Variable Parts   It could be unfair to consider that a model generates   inconsistent NLEs if the adversarial variable parts   are unnatural . Hence , we manually evaluated 50   random samples of generated adversarial variable   parts for each model ( or all samples when there   were less than 50 pairs of inconsistencies found ) .   On e - SNLI , we observe that , on average , 81.5 %   ( ±1.91 ) of the reverse variable parts were natural   instances , i.e. , semantically valid and not contra-   dicting commonsense . The speciﬁc ﬁgures for each   e - SNLI model were 80 % , 80 % , 84 % , and 82 % for   KnowNILE , NILE , WT5 , and KnowWT5 , respec-   tively . We adapted the results in Table 1 to reﬂect   the number of inconsistencies caused only by natu-   ral variable parts .   For the Cos - E dataset , we considered that the   variable parts ( the two incorrect answer choices )   are unnatural if ( 1 ) the answer choices are stop-   words of the NLTK package or ( 2 ) the correct an-   swer is repeated . We observed only one unnatural   case for KnowWT5 and WT5 , respectively , and   none for the other two models . We eliminated the   two cases from the counts .   A.5 Design of Human Evaluation Process for   Assessing NLE Quality   For the human evaluation , we sampled 200 gen-   erated NLEs for each model . Three Anglophone   annotators are employed per instance . We selected   annotators with a Lifetime HITs acceptance rate   of at least 98 % and an accepted number of HITs   greater than 1,000 . However , it is widely known   that the quality of MTurk annotation is not guaran-   teed even for Master workers ( Rouse , 2019 ) . When   we used the e - ViL evaluation framework off - the-   shelf ( Kayser et al . , 2021 ) , we found that many   workers do annotations without due consideration   by simply checking “ yes ” in most cases . We also   initially obtained an inter - annotator agreement cap-   tured by Fleiss ’s Kappa ( K ) of only 0.06 on average   for Cos - E , which casted doubt on the quality of the   evaluation . This prompted us to add a quality con-   trol measure to the evaluation framework . We care-   fully collected trusted examples where the quality   of the NLEs is objectively “ yes ” or “ no ” . For each   HIT consisting of 10 examples , we incorporated   in random locations two trusted examples with the   correct answers being “ yes ” and “ no ” , respectively .   After annotation , we discarded the HITs where547   the annotators gave a wrong answer for any of the   trusted examples ( we consider correct a “ weak yes ”   answer for a “ yes ” trusted example and a “ weak no ”   for a “ no ” trusted example ) . We repeated this pro-   cess until the number of rejected HITs was fewer   than 15 % of the total HITs . We achieved an in-   creasedKvalue of 0.46 and 0.34 for e - SNLI and   Cos - E , respectively , from 0.35 and 0.06 ( without   trusted examples ) . Similar levels of Kas ours were   obtained in other studies , such as ( Marasovi ´ c et al . ,   2022 ; Yordanov et al . , 2022 ) .   A.6 Quality Evaluation on the Generated   NLEs   Table 6 shows the detailed results of human eval-   uation on the quality of generated NLEs . In addi-   tion to the e - ViL score , we followed the evaluation   method of Marasovi ´ c et al . ( 2020 ) by merging weak   noandweak yes tonoandyes , respectively , and   reporting the ratios of w / yes andw / no . Also , the   results of the automatic evaluation metrics are pro-   vided in Table 7 . The results show that all the   Know - models show similar or better results than   their original counterparts.548A.7 Examples549550551ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   We discuss our limitations in the " Limitations " section .   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Section 1 . Introduction   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Sections 3 and 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A.1.552 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Sections 2 and 4 .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Table 2 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4 , Appendix A.1 A. 4 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Appendix A.5 .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix A.5 .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Appendix A.5 .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Section 4.1 . and Appendix A.5 .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Appendix A.5.553