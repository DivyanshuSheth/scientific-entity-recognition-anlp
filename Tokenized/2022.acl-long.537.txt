  Sungho Jeon and Michael Strube   Heidelberg Institute for Theoretical Studies gGmbH   { sungho.jeon , michael.strube}@h-its.org   Abstract   In this paper , we propose an entity - based neu-   ral local coherence model which is linguis-   tically more sound than previously proposed   neural coherence models . Recent neural co-   herence models encode the input document   using large - scale pretrained language models .   Hence their basis for computing local coher-   ence are words and even sub - words . An anal-   ysis of their output shows that these models   frequently compute coherence on the basis of   connections between ( sub-)words which , from   a linguistic perspective , should not play a role .   Still , these models achieve state - of - the - art per-   formance in several end applications . In con-   trast to these models , we compute coherence   on the basis of entities by constraining the in-   put to noun phrases and proper names . This   provides us with an explicit representation of   the most important items in sentences leading   to the notion of focus . This brings our model   linguistically in line with pre - neural models of   computing coherence . It also gives us better   insight into the behaviour of the model thus   leading to better explainability . Our approach   is also in accord with a recent study ( O’Connor   and Andreas , 2021 ) , which shows that most   usable information is captured by nouns and   verbs in transformer - based language models .   We evaluate our model on three downstream   tasks showing that it is not only linguistically   more sound than previous models but also that   it outperforms them in end applications .   1 Introduction   Coherence describes the semantic relation between   elements of a text . It recognizes how well a text is   organized to convey the information to the reader   effectively . Modeling coherence can be beneﬁcial   to any system which needs to process a text . Example Sentence 1   Mr. Specter , seeming exasperated , said in an inter-   view Thursday .   Focus candidates captured by XLNet   “ _ said ” , “ _ in ” , “ day ” , “ _ interview ” , “ _ , ” , “ er ” ,   “ _ an ” , “ th ” , “ s ” , “ _ exasperated ” , ... , “ spect ”   Example Sentence 2   At the same time , unadvertised products may   have almost identical ingredients but less name-   recognition .   Focus candidates captured by XLNet   “ _ name ” , “ ition ” , “ _ products ” , “ - ” , ” _ un ” , “ _ may ” ,   “ _ less ” , “ _ ingredients ” , “ _ have ” , ... , “ _ same ”   Table 1 : The pretrained language model , XLNet Yang   et al . ( 2019 ) , captures undesirable ( sub-)words as focus   ( Jeon and Strube , 2020 ) . The sub - words are sorted by   their attention scores in descending order . In the ﬁrst   example , “ Thursday ” is split into four : “ th ” , “ ur ” , “ s ” ,   and “ day ” . In the second example , some sub - words ,   such as “ ition ” , might be beneﬁcial in their vector space   but the model might exploit spurious information .   Recent neural coherence models ( Mesgar and   Strube , 2018 ; Moon et al . , 2019 ) encode the input   document using large - scale pretrained language   models ( Peters et al . , 2018 ) . These neural models   compute local coherence , semantic relations be-   tween items in adjacent sentences , on the basis of   words and even sub - words .   However , it has been unclear on which basis   these models compute local coherence . Jeon and   Strube ( 2020 ) present a neural coherence model ,   which allows to interpret focus information for the   ﬁrst time . Their investigation reveals that neural   models , adopting large - scale pretrained language   models , compute coherence on the basis of connec-   tions between any ( sub-)words or function words   ( Table 1 , 11 ) . In these cases , the model might   capture the focus based on spurious information .   While such a model might reach or set the state of   the art in some end applications , it will do so for7787the wrong reasons from a linguistic perspective .   This problem did not appear with pre - neural   models , since they compute coherence on the basis   of entities . Early work about pronoun and anaphora   resolution by Sidner ( 1981 , 1983 ) assumes that   there is one single salient entity in a sentence , its   focus , which serves as a preferred antecedent for   anaphoric expressions . Centering theory ( Joshi and   Weinstein , 1981 ; Grosz et al . , 1995 ) builds on these   insights and introduces an algorithm for tracking   changes in focus . Centering theory serves as ba-   sis for many researchers to develop systems com-   puting local coherence by approximating entities   ( Barzilay and Lapata 2008 ; Feng and Hirst 2012 ;   Guinaudeau and Strube 2013 , inter alia ) .   In this paper , we propose a neural coherence   model which is linguistically more sound than pre-   viously proposed neural coherence models . We   compute coherence on the basis of entities by   constraining our model to capture focus on noun   phrases and proper names . This provides us with   an explicit representation of the most important   items in sentences , leading to the notion of focus .   This brings our model linguistically in line with   pre - neural models of coherence .   Our approach is not only linguistically more   sound but also is in accord with a recent empirical   study by O’Connor and Andreas ( 2021 ) who inves-   tigate what contextual information contributes to   accurate predictions in transformer - based language   models . Their experiments show that most usable   information is captured by nouns and verbs . Their   ﬁndings suggest that we can design better neural   models by focusing on speciﬁc context words . Our   work follows their ﬁndings by modeling entity-   based coherence in an end - to - end framework to   improve a neural coherence model .   Our model integrates a local coherence module   with a component which takes context into account .   Our model ﬁrst encodes a document using a pre-   trained language model and identiﬁes entities using   a linguistic parser . The local coherence module   captures the most related representations of entities   between adjacent sentences , the local focus . Then   it tracks the changes of local foci . The second com-   ponent captures the context of a text by averaging   sentence representations .   We evaluate our model on three downstream   tasks : automated essay scoring ( AES ) , assessing   writing quality ( AWQ ) , and assessing discourse   coherence ( ADC ) . AES and AWQ determine textquality for a given text , aiming to replicate human   scoring results . Since coherence is an essential fac-   tor in assessing text quality , many previous coher-   ence models are evaluated on AES and AWQ . ADC   evaluates coherence models on informal texts such   as emails and online reviews . In our evaluation , our   model achieves state - of - the - art performance .   We also perform a series of analyses to investi-   gate how our model works . Our analyses show that   capturing focus on entities gives us better insight   into the behaviour of the model , leading to better   explainability . Using this information , we examine   statistical differences of texts assigned to different   qualities . From the perspective of local coherence ,   we ﬁnd that texts of higher quality are neither se-   mantically too consistent nor too variant . Finally ,   we inspect error cases to examine how our model   works differently compared to previous models .   2 Related Work   Entity - based modeling has been the prevailing ap-   proach to model coherence in pre - neural models .   The entity grid is its most well - known implementa-   tion ( Barzilay and Lapata , 2008 ) . It represents enti-   ties in a two - dimensional array to track their tran-   sitions between sentences . Many variations have   been proposed to improve this model , e.g. , pro-   jecting the grid into a graph representation ( Guin-   audeau and Strube , 2013 ) or converting the grid to   a neural model ( Tien Nguyen and Joty , 2017 ) .   However , the neural version of the entity grid   ( Tien Nguyen and Joty , 2017 ) has two limitations .   First , Lai and Tetreault ( 2018 ) state that entity grids   applied to downstream tasks are often extremely   sparse . In their evaluation , it is difﬁcult to ﬁnd   meaningful entity transitions between sentences in   the grids . Accordingly , this model performs worse   than other neural models . More importantly , this   neural model can not provide any clues of how this   model works since Tien Nguyen and Joty ( 2017 )   apply a convolutional layer on the entity grid . The   feature map of the convolutional layer is not inter-   pretable . They can not examine which entity is as-   signed more importance than others by their model .   In contrast , we constrain our model to capture fo-   cus on entities using noun phrases . Then our model   tracks the changes of focus . Hence , it provides us   with an interpretable focus ( Section 5 ) .   More recently , Moon et al . ( 2019 ) propose a neu-   ral coherence model to exploit both local and struc-   tural aspects . They evaluate their model on an arti-7788ﬁcial task only , the shufﬂe test , which determines   whether sentences in a document are shufﬂed or not .   However , recent studies ( Pishdad et al . , 2020 ) claim   that this artiﬁcial task is not suitable to evaluate co-   herence models . Lai and Tetreault ( 2018 ) show that   the neural coherence models , which achieve the   best performance on this task , do not outperform   non - neural models on downstream tasks . More   recently , Mohiuddin et al . ( 2021 ) ﬁnd a weak corre-   lation between the model performance in artiﬁcial   tasks and downstream tasks . In our evaluation , we   compare Moon et al . ( 2019 ) with ours in an arti-   ﬁcial task as well as in three downstream tasks .   Moon et al . ( 2019 ) perform the best in the artiﬁ-   cial task , but do not outperform our model in three   downstream tasks ( Section 4 ) .   3 Our Model   Figure 1 presents the architecture of our model .   We ﬁrst introduce our entity representation and sen-   tence encoding using a pretrained language model .   Next , we describe a novel local coherence model .   We then combine the two representations of local   coherence and the context vector , simply averaged   sentence representations . Finally , we apply a feed-   forward network to produce a score label .   3.1 Sentence Encoding   We use a pretrained language model ( Yang et al . ,   2019 ) to encode sentences . XLNet learns bidirec-   tional contexts by maximizing expected likelihood   using an autoregressive training objective . Hence   it allows to capture the focus in sentences . XLNet   outperforms other language models in tasks which   require processing long texts .   Recent work investigates that pretrained lan-   guage models learn linguistic features that are help-   ful for language understanding ( Tenney et al . , 2019 ;   Warstadt et al . , 2020 ) . Inspired by this , we encode   two adjacent sentences at once to capture discourse   features , such as coreference relations . In this strat-   egy , items are encoded twice except the items in-   cluded in the ﬁrst and the last sentence . We interpo-   late items encoded twice to consider context with   regard to the preceding and succeeding sentence .   We encode an input document using XLNet to   obtain word representations . Sentence represen-   tations are means of all word representations in a   sentence . We then feed sentence representations   and the noun phrase representations into the the   coherence modules .   In formal deﬁnitions , let E =   [ h;:::;h;h;:::;h ]   denote the output of encoding , where eindicates   the index of encoding , and mindicates the index of   a subword ( w ) in the sentence ( s).hindicates the   encoded representation of w. This encoding output   includes the encoded representations of sands   since we encode two adjacent sentences at once .   Likewise , E= [ h;:::;h ]   is the output in the next encoding , and it includes   the encoded representations of sands . Then ,   the encoded representation of sis a sequence of   ih = avg(h;h ) , which   is the interpolated representation of sin the   two encoding stages ( eande+ 1 ) . We iterate this   process to encode all adjacent sentences .   3.2 Entity Identiﬁcation   Pretrained language models encode sequences as   sub - words , but to our knowledge , there is no lin-   guistic parser using sub - words as input . Hence , we   use a linguistic parser to identify noun phrases in   each sentence separately . Kitaev and Klein ( 2018 )   present a neural constituency parser which deter-   mines the syntactic structure of a sentence . To   identify noun phrases and proper names , we ap-7789ply this parser to the original sentences , then map   parsed constituents to sub - word tokens .   Since pretrained language models do not have   the means to represent phrase meaning compo-   sition , we average sub - word representations for   phrases which consists of multiple sub - words .   While this implementation does not capture the   complex meaning of phrases , Yu and Ettinger   ( 2020 ) report that it shows higher correlation with   human annotations than using the last word of   phrases , assuming that the last word of a phrase is   its head .   LetNP= [ np;np;:::;np]denote a se-   quence of noun phases ( np ) in theith sentence ,   andjindicates the index of a noun phrase in the   sentence . Each representation of a noun phrase   is obtained as np = avg(ih;:::;ih ) , where   ihindicates the subword tokens contributing to   the same entity .   3.3 Local Coherence Module   We compare the semantic representations of noun   phrases between adjacent sentences . The two most   similar representations of noun phrases are taken as   local focus of the respective sentences . These two   representations are averaged to capture the com-   mon context . We use cosine similarity to measure   semantic similarity .   We notice that some sentences do not include   noun phrases , approximately 3.5 % in the three   datasets used in our evaluation . This mostly oc-   curs when some words are omitted as in cases of   ellipsis ( Hardt and Romero , 2004 ) . In such cases ,   we maintain the focus of the previous sentence to   preserve the context .   A depthwise convolutional layer is applied to the   local focus to record its transitions . Unlike a typical   convolutional layer , the depthwise convolutional   layer captures the patterns of semantic changes   between different time - steps for the same spatial   information ( Chollet , 2017 ) . In our model , this   layer captures the semantic changes between local   foci considering the context but on the same spatial   dimension of each focus . Hence , it does not hurt the   explainability of our model . We use the lightweight   depthwise convolutional layer ( Wu et al . , 2019 ) .   Then we update the representations of local foci   to track the semantic changes between them . We   use the Tree - Transformer which updates its hid-   den representations by inducing a tree - structure   from a document ( Wang et al . , 2019 ) . It generatesconstituent priors by calculating neighboring atten-   tion which represents the probability of whether   adjacent items are in the same constituent . The   constituent priors constrain the self - attention of the   transformer to follow the induced structure .   Finally , we apply document attention to produce   the weighted sum of all the updated local focus   representations . The document attention identiﬁes   relative weights of updated representations which   enables our model to handle any document length .   In formal descriptions , let mnpdenote the rep-   resentations of two noun phrases which have the   highest cosine similarity scores between the ith   andi+ 1th sentence . Then , we deﬁne LocalF =   [ localf;:::;localf ] , wherelocalfis an averaged   representation of mnpandmnp . It rep-   resents the sequence of local foci between the   ith andi+ 1th sentence , and lindicates the in-   dex of the local focus in the document . Finally ,   the local coherence representation is obtained as   lcr = doc_attn(tree_trans ( dconv ( LocalF ) ) )   wheredconv indicates the depthwise convolutional   layer , tree_trans indicates the Tree - Transformer ,   anddoc_attn indicates the document attention .   4 Experiments   4.1 Implementation Details   We implement our model using the PyTorch library   and use the Stanford Stanza libraryfor sentence   tokenization . We employ XLNet for the pretrained   language model . For the baselines which do not   employ a pretrained language model ( Dong et al . ,   2017 ; Mesgar and Strube , 2018 ) , GloVe is em-   ployed for word embeddings , trained on Google   News ( Pennington et al . , 2014 ) ( see Appendix A   for more details ) .   To compare baselines within the same frame-   work , we re - implement all of them in PyTorch . We   then use our re - implementation to report the per-   formance of models with 10 runs with different   random seeds . We verify statistical signiﬁcance ( p-   value<0.01 ) with both a one - sample t - test , which   veriﬁes the reproducibility of the performance of   each model , and a two - sample t - test , which veriﬁes   that the performance of our model is statistically   signiﬁcantly different from other models .   Within the same framework we compare the size   of models used in our experiments . Our neural   model uses a number of parameters comparable to   the state of the art , the transformer - based model7790(Moon et al . ( 2019 ): 118 M < Jeon and Strube   ( 2020 ): 136 M < Our model : 137 M ) .   4.2 Baselines : Neural Coherence Models   In all three downstream tasks , we compare our   model against recent neural coherence models .   First , Mesgar and Strube ( 2018 ) propose a neural   local coherence model , based on Centering theory .   This model connects the most related states of a   Recurrent Neural Network , then represents the co-   herence patterns using semantic distances between   the states . Second , Moon et al . ( 2019 ) propose   a uniﬁed neural coherence model to consider lo-   cal and structural aspects . This model consists of   two modules when they employ a pretrained lan-   guage model ( Peters et al . , 2018 ): a module of   inter - sentence relations using a bilinear layer and a   topic structure module applying a depth - wise con-   volutional layer to the sentence representations . To   ensure fair comparison , XLNet is employed for   this model as well , instead of ELMo ( Peters et al . ,   2018 ) . More recently , Jeon and Strube ( 2020 ) pro-   pose a neural coherence model approximating the   structure of a document by connecting linguistic in-   sights and a pretrained language model . This model   consists of two sub - modules . First , a discourse   segment parser constructs structural relationships   for discourse segments by tracking the changes   of focus between discourse segments . Second , a   structure - aware transformer updates sentence rep-   resentation using this structural information .   4.3 Artiﬁcial Task : Shufﬂe Test   We ﬁrst evaluate our model on the artiﬁcial setup ,   the shufﬂe test , used in earlier works ( Table 2 ) . We   follow the setup used in Lai and Tetreault ( 2018 ) .   In this setup , our model outperforms a simple neu-   ral model relying on the pretrained language model .   Moon et al . ( 2019 ) evaluate their models only in   this setup . It achieves outstanding performance in   this setup . However , in the following sections , our   results show that this model does not outperform   our model in downstream tasks .   Avg Acc   Moon et al . ( 2019)-XLNet-1Sent 90.57   Our Model 84.35This result is not surprising . There is a line of   recent work which shows that this setup is not ca-   pable of evaluating coherence models from diverse   perspectives . Laban et al . ( 2021 ) show that employ-   ing ﬁne - tuned language models simply achieves   a near - perfect accuracy on this setup . O’Connor   and Andreas ( 2021 ) measure usable information by   selectively ablating lexical and structural informa-   tion in transformer - based language models . Their   ﬁndings show that prediction accuracy depends on   information about local word co - occurrences , but   not word order or global position . We suspect that   exploiting all information of a sentence is sufﬁcient   for shufﬂe tests to capture patterns to distinguish   whether sentences in a document are shufﬂed or not .   Based on these ﬁndings , we evaluate our model on   three downstream tasks used for evaluating coher-   ence models , automated essay scoring , assessing   writing quality , and assessing discourse coherence .   We advise future work not to evaluate coherence   models on the artiﬁcial setup solely .   4.4 Automated Essay Scoring ( AES )   Dataset . To evaluate the coherence models on   AES , we evaluate them on the Test of English as   a Foreign Language ( TOEFL ) dataset ( Blanchard   et al . , 2013 ) . While the Automated Student As-   sessment Prize ( ASAP ) datasetis frequently used   for AES , TOEFL has a generally higher quality of   essays compared to essays in ASAP . The prompts   in ASAP are written by students in grade levels 7   to 10 of US middle schools . Many essays in ASAP   consist of only a few sentences . In contrast , the   prompts in TOEFL are submitted for the standard   English test for the entrance to universities by non-   native students . The prompts in TOEFL do not vary   so much , the student population is more controlled ,   and essays have a similar length .   Evaluation Setup . We follow the evaluation setup   of previous work on AES ( Taghipour and Ng ,   2016 ) . For TOEFL , we evaluate performance with   accuracy for the 3 - class classiﬁcation problem with   5 - fold cross - validation . We use the same split   for the cross - validation , used by Jeon and Strube   ( 2020 ) . The cross - entropy loss is deployed for train-   ing . The ADAM optimizer is used for our model   with a learning rate of 0.003 . We evaluate perfor-   mance for 25 epochs on the validation set with a   mini - batch size of 32 . The model which reaches the7791ModelPromptAvg1 2 3 4 5 6 7 8   Dong et al . ( 2017 ) 69.30 66.47 65.84 66.38 68.89 64.20 67.11 65.73 66.74   Mesgar and Strube ( 2018 ) 56.25 55.94 55.20 57.20 56.57 55.10 56.97 58.39 56.45   Averaged - XLNet-1S 70.73 69.48 68.98 67.52 72.35 70.94 70.14 69.01 69.89   Moon et al . ( 2019)-XLNet 73.75 72.13 72.92 73.29 75.12 74.69 72.89 72.09 73.36   Jeon and Strube ( 2020)-1S 75.10 73.35 74.75 74.18 76.38 74.30 73.61 73.44 74.39   Jeon and Strube ( 2020)-2S 76.35 75.40 75.00 74.85 77.63 74.06 73.71 74.00 75.12   Our Model 78.38 75.70 76.58 76.56 79.10 76.41 75.03 74.57 76.54   best accuracy on the validation set is then applied   to the test set .   Baselines . We compare against Dong et al . ( 2017 ) ,   a neural model proposed for AES . They present a   model consisting of a convolutional layer , followed   by a recurrent layer , and an attention layer ( Bah-   danau et al . , 2015 ) between the adjacent tokens .   Results . Table 3 reports the performance on   TOEFL . Dong et al . ( 2017 ) report better perfor-   mance than the more recent neural model based   on Centering theory ( Mesgar and Strube , 2018 ) . A   simple model relying on the pretrained language   model outperforms this model , which averages all   sentence representations ( henceforth , Avg - XLNet ) .   Moon et al . ( 2019 ) show that their uniﬁed model   outperforms previous models on the artiﬁcial task ,   the shufﬂe test . However , it does not outperform   the previous models on the AES task . Jeon and   Strube ( 2020 ) outperform previous models . Finally ,   our model , which integrates local and structural as-   pects , achieves state - of - the - art performance . We   perform an ablation study to investigate the con-   tribution of individual components . We compare   with Jeon and Strube ( 2020 ) who encode two adja-   cent sentences using the pretrained language model   ( 2SentsEnc ) . Our results verify that this encoding   improves performance , but our model beneﬁts from   the novel local coherence module even more .   4.5 Assessing Writing Quality ( A WQ )   Dataset . Louis and Nenkova ( 2013 ) create a   dataset of scientiﬁc articles from the New York   Times ( NYT ) for assessing writing quality . They   assign each article to one of two classes by a semi-   supervised approach : typical or good . Though   articles included in both classes are of good quality   overall , Louis and Nenkova ( 2013 ) show that lin - NYT   Liu and Lapata ( 2018)-reimpl 54.35 ( 1.00 )   Averaged - XLNet-1SentEnc 67.53 ( 3.48 )   Moon et al . ( 2019)-XLNet-1Sent 74.75 ( 1.27 )   Jeon and Strube ( 2020)-1Sent 75.12 ( 1.10 )   Jeon and Strube ( 2020)-2Sents 76.43 ( 0.88 )   Our Model 77.52 ( 0.42 )   guistic features contribute to distinguish different   classes of writing quality .   Evaluation Setup . For NYT , we follow the setup   used in previous work . Louis and Nenkova ( 2013 )   and Ferracane et al . ( 2019 ) undersample the dataset   to mitigate the bias of the uneven label distribution .   Following Ferracane et al . ( 2019 ) , Jeon and Strube   ( 2020 ) partition the dataset into 80 % training , 10 %   validation , and 10 % test set , respectively . We use   the ADAM optimizer with a learning rate of 0.001   and a mini - batch size of 32 . We evaluate perfor-   mance for 25 epochs .   Baselines . Liu and Lapata ( 2018 ) propose a neural   model which induces structural information with-   out a labeled resource . It induces a non - projective   dependency structure by structured attention .   Results . Table 4 shows the performance on NYT .   Ferracane et al . ( 2019 ) reported the best perfor-   mance of the latent learning model for discourse   structure ( Liu and Lapata , 2018 ) on NYT . How-   ever , Jeon and Strube ( 2020 ) show that the good   results are due to embeddings obtained by train-   ing on the target dataset . They also report that   Avg - XLNet outperforms this model which employs   Glove embeddings . Moon et al . ( 2019 ) show better   performance than this simple model , but it does7792Model Yahoo Clinton Enron Yelp Avg Acc   Li and Jurafsky ( 2017 ) 53.5 61.0 54.4 49.1 51.7   Mesgar and Strube ( 2018 ) 47.3 ( 1.8 ) 57.7 ( 0.6 ) 50.6 ( 1.2 ) 54.6 ( 0.3 ) 52.6   Lai and Tetreault ( 2018 ) 54.9 60.2 53.2 54.4 55.7   Avg - XLNet-1Sent 58.0 ( 3.9 ) 57.6 ( 0.3 ) 54.3 ( 0.8 ) 55.9 ( 0.4 ) 56.4   Moon et al . ( 2019)-XLNet-1SentEnc 56.2 ( 0.5 ) 61.0 ( 0.4 ) 53.6 ( 0.5 ) 56.6 ( 0.4 ) 56.9   Jeon and Strube ( 2020)-1SentEnc 56.4 ( 0.6 ) 62.5 ( 0.9 ) 54.5 ( 0.4 ) 56.9 ( 0.3 ) 57.6   Jeon and Strube ( 2020)-2SentsEnc 57.2 ( 0.5 ) 63.0 ( 0.4 ) 54.4 ( 0.4 ) 56.9 ( 0.2 ) 57.9   Our Model 58.4 ( 0.2 ) 64.2 ( 0.4 ) 55.3 ( 0.3 ) 57.3 ( 0.2 ) 58.9   not outperform Jeon and Strube ( 2020 ) . Our model   achieves state - of - the - art performance . An abla-   tion study of the joint sentence encoding , Jeon and   Strube ( 2020)-2SentsEnc , veriﬁes that our model   gains improvements not only from this encoding   but also from our local coherence module .   4.6 Assessing Discourse Coherence ( ADC )   Dataset . While previous work evaluates coherence   models on formally written texts ( Barzilay and La-   pata , 2008 ) , GCDC ( Lai and Tetreault , 2018 ) is   designed to evaluate coherence models on infor-   mal texts , such as emails or online reviews . The   dataset contains four domains : Clinton and Enron   for emails , Yahoo for questions and answers in an   online forum , and Yelp for online reviews of busi-   nesses . The quality of the dataset is controlled to   have evenly - distributed scores and a low correla-   tion between discourse length and scores .   Evaluation Setup . For GCDC , we perform the   experiments following previous work ( Lai and   Tetreault , 2018 ) . We perform 10 - fold cross-   validation , use accuracy as evaluation measure on   the 3 - class classiﬁcation , and use the cross - entropy   loss function .   Baselines . Li and Jurafsky ( 2017 ) propose a neu-   ral model based on cliques , that are sets of adja-   cent sentences . This model uses the cliques taken   from the original article as a positive label and uses   cliques with randomly permutated ones as a neg-   ative label . Lai and Tetreault ( 2018 ) show that a   simple neural model which uses paragraph infor-   mation outperforms previous models on GCDC .   Results . Table 5 summarizes the performance on   GCDC . While Avg - XLNet outperforms previous   baselines , other advanced neural models show sim - ilar performance . Our model performs slightly   better than Jeon and Strube ( 2020 ) with two sen-   tences encoding . This shows that the gains mainly   beneﬁt from this encoding strategy . We suspect   that Jeon and Strube ( 2020 ) do not beneﬁt from   structural information since texts on GCDC are not   well - organized . The texts mostly consist of a few   sentences , and they express the writers ’ emotion .   Based on this , Lai and Tetreault ( 2018 ) state that   texts of lower quality have sudden topic changes .   We also suspect that human annotators recognize   important entities in the texts , such as the name of   a person in the US government .   4.7 Ablation Study   Since our model consists of several components ,   we examine the inﬂuence of each component on   the performance of the AES task . Speciﬁcally , we   ﬁrst examine the inﬂuence of our local coherence   module . Then we examine the inﬂuence of the   Tree - Transformer compared to a naive Transformer .   Lastly , we examine the inﬂuence of the depth - wise   convolutional layer deployed ahead of the Tree-   Transformer .   Table 7 shows that each component contributes   to the performance meaningfully while the depth-   wise convolutional layer increases the performance   slightly . This suggests that we could design a bet-   ter component in future work to capture semantic   transitions between local foci .   Avg Acc   Ours - Local Coherence Module 72.27   Ours - Tree - Transformer - Depth - Conv 75.69   Ours - Depth - Conv 76.25   Our Full Model 76.547793TOEFL : Prompt 1 NYT-1516415   Focus on any ( % ) Focus on noun phrases ( % ) Focus on any ( % ) Focus on noun phrases ( % )   _ broad ( 3.63 ) i ( 5.45 ) _ theory ( 4.03 ) it ( 4.96 )   _ many ( 1.79 ) you ( 2.74 ) _ universe ( 3.22 ) we ( 4.13 )   _ special ( 1.50 ) broad knowledge ( 2.64 ) _ said ( 2.42 ) the universe ( 2.48 )   i ( 1.47 ) it ( 2.38 ) stan ( 2.42 ) he ( 2.48 )   _ specialize ( 1.46 ) we ( 1.74 ) ein ( 2.42 ) physics ( 1.65 )   _ know ( 1.05 ) knowledge ( 1.34 ) dr ( 2.42 ) space ( 1.65 )   _ specialized ( 0.99 ) he ( 1.30 ) _ do ( 2.42 ) string theory ( 1.65 )   5 Analysis   5.1 Capturing Focus Using Entities   In Centering theory , the focus is described as the   most important item in a sentence . Jeon and Strube   ( 2020 ) capture the focus using attention scores and   analyze texts assigned to different qualities using   this focus . They state that the focus is difﬁcult to   interpret when it is composed of sub - words . To   investigate this further , we compare the focus cap-   tured on any ( sub-)words and the focus constrained   to entities . Table 6 indicates that constraining focus   to entities leads to better explainability , in partic-   ular on NYT . For example , in the NYT-1516415   news article about String theory , a subword of “ ein ”   is not an interpretable focus . It may , however , in-   clude useful information in the vector space for a   neural model . In contrast , our entity - based model   leads to better explainability . Instead of “ ein ” , it   provides the more interpretable focus , “ Einstein ” ,   a theoretical physicist . In TOEFL , “ broad knowl-   edge ” is a more interpretable focus than a focus   consisting of the single subword tokens , “ broad ” .   Table 6 also shows that our model mainly uses pro-   nouns , and noun phrases are playing an important   role to represent focus . This suggests that further   investigation is needed to understand how language   models work on pronouns to process a text .   5.2 Local Coherence Patterns   Using interpretable focus information , we inves-   tigate differences in focus transitions of texts as-   signed to different scores . Motivated by the def-   inition of the continue and the shift transition in   Centering theory , we deﬁne semantic consistency   which represents the degree of semantic changes   between local foci . Two adjacent sentences are   semantically consistent when the semantic simi - larity ( sim ) between the local foci ( lf ) is higher   than a semantic threshold (  ) . This thresh-   old is determined as the average of semantic sim-   ilarities between local foci of adjacent sentences   in texts assigned the same score . Otherwise , a   semantic transition ( st)occurs between the local   foci : st= 1 ifsim < . Finally , the   semantic consistency ( SC ) is deﬁned as follows :   SC= 1 (count ( st)=jlfj ) .   Figure 2 illustrates the semantic consistency on   TOEFL , and Table 8 shows the statistics of the se-   mantic consistency on texts assigned to different   scores . Texts assigned a high score show lower   semantic consistency on average . This indicates   that texts of higher quality are overall more se-   mantically variant than texts of lower quality . Ad-   ditionally , we observe that texts assigned a low   score show signiﬁcantly larger proportions of an   extreme level of semantic consistency . We deﬁne   the extreme level as either texts whose semantic   consistency is lower than 5 % , indicating texts are   highly variant , or texts whose semantic consistency   is higher than 75 % , indicating texts are highly con-   sistent . Hence , these ﬁndings indicate that texts of   lower quality are semantically too variant or too   consistent . Texts of higher quality are neither too   variant nor too consistent .   We next inspect the focus of texts assigned to   different scores ( see Table 15,16 , and 17 in the   Appendix D for more details ) . This shows that   pronouns more frequently indicate the local focus   in texts of lower quality than in texts of higher   quality . The essays in TOEFL are argumentative   essays , and good essays should use facts and evi-   dence to support their claim ( Wingate , 2012 ) . We   observe that texts assigned a low score frequently   include claims without convincing evidence . This7794   causes our model to capture focus based on pro-   nouns more frequently in these texts . In contrast ,   texts assigned a high score include convincing ev-   idence to support claims , and this lets our model   capture different types of foci in these texts .   5.3 Error Analysis   Finally , we conduct an error analysis to investigate   how our model works differently compared to pre-   vious coherence models on TOEFL . We ﬁrst com-   pare the predicted scores with Moon et al . ( 2019 )   and a simple model which only considers context ,   averaged - XLNet . These two baselines show biased   predictions in the middle score . We suspect that   this is caused by the label bias in TOEFL ( Blan-   chard et al . , 2013 ) . Biased label distributions cause   biased predictions , and they beneﬁt from these bi-   ased predictions . In contrast , our model beneﬁts   more from predicting high scores correctly as well   as other scores , indicating that our coherence model   assesses text quality better .   We then compare with the previous state of the   art ( Jeon and Strube , 2020 ) . This baseline induces   discourse structure to model structural coherence .   It captures semantic relations between discourse   segments , not just between adjacent sentences . We   observe two error cases when this baseline strug-   gles to predict correctly . It predicts scores lower   than the ground - truth score for texts which lack   support and evidence for claims . However , these   texts have a well - organized paragraph for one or   two claims . We suspect that this leads human an-   notators to assign a mid or a high score though   the text is not well - organized overall . In contrast ,   it predicts scores higher than ground - truth scores   when unrelated claims are listed or claims are listedSSS   Avg SC 55.87 54.45 54.05   ( std ) ( 24.53 ) ( 21.38 ) ( 19.70 )   Prop of Ext level 17.63 11.54 8.59   without evidence . Our model , which captures lo-   cal coherence between adjacent sentences , deals   with these cases better ( see Table 18 and 19 in the   Appendix D for more details ) .   6 Conclusions   We propose a neural coherence model based on   entities by constraining the input to noun phrases .   This makes our model better explainable and sets   a new state of the art in end applications . It also   allows us to reveal that texts of higher quality are   neither semantically too consistent nor too variant .   Our ﬁndings suggest a few interesting directions   for future work . Our analysis shows that pretrained   language models frequently exploit coreference re-   lations to capture semantic relations . We could   design an advanced neural model which exploits   these relations explicitly . Lastly , our work could   be extended to a multilingual setup . Our model   is not tied to a speciﬁc pretrained language model   but connect a language model with linguistic in-   sights . It can employ a multilingual model ( Xue   et al . , 2021 ) , and our datasets can be translated to   other languages.7795Acknowledgments   The authors would like to thank the anonymous   reviewers for their comments . This work has been   funded by the Klaus Tschira Foundation , Heidel-   berg , Germany . The ﬁrst author has been supported   by a Heidelberg Institute for Theoretical Studies   Ph.D. scholarship .   References77967797A Training and Parameters   For the three datasets , we use a mini - batch size   of 32 with random - shufﬂe . The ADAM optimizer   is used to train our models with a learning rate   of 0.001 and epsilon of 1e-4 . We evaluate perfor-   mance for 25 epochs . For the baseline models   which do not use a pretrained language model ,   we use Glove pretrained embeddings with 100-   dimensional for TOEFL and with 50 - dimensional   for NYT . We clip gradients by 1.0 . To update sen-   tence representations obtained by a pretrained lan-   guage model , we use the same dimension of the   pretrained language model on a tree - transformer .   We manually tune hyperparameters .   We encode adjacent two sentences at once us-   ing XLNet instead of the whole document at once .   Our dataset consists of long documents i.e. , journal   articles with more than 3,000 tokens . For employ-   ing the pretrained model , it is practically infeasible   to encode all words in a document at once due to   memory limitations . We use 23 GB GPU memory   a NVidia P40 on ADC and AES and 46 GB GPU   memory of two NVidia P40s for each run on AWQ .   For training our model , it takes approximately 0.8   days on TOEFL , 6.5 days on NYT , and 0.6 days on   GCDC .   B Data Description Details   Table 9 describes statistics on two datasets ,   TOEFLand NYT . We split a text at the sentence   level by Stanford Stanza library , and tokenize them   by the XLNet tokenizer . Table 10 describes the   topic of each prompt in TOEFL . They are all open-   ended tasks , that do not have given context but   require students to submit their opinion .   C Focus Examples   Table 11 shows the cases that the pretrained lan-   guage model , XLNet , captures the undesirable ( sub-   ) words as focus . We observe that the subword   tokenizer often split named entities into subword   tokens unexpectedly , and some words are unex-   pectedly split into subword tokens as preﬁxes and   sufﬁxes , such as “ _ un ” or “ ition ” . These observa-   tions suggest that we need to consider tokens as a   span to capture the meaning of words better .   D Evaluations Details   We report not only the more details of the perfor-   mance on test sets ( Table 12 ) but also the perfor-   mance on validation sets on the AES task ( Table   13 ) .   E Analysis Details   We compare the focus captured on ( sub-)words and   the focus constrained to entities on more datasets   ( Table 14 ) . We observe that our entity modeling   leads to better explainability.7798Prompt 1 Agree or Disagree : It is better to have   broad knowledge of many academic   subjects than to specialize in one spe-   ciﬁc subject .   Prompt 2 Agree or Disagree : Young people en-   joy life more than older people do .   Prompt 3 Agree or Disagree : Young people   nowadays do not give enough time   to helping their communities .   Prompt 4 Agree or Disagree : Most advertise-   ments make products seem much bet-   ter than they really are .   Prompt 5 Agree or Disagree : In twenty years ,   there will be fewer cars in use than   there are today .   Prompt 6 Agree or Disagree : The best way to   travel is in a group led by a tour guide .   Prompt 7 Agree or Disagree : It is more impor-   tant for students to understand ideas   and concepts than it is for them to   learn facts .   Prompt 8 Agree or Disagree : Successful people   try new things and take risks rather   than only doing what they already   know how to do well.77997800780178027803Error Type Example Essay   C In my opinion is better to have a knowledge specialize in one particular subject since   this is better to know a thing as well as you can . This is true in all the experiences of   the life : refered to the university , e.g. , the italian university , we can take the example   of the of the two years of specialization . An other example we can see in a top - tier   company , in fact each people that there are in this have a speciﬁc work to do and this   bring to an excellent ﬁnal operation . A person that are magniﬁcally prepared on one   thing will arrive at a sicure result because that " " is your bred " " ; we can also observe   that the most good professors , scientists , sport players are all specialize on that they   work and do not specialize on many works . We can also observe that the colloboration   of great brains , each of them specialized on a thing , is important in many ways of the   our life .   C I strongly agree with the statement that knowing several subjects and being polyvalent   in various ﬁelds is much more important that specializing in one area .   These days , things are changing so fast that the moment you start a career or a   specialization , the minute the facts and ﬁgures of the subject have changed . This   essence of broad knowledge is what makes people succeed in the world . Unless you   are 100 % sure that you vocationally desire to specialize in a subject , the risk of not   ﬁnding a suitable job because of the deviation of job offering is too high . Both with   respect to time and money . For example , imagine that you decide to study IT sometime   around the Internet boom . After you ﬁnish the 5 years of studying , you get out to   society with high hopes and great expectations and suddenly you realize that the world   does not need for IT people anymore because the market crashed down ! Then you   would most probably regret not to have chosen a more general Engineering degree   such as an Electronical Engineering degree . Take the example of a devoted music   students that really loves to play music to the point that they drop classes so they can   go and play their music . Perhaps , they will become a succesful singer or solo player ,   but the chances that they fail are there and when that really comes true , they will not   be able to attend university classes because they did n’t passed high - school . Good and   innovative ideas often are the result of composing other ideas . If on one side , you know   how pollution of carbon dioxide is chemically produced and on the other , you are an   expert on plant species , perhaps you can ﬁnd a way to create a system to purify the air   in the world . And moreover , if you have skills of marchandising and marketing , you   can probably be in the Forbes ’ next month main page .   Think that you can always specialize in the future . Going from the trunck of a tree to   the tip of a branch is easy , but getting from one tip to another tip is , literally , as going   back in time.7804Error Type Example Essay   C It seems difﬁcult to choose one direction , becuse they are also have colorful life   between the young people and the older people , but it does not mean , they are similar   to me . I would like to agree with the young people enjoy life more than older people   do , if a personal quality can be considerated as criterion to choose things .   First of all , nowadays , era of information , many young people enjoy their life via the   internet , even everything is possible in the digital industy . For instance , if a grandson   of the older people live abroad , and the communication between the grandson and   the grandfather is only via the telephone instead the internet online chatting what is   cheaper than the international telephone call , but the older people can not use the   internet , even they can not use a computer .   On the other hand , the young people can adapt an environment quickly , so that they   can migrate to another city for the different experience . most of older persons can not   accept the different enviroment and what they will eat in the different areas , if the older   person migrate to other citys or countries , they will be illness easier .   The important things determining the young people enjoy life better is that they are   educated in the signifcant era of information , so they are developed with the world   development .   For all mentioned above is why I agree with the statement that young people enjoy life   more than older people do . Now , I do strongly agree with the statement .   C Yes , it is better to have a broad knowledge of many academic subjects than specialisze   in one speciﬁc area because of various reasons .   If people have knowledge about a particular subject , it is good . But if they want to   refrain themselves from foraying from other subjects they should make sure that they   are very thorough with that subject . Because ﬁnally they should ﬁnd a job on that basis   only and more ver all the academic topics are interconnected so , it imperative to have   knowledgein various ﬁelds .   The above option would be good only if they ﬁnd a job . They should always keep in   mind the different possibilities in their carreer . They should ask themselves " " what if i   do nt get a job in my desired ﬁeld of study ? " "   For instance I am a mechanical engineering student . as every one knows there is a   difﬁcult of getting jobs for mechanical engineers.if i continue with the same ﬁeld   would be left unemployed . Here I need to have an alternate option . I have my alternate   option as computer sciences .I started learning some computer subjects . Now even if   i do not get a job in my ﬁeld of study , i may have a chance of getting it in ﬁeld of   computers . This would not leave me unemployed . I personally feel that being employed   is better than being unemployed .   This criteria not only works for two ﬁelds of same backround , it also works for a   technical background and an arts background . For example , an electrical engineer who   does not have a job and whose hobby is singing , can survive by giving some stage   shows . Which would also be considered as an employment .   Additionally , broader knowlege would not leave you speechless when you are in a   group . Because when a group is discussing a topic and if you are silent , you may feel   embarassing with that . But if you are familiar with the topic you can also give your   opinion on the topic . this is possible only if you do not conﬁne yourself to a particular   ﬁeld .   Therefore , I conclude that having a broad knowledge is better than to specialize in one   subject.7805