  Em¯ıls Kadikis , Vaibhav Srivastav , and Roman Klinger   Institut für Maschinelle Sprachverarbeitung   University of Stuttgart   Pfaffenwaldring 5b , 70569 Stuttgart   { emils.kadikis,vaibhav.srivastav,klinger@ims.uni-stuttgart.de }   Abstract   The task of abductive natural language infer-   ence ( αNLI ) , to decide which hypothesis is   the more likely explanation for a set of obser-   vations , is a particularly difficult type of NLI .   Instead of just determining a causal relation-   ship , it requires common sense to also eval-   uate how reasonable an explanation is . All   recent competitive systems build on top of con-   textualized representations and make use of   transformer architectures for learning an NLI   model . When somebody is faced with a particu-   lar NLI task , they need to select the best model   that is available . This is a time - consuming and   resource - intense endeavour . To solve this prac-   tical problem , we propose a simple method   for predicting the performance without actually   fine - tuning the model . We do this by testing   how well the pre - trained models perform on   theαNLI task when just comparing sentence   embeddings with cosine similarity to what the   performance that is achieved when training a   classifier on top of these embeddings . We show   that the accuracy of the cosine similarity ap-   proach correlates strongly with the accuracy of   the classification approach with a Pearson cor-   relation coefficient of 0.65 . Since the similarity   computation is orders of magnitude faster to   compute on a given dataset ( less than a minute   vs. hours ) , our method can lead to significant   time savings in the process of model selection .   1 Introduction   Abduction is a type of reasoning that infers an ex-   planation for some observations ( Peirce , 1931 ) . It   is a particularly challenging type of inference ; as   opposed to deduction and induction , which derive   the conclusion from only the information present   in the observations , abduction requires making as-   sumptions about an implicit context beyond just the   given observations . Abductive reasoning is there-   fore at the core of the way humans understand the   world and how world knowledge is involved . Abductive reasoning in the natural language do-   main has been introduced by Bhagavatula et al .   ( 2020 ) who defined the abductive natural language   inference ( αNLI ) task . In it , we are given four   sentences – two observations oandoand two   hypotheses handh , where we know that the se-   quence of events was o→(h|h)→o . The   task then is to decide which of the two hypotheses   is the more plausible one .   An example from Bhagavatula et al . ( 2020 ) is   the following :   o : It was lunchtime and Kat was hungry .   o : Kat and her coworkers enjoyed a nice lunch   outside .   h : Kat went to get a salad .   h : Kat decided to take a nap instead of eating .   While it is not inconceivable that someone would   decide to take a nap on their lunch break ( h ) , given   othe first hypothesis becomes more likely .   Currently , transformer - based architectures   ( Vaswani et al . , 2017 ) are state of the art in a wide   variety of natural language processing ( NLP ) tasks   ( Devlin et al . , 2019 ; He et al . , 2021 ; Li et al . , 2021 ) ,   including αNLI . However , with an ever - changing   landscape of transformer models and pre - training   techniques ( with over 10000different fine - tuned   models available on the HuggingFace hub ( Wolf   et al . , 2020 ) ) , finding the best model for a given   task has become a time - consuming process since ,   in order to compare multiple models , they each   need to be separately fine - tuned on the task .   This model selection process might lead to a   prohibitive runtime , which has led to research on   performance prediction , namely to predict the ex-   pected performance out of parameters of the model   configuration , without actually training the model .   This procedure has been evaluated for a set of NLP   tasks , including span prediction ( Papay et al . , 2020 )   and language modelling ( Chen , 2009 ) . However,6031we are not aware of any previous work that per-   formed performance prediction for αNLI .   In this paper , we introduce a fast performance   prediction method for the αNLI task that allows   a more guided way of choosing which models to   fine - tune . We use various pre - trained transformer   models to embed the observations and hypothe-   ses with the approach introduced in Reimers and   Gurevych ( 2019 ) , then compare which hypothesis   is closer to the observations with cosine similar-   ity . We find that the performance of the similarity-   based approach is correlated to results obtained   via fine - tuning . Therefore , the similarity - based   approach can serve as a performance prediction   method .   2 Related work   There are three research topics that need to be men-   tioned . Approaches to abductive reasoning , pre-   trained language models , and performance predic-   tion . In this section , we explore them in detail .   Abductive natural language inference . NLI has   been proposed as the task of recognizing textual en-   tailment by MacCartney and Manning ( 2008 ) and   now constitutes a major challenge in NLP which   has found application for other downstream tasks ,   including question answering or zero - shot classifi-   cation ( Yin et al . , 2019 ; Mishra et al . , 2020 ) . Based   on the initial goal of establishing inference relations   between two short texts , a myriad of variants have   been proposed ( Yin et al . , 2021 ; Williams et al . ,   2018 ; Bowman et al . , 2015 ) . One such variant is   abductive NLI ( αNLI , Bhagavatula et al . , 2020 ) .   Transformer - based architectures dominate the   αNLI leaderboard , including RoBERTa - based   models ( Liu et al . , 2020 ; Mitra et al . , 2020 ) which   explore how additional knowledge can improve per-   formance on reasoning tasks , and Zhu et al . ( 2020 )   who approach αNLI as a ranking task . The task   authors improved upon their result in Lourie et al .   ( 2021 ) by using a T5 model ( Raffel et al . , 2020 ) and   experimenting with multi - task training and fine-   tuning over multiple reasoning tasks . Both the   multi - task criteria and the Text - to - Text framework   of T5 help the model generalize and understand the   context better .   The second - best model on the leaderboard is a   DeBERTa model ( He et al . , 2021 ) . The model re-   places the masked language modelling task with areplaced - token detection task . It also uses a disen-   tangled attention mechanism to encode the position   and content information .   The current state of the art shows an accuracy   of 91.18 % using a new unified - modal pre - training   method to leverage multi - modal data for single-   modal tasks ( Li et al . , 2021 ) . This result approaches   the human baseline of 92.9 % .   Pre - trained language models . TheαNLI task   requires the model to successfully “ understand ” the   context of both the observations and use that un-   derstanding to identify the more likely hypothe-   sis entailing it . Most semantic representations in   practical applications rely on distributional seman-   tics . Such methods include the word - level embed-   ding methods Word2Vec ( Mikolov et al . , 2013 )   and GloVe ( Pennington et al . , 2014 ) and language   model - based word representation like ELMo ( Pe-   ters et al . , 2018 ) , ULMFit ( Howard and Ruder ,   2018 ) , and GPT ( Radford et al . , 2018 ) .   The current state of the art are pre - trained trans-   former architectures ( Vaswani et al . , 2017 ) like   BERT ( Devlin et al . , 2019 ) , which use a masked   language modelling and next sentence prediction   objective . This not only helps the model under-   stand the context within a sentence but also in-   between consecutive sentences . There is however a   trade - off in terms of the time it takes to train trans-   former models . For example , a from - scratch train-   ing of BERT takes 6.4 days on an 8 GPU Nvidia   V100 server . Devlin et al . ( 2019 ) recommend fine-   tuning the language model between 2 - 4 epochs for   a given task . However , in practice , multiple trials   are required to find the optimal hyperparameters .   These long training times and multiple fine - tuning   runs make model selection a time - intensive process   ( Liu and Wang , 2021 ) .   Performance prediction . The task of perfor-   mance prediction is to estimate the performance of   a specific system without explicitly running it . It   helps in setting hyperparameters , finding feature   sets , or identifying candidate language models for   a downstream NLP task . Chen ( 2009 ) develop ,   for instance , a generalized method for predicting   the performance of exponential language models .   They analyze the backoff features in an exponen-   tialn - gram model . Papay et al . ( 2020 ) leverage6032meta - learning to identify candidate model perfor-   mance on the task of span identification . They train   a linear regressor as a meta - model to predict span   ID performance based on model features and task   metrics for an unseen task . Ye et al . ( 2021 ) pro-   pose performance prediction methods particularly   suited for fine - grained evaluation metrics . They   also develop methods for estimating the reliabil-   ity of these performance predictions . Contrary to   the previously mentioned papers , Xia et al . ( 2020 )   build regression models to predict the performance   across a variety of NLP tasks , however , they do not   consider NLI as one of them .   In contrast to our work , all these previous meth-   ods build on top of the idea to train a surrogate   model for performance prediction and depend on   the information about past runs of these models .   Our approach focuses solely on the embeddings   provided by the language model and leverages   those as a predictor of performance . This particular   setup is also motivated by the αNLI task itself , in   which a sentence needs to be chosen for a given set   of other sentences .   3 Methods   Our paper investigates how well a fine - tuned trans-   former model ’s performance on the αNLI task   ( Bhagavatula et al . , 2020 ) is approximated by the   cosine similarity of embeddings of the input sen-   tences which we obtain from the pre - trained models   before fine - tuning them .   Intuitively , if a model embeds the correct hypoth-   esis close to the observations in some latent space   ( not necessarily a semantic similarity space ) , then a   classification model built on top of that latent space   should have an easier time discerning which is the   correct hypothesis , because apparently that latent   space captured some features that were salient for   theαNLI task .   3.1 Sentence Embedding   For both the similarity baseline and the fine - tuned   classification model , the starting point is the pre-   trained transformer model itself . We add a mean   pooling layer to convert the token - by - token out-   put of the model into a single sentence embed-   ding ( Reimers and Gurevych , 2019 ) . Given some   tokenized input x= [ x , x , . . . , x]and a pre-   trained transformer model Ewhich encodes each   token E(x ) , we calculate the sentence embedding   emb(x ) = /summationtextE(x).Al alternative to mean pooling would have been   to use the embedding of the CLS token . We opted   against that for three reasons . Firstly , Reimers and   Gurevych ( 2019 ) show that mean pooling slightly   outperforms using the CLS token in their seman-   tic similarity models . Secondly , for some mod-   els , the CLS token does not have any particular   significance before fine - tuning on the downstream   task due to the training objective they use ( such as   RoBERTa ( Liu et al . , 2020 ) , which uses masked   language modelling ) . Thirdly , pooling is a general   approach that can be adopted for any model , even   if it does not output a CLS token . Since our goal   was to avoid any model - specific enhancements , a   universal blanket approach like this was preferable .   3.2 Similarity - based αNLI   To perform αNLI on a validation instance , we ob-   tain three sentence embeddings – one for the com-   bined observations o+oand one for each of the   hypotheses h , h. To predict the more plausible   hypothesis , we calculate which of them is closer to   the observations with cosine similarity :   ˆh= arg maxcos(emb ( o+o),emb ( h ) )   3.3 Classification - based αNLI   For the classification model , we add a classification   head on top of the pre - trained model , which con-   sists of a mean pooling layer to get sentence embed-   dings , then a fully - connected layer and a softmax   output layer . For each instance of ( o , o , h , h ) ,   the model takes two different inputs which consist   of both observations with each of the hypotheses ,   namely emb ( o+o+h)andemb ( o+o+h ) .   Both of these input representations are then used   in a fully connected layer fwith a softmax output   layer to get the probability score for each input .   The hypothesis that is assigned the largest proba-   bility constitutes the prediction :   ˆh= arg maxsoftmax ( f(emb ( o+o+h ) ) )   In our experiments , we fine - tune the classifica-   tion head on the αNLI training set without updating   weights in the underlying language model . This is   mostly due to time and resource constraints , how-   ever , we believe that while fine - tuning would im-   prove classification performance across the board ,   it would not affect the ranking as such . Since we   are comparing models amongst themselves , the   ranking is more important.6033   4 Experiments   We compare the similarity - prediction - based αNLI   approach and the classification - based αNLI ap-   proach to evaluate if the first can act as an ap-   proximation for the performance expected by the   second . We use the pre - trained transformer mod-   els which are available on the HuggingFace ( Wolf   et al . , 2020 ) hub . The full list of models we use is   listed in Table 1 . The code for the experiments is   available online ..   4.1 Dataset   All of our experiments were run on the train and   validation split in the ART dataset provided for   theαNLI challenge ( Bhagavatula et al . , 2020 ) . It   consists of 169,654 training and 1,532 validation   samples , each consisting of two observations and   two hypotheses obtained from a narrative short   story corpus and augmented with wrong hypothe-   ses written by crowd - sourced workers .   The training data contains repetitions of the same   ( o , o)pairs with different sets of hypotheses ,   ranging from one plausible and one implausible   hypothesis to two plausible hypotheses where one   of them is more plausible . The validation set was   constructed using adversarial filtering , which se-   lects one plausible and one implausible hypothesisfor each set of observations that are hard to dis-   tinguish . This increases the probability that the   instances are free of annotation artifacts , which the   authors defined as “ unintentional patterns in the   data that leak information about the target label ”   ( Bhagavatula et al . , 2020 ) .   4.2 Experimental Setup   All of our classification and similarity experiments   were run on an Nvidia RTX 2080 GPU . For training   the classifier we used the maximum batch size that   fit on the GPU ( which is different for different sized   models , ranging between 12 and 128 ) . For similar-   ity experiments , we only infer the embeddings from   pre - trained models . For hyperparameter selection ,   to keep the comparison fair , we tuned the batch size   and learning rate and considered the same set of   possible combinations across all the models . The   specific values used for each model are available   in Table 2 in the appendix . We train for 3 epochs   with learning rates ranging [ 10 ; 9·10]and a   weight decay of 0.01 . For each pre - trained model ,   we pick the one that achieved the highest accuracy   on the validation set .   4.3 Evaluation and Results   Table 1 shows the results as accuracy scores , ob-   tained with each pre - trained model when using   cosine similarity and when using a classifier built   on top of it . We also show the training runtimes.6034   The primary observation is that the accuracy   scores of classification and similarity are corre-   lated . This can be seen in Figure 1 . The Pearson   correlation coefficient is r=.65(p= 0.005 ) .   The Spearman ’s correlation coefficient is ρ=.67   ( p= 0.003 ) , indicating that the ranking obtained   with the similarity - based prediction is a reliable   indicator that is helpful for model selection . Ad-   ditionally , model fine - tuning takes on average 620   times longer than the similarity - based estimate .   Tuning the hyperparameters involved training each   model multiple times .   5 Conclusions & Future Work   In this paper , we have shown that similarity mea-   sures based on the distributional semantic represen-   tation in pre - trained transformer models serve as   an effective proxy for fine - tuned transformer - based   classification in αNLI . Since fine - tuning a trans-   former model takes notably more time than per-   forming similarity comparisons , our approach sup-   ports efficient model selection procedures . Future   work should investigate the suitability of similarity-   based performance prediction for other similar   tasks , like next sentence prediction , question an-   swering , summarization .   Acknowledgements   We thank the anonymous reviewers and the action   editor at ACL Rolling Review for their helpful com-   ments . This work has been conducted in the context   of projects funded by the German Research Coun-   cil ( DFG ) , project “ Computational Event Analysisbased on Appraisal Theories for Emotion Analysis ”   ( CEAT , project number KL 2869/1 - 2 ) and project   “ Automatic Fact Checking for Biomedical Infor-   mation in Social Media and Scientific Literature ”   ( FIBISS , KL 2869/5 - 1 ) .   References60356036   A Model Hyperparameters   All models were trained for 3 epochs with a weight   decay of 0.01 . The batch size and learning rate   used for each model can be seen in Table 26037