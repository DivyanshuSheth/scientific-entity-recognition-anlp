  Ting Hua , Yen - Chang Hsu , Felicity Wang , Qian Lou , Yilin Shen , Hongxia Jin   Samsung Research America   { ting.hua,yenchang.hsu,f.wang1,qian.lou}@samsung.com   { yilin.shen,hongxia.jin}@samsung.com   Abstract   Singular value decomposition ( SVD ) is one of   the most popular compression methods that ap-   proximate a target matrix with smaller matrices .   However , standard SVD treats the parameters   within the matrix with equal importance , which   is a simple but unrealistic assumption . The pa-   rameters of a trained neural network model may   affect the task performance unevenly , which   suggests non - equal importance among the pa-   rameters . Compared to SVD , the decomposi-   tion method aware of parameter importance is   the more practical choice in real cases . Unlike   standard SVD , weighted value decomposition   is a non - convex optimization problem that lacks   a closed - form solution . We systematically in-   vestigated multiple optimization strategies to   tackle the problem and examined our method   by compressing Transformer - based language   models . Further , we designed a metric to pre-   dict when the SVD may introduce a significant   performance drop , for which our method can   be a rescue strategy . The extensive evaluations   demonstrate that our method can perform bet-   ter than current SOTA methods in compressing   Transformer - based language models .   1 Introduction   Transformer - based language models such as BERT   ( Devlin et al . , 2018 ) have obtained significant suc-   cess in a variety of Natural Language Processing   tasks , such as language modeling ( Radford et al . ,   2018 ) , text classification ( Wang et al . , 2018 ) , ques-   tion answering ( Rajpurkar et al . , 2016 ) , and sum-   marization ( Liu , 2019 ) . Despite their success , these   models usually contain millions or even billions of   parameters , pre - trained by the large corpus . How-   ever , the downstream tasks may only focus on a   specific scenario , such that only a small amount of   parameters in the big Transformer model will con-   tribute to the performance of the target task . Also ,   the massive size of Transformer models prohibitstheir deployments to resource - constrained devices .   Therefore , compression of the Transformer - based   language model attracts extensive interests .   Low - rank factorization ( Golub and Reinsch ,   1971 ; Noach and Goldberg , 2020 ) aims to approx-   imate each parameter matrix in the trained model   by two smaller matrices . This line of compression   strategy will naturally inherit the knowledge of the   big trained model without expensive generic re-   training , and is the orthogonal direction to other   compression approaches such as Knowledge dis-   tillation ( Sun et al . , 2019 ; Sanh et al . , 2019 ; Jiao   et al . , 2019 ) or Quantization ( Shen et al . , 2020 ;   Zhao et al . , 2021 ) .   However , applying standard SVD to approxi-   mate the learned weights often results in a signifi-   ca nt task performance drop . Previous work shows   that this phenomenon may be caused by a strong   assumption held by the standard SVD , that the pa-   rameters in the matrix are equally crucial to the   performance ( Hsu et al . , 2021 ) . Also , it has been   observed that different parameters in Transformer   models have different impacts on the overall task   performance ( Shen et al . , 2020 ) .   Following FWSVD ( Hsu et al . , 2021 ) , we utilize   Fisher information ( Pascanu and Bengio , 2014 )   to weigh the importance of parameters , so that   the objective of matrix factorization will jointly   consider matrix reconstruction error and the tar-   get task performance . In the standard SVD , all   the local minima are saddle points , ensuring a   closed - form global optimal solution ( Srebro and   Jaakkola , 2003 ) . This property no longer holds true   to our new objective weighted by Fisher informa-   tion . Without the closed - form solution , we revert   to the numerical optimization methods to minimize   the weighted objective . As our method can provide   a more accurate solution than FWSVD ( Hsu et al . ,   2021 ) , we name our proposed method as TFWSVD   ( True Fisher Weighted SVD ) . Our results reveal the   hybrid optimizer we called Adam_SGD can best1404fit our problem , with its switching point estimated   by the row - based analytic solution . We also in-   vestigated the scenarios where SVD fails , under   the guidance of the metric we introduced to mea-   sure the variance of parameter importance , with   the example of analyzing the matrices within the   Transformer blocks .   In summary , this work makes the following con-   tributions : ( 1 ) we provide several optimization   methods to search for the best numerical solution   for low - rank estimation weighted by the Fisher in-   formation ; ( 2 ) we perform extensive evaluations   on various language tasks , showing our TFWSVD   achieves better performance than the SOTA com-   pression methods , and can further compress already   compact models ; ( 3 ) through the analysis of factor-   izing sub - structures inside the Transformer blocks ,   we provide the guide about when SVD may fail but   TFWSVD can retain the performance .   2 Background   2.1 Model Compression with SVD   Singular value decomposition ( SVD ) decomposes   a matrix , e.g. ,W∈Rinto three matrices :   W = USV≈USV , ( 1 )   where U∈R , V∈R , and lis the rank of   matrix W.Sis a diagonal matrix of non - zero sin-   gular values diag(σ , , ... , σ ) , where σ≥σ≥   · · · σ>0.U , S , andVrepresent the truncated   matrices with rank rand approximate the original   matrix with a less total number of parameters .   The computation of a linear layer in neural net-   works can be rewritten as below with input data   X∈R , weight matrix W∈R , and bias   b∈R :   Z = XW + b≈(XUS)V+b.(2 )   The typical implementation of factorization is to   replace the large Wwith two smaller linear lay-   ers : 1 ) The weight matrix of the first layer is US ,   which has Nrparameters without bias . 2 ) While   the weight matrix of the second layer is V , with   Mrparameters plus bias . The truncation happens   when ris less than l. For example , if the total   number of parameters for approximating Wis   Nr+Mr , then the reduced number of parame-   ters will be NM−(Nr+Mr).2.2 Fisher information   A classical way to measure the importance of   parameters is through the observed information ,   i.e. Fisher information . It measures the amount of   information that an observable dataset Dcarries   about a model parameter w. The accurate values of   Fisher information are generally intractable since   the computation will require marginalizing over the   dataD. In practice , the empirical Fisher informa-   tion is estimated as follows :   I = E / bracketleftigg / parenleftbigg∂   ∂wlogp(D|w)/parenrightbigg / bracketrightigg   ≈1   |D|/summationdisplay / parenleftbigg∂   ∂wL(d;w)/parenrightbigg   = ˆI.(3 )   Given a target task objective L(e.g . , cross - entropy   for a classification task ) , the estimated information   ˆIaccumulates the squared gradients over the train-   ing data d∈ D. The parameters that cause large   absolute gradient of the task objective will have a   large value in ˆI , and are considered important to   the target task .   2.3 Related works   The report of applying SVD to the Transformer lay-   ers is scarce . Several previous works applied SVD   to compress the word embedding layer ( Chen et al . ,   2018a ; Acharya et al . , 2019 ) . Although ( Noach   and Goldberg , 2020 ) combined knowledge distil-   lation to fine - tune the resulting compressed model ,   they did n’t address the issue of poor performance   when fine - tuning is not applied . Experiments show   that our proposed method can retrain most of the   performance , providing a much better initialization   for the fine - tuning .   The use of Fisher information has appeared in   many problem settings that also need to estimate   the importance of model parameters , for example ,   to avoid catastrophic forgetting in continual learn-   ing ( Kirkpatrick et al . , 2017 ; Hua et al . , 2021 ) or   model pruning ( Liu et al . , 2021 ; Molchanov et al . ,   2019b ) . However , none of these work has explored   its potential in assisting low - rank approximation   for model compression .   Most previous work seeking the numerical so-   lution for low - rank approximation is designed for   unweighted cases , with applications such as pre-   dicting the missing values recommendation system   ( Yu et al . , 2014 ; Zhou et al . , 2008 ) . Also , a few1405attempts have been made to solve the weighted low-   rank approximation problem through EM - based al-   gorithm ( Srebro and Jaakkola , 2003 ) , or alternating   least squares ( He et al . , 2016 ) .   The closest previous work to this paper is   FWSVD ( Hsu et al . , 2021 ) , which points out that   the “ even importance ” assumption held by SVD   may cause a performance drop . FWSVD also uti-   lizes Fisher information to weigh the importance   of parameters . However , during the decomposition   process , FWSVD assumes that parameters within   each weight matrix row share the same importance   value , which is still a strong assumption . Exper-   imental results show that our TFWSVD can find   more accurate solutions than FWSVD , as each pa-   rameter is associated with its own importance in   TFWSVD .   3 Method   3.1 Low - rank factorization objective weighted   by Fisher information   The objective of the generic low - rank approx-   imation is to minimize the Frobenius norm   ||W−AB|| , which is the sum squared differ-   ences of a reconstructed matrix AB to the target   matrix W. As mentioned above , Singular value   decomposition ( SVD ) can solve this problem effi-   ciently by having A = USandB = V. As the   importance of each element winWcan be cal-   culated through its Fisher information , we would   like to find the reconstructed matrix AB that mini-   mizes the weighted Frobenius distance J(A , B)as   follows ( ⊗denotes element - wise multiplication ):   J(A , B ) = ˆI⊗(W−AB )   = /summationdisplayˆI(w−ab).(4 )   To prevent over fitting , Lregularization terms con-   trolled by parameter λcan be added to the objec-   tive , so that Equation ( 4 ) can be rewritten as :   J(A , B ) = /summationdisplayˆI(w−ab )   + λ(/summationdisplay||a||+/summationdisplay||b||).(5 )   3.2 Optimization methods   SVD has an analytic solution , since all of its local   minima are global . However , this can not hold truewhen weights are introduced . Without a closed-   form solution , we discuss several numerical opti-   mization methods to minimize J(A , B ) .   3.2.1 Alternating Least Squares   Although the optimization problems in ( 4 ) and ( 5 )   are non - convex , they can be converted to quadratic   problems with globally optimal solutions , if Aor   Bis fixed . Therefore , Alternating Least Squares   ( ALS ) is suitable to solve such problems ( Hastie   et al . , 2015 ) . ALS will alternately optimize Aor   Bby keeping the other one fixed , and decrease   J(A , B)until convergence . When the other ma-   trix is fixed , minimizing J(A , B)with respect to   AorBis equivalent to minimize the following   objectives :   J(a ) = ||ˆI(W−Ba)||+λ||a||   J(b ) = ||ˆI(W−Ab)||+λ||b|| ,   ( 6 )   which can lead to the closed - form solutions :   a= ( BˆIB+λΣ)BˆIW   b=(AˆIA+λΣ)AˆIW ,   ( 7 )   where Σis the identity matrix , while ˆIand   ˆIare the Fisher information vector of i - th row   andj - th column in original matrix W , respectively .   3.2.2 Stochastic Gradient Descent   Stochastic Gradient Descent ( SGD ) is also shown   to be effective for matrix factorization problems   ( Koren et al . , 2009 ) . Specifically in our problem ,   each update of SGD can be represented as :   a←a+ 2η(eb−λa )   b←b+ 2η(ea−λb),(8 )   where ηis the learning rate , and e=   ˆI(w−ab ) . More generally , the iterations   of SGD can be described as :   h←h−η∇J(h ) , ( 9 )   where hdenotes the k - th iterate that can be sub-   stituted by aorb .   3.2.3 Adaptive Moment Estimation   SGD will scale gradient uniformly in all directions ,   making the training process inefficient and sensi-   tive to the learning rate . Several adaptive methods   have been proposed to overcome this shortcom-   ing , among which Adaptive Moment Estimation1406(Adam ) is one of the most widely used approaches   ( Kingma and Ba , 2015 ) . Following the form of   SGD updates shown in ( 9 ) , the Adam update itera-   tions can be written as :   h←h−η·/radicalig   1−β   1−β·m   /radicalbig   v+ε ,   ( 10 )   where handηare the same as Equation ( 9 ) ,   mandvare calculated as follows :   m = βm+ ( 1−β)∇J(h )   v = βv+ ( 1−β)∇J(h ) .   ( 11 )   Although Adam requires minimal tuning and   enjoys fast initial progress , it is not without faults .   Recent work has shown that the solutions found by   Adam can be much worse at generalization than   those found by SGD ( Akiba et al . , 2017 ; Ida and   Fujiwara , 2020 ) .   3.2.4 Adam Switching to SGD   Previous studies show that switching from Adam to   SGD may contribute to the performance , however ,   the switching point is crucial for the overall per-   formance and usually is task - dependent ( Ida and   Fujiwara , 2020 ) . Here we propose a simple method   to calculate the switching point for our Fisher in-   formation weighted matrix factorization problem .   Although weighted SVD does not have a closed-   form solution when each element has its weight ,   the optimization problem ( 5 ) has a close form in the   case that elements within the same row share the   same weight ( Hsu et al . , 2021 ) . Therefore , we can   calculate an approximate solution for the optimiza-   tion problem ( 5 ) based on row - wise Fisher infor-   mation , which can be solved as the “ threshold ” for   our switching point from Adam to SGD ( Hsu et al . ,   2021 ) . If we define the importance for the row ito   be the summation of the row , i.e. ,ˆI=/summationtextˆI   and diagonal matrix ˆI = diag(/radicalig   ˆI , ... , /radicalig   ˆI ) ,   then the optimization problem of Equation ( 4 ) can   be written as :   J(A , B)≈ˆJ(A , B ) = ||ˆIW−ˆIAB||.(12 )   Optimization problem ( 12 ) can be solved by the   standard SVD on ˆIW . If we denote svd(ˆIW ) =   ( U , S , V ) , then the solution of Equation ( 12 )   will be A=ˆIUS , and B = V. The value   ofˆJ(A , B)is served as our switching point fromAdam to SGD , that the training process will be   optimized by Adam when the current loss is larger   than ˆJ(A , B ) , and then taken over by SGD when   its loss is smaller than ˆJ(A , B ) .   Besides the hard threshold calculated in ( 12 ) ,   we also set a soft threshold that restricts our un-   weighted reconstruction error with the same order   of magnitude as that of SVD . Experiments in Sec-   tion 4.5 show that our switching point can well   balance the speed and convergence of the optimiza-   tion process .   3.3 Metric measuring when SVD may fail   Besides an accurate solution to the J(A , B ) ,   whether TFWSVD can obtain a performance gain   is also decided by the properties of the target ma-   trixWitself . TFWSVD is to capture the different   importance of parameters . However , if the param-   eters in Wequally contributed to the model per-   formance , then the standard SVD should be good   enough . Driven by these factors , we are interested   in this question : Is there a method that can “ foresee ”   when SVD will fail , and TFWSVD can help retain   performance ?   Given target matrix W , here we propose a sim-   ple but effective metric called Fisher information   variance φ(W ) , which is calculated as the variance   of the Lnormalization of its corresponding Fisher   information ˆI :   φ(W ) = V ar(ˆI   max(||ˆI|| , ε ) ) . ( 13 )   As shown in Section 4.6 , this metric can qualita-   tively measure whether the targeted matrix is too   challenging to SVD and therefore needs help from   TFWSVD .   4 Experiment   4.1 Language tasks and datasets   We evaluate our proposed methods and baselines on   the General Language Understanding Evaluation   ( GLUE ) benchmark ( Wang et al . , 2019 ) and a token   classification task . More details about datasets and   tasks can be found in Appendix A.   4.2 Implementation details and baselines   For generic compact methods ( MiniLM , Distil-   BERT , and TinyBERT ) , we use the models pro-   vided by the original authors as the initialization ,   then directly fine - tune them on the training data   of the target task . The fine - tuning is optimized by1407   Adam with a learning rate of 2×10and batch   size of 32 on one GPU .   Besides FWSVD ( Hsu et al . , 2021 ) and our pro-   posed TFWSVD , we also provide a baseline using   first - order Taylor expansion for value decomposi-   tion ( TVD ) . The details of TVD can be found in   Appendix B.   For low - rank factorization methods ( TFWSVD ,   FWSVD , TVD , and SVD ) , we use the pre - trained   12 - layer BERT model ( Devlin et al . , 2018 ) as the   start . And then , the large BERT model is fine-   tuned on the task - specific data . Next , we apply the   low - rank factorization , followed by another fine-   tuning . We reported the results with and without   fine - tuning to reveal the native results of low - rank   factorization .   To make a fair comparison , only the linear layers   in the transformer blocks are compressed in this   work . The non - Transformer modules , such as the   token embedding , are not compressed . Previous   works ( Chen et al . , 2018a ) have shown significant   success in applying low - rank factorization to com-   press the embedding layer , which occupies 23.4 M   ( 21.3 % ) parameters in the standard BERT model .   Thus , the results we reported in this paper can be   further improved by applying our method to non-   transformer modules .   All of our implements are created on the base   of HuggingFace Transformer library ( Wolf et al . ,   2020 ) . The settings not mentioned use the default   configuration of the HuggingFace Transformer li-   brary . We directly reported the results on the dev   set for all datasets , as hyper - parameter searching is   not involved in our experimental evaluations.4.3 Performance comparisons with SOTA   Table 1 reports the results of GLUE tasks and one   NER task CoNLL . Our TFWSVD with 66.5 M pa-   rameters obtains G - Avg score of 83.1and A - Avg   score of 84.4 , which are better than the scores   of SOTA models ( MiniLMv2 , TinyBERT6 , dis-   tilBERT ) requiring generic re - training . TFWSVD   consistently yields good results on all the tasks ,   while the other generic re - training methods dis-   play obvious performance variance among differ-   ent tasks . For example , TinyBERT6 is good at   the STSB task but poor at CoLA ; oppositely , dis-   tilBERT has strong performance on CoLA but is   weak at STSB .   In the comparisons among low - rank factoriza-   tion methods ( TFWSVD , FWSVD , TVD , and   SVD ) , our TFWSVD beats other methods with ap-   parent better performance in both scenarios with or   without fine - tuning . One interesting phenomenon   is that TVD can yield better results than SVD with-   out fine - tuning . However , after fine - tuning , its ad-   vantages disappear , and SVD can achieve better   average scores ( G - Avg and A - Avg ) . This is not sur-   prising . Similar to our proposed TFWSVD , TVD is   also a loss - aware method that definitely will be bet-   ter than the loss - unaware SVD . But this gap can be   narrowed or even eliminated with fine - tuning since   SVD can also “ see ” the loss in this case . Therefore ,   within loss - aware methods , the weighting metric   itself plays an important role in keeping the perfor-   mance advantage . Also , TFWSVD obtains better   performance than FWSVD , which indicates it is too   “ aggressive ” for FWSVD to assume that parameters   in the same row share the same importance.1408   4.4 Under high compression rates   In this part , we compared low - rank methods   under high compression rates . Because TVD   did n’t show an apparent advantage over SVD ,   here we mainly focus on comparing our proposed   TFWSVD , FWSVD , and standard SVD .   As can be seen from Table 2 , TFWSVD always   enjoys obvious advantages over the other two meth-   ods . Also , the performance gap between TFWSVD   and FWSVD is enlarged as the compression rate   goes higher . In fact , under the extremely compact   setting of 37.2 M , FWSVD shows worse perfor-   mance compared to SVD . This phenomenon further   proves that the row - based importance assumption   held by FWSVD may hurt the performance . While   the privilege of TFWSVD always exists and be-   comes more prominent in the high compression   rate of 49.9 M and 37.2M. Especially in the sce-   nario without fine - tuning , which can best reveal   the pure performance of low - rank factorization ,   TFWSVD has performance scores almost double   that of FWSVD and SVD .   4.5 Optimization methods   In this part , we compare optimization procedures   mentioned in Section 3.2 to identify the best opti-   mizer for our approximation problem.4.5.1 ALS and SGD   In order to update the latent vectors , ALS needs   O(r)time to form the r×rmatrix , with an addi-   tional O(r)time to solve the least - squares prob-   lem . Therefore , to reconstruct the target matrix   W∈Rwith rank r , the time complexity of   one ALS iteration is O((M+N)r+MNr ) . It   has been pointed out that ALS can be speeded up   by parallelly updating each row of AorBindepen-   dently ( Zhou et al . , 2008 ) . While for SGD , the time   complexity per iteration is only O(MNK ) . Com-   pared to ALS , SGD seems to be faster in terms   of the time complexity for one iteration . How-   ever , typically it requires more iterations than ALS   to achieve relatively good performance ( Yu et al . ,   2014 ) . As shown in Table 3 , in order to obtain the   performance close to that of Adam / Adam_SGD ,   ALS and SGD need 50∼60times more steps ,   which makes them impractical to be used in the   real - world Transformer compression . Therefore , in   the rest of this part , we will focus on comparing   the performance of Adam and Adam_SGD .   4.5.2 Adam and Adam_SGD   The goal of hybrid optimizer Adam_SGD is to   combine the benefits of Adam ( fast initial progress   and minimal efforts in hyperparameter tuning ) and1409   SGD ( good convergence and generalization ) .   As seen from Figure 2 , Adam and Adam_SGD   share the same trajectory in the initial steps . Af-   ter the switching point ( around 22400 in Figure   2 ) , Adam_SGD converges to a low error solu-   tion ( 1.28E-06 as shown in Table 3 ) , which is   much smaller than the row - based analytic solution   ( 9.87E-06 ) . In contrast , Adam fluctuates in perfor-   mance and ends with a much larger error 1.06E-   05 . These phenomena prove the effectiveness of   Adam_SGD in solving the weighted Frobenius dis-   tance optimization problem in ( 4 ) and ( 5 ) . And   the reconstruction errors of final solutions obtained   by Adam_SGD are 5 ∼10 times smaller than the   row - wise approximations .   4.6 Fisher information variance   What is the secret behind TFWSVD ’s good perfor-   mance on Transformer - based model compression?In this part , we utilize the metric Fisher informa-   tion variance φ(W)introduced in Section 3.3 to   reveal the secret by analyzing the sub - structures   inside the Transformer blocks .   According to the implementation of Hugging-   Face Transformer library ( Wolf et al . , 2020 ) , there   are five kinds of linear layers within the Trans-   former block , which can be set into two groups by   their dimensions : Query , Key , Value , and Multi-   head Attention layers are matrices with the dimen-   sion of 768×768 ; and two feed - forward layers   called Intermediate and Output , are 768×3072 in   dimension . Figure 1 plot the performance changes   along with varying the rank ratio for matrices with   the dimension of 768×768 , when only decom-   posing one type of sub - structure . More results are   plotted in Figure 3 in Appendix .   Compared to the overall performance compari-   son in Section 4.3 , the purpose of this experiment is   to evaluate the performance of SVD and TFWSVD   on the finer - level sub - structures within Transformer   blocks . Taking Figure 1a for example , the yellow   line denoting “ Value ” means : only the “ Value ” sub-   structures are decomposed by SVD , while other   types of sub - structures are kept the same as the   original model . We calculate the Fisher informa-   tion variance φ(W)via Equation ( 13 ) , and mark   the values besides the corresponding sub - structures .   Several observations can be made from Figure 1 .   Different matrix has different sensitiveness to   SVD . As shown in Figure 1a , Attention_out layer   is relatively easy to compress . Even with standard   SVD , it can still achieve good performance as low1410   as a rank ratio 0.1 . While compressing matrix In-   termediate is rather difficult , its performance will   drop down to 17 % with a rank ratio 0.1 .   Metric Fisher information variance φ(W)can   ‘ foresee ” the performance of SVD . In Figure 1a ,   decomposing sub - structures with larger φ(W)via   SVD will always cause the more serious perfor-   mance drop . Especially , the performance changes   of sub - structure Query and Key are almost identi-   cal , and their φ(W)are extremely close ( 1.16E-   03 for Query and 1.17E-03 for Key ) . This phe-   nomenon implies the metric φ(W)can well reflect   the variance of parameter importance within the   matrix , and therefore can be a good performance   indicator for SVD .   TFWSVD can always help improve the perfor-   mance . Figure 1b shows that applying TFWSVD   will bring significant performance gain to all   the sub - structures . Especially for the challeng-   ing matrix Intermediate ( Figure 3b in Appendix ) ,   TFWSVD achieves an excellent performance of   60 % at a low - rank ratio 0.1 , which is a 200 % im-   provement compared to the corresponding SVD   performance 17 % .   4.7 Compress the already compact models   The matrix factorization direction is thought to   be orthogonal to other compression methods such   as knowledge distillation . But in practice , perfor-   mance drops are often observed when combiningthe different lines of compression technologies . Ta-   ble 4 reports the results of applying TFWSVD ,   FWSVD , and SVD to compress the lightweight   models further . In general , TFWSVD can reduce   30 % more parameters for the compact models , with   even improved performance . In fact , the perfor-   mance gains by applying TFWSVD are observed   on all compact models in Table 4 , while both SVD   and FWSVD will cause performance drops more   or less when combined with those compact mod-   els . These results indicate that SVD and FWSVD   may not be fully integrated with other compres-   sion technologies due to the the strong assumptions   they held . And our TFWSVD can best explore the   potential of combining other lines of compression   methods with matrix factorization .   4.8 Discussion   The incorrect predictions from the trained model   will bring larger gradients than the correctly labeled   examples , which means these incorrect predictions   may be the better choices to compute Fisher in-   formation . It is different from our intuitions , but   not surprising , since all these examples can reflect   the features of trained parameters . In fact , the mis-   labeled examples may better “ describe ” the fea-   tures of the trained model ( for example , these ex-   amples are around the boundary ) . Also , we can use   incorrect - only labels to estimate the Fisher infor-   mation to further reduce computation time . More   details can be found in Appendix C.   5 Conclusion   Unlike SVD , there is no closed - form solution   for the weighted low - rank estimation problem ,   which therefore has to be approximated via nu-   merical optimization methods . We managed to   obtain the practical solutions through our hy-   brid Adam_SGD optimizer with the specially de-   signed switching point . Our TFWSVD consis-   tently works better than other low - rank factoriza-   tion methods ( FWSVD , TVD , and SVD ) . Com-   pared to SOTA methods that requiring expensive   generic re - training , our TFWSVD shows more sta-   ble performance on various tasks . Also , TFWSVD   can efficiently further compress and optimize the   already compact models . We also investigate the   properties of the targeted matrix , where that SVD   may fail , and TFWSVD can be the rescuer . We   believe our TFWSVD could be the best alternative   to SVD for language model compression.14116 Limitations   The most significant limitation of TFWSVD is that   it may cost more time than SVD and FWSVD .   Compared to FWSVD , TFWSVD will need more   time in numerical optimization , which is decided   by the number of parameters in a model and is   fixed for all downstream tasks . For GLUE tasks   trained with the BERT model , the extra time cost   of TFWSVD is around 1.5 V100 GPU hours . Also ,   both TFWSVD and FWSVD need time for Fisher   information calculation . For example , this calcu-   lation takes about 8 minutes on SST-2 task . And   it can be further reduced to around 5 seconds if   we only use incorrect predictions ( see details in   Appendix C ) .   In summary , compared to SVD and FWSVD ,   TFWSVD will cost at least 1.5 more V100 GPU   hours when compressing the BERT model for a   GLUE task . This cost is worthy , considering the   stable performance gain that TFWSVD can bring .   On the other hand , TFWSVD is still a much faster   choice than the generic re - trained compact models   such as distilBERT and MiniLM . For example , dis-   tilBERT needs 720 V100 GPU hours for re - training   a BERT model . Similar to SVD and FWSVD , our   TFWSVD can avoid such expensive re - training and   can be applied to the directly downloaded BERT   model .   7 Ethical Considerations   Our work is to better compress the language model   with an improved low - rank estimation method . For   our experiments , we used open datasets without   sensitive information , which have been widely men-   tioned in previous work . No license is required for   the GLUE dataset , and we have purchased the li-   cense for the CoNLL dataset . In the application   of our model , we do not think there is an obvious   issue that may lead to a risk to ethics .   References141214131414A Details of tasks and datasets   We include two single sentence tasks : CoLA   ( Warstadt et al . , 2018 ) measured in Matthew ’s cor-   relation , SST2 ( Socher et al . , 2013 ) measured in   classification accuracy ; three sentence similarity   tasks : MRPC ( Dolan et al . , 2005 ) measured in   F-1 score , STS - B ( Cer et al . , 2017 ) measured in   Pearson - Spearman correlation , QQP ( Chen et al . ,   2018b ) measured in F-1 score ; and three natural   language inference tasks : MNLI ( Williams et al . ,   2018 ) measured in classification accuracy with the   average of the matched and mismatched subsets ,   QNLI ( Rajpurkar et al . , 2016 ) measured in accu-   racy . The token classification task we used is the   named entity recognition ( NER ) on the CoNLL-   2003 dataset ( Sang and De Meulder , 2003 ) . In   summary , our evaluation includes eight different   natural language tasks .   B TVD   In this section , we provide the details about the   baseline using first - order Taylor expansion for   value decomposition ( TVD ) . Following ( Hou et al . ,   2020 ; V oita et al . , 2019 ; Molchanov et al . , 2019a ) ,   we utilize the first - order Taylor expansion as the   alternative importance score for matrices :   T=|L − L| ( 14a )   = |L − ( L −∂L   ∂w(w−0 ) + R)|(14b )   ≈ |∂L   ∂ww| . ( 14c )   As shown in equation 14a , the intuition behind   TVD is that the importance of a parameter wcan   be calculated by the variation in the training loss   when removing this parameter . If we ignore the   remainder R , then we can simply calculate the   importance via equation 14c , which is the product   of the parameter value and its 1st - order gradient .   C Effect of incorrect predictions   In this section , we evaluate whether the incor-   rect predictions will have negative impacts on   the estimation of Fisher information . In order to   achieve this goal , we report the performance of   classification tasks in Table 5 , when we use in-   correctly / correctly predicted examples to estimate   Fisher information .   Several observations can be made as follows .   First , the final performances are close , no mat-   ter using correct - only examples , incorrect - onlyexamples , or all examples . It demonstrates all   kinds of examples can somehow reflect the impor-   tance of parameters . Meanwhile , the performances   using all examples are always the best , confirming   the better estimation of empirical Fisher informa-   tion with more data . Second , using the incorrect-   only examples will generate bigger values and   better performance than using correct - only pre-   dictions . Although only 1 - 2 % examples are incor-   rectly predicted , choosing these examples to esti-   mate Fisher information will produce close num-   bers to those generated using all examples . This is   because Fisher information is calculated via loss ,   and incorrect predictions will produce larger losses   than the correct predictions . And compared to us-   ing correct - only examples , computations through   incorrect - only examples may even bring better re-   sults for most tasks .   In summary , the wrong labeled examples will   generate larger Fisher information , but it does n’t   mean that the Fisher information learned from the   incorrectly labeled data is “ wrong ” . Instead , the   mislabeled examples are better choices than correct   predictions , which will produce better results with   fewer computations .   D Training Time   This part discusses the training time of different   approaches mentioned in this paper .   TFWSVD versus FWSVD , TVD , and SVD . First ,   we compare the time costs of low - rank estimation   methods SVD , TVD , FWSVD , and our proposed   TFWSVD . In general , SVD is the fastest method   that can be done immediately as it has a close-   form solution . FWSVD is the second fast method ,   which needs time for Fisher information calcula-   tion . TFWSVD and TVD will cost more time in   the numerical optimization process .   1.FWSVD versus SVD : Compared to SVD ,   FWSVD needs extra time for Fisher informa-   tion calculation . The time of this process is   similar to one epoch of regular training . For   example , SST-2 task in this paper takes about   8 minutes to calculate the Fisher information .   This process is generally fast , and it can be   further reduced to around 5 seconds if we only   use incorrect predictions ( e.g. , 1 % of all ex-   amples , mentioned in Appendix C ) .   2.TFWSVD versus FWSVD : Compared to   FWSVD , TFWSVD needs extra time for fac-1415   torizing the weighted matrices through opti-   mizations . The time cost of factorization is de-   cided by the number of parameters in a model ,   and is fixed for all its downstream tasks . For   GLUE tasks trained with the BERT model ,   TFWSVD will cost 1.5 more V100 GPU hours   than FWSVD .   3.TFWSVD versus TVD : TFWSVD and TVD   will cost the same time as these approaches   are almost the same except for the weighting   scheme .   TFWSVD versus generic re - trained models . The   generic re - trained compact models such as distil-   BERT and MiniLMv2 require a large amount of   re - training time . For example , distilBERT needs   720 V100 GPU hours for retraining a pre - trained   BERT model . Compared to these methods , our   TFWSVD is much faster , since TFWSVD can be   applied to the directly downloaded BERT model   without expensive re - training.1416