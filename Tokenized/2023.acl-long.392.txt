  Brihi JoshiZiyi LiuSahana RamnathAaron ChanZhewei Tong   Shaoliang NieQifan WangYejin ChoiXiang RenUniversity of Southern CaliforniaTsinghua UniversityMeta AIAllen Institute for Artificial IntelligenceUniversity of Washington   Abstract   Among the remarkable emergent capabilities   of large language models ( LMs ) is free - text ra-   tionalization ; beyond a certain scale , large LMs   are capable of generating seemingly useful ra-   tionalizations , which in turn , can dramatically   enhance their performances on leaderboards .   This phenomenon raises a question : can ma-   chine generated rationales also be useful for   humans , especially when lay humans try to   answer questions based on those machine ra-   tionales ? We observe that human utility of ex-   isting rationales is far from satisfactory , and   expensive to estimate with human studies . Ex-   isting metrics like task performance of the LM   generating the rationales , or similarity between   generated and gold rationales are not good indi-   cators of their human utility . While we observe   that certain properties of rationales like con-   ciseness and novelty are correlated with their   human utility , estimating them without human   involvement is challenging . We show that , by   estimating a rationale ’s helpfulness in answer-   ing similar unseen instances , we can measure   its human utility to a better extent . We also   translate this finding into an automated score ,   G - U , that we propose , which can help im-   prove LMs ’ ability to generate rationales with   better human utility , while maintaining most   of its task performance . Lastly , we release all   code and collected data with this project .   1 Introduction   In recent years , there has been a surge of interest   in using language models ( LMs ) for human - AI col-   laboration ( Wiegreffe et al . , 2022 ; You and Lowd ,   2022 ) . For example , LMs have played a large   role in reducing human effort for dataset creation   ( Bonifacio et al . , 2022 ; Yuan et al . , 2021 ; Liu et al . ,   2022 ) and helping humans critique text ( SaundersFigure 1 : An illustration of Human Utility of ratio-   nales : Here , we show Chains of Thought ( rationales )   generated by GPT-3 in two scenarios . The first one is   providing knowledge to the human to be able to answer   the question , but the second rationale is not useful , and   is in fact , misleading the human to answer incorrectly .   et al . , 2022 ) . However , the opaque reasoning pro-   cesses of these LMs pose serious concerns about   their role in high - stakes decision - making ( Bender   et al . , 2021 ; Doshi - Velez and Kim , 2017 ) . Recently ,   many works have explored using LMs to generate   fluent , human - like free - text rationalesvia natural   language ( Ehsan et al . , 2018 ; Rajani et al . , 2019a )   that can explain their decisions . Further , rationales   can reference things beyond the task input , and   also support high flexibility in content , style , and   length ( Narang et al . , 2020 ; Wiegreffe et al . , 2022 ,   2021 ; Chan et al . , 2022 ) . However , evaluating if a   rationale of a task - instance contains enough knowl-   edge to help lay humans understand and solve that   instance correctly is still under - explored .   Prior literature for human - AI collaboration has   studied plausibility ( Wiegreffe and Marasovi ´ c ,   2021 ) . However , plausibility only aims to cap-7103   ture human judgement of the rationale supporting   LM ’s predicted label . There has been little work   done on evaluating actionable advantages offered   by rationales to lay humans in understanding a   task , despite the promise of human - AI collabora-   tion ( Schuff et al . , 2022 ) . Studying human utility   of rationales is important to not only situate them   in real - world use cases beyond the involvement   of researchers , but also to bridge the gap between   human and AI understanding , specifically in sce-   narios where AI systems perform better . In this   work , we shift the paradigm of rationale evaluation ,   by investigating human utility of rationales in help-   ing lay humans understand and solve a given task   correctly .   In our study , we observe that human utility of cur-   rent LMs is far from satisfactory ( including large   LMs like GPT-3 ) , with only 20 % of generated ratio-   nales being actually useful ( § 2 ) . Given that human   evaluations are expensive , we should find a reli-   able way to measure human utility . We examine   the correlation of two straightforward measures   like LM task performance and alignment with gold   rationales , with human utility and find no usable   insights . We also ask humans to evaluate ratio-   nales w.r.t eight granular - level properties . While   we observe that six out of these eight properties   are correlated with human utility , reliably estimat-   ing them without human evaluation is still an open   question ( Golovneva et al . , 2022 ) .   In addition to the above observation , we find that   high - utility rationales effectively transfer knowl-   edge to humans for solving new instances . ( § 3 )   We create new instances ( e.g. , questions ) by either   paraphrasing the original instance in a nontrivial   manner ( rephrase ) , editing the original instance so   that its correct label is changed ( counterfactual ) , or   writing an instance that requires a similar reasoning   process as the original instance ( similar reasoning).We observe that useful rationales help humans gen-   eralize better to new instances , whereas non - helpful   rationales even mislead them to answer incorrectly .   To follow up on the above finding , we show that   we can improve an LM ’s ability to generate ratio-   nales with better human utility . ( § 4 ) We translate   this finding into an automated score , G - U , that   reflects the ability of a rationale to help an LM an-   swer generalization instances , that better correlates   with human utility ( when compared to other met-   rics like LMs ’ task accuracy ) . We use G - Uas   a reward ( Lu et al . , 2022 ) while generating ratio-   nales and observe that the updated LM generates   2%more useful rationales and gets rid of 4%mis-   leading rationales than before , via human - subject   evaluations , without hindering the LMs ’ task per-   formance .   This paper presents the first comprehensive study   of lay human utility of free - text rationales . By   introducing lay humans in the rationale evaluation   pipeline , and using their insights into existing LMs ,   we believe our findings can help guide future work   on developing methods for efficient and reliable   human - AI collaboration .   2 Human Utility of LM - generated   Rationales   We begin by defining human utility , intuitively and   formally , and describing the LMs that we use for   the rest of the paper . Based on this definition , we   conduct human studies to investigate whether exist-   ing LMs are capable of generating useful rationales .   Finally , we follow this up by identifying granular-   level syntactic and semantic properties of rationales   can indicate their human utility .   Human Utility of Machine Rationales . We first   define human utility of rationales as the advantage   that rationales offer lay humans to solve tasks , that7104   they are otherwise unable to ( Schuff et al . , 2022 ;   Idahl et al . , 2021 ; Chu et al . , 2020 ) ( Figure 2 ) . In   theory , we can estimate human utility of a rationale   in a forward simulation - like ( Doshi - Velez and Kim ,   2017 ) setup : the difference in human performance   of a task , with and without the assistance of a ratio-   nale . In this work , we reformulate this definition of   utility for a classification task ( multi - choice ques-   tion answering ) . We use the StrategyQA ( Geva   et al . , 2021 ) and OBQA ( Mihaylov et al . , 2018 )   datasets for our paper . The reason for doing so is   to pick tasks where humans are not already better   than LMs ( unlike NLI and CommonsenseQA ( Nan-   gia and Bowman , 2019 ; Talmor et al . , 2021 ) ) , and   study cases where rationales are capable of knowl-   edge transfer that can help humans . More details   about our task and dataset selection reasoning is   highlighted in § A.1 .   Formal setup for calculating human utility . Let   Fbe a self - rationalizing LM ( Wiegreffe et al . ,   2020 ) that can generate rationales with its predic-   tions , and a corresponding input - output pair x , y.   Ftakes in xas an input and generates a predic-   tiony , and a rationale that corresponds to this   prediction r.   LetHbe a human predictor that first takes in   the instance xand predicts a label for that instance ,   y. Then , His also shown the rationale rand now   takes both the instance and rationale x , ras an   input , and predicts a label y. Therefore , human   utility of the rationale ris calculated as:=⎧⎪⎪⎪⎨⎪⎪⎪⎩   In other words , rationales are useful if a human   incorrectly solved the task before , and with the in - troduction of the rationale , is able to correct their   answer . If even after being shown the rationale ,   the human is still solving the task incorrectly , this   implies that the rationale has notbeen useful . How-   ever , if the human was correct both before and after   being shown the rationale , we can not conclusively   determine the role of the rationale in helping solve   the task . We term these rationales as unsure . This   category of instances can either be too easy , or it   can be the case that the human was already aware of   the answer even before being shown the rationale .   Of course , this can also imply that the rationale   has still been useful in answering the task correctly ,   however , our definition of utility specifically evalu-   ates cases where rationales are solely responsible   for human utility .   Self - rationalizing Models . For our choice of F ,   we experiment with in - context learning and fine-   tuning based approaches . For the rest of our paper ,   we pick three LM configurations that provide us the   best task accuracy for the rest of our experiments   in this paper : davinci - instruct - beta ( GPT3 ) ( Brown   et al . , 2020b ) with six randomly picked demon-   strations , with the FEB ( Marasovic et al . , 2022 )   template , where rationales are generated after the   predicted answer , T5 - large with full fine - tuning   and infilling template ( Marasovic et al . , 2022 ) and   T5 - 3B with 128 - shot fine - tuning and infilling tem-   plate . Details about prompt templates , experiment   settings and model selection are in § A.2 .   To what extent do LM - generated rationales pro-   vide utility to humans ? We conduct human-   subject studies to evaluate utility of free - text ra-   tionales . We use Amazon Mechanical Turkto7105   first curate a set of annotators that understand the   task well ( via extensive qualification tests ) . Each   instance is answered by five annotators . ( The an-   notator agreements are shown in Table 18 ) . For   each StrategyQA and OBQA test instance , we ask   humans to first provide an answer given the ques-   tion . We then show them a rationale and ask them   to answer the question again . The rationale shown   to them is generated by either of the three selected   LMs . Details about MTurk experiment setup and   annotation agreements are in § A.6 . For each in-   stance , we calculate human utility as defined above ,   where predictions made by five annotators are ag-   gregated by taking a majority vote .   We observe that ( Table 3 ) for all the LMs com-   bined only a small amount of rationales generated   are actually useful for humans . A large chunk of ra-   tionales also mislead humans to select the incorrect   answer ( NU ) . In fact , for T5 - Large and   UnifiedQA - Large , the configuration that led to the   best task performance for StrategyQA and OBQA ,   has the highest % of NU rationales .   Do existing metrics correlate with human util-   ity ? Overall , while including annotations for all   models combined , we observe that the correlation7106   between task accuracy ( whether a given instance   was correctly predicted by the self - rationalizing   model ) and human utility of a rationale ( useful ,   not useful and unsure ) was close to none ( Theill ’s   U=0.0359 andU=0.0221 for StrategyQA and   OBQA respectively ) . This indicates that while gen-   erating rationales might improve overall task per-   formance , there is no guarantee that these rationales   are useful for humans in solving the task correctly .   In fact , if we look at the correlations for each   LM separately , we observe Theill ’s Ufor GPT-   3 , T5 - 3B and T5 - Large were 0.111(0.092),0.034   ( 0.029 ) and 0.005(0.016 ) for StrategyQA ( OBQA )   respectively ( Table 4 ) . This also demonstrates that   even though T5 - Large , which was fine - tuned on the   entire training set had the highest task performance ,   it has the lowest correlation with human utility .   We also compute the similarity between ratio-   nales and their corresponding gold rationale us-   ing BERTScore ( Zhang * et al . , 2020 ) for the test   set , and compute their correlation with their hu-   man utility ( Table 4 ) . For StrategyQA , the Corre-   lation Ratio η=0.041 for all three LMs com-   bined , and η=0.021,0.017,0.002 for GPT-3 ,   T5 - 3B and T5 - Large respectively , whereas for   OBQA η=0.055for all three LMs combined ,   andη=0.018,0.026,0.017for GPT-3 , T5 - 3B and   T5 - Large respectively .   What rationale properties are associated with   human utility of rationales ? We conduct a case-   study for the StrategyQA dataset . We list a set of   desirable properties of that useful rationales should   satisfy ( Wiegreffe et al . , 2021 , 2022 ; Golovnevaet al . , 2022 ) . These properties evaluate rationales   along four axes - surface form qualities , support   towards predicted labels , informativeness and style .   Surface form qualities test whether a rationale is   grammatical andfactually valid .Association with   label and contrast between different labels mea-   sure the extent to which rationales support the   labels that were generated with them . We also   evaluate the informativeness of a rationale , which   is determined by novel information that the ratio-   nale provides over the question , along with asking   whether it directly leaks the answer . Lastly , we also   check whether the rationale contains irrelevant hal-   lucinations or relevant but redundant information .   Descriptions and examples of these properties are   shown in detail in Figure 3 .   We use a Generalized Linear Mixed - Effects   Model ( GLMEM ) ( similar to Lamm et al . ( 2020 ) )   to estimate the importance of different properties   and their interactions in predicting the human util-   ity of rationales . We observe that while in isolation   or pairs , these properties are not sufficient indica-   tors of human utility ( § A.3.1 ) , when all possible   combinations of properties are considered , pres-   ence of all but coherence and association leads   to a positive log odds for rationale utility : 0.139 .   This implies that humans are generally robust to   hallucinations that are irrelevant to the question .   Furthermore , association of the rationale with its   predicted label is also not an important property for   rationale utility , as the rationale may not be associ-   ated with the correct answer and therefore , mislead   the human into making an incorrect choice.7107   3 Measuring Rationale Utility by   Answering Generalization Questions   As defined in § 2 , human utility of rationales is   determined by their ability to guide humans to cor-   rectly solve the task ( instances ) . We follow this   up by investigating if humans can generalize to   syntactic or semantic perturbations of the original   question , while being shown rationales of the origi-   nal question . This will help us understand if human   utility of rationales can also indicate whether ra-   tionales help with knowledge transfer for unseen   instances . For all our experiments , we use the Strat-   egyQA Dataset .   Types of Generalization Questions . For our   study , we consider three distinct types of gener-   alization setups . Firstly , we evaluate the human   H ’s ability to generalize to non - trivial rephrases   of the original question . We avoid simple rephrases   like changing a preposition , or removing an adverb   so as to avoid near duplicates of the original ques-   tion . Next , we look at counterfactual questions .   These questions follow the same reasoning steps as   the original question , however , they flip the answer   of the original question . Lastly , we test H ’s ability   to understand questions that follow a similar rea-   soning process as the original question , but are not   related to the original question . These questions   can entail entity swaps , or questions that use one of   the reasoning steps to answer the original question .   Examples of each type of generalization question   is shown in Table 5 .   Generating Generalization Questions . For gen-   erating generalization questions as described above ,   we follow the Human and AI collaboration   paradigm for dataset collection as introduced by   Liu et al . ( 2022 ) . We first start by manually creating   templates with instructions for each type of general-   ization question . We then select six demonstrations   for these templates . The selected instructions anddemonstrations are in Appendix ( Table 21 ) . These   demonstrations are fixed for each type ( however ,   may differ across the different types ) and are se-   lected from the training set . For every test instance ,   we insert it at the end of the corresponding tem-   plate , which is then used as a prompt for GPT-3   to generate questions . To increase the number of   good - quality generalization questions , we use GPT-   3 to generate 5 generalization questions of each   type for a given question , along with their answers .   We also vary the temperature ( 0.7 ) to control for   diversity in generated questions . The generated   questions and their answers are then validated by   a human study , to make sure that the final set of   questions is of good quality ( Details in § A.6.2 ) .   In the end , for each original question in the Strat-   egyQA dataset , we obtain generalization questions   of three different types , although the number of   generalization questions per original question can   vary . Overall , we collected 9659 , 1164 and2608   generalization questions for the training , validation   and test set , with 5.86,6.32and5.70generaliza-   tion questions per original question on average ,   respectively .   Human generalization is a good indicator of hu-   man utility . Similar to § 2 , we first ask the anno-   tators to answer a generalization question without   the rationale . We then show them the rationale   of the original question , and ask them to answer   the generalization question again , taking the ra-   tionale into account . We repeat the experiment   above with rationales from the three LMs , along   with gold rationales . Each instance is annotated by   five annotators . Given that there are no correspond-   ing rationales for the generalization questions , this   annotation setup would measure the impact of ra-   tionales of the original question towards answering   the generalization questions .   In Figure 4 , we plot the difference between the   generalization accuracies after and before being7108   shown the rationale of the original question . We   observe that gold rationales form an upper bound   for generalization difference , across all types of   generalization questions and types of rationale util-   ity . Useful rationales are able to help humans gen-   eralize better to new instances , whereas non - useful   rationales often mislead humans to make incorrect   choices , who might have correctly answered the   question before , which is indicated by the negative   plot bars in the Figure . Rationales about which we   are unsure are better or close to useful rationales   for rephrase and counterfactuals , as these general-   ization questions are relatively simpler .   However , for similar reasoning questions , they   underperform useful rationales . This indicates that   for rationales that are unsure , either the human   was already aware of the answer or the questions   are easier to answer as humans are able to answer   rephrases and counterfactuals correctly , but fail   in generalizing to questions that follow a similar   reasoning process . We can also note that GPT-3   generated rationales help generalize better to more   difficult settings like counterfactuals or similar rea-   soning questions . Examples of generalization ques-   tions that were answered correctly / incorrectly for   rationales that have high or low human utility is   shown in the Appendix ( Table 19 ) .   4 Improving Human Utility of   Self - Rationalising LMs   Smaller LMs like T5 - large have better task accu-   racy , but lack in generating more useful rationales .   It can be observed ( § 2 ) that the task performanceof a self - rationalizing LM and the human utility   of its corresponding generated rationales are not   correlated . Based on our insights about how use-   ful rationales can help humans generalization to   unseen questions , we propose G - U , which sim-   ulates a human through an LM : we define and use   G - Uto improve human utility of smaller LMs   like T5 - large , while aiming to maintain their task   accuracy ( Figure 5 ) . For all our experiments , we   use the StrategyQA Dataset .   LM generalization is a better indicator of ra-   tionale ’s human utility . § 3 indicated that gen-   eralization to unseen but similar questions via ra-   tionales of the original question is a reasonable   proxy for human utility of rationales . Based on this   insight , we propose G - U , which estimates the   generalization performance of an LM variant , after   and before being shown a rationale generated by a   self - rationalizing model .   For a given input - output pair x , y , there ex-   ist a set of ngeneralization questions X , Y=   { ( x , y),(x , y ) , . . . , ( x , y)}that is cre-   ated as per § 3 . Let Fbe a self - rationalising LM   as defined in § 2 , for which we want to estimate   the score . Let Fbe an LM that takes in Xas   its input and predicts a set of labels Y. Similarly ,   Fbe an LM that takes in Xand the rationale   rgenerated for xbyF , and predicts a set of labels   Y.   G - U for xis defined as:7109({(1−1(y = y))y = y   −1 y≠y )   Here , returns the most frequently occur-   ring value from the set ( similar to majority voting   in a set ) . In other words , if a generalization ques-   tion is answered incorrectly after being shown the   rationale , G - Uis−1 , otherwise , G - Ucali-   brates itself w.r.t the answer before being shown   the rationale , to accommodate for cases where the   question is easy - to - answer or the LM already con-   tains relevant background knowledge . Then , we   pick the majority vote of the scores ( depicted by   the mode ) for all the generalization questions for a   given original question as its score .   To validate if G - Uis indeed usable , we cal-   culate correlations between G - Uand human   utility of the corresponding rationales . We find that   Theill ’s U=0.22 , which is indicates that G-   Uis a better estimate that F ’s task accuracy or   BERTScore similarity between generated and gold   rationales ( refer Table 6 for correlation scores ) .   G - U as a reward for updating LM . We use   the Quark ( Lu et al . , 2022 ) algorithm with G - U   to improve the human utility of rationales generated   byF. Quark is an RL - inspired training algorithm   that uses reward signals as control tokens on the en-   coder ( or decoder ) side , to condition the generation   of text .   ForF , we use the same T5 - large setup used in   § 2 . For implementing G - U , we use T5 - base   LMs for FandF , which are both finetuned   on the StrategyQA dataset . We begin by first fine-   tuning Ffor25epochs with supervised learning   on the StrategyQA data , after which we continue   training with Quark . The final Fis obtained after   finding the best hyperparameter choices based on   G - U scores for the validation set .   Table 7 demonstrates the G - Uscores before   and after using Quark to update F. On the updated   LMF , we conduct the same human utility evalu-   ations as done in § 2 to evaluate the improvement   observed by lay humans . We note that the updated   LM is able to retain most of the task performance ,   while improving the % of U rationales by   2%.G - Ualso helps in getting rid of 4%of mis-   lead ( NU ) rationales . We also compare   the updated LM with GPT-3 , which yielded the   best human utility of rationales . G - Uis able to   make the updated LM closer to the human utility   of GPT-3 , while ensuring the task performance for   the updated LM remains better than GPT-3 . This   indicates that while incorporating human utility   while generating rationales is a difficult problem   and there is room for improvement , smaller LMs   like T5 - large are capable of improving , without   compromising on the task accuracy that is obtained   via fine - tuning .   5 Related Work   Evaluating free - text rationales Extractive ex-   planations have been used to improve human ’s un-   derstanding of the model ( Wang and Yin , 2021 ;   Feng and Boyd - Graber , 2018 ; Carton et al . , 2020 ;   Chen et al . , 2022b ; Idahl et al . , 2021 ; Chu et al . ,   2020 ) or detecting errors in model predictions   ( González et al . , 2021 ) . Although prior motivation   of generating rationales has been primarily to im-   prove task model performance ( Rajani et al . , 2019b ;   Zelikman et al . , 2022 ; Wei et al . , 2022 ; Lampinen   et al . , 2022 ) , recent works have evaluated ratio-   nales in various ways . Wiegreffe et al . ( 2022 ) use   human acceptability judgements on over - generated   rationales by GPT-3 ( Brown et al . , 2020a ) . They   also evaluate the rationales across seven axes like7110grammar , factuality , etc . Sun et al . ( 2022 ) measure   benefits of rationales to LMs and compared human   written rationales with those generated by GPT-3   across two axes : rationales that provide new infor-   mation over the input , and those that leak the label   directly .   Rationale Generation There are two distinct   methods of generating free - text rationales . The first   way is to fine - tune an encoder - decoder like model ,   for example , T5 or it ’s variations like UnifiedQA   ( Raffel et al . , 2020 ; Khashabi et al . , 2022 , 2020a ) .   Finetuning T5 to generate rationales ( Narang et al . ,   2020 ; Paranjape et al . , 2021 ) entails appending a   tag like explain : in the input text , to nudge the   LM to generate rationales during prediction . The   generated text can either be separated by structured   tags like answer : , explanation : , or it can be un-   structured , with the answer followed by a because   keyword , followed by the rationale . Recent meth-   ods have also analysed few - shot prompting of T5   with different input - output templates ( Marasovic   et al . , 2022 ) . Another recent approach of generat-   ing free - text rationales is via in - context learning   ( Wei et al . , 2022 ; Kojima et al . , 2022 ; Marasovic   et al . , 2022 ; Wiegreffe et al . , 2022 ) . A decoder-   only model like GPT-3 or its variants ( Brown et al . ,   2020a ; Wang and Komatsuzaki , 2021 ) that are pre-   trained on a larger corpora of world - knowledge are   prompted with demonstrations ( Wei et al . , 2022 ) ,   wherein each example contains its corresponding   explanation .   Human Utility of Human Rationales Several   works in Psychology and Cognitive Science de-   tail the role that human rationales play for human   understanding . These studies have shown that hu-   man rationales are inherently incomplete and do   not capture the complete deductive reasoning pro-   cess . ( Tan , 2021 ) . These rationales are used to   either provide evidence orprocedure behind obtain-   ing a given conclusion for a situation ( Lombrozo ,   2006 ) . Furthermore , some works have also detailed   the utility human rationales have for human under-   standing . Human rationales have shown to help   better generalise to unknown circumstances ( Lom-   brozo and Gwynne , 2014 ) , justify decision - making   ( Patterson et al . , 2015 ) , understand relationships   between different world entities ( Hummel et al . ,   2014 ) , diagnose when something went or might go   wrong , as well as explain one off events that are   bizarre ( Keil , 2006).Updating LMs with Generation Feedback   There are several ways to update language mod-   els with rewards to correct misaligned behaviour   that models learn ( Chen et al . , 2021 ; Janner et al . ,   2021 ) . Lu et al . ( 2022 ) unlearn these misalign-   ments by fine - tuning the language model on sig-   nals of what not to do . Similarly , Zelikman et al .   ( 2022 ) iteratively leverage a small number of ra-   tionale examples to training and only keep good   examples . Our method is inspired by several eval-   uation methods ( Chen et al . , 2022a ; Chan et al . ,   2022 ; Wiegreffe et al . , 2020 ; Hase et al . , 2020 )   which discussed how to better evaluate the qual-   ity of free - text rationales with regard to labels and   contexts .   6 Conclusion and Future Work   In this work , we study human utility of free - text   rationales , by measuring how well lay humans are   able to solve tasks with their help . Through exten-   sive human evaluations , we show that human utility   of rationales generated by current LMs is rather un-   satisfactory , and existing available measures do not   correlate well with it . We find that generalization   ability with rationales as context is a good proxy   for human utility , and use it as a reward to improve   human utility of LMs .   There are a lot of scopes to improve human util-   ity of self - rationalising LMs , where granular - level   properties of rationales can be leveraged directly .   Furthermore , evaluation of human utility on other   tasks ( like closed - book QA ) is something that is   also worth looking at , given that human annotators   can not ‘ guess ’ answers for these tasks , making it   harder for LMs and humans alike .   7 Acknowledgments   This research is supported in part by the Of-   fice of the Director of National Intelligence   ( ODNI ) , Intelligence Advanced Research Projects   Activity ( IARPA ) , via Contract Nos . 2019-   19051600007 and 2022 - 22072200006 , Defense   Advanced Research Projects Activity ( DARPA )   No . HR00112220046 , NSF IIS 2048211 , and gift   awards from Google , Amazon , JP Morgan , and   Sony . We would like to thank all of our collabora-   tors at USC NLP Group , USC INK Research Lab ,   Meta AI and AI2 , specially Swabha Swayamdipta   and Ameya Godbole for their constructive feedback   on this work.7111Limitations   Estimating human utility is expensive . The   core of our work is built on conducting extensive   human evaluations , to understand how well lay hu-   mans can solve tasks with rationales . In order to   replicate these findings to other tasks , one would re-   quire the same scale of human evaluations , which   are expensive and tedious . These tasks are also   difficult to explain to lay crowdworkers , because   of which several rounds of turking are required   to reach good annotator agreements . Given these   shortcomings of human evaluation , a reliable met-   ric that estimates human utility is necessary .   Generating generalization questions is not com-   pletely automated . Even though we prompt   GPT-3 with varied demonstrations to generate gen-   eralization questions of each type , we still have to   manually filter them ( via crowdsourcing ) to obtain   a cleaner set of questions . Furthermore , in order   to obtain gold answers of these questions , we gen-   erate answers by prompting GPT-3 again , which   also requires further validation . A completely auto-   mated method of generating these questions would   lead LM updates to be independent of human in-   volvement .   Even though G - Uhas a better correlation   with human utility , the correlation is still low .   To train models to produce free - text rationales with   more human - utility through Quark ( Lu et al . , 2022 ) ,   it is first necessary to have an accurate metric that   can serve as a reward function / scoring metric for   human utility . In this work , we found that human   generalization is good indicator of human - utility .   However , given that Quark requires frequent re-   ward scoring , it is infeasible to use human annota-   tions for the same . Our proposed automatic metric   G - Uthat simulates human generalization has   a good correlation with human utility ( better than   task accuracy , or BERTScore ) , but overall , it still   has a low correlation with human utility of ratio-   nales . Developing a score with better correlation   with human utility ( perhaps even a stronger version   ofG - U ) will decrease the effect of this limi-   tation and lead to training that further increases   human utility of generated rationales .   Ethics Statement   Data . All the datasets that we use in our work   are released publicly for usage and have been dulyattributed to their original authors . Data for all hu-   man studies that we conduct is publicly released   with this work , with appropriate annotator anonymi-   sations .   Crowdsourcing . All our crowdworkers are from   countries where English is the primary language .   For all our human studies , the task is setup in a   manner that ensure that the annotators receive com-   pensation that is above minimum wage ( $ 15 / hour ) .   Since we conduct extensive qualification tasks be-   fore annotations , crowdworkers that participate in   the qualification are compensated more than the   task , given the time taken to read and understand   task instructions and examples . Furthermore , we   ensure that we correspond with crowdworkers over   email to address their queries . Crowdworkers have   also been given bonuses for flagging errors in the   task , or consistently providing good - quality anno-   tations .   References711271137114A Appendix   A.1 Task and Dataset Selection   We refrain from tasks used in existing free - text   rationale works ( Wiegreffe and Marasovi ´ c , 2021 )   like NLI ( Camburu et al . , 2018 ) and Commonsense   QA ( Aggarwal et al . , 2021 ) . A primary reason for   this is that humans are already able to reason bet-   ter than models for NLI and Commonsense QA   ( Nangia and Bowman , 2019 ; Talmor et al . , 2021 ) .   Therefore , the objective of machine rationales in   this case is just to establish trust or generate faith-   ful rationales . We aim to study rationale utility   specifically in cases where the rationales can help   with knowledge transfer that helps humans to cor-   rectly solve a task . We thus impose the following   constraints in our task and dataset selection :   •Added advantage : Tasks where machines   can provide added advantage and that are not   trivial or obvious for humans to solve .   •Objectivity : Tasks where the reasoning has a   limited scope of subjectivity .   •Dataset size ( of rationale annotations ): Size   of gold rationales is considerably larger in   the dataset , so as to provide room for training   LMs with those rationales .   In this work , we choose the StrategyQA dataset   ( Geva et al . , 2021 ) , which is an open - domain binary   QA benchmark , where questions require implicit   reasoning steps to be answered . The StrategyQA   dataset consists of an input question , the answer ,   along with intermediate implicit reasoning steps   that are used to answer the questions . The implicit   reasoning steps were generated by decomposing   the original question into multiple questions . For   our project , we combine these implicit reasoning   steps and use them as rationales for a given instance .   We also use the OpenBookQA Dataset ( Mihaylov   et al . , 2018 ) for validating human utility of ratio-   nales for existing LMs . Both of these datasets are   available publicly for use , and have been checked   manually by authors for toxic / offensive content .   A.2 Self - Rationalising Models   We try variations of in - context learning based ap-   proaches ( Wei et al . , 2022 ) , as well as few - shot   and full finetuning approaches ( Marasovic et al . ,   2022 ) to generate rationales . For in - context learn-   ing based approaches , we vary the demonstrations   based on the number of demonstrations desired,7115   and the selection strategy for these demonstrations .   These demonstrations can either be fixed across   all instances vs. randomly picked for each in-   stance , from the training set . Demonstrations that   are picked randomly can either be six in number   ( to match a fixed number of demonstrations as per   Wei et al . ( 2022 ) ) , or determined by a maximum   token length that is specific beforehand ( for our   experiments , we use 2048 as the maximum token   length of an input ) . For these settings , we imple-   ment two input - output templates – where rationales   rcome after ( FEB ) ( Marasovic et al . , 2022 ) or   before the prediction yrespectively ( Chain - of-   Thought or CoT ) ( Wei et al . , 2022 ) . The LM used   for all in - context learning experiments is GPT-3   ( Brown et al . , 2020a ) . For fine - tuning approaches ,   we fine - tune two LMs - T5 ( Raffel et al . , 2019 ) and   UnifiedQA ( Khashabi et al . , 2020b ) , with varying   sizes - large and 3B. For each of these two LMs , we use four variations of input - output templates   ( SQuAD - T5 , Infilling , T5 - Like and QA - simple ) , as   defined by Marasovic et al . ( 2022 ) . Examples of   each of these templates are provided in Figure 6 .   As seen in Tables 9 , 10 and 11 , for the Strate-   gyQA and OBQA datasets , FEB templates with ran-   domly selected demonstrations provides the high-   est accuracy for in - context learning approaches ,   whereas the infilling template consistently outper-   forms other input - output templates for fine - tuning   approaches . For the rest of our work , we select   three best performing LM configurations with vary-   ing sizes – ( 1 ) GPT-3 ( with FEB template , and 6   randomly selected demonstrations ) , ( 2 ) T5 - large   ( with infilling template , fine - tuned on the entire   training set ) and ( 3 ) T5 - 3B ( with infilling template   and 128 - shot fine - tuning ) .   Task Performance . For the three selected best   performing LM configurations , we note ( Tables   9 , 10 ) that task performance increases after the   LM is forced to generate rationales . This is also   consistent with prior findings ( Wei et al . , 2022 ;   Marasovic et al . , 2022 ) .   A.2.1 Self - Rationalising Models Training   Details   In the experiments , we mainly used 3 models : T5-   Large , T5 - 3B , and GPT-3 ( model details and hyper-7116   parameters are shown in Table 12 ) . For T5 - Large ,   we used the full train set for finetuning . For T5-   3B , we trained in 2 settings : 48 - shot and 128 - shot .   We used 3 seeds for generating shots for T5 - 3B.   For GPT-3 , we only used the OpenAI GPT-3 API   ( Brown et al . , 2020b ) to do inference .   A.3 Property Analysis   For rationales generated by all three LMs , as well   as gold rationales , we conduct human studies to   evaluate whether the rationales satisfy the given   properties . For each instance , a property is marked   on a binary scale ( Yes / No ) , indicating the pres-   ence or absence of that property and evaluated by   five annotators . Each category of properties is eval-   uated on a separate HIT , for which instructions   have been modified so as to ensure that the anno-   tators understand our definitions of the properties .   Given the complex nature of the human study , we   make sure that the property annotations reach low   to moderate agreement across all annotators ( Table   13 ) .   Presence of properties in Gold and LM-   generated Rationales We first study the pres-   ence of these properties in rationales , without con-   sidering the utility of these rationales . Figure 7   plots the distribution of these properties , split by7117   the models that generate these rationales , along   with Gold rationales . The distributions are obtained   by taking the mean of ratings from five annotators   for a given instance , where a higher value indicates   a more frequent presence of that particular prop-   erty in the set of rationales . We observe that Gold   rationales , in comparison to other model - generated   rationales , have lower scores for leakage and higher   scores for other properties . In fact , Gold rationales   are always associated with the gold label , which   serves as a sanity check , as they are designed tohelp answer the gold label . While all types of ratio-   nales are mostly grammatically correct , T5 - Large   and T5 - 3B suffer at producing rationales that are   factually correct , and T5 - Large rationales also tend   to hallucinate and produce redundant sentences in   rationales more often . While GPT-3 rationales tend   be generally better than T5 - Large and T5 - 3B for   surface - form and stylistic properties , they leak the   predicted label more often than them . There is   high variation for rationale - label association and   contrasting features in rationales for all model-   generated rationales , however on average , GPT-3   generated rationales are better on these metrics too .   A.3.1 Property Correlations with Human   Utility   We use a Generalized Linear Mixed - Effects Model   ( GLMEM ) ( similar to Lamm et al . ( 2020 ) ) to model   the correlation of different properties and their   interactions with that of human utility . The for-   mula used for modelling the GLMEM is as fol-   lows:= ( + + + + + + + ) + ( 1∣)+   ( 1∣)+(1∣ )   The response ( dependent variable ) is human ac-   curacy after the human was shown the rationale.7118   More formally,= {   All properties , along with their second - order in-   teractions ( implemented using the squared term   above ) are dependent variables . Furthermore , we   try to control for random effects whose variabil-   ity might influence the response . We control for   randomness induced by a particular question , the   model generating the rationales or whether the hu-   man had correctly answered the question before   ( Human Prior ) . More formally,= {   Table 14 shows the log odds of a rationale being   useful when a certain property is present or absent ,   while averaging over other properties . We note   that all of the log odds are negative , which means   that in isolation , the presence or absence of any   property does not correlate well with rationales of   high utility .   We then look at pairwise interactions . Table 15   shows the top ten pairs which lead to an increase   in utility log odds from the base level ( Intercept ) ,   which is when a rationale does not satisfy any prop-   erty . A grammatically correct rationale that explic-   itly leaks the answer leads to the highest increase   in log odds . This is also intuitive , as leakage is a   direct signal to a human to select a given answer ,   without any reasoning from the human ’s behalf .   When all possible combinations of properties   are considered , presence of all but coherence and   association leads to a positive log odds for rationale   utility : 0.139 .   A.4 Quark training details   For the Quark experiments , we used T5 - Large as   the self - rationalizing LM , and T5 - Base for G - U.   The hyperparameters used for running Quark ( Lu   et al . , 2022 ) are shown in Table 16 .   A.5 Examples   In Table 21 we provide the demonstrations used   to generate generalization questions using GPT-   3 . In Table 19 , we provide examples of useful ,   unsure and non - useful rationales with respect to   human generalization . In Table 20 ( corresponding   to Figure 4 ) we provide results for the difference   in accuracies of human generalization , before and   after a human annotator was shown the original   question ’s rationale.7119   A.6 MTurk Details   In this section , we describe the MTurk experiment   setup . The details of MTurk experiments including   how many Turkers took the evaluation , and average   time used to finish evaluations are shown in Table   17 . Each MTurk annotator is paid above minimum   wage . Figure 8 demonstrates the setup for human   utility evaluation . Figure 9 demonstrates the setup   for property evaluation . Figures 10 demonstrates   the setup for validating generalization questions .   Figure 11 demonstrate the setup for utility   evaluation towards generalization questions .   Since the dataset we used is carefully annotated   by human , we can assure there is no toxic content   and our experiment setup was submitted to IRB   for ethical review . We limited our Turkers to   English speaking nations - United States , Canada ,   Australia , New Zealand and United Kingdom .   To ensure the quality of evaluation , we did a   round of qualification task before each task   which include a small set of evaluations . Turkers   need to finish the qualification task first and get   results of it , then we will show them the whole task .   A.6.1 Worker Selection and Quality Control   Here , we describe details about how workers are se-   lected and how annotations are ensured to be clean .   First , we employ multiple rounds of trials before   deploying the actual task so as to get feedback   from annotators whether they understand the task   correctly . This includes in - house tests , tested viaAmazon Turk Sandboxand small batches tested   on Turk . Second , we create a set of medium to hard   qualification tasks for each task that the annotators   have to work on . These tasks are hand curated   that cater certain parts of the instruction – whether   the annotators are reading the rationale correctly ,   or whether they are able to make appropriate con-   nectections between the rationale and the question .   This weeds out a lot of annotators who do not un-   derstand the task or are cheating . We also weed out   workers who are too ‘ fast ’ ( completing the task in   less than 5seconds , which is indicative of potential   slacking in the task ) . Third , we constantly monitor   task responses and feedback provided to annotators   about their task . We also collect feedback from   them which we adapt in new versions of the task .   A.6.2 Turking for Generalization Questions   Each generalization question is validated by 3an-   notators each . The validation process includes :   checking if the generated question can be answered   by the gold rationale , answering the generated ques-   tion , and checking if the generated question follows   the instructions for a given type ( being a rephrase ,   counterfactual or a similar reasoning question ) .   The annotation agreement observed here is high   ( Krippendorf ’s α=0.68 ) .   A.6.3 Annotation Agreements   We observe that StrategyQA instances are diffi-   cult to annotate by humans , as many of them are   fact - based , which the human might or might not   know beforehand . Therefore , human agreement   before the rationale is shown is low ( Krippendorf ’s   α=0.18 ) . However , after being shown the ratio-   nale , the agreement increases , as shown in Table 18 .   Examples of rationales annotated into each of the   three human utility categories ( useful , not useful ,   unsure ) is shown in Table 1.7120712171227123712471257126ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 7   /squareA2 . Did you discuss any potential risks of your work ?   Section 7   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 2,3 Appendix A.2 , A.7   /squareB1 . Did you cite the creators of artifacts you used ?   Appendix A.2   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix A.2   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Appendix A.2   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Appendix A.2   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Appendix A.7   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Appendix A.2   C / squareDid you run computational experiments ?   Section 2,3,4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A.3.17127 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix A.3.1   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 2   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix A.3.1   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 2,3,4 ; Appendix A.7   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix A.7   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Appendix A.7   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Appendix A.7   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Appendix A.7   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Appendix A.77128