  Yuan Xia , Zhenhui Shi , Jingbo Zhou , Jiayu Xu , Chao Lu , Yehui Yang ,   Lei Wang , Haifeng Huang , Xia Zhang , Junwei LiuBaidu Inc. , China . Neusoft Corporation , China.{xiayuan , shizhenhui , zhoujingbo , xujiayu03,luchao , yangyehui01 ,   wanglei15 , huanghaifeng , liujunwei}@baidu.com,zhangx@neusoft.com   Abstract   With the development of medical digitization ,   the extraction and structuring of Electronic   Medical Records ( EMRs ) have become chal-   lenging but fundamental tasks . How to ac-   curately and automatically extract structured   information from medical dialogues is espe-   cially difficult because the information needs   to be inferred from complex interactions be-   tween the doctor and the patient . To this end ,   in this paper , we propose a speaker - aware co-   attention framework for medical dialogue in-   formation extraction . To better utilize the pre-   trained language representation model to per-   ceive the semantics of the utterance and the   candidate item , we develop a speaker - aware di-   alogue encoder with multi - task learning , which   considers the speaker ’s identity into account .   To deal with complex interactions between dif-   ferent utterances and the correlations between   utterances and candidate items , we propose a   co - attention fusion network to aggregate the   utterance information . We evaluate our frame-   work on the public medical dialogue extraction   datasets to demonstrate the superiority of our   method , which can outperform the state - of - the-   art methods by a large margin .   1 Introduction   In the past decade , the collection and usage of Elec-   tronic Medical Records ( EMRs ) have been proved   as one of the most important applications in the pro-   cess of medical digitization . However , the record-   ing and writing of the EMRs may bring a signif-   icant burden to doctors . Given the breakthrough   advance of speech recognition technology , conver-   sations between doctors and patients can be accu-   rately recorded as text . However , such unstructured   medical dialogue data can not be easily utilized for   medical research . How to automatically extract   the structured information from these unstructuredFigure 1 : An example of a patient - doctor dialogue and   the corresponding annotated labels .   textual medical dialogue data is an essential step to   accelerate medical digitization .   Compared with the general medical information   extraction , the crucial challenge of the medical dia-   logue extraction is that it has to take the speaker ’s   identity and utterance interactions into consider-   ation . In conventional information extraction , a   relation can largely be inferred by a sentence or   a paragraph . However , in the medical dialogue   extraction task , the candidate item and status infor-   mation need to be detected and then verified by the   conversations between the doctor and the patient .   An example of a patient - doctor dialogue and the   corresponding annotated labels is shown in Figure   1 . For instance , the doctor asks the patient , “ Did4777you feel angina ? ” , the patient responds , “ No , I   felt there is a sense of suppression in the chest . ” ,   the ground truth labels for correct extraction are   ( chest tightness : patient - positive ) , ( angina : patient-   negative ) . If only considering the utterance of the   patient or the doctor alone , we can not make correct   information extraction .   However , how to leverage the speaker ’s identity   and utterance interactions information to facilitate   medical information extraction is not well explored .   Du et al . ( 2019 ) describe a novel model that extracts   the symptoms mentioned in clinical conversations   along with their status . The annotation of their sta-   tus does not consider the speaker ’s identity into ac-   count . Lin et al . ( 2019 ) make symptom recognition   and symptom inference in medical dialogues , and   propose a global attention mechanism to capture   symptom - related information . Zhang et al . ( 2020 )   develop a medical information extractor based on a   simple deep matching module to take turn interac-   tion into consideration . Thus , all existing methods   fail to take the speaker into consideration , and the   simple utterance combination method such as just   concatenating all utterances together with flat at-   tention can not grasp sufficient information among   utterance interactions in the medical dialogue .   To tackle the above challenge , we propose a   Speaker - aware co- Attention Framework for med-   ical dialogue Extraction ( name SAFE for short ) .   First , to better predict the status of the candidate   item in the medical dialogue , we should both con-   sider the contextual information from the dialogue   and be aware of the identity of the speaker . For   the annotated label ( echocardiography : doctor-   positive ) in the dialogue shown in Figure 1 , be-   ing aware of the identity ( patient ordoctor ) of the   speaker can help make a correct inference . Second ,   we propose an utterance - based co - attention graph   network to perceive complex correlations between   different utterances .   We summarize our contributions as follows :   •We propose a new framework ( SAFE ) for   medical dialogue extraction , which can better   utilize the pre - trained language representation   model to grasp the semantics of both utter-   ances and candidate items .   •We develop a novel speaker - aware encoder   and a co - attention fusion method with multi-   task learning and graph networks , which takes   the speaker ’s identity and correlations be - tween utterances and candidate items into con-   sideration .   •We evaluate our framework on the public med-   ical dialogue datasets to demonstrate the supe-   riority of our method , which can outperform   the state - of - the - art methods by a large margin .   2 Related Work   2.1 Pre - trained Language Models   Pre - trained language models , like BERT ( Devlin   et al . , 2019 ) , Roberta ( Liu et al . , 2019 ) , XLNet   ( Yang et al . , 2019 ) , ERNIE ( Sun et al . , 2020 ) ,   T5(Raffel et al . , 2020 ) , BART(Peng et al . , 2021 )   and GPT3 ( Brown et al . , 2020 ) , can achieve huge   gains on many Natural Language Processing ( NLP )   tasks , such as GLUE ( Wang et al . , 2018 ) and Super-   GLUE ( Wang et al . , 2019 ) benchmarks . In our pro-   posed framework , we utilize the fine - tuned BERT   model as the initial encoder to obtain the represen-   tations for the utterance and the candidate item .   2.2 Medical Dialogue Extraction   Extracting information from EMR texts has at-   tracted much research attention in both NLP and   biomedical domains ( Xia et al . , 2021 ) . Du et al .   ( 2019 ) propose a span - attribute tagging ( SAT )   model and a variant of the sequence - to - sequence   model to solve the symptom tagging and extraction   problems . Lin et al . ( 2019 ) present a global at-   tention mechanism , which perceives the symptom-   related information from both dialogues and corpus   to improve the performance of symptom recogni-   tion and symptom inference . However , the above   works mainly focus on the sequential labeling and   medical name entity recognition ( NER ) , and fail   to consider the complex interaction between utter-   ances . In industrial applications , Peng et al . ( 2021 )   propose a dialogue - based information extraction   system that integrates existing NLP technologies   for medical insurance assessment , while their mo-   tivation is to reduce the time cost of the insurance   assessment procedure .   The most similar work related to to our study   is ( Zhang et al . , 2020 ) , which proposes a medical   information extractor ( MIE ) by using an LSTM   ( Hochreiter and Schmidhuber , 1997 ) model as an   encoder module , and then adopting an aggregate   module to take the utterance interaction into consid-   eration . Our study is different from ( Zhang et al . ,   2020 ) in the following two points . On the one4778   hand , we develop a multi - task learning method to   train a speaker - aware dialogue encoder module that   takes the speaker ’s information into consideration .   On the other hand , we utilize a co - attention fusion   mechanism to perceive complex interactions be-   tween different utterances and the correlation with   the candidate item .   3 Preliminaries   In this section , we formally define the problem of   medical dialogue extraction ( MDE ) . For a dialogue   withntokens and mutterances , it can be defined   asD= ( U , U , · · · , U ) , where Uis the   i - th utterance in the dialogue , r∈ { 0,1 } , which   indicates the speaker ’s identity ( e.g. belongs to   patient ordoctor ) . The candidate item I∈ I is   a medical term ( like symptom , disease , surgery ,   etc . ) which can be extracted from a dialogue D.   For each candidate item I , we also need to iden-   tify its status S∈ S where Sis an element from   the set { patient - negative , patient - positive , patient-   unknown , doctor - positive , doctor - negative } which   indicates whether the candidate item is confirmed   or denied by doctors and patients .   Finally , we define the task of medical dialogue   extraction as follows : given a medical dialogue   D∈ D , candidate item I∈ Iand its status S∈ S ,   the MDE can be formulated to predict the label   f : D→ Y where Yis a matrix generated by   Cartesian product of the candidate item Iand its   statusS , i.e. Y= ( y)∈R , and y= 1   indicates that the medical dialogue Dcontains the   candidate Iwith the status S. Note that different   from the task for relation extraction ( RE ) , the label   space for the MDE is very sparse , which causes it   a more challenging problem.4 Method   We develop a three - stage pipeline system : ( 1 )   Speaker - Aware Dialogue Encoder Module ( SAE ) ,   a module to turn the utterances in the medical di-   alogue and the candidate item into node feature   representations , which also takes the speaker iden-   tity into account ; ( 2 ) Co - Attention Fusion Module   ( CAF ) , a module to involve the interactions be-   tween the utterances and the correlation between   utterance and candidate item into consideration ;   and ( 3 ) Inference Module ( IM ) , a module to utilize   the fusion representation for final dialogue infor-   mation extraction . The full pipeline of our pro-   posed medical dialogue extraction framework is   illustrated in Figure 2 .   4.1 Speaker - Aware Dialogue Encoder Module   An effective medical dialogue encoder should cap-   ture the semantics of the utterance and perceive   the speaker ’s identity . In this work , we designed a   multi - task learning method to pre - train our speaker-   aware dialogue encoder . Our dialogue encoder is   pre - trained on a Speaker Recognition Task ( SRT )   and a Status Entailment Task ( SET ) . For the SRT   task , we design a speaker recognition task to distin-   guish the identity of the speaker . For the SET task ,   we leverage the pre - trained language model like   BERT to train a status entailment task to perceive   the semantics in the dialogue . In Figure 3 , we il-   lustrated the training process of our speaker - aware   dialogue encoder module .   Speaker Recognition Task   Given an utterance in a dialogue , if the encoder   itself can be aware of whether the speaker is a   patient or a doctor , it will help to infer the corre-4779   sponding status for the candidate item . We pre - train   the BERT - base encoder with the auxiliary speaker   recognition task , which is designed to distinguish   whether the utterance in the medical dialogue is   spoken by the patient or by the doctor . The speaker   recognition task is illustrated in the upper side of   Figure 3 . We construct the binary training samples   from the medical dialogues corpus . The utterances   from the patient are labeled as 1 , and the utterances   from the doctor are labeled as 0 . We mask the   word patient anddoctor at the beginning of each   utterance , which can prevent the model from dis-   tinguishing the speaker only with the beginning   prompt words .   First , we take the utterance Uinto the BERT-   base encoder to get the utterance representation   U :   U= Encoder(U ) . ( 1 )   Then , we fed the utterance representation into   a binary classifier , which is imposed of a dense   layer and a softmax layer . The speaker recognition   probability is as follows :   P(r= 1|U ) = softmax ( WU ) , ( 2 )   whereW∈Rdenotes weight matrix , dis the   number of hidden dimensions of the encoder . The   loss function of the SRT for a single dialogue is as   follows :   L=1   M / summationdisplay−rlogP(r= 1|U )   −(1−r)logP(r= 0|U).(3 )   where Mis the number of utterances in a dialogue ,   andris the label of the speaker .   Status Entailment Task   We jointly pre - train the BERT encoder with another   auxiliary status entailment task , which is designedto entail the status of the candidate item . The status   entailment task is illustrated at the bottom of Figure   3 . We re - formulate the medical dialogue informa-   tion extraction into a status entailment task . Given   a medical dialogue and the candidate item , we need   to entail the status of the candidate item . The model   should make an inference on the candidate ’s sta-   tus conditioned on the dialogue and candidate item   information .   First , we concatenate all the utterances in a med-   ical dialogue Dand the candidate item Itogether ,   and fed them into the BERT - base encoder to get   the dialogue representation D :   D= Encoder(D , I ) . ( 4 )   Then , we fed the dialogue representation into a   multi - class ( multi - status ) classifier , which is also   imposed of a dense layer and a softmax layer . The   status entailment probability is as follows :   P(y|D , I ) = softmax ( WD ) , ( 5 )   whereW∈Rdenotes weight matrix , dis the   number of hidden dimensions of the encoder , Cis   the number classes of the status . The loss function   for the SET is as follows :   L= CrossEntropy ( y , P(y|D , I)).(6 )   where yis ground truth status label for candidate   item in the dialogue , and CrossEntropy ( · ) is cross   entropy loss function .   Joint Optimizing   The final loss function for the speaker - aware en-   coder Encoderis as follows :   L = λL+ ( 1−λ)L. ( 7 )   where L andL are the losses for speaker   recognition task and status entailment task , re-   spectively , λis the hyper - parameter to control the   weight of each task.47804.2 Co - Attention Fusion Module   Given the medical dialogue , we employ our pre-   trained speaker - aware encoder Encoderas   our utterance encoder by extracting the final hid-   den state of the [ CLS ] token as the represen-   tation , where [ CLS ] is the special classifica-   tion embedding in our pre - trained model . In or-   der to involve the correlation between the utter-   ance and the candidate item , given mutterances   ( U , U , · · · , U)in a dialogue and a candidate   itemI , we feed each utterance - candidate item pair   ( U , I)into our speaker - aware encoder to obtain   the utterance representation U. We also feed the   candidate item Iinto the speaker - aware encoder   alone to obtain the candidate item representation I :   U= Encoder(U , I ) ,   I= Encoder(I),(8 )   To better capture complex interactions between   utterances , we use a co - attention fusion mechanism   to aggregate the utterance information . We treat   each utterance as a node and define other utterances   in the same sliding window as its neighbors . Then   we calculate the attention coefficient between a   node iand its neighbor j(j∈ N ) .   c = W(ReLU ( W(concat ( U , U ) ) ) ) ,   ( 9 )   where j∈ Nis the in - window neighbors of the   node i , W∈RandW∈Rare   weight matrices , and concat ( · , · ) is concatenation   operation . dis the number of dimensions of the   utterance feature representation , wis the number   of dimensions of the intermediate hidden state .   We use a softmax function to normalize the   utterance - utterance co - attention coefficients ϕ ,   ϕ= softmax ( c ) = exp(c)/summationtextexp(c).(10 )   Then , given the utterance - utterance co - attention   matrix ϕ , inspired by ( Kipf and Welling , 2017 ;   Veliˇckovi ´ c et al . , 2018 ; Zhou et al . , 2019 ) , we em-   ploy a simple GCN layer for information fusion .   /tildewideU = σ / parenleftig / summationdisplayϕW / tildewideU / parenrightig   , ( 11 )   where / tildewideUis initialized with U , W∈R , l   is the number of layers for propagation . We also explicitly involve the correlation be-   tween the utterance /tildewideUand the candidate item   Iby another co - attention layer :   p = W(ReLU ( W(concat ( /tildewideU , I ) ) ) ) ,   ( 12 )   where W∈RandW∈Rare   weight matrices .   Similarly , we adopt a softmax function to nor-   malize the utterance - candiate item co - attention co-   efficients ψ ,   ψ= softmax ( p ) = exp(p)/summationtextexp(p ) , ( 13 )   Finally , the normalized co - attention coefficients   are used to compute a linear combination of ut-   terance features of neighbors for final information   extraction :   T= CoAttn ( D , I ) = /summationdisplayψ / tildewideU. ( 14 )   4.3 Inference Module   The output representation Tof the Co - Attention   Fusion module ( CAF ) is then fed into the final in-   ference module to extract the medical information   from the dialogue .   ˜y= softmax ( WT+b ) , ( 15 )   whereTis the c - th index of the candidate item ,   W∈Randb∈Rare weight matrix   and bias , respectively . ˜yis the predicted probabil-   ity of the candidate item ’s status , yis the ground-   truth label .   The final loss function is as follows :   L=1   NC / summationdisplay / summationdisplayylog˜y . ( 16 )   where Nis number of dialogues in the training   corpus , Cis the number of classes for candidate   item status .   5 Experiments   5.1 Datasets   To verify the effectiveness of our SAFE framework ,   we conduct extensive experimental evaluations on   the Medical Information Extraction MIE dataset4781(Zhang et al . , 2020 ) . The dataset involves doctor-   patient dialogues collected from a Chinese medical   consultation website . The MIE dataset is repre-   sentative for medical dialogue task from EMR . On   the one hand , the dialogues from the MIE dataset   are collected from real doctor - patient conversations ,   it can reflect the data characteristics from EMRs .   On the other hand , for industrial applications , the   problem of extracting and structuring of EMRs   raised by the MIE dataset has become a fundamen-   tal task in downstream medical applications , such   as text - based dialogue systems or cascaded with   ASR ( Automatic Speech Recognition ) systems .   In the MIE dataset , the dialogues are already in   text format . As the dialogues turn to be too long ,   the medical dialogues are processed into pieces   using a sliding window . A window consists of mul-   tiple consecutive turns of a dialogue . The sliding   window size is set to 5 , because this size allows   the included dialogue turns contain proper amount   of information . For windows with less than 5 ut-   terances , the dataset pads them at the beginning   with empty strings . Then , it uses a window - to-   information annotation method , and annotates the   candidate item and its status in each window in   the dialogue . Annotators of the MIE dataset are   guided by two physicians to ensure the correctness   and the cohen ’s kappa coefficient of the labeled   data is 0.91 . It defines four categories ( i.e. symp-   tom , surgery , test , and other information ) and 71   candidate items which are frequent items in doctor-   patient dialogues and are fixed in the MIE dataset .   The candidate item has five statuses ( i.e. patient-   pos , patient - neg , doctor - pos , doctor - neg , patient-   unknown ) . In total , the corpus has 1,120 dialogues   and 18,212 windows . For the dialogue - level , the   dataset is split into three parts : training , validation   and testing , and the sizes are 800 , 160 , and 160 ,   respectively ; for the window - level , the correspond-   ing sizes are 12,932 , 2,587 , and 4,254 , respectively .   The detailed annotation statistics of the MIE dataset   are shown in Table 1 .   5.2 Evaluation Metrics   For the MIE dataest , we evaluate the extracted med-   ical dialogue information results with Precision ,   Recall andF1 - Score . In accordance with the evalu-   ation metrics described in the ( Zhang et al . , 2020 ) ,   a correct result should both correctly predict the   candidate item and its status . The results are evalu-   ated in window - level and dialogue - level as follows :   •Window - level . The evaluation is calculated   with each segmented window , and report the   micro - average of all the test windows .   •Dialogue - level . First , we merge the results of   windows belonging to the same conversation .   For mutually exclusive status , we update the   previous status with the latest status . Then ,   we evaluate the results of each dialogue and   report the micro - average of all test dialogues .   5.3 Experiment Settings   Task Training Settings   For the speaker recognition task , the label of the   speaker in each utterance is generated by the begin-   ning prompt words ( e.g. patient : ordoctor : ) . In   the training stage , we mask the beginning prompt   words to prevent the leakage of labels . For the sta-   tus entailment task , in addition to the origin status   labels ( e.g. patient - pos ) , we add the None status   label as the negative label . Because the candidate   item is not provided in the inference stage , thus   we have to traverse the candidate item space to   make a prediction . For a given dialogue and the   provided candidate item - status pair information ,   suppose there are Bcandidate items labels pre-   sented in a dialogue , we randomly select N×B   items which are not presented in the ground - truth   candidate items and label them with the None sta-   tus . In our experiments , we set Nas 2 . In the   inference stage , we make prediction on the whole   candidate item space . Only the candidate item with   non - None status is left for final evaluation .   Hyperparameter Settings   For the speaker - aware dialogue encoder module ,   we use a BERT - base network structure to initialize   the base dialogue encoder . The BERT - base ( 110 M )   namodel has 12 layers , the number of hidden state   dimensions is set to 768 , and the number of heads4782   is set to 12 . We use the Adam optimizer ( Kingma   and Ba , 2015 ) with a batch size of 32 for 20 epochs .   The learning rate αfor SAE pre - training is set to   2e-5 . The warmup proportion is set to 0.1 . The   maximum sequence length is 512 . The λfor con-   trolling the task weight is set to 0.5 with grid search   strategy . For the co - attention fusion module , the   number of hidden dimensions of the dense layer is   set to 64 , and the number of layers for utterance   propagation is set to 2 . The final inference module   is trained to minimize the cross - entropy loss on   the predicted label using the Adam optimizer with   a batch size of 128 for 15 epochs , and the initial   learning rate αfor co - attention fusion method is   set to 1e-3 . The models are trained on the NVIDIA   Tesla V100 32 GB GPU with 4 hours .   5.4 Model Comparisons   In this section , we compare our proposed frame-   work with several baselines to verify the effective-   ness of our approach .   •LSTM - Classifer The model only uses the   LSTM encoder to get the representation of   the concatenation of each utterance and uses a   self - attention layer and an MLP layer to make   predictions .   •MIE - Single ( Zhang et al . , 2020 ) The model   uses the LSTM model as the encoder mod-   ule , and only consider the interaction within a   single utterance .   •MIE - Multi ( Zhang et al . , 2020 ) The model   uses the LSTM model as an encoder module   and proposes a simple aggregate module to   take the utterance interaction into considera-   tion .   •MIE - Multi ( BERT ) The model architecture   is the same with the MIE - Multi , except that   we replace the original LSTM encoder with   the BERT encoder .   •SAFE ( Ours ) Our speaker - aware co - attention   framework takes the speaker ’s identity and the   correlations between utterances and candidate   items into consideration .   5.5 Main Results   In accordance with the evaluation metrics intro-   duced by Zhang et al . ( 2020 ) , we report both   window - level and dialogue - level results . Table 2   shows the performance comparisons with differ-   ent methods on the MIE dataset . We observe that   the LSTM - Classifier performs the worst , under the   dialogue - level metrics . The LSTM - Classifier only   has a precision of 61.34 and a recall of 52.65 , be-   cause it fails to consider interactions between each   utterance . The performance of the MIE - Multi is   better than the MIE - Single , as the latter model takes   the turn interactions into account . The MIE - Multi   achieves better performance at a precision of 76.83   and a recall of 64.07 under the dialogue - level met-   rics . The MIE - Multi is a state - of - art framework   for medical dialogue extraction . However , without   taking the speaker ’s identity into consideration , the   MIE - Multi can not tackle complex interactions be-   tween utterances and candidate items , it perform4783   less effectively compared to our SAFE framework .   For a more fair comparison , to eliminate the per-   formance boost brought by pre - trained language   models like BERT , we re - implement the MIE-   Multi with a BERT - based structure , the MIE - Multi   ( BERT ) gets an F1 - Score of 71.31 under window-   level metrics and an F1 - Score of 72.69 under the   dialogue - level metrics , which is better than the orig-   inal MIE - Multi , while still getting worse results   compare to our method . Our SAFE framework   achieves the state - of - the - art F1 - Score of 75.86   which demonstrates the superiority of our method   by a large margin .   5.6 Ablation Study   We conduct ablation studies on the MIE dataset   to analyze the contribution of each component of   our proposed SAFE model . The main results are   shown in Table 3 .   Effectiveness of Speaker - Aware Encoder   First , we evaluate the effect of the speaker - aware   encoder module . The removal of the SAE mod-   ule causes the overall performance of the F1-   score to decline from 75.86 to 73.29 under the   dialogue - level metrics , which suggests that taking   the speaker ’s information into account can help im-   prove the dialogue extraction performance . Addi-   tionally , to quantitatively demonstrate that the SAE   module can identify the speaker better , we calculate   the speaker misidentification error rate in the test   set , which indicates how many bad cases are owing   to the error of speaker identity ( e.g. , pred : doctor-   pos , label : patient - pos ) . The speaker misidentifi-   cation error rate is decreased from 5.0 % to 4.1 %   compared to the method without the SAF module .   Effectiveness of Co - Attention Fusion Module   Second , we evaluate the effect of the co - attention   fusion module . Removing the CAF module reduces   the overall performance of the F1 - score by 5.49 %   ( from 75.86 to 71.91 ) under the dialogue - level met-   rics , which proves that adopting the co - attention   graph network to capture the complex interactions   between the utterances is significant for medical   dialogue extraction . We also analyze the effect of   the different number of co - attention layers on the   performance of medical dialogue extraction . The   results are shown in Table 4 . Note that when the   co - attention layer is equal to 1 , the CAF is equiv-   alent to the flat attention over utterances . We can   discover from the table that the model with two   co - attention layers achieves the best result , which   indicates that the proper propagation of each utter-   ance can help to perceive complex interactions in   the medical dialogue .   5.7 Case Study   In previous sections , we provide a quantitative anal-   ysis of the experiment results . In this section , to   help better understand that our SAFE framework   can better capture utterance interactions in the dia-   logue , we provide a case study from the test set .   Figure 4 shows a case study on a patient - doctor   dialoguein the test set . To illustrate how our co-   attention fusion module can capture interactions be-   tween each speaker and the correlation with the can-   didate item , we visualize the utterance - utterance4784interaction with an attention map . From the Figure   5 , we can find that the third column ( Doctor : Do   you have a fever ? ) and the fourth column ( Patient :   No , everything is OK . ) of the matrix have domi-   nantly higher values , because these two utterances   are important for the model to extract the annotated   label ( fever : patient - negative ) . We can also dis-   cover that the co - attention coefficients ( i.e. ϕ   andϕ ) of these two utterances are also very high ,   because we need to consider the interactions be-   tween these two utterances to infer the ground - truth   status as patient - negative .   5.8 Discussion   Here we would like to give a brief discussion about   how the proposed system connects with clinical   practice . For text - based systems , the structured   information from the text - based dialogues can be   extracted to form the medical knowledge graph ,   which would benefit primary doctors . The struc-   tured information from medical dialogues can also   bring benefits for many clinical applications , such   as automatic diagnosis systems ( Liu et al . , 2018 ;   Xu et al . , 2019 ; Xia et al . , 2020 ) and clinical de-   cision support systems to assist doctors . For ASR   systems , it is also important to utilize the speaker   identity recognition in the system to facilitate medi-   cal information extraction after speech recognition .   6 Conclusion   In this paper , we propose a speaker - aware co-   attention framework for medical dialogue infor-   mation extraction . We design a speaker - aware   dialogue encoder module , which considers the   speaker ’s identity into account and can better uti-   lize the pre - trained language model to capture the   semantics of the utterance and the candidate item .   Moreover , we propose a co - attention fusion net-   work to aggregate the utterance information , which   tackles complex interactions between different ut-   terances and the correlation between utterances and   candidate items . The experiment results demon-   strate the effectiveness of the proposed framework .   7 Limitations   While perceiving the speaker ’s identity and com-   plex utterance interactions is essential for medical   dialogue information extraction , the limitation of   our work is that we do not explicitly involve the   prior medical knowledge such as the existing med-   ical knowledge graph ( MKG ) to further improvethe overall performance with less annotated labels .   To deal with the limitation , in the future , we should   leverage the medical entity relations in the medi-   cal knowledge graph , and introduce the medical   knowledge enhanced pre - train language model into   our work to further improve the results of medical   dialogue information extraction .   8 Ethical Considerations   It should be mentioned that the doctor - patient di-   alogues in the MIE dataset are collected from the   openly accessible online health forum Chunyu-   Doctor whose owners make such information visi-   ble to the public . All the patients ’ information has   been anonymized . Apart from the personal infor-   mation de - identified by the Chunyu - Doctor forum   officially , we manually reviewed the collected data   to prevent privacy leaks . We ensure there is no   identifiable or offensive information in the experi-   mental dataset .   The model and framework proposed in this pa-   per are for research purposes only and intended to   facilitate studies of using NLP methods to better   extract the structure information from medical dia-   logues , which can alleviate the doctor ’s burdens for   recording EMRs and accelerate the development   of medical digitization .   Acknowledgement   Our work is supported by the National Key   Research and Development Program of China   No.2020AAA0109400 .   References47854786