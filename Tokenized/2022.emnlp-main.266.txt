  Jie HuangHanyin ShaoKevin Chen - Chuan Chang   Jinjun XiongWen - mei HwuUniversity of Illinois at Urbana - Champaign , USAUniversity at Buffalo , USANVIDIA , USA   { jeffhj , hanyins2 , kcchang , w-hwu}@illinois.edu   jinjun@buffalo.edu   Abstract   Can machines know what twin prime is ? From   the composition of this phrase , machines may   guess twin prime is a certain kind of prime ,   but it is still difficult to deduce exactly what   twin stands for without additional knowledge .   Here , twin prime is a jargon – a specialized term   used by experts in a particular field . Explaining   jargon is challenging since it usually requires   domain knowledge to understand . Recently ,   there is an increasing interest in extracting and   generating definitions of words automatically .   However , existing approaches , either extrac-   tion or generation , perform poorly on jargon .   In this paper , we propose to combine extraction   and generation for jargon definition modeling :   first extract self- and correlative definitional in-   formation of target jargon from the Web and   then generate the final definitions by incorpo-   rating the extracted definitional information .   Our framework is remarkably simple but ef-   fective : experiments demonstrate our method   can generate high - quality definitions for jargon   and outperform state - of - the - art models signif-   icantly , e.g. , BLEU score from 8.76 to 22.66   and human - annotated score from 2.34 to 4.04 .   1 Introduction   Jargons are specialized terms associated with a par-   ticular discipline or field . To understand jargons ,   a straightforward approach is to read their defini-   tions , which are highly summarized sentences that   capture the main characteristics of them . For in-   stance , given jargon twin prime , people can know   its meaning by reading its definition : “ A twin prime   is a prime number that is either 2 less or 2 more   than another prime number . ”   Recently , acquiring definitions of words / phrases   automatically has aroused increasing interest .   There are two main approaches : extractive , corre-   sponding to definition extraction , where definitionsare extracted from existing corpora automatically   ( Anke and Schockaert , 2018 ; Veyseh et al . , 2020 ;   Kang et al . , 2020 ) ; and abstractive , corresponding   todefinition generation , where definitions are gen-   erated conditioned with the target words / phrases   and the contexts in which they are used ( Noraset   et al . , 2017 ; Gadetsky et al . , 2018 ; Bevilacqua et al . ,   2020 ; August et al . , 2022 ; Gardner et al . , 2022 ) .   In this paper , we study jargon definition mod-   eling , which aims to acquire definitions for jargon   automatically . Jargon definition modeling is impor-   tant since definitions of jargon are less likely to be   organized in an existing dictionary / encyclopedia   and such terms are difficult for non - experts to   understand without explanations ( Bullock et al . ,   2019 ) . This is particularly true for new jargon from   fast - advancing fields . For instance , neither Oxford   dictionary ( Butterfield et al . , 2016 ) nor Wikipedia   includes few - shot learning – an important setup in   machine learning .   However , to acquire definitions for jargon , both   extractive and abstractive approaches may fail . Ex-   tracting high - quality definitions would be difficult   due to the incompleteness and low quality of data   sources ( this issue is more serious for jargon since   jargon is usually less frequently used than general   words / phrases ) . E.g. , a good definition may not   be available in the corpus ; even if it existed , it   might be difficult to select from a large set of can-   didate sentences ( Kang et al . , 2020 ) . Generating   definitions for jargon would be challenging since   jargons are usually technical terms that need do-   main knowledge to understand , while the contexts   in which they are used can not provide sufficient   knowledge . For instance , it is almost impossible   for a model to generate the definition for twin prime   only with context “ proof of this conjecture would   also imply the existence of an infinite number of   twin primes ” since the context does not explain   twin prime , and the specific meaning is difficult3994   to infer from the surface form , leading to halluci-   nations , i.e. , generating irrelevant or contradicted   facts ( Bevilacqua et al . , 2020 ) . Consequently , ex-   isting models designed for general words / phrases   perform poorly on jargon . In our evaluation ( Tables   5 and 6 ) , we find most definitions produced by the   state - of - the - art model contain wrong information .   Fortunately , definition extraction and definition   generation can complement each other naturally .   On one hand , definition generator has the potential   to help the extractor by refining and synthesizing   the extracted definitions ; therefore , the extracted   sentences are not required to be perfect definitions   of the target jargon . On the other hand , definition   extractor can retrieve useful definitional informa-   tion as knowledge for the generator to produce   definitions of jargon . However , surprisingly , exist-   ing works are either extractive or abstractive , even   do not connect and compare them .   Therefore , in this work , we propose to combine   definition extraction and definition generation for   jargon definition modeling . We achieve this by in-   troducing a framework consisting of two processes :   extraction , where definitional information of jargon   is extracted from the Web ; and generation , where   the final definition is generated with the help of the   extracted definitional information .   We build models for extraction and generation   based on Pre - Trained Language Models ( Devlin   et al . , 2019 ; Lewis et al . , 2020a ; Brown et al . ,   2020 ) . Specifically , for extraction , we propose   a BERT - based definition extractor to extract self-   definitional information ( i.e. , definitional sentences   of the target jargon ) . We also suggest that relatedterms can help define the target jargon and lever-   age Wikipedia as the external knowledge source   to retrieve correlative definitional information ( i.e. ,   definitions of related terms ) . For generation , we de-   sign a BART - based definition generator to produce   the final definition by incorporating the extracted   knowledge . An example is shown in Figure 1 .   Our framework for jargon definition modeling   is remarkably simple that can easily be further   expanded by leveraging more advanced language   models , e.g. , we can replace the BART generator   with larger models such as Meta OPT ( Zhang et al . ,   2022 ) with a simple modification . Besides , since   our framework does not require a domain - specific   corpus or ontology like the ones used in Vanetik   et al . ( 2020 ) ; Liu et al . ( 2021 ) , it is easy to apply   to a variety of domains . Experimental results on   four datasets demonstrate our simple model out-   performs state - of - the - art models significantly ( e.g. ,   BLEU score from 8.76 to22.66 , human - annotated   score from 2.34 to4.04 ) .   Our contributions are summarized as follows :   •We report the first attempt to connect and com-   bine definition extraction and definition genera-   tion .   •We introduce jargon definition modeling and   solve it by incorporating both self- and correl-   ative definitional information of jargon .   •Experimental results show that our simple model   substantially outperforms SOTA models for defi-   nition modeling .   •We publish several datasets , along with defini-   tions ( e.g. , of ~75,600 computer science terms )   generated by our proposed model.39952 Related Work   Definition Extraction . Definition extraction ,   which aims to extract definitions from corpus auto-   matically , has been studied for a long period . Exist-   ing works for definition extraction can be roughly   divided into three categories : 1 ) rule - based , which   extracts definitions with defined linguistic rules   and templates ( Klavans and Muresan , 2001 ; Cui   et al . , 2004 ; Fahmi and Bouma , 2006 ) ; 2 ) machine   learning - based , which extracts definitions by sta-   tistical machine learning with carefully designed   features ( Westerhout , 2009 ; Jin et al . , 2013 ) ; 3 )   deep learning - based , the state - of - the - art approach   for definition extraction , which is based on deep   learning models such as CNN , LSTM , and BERT   ( Anke and Schockaert , 2018 ; Veyseh et al . , 2020 ;   Kang et al . , 2020 ; Vanetik et al . , 2020 ) .   Definition Generation . Definition generation , or   definition modeling , has aroused increasing inter-   est in recent years . The first study on definition   generation was presented in Noraset et al . ( 2017 ) ,   which aims to generate definitions of words with   word embeddings . Later works on definition gener-   ation put more emphasis on generating definitions   of words / phrases with given contexts ( Gadetsky   et al . , 2018 ; Ishiwatari et al . , 2019 ; Washio et al . ,   2019 ; Mickus et al . , 2019 ; Li et al . , 2020 ; Reid   et al . , 2020 ; Bevilacqua et al . , 2020 ; Huang et al . ,   2021a ) . For example , Bevilacqua et al . ( 2020 ) ap-   ply pre - trained BART ( Lewis et al . , 2020a ) for def-   inition generation with a simple context encoding   scheme . Huang et al . ( 2021a ) employ three T5 mod-   els ( Raffel et al . , 2020 ) for definition generation   with a re - ranking mechanism to model specificity   of definitions . Liu et al . ( 2021 ) study the graph-   aware definition modeling problem by incorporat-   ing biomedical ontology . August et al . ( 2022 ) study   the problem of generating definitions of scientific   and medical terms with varying complexity . Huang   et al . ( 2022 ) propose to generate definitional - like   sentences to describe relations between entities .   There are also recent works on definition modeling   for other languages , e.g. , Chinese , by incorporat-   ing the special properties of the specific language   ( Yang et al . , 2020 ; Zheng et al . , 2021 ) .   However , although definition extraction and def-   inition generation are quite relevant tasks , surpris-   ingly , existing works do not connect and compare   them . In this work , we report the first attempt to   combine them.3 Methodology   Our framework for jargon definition modeling con-   sists of two processes : extraction , which extracts   self- and correlative definitional information of the   target jargon from the Web ; and generation , which   generates the final definition by incorporating the   extracted definitional information . The overview   of the framework is shown in Figure 1 .   3.1 Extraction   3.1.1 Self - Definitional Information   Since jargons are specialized terms used in a par-   ticular field , to understand jargon , we need back-   ground knowledge of jargon . To acquire useful   information for defining jargon , it is natural to refer   to definitional sentences containing the target jar-   gon , named Self - Definitional Information ( SDI ) .   We achieve SDI by first extracting sentences con-   taining the target jargon from the Web ( more details   are in Section 4.1 ) and then using a classifier to rank   the extracted sentences .   To build the classifier , we apply the BERT model   ( Devlin et al . , 2019 ) , which has achieved excel-   lent results on various text classification tasks . We   adopt a simple encoding scheme , which is “ [ CLS ]   jargon [ DEF ] sentence ” , e.g. , “ [ CLS ] machine   learning [ DEF ] machine learning is the study of   computer algorithms that improve automatically   through experience and by the use of data . ” The   final hidden state of the first token [ CLS ] is used   as the representation of the whole sequence and a   classification layer is added . After fine - tuning on   the jargon - sentence pairs , the model has a certain   ability to distinguish whether the sentence contains   representative definitional information of the target   jargon . SDI is then obtained as the top definitional   sentences by ranking the sentences according to   the confidence of the prediction . We refer to this   model as SDI - Extractor .   3.1.2 Correlative Definitional Information   To explain a jargon , in addition to utilizing SDI ,   we can also refer to the definitions of its related   terms , i.e. , Correlative Definitional Information   ( CDI ) . For instance , to define few - shot learning , we   can incorporate definitions of zero - shot learning   andmeta learning , with which we can know the   meaning of “ shot ” and “ learning ” and may define   few - shot learning similarly to zero - shot learning .   To get related terms and their definitions , we   leverage Wikipedia as the external knowledge3996source , which covers a wide range of domains and   contains high - quality definitions for a large number   of terms . Specifically , we follow the core - fringe   notion in Huang et al . ( 2021b ) , where core terms   are terms that have corresponding Wikipedia pages ,   andfringe terms are ones that are not associated   with a Wikipedia page . For each jargon , we treat it   as query to retrieve the most relevant core terms via   document ranking based on Elasticsearch ( Gorm-   ley and Tong , 2015 ) , and extract first sentences on   the corresponding Wikipedia pages as the defini-   tions of related terms . We refer to this model as   CDI - Extractor .   3.2 Generation   After extraction , we acquire the self- and correla-   tive definitional information of jargon . This kind of   information captures important characteristics of   jargon and can be further refined and synthesized   into the final definition by a definition generator .   Definition generation can be formulated as a con-   ditioned sentence generation task – generating a   coherent sentence to define the target jargon . For-   mally , we apply the standard sequence - to - sequence   formulation : given jargon x , combining with the   extracted sentences S(for SDI ) and S(for CDI ) ,   the probability of the generated definition dis com-   puted auto - regressively :   P(d|x , S , S ) = /productdisplayP(d|d , x , S , S ) ,   where mis the length of d , dis the ith token of d ,   anddis a special start token .   Following Bevilacqua et al . ( 2020 ) , to build the   generator , we employ BART ( Lewis et al . , 2020a ) ,   a pre - trained transformer - based encoder - decoder   model that can be fine - tuned to perform specific   conditional language generation tasks with specific   training input - output pairs . Different from exist-   ing works ( Gadetsky et al . , 2018 ; Ishiwatari et al . ,   2019 ; Bevilacqua et al . , 2020 ) which aim to learn   to define a word / phrase in a given context , we pro-   pose to learn to define a jargon using the extracted   knowledge . To be specific , we aim to fine - tune the   BART model to generate the definition of the target   jargon based on the surface name of the jargon and   the extracted definitional information .   To apply the BART model , for a target jargon ,   we adopt the following encoding scheme : “ jargon   [ DEF ] sent[SEP ] sent ... [ SEP ] sent[DEF ]   sent[SEP ] sent ... [ SEP ] sent ” , where sentandsentare the ith sentences ranked by SDI-   Extractor andCDI - Extractor , respectively . We   fine - tune BART to produce the ground - truth defini-   tion conditioned with the encoded input .   After training , given a new jargon , we get cor-   responding SDI and CDI according to Section 3.1 .   We encode the jargon and the top kranked sen-   tences of SDI and top kranked sentences of CDI   as described above and use the generator to produce   the final definition . We refer to this model as CDM-   Sk , Ck , i.e. , Combined Definition Modeling .   Here we would like to mention that our com-   bined definition modeling framework is modular   and can be applied to different extractor - generator   combinations commonly proposed for definition   extraction / generation , which means that the pro-   posed framework can improve the performance for   a variety of definition modeling systems . For in-   stance , we can replace the BART generator with   GPT-2/3 generator ( Radford et al . , 2019 ; Brown   et al . , 2020 ) or DMAS ( Huang et al . , 2021a ) by   simply modifying the encoding scheme .   4 Experiments   4.1 Datasets   Existing datasets for definition modeling are mainly   for general words / phrases . In this paper , we build   several datasets ( UJ - CS , UJ - Math , UJ - Phy ) for   jargon based on Wikipedia and CFL ( Huang et al . ,   2021b ) . Compared to general words / phrases , jar-   gons are less ambiguous but more specialized , i.e. ,   a jargon usually only has one meaning , but it re-   quires domain knowledge to understand . We also   conduct experiments on the dataset ( Sci&Med )   provided in August et al . ( 2022 ) , which contains   definitions of scientific and medical terms derived   from Wikipedia science glossaries and MedQuAD   ( Ben Abacha and Demner - Fushman , 2019 ) .   Definition Extraction . We build a dataset for   jargon definition extraction with Wikipedia . We   first collect jargons with Wikipedia Category .   Specifically , we traverse from three root cate-   gories , including Category : Subfields of computer   science , Category : Fields of mathematics , and   Category : Subfields of physics , and collect pages3997   at the first three levels of the hierarchies . For each   page , we process the title with lemmatization as   the jargon , extract the first sentence in the summary   section as the corresponding definition , and sam-   ple≤5sentences containing the target jargon from   other sections as negatives ( they are less likely to be   definitional sentences ) . We filter out jargons with   surface name frequency < 5 in the arXiv corpus   ( to filter out some noisy phrases , e.g. , List of arti-   ficial intelligence projects ) . The dataset contains   26,559 positive and 121,975 negative examples ,   and the train / valid / test split is 0.8/0.1/0.1 .   Definition Generation . Following ( Huang et al . ,   2021b ) , we focus on generating definitions for jar-   gon in three fields : computer science ( UJ - CS ) ,   mathematics ( UJ - Math ) , and physics ( UJ - Phy ) .   We collect jargons in two ways . For computer   science , we collect jargons ( author - assigned key-   words ) by web scraping from Springer publications   on computer science . We filter out jargons with   frequency < 5 . For mathematics and physics , we   collect jargons with the CFL model proposed in   Huang et al . ( 2021b ) . Specifically , we collect terms   with domain relevance score > 0.5as jargons . For   each jargon in the list , URLs of the top 20 results   from Google search are visited . Then the sentences   containing the target jargon are extracted . For train-   ing and evaluation , we only keep jargons that have   a corresponding Wikipedia page and extract the   first sentence on each page as the ground - truth def-   inition . Table 1 summarizes the statistics of the   data .   4.2 Experimental Setup   Baselines . For extraction , we compare SDI-   Extractor with a CNN baseline and a CNN-   BiLSTM baseline proposed in Anke and Schock-   aert ( 2018 ) . Here we should mention that the more   recent models ( Veyseh et al . , 2020 ; Kang et al . ,   2020 ) can not be compared directly since these   works focus on a fine - grained sequence labeling   task , where the training data also requires addi-   tional labeling . Besides , extraction is not the focus   of this paper ; therefore , we put more emphasis on   the evaluation for generation . For generation , we   evaluate on the following models :   •Gen ( w/o context ) : A simple version of Gener-   ationary ( Bevilacqua et al . , 2020 ) , where BART   ( Lewis et al . , 2020a ) is fine - tuned on jargon-   definition pairs .   •Gen ( w/ context ) : Generationary with a sentence   containing the target jargon as context , where   BART is fine - tuned on context - definition pairs .   •DMAS ( Huang et al . , 2021a ): A definition mod-   eling model with three T5 ( Raffel et al . , 2020 ) ,   where a re - ranking mechanism is included to   model the specificity of definitions . Context is   given by a sentence containing the target jargon .   •BART NO SD and BART SD : For the Sci&Med   dataset ( August et al . , 2022 ) , we also compare   with the two best methods introduced in their   paper : BART SD , where BART is fine - tuned   with the term question , e.g. , What is ( are ) carbon   nanotubes ? , concatenated with the supporting   document ; and BART NO SD , where BART is   fine - tuned with just the question and definition ,   without the support documents .   •Extractive : An extractive baseline , which out-   puts the candidate definition with the highest con-   fidence score predicted by SDI - Extractor ( Sec-   tion 3.1.1 ) .   •CDM - S k , Ck : The combined definition model-   ing model introduced in Section 3.2 . S kor Ck   is omitted when korkis equal to 0 .   Metrics . For extraction , we use the standard pre-   cision , recall , and F1 scores to evaluate the perfor-   mance . For generation , we follow Bevilacqua et al .   ( 2020 ) and apply several automatic metrics , includ-   ing BLEU ( BL)(Papineni et al . , 2002 ) , ROUGE-   L ( R - L ) ( Lin , 2004 ) , METEOR ( MT ) ( Banerjee   and Lavie , 2005 ) , and BERTScore ( BS ) ( Zhang   et al . , 2019 ) . BLEU , ROUGE - L , and METEOR   focus on measuring surface similarities between   the generated definitions and the ground - truth def-   initions , and BERTScore is based on the similar-   ities of contextual token embeddings . The signa-3998   ture of BERTScore is : roberta - large - mnli L19 no-   idf version=0.3.0(hug trans=2.8.0 ) . We also ask   three human annotators ( graduate students doing   research on computational linguistics ) to evaluate   the output definitions with a 1 - 5 rating scale used   in Ishiwatari et al . ( 2019 ): 1 ) completely wrong or   self - definition ; 2 ) correct topic with wrong infor-   mation ; 3 ) correct but incomplete ; 4 ) small details   missing ; 5 ) correct .   Implementation Details . For SDI extraction , we   adopt BERT - base - uncased from huggingface trans-   formers framework ( Wolf et al . , 2020 ) . We apply   the BertForSequenceClassification in huggingface   ( with a linear layer on top of the pooled output ) . We   use the default hyperparameters and fine - tune the   model using Adam ( Kingma and Ba , 2015 ) with   learning rate of 2×10 . All the layers of the   BERT model are fine - tuned . For the two baselines ,   we train the models on our data with the official   implementation . For the extracted SDI , we exclude   sentences from Wikipedia to avoid the models to   see the ground truth .   For CDI extraction , following Huang et al.(2021a ) , we use the built - in Elasticsearch - based   Wikipedia search engineto collect related core   terms for jargon ; and then , we extract the first sen-   tence on the corresponding Wikipedia page as the   definition of each related term .   For generation , we employ the fairseq libraryto   build the BART - base generator and adopt the hyper-   parameters and settings as suggested in Bevilacqua   et al . ( 2020 ) . We set the learning rate as 5×10   and use batch size of 1,024tokens , updating every   16iterations , with the number of warmup steps as   1,000 . For all the datasets , we use the same trained   SDI - extractor as described above to extract SDI .   We adopt the default / suggested hyperparameters   for the baselines . We train and evaluate all the base-   lines and variants on the same train / valid / test split   on NVIDIA Quadro RTX 5000 GPUs . The training   of CDM can be finished in one hour .   4.3 Definition Extraction   Table 2 reports the results of definition extraction .   We observe that SDI - Extractor outperforms base-   lines significantly and the performance is quite sat-   isfactory ( with an F1 score higher than 0.97 ) , which   indicates our definition extractor can extract useful   self - definitional information for jargon .   4.4 Definition Generation   We provide both quantitative and qualitative evalu-   ations for definition generation .   4.4.1 Automatic Evaluation   Tables 3 and 4 show the results on automatic met-   rics . We observe the proposed CDM model out-3999   performs the SOTA baselines significantly . Com-   paring Gen ( w/ context ) with Gen ( w/o context ) ,   we find contexts ( random sentences containing   the target jargon ) only have limited help with jar-   gon definition modeling . Besides , CDM - S 5out-   performs CDM - S 3 , while CDM - S 3outperforms   CDM - S 1 , which means the sentences extracted by   SDI - Extractor can provide important definitional   information . Comparing CDM - C 5with Gen ( w/   context ) and Gen ( w/o context ) , we can verify CDI   is also helpful for definition generation , while the   improvement is not as significant as the models   with SDI , e.g. , CDM - S 5 . Among all the models ,   CDM - S 5,C5usually achieves the best performance ,   which demonstrates the combination of SDI and   CDI is the most significant for jargon definition   modeling .   An interesting finding is that our simple extrac-   tive model is comparable to the SOTA abstrac-   tive baselines ( except for Table 4 , because most   of the definitions in the dataset are not complete   sentences , e.g , “ the science of automatic control   systems ” for cybernetics , while SDI - Extractor usu-   ally extracts complete sentences ) . We suppose this   is because , compared to general words / phrases , jar-   gons are more difficult to define without external   knowledge . For instance , it is almost impossible   for a model to generate the definition for twin prime   only with context “ proof of this conjecture would   also imply the existence of an infinite number of   twin primes ” , while the definition can possibly be   retrieved from the Web . The results also demon-   strate that existing context - aware definition mod-   eling systems are hard to handle jargon , while our   proposed extraction - generation framework is quite   practical for jargon definition modeling .   4.4.2 Human Evaluation   We conduct human evaluation for the computer   science field ( UJ - CS ) . Specifically , we randomly   sample 50 jargons from the test set , and ask three   human annotators to evaluate the definitions pro-   duced by different models with the rating scale   described in Section 4.2 . Table 5 reports the human   evaluation results , where the average pairwise Co-   hen ’s κis 0.69 ( good agreement ) . We observe the   state - of - the - art baseline Gen ( w / context ) is difficult   to generate reasonable definitions for jargon . In   contrast , the proposed CDM - S 5,C5model can pro-   duce high - quality definitions in most cases ( with   a human - annotated score higher than 4 ) . The hu-   man evaluation results are also consistent with the   automatic evaluation results presented in Table 3 .   4.5 Sensitivity to Frequency   To investigate the sensitivity of the models with   respect to the popularity of jargon , we report the   results according to jargon frequency in Figure 2 .   We observe that Generationary ( Bevilacqua et al . ,   2020 ) achieves slightly worse performance for less   popular jargon on all metrics , while CDM performs   well for low - frequency jargon , which indicates our   framework can produce high - quality definitions for   long - tail jargon . We suppose this is because , al-   though long - tail jargon is less frequent , we can   still extract useful definitional information from   the entire Web and incorporate it for definition gen-   eration .   4.6 Generation Examples and Error Analysis   In Table 6 , we show some sample outputs in the test   set of three models : Extractive , Gen ( w/ context ) ,   andCDM - S 5,C5 , with ground - truth definitions in   Wikipedia ( Gold ) as references .   From the results , we observe although the extrac-   tive baseline can produce reasonable sentences , the   output sentences may not be high - quality defini-   tional sentences of the target jargon . For instance,4000   the extracted sentence for wear leveling in fact is   the definition of preemptive wear leveling . We also   find Gen ( w/ context ) suffers severely from halluci-   nations , i.e. , generating irrelevant or contradicted   facts . For instance , gittins index is described as a   decision - making tool instead of a measure / value ,   which is completely wrong . This is mainly because   the contexts of jargon may not provide sufficient   knowledge to define jargon . In contrast , the quality   of definitions generated by CDM - S 5,C5is high –   all the generated definitions capture the main char-   acteristics of the target jargon correctly .   Error Analysis . To further understand the results   and identify the remaining challenges , we analyze   the human evaluation results . We find that errors   could be introduced in either the extraction or the   generation process . E.g. , 1 ) for intelligent user in-   terfaces in Table 6 , the top 1 sentence extracted   by SDI - Extractor ( “ ACM IUI ... interfaces . ” ) can-   not provide meaningful knowledge to the generator .   Although by incorporating other sentences , CDM-   S5,C5can generate a reasonable definition , the def-   inition still contains minor errors . 2 ) For markup   languages , although SDI - Extractor extracts reason-   able definitions ( e.g. , “ Markup languages are lan-   guages used by a computer to annotate a docu-   ment . ” ) , the generator mistakenly synthesizes the   SDI and CDI into “ A markup language is a se - ries of tags mixed with plain text . ” Nonetheless ,   compared to existing models that do not combine   extraction and generation , CDM greatly reduces   hallucination .   5 Discussion   In this work , we focus on jargon definition mod-   eling . The proposed framework can be further ex-   tended to general words / phrases in a context - aware   setting ( Gadetsky et al . , 2018 ) . For instance , to re-   trieve the definitional information , we can incorpo-   rate the context the target word / phrase used in . E.g. ,   the BERT extractor can be trained with a modified   encoding scheme : “ [ CLS ] word / phrase [ SEP ]   context [ DEF ] sentence ” . Similarly , the genera-   tor can produce the final definition conditioned on   the context . E.g. , the input of the generator can be   encoded as “ word / phrase [ SEP ] context [ DEF ]   sent[SEP ] sent ... [ SEP ] sent[DEF ] sent   [ SEP ] sent ... [ SEP ] sent ” . Since our frame-   work is modular , the BERT extractor and BART   generator can also be replaced with more advanced   language models . It is also interesting to train the   extractor and generator jointly or iteratively ( Guu   et al . , 2020 ; Lewis et al . , 2020b ) . We keep the   proposed model simple and leave context - aware   combined definition modeling and more compli-   cated combinations as future work.40016 Conclusion   We present the first combination of definition ex-   traction and definition generation . We show that ,   by incorporating extracted self- and correlative def-   initional information , the generator can produce   high - quality definitions for jargon . Experimental   results demonstrate the effectiveness of our frame-   work , where the proposed method outperforms re-   cent baselines by a large margin . We also publish   several datasets for jargon definition modeling . In   future work , we plan to improve our framework as   discussed in Section 5 and apply our methods to   construct several online domain dictionaries .   Limitations   One limitation of this paper is that it does not con-   sider the diversity of definitions . Definitions from   different perspectives can facilitate a more com-   prehensive understanding . For instance , to define   artificial intelligence , we may relate it to or contrast   it with other concepts , e.g. , “ artificial intelligence   refers to systems or machines that mimic human   intelligence to perform tasks and can iteratively   improve themselves based on the information they   collect . ” or “ artificial intelligence is intelligence   demonstrated by machines , as opposed to the natu-   ral intelligence displayed by animals including hu-   mans . ” Recent work starts to model the specificity   and complexity for definition modeling ( Huang   et al . , 2021a ; Gardner et al . , 2022 ) ; however , the di-   versity of generative definitions is still limited . We   believe our framework can benefit diversity since   the generator has the potential to generate defini-   tions with different styles by incorporating diverse   definitional information extracted from the Web .   Acknowledgements   We thank the reviewers for their constructive feed-   back . This material is based upon work supported   by the National Science Foundation IIS 16 - 19302   and IIS 16 - 33755 , Zhejiang University ZJU Re-   search 083650 , IBM - Illinois Center for Cognitive   Computing Systems Research ( C3SR ) – a research   collaboration as part of the IBM Cognitive Horizon   Network , grants from eBay and Microsoft Azure ,   UIUC OVCR CCIL Planning Grant 434S34 , UIUC   CSBS Small Grant 434C8U , and UIUC New Fron-   tiers Initiative . Any opinions , findings , and conclu-   sions or recommendations expressed in this publi-   cation are those of the author(s ) and do not neces-   sarily reflect the views of the funding agencies . References400240034004