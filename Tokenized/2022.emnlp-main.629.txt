  Mahshid Hosseini Cornelia Caragea   Computer Science   University of Illinois at Chicago   mhosse4@uic.edu cornelia@uic.edu   Abstract   Knowledge Distillation ( KD ) is an effective   method to transfer knowledge from one net-   work ( a.k.a . teacher ) to another ( a.k.a . student ) .   In this paper , we study KD on the emotion-   related tasks from a new perspective : calibra-   tion . We further explore the impact of the   mixup data augmentation technique on the dis-   tillation objective and propose to use a sim-   ple yet effective mixup method informed by   training dynamics for calibrating the student   models . Underpinned by the regularization im-   pact of the mixup process by providing better   training signals to the student models using   training dynamics , our proposed mixup strat-   egy gradually enhances the student model ’s   calibration while effectively improving its per-   formance . We evaluate the calibration of pre-   trained language models through knowledge   distillation over three tasks of emotion detec-   tion , sentiment analysis , and empathy detection .   By conducting extensive experiments on differ-   ent datasets , with both in - domain and out - of-   domain test sets , we demonstrate that student   models distilled from teacher models trained   using our proposed mixup method obtained the   lowest Expected Calibration Errors ( ECEs ) and   best performance on both in - domain and out-   of - domain test sets .   1 Introduction   It has been shown that transferring knowledge from   a teacher model with desired high performance to   a student model , through knowledge distillation ,   can lead to better performance of student models   distilled ( Furlanello et al . , 2018 ; Yim et al . , 2017 ) .   However , little is known about the impact of the   distillation process on the calibration of the student   model . Evaluating the uncertainty of a model ’s   predictions is crucial , specifically in applications   where the cost of an error is high . For instance , in   a computer - assisted therapy session , an accurate   and calibrated emotion or empathy detection model   can inform the doctor when a model ’s predictionsshould ( or should not ) be trusted , which is help-   ful for them in deciding the preferred treatment   for patients . In this work , we aim to shed light on   the impact of knowledge distillation on the calibra-   tion of the student models on emotion - related tasks .   Calibration measures the discrepancy between the   correctness of the prediction ( i.e. , accuracy ) and   the ( empirical ) probability that a model assigns to   a prediction ( i.e. , confidence ) . A well - calibrated   model knows how often it is correct or wrong ; pre-   dicting an event with pconfidence shall empirically   be true pof the time ( Guo et al . , 2017 ) .   Recently , a large body of work has investigated   why neural networks have become miscalibrated   ( Platt et al . , 1999 ; Niculescu - Mizil and Caruana ,   2005 ; Nguyen and O’Connor , 2015a ; Kuleshov   and Liang , 2015 ; Kuleshov and Ermon , 2016 ; Guo   et al . , 2017 ; Desai and Durrett , 2020 ) . More re-   cent attention , however , has focused on methods   to alleviate this problem . Specifically on natural   language processing tasks , Guo et al . ( 2017 ) pro-   posed a simple extension of Platt scaling ( Platt   et al . , 1999 ) that softens the softmax by a learned   scalar parameter which can effectively calibrate   probabilistic models . Pereyra et al . ( 2017a ) ; Müller   et al . ( 2019 ) ; Desai and Durrett ( 2020 ) also showed   that regularization techniques such as label smooth-   ing could prevent over - confident predictions and   result in better model calibration .   Along these lines , in this paper , we empirically   examine the impact of the mixed sample data aug-   mentation technique , Mixup ( Zhang et al . , 2018 ) ,   on the performance and calibration of the student   models in a distillation setup and propose a simple   yet effective mixup strategy to attain more accu-   rate and better - calibrated models . Mixup ( Zhang   et al . , 2018 ) is a popular data augmentation and   regularization technique that generates a weighted   combination of random input pairs from the train-   ing data . It has been empirically shown that mixup   can hone the accuracy and calibration of the pre-9266diction emanating from the desirable regularization   effects it induces ( Carratino et al . , 2020 ; Zhang   et al . , 2018 ; Thulasidasan et al . , 2019 ) . By em-   ploying mixup , the goal is to provide the teacher   models with more useful information and impart   the student models with better supervision signals   during the distillation of the emotion - related mod-   els . In addition , mixup may introduce some noise   to the training data ( as real - world emotion - related   datasets ) , which enables us to gain additional in-   formation about relatively similar data . This , in   turn , makes teacher models more robust , helping   the student models to be more accurate and produce   better - calibrated predictions .   While mixup is making significant inroads in   a broad range of tasks ranging from computer vi-   sion ( Zhang et al . , 2018 ; Thulasidasan et al . , 2019 ;   Carratino et al . , 2020 ; Wang et al . , 2020a ) to nat-   ural language processing ( Guo et al . , 2019 ; Guo ,   2020 ; Chen et al . , 2020 ; Yin et al . , 2021 ; Kong   et al . , 2020 ; Liang et al . , 2021 ) , there has hitherto   been a limited number of works focusing on its   effectiveness on model calibration specifically in   NLP ( Kong et al . , 2020 ; Park and Caragea , 2022 ) .   With that caveats , what is not yet studied is using   mixup for calibrating the student model predictions   on the knowledge distillation setting ; that is what   this paper focuses on .   In this paper , we study , for the first time to our   knowledge , the impact of the mixup data augmen-   tation technique on the distillation objective and   propose a simple yet effective mixup strategy that   is informed by training dynamics ( Swayamdipta   et al . , 2020 ) for calibrating the student models .   To this end , we first characterize data instances   based on their contributions to the model ’s learn-   ing , which yields distinct regions in the data , pre-   senting easy - to - learn , ambiguous , or hard - to - learn   instances . Then , we generate mixup samples by in-   terpolating easy - to - learn with ambiguous samples   as a regularization technique to promote general-   ization to both in - domain ( ID ) and out - of - domain   ( OOD ) test sets and improve the student model cal-   ibration . While ambiguous / hard - to - learn instances   are intuitively the most challenging yet informative   for learning , easy - to - learn instances are essential   for convergence ( Swayamdipta et al . , 2020 ) . There-   fore , interpolating samples from different regions   ( e.g. , easy - to - learn with ambiguous ) in the teacher   model can potentially result in a better - calibrated   student model with improved ID and OOD perfor - mance . To contextualize examples in our datasets   based on training dynamics , we utilize data maps   ( Swayamdipta et al . , 2020 ) . Data maps is a model-   based tool that characterizes datasets based on the   model ’s behavior on each of the instances . By lever-   aging training dynamics , data maps estimates two   measures , i.e. , confidence and variability , the mean   and standard deviation of the ground - truth proba-   bilities , predicted for each instance across training   epochs .   We further experimentally explore the effect of   popular regularization techniques like temperature   scaling ( Guo et al . , 2017 ) and label smoothing   ( Pereyra et al . , 2017a ) along with our informed   mixup on the calibration of the student models   in a teacher - student training set - up . We carried   out extensive experiments to evaluate the proposed   informed mixup data augmentation technique by   creating teacher networks on two pre - trained mod-   els , BERT ( Devlin et al . , 2019 ) , and RoBERTa ( Liu   et al . , 2019 ) . Student networks will then be distilled   and evaluated against several data sets on three dif-   ferent text classification tasks , including emotion   detection ( Demszky et al . , 2020 ) , sentiment anal-   ysis ( Zhang et al . , 2015 ) , and empathy detection   ( Sharma et al . , 2020 ) . Our contributions are thus   summarized as follows :   •We show that the dark knowledge of a pre-   trained language teacher model can act as a   regularization process , helping to calibrate the   student model ’s confidence in its predictions .   •We demonstrate that using training dynam-   ics to inform the interpolation process in the   mixup data augmentation on a teacher model   can effectively improve the calibration of the   student model in a distillation setting . Based   on the confidence and variability of each ex-   ample , we divide training samples into distinct   categories where we propose to mix easy - to-   learn and ambiguous samples in the teacher   model for the student model calibration .   •We also examine the performance of the dis-   tilled student models under distributional shift ,   and show the effectiveness of the informed   mixup method to coax the student model into   generating more calibrated predictions .   •Through extensive experiments , we show that   student models distilled from teacher models   trained using our proposed mixup are not only   more accurate but also better - calibrated on9267both in - domain and out - of - domain test sets   than strong baselines on different text classifi-   cation tasks of emotion detection , sentiment   analysis , and empathy detection .   2 Related Work   Knowledge Distillation : Knowledge distillation   ( KD ) is an efficient method broadly used for trans-   ferring knowledge from a teacher network to a   student network . In the knowledge distillation   setting , a student model is trained to obtain the   knowledge of a deeper or more complex teacher   model and can therefore estimate the capacity of   the powerful teacher model by incorporating the   extra knowledge . KD was first introduced as an   approach to compress large networks into smaller   networks ( Ba and Caruana , 2014 ; Bucilu ˇa et al . ,   2006 ) for computational efficiency . The advances   of KD , however , go beyond model compression .   Zhang and Sabuncu ( 2020 ) empirically explained   the reason behind the enhanced performance of   self - distillation and proposed a framework that em-   ploys instance - specific regularization for teacher   predictions . Phuong and Lampert ( 2019 ) exam-   ined the impact of distillation on student mod-   els by analyzing linear and deep linear classifiers .   Unlike previous works , we are interested in an-   alyzing the impact of knowledge distillation on   the calibration of the models . Thus , we examine   the calibration of large - scale pre - trained models   through knowledge distillation . We further analyze   the impact of dataset shift on calibration for all   these settings . We evaluate the predictive uncer-   tainty on both in - domain and out - of - domain test   sets from known and unknown distributions on   emotion - related datasets .   Mixup : Mixup ( Zhang et al . , 2018 ) was first pro-   posed to improve the generalization of deep neural   networks in computer vision . Since then , many   studies have explored mixup in natural language   processing tasks ( Guo et al . , 2019 ; Guo , 2020 ;   Chen et al . , 2020 ; Yin et al . , 2021 ; Kong et al . ,   2020 ; Liang et al . , 2021 ) . Liang et al . ( 2021 ) pro-   posed a data - agnostic distillation framework that   leverages mixup to confer the student model with   better generalization ability . Kong et al . ( 2020 ) ex-   amined BERT calibration using mixup by generat-   ing augmented samples based on a cosine distance   of extracted features . Park and Caragea ( 2022 ) also   improved pre - trained language models calibration   by leveraging Area Under the Margins ( Pleiss et al . ,2020 ) along with saliency maps ( Simonyan et al . ,   2014 ) to generate mixup samples . In contrast to   these works , we study the impact of mixup data   augmentation technique on the distillation objec-   tive .   Calibration : Calibration and uncertainty of the   models have been investigated on several natural   language processing tasks , including question an-   swering ( Zhang et al . , 2021 ) , neural machine trans-   lation ( Lu et al . , 2021 ; Müller et al . , 2019 ; Kumar   and Sarawagi , 2019 ; Wang et al . , 2020b ) , language   understanding ( Desai and Durrett , 2020 ) , estimat-   ing proportions from annotations ( Card and Smith ,   2018 ) , and coreference resolution ( Nguyen and   O’Connor , 2015b ) . Ovadia et al . ( 2019 ) provided   a benchmark of models on image and text classifi-   cation tasks and explored the influence of distribu-   tional shift on accuracy and calibration . Focusing   on the pre - trained models , Desai and Durrett ( 2020 )   examined calibration over three tasks of paraphrase   detection , natural language inference , and common-   sense reasoning . Unlike these works , we study the   calibration of emotion - related tasks through knowl-   edge distillation and propose a mixup strategy to   enhance the performance and calibration of the   Transformer - based student models .   3 Methods   3.1 Teacher - Student Training   Given a k - class classification task and a datasetconsisting of sentence - label pairs ,   standard supervised learning is optimized based on   the one - hot labels by minimizing the cross - entropy   lossLof training data which is defined as :   where pindicates the softmax outputs . In knowl-   edge distillation , a teacher - student training method   is employed to enhance the performance of the   student model where the softmax outputs of the   teacher model , p , is computed as :   where τis the softmax temperature , and zis the log-   its from the teacher model . In general , the knowl-   edge distillation framework ( Hinton et al . , 2015 )   incorporates the knowledge obtained from the log-   its of a teacher model and transfers the knowledge9268   to a small student model . In this way , better train-   ing signals can be retrieved from the data using a   teacher - student framework . This is done by mini-   mizing the sum of cross - entropy loss between hard   labels and student ’ predictions and the difference   loss between the student ’s and teacher ’s predic-   tions :   whereLis Kullback - Leibler ( KL ) divergence   loss , and α∈[0,1]is the hyper - parameter that   controls the impact of cross - entropy loss and the   KL divergence loss .   Self - distillation is a particular case of teacher-   student training where both the teacher and student   models have the same architecture . For example , in   Figure 1 we have both teacher and student models   based on RoBERTa .   3.2 Mixup Training   In an attempt to provide the teacher models with   more useful information and impart the student   models with better supervision signals during the   distillation of the emotion - related models , we em-   pirically examine standard mixup and propose a   simple yet effective strategy to hone the perfor - mance and calibration of the student model in a   distillation setup .   Given a training dataset of sentence - label pairsand a language model f , stan-   dard mixup creates the vicinal dataset by calculat-   ing a weighted average of training points based on   the following simple rule by ( Zhang et al . , 2018 ):   where ( x , y)and(x , y)are two input exam-   ples that are randomly drawn from the training set ,   and weight λis sampled from a beta distribution ,   β(α , α)with parameter α > 0 , generally taken to   be relatively small , so that the weighted averages   do not stray too far from the original data points .   Mixup augments the training data by linearly inter-   polating training samples and their corresponding   labels in the input space .   We propose to use a novel mixup data augmen-   tation technique on the teacher models that is in-   formed by training dynamics to improve the student   model calibration on the distillation objective . Our   proposed mixup creates vicinal distribution steered   by the data maps ( Swayamdipta et al . , 2020 ) as   described below.9269Mixup with Training Dynamics . Figure   1 depicts our proposed mixup strategy for   teacher – student framework in the self - distillation   setting ( where both the teacher and student mod-   els have the same architecture , e.g. , RoBERTa ) .   We first contextualize each training instance of our   D into three categories , namely easy - to - learn ,   ambiguous , and hard - to - learn , based on training   dynamics ( statistics deriving from the behavior of   the model across time ) . The training dynamics   of instance ( x , y)are defined as statistics , i.e. ,   confidence and variability computed across the   Eepochs . Confidence is calculated as the mean   model probability of the true label yacross epochs :   where θdenotes our model parameters and p   indicates the model ’s probability at the end of the   eepoch . Intuitively , a high - confidence instance   is easier for the given learner .   Variability is defined as the standard deviation   of the ground - truth probabilities p(y|x)across   different epochs :   Intuitively , samples to which the model confi-   dently assigns the true label ( i.e. , high confidence )   and constantly the same label ( i.e. , low variabil-   ity ) corresponds to easy - to - learn examples ( for the   model ) . The samples with low confidence and low   variability resemble hard - to - learn examples ( for   the model ) , and examples with high variability that   the model is uncertain about during training are   ambiguous ( to the model ) .   Using the model ’s confidence and variability of   the instances , our informed mixup method first   splitsD into three distinct categories , i.e. ,   D , D , andD ( Figure 2 ) , each con-   taining 33 % of train set . Then , it generates mixup   samples by randomly selecting and interpolating   samples from our DandD as a regulariza-   tion technique to improve the calibration of the stu-   dent model ( with all original examples , including   hard - to - learn examples be used during the training   process ) . We mix samples from two distinct groups   of easy - to - learn and ambiguous , as easy samples   play an important role in model optimization , and   ambiguous samples are essential for learning . In   this way , we confer the augmented samples in the   teacher model to embrace useful information from   both easy - to - learn and ambiguous samples , adjust-   ing the difficulty of samples and hence perturbing   the student model ’s predictions to be better cali-   brated . Our informed mixup approach interpolates   samples on the final hidden state corresponding   to the [ CLS ] token generated by the task - specific   layer on top of our teacher model . We conduct our   mixup procedure using mini - batch SGD to update   the model weights in our experiments .   3.3 Calibration   A probabilistic model is considered calibrated if   its predicted probabilities of classes are equivalent   to the actual probabilities of those classes . Intu-   itively , if a model allots 80 % posterior probability   to a class , that class should appear 80 % of the time .   Considering class predictions , suppose a model as-   signs probability qto a class y , formally the model   is perfectly calibrated if∀p∈[0,1],P[Y = y|q=   p ] = p(i.e . , the model is calibrated when qis   always the true probability p ) . To evaluate the   calibration , following Guo et al . ( 2017 ) , we use   the expected calibration error ( ECE ) ( Naeini et al . ,   2015 ) . ECE measures model miscalibration by bin-   ning the predicted probabilities and measuring the   gap between them and the average accuracies of   these bins:/summationdisplayb   N|acc(s)−conf(s)| , where Sis   the overall number of bins , and brepresents the   number of predictions in the s - th bin . Ndenotes   the total number of data points , and acc(s)and   conf(s)represent the accuracy and confidence of   thes - th bin , respectively . We use S= 10 for the   experiments in this paper.92703.4 Post - processing and Regularization   We additionally experiment with post - processing   methodsemployed to tune a model ’s calibration .   Temperature Scaling . In temperature scaling   ( Guo et al . , 2017 ) , before the softmax operation ,   a single scalar hyperparameter Tdivides logits   ( which then go through softmax ) . T−→ ∞ yields   maximum uncertainty with uniform probabilities ,   and as T−→0 , the probability drops to a point   mass . T= 1 obtains the original probabilities ,   i.e. ,T= 1corresponds to no temperature scaling .   This process is shown to make the re - calibrated   probabilities in over - confident models smaller than   the main probabilities and helps the models to be   slightly less confident .   Label Smoothing . Label smoothing ( LS )   ( Szegedy et al . , 2016 ; Pereyra et al . , 2017b ) is a   regularization technique that preserves a reason-   able proportion between the logits of the incor-   rect classes by keeping uncertainty across the label   space throughout training ( Szegedy et al . , 2016 ) .   Therefore , without changing the model architec-   ture , LS prevents overconfident predictions and   could results in better model calibration ( Müller   et al . , 2019 ) . Having a hyperparameterα∈(0,1 ) ,   we use label smoothing to regularize a model with   koutput values by converting the hard 0and1tar-   gets with targets ofand1−α , respectively .   The case of α= 0 corresponds to learning from   one - hot labels .   4 Experiments   4.1 Tasks and Datasets   We perform evaluations on three text classification   tasks of emotion detection , sentiment analysis , and   empathy detection . We analyze tasks with chal-   lenging domain shifts where out - of - domain per-   formance is considerably lower . We explain our   in - domain and out - of - domain datasets below .   Emotion Detection . GoEmotions corpus is a large-   scale emotion detection dataset from Reddit com-   ments labeled with 27emotion categoriesor neu-   tral ( Demszky et al . , 2020 ) . We use the 6basic   emotions ( joy , anger , fear , sadness , disgust , and sur-   prise ) and neutral , proposed by Ekman ( 1992 ) andconduct an Ekman - style grouping into six coarse   categories . Meld ( Poria et al . , 2019 ) contains di-   alogues from the popular Friends TV series an-   notated with Ekman ’s six universal emotions and   two additional emotion labels of neutral and non-   neutral , which we use as unseen test domains . For   consistency , in Meld , we use Ekman-6 emotions   and neutral similar to GoEmotions .   Sentiment Analysis . Yelp is a dataset for binary   sentiment classification , which consists of reviews   from Yelp ( Zhang et al . , 2015 ) . Our out - of - domain   setting is Stanford Sentiment Treebank ( SST-2 )   ( Socher et al . , 2013 ) , which is composed of sen-   tences of movie reviews and their sentiment .   Empathy Detection . EPITOME is a corpus anno-   tated with three levels of empathy communication   ( Sharma et al . , 2020 ) . We consider weak and strong   communications as our positive class , and no com-   munication as negative class . Buechel et al . ( 2018 )   dataset on empathy ( which we refer to as NewsEm-   pathy ) is our out - of - domain empathy dataset con-   sisting of empathic reactions to news stories . For   consistency , we use binary empathy labels to model   empathy in NewsEmpathy in a binary setting .   4.2 Models   To evaluate calibration of large - scale pre - trained   models , we fine - tune BERT ( Devlin et al . , 2019 )   and RoBERTa ( Liu et al . , 2019 ) . We addition-   ally experiment with self - distillation ( a particular   case of the teacher - student training ) , where both   teacher and student models have the same archi-   tecture ( Zhang and Sabuncu , 2020 ) . In this set-   ting , we first train a pre - trained language model   ( i.e. , BERT or RoBERTa ) as a teacher model and   then train a student model ( with the same architec-   ture ) to mimic the output of the teacher model . We   also experiment with knowledge distillation , where   we distill knowledge from a pre - trained language   model with a different architecture than the student   model . We present the result of the knowledge dis-   tillation setting in Appendix B. We further compare   the performance and calibration of the standalone   and distilled models using standard Mixup ( Zhang   et al . , 2018 ) and our proposed mixup method . The   experimental setting are discussed in Appendix A.   4.3 Results   Test accuracies and expected calibration errors   ( ECE ) are summarized in Tables 1 and 2 , respec-   tively . First , we train the model on the in - domain92719272training set for each task . Then , we evaluate its per-   formance on both the in - domain and out - of - domain   test sets . We make a few remarks below .   First , distillation leads to improved accuracy and   model calibration compared to the standalone mod-   els ( BERT or RoBERTa ) , both in the in - domain   and out - of - domain settings . We can see from Ta-   ble 2 that SDyield better - calibrated models with   lower ECE in all of the experiments with our setup .   As shown in Table 2 , the errors obtained with self-   distillation are much smaller in general compared   to the standalone models . For example , on Yelp ,   SD reduces ECE by a factor of 2compared to   the vanilla pre - trained BERT . The results indicate   that the dark knowledge of a teacher model can   act as a regularization process , helping to calibrate   the student model ’s confidence in its predictions .   This phenomenon is visually shown in Figure 3 in   Appendix C.   Second , compared to the vanilla pre - trained mod-   els ( with or without distillation ) , label smoothing   with temperature scaling does not al-   ways improve the calibration or accuracy of the   models , specifically in the in - domain setting . For   example , on EPITOME , the accuracy of the BERT   model is decreased by 1.17 % , and the ECE is in-   creased from 5.53to6.49 . On the other hand , in   the out - of - domain setting , employing label smooth-   ing with temperature scaling results in   a decrease in the calibration error in most settings .   We also observe that in the distillation settings with , the increase in calibration , in most   cases , coincides with the stagnation of student test   accuracy , which in turn shows the inefficacy of   such regularization techniques ( especially for the   in - domain setting ) . The results indicate that la-   bel smoothing with temperature scaling may not   always be effective in calibrating the pre - trained   language model ’s predictions as they do not show   a consistent behavior on the calibration and perfor-   mance . Consequently , we conclude that stronger   regularization strategies are required to temper the   miscalibration of the pre - trained language models .   Third , as shown in Tables 1 and 2 , no significant   improvements in calibration or performance are   observed by solely incorporating plain mixup ( i.e. ,Mixup ) on the standalone models ( i.e. , BERT or   RoBERTa ) . Similarly , we observe that if a teacher   model is trained using plain mixup , the student   model distilled from it is impaired in calibration   and its generalization capabilities in most cases .   Such an aggravation of miscalibration may be due   to the quality of the generated augmented samples   in the mixup process that afflicts the models to cap-   ture the intricacies of the data . We hypothesize that   this adversarial impact leads to a loss in the quality   of the supervision signal during training or distilla-   tion . In contrast , incorporating on plain   mixup leads to lower ECEs on some cases . Never-   theless , solely incorporating plain mixup without   other regularization strategies ( in our case ) is not effective in calibrating the model ’s   predictions .   Finally , it is worth noting that we obtain encour-   aging results with our proposed informed mixup .   From the Table 2 , we see that the errors obtained   with our mixup method are much smaller in gen-   eral compared to the other settings ( Figure 4 in   Appendix C ) . Interestingly , we observe that the stu-   dent models distilled from a teacher trained using   our mixup strategy yield the best - calibrated mod-   els on both the in - domain and out - of - domain data   ( see self - distillation + Ours ECE compared with   other settings ) . Moreover , we find that incorporat-   ing generally helps further to improve   the calibration and performance of the pre - trained   language models . The results suggest that , unlike   the baseline mixup method that focuses more on   the class - specific features , by incorporating train-   ing dynamics into the mixup process , we focus   more on the instance - specific major features that   lead to the more calibrated models . In that capacity ,   we boost the amount of information encoded in all   the latent features encoded by the teacher model ,   which spurs the student model to generate more   generalized and calibrated predictions .   5 Conclusion   In this paper , we showed that the dark knowledge   of a pre - trained language teacher model could act   as a regularization process , helping to calibrate   the student model ’s confidence in its predictions .   We further proposed an informed mixup process   and demonstrated that using training dynamics to   guide the interpolation process in the mixup data   augmentation on a teacher model can effectively   improve the calibration of the student model in a9273distillation setting . We showed that student models   distilled from such teacher models trained using our   proposed mixup method not only achieved the best   performance but also obtained the lowest expected   calibration errors ( ECEs ) on both in - domain and   out - of - domain test sets on emotion - related tasks .   6 Limitations   Our proposed approach shows that using training   dynamics to generate mixup samples along with the   dark knowledge of a pre - trained language teacher   model can act as a regularization process , which   helps to calibrate the student model ’s confidence in   its predictions . It would be interesting to analyze   the impact of adding mixed data augmentation tech-   niques to the student networks on the calibration   of the pre - trained language models . One potential   limitation of our approach is using a small addi-   tional overhead for calculating statistics with the   data maps tool . However , this is a common limita-   tion for all approaches that use this data maps .   Acknowledgements   This research is supported in part by NSF Conver-   gence Accelerator award # 2137846 , NSF IIS award   # 2107487 , and NSF BigData award # 1912887 .   Any opinions , findings , and conclusions expressed   here are those of the authors and do not necessar-   ily reflect the views of NSF . We thank AWS for   computational resources that supported this work .   We also thank our anonymous reviewers for their   constructive feedback .   References927492759276A Implementation Details   For creating the data maps , we use   RoBERTa - base ( Liu et al . , 2019 ) with the   same set of hyper - parameters as ( Swayamdipta   et al . , 2020 ) . For the experiments , we fine - tune   BERT bert - base - uncased ( Devlin et al . ,   2019 ) , and RoBERTa roberta - base from the   HuggingFace Transformers library ( Wolf et al . ,   2019 ) . All the models are trained with a maximum   of 3 epochs . BERT is fine - tuned with a batch size   of16 , learning rate of 2e−5 , gradient clip of 1.0 ,   and no weight decay . RoBERTa is fine - tuned with   a batch size of 32 , learning rate of 1e−5 , gradient   clip of 1.0 , and weight decay of 0.1 . Models are   optimized with AdamW ( Loshchilov and Hutter ,   2019 ) . We train our models with subsets containing   top[50%,33%,25 % ] easy - to - learn and ambiguous   samples and find 33 % ( i.e. , total 66 % train set )   results in the best in - domain and out - of - domain   performance . For mixup , the αparameter 0.4 in   the beta distribution works best in our settings   amongst [ 0.3,0.4,0.5 ] . For label smoothing , we   change the smoothing hyper - parameter αin the   range [ 0.05,0.1,0.2,0.3,0.4 ] , and find 0.1to work   best for our settings . We utilize the in - domain de-   velopment set for temperature scaling to obtain an   optimum temperature Tin the range of [ 0.01,5.0 ]   with a granularity of 0.01 . For distillation with   data augmentation , we first train the pre - trained   language models ( i.e. , BERT or RoBERTa ) using   the data augmentation techniques discussed above   ( i.e. , standard mixup or our informed mixup ) on   each task ’s training dataset . Then , the student   networks are built from the BERT or RoBERTa   with no data augmentation techniques added .   In the knowledge distillation setting , all the   regularization methods ( i.e. , mixup methods , label   smoothing , and temperature scaling ) are only   applied to the teacher model before using the   model as a teacher for the student model . For all   the results , we report the mean performance over 3   random seeds . Finally , fine - tuning all the models   took in total less than one day on our NVIDIA   GP100 16 GB GPU .   B Knowledge Distillation Experiments   Tables 3 and 4 present the comparison results of the   knowledge distillation setting with baselines ( the   baseline results are borrowed from Tables 1 and 2 ) .   Unlike the self - distillation setting , in this setting ,   we first train a pre - trained language model ( i.e. ,BERT or RoBERTa ) as a teacher model and then   train a student model ( with a different architecture )   to mimic the output of the teacher model . For   example , if we choose to train RoBERTa as the   teacher model , then we train BERT as the student   model to learn from the output of the RoBERTa   teacher model , and vice versa .   From the Tables 3 and 4 , we can observe that in   most cases teacher - student training in the knowl-   edge distillation setting ( i.e. , KD ) results in better-   calibrated student models compared to the stan-   dalone models . For example , on NewsEmpathy ,   KD reduces ECE by 3.29 % compared to   the vanilla pre - trained RoBERTa .   C Visualisations of Comparison of the   Knowledge and Self - distillation with   Vanilla Models   Figure 3 compares knowledge and self - distillation   with vanilla models and illustrates that distillation   settings yield better - calibrated models with lower   ECE in all of the experiments with our setup ( with   the self - distillation being the best setting ) . Figure   4 shows that the errors obtained with our mixup   method are much smaller in general compared to   the other settings and yields best calibrated models.92779278