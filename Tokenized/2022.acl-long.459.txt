  Thai Le   Penn State University   thaile@psu.eduNoseong Park   Yonsei University   noseong@yonsei.ac.krDongwon Lee   Penn State University   dongwon@psu.edu   Abstract   Even though several methods have proposed   to defend textual neural network ( NN ) models   against black - box adversarial attacks , they   often defend against a specific text perturbation   strategy and/or require re - training the models   from scratch . This leads to a lack of general-   ization in practice and redundant computation .   In particular , the state - of - the - art transformer   models ( e.g. , BERT , RoBERTa ) require great   time and computation resources . By borrowing   an idea from software engineering , in order   to address these limitations , we propose a   novel algorithm , S , which modifies and   re - trains only the last layer of a textual NN ,   and thus it “ patches ” and “ transforms ” the   NN into a stochastic weighted ensemble of   multi - expert prediction heads . Considering   that most of current black - box attacks rely on   iterative search mechanisms to optimize their   adversarial perturbations , S confuses the   attackers by automatically utilizing different   weighted ensembles of predictors depending   on the input . In other words , S breaks a   fundamental assumption of the attack , which is   a victim NN model remains constant during   an attack . By conducting comprehensive   experiments , we demonstrate that all of CNN ,   RNN , BERT , and RoBERTa - based textual   NNs , once patched by S , exhibit a   relative enhancement of 15%–70 % in accuracy   on average against 14 different black - box   attacks , outperforming 6 defensive baselines   across 3 public datasets . Source code will   be published at github.com/lethaiq/   shield - defend - adversarial - texts .   1 Introduction   Adversarial Text Attack and Defense . After be-   ing trained to maximize prediction performance ,   textual NN models frequently become vulnerable   to adversarial attacks ( Papernot et al . , 2016 ; Wang   et al . , 2019a ) . In the NLP domain , in general , ad-   versaries utilize different strategies to perturb aninput sentence such that its semantic meaning is   preserved while successfully letting a target NN   model output a desired prediction . Text perturba-   tions are typically generated by replacing or insert-   ing critical words ( e.g. , HotFlip ( Ebrahimi et al . ,   2018 ) , TextFooler ( Jin et al . , 2019 ) ) , characters   ( e.g. , DeepWordBug ( Gao et al . ) , TextBugger ( Li   et al . , 2018 ) ) in a sentence or by manipulating a   whole sentence ( e.g. , SCPNA ( Iyyer et al . , 2018 ) ,   GAN - based(Zhao et al . , 2018 ) ) .   Since many recent NLP models are known to be   vulnerable to adversarial black - box attacks ( e.g. ,   fake news detection ( Le et al . , 2020 ; Zhou et al . ,   2019b ) , dialog systems ( Cheng et al . , 2019 ) , and so   on ) , robust defenses for textual NN models are re-   quired . Even though several papers have proposed   to defend NNs against such attacks , they were de-   signed for either a specific type of attack ( e.g. ,   word or synonym substitution ( Wang et al . , 2021 ;   Dong et al . , 2021 ; Mozes et al . , 2020 ; Zhou et al . ,   2021 ) , misspellings ( Pruthi et al . , 2019 ) , character-   level ( Pruthi et al . , 2019 ) , or word - based ( Le et al . ,   2021 ) ) . Even though there exist some general de-   fensive methods , most of them enrich NN mod-   els by re - training them with adversarial data aug-   mented via known attack strategies ( Miyato et al . ,   2016 ; Liu et al . , 2020 ; Pang et al . , 2020 ) or with   external information such as knowledge graphs ( Li   and Sethy , 2019 ) .   However , these augmentations often induce sub-   stantial overhead in training or are still limited to   only a small set of predefined attacks ( e.g. , ( Zhou   et al . , 2019a ) ) . Hence , we are in search of defense   algorithms that directly enhance NN models ’ struc-   tures ( e.g. , ( Li and Sethy , 2019 ) ) while achieving   higher generalization capability without the need   of acquiring additional data .   Motivation ( Fig . 1 ) . Different from white - box   attacks , black - box attacks do not have access to   a target model ’s parameters , which are crucial for   achieving effective attacks . Hence , attackers often6661   query the target model repeatedly to acquire the   necessary information for optimizing their strat-   egy . From our analyses of 14 black - box attacks   published during 2018–2020 ( Table 1 ) , all of them ,   except for SCPNA ( Iyyer et al . , 2018 ) , rely on a   searching algorithm ( e.g. , greedy , genetic ) to iter-   atively replace each character / word in a sentence   with a perturbation candidate to optimize the choice   of characters / words and how they should be crafted   to attack the target model ( Fig . 1A ) . Even though   this process is effective in terms of attack perfor-   mance , they assume that the model ’s parameters   remain “ unchanged ” and the model outputs “ coher-   ent ” signals during the iterative search ( Fig . 1A and   1B ) . Our key intuition is , however , to obfuscate the   attackers by breaking this assumption . Specifically ,   we want to develop an algorithm that automati-   cally utilizes a diverse set of models during infer-   ence . This can be done by training multiple sub-   models instead of a single prediction model and   randomly select one of them during inference to ob-   fuscate the iterative search mechanism . However ,   this then introduces impractical computational over-   head during both training and inference , especially   when one wants to maximize prediction accuracy   by utilizing complex SOTA sub - models such as   BERT ( Devlin et al . , 2019 ) and RoBERTa ( Liu   et al . , 2019b ) . Moreover , it also does not guarantee   that trained models are sufficiently diverse to fool   attackers . Furthermore , applying this strategy to   existing NN models would also require re - training   everything from the scratch , rendering the approach   impractical .   Proposal . To address these challenges , we borrow   ideas from software engineering where bugs can be   readily removed by an external installation patch .   Specifically , we develop a novel neural patching   algorithm , named as S , which patches only   the last layer of an already deployed textual NN   model ( e.g. , CNN , RNN , transformers(Vaswani   et al . , 2017 ; Bahdanau et al . ) ) and transforms it into   an ensemble of multi - experts or prediction heads   ( Fig . 1C ) . During inference , then S automat-   ically utilizes a stochastic weighted ensemble of ex-   perts for prediction depending on inputs . This will   obfuscate adversaries ’ perturbation search , making   black - box attacks much more difficult regardless   of attack types , e.g. , character or word level at-   tacks ( Fig . 1C , D ) . By patching only the last layer   of a model , S also introduces lightweight   computational overhead and requires no additional   training data . In summary , our contributions are as   follows:6662•We propose S , a novel neural patching   algorithm that transforms a already - trained NN   model to a stochastic ensemble of multi - experts   with little computational overhead .   •We demonstrate the effectiveness of S .   CNN , RNN , BERT , and RoBERTa - based tex-   tual models patched by S achieve an in-   crease of 15%–70 % on their robustness across   14 different black - box attacks , outperforming 6   defensive baselines on 3 public NLP datasets .   •To the best of our knowledge , this work by far   includes the most comprehensive evaluation for   the defense against black - box attacks .   2 The Proposed Method : S   We introduce Stochastic Multi - Expert Neural   Patcher ( S ) which patches only the last layer   of an already trained NN model f(x , θ)and trans-   forms it into an ensemble of multiple expert predic-   tors with stochastic weights . These predictors are   designed to be strategically selected with different   weights during inference depending on the input .   This is realized by two complementary modules ,   namely ( i ) a Stochastic Ensemble ( SE ) module that   transforms f(·)into a randomized ensemble of dif-   ferent heads and ( ii ) a Multi - Expert ( ME ) module   that uses Neural Architecture Search ( NAS ) to dy-   namically learn the optimal architecture of each   head to promote their diversity .   2.1 A Stochastic Ensemble ( SE ) Module   This module extends the last layer of f ( · ) , which   is typically a fully - connected layer ( followed by a   softmax for classification ) , to an ensemble of K   prediction heads , denoted H={h ( · ) } . Each head   h ( · ) , parameterized by θ , is an expert predictor   that is fed with a feature representation learned by   up to the second - last layer off(·)and outputs a   prediction logit score :   h : f(x , θ)∈R7→˜y∈R , ( 1 )   where θarefixed parameters of fup to the last   prediction head layer , Qis the size of the feature   representation of xgenerated by the base model   f(x , θ ) , and Mis the number of labels . To ag-   gregate all logit scores returned from all heads ,   then , a classical ensemble method would aver-   age them as the final prediction : ˆy = P˜y .   However , this simple aggregation assumes each   h(·)∈ H learns from very similar training signals . Hence , when θalready learns some of the task-   dependent information , Hwill eventually converge   notto a set of experts but very similar predictors .   To resolve this issue , we introduce stochasticity   into the process by assigning prediction heads with   stochastic weights during both training and infer-   ence . Specifically , we introduce a new aggregation   mechanism :   ˆy=1   KXαw˜y , ( 2 )   where wweights ˜yaccording to head j ’s ex-   pertise on the current input x , and α∈[0,1]is a   probabilistic scalar , representing how much of the   weight wshould be accounted for . Let us denote   w , α∈Ras vectors containing all scalars w   andα , respectively , and ˜y∈Ras the con-   catenation of all vectors ˜yreturned from each of   the heads . We calculate wandαas follows :   w = W(˜y⊕f(x , θ ) ) + b , ( 3 )   α= softmax ( ( w+g)/τ ) , ( 4 )   where W∈R , b∈Rare train-   able parameters , g∈Ris a noise vector sam-   pled from the Standard Gumbel Distribution and   therefore , probability vector αis sampled by a   technique known as Gumbel - Softmax ( Jang et al . ,   2016 ) controlled by the noise vector gand the   temperature τ . Unlike the standard Softmax , the   Gumbel - Softmax is able to learn a categorical dis-   tribution ( over Kheads ) optimized for a down-   stream task ( Jang et al . , 2016 ) . Annealing τ→0   encourages a pseudo one - hot vector ( e.g. , [ 0.94 ,   0.03 , 0.01 , 0.02 ] when K=4 ) , which makes Eq .   ( 2 ) a mixture of experts ( Avnimelech and Intrator ,   1999 ) . Importantly , αis sampled in an inherently   stochastic way depending on the gumbel noise g.   While W , bis learned to deterministically as-   signs more weights wto heads that are experts for   each input x(Eq . ( 3 ) ) , αintroduces stochasticity   into the final logits . The multiplication of αw   in Eq . ( 2 ) then enables us to use different sets of   weighted ensemble models while still maintaining   the ranking of the most important head . Thus , this   further diversifies the learning of each expert and   confuse attackers when they iteratively try different   inputs to find good adversarial perturbations .   Finally , to train this module , we use Eq . ( 2 ) as   the final prediction and train the whole module with6663Algorithm 1 Training S Algorithm .   Negative Log Likelihood ( NLL ) loss following the   objective :   minL=−1   NXylog(softmax(ˆ y ) ) .   ( 5 )   2.2 A Multi - Expert ( ME ) Module   While the SEmodule facilitates stochastic weighted   ensemble among heads , the MEmodule searches   for the optimal architecture for each head that   maximizes the diversity in how they make predic-   tions . To do this , we utilize the DARTS algo-   rithm ( Liu et al . , 2019a ) as follows . Let us denote   O={o(·)}where Tis the number of possible   architectures to be selected for h∈ H. We want to   learn a one - hot encoded selection vector β∈R   that assigns h(·)←o(·)during pre-   diction . Since argmax ( · ) operation is not differ-   entiable , during training , we relax the categorical   assignment of the architecture for h(·)∈ H to a   softmax over all possible networks in O :   h(·)←−1   TXexp(β )   Pexp(β)o(·).(6 )   However , the original DARTS algorithm only op-   timizes prediction performance . In our case , we   also want to promote the diversity among heads .   To do this , we force each h(·)to specialize in dif-   ferent features of an input , i.e. , in how it makes   predictions . This can be achieved by maximizing   the difference among the gradients of the word-   embedding eof input xw.r.t to the outputs of   eachh(·)∈ H. Hence , given a fixed set of param-   eters θof all possible networks for every heads ,   we train all selection vectors { β}by optimizing   the objective :   minimizeL = XX   d(∇J;∇J)− ||∇J−∇J||   ,   ( 7 )   where d(·)is the cosine - similarity function , and J   is the NLL loss as if we only use a single prediction   headh . In this module , however , not only do we   want to maximize the differences among gradients   vectors , but also we want to ensure the selected ar-   chitectures eventually converge to good prediction   performance . Therefore , we train the whole ME   module with the following objective :   minimizeL = L+γL .(8 )   2.3 Overall Framework   To combine the SEandMEmodules , we replace Eq .   ( 6 ) into Eq . ( 1 ) and optimize the overall objective :   minimizeL+γL s.t .   W , b , θ= minimizeL.(9 )   We employ an iterative training strategy ( Liu   et al . , 2019a ) with the Adam optimization algo-   rithm ( Kingma and Ba , 2013 ) as in Alg . 1 . By al-   ternately freezing and training W , b , θand{β }   using a training set D and a validation set D ,   we want to ( i ) achieve high quality prediction per-   formance through Eq . ( 5 ) and ( ii ) select the optimal   architecture for each expert to maximize their spe-   cialization through Eq . ( 7 ) .   3 Experimental Evaluation   3.1 Set - up   Datasets & Metric . Table 2 shows the statistics of   all experimental datasets : Clickbait detection ( CB )   ( Anand et al . , 2017 ) , Hate Speech detection ( HS )   ( Davidson et al . ) and Movie Reviews classification   ( MR ) ( Pang and Lee , 2005 ) . We split each dataset   intotrain , validation andtestset with the ratio of   8:1:1 whenever standard public splits are not avail-   able . To report prediction performance on clean   examples , we use the weighted F1 score to take the   distribution of prediction labels into consideration .   To report the robustness , we report prediction accu-   racy under adversarial attacks ( Morris et al . , 2020 ) ,   i.e. , # of failed attacks over total # of examples . A   failed attack is only counted when the attacker fails6664to perturb ( i.e. , fail to flip the label of a correctly   predicted clean example ) .   Defense Baselines . We want to defend four tex-   tual NN models ( base models ) of different architec-   tures , namely RNN with GRU cells ( Chung et al . ) ,   transformer -based BERT ( Devlin et al . , 2019 ) and   RoBERTa ( Liu et al . , 2019b ) . We compare S   with the following six defensive baselines :   •Ensemble ( Ens . ) is the classical ensemble of 5   different base models . We use the average of   all NLL losses from the base models as the final   training loss .   •Diversity Training ( DT ) ( Kariyappa and Qureshi ,   2019 ) is a variant of the Ensemble baseline where   a regularization term is added to maximize the   coherency of gradient vectors of the input text   w.r.t each sub - model . DT diversifies the feature-   level expertise among heads .   •Adaptive Diversity Promoting ( ADP ) ( Pang et al . ,   2019 ) is a variant of Ensemble baseline where   a regularization term is added to maximize the   diversity among non - maximal predictions of in-   dividual sub - models . ADP diversifies the class-   level expertise among heads .   •Mixup Training ( Mixup ) ( Zhang et al . , 2018 ; Si   et al . ) trains a base model with data constructed   by linear interpolation of two random training   samples . In this work , we use Mixup to regularize   a NN to adapt linear transformation in - between   the continuous embeddings of training samples .   •Adversarial Training ( AdvT ) ( Miyato et al . , 2016 )   is a semi - supervised algorithm that optimizes the   NLL loss on the original training samples plus   adversarial inputs .   •Robust Word Recognizer ( ScRNN ) ( Pruthi et al . ,   2019 ) detects and corrects potential adversarial   perturbations or misspellings in a text before   feeding it to the base model for prediction .   Note thatduetotheinsufficient memory ofGPU   Titian Xptosimultaneously train several BERT   andRoBERTa sub - models , weexclude Ensemble ,   DT , andADP baseline forthem .   Attacks . We comprehensively evaluate S   under 14 different black - box attacks ( Table 1 ) .   These attacks differ in their attack levels ( e.g. ,   character , word , sentence - based ) , optimization al-   gorithms for searching adversarial perturbations   ( e.g. , through fixed templates , greedy , genetic-   based search ) . Apart from lexical constraints such   as limiting # or % of words to manipulate in a   sentence , ignoring stop - words , etc . , many of them   also preserve the semantic meanings of a generated   adversarial text via constraining the l2 distance   between its representation vector and that of the   original text produced by either Universal Sentence   Encoder ( USE ) ( Cer et al . , 2018 ) or GloVe em-   beddings ( Pennington et al . , 2014 ) . Moreover , to   ensure that the perturbed texts still look natural , a   few of the attack methods employ an external pre-   trained language model ( e.g. , BERT(Devlin et al . ,   2019 ) , L2W ( Holtzman et al . , 2018 ) ) to optimize   the log - likelihood of the adversarial texts . Due   to computational limit , we only compare S   with other baselines in 3 representative attacks ,   namely TextFooler ( Jin et al . , 2019 ) , DeepWord-   Bug ( Gao et al . ) and PWWS ( Ren et al . , 2019 ) .   They are among the most effective attacks . To   ensure fairness and reproducibility , we use the ex-   ternal TextAttack ( Morris et al . , 2020 ) and OpenAt-   tack ( Zeng et al . , 2021 ) . framework for adversarial   text generation and evaluation .   Implementation . We train S of 5 experts6665   ( K=5 ) with γ=0.5 . For each expert , we set   Oto 3 ( T=3 ) possible networks : FCN with 1 ,   2 and 3 hidden layer(s ) . For each dataset , we   usegrid - search to search for the best τvalue   from{1.0,0.1,0.01,0.001}based on the averaged   defense performance on the validation set un-   der TextFooler ( Jin et al . , 2019 ) and DeepWord-   Bug ( Gao et al . ) . We use 10 % of the training set   as a separate development set during training with   early - stop to prevent overfitting . We report the   performance of the best single model across all   attacks on the test set . The Appendix includes all   details on all models ’ parameters and implementa-   tion .   3.2 Results   Fidelity We first evaluate S ’s prediction   performance without adversarial attacks . Table 3   shows that all base models patched by S   still maintain similar F1 scores on average across   all datasets . Although S with RNN has a   slightly decrease in fidelity on Hate Speech dataset ,   this is negligible compared to the adversarial ro-   bustness benefits that S will provide ( More   below ) .   Computational Complexity Regarding the space   complexity , S can extend a NN into an en-   semble model with a marginal increase of # of pa-   rameters . Specifically , with Bdenoting # of param-   eters of the base model , S has a space com - plexity of O(B+KU)while both Ensemble , DT   andADP have a complexity of O(KB)andU≪B.   In case of BERT with K=5,S only requires   an additional 8.3 % . While traditional ensemble   methods require as many as 4 times additional   parameters . During training , S only trains   O(KU)parameters , while other defense methods ,   including ones using data augmentation , update all   of them . Specifically , with K=5,S only   trains 8 % of the parameters of the base model and   1.6 % of the parameters of other BERT - based en-   semble baselines . During inference , S is   also 3 times faster than ensemble - based DTand   ADP on average .   Robustness Table 4 shows the performance of   S compared to the base models . Over-   all , S consistently improves therobustness   ofbase models in154/168 ( 92 % ) cases across   14adversarial attacks regardless oftheir attack   strategies . Particularly , all CNN , RNN , BERT and   RoBERTa - based textual models that are patched   byS witness relative improvements in the   average prediction accuracy from 15 % to as much   as 70 % . Especially in the case of detecting click-   bait , S can recover up to 5 % margin within   the performance on clean examples in many cases .   This demonstrates that S provides a versa-   tile neural patching mechanism that can quickly   and effectively defends against black - box adver-   saries without making any assumptions on the at-   tack strategies.6666   We then compare S with all defense base-   lines under TextFooler ( TF ) , DeepWordBug ( DW ) ,   and PWWS ( PS ) attacks . These attacks are selected   as ( i ) they are among the strongest attacks and ( ii )   they provide foundation mechanisms upon which   other attacks are built . Table 5 shows that S   achieves the best robustness across all attacks and   datasets . On average , S observes an absolute   improvement from +9 % to +18 % in accuracy over   the second - best defense algorithms ( DT in case   of RNN , and AdvT in case of BERT , RoBERTa ) .   Moreover , S outperforms other ensemble-   based baselines ( DT , ADP ) , and can be applied on   top of a pre - trained BERT or RoBERTa model with   only around 8 % additional parameters . However ,   that # would increase to 500 % ( K←5)in the case   of DT and ADP , requiring over half a billion # of   parameters .   4 Discussion   Performance under Budgeted Attacks . S   not only improves the overall robustness of the   patched NN model under a variety of black - box   attacks , but also induces computational cost that   can greatly discourage malicious actors to exercise   adversarial attacks in practice . We define compu-   tational cost as # of queries on a target NN model   that is required for a successful attack . Since ad-   versaries usually have an attack budget on # of   model queries ( e.g. a monetary budget , limited   API access to the black - box model ) , the higher   # of queries required , the less vulnerable a target   model is to adversarial threats . A larger budget is   crucial for genetic - based attacks because they usu-   ally require larger # of queries than greedy - based   strategies . We have demonstrated in Sec . 3.2 that   S is robust even when the attack budget is   unlimited . Fig . 2 shows that the performance of   RoBERTa after patched by S also reduces   at a slower rate compared to the base RoBERTa   model when the attack budget increases , especially   under greedy - based attacks .   Effects of Stochasticity on S ’s Perfor-   mance . Stochasticity in S comes from two   parts , namely ( i ) the assignment of the main pre-   diction head during each inference call and ( ii ) the   randomness in the Gumbel - Softmax outputs . Re-   garding ( i ) , it happens because during a typical iter-   ative black - box process , an attacker tries different   manipulations of a given text . When the attacker   does so , the input text to the model changes at every   iterative step . This then leads to the changes of pre-   diction head assignment because each prediction   head is an expert at different features – e.g. , words   or phrases in an input sentence . Thus , given an   input , the assignment of the expert predictors for a   specific set of manipulations stays the same . There-   fore , even if an attacker repeatedly calls the model   with a specific changes on the original sentence ,   the attacker will not gain any additional informa-   tion . Regarding ( ii ) , even though Gumbel - Softmax   outputs are not deterministic , it always maintains6667the relative ranking of the expert predictor during   each inference call with a sufficiently small value   ofτ . In other words , it will not affect the fidelity   of the model across different runs .   Parameter Sensitivity Analyses . Training   S requires hyper - parameter K , T , γ andτ .   We observe that arbitrary value γ=0.5 , K=5 , T=3   works well across all experiments . Although we   did not observe any patterns on the effects of K   on the robustness , a K≥3performs well across   all attacks . On the contrary , different pairs of the   temperature τduring training and inference wit-   ness varied performance w.r.t to different datasets .   τgives us the flexibility to control the sharpness   of the probability vector α . When τ→0,αto get   closer to one - hot encoded vector , i.e. , use only one   head at a time .   Ablation Tests . This section tests S with   only either the SEorMEmodule . Table 6 shows   thatSEandMEperforms differently across differ-   ent datasets and models . Specifically , we observe   thatMEperforms better than the SEmodule in case   of Clickbait dataset , SEis better than the MEmod-   ule in case of Movie Reviews dataset and we have   mixed results in Hate Speech dataset . Nevertheless ,   the final S model which comprises both the   SEandMEmodules consistently performs the best   across all cases . This shows that both the MEand   SEmodules are complementary to each other and   are crucial for S ’s robustness .   5 Limitations and Future Work   In this paper , we limit the architecture of each ex-   pert to be an FCN with a maximum of 3 hidden   layers ( except the base model ) . If we include more   options for this architecture ( e.g. , attention ( Luong   et al . , 2015 ) ) , sub - models ’ diversity will signifi-   cantly increase . The design of S is model-   agnostic and is also applicable to other complex   and large - scale NNs such as transformers - based   models . Especially with the recent adoption of   transformer architecture in both NLP and com-   puter vision ( Carion et al . , 2020 ; Chen et al . , 2020 ) ,   potential future work includes extending S   to patch other complex NN models ( e.g. , T5 ( Raf-   fel et al . , 2020 ) ) or other tasks and domains such   as Q&A and language generation . Although our   work focus is not in robust transferability , it can   accommodate so simply by unfreezing the base lay-   ersf(x , θ)in Eq . ( 1 during training with some   sacrifice on running time .   6 Related Work   Defending against Black - Box Attacks . Most of   previous works ( e.g. , ( Le et al . , 2021 ; Zhou et al . ,   2021 ; Keller et al . , 2021 ; Pruthi et al . , 2019 ; Dong   et al . , 2021 ; Mozes et al . , 2020 ; Wang et al . , 2021 ;   Jia et al . , 2019 ) in adversarial defense are designed   either for a specific type ( e.g. , word , synonym-   substitution as in certified training ( Jia et al . , 2019 ) ,   misspellings ( Pruthi et al . , 2019 ) ) or level ( e.g. ,   character or word - based ) of attack . Thus , they are   usually evaluated against a small subset of ( ≤4 )   attack methods . Despite there are works that pro-   pose general defense methods , they are often built   upon adversarial training ( Goodfellow et al . , 2015 )   which requires training everything from scratch   ( e.g. , ( Si et al . ; Miyato et al . , 2016 ; Zhang et al . ,   2018 ) or limited to a set of predefined attacks ( e.g. ,   ( Zhou et al . , 2019a ) ) . Although adversarial training-   based defense works well against several attacks   on BERT and RoBERTa , its performance is far   out - weighted by S ( Table 5 ) .   Contrast to previous approaches , S ad-   dresses not the characteristics of the resulted per-   turbations from the attackers but their fundamental   attack mechanism , which is most of the time an   iterative perturbation optimization process ( Fig . 1 ) .   This allows S to effectively defend against   14 different black - box attacks ( Table 1 ) , showing   its effectiveness in practice . To the best of our   knowledge , by far , this works also evaluate with6668the most comprehensive set of attack methods in   the adversarial text defense literature .   Ensemble - based Defenses . S is distinguish-   able from previous ensemble - based defenses on   two aspects . First , previous approaches such as   DT ( Kariyappa and Qureshi , 2019 ) , ADP ( Pang   et al . , 2019 ) are mainly designed for computer vi-   sion . Applying these models to the NLP domain   faces a practical challenge where training multi-   ple memory - intensive SOTA sub - models such as   BERT or RoBERTa can be very costly in terms of   space and time complexities .   In contrast , S enables to “ hot - fix ” a com-   plex NN by replacing and training only the last   layer , removing the necessity of re - training the en-   tire model from scratch . Second , previous meth-   ods ( e.g. , DT and ADP ) mainly aim to reduce the   dimensionality of adversarial subspace , i.e. , the   subspace that contains all adversarial examples ,   by forcing the adversaries to attack a single fixed   ensemble of diverse sub - models at the same time .   This then helps improve the transferability of ro-   bustness on different tasks . However , our approach   mainly aims to dilute not transfer but direct attacks   by forcing the adversaries to attack stochastic , i.e. ,   different , ensemble variations of sub - models at ev-   ery inference passes . This helps S achieve a   much better defense performance compared to DT   and ADP across several attacks ( Table 5 ) .   7 Conclusion   This paper presents a novel algorithm , S ,   which consistently improves the robustness of tex-   tual NN models under black - box adversarial at-   tacks by modifying and re - training only their last   layers . By extending a textual NN model of   varying architectures ( e.g. , CNN , RNN , BERT ,   RoBERTa ) into a stochastic ensemble of multi-   ple experts , S utilizes differently - weighted   sets of prediction heads depending on the input .   This helps S defend against black - box ad-   versarial attacks by breaking their most fundamen-   tal assumption – i.e. , target NN models remain un-   changed during an attack . S achieves aver-   age relative improvements of 15%–70 % in predic-   tion accuracy under 14 attacks on 3 public NLP   datasets , while still maintaining similar perfor-   mance on clean examples . Thanks to its model-   and domain - agnostic design , we expect S to   work properly in other NLP domains . Broad Impact   We address two practical adversarial attack sce-   narios and how S can help defend against   them . First , adversaries can attempt to abuse social   media platforms such as Facebook by posting ads   or recruitment for human - trafficking , protests , or   by spreading misinformation – e.g. , vaccine - related .   To do so , the adversaries can directly use one of   the black - box attacks in the literature to iteratively   craft a posting that will not be easily detected and   removed by the platforms . In some cases , a good   attack method only requires a few trials to success-   fully fool such platforms . Our method can help con-   fuse the attackers with inconsistent signals , hence   reduce the chance they succeed . Second , many pop-   ular services and platforms such as the NYTimes ,   the Southeast Missourian , OpenWeb , Disqus , Red-   dit , etc . rely on a 3rd party APIs such as Perspec-   tive APIfor detecting toxic comments online – e.g. ,   racist , offensive , personal attacks . However , these   public APIs have been shown to be vulnerable   against black - box attacks in literature ( Li et al . ,   2018 ) . The attacker can use a black - box attack   method to attack these public APIs in an iterative   manner , then retrieve the adversarial toxic com-   ments and use those on these platforms without   the risk of being detected and removed by the sys-   tem . Since these malicious behaviors can endanger   public safety and undermine the quality of online   information , our work has practical values and can   have broad societal impacts .   Acknowledgement   This research was supported in part by NSF awards   # 1820609 , # 1915801 , and # 2114824 . The work of   Noseong Park was partially supported by the Yon-   sei University Research Fund of 2021 , and the Insti-   tute of Information & Communications Technology   Planning & Evaluation ( IITP ) grant funded by the   Korean government ( MSIT ) ( No . 2020 - 0 - 01361 ,   Artificial Intelligence Graduate School Program   ( Yonsei University)).6669References667066716672   A ADDITIONAL RESULTS   •Table A.1 shows the performance of S   against all 14 black - box attacks on CNN - based   NN models .   B REPRODUCIBILITY   B.1 Infrastructure and Source Code   •Software : All the implementations are written   in Python ( v3.7 ) with Pytorch ( v1.5.1 ) , Numpy   ( v1.19.1 ) , Scikit - learn ( v0.21.3 ) . We rely on   Transformers ( v3.0.2 ) library for loading and   training transformers - based models ( e.g. , BERT ,   RoBERTa ) .   •Hardware : We run all of the experiments on   standard server machines installed with Ubuntu   OS ( v18.04 ) , 20 - Core Intel(R ) Xeon(R ) Silver   4114 CPU @ 2.20GHz , 93 GB of RAM , and a   Titan Xp GPU .   •Dataset : We use the python library datasets   ( v.1.2.0)byHugginface to load all the bench-   mark datasets used in the paper .   •Random Seed : To ensure reproducibility ,   we set a consistent random seed using   torch.manual_seed andnp.random.seed func-   tion for all experiments . B.2 Experimental Settings for Base Models   B.2.1 Architectures and Parameters   •CNN : We implement the CNN sentence classifi-   cation model ( Kim , 2014 ) with three 2D CNN   layers , each of which is followed by a Max-   Pooling layer . Concatenation of outputs of all   Max - Pooling layers is fed into a Dropout layer   with 0.5 probability , then an FCN + Softmax for   prediction . We use an Embedding layer of size   300 with pre - trained GloVe embedding - matrix   to transform each discrete text tokens into con-   tinuous input features before feeding them into   the CNN network . Each of CNN layers uses 150   kernels with a size of 2 , 3 , 4 , respectively .   •RNN : Because the original PyTorch implemen-   tation of RNN does not support double back-   propagation on CuDNN , which is required by   DTandS to run the model on GPU , we   use a publicly available Just - in - Time ( JIT ) ver-   sion of GRU of one hidden layer as RNN cell .   We use an Embedding layer of size 300 with   pre - trained GloVe embedding - matrix to trans-   form each discrete text tokens into continuous   input features before inputting them into the   RNN layer . We flatten out all outputs of the   RNN layer , followed by a Dropout layer with   0.5 probability , then an FCN + Softmax for pre-   diction .   •BERT & RoBERTa : We use the transformers   library from HuggingFace to fine - tune BERT   and RoBERTa model . We use the bert - base-   uncased version of BERT and the RoBERTa-   base version of RoBERTa .   B.2.2 Vocabulary and Input Length   Due to limited GPU memory , we set the maxi-   mum length of inputs for transformer - based mod-   els , i.e. , BERT and RoBERTa , to 128 during train-   ing . For CNN and RNN - based models , we use all   the vocabulary tokens that can be extracted from   the training set , and we use all of the vocabulary   tokens provided by pre - trained models for BERT   and RoBERTa - based models .   B.3 Experimental Settings for Defense   Methods   1.S : For hyper - parameter γ , KandT , we   arbitrarily set γ←0.5,K←5andT←3and they   work well across all datasets . For τ , we already6673described how to choose the best pair of τduring   training and testing in Sec . 3.1 .   2.Ensemble : We train an ensemble model of 5 sub-   models , all of which have the same architecture   as the base model . We use the average loss of all   sub - models as the final loss to train the model .   3.DT : We follow the implementation described   in Section 3 of the original paper ( Kariyappa   and Qureshi , 2019 ) and train an ensemble DT   model with 5 sub - models , all of which have the   same architecture as the base model . We set the   hyper - parameter λ←0.5as suggested by the   original paper .   4.ADP : We follow the implementation described   in Section 3 of the original paper ( Pang et al . ,   2019 ) and train an ensemble ADP model with   5 sub - models , all of which have the same ar-   chitecture as the base model . We set the hyper-   parameters required by ADP to default values   ( α←1.0andβ←0.5 ) as suggested by the   original implementation .   5.Mix - up Training ( Mix ) : We sample λ∈   Beta ( 1.0,1.0)as suggested by the implementa-   tion provided by the original paper ( Zhang et al . ,   2018 ) .   6.Adversarial Training : We use a 1:1ratio be-   tween original training samples and adversarial   training samples as suggested by ( Miyato et al . ,   2016 ) . We specifically use the ATmethod as de-   scribed in Sec . 3 of the original paper ( Miyato   et al . , 2016 ) .   7.ScRNN : We use the implementation and pre-   trained model provided by the original paper   ( Pruthi et al . , 2019 ) that is available at https :   //github.com / danishpruthi/   Adversarial - Misspellings .   B.4 Experimental Settings for Attack   Methods   Since we use external open - source TextAttack ( Mor-   ris et al . , 2020)andOpenAttack ( Zeng et al . ,   2021 ) framework for evaluating the performance   ofS and all defense baselines under adver-   sarial attacks , implementation of all the attacks are   publicly available . Specifically , we use the TextAt-   tack framework for evaluating all the word- andcharacter - level attacks , and use the OpenAttack for   evaluating the sentence - level attack SCPNA .   B.5 Experimental Settings for Training and   Evaluation   For every dataset , we train a single S model   with the best τparameters and evaluate this model   with all of the adversarial attacks . In other words ,   since we have a total of 3 datasets ( Movie Reviews ,   Hate Speech , Clickbait ) and 4 base architectures   ( CNN , RNN , BERT , RoBERTa ) , we train a total   of 12S models for evaluation . This is done   to ensure that we can evaluate the versatility of   S ’s robustness against different types of at-   tacks without making any assumptions on their   strategies . During training , we use a batch size   of 32 , learning rate of 0.005 , gradient clipping of   10.0 .   For every attack evaluation , we generate a new   set of adversarial examples for every pair of attack   method andtarget model . In other words , since   we have a total of 14 different attack methods , 3   datasets , and 4 possible architectures for the base   models , this results in a total of 168 different sets   of adversarial examples to evaluate in Table 4.6674