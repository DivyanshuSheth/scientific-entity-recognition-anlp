  Faisal LadhakEsin DurmusHe HeClaire CardieKathleen McKeownColumbia UniversityStanford UniversityNew York UniversityCornell University   Abstract   Despite recent progress in abstractive summa-   rization , systems still suffer from faithfulness   errors . While prior work has proposed models   that improve faithfulness , it is unclear whether   the improvement comes from an increased   level of extractiveness of the model outputs   as one naive way to improve faithfulness is   to make summarization models more extrac-   tive . In this work , we present a framework for   evaluating the effective faithfulness of summa-   rization systems , by generating a faithfulness-   abstractiveness trade - off curve that serves as a   control at different operating points on the ab-   stractiveness spectrum . We then show that the   baseline system as well as recently proposed   methods for improving faithfulness , fail to con-   sistently improve over the control at the same   level of abstractiveness . Finally , we learn a   selector to identify the most faithful and ab-   stractive summary for a given document , and   show that this system can attain higher faith-   fulness scores in human evaluations while be-   ing more abstractive than the baseline system   on two datasets . Moreover , we show that our   system is able to achieve a better faithfulness-   abstractiveness trade - off than the control at the   same level of abstractiveness .   1 Introduction   Generating abstractive summaries of documents   has been a long - standing goal of summarization .   While there has been tremendous progress towards   this goal ( Kry ´ sci´nski et al . , 2018 ; Dong et al . , 2019 ;   Zhang et al . , 2019 ; Lewis et al . , 2020 ) , abstractive   summarization systems still suffer from faithful-   ness errors ( Cao et al . , 2018 ) , generating informa-   tion that is not present in the original text . This has   led to an increased research in faithfulness evalua-   tion of summarization systems ( Falke et al . , 2019 ;   Kryscinski et al . , 2020 ; Durmus et al . , 2020 ) aswell as methods to improve faithfulness of gen-   erated summaries ( Kang and Hashimoto , 2020 ;   Chen et al . , 2021 ) . Intuitively , one straightfor-   ward way of improving faithfulness of generated   summaries is to copy a larger amount of content   from the source article ( i.e. more extraction ) . Thus ,   any methods that increase the level of extractive-   ness , whether intentionally or not , would improve   faithfulness . Without reported extractiveness , it is   unclear whether prior improvements mainly arise   from increased extractiveness . We argue that in   order to make progress in abstractive summariza-   tion , it is important to tease apart faithfulness im-   provements due to increased extractiveness versus   improvements due to improved abstraction .   In order to tease this apart , we develop a frame-   work for evaluating progress in faithfulness , by con-   sidering the effective faithfulness , i.e. the improve-   ment in faithfulness over a baseline system ( con-   trol ) operating at the same level of extractiveness .   In particular , we split the training examples into dif-   ferent groups by the extractiveness of the summary ,   and train the control models on each group . Each   of these models corresponds to a speciﬁc tradeoff   between abstractiveness and faithfulness , forming   atrade - off curve indicating how much faithfulness   can be improved solely by increasing extractive-   ness . Systems that improve effective faithfulness   should lie above this curve .   Using this framework , we show that the im-   proved faithfulness of recently proposed methods   comes mainly from an increased extractiveness .   We then conduct further analysis to explore whether   it is possible to have a system that can be both more   abstractive and more faithful than the baseline sys-   tem . We train a selector on a small set of human-   annotated data that , given a set of output summaries   with varying levels of extractiveness , picks the most   abstractive output that is faithful to the source . Our   proposed system is both more abstractive and more   faithful than the baseline . Moreover , we show that1410   our system is able to improve the effective faithful-   ness , achieving a better trade - off than the control   at the same point on the abstractiveness spectrum .   To summarize , our contributions are as follows :   1.We present a framework to evaluate the   progress in improving effective faithfulness   of models considering the control at the same   level of extractiveness .   2.We illustrate the importance of considering   effective faithfulness by showing that recently   proposed methods for improving faithfulness   are able to attain higher faithfulness scores   than the baseline , but do not consistently im-   prove over the control curve , indicating that   most of their improvements come from gener-   ating more extractive outputs , on average .   3.We propose a selector that picks the most   abstractive and faithful summary from a set   of possible summaries , and show that this   method gets higher effective faithfulness com-   pared to the existing methods .   2 Dataset   We conduct our study on two English abstractive   summarization datasets , one from the news domain , and one from a non - news domain . For the news do-   main dataset , we decided against using the popular   CNN / Dailymail dataset since its reference sum-   maries tend to be very extractive ( Kedzie et al . ,   2018 ; Bommasani and Cardie , 2020 ) , making it   a poor choice for studying faithfulness in abstrac-   tive summarization . Similarly , we also decided   against using XSum , another popular news summa-   rization dataset , since almost 77 % of the gold ref-   erence summaries contain hallucinations ( Maynez   et al . , 2020 ) . Instead , we opted for Gigaword and   Wikihow , which are datasets with substantial ab-   straction without as much hallucination problems   as XSum . Gigaword reference summaries have   substantially less hallucinations than XSum ( Kang   and Hashimoto , 2020 ) , and WikiHow summaries   tend to be of a higher quality since they are written   and curated by humans ( Koupaee and Wang , 2018 ;   Ladhak et al . , 2020 ) .   Wikihow ( Koupaee and Wang , 2018 ) is a dataset   of how - to articles covering a diverse set of topics ,   collected from the wikihow.com website . Each   article contains several paragraphs detailing step   by step instructions for a procedural task . There are   about 12 M such paragraphs in the dataset , paired   with a one sentence summary .   Gigaword ( Rush et al . , 2015 ) is a headline gener-   ation dataset that contains around 4 M examples,1411extracted from news articles that were collected as   part of the Gigaword corpus ( Graff et al . , 2003 ) .   The model is tasked with generating the headline   of the article given the ﬁrst sentence .   2.1 Dataset Extractiveness   We follow the process detailed by Grusky et al .   ( 2018 ) , and use extractive fragment coverage and   extractive fragment density as the measures of ex-   tractiveness of a given summary . Henceforth we   will refer to these as coverage and density respec-   tively . Coverage is the percentage of words in a   summary that are from the source article . Density   is the average length of the text spans copied from   the document that are contained in the summary . A   summary that copies larger chunks of text from the   source article will have a higher density .   3 Analysis on Metrics of Faithfulness   Recent studies of faithfulness evaluation have pro-   posed model - based automated metrics to detect   whether a given summary is faithful to the source   article . For example , Falke et al . ( 2019 ) ( Entail-   ment ) have studied using pretrained entailment   based methods to assess the probability of the gen-   erated output being entailed by the source article .   Kryscinski et al . ( 2020 ) ( FactCC ) augment hallu-   cinated summaries by applying rule - based trans-   formations to the document sentences and train a   BERT - based model to classify whether the gener-   ated output is faithful . Goyal and Durrett ( 2021 )   ( DAE ) have collected ﬁne - grained annotations to   study word- , dependency- and sentence - level faith-   fulness and use these annotations to train a factual-   ity detection model .   Figure 1 shows the relationship between the av-   erage coverage of the generated outputs ( extrac-   tiveness ) vs. average metric scores ( faithfulness )   assigned to various abstractive summarization mod-   els trained on Gigaword . We observe that there is   a positive correlation between extractiveness and   faithfulness scores , as models whose generated   summaries have a higher average coverage tend   to also get higher scores for each of the faithfulness   metrics . This correlation between exractiveness   and faithfulness makes it unclear whether a model   gets higher factuality scores simply because it is   more extractive or it is capable of generating faith-   ful summaries at the original level of extractiveness . This highlights the need for accounting for extrac-   tiveness in order to compare faithfulness across   different abstractive summarization systems .   4 Evaluating Effective Faithfulness   Given that extractiveness is confounded with faith-   fulness , we propose a framework for evaluating   effective faithfulness , which takes into account the   extractiveness of a system . In order to do this , we   ﬁrst need to determine the faithfulness of a system   operating at a given level of extractiveness . We   call this the Faithfulness - Abstractiveness Tradeoff   and we describe it further in § 4.1 . The effective   faithfulness of a system is then simply the relative   difference between the faithfulness score assigned   to the system , and the score of a system operating   with the same average extractiveness according to   the trade - off curve .   4.1 Faithfulness - Abstractiveness Tradeoff   In order to understand the effectiveness of a pro-   posed system for improving faithfulness , we need   to be able to account for its extractiveness . We   ﬁnetune pre - trained BART models ( Lewis et al . ,   2020 ) for different levels of extractiveness , without   any explicit recourse for improving faithfulness .   We then use these systems to create a faithfulness-   abstractiveness trade - off curve that can serve as   a control to measure the effective faithfulness of   summarization systems . Models that improve effec-   tive faithfulness should lie above the faithfulness-   abstractiveness trade - off curve .   In particular , we sub - sample the training data   into extractiveness quartiles by computing the cov-   erage of the references with respect to the source   articles . We then ﬁne - tune BART on each of these   quartiles to obtain quartile models with varying   level of extractiveness . In addition , we also ﬁne-   tune BART on all of the data , which we call the   baseline .   We collect faithfulness annotations for sum-   maries generated by each of these models for a   random sample of 200articles . We collect three   annotations per example on Amazon Mechanical   Turk asking whether an output is faithful or unfaith-   ful with respect to the corresponding source article .   We then compute the percentage of annotators that   selects " faithful " , and use this as the faithfulness1412Article Once you decide what to outsource , look for the right contractors . Start by asking for refer-   rals from your own professional network . Talk to other business owners and professionals   about how and where they outsource . You can also check professional associations or trade   groups ﬁeld in which you are trying to outsource work . Use other social media platforms   such as Facebook or Twitter to advertise what you are looking for . Alternately , you can   connect with contractors and freelancers on sites such as eLance , Guru and oDesk . These   websites allow business owners to place an ad that describes what kind of work they need to   have done , and contractors respond with their qualiﬁcations and rates . [ TRUNCATED ] ...   Baseline Search for contractors and freelancers to outsource the work .   Q1 Conduct an initial search for qualiﬁed contractors and freelancers .   Q2 Search for qualiﬁed contractors and freelancers to work on your project .   Q3 Search for contractors and freelancers to do the work .   Q4 Look for contractors and freelancers to bid on the work .   Dataset Model Coverage Faithfulness   GigawordBaseline 76.12 83.33   Q1 50.25 71.83   Q2 60.57 79.50   Q3 73.64 86.67   Q4 86.94 89.17   WikihowBaseline 88.28 82.52   Q1 81.34 67.82   Q2 85.34 76.21   Q3 87.59 80.35   Q4 90.19 91.08   score for each example .   Table 2 shows the coverage and faithfulness   scores for the baseline and the quartile models ,   where Q1 is the most abstractive and Q4 is the   most extractive quartile . We observe that the mod-   els that are ﬁne - tuned on more extractive quartiles   produce outputs with signiﬁcantly higher coverage   and faithfulness scores . The baseline model gen-   erates relatively extractive outputs with coverage   closest to Q3 on both Gigaword and Wikihow . Fur-   thermore , we observe that the baseline model has a   higher coverage than the model ﬁne - tuned on Q3   but it has lower faithfulness score for Gigaword . Table 1 shows an article from the Wikihow   dataset and corresponding output summaries gener-   ated by the baseline and each of the quartile mod-   els . We observe that the generated summaries are   very similar in meaning ; however , the output gener-   ated by the Q1 model includes a higher number of   novel words ( i.e. lower coverage ) compared to the   other models while staying faithful to the article .   Conversely , Q4 model has a coverage of 1 in this   example ; all the words generated by this model are   from the source article . On average , the Q1 model   generates outputs that are more abstractive and less   faithful while Q4 generates outputs that are more   extractive and more faithful .   5 Mitigating the Trade - off   5.1 Oracle Experiments   We ﬁrst aim to understand whether it is possible   to mitigate the faithfulness - abstractiveness tradeoff   by designing several oracle experiments where we   have access to human judgments .   baseline + faithfulness ( bf ) . We use the output   from the baseline model if it is faithful ( i.e. at least   two out of three annotators agree that the output   is faithful ) . If the baseline output is not faithful ,   we select the output from the quartile model that   is more extractive than the baseline to see whether   we can have a similar coverage as the baseline but   preserve faithfulness .   baseline + faithfulness - extractiveness ( bfe ) .   This oracle system behaves similar to the one de-   scribed above when the baseline output is unfaith-   ful . However , rather than always selecting the base-1413Dataset Cov . Faithfulness   GigawordBaseline 76.12 83.33   bf 77.74 89.57   bfe 61.87 90.67   qfe 63.55 98.00   WikihowBaseline 82.52 88.28   bf 83.95 92.20   bfe 70.52 91.32   qfe 72.58 98.61   line output when it is faithful , we pick the output   from the quartile model that is more abstractive   than the baseline whenever it is also faithful ac-   cording to human judgement .   quartile + faithfulness - extractiveness ( qfe ) .   Amongst the outputs of all four quartile models ,   we pick the most faithful output with the highest   level of abstractiveness to understand whether it   is possible to generate abstractive output while re-   maining faithful .   Analysis . Table 3 shows the coverage and faith-   fulness of the baseline and each of these oracles   for Gigaword and Wikihow . We observe that it   is possible to be more faithful than the baseline   at a similar level of abstractiveness ( bf ) . Further-   more , we can be more abstractive than the baseline   while being more faithful ( bfe ) . Selecting the most   faithful and abstractive output from the quartile   models achieves a really high faithfulness score   ( 98 % ) while having signiﬁcantly less coverage   than the baseline . This oracle analysis suggests that   it should be possible to build models that can mit-   igate the faithfulness - abstractiveness trade - off by   controlling the level of extractiveness . Given this ,   we further explore whether we can learn a selector   that is capable of doing this selection automatically   to mitigate the faithfulness - abstractiveness trade-   off .   5.2 Loss Truncation   Kang and Hashimoto ( 2020 ) have proposed a   method to adaptively remove high loss examples   to optimize the distinguishability of samples from   the model and the reference . They have shown   that the samples generated by this Loss Truncation   model achieves higher factuality ratings compared   to the baseline methods . We study this method tounderstand where it lies in terms of faithfulness-   abstractiveness trade - off and whether it can achieve   a improved effective faithfulness over the control .   5.3 Dependency Arc Entailment ( DAE )   Goyal and Durrett ( 2020 ) have proposed a factual-   ity evaluation metric ( DAE ) that evaluates whether   each dependency arc in the generated output is   consistent with the input . They show that their   proposed metric works better than existing factu-   ality metrics , while also being able to localize the   parts of the generated output that are non - factual .   Goyal and Durrett ( 2021 ) take advantage of DAE ’s   ability to localize factuality errors , and train a sum-   marization model only on the subset of tokens that   is deemed factual according to the DAE metric .   We follow their methodology to train summariza-   tion models , and assess them using our evaluation   framework .   5.4 Selector Model   We aim to understand whether we can build a   model that achieves a better effective faithfulness   than Loss Truncation . We propose a selector that   can identify the most abstractive but faithful output   to improve this trade - off . We ﬁrst generate four   possible candidate summaries using the quartile   models for each example in the validation set . This   results in outputs with varying levels of extractive-   ness . For our selector , we ﬁne - tune a FactCC model   ( Kryscinski et al . , 2020 ) on the data we collected   to generate the trade - off curve , using 10 - fold cross   validation , to assign faithfulness scores to the gen-   erated summaries ( in the test folds).In addition ,   we learn a threshold for the faithfulness score that   maximizes the area under the ROC curve ( Selector-   ROC ) ( also using 10 - fold cross validation ) . For   each example in the test fold , we select the most   abstractive candidate ( amongst the four possible   candidates from the quartile models ) that is con-   sidered faithful according to the ﬁntuned FactCC   model ( i.e. the faithfulness score is above the tuned   threshold ) .   Instead of maximizing for the area under the   ROC curve , we can also tune the faithfulness thresh-   old to maximize Fscores ( Selector - F ) . Using   Fscore with   < 1allows us to assign a higher   weight to the precision of our selector which would   result in outputs with higher coverage and faithful-   ness.1414Gigaword Wikihow   Coverage Faitfulness Coverage Faithfulness   Baseline 76.12 83.33 82.76 86.94   Loss Truncation 79.55 87.17 84.93 87.84   DAE 78.23 86.33 84.15 88.83   Selector - ROC ( Ours ) 64.58 84.17 78.67 87.84   Selector - F(Ours )    0.5 54.77 76.83 64.24 79.82   0.4 59.79 81.67 67.81 81.71   0.3 60.72 82.00 68.53 83.15   0.2 68.38 86.00 78.67 87.84   0.1 79.92 88.00 84.72 89.19   We ﬁnd that the ﬁne - tuning FactCC is important   since the pre - trained FactCC model is trained on a   different dataset and does not transfer well to our   setttings . This is consistent with the ﬁndings of   Goyal and Durrett ( 2021 ) .   5.5 Results   Table 4 shows the coverage and faithfulness re-   sults for the baseline , Loss Truncation , DAE , and   the selectors . We observe that as we use smaller   values for  for Selector - F , we get more extrac-   tive and more faithful outputs . This allows us to   have a trade - off between faithfulness and abstrac-   tiveness . Moreover , with both Selector - ROC and   Selector - F , we produce output with less cover-   age but higher faithfulness scores than the baseline .   For Wikihow , Selector - ROC produces outputs with   lower coverage but similar faithfulness scores to   Loss Truncation . We can further obtain a higher   faithfulness score at a similar coverage level as   DAE and Loss truncation with Selector - Fwith   = 0:1 . For Gigaword , Select - ROC produces   output with signiﬁcantly lower coverage than Loss   Truncation and DAE . Selector - Fproduces output   with similar coverage to Loss Truncation with a   higher faithfulness score (  = 0:1 ) .   It is important to understand whether models im-   prove faithfulness by simply being more extractive   or if they are able to improve effective faithfulness .   In order to understand this , we measure whether the   models get improvement in faithfulness over the   control operating at the same level of extractiveness .   In Figure 2 , we plot the faithfulness - abstractiveness   curve with the faithfulness and abstractiveness ofthe quartile models . If a model lies above this   curve , it improves the effective faithfulness . If the   model is below this curve , it is not able to improve   theeffective faithfulness and it has a worse trade-   off than the control operating at the same level of   extractiveness .   For both Gigaword and Wikihow , Selector - ROC   lies above the curve improving this trade - off . How-   ever , both the baseline and Loss Truncation models   get worse trade - off than the control operating at   the same level of extractiveness . Similarly , we   can obtain several models that lie above the curve   for both Gigaword and Wikihow using Selector-   F. The selector approach allows us to get bet-   tereffective faithfulness at different points in the   abstractiveness - extractiveness spectrum . The DAE   based model is able to improve effective faithful-   ness on the Wikihow dataset , but not on the Giga-   word dataset , indicating that the improvements are   not consistent across datasets . Table 5 shows ex-   ample summaries generated by the baseline , Loss   Truncation , DAE and the Selector - ROC models .   We observe that selector model is able to generate   summaries that are faithful to the original article   while having more novel words and phrases in the   generated summaries .   6 Related Work   There has been a lot of recent work in abstractive   summarization showing that state - of - the - art sys-   tems suffer from generating inconsistent informa-   tion with respect to the source article , despite their   improved success in producing ﬂuent summaries1415   ( Falke et al . , 2019 ; Lux et al . , 2020 ; Wilber et al . ,   2021 ) . Since word - overlap based metrics such as   ROUGE have low correlation with human scores of   faithfulness ( Kryscinski et al . , 2019 ; Fabbri et al . ,   2020 ) , there has been signiﬁcant effort to develop   automated metrics that can detect such errors ( Zhou   et al . , 2021 ; Gabriel et al . , 2021 ; Pagnoni et al . ,   2021a ) . For example , Falke et al . ( 2019 ) , Maynez   et al . ( 2020 ) and Goyal and Durrett ( 2020 ) have   proposed to assess faithfulness using entailment   models , where a faithful summary should be as-   signed a high entailment score with respect to the   original article . Kryscinski et al . ( 2020 ) presented   FactCC , a weakly - supervised BERT - based entail-   ment model , by augmenting the dataset with artiﬁ-   cial faithfulness errors . Durmus et al . ( 2020 ) and   Wang et al . ( 2020 ) proposed question - answering   based evaluation frameworks by automatically gen-   erating questions from the generated summary , and   comparing the corresponding answers from both   the source and the generated summary in order   assess information consistency . Furthermore , sev - eral benchmarks have been proposed to evaluate   the strengths and weaknesses of these evaluation   metric ( Gabriel et al . , 2021 ; Pagnoni et al . , 2021b ) .   Previous studies in faithfulness evaluation , how-   ever , has not accounted for the effect of extractive-   ness of the output summaries . As we show in this   study , the extractiveness of the output is correlated   with the faithfulness scores assigned by these au-   tomated metrics . Therefore , it is not clear whether   the models with higher scores are better at abstrac-   tion , or extract more from the source article . We   suggest that we need to account for this confound-   ing factor in order to assess the real progress in   building models that are better at abstraction . We   note that there is concurrent work that also argues   for accounting for extractiveness in assessing the   faithfulness of models ( Dreyer et al . , 2021 ) , how-   ever , unlike our work , they do they do not propose   any mitigation for the faithfulness - abstractiveness   trade - off .   Improving faithfulness of summarization sys-   tems is essential for deploying these systems in real-1416Article If applicable , the description of any people who take part in your study   should be extremely thorough . Each person should be identiﬁable within   the research . Further , how people join and leave the study should be noted .   If people were selected at random , or if they were family members , is   important to the study . Be sure to consider various ethical concerns ( e.g. risk   and consent of participants ) if people are involved in your research .   BaselineDescribe who is involved in the study .   DAEIdentify the people who take part in the study .   Loss TruncationDescribe people who take part in your study .   Selector - ROC   ( Ours)Describe all participants thoroughly and with care .   Article Because diarrhea frequently causes dehydration , it is crucial that patients   with IBD remain hydrated . Drink at least 8 glasses of water every day ( or 64   oz ) . Foods that have a high water content ( like watermelon ) can also count   toward this minimum . If you have a severe attack of diarrhea , you are likely   to lose electrolytes . In these cases , you might need to consume beverages   such as Pedialyte or Gatorade to help replenish them [ TRUNCATED ] ...   Baseline Drink plenty of water to stay hydrated .   Loss Truncation Drink plenty of water .   DAE Drink plenty of water to stay hydrated .   Selector - ROC   ( Ours)Drink plenty of ﬂuids to stay hydrated .   world scenarios , as such recent work has studied   methods to improve the faithfulness of abstractive   summarization systems ( Matsumaru et al . , 2020 ;   Zhao et al . , 2020 ; Dong et al . , 2020 ; Goyal and   Durrett , 2021 ; Xu et al . , 2020 ; Chen et al . , 2021 ;   Zhu et al . , 2021 ) . For example , Goyal and Durrett   ( 2021 ) train summarization systems by modifying   the training objective to maximize the likelihood   of the subset of summary tokens that are consid-   ered faithful according to their factuality detection   model . Zhao et al . ( 2020 ) speciﬁcally target hallu-   cination of quantities in generated summaries , and   train a veriﬁcation model that they use to re - rank   summaries such that summaries containing quanti-   ties consistent with the source article are up - ranked .   Although these methods have shown improvements   over the compared baselines , unlike our work , they   do not measure the effective faithfulness taking ex-   tractiveness of the generated outputs into account .   7 Implications and Limitations   Recent studies that propose methods to improve   faithfulness evaluate progress by conducting hu-   man evaluation on generated summaries and checkwhether the faithfulness scores are higher for their   proposed system as compared to their baselines .   We show that there is a strong relationship between   the extractiveness and faithfulness of generated out-   puts ( i.e. , more extractive outputs tend to be more   faithful ) , and therefore we can not simply disregard   extractiveness in faithfulness evaluation .   We propose that we should instead be measur-   ingeffective faithfulness and introduce a frame-   work that takes into account the faithfulness-   abstractiveness trade - off curve that is generated   by training control models at different points in   the abstractiveness spectrum . We demonstrate the   importance of measuring effective faithfulness by   showing that recently proposed methods that im-   prove faithfulness over the baseline fails to consis-   tently improve over a simple control operating at   the same level of abstractiveness .   We argue that measuring effective faithfulness   is important since our goal is to build abstractive ,   faithful summarization systems . If the objective   was to optimize for faithfulness alone , we could   do so by simply building more extractive systems   ( such as the Q4 model we trained above).1417Limitations . Note that this method relies on   some diversity in the extractiveness of reference   summaries , since we rely on sub - sampling to train   models for the control . It is less likely to be effec-   tive for datasets with very little variation in the ex-   tractiveness of the generated summaries . However ,   in general , we see signiﬁcantly more faithfulness   problems for datasets with higher diversity of ab-   stractiveness . Therefore , we suggest to account for   the faithfulness - abstractiveness trade - off for such   datasets in future work .   8 Acknowledgements   This work was partly supported by the Ofﬁce of   the Director of National Intelligence ( ODNI ) , In-   telligence Advanced Research Projects Activity   ( IARPA ) , via contract # FA8650 - 17 - C-9117 , Sam-   sung Advanced Institute of Technology ( Next Gen-   eration Deep Learning : From Pattern Recognition   to AI ) , and a collaborative grant from Amazon   to the Columbia Center for Artiﬁcial Intelligence   entitled “ Extremely Abstractive Summarization ” .   The views and conclusions contained herein are   those of the authors and should not be interpreted   as necessarily representing the ofﬁcial policies , ei-   ther expressed or implied , of the funding agencies .   We further thank the anonymous reviewers and the   Stanford NLP group for their helpful feedback .   References14181419A Data Statistics   Number of examples , source article length and tar-   get summary length for each quartile are shown in   Table 6 . To create the quartiles , we ﬁrst compute   the extractiveness ( e ) of the reference summary ,   for each training example x , and compute the 25th   ( a ) , 50th ( b ) , and 75th ( c ) percentile of the extrac-   tiveness of the training data . The quartiles are then   created as follows :   q1 = fxjeag   q2 = fxja < ebg   q3 = fxjb < ecg   q4 = fxje > cg   Note that it is possible for there to be several   points at the boundary , and therefore there are   unequal number of examples in each quartile as   shown in Table 6 . For Gigaword , the article and   summary lengths are very similar for each of the   quartiles . For Wikihow , we observe that the article   length is longer and summary length is shorter for   more extractive quartiles .   B Human Annotation Details   We follow a similar procedure as the prior work to   collect human evaluations for faithfulness of the   generated summaries ( Fabbri et al . , 2020 ) . Given   the source articles and generated summaries , we   ask annotators to judge whether the generated sum-   mary is supported by the article . The output is   supported by the article if all the information ex-   pressed by the output can also be inferred from the   article . We ask annotators to ignore minor gram-   matical errors and focus on the information content   of the generated summaries . Figure 3 shows an   example from our human evaluation .   Computing faithfulness scores . We evaluate   200output summaries per system and each output   is evaluated by 3annotators . We restricted the   study to the annotators with a high acceptance rate   ( 98 % ) and at least 500 HITs to ensure annotation   quality . We follow prior work ( Durmus et al . ,   2020 ) and take the percentage of annotators who   judge the summary as faithful to be the faithfulness   score of a summary . To get the faithfulness score   for a system , we average the summary scores across   all200samples.1420Dataset Quartile # Examples Article Length Summary Length   GigawordQ1 985,931 30.58 8.03   Q2 961,970 32.02 8.32   Q3 952,833 31.77 8.41   Q4 903,223 31.05 8.17   WikihowQ1 328,470 50.73 7.63   Q2 221,452 75.69 7.40   Q3 206,558 85.44 5.96   Q4 243,837 92.09 5.49   C Model details   For all summarization models , we ﬁnetune BART   ( 406 M parameters ) on a single Nvidia A-100 GPU .   Each model takes roughly 3hours to train to con-   vergence . For the selector , we ﬁnetune FactCC , on   a single Nvidia A-100 GPU , using 10 - Fold cross   validation . Finetuning for the entire cross valida-   tion procedure takes roughly 15minutes . We used   all artifacts according to the terms indicated in their   respective licenses.1421