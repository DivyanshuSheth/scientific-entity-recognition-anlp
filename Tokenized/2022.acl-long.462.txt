  Rajkumar Pujariand Erik Ovesonand Priyanka Kulkariand Elnaz NouriPurdue UniversityMicrosoft , RedmondMicrosoft Research , Redmond   Abstract   As large Pre - trained Language Models ( PLMs )   trained on large amounts of data in an unsuper-   vised manner become more ubiquitous , iden-   tifying various types of bias in the text has   come into sharp focus . Existing ‘ Stereotype   Detection ’ datasets mainly adopt a diagnostic   approach toward large PLMs . Blodgett et al .   ( 2021a ) show that there are significant reliabil-   ity issues with the existing benchmark datasets .   Annotating a reliable dataset requires a precise   understanding of the subtle nuances of how   stereotypes manifest in text . In this paper , we   annotate a focused evaluation set for ‘ Stereo-   type Detection ’ that addresses those pitfalls by   de - constructing various ways in which stereo-   types manifest in text . Further , we present a   multi - task model that leverages the abundance   of data - rich neighboring tasks such as hate   speech detection , offensive language detection ,   misogyny detection , etc . , to improve the empiri-   cal performance on ‘ Stereotype Detection ’ . We   then propose a reinforcement - learning agent   that guides the multi - task learning model by   learning to identify the training examples from   the neighboring tasks that help the target task   the most . We show that the proposed models   achieve significant empirical gains over exist-   ing baselines on all the tasks .   1 Introduction   Following Peters et al . ( 2018 ) and then Vaswani   et al . ( 2017 ) , several empirically effective large   pre - trained language models ( PLMs ) ( Devlin et al . ,   2019 ; Yang et al . , 2019 ; Liu et al . , 2019 ; Lewis   et al . , 2020 ; Brown et al . , 2020 ) have been pro-   posed . Their empirical success on several existing   tasks has resulted in them being ubiquitously used   in many NLP applications that interact with hu-   mans on a daily basis ( Olteanu et al . , 2020 ) . As   they are trained in an unsupervised manner on large   amounts of arbitrary web data , harmful languageand bias creeps into their output . This , in turn , is   reflected in the applications that are built on top of   them and further propagated in society through the   use of those applications ( Berk , 2017 ) . This issue   motivates the need for a two - pronged solution : 1 )   to diagnose and de - noise the bias in the PLMs and   2 ) to identify and regulate harmful text externally   at the output level . Progress on the first front is   necessary to de - bias the PLMs and prevent subtle   biases that creep in through their distributed repre-   sentations which can often only be recognized in   retrospect . This results in deep distrust of such sys-   tems among general population . The second front   is paramount to detect and potentially block harm-   ful output and also ensure that future iterations and   novel proposals of the language models do n’t take   spurious paths to gaming the de - biasing techniques .   In this work , we focus on identifying harmful text   with stereotypical associations externally .   S : She may or may not be a jew but she ’s cer-   tainly cheap ! ( insult , stereotype )   S : Burn in hell , you Asian bastard ! ( abuse ,   stereotype )   S : Asians are good at math . ( stereotype )   S : My African - American friend owns a water-   melon patch . ( stereotype )   There exist several types of harmful language   such as hate - speech , misogyny , stereotypes , abuse ,   threats , insult etc , . Each type of offensive language   has subtle linguistic nuances that are specific to   the type of offensive language . Often , offensive   text contains multiple types of offense . From the   examples above , consider SandS. Both , consist   of multiple modes of offense . While Sis purely a   stereotype , it is still undesirable to be perpetuated .   Cardwell ( 1996 ) defines stereotype as a “ fixed ,   over - generalized belief about a particular group   or class of people ” . Stereotypes differ from other   types of offensive text in two key aspects : ( 1 ) they   require knowledge of their existence in the soci-   ety to be identified , and ( 2 ) they might also often6703express positive sentiment about the target group .   Although some stereotypes ostensibly express pos-   itive sentiment towards the target group , they are   still undesirable as they propagate false biases in   the society and are offensive to the target group .   Consider sentences SandSfrom above exam-   ples . While Sexpresses positive sentiment , it is   still false and undesirable . Srequires knowledge   of that particular stereotype ’s history to understand   its offensive nature . Requiring prior knowledge   makes annotating data for the task of ‘ Stereotype   Detection ’ harder , as annotators are unlikely to be   aware of all the stereotypes that exist in the society .   ( Czopp , 2008 ) .   Two recent works have proposed pioneering di-   agnostic datasets for measuring stereotypical bias   of large PLMs ( Nadeem et al . , 2020 ; Nangia et al . ,   2020 ) . But , Blodgett et al . ( 2021b ) has demon-   strated that these datasets suffer from two major   types of issues : ( 1 ) conceptual : include harmless   stereotypes , artificial anti - stereotypes , confusing   nationality with ethnicity etc , and ( 2 ) operational :   invalid perturbations , unnatural text , incommen-   surable target groups etc , . In addition , diagnostic   datasets also suffer from lack of sufficient coverage   of subtle nuances of manifestations of stereotypes   in text . This makes them less suitable for training   an effective discriminative classifier . Hence , we   undertake a focused annotation effort to create a   fine - grained evaluation dataset . We mainly aim   to alleviate the conceptual issues of anti- vs. non-   stereotypes , containing irrelevant stereotypes and   operational issues of unnatural text , invalid pertur-   bations . We achieve this by a mix of ( 1 ) selecting   more appropriate data candidates and ( 2 ) devising   a focused questionnaire for the annotation task that   breaks down different dimensions of the linguistic   challenge of ‘ Stereotype Identification ’ . Collecting   real - world data from the social forum Reddit for   annotation also results in better coverage of subtle   manifestations of stereotypes in text .   Although stereotypes differ from other types of   offensive language in multiple ways , they also over-   lap to a significant extent . Often , various types   of offensive text such as abuse , misogyny and   hate speech integrally consists stereotypical as-   sociations . Abundance of high - quality annotated   datasets are available for these neighboring tasks .   We leverage this unique nature of Stereotype De-   tection task to propose a multi - task learning frame-   work for all related tasks . As the overlap betweenthe tasks is only partial , we then propose a rein-   forcement learning agent that learns to guide the   multi - task learning model by selecting meaningful   data examples from the neighboring task datasets   that help in improving the target task . We show   that these two modifications improve the empirical   performance on all the tasks significantly . Then ,   we look more closely at the reinforcement - learning   agent ’s learning process via a suite of ablation stud-   ies that throw light on its intricate inner workings .   To summarize , our main contributions are :   1.We devise a focused annotation effort for   Stereotype Detection to construct a fine-   grained evaluation set for the task .   2.We leverage the unique existence of sev-   eral correlated neighboring tasks to propose   a reinforcement - learning guided multitask   framework that learns to identify data exam-   ples that are beneficial for the target task .   3.We perform exhaustive empirical evaluation   and ablation studies to demonstrate the effec-   tiveness of the framework and showcase intri-   cate details of its learning process .   2 Related Work   With the rise of social media and hate speech fo-   rums online ( Phadke and Mitra , 2020 ; Szendro ,   2021 ) offensive language detection has become   more important that ever before . Several recent   works focus on characterizing various types of of-   fensive language detection ( Fortuna and Nunes ,   2018 ; Shushkevich and Cardiff , 2019 ; Mishra et al . ,   2019 ; Parekh and Patel , 2017 ) . But , works that   focus solely on Stereotype Detection in English   language are scarce . This is partly because stereo-   types tend to be subtler offenses in comparison   to other types are offensive languages and hence   receive less immediate focus , and in part due to   the challenge of requiring the knowledge of the   stereotype ’s existence in society to reliably anno-   tate data for the task . We approach this problem   by breaking down various aspects of stereotypical   text and crowd - sourcing annotations only for as-   pects that require linguistic understanding rather   than world - knowledge .   Few recent works have focused solely on   stereotypes , some proposing pioneering diagnos-   tic datasets ( Nadeem et al . , 2020 ; Nangia et al . ,6704   2020 ) while others worked on knowledge - based   and semi - supervised learning based models ( Fraser   et al . , 2021 ; Badjatiya et al . , 2019 ) for identify-   ing stereotypical text . Computational model based   works either use datasets meant for other tasks such   as hate speech detection etc , or focus mainly on the   available diagnostic datasets modified for classifica-   tion task . But , diagnostic datasets suffer from lack   of sufficient coverage of naturally occurring text   due to their crowd - sourced construction procedure   ( Blodgett et al . , 2021b ) . We address these issues in   our work by collecting natural text data from social   forum Reddit , by mining specific subreddits that   contain mainly subtle stereotypical text .   Multi - task learning ( Caruana , 1997 ) , can be   broadly classified into two paradigms ( Ruder ,   2017 ): hard parameter sharing ( Caruana , 1997 )   and soft parameter sharing ( Yang and Hospedales ,   2016 ; Duong et al . , 2015 ) . We implement hard-   parameter sharing based multi - task model for our   experiments .   Given the low - resource setting on Stereotype De-   tection task , semi - supervised data annotation is one   plausible solution for the problem . Several recent   works have also been focusing on reinforcement-   learning guided semi - supervision ( Ye et al . , 2020 ;   Konyushkova et al . , 2020 ; Laskin et al . , 2020 ) .   Ye et al . ( 2020 ) , in particular , work with a single-   task and unsupervised data to generate automated-   annotations for new examples . In contrast , we use   the data from neighboring tasks with different la-   bels for multi - task learning and apply an RL agent   to select examples for training the neighboring task   in such a way that benefits the target task the most .   3 Our Dataset   As Blodgett et al . ( 2021a ) demonstrates , existing di-   agnostic datasets such as Stereoset ( Nadeem et al . ,   2020 ) and CrowS - Pairs ( Nangia et al . , 2020 ) suf-   fer from several conceptual and operational issues .   Additionally diagnostic datasets , by nature , also   suffer from lack of coverage of subtle manifesta-   tions of stereotypes in text . But , this is mandatoryin the dataset used to train an effective discrimina-   tory classifier that can generalize well to identify-   ing stereotypical associations in open domain use   cases . This is often the case in the examples en-   countered in downstream applications that interact   with humans on a daily basis .   Nadeem et al . ( 2020 ) and Nangia et al . ( 2020 )   both follow the following dataset construction pro-   cedure : they provide a prompt word or phrase ,   which is the target group of the stereotype , and ask   the crowd workers to construct a stereotypical sen-   tence which includes the provided prompt . Then ,   they either flip the identity of the target group ( Nan-   gia et al . , 2020 ) , the associated characteristic / action   of the group or both ( Nadeem et al . , 2020 ) to obtain   negative ( anti - stereotypes ) examples .   Although this process results in good coverage   of existing stereotypes within society due to the va-   riety of prompts provided , it also results in data   examples that either explicitly state the stereo-   type or are anti - stereotypes that are mostly con-   trived sentences and are unlikely to occur in regu-   lar discourse . An example from Stereoset dataset   ( Nadeem et al . , 2020 ) is shown below to illustrate   this point . 1is an explicitly states stereotype where   as2and3are unlikely to occur in regular discourse .   1 . He is an Arab . Must be a terrorist . ( stereotype )   2 . He is an Arab . Must be a pacifist . ( anti-   stereotype )   3 . He is an Arab . Dog needs walk . ( unrelated )   We address this issue by additionally annotating   text from actual discussions from Reddit to create   a more representative dataset for the task . We use   the collected examples for a crowd - sourced anno-   tation task to build the dataset . The annotation   task is explained in further detail below . We use   the prompts from Nadeem et al . ( 2020 ) and Nangia   et al . ( 2020 ) to collect candidates from selected sub-   reddits which are likely to contain a good coverage   of stereotypical and non - stereotypical text . We also   use the explicit stereotypes from the Stereoset and   CrowS Pairs datasets as candidates . Then , we de-   sign a focused annotation task for these candidates6705to identify various types of Stereotypes .   3.1 Construction Procedure   We make the key observation that there are three   broad classes of text that contain potentially harm-   ful over - simplified beliefs : 1 ) text that is mainly   intended to express a stereotype ( explicit stereo-   type),2 ) text that is not mainly intended to convey   a stereotype but nevertheless propagates a stereo-   typical association ( implicit stereotypical associa-   tion ) , and 3 ) text that expresses an over - simplified   belief which is notthat widely - known , hence is   not considered stereotypical ( anti - stereotype ) . In   addition , there is 4 ) text that does n’t contain any   over - simplified beliefs about a group of people . We   call this type of text as non - stereotypes . Examples   of different types text are shown in table 1 .   Nadeem et al . ( 2020 ) and Nangia et al . ( 2020 )   are mainly made up of explicit - stereotypes and   anti - stereotypes . They lack coverage of implic-   ity stereotypical associations andnon - stereotypes   due to their construction process and the nature of   diagnostic datasets . These are necessary to build a   task - representative classification dataset . Hence , in   our annotation task we aim to add non - stereotypical   examples that contain the same prompts as the   ones that are used to create the stereotypical ex-   amples . To this end , we collect candidate ex-   amples from two subreddit forums /r / Jokes and   /r / AskHistorians . We observe that /r / Jokes consists   of a high percentage of text with stereotypical as-   sociations ( both explicit andimplicit stereotypes )   and / r / AskHistorians tends to consist mainly factual   text that is carefully constructed to avoid stereotyp-   ical associations . We collect examples that contain   the prompts provided in the Nadeem et al . ( 2020 )   dataset as candidates for annotation . We also use   the explicit stereotypes from Stereoset andCrowS-   Pairs datasets for annotation . We perform anno-   tation using Amazon Mechanical Truk . For each   candidate sentence , we ask the annotators to answer   the following questions :   1.Is there an over - simplified belief about a   particular type of person “ intentionally ” ex-   pressed in the text ?   2.Is there an “ unintentional ” , widely - known   stereotypical association present in the text ?   3.Does the sentence seem made up ( unlikely to   occur in regular discourse ) ?   Each example is annotated by three annotators and   we use the majority answer as the gold label . Thisannotation allows us to separate the text into one   of the above 4categories . Our dataset consists of   742explicit stereotypes , 282implicit stereotypes   and1,197non - stereotypes . We show the summary   statistics of the annotated dataset in table 2 .   3.2 Ethics Statement   We conducted a qualification test to select workers   based on their performance . The workers were paid   a bonus of USD 0.10for taking the qualification   text . We paid USD 0.25for a batch of 10examples ,   each batch taking 45 - 60seconds on average . This   amounts to USD 15−20 / hour . We displayed a   warning on the task that said that the task might   contain potentially offensive language . We did n’t   collect any personal identifying information of the   workers other than their worker ID for assigning   qualifications . We restricted the workers location   to the USA with minimum of 5,000approved HITs   and98 % HIT approval rate .   Data Type Size   Explicit Stereotypes 742   Implicit Stereotypes 282   Non - Stereotypes 1,197   Total Examples 2,221   4 Model   As discussed in section 1 , high - quality gold data   forStereotype Detection is scarce . But , several   tasks with correlating objectives have abundance   of high - quality annotated datasets . We observe that   several tasks under the general umbrella of Offen-   sive Language Detection such as Abuse Detection ,   Hate Speech Detection & Misogyny Detection of-   ten include text with stereotypical associations , as   demonstrated in examples SandSin section 1 .   We call these tasks neighboring tasks . We leverage   the neighboring task datasets to improve the per-   formance on the low - resource setting of Stereotype   Detection . First , we propose a multi - task learning   model for all the tasks . Then , we make the key ob-   servation that “ all examples from the neighboring   tasks are notequally useful for the target task ” as   the objectives only overlap partially . Further , we   propose a reinforcement - learning agent , inspired   from Ye et al . ( 2020 ) , that learns to select data ex-   amples from the neighboring task datasets which   are most relevant to the target task ’s learning ob-6706jective . We guide the agent via reward assignment   based on shared model ’s performance on the evalu-   ation data of the target task . We experiment both   the settings with 4popular large PLMs as base clas-   sifiers and demonstrate empirical gains using this   framework .   In subsection 4.1 , we describe the multi - task   learning ( MTL ) model followed by the Reinforce-   ment Learning guided multi - task learning model   ( RL - MTL ) in subsection 4.2 . Then , in subsection   5.1 , we describe the baseline classifiers we use for   our experiments .   4.1 Multi - Task Learning Model   The motivation behind our Multi - Task Learning   model is to leverage the transfer learning gains   from the neighboring tasks to improve the target   task . As the tasks have partially overlapping objec-   tives , solving the selected neighboring tasks effec-   tively requires an understanding of largely similar   linguistic characteristics as the target task . Hence ,   leveraging the intermediate representations of the   text from the neighboring task to boost the classifier   is expected to benefit the target task .   Following this motivation , our proposed multi-   task model consists of a fixed PLM - based repre-   sentation layer , followed by shared parameters that   are common for all the tasks . Then , we add sep-   arate classification heads for each task . We im-   plement hard parameter sharing ( Caruana , 1997 ;   Ruder , 2017 ) in our model . The shared parame-   ters compute intermediate representations for the   text input . These intermediate representations are   shared by all the tasks . Parameters for the shared   representation layers are first optimized by training   on the neighboring tasks . Then , they are leveraged   as a more beneficial parameter initialization for   training on the target task data .   The input to the multi - task model is the text of   the data example and a task ID . Output of the model   is predicted label on the specified task . Each task   in the model could either be a single - class classifi-   cation task or a multi - label classification task . Clas-   sification heads for single - class classification tasks   have a softmax layer after the final layer . Multi-   label tasks have a sigmoid layer for each output   neuron in the final layer of the classification heads .   First , we jointly train the model on each of the   neighboring tasks in a sequential manner . Then ,   we train the multi - task model on the target task and   evaluate it on the test set of the target task.4.2 Reinforcement Learning Guided MTL   The RL - guided multi - task model has an addi-   tional RL agent on top of the MTL model to select   examples from the neighboring task datasets that   would be used to train the shared classifier . Key   intuition behind the introduction of the RL agent is   that , not all data examples from the neighbor task   are equally useful in learning the target task . Ar-   chitecture of the RL - guided MTL model is shown   in figure 1 .   Following the above observation , we employ   the agent to identify examples that are useful   for the target objective and drop examples that   distract the classifier from the target task . The   agent is trained using an actor - critic reinforcement   paradigm ( Konda and Tsitsiklis , 2000 ) . For each   example in the neighbor task , the Actor decides   whether or not to use it for training the shared clas-   sifier . Critic computes the expected reward based   onActor ’s actions for a mini - batch . Upon training   using the selected examples , we then assign reward   to the agent by evaluating the performance of the   shared classifier on the target task . If the Fscores   on the valuation set for bmini - batches , each of size   z , are { F , F , . . .,F } and expected rewards pre-   dicted by the critic are { e , e , . . .,e } , then the   policy loss is computed as follows :   ˆF = F−µ   σ+ϵ(1)6707p=−1   bΣ(ˆF−e)×1   zΣlog(P[a])(2 )   v=1   bΣL - loss(1,ˆF ) ( 3 )   total loss = policy loss ( p ) + value loss ( v ) ( 4 )   where ϵis a smoothing constant , ais the action   decided by the Actor for the jexample of mini-   batch i,µandσare mean and standard devia-   tions of the macro- Fscores , respectively .   The algorithm for RL - guided Multitask learning   is shown in algorithm 1 . Input to the RL - MTL   model is a set of neighboring task datasets and   a target task dataset . Output is trained classifier   C. We initialize the parameters of the RL - MTL   base classifier with the trained parameters of the   MTL model . Later , we evaluate the impact of this   initialization via an ablation study in section 7.1 .   Algorithm 1 RL - Guided MTL   Require : Neighbor Datasets { N , N , . . .,N } ,   Target Dataset T   Parameters : Policy Network Pthat includes Actor   Network Aand Critic Network RSelect baseline classifier Cforepisode i = 1,2 , . . .,edo forneighbor dataset j = 1,2 , . . .,ddo formini - batch k = 1,2 , . . .,bdo Actor Network Amakes binary SE-   LECT / REJECT decision for each ex-   ample in N Critic Network Rcomputes expected   reward based on examples selected by   ActorA = E[r ] TrainCon the SELECTED mini - batch   subset N Evaluate on Target Dataset Tand ob-   tainFon target dataset evaluation set   F end for UseFs and E[r]s to compute loss   according to equation 4 Update parameters of AandR end forend forreturn Trained classifier C   5 Experiments   We perform experiments on sixdatasets in three   phases . In the first phase , we experiment withPLM - based fine - tuned classifiers for each task as   baselines . In the second phase , we experiment   with all the tasks using the multi - task learning   model described in section 4.1 , with each PLM   as a base classifier . In the third phase , we train the   reinforcement - learning guided multi - task learning   framework ( section 4.2 ) for all the tasks with each   of the PLMs as base classifier .   5.1 Base Classifiers   We select four popular PLMs as base classifiers   for our empirical experiments , namely , BERT-   base , BERT - large ( Devlin et al . , 2019 ) , BART - large   ( Lewis et al . , 2020 ) and XLNet - large ( Yang et al . ,   2019 ) . We use the implementations from Wolf et al .   ( 2020 ) ’s huggingface transformers libraryfor ex-   perimentation . We fine - tune a classification layer   on top of representations from each of the PLMs   as baseline to evaluate our framework .   5.2 Datasets   We use sixdatasets for our empirical evaluation ,   namely , Jigsaw Toxicity Dataset , Hate Speech De-   tection ( de Gibert et al . , 2018 ) , Misogyny Detection   ( Fersini et al . , 2018 ) , Offensive Language Detec-   tion ( Davidson et al . , 2017 ) , coarse - grained Stereo-   type Detection ( combination of Stereoset , CrowS-   Pairs and Reddit Data ) and finally fine - grained   Stereotype Detection Data ( as described in section   3 ) . We describe each dataset briefly below .   Hate Speech Detection ( de Gibert et al . , 2018 )   dataset consists of 10,944data examples of text   extracted from Stromfront , a white - supremacist   forum . Each piece of text is labeled as either hate   speech ornot .   Misogyny Detection ( Fersini et al . , 2018 ) dataset   consists of 3,251data examples of text labeled   with the binary label of being misogynous ornot .   Offensive Language Detection ( Davidson et al . ,   2017 ) dataset was built using crowd - sourced hate   lexicon to collect tweets , followed by manual an-   notation of each example as one of hate - speech ,   only offensive language orneither . This dataset   contains 24,783examples .   Coarse - Grained Stereotype Detection : We create   this dataset by combining stereotypical examples   from Stereoset andCrowS - Pairs datasets to get   positive examples , followed by adding negative   examples from the subreddit /r / AskHistorians . We6708do not use crowd sourced labels in this dataset .   We use the labels from the original datasets . The   dataset consists of 23,900data examples .   Fine - Grained Stereotype Detection : This dataset   is the result of our annotation efforts in section 3 .   It consists of 2,221examples , each annotated with   one of three possible labels : explicit stereotype ,   implicit stereotype and non - stereotype .   Jigsaw Toxicity Datasetconsists of 159,571   training examples and 153,164test examples la-   beled with one or more of the seven labels : toxic ,   severely toxic , obscene , threat , insult , identity hate ,   none . We use this data only for training . We do n’t   evaluate performance on this dataset .   6 Results   We present the results of the empirical evaluation   tasks in table 3 . In Hate Speech Detection task , we   observe that RL - MTL learning results in signifi-   ca nt improvements over all the baseline classifiers .   Plain MTL model also improves upon the base-   line classifiers except in the case on BART - large .   The best model for this task is BERT - base + RL-   MTL which achieves a macro - F1 score of 72.06   compared to 68.91obtained by the best baseline   classifier . Best MTL model obtains 69.78F1 .   ForHate Speech and Offensive Language Detec-   tiontask , the respective numbers for baseline , MTL   and RL - MTL models are 66.13,68.57and68.97 .   The models achieve 74.16,74.40and75.21on   Misogyny Detection task , respectively . In Coarse-   Grained Stereotype Detection task , they achieve   65.71,68.29&74.18 , which is a significant grada-   tion over each previous class of models . On our   focus evaluation set of Fine - Grained Stereotype De-   tection , we achieve 61.36,65.00&67.94 in each   class of models . The results on this dataset are   obtained in a zero - shot setting as we only use this   dataset for evaluation .   7 Analysis & Discussions   In the first ablation study described in subsection   7.1 , we study the importance of initializing RL-   MTL model with the trained parameters of MTL   model . Following that , we look into more detail   about the usefulness of neighbor tasks on the tar-   get task via an ablation study . We describe these   experiments in further detail in subsection 7.2.7.1 Impact of MTL Prior on RL - MTL   In our original experiments , we initialize the param-   eters of RL - MTL model with trained parameters   from the MTL model . This allows the RL agent to   begin from a well - optimized point in the parameter   sample space . In this ablation study , we initialize   the RL - MTL model from scratch to see how it im-   pacts the performance of the RL - MTL model . We   perform this experiment with BERT - base as base   classifier . The performance of the RL- MTL model   without initialization drops to 70.23on HS task ,   67.23on HSO task , 71.10on MG task , 60.42on   CG - ST task and 57.32on FG - ST task . The respec-   tive numbers for the MTL initialized model are   72.06,68.97,74.78,74.18and65.72 . Initializa-   tion has biggest impact on the Coarse- andFine-   Grained Stereotype Detection tasks . Overall , ini-   tialization with MTL trained parameters results in   a better convergence point for the RL - MTL model .   7.2 Neighbor - Task Ablation Study   In this task , we aim to study the neighbor tasks   that are most useful for each target task . For   each dataset , we train RL - MTL framework with   only one other neighbor dataset . We see which   task yields biggest improvement for each target   task . We experiment with various combinations of   datasets for this dataset . Results for this ablation   study are shown in table 4 . All experiments in this   ablation study are performed using BERT - base as   the base classifier .   Results in table 4 show that for both Hate   Speech Detection ( HS ) and Hate Speech and Of-   fensive Language Detection ( HSO ) tasks , Coarse-   Grained Stereotype Detection ( C - ST ) neighboring   task yields the best improvements to 71.1and67.39   macro - F1 , respectively . All the other three neigh-   boring tasks are useful in improving the perfor-   mance of the base classifier from 66.47and66.13   F1 scored . For Misogyny Detection ( MG ) task ,   HSO neighboring task results in an improvement   from74.16to75.87 , while the other two tasks dete-   riorate the performance on the task . It is also inter-   esting to note that , the combined performance on   the task with all three datasets is lower ( 74.78 ) than   when using HSO data alone . For both Coarse- and   Fine - grained Stereotype Detection ( F - ST ) tasks ,   HS and HSO datasets improve the performance   over the baseline , while MG deteriorates the per-   formance . The combined improvement of all the   neighboring tasks together is higher than either HS6709ModelHate Speech   DetectionOffense   DetectionMisogyny   DetectionCoarse   StereotypesFine   Stereotypes   BERT - base 66.47 66 .13 74 .16 65 .71 61 .36   BERT - large 67.05 63 .90 72 .13 59 .63 55 .42   BART - large 68.91 65 .86 73 .12 63 .40 54 .64   XlNet - large 59.14 48 .33 63 .16 63 .71 53 .80   Multi - Task Learning   BERT - base + MTL 69.2168.5773.48 68 .2965.00   BERT - large + MTL 69.7865.1473.9461.9661.65   BART - large + MTL 67.79 68 .0374.4065.7764.90   XlNet - large + MTL 61.6846.35 64 .4265.2157.00   RL - guided MTL   BERT - base + RL - MTL 72.0668.97 74 .7874.1865.72   BERT - large + RL - MTL 69.82 65 .9775.2170.8864.74   BART - large + RL - MTL 69.6066.76 75 .1474.1167.94   XlNet - large + RL - MTL 61.97 47 .6063.21 67 .9856.37   TNHS HSO MG C - ST   HS - 69.6970.0771.10   HSO 66.71 - 66.5667.39   MG 70.9875.87 - 73.89   C - ST 66.1567.4063.82 -   F - ST 63.8063.6559.9456.12   or HSO neighboring tasks alone . It is also interest-   ing to note that the C - ST task does n’t contribute   significantly to performance improvement on F - ST   task . This might be due to the presence of anti-   stereotypes and several other issues pointed out in   Blodgett et al . ( 2021b ) .   8 Conclusion   We tackle the problem of Stereotype Detection from   data annotation andlow - resource computational   framework perspectives in this paper . First , we dis-   cuss the key challenges that make the task unique   and a low - resource one . Then , we devise a focused   annotation task in conjunction with selected data   candidate collection to create a fine - grained evalua - tion set for the task .   Further , we utilize several neighboring tasks that   are correlated with our target task of ’ Stereotype   Detection ’ , with an abundance of high - quality gold   data . We propose a reinforcement learning - guided   multitask learning framework that learns to select   relevant examples from the neighboring tasks that   improve performance on the target task . Finally ,   we perform exhaustive empirical experiments to   showcase the effectiveness of the framework and   delve into various details of the learning process   via several ablation studies .   Acknowledgments   We thank the anonymous reviewers and meta-   reviewer for their insightful comments that helped   in improving our paper .   References671067116712