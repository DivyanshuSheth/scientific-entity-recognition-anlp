  Soumya Sanyal Zeyi Liao Xiang Ren   University of Southern California   { soumyasa , zeyiliao , xiangren}@usc.edu   Abstract   Transformers have been shown to be able to per-   form deductive reasoning on inputs containing   rules and statements written in English natural   language . However , it is unclear if these mod-   els indeed follow rigorous logical reasoning to   arrive at the prediction , or rely on spurious cor-   relation patterns in making decision . A strong   deductive reasoning model should consistently   understand the semantics of different logical   operators . To this end , we present R LR ,   a deductive reasoning - based diagnostic bench-   mark that evaluates the robustness of language   models to minimal logical edits in the inputs   and different logical equivalence conditions . In   our experiments with RoBERTa , T5 , and GPT3 ,   we show that the models trained on deductive   reasoning datasets with various logical opera-   tions do not perform consistently on the R - LRtest set , thus showing that the models   are not robust to our proposed logical pertur-   bations . Further , we observe that the models   find it especially hard to learn logical negation   operator . Our results demonstrate the shortcom-   ings of current language models in logical rea-   soning , and call for the development of better   inductive biases to teach the logical semantics   to language models . All the datasets and code   base have been made publicly available .   1 Introduction   Building systems that can automatically reason   over a given context to generate valid logical in-   ferences is a long pursued goal within the field   of AI ( McCarthy , 1959 ; Rocktäschel and Riedel ,   2017 ; Manhaeve et al . , 2019 ) . Recently , Clark et al .   ( 2020 ) have shown that language models ( Liu et al . ,   2019 ; Raffel et al . , 2020 ) are able to emulate de-   ductive reasoning on a logical rulebase ( theory )   containing rules and declarative statements written   in natural language . While this is impressive , it is   unclear if these models are able to perform logicalFigure 1 :   reasoning robustly by understanding the semantics   of the logical operators and the different logical   conditions involving such operators .   Logical reasoning is an important skill required   in various NLP tasks such as NLI ( Dagan et al . ,   2006 ) , Question Answering ( Yang et al . , 2018a ) ,   Multi - turn Dialogue Reasoning ( Cui et al . , 2020 ) ,   etc . Models used to solve such tasks may use spuri-   ous patterns to reach to the predictions , rather than   following the intended logical reasoning process .   Additionally , these models might only understand   certain ways of expressing the inference knowledge   ( e.g. , rules ) and not possess systematic generaliza-   tion ( Gontier et al . , 2020 ) . Hence , it is important   to ensure that language models can consistently   use the logical operators when described in natural   language . Prior works ( Gururangan et al . , 2018 ;   Chen and Durrett , 2019 ; McCoy et al . , 2019 ) have   found that models solving different reasoning tasks   tend to exploit spurious correlations between the   context / question and the label . But logical reason-   ing needs special considerations as there are very   well - defined relationships on how different logical   operators modify any given context . Hence , it is   important to understand if models use these logi-   cal relationships consistently to solve a task . To   the best of our knowledge , a study evaluating a   language model ’s logical consistency on different   logical operations is currently missing.9614A key desirable property of a strong deductive   reasoning model is logical robustness . This is the   ability to make consistent predictions on inputs that   have some logical modifications . In Figure 1 , we   show how the lack of logical robustness can lead   to wrong inferences in a model . Thus , to test this ,   we develop R LR , a diagnostic benchmark   for evaluating logical robustness across two main   aspects . First , we aim to evaluate how robust these   models are when tested on the three logical opera-   tors : conjunction ( ∧ ) , disjunction ( ∨ ) , and negation   ( ¬ ) . Inspired by the idea of contrast sets ( Gardner   et al . , 2020 ) , we design the Logical Contrast set ,   where theories are minimally modified so that we   can test the model ’s robustness across logical op-   erators . Examples of this are shown in Figure 3(b )   and 3(c ) . Next , we study the model ’s ability of   reasoning consistently across logical paraphrases .   A logical paraphrase uses equivalence conditions   in logic to replace a rule with another equivalent   form , essentially rewriting the surface form of the   rule . This poses a different challenge than the more   common language paraphrases such as synonym   changes , voice modifications , style changes , etc . ,   because the model needs to understand that the un-   derlying logical structure of the two paraphrased   sentences mean the same thing . An example of the   equivalence perturbation is shown in Figure 3(d ) .   Based on this , we design the Logical Equivalence   set containing three logical equivalences .   In this work , we study three language models :   RoBERTa ( Liu et al . , 2019 ) , T5 ( Raffel et al . ,   2020 ) , and GPT-3 ( Brown et al . , 2020 ) . To test the   model performance on R LR , we first fine-   tune them on deductive reasoning training datasets   containing the logical operators and then evalu-   ate on the R LRtest sets . Overall , we find   that language models ( LMs ) fine - tuned on different   deductive reasoning datasets are not sufficiently   robust to the Logical Contrast and Logical Equiv-   alence sets . Specifically , we find that models are   more inconsistent with logical negations in sen-   tences . We also find that using larger models such   as T5 - 11B improves the performance to an extent ,   but they still perform worse compared to human   performance on R LR . We show that it is   partly due to spurious correlations in the data and   the inherent difficulty of the task . Thus , we use   R LRto demonstrate some key limitations   of the language models trained for deductive rea-   soning . We hope that this research will encourageas a test bed to evaluate robustness of deductive   reasoning models .   2 Deductive Reasoning   In deductive reasoning , we predict whether a given   theory Tsupports a statement sor not . We de-   fine a theory Tas a set of facts   and rulesexpressed in natural   language ( See Figure 3 for an example ) . For a   given theory , a statement can be either provably   supported , provably unsupported ( i.e. , the negation   of the statement is provable ) , or not provable at all .   This is a 3 - class classification problem , with the   labels True , False , and Unknown , respectively . In   this work , we focus on this task , where we expect   the model to correctly predict the entailment of a   statement for a given theory . In Figure 3(a ) , 3(c ) ,   and 3(d ) , the statement is entailed by the theory ,   leading to the label True while in Figure 3(b ) , the   statement is not provable given the facts and rules .   It can be proved by simply using fact fand rule   rto derive the statement . Formally , we define the   proof set of a statement s , denoted by G(T , s ) , as   the set of rules and facts that are required to obtain   the statement sfrom the theory .   3Evaluating LMs for Logical Robustness   3.1 Logical Robustness   We consider a deductive reasoner ( language model )   to be logically robust if the model behavior is con-   sistent across various logical perturbations , as illus-   trated in Figure 1 . Specifically , we evaluate logical   robustness on two types of perturbations .   Logical contrastive edits Here , we test the   model ’s ability to correctly capture the semantics   of different logical operators , when presented in   minimally edited contrast inputs . A contrast set   ( Gardner et al . , 2020 ) is one where the input is   changed minimally , but meaningfully , such that   there is ( typically ) some change in label . These   probes test the LM ’s robustness to conjunction ( ∧ ) ,   disjunction ( ∨ ) , and negation ( ¬ ) .   Logical paraphrases Here , we evaluate whether   the model performs consistently when shown the   same input with different logical paraphrases . A   theory can be logically paraphrased by modifying   the rules using standard logical equivalence condi-   tions . These probes evaluate the model ’s consis-9615   tency in solving logically equivalent theories , when   logical conditions are used to rewrite the input .   A strong deductive reasoning model should be   robust to both the minimally edited contrast inputs   and logical paraphrases . Overall , these evaluation   sets probe a deductive reasoning model to check   whether it indeed learns the semantics of the logical   operators and their underlying working principles .   3.2 Notations   In this work , we consider two predicate forms :   unary and binary . A unary predicate contains one   argument and is denoted by X(a ) . Similarly , a bi-   nary predicate is represented as X(a , b ) . Here , X   is the predicate relation and a , bare the variables .   Anatomic predicate is defined as either a predi-   cate or the negation of the predicate ( denoted as   ¬X(a ) ) . Acomplex predicate can contain multiple   predicates ( or their negated forms ) combined using   logical operators conjunction and disjunction .   Internally , we maintain a symbolic representa-   tion of these facts and rules , enabling us to later   create the different evaluation sets of R LR .   A fact is symbolically represented by a predicate .   In this work , we consider all facts as atomic pred-   icates . A rule is symbolically represented by a   logical connection between predicates , separated   by the “ implies that ” logical symbol ( = ⇒ ) . Thus ,   a rule can be defined as p=⇒q , where the LHS   pand RHS qare atomic or complex predicates . If   bothpandqconsist of atomic predicates , then the   rule is called a simple rule . A compound rule is one   where pand / or qcontain some complex predicates   connected by the conjunction or disjunction opera-   tor . An example of a natural language theory and   its corresponding logical form is shown in Figure 2 .   Here , facts fandfare unary atomic and binary   predicates , respectively . Rule ris a compound   rule , with the LHS pof the rule being a complex   predicate . Rule ris a simple rule .   3.3 Logical Contrast Sets   In this evaluation set , we probe the ability of the   model to robustly understand the three different   logical operators ( ∧,∨,¬ ) . For this , we develop   different contrast sets ( Gardner et al . , 2020 ) with   minimal editing of the theory , probing specific rea-   soning abilities of different operators . The key   intuition is to evaluate if the model is able to un-   derstand the minor changes in the theory brought   by the addition of logical operators , and predict the   change in label accordingly .   For a given theory Tand statement s , we first   select a rule to be modified such that it is part of   the proof set G(T , s ) . This ensures that our per-   turbation would influence the model ’s reasoning   process while predicting entailment of the state-   ment s. Next , we add an unseen predicate tto the   rule LHS pof one of the rules using conjunction   ( ∧ ) or disjunction ( ∨ ) . In some further variants of   perturbations , we include the predicate t(or the   negated ¬t ) as a fact in the theory , leading to dif-   ferent labels . Lastly , we also negate the rule RHS q   to introduce the logical negation ( ¬ ) perturbations .   Based on the logical operator in the perturbation ,   we broadly divide the Logical Contrast set into   three types : Conjunction Contrast Set ( C - CS ) , Dis-   junction Contrast Set ( D - CS ) , and Negation Con-   trast Set ( N - CS ) . Examples of these perturbations   are shown in Figure 3 . The perturbations for C - CS   are listed in Table 1 . Please refer to Appendix E   for the other perturbations . We categorize these   perturbations into groups based on the logical op-   erators involved in the perturbation with respect to   the base theory . E.g. , the three groups for C - CS   are , , + , as shown in Table   1 . If a model performs accurately on the Logical   Contrast set , we expect that the model understands   the semantics of the logical operators robustly.9616   Evaluation Protocol To evaluate the logical ro-   bustness to contrast perturbations , we first fine-   tune the language model on a deductive reasoning   dataset containing different combination of logical   operators . Then , we report the model performance   on these evaluation datasets as the weighted - F1   score from the Scikit - learn ( Pedregosa et al . , 2011 ) .   The weighted - F1 score modifies the macro - F1 to   take any label imbalance into account . For instance ,   we have label imbalance by design of the perturba-   tions in the Logical Contrast sets shown in Tables   1 , 9 , and 10 . We use the model ’s prediction for the   base theory and all its perturbations to compute the   F1 - score at a theory level , and then average this   score across all theories in the evaluation set .   3.4 Logical Equivalence Sets   The Logical Equivalence set contain theories where   the underlying symbolic representation of a rule is   replaced by another representation that is logically   equivalent . The logical equivalent form of a rule   can be derived from standard logical equivalence   conditions , as defined below :   Herep , q , r can be both atomic predicates or com-   plex predicates . Based on the above conditions , the   Logical Equivalence set is divided into three types :   Contrapositive Equivalence Set ( C - ES ) , Distribu-   tive 1 Equivalence Set ( D1 - ES ) , and Distributive   2 Equivalence Set ( D2 - ES ) . For the ( C - ES ) set ,   every rule rin the theory Tis replaced by the   logically equivalent form to create a new logicallyequivalent theory T. An example of this pertur-   bation is shown in Figure 3 ( d ) . Similarly , for the   D1 - ES andD2 - ES sets , a pair of rules in Tare   merged according to the equivalence to create a   new theory T.   In both instances , the theory Tstill has the same   label for a given statement , as the logical steps   required to solve the task remains the same . These   modifications are more challenging than traditional   surface - level paraphrases of the natural language   text , as it requires the model to understand the   equivalence of different symbolic representations .   Evaluation Protocol Similar to the Logical Con-   trast set evaluation , we finetune a language model   on a deductive reasoning dataset and report the   weighted - F1 score for the base theory and the cor-   responding logical paraphrase , averaged across all   theories in the evaluation set .   4 The R LRDataset   In this section , we describe details about the R - LRdataset domains , sampling , and filtering   procedure .   4.1 Dataset Domain   Facts The domains of the predicate relation X   and variable ain the unary predicate X(a)are the   simple English adjectives and the proper names ,   respectively . Examples of this predicate form are   “ green(Alex ) ” , “ kind(John ) ” , etc . Each predicate is   associated to the English template sentence form   “ { a } is { X } . ” . For the binary predicate X(a , b ) ,   we consider family relationships and proper names   as the domain of Xandarespectively . Some ex-   amples of this predicate form are “ daughter(Mary ,   Gary)”,“father(Bob , John ) ” , etc . Each predicate is   associated with template sentences such as “ { a } is   the { X } of { b } . ” , “ The { X } of { b } is { a } . ” , etc .   Note that , currently , we do not enforce any gender   constraints on the names , thus allowing predicates   such as “ daughter(Bob , Gary ) ” , which might be un-   likely based on the genders associated statistically   to names in English .   Rules For the rules , we follow the same do-   main as mentioned above for facts . We allow   rules containing unary predicates , binary predi-   cates , or a combination of both . Examples of some   simple rules consisting of atomic predicates are   “ green(Alex ) = ⇒daughter(Bob , Gary ) ” , “ ¬fa-   ther(Bob , John ) = ⇒ kind(John ) ” , etc . Simi-9617larly , examples of some compound rules containing   complex predicates are “ green(Alex ) ∨smart(Bob )   = ⇒daughter(Bob , Gary ) ∧ ¬ kind(John ) ” , etc .   We note that , for the sake of keeping the theories   deterministic , we do not allow the disjunction oper-   ator in the RHS of a compound rule . A rule of the   formp=⇒qis associated with templates such   as “ If { p } then { q } . ” , “ { q } if { p } . ” , etc . , where the   p ’s and q ’s can be recursively resolved to their own   templates as defined in the predicates .   4.2 Dataset Sampling   For sampling the theories in R LR , we use   a modified version of the Label - Priority sampling   ( Zhang et al . , 2022 ) . The detailed algorithm is   described in Algorithm 1 in Appendix . At a high-   level , we sample different predicates from the set of   templates and assign the value 0or1to them . After   that , we divide the predicate set into multiple levels .   This helps us in sampling theories with multi - hop   reasoning depths . After that , rules are derived by   connecting predicates with the same label between   two different levels . Finally , the predicates with   value 1at the 0level form the facts in the theory ,   the connections denote the rules , and the predicates   in the last level denote some candidate statements .   4.3 Filtering Statistical Features   In a contemporary work , Zhang et al . ( 2022 ) find   that LMs are specifically prone to pick up any ex-   isting statistical features that can be present in the   training datasets . These are described as certain   statistic of an instance that has a strong correlation   with the label . Examples of statistical features are   # facts , # rules , # negation op , # facts with negation ,   etc .   We introduce some changes in our sampling al-   gorithm to minimize the influence of such statisti-   cal features . We define a check to ensure that the   number of statements with the different labels are   similar for any given theory in the training dataset .   Since we have negations in the dataset , it allows us   to exactly control the True andFalse label distri-   bution per theory instance systematically . We con-   trol the Unknown label by oversampling theories   and discarding ones with skewed label distribution .   Please refer to Appendix D for more details .   5 Experimental Setup   Training Data Details We use four different   training datasets to fine - tune baselines , describedas follows : NOT : In this dataset , we allow nega-   tions in both facts and rules , but restrict to only   using simple rules . Note that it is not possible   to create a dataset without any operators ( i.e. , no   negation , conjunction , and disjunction ) as it would   not be possible to have the False label in that   dataset . AND+NOT : Here , we restrict the connec-   tor for compound rules to AND(∧ ) . As before , we   allow negations in facts and rules . OR+NOT : Similar   toAND+NOT , we restrict the connector of the com-   pound rules to OR(∨ ) . We allow negations in facts   and rules as before . All : This dataset has all the   three logical operators ( AND , OR , NOT ) present .   The dataset Allcontains all the logical opera-   tors we consider in the Logical Contrast set . Thus ,   instances from the Alldataset cover all forms of   rules seen in these test sets . We aim to understand   the effect of these training datasets on the evalua-   tion sets by fine - tuning the model on each dataset   separately . Please refer to Appendix C for more   details on the training and evaluation data statistics .   Models and Experiment Details Following   prior works ( Clark et al . , 2020 ; Tafjord et al . , 2021 ;   Sanyal et al . , 2022 ) , we evaluate the performance   of three language models : RoBERTa ( Liu et al . ,   2019 ) , T5 ( Raffel et al . , 2020 ) , and GPT-3 ( Brown   et al . , 2020 ) . Specifically , we evaluate the model   checkpoints RoBERTa - Large , T5 - Large , T5 - 3B ,   T5 - 11B , and GPT-3 . To evaluate a model , we first   fine - tune it on one of the training dataset mentioned   above , and then evaluate on the Logical Contrast   ( Section 6.2 ) and Logical Equivalence ( Section 6.3 )   evaluation sets . For T5 - 11B , we only finetune it   on the Alldataset due to compute constraints . For   GPT-3 , we evaluate its performance on a subset   of the test sets using demonstrations . Please refer   to Appendix A for details on the input formats for   each model and Appendix B for the hyperparame-   ter settings and other implementation details .   6 Results   6.1 In - domain Performance   The performance of the LMs on the in - distribution   held - out data are shown in Table 3 . We note that   the models are able to solve the in - distribution test   dataset almost perfectly in all cases . This either   means the model understands the logical reasoning   task perfectly ( which is unlikely ) or it learns some   spurious features to solve the task using shortcuts .   Now , we evaluate these models on our test sets to9618   check the logical robustness .   6.2 Performance on Logical Contrast set   Overall Result We finetune RoBERTa - Large ,   T5 - Large , and T5 - 3B models on different train-   ing datasets and evaluate them on the three types of   Logical Contrast set . The results are shown in Ta-   ble 2 . Based on the Avgperformance , we find that   the models perform significantly worse on the Log-   ical Contrast set , compared to the almost perfect   performance on the in - distribution test sets in Table   3 . This shows that even after finetuning on the log-   ical deductive reasoning datasets , these models do   not learn the semantics of the logical operators in a   robust manner , but likely use spurious correlations .   Additionally , we find that on average , model per-   formance is similar for different training datasets ,   except for NOTdataset . While this result seems a bit   surprising at first because we expect different train-   ing datasets to have varying effect on the perfor-   mance , but computing the performance breakdown   by perturbation type across different datasets re-   veals an interesting trend . We observe that training   on related operators as the perturbation in the eval-   uation subset usually leads to better performance .   For instance , we find that models trained on the   OR+NOT training data perform better on the D - CS   compared to when trained on NOTorAND+NOT train-   ing data . Taking RoBERTa - Large as an example ,   we observe that its performance on D - CS is 0.61   when finetuned on OR+NOT and is apparently greater   than the performance on other contrast sets . A sim-   ilar trend is observed for C - CS . This is intuitive as   training on the related operators helps the model   to understand the semantics of the logic relatively   better than just relying on having seen these at pre-   training time . This shows that the model indeed   requires some training data that is strongly aligned   with the operators being evaluated the test set .   Variation with logical operators Next , we want   to understand which among the three operators are   more challenging for the models to learn . To bet-   ter understand this , we evaluate the models after   finetuning on the Alldataset and plot the model   performance for different perturbation groups in   Figure 4 . These groups ( defined in Section 3.3 )   contain perturbations of a specific operator , as sug-   gested by their names . We find that the most chal-   lenging operator is negation . This is evident from   the lowest scores on the perturbation group   among , , and . Further , this is also   observed from the fact that performance generally   drops when negation perturbations are introduced   along with any other perturbations . For instance ,   we see an average drop of around 25 % between and+ . This demonstrates that the   model not able to learn the negation semantics very   well . Lastly , we find that models find conjunction   relatively harder than disjunction . Please refer to   Appendix F for more details.9619   6.3 Performance on Logical Equivalence set   Results on Contrapositive Equivalence Next ,   we evaluate the fine - tuned LMs on the Logical   Equivalence sets . In Table 4 , we observe that   the model performance degrades by approximately   20 % for the C - ES , compared to the in - distribution   performance in Table 3 . Contraposition involves   changing the rule into a format that has two nega-   tions , thus testing the limits of the model on un-   derstanding negations . From the experiments on   Logical Contrast sets , we know that negations are   not well understood by the model . Thus , these re-   sults reinforce our previous findings . We do not   find any significant changes in performance when   trained on different operators . Thus , we conclude   that , for this test set , it is not sufficient to just under-   stand the semantics of the logical operators , but it   rather requires a higher order understanding about   the interactions between the logical operators and   implications . Including such knowledge in deduc-   tive reasoning models is an interesting direction for   future works .   Results on Distributive Equivalence ForD1-   ESandD2 - ES test sets , we see a higher perfor-   mance compared to C - ES as these equivalence   conditions are relatively easier than the contrapo - sition rule . This is because the distributive rules   are similar to having compound rules in the dataset   ( that is already present in AND+NOT , OR+NOT , and   Alldatasets ) . Between the two sets , we observe   that D1 - ES is more challenging for the model .   This indicates that models find conjunction opera-   tor harder than the disjunction , similar to our ob-   servations from Figure 4 . One reason for this can   be the strictness involved in the conjunction oper-   ation . A rule with conjunction is true only if the   individual parts are independently true .   6.4 Human Evaluation   To better understand the upper limit of R LR   evaluation sets , we ask 3 Computer Science grad-   uate students to annotate 30 randomly sampled   theories from each subset of Logical Contrast and   Logical Equivalence sets . The results are shown in   the last row of Table 5 . We find that humans are   significantly better than other baselines , perform-   ing around 27 % higher on the Logical Contrast set   compared to T5 - 11B on average . Additionally , we   see similar trends that humans find negation based   perturbations ( N - CS ) hardest , followed by con-   junction ( C - CS ) , and disjunction ( D - CS ) . Please   refer to Appendix H for further details .   6.5 Analysis   Performance of Larger LMs Here we evaluate   the performance of some larger models on R - LR . The goal is to estimate the possible gains   with scaling to large LMs . For this , we finetune   a T5 - 11B using the Alltraining dataset for two   epochs . Additionally , we evaluate GPT3 ( Brown   et al . , 2020 ) on a subset of 500 samples per test   set , using demonstration - based in - context learning .   The results are shown rows 5 - 6 in Table 5 . We ob-   serve that increasing the model size from T5 - Large   to T5 - 11B indeed leads to a significant performance   gain on most datasets . But it is still quite far com-   pared to human performance . This suggests that   scaling can potentially help with learning robust9620   logical operations to an extent . Additionally , we   find that GPT-3 is not able to perform well on these   test sets . We hypothesize this is likely due to the   mismatch of the training distribution of the GPT-3   model , versus our synthetic datasets , as we do not   finetune the GPT-3 model on our training sets .   Effect of LM Pre - training In this part , we evalu-   ate the usefulness of using a pre - trained checkpoint   in our experiments , in comparison with training   a RoBERTa - Large architecture from scratch . We   train both RoBERTa - Large pre - trained checkpoint   and a similar model from scratch using the All   dataset , and evaluate on the R LRsets . The   results are shown in rows 1 - 2 in Table 5 . We ob-   serve a significant drop in performance , demon-   strating that knowledge learned during pre - training   is crucial for this task .   Effect of size of training data Next , in Figure   5 , we plot the overall performance of RoBERTa-   Large model finetuned on a varying amount of All   training data . We observe that the model perfor-   mance increases with increasing amount of training   data , as expected , and then saturates at a fixed level .   This shows that there is no significant effect of us-   ing larger training datasets on model performance .   We use 50k training samples in all datasets .   Effect of distractors We define distractors as the   facts and rules that are not part of the proof set   for a given theory and statement . Figure 6 depicts   the effect of distractors on the model performance ,   when finetuned on the Alldataset and evaluated   on the Logical Contrast set . We observe that the   performance generally improves ( or stays similar )   on the variant without distractors . This shows that   retrieving the relevant facts and rules in the the-   ory from a given set of sentences is a non - trivial   challenge . Thus , performing both retrieval and en-   tailment prediction in a single model can lead to   some performance degradation .   Effect of Statistical Features In a contemporary   work , Zhang et al . ( 2022 ) claims that deductive   reasoning models inherently learn to use statistical   features in the training data such as # rules , # facts ,   etc . Here , we demonstrate that it is not the com-   plete reason for failure using the following control   study . We finetune the RoBERTa - Large model on   a subset of the Alldataset , where we restrict to   two labels : True andFalse , by filtering out the   Unknown label . In our sampling algorithm , we en-   sure that each theory has the exact same number of   True andFalse labeled statements in the training   set . Thus , it is not possible to learn any statistic   of the data , as the label distribution per theory is   exactly uniform . Next , we evaluate the model on   theR LRevaluation sets , with the Unknown   label filtered in each set . The results are shown in   Table 6 . We observe that , although the performance   on this reduced test set is improved , there is still   around 15 % gap on average with respect to perfor-   mance on in - distribution data . This gap suggests   that the model is not able to learn the logical opera-   tors robustly , even without any scope of spurious   statistical features in the training set . Thus , this   shows that although spurious correlation can lead   to non - robust model behavior , it is not the sole rea-   son for failure of these deductive reasoning models .   This calls for the development of better inductive9621biases to teach the logical semantics more robustly   to the language models .   7 Related Works   Reasoning in natural language has been a preva-   lent problem in NLP . There are multiple reasoning   datasets , studying different aspects of reasoning   over textual inputs . Natural Language Inference   ( NLI ) ( Dagan et al . , 2006 ) is a prominent dataset   that requires reasoning over text to answer if a state-   ment is entailed , contradicted , or neutral given a   hypothesis . HotpotQA ( Yang et al . , 2018b ) tests   multi - hop reasoning abilities that require compar-   isons and inferring missing bridge between sen-   tences . CLUTRR ( Sinha et al . , 2019 ) tests whether   models can infer biological relationships between   entities in a context . RICA ( Zhou et al . , 2021 )   requires the model to employ commonsense rea-   soning to answer questions based on a context .   Recently , there has been an increasing focus on   evaluating the logical reasoning abilities of LMs .   ReClor ( Yu et al . , 2020 ) and LogiQA ( Liu et al . ,   2021 ) are logical reasoning datasets derived from   examinations . RuleTaker ( Clark et al . , 2020 ) pro-   poses synthetic deductive reasoning datasets that   uses only the knowledge in the context . There are   very limited works that probe the logical reasoning   abilities of language models ( LMs ) . FaiRR ( Sanyal   et al . , 2022 ) tests the robustness of logical reason-   ing models when the subjects and attributes in the   context are altered to out - of - distribution terms . In a   contemporary work , Zhang et al . ( 2022 ) show that   language models can learn to use statistical features   that can be present in deductive reasoning datasets .   To the best of our knowledge , R LRis the   first dataset that tests how robust these LMs are to   different logical perturbations .   8 Conclusion   In this paper , we proposed R LR , a diagnos-   tic benchmark to test the logical robustness of de-   ductive reasoning models . In R LR , we pro-   pose two evaluation sets , Logical Contrast and Log-   ical Equivalence , each probing different logical rea-   soning abilities . Overall , we find that fine - tuning   LMs such as RoBERTa and T5 on deductive rea-   soning datasets is not sufficient to learn the seman-   tics of the logical operators conjunction , disjunc-   tion , and negation . Although well - aligned training   dataset improves model performance , the models   still find it challenging to understand negations , both in Logical Contrast and Logical Equivalence   sets . We demonstrate some interesting shortcom-   ing of LMs designed for logical reasoning , that can   eventually enable building better reasoning models .   9 Limitation   A key limitation of the work is the synthetic nature   of the dataset . While it is ideal to explore more   natural theories , it makes the systematic logical   perturbation process very challenging . Thus , in   this work , we resort to using synthetic datasets , but   aim to bridge this gap in future works . Another lim-   itation is the complexity of the datasets we explore .   We use fairly simple logical rules and constructs   forR LR . Some more complex forms of log-   ical reasoning - based theories can potentially reveal   even more limitations of deductive reasoning mod-   els . Another interesting aspect we do not explore in   this scope is potential techniques to improve these   models on deductive reasoning tasks . This might   involve trying different inductive biases in the form   of architectural designs , more specialized datasets ,   etc .   Acknowledgments   This research is supported in part by the Office   of the Director of National Intelligence ( ODNI ) ,   Intelligence Advanced Research Projects Activity   ( IARPA ) , via Contract No . 2019 - 19051600007 ,   the DARPA MCS program under Contract No .   N660011924033 , the Defense Advanced Research   Projects Agency with award W911NF-19 - 20271 ,   NSF IIS 2048211 , NSF SMA 1829268 , and gift   awards from Google , Amazon , JP Morgan and   Sony . We would like to thank all the collabora-   tors in USC INK research lab for their constructive   feedback on the work .   References962296239624A Model implementation details   In this section , we describe the implementation   details of the language models used to evaluate   R LR .   •RoBERTa - Large : Following Rule-   Taker ( Clark et al . , 2020 ) , we use a   pre - trained RoBERTa - Large ( Liu et al . ,   2019 ) model to perform the classifica-   tion task . Specifically , we input in the   format [ CLS]T[SEP ] s[SEP ] to the   RoBERTa - Large model , and extract the   [ CLS]embedding to predict the label . The   schematics of the RoBERTa model input is   shown in Figure 7 . Here , Tis the theory   which is the concatenation of the facts and   rules , and sis the statement . We use Cross   Entropy loss to fine - tune the model on the   training dataset .   •T5 - Large : Following ProofWriter ( Tafjord   et al . , 2021 ) , we train a T5 - Large ( Raffel et al . ,   2020 ) model for the deductive reasoning task .   For this , we add a prefix to the T5 - Large ’s in-   put and generate the output in a fixed format .   Specifically , we give the input in the format :   $ answer $ ; $ question $ = s ; $ context $ =   T. Here , Tis the theory which is the concate-   nation of the facts and rules , and sis the state-   ment . And the output is defined to be in for-   mat:$answer $ = True / False / Unknown .   The model is trained on the default language   modeling loss to match the output format . At   evaluation time , we match the output template   with the above description and generate the   model ’s predicted label accordingly .   •T5 - 3B : Similar to T5 - Large above , we use the   T5 - 3B checkpoint .   •T5 - 11B : Similar to T5 - Large above , we use   the T5 - 11B checkpoint .   •GPT3 : We use GPT-3 ( Brown et al . , 2020 )   for model evaluation to check its performance   on our C - CS , D - CS , N - CS . Following ( Sanh   et al . , 2021 ) , we experiment with all the   prompts for the NLI task and select a prompt   which performs the best on the evaluation   datasets . We experiment with inserting 3   demonstrations and 10 demonstrations before   the sentence and find that the performances   are nearly same . So , we finally use the prompt   named “ based on the previous passage ” which   will give the input in a format as shown below :   Input Template :   { { premise } } Based on the previous passage ,   is it true that " { { hypothesis } } " ? Yes or no ?   Output Template :   { { answer_choices[label ] } }   Answer Choices Template :   Yes ||| Maybe ||| No   We use 3 demonstrations for each sample to   limit the total tokens evaluated using the Ope-   nAI GPT-3 API .   B Hyperparameter Details   Here we use RoBERTa - Large ( Liu et al . , 2019)and   T5 - Large , T5 - 3B , T5 - 11B ( Raffel et al . , 2020 ) mod-   els for the 3 - class deductive reasoning classifica-   tion task . Only use GPT-3 ( Brown et al . , 2020 )   for evaluation . We train the pre - trained check-   points available in the Hugging Face ( Wolf et al . ,   2020 ) Transformers library . For RoBERTa - Large   model , we use AdamW ( Loshchilov and Hutter ,   2019 ) with learning rate 1e-5 . For T5 - Large T5 - 3B   and T5 - 11B , we use AdamW with learning rate   1e-4 , adamw_epsilon 1e-6 , warmup_ratio 1e-1 ,   weight_decay 1e-2 . All these models are trained   with batch size 8on Nvidia Quadro RTX 8000   GPUs . For RoBERTa - Large and T5 - Large , training   a single task on one GPU costs nearly 8 hours on   average . For T5 - 3B and T5 - 11B , we use 4 GPUs to   train the model and averagely need 5 and 10 hours   for one epoch.9625   C Dataset Statistics   In this section , we describe the training and evalua-   tion dataset statistics . We first train the model on   the datasets in Table 7 . Each dataset comprises of   different types of logical operators to help us in un-   derstanding the effect of different logical operators .   Then we evaluate the trained models on evaluation   datasets mentioned in Table 8 . For evaluation , we   test the model on two subsets of R LR : Logi-   cal Contrast set and Logical Equivalence set . Each   set is further sub - categorized into three different   parts , based on the type of perturbations .   D Filtering Statistical Features   In Figure 8 , we show the plots of the label distri-   bution for the following statistical features in the   input theory and statement : # rules , # facts , # facts   with negation , # rules with negation , # rules with   conjunction , # rules with disjunction , and # state-   ments with negations . We observe that there is no   significant bias between any of these features and   the task label . Additionally , we show the count his-   togram of the instances in blue . Overall , our dataset   filtering is able to remove some of the count - based   statistical features .   E Contrast Perturbations   Following Section 3.3 , we show the conjunction ,   disjunction , and negation contrast perturbations for   the case when base theory ’s label is False in Tables   11 , 12 , and 13 , respectively .   For the conjunction contrast set perturbations in   Table 1 and 11 , the first row is a base theory which   is used to generate these contrast sets . In the next   set of triads , the rule is modified to have an unseen   predicate tin conjunction with the existing rule   LHS . Here tis a predicate that is not part of the   existing facts and inferences in the theory ( hence ,   referred to as unseen predicate ) . Additionally , we   addt(or¬t ) as part of the facts in the theory . This   lead to modification of the label as shown in rows   3 - 4 . For the next set of triads , we modify the base   rule to have a negated rule RHS ¬q . The corre-   sponding label changes are shown in rows 5 - 7 . In   Table 11 , we assume the label of the statement is   False for the base theory in row 1 . Similar perturba-   tions are possible for the label True , and is shown   in Table 1 in 3.3 . We group these perturbations into   three classes as shown in Table 11 : , , + . These groups are based on which logi-   cal operator is the new addition with respect to the   base theory . If a model performs accurately on this   contrast set , we expect that the model understands   the semantics of conjunction and negation logical   operators reasonably well .   Similar to the C - CS above , we show the per-   turbations considered in the D - CS in Table 9 and   12 , where the distractor is added to the rule LHS   using disjunction ( ∨ ) . Lastly , we show the N - CS   perturbations in Tables 10 and 13 , where negations   are added to the rule LHS and/or RHS.9626   F Logical Contrast set breakdown   In this section , we further discuss the performance   of the LMs on each group of the Logical Contrast   set . From Tables 14 , 15 and 16 we can say that the   models generally perform worse when they need to   handle more complicated compound rules ( C+   N > C > B ( where > means harder ) ) .   Additionally , we find that when we add more com-   pound rules in the training dataset , the performance   is generally better . Giving more complex rules can   lead to further drops in performance , as noted by   performance on the C+NandD+N.   Models trained on the dataset with aligned oper-   ators instead of Alldataset is better e.g. , model   trained on AND+NOT get best result at C+N.   It is easy to see that model with larger amount of   parameters give more consistent and better result   atC - CS , D - CS , N - CS which means the model   learned more semantics of logic from language and   is more robust .   G Result breakdown by label   We report the performance for each label in Tables   17 to 22 , for both the Logical Contrast and Logi-   cal Equivalence sets . We find that T5 - 3B model ,   the largest model among the three models , get a   good result for Unknown while other two models   are not good at it . It shows that models with large   amount of parameters can better learn to predict the   Unknown label , which is relatively harder than the   other two labels . Also , we find that Logical Equiv-   alence set is an easier task in general than Logical   Contrast set and the performances are stable across   three models .   H Human Evaluation   We recruit three Computer Science graduates to an-   notate the datasets . To keep the annotation realistic ,   we sample 30 instances from each test subset of   R LRand ask the annotators to mark a la-   bel from True , False , and Unknown . The question   asked is : “ Does the theory entail or contradict the   statement , or we can not say anything about it ? ” .   Overall , we find the average inter - annotator agree-   ment to be around 0.79 , evaluated using the Fleiss ’   kappa score.9627962896299630Algorithm 1 : Sampling Algorithm9631