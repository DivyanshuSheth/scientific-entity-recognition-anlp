  Nihir Vedd , Zixu Wang , Marek Rei , Yishu Miao , andLucia Specia   Imperial College London   { n.vedd19 , zixu.wang , marek.rei , y.miao20 , l.specia}@imperial.ac.uk   https://github.com/nihirv/guiding-vqg   Abstract   In traditional Visual Question Generation   ( VQG ) , most images have multiple concepts   ( e.g.objects and categories ) for which a ques-   tion could be generated , but models are trained   to mimic an arbitrary choice of concept as given   in their training data . This makes training dif-   ficult and also poses issues for evaluation –   multiple valid questions exist for most images   but only one or a few are captured by the hu-   man references . We present Guiding Visual   Question Generation - a variant of VQG which   conditions the question generator on categori-   cal information based on expectations on the   type of question and the objects it should ex-   plore . We propose two variant families : ( i ) an   explicitly guided model that enables an actor   ( human or automated ) to select which objects   and categories to generate a question for ; and   ( ii ) 2 types of implicitly guided models that   learn which objects and categories to condition   on , based on discrete variables . The proposed   models are evaluated on an answer - category   augmented VQA dataset and our quantitative   results show a substantial improvement over   the current state of the art ( over 9 BLEU-4   increase ) . Human evaluation validates that   guidance helps the generation of questions that   are grammatically coherent and relevant to the   given image and objects .   1 Introduction   In the last few years , the AI research community   has witnessed a surge in multimodal tasks such as   Visual Question Answering ( VQA ) ( Antol et al . ,   2015 ; Anderson et al . , 2018 ) , Multimodal Machine   Translation ( Specia et al . , 2016 ; Elliott et al . , 2017 ;   Barrault et al . , 2018 ; Caglayan et al . , 2019 ) , and   Image Captioning ( IC ) ( Vinyals et al . , 2015 ; Karpa-   thy and Fei - Fei , 2015 ; Xu et al . , 2015 ) . Visual   Question Generation ( VQG ) ( Zhang et al . , 2016 ;   Krishna et al . , 2019 ; Li et al . , 2018 ) , a multimodal   task which aims to generate a question given an   image , remains relatively under - researched despitethe popularity of its textual counterpart . Through-   out the sparse literature in this domain , different   approaches have augmented and/or incorporated   extra information as input . For example , Pan et al .   ( 2019 ) emphasised that providing the ground truth   answer to a target question is beneficial in generat-   ing a non - generic question . Krishna et al . ( 2019 )   pointed out that requiring an answer to generate   questions violates a realistic scenario . Instead , they   proposed a latent variable model using answer cate-   gories to help generate the corresponding questions .   Recently , Scialom et al . ( 2020 ) incorporated a pre-   trained language model with object features and   image captions for question generation .   In this work , we explore VQG from the perspec-   tive of ‘ guiding ’ a question generator . Guiding has   shown success in image captioning ( Zheng et al .   ( 2018 ) and Ng et al . ( 2020 ) ) , and in this VQG work   we introduce the notion of ‘ guiding ’ as condition-   ing a generator on inputs that match specific cho-   sen properties from the target . We use the answer   category and objects / concepts based on an image   and target question as inputs to our decoder . We   propose our explicit guiding approach to achieve   this goal . We additionally investigate an implicit   guiding approach which attempts to remove the   dependency on an external actor ( see more below ) .   The explicit variant ( Section 3.1 ) is modelled   around the notion that an actor can select a subset   of detected objects in an image for conditioning   the generative process . Depending on the appli-   cation , this selection could be done by a human ,   and algorithm or chosen randomly . For example ,   imagine either a open - conversation chat - bot or a   language learning app . In the chat - bot case , a hu-   man may show the bot a picture of something . The   bot may use randomly sampled concepts from the   image ( e.g. an object - detected tree ) to ask a human   a question upon . In the language learning case , the   human may wish to select certain concepts they   want the generated question to reflect . For exam-1640ple , they might select a subset of animal - related   objects from the whole set of detected objects in   order to generate questions for teaching the animal-   related vocabulary in a language learning setting .   Alongside the objects , the actor may also provide ,   or randomly sample , an answer category to the   question generator .   The implicit variant ( Section 3.2 ) , on the other   hand , is motivated by removing the dependency on   the aforementioned actor . We provide two method-   ologies for our proposed implicit variant . The first   uses a Gumbel - Softmax ( Jang et al . , 2016 ) to pro-   vide a discrete sample of object labels that can be   used for generating a question . The second method   employs a model with two discrete latent variables   that learn an internally - predicted category and a set   of objects relevant for the generated question , opti-   mised with cross - entropy and variational inference   ( Kingma and Welling , 2014 ; Miao et al . , 2016 ) .   Human evaluation shows that our models can   generate realistic and relevant questions , with our   explicit model almost fooling humans when asked   to determine which , out of two questions , is the   generated question . Our experiments and results   are presented in Section 5 .   To summarise , our main contributions are : 1 )   The first work to explore guiding using object la-   bels in Visual Question Generation ; 2 ) A novel   generative Transformer - based set - to - sequence ap-   proach for Visual Question Generation ; 3 ) The   first work to explore discrete variable models in   Visual Question Generation ; and 4 ) A substantial   increase in quantitative metrics - our explicit model   improves the current state of the art setups by over   9 BLEU-4 and 110 CIDEr .   2 Related Work   2.1 Visual Question Generation   Zhang et al . ( 2016 ) introduced the first paper in the   field of VQG , employing an RNN based encoder-   decoder framework alongside model - generated cap-   tions to generate questions . Since then , only a   handful of papers have investigated VQG . Fan et al .   ( 2018 ) demonstrated the successful use of a GAN   in VQG systems , allowing for non - deterministic   and diverse outputs . Jain et al . ( 2017 ) proposed   a model using a V AE instead of a GAN , however   their improved results require the use of a target   answer during inference . To overcome this unreal-   istic requirement , Krishna et al . ( 2019 ) augmented   the VQA ( Antol et al . , 2015 ) dataset with answercategories , and proposed a model which does n’t   require an answer during inference . Because their   architecture uses information from the target as   input ( i.e. an answer category ) , their work falls   under our definition of guided generation . More   recently , Scialom et al . ( 2020 ) investigate the cross   modal performance of pre - trained language mod-   els by fine - tuning a BERT ( Devlin et al . , 2018 )   model on model - based object features and ground-   truth image captions . Other work , such as Patro   et al . ( 2018 ) , Patro et al . ( 2020 ) and Uppal et al .   ( 2020 ) , either do not include BLEU scores higher   than BLEU-1 , which is not very informative , or   address variants of the VQG task . In the latter case   the models fail to beat previous SoTA on BLEU-4   for standard VQG . Recently and ( Xu et al . , 2021 )   and ( Xie et al . , 2021 ) achieve SoTA in VQG us-   ing graph convolutional networks . However , both   works follow an unrealistic setup by conditioning   their model on raw answers during training and   inference - a dependency we attempt to remove .   2.2 Discrete ( Latent ) Variable Models   Discrete variable models are ideal for tasks which   require controllable generation ( Hu et al . , 2017 ) or   ‘ hard ’ indexing of a vector ( Graves et al . , 2016 ) . Ex-   isting literature provide several methods to achieve   discretization . NLP GAN literature ( such as Seq-   GAN ( Yu et al . , 2016 ) and MaskGAN ( Fedus et al . ,   2018 ) ) commonly use REINFORCE ( Williams ,   1992 ) to overcome differentiability issues with dis-   crete outputs . Other discretization methodologies   can be found in Variational Auto Encoder ( V AE )   literature ( Kingma and Welling , 2014 ) . Some older   methodologies are NVIL ( Mnih and Gregor , 2014 )   and VIMCO ( Mnih and Rezende , 2016 ) . However ,   V AE literature also introduced Concrete ( Maddi-   son et al . , 2016 ) , Gumbel - Softmax ( Jang et al . ,   2016 ) and Vector Quantization ( Oord et al . , 2017 )   as discretization strategies ( technically speaking ,   Concrete and Gumbel - Softmax are strongly peaked   continuous distributions ) .   In this work , we use a Gumbel - Softmax ap-   proach to sample a distribution over objects . At in-   ference time , given a set of object tokens , learning   this ‘ hard ’ distribution allows the model to inter-   nally sample a subset of objects that produce the   most informative question . Our variational model   additionally learns a generative and variational dis-   tribution that allow the model to implicitly learn   which objects are relevant to a question and an-1641   swer pair whilst incorporating non - determinism for   diverse outputs .   3 Methodology   We introduce the shared concepts of our explicit   and implicit model variants , before diving into the   variant - specific methodologies ( Section 3.1 & 3.2 ) .   For both variants , we keep the VQG problem   grounded to a realistic scenario . That is , during   inference , we can only provide the model with an   image , and data that can either be generated by   a model ( e.g.object features or image captions )   and/or trivially provided by an actor ( i.e.answer   category and a selected subset of the detected ob-   jects ) . However , during training , we are able to use   any available information , such as images , captions ,   objects , answer categories , answers and target ques-   tions , employing latent variable models to min-   imise divergences between feature representationsof data accessible at train time but not inference   time . This framework is inspired by Krishna et al .   ( 2019 ) . In Appendix A , we discuss the differences   of input during training , testing and inference .   Formally , the VQG problem is as follows : Given   an image ˜i∈˜I , where ˜Idenotes a set of images ,   decode a question q. In the guided variant , for each   ˜i , we also have access to textual utterances , such as   ground truth answer categories and answers . The   utterances could also be extracted by an automated   model , such as image captions ( Li et al . , 2020 ) , or   object labels and features ( Anderson et al . , 2018 ) .   In our work , answer categories take on 1 out of 16   categorical variables to indicate the type of ques-   tion asked . For example , “ how many people are   in this picture ? ” would have a category of “ count ”   ( see Krishna et al . ( 2019 ) for more details ) .   Text Encoder . For encoding the text , we use   BERT ( Devlin et al . , 2018 ) as a pre - trained lan-1642guage model ( PLM ) . Thus , for a tokenised textual   input ˜Sof length T , we can extract a d - dimensional   representation for ˜s∈˜S :X = PLM(˜S)∈R   Image Encoder . Given an image ˜i , we can ex-   tract object features , f∈R , and their re-   spective normalized bounding boxes , b∈R ,   with the 4 dimensions referring to horizontal and   vertical positions of the feature bounding box .   Following the seminal methodology of Anderson   et al . ( 2018 ) , kis usually 36 . Subsequent to   obtaining these features , we encode the image   using a Transformer ( Vaswani et al . , 2017 ) , re-   placing the default position embeddings with the   spatial embeddings extracted from the bounding   box features ( Krasser and Stumpf , 2020 ; Cornia   et al . , 2019 ) . Specifically , given f , bfrom image ˜i :   i = Transformer ( f , b)∈R   Text Decoder . We employ a pretrained Trans-   former decoder for our task ( Wolf et al . , 2020 ) .   Following standard sequence - to - sequence causal   decoding practices , our decoder receives some en-   coder outputs , and auto - regressively samples the   next token , for use in the next decoding timestep .   Our encoder outputs are the concatenation ( ; oper-   ator ) of our textual and vision modality represen-   tation : X= [ S;i]∈R , and our decoder   takes on the form : ˆq = Decoder ( X ) , where ˆqis   the predicted question .   In this work , we primarily focus on a set - to-   sequence problem as opposed to a sequence - to-   sequence problem . That is , our textual input is not   a natural language sequence , rather an unordered   set comprising of tokens from the answer category ,   the object labels , and the caption . How this set is   obtained is discussed in following section . Due to   the set input format , we disable positional encoding   on the PLM encoder ( Text Encoder in Figure 1 ) .   3.1 Explicit Guiding   As mentioned in Section 1 , the explicit variant re-   quires some actor in the loop . Thus , in a real world   setting , this model will run in two steps . Firstly ,   we run object detection ( OD ) and image caption-   ing ( IC ) over an image and return relevant guiding   information to the actor . The actor may then select   or randomly sample a subset of objects which are   sent to the decoder to start its generation process .   If the actor opts for a random sample strategy , no   human is needed during the inference process ( see   Appendix A for examples ) .   To enable this setup , we create paired data basedon the guided notion . At a high level , our approach   creates this data in three steps : 1 ) obtain object   labels ; 2 ) obtain concepts via IC Formally ,   objects = OD(i)∈R   cap = CaptionModel ( i)∈R   cap = rmStopWords ( caption ) ∈R   candidate_concepts = set(objects ; cap)∈R   ( 1 )   Here , ODstands for an object detector model ,   rmStopWords is a function which removes the   stop words from a list , and set is a function which   creates a set from the concatenation ( the ; operator )   of the detected objects and obtained captions . cap   stands for caption . The set is of size T < k+   T. Using this obtained candidate_concepts set ,   we run our filtration process .   Once the set of candidate concepts has been con-   structed , we filter them to only retain concepts rel-   evant to the target QA pair . After removing stop   words and applying the set function to the words   in the QA pair , we use Sentence - BERT ( Reimers   and Gurevych , 2019 ) to obtain embeddings for the   candidate QA pair and candidate_concepts ( Eq 1 ) .   We subsequently compute a cosine similarity ma-   trix between the two embedding matrices , and then   select the top kmost similar concepts . The chosen   kconcepts , ˜S , are always a strict subset of the can-   didate concepts that are retrieved using automated   image captioning or object detection . This process   emulates the selection of objects an actor would   select in an inference setting when given a choice   of possible concepts , and creates paired data for the   guided VQG task . We now concatenate an answer   category to ˜S : S = PLM([˜S;category ] ) ∈R.   With text encoding S , we run the model , op-   timizing the negative log likelihood between the   predicted question and the ground truth . Note that   the concatenation in the decoder below is along the   sequence axis ( resulting in a tensor ∈R ) .   ˆq = Decoder ( [ S;i ] )   L = CrossEntropy ( ˆq , q)(2 )   3.2 Implicit Guiding   We now introduce our experiments for the im-   plicit variant for VQG . This variant differs from its   explicit counterpart as it aims to generate ques-   tions using only images as the input , while in-   ternally learning to predict the relevant category1643and objects . Mathematically , the explicit variant   models ˆq = p(w|i,˜S , category , w , ... , w;θ )   where ˜Sandcategory are obtained as described   in Section 3.1 . During inference , the im-   plicit variant instead attempts to model ˆq=   p(w|i,˜e , e , w , ... , w;θ)where ˜e , e   arenotexplicitly fed in to the model . Rather , they   are determined internally as defined in Equation 6 .   Given an image , we apply the same object de-   tection model as in the explicit variants to extract   object labels , which are then encoded using an   embed layer . Formally ,   objects = OD(i)∈R   e = embed ( objects ) ∈R(3 )   Since we would like the implicit model to learn   relevant objects for an image internally , we project   each object in eto a real - valued score :   scores = MLP(e)∈R(4 )   Subsequently , we apply a hard Gumbel - Softmax   ( Jang et al . , 2017 ) to obtain predictions over se-   lected objects . Because Gumbel - Softmax samples   from a log - log - uniform distribution , stochasticity   is now present in our sampled objects . To sample   kobjects , we tile / repeat scores ktimes before in-   putting it into the Gumbel - Softmax . ˜z , ourk - hot   sampled objects vector , is then used to mask object   embeddings for use in decoding :   ˜z = gumbel - softmax ( scores , k)∈R   ˜e= ˜z∗e∈R(5 )   Where ∗denotes element - wise multiplication .   Categories can also be a strong guiding factor and   instead of making it an explicit input , we build   a classifier to predict possible categories . In this   variant , ˜eis used as an input to both our text   encoder , and the MLP responsible for the category   prediction :   S = PLM(˜e)∈R   p(ˆcat|˜e ) = softmax ( MLP(˜e))∈R   ( 6 )   Using the one - hot representation of the predicted   category ( i.e. e = one - hot ( p(ˆcat|˜e ) ) , we   can concatenate our image , PLM representation   of objects , and predicted category to feed into   the decoder : ˆq = Decoder ( [ i;S;e])∈R.   However , during training , we teacher force againstthe ‘ gold ’ set of objects , ˜S(obtained using candi-   date_concepts in Equation 1 ) . Training and opti-   mization thus follow :   ˆq = Decoder ( [ i;˜S;e])∈R   L = CrossEntropy ( ˆq , q)+   CrossEntropy ( p(ˆcat|˜e ) , cat)+   StartEnd ( ˜e,˜S)(7 )   where StartEnd is a BERT QA - head style loss ( De-   vlin et al . , 2018 ) that uses binary cross entropy for   eachkin˜e .   Variational Implicit . Hypothesising that   ground - truth QA pairs might provide information   useful to selecting objects , we additionally attempt   to extend our model to incorporate QA pairs to   learn a latent variational distribution over the ob-   jects . However , since QA pairs can only be used   during training to learn a variational distribution ,   we introduce another generative distribution that is   only conditioned on the images and extracted ob-   jects . We borrow the idea from latent variable mod-   els to minimise Kullback - Leibler ( KL ) divergence   between the variational distribution and generative   distribution , where the variational distribution is   used during training and the generative distribution   is used in inference .   Continuing from Equation 3 , we build two matri-   ces , MandM. The former is a concatenation   of the image features and object embeddings , and   the latter the concatenation between the encoded   QA pair and M. Depending on whether we ’re   in a training or inference regime , the CLS token   of the relevant matrix is used to sample a mask ,   ˜z , which is subsequently applied on the aforemen-   tioned object embeddings :   M = encode ( [ e;i])∈R   e = embed ( Q;A)∈R   M = encode ( [ e;M])∈R   q(z|M , M ) = MLP(M;M)∈R   p(z|M ) = MLP(M)∈R   ˜z = gumbel - softmax ( z , k)∈R   ˜e= ˜z∗e∈R   where q(z|M , M)is the variational distri-   bution , p(z|M)is the generative distribution ,   and MLP denotes a multilayer perceptron for learn-   ing the alignment between objects and QA pairs.1644encode is an attention - based function such as   BERT ( Devlin et al . , 2018 ) . From here , our method-   ology follows on from Equation 6 . However , our   loss now attempts to minimise the ELBO :   L = E[logp(ˆq|z,ˆcat ) ]   −D[q(z|M , M)||p(z|M ) ]   + log p(ˆcat|M )   4 Experiments   4.1 Datasets   We use the VQA v2.0 dataset(Antol et al . , 2015 )   ( CC - BY 4.0 ) , a large dataset consisting of all rele-   vant information for the VQG task . We follow the   official VQA partition , with i.e.443.8 K questions   from 82.8 K images for training , and 214.4 K ques-   tions from 40.5 K images for validation . Following   Krishna et al . ( 2019 ) , we report the performance   on validation set as the annotated categories and   answers for the VQA test set are not available .   We use answer categories from the annotations   of Krishna et al . ( 2019 ) . The top 500 answers   in the VQA v2.0 dataset are annotated with a la-   bel from the set of 15 possible categories , which   covers up the 82 % of the VQA v2.0 dataset ; the   other answers are treated as an additional category .   These annotated answer categories include objects   ( e.g. “mountain ” , “ flower ” ) , attributes ( e.g. “cold ” ,   “ old ” ) , color , counting , etc .   We report BLEU ( Papineni et al . , 2002 ) ,   ROUGE ( Lin , 2004 ) , CIDEr ( Vedantam et al . ,   2015 ) , METEOR ( Lavie and Agarwal , 2007 ) , and   MSJ ( Montahaei et al . , 2019 ) as evaluation metrics .   The MSJ metric accounts for both the diversity of   generated outputs , and the n - gram overlap with the   ground truth utterances .   4.2 Comparative Approaches   We compare our models with four recently pro-   posed VQG models Information Maximising VQG   ( IMVQG ; supervised with image and answer cat-   egory ) ( Krishna et al . , 2019 ) , What BERT Sees   ( WBS ; supervised with image and image caption )   ( Scialom et al . , 2020 ) , Deep Bayesian Network   ( DBN ; supervised with image , scenes , image cap-   tions and tags / concepts ) ( Patro et al . , 2020 ) , and   Category Consistent Cyclic VQG ( C3VQG ; su-   pervised with image and answer category ) ( Uppal   et al . , 2020 ) . We follow IMVQG ’s evaluation setupbecause they hold the current SoTA in VQG for re-   alistic inference regimes . We omit ( Xu et al . , 2021 )   and ( Xie et al . , 2021 ) from our table of results be-   cause these models follow an unrealistic inference   regime , requiring an explicit answer during train-   ing and inference . Our baseline is an image - only   model , without other guiding information or latent   variables .   4.3 Implementation Details   In Section 3 we described the shared aspects of our   model variants . The reported scores in Section 5   use the same hyperparameters and model initial-   isation . A table of hyperparameters and training   details can be found in Appendix B. BERT Base   ( Devlin et al . , 2018 ) serves as our PLM encoder and   following Wolf et al . ( 2020 ) ; Scialom et al . ( 2020 ) ,   we use a pre - trained BERT model for decoding too .   Though typically not used for decoding , by con-   catenating the encoder inputs with a [ MASK ] token   and feeding this to the decoder model , we are able   to obtain an output ( e.g.ˆq ) . This decoded output is   concatenated with the original input sequence , and   once again fed to the decoder to sample the next   token . Thus , we use the BERT model as a decoder   in an auto - regressive fashion .   To encode the images based on the Faster - RCNN   object features ( Ren et al . , 2015 ; Anderson et al . ,   2018 ) , we use a standard Transformer ( Vaswani   et al . , 2017 ) encoder . Empirically , we find k= 2   to be the best number of sampled objects .   5 Results   We present quantitative results in Table 1 and qual-   itative results in Figure 2 . We evaluate the explicit ,   implicit and variational implicit models in a single-   reference setup , as the chosen input concepts are   meant to guide the model output towards one par-   ticular target reference .   5.1 Quantitative Results   Starting with the explicit variant , as seen in Ta-   ble 1 , we note that our image - only baseline model   achieves a BLEU-4 score of 5.95 . We test our   model with different combinations of text features   to identify which textual input is most influential to   the reported metrics . We notice that the contribu-   tion of the category is the most important text input   with respect to improving the score of the model ,   raising the BLEU-4 score by more than 11 points   ( image - category ) over the aforementioned baseline.1645   However , whilst the BLEU-4 for the image - object   variant is 2.3 points lower , it outperforms the image-   category variant by 3.9 points on the diversity ori-   entated metric MSJ-5 - indicating that the image-   category variant creates more generic questions .   As expected , the inclusion of both the category   and objects ( image - guided ) outperforms either of   the previously mentioned models , achieving a new   state - of - the - art result of 24.4 BLEU-4 . This combi-   nation also creates the most diverse questions , with   an MSJ-5 of 57.3 .   We also test our hypothesis that guiding pro-   duces questions that are relevant to the fed in con-   cepts . This is tested with ‘ image - guided - random ’   variant . This variant is the same trained model   as ‘ image - guided ’ , but uses k= 2 random con-   cepts from a respective image instead of using the   ground truth question to generate concepts . Our   results show that guiding is an extremely effective   strategy to produce questions related to conceptual   information , with a BLEU-4 score difference of   over 20 points . We refer the reader to Section 5.3   for human evaluation which again validates this   hypothesis , and Section 3.1 for an explanation of   why guiding is valid for evaluating VQG models .   We evaluate the implicit models as follows . Theimplicit image - category variant does not predict   any objects internally . It uses all image features and   object embeddings alongside the category supervi-   sion signal as described in Equation 7 . The implicit   image - guided models use the ‘ gold ’ objects at in-   ference ( See Section 3.1 ) . If these variants fit the   ‘ gold ’ objects well , it indicates that their generative   abilities are suitable for guiding / conditioning on   predicted or random objects . The image - guided-   pred variants are evaluated using internally pre-   dicted objects - and the model variant that would   be used in a real inference setting . Finally , the   image - guided - random variants are fed in random   object labels at inference .   For implicit guiding to be a valid methodology ,   we need to validate two criteria : 1 ) Successfully   conditioning the decoder on guiding information ;   2 ) Better than random accuracy of object predic-   tion / selection . Note that intuitively , the implicit   model is expected to perform worse than the ex-   plicit model in terms of the language generation   metrics . This is because of the inherently large   entropy of the relevant answer category and the   objects given an image . However , if the learned   distributions over the categories and objects can   capture the relevant concepts of different images,1646they may benefit the question generation when com-   pared with image - only .   According to Table 1 , by predicting just an an-   swer category and no objects ( image - category ) , the   proposed implicit model beats the image - only base-   line . The BLEU-4 score difference is less than   1 with the best performing WBS model ( Scialom   et al . , 2020 ) – which also generates questions with-   out explicit guided information .   As mentioned above , we can evaluate the im-   plicit model by either feeding the ‘ gold ’ objects   obtained as described in Section 3.1 , or by the inter-   nally predicted objects as described in Section 3.2 .   These form the variants image - guided and image-   guided - pred respectively . For both the implicit and   variational implicit models , image - guided is ex-   pected to perform the best . Results validate this ,   showing a performance of 14.2 and 12.6 BLEU-4   respectively . Importantly , the relatively high scores   of these guided models ( compared to the compar-   ative approaches ) show that these models can suc-   cessfully be conditioned on guiding information .   We also notice that for both types of im-   plicit models , image - guided - pred outperforms   image - guided - random . Specifically for the non-   variational implicit , we see a higher BLEU-4   score difference of 2.7 . Interestingly , despite this   BLEU-4 difference being higher than its variational   counterpart , there is a trade - off for the diversity-   orientated MSJ metric . This indicates that although   generated questions are discretely ‘ closer ’ to the   ground truth , similar phrasing is used between the   generated questions . In fact , an acute case of this   phenomena occurs for the image - category variant   where the BLEU-4 variant is higher than image-   guided - pred or image - guided - random . In this case ,   qualitative analysis shows us that the higher BLEU-   4 score can be attributed to the generic nature of   the generated question . Failure cases of automatic   evaluation metrics in NLP is discussed further in   ( Caglayan et al . , 2020 ) .   To satisfy the ‘ better than random accuracy of   object prediction / selection ’ criteria previously out-   lined , we measure the overlap of the kpredicted ob-   jects vs k‘gold ’ object labels . These ‘ gold ’ object   labels are obtained similarly to the explicit variant   ( Section 3.1 ) , however the caption tokens are not   fed to the filtering process . Random accuracy for   selecting objects is 12.5 % . Our overlap accuracy   on implicit image - pred is 18.7 % - outperforming   random selection . Variational implicit image - pred   failed to outperform random accuracy .   5.2 Qualitative Results   Qualitative results are shown in Figure 2 and Ap-   pendix D. Figure 2 depicts how outputs from differ-   ent model variants compare to ground truth ques-   tions . Without any guiding information , the image-   only variant is able to decode semantic information   from the image , however this leads to generic ques-   tions . The implicit variant , for which we also report   the predicted category and objects , mostly gener-   ates on - topic and relevant questions . Focusing on   the explicit variant , we witness high - quality , inter-   esting , and on - topic questions .   Appendix D depicts how well our explicit image-   guided variant handles a random selection of de-   tected objects given the image . This experiment   intends to gauge the robustness of the model to   detected objects which may fall on the low tail of   the human generating question / data distribution .   To clarify , humans are likely to ask commonsense   questions which generally focus on obvious objects   in the image . By selecting objects at random for the   question to be generated on , the model has to deal   with object permutations not seen during training ,   and categories that are invalid for an image .   Analysing the outputs , when viable categories   and objects that are expected to fall in a common-   sense distribution are sampled , the model can gen-   erate high quality questions . Interestingly , we ob-   serve that when the sampled objects are not com-   monsense ( e.g. “ears arms ” for the baby and bear   picture ) , the model falls back to using the object   features instead of the guiding information . This   phenomenon is also witnessed when the sampled   category does not make sense for the image ( e.g.   category ‘ animal ’ in image 531086 ) . Despite the   category mismatch , the model successfully uses   the object information to decode a question .   5.3 Human Evaluation   We ask seven humans across four experiments to   evaluate the generative capabilities of our models .   Experiment 1 is a visual Turing test : given an im-   age , a model generated question and a ground truth   question , we ask a human to determine which ques-1647   tion they believe is model generated . Experiment   2attempts to discern the linguistic and grammat-   ical capabilities of our model by asking a human   to make a binary choice about whether the gener-   ated question seems natural . Experiment 3 shows a   human an image alongside a model generated ques-   tion ( explicit variant ) . Then , we ask the human to   make a choice about whether the generated ques-   tion is relevant to the image ( i.e.could an annotator   have feasibly asked this question during data collec-   tion ) . Finally , experiment 4 judges whether objects   are relevant to a generated question . The experi-   ment is set up with true - pairs and adversarial - pairs .   True - pairs are samples where the shown objects are   the ones used to generate the question . Adversarial-   pairs show a different set of objects than those   which generated the question . If more true - pairs   are are marked correct ( i.e.if at least one of the   objects is relevant to the generated question ) than   the adversarial - pairs , then our model successfully   generates questions on guiding information .   Inexperiment 1 , a model generating human - level   questions should be expected to score 50 % , as a   human would not be able to reliably distinguish   them from the manually created questions . Our   results show the explicit and non - variational im-   plicit model outperforming the variational implicit   and baseline variants , fooling the human around   45 % of the time . Whilst not yet at the ideal 50 % ,   the explicit approach provides a promising step to-   wards beating the visual Turing Test . Experiment 2evaluates the grammatical plausibility of the gen-   erated questions . In general , all models perform   extremely well in this experiment , with the baseline   variant generating grammatically correct sentences   96 % of the time . This is expected , as the base-   line typically falls back to decoding easy / generic   questions . Experiment 3 , is evaluated on our best   performing model ( explicit image - guided ) . Here ,   78 % of the generated questions are marked as   relevant / on - topic given an image . Finally , experi-   ment 4 ’s results show true - pairs marked as correct   vs adversarial - pairs ( incorrectly ) marked as correct .   Since the former is larger than the latter - 72 % vs   42 % , the model can successfully use guiding / object   information to create on - topic questions .   6 Conclusions   We presented a guided approach to visual question   generation ( VQG ) , which allows for the generation   of questions that focus on specific chosen aspects   of the input image . We introduced three variants   for this task , the explicit , implicit , and variational   implicit . The former generates questions based on   an explicit answer category and a set of concepts   from the image . In contrast , the latter two discretely   predict these concepts internally , receiving only   the image as input . The explicit model achieves   SoTA results when evaluated against comparable   models . Qualitative evaluation and human - based   experiments demonstrate that both variants produce   realistic and grammatically valid questions.1648Acknowledgments   Lucia Specia , Zixu Wang and Yishu Miao received   support from MultiMT project ( H2020 ERC Start-   ing Grant No . 678017 ) and the Air Force Office of   Scientific Research ( under award number FA8655-   20 - 1 - 7006 ) .   References164916501651A Training , testing and inference   Here , using an example , we clarify the inputs to   our explicit model ( Section 3.1 ) in the training ,   testing and inference setups .   Training   •Ground truth question : What is the labrador   about to catch ?   • Answer : Frisbee   • Category : Object   • Image : i∈R   •{Caption } : A man throwing a frisbee to a dog   • { Objects } : person , dog , frisbee , grass   N.B. { Caption } and { Objects } are both model   generated , requiring only an image as input . These   inputs are thus available at inference time .   Firstly , we create a set of candidate_concepts   ( see eq . 1 ) from the caption and objects : [ person ,   dog , frisbee , grass , man , throwing ] ( ∈R ) . These   words are individually embedded . Secondly , we   concatenate and embed the set of question and an-   swer tokens ( ∈R ) .   Then , we construct a matrix which gives us co-   sine similarity scores for each candidate_concepts   token to a QA token ( ∈R ) . We choose k= 2   tokens from the candidate_concepts which are   most similar to the words from the QA . Here , “ dog ”   and “ frisbee ” are likely chosen . Our input to the   model is then < i , “ object ” , “ dog ” , “ frisbee ” > .   Notice that it is possible for these words to be   in the QA pair ( e.g. “ frisbee ” ) . Importantly , these   words have not been fed from the QA pair - they   have been fed in from model - obtained concepts   ( { Object } and { Caption } ) . Philosophically similar ,   Krishna et al . ( 2019 ) constructed inputs based on   target information for use in training and bench-   marking .   Testing . Imagine a data labeler creating questions   based on an image . They would look at the image ,   and decide on the concepts to create the question   for . Our testing methodology follows this intu-   ition using the strategy outlined above : the k= 2   selected objects from candidate_concepts is a pro-   grammatic attempt for selecting concepts which   could generate the target question . Note that there   can be many questions generated for a subset ofconcepts ( e.g. ‘ is the dog about to catch the fris-   bee ? ’ , ‘ what is the flying object near the dog ? ’ etc . ) .   As outlined above , we are not taking concepts from   the target . Rather we use information from the tar-   get to emulate the concepts an actor would think of   to generate the target question . Because there can   be different concepts questions are based on for one   image ( see ground - truth questions in Appendix D ) ,   our strategy allows us to generate questions which   might be similar to a singular target question . This   leads to an evaluation which fairly uses information   a human has access to to generate a question .   Inference . However , in the real world , there is no   ‘ ground - truth ’ question . In this case , we simply   feed image features , and actor selected concepts   to our question generator model . The selection   process of the actor may be random - in which case   a human agent does not need to be involved in the   question generation process . The k≤2selected   concepts here are a subset of candidate_concepts ,   which are fully generated from models .   B Hyperparameters and training details   Batch size 128   Learning rate 1e-5   Text model layers 12   Text model dimension 768   Image encoder layers 6   Image encoder dimension 768   Image encoder heads 8   Empirically , for both variants , we find k= 2to   be the best number of sampled objects . All exper-   iments are run with early stopping ( patience 10 ;   training iterations capped at 35000 ) on the BLEU-   4 metric . Scores reported ( in Section 5 ) are from   the highest performing checkpoint . We use the Py-   Torch library and train our model on a V100 GPU   ( 1.5 hours per epoch ) .   C Impact of model size on results1652Our models use the heavier Transformers than   previous SoTA we compare to . For example , ( Kr-   ishna et al . , 2019 ) use ResNet and RNNs for their   image encoder and question generator ( ∼18 M pa-   rameters ) . Our models have between 200 - 300 M   parameters . To validate that our results are not   purely attributable to model size , we train a trun-   cated version of image - category and image - guided   ( explicit only ) . We truncate our models by using   only the first and last layers of our BERT based   encoders and decoders ( ∼36 M parameters ) . Our   closest model to theirs is the ( truncated ) explicit   image - category , which achieves a BLEU-4 of 16.2   as seen in Table 4 - an improvement of 1.7 BLEU-4   over IMVQG ’s t - path . Even if we attribute 100 %   of this score improvement to the pre - trained na-   ture of the BERT models we use , our methodol-   ogy still introduces a 5.9 BLEU-4 increase over   the image - category combination ( truncated image-   guided achieves a BLEU-4 of 22.1 ) .   D More Qualitative Examples .   Examples can be seen in Figure 2 ( next page ) .   When examined , we see that the generated ques-   tion accurately uses the guiding category when the   category is valid for the given image . For exam-   ple , 531086/1 has animal as the sampled category .   Because no animal is present in the image , this   category is n’t valid for the image . The generated   question then correctly relies on the object labels   and visual modality to generate a valid question   given the image . Similarly for 490505/2 .   There are some cases where a sampled ob-   ject / concept is not valid given an image . For exam-   ple , at least one of the objects in 22929/1 , 41276/1 ,   531086/2 , 281711/1 , 490505/1 is not valid . In this   case the model usually relies on the other available   guiding information , prioritising the category infor-   mation ( e.g. 531086/2 ) . In rare cases , the model   has failure cases where some of the valid sampled   objects may not be used in the generated question   ( e.g. 293705/2 and 490505/2 ) .   The concept extractor utilises a pre - trained im-   age captioning model and object detector model .   This may lead to an accumulation of downstream   errors , especially if the data fed into the pre - trained   models are from a significantly different data gen-   erating distribution than those used to train the   model . In this erroneous case , the model will likely   fallback to rely on the image modality and cate-   gory information to produce a generic question(e.g . 22929/1 , 22929/2 , 531085/1 , 293705/2 ) .   E Responsible NLP Research   E.1 Limitations   Our approach claims to achieve SoTA in Visual   Question Generation . However , we are only able to   train and test our model on one dataset because it   is the only existing dataset which contains answer   categories . It is possible that our work may be   suitable for use in a zero - shot setting , but we have   not evaluated or tested our model in this setup .   E.2 Risks   Our model could be used to generate novel ques-   tions for use in Visual Question Answering . This   may have a knock - on effect which leads to training   more VQA models , thus having a negative impact   on the environment .   Our model could be used in downstream tasks   such as language learning . There may be incor-   rectness in the generated questions which has a   knock on effect to a user using this model ( e.g. the   user may gain a wrong understanding of a concept   because of a question the model has generated)16531654