  Tom Hosking Hao Tang Mirella Lapata   Institute for Language , Cognition and Computation   School of Informatics , University of Edinburgh   10 Crichton Street , Edinburgh EH8 9AB   tom.hosking@ed.ac.uk hao.tang@ed.ac.uk mlap@inf.ed.ac.uk   Abstract   We propose a generative model of paraphrase   generation , that encourages syntactic diversity   by conditioning on an explicit syntactic sketch .   We introduce Hierarchical Reﬁnement Quan-   tized Variational Autoencoders ( HRQ - V AE ) , a   method for learning decompositions of dense   encodings as a sequence of discrete latent vari-   ables that make iterative reﬁnements of in-   creasing granularity . This hierarchy of codes is   learned through end - to - end training , and repre-   sents ﬁne - to - coarse grained information about   the input . We use HRQ - V AE to encode the   syntactic form of an input sentence as a path   through the hierarchy , allowing us to more eas-   ily predict syntactic sketches at test time . Ex-   tensive experiments , including a human evalu-   ation , conﬁrm that HRQ - V AE learns a hierar-   chical representation of the input space , and   generates paraphrases of higher quality than   previous systems .   1 Introduction   Humans use natural language to convey informa-   tion , mapping an abstract idea to a sentence with   a speciﬁc surface form . A paraphrase is an alter-   native surface form of the same underlying seman-   tic content . The ability to automatically identify   and generate paraphrases is of signiﬁcant interest ,   with applications in data augmentation ( Iyyer et al . ,   2018 ) , query rewriting , ( Dong et al . , 2017 ) and   duplicate question detection ( Shah et al . , 2018 ) .   While autoregressive models of language ( in-   cluding paraphrasing systems ) predict one token   at a time , there is evidence that in humans some   degree of planning occurs at a higher level than in-   dividual words ( Levelt , 1993 ; Martin et al . , 2010 ) .   Prior work on paraphrase generation has attempted   to include this inductive bias by specifying an al-   ternative surface form as additional model input ,   either in the form of target parse trees ( Iyyer et al . ,   2018 ; Chen et al . , 2019a ; Kumar et al . , 2020 ) , ex-   emplars ( Meng et al . , 2021 ) , or syntactic codes   Figure 1 : The generative models underlying our ap-   proach . Given some semantic content z , we predict   a hierarchical set of syntactic codes qthat describe the   output syntactic form at increasing levels granularity .   These are combined to give a syntactic embedding z ,   which is fed to the decoder along with the original se-   mantic content to generate the output sentence y. Dur-   ing training , the encoder is driven by a paraphrase x   and a syntactic exemplar x.   ( Shu et al . , 2019 ; Hosking and Lapata , 2021 ) . Most   of these approaches suffer from an ‘ all or noth-   ing ’ problem : the target surface form must be fully   speciﬁed during inference . However , predicting the   complete syntactic structure is almost as difﬁcult as   predicting the sentence itself , negating the beneﬁt   of the additional planning step .   In this paper , we propose a generative model for   paraphrase generation , that combines the diversity   introduced by an explicit syntactic target with the   tractability of models trained end - to - end . Shown   in Figure 1 , the model begins by assuming the exis-   tence of some semantic content z. Conditioned   on this semantic information , the model predicts   a syntactic ‘ sketch ’ in the form of a hierarchical   set of discrete codes q , that describe the target   syntactic structure with increasing granularity . The   sketch is combined into an embedding z , and   fed along with the original meaning zto a de-2489coder that generates the ﬁnal output utterance y.   Choosing a discrete representation for the sketch   means it can be predicted from the meaning as a   simple classiﬁcation task , and the hierarchical na-   ture means that the joint probability over the codes   admits an autoregressive factorisation , making pre-   diction more tractable .   The separation between zandzis induced   by a training scheme introduced in earlier work   ( Hosking and Lapata , 2021 ; Huang and Chang ,   2021 ) and inspired by prior work on separated la-   tent spaces ( Chen et al . , 2019b ; Bao et al . , 2019 ) ,   whereby the model must reconstruct a target out-   put from one input with the correct meaning , and   another input with the correct syntactic form . To   learn the discretized sketches , we propose a vari-   ant of Vector - Quantized Variational Autoencoders   ( VQ - V AE , or VQ ) that learns a hierarchy of embed-   dings within a shared vector space , and represents   an input encoding as a path through this hierar-   chy . Our approach , which we call Hierarchical   Reﬁnement Quantized Variational Autoencoders or   HRQ - V AE , leads to a decomposition of a dense   vector into embeddings of increasing granularity ,   representing high - level information at the top level   before gradually reﬁning the encoding over subse-   quent levels .   Our contributions are summarized as follows :   •We propose a generative model of natural lan-   guage generation , HRQ - V AE , that induces   a syntactic sketch to account for the diver-   sity exhibited by paraphrases . We present   a parameterization of our generative model   that is a novel method for learning hierarchi-   cal discretized embeddings over a single la-   tent encoding space . These embeddings are   trained end - to - end and jointly with the en-   coder / decoder .   •We use HRQ - V AE to induce hierarchical   sketches for paraphrase generation , demon-   strating that the known factorization over   codes makes them easier to predict at test time ,   and leads to higher quality paraphrases .   2 Latent Syntactic Sketches   2.1 Motivation   Letybe a sentence , represented as a sequence of to-   kens . We assume that ycontains semantic content ,   that can be represented by a latent variable z.   Types of semantic content might include the de-   scription of an image , or a question intent . How - ever , the mapping from semantics to surface form is   not unique : in general , there is more than one way   to express the semantic content . Sentences with the   same underlying meaning zbut different sur-   face form yareparaphrases . Standard approaches   to paraphrasing ( e.g. , Bowman et al . 2016 ) map   directly from ztoy , and do not account for this   diversity of syntactic structure .   Following recent work on syntax - guided para-   phrasing ( Chen et al . , 2019a ; Hosking and Lap-   ata , 2021 ) , and inspired by evidence that humans   plan out utterances at a higher level than individ-   ual words ( Martin et al . , 2010 ) , we introduce an   intermediary sketching step , depicted in Figure 1b .   We assume that the output sentence yis generated   as a function both of the meaning zandof a   syntactic encoding zthat describes the struc-   ture of the output . Moreover , since natural lan-   guage displays hierarchical organization in a wide   range of ways , including at a syntactic level ( con-   stituents may contain other consituents ) , we also   assume that the syntactic encoding zcan be de-   composed into a hierarchical set of discrete latent   variablesq , and that these qare conditioned   on the meaning z. This contrasts with popular   model architectures such as V AE ( Bowman et al . ,   2015 ) which use a ﬂatinternal representation in a   dense Euclidean vector space .   Intuitively , our generative model corresponds to   a process where a person thinks of a message they   wish to convey ; then , they decide roughly how to   say it , and incrementally reﬁne this decision ; ﬁ-   nally , they combine the meaning with the syntactic   sketch to ‘ spell out ’ the sequence of words making   up the sentence .   2.2 Factorization and Objective   The graphical model in Figure 1b factorizes as   p(y;z ) = Xp(yjz;z )   p(zjq )   p(z)p(qjz)Yp(qjq;z):(1 )   Althoughqare conditionally dependent on   z , we assume that zmay be determined from   ywithout needing to explicitly calculate qor   z. We also assume that the mapping from dis-   crete codes qtozis a deterministic func-2490tionf( ) . The posterior therefore factorises as   ( z;zjy ) =  ( zjy)  ( zjy )     ( qjz)Y  ( qjq;z):(2 )   The separation between zandq , such that   they represent the meaning and form of the input re-   spectively , is induced by the training scheme . Dur-   ing training , the model is trained to reconstruct   a target yusing zderived from an input with   the correct meaning ( a paraphrase ) x , andq   from another input with the correct form ( a syn-   tactic exemplar ) x. Hosking and Lapata ( 2021 )   showed that the model therefore learns to encode   primarily semantic information about the input in   z , and primarily syntactic information in q.   Exemplars are retrieved from the training data fol-   lowing to the process described in Hosking and   Lapata ( 2021 ) , with examples in Appendix C. The   setup is shown in Figure 1a ; in summary , during   training we set  ( zjy ) =  ( zjx)and   ( qjy;q ) =  ( qjx;q ) . The ﬁnal objec-   tive is given by   ELBO = E    logp(yjz;q ) )    logp(qjz) Xlogp(qjq;z)   + KL   ( zjx)jjp(z)   ; ( 3 )   whereq  ( qjx)andz  ( zjx ) .   3 Neural Parameterisation   We assume a Gaussian distribution for z ,   with prior p(z) N ( 0;1 ) . The en-   coders  ( zjx)and  ( zjx)are   Transformers ( Vaswani et al . , 2017 ) , and we   use an autoregressive Transformer decoder for   p(yjz;z ) . The mapping f()fromq   tozand the posterior network  ( qjq;z )   are more complex , and form a signiﬁcant part of   our contribution .   Our choice of parameterization is learned end-   to - end , and ensures that the sketches learned are   hierarchical both in the shared embedding space   and in the information they represent .   3.1 Hierarchical Reﬁnement Quantization   Letz2Rbe the output of the encoder net-   work  ( zjy ) , that we wish to decompose as asequence of discrete hierarchical codes . Recall that   q2[1;K]are discrete latent variables correspond-   ing to the codes at different levels in the hierarchy ,   d2[1;D ] . Each level uses a distinct codebook ,   C2R , which maps each discrete code to a   continuous embedding C(q)2R.   The distribution over codes at each level is a   softmax distribution , with the scores sgiven by   the distance from each of the codebook embed-   dings to the residual error between the input and   the cumulative embedding from all previous levels ,   s(q ) =   "   x XC(q ) #    C(q ) !   :( 4 )   Illustrated in Figure 2 , these embeddings therefore   represent iterative reﬁnements on the quantization   of the input . The posterior network  ( qjq;z )   iteratively decomposes an encoding vector into a   path through a hierarchy of clusters whose cen-   troids are the codebook embeddings .   Given a sequence of discrete codes q , we   deterministically construct its continuous represen-   tation with the composition function f( ) ,   z = f(q ) = XC(q ): ( 5 )   HRQ - V AE can be viewed as an extension of   VQ - V AE ( van den Oord et al . , 2017 ) , with two sig-   niﬁcant differences : ( 1 ) the codes are hierarchically   ordered and the joint distribution p(q;:::;q)ad-   mits an autoregressive factorization ; and ( 2 ) the   HRQ - V AE composition function is a sum , com-   pared to concatenation in VQ or a complex neural   network in VQ - V AE 2 ( Razavi et al . , 2019 ) . Un-   der HRQ , latent codes describe a path through the   learned hierarchy within a shared encoding space .   The form of the posterior  ( qjq;z)and the   composition function f()do not rely on any   particular properties of the paraphrasing task ; the   technique could be applied to any encoding space .   Initialisation Decay Smaller perturbations in en-   coding space should result in more ﬁne grained   changes in the information they encode . Therefore ,   we encourage ordering between the levels of hier-   archy ( such that lower levels encode ﬁner grained   information ) by initialising the codebook with a   decaying scale , such that later embeddings have   a smaller norm than those higher in the hierarchy .   Speciﬁcally , the norm of the embeddings at level d   is weighted by a factor (  ) .2491   Depth Dropout To encourage the hierarchy   within the encoding space to correspond to hierar-   chical properties of the output , we introduce depth   dropout , whereby the hierarchy is truncated at each   level during training with some probability p .   The output of the quantizer is then given by   z = X   C(q)Y   !   ; ( 6 )   where   Bernoulli ( 1 p ) . This means   that the model is sometimes trained to reconstruct   the output based only on a partial encoding of the   input , and should learn to cluster similar outputs   together at each level in the hierarchy .   3.2 Sketch Prediction Network   During training the decoder is driven using sketches   sampled from the encoder , but at test time exem-   plars are unavailable and we must predict a distri-   bution over syntactic sketches p(qjz ) . Mod-   elling the sketches as hierarchical ensures that this   distribution admits an autoregressive factorization .   We use a simple recurrent network to in-   fer valid codes at each level of hierarchy , us-   ing the semantics of the input sentence and   the cumulative embedding of the predicted path   so far as input , such that qis sampled from   p(qjz;q ) = Softmax ( MLP(z;z ) ) ,   where z = PC(q ) . This MLP is trained   jointly with the encoder / decoder model , using the   outputs of the posterior network  ( qjx;q )   as targets . To generate paraphrases as test time ,   we sample from the sketch prediction model   p(qjz;q)using beam search and condition   generation on these predicted sketches.3.3 Training Setup   We use the Gumbel reparameterisation trick ( Jang   et al . , 2016 ; Maddison et al . , 2017 ; Sønderby et al . ,   2017 ) for the discrete codes and the standard Gaus-   sian reparameterisation for the semantic represen-   tation . To encourage the model to use the full code-   book , we decayed the Gumbel temperature  , ac-   cording to the schedule given in Appendix A. We   approximate the expectation in Equation ( 3 ) by   sampling from the training set and updating via   backpropagation ( Kingma and Welling , 2014 ) . The   full model was trained jointly by optimizing the   ELBO in Equation ( 3 ) .   4 Experimental Setup   Datasets A paraphrase is ‘ an alternative surface   form in the same language expressing the same   semantic content as the original form ’ ( Madnani   and Dorr , 2010 ) , but it is not always clear what   counts as the ‘ same semantic content ’ . Our ap-   proach requires access to reference paraphrases ;   we evaluate on three English paraphrasing datasets   which have clear grounding for the meaning of each   sentence : Paralex ( Fader et al . , 2013 ) , a dataset of   question paraphrase clusters scraped from WikiAn-   swers ; Quora Question Pairs ( QQP)sourced from   the community question answering forum Quora ;   and MSCOCO 2017 ( Lin et al . , 2014 ) , a set of   images that have been captioned by multiple anno-   tators . For the question datasets , each paraphrase is   grounded to the ( hypothetical ) answer they share .   We use the splits released by Hosking and Lapata   ( 2021 ) . For MSCOCO , each caption is grounded   by the image that it describes . We evaluate on the   public validation set , randomly selecting one cap-2492   tion for each image to use as input and using the   remaining four as references .   Model Conﬁguration Hyperparameters were   tuned on the Paralex development set , and reused   for the other evaluations . We set the depth of   the hierarchy D= 3 , and the codebook size   K= 16 . The Transformer encoder and decoder   consist of 5 layers each , and we use the vocabulary   and token embeddings from BERT - Base ( Devlin   et al . , 2018 ) . We use an initialisation decay factor   of  = 0:5 , and a depth dropout probability   p = 0:3 . A full set of hyperparameters is   given in Appendix A , and our code is available at .   Comparison Systems As baselines , we consider   three popular architectures : a vanilla autoencoder   ( AE ) that learns a single dense vector representa-   tion of an input sentence ; a Gaussian Variational   AutoEncoder ( V AE , Bowman et al . , 2015 ) , which   learns a distribution over dense vectors ; and a   Vector - Quantized Variational AutoEncoder ( VQ-   V AE , van den Oord et al . , 2017 ) , that represents   the full input sentence as a set of discrete codes .   All three models are trained to generate a sentence   from one of its paraphrases in the training data , and   are not trained with an autoencoder objective . We   implement a simple tf - idf baseline ( Jones , 1972 ) ,   retrieving the question from the training set with   the highest cosine similarity to the input . Finally ,   we include a basic copy baseline as a lower bound ,   that simply uses the input sentences as the output .   We also compare to a range of recent para-   phrasing systems . Latent bag - of - words ( BoW , Fu   et al . , 2019 ) uses an encoder - decoder model with   a discrete bag - of - words as the latent encoding .   SOW / REAP ( Goyal and Durrett , 2020 ) uses a two   stage approach , deriving a set of feasible syntac-   tic rearrangements that is used to guide a second   encoder - decoder model . BTmPG ( Lin and Wan,2021 ) uses multi - round generation to improve di-   versity and a reverse paraphrasing model to pre-   serve semantic ﬁdelity . We use the results after 10   rounds of paraphrasing . Separator ( Hosking and   Lapata , 2021 ) uses separated , non - hierarchical en-   coding spaces for the meaning and form of an input ,   and an additional inference model to predict the   target syntactic form at test time . All comparison   systems were trained and evaluated on our splits of   the datasets .   As an upper bound , we select a sentence from   the evaluation set to use as an oracle syntactic ex-   emplar , conditioning generation on a sketch that is   known to represent a valid surface form .   5 Results   Our experiments were designed to test two primary   hypotheses : ( 1 ) Does HRQ - V AE learn hierarchical   decompositions of an encoding space ? and ( 2 )   Does our choice of generative model enable us to   generate high quality anddiverse paraphrases ?   5.1 Probing the Hierarchy   Figure 3 shows a t - SNE ( van der Maaten and Hin-   ton , 2008 ) plot of the syntactic encodings zfor   10,000 examples from Paralex . The encodings are   labelled by their quantization , so that colours in-   dicate top - level codes q , shapes denote q , and   patternsq . The ﬁrst plot shows clear high level   structure , with increasingly ﬁne levels of substruc-   ture visible as we zoom into each cluster . This   conﬁrms that the discrete codes are ordered , with   lower levels in the hierarchy encoding more ﬁne   grained information .   To conﬁrm that intermediate levels of hierarchy   represent valid points in the encoding space , we   generate paraphrases using oracle sketches , but   truncate the sketches at different depths . Masking   one level ( i.e. , using only q;q ) reduces perfor-   mance by 2:5iBLEU points , and two levels by 5:5.2493   ( iBLEU is an automatic metric for assessing para-   phrase quality ; see Section 5.2 ) . Although encod-   ings using the full depth are the most informative ,   partial encodings still lead to good quality output ,   with a gradual degradation . This implies both that   each level in the hierarchy contains useful informa-   tion , and that the cluster centroids at each level are   representative of the individual members of those   clusters .   5.2 Paraphrase Generation   Metrics Our primary metric is iBLEU ( Sun and   Zhou , 2012 ) ,   iBLEU =  BLEU ( outputs;references )    (1   ) BLEU ( outputs;inputs ) ; ( 7 )   that measures the ﬁdelity of generated outputs   to reference paraphrases as well as the level of   diversity introduced . We use the corpus - level   variant . Following the recommendations of Sun   and Zhou ( 2012 ) , we set  = 0:8 , with a sen-   sitivity analysis shown in Appendix A. We also   report BLEU ( outputs;references ) as well as   Self - BLEU ( outputs;inputs ) . The latter allows   us to examine the extent to which models generate   paraphrases that differ from the original input .   To evaluate the diversity between multiple can-   didates generated by the same system , we report   pairwise - BLEU ( Cao and Wan , 2020 ) ,   P - BLEU = E[BLEU ( outputs;outputs ) ] :   This measures the average similarity between the   different candidates , with a lower score indicating   more diverse hypotheses .   Automatic Evaluation Shown in Table 1 , the re-   sults of the automatic evaluation highlight the im-   portance of measuring both paraphrase quality and   similarity to the input : the Copy baseline is able to   achieve high BLEU scores despite simply duplicat-   ing the input . The V AE baseline is competitive but   tends to have a hi   gh Self - BLEU score , indicating that the seman-   tic preservation comes at the cost of low syntactic   diversity . HRQ - V AE achieves both higher BLEU   scores and higher iBLEU scores than the compar-   ison systems , indicating that it is able to generate2494   higher quality paraphrases without compromising   on syntactic diversity .   The examples in Table 2 demonstrate that HRQ   is able to introduce signiﬁcant syntactic variation   while preserving the original meaning of the in-   put . However , there is still a gap between genera-   tion using predicted sketches and ‘ oracle ’ sketches   ( i.e. , when the target syntactic form is known in   advance ) , indicating ample scope for improvement .   Worked Example Since the sketches qare   latent variables , interpretation is difﬁcult . However ,   a detailed inspection of example output reveals   some structure .   Table 3 shows the model output for a single se-   mantic input drawn from Paralex , across a range   of different syntactic sketches . It shows that q   is primarily responsible for encoding the question   type , withq= 13 leading to ‘ what ’ questions and   q= 2 ‘ how ’ questions . qandqencode more   ﬁne grained details ; for example , all outputs shown   withq= 6use the indeﬁnite article ‘ a ’ .   We also examine how using increasingly granu-   lar sketches reﬁnes the syntactic template of the out-   put . Table 4 shows the model output for a single se-   mantic input , using varying granularities of sketch   extracted from the exemplar . When no sketch is   speciﬁed , the model defaults to a canonical phras-   ing of the question . When only qis speciﬁed ,   the output becomes a ‘ how many ’ question , and   when a full sketch is included , the output closely   resembles the exemplar .   Generating Multiple Paraphrases We evalu-   ated the ability of our system to generate multiple   diverse paraphrases for a single input , and com-   pared to the other comparison systems capable of   producing more than one output . For both HRQ-   V AE and Separator , we used beam search to sam-   ple from the sketch prediction network as in the   top-1 case , and condition generation on the top-3   hypotheses predicted . For BTmPG , we used the   paraphrases generated after 3 , 6 and 10 rounds . For   the V AE , we conditioned generation on 3 different   samples from the encoding space . The results in   Table 5 show that HRQ - V AE is able to generate   multiple high quality paraphrases for a single input ,   with lower similarity between the candidates than   other systems .   5.3 Human Evaluation   In addition to automatic evaluation we elicited   judgements from crowdworkers on Amazon Me-   chanical Turk . They were shown a sentence and   two paraphrases , each generated by a different sys-   tem , and asked to select which one was preferred   along three dimensions : the dissimilarity of the   paraphrase compared to the original sentence ; how2495   well the paraphrase reﬂected the meaning of the   original ; and the ﬂuency of the paraphrase ( see Ap-   pendix B ) . We evaluated a total of 300 sentences   sampled equally from each of the three evaluation   datasets , and collected 3 ratings for each sample .   We assigned each system a score of +1when it   was selected, 1when the other system was se-   lected , and took the mean over all samples . Nega-   tive scores indicate that a system was selected less   often than an alternative . We chose the four best   performing models for our evaluation : HRQ - V AE ,   Separator , Latent BoW , and V AE .   Figure 4 shows that although the V AE baseline   is the best at preserving question meaning , it is   also the worst at introducing variation to the output .   HRQ - V AE better preserves the original question   intent compared to the other systems while intro-   ducing more diversity than the V AE , as well as   generating much more ﬂuent output .   5.4 Ablations   To conﬁrm that the hierarchical model allows for   more expressive sketches , we performed two abla-   tions . We compared to the full model using oracle   sketches , so that code prediction performance was   not a factor . We set the depth D= 1andK= 48 ,   giving equivalent total capacity to the full model   ( D= 3;K= 16 ) but without hierarchy . We also   removed the initialisation scaling at lower depths ,   instead initialising all codebooks with the same   scale . Table 6 shows that a non - hierarchical model   with the same capacity is much less expressive .   We also performed two ablations against the   model using predicted sketches ; we removed depth   dropout , so that the model is always trained on a   full encoding . We conﬁrm that learning the code-   books jointly with the encoder / decoder leads to   a stronger model , by ﬁrst training a model with   a continuous Gaussian bottleneck ( instead of the   HRQ - V AE ) ; then , we recursively apply k - means   clustering ( Lloyd , 1982 ) , with the clustering at each   level taking place over the residual error from all   levels so far , analogous to HRQ - V AE . The results   of these ablations shown in Table 6 indicate that our   approach leads to improvements over all datasets .   6 Related Work   Hierarchical V AEs VQ - V AEs were initially pro-   posed in computer vision ( van den Oord et al . ,   2017 ) , and were later extended to be ‘ hierarchical ’   ( Razavi et al . , 2019 ) . However , in vision the term   refers to a ‘ stacked ’ version architecture , where the   output of one variational layer is passed through a   CNN and then another variational layer that can be   continuous ( Vahdat and Kautz , 2020 ) or quantized   ( Williams et al . , 2020 ; Liévin et al . , 2019 ; Willetts   et al . , 2021 ) . Unlike these approaches , we induce a   single latent space that has hierarchical properties .   Other work has looked at using the properties of   hyperbolic geometry to encourage autoencoders   to learn hierarchical representations . Mathieu   et al . ( 2019 ) showed that a model endowed with a   Poincaré ball geometry was able to recover hierar-   chical structure in datasets , and Surís et al . ( 2021 )   used this property to deal with uncertainty in pre-   dicting events in video clips . However , their work   was limited to continuous encoding spaces , and the   hierarchy discovered was known to exist a priori .   Syntax - controlled Paraphrase Generation   Prior work on paraphrasing has used retrieval   techniques ( Barzilay and McKeown , 2001 ) ,   Residual LSTMs ( Prakash et al . , 2016 ) , V AEs   ( Bowman et al . , 2016 ) , VQ - V AEs ( Roy and   Grangier , 2019 ) and pivot languages ( Mallinson   et al . , 2017 ) . Syntax - controlled paraphrase   generation has seen signiﬁcant recent interest , as a2496means to explicitly generate diverse surface forms   with the same meaning . However , most previous   work has required knowledge of the correct or   valid surface forms to be generated ( Iyyer et al . ,   2018 ; Chen et al . , 2019a ; Kumar et al . , 2020 ;   Meng et al . , 2021 ) . It is generally assumed that   the input can be rewritten without addressing   the problem of predicting which template should   be used , which is necessary if the method is to   be useful . Hosking and Lapata ( 2021 ) proposed   learning a simpliﬁed representation of the surface   form using VQ , that could then be predicted at   test time . However , the discrete codes learned by   their approach are not independent and do not   admit a known factorization , leading to a mismatch   between training and inference .   7 Conclusion   We present a generative model of paraphrasing ,   that uses a hierarchy of discrete latent variables as   a rough syntactic sketch . We introduce HRQ - V AE ,   a method for mapping these hierarchical sketches   to a continuous encoding space , and demonstrate   that it can indeed learn a hierarchy , with lower lev-   els representing more ﬁne - grained information . We   apply HRQ - V AE to the task of paraphrase genera-   tion , representing the syntactic form of sentences as   paths through a learned hierarchy , that can be pre-   dicted during testing . Extensive experiments across   multiple datasets and a human evaluation show that   our method leads to high quality paraphrases . The   generative model we introduce has potential ap-   plication for any natural language generation task ;   zcould be sourced from a sentence in a differ-   ent language , from a different modality ( e.g. , im-   ages or tabular data ) or from a task - speciﬁc model   ( e.g. , summarization or machine translation ) . Fur-   thermore , HRQ - V AE makes no assumptions about   the type of space being represented , and could in   principle be applied to a semantic space , learning a   hierarchy over words or concepts .   Acknowledgements   We thank our anonymous reviewers for their feed-   back . This work was supported in part by the   UKRI Centre for Doctoral Training in Natural   Language Processing , funded by the UKRI ( grant   EP / S022481/1 ) and the University of Edinburgh .   Lapata acknowledges the support of the European   Research Council ( award number 681760 , “ Trans-   lating Multiple Modalities into Text”).References24972498   A Hyperparameters   The hyperparameters given in Table 7 were se-   lected by manual tuning , based on a combination of :   ( a ) validation iBLEU scores with depth masking ,   ( b ) validation BLEU scores using oracle sketches ,   and ( c ) validation iBLEU scores using predicted   syntactic codes .   The Gumbel temperature  is decayed during   training as a function of the step t , according to the   following equation :   ( t ) = max(2 2   1 + e;0:5 ): ( 8)   Intuitively , this smoothly decays  from an initial   value of 2 , with a half - life of 10k steps , to a mini-   mum value of 0.5 .   We use  = 0:8when calculating iBLEU , but as   shown in Figure 5 our conclusions are not sensitive2499   to this value , and our model outperforms all com-   parison systems on all datasets for 0:7  0:9 .   Models were trained on a single GPU , with train-   ing taking between one and three days depending   on the dataset . We use SacreBLEU ( Post , 2018 ) to   calculate BLEU scores .   B Human Evaluation   Annotators were recruited from the UK and USA   via Amazon Mechanical Turk , and were compen-   sated for their time above a living wage in those   countries . A full Participant Information Sheet was   provided , and the study was approved by an inter-   nal ethics committee . Annotators were asked to   rate the outputs according to the following criteria :   •Which system output is the most ﬂuent and   grammatical ?   •To what extent is the meaning expressed in   the original sentence preserved in the rewrit-   ten version , with no additional information   added ?   •Does the rewritten version use different words   or phrasing to the original ? You should choose   the system that uses the most different words   or word order .   C Exemplar Retrieval Process   Our approach requires exemplars during training   to induce the separation between latent spaces . We   follow the approach introduced by Hosking and   Lapata ( 2021 ) . During training , we retrieve ex-   emplars xfrom the training data following a   process which ﬁrst identiﬁes the underlying syntax   ofY , and ﬁnds a question with the same syntactic   structure but a different , arbitrary meaning . We use   a shallow approximation of syntax , to ensure the   availability of equivalent exemplars in the training   data . An example of the exemplar retrieval pro-   cess is shown in Table 8 ; we ﬁrst apply a chunker   ( FlairNLP , Akbik et al . , 2018 ) to Y , then extract   the chunk label for each tagged span , ignoring stop-   words . This gives us the template thatYfollows .   We then select a question at random from the train-   ing data with the same template to give x. If no   other questions in the dataset use this template , we   create an exemplar by replacing each chunk with a   random sample of the same type .   D Analysis of Code Properties   We deﬁne two features of sentences : ( 1 ) the pres-   ence of common auxiliary verbs that roughly indi-   cate the tense of the sentence ( present , future , etc . ) ;   and ( 2 ) the presence of different question or ‘ wh- ’   words . We calculate the distributions of these fea-   tures for each code qat different levels , with the   results shown in Figure 6 . Each column represents   the distribution over the feature for a speciﬁc code .   Figure 6a shows clear evidence that the sentences   are ( at least partly ) clustered at the top level based   on the verb used , while Figure 6b shows that level   2 encodes the question type.25002501