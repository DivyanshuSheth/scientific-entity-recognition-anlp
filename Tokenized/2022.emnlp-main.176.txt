  Xiaobao WuAnh Tuan LuuXinshuai DongNanyang Technological UniversityCarnegie Mellon University   xiaobao002@e.ntu.edu.sg , anhtuan.luu@ntu.edu.sg   xinshuad@andrew.cmu.edu   Abstract   To overcome the data sparsity issue in short   text topic modeling , existing methods com-   monly rely on data augmentation or the data   characteristic of short texts to introduce more   word co - occurrence information . However ,   most of them do not make full use of the aug-   mented data or the data characteristic : they   insufﬁciently learn the relations among sam-   ples in data , leading to dissimilar topic dis-   tributions of semantically similar text pairs .   To better address data sparsity , in this paper   we propose a novel short text topic modeling   framework , Topic - Semantic Contrastive Topic   Model ( TSCTM ) . To sufﬁciently model the   relations among samples , we employ a new   contrastive learning method with efﬁcient pos-   itive and negative sampling strategies based   on topic semantics . This contrastive learning   method reﬁnes the representations , enriches   the learning signals , and thus mitigates the   sparsity issue . Extensive experimental results   show that our TSCTM outperforms state - of-   the - art baselines regardless of the data aug-   mentation availability , producing high - quality   topics and topic distributions .   1 Introduction   Topic models aim to discover the latent topics of   a document collection and infer the topic distribu-   tion of each document in an unsupervised fashion   ( Blei et al . , 2003 ) . Due to the effectiveness and   interpretability , topic models have been popular for   decades with various downstream applications ( Ma   et al . , 2012 ; Mehrotra et al . , 2013 ; Boyd - Graber   et al . , 2017 ) . However , despite the success on long   texts , current topic models generally can not handle   well short texts , such as tweets , headlines , and com-   ments ( Yan et al . , 2013 ) . The reason lies in that   topic models rely on word co - occurrence informa-   tion to infer latent topics , but such information is   Figure 1 : ( a ): Examples of short texts from   TagMyNews title dataset . Text xis an augmented   view of x , and xandxare relevant while x   andxare irrelevant . ( b , c ): Heat map of cosine simi-   larity between learned topic distributions . The similari-   ties of our TSCTM are more reasonable than NQTM .   extremely scarce in short texts ( Qiang et al . , 2020 ) .   This issue , referred to as data sparsity , can hin-   der state - of - the - art topic models from discovering   high - quality topics and thus has attracted much   attention .   To overcome the data sparsity issue , traditional   wisdom can be mainly categorized into two lines :   ( i ) Augment datasets with more short texts contain-   ing similar semantics ( Phan et al . , 2008 ; Jin et al . ,   2011 ; Chen and Kao , 2015 ) . This way can feed   extra word co - occurrence information to topic mod-   els . ( ii ) Due to the limited context , many short texts   in the same collection , such as tweets from Twitter ,   tend to be relevant , sharing similar topic semantics   ( Qiang et al . , 2020 ) ; to leverage this data character-   istic , models such as DMM ( Yin and Wang , 2014 ;   Li et al . , 2016 ) and state - of - the - art NQTM ( Wu   et al . , 2020b ) learn similar topic distributions from   relevant samples . These two lines of thought have   been shown to achieve good performance and miti-   gate data sparsity to some extent .   However , existing short text topic models neither   make full use of the augmented data nor the crucial2748data characteristic . To begin with , an augmented   text is expected to have a similar topic distribution   as the original text since they share similar topic   semantics , but existing approaches tend to overlook   this important relation between samples . As shown   in Figure 1b , text xand its augmented view x   have similar topic semantics , but their topic dis-   tributions inferred by NQTM are far from simi-   lar . Moreover , guided by the aforementioned data   characteristic , state - of - the - art methods like NQTM   attempt to learn similar topic distributions for rele-   vant samples , yet they could inappropriately do so .   Figure 1b shows that text xandxare relevant ,   but their learned topic distributions are dissimilar ;   xandxare irrelevant , but theirs are similar .   In a word , current approaches insufﬁciently model   the relations among samples in data , which hinders   fully addressing the data sparsity issue .   To better mitigate data sparsity , we in this paper   propose Topic - Semantic Contrastive Topic Model   ( TSCTM ) , a novel short text topic modeling frame-   work that uniﬁes both cases with and without data   augmentation . To be speciﬁc , TSCTM makes full   use of relations among samples with a novel topic-   semantic contrastive learning method . In the case   without data augmentation , TSCTM effectively   samples positive and negative text pairs based on   topic semantics . In the case with data augmenta-   tion , TSCTM also smoothly incorporates the re-   lations between augmented and original samples ,   enabling better utilization of data augmentation .   Through the novel contrastive learning method ,   TSCTM sufﬁciently models the relations among   samples , which enriches the learning signals , re-   ﬁnes the learning of representations , and thus miti-   gates the data sparsity issue ( see Figure 1c for an   illustration ) . We summarized the main contribu-   tions of this paper as follows :   •We follow a contrastive learning perspec-   tive and propose a novel contrastive learning   method with efﬁcient positive and negative   pairs sampling strategies to address the data   sparsity issue in short text topic modeling .   •We propose a novel short text topic model-   ing framework , Topic - Semantic Contrastive   Topic Model ( TSCTM ) , which is the ﬁrst such   framework that concerns both cases with and   without data augmentation .   •We validate our method with extensive exper-   iments where TSCTM effectively mitigatesdata sparsity and consistently surpasses state-   of - the - art baselines , producing high - quality   topics and topic distributions .   2 Related Work   Topic Modeling Based on classic long text topic   models ( Hofmann , 1999 ; Blei et al . , 2003 ; Lee   et al . , 2020 ) , various probabilistic topic models for   short texts have been proposed ( Yan et al . , 2013 ;   Yin and Wang , 2014 ; Li et al . , 2016 ; Wu and Li ,   2019 ) . They use Gibbs Sampling ( Grifﬁths and   Steyvers , 2004 ) or Variational Inference ( Blei et al . ,   2017 ) to infer model parameters . Later , due to the   effectiveness and brevity of Variational AutoEn-   coder ( V AE , Kingma and Welling , 2014 ; Rezende   et al . , 2014 ) , many neural topic models have been   introduced ( Miao et al . , 2016 , 2017 ; Srivastava and   Sutton , 2017 ; Card et al . , 2018 ; Nan et al . , 2019 ; Di-   eng et al . , 2020 ; Li et al . , 2021 ; Wu et al . , 2020a , b ,   2021 ; Wang et al . , 2021 ) . Among those methods ,   the most related one to this paper is NQTM ( Wu   et al . , 2020b ) . Although NQTM also uses vec-   tor quantization to aggregate the short texts with   similar topics , however , we note that our method   differs signiﬁcantly in that : ( i ) Our TSCTM frame-   work uses the novel topic - semantic contrastive   learning method that fully considers the relations   among samples with effective positive and negative   sampling strategies , while NQTM only considers   the relations between samples with similar seman-   tics . ( ii ) Our TSCTM framework can adapt to the   case with data augmentation by sufﬁciently mod-   eling the relations brought by augmented samples ,   achieving higher performance gains , while NQTM   can not fully incorporate such relations .   Contrastive Learning The idea of contrastive   learning is to measure the similarity relations of   sample pairs in a representation space ( Hadsell   et al . , 2006 ; Oh Song et al . , 2016 ; Hjelm et al . ,   2018 ; Van den Oord et al . , 2018 ; Frosst et al . ,   2019 ; Wang et al . , 2019 ; He et al . , 2020 ; Wang   and Isola , 2020 ) . It has been widely explored in   the visual ﬁeld , such as image classiﬁcation ( Chen   et al . , 2020 ; Khosla et al . , 2020 ) , objective detec-   tion ( Xie et al . , 2021 ) , and image segmentation   ( Zhao et al . , 2021 ) . For text data , some studies   use contrastive loss ( Gao et al . , 2021 ; Nguyen and   Luu , 2021 ) by sampling salient words from texts   to build positive samples , but they could be inap-   propriate for short text topic modeling due to the   limited context of short texts ( shown in Sec . 5.1).2749In contrast , our new framework can discover ef-   fective samples for learning contrastively based on   the topic semantics and can smoothly adapt to the   case with augmentations , both of which better ﬁt   the short text modeling context .   3 Methodology   In this section , we ﬁrst review the background of   topic modeling . Then we introduce topic - semantic   contrastive learning , a novel approach for short text   topic modeling . Finally , we put this contrastive   learning into the topic modeling context and pro-   pose our Topic - Semantic Contrastive Topic Model .   3.1 Notations and Problem Setting   Our notations and problem setting of topic mod-   eling follow LDA ( Blei et al . , 2003 ) . Consider a   collection of Ndocuments{x , ... , x}with   Vunique words , i.e. , vocabulary size . We require   to discover Ktopics from the collection . Each   topic is interpreted as its relevant words and deﬁned   as a distribution over all words ( topic - word distri-   bution ): β∈R. Then , β= ( β , ... , β)∈   Ris the topic - word distribution matrix . A   topic model also infers what topics a document   contains , i.e. , the topic distribution of a document ,   denoted as θ∈∆.   3.2 Topic - Semantic Contrastive Learning   The core difference between our TSCTM and a   conventional topic model lies in that we employ the   novel topic - semantic contrastive learning method   to model the relations among samples . As such , the   learning signals are enriched through sufﬁciently   modeling the relations among texts to address the   data sparsity issue . Figure 2 illustrates our topic-   semantic contrastive learning method .   3.2.1 Encoding Short Texts   To employ our topic - semantic contrastive learning ,   the ﬁrst step is to encode short text inputs into a   semantic space and obtain the corresponding rep-   resentations and topic distributions . Speciﬁcally ,   we employ an encoder neural network fwith   parameter Θto encode short text xand get its   representation h = f(x ) . The topic distribu-   tion of xis denoted as θand is computed by   normalizing hinto a probability simplex with a   softmax function as θ= softmax ( h ) . Note   that we train topic distribution θwith a topic   modeling objective , which will be introduced later .   3.2.2 Positive Pairs for Contrastive Learning   To utilize the vital characteristic of short texts   ( many short texts in a collection like Twitter tend   to share similar topics due to the limited context ) ,   we propose to ﬁnd those semantically similar texts   and model them as positive pairs to each other for   contrastive learning . Therefore , we can employ a   contrastive learning objective to align those seman-   tically similar texts in terms of representations and   thus topic distributions .   However , it is non - trivial to ﬁnd those semanti-   cally similar texts as positive pairs . Some previous   methods like CLNTM ( Nguyen and Luu , 2021 )   samples salient words to build positive pairs for   long texts , but this way does not ﬁt short texts   well due to the extremely limited context ( shown   in Sec . 5.1 ) . Differently , DMM ( Yin and Wang ,   2014 ; Li et al . , 2016 ) follows a clustering process   to aggregate short texts with similar topics , but   lacks the ﬂexibility of model design as it requires   model - speciﬁc derivations for parameter inference .   As such , we propose to employ vector quantization   ( van den Oord and Vinyals , 2017 ) to ﬁnd positive   pairs for short texts .   Speciﬁcally , as shown in Figure 2 , we ﬁrst quan-   tize topic distribution θto the closest embedding   vector , and its quantized topic distribution θis   computed as :   θ = e(1 )   q(θ ) = argmin / bardblθ−e / bardbl . ( 2)2750Here , ( e , e , ... , e)∈RareKpredeﬁned   embedding vectors , and q(·)∈{1, ... ,K}outputs   the index of the quantized embedding vector . These   embedding vectors are initialized as different one-   hot vectors before training to ensure that they are   far away from each other for distinguishable quanti-   zation ( Wu et al . , 2020b ) . We then model the short   texts with the same quantization indices as positive   pairs , as follows :   { x , x}whereq(θ ) = q(θ).(3 )   This is because topic distributions of short texts   with similar semantics are learned to be quantized   to the same embedding vectors .   3.2.3 Negative Pairs for Contrastive Learning   We ﬁrst explain why we need to push negative   pairs away from each other . Then we propose a   novel semantic - based negative sampling strategy   to sample semantically effective negative pairs .   Why Negative Pairs ? We also need negative   pairs to sufﬁciently model the relations among sam-   ples . Pulling close semantically similar short texts   provides additional learning signals to address data   sparsity , however two texts with different seman-   tics can sometimes be wrongly viewed as a positive   pair , leading to less distinguishable representations   ( see Figure 1b ) . To mitigate this issue , we propose   to ﬁnd negative pairs in the data and explicitly push   them away , so we can sufﬁciently model the rela-   tions among samples to better improve topic mod-   eling for short texts . The use of negative pairs can   also be supported from an information - theoretical   perspective following Wang and Isola ( 2020 ): push-   ing away negative pairs facilitates uniformity , thus   maximizing the mutual information of the repre-   sentations of positive pairs . Otherwise , if we only   pull close positive pairs , chances are high that all   the representations will collapse towards each other   and become less distinguishable .   In a word , pulling close positive pairs and push-   ing away negative pairs are both vital for better   representations and topic distributions , and they   together justify the use of contrastive learning to   regularize the learning of short text topic models   ( see empirical support in Sec . 5.1 and 5.2 ) .   Semantic - based Negative Sampling Conven-   tional contrastive learning methods such as He et al .   ( 2020 ) ; Chen et al . ( 2020 ) simply take different   samples as negative pairs . This is reasonable inthe context of long text topic modeling as different   samples in a long text dataset have sufﬁciently var-   ious contexts to contain different topics . However ,   for short text topic modeling , many samples actu-   ally share similar topics as the aforementioned data   characteristic . Therefore , simply taking different   samples as negative pairs can wrongly push away   semantically similar pairs , which hampers topic   modeling performance ( shown in Sec . 5.2 ) .   To overcome this issue , we here propose a neat   and novel semantic - based negative sampling strat-   egy . Similar to our positive pair sampling strategy ,   we sample negative pairs according to the quanti-   zation result as in Eq . ( 2 ) . Speciﬁcally , two texts   are expected to contain different topics semantics   if their topic distributions are quantized to different   embedding vectors ; thus we take such a pair of   texts as a negative pair { x , x } :   { x , x}whereq(θ)/negationslash = q(θ).(4 )   Our negative sampling strategy better aligns with   the characteristic of short texts , and does not in-   troduce complicated preprocessing steps or addi-   tional modules , which simpliﬁes the architecture   and eases computational cost .   3.2.4 Topic - Semantic Contrastive Objective   We have positive and negative pairs through our   sampling strategies deﬁned in Eq . ( 3 ) and Eq . ( 4 ) .   Now as illustrated in Figure 2 , we formulate our   topic - semantic contrastive ( TSC ) objective follow-   ing Van den Oord et al . ( 2018 ):   L(x ) = /summationdisplay−logexp ( g(h , h))/summationtextexp ( g(h , h ) ) ,   wherej∈{j|q(θ)/negationslash = q(θ ) }   and / lscript∈{/lscript|q(θ ) = q(θ ) } . ( 5 )   In Eq . ( 5),g(·,·)can be any score function to mea-   sure the similarity between two representations ,   and we follow Wu et al . ( 2018 ) to employ the co-   sine similarity as g(a , b ) = cos(a , b)/τwhereτ   is a hyper - parameter controlling the scale of the   score . This objective pulls close the representations   of positive pairs ( h , h ) and pushes away the   representations of negative pairs ( h , h ) . Thus   this provides more learning signals to topic mod-   eling by correctly capturing the relations among   samples , which alleviates the data sparsity issue.27513.3 Topic - Semantic Contrastive Topic Model   Now we are able to combine the topic - semantic   contrastive objective with the objective of short text   topic modeling to formulate our Topic - Semantic   Contrastive Topic Model ( TSCTM ) .   Short Text Topic Modeling Objective We fol-   low the framework of AutoEncoder to design our   topic modeling objective . As the input short text   xis routinely transformed into Bag - of - Words ,   its reconstruction is modeled as sampling from a   multinomial distribution : Mult(softmax ( βθ ) )   ( Miao et al . , 2016 ) . Here , θis the quantized   topic distribution for reconstruction , and βis a   learnable parameter to model the topic - word distri-   bution matrix . Then , the expected log - likelihood is   proportional to xlog(softmax ( βθ))(Srivas-   tava and Sutton , 2017 ) . Therefore , we deﬁne the   objective for short text topic modeling ( TM ) as :   L(x ) = −xlog(softmax ( βθ ) )   + /bardblsg(θ)−θ / bardbl+λ / bardblsg(θ)−θ / bardbl(6 )   where the ﬁrst term measures the reconstruction   error between the input and reconstructed text . The   last two terms refer to minimizing the distance   between the topic distribution θand quantized   topic distribution θrespectively weighted by λ   ( van den Oord and Vinyals , 2017 ) . Here sg(·)de-   notes a stop gradient operation that prevents gradi-   ents from back - propagating to its inputs .   Overall Learning Objective of TSCTM The   overall learning objective of TSCTM is a combina-   tion of Eq . ( 6 ) and Eq . ( 5 ) , as :   L(x ) + λL(x ) , ( 7 )   whereλis a hyper - parameter controlling the   weight of topic - semantic contrastive objective .   This learning objective can learn meaningful rep-   resentations from data and further reﬁne the repre-   sentations through modeling the relations among   samples to enrich learning signals , which mitigates   the data sparsity issue and improves the topic mod-   eling performance of short texts .   3.4 Learning with Data Augmentation   In this section , we adapt our Topic - Semantic Con-   trastive Topic Model to the case where data aug-   mentation is available to fully utilize the introduced   augmentations .   Incorporating Data Augmentation Letxde-   note one augmented view of x. As our augmenta-   tion techniques can ensure that xandxshare   similar topic semantics as much as possible ( details   about how we augment data will be introduced in   Sec . 4.2 ) , we explicitly consider xandxas a   positive pair . Besides , we consider xandx   as a negative pair if xandxare so . This is   because if xandxpossess dissimilar topic   semantics , then xandxshould as well . Tak-   ing these two points into consideration , as shown   in Figure 2 , we formulate our topic semantic con-   trastive objective with data augmentation as   L(x , x ) = −logexp ( g(h , h ) )   D   + λ / summationdisplay−logexp ( g(h , h ) )   D ,   D=/summationdisplayexp ( g(h , h ) ) + exp ( g(h , h ) ) ,   wherej∈{j|q(θ)/negationslash = q(θ ) }   and / lscript∈{/lscript|q(θ ) = q(θ ) } . ( 8)   Hereλ is a weight hyper - parameter of the con-   trastive objective for the positive pairs in the origi-   nal dataset . Compared to Eq . ( 5 ) , this formulation   additionally incorporates the relation between posi-   tive pair x , xby making their representations   handhclose to each other and the relation   between negative pair x , xby pushing away   their representations handh .   Overall Learning Objective of TSCTM with   Data Augmentation Combining Eq . ( 6 ) with   augmented data and Eq . ( 8) , we are able to for-   mulate the ﬁnal learning objective of TSCTM with   data augmentation as follows :   L(x ) + L(x ) + λL(x , x ) ,   ( 9 )   where we jointly reconstruct the positive pair   x , xand regularize the learning by the topic-2752   semantic contrastive objective with augmented   samples . Accordingly , our method smoothly adapts   to the case with data augmentation .   4 Experimental Setting   In this section , we conduct comprehensive experi-   ments to show the effectiveness of our method .   4.1 Datasets   We employ the following benchmark short text   datasets in our experiments : ( i ) TagMyNews ti-   tlecontains news titles released by Vitale et al .   ( 2012 ) with 7 annotated labels like “ sci - tech ” and   “ entertainment ” . ( ii ) AG News includes news di-   vided into 4 categories like “ sports ” and “ business ”   ( Zhang et al . , 2015 ) . We use the subset provided   by Rakib et al . ( 2020 ) . ( iii ) Google News is from   Yin and Wang ( 2014 ) with 152 categories .   We preprocess datasets with the following steps   ( Wu et al . , 2020b ): ( i ) tokenize texts with nltk ;   ( ii ) convert characters to lower cases ; ( iii ) ﬁlter out   illegal characters ; ( iv ) remove texts with length   less than 2 ; ( v ) ﬁlter out low - frequency words . The   dataset statistics are reported in Table 1.4.2 Data Augmentation Techniques   To generate augmented texts , we follow Zhang et al .   ( 2021 ) and employ two simple and effective tech-   niques : WordNet Augmenter and Contextual Aug-   menter . WordNet Augmenter substitutes words in   an input text with their synonymous selected from   the WordNet database ( Ma , 2019 ) . Then , Contex-   tual Augmenter leverages the pre - trained language   models such as BERT ( Devlin et al . , 2018 ) to ﬁnd   the top - n suitable words of the input text for inser-   tion or substitution ( Kobayashi , 2018 ; Ma , 2019 ) .   To retain the original semantics as much as pos-   sible , we only change 30 % words and also ﬁlter   low - frequency words following Zhang et al . ( 2021 ) .   With these augmentation techniques , we can suf-   ﬁciently retain original semantics and meanwhile   bring in more word - occurrence information to alle-   viate the data sparsity of short texts .   4.3 Baseline Models   We compare our method with the following state-   of - the - art baseline models : ( i ) ProdLDA ( Srivas-   tava and Sutton , 2017 ) , a neural topic model   based on the standard V AE with a logistic nor-   mal distribution as an approximation of Dirichlet2753   prior . ( ii ) WLDA ( Nan et al . , 2019 ) , a Wasserstein   AutoEncoder ( Tolstikhin et al . , 2018 ) based topic   model . ( iii ) CLNTM ( Nguyen and Luu , 2021 ) ,   a recent topic model with contrastive learning de-   signed for long texts , which samples salient words   of texts as positive samples . ( iv ) NQTM ( Wu et al . ,   2020b ) , a state - of - the - art neural short text topic   model with vector quantization . ( v ) WeTe ( Wang   et al . , 2022 ) , a recent state - of - the - art method us-   ing conditional transport distance to measure the   reconstruction error between texts and topics which   both are represented with embeddings . Note that   the differences between NQTM and our method   are described in Sec . 2 . The implementation detail   of our method can be found in Appendix A.   5 Experimental Result   5.1 Topic Quality Evaluation   Evaluation Metric Following Nan et al . ( 2019 ) ;   Wang et al . ( 2022 ) , we evaluate the quality of dis-   covered topics from two perspectives : ( i ) Topic   Coherence , meaning the words in a topic should   be coherent . We adopt the widely - used Coherence   Value ( C , Röder et al . , 2015 ) following Wu et al .   ( 2020b ) . We use external Wikipedia documentsas   its reference corpus to estimate the co - occurrence   probabilities of words . ( ii ) Topic Diversity , mean-   ing the topics should be distinct from each other in-   stead of being repetitive . We use Topic Uniqueness   ( TU , Nan et al . , 2019 ) which measures the pro-   portions of unique words in the discovered topics .   Hence a higher TUscore indicates the discovered   topics are more diverse . With these two metrics ,   we can comprehensively evaluate topic quality . We   run each model 5 times and report the experimen - tal results in the two cases : without and with data   augmentation as follows .   Without Data Augmentation In the case with-   out data augmentation , only original datasets are   used for all models in the experiments , and our   TSCTM uses Eq . ( 7 ) as the objective function . The   results are reported in the upper part of Table 2 . We   see that TSCTM surpasses all baseline models in   terms of both coherence ( C ) and diversity ( TU )   under 50 and 100 topics across all datasets . Besides ,   it is worth mentioning that our TSCTM signiﬁ-   cantly outperforms NQTM and CLNTM . NQTM   insufﬁciently models the relations among samples   since it only considers texts with similar seman-   tics , and CLNTM samples salient words from texts   for contrastive learning , which is ineffective for   short texts with limited context . In contrast , our   TSCTM can discover effective samples for learning   contrastively based on the topic semantics , which   sufﬁciently models the relations among samples ,   thus achieving higher performance . Note that ex-   amples of discovered topics are in Appendix B.   These results show that TSCTM is capable of pro-   ducing higher - quality topics with better coherence   and diversity .   With Data Augmentation In the case with data   augmentation , we produce augmented texts to en-   rich datasets for all models through the techniques   mentioned in Sec . 4.2 , so all models are under the   same data condition for fair comparisons . Note   that our TSCTM uses Eq . ( 9 ) as the objective func-   tion in this case . The results are summarized in   the lower part of Table 2 . We have the following   observations : ( i ) Data augmentation can mitigate   the data sparsity issue of short text topic model-   ing to some extent . Table 2 shows that the topic   quality of several baseline models is improved   with augmentations compared to the case without .   ( ii ) TSCTM can better utilize augmentations and2754   consistently achieves better topic quality perfor-   mance . As shown in Table 2 , we see that TSCTM   reaches the best CandTUscores compared to   baseline models under 50 and 100 topics . This   shows that our method can better leverage augmen-   tations through the new topic - semantic contrastive   learning to further alleviate data sparsity and im-   prove short text topic modeling .   The above results demonstrate that TSCTM can   adapt to both cases with or without data augmenta-   tion , effectively overcoming the data sparsity chal-   lenge and producing higher - quality topics .   5.2 Ablation Study   We conduct an ablation study that manifests the   effectiveness and necessity of our topic - semantic   contrastive learning method . As shown in Table 3 ,   our TSCTM signiﬁcantly outperforms the tradi-   tional contrastive learning ( Chen et al . , 2020 ) ( w/   traditional contrastive ) . This shows the better ef-   fectiveness of our novel topic - semantic contrastive   learning with the new positive and negative sam-   pling strategies . Besides , if without modeling nega-   tive pairs ( w/o negative pairs ) , the coherence ( C )   and diversity ( TU ) performance both greatly de-   grades , e.g. , from 0.479 to 0.397 and from 0.969 to   0.503 on AG News . This is because only modeling   positive pairs makes the representations all collapse   together and become less distinguishable , which   hinders the learning of topics and leads to repeti-   tive and less coherent topics ( see also Sec . 3.2.3 ) .   Moreover , Table 3 shows that the coherence per-   formance is hampered in the case without positive   pairs ( w/o positive pairs ) . The reason lies in that the   method can not capture the relations between posi-   tive pairs to further reﬁne representations , and thus   the inferred topics become less coherent . These   results show the effectiveness and necessity of the   positive and negative sampling strategies of our   topic - semantic contrastive learning method .   5.3 Short Text Clustering   Apart from topic quality , we evaluate the quality of   inferred topic distributions through short text clus-   tering following Wang et al . ( 2022 ) . Speciﬁcally ,   we use the most signiﬁcant topics in the learned   topic distributions of short texts as their cluster   assignments . Then , we employ the commonly-   used clustering metrics , Purity andNMI ( Man-   ning et al . , 2008 ) to measure the clustering perfor-   mance as Wang et al . ( 2022 ) . Note that our goal   is not to achieve state - of - the - art clustering perfor-   mance but to compare the quality of learned topic   distributions . Table 4 shows that the clustering per-   formance of our model is generally the best over   baseline models concerning both Purity and NMI .   This demonstrates that our model can infer more   accurate topic distributions of short texts .   5.4 Short Text Classiﬁcation   In order to compare extrinsic performance , we   conduct text classiﬁcation experiments as a down-   stream task of topic models ( Nguyen and Luu ,   2021 ) . In detail , we use the learned topic distribu-   tions by different models as features and train SVM   classiﬁers to predict the class of each short text . We   use the labels from the adopted datasets . Figure 3   shows that our TSCTM consistently achieves the   best classiﬁcation performance compared to base-   line models . Note that the p - values of signiﬁcance   tests are all less than 0.05 . This shows that the   learned topic distributions of our model are more   discriminative and accordingly can be better em-   ployed in the text classiﬁcation downstream task.2755   5.5 Analysis of Topic Distributions   In this section we analyze the learned topic distri-   butions of short texts to evaluate the modeling of   relations among samples . Figure 4 illustrates the   t - SNE ( van der Maaten and Hinton , 2008 ) visual-   ization for the learned topic distributions of original   and augmented short texts by ProdLDA , NQTM ,   and our TSCTM . It shows that the topic distribu-   tions learned by our TSCTM are more aggregated   together and well separately scattered in the space ,   in terms of only original short texts or both original   and augmented short texts . In addition , we report   the cosine similarity between the topic distributions   of original and augmented short texts in Table 5 .   Their similarity should be high since they have   similar semantics . We see that TSCTM has the   highest similarity among all models . These are be-   cause TSCTM can sufﬁciently model the relations   among samples with the novel topic - semantic con-   trastive learning , which reﬁnes the representations   and thus topic distributions . These results can fur-   ther demonstrate the effectiveness of our proposed   topic - semantic contrastive learning method.6 Conclusion   In this paper , we propose TSCTM , a novel and uni-   ﬁed method for topic modeling of short texts . Our   method with the novel topic - semantic contrastive   learning can reﬁne the learning of representations   through sufﬁciently modeling the relations among   texts , regardless of the data augmentation avail-   ability . Experiments show our model effectively   alleviates the data sparsity issue and consistently   outperforms state - of - the - art baselines , generating   high - quality topics and deriving useful topic distri-   butions of short texts .   Limitations   Our method achieves promising performance to   mitigate data sparsity for short text topic modeling ,   but we believe that there are two limitations to be   explored for future works : ( i ) More data augmenta-   tion techniques may be studied to further improve   short text topic modeling performance . ( ii ) The   possible metadata of short texts , like authors , hash-   tags , and sentiments , can be considered to further   assist the modeling of relations .   Acknowledgement   We want to thank all anonymous reviewers for their   helpful comments .   References2756275727582759   A Model Implementation   We conduct experiments on NVIDIA GPU , and it   takes less than 0.5 GPU hours to train our model   on each dataset . For our model , the encoder net-   workfis a two - layer MLP with softplus as the   activation function , same as Wu et al . ( 2020b ) ,   and we use Adam ( Kingma and Ba , 2014 ) to opti-   mize model parameters . We run our model for 200   epochs with learning rate as 0.002 following Sri-   vastava and Sutton ( 2017 ) , and λas 0.1 following   van den Oord and Vinyals ( 2017 ) .   B Examples of Discovered Topics   Following Nan et al . ( 2019 ) ; Wu et al . ( 2020b ) , we   randomly select some examples of discovered top-   ics by ProdLDA , NQTM , and our TSCTM from   Google News for qualitative study since they per-   form relatively better among baselines . As shown   in Table 6 , ProdLDA produces several redundant   topics including “ giraffe ” , and these topics are   less informative as they are associated with irrele-   vant words like “ fundamentalist ” and “ animation ” .   NQTM also has repetitive topics about “ kanye ” .   In contrast , our TSCTM only generates one coher-   ent topic about “ animation ” , “ kanye ” , and “ giraffe ”   with relevant words . For example , the topic of   TSCTM is more focused on “ animation ” with “ dis-   ney ” , the movie name “ frozen ” and its theme song   singer “ idina menzel”.2760