  Swaroop MishraDaniel KhashabiChitta BaralHannaneh HajishirziAllen Institute for AIUniversity of WashingtonArizona State University   Abstract   Humans ( e.g. , crowdworkers ) have a remark-   able ability in solving different tasks , by sim-   ply reading textual instructions that deﬁne   them and looking at a few examples . Despite   the success of the conventional supervised   learning on individual datasets , such mod-   els often struggle with generalization across   tasks ( e.g. , a question - answering system can-   not solve classiﬁcation tasks ) . A long - standing   challenge in AI is to build a model that   learns a new task by understanding the human-   readable instructions that deﬁne it . To study   this , we introduce N I ,   a dataset of 61 distinct tasks , their human-   authored instructions , and 193 ktask instances   ( input - output pairs ) . The instructions are ob-   tained from crowdsourcing instructions used   to create existing NLP datasets and mapped   to a uniﬁed schema . Using this meta - dataset ,   we measure cross - task generalization by train-   ing models on seen tasks and measuring gen-   eralization to the remaining unseen ones . We   adopt generative pre - trained language models   to encode task - speciﬁc instructions along with   input and generate task output . Our results   indicate that models beneﬁt from instructions   when evaluated in terms of generalization to   unseen tasks ( 19 % better for models utilizing   instructions ) . These models , however , are far   behind an estimated performance upperbound ,   indicating signiﬁcant room for more progress   in this direction .   1 Introduction   We have witnessed great progress in solving many   NLP datasets through ﬁne - tuning pre - trained lan-   guage models ( LMs ) ( Peters et al . , 2018 ; Brown   et al . , 2020 ) . More recent studies show tremendous   promise in generalization within the set of observed   tasks through multi - task training and uniﬁed en-   coding ( Khashabi et al . , 2020 ; Aghajanyan et al . ,Figure 1 : We construct the N I   dataset from crowdsourcing instructions and instances   of different NLP datasets . We study if models can learn   from seen tasks and generalize to unseen tasks given   their natural crowdsourcing instructions .   2021 ) . However , cross - task generalization – gener-   alization tounseen tasks – has generally remained   under - explored . For example , can we supervise   a model with instances of grammar checking or   question answering tasks , yet expect it to solve   a different task like question typing ( Fig.1 ) . Evi-   dently , humans are capable of such generalizations ;   an average human can follow natural language in-   structions to solve a variety of problems , as evident   by the success of crowdsourcing platforms ( also   argued in Efrat and Levy ( 2020 ) ) . In this paper ,   we study if models can generalize to unseen tasks   given their crowdsourcing instructions ( Fig.1 ) .   We build N I , a dataset   consisting of natural crowdsourcing instructions   for various tasks and their instances . Training on   seen tasks Tin our dataset , we build a model   that learns to follow natural instructions that deﬁne   a task and perform tasks ( i.e. , mapping input to out-   put ) . Testing on unseen tasks T , we evaluate   if the model can perform unseen tasks solely from3470   their instructions and without any task - speciﬁc la-   beled data ( Table 2a ; right ) . In contrast to the   instance - level generalization ( Table 2a ; left ) , our   model uses instruction as additional input , and eval-   uations are done on tasks that were not observed in   the training stage .   We compile N I from   task instructions written by researchers for crowd-   sourcing existing NLP datasets . Such crowdsourc-   ing instructions often elaborate a variety of details   about how a task should ( and should not ) be done .   To provide a systematic study of various elements   of crowdsourcing instructions , we map them to   a uniﬁed schema to cover the most important el-   ements of task descriptions — such as deﬁnition ,   constraints , positive and negative examples . We   collect tasks in N I as min-   imal stand - alone steps provided to crowdworkers   to complete a downstream NLP task . For exam-   ple , tasks collected from QASC ( Khot et al . , 2020 )   include sub - tasks about generating topic words or   combining facts , as well as answering multi - hop   questions . Therefore our dataset not only contains   typical downstream tasks in NLP , but also the inter-   mediate subtasks that are not well - represented in   the common benchmarks . The uniﬁed schema and   the collection of minimal subtasks enable training   LMs that can generalize across different tasks by   learning from instructions . In total , our dataset con-   sists of 61 distinct NLP tasks and 193kinstances .   Our experimental results indicate that LMs learn   to leverage natural language instructions as they   show improved generalization to new tasks . For   example , a BART ( Lewis et al . , 2019 ) achieves   a 19 % gain in terms of cross - task generalization   compared to a model not using instructions ( § 6).Importantly , LMs can generalize better to unseen   tasks if they observe more tasks in training ( Fig.2b ) .   This upward trajectory suggests the potential for   stronger cross - task generalizable models upon scal-   ing up the diversity of tasks represented in a meta-   dataset of task instructions . Despite the beneﬁts   of instructions , we observe a sizable gap between   models ’ generalization and their estimated upper-   bounds ( 6.4 ) , encouraging the community to work   on this challenging problem .   Contributions : In summary , the contributions   of this work are as follows : ( a ) we introduce   N I , a dataset of human-   authored instructions curated from existing well-   known datasets mapped to a uniﬁed schema , provid-   ing training and evaluation data for learning from   instructions ; ( b ) we build models that can encode   instructions and show : ( b.1 ) the beneﬁt of cross-   task generalization by leveraging instructions ; ( b.2 )   the importance of different elements of instructions   in the performance ; ( b.3 ) noteworthy headroom for   improvement on our benchmark , which hopefully   will motivate further work in this direction .   2 Related Works   Learning from instructions . There is recent lit-   erature on the extent to which models follow lan-   guage instructions ( Hase and Bansal , 2021 ; Ye and   Ren , 2021 ; Gupta et al . , 2021 ; Zhong et al . , 2021 ) .   For example , Efrat and Levy ( 2020 ) examine if   language models can follow crowdsourcing instruc-   tions with no further training . On the contrary , our   work is pursuing a fundamentally different goal :   creating a dataset of crowdsourcing instructions   and task instances and formulating cross - task gen-   eralization by training models on seen tasks and3471measuring generalization to the remaining unseen   ones . Weller et al . ( 2020 ) construct a crowdsourced   dataset with short question - like task descriptions .   Compared to this work , our instructions are longer ,   more complex and natural since they were used to   collect datasets through crowdsourcing .   PromptSource and FLAN ( Wei et al . , 2022 ; Sanh   et al . , 2022 ) are two concurrent works that pursue a   similar goal as ours . A key difference between our   work to these works is in terms of data collection   strategy . Our work uses natural instructions created   by NLP researchers before the dataset instances   were created by crowd workers , and hence it con-   tains the complete deﬁnition of each task ( deﬁni-   tion , things to avoid , negative examples , etc . ) . On   the other hand , instructions in the concurrent work   are collected retroactively based on the already-   available task instances . Our natural instructions   enable evaluating models on how they learn tasks   given different elements of task descriptions . ( See   § A.5 for further comparisons . ) Nevertheless , we   believe that all these approaches to constructing   instructions and task categories are complementary   and the community will beneﬁt from considering   both towards solving the challenging problem of   cross - task generalization .   Prompt engineering . Constructing effective dis-   crete prompts for language models to perform NLP   tasks is an active area of research ( Schick and   Schütze , 2021 ; Reynolds and McDonell , 2021 ; Liu   et al . , 2021 ) . Such prompts are often extremely   short and may not include a complete deﬁnition of   complex tasks . In contrast , our instructions encode   detailed instructions as they were used to collect the   datasets . Moreover , the goals are different : Most   prompt - engineering approaches seek prompts with   higher performance on a particular task , typically   through assumptions about their target task which   make them non - trivial to generalize to any other   task . However , our introduced meta dataset enables   the measurement of generalization to unseen tasks .   Beyond standard multi - task learning . Multi-   task learning is a long - standing goal for AI ( Caru-   ana , 1997 ) and has led to successful models that   can support a wider range of tasks ( McCann et al . ,   2018 ; Raffel et al . , 2020 ; Khashabi et al . , 2020 ;   Mishra et al . , 2020 ; Aghajanyan et al . , 2021 ; Ye   et al . , 2021 ) . Most of the conventional setups in   the multi - tasking literature evaluate on instances   that belong to the tasks that are seen , i.e. , their la-   beled instances were observed during training ( 1stcolumn of Table 2a ) . We augment this setup by   introducing natural language instructions which en-   able our models to bridge to tasks that were not   seen during training .   3 Deﬁning Cross - Task Generalization   Here we formally deﬁne the problem setup for gen-   eralization across tasks . Each task tconsists of   input / output instances pX ; Yqand is described in   terms of its natural language instructions I.   Task - speciﬁc models . Standard supervised   learning algorithms use task - speciﬁc labeled   instances to learn a mapping from input xto output   y : Mpxq yforpx ; yq P pX ; Yqand is   evaluated on the test instances of the same ( or   similar ) taskpX ; Yq . We refer to this as the   instance - level generalization ( Table 2a ; left ) .   Cross - task models . In this setup , the goal is to   learn a model Mthat at inference obtains the out-   putygiven the input xand the task instruction I :   MpI ; xqy;forpx ; yqPpX ; Yq . In contrast to   the task - speciﬁc models , no task - speciﬁc training   data is used to learn the mapping M. We collect   N I ( § 4 ) to study this ques-   tion : can a model be trained to follow instructions   via training tasks Tand be generalized to follow   instructions for a task tPT . We refer to this   as atask - level generalization ( Table 2a ; right ) .   4 N I   N I consists of instructions   that describe a task ( e.g. , question answering ) and   instances of that task ( e.g. , answers extracted for a   given question ) . Fig.3 shows an example instruc-   tion for the task of ‘ generating questions that re-   quire an understanding of event duration ’ accom-   panied with positive and negative examples that   contextualize the task . Here we introduce a schema   for representing instructions ( § 4.1 ) and then de-   scribe how existing datasets ( their crowdsourcing   templates ) are mapped into our schema ( § 4.2 ) .   4.1 Instruction Schema   Instructions used in crowdsourcing various   datasets , are written by distinct authors for differ-   ent purposes , and they are different in a variety   of ways ( see Appendix A.2 for their differences . )   We introduce a uniﬁed schema ( Fig.4 ) to consis-   tently represent these diverse forms of instructions .   Our instruction schema is the result of our pilot3472   study conducted on a subset of datasets . Below we   describe the ingredients of this schema :   • T provides a high - level description of a task   and its associated skill ( such as question genera-   tion , answer generation ) .   • P is a single sentence command that often   appears before the input instance and connects it   to the instructions .   • D provides the core detailed instruc-   tions for a task .   • T A contain instructions regard-   ing undesirable annotations that must be avoided .   These help to deﬁne the scope of a task and the   space of acceptable responses .   • E C are short , but impor-   tant statements highlighted in the crowdsourcing   templates which were intended to be emphasized   or warned against .   • P E contain inputs / outputs   similar to the input given to a worker / system and   its expected output , helping crowdworkers better   understand a task ( Ali , 1981 ) .   • N E contain inputs / outputs   to emphasize T A by providing   examples that must not be produced .   • R provides explanations behind why an   example is positive or negative .   • S contains suggestions on how a   negative example could be modiﬁed to turn it   into a positive example .   The next section describes the process of map-   ping the raw instructions ( designed for crowdwork-   ers ) to our instruction schema .   4.2 Constructing N I   4.2.1 Collecting Data   Collecting raw instructions and instances . We   use existing , widely adopted NLP benchmarks   that are collected via crowdsourcing platforms   and hence , come with crowdsourcing templates .   In the ﬁrst step , we identiﬁed several datasets   and engaged with their authors to get their   crowdsourcing templates and raw data . This   yields the following datasets : CosmosQA ( Huang   et al . , 2019 ) , DROP ( Dua et al . , 2019 ) , Essential-   Terms ( Khashabi et al . , 2017 ) , MCTACO ( Zhou   et al . , 2019 ) , MultiRC ( Khashabi et al . , 2018 ) ,   QASC ( Khot et al . , 2020 ) , Quoref ( Dasigi et al . ,   2019 ) , ROPES ( Lin et al . , 2019 ) and Wino-   grande ( Sakaguchi et al . , 2020 ) .   Splitting crowdsourcing instructions into mini-   mal tasks . Almost all the crowdworking instruc-   tions include sequences of steps to guide crowd-   workers in creating task instances . For example ,   QASC and MCTACO include 7 and 19 steps in   the data creation process , respectively . We divide3473   crowdsourcing instructions into their underlying   steps and generate multiple subtasks that are min-   imal and standalone . Table 1 shows subtasks ex-   tracted for Quoref and QASC . For example , the   main task in Quoref is to answer a question given a   context paragraph , but the crowdsourcing template   consists of two sub - tasks of question generation   andanswer generation with their separate instruc-   tions . This process results in a more consistent   deﬁnition of tasks , enabling a successful mapping   of instructions into our schema , in contrast to the   work of Efrat and Levy ( 2020 ) that uses crowd-   sourcing instructions as - is .   In total , there are 61 tasks , which are categorized   into 6 semantic categories ( Table 2 ) . We assigned   these broad categories to the tasks to understand   their collective behavior in the experiments . It is   noteworthy that , despite the apparent resemblance   of the tasks included in the same category , any   pair of tasks are distinct . For example , while ques-   tion generation is part of Quoref , CosmosQA , and   QASC , each has its own separate variant of the   question generation task ( see Fig.10 in Appendix ) .   4.2.2 Mapping Raw Instructions to Schema   We manually ﬁll in the ﬁelds of our instruction   schema with the content from the crowdsourcinginstructions . For instance , parts of the raw instruc-   tions that are highlighted for emphasis are incor-   porated as part of our emphasis / caution ﬁeld . The   modiﬁcations suggested in this step were applied   by one author and were veriﬁed by another author .   Improving description quality and consistency .   We edit raw instructions to ensure their quality .   Particularly , we ﬁx writing issues ( typos , ambigui-   ties , etc . ) and redact repetitions . While repetition   often helps in augmenting human understanding ,   short and concise instructions are often more ef-   fective for computers due to their limited attention   span ( Beltagy et al . , 2020 ) .   Augmenting examples and reasons . There is a   large variance in the number of examples provided   in the raw instructions . Instructions often include   more positive examples , or some instructions do   not include any negative examples ( e.g. , QASC ) .   Whenever possible , we add negative examples such   that each task has at least two negative examples .   Furthermore , not all raw instructions contain- or for each of their examples .   For example , positive examples are usually not ac-   companied by explanations , and most datasets do   not include suggestions . We add them , wherever   such information is missing in the instructions .   Collecting input / output instances for subtasks .   Most of our tasks are the intermediate steps in   the crowdsourcing process . Therefore , to extract   input / output instances for each task , we need to   parse the raw annotations of crowdworkers for ev-   ery step . Since each dataset stores its annotations in   a slightly different format , extracting and unifying   such intermediate annotations can be non - trivial .   Veriﬁcation . An annotator veriﬁed the quality of   the resulting data in consultation with dataset au-   thors . The annotator iterated on the authors ’ feed-   back ( avg of 3 iters ) until they were satisﬁed .   Quality assessment . We ask independent human   annotators to answer 240 random instances ( 20 in-   stances from 12 random tasks , used later for our   evaluation § 5.1 ) . The subsequent evaluation of the   human - generated responses results in more than   96 % accuracy , which indicates that humans can ef-   fortlessly understand and execute our instructions .   4.2.3 N I Statistics   In summary , N I consists   of subtasks each with a set of instructions and in-3474put / output instances ( Fig.3 and 4 ) . The complete   list of instructions is included in the appendix . In   total , the dataset includes 61 tasks and 193 kin-   stances . Table 2 shows data statistics for each task   category . On average , instructions contain 4.9   positive examples and 2.2 negative examples . The   longest element of instructions is usually D - with 65.5 tokens and the shortest is   with 8.3 tokens ( more statistics in Table 3 ) .   5 Problem Setup and Models   Here we deﬁne different cross - task generalization   settings ( § 5.1 ) and the models ( § 5.2 ) .   5.1 Task Splits and Generalizations Types   Random split . This setup follows the common   practice in benchmarking NLP models with ran-   dom data splits . Here , two tasks from each task   category ( Table 2 ) in N I   are randomly selected for evaluation , and the rest   of the tasks are used for training . This leads to 12   tasks in T and 49 tasks in T.   Leave - one - out generalization . To better under-   stand the nature of cross - task generalization , we   study more restrictive settings of dividing training   and evaluation tasks .   leave -one - category : evaluates how well a model   generalizes to a task category if it is trained on   others – no task of that category is in T.   leave -one - dataset : evaluates how well a model can   generalize to all tasks in a particular dataset if it is   trained on all other tasks – no task of that dataset   is inT. This split prevents any leakage across   tasks that belong to the same source datasets .   leave - one - task : evaluates how well a model can   learn a single task by training on all other tasks .   5.2 Models   We build models using pre - trained LMs with   encoder - decoder architectures BART ( Lewis et al . ,   2019 ) for ﬁne - tuning and GPT3 ( Brown et al . ,   2020 ) for few - shot experiments .   Encoding instructions and instances . For ev-   ery problem setup , we map a given instruction I   and an input instance xinto a textual format and   decode an output yand obtain encpI ; xq . This en-   coding function is then fed to an encoder - decoder   model to predict y : M : encpI ; xqÑy .   Encoding instances follows a standard NLP   paradigm of mapping an input instance to text .   Each instruction Iconsists of multiple elements as   described in our instruction schema ( § 4.1 ) . Here ,   we map each element of the instruction to a tex-   tual format and append it before the input instance .   Fig.5 shows how we encode the full instruction .   To study the impact of each instruction element   for cross - task generalization , we compare these en-   codings : ( 1 ) , ( 2 ) . , ( 3 ) + , ( 4 ) + , ( 5 ) + , ( 6 )   + . , ( 7 ) + +   + . , and ( 8) F .   Each of these ( e.g. , and . )   correspond to prompting setups in the recent litera-   ture ( Le Scao and Rush , 2021 ; Lu et al . , 2021 ) .   BART . We use BART ( base ) ( Lewis et al . , 2019 )   which allows us to ﬁne - tune its model parameters .   This is an encoder - decoder architecture with 140 m   parameters . For each setup , the input is encoded3475   using different instruction elements , trained on all   Ttasks , and evaluated on T ( § 5.1 ) .   GPT3 . As a comparison , we evaluate   GPT3 ( Brown et al . , 2020 ) which is a 175B   parameter autoregressive LM ( 1:2klarger   than BART ) and has shown promising results in   mimicking demonstrations provided in its prompt .   We can not ﬁne - tune the parameters of this massive   model and use it as - is under its default setting   on the evaluation tasks in T ( § 5.1 ) using the   encoding introduced earlier .   6 Experiments   Evaluation metrics . We treat all of our tasks as   text generation problems and evaluate them with   automated evaluation metrics for text generation .   In particular , we use ROUGE - L ( Lin , 2004 ) to au-   tomatically evaluate the generated outputs .   Implementation details . For BART , our models   are trained for 3 epochs with a learning rate of 5e-5   for a given training split and input encoding . For   GPT3 , we use the davinci - instruct engine   and produce outputs with greedy decoding , gener-   ating up to a maximum number of tokens of 16 ( the   default value ) . We use the default stop condition   which is 2 newline tokens .   6.1 Generalization Under Various Task Splits   Table 4 reports the results of the BART model train   and evaluated with various task splits ( § 5.1 ) . For   comparison , we evaluate GPT3 which uses no ﬁne-   tuning , unlike BART that is ﬁne - tuned with the   Ttasks . The ﬁrst column corresponds to ran-   dom split of tasks , while the remaining columns re-   port cross - task generalization results of the BART   model under leave - one- xsplits ( § 5.1 ) . For x   category , the tasks in question - generation categoryare held out during training . For xdataset , the   tasks that were extracted from the QASC dataset   were excluded from training . For xtask , we   train a model on all tasks , except QASC question   generation task which is used for evaluation .   Instructions beneﬁt cross - task generalization .   The results indicate that BART beneﬁts from in-   structions in generalizing to new tasks , regardless   of task splits . For example , under random split , the   model using F I results in +19 %   gains over a model that is not using instructions .   This is particularly interesting for leave - one- cat-   egory - out split since the trained model can gen-   eralize to the tasks of a particular semantic cate-   gory , without being exposed to it . In comparison   to GPT3 , the ﬁne - tuned BART model that utilizes   instructions achieves a stronger performance de-   spite being1ksmaller than GPT3 . For exam-   ple , a BART models using F I   achieves 8 % higher performance than GPT3 under   random split of tasks .   Note that the absolute values in leave - one-   category are lower due to the difﬁculty of this setup   compared to , for example , the random split setup .   While all settings involve evaluating on tasks not   seen during training , the leave - one - category set-   ting enforces more dissimilarity among training   and evaluation tasks .   6.2 Generalization Under Instruction   Encoding and Task Categories   Table 5 reports the results of the BART model per   encodings of different instruction elements ( § 5.2 )   and for different task categories . The table shows   that encoding more elements of the instructions   generally achieves better results than just using or . It additionally   shows that the beneﬁt of the instruction elements   seems to depend on the target task category . We ob-   serve that the question - generation ( QG ) tasks ben-   eﬁt the most from , whereas   inclassiﬁcation ( CF ) , are of3476   little help . We hypothesis this is because it is easier   to mimic question - generation based on a few ex-   amples , whereas it is difﬁcult to deﬁne classes via   a few examples , where can be more   helpful . The models show little improvement in   veriﬁcation ( VF ) . We hypothesize these tasks are   inherently more difﬁcult , partially because of their   distinctness from the rest of the tasks in the dataset .   We hope future work on this line will study a wider   variety of tasks and will improve our understanding   of such failure cases .   6.3 Generalization vs. Number of Seen Tasks   Fig.2b compares the impact of the number of seen   tasks for cross - task generalization . For supervi-   sion , we randomly sample a few tasks as T   and evaluate on 6 tasks ( one from each category ) .   ( each point in the ﬁgure is averaged over 5 ran-   dom subsamples . ) The results show that with- encoding there is no tangible value   in observing more tasks . In contrast , the gener-   alization of the models that encode instructions   improves with observing more tasks . This is an   exciting observation since it suggests that scaling   up our dataset to more tasks may lead to stronger   instruction - following systems .   6.4 Analyses   Upperbound : Task - speciﬁc Models . For each   task , we obtain a task - speciﬁc model ( § 3 ) by   training BART separately on each task ’s annotated   training data . We evaluate these task - speciﬁc mod-   els to obtain a loose estimate of upperbounds for   each task . On average , task - speciﬁc models score   66 % which is considerably higher than our mod-   els ’ best generalization ( 32 % ; Table 4 ) . This indi-   cates that there is considerable room for improving   generalization - based models that use instructions .   Impact of Negative Examples . Crowdsourcing   instructions often include negative examples to ex-   emplify undesirable responses . We study how neg-   ative examples in instructions affect cross - task gen-   eralization . Our cases study ( Table 6 ) indicates   that the models work better without ( w/o ) nega-   tive examples , contrary to the previously - observed   beneﬁts of other instructional elements ( e.g. , def-   inition , positive examples ) . This is aligned with   the previous studies ( Xuan et al . , 2020 ; Lin et al . ,   2003 ) that discuss the challenges of learning from   negative examples . Interestingly , GPT3 ’s drop ( 44   vs 24 ) is more signiﬁcant than BART ( 35 vs 32 ) ,   showing that BART can partly recover through the   training step .   Error Analysis . We randomly sample 30 erro-   neous predictions of our ﬁne - tuned BART on 3 dis-   tinct tasks ( Winogrande answer generation ; QASC3477   question generation ; MC - TACO incorrect answer   generation ) . We categorize the errors into common   patterns ( Table 8) .   Unlike GPT3 which generally suffers from gen-   erating redundant content irrelevant to instructions ,   our BART model provides more control , however ,   it can fail to generate proper output . Here are sev-   eral erroneous predictions from our model :   Perceived Impact of Instruction Elements .   We survey human annotators to ﬁnd out the value   of instruction elements to humans . Except for the   negative examples which were shown to be difﬁ-   cult for models , we observe similar trends betweenhumans ’ perceived value of those elements ( Ta-   ble 7 ) and their contributions to the model perfor-   mance ( Table 5 ) . For example , humans viewed   D andT A as necessary   ﬁelds for classiﬁcation andminimal text modiﬁca-   tioncategories , respectively , which is compatible   with our empirical observations ( e.g. , + has the highest score on CF category   in Table 5 ) .   7 Conclusion   In this paper , we studied the goal of building mod-   els that generalize to new tasks by encoding and   understanding crowdsourcing instructions . We in-   troduced N I , which is built   based on existing crowdsourced datasets , that en-   ables building such models and systematically eval-   uate them . To the best of our knowledge , this is   the ﬁrst work to show the beneﬁt of instructions   towards improved cross - task generalization . Addi-   tionally , we observe that our proposed task has a   large room for improvement , which we believe will   bring more attention to building stronger models   that can generalize to a wider range of tasks .   Acknowledgements   We thank OpenAI for providing access to the GPT3   API , authors who generously shared their dataset   templates with us , Matt Peters and Nicholas Lourie   for helpful input , the Beaker team for their support   with experiments , and the anonymous reviewers   for their helpful feedback . The support of DARPA   SAIL - ON , DARPA CHESS program , NSF IIS-   2044660 , ONR N00014 - 18 - 1 - 2826 , and Paul G.   Allen Foundation is gratefully acknowledged.3478References34793480   A Datasets and their Templates   A.1 Division of Crowdsourcing Instructions   into Minimal Tasks   Fig . 9 shows an example of how a task is divided   into multiple subtasks for the MC - TACO dataset .   MC - TACO has ﬁve categories ( Event Duration ,   Event Frequency etc . ) . Each category contributes   to 2 subtasks one for question generation and one   for answer generation .   Number of tasks in each dataset . Fig . 6 illus-   trates how the number of steps in the data creation   process varies across the 6 datasets . QASC and   MC - TACO contain a relatively higher number of   steps in the data creation process in comparison to   DROP , Quoref , CosmosQA , and Winogrande .   A.2 Analysis of Crowdsourcing Templates   We analyzed crowdsourcing templates of 6 datasets :   CosmosQA ( Huang et al . , 2019 ) , DROP ( Dua et al . ,   2019 ) , MC - TACO ( Zhou et al . , 2019 ) , QASC ( Khot   et al . , 2020 ) , Quoref ( Dasigi et al . , 2019 ) , and Wino-   grande ( Sakaguchi et al . , 2020 ) . Our intention be-   hind the analysis is to identify similarities and dif-   ferences across templates and subsequently decide   regarding the collection of more templates .   Size of the instructions . We observe signiﬁcant   variation in size across the 6 datasets ( Fig . 8) . In   the case of QASC , the instruction size associated   with each step of the data creation process is very   high , whereas for Winogrande , it is exactly the   opposite – instruction size associated with each step   of the data creation process is very low . Instead ,   the size of the common instruction ( i.e. , the in-   struction preceding the ﬁrst step of the data cre-   ation process ) is high in Winogrande ; this is also   seen for DROP . The major mode of instructionvaries across datasets . Examples and instructions   associated with each step of data creation respec-   tively take up the majority of space in Quoref and   CosmosQA . MC - TACO relies on examples to ex-   plain the crowdsourcing task , while Winogrande   and QASC depend mostly on common instructions   and instructions associated with each step of the   data creation process respectively , to explain the   task to the crowdworker .   The number of positive / negative examples .   Variation in the occurrence of P andN- Examples across datasets has been illus-   trated in Fig . 7 . Only Winogrande provides an   equal number of P andN Ex-   amples . QASC instructions do not contain any   N Examples . Overall , DROP instructions   consist of a relatively higher number of examples   than other datasets .   Presence of reasons / suggestions in examples .   All datasets except QASC contain both P   andN Examples . However , Quoref is   the only dataset to provide R for all the   P andN Examples . There are   explanations associated with each of the N- Examples , but the presence of explanations3481   associated with P Examples varies across   datasets . Finally , Quoref is the only dataset to   provide S along with the R   associated with the N Examples .   A.3 Qualitative Analysis   Writing Style . There are signiﬁcant variation in   writing style across the datasets , even among those   datasets that have the common a objective ( e.g. ,   DROP , Quoref and QASC ) . DROP instructions say   " There is an AI running in the background which   will also try to answer the question . You wo n’t be   able to submit the question if the AI gives the same   response . " The writing style in Quoref however is   different : " We also want you to avoid questions   that can be answered correctly by someone without   actually understanding the paragraph . ... "   Information . We observe that sometimes in-   structions of a dataset contain information that is   relevant to several other datasets , which do not con-   tain similar instruction information . For example ,   Quoref , DROP and CosmosQA are datasets that   are all based on reading comprehension tasks . Cos-   mosQA contains a step in the data creation process   asking users to skip passages containing inappro-   priate or offensive content . This information is also   relevant to Quoref and DROP , but is not mentioned   in their respective instructions .   Hardness . In a typical crowdsourcing task , cer-   tain tasks may be harder than the others , often these   are the core tasks , e.g. : question generation , adver-   sarial data creation , etc . Additional information ,   especially in the form of tips is always helpful in   solving these hard tasks . Figure 10 illustrates that   the task of question generation is stated differently   in Quoref , CosmosQA , and QASC . QASC men-   tions an easy and detailed way to create questions ,   whereas CosmosQA mentions several different at-   tributes of a good quality question . Knowing about   the CosmosQA and QASC question generation pro-   cesses may help with data creation for Quoref and   other such question generation tasks , where less ad-   ditional information is provided regarding question   creation .   A.4 Data Curation Effort   Table 9 shows the effort distribution in the data cu-   ration process of N I . Step-   8 which involves parsing instances is the main   bottleneck in the data curation process . Table 10   shows the detailed structure of tasks in N   I . Fig . 11 shows examples of four   different tasks in N I .348234833484A.5 Qualitative Comparison to PromptSource   We provide a comparison between our proposed dataset and PromptSource ( Sanh et al . , 2022 ) . Prompt-   Source tasks are mainly focused on the common NLP downstream tasks ( such as question - answering ,   coreference , NLI , etc ) . However , since we create tasks from various steps ( including the intermediate   steps ) in a data creation process , our instructions contain a broader variety of tasks . For example , tasks for   chaining facts ( task 38 ; Table 10 ) , question typing ( task 27 ; Table 10 ) or detecting inappropriate content   ( task 22 ; Table 10 ) are unique additions in N I . Additionally , since our instructions   were originally written by various researchers targeted for crowdworkers , they are elaborate and contain   the complete deﬁnition of each task . This is somewhat evident from observation that GPT3 leads to higher   performance on our instructions ( Table 11 ) . Last but not least , since we represent the instructions in a   structured format , we are able to ablate various elements of the instructions ( deﬁnition , negative / positive   examples , etc . ) and empirically quantify their contributions ( § 6).3485B Building Baselines for N   I   In this section , we provide several details on the   baselines included in our work .   B.1 Encoding of the instructions   According to our schema ( § 4.1 ) , each instruction I   for the t - th task is a set that contains the following   ﬁelds :   I   I ; I ; I ; I ; I ; I ; I (   To feed the instances to LMs , we ﬁrst encoder   them into plain text . Let encpI ; xqdeﬁne a function   that maps a given instruction Iand input instance   xto plain text . Evidently , there are many choices   for this function . In our study , we consider the   following encodings :   N- encoding . This encoding   is the conventional paradigm where no instructions   exist : encoding . In this encoding , we append   the prompt message before the input :   P + D encoding . In this en-   coding , the prompt message and the task deﬁnition   appear before the input :   Intuitively , this encoding is more informative and   more complex than “ prompt ” encoding . F I encoding . This encod-   ing contains all the instruction content :   where encpIqis an alternating encoding pos-   itive and negative examples . We include as many   examples as possible , before exceeding the input   limit .   P E encoding . This encod-   ing contains only positive examples of the subtask   ( no task description , etc ) .   Such example - only have been used in several re-   cent studies in the ﬁeld ( Zhao et al . , 2021).3486C Analysis on Baseline Results   C.1 Comparison to Raw Instructions   We seek to understand the value of breaking the   tasks into sub - tasks and mapping them into our pro-   posed schema ( § 4.2 ) . We compute performance   of raw instructions ( ﬁrst sub - task of four datasets ) ,   in the same vein as ( Efrat and Levy , 2020 ) ’s setup .   We compare this to our F I - encoding . The results in Table 13 in-   dicate that GPT3 leads to higher performance with   our encoding ( 2nd row ) compared to raw instruc-   tions ( ﬁrst row ) . Weak performance of LMs on raw   instructions aligns with ( Efrat and Levy , 2020 ) ’s   ﬁnding that “ language model performs poorly ” .   This might be partly due to the verbose language   of the raw instructions : the average length of the   raw instructions is 2:5ktokens , in comparison to   950tokens for our encoding . While repetition often   helps human understanding , concise instructions   seem to be more effective for computers.3487