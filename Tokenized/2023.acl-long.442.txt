  Yu Zhao , Hao Fei , Wei Ji , Jianguo Wei   Meishan Zhang , Min Zhang , Tat - Seng ChuaCollege of Intelligence and Computing , Tianjin University , ChinaSea - NExT Joint Lab , National University of Singapore , SingaporeHarbin Institute of Technology ( Shenzhen ) , China   { zhaoyucs,jianguowei}@tju.edu.cn , { haofei37,jiwei,dcscts}@nus.edu.sg   mason.zms@gmail.com , zhangmin2021@hit.edu.cn   Abstract   Visual spatial description ( VSD ) aims to gen-   erate texts that describe the spatial relations   of the given objects within images . Existing   VSD work merely models the 2D geometrical   vision features , thus inevitably falling prey to   the problem of skewed spatial understanding   of target objects . In this work , we investigate   the incorporation of 3D scene features for VSD .   With an external 3D scene extractor , we ob-   tain the 3D objects and scene features for input   images , based on which we construct a target   object - centered 3D spatial scene graph ( G3D-   SG ) , such that we model the spatial semantics   of target objects within the holistic 3D scenes .   Besides , we propose a scene subgraph select-   ing mechanism , sampling topologically - diverse   subgraphs from G3D - SG , where the diverse   local structure features are navigated to yield   spatially - diversified text generation . Experi-   mental results on two VSD datasets demon-   strate that our framework outperforms the base-   lines significantly , especially improving on the   cases with complex visual spatial relations .   Meanwhile , our method can produce more   spatially - diversified generation . Code is avail-   able at https://github.com/zhaoyucs/VSD .   1 Introduction   Visual spatial description is a newly emerged   vision - language task , which aims to generate a tex-   tual descriptive sentence of the spatial relationship   between two target visual objects in a given image   ( Zhao et al . , 2022 ) . VSD falls into the category of   image - to - text generation , while in particular focus-   ing on the visual spatial semantics understanding ,   which has great values on the real - world human-   computer interaction ( HCI ) applications ( Heuser   et al . , 2020 ) , e.g. , automatic navigation ( Pendão   and Moreira , 2021 ; Wang et al . , 2023b ) , personal   assistance ( Vanhooydonck et al . , 2010 ) , and un-   manned manipulation ( Castaman et al . , 2021 ; Wang   et al . , 2023a , 2022a).Figure 1 : Examples of the visual spatial description .   Zhao et al . ( 2022 ) pioneer the VSD task by   manually annotating the spatial descriptions to the   images based on the visual spatial classification   datasets ( Krishna et al . , 2017 ) . Also they solve   VSD as a general image - to - text ( I2 T ) task via   vision - language pre - trained models ( VL - PTMs ) ,   i.e. , inputting images and outputting texts . How-   ever , modeling VSD as a regular I2 T job with   open - ended VL - PTMs can be problematic . Unlike   the existing I2 T tasks , such as image captioning   ( Vinyals et al . , 2015 ) , verb - specific semantic roles   ( VSR ) guided captioning ( Chen et al . , 2021 ) and vi-   sual question answering ( VQA ) ( Antol et al . , 2015 )   that focus on the content semantics understanding ,   VSD emphasizes more on the spatial semantics rea-   soning , according to its definition . Thus , directly   adapting VSD with general - purpose VL - PTMs will   lead to inferior task performances . We note that   there are at least two observations that should be   taken into account for VSD enhancement .   From the image encoding aspect , it is critical to   model the holistic 3D scene semantics of the input   image . In Zhao et al . ( 2022 ) , their VL - PTM - based   methods model the visual geometrical features at   merely 2D flat space ( e.g. , superficial features ) . Yet   directly perceiving objects from the first - person per-   spective will inevitably result in skewed angle of   view and biased spatial understanding , and thus fail7960   to handle complex cases ( e.g. layout overlap , per-   spective illusion ) or generate incorrect descriptions .   For example as in Figure 1(a ) , with the 2D - level   visual understanding , the spatial relation between   two cars is wrongly decided . As a reference , we   human always first project the visual contents into   the 3D space and reckon the scene layout and ob-   ject attributes ( e.g. , depth , shapes , camera poses ) ,   and then narrate the spatial relations based on such   holistic 3D clues .   From the text decoding aspect , it is necessary   yet challenging to generate diverse sentences of the   object pair relation . In generic I2 T task , prior meth-   ods strengthen the text diversification by equipping   with beam search ( Vijayakumar et al . , 2018 ) or in-   tegrating external knowledge ( Yu et al . , 2022 ) . Dif-   ferent from the generic diversified generation , VSD   requires the diversification with respect to the spa-   tial descriptions , rather than the diverse linguistics .   We can again place the emphasis on the modeling   of holistic 3D scene features . For example , with a   precise understanding of the spatial relations , it is   both viable to generate ‘ A is on the left of B ’ or ‘ B   ison the right of A ’ . Also , as illustrated in Figure   1(b ) , by comprehensively modeling the surround-   ing relations of the neighbor objects connecting   to the target objects in the holistic scene , more   spatially - diverse texts can be yielded via different   path traversing .   In this paper , we propose enhancing VSD by   modeling the holistic 3D scene semantics . We build   an encoder - decoder VSD framework ( cf . Figure 2 ) ,   where the encoder learns the 3D spatial features ,   and the decoder generates spatially - diversified de - scriptions based on the spatial semantic features .   Specifically , at encoding side , we first employ   an off - the - shelf 3D scene extractor ( Nie et al . ,   2020 ) to produce 3D objects and the correspond-   ing scene features ( i.e. , layout , location , size and   visual features ) for the input monocular RGB im-   age , via which we build a tar getobject - centered 3D   spatial scene graph ( namely , G3D - SG ) . We then   present an object - centered graph convolutional net-   work ( OGCN ) to encode the G3D - SG . At de-   coding side , we devise a scene subgraph selecting   ( S ) mechanism to sample topologically - diverse   object - neighboring subgraphs from G3D - SG ,   which allows to generate descriptions focusing on   the near surroundings of target object . Based on   the sampled subgraphs , we then create prompt texts   to ground the focused objects and their prototype   directions . Finally , a backbone VL - PTM is used to   encode the prompt texts , input images as well as   3D scene features , then to produce VSD texts .   We experiment on two versions of VSD datasets   ( Zhao et al . , 2022 ) , where one is with simple an-   notations and one has more complex and human-   friendly descriptions . The results indicate that   our system outperforms the best baseline with   significant margins , where our method especially   improves on the complex cases , such as layout-   overlapped and irregularly - posed objects . We fur-   ther reveal how the 3D scene graph modeling as   well as the Smechanism facilitate the task , and   also quantify the influence of the external 3D scene   extractor . All in all , this work contributes by verify-   ing that modeling the 3D scene of 2D image helps   the understanding of visual spatial semantics.7961   2 Methodology   Problem Definition Given an image Iwith two   object proposals < O , O > inI , VSD generates a   sequence of words S={w , ... , w}that describes   the spatial relationship between OandO. The   input OandOcontain the object tags and their   2D location coordinates . Different from image   captioning , the generated sentences of VSD must   directly or indirectly express the spatial relation   between the target objects .   Overall Framework As shown in Figure 2 , our   framework ( namely 3D ) is built upon an   encoder - decoder paradigm , where the encoder is   responsible for the 3D scene feature modeling , and   the decoder generates spatially - diversified descrip-   tions based on the spatial semantic features learned   from encoder .   2.1 Encoder : 3D Scene Feature Modeling   We first extract 3D scene features via an external   extractor . Then we build a target object - centered   3D spatial scene graph ( G3D - SG ) , which is   encoded and propagated with an object - centered   GCN ( OGCN ) .   Extracting 3D Scene Features We adopt the 3D   scene extractor as in Nie et al . ( 2020 ) , which is a   joint layout estimator and 3D object detector . It   first processes the 2D object detection for the in-   put RGB image , based on which the 3D relative   coordinates ( location ) and pose parameters of all   the objects will be estimated . Formally , we set   up the world system located at the camera cen-   ter with its vertical axis perpendicular to the floor ,   and its forward axis toward the camera , such that   the camera pose R(β , γ)can be decided by the   pitch and roll angles ( β , γ ) . In the world system ,   an object Ocan be determined by a 3D center   loc∈R , spatial size size∈R , orientation an-   gleori∈[−π , π ) . Finally , we obtain the loc ,   size , oriand the region - of - interest ( RoI ) vis   ( with its representation r ) of each 3D object ,   which is summarized in Table 1 . We extend the 3D   scene generating details at Appendix A.1 .   Constructing Target Object - centered 3D Spa-   tial Scene Graph Based on the 3D objects and   the corresponding 3D scene features , we now con-   struct the G3D - SG . The graph is centered on the   two target objects , placing the focus on the spatial   relationships between the target objects and their   surrounding neighbor objects in the scene . Techni-   cally , we denote G3D - SG asG=(E , V ) , where   Vis the set of 3D nodes v(i.e . , 3D objects ) . Note   that as the input images are likely to contain some   noisy objects that are less - informative to the task ,   we remove those objects by comparing their con-   fidence f(i.e . , the logit from the object detector )   with a threshold p. Eis the set of edges e , con-   sisting three types :   •Target - pair edge , which connects two given   target objects .   •Target - surrounding edge , which connects   each target object to all their surrounding non-   target objects .   •Near - neighbor edge , which connects those   non - target objects in near neighbor that may   have implicit correlations between each other .   We build the edges by calculating their coor-   dinates ( loc ) , with those values larger than a   pre - defined threshold das valid edges .   The edge e=1 when there is an edge between v   andv . Figure 3 illustrates the edge constructions .   Encoding Graph with Object - centered GCN   While graph convolutional network ( GCN )   ( Marcheggiani and Titov , 2017 ) has been shown   effective for aggregating graph data , it may fail   to model the centralization of the target objects   ofG3D - SG structure ( as GCN treats all nodes   equally ) . Thus , we devise an object - centered GCN ,   which advances in modeling both the edge features   and the target objects . OGCN first creates initial7962   representations of node sand edge s.   where Embed ( ) is the looking - up operation , FFN ( )   is the non - linear feedforward network .   Then , OGCN updates the G3D - SG :   where sis the node representation of last layer ,   asOGCN has total Llayers . s = s⊕sis   the summary of the two target objects . [ ; ] is the   concatenation operation . W , W , bare learnable   parameters . The weight γreflects the contribu-   tion of each object when propagating the spatial   attributes towards target objects .   2.2 Decoder : Spatially - diversified Text   Generation   In decoding stage , we use a VL - PLM to generate   VSD texts , as shown in Figure 2 . We first per-   form scene subgraph selection over G3D - SG ,   where the diverse local structures lead to spatially-   diversified text generation . Also we create prompt   texts to hint model to generate relevant contents .   Scene Subgraph Selecting As cast earlier , we   can make use of the neighbor non - target objects   of the two target objects in the scene as type of   ‘ bridges ’ to diversify the generation . Thus , we pro-   pose a scene subgraph selecting ( namely , S ) mech-   anism to sample sub - structures of G3D - SG .   Concretely , Scontains three steps , as illustrated   in Figure 4 . First , we calculate the connecting-   strength score for each edges in G3D - SG via   a simple FFN transformation : a = FFN ( ˆs ) ,   where ˆsis the edge representation of final - layer   OGCN . In the second step , we take a first - order   traversal to search the best neighbor nodes of two   target objects , respectively , where the best neigh-   bor nodes have the highest connecting scores to   their target objects . Note that we only consider the   direct neighbor of target objects ( i.e. , first - order   connection ) , because including nodes in too distant   will rather lead to inaccurate descriptions . In the   third step , we assemble the two perspective struc-   tures into one , and then prune the conflicting edge   with lower connecting score if a cycle exists ( i.e.   two target objects connects to a common neighbor ) ,   resulting in a successful subgraph .   It is noteworthy that during training we sample   only one subgraph with highest - scored edge , where   reparameterization trick ( Blum et al . , 2015 ) is used   for gradient propagation . During inference , we   sample multiple scene subgraphs with top- khigh-   est score edges , i.e. , giving diverse descriptions .   With the subgraph at hand , we create its repre-   sentations via a mean - pooling operation over it :   λ = δ(v∈G ) + a+a / summationtext(a+a ) ,   r = MeanPool ( { λˆs , λˆs|G}),(3 )   where ais the strength score , tandtare the   target nodes , δ(exp)funtion outputs 1 when exp   is true , otherwise 0 , ˆs,ˆsare the node and edge   representations of last - layer OGCN . The local   scene graph feature rwill be used during text   generation at following stage .   Building Directional Prompts Now we try to   guide the VL - PLM to generate contents closely   relating to the target objects and the nodes in the   sampled subgraph . We thus build two types of   prompt texts , 1 ) target object prompt , e.g. ,   < TGT > table < TGT > sofa7963   and 2 ) spatial relation prompt , e.g. ,   < OBJ > table < REL > near < OBJ > sofa   < OBJ > sofa < REL > left < OBJ > bed   Two types of prompts are concatenated as one via a   ‘ < SEP > ’ token . The former enlightens model what   the target objects are , and the latter tells model   what possible relational triplets are , i.e. , “ < object ,   relation , object > ” , where “ relation ” is the prede-   fined relation term . For each pair of O , Oin   G3D - SG , we map their edge eto a specific   direction term based on their centroid coordinates .   We maintain a prototype of 3D universal   direction - term mapping , as shown in Figure 5 ,   where we define 26 directions in the whole sphere ,   with each direction binding certain directional   terms . Even a strong VL - PLM may fail to map a   direction to a term accurately . We thus additionally   perform pre - training to strengthen the perception   of direction . The detailed mapping rules of univer-   sal 3D direction - term are shown in Table 2 . With   the predefined 26 directions , we compare the cen - troid coordinates of a pair of objects to decide the   direction terms . Note that according to the rules   in Table 2 , there may be multiple terms for the   same direction , e.g. , “ left up front ” and “ up left   front ” and we keep these redundant terms in our   implementation . We add an extra term “ next to ”   to describe the situation that two objects are close   to each other . For some types of object that not   exists during the 3D Scene Extractor pretraining ,   we just use their 2D locations and treat the depth   coordinate to 0 .   Moreover , we conduct a pre - training to   strengthen the perception of direction for VL - PTM .   Concretely , we utilize the 3D scene extractor and   relation triplets ground - truth in VSD dataset to gen-   erate a set of pseudo data . For example , if we have   two target objects O , Owith theirs names Tag ,   Tag , ground - truth relation term Rel(in VSD an-   notations ) , and 2D boxes Box , Box , we could   get their 3D centroid coordinates loc , locthrough   off - the - shelf 3D scene extractor . Then we map the   3D centroid coordinates to 3D direction term Rel .   We use “ < OBJ > Tag < REL > Rel < OBJ > Tag ” as   inputs and “ Tag , Rel , Tag ” as outputs to train   the VL - PTM . Moreover , we randomly replace the   Relwith some synonyms for data augmentation .   Generating Text Finally , we feed the prompt   text and the raw image as input into our backbone   VL - PLM encoder , where the resulting represen-   tations randrand the local scene graph fea-   turerare fused together via cross - attention , i.e. ,   r = CrossAtt ( r , r , r ) . The VL - PLM decoder   then performs text generation based on r.   3 Experiments   3.1 Settings   Dataset We evaluate our model on two datasets :   VSD - v1 and VSD - v2 ( Zhao et al . , 2022 ) . VSD - v1   is the initial version of VSD datasets , which has   a large scale but simple annotations . VSD - v2 has   the same image source while more complex and   human - friendly descriptions , which is more chal-   lenging . We use the original split of train / dev / test   set of each dataset .   Implementation Our model takes the pre - trained   3D extractor from Nie et al . ( 2020 ) , containing   the layout estimation network and the 3D object   detection network . The hidden size of OGCN   is 768 which is the same with text decoder . The   dimension of edge feature sis also 768 . We   adopt the OFAas our backbone VL - PLM.7964   Evaluation We make comparisons with 1 ) the   existing popular image - to - text VL - PLMs , includ-   ing OSCAR ( Li et al . , 2020 ) , VLT5 / VLBart ( Cho   et al . , 2021 ) , OFA ( Wang et al . , 2022b ) ; 2 ) the mod-   els introduced in Zhao et al . ( 2022 ) , including the   pipeline ( ppl ) and the end - to - end ( e2e ) paradigms .   Zhao et al . ( 2022 ) use the visual spatial relations   classification ( VSRC ) results as intermediate fea-   tures for VSD . Following Zhao et al . ( 2022 ) , we   adopt five automatic metrics to evaluate perfor-   mances , including BLEU-4 , MENTEOR , ROUGE ,   CIDEr and SPICE . We measure the diversity with   three metrics , i.e. , mBLEU-4 , BLEU-4@K and   SPICE@K. All the used VL - PLMs are the base   version . Our results are the average scores over   five runs . Appendix § B.3 details all the experimen-   tal settings .   3.2 Main Observations   Main Results As shown in Table 3 , overall , the   VSD - v2 can be more challenging than VSD - v1 ,   where model scores on all the metrics are lower .   Also we see that four different VL - PTMs show the   similar level of performances , due to their general   purpose of pre - training for multimodal learning .   By taking advantages of the VSRC features , Zhao   et al . ( 2022 ) ’s methods outperform the baseline   vanilla VL - PTMs on the task . However , the im-   provements from Zhao et al . ( 2022 ) ’s models can   be incremental , due to the reason that Zhao et al .   ( 2022 ) model the input images with only 2D in-   formation . On the contrast , our proposed 3D   model achieves significant improvement over the   baselines cross two datasets on all the metrics , evi-   dently demonstrating its efficacy .   In addition , our model shows larger improve-   ments on the harder VSD - v2 data than that on   VSD - v1 . To directly measure the capability of our   method , we further collect a subset from VSD - v2 ,   where the target objects in images are irregularly-   posed with complex spatial relation , and also there   are overlapped layouts . We perform experiments   on the set , where the results are shown in Table 4 .   We can find that our 3D model improves the   best - performing baseline with marked boosts , i.e. ,   1.87 BLEU-4 , 1.94 METROR , 1.24 ROUGE and   5.11 CIDEr . This significantly indicates that our   method is capable of well understanding the visual   spatial semantics and thus generating more diverse   and flexible VSD sentences.7965   Model Ablation Now we quantify the contribu-   tion of each design in our systems via model abla-   tion , as shown in Table 5 . First , we can see that the   3D scene feature from G3D - SG graph model-   ing gives the biggest influences , i.e. , contributing   3.09 BLEU-4 and 3.08 SPICE scores . Besides , the   OGCN encoder , the Smechanism as well as the   direction - term prompting also plays an essential   role to the overall system , respectively .   Evaluation on Spatial - diversity Generation As   we equip our system with the Smechanism , we   can generate spatially - diversified texts . Next , we   directly assess the ability on the generation spatial-   diversification . We first make comparisons with   the beam search method using automatic metrics   ( Deshpande et al . , 2019 ) , including mBLEU-4 ,   BLEU-4@K and SPICE@K. mBLEU-4 compares   the 4 - gram matching between one of the generated   sentence and the remaining generated sentences for   an image , and thus lower mBLEU-4 score means   more diversity . BLEU-4@K and SPICE@K rep-   resent the highest BLEU-4 and SPICE score for   the top- kgenerated sentences for an image , where   higher BLEU-4@K and SPICE@K prove that the   a model can keep better semantics accuracy while   generating diverse results . As shown in Table 6 ,   our Smechanism achieves lower mBLEU-4 and   higher BLEU-4@K and SPICE@K , demonstrating   that our method could generate diversified descrip-   tions with enough semantics accuracy .   We also provide a human evaluation to describe   spatial diversity with respect to the Spatial Accu-   racy , Spatial Diversity andFluency . We ask 10 En-   glish speakers to answer the 5 - point Likert scale on   100 samples , where the average results are shown   in Table 7 . Overall , all the models can achieve   competitive score on language fluency , thanks to   the superiority of VL pretraining . Also the tenden-   cies of spatial accuracy and diversity is consistent   with the automatic evaluation . Remarkably , our   Smechanism shows the best capability on spatial   diversity , demonstrating its effectiveness .   3.3 Analyses and Discussions   To gain an in - depth understanding of our method ’s   strengths , we try to answer following research ques-   tions via further analyses :   •RQ1 : How 3D scene features help understanding   the spatial semantics of input images ?   A : The key of our method is the leverage of 3D   scene features . Now , we first consider downgrad-   ing the 3D features into the 2D ones , such that we   can gain the perception of its necessity . To ensure   the fair comparison , we remove the 3D scene ex-   tractor and replace the 3D pose feature pose(Eq .   1 ) with the 2D size . Also we replace the 3D coor-   dinates by 2D coordinates . And the other settings   are kept the same . As shown the results in Figure   6 , the 2D scene modeling results in the markedly   performance decrease .   We can further quantify the contributions of each   type of 3D features via feature ablation , including   the orientation feature oriand the size feature   size . As seen in Figure 7 , both 3D orientation and7966   3D size features contribute to the overall system .   Also the influence on SPICE is larger , which indi-   cates that the orientation and size of the objects es-   pecially help recognize the spatial relation . Finally ,   in Figure 8 we empirically show the OGCN ’s ker-   nel weight γ(Eq . 2 ) on two pieces of instances ,   where the attention values reflect the contribution   of each object . It is clear that our model has suc-   cessfully captured the spatial semantics of the tar-   get object pairs .   •RQ2 : How does Smechanism aid the diversified   spatial description generation ?   A : Next , we consider investigating how exactly   the Smechanism contribute to the spatial descrip-   tion generation . Through our Smechanism , we   could generate 4 types of subgraphs : 1 ) with only   target nodes ( 2 - hop ) ; 2 ) with one non - target neigh-   bor node linked with subject node ( 3 - hop - s ) ; 3 )   with one non - target neighbor node linked with ob-   ject node ( 3 - hop - o ) ; 4 ) with two non - target neigh-   bor nodes . In our implementation , we use a thresh-   oldpto filter out edges with very low scores .   Figure 10 show the distribution of four subgraphs   withp=0.1 or 0.2 . We see that different pval-   ues help generates spatial descriptions with vary-   ing numbers of objects attended into the subgraphs .   That is , Smechanism aids the diversified spatial   description generation by producing multiple het-   erogeneous subgraphs structures .   We also empirically show the qualitative results   in Figure 8 . We notice that the beam search method   can generate multiple alternative texts in both cases ,   where unfortunately the diversification on describ-   ing the spatial relation is much inferior and limited .   In contrast , with our Smethod , the system gen-   erates spatial - diversified descriptions for both two   images . Some surrounding objects , e.g. , ‘ shelf ’ ,   ‘ table ’ , ‘ door ’ in the first case and ‘ chair ’ , ‘ bed ’ ,   ‘ shelf ’ and ‘ desk ’ in the second case , are leveraged   to aid describe the target objects .   •RQ3 : To what extent the external 3D scene ex-   tractor quality influence the VSD performances ?   A : As we obtain the initial 3D scene features   from the external extractor , the quality of the ex-   tractor is key to our final VSD performance . Note   that the off - the - shelf 3D extractor is trained on a   datasets of indoor scenes , while in VSD dataset the   images contain both types of indoor ( NYU ) and out-7967door ( Visual Genome ( VG ) and Flickr ) . As shown   in Figure 9 , the indoor images actually are the mi-   nority in our VSD data . Here we perform analysis   to valid the influence on 3D scene extractor . We   split the VSD - v2 test set into indoor and outdoor   subsets according to the domain types . Then we   run our model on the two sets separately . As shown   in the figure , the results on the indoor NYU set   exceed those on the outdoor VG&Flickr set clearly ,   demonstrating the domain shift issue in our system .   Given that our system has already secured consid-   erable performance increase than existing models ,   we presume that when obtaining a 3D extractor ca-   pable of detecting higher - quality 3D scene features   for any domain and scenario , our method has the   greater potential to gain more task improvements .   4 Related Work   Image - to - text ( I2 T ) is a fundamental task category   of the vision - language multimodal topic . Exist-   ing I2 T tasks , e.g. , image captioning ( Vinyals et al . ,   2015 ; Cornia et al . , 2019 ; Mathews et al . , 2018 ) and   VQA ( Antol et al . , 2015 ; Lubna et al . , 2019 ; Man-   madhan and Kovoor , 2020 ) , attempt to generate   textual pieces to understand the image content se-   mantics through different perspectives . VSD is also   a subtask of I2 T , which however places the focus on   the spatial semantics understanding . Within recent   years , VL - PTMs are extensively employed for the   I2 T tasks , which have helped achieve state - of - the-   art performances on many benchmarks ( Lu et al . ,   2019 ; Chen et al . , 2019 ; Zhou et al . , 2020 ; Li et al . ,   2020 ; Wang et al . , 2022b ) . Prior VSD study ( Zhao   et al . , 2022 ) has benchmarked the VSD task with   these general - purpose VL - PTMs . Unfortunately ,   different from the content - semantic I2 T tasks , the   goal of VSD is to grasp the spatial semantics .   This work also relates to the theme of visual   spatial understanding , which has been intensively   investigated in the past years . Yang et al . ( 2019 ) ;   Jänner et al . ( 2018 ) propose the visual spatial rela-   tion classification ( VSRC ) task , aiming to predict   whether a spatial description is reasonable for the   input image . Differently , VSD aims to directly gen-   erate the descriptions of the spatial relations . The   bottleneck of visual spatial understanding tasks is   the capturing of spatial semantics . Thus , in this   paper , we propose improving the visual spatial   understanding for VSD via modeling the holistic   3D scene features . Our work takes the advance-   ments from the topic of 3D scene parsing ( Huanget al . , 2018a , b ; Nie et al . , 2020 ; Zhang et al . , 2021 ) ,   which reconstructs the 3D scene from a single RGB   image . By using the 3D scene features , e.g. , 3D   layout and 3D object , we achieve the goal of more   in - depth understanding of the spatial semantics .   5 Conclusion   In this work we incorporate the 3D scene features   for improving the visual spatial description ( VSD )   task . We first employ an off - the - shelf 3D scene   extractor to produce 3D objects and scene fea-   tures for the input images . We then build a target   object - centered 3D spatial scene graph ( G3D-   SG ) structure , which is encoded with an object-   centered graph convolutional network . Next , we de-   vise a scene subgraph selecting mechanism to sam-   ple topologically - diverse subgraphs from G3D-   SG , where the local features are used to guide to   generate spatially - diversified descriptions . On the   VSD datasets our framework shows superiority on   the task , especially for solving the complex cases ,   such as layout - overlapped and irregularly - posed   object . Meanwhile our method can produce more   spatially - diversified generation . Finally , we demon-   strate the influence of quality of the external 3D   scene extractor .   Acknowledgments   This research is supported by the National Natu-   ral Science Foundation of China under Grant No .   62176180 . The work is also supported by the Sea-   NExT Joint Lab .   Limitations   This work has one major risk . As the main idea   proposed in this work heavily relies on the exter-   nal 3D scene extractor , the quality of extractor on   our used VSD images largely influences the task   performance . However , we reveal in analysis that   although suffering from the domain shift issue by   the out - of - domain 3D scene extractor , our method   still improves the VSD task . We show that when   handling the in - domain VSD images as used for   training the 3D scene extractor , the VSD perfor-   mance has been boosted remarkedly . Thus , with   better a 3D scene extractor , it can be expected that   our system will exhibit much stronger capability   and advance the VSD task more significantly.7968References79697970A Model Specification   Here we provide the detailed calculations of our   system . For the MeanPool operation in Equation 3 ,   we calculate ras :   r=1   M / summationdisplay / summationdisplayλ / parenleftbigˆs⊕ˆs / parenrightbig   ( 4 )   where Mis the node number of the G3D - SG ,   λis a weight expansion ( shown in Equation 3 ) ,   which could capture the subgraph features with a   very high weight , ˆsandˆsare the node and edge   representations of last - layer OGCN .   For VL - PTM encoding , we adopt the approach   in OFA model ( Wang et al . , 2022b ) . The input   image Iis first embedded by a inbuilt ResNet and   flatted to a tokenized visual feature . Then the vi-   sual features are projected to the encoder model   dimension , obtaining the global visual feature r.   The process could be formalized as :   ˆr = ResNet ( I ) ,   r = Wˆr+b,(5 )   where Wandbare model parameters . Then the   tokenized visual features are concatenated with em-   bedded text sequence as the inputs of VL - PTM   encoder :   h = Encoder ( r⊕r ) . ( 6 )   After that , the encoder outputs and graph feature   rare fused through cross - attention in VL - PTM   decoder . The cross - attention operation in a trans-   former unit could be formalized as :   X=/braceleftbig   r;h / bracerightbig   ,   K = WX , V = WX , Q = WD ,   Attention ( Q , K , V ) = softmax / parenleftbiggQK√   d / parenrightbigg   V ,   ( 7 )   where ris the graph feature , his the encoder   outputs , W , W , Ware model parameters , dis   the out dimension of W , Dis the outputs of   self - attention unit in Transformer decoder .   A.1 3D Scene Extracting   We employ an external 3D scene extractor from   ( Nie et al . , 2020 ) , which is pretrained on the SUN   RGB - D dataset ( Song et al . , 2015 ) . The model con-   tains three modules : 1 . Layout Estimation Network   ( LEN ) ; 2 . 3D Object Detection Network ( ODN ) ; 3 .   Mesh Generation Network ( MGN ) . In our frame-   work , we only use LEN and ODN , which outputs   the layout coordinates system and the 3D location   and pose of each object . We also need a 2D Ob-   ject Detector to generate appearance feature and   2D box as the inputs for ODN . We directly borrow   the Faster - RCNN ( Ren et al . , 2015 ) to process the   images to obtain the necessary RoI features , boxes ,   and object tags . For the LEN , we need the camera   intrinsic parameters to adjust the coordinates sys-   tem . However , the VSD dataset does not provide   this information of the images . We just generate a   pseudo camera intrinsic matrix for all the images   in our implementation . The method works in VSD   because we do not need the accurate location of   each object , we just need to capture the relative re-   lations among the object pairs . Thus , the distortion   of absolute coordinates will hardly impact the final   results of VSD . As shown in Figure 12 , though   the 3D boxes are distorted , their relative spatial   relations remain unchanged .   A.2 Spatial Scene Graph Creating   The detailed algorithm of Sis shown in Algorithm   1 . Suppose that the max object number is N(N=36   in our implementation ) . We first initialize an N×   Nadjacency matrix A=0 . When adding the   Target - pair and Target - surrounding edges , we set   the columns and rows of the target object index   to 1 . When adding the Near - neighbor edge , we   traverse the edges and set the one with dist > din   Ato 1 . At last , we should remove the edges of   noisy objects , setting the related items of Ato 0 .   A.3 Overall Training Procedure   We have several pretrained external modules in our   system . First , we prepare the pretrained ResNet for   image embedding , which will be used in 3D layout   estimation , 3D object detection and the OFA image   embedding . Before training , we use the pretrained7971   Algorithm 1 : G3D - SG Creating   Input : max object number N ,   two target objects index o , o ,   confidency of each object f ,   centroid of each object C ,   distance threshold d ,   noise confidency threshold p   Output : adjacency matrix A   initialization : A=0 .   // target object edges   A[o , :] = 1 , A[:,o ] = 1 ,   A[o , :] = 1 , A[:,o ] = 1 ,   // add special edges   fori inNdo   forj inNdo   dist = ||C−C||   ifdist > dthen   A= 1   end   end   end   // remove noise objects   fori inNdo   iff < pandois not target object then   A[i , :] = 0 , A[:,i ] = 0   end   end   Faster - RCNN to preprocess all the images for 2D   object detection , obtaining the 2D boxes and RoI   features . We also prepare the 3D scene extractor   pretrained on SUNRGB - D with the method in ( Nie   et al . , 2020 ) .   Then we collect the three modules and train our   whole system , where the 3D extractor and the de-   coder ( VL - PTM ) are initialized by pretrained pa-   rameters . The OGCN and other parameters ( e.g.   edge embedding , connecting - strength scorer ) are   initialized randomly . Before global training , we   pretrain the VL - PTM with direction terms as Table   2 . During global training , the 3D scene extractor   are frozen while others will be updated .   We have two main training targets , e.g. , the cross-   entropy between Vl - PTM decoder outputs and the   ground - truth description of VSD L , and the cross-   entropy between connecting - strength score distri-   bution and target object index L :   L=−/summationdisplaylogp(y|x , y ) ,   L=−/summationdisplayδ(i∈O)(log a+loga),(8 )   where Nis the text length , xis the VL - PTM   encoder outputs and yis decoder outputs in the   l - th step , Nis the node number of G3D - SG ,   ais connecting - strength score during S , t , t   are the two target objects , Ois a set that contains   the objects appear in the ground - truth description ,   andδ(·)is the two value function that outputs 1   when·is true , otherwise 0 . We obtain Oby string   searching in the descriptions . Then the final loss is   L = L+L.   B Experiment Specification   B.1 Data Analyses   The VSD dataset contains images from Visual   Genome , Flickr , and NYU - Depth ( Zhao et al . ,7972   2022 ; Yang et al . , 2019 ) , where NYU - Depth is   the indoor dataset ( Silberman et al . , 2012 ) .   Figure 13 shows the distribution of object tags   in the VSD dataset . Table 8 lists the object types in   SUNRGB - D dataset which are used to pretrain 3D   scene extractor . There are several common types ,   such as “ person ” , “ table ” , “ chair ” , “ wall ” and “ win-   dow ” . Thus the 3D scene extractor could achieve a   comparable performance in VSD . For other object   types , the built - in 2D object detector ( pretrained   on VG or COCO ) could provide a passable 2D   location information .   There are two versions of VSD dataset released ,   where the VSD - v1 contains large - scale while rela-   tive simple descriptions and the VSD - v2 contains   small - scale while diversified descriptions . The two   versions share the same image set . The statistics of   the two datasets are shown in Table 9 . Figure 14   shows the difference between the annotations of the   two datasets . The annotated sentence in VSD - v1   is relative short and simple while those in VSD - v2   contains more semantics and be more challenging .   B.2 Extended Experimental Implementations   Data Preprocess For memory space and time   saving , we preprocess the data through Faster-   RCNN to obtain the RoI featues , object tags and   2D boexs and save them to files . For 3D scene   extractor , we also preprocess the images , saving   their ResNet features to files .   VL - PTM Configuration For fairly comparison ,   we employ the OFA - base as our VL - PTM , which   has the similar scale of parameters to the baselines .   The layers number is 6 for both encoder and de-   coder , and the head number of the multi - head atten-   tion is 6 as well . Besides OFA , we can use any type   of VL - PTMs such as VLT5 / VLBart , or combina-   tion of single VL encoder and text decoder , such as   “ CLIP - GPT ” , “ ViT - GPT ” . For more comparison ,   we leave them in future works .   B.3 Hyperparameters   Table 11 lists the hyperparameters of our imple-   mentation .   B.4 Baselines   We use some strong image - to - text models as our   baselines .   •Oscar ( Li et al . , 2020 ) is a BERT - like visual-   language pretrained model , only containing   the Transformer encoder . Oscar takes tok-   enized RoI features of the image as visual   inputs . This model has shown strong perfor-   mance on Image Captioning and VQA.7973   •VLT5 / VLBart ( Cho et al . , 2021 ) are continue   pretrained on image - to - text tasks from gener-   ative encoder - decoder language model Bart   and T5 . Previous work ( Zhao et al . , 2022 ) has   proven their capability on solving VSD task .   •OFA ( Wang et al . , 2022b ) is a new vision-   language model , which are pretrained by a   mount of vision - language multi - modal tasks .   It has achieved several SoTA results on exist-   ing image - to - text tasks .   C Extended Experiments   C.1 Diversity Evaluation   We compare more results of sampling scale for   diversity evaluations . Figure 10 reports the results   of K=5 and K=10 on VSD - v2 . The tendency of   K=10 is consistent with that of K=5 , where the Sachieves lower mBLEU-4 and higher BLEU-4@K   and SPICE@K.   For human evaluation , our 5 - point Likert scale   is designed as follows :   •Spatial Accuracy : The sentences correctly   describe the spatial relationship of the target   objects .   •Spatial Diversity : These sentences describe   diversified spatial semantics .   •Fluency : The sentences are readable and not   different from human sentence - making .   Each question will be answered by a number   from 1 to 5 , denoting “ Strongly Disagree ” , “ Dis-   agree ” , “ Neither disagree nor agraa ” , “ Agree ” and   “ Strongly agree ” . We select 100 samples and gen-   erated 5 sentences for each , and sent them to the   evaluators for scoring .   C.2 More Case Study   Figure 15 provides some more qualitative compar-   isons among models and human . Compared with   baselines , our model could generate description   more like human - made.79747975ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   section 6   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . We use public datasets   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   section 4 and appendix   /squareB1 . Did you cite the creators of artifacts you used ?   section 4 and appendix   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   section 4 and appendix   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   section 4 and appendix   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . We use public datasets without people or offensive content .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   appendix   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   appendix   C / squareDid you run computational experiments ?   section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   section 4 and appendix7976 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   section 4 and appendix   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   section 4 and appendix   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   section 4 and appendix   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.7977