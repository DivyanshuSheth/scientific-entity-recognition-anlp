  Xuancheng HuangZijun LiuPeng LiTao LiMaosong SunYang LiuMeituan , ChinaDept . of Comp . Sci . & Tech . , Institute for AI , Tsinghua University , Beijing , ChinaInstitute for AI Industry Research ( AIR ) , Tsinghua University , Beijing , ChinaBeijing National Research Center for Information Science and TechnologyShanghai Artificial Intelligence Laboratory , Shanghai , China   Abstract   Recently , multi - aspect controllable text genera-   tion that controls the generated text in multiple   aspects ( e.g. , sentiment , topic , and keywords )   has attracted increasing attention . Although   methods based on parameter efficient tuning   like prefix - tuning could achieve multi - aspect   controlling in a plug - and - play way , the mutual   interference of multiple prefixes leads to sig-   nificant degeneration of constraints and limits   their extensibility to training - time unseen as-   pect combinations . In this work , we provide a   theoretical lower bound for the interference and   empirically found that the interference grows   with the number of layers where prefixes are   inserted . Based on these analyses , we propose   using trainable gates to normalize the interven-   tion of prefixes to restrain the growing inter-   ference . As a result , controlling training - time   unseen combinations of aspects can be realized   by simply concatenating corresponding plugins   such that new constraints can be extended at a   lower cost . In addition , we propose a unified   way to process both categorical and free - form   constraints . Experiments on text generation and   machine translation demonstrate the superior-   ity of our approach over baselines on constraint   accuracy , text quality , and extensibility .   1 Introduction   Multi - aspect controllable text generation ( MCTG ) ,   which aims at generating fluent text while satis-   fying multiple aspects of constraints simultane-   ously , has attracted increasing attention in recent   years ( Chan et al . , 2021 ; Qian et al . , 2022 ; Gu et al . ,   2022 ) . To effectively control diverse aspects such   as sentiment , topic , and detoxification , extensive ef-   forts have been devoted to the task , including meth-   ods based on conditional generative model ( Keskaret al . , 2019 ) , decoding - time regulation ( Lin and   Riedl , 2021 ; Kumar et al . , 2021 ) , and parameter   efficient tuning ( Qian et al . , 2022 ; Gu et al . , 2022 ) .   Despite their effectiveness , existing methods still   suffer from low extensibility . Ideally , suppose a   multi - aspect controllable text generation system   has learned how to control sentiment , topic and   keywords separately , it should be extensible to any   combinations of the three aspects , e.g. , generating   asports - themed sentence with negative sentiment   containing keywords “ New York ” ( see Figure 1 ) .   Moreover , an extensible system should also be eas-   ily extended to control new aspects in a plug - and-   play way . However , it is non - trivial for existing   methods to achieve this goal . Specifically , the dedi-   cated conditional generative models ( Keskar et al . ,   2019 ) mostly need to be trained from scratch or   finetuned when facing new aspect combinations .   The decoding - time regulation based methods ( Lin   and Riedl , 2021 ; Kumar et al . , 2021 ) intervene   in the probabilities of sentences by light - weight   attribute classifiers or language models during in-   ference , which significantly impairs text fluency   when multiple distributions are interpolated . The   parameter efficient tuning based methods ( Qian   et al . , 2022 ; Gu et al . , 2022 ) control aspects by in-   serting trainable prompts or prefixes into the model ,   referred to as plugins . By leveraging one plugin   for each aspect , these methods can naturally work   in a plug - and - play way , showing better potential to   achieve high extensibility .   However , existing studies show that directly   combining multiple plugins results in significantly   lower controllability of the corresponding aspects   than before combining ( i.e. , attribute degenera-   tion ) ( Qian et al . , 2022 ; Gu et al . , 2022 ) . Gu et al .   ( 2022 ) argue that mutual interference of the plug-   ins is the major reason for attribute degeneration ,   which is further justified by our theoretical and   empirical analyses . Previous works alleviate the   problem by introducing connectors to connect mul-15233   tiple plugins ( Yang et al . , 2022 ) , latent variables   to represent the unsupervised aspects ( Qian et al . ,   2022 ) , or objectives to narrow the discrepancy of   aspects ( Gu et al . , 2022 ) . However , these methods   require joint training of plugins and are designed   for pre - defined closed - set constraints . In conse-   quence , their extensibility is limited .   In this paper , we propose an extensible plug - and-   play method , P G , for multi - aspect   controllable text generation . We derive a theoreti-   cal lower bound for the interference of plugins and   reveal that it accumulates with the increasing num-   ber of layers where prefixes are inserted . Based on   these findings , we propose attaching trainable gates   to the plugins , which normalize the interventions   of plugins . As a result , the mutual interference has   been significantly reduced such that the control of   arbitrary combinations of aspects can be realized   by simply concatenating the corresponding plugins .   Thus , our method is both extensible and plug - and-   play . Moreover , we represent the constraints of the   aspects in textual form , which makes our method   applicable not only to categorical aspects ( e.g. , sen-   timent ) but also to free - form aspects ( e.g. , lexical   constraint ) .   Our contributions are three - fold :   •We propose an extensible plug - and - play   method , P G , for multi - aspect   controllable text generation , which is able to   control training - time unseen aspect combina-   tions by simply concatenating plugins .   •We provide a theoretical lower bound along   with empirical analyses for the mutual interfer-   ence problem , which we believe will facilitate   future research.•Experiments show that our approach has lower   mutual interference , leading to superiority   over strong baselines on text quality , con-   straint accuracy , and extensibility .   2 Background   In this section , we illustrate the widely - used prefix-   tuning - based method ( Qian et al . , 2022 ; Gu et al . ,   2022 ; Yang et al . , 2022 ) for multi - aspect control-   lable text generation . Generally , prefix - tuning ( Li   and Liang , 2021 ) prepends light - weight continuous   vectors to the multi - head attention sublayer of each   Transformer layer ( Vaswani et al . , 2017 ):   H= Att / parenleftig   Q,/bracketleftbig   P;K / bracketrightbig   , /bracketleftbig   P;V / bracketrightbig / parenrightig   , ( 1 )   where Att(·)is the attention function , Qare   queries of inputs , KandVare activations of inputs ,   PandPare trainable prefixes , [ · ; · ] denotes the   concatenation operation , His the output of the at-   tention sublayer . We use ϕto denote the set of   prefixes in all Transformer layers .   Specifically , for multi - aspect controllable text   generation , we assume that there are Naspects   of constraints . Due to the lack of multi - aspect la-   beled data , each set of prefixes , which usually rep-   resents a specific constraint ( e.g. , “ positive ” for the   sentiment aspect ) , is trained on the corresponding   single - aspect labeled data :   ˆϕ= argmax / braceleftig   P(y|x;θ,ϕ)/bracerightig   , 1≤i≤N,(2 )   where θare the fixed parameters of the pretrained   model , yis the controlled target sentence , xis the15234input sentence , P(y|x;θ,ϕ)is the conditional   probability of y , and ˆϕare the learned parameters   of prefixes for the i - th aspect .   During inference , for a combination of multiple   aspects , corresponding prefixes are selected and   synthesized by either concatenating ( Qian et al . ,   2022 ; Yang et al . , 2022 ) or finding their intersec-   tion ( Gu et al . , 2022 ) , and then the generation is   conditioned on the synthesis . Without loss of gen-   erality , we take two aspects as an example . The   conditioned probability can be represented as   P / parenleftig   ˆy|x;θ , syn(ˆϕ,ˆϕ)/parenrightig   , ( 3 )   where syn(·)is a synthesize function , ˆyis the can-   didate sentence , ˆϕandˆϕare two sets of prefixes   corresponding to two aspects ( e.g. , “ positive ” for   sentiment and “ sports ” for topic ) , respectively .   Although existing methods alleviate the mutual   interference of prefixes by joint training ( Qian et al . ,   2022 ; Gu et al . , 2022 ; Yang et al . , 2022 ) , they are   based on pre - defined closed - set constraints , which   increases the overhead of extending a new con-   straint and thus limits extensibility . Thus , to main-   tain high extensibility , reducing mutual interfer-   ence without joint training still remains a challenge .   3 Analyses on Mutual Interference   To alleviate mutual interference while maintaining   extensibility , we conduct theoretical and empirical   analyses . First , we provide a definition of mutual   interference as follows .   Definition . Mutual interference ( MI ) is the inter-   ference between multiple plugins which are trained   separately during training but are combined to   guide the pretrained model simultaneously during   inference ( i.e. , in the zero - shot setting ) . However ,   the exact interference is hard to analyze because of   the complexity of deep neural networks . Intuitively ,   suppose multiple plugins are optimized simultane-   ously during training , which requires multi - aspect   labeled data , their interference will be minimized   because they have learned to work cooperatively   under supervision ( i.e. , in the supervised setting ) .   Therefore , we use the differences between the hid-   den states of the supervised and zero - shot settings   to approximate the mutual interference of multi-   ple plugins . Specifically , let ˆϕand˜ϕbe the pa-   rameters of plugins learned from the single- and   multi - aspect labeled data , respectively . Taking two-   aspect controlling as an example , the output of a   Transformer layer is given by H(x,ϕ,ϕ ) , where   xis the layer input , then mutual interference can   be defined as   MI≈/vextenddouble / vextenddouble / vextenddoubleH(x,ˆϕ,ˆϕ)−H(x,˜ϕ,˜ϕ)/vextenddouble / vextenddouble / vextenddouble.(4 )   Empirical Analysis . Then , as mutual interfer-   ence has been defined as the norm of gap between   hidden states in the zero - shot and supervised set-   tings , we can empirically estimate it on the authen-   tic dataset . By calculating the averaged norm on   the Yelp dataset ( Lample et al . , 2019 ) , we plot the   variations of mutual interference with the number   of Transformer layers for Prompt - tuning ( Lester   et al . , 2021 ) and Prefix - tuning ( Li and Liang , 2021 )   in Figure 2 . We can find that the interference ac-   cumulates with insertions of trainable parameters .   Moreover , the magnitude of mutual interference at   the last Transformer layer ( i.e. , the 12 - th layer in   Figure 2 ) is consistent with the performance gap ,   which is the difference between the fulfillment of   constraints in single- and multi - aspect settings ( see   Table 1 ) . Meanwhile , too few trainable parameters   can not guide the pretrained model effectively . In   summary , the key point for remaining effective in   the zero - shot setting is restraining the growth of   mutual interference ( for a lower performance gap)15235while providing sufficient trainable parameters ( for   better supervised performance ) .   Theoretical Analysis . Next , to find a way to alle-   viate mutual interference , we conducted a theoreti-   cal analysis . As a result , we found that the mutual   interference , which is caused by the interactions in   attention sublayers , has a theoretical lower bound :   MI > α∥∆h(x,ˆϕ)∥+β∥∆h(x,ˆϕ)∥,(5 )   where 0 < α , β < 1 , and∥∆h(x,ˆϕ)∥is a norm   that is positively related to the magnitude of ˆϕ.   Moreover , the lower bound might accumulate with   Transformer layers like in Figure 2 . Intuitively ,   applying normalization ( e.g. , gates ) to the parame-   ters of the i - th plugin to reduce its magnitude will   decrease the lower bound of mutual interference .   4 P G   We propose a novel approach that attaches trainable   gates to the plugins , which alleviates the mutual in-   terference of multiple plugins and makes the model   highly extensible . Figure 3 shows the architecture   of our approach . We first provide intuition in § 4.1 ,   then define our approach formally in § 4.2 .   4.1 Intuition   Although prefix - tuning provides sufficient interven-   tions and avoids long - range dependencies by insert-   ing continuous vectors into each attention sublayer ,   it suffers from the accumulation of mutual interfer-   ence of these plugins ( see § 3 ) . On the one hand ,   the vectors are inserted into the attention sublayer ,   where they interact with each other , which directly   enhances mutual interference . On the other hand ,   the vectors are not normalized , which leads to a   large lower bound of mutual interference ( Eq . ( 5 ) ) .   Intuitively , injecting the vectors in a position - wise   manner will avoid direct interaction between them .   Moreover , normalizing the vectors can limit the   magnitude of the lower bound , which might de-   crease mutual interference . Therefore , we first pro-   pose attaching vectors outside the attention sub-   layer , which can be realized by appending trainable   vectors to the output of the embedding layer and   adding trainable vectors to the hidden states in each   Transformer layer ( see Figure 3 ) . Then , trainable   gates are applied to these hidden states to alleviate   mutual interference further . In this way , we expect   our approach to restrain the growth of mutual inter-   ference while providing sufficient interventions .   4.2 Method   Prompting . We present our model in the order of   forward propagation . To change how the trainable   parameters are injected into the model , we first fol-   low prompt - tuning ( Lester et al . , 2021 ) to append   trainable prompts to the output of the embedding   layer . Moreover , to make our model applicable not   only to categorical aspects ( e.g. , sentiment ) but also   to free - form aspects ( e.g. , lexical constraint ) , we   present the constraints of aspects in textual form   and feed them to the model . When two aspects   of constraints are required during inference , the   model input is given by   H=/bracketleftig   E(x ) ; E(c ) ; E(c);P;P / bracketrightig   , ( 6 )   where E(·)is the embedding function , and xis the   source sentence for sequence - to - sequence genera-   tion like machine translation and can be eliminated15236for text generation . candcare textual form   of constraints ( e.g. , “ This is a positive review ” for   positive review generation , and “ New York ” for lex-   ical constraint ) . P , P∈Rare continuous   prompts , where the right superscriptrepresents   thej - th layer , pis the number of continuous vec-   tors , and dis the dimension of hidden states . To   avoid the discrepancy between training and infer-   ence in position , each textual sequence has its own   position indexes starting from 1 and its own seg-   ment embedding ( Devlin et al . , 2019 ) . Note that   only one textual constraint and one set of trainable   parameters are injected during training .   Gating . Then , the model input His fed to the   encoder , where trainable gates are attached to the   hidden states in a position - wise manner , which al-   leviates mutual interference as well as steers the   model . Formally , A= Self -Att(H)is the   output of the j - th attention sublayer , and it is nor-   malized by the gates :   ˜A=/bracketleftig   A;σ / parenleftbig   G / parenrightbig   ⊙/parenleftbig   A+P / parenrightbig   ;   σ / parenleftbig   G / parenrightbig   ⊙/parenleftbig   A+P / parenrightbig / bracketrightig   , ( 7 )   where A∈RandA∈R   are hidden states split from A , P∈R   are trainable vectors that add to the hidden states ,   σis the sigmoid ( · ) function and G∈R   are trainable vectors . ⊙denotes the Hadamard   product and the normalized vectors σ(G)serve   asgates to selectively rescale the output of the   attention sublayer in a position - wise manner and   ˜A∈Ris the result of the   normalization . Next , the normalized output is fed   to the feed - forward sublayer : H= FFN ( ˜A ) .   Finally , the output of the last encoder layer is fed   to a standard Transformer decoder to guide the   generation .   Training & Inference . As shown in Figure 1 ,   during training , each plugin ( including prompts   and gates ) for a single aspect of constraints is at-   tached to the pretrained generative model and opti-   mized by corresponding single - aspect labeled data   separately ( refer to Eq . ( 2 ) ) . In contrast , during   inference , the control of arbitrary combinations of   aspects can be realized by simply concatenating   the corresponding plugins ( refer to Eq . ( 3 ) ) .   Moreover , our approach treats the training and   inference processes for pre - existing and newly - introduced constraints identically . The total train-   ing cost of Npre - existing aspects and Mnewly-   added aspects is O((N+M)C ) , where Cdenotes   the cost of training on a single aspect . In this way ,   the cost of introducing new constraints is relatively   low .   5 Experiments   We conducted experiments on two representative   tasks in natural language generation , which are text   generation and machine translation .   5.1 Multi - Aspect Controllable Text   Generation   Dataset . Following previous work ( Yang   et al . , 2022 ) , we adopted the widely - used Yelp   dataset ( Lample et al . , 2019 ) , which contains   restaurant reviews with the sentiment ( positive   and negative ) and the topic ( American , Mexican ,   and Asian ) labels . To evaluate the extensibility   of methods , we added two additional aspects   of constraints : keywords ( He , 2021 ) and tense   ( past and present ) ( Ficler and Goldberg , 2017 ) ,   where their labels are automatically extracted   from the reviews . Due to the page limit , please   refer to Appendix B for more details about the   experimental setup .   Evaluation . Following previous work , we   adopted automatic and human evaluations for con-   straint accuracy and text quality ( Lyu et al . , 2021 ;   Dathathri et al . , 2019 ; Gu et al . , 2022 ) . Specifi-   cally , we finetuned two RoBERTa - based ( Liu et al . ,   2019 ) classifiers for the evaluations of sentiment   and topic . The tense accuracy was evaluated by   the same tool adopted in the training set , and we   used word - level Copy Success Rate ( CSR ) ( Chen   et al . , 2020 ) to evaluate the lexical constraint . In   addition , we used the perplexity ( PPL ) given by   GPT-2 ( Radford et al . , 2019 ) and averaged   distinctness ( Li et al . , 2016 ) to evaluate the fluency   and diversity of the generated text , respectively .   For human evaluation , each sentence received a   score of 1 to 5 on sentiment and topic relevance as   well as fluency given by three evaluators . The final   scores are averaged over three ratings .   Baselines . We compared our approach with sev-   eral representative methods for multi - aspect con-   trollable text generation :   •GD(Krause et al . , 2021 ): a decoding - time   regulation method that uses light - weight con-15237   ditional generative discriminator to guide pre-   trained models . The distributions given by   multiple discriminators are normalized for   controlling multiple aspects of target sen-   tences .   •D. L(Gu et al . , 2022 ): a prefix - tuning-   based method that introduces an autoencoder   and additional objectives to map several con-   straints of attributes to one latent space ( i.e. ,   joint training of prefixes ) . It finds the intersec-   tion of prefixes of multiple constraints during   inference .   •P - ( Lester et al . , 2021 ): a pa - rameter efficient method that appends contin-   uous prompts to the model input . Multiple   prompts are trained separately and are simply   concatenated during inference .   •P - ( Li and Liang , 2021 ): a pa-   rameter efficient method that appends contin-   uous prefixes to the activations at attention   sublayers . Multiple prefixes are trained sep-   arately and are simply concatenated during   inference .   •T ( Yang et al . , 2022 ): a prompt - tuning-   based method that further modifies the atten-   tion mask and position indexes during infer-   ence to narrow the gap between training and   inference .   Results . Table 1 shows the automatic evaluation   on double - aspect controllable text generation . We   demonstrate the averaged accuracies to represent   the overall performance on satisfying multiple as-   pects of constraints . Furthermore , we provide   the performance gap between double- and single-15238   aspect settings to represent the ability to combine   multiple plugins in a zero - shot manner . Although   GDachieves the highest scores on sentiment   accuracy and distinctness , its perplexity explodes ,   and its tense accuracy is significantly decreased ,   which can be attributed to the interpolation of mul-   tiple discriminators . As P -T does   not have sufficient trainable parameters , it performs   poorly on constraint accuracies . However , it has a   relatively minor performance gap due to only in-   serting vectors once . P - suffers from   severe mutual interference because of the insertions   in all Transformer layers , leading to poor perfor-   mance on either constraint accuracies or perplexity .   Compared with P - , D. Lhas   better constraint accuracies and lower performance   gaps because of the joint training of prefixes . We   found that D. L is sensitive to constraint   distributions in the training set because it attempts   to find the intersection of multiple constraints . Our   approach ( P G ) achieves the highest   constraint accuracies , lowest perplexity and a rela-   tively small performance gap while our plugins are   trained separately .   Table 2 shows the extensibility of the methods .   When extended from double - aspect to triple - aspect ,   D. L has to be retrained because of its   joint training strategy . In contrast , our approach   andP - only need to train one plu-   gin , then combine plugins and plug them into the   pretrained model . Unfortunately , when extended   from triple - aspect to quadruple - aspect , as plugins   ofP - badly interfere with each other ,   its ability to control multiple aspects significantly   degenerates . However , our approach has a slight   performance gap with a relatively small training   cost , revealing its high extensibility .   The human evaluation results are illustrated inTable 3 with an inter - annotator agreement of 0.31   ( Fleiss ’ κ ) . Experiments indicate that our ap-   proach significantly outperforms both baselines   with p < 0.01on all three aspects , determined   by paired bootstrap and t - test using a popular open-   source tool ( Dror et al . , 2018 ) . Unlike automatic   evaluations , GDperforms the worst in sentiment   relevance . It can probably be attributed to the fact   thatGDoften generates ambivalent - sentiment   and non - fluent sentences , and human annotators   tend to give low ratings to them . The other results   are in line with automatically evaluated results .   5.2 Multi - Aspect Controllable Machine   Translation   Dataset . To thoroughly compare our approach   with baselines , we also adopted a sequence - to-   sequence generation task ( i.e. , machine transla-   tion ( Bahdanau et al . , 2015 ) ) . Experiments are con-   ducted on the WMT14 German →English bench-   mark . We adopted three aspects of constraints in   machine translation , and the labels are all auto-   matically obtained from target sentences . We use   keywords ( Post and Vilar , 2018 ) and tense ( Ficler   and Goldberg , 2017 ) like in the text generation   task to control translations . Specifically , we adopt   French sentences with the same meaning as the   German sources , which can be seen as an external   knowledge to improve translation quality ( Zoph   and Knight , 2016 ) , as the third constraint .   Evaluation . We adopted S BLEU(Post ,   2018 ) to calculate BLEU scores ( Papineni et al . ,   2002 ) to evaluate the translation quality . Similar to   text generation ( § 5.1 ) , we used CSR ( Chen et al . ,   2020 ) and tense accuracy to evaluate lexical and15239   temporal constraints , respectively .   Baselines . Besides P - ( Lester   et al . , 2021 ) and P - ( Li and Liang ,   2021 ) ( § 5.1 ) , we adopted another two representa-   tive parameter efficient methods as baselines :   •LRA(Hu et al . , 2021 ): a method that adds   trainable rank decomposition matrices into   attention sublayers .   •P A ( Houlsby et al . , 2019 ):   a method that parallelly inserts feed - forward   sublayers between pre - trained sublayers .   Similar to P - , for both LRAand   P A , each plugin is trained sep-   arately , and multiple plugins are simply concate-   nated for multi - aspect setting during inference .   Results . Table 4 shows the results on controllable   machine translation . Unlike text generation , con-   straints in machine translation do not merely con-   tain attribute - based constraints . Therefore , meth-   ods specially designed for attribute - based con-   straints can not be applied to this task . Surprisingly ,   P - achieves the highest constraint   accuracies and translation quality among baselines   because it largely retains the capabilities of plugins   to satisfy constraints . P - faces the se-   vere degeneration of both accuracies of constraints   and BLEU scores , which might be attributed to   the more complicated model structure in machine   translation than text generation . Our approach out-   performs all baselines in machine translation , and   the consistent superiorities on both tasks show its   generalizability .   5.3 Analysis   Mutual Interference . Similar to empirical anal-   ysis on mutual interference for P -andP - ( see § 3 ) , we also plotted the   variation of the mutual interference with the num-   ber of injections of our approach in Figure 2 . With   the gates to selectively rescale the interventions of   plugins , the growth of interference is restrained .   Ablation Study . Table 5 shows the ablation study   and comparison with the variant of our approach .   According to the performance gaps corresponding   to the changes , we can find that the textual context   of constraints slightly affects the constraint accu-   racies , and the normalization of the trainable gates   is a key point for good performance . Moreover ,   the trainable gates should be placed where the in-   teractions have just happened ( i.e. , after attention   sublayers ) . Please refer to Appendix C and D for   more results , analyses , and cases .   6 Related Work   Multi - aspect controllable text generation   ( MCTG ) ( Qian et al . , 2022 ; Yang et al . , 2022 ; Gu   et al . , 2022 ) that simultaneously satisfies multiple   constraints is a challenging task for which highly   extensible methods make more practical sense .   Approaches to it can be roughly divided into the   following three categories .   Dedicated Model . The dedicated conditional   generative models ( Keskar et al . , 2019 ; Dou et al . ,   2021 ; Huang et al . , 2021 ; Chen et al . , 2020 ) can ac-   cept multiple constraints by training from scratch   or full - parameter finetuning on the multi - aspect   labeled data . However , the multi - aspect labeled   data is hard to obtain , and the constraints that can   be satisfied are already determined during training .   Thus it is usually too expensive to apply dedicated   models to MCTG .   Decoding - Time Regulation . Although multi-   aspect controlling can be achieved by interpolating   distributions of multiple discriminators ( Dathathri   et al . , 2019 ; Chen et al . , 2021 ; Krause et al . , 2021 ;   Lin and Riedl , 2021 ) or optimizing towards multi-   ple objectives ( Qin et al . , 2022 ; Kumar et al . , 2021 ) ,   they usually significantly impair text fluency be-   cause of the intervention in the decoding stage ( Gu   et al . , 2022 ) .   Parameter Efficient Tuning . Unlike the above   two branches , PET introduces plugins trained with   fixed pretrained models for generating required   text ( Li and Liang , 2021 ; Lester et al . , 2021 ; Wang   et al . , 2022 ) . Because of its potential to achieve15240high extensibility in a plug - and - plug manner , our   work also falls in this line . However , when multiple   constraints are required , joint training of plugins is   introduced to alleviate the mutual interference of   plugins ( Chan et al . , 2021 ; Qian et al . , 2022 ; Yang   et al . , 2022 ; Gu et al . , 2022 ) , which hurts extensibil-   ity . Differently , our work aims at reducing mutual   interference while maintaining separate training .   Similar to our work , Yang et al . ( 2022 ) proposes   preventing two prompts from interactions in atten-   tion layers by modifying attention masks . Never-   theless , their method like prompt - tuning ( Lester   et al . , 2021 ) only introduces trainable parameters   to the model input , leading to insufficient trainable   parameters and dissatisfied constraints . In con-   trast , we propose a novel PET method that attaches   trainable gates to the pretrained model , alleviating   mutual interference while providing sufficient in-   terventions , leading to both desired extensibility   and effectiveness .   7 Conclusion   In summary , we propose an extensible plug - and-   play method for multi - aspect controllable text gen-   eration . By replacing trainable prefixes with train-   able prompts and gates , our approach alleviates   the mutual interference of multiple plugins while   providing sufficient interventions . Experiments on   text generation and machine translation show its   superiorities over baselines on the cost of extend-   ing to new combinations of aspects , the fulfillment   of constraints , and text fluency .   Limitations   First , although our approach and existing methods   for controllable text generation can improve the   constraint accuracies , they are currently unable to   achieve 100 % accuracies in the vast majority of   aspects ( e.g. , sentiment or topic ) . This makes them   not yet applicable in scenarios with requirements   of 100 % control fulfillment . Second , there is still a   gap between the automatic and human evaluation   of text generation , which makes there a trade - off   between precision and efficiency in the evaluation   of controllable text generation . Third , although   our approach reduces the mutual interference of   plugins so that multiple plugins can be combined   at a relatively small cost ( a decrease in constraint   accuracy ) , this cost will not be zero , which puts an   upper limit on the number of plugins can be applied   simultaneously . Fortunately , for controllable textgeneration , the number of controls applied simulta-   neously is generally not too large ( e.g. , four or five   aspects ) .   Ethics Statement   Since the text generation model is trained on data   collected from the web and often not thoroughly   cleaned , it can generate offensive or toxic text . We   must state that the texts generated by our approach   do not represent our opinion . To alleviate these   issues , we can take detoxification and politeness   as the default aspects of constraints in our multi-   aspect controllable method .   Acknowledgments   This work is supported by the National Key R&D   Program of China ( 2022ZD0160502 ) , the Na-   tional Natural Science Foundation of China ( No .   61925601 , 62276152 ) , the National Social Science   Fund of China ( 20&ZD279 ) , and a grant from the   Guoqiang Institute , Tsinghua University . We thank   Kaiyu Huang , Fuchao Wei , Yuanhang Zheng and   all the anonymous reviewers for their valuable com-   ments and suggestions on this work , as well as all   the volunteers who participated in the human eval-   uation .   References152411524215243A Theoretical Analysis   In this section , we theoretically analyze mutual in-   terference ( MI ) and derive a lower bound of MI for   prefix - tuning ( Li and Liang , 2021 ) . As Feed For-   ward and Layernorm sublayers are position - wise   operations ( Vaswani et al . , 2017 ) which would not   introduce the interference of plugins , we focus on   analyzing the multi - head attention ( MHA ) sublay-   ers .   According to the previous study ( He et al . , 2021 ) ,   the output of a single head of attention with prefixes   of the i - th plugin , which is represented by h , could   be described as   h = λ(x)h+/parenleftbig   1−λ(x)/parenrightbig   ∆h   = sh+t∆h , ( 8)   where hdenotes the original output of the pre-   trained generative model with xas input . λ(x )   is a scalar related to the attention weights , where   λ(x ) = s= 1−t∈(0,1 ) . In addition , ∆h   is an offset determined by the i - th plugin , and its   magnitude is positively correlated with the magni-   tude of ϕ , where ϕis the set of parameters of the   i - th plugin .   Following the pattern above , when the i - th and j-   th plugins are inserted at the same time , the output   of the head ( i.e. , h ) turns to be   h = γh+α∆h+β∆h , ( 9 )   where his the output of pretrained generative   model , and α , β , γ ∈(0,1 ) , α < t , β < t , γ <   s , γ < s. Similarly , ∆hand∆hare determined   by the i - th and j - th plugins .   According to the definition in Eq . ( 4 ) , let ˜h   andˆhbe the outputs like hafter training on   multi- and single - aspect labeled data , respectively .   The mutual interference of two plugins in a single   head ( i.e. , MI ) can be measured by the norm of   the gap between outputs under supervised and zero-   shot inference :   MI=/vextenddouble / vextenddouble˜h−ˆh / vextenddouble / vextenddouble   = /vextenddouble / vextenddouble˜h−(γh+α∆ˆh+β∆ˆh)/vextenddouble / vextenddouble   ≥/vextenddouble / vextenddouble˜h−γh / vextenddouble / vextenddouble−/vextenddouble / vextenddoubleα∆ˆh+β∆ˆh / vextenddouble / vextenddouble ,   ( 10 )   where ∆ˆhand∆ˆhcorrespond to offsets that plu-   gins are trained on single - aspect labeled data .   Considering that the intervention caused by two   plugins simultaneously should larger than the sumof two interventions caused by two plugins respec-   tively because of the interaction between two plug-   ins , we assume that there is   /vextenddouble / vextenddouble˜h−h / vextenddouble / vextenddouble>/vextenddouble / vextenddoubleˆh−h / vextenddouble / vextenddouble+/vextenddouble / vextenddoubleˆh−h / vextenddouble / vextenddouble.(11 )   Based on this , we can derive   MI>/vextenddouble / vextenddoubleˆh−γh / vextenddouble / vextenddouble+/vextenddouble / vextenddoubleˆh−γh / vextenddouble / vextenddouble   −/vextenddouble / vextenddoubleα∆ˆh+β∆ˆh / vextenddouble / vextenddouble . ( 12 )   Given that s > γ , s > γ , and ˆh = sh+t∆ˆh   ( Eq . ( 8) ) , MIsatisfies   MI>/vextenddouble / vextenddoubleˆh−sh / vextenddouble / vextenddouble+/vextenddouble / vextenddoubleˆh−sh / vextenddouble / vextenddouble   −/vextenddouble / vextenddoubleα∆ˆh+β∆ˆh / vextenddouble / vextenddouble   = ∥t∆ˆh∥+∥t∆ˆh∥ − ∥ α∆ˆh+β∆ˆh∥   ≥(t−α)∥∆ˆh∥+ ( t−β)∥∆ˆh∥,(13 )   where 1 > t−α > 0and1 > t−β > 0 .   Therefore , the mutual interference of two plugins   in a single head has a positive lower bound , and it   is positively correlated with the magnitude of ˆϕ   andˆϕ.   To step further , we derive the lower bound of MI   in the multi - head scenario . Assume that Kdenotes   the number of heads , Wdenotes the fixed output   projection matrix in the MHA , W = QRis the   QR - decomposition format of W,ˆλis the average   of absolute eigenvalues . Specifically , ˆhand˜h   denotes ˆhand˜hin the k - th head , respectively .   Then , the lower bound of MI in MHA ( i.e. , MI )   can be derived as ( viewing Ras a diagonal matrix   for simplicity )   MI=/vextenddouble / vextenddoubleconcat ( ˜h−ˆh)W / vextenddouble / vextenddouble   = /vextenddouble / vextenddoubleconcat ( ˜h−ˆh)QR / vextenddouble / vextenddouble   ≈ˆλ / vextenddouble / vextenddoubleconcat ( ˜h−ˆh)Q / vextenddouble / vextenddouble   = ˆλ / radicaltp / radicalvertex / radicalvertex / radicalbt / summationdisplay / vextenddouble / vextenddouble˜h−ˆh / vextenddouble / vextenddouble   ≥ˆλ√n / summationdisplay / vextenddouble / vextenddouble˜h−ˆh / vextenddouble / vextenddouble   > ˆλ√n / summationdisplay / parenleftig   ( t−α)∥∆ˆh∥   + ( t−β)∥∆ˆh∥/parenrightig   , ( 14 )   where 1 > t−α>0and1 > t−β>0 , and   ∆ˆhand∆ˆhare also positively correlated with   the magnitude of ˆϕandˆϕ , respectively.15244Therefore , the mutual interference of multiple   plugins has a theoretical positive lower bound ,   which implies concatenating prefixes that are sep-   arately trained has an irreparable gap against   supervised - trained prefixes . As a result , MI might   accumulate along with the depth of the model , like   in Figure 2 . Intuitively , introducing gates , which   contain trainable coefficients between 0 to 1 , to ˆϕ   is helpful for decreasing the offsets in Eq . ( 14 ) and   thus mutual interference .   B Reproducibility   B.1 Data Preparation   For text generation , we adopted the widely - used   Yelp dataset(Lample et al . , 2019 ) , which contains   restaurant reviews with sentiment ( positive and neg-   ative ) and topic ( American , Mexican , and Asian )   labels . Specifically , following previous work ( Yang   et al . , 2022 ) , we randomly sampled 30K/3 K sen-   tences for each attribute for training / validation   while ensuring the balance of different attributes   in the final dataset ( Table 6 ) . For evaluation , we   sampled 25 sentences for each given textual prefix   and combination of aspects . In addition , we elimi-   nated the sentences rated 3 in sentiment . To eval-   uate the extensibility of methods , we added two   additional aspects of constraints : keywords ( He ,   2021 ) and tense ( past and present ) ( Ficler and Gold-   berg , 2017 ) , where their labels are automatically   extracted from the reviews . More precisely , we ran-   domly extracted 1 to 3 words as keywords for each   sentence ( Post and Vilar , 2018 ) , and the tenses of   sentences are labeled by an open - source toolkit   that is based on a POS tagger ( Nguyen and Ver-   spoor , 2018 ) .   For machine translation , we adopted the   WMT14 German →English benchmark . Specifi-   cally , the training , validation , and test sets contain   4,500 K , 3 K , and 3 K sentences , respectively . We   adopted three aspects of constraints in machine   translation , and they are all automatically obtained   from target sentences . We use keywords ( Post and   Vilar , 2018 ) and tense ( Ficler and Goldberg , 2017 )   like the text generation task to control translations .   For the third constraint , we adopt French synony-   mous sentences as external knowledge , which is   beneficial to disambiguation . Note that it does   not directly control any attribute of translations   but will improve the translation quality ( Zoph and   Knight , 2016 ) . The French synonymous sentences   are given by a Transformer - based English →French   translation model ( Vaswani et al . , 2017 ) .   B.2 Evaluation Metrics   For text generation , following previous work ( Lyu   et al . , 2021 ; Dathathri et al . , 2019 ; Gu et al . ,   2022 ) , we adopted automatic and human evalu-   ation for constraint accuracy and text quality . For   the evaluation of sentiment and topic , we finetuned   two RoBERTa - based ( Liu et al . , 2019 ) classifiers   on the Yelp dataset . Specifically , we randomly   over - sampled 1,500K/15K/15 K sentences for train-   ing / validation / test set of topic and 1,380K/1K/1 K   sentences for training / validation / test set of senti-   ment . The F1 scores for sentiment and topic are   98.71 and 89.62 , respectively . The same toolkit as   training evaluated the accuracy of tense , and we   used word - level Copy Success Rate ( CSR ) ( Chen   et al . , 2020 ) to evaluate the lexical constraint . In   addition , we used the perplexity ( PPL ) given by   GPT-2 ( Radford et al . , 2019 ) and averaged   distinctness ( Li et al . , 2016 ) to evaluate the flu-   ency and diversity of the generated text , respec-   tively . Similar to previous work ( Dathathri et al . ,   2019 ; Yang and Klein , 2021 ) , we used 15 textual   prefixesand asked models to start writing from   them for each combination of constraints during   inference . For human evaluation , each sentence   received a score of 1 to 5 on sentiment and topic   relevance as well as fluency given by three eval-   uators . The final scores are averaged over three   ratings .   Specifically , the 15 textual prefixes are : “ Once   upon a time”,“The book”,“The chicken”,“The15245   city”,“The country”,“The lake”,“The movie”,“The   painting”,“The weather”,“The food”,“While this is   happening”,“The pizza”,“The potato”,“The presi-   dent of the country”,“The year is 1910 . ” .   For machine translation , we adopted S -   BLEU(Post , 2018 ) to evaluate the translation   quality . Similar to text generation , we used   CSR ( Chen et al . , 2020 ) and tense accuracy to eval-   uate lexical and tense constraints , respectively .   B.3 Model and Hyper - parameters   As our approach has both encoder and decoder , we   adopted BART - large(Lewis et al . , 2020 ) for text   generation and mBART - large - cc25(Liu et al . ,   2020 ) for machine translation .   ForGD(Krause et al . , 2021 ) , D. L(Gu   et al . , 2022 ) , and T ( Yang et al . , 2022 ) , we   follow the settings in their paper . Specifically , we   found that the weights for attribute balance and the   number of candidates in the decoding stage of D.   Lsignificantly affect constraint accuracies . For   the weights for attribute balance and the number of   candidates , we searched in { 0.1,0.2,0.5,1,1.5,2,15246   3,4,5,8,10,25,50 } and { 1,2,5,10 } , respectively .   For the other methods , we demonstrate their hyper-   parameters in Table 7 . Table 8 shows the number of   trainable parameters of each method . The learning   rates were determined by searching in { 1×10 ,   2×10,4×10,8×10,1×10,2×10 ,   4×10,1×10 } on the development set . In   our approach , the textual contexts used for attribute-   based constraints ( see § 4.2 ) are :   •Sentiment : “ This is a { } review . ” for “ posi-   tive / negative ” .   •Topic : “ The following is about { } food . ” for   “ Asian / American / Mexican ” .   •Tense : “ The tense of this sentence is the { }   tense . ” for “ past / present / future ” , and “ The   tense of this sentence is undecided . ” for sen-   tences that do not have an explicit tense .   Note that our use of existing artifact(s ) was con-   sistent with their intended use . The source code   of this work is available at https://github.com/   THUNLP - MT / PromptGating4MCTG and it was devel-   oped based on THUMT ( Tan et al . , 2020 ) , an open-   source toolkit for machine translation .   C Experimental Results   C.1 More Analyses   Visualization of P G .To further   investigate how our approach alleviates the mutual   interference of plugins , we visualized the trainable   parameters in P G . Specifically , we   first extracted Pandσ(G)in Eq . ( 7 ) from each   layer jfor every single aspect . Then we calculated   the average of σ(G)and the Lnorm of P   over all the layers , which are represented by red   and blue bars respectively in Figure 4 and Figure 5 .   We can find that when the magnitude of P(i.e . ,   trainable vectors added to hidden states ) becomes   larger , the values of σ(G)(i.e . , trainable gates )   tend to become smaller . In other words , these train-   able gates attempt to normalize or stabilize the mag-   nitude of hidden states and thus alleviate mutual   interference .   Efficiency . Table 9 and Table 10 show the train-   ing and inference speeds of each method in text   generation and machine translation . All training   and inference were run on a single GeForce RTX   3090 GPU .   C.2 Detailed Results   Table 11 and 12 are the detailed versions of Table 1   and 4 , respectively . We provide detailed results   of both single- and multi - aspect models . For text   generation , we further demonstrate the accuracy of   each attribute .   D Case Study   To further investigate the fulfillment and text qual-   ity of each combination of constraints of these   methods , Table 13 and Table 14 demonstrate ex-   amples of text generation and machine translation ,   respectively . Models only trained on single - aspect   data are required to give results satisfying multiple   aspects of constraints .   E Details in Human Evaluation   In this section , we show more details about the   human annotation adopted for evaluating model   performance on text generation . We recruited three   volunteers from schools , shuffled the output of   models and provided it to them for scoring . Since15247they are volunteers , they were not paid . Their av-   erage age is 25 years old and they have enough   daily English communication skills . The instruc-   tion we provided to them like “ This human evalua-   tion aims to evaluate the model - generated review   texts in three aspects : sentiment and topic rele-   vance , and text fluency . All three integer scores   are on a scale of 1 - 5 , with a higher degree of   topic / sentiment relevance representing a more con-   sistent theme / sentiment , and a higher degree of text   fluency representing a more fluent text . Your per-   sonal information will not be retained and these   scores will only be used for human evaluation in   research”.15248152491525015251WARNING : Next may contain contents that are offensive in nature.1525215253WARNING : Next may contain contents that are offensive in nature.15254ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section * Limitations after the conclusion .   /squareA2 . Did you discuss any potential risks of your work ?   Section * Ethics Statement after the limitations .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1 Introduction   /squareA4 . Have you used AI writing assistants when working on this paper ?   Grammarly , grammar error correction , the whole paper .   B / squareDid you use or create scientiﬁc artifacts ?   Section B Reproducibility   /squareB1 . Did you cite the creators of artifacts you used ?   Section B Reproducibility   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Section B Reproducibility   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section B Reproducibility   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section B Reproducibility   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section B Reproducibility   C / squareDid you run computational experiments ?   Section 5 Experiments   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section C Experimental Results15255 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section B Reproducibility   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section C Experimental Results   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section B Reproducibility   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Seciton E Details in Human Evaluation   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Seciton E Details in Human Evaluation   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Seciton E Details in Human Evaluation   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Seciton E Details in Human Evaluation   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Seciton E Details in Human Evaluation15256