  Elias Stengel - Eskin   Johns Hopkins University   elias@jhu.eduBenjamin Van Durme   Johns Hopkins University   vandurme@jhu.edu   Abstract   Children acquiring English make systematic er-   rors on subject control sentences even after they   have reached near - adult competence ( Chom-   sky , 1969 ) , possibly due to heuristics based   on semantic roles ( Maratsos , 1974 ) . Given the   advanced fluency of large generative language   models , we ask whether model outputs are con-   sistent with these heuristics , and to what degree   different models are consistent with each other .   We find that models can be categorized by be-   havior into three separate groups , with broad   differences between the groups . The outputs of   models in the largest group are consistent with   positional heuristics that succeed on subject   control but fail on object control . This result is   surprising , given that object control is orders of   magnitude more frequent in the text data used   to train such models . We examine to what de-   gree the models are sensitive to prompting with   agent - patient information , finding that raising   the salience of agent and patient relations re-   sults in significant changes in the outputs of   most models . Based on this observation , we   leverage an existing dataset of semantic proto-   role annotations ( White et al . , 2020 ) to explore   the connections between control and labeling   event participants with properties typically as-   sociated with agents and patients .   1 Introduction   Normally - developing children learning English   struggle with subject control clauses long after   they have successfully acquired the components to   understand them ( Chomsky , 1969 ; Cromer , 1970 ;   Maratsos , 1974 ; Sherman and Lust , 1993 ) . A sen-   tence with a subject control clause has a matrix   ( main ) clause containing a main verb , an agent , and   a patient , and an embedded infinitival clause . For   example , in Cole promised Joe to call , the agent is   Cole , the patient is Joe , and the embedded clause isto call . Crucially , the embedded verb here does not   have an overt subject , but rather implicitly refers to   a subject in the matrix clause ( in this case , Cole ) .   In a subject control clause , this latent subject of   the embedded infinitival clause ( usually written as   PRO ) is coindexed with the subject ( Cole ) rather   than the object ( Joe ) of the matrix ( main ) clause ,   i.e. Cole ( the agent ) is doing the calling . This is   typically written :   [ Cole]promised [ Joe]PROto call ( 1 )   where subscripts indicate the noun phrase ( NP )   “ Cole ” is the subject of “ to call ” . ( 1 ) can be con-   trasted with the more common case of object con-   trol ; for example , if the matrix verb “ promised ” is   swapped with an object control verb like “ told ” ,   then the coreferrent of PRO changes :   [ Cole]told [ Joe]PROto call ( 2 )   Chomsky ( 1969)finds that children ages 5 to   10 regularly misinterpret subject control ( 1 ) for   object control ( 2 ) while correctly interpreting ob-   ject control clauses , and proposes that children are   following the Minimal Distance Principal ( MDP ) ,   choosing the linearly closest noun phrase ( NP ) to   govern PRO . Cromer ( 1970 ) highlights the system-   aticity with which children mistake subject control   for object control and provides evidence for the   MDP . However , Maratsos ( 1974 ) argues against   the MDP ; while his results support the observation   that children struggle with subject control , they do   not support the MDP , favoring an alternative based   on semantic roles . Maratsos changes the subject   and object order through passivization :   [ Joe]was told by [ Cole]PROto call ( 3 )   finding that children correctly coindex PRO with   the ( further away ) object , violating the MDP .   Recently , large pre - trained language models   have shown an impressive ability not only to pro-   duce fluent text , but also to perform tasks by “ filling11065   in the blank ” in question - answering prompts , ei-   ther with no previous examples ( zero - shot ) or with   a few representative examples ( few - shot ) ( Brown   et al . , 2020 ; Raffel et al . , 2020 ; Sanh et al . , 2021 ) .   In light of the difficulty children have in acquir-   ing subject control constructions , we explore how   the outputs of the language models tested com-   pare with adult and child strategies for coindexing   PRO . In Section 3.2 , we examine this question   in the zero - shot setting where we give the mod-   els only a single question , treating each model as   a sort of experimental subject ( cf . Fig . 1 ) . Our   initial hypothesis is that model outputs will be con-   sistent with child strategies , i.e. the models will   perform well on object control examples , but mis-   interpret subject control for object control . This is   informed by two factors : object control is orders of   magnitude more frequent than subject control ( cf .   Section 3.2 ) , and active object control ( i.e. ( 2 ) ) re-   quires resolving a shorter dependency than subject   control . We instead find that the tested models fall   into three groups , with the majority in fact produc-   ing responses mistaking object control for subject   control – the opposite of what children do .   Following these observations , in Section 4 we   examine to what degree this behavior is sensitive   to semantic roles , following Maratsos ( 1974 ) . To   test this , we investigate a “ few - shot ” setting , where   we prompt the model not only with the context and   a single question , but also with a set of question-   answer pairs that raise the salience of the matrix   agent and patient ( cf . Fig . 2 ) .   We find that the models whose behavior in the   zero - shot setting was consistent with a positional   heuristic have significant differences in the few - shot setting , and the directions of these difference   are consistent with a sensitivity to semantic roles .   Finally , in Section 5.2 we investigate whether   the sensitivity to semantic roles corresponds to per-   formance on a semantic proto - role labeling task ,   where models are tested with questions about voli-   tion and change of state , properties associated with   agents and patients ( respectively ) . We find that   while some models are able to perform the label-   ing task surprisingly well , the differences between   models do not necessarily map to the differences   in Section 4 . We offer three key takeaways :   1.For many models ( all GPT - Neo variants , Juras-   sic Jumbo ) the outputs are surprising given their   training distributions ; while object control is   orders of magnitude more common in the text   data used in training these models , they perform   better on subject control .   2.Large pretrained models are not consistent   among themselves . Even models with similar   architectures can have very different trends in   their outputs , and the outputs of autoregressive   and text - to - text models differ substantially .   3.The associations in the autoregressive models   tested form outputs that can be explained by sim-   ple , often position - based heuristics . For text - to-   text models ( e.g. T0 , T5 ) the output patterns can   also be captured by heuristics based on agent   and patient relations . However , sensitivity to   agent and patient relations in subject and object   control clauses does not always entail higher   performance on semantic proto - role labeling .   2 Models and Metrics   We explore both autoregressive models and text-   to - text models . Autoregressive models are opti-   mized by minimizing −log(P(w|w))for words   w , . . . win a given context . These models ( also   called “ decoder - only ” models ) are composed of   just a decoder , which encodes the previously ob-   served tokens w , . . . wproduces a probability   distribution over the vocabulary for the next token ,   w. Text - to - text models are encoder - decoder mod-   els , optimized to reconstruct a noised version of the   input via the decoder . An encoder takes a corrupted   version of whole sequence w , . . . was input , en-   coding it into a dense representation from which   the decoder reconstructs the original w , . . . w.   The autoregressive models considered are :   •GPT-3 Davinci : this model is only available   through the OpenAI API , and its exact training11066details are unclear . It is based on the GPT-3   model ( Brown et al . , 2020 ) which was trained on   Common Crawl ( Raffel et al . , 2020 ) with 175B   ( billion ) parameters . Among the several versions   of GPT-3 , Davinci is generally regarded as the   highest - performing ( OpenAI ) .   •GPT - Neo : this is an open - source replication of   GPT-3 introduced by Black et al . ( 2021 ) , trained   on The Pile ( Gao et al . , 2020 ) , a 800Gb dataset   of web - text intended for pre - training . GPT - Neo   has 3 sizes : 1.3B , 2.7B , and 6B parameters ( GPT-   J ) , all trained on the same dataset , allowing for   direct comparison .   •Jurassic : Jurassic Large ( 7.5B parameters ) and   Jurassic Jumbo ( 178B parameters ) ( Lieber et al . ,   2021 ) are also accessible only through an API .   The training data is based on Common Crawl ,   though similarly to GPT-3 Davinci , the details of   the training data filtering process are unclear . Rel-   evant differences to GPT-3 are in the tokenization   ( which includes multi - word expressions ) and use   of fewer , wider layers .   The text - to - text models we consider are :   •T5 for QA : The T5 - base text - to - text model ( 220-   million parameters ) ( Raffel et al . , 2020 ) was pre-   trained on cleaned Common Crawl data ( C4 ) and   fine - tuned on SQuaD question answering data   ( Rajpurkar et al . , 2016 ) .   •T0pp : presented by Sanh et al . ( 2021 ) , T0pp is   an 11B parameter model with a T5 - like archi-   tecture , pre - trained on Common Crawl data and   finetuned specifically for zero - shot question an-   swering on the P3 dataset of NLP benchmarks .   This dataset recasts a large number of NLP bench-   mark datasets into question answering prompts .   Note that because of fine - tuned nature of the “ T5   for QA ” model , the expected prompt format is   fixed , unlike the unrestricted prompt format for   the other models . Thus , prompt hacking can not be   done on this version of T5 , and so it is only used   in Section 3.2 . We access non - API models via the   Transformers library ( Wolf et al . , 2020 ) ; due to   computational constraints , they are run on single   GPUs at 1/2 precision . For all models , we decode   with the temperature parameter set to 0 .   2.1 Metrics   Online APIs make forced decoding very costly   ( Shin and Van Durme , 2021 ) . Rather than com-   paring logits for a restricted output vocab , we al-   low the model to freely generate tokens , letting   the model produce a larger variety of answers . Inother words , rather than comparing the output prob-   abilities for particular tokens ( the logits ) given a   fixed prefix , we compare full strings of output to-   kens . However , this method requires heuristics to   classify the output strings into categories . In Sec-   tion 3.2 we validate our heuristics , verifying that   for locally - run models the trends are similar when   using logits .   Our metric first extracts single word answers   and then searches for answers like “ The answer is :   NAME ” . For some models and settings , the model   re - generates the entire prompt before answering ,   i.e. it copies the instructions , context , and ques-   tion , before copying the answer continuation and   finally producing an answer . We use Levenshtein   distance to check whether the prompt has been   regenerated ; if it has , it is removed and the first   string following the prompt is checked for answer   strings . The extraction function always returns the   first valid answer produced by the model . If the   extraction function fails to find any valid answer   strings , the example is skipped in evaluation rather   than counted as wrong . We measure significance in   model differences with McNemar ’s test ( McNemar ,   1947 ) , following Dietterich ( 1998 ) .   3 Experiment 1   In order to examine what types of generalizations   are made by the examined models when prompted   for subject and object control information , we   construct a number of question - answering - style   prompts , where the models fill in the answer . This   approach follows recent literature ( Raffel et al . ,   2020 ; Brown et al . , 2020 ) and takes advantage of   the models question - answering abilities . Moreover ,   using models pretrained with a language - modeling   loss rather than training a model specifically for   control lets us examine what types of general-   izations are captured by the models ’ associations   learned from its original training data , rather than   whether a very large model can learn to answer   subject and object control questions correctly . We   first describe the construction of the prompts used   in this set of experiments . Using those prompts ,   we validate the choice to use heuristic extraction   from open generation ( i.e. allowing the model to   generate tokens up to an end - of - sequence token )   rather than logits and confirm that in the C4 dataset ,   object control is more frequent than subject control .   Then , we analyze the zero - shot performance of the   models on the subject and object control prompts.110673.1 Subject and Object Prompts   While pretrained language models used for QA are   often evaluated in a “ few shot ” setting , where they   are given a few “ training ” prompts before answer-   ing a “ test ” prompt , in our main experiments we   focus on the zero - shot setting . This is in order to   avoid learning effects that might result from few-   shot prompting ( as one would with human subjects )   and follows human experiment paradigms , where   experimenters are careful not to provide feedback   to subjects about the expected answer until after   all trials are complete . The prompts used in Sec-   tion 3.2 and Section 4 have an instruction sentence ,   a context ( like ( 1)-(3 ) ) , a question ( e.g. “ Who   called ? ” ) , and an answer continuation . See Fig . 1   for an example zero - shot object control prompt .   We take the max over two instruction types ( long   and short ) in our analyses . Fig . 1 shows the long-   form instructions , which include the options in the   instructions ( e.g. Answer the question with either   " Casey " or " Avery " ) and in the question ( e.g. Casey   or Avery ) . The short - form instructions omit these   prompts in the instructions and questions .   Since the models examined can be sensitive   to specific tokens , we cover 9 embedding verbs   for object control , chosen from a selection of   common linguistics examples : “ told ” , “ ordered ” ,   “ called upon ” , “ urged ” , “ asked ” , “ persuaded ” , “ con-   vinced ” , “ forced ” , and “ pushed ” . These verbs are   presented both in the active ( object control experi-   ments ) and passive ( passive object control experi-   ments ) . These include verbs that trigger a factual-   ity inference in the affirmative ( e.g. “ persuaded ” ,   “ convinced ” , “ forced ” ) . For subject control , we   follow previous work ( Chomsky , 1969 ; Maratsos ,   1974 ) and use “ promise ” ; we also include “ threaten ”   as a subject control verb . These verbs are pre-   sented only in the active voice , as sentences such   as “ Casey was promised by Avery to call ” were   deemed too ambiguous ( if grammatical at all ) . In   our main experiments , we use names as NPs ; we   also report results in Appendix A using common   professions to ensure that the trends observed with   names hold . We chose 2 male names , 2 female   names , and 2 gender - neutral names ; these were   chosen by taking the top 2 names in each reported   gender category in US Social Security data from   1970 to 2019.The gender - neutral names were   chosen by taking the top names in the intersec - tion of male and female names . We run the same   prompt with each name combination in both or-   ders , to avoid possible biases the model may have   towards particular names . When the names are in-   cluded in the instruction , we add an example with   the name order swapped to avoid confounding due   the model simply copying the first or last name to   appear in the instructions . Finally , for the action   infinitive ( i.e. the embedded verb ) we chose the   first 5 coherent verbs ( i.e. intransitive infinitives )   from a frequency list of English verbs ( Yu et al . ,   2020 ; Sharov , 2020 ) . This yields 1500 sentences   for object control and 150 for subject control ( 3000   and 300 with swapped names ) .   3.2 Results and Analysis   Frequency of Subject and Object Control In   Section 1 , we claimed that object control is more   frequent that subject control . To support this claim   in the context of the models examined here , we   conduct a search of a subset of the C4 dataset ( Raf-   fel et al . , 2020 ) for sentences fitting subject control   and object control templates . While there are many   types of subject and object control , we focus on in-   finitival complements with transitive matrix verbs ,   searching with templates similar to the sentences   in examples 2 and 1 . For object control , we use the   same verb list as in Section 3.1 . For subject control ,   we use “ promise ” and “ threaten ” , as in Section 3.1 .   We allow the embedded verb to be any verb . We   sub - sample the first 1 000 000 sentences of C4 and   search them with the templates , finding that object   control occurs 10 435 times , while subject control   occurs only 209times , i.e. object control is ∼50   times more frequent .   Validating Logits Fig . 3 shows the zero - shot ac-   curacy of the best instructions using logit scoring ,   for the models for which we have access to the full   output distribution ( non - API models ) .   Comparing the results to Fig . 4 , we see similar   but less pronounced trends . As in Fig . 4 , GPT-   Neo models perform better on subject control and11068   passive object control , while text - to - text models   perform better on object control and passive object   control . This validates our choice to use heuristics   in later experiments . We note also that for GPT-   Neo models , the heuristic results in Fig . 4 reflect   higher accuracies than the logit - based results in   Fig . 3 , indicating that the heuristics capture broader   range of outputs corresponding to valid answers .   Zero - shot performance In Fig . 4 , we see that   model classes have different results ; we further   classify models into 3 groups :   1.The GPT - Neo variants and Jurassic Jumbo are   better on subject and passive object control than   object control . This pattern can be accounted for   by a positional heuristic , namely to take the first   NP in the matrix clause ( i.e. Maximum Distance   Principle rather than the minimum distance prin-   ciple of Chomsky ( 1969 ) ) .   2.T5 and T0 are consistent with the observations   in Maratsos ( 1974 ) ; both models do better on   object control ( active and passive ) than subject   control . This contradicts the MDP but is consis-   tent with a heuristic choosing the matrix patient .   3.GPT-3 and Jurassic Large both perform well   above chance on object control and subject con-   trol , with their best performance on subject con-   trol , but both perform worse on passive object   control . This could be matched to a positional   heuristic ( take the second NP ) for object control   verbs , rather than an agency - based heuristic .   Further observations Even within model fam-   ilies , there are measurable differences : although   GPT-3 and Jurassic Jumbo are roughly the same   size and share a general architecture , and are os-   tensibly trained on similar data , the changes made   by Lieber et al . ( 2021 ) seem to have a measur-   able impact , with Jurassic Jumbo performing dif-   ferently on zero - shot object control examples . For   active object control , the difference ∆ =   acc−acc = 29 ( p < 0.01 ) , and for   passive ∆=−9(p < 0.01 ) . Similarly ,   GPT-3 differs from GPT - Neo-1.3B on active objectcontrol , and from GPT - Neo-2.7B and GPT - J on   both forms of object control , despite sharing an ar-   chitecture . Further analysis is impeded by a lack of   details on the data used to train GPT-3 and Jurassic ;   this underscores the need for model creators to be   transparent about training data and details .   We also observe that larger models tend to have   higher performance : GPT - J is better on all settings   than GPT - Neo-1.3B and 2.7B , and Jurassic Jumbo   is better than Jurassic Large on passive object con-   trol . That said , some larger models are also slightly   worse than their smaller counterparts ( e.g. Juras-   sic Jumbo on object control ) . This suggests that   larger models may be more prone to learning pat-   terns corresponding to simple heuristics ; however ,   additional evidence is needed .   4 Experiment 2   Recent work has shown that providing examples in   the prompt to a frozen pre - trained model can yield   higher performance on QA tasks ( Brown et al . ,   2020 ) . In this setting , typically called “ in context   learning ” , some number of demonstrations of ques-   tions and answers are given in the context , followed   by a test example , to which the model produces an   answer . The demonstrations in the context give the   model additional guidance on which task is being   evaluated . While we do not do in - context learn-   ing with training questions about object in subject   control , we do experiment with adding information11069to the prompt to raise the salience of agents and   patients ( e.g. “ Q : Who told someone to call ? A :   Cole ” for ( 2 ) . ) An example can be seen in Fig . 2 ;   the QA pairs given in the context give the agent   and the patient of the matrix clause , but not the   embedded clause . This can be thought of as a form   of chain - of - thought prompting ( Wei et al . , 2022 )   or a scratchpad ( Nye et al . , 2021 ) where the gold   answers are provided .   We hypothesize augmenting the prompt with   agent - patient questions will affect each group as   follows :   1.For Group 1 ( GPT - Neo , Jurassic Jumbo ) where   the models ’ outputs are consistent with posi-   tional heuristics , the additional prompts will   provide some evidence inconsistent with the   heuristic . This evidence may lead to changes in   the models ’ associations that result in outputs   less consistent with the heuristics . For example ,   in Fig . 2 the question Who was told to come ,   Avery or Casey ? the answer Casey provides ev-   idence against taking the furthest - away NP as   the answer . In that case , we would expect a drop   in performance in passive object control and in   subject control , where the heuristic is benefi-   cial , and an increase in object control , where the   heuristic does not help .   2.In Group 2 ( T0 and T5 ) , since the model out-   puts are already consistent with a semantic   role - based explanation , we do not expect much   change in any setting . In other words , the   model outputs are already consistent with ac-   cess to agent - patient relations combined with an   incorrect association for outputting the matrix   clause ’s agent as the embedded subject .   3.Finally , in Group 3 ( GPT-3 , Jurassic Large ) , we   see that the models ’ outputs on object control   are consistent with an object control - specific   heuristic ( to take the first NP ) the models have   lower performance for passive object control   than active object control . Thus , as in Group 1 ,   we expect that evidence against the positional   heuristic in the prompts will boost performance   in passive object control , while reducing it on   active object control .   4.1 Results and Analysis   Fig . 5 shows the results after applying prompts with   questions about agents and patients . Here , we see   that for Group 1 ( GPT - Neo and Jurassic Jumbo )   the performance does decrease for subject control   and passive object control . This decrease is signifi - cant for all models and settings ( p < 0.01 ) except   Jurassic Jumbo in subject control ( ∆=−7 ,   p= 0.06 ) . At the same time , all object control   performance increases significantly for Group 1   ( p < 0.01 ) . These results confirm our hypothe-   ses , supporting the notion that these models are   consistent with a positional rather than semantic   heuristic . For Group 2 ( T0 ) , we find a significant   decrease in performance on both object control   types ( ∆=−20,p < 0.01 ) , and no signifi-   ca nt difference for subject control ( p= 0.14 ) ; this   is roughly consistent with our predictions , since   the size of the decrease for object control is rel-   atively small ( e.g. compared to the decrease for   GPT-3 in the passive ) . Finally , for Group 3 ( GPT-3   and Jurassic Large ) we largely see the opposite of   what we expected : GPT-3 ’s performance on object   control goes close to ceiling after prompting with   agent - patient questions , while the passive perfor-   mance drops to 0 ; similarly , Jurassic Large ’s per-   formance drops on passive object control , dropping   slightly also on active object control . Both models   improve on subject control , with significant dif-   ferences from the zero - shot setting ( ∆= 15 ,   p < 0.01,∆= 9,p < 0.01 ) , perhaps reflect-   ing an effect of the semantic role - based priming .   Note that for GPT-3 , the passive performance drop   is from a lack of parseable , non - empty strings be-   ing produced , rather than incorrect predictions . For   active object control , GPT-3 outputs the second NP   more often with additional prompts , increasing the   score from 78 % to99 % . It may be that the associa-   tions responsible for this are also to blame for the   degenerate behavior seen in the passive , where the   model produces only an end - of - sequence token .   5 Experiment 3   Following Maratsos ( 1974 ) ’s hypothesis that the   observed mistakes children make on subject control   sentences is driven by semantic roles , in Section 5.2   we examine the relationship between a model ’s   ability to perform zero - shot object and subject con-   trol and its accuracy on identifying attributes com-   monly associated with agents and patients . Query-   ing language models using fixed semantic role on-   tologies ( e.g. AGENT , PATIENT , THEME ) may   be difficult as these ontologies may be absent from   pretraining corpora . We instead measure the mod-   els ability to perform semantic proto - roles labeling   ( SPRL ) for the volition and change of state prop-   erties . We use the SPRL data provided in the Uni-11070versal Decompositional Semantics ( UDS ) dataset   introduced by White et al . ( 2020 ) . These proper-   ties , first proposed by Dowty ( 1991 ) , were found   to be strongly prototypical of agents and patients ,   respectively ( Reisinger et al . , 2015).Proto - role   inferences are elicited with simple prompts , circum-   venting brittle and complicated ontologies . Indeed ,   the UDS dataset was built by asking annotators   questions like “ How likely is it that ARG chose to   be involved in the PRED ? ” and normalizing their   scalar ratings to [ −3,3 ] .   5.1 Semantic Proto - Role Labeling Prompts   To construct a dataset of SPRL prompts , we first fil-   ter the UDS dataset for sentences with < 35tokens   – this eliminates many long sentences , which are   often more difficult to answer . We then eliminate   examples with scalar annotations ∈(−1,1 ) , keep-   ing only examples with strong inferences about the   properties . The annotations are binarized with val-   ues≥1leading to a “ Yes ” answer and values ≤ −1   leading to “ No ” . The annotations are balanced be-   tween “ Yes ” and “ No ” , with the excess examples   from the more frequent category being removed .   Two templates are used for each property ; an   example template is shown in Fig . 6 . For volition ,   one template asks , ‘ In the event “ PRED ” , does the   participant “ ARG ” act with volition ? ’ while the   other asks ‘ In the event “ PRED ” , does the par-   ticipant “ ARG ” act on purpose ? ’ . For change of   state , the first template asks , ‘ In the event “ PRED ” ,   does the state of the participant “ ARG ” change ? ’   and the other asks , ‘ In the event “ PRED ” , does the   participant “ ARG ” change in state ? ’ We take the   maximum over these two templates .   In our first set of experiments we are interested in   the raw ability of the model to perform the semanticproto - role labeling ( SPRL ) task , and so we allow   for full prompt hacking , where demonstrations of   the task are provided as part of the context . Ac-   cordingly , we stratify the annotations into 4 stages ;   the bottom stage always forms the “ test ” prompt ,   with the answer blank . The remaining 3 stages are   added for increasing levels of in - context learning   with “ training ” question - answer pairs ( i.e. when   the 3rd layer is added , there are 3 example question-   answer pairs with answers , and one “ test ” pair that   has no answer , where the model must fill in the   answer . ) Fig . 6 shows a prompt with one training   stage , followed by one test example . We ensure   that we use each annotation only once , and that   all test annotations paired across across prompting   settings . This results in 118 change - of - state test   prompts and 168 volition prompts .   Hypotheses We expect that models which per-   form well on zero - shot subject and object control   ( e.g. those that can model both the active and pas-   sive of object control ) will also have higher perfor-   mance on SPRL , since both require semantic role   information . Specifically , we expect to see higher   performance from T0 and T5 on at least one of the   properties , since their outputs on active and pas-   sive object control are consistent with a heuristic   that identifies patients as embedded subjects , rather   than a positional heuristic . Thus , it may be that the   representations learned by T0 and T5 contain more   information on agency and patienthood , leading to   better performance on SPRL .   5.2 Results and Analysis   Table 1 shows the accuracy on binary semantic   proto - role labeling of all models with performance   significantly above a random baseline . For change   Setting Model # shots Acc . # valid   ∆State GPT-3 1 0.61 118   V olitionGPT-3 3 0.77 168   GPT - J 0 0.69 111   T0 0 0.60 168   of state , only GPT-3 performs above chance , while   for volition , GPT-3 , GPT - J , and T0 perform above   chance . T0 ’s lower performance is surprising , as   the performance of T0 in Fig . 4 is more consistent   with an role - based heuristic . In other words , we11071do not find that models performing well on both   passive and active control perform well on SPRL .   However , these are separate tasks – thus , it is pos-   sible for GPT-3 and GPT - J to be consistent with   non - role - based heuristics in one task while still   encoding information about agent and patient prop-   erties . Finally , we note that in both Fig . 4 and Fig . 5 ,   GPT-3 performs well on subject control and object   control in the active , which is consistent with it   containing information on agency and patienthood .   6 Related Work   Following the advent of pretrained language mod-   els there has been an explosion of work examining   what kinds of linguistic knowledge such models   contain . Rogers et al . ( 2020 ) provide a comprehen-   sive survey of probing work on BERT , covering   probing for syntactic , semantic , and world knowl-   edge . This line of probing work generally makes   use of linear classifiers on top of frozen represen-   tations , tuned on a training set ( Adi et al . , 2016 ;   Hupkes et al . , 2018 ; Hewitt and Manning , 2019 ) .   In contrast , we follow more recent work in   probing large generative models using cloze - style   prompts and relatively open generation ( Schick and   Schütze , 2021 ) . Such models ( generative and non-   generative ) have been probed for diverse knowl-   edge , including syntax ( Futrell et al . , 2019 ) , sym-   bolic reasoning ( Talmor et al . , 2020 ) , and common-   sense knowledge ( Petroni et al . , 2019 ; Kassner and   Schütze , 2020 ; Sakaguchi et al . , 2020 ) . This has   often been done by recasting benchmark datasets   into text , either with zero examples ( Sanh et al . ,   2021 ) or in the form of in - context learning ( Brown   et al . , 2020 ; Raffel et al . , 2020 ) . Ettinger ( 2020 )   present a suite of comparisons between pretrained   language models and psycholinguistic experiments .   In a similar vein to our work , Lee and Schuster   ( 2022 ) examine GPT-2 ’s performance on reflexive   anaphor agreement in subject and object control   clauses , finding that GPT-2 performs well on ob-   ject control but not transitive subject control ; we   do not examine reflexive anaphora , and expand our   analysis to multiple model classes .   Language models can be sensitive to the for-   mat of a prompt – in order to improve extraction   of relational knowledge from large language mod-   els , Jiang et al . ( 2020 ) propose automatic methods   for mining new prompts and paraphrasing existing   prompts . Similarly , Qin and Eisner ( 2021 ) propose   a method for gradient - based prompt optimization , and Shin et al . ( 2020 ) propose a gradient - based   search for prompt token replacement . As our exper-   iments require a specific prompt syntax , we choose   to instead run prompts across different instruction   styles and name - verb combinations . Large lan-   guage models are also sensitive to the frequency of   terms in their training data ; Razeghi et al . ( 2022 )   show a strong correlation between frequency of   a term in a corpus and performance on tasks re-   quiring that term . This further hightlights how   surprising it is that several models perform better   on subject control than object control .   7 Conclusion   The results in Fig . 4 indicate that differences be-   tween models are not merely of degree , but of kind ,   with groups of models following different patterns ,   many of which are inconsistent with the dominance   of object control in English . This highlights the   pressing need for transparency in the reporting of   model details , and especially of training data , with-   out which it is impossible to hypothesize whythese   differences are observed . We also find that , despite   there being no trainable parameters in the few - shot   setup of Fig . 5 , the models tested are in many cases   predictably sensitive to semantic role information .   Our results in Section 5 suggest that some models   appear able to perform semantic proto - role label-   ing for volition and change of state ( Table 1 ) , but   this ability is not directly tied to the sensitivity to   semantic roles in Fig . 5 . In other words , some mod-   els contain information on semantic roles but may   not recruit that information in producing answers   for control examples . This leads to an interesting   direction of future work in applying causal media-   tion analysis ( Vig et al . , 2020 ; Elazar et al . , 2021 )   to control clauses , to disentangle the information   present in a model from the process by which the   model produces an output .   8 Limitations   Firstly , this work is limited by its focus on English   syntax , models , and examples . Control construc-   tions exist a variety of languages ( Landau , 2001 ) ;   unfortunately , large pre - trained models currently   exist primarily in English alone . Another limita-   tion is the use of fixed prompts : all models tested   were found to be sensitive to the prompt format ,   and while a large number of prompts were explored   by varying instructions , names , verbs , and actions ,   there may be more optimal prompts for the task.11072Our work is also limited by the use of open genera-   tion . While open - ended generation allows for more   flexibility than constrained decoding , it introduces   the challenge of interpreting the model outputs ,   though we do validate the use of open generation   in Section 3.2 . We note that both these limitations   are also common in human subject research .   While our results show that model outputs are   consistent with simpler heuristics , some of which   are observed in human children , we have attempted   to clearly separate this from any anthropomorphic   claims that the models might be actively “ follow-   ing ” such a heuristic . The claim made is that the   associations learned from large - scale pretraining   lead to outputs with patterns that can be described   by simple heuristics , and that those heuristics at   times differ from or resemble heuristics seen in hu-   man data . We also note that while we do not make   strong commitments to any particular account of   human language acquisition , all such accounts dif-   fer substantially from how the models tested are   trained , i.e. on extremely large text - only corpora .   Acknowledgements   Elias Stengel - Eskin is supported by an NSF Gradu-   ate Research Fellowship . This work was supported   by NSF # 1749025 . We would like to thank Made-   laine Thomas and Kate Sanders for feedback on   previous drafts , as well as the ARR reviewers for   their constructive comments and suggestions .   References110731107411075A Additional Controls   To ensure that our results are not biased by the use   of names , we replicated a round of experiments   using common professions rather than names . The   professions used were “ doctor " , “ lawyer " , “ engi-   neer " , “ writer " , “ janitor " , “ bartender " , e.g. “ The   bartender told the engineer to come . ”   Profession prompts were run only with the long   instruction format . In Fig . 7 we include for refer-   ence the results from the name experiments shown   in Fig . 4 but restricted to only the long prompt   format ( Fig . 4 took the best accuracy over short   and long instructions ) . Fig . 8 shows that similar   trends can be seen when using professions rather   than names , confirming that the results are not due   to name - specific processing .   B Licensing   All data and code is released under an MIT license.11076