  Puneet Mathur , Vlad I Morariu , Verena Kaynig - Fittkau , Jiuxiang Gu ,   Franck Dernoncourt , Quan Hung Tran , Ani Nenkova , Dinesh Manocha , Rajiv Jain   ‡University of Maryland , College Park   †Adobe Research{puneetm,dmanocha}@umd.edu{morariu,kaynigfi,jigu,dernonco}@adobe.com{qtran,nenkova,rajijain}@adobe.com   Abstract   We introduce DocTime - a novel temporal   dependency graph ( TDG ) parser that takes as   input a text document and produces a tempo-   ral dependency graph . It outperforms previous   BERT based solutions by a relative 4 - 8 % on   three datasets from modeling the problem as   a graph - network with path - prediction loss to   incorporate longer range dependencies . This   work also demonstrates how the TDG graph   can be used to improve the downstream tasks   of temporal questions answering and NLI by a   relative 4 - 10 % with a new framework that in-   corporates the temporal dependency graph into   the self - attention layer of Transformer models   ( Time - transformer ) . Finally , we develop   and evaluate on a new temporal dependency   graph dataset for the domain of contractual   documents , which has not been previously ex-   plored in this setting .   1 Introduction   Understanding the temporal relations between   events mentioned in a document is an important nat-   ural language task with applications in downstream   tasks such as timeline creation ( Leeuwenberg and   Moens , 2018 ) , time - aware summarization ( Noh   et al . , 2020 ) , temporal question - answering ( Ning   et al . , 2020a ) , and temporal information extraction   ( Leeuwenberg and Moens , 2019 ) . This area of re-   search remains important yet challenging due to   several limitations such as confounded modalities   ( eg . events that are certain to happen vs the ones   that might happen ) , event ambiguity ( eg . agreeing   to terms of a contract vs signing a contract ) and   need for complete annotation of all event pairs for   precise temporal localization ( Yao et al . , 2020a ) .   Early work densely annotated all pairs of events   to address this problem ( Cassidy et al . , 2014 ) , but   was limited to short passages or adjacent sentences   due to the / parenleftbig / parenrightbig   complexity of the task , especially   for long documents . Recently this problem formu-   lation was significantly simplified using temporaldependency trees ( TDT ) ( Zhang and Xue , 2019 )   and temporal dependency graphs ( TDG ) ( Yao et al . ,   2020a ) by only capturing the reference TIMEX or   event to build a dependency graph to capture this   information . This enabled the development of tem-   poral dependency parsers ( Zhang and Xue , 2018a ;   Ross et al . , 2020a ) to infer temporal relationships   more robustly and efficiently .   We introduce DocTime - a state - of - the - art tem-   poral dependency parser that parses document - level   text to produce temporal dependency graphs . Un-   like previous approaches using contextual features   such as BERT(Ross et al . , 2020b ) , our model uti-   lizes a graph network and a novel path prediction   loss to reason over long - range multi - hop depen-   dencies while maintaining global consistency of   temporal ordering of inter - dependent events .   To validate the utility of DocTime and our   generated temporal dependency graph , we go   one step further than prior work and explore the   question of whether temporal dependency graphs   are useful for downstream tasks by introducing   Time - Transformer . It is a framework to incor-   porate temporal dependency graphs into existing   transformer - based architectures without retraining   from scratch . We demonstrate the usefulness of   our proposed Time - Transformer on temporal   NLI ( Vashishtha et al . , 2020 ) and time - sensitive   question answering ( Chen et al . , 2021 ) tasks .   Prior work on temporal relationship extraction   and temporal dependency parsing have been mostly   limited to news ( Zhang and Xue , 2019 ; Yao et al . ,   2020a ; Pustejovsky et al . , 2003a ) , narrative stories   ( Zhang and Xue , 2018b ; Kolomiyets et al . , 2012 )   or clinical notes ( Bethard et al . , 2016 ) . In addi-   tion to experimenting with existing temporal de-   pendency parsing datasets , we introduce a dataset   for temporal dependency graphs in a new domain   - contractual documents , where temporal reason-   ing over events has real world legal and monetary   implications for users.993   Our main contributions include :   •A novel document - level temporal dependency   parser ( DocTime ) that predicts the tempo-   ral dependency graph from text in an end - to-   end manner with a novel path prediction loss ,   which outperforms the current SOTA by a rel-   ative 4 - 8 % on three datasets .   •Time - Transformer , a novel framework   to incorporate Temporal Dependency Graphs   into transformer models for downstream tasks   without needing to retrain from scratch . Re-   sults on natural language inference and ques-   tion answering with a new self - attention mod-   ule show a relative 4%-10 % improvement .   •Development of new document - level ( > 1500   words ) TDG dataset in the domain of contrac-   tual documents ( ContractTDG ) .   2 Related Work   Temporal Dependency Parsing : Previous work   has been devoted to pairwise classification of rela-   tions between events and time expressions , notably   TimeBank ( Pustejovsky et al . , 2003b ) and its exten-   sions like Cassidy et al . ( 2014 ) annotated all rela-   tions . Pair - wise annotation have multiple problems   including polynomial square complexity , global in-   consistencies in predictions due to relation transitiv-   ity and forced annotation of vague relations ( Ning   et al . , 2018 ) . Prior work focuses on extracting tem-   poral relations between event pairs in the same   sentence or adjacent sentences ( Goyal and Durrett ,   2019 ; Ning et al . , 2019a ; Han et al . , 2019a , c , b ,   2020 ; Ballesteros et al . , 2020 ; Zhao et al . , 2020 ) .   TIMERS Mathur et al . ( 2021a ) presented temporal   relation extraction in long document .   Temporal Dependency Parsing ( TDP ) : Tem - poral dependency trees were first proposed by   Kolomiyets et al . ( 2012 ) . ( Zhang and Xue , 2018b )   provided the the earliest TDT corpus on news data   and narrative stories , ( Zhang and Xue , 2019 ) re-   leased the first English TDT corpus . Yao et al .   ( 2020a ) relaxed the assumption of single reference   edge in dependency trees to form the improved   TDG . ( Zhang and Xue , 2018a ) built an end - to - end   neural temporal dependency parser using BiLSTM   and Ross et al . ( 2020b ) improved it further incorpo-   rating BERT . Our approach improves by modeling   complex dependencies and introduces a new re-   source for TDG in contracts .   Linguistically - aware Transformers : Recent   works have investigated using linguistic features   as a prior for Transformer models . Syntax - bert   ( Bai et al . , 2021a ) uses syntactic and constituency   dependency on NLI and GLUE benchmarks . Coref-   BERT Coreference - Informed Transformer ( Liu   et al . , 2021 ) performs coreference - aware dialogue   summarization . Temporal reasoning about event or-   dering can find applications in many tasks such as   summarization ( Noh et al . , 2020 ) , question answer-   ing ( Chen et al . , 2021 ; Ning et al . , 2020b ; Jin et al . ,   2020 ) , commonsense reasoning ( Qin et al . , 2021 ) ,   and natural language inference ( Vashishtha et al . ,   2020 ) . We propose to use TDG as priors to Trans-   former models to make them temporally - aware for   use in downstream tasks .   3DocTime : Document TDG Parsing   Task Formulation : Let document Dbe defined as   a sequence of ntokens [ x , · · · , x ] . The entire   document can be seen as sequence of msentences   [ s , · · · , s ] . Each document has a set of pevents   E= [ e , · · · , e]andqtimexes T= [ t , · · · , t ] ,   where p , q≤n . The creation date of the doc-   ument is represented by timestamp t. Yao994et al . ( 2020a ) defines a temporal dependency graph   ( TDG ) where each timex node always has a ref-   erence timex , which is the most specific narrative   time related to the event ( Pustejovsky and Stubbs ,   2011 ) . If such a narrative time is not available ,   the timex should be anchored to the DCT . An   event node can either have a reference timex or   be connected to a reference event , which is an   event that provides the most specific temporal lo-   cation . The task of temporal dependency graph   parsing of a text document Dresults in a depen-   dency graph G= ( C , V ) , where Crepresents the   set of all events , timexes and the document cre-   ation date ( DCT ) . Vis the set of all edges in the   graph , where each edge represents a temporal rela-   tionship ℜbetween corresponding entity node pair   V={(t , t),(e , e),(e , t)}∀i , j∈C.   Model Overview : Figure 1 shows an overview of   our network architecture for temporal dependency   parsing . We first extract token level BERT features   from the input document , which are then enriched   by three graph networks that encode structural , syn-   tactic , and semantic relationships . This is followed   by Iterative Deep Graph Learning over the TIMEX   and Event entities to learn an initial dependency   structure . This is passed through a Graph U - net to   allow the model to incorporate longer range depen-   dencies before predicting the final temporal depen-   dency graph and relationships . The model is also   trained with a novel auxiliary path prediction loss .   3.1 Feature Encoding   We leverage the pre - trained BERT language model   to obtain the embeddings for each token as fol-   lows : w , w , · · · , w = BERT ( [ x , x , · · · , x ] ) ,   where wis the embedding of the token x. As   document sequence lengths can be larger than 512 ,   we use a sliding window encoding technique to   encode whole documents . We average the embed-   dings of overlapping tokens of different windows   to obtain the final representations . These token rep-   resentations are enriched with slightly enhanced   variants of the structural ( G ) , syntactic ( G )   and semantic ( G ) graphs utilized by ( Mathur   et al . , 2021b ) for document - level temporal relation-   ship extraction . The key differences are the use of   BERT - GCN ( Lin et al . , 2021 ) to combine contex-   tual and structural graph features , the addition of   co - reference relationships to the syntactic graph ,   and the use of a hypergraph convolution ( Bai et al . ,   2021b ) to allow for token level features in the se - mantic graph . All aspects of these features and the   changes are presented in Appendix B.   3.2 Temporal Dependency Prediction   We combine the learned representation for each en-   tity node ( timex , event , DCT ) by concatenating the   node embeddings learned from structural , syntacti-   cal and semantic graphs to obtain a D - dimensional   feature vector for each of zentities in the document   given by F(w ) = g⊕g⊕g , where ⊕rep-   resents concatenation . We retain only the enriched   node embeddings for each word . We then utilize It-   erative Deep Graph Learning ( IDGL)(Chen et al . ,   2020 ) to dynamically learn an initial dependency   graph structure from the combined node embed-   dings . Given a noisy graph input feature matrix   F∈R , IDGL produces an implicitly learned   graph structure G={A , F , 𭟋}with a jointly   refined corresponding graph node embeddings F   with adjacency matrix Aby optimizing with re-   spect to downstream link prediction task 𭟋be-   tween entity nodes .   3.2.1 Graph U - net For Higher Level Features   The Graph U - net ( Gao and Ji , 2019 ) is a U - shaped   graph encoder - decoder architecture containing two   down - sampling graph pooling ( gPool ) layers and   two up - sampling graph unpooling ( gUnpool ) lay-   ers with skip connections . gPool layers reduce the   size of the graph to encode higher - order features ,   while the gUnpool layer restores the graph into its   higher resolution structure , thereby promoting in-   formation exchange between entity pairs through   an enlarged receptive field . Each graph pooling and   unpooling layer is followed by a GCN layer to im-   plicitly capture the topological information in the   input graph . Taking the dynamically learned graph   structure G , a graph embedding layer converts   input node features F’into low - dimensional rep-   resentations that are then passed through a graph   U - net encoder - decoder ℧ to acquire entity - level   relation matrix Y= ℧ (F’),Y∈R.   3.2.2 Temporal Dependency Link Prediction   and Relation Classification   Given entity adjacency matrix Aand entity - level   relation matrix Y , we use a bilinear function to   map them to link and relation probabilities Z   andZ , respectively . Formally , we have Z=   σ(YWY+b)andZ = σ(AWA+b ) , where995   W , W , b , b∈Rrepresent learnable pa-   rameters . This is followed by a Softmax layer for   link prediction and relations classification .   3.3 Training DocTime   Path Reconstruction Loss : In a document - level   temporal parsing setup , the majority of node pairs   may not have any ground truth link or temporal   relation . Graph representation learning methods   universally model relations between all entity pairs   regardless of whether the entity pair has any rela-   tionship , leading to dispersion of attention in learn-   ing most non - existent edge connections . We pro-   pose path reconstruction loss L , which forces   the model to pay more attention to learn entity pairs   with relationships rather than ones without relation-   ships . Equation 1 gives the cross entropy loss over   all direct edge connection between all pairs of en-   tities , where rindicates the relation between the   entity pair and P(r)is probability of relation label   r. Path reconstruction loss Lmodifies the cross   entropy loss Lfunction as shown in Equation 2   by sampling all nentity pairs and maximizing the   probability of the shortest dependency path N(ϕ )   between the entity pair nodes . Finally , the path re-   construction loss and the existing classification loss   are added as the training objective for DocTime ,   given by L = L+L.L=−1 / summationtextN / summationdisplay / summationdisplay{rlogP(r )   + ( 1−r ) log(1 −P(r))}(1 )   L=−1 / summationtextN / summationdisplay / summationdisplay{rlogN(ϕ )   + ( 1−r ) log(1 −N(ϕ))}(2 )   Multi - task Training : Dependency link prediction   and entity - level relation classification are corre-   lated tasks and reinforce each other . We use multi-   task training to optimize both tasks simultaneously   using the path prediction cross entropy loss . The fi-   nal optimization uses a weighted sum of the depen-   dency link prediction loss and entity - level relation   classification loss L = λL+(1−λ)L , where the   weighting factor λis a hyperparameter .   4 Time - Transformer   We would also like to understand our temporal de-   pendency parsing can be useful for downstream   tasks requiring temporal reasoning . Here we intro-   duce the Time - Transformer , which allow a TDG   generated by DocTime to be combined with state-   of - the - art transformer models for temporal tasks .   The Time - Transformer augments the flow of infor-   mation in a Transformer network via a temporally-   informed self - attention mechanism . We first for-996mulate the Time - Transformer architecture in § 4   and then construct of temporally - informed atten-   tion layers in § 4 .   Architecture : Time - Transformer was motivated   by recent work incorporating syntax ( Bai et al . ,   2021a ) or co - reference graphs(Liu et al . , 2021 )   into the transformer architecture to improve down-   stream tasks . In each case , these approaches   encode additional knowledge from the sparse   graphs as a masked self attention layer into the   transformer . Figure 2 shows the architecture of   Time - Transformer incorporating temporal knowl-   edge into the self - attention layer during fine-   tuning of the Transformer model . Input text is   converted into a temporal dependency graph us-   ingDocTime parser . The graph is then con-   verted into a set of masks that encodes the tem-   poral relationship between each entity ( i.e. Af-   ter ) explained in more detail in the next sec-   tion : Temporally - informed Self - Attention . The in-   put embedding ( token+positional+attention masks )   is passed through the Time - Transformer model   which modifies the self - attention layer of the stan-   dard Transformer architecture with a temporally-   informed self - attention layer to be fine - tuned on   downstream tasks .   TISA : Temporally - informed Self - Attention :   The TDG produced by DocTime is sparse and   to effectively utilize the graph extracted by the tem-   poral dependency parser for longer range temporal   relationships , we utilize Kself - attention layers that   encode the temporal relationship if traversing K   hops in the TDG as shown in 2 . More formally   starting from node A , the minimum number of   hops ( k ) required to reach another node Bcan   be regarded as k - hop distance between AandB ,   written as k - hop(A , B ) . We create Kmasks to   represent the ( k)-hop distance between two nodes   to allow the model to aggregate information across   longer ranges in the TDG . Specifically , a mask   M∈ { 0,1,2 , · · · , r}denotes if there is a re-   lation between entity iandj , and nis the number   of tokens in the input text . The value of the mask   is the relationship type for iandj . It is found by   inferring the relationship using Allen ’s interval al-   gebra ( Allen , 1983 ) and is set to 0 if there is no   relationship or set to " Overlap " if there is a conflict .   We adopt a soft - mask learning strategy to enable   the self - attention layer to re - weight the importance   of each mask and avoid the problem of vanishing   gradient . A hyperbolic feed - forward layer is usedto learn the mask weights as research has shown it   can avoid distortion of the feature space in graph   representations ( Ganea et al . , 2018 ) . The value   ofKis a hyperparameter that can be customized   according to the nature of input dependency graph .   Training Time - Transformer : For each   dataset , we optimize the hyper - parameters of   Time - Transformer through grid search on the   validation data . In all our experiments , we limit the   maximum value of k - hop to 15 . Detailed settings   can be found in the appendix .   5 Experiment   5.1 Temporal Graph Parsing Datasets   We train and evaluate DocTime on three datasets .   First is the Temporal Dependency Graphs ( TDG )   dataset ( Yao et al . , 2020a ) made up of 500   Wikinews articles annotated with document - level   temporal dependency graphs . Second is the Tem-   poral Dependency Trees ( TDT ) dataset Zhang   and Xue ( 2019 ) made from 183 documents derived   from TimeBank ( Pustejovsky et al . , 2003a ) anno-   tated with a temporal dependency tree structure .   The third dataset we created as part of this paper   and is describe in more detail below .   Contract - TDG : Understanding the temporal rela-   tionship of events in contracts is an important busi-   ness problem , where understanding event timelines   can have legal and monetary consequences . Pre-   vious work on temporal relationships has largely   focused on clinical , news or narrative text , whereas   to the best of our knowledge the contractual do-   main has not been explored for this problem . To   construct this dataset , we used 100 contracts from   the Atticus contracts dataset(Hendrycks et al . ,   2021 ) , which were sourced from public domain   SEC contracts . Due to the multi - page length of   these documents , we limited the annotations to the   first 1500 words . We did not include definition   sections , since they did not contain many events   of interest for this task . The documents have a   70 - 10 - 20 split for training , validation , and testing .   To obtain the TDG annotations required for our   task , we followed the 5 steps procedure outlined   by the original TDG dataset in ( Yao et al . , 2020b ):   ( i ) TIMEX Identification ( TE ) , ( ii ) Identifying ref-   erence times for TE , ( iii ) Event identification , ( iv )   Identifying reference times for events , ( v ) Identify-   ing reference events for events . Document Creation997   Times ( DCT ) were provided as effective dates in   the ATTICUS corpus .   Similar to ( Yao et al . , 2020b ) for tasks 1 ( TE )   and 3 ( Event ID ) , we used the Mechanical Turk   platform to obtain two annotations to validate text   spans of noisy TIMEXes extracted by HeidelTime   software(Strötgen and Gertz , 2013 ) and verbs that   were possible events . Disagreements were resolved   by an expert annotator . However , for the reference   tasks , we decided against using Mechanical Turk   due to the difficulty and length of the contracts as   well as the lower agreement faced by the original   TDG system for the last two tasks . We instead   used the BRAT annotation tool(Stenetorp et al . ,   2012 ) with an expert annotator for tasks 2,4 , and 5 ,   following the ( Yao et al . , 2020b ) guidelines . Con-   tractTDG is annotated for four temporal relations -   after , before , overlaps , and includes .   Table 1 compares the data statistics of the Con-   tractTDG to previous temporal relationship andtemporal dependency corpora . Even though this   dataset has many fewer documents than the TDG   dataset , it has a large number of TIMEX , Events ,   and Temporal relationships due to the document   length . Table 2 reports the F1 IAA metrics for Con-   tractTDG dataset to directly compare to the original   TDG dataset . For Tasks 1 and 3 we report IAA F1   for the two crowd sourced worker annotations and   for the relationship tagging tasks ( 2,4,5 ) , we report   IAA metrics calculated on the test postion ( 20 % of   the data ) that was reviewed by two experts . The   agreement is slightly lower for the TIMEX / Event   identification tasks but higher for the three relation-   ship tasks . We evaluate DocTime for dependency   structure as well as structure+relation prediction   for both development and test splits .   5.2Time - Transformer Experiments for   Downstream Tasks   We adopt Time - Transformer on BERT ( De-   vlin et al . , 2019 ) , RoBERTa ( Liu et al . , 2019a ) , Big-   Bird ( Zaheer et al . , 2020a ) and FiD ( Izacard and   Grave , 2021 ) for evaluation on two downstream   tasks in § 6.2 . We utilized the official checkpoint   for each pre - trained language model as provided   by respective authors . First , we test Time - BERT   andTime - RoBERTa on Temporal NLI dataset ,   which consists of 5 sub - datasets ( Vashishtha et al . ,   2020 ) to study the effect of temporal reasoning   for predicting event ordering and duration . Sec-   ond , we run experiments on the TimeQA dataset   ( Chen et al . , 2021 ) to evaluate the performance   ofTime - BigBird andTime - FiD for the long-   document question - answering task . We report Ex-   act Match ( EM ) and F1 scores as evaluation metrics   on dev and test sets of easy and hard versions.998   6 Results and Analysis   6.1 Temporal Graph Parsing   Performance of DocTime w.r.t . baselines : Table   3 compares the performance of DocTime against   other baseline methods on TDT , TDG and Con-   tractTDG . We also provide a majority baseline Con-   tractTDG to evaluate whether the methods work   better than a random label assignment as imple-   mented in ( Yao et al . , 2020a ) . We also include   the two current SOTA approaches for temporal de-   pendency parsing : The BiLSTM attention - based   Neural Ranking Parser proposed by ( Zhang and   Xue , 2018a)and the BERT Ranking Parser ( Ross   et al . , 2020b ) on each dataset . We also report   results for a logistic regression baseline proposed   by ( Zhang and Xue , 2018a ) . Results in Table 3   show that DocTime outperforms both Neural and   BERT Ranking Parser by a significant margin on   the TDT ( 2 - 4 % ) TDG ( 5 - 6 % ) and ContractTDG   ( 3 - 4 % ) datasets . We believe its primarily because   they formulate temporal dependency parsing as a   ranking task designed to select the best reference   event / timex for each node . However , TDG pars-   ing requires the model to be able to reason over   multiple dependencies originating from each node   while maintaining global consistency of temporal   ordering of inter - dependent events . We perform   experiments for dependency structure prediction   and structure+relation prediction and find that pre-   dicting labeled dependency edges is a much more   challenging task across all datasets . DocTime   achieves state - of - the - art performance on all three   datasets ( see bold ) , and shows that it can success-   fully handle document - level long - range dependen-   cies in the challenging ContractTDG dataset from   the 6 - 12 % relative improvement over the BERT   based ranking parser . A more detailed analysis of   performance per temporal relationship type can be   found in the Appendix , where largest gains are seen   for event - event pairs .   Ablation Study of DocTime : To assess the con-   tribution of structure and syntactic and semantic   graph features , we performed ablation experiments   as reported in Table 3 highlighted in red . We also   analyzed the effect of different types of training   loss . We observe that removing the semantic graph   consistently degrades performance , indicating the   need for hypergraph learning over temporal argu-   ments and RST features to capture document - level   discourse relations . We see that removing structure   graph reduced the performance to below the BERT   Ranking Parser , as DocTime leverages BERT ’s   contextual learning through a structural graph . Syn-   tactic graph adds incremental value to DocTime   due to its relational learning of syntactic depen-   dencies within each sentence through relational   GCN . We evaluated the model performance in case   all edges of the TDG are used for one forward   pass and call it ” Graph Prediction ” . Training the   model by evaluating a single edge in one pass ( sim-   ilar to temporal relation prediction in ( Pustejovsky   et al . , 2003b ) is referred to as ” Pairwise Predic-   tion " . We explore the impact of different training999   losses for the proposed model ( Table 3 , highlighted   ingreen ) . Learning DocTime by propagating   losses over the entire document graph severely de-   teriorates model performance as the model has very   limited training documents samples ( 182 for TDT ,   400 for TDG , 80 for ContractTDG ) . Our proposed   path prediction loss shows superior performance   over pairwise link prediction as it jointly learns the   relation label between a pair of nodes as well as the   shortest dependency path linking them . As a result ,   the model can recover from structure prediction er-   rors between nodes by learning an alternative path   reconstructed through multi - hop connections .   6.2 Application of Temporal Dependency   Parsing for downstream tasks   We train the DocTime model on the TDG corpus ,   which can be used to infer a temporal dependency   graph from raw text samples . We extract events   and timexes using CAEVO ( Chambers et al . , 2014 )   for all data samples in train , validate , and test . The   temporal dependency graph acquired for each docu-   ment is used as a prior for Time - Transformer   to perform downstream tasks .   Performance of Time - Transformer on Tem-   poral NLI : The temporal NLI task requires a   model to identify the semantic relationship ( en-   tailed , not - entailed ) between the context and cor-   responding hypothesis sentence based on tem-   poral information from text . The temporal de-   pendency graphs extracted using the DocTime   trained on the TDG corpus are used as prior for   Time - BERT for entailment classification . Table 4   shows the test accuracies of Time - BERT - large ,   Time - RoBERTa - large and other competi - tive baselines [ ( Iyyer et al . , 2015),(Conneau   et al . , 2017 ) ] reported by ( Vashishtha et al . ,   2020 ) . The temporal information prior proposed   inTime - Transformer helps the BERT and   RoBERTa models perform much better on the NLI   task . The accuracy improved by 1.5 - 2.3 F1 points   by applying our framework on the RoBERTa model   across the five subsets . We observe the perfor-   mance gain in the case of the Euclidean version of   Time - RoBERTa to be modest as compared to its   hyperbolic counterpart .   Performance of Time - Transformer on   TimeQA : The TimeQA task focuses on under-   standing the time scope of facts in the long text   followed by answering questions conditioned   on the query and the document using implicit   temporal information . We then apply the DocTime   model output trained on the TDG corpus to the   Time - Transformer framework on BigBird   and FiD language models for long document   question answering task . Following ( Chen et al . ,   2021 ) , we experiment with three variants of   pre - trained settings : ( 1 ) fine - tuned on the TimeQA   training set ; ( 2 ) fine - tuned on NQ / TriviaQA data   ( 3 ) fine - tuned on NQ / TriviaQA data and TimeQA .   Table 5 shows the effectiveness of   Time - BigBird andTime - FiD in consis-   tently outperforming their corresponding baselines   in all three settings . More specifically , we see a re-   altive gain of 10 - 14 % in F1 and exact match scores   ( EM ) for both easy and hard sections of the dataset .   It is impressive to note that the improvements due   to the Time - BigBird andTime - FiD models   are steady with different pre - training setups with   the addition of only a few extra parameters to the   baseline model . An important observation here is   that the Euclidean versions of Time - BigBird   andTime - FiD show persistent performance   deterioration across all settings for TimeQA . We   attribute this phenomenon to our initial hypothesis   behind using hyperbolic operations in the proposed   Temporally - informed self attention ( TISA ) layer .   As the text length grows , the complexity of   geometric operations increases , leading to vectorial   distortions in Euclidean spaces ( Ganea et al . , 2018 ) .   This is remedied by hyperbolic transformations   of masked self - attention learning in the proposed   Time - Transformer .   Our experiments provide evidence that tempo-   ral dependency graphs extracted using DocTime   and then utilized as a prior by temporally-1000   informed Transformer architectures such as   Time - Transformer can improve the perfor-   mance of several downstream tasks that require   temporal reasoning at the sentence - level as well as   at the document - level .   Impact of Long - term Dependency on   Time - Transformer performance : We   plot Fig . 4 to understand the capability of Trans-   former models to handle the long - term dependency   in temporal reasoning on the TimeQA dataset . Plot   shows the exact match ( EM ) accuracy vs length of   the input document for hard samples . We use Big-   Bird and FiD models fine - tuned on NQ + TimeQA   as backbone models . BigBird ’s performance   degrades rapidly as the length increases to over   5000 tokens , while the FiD ’s performance is quite   uniformly distributed across different document   lengths due to it ’s strong capability to deal with   long - term dependency . Time - BigBird and   Time - FiD follow a similar trend and maintain   steady improvements over their corresponding   baseline models with increasing in input lengths .   Space complexity analysis : We choose RoBERTa - base as the base model to analyze the space com-   plexity . Liu et al . ( 2019b ) reported the number   of trainable parameters in RoBERTa - Base to be   about 123 million . Time - RoBERTa introduces   an additional 2 million parameters in total due to   k - hop mask learning in the TISA layer . Therefore ,   Time - BERT adds few parameters to the base model   without affecting its original space complexity .   Time Complexity analysis : We assume the num-   ber of tokens in each sentence to be nand extract k-   hop mask matrices from a text document is O(n )   in the online inference phase . The time complex-   ity of the Transformer embedding lookup layer   isO(n ) . The TISA layer calculates the attention   score in O(KDn)for both QKand learns the   mask weights using a hyperbolic feedforward layer   ( MW ) , where Dis dimension of Qand K is   the number of sub - networks . The time complex-   ity of the Time - BERT remains the same for small   enough value of k(k≤15 in experiments ) .   7 Conclusion   We present DocTime , a new temporal dependency   parsing approach that improves upon previous ap-   proaches by integrating longer term temporal infor-   mation through a graph network with a novel path   prediction loss . Additionally , we are able to show   how a TDG can be incorporated into Transformer   networks with Time - Transformer to improve on   down stream tasks for NLI and question answering .   Finally we introduce a TDG dataset in a new do-   main ( Contractual documents ) to expand research   in this temporal reasoning to a new application   domain . Future works will aim to explore more   ways for integrating temporal dependency graphs   into neural architectures across different applica-   tion domains . In future , we would like to explore   temporal event mining to aid various social media   applications such as improving hate speech detec-   tion ( Mathur et al . , 2018b ; Chopra et al . , 2020 ) ,   analyzing temporality in suicidal ideation detection   ( Mishra et al . , 2019 ; Mathur et al . , 2020 ) and abuse   detection ( Gautam et al . , 2020 ; Sawhney et al . ,   2021 ) . The proposed Time - Transformer can find   applications in augmenting financial tasks ( Sawh-   ney et al . , 2020 ) , affective computing ( Mittal et al . ,   2021 ) , and AI for social good ( Mathur et al . , 2018a )   with temporal common sense reasoning.1001References1002100310041005A Ethics Statement   We utilize two publicly available datasets - TDT   and TDG for evaluating temporal dependency   parser . We also curated dataset for TDG on contract   documents . We source these contract documents   from a publicly available resource - ATTICUS . We   repurpose the document in this dataset for our task   and provide new annotations . ContractTDG dataset   does not violate any privacy as these documents   are already in public domain . There is no human   bias involved in such documents as they are busi-   ness contracts filed on the SEC website . These   documents do not restrict reuse for academic pur-   poses and any personal information was already   redacted before their original release . All docu-   ments and our experiments are restricted to English   language . Temporal NLI and TimeQA datasets that   are publicly available for research purposes . The   crowd workers are paid a fair wage . There was no   sensitive data involved in the studies .   B Details on Graph Feature Extraction   B.1 Structural Graph Features   The Structural Graph ( G ) enriches the token   level features with a hierarchical textual struc-   ture formed by grouping word tokens into lists   of sentences that bind together to form the text   document . Prior work has shown that transduc-   tive graph learning over Gcan help learn the   long range word - word dependencies set several   sentences apart through hierarchical text model-   ing ( Yao et al . , 2019 ) . The directed edges of   the Structural Graph encode the following rela-   tionships : ( 1 ) Document - Sentence Affiliation ,   which connects each document - node to a sentence-   node ; ( 2 ) Sentence - Word Affiliation , which joins   each sentence node to its constituent word nodes ;   ( 3 ) Sentence - Sentence Adjacency and(4 ) Word-   Word Adjacency , which preserve sequential or-   dering for consecutive sentence and word nodes ,   respectively . For the structural graph , a sentence   node embedding sis obtained by passing sen-   tences through a pre - trained SentenceBERT model   ( Reimers and Gurevych , 2019 ) and the document   node embedding Dis calculated as the average of   all sentence embeddings ( D=/summationtextv ) .   BertGCN ( Lin et al . , 2021 ) combines the advan-   tages of both large - scale pre - training and transduc-   tive learning . We input the structural graph Gto BertGCN modelwhere each node represents a   word , a sentence or the document . BertGCN pro-   cesses the input node feature matrix sequentially   through a Bert model to fine - tune each node to   learn local contextual representations . This is fol-   lowed by passing the learned node feature matrix   through two layers of graph convolution to take   advantage of global influence propagation through   graph edges across multi - hop nodes .   B.2 Syntactic Graph Features   Syntactic cues are useful priors for learning based   NLP tasks ( Kiperwasser and Ballesteros , 2018 ) .   Pre - trained transformer models can capture certain   syntactic information implicitly ( Hewitt and Man-   ning , 2019 ) but Jawahar et al . ( 2019 ) showed that   BERT needs to be trained with deeper layers for   handling harder cases involving long - distance de-   pendency information . Moreover , past studies have   pointed to the existence of multi - hop coreferring   expressions in document - level text due to anaphora   and cataphora ( Joshi et al . , 2020 ) .   Gis made of separate nodes to represent each   constituent word win the document . For each doc-   ument , there is also a set of co - reference clusters   { ∁,∁,···,∁ } referring to the same entities in the   graph . We define four types of directed edges in   Gas described below where ξdenotes the set   of syntactic dependency arcs inside sentences , S   denotes root of the sentence in which wbelongs ,   andS→Srepresents whether sentences con-   taining words wandware adjacent .   The first two edge types are introduced to allow   information flow along and against syntactic arcs   between intra - sentential dependency relations to en-   rich contextually learned embeddings of each word .   We connect parse tree roots of adjacent sentences to   encode document level long - range syntactic relat-   edness between sentences . We add an undirected   edge between word nodes if both belong to the   same co - reference cluster . Inspired by ( Kipf and   Welling , 2016 ) , self - loop edges are added for better   message passing iterations . Gis instantiated1006as a gated variant of Weighted Relational Graph   Convolutional Network ( WR - GCN ) ( Zhang et al . ,   2020 ) with k - layers . WR - GCN can able to model   diverse relations in a heterogeneous graph by treat-   ing different types of edges with unequal weights   assigned during message passing .   B.3 Semantic Graph Features   Semantic Role Labeling ( SRL ) parses text se-   quences to recognize the predicate - argument struc-   ture in the sentence to answer who didwhat and   when . Anchoring verb events to their temporal   argument spans extracted from semantic parsing   helps infer event relationships with their associ-   ated time expressions . This can be complemented   by discourse features in the form of RST con-   nections can help leverage long - range document-   level interactions between phrase units ( Bhatia   et al . , 2015 ) and identify background - foreground   events(Aldawsari et al . , 2020 ) and improve tempo-   ral relationship parsing ( Mathur et al . , 2021b ) . We   utilize Document - level Rhetorical Structure Theory   ( RST ) parser ( Shi et al . , 2020 ) to organize contigu-   ous semantic text spans of a document into a hi-   erarchical dependency structure labeled with their   rhetorical relations .   Gconsists of individual nodes for each con-   stituent word win the document . Discourse units   and temporal arguments may span several word   tokens { w , w,···w } . We add two types of di-   rected edge connections between - ( 1 ) event verb   predicate - temporal argument edge ( ε ) such that   ( w→ { w , w,···w } ∈ε ) ; ( 2 ) Rhetorical pair   edges ( ε ) labelled by the type of the rhetorical rela-   tion({w , w,···w } → { w , w,···w } ∈ε ) .   The nature of edge connections in Gextends   beyond pairwise interactions as each edge may con-   nect to one or more word nodes . Hence , we for-   mulate the semantic graph as a hypergraph ( Feng   et al . , 2019 ) where an edge can join an arbitrary   number of vertices . We construct G=(ν , ε , W )   where νis the set of all word nodes w , andεis the   subset of hyperedges such that ε = ε∪ε . Each   hyperedge eis assigned a positive weight corre-   sponding to the type of edge relation and is stored   in a diagonal matrix W∈ ℜ. The semantic   graph is learned using hypergraph convolution lay-   ers ( Bai et al . , 2021b ) to obtain discriminative nodeembeddings for each word node .   C Training Setup   Hyperparameter : Hyper - parameters for   DocTime were tuned on the respective vali-   dation set to find the best configurations for   different datasets . We summarize the range   of our model ’s hyper parameters such as :   number of hidden layers in WR - GCN / BERT-   GCN / HyperGraphGCN { 1,2,3 } , size of hidden   layers in WR - GCN / BERT - GCN / HyperGraphGCN   { 64,128,256,512 } , BERT embedding size ( 768 ) ,   dropout δ∈ { 0.2,0.3,0.4,0.5.0.6 } , learning rate   λ∈ { 1e−5,1e−4,1e−3,1e−2,1e−1 } ,   weight decay ω∈ { 1e−6,1e−5,1e−4,1e−3 } ,   batch size b∈ { 16,32,64}and epochs ( ≤100 ) ,   ϵ-sparsity ∈[0,1 ] , IDGL smoothness ratio=0.5 ,   IDGL sparsity ratio=0.5 , IDGL connectivity   ratio=0.5 , size of hidden layers in Graph U - net   { 64,128,256,512 } .   Loss Function and Inference :   Time - Transformers are trained using   Cross Entropy loss with Adam optimizer . Across   both TempNLI and TimeQA datasets , we found   the best results correspond with the use of Adam   optimiser set with default values β= 0.9 ,   β= 0.999,ϵ= 1e−8 , weight - decay of 5e−4   and an initial learning rate of 0.001 .   DocTime uses cross entropy loss for structure   prediction . For structure+relation classification , it   uses the path prediction loss as defined in Method-   ology .   Computing Infrastructue : DocTime and   Time - Tranformers are written in PyTorch li-   brary and were trained on 4 and 6 Nvidia GeForce   RTX 2080 GPU , respectively . Average Run-   time : DocTime takes a maximum of approx-   imately 5 hrs to train once on TDG datasets .   Time - BERT , Time - RoBERTa take 3 hrs to fine-   tune on TempNLI . Time - BigBird , Time - FiD   takes 8,12 hours to fine - tune , respectively .   Dataset Access   Links to download TDT dataset :   Link to download TDG dataset :   Link to download Temporal NLI dataset :   Link to download TimeQA dataset:1007   D Hyperparameters   Table 8 show the Training hyperparameters of   DocTime for TDT , TDG , ContractTDG datasets .   E More Results   Performance across different relation types :   We analyze the benefits of DocTime for differ-   ent types of relations in document - level TDG   datasets in Table 7 . We report F1 scores for struc-   ture+relation prediction for timex - timex , event-   timex and event - event pairs . We observe a rela-   tively smaller performance gap between the BERT   Ranking parser and DocTime for event - timex   relations . However , DocTime shows relatively   stronger performance for event - event relations .   This phenomenon can be attributed to the fact that   both datasets tend to have event - event links be-   tween event pairs that are on an average closer   in word distance , whereas a higher ratio of event-   timex and timex - timex pairs are several sentences   apart . DocTime can integrate long - range inter-   dependencies between entity pairs that are several   sentences ( or paragraphs in Contract TDG ) apart.10081009