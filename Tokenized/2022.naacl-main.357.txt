  Lulu Zhao , Fujia Zheng , Weihao Zeng , Keqing He , Weiran Xu ,   Huixing Jiang , Wei Wu , Yanan WuPattern Recognition & Intelligent System LaboratoryBeijing University of Posts and Telecommunications , Beijing , ChinaMeituan Group , Beijing , China   { zhaoll,fujia_zheng,ZengWH,xuweiran}@bupt.edu.cn   { kqin}@bupt.cn , { jhx_bupt}@163.com , { wuwei19850318}@gmail.com   Abstract   The most advanced abstractive dialogue sum-   marizers lack generalization ability on new   domains and the existing researches for do-   main adaptation in summarization generally   rely on large - scale pre - trainings . To explore   the lightweight fine - tuning methods for domain   adaptation of dialogue summarization , in this   paper , we propose an efficient and general-   izable Domain- Oriented Prefix - tuning model ,   which utilizes a domain word initialized prefix   module to alleviate domain entanglement and   adopts discrete prompts to guide the model to   focus on key contents of dialogues and enhance   model generalization . We conduct zero - shot ex-   periments and build domain adaptation bench-   marks on two multi - domain dialogue summa-   rization datasets , TODSum and QMSum . Ad-   equate experiments and qualitative analysis   prove the effectiveness of our methods .   1 Introduction   Abstractive dialogue summarization task aims to   distill the most critical information in a conversa-   tion to produce a concise version , involving chit-   chat ( Gliwa et al . , 2019 ; Chen and Yang , 2020 ) ,   meeting(Zhong et al . , 2021 ) , customer service ( Liu   et al . , 2019 ; Zou et al . , 2021b ) , and task - oriented   dialogue scenarios ( Zhao et al . , 2021b ) . Compared   to the single - speaker texts , summarizing a dialogue   presents a unique set of challenges , such as un-   structured expressions and information sparsity is-   sues . Recently , large - scale generative pre - trained   models ( Lewis et al . , 2020 ; Liu and Lapata , 2019 )   have promoted the development of abstractive di-   alogue summarization but they all require exten-   sive human - annotated golden summaries , which   makes them not scalable to new domains where   only few / no labeled data is available . Considering   that real - world applications often face the problemFigure 1 : An example of TODSum dataset ( Zhao et al . ,   2021b ) with dialogue state and an example of QMSum   dataset ( Zhong et al . , 2021 ) with query . Note that di-   alogue state and query are existing characteristics of   these two datasets , respectively .   of data in the new domain , it is vital to develop   low - resource dialogue summarization models for   the target domain by leveraging limited annotated   data of source domains .   Therefore , we try to explore the efficient domain   adaptation of dialogue summarization models from   the source domain Dto the target domain D ,   where Donly has limited annotated summaries   andDhas few / no labeled data . There exist some   domain adaptation approaches that focus on con-   tinual pre - trainings using some large domain / task-   related corpora . Yu et al . ( 2021 ) added multiple   pre - training stages both on source domains and   target domains . Further , Zou et al . ( 2021c ) de-   composed the pre - training into three procedures ,   i.e. , the pre - training of encoder , decoder , and the   combined encoder - decoder model . Fabbri et al .   ( 2021 ) constructed pseudo summaries based on ex-   ternal Wikipedia data to simulate characteristics   of target dataset . Note that all these methods re-4848quire time - consuming pre - trainings or large - scale   external corpora . They only focus on the heavy   pre - training stage rather than the lightweight fine-   tuning , which makes it labor - expensive and envi-   ronmentally unfriendly ( Schwartz et al . , 2020 ) to   practical applications .   Different from existing works that adopt general   pre - trainings on the large - scale external corpora ,   we focus on exploring efficient fine - tuning methods   specifically targeted at domain adaptation for the di-   alogue summarization task . We consider the follow-   ing key principles while designing our methods : ( 1 )   Efficiency : We do not use any external data or pre-   training and aim at leveraging efficient fine - tuning   mechanisms based on existing summarization mod-   els . ( 2 ) Domain Disentanglement : Traditional   models often memorize excessive knowledge from   Dand generate wrong summaries containing spe-   cific domain words in D. We aim to disentangle   shared domain knowledge from D. ( 3 ) General-   ization : Models often learn specific features of the   source domain , making it difficult to generalize in a   new domain ( Peng et al . , 2019 ) . For example , mod-   els learn the surface language style specific to D   rather than adapting the way of saliency estimation   and summary generation to D. We encourage the   summarizer to only focus on generic key contents   rather than domain - specific attributes .   To be consistent with above principles , we pro-   pose a lightweight and efficient Domain- Oriented   Prefix - tuning method , DOP , for domain adaptation   of dialogue summarization . For efficiency , we fo-   cus on fine - tuning summarization models instead of   performing pre - trainings like existing works , which   reduces expensive and time - consuming computa-   tion . For domain disentanglement , we design a   domain - oriented prefix module , which contains a   novel prompt initialization mechanism . Concretely ,   we use domain words extracted by unsupervised   LDA ( Hoffman et al . , 2010 ) to initialize continuous   prompt vectors and fit the outputs of MLP and pre-   computed BART to obtain initial parameters and   representations of the prefix module . We also add a   domain - oriented prefix sequence of key - value pairs   to augment the classical attention layer , which is   independently applied to all Transformer layers of   pre - trained models to elicit the knowledge inter-   actively and achieve overall optimization . In this   case , different domain words from DandDcan   induce relevant domain knowledge while adapting   to a new domain . For generalization , we constructdiscrete prompts using dialogue states or queries ,   as shown in Figure 1 , to guide the model to focus   on key contents in dialogues and enhance general-   ization capability on unseen domains . Considering   there is no unified and practical benchmark for   domain adaptation of dialogue summarization ,   we build domain adaptation benchmarks based on   two existing multi - domain summarization datasets   TODSum ( Zhao et al . , 2021b ) and QMSum ( Zhong   et al . , 2021 ) . Extensive experiments demonstrate   the benefits of our methods both in zero - shot and   few - shot settings for domain adaptation .   Our contributions are threefold : ( 1 ) To the best   of our knowledge , we are the first to explore fine-   tuning methods for domain adaptation of dialogue   summarization task , and establish two practical and   comprehensive benchmarks for TODSum and QM-   Sum datasets . ( 2 ) We propose a lightweight and   efficient Domain - Oriented Prefix - tuning model ,   with domain word initialized prefix and discrete   prompts , to elicit knowledge from large - scale pre-   trained models interactively . ( 3 ) We conduct suffi-   cient experiments and qualitative analysis to prove   the effectiveness of our methods and discuss cur-   rent challenges of domain adaptation for dialogue   summarization .   2 Related Work   Abstractive Dialogue Summarization Dia-   logue summarization has drawn much attention   recently . For chit - chat scenarios , researchers   improved the performance on SAMSum dataset   ( Gliwa et al . , 2019 ) via topic word information   ( Zhao et al . , 2020 ; Liu et al . , 2021 ) , conversa-   tional structures ( Chen and Yang , 2020 , 2021 ) ,   personal named entity planning ( Liu and Chen ,   2021 ) , and semantic slots ( Zhao et al . , 2021a ) . Liu   et al . ( 2019 ) , Zou et al . ( 2021a , b ) , and Lin et al .   ( 2021 ) proposed customer - service dialogue summa-   rization datasets under diverse business scenarios .   Besides , meeting transcripts , such as AMI ( Car-   letta et al . , 2005 ) , ICSI ( Janin et al . , 2003 ) , Media-   Sum ( Zhu et al . , 2021 ) , and QMSum ( Zhong et al . ,   2021 ) , were also studied to promote dialogue sum-   marization technologies . Zhao et al . ( 2021b ) fur-   ther proposed a task - oriented dialogue summariza-4849   tion dataset , TODSum , with dialogue state knowl-   edge . Although great progress has been made in   dialogue summarization , few people pay attention   to the issue of domain adaptation in dialogue sum-   marization . In this paper , we explore this issue in   two multi - domain dialogue summarization datasets ,   i , e , TODSum and QMSum .   Domain Adaptation in Summarization Hua   and Wang ( 2017 ) and Wang et al . ( 2019 ) adopted   the document categories in news publications to   build a multi - domain summarization dataset and   investigated the domain shift for extractive sum-   marization task . Yang et al . ( 2020 ) , Zhang et al .   ( 2020 ) , Magooda and Litman ( 2020 ) , and Fab-   bri et al . ( 2021 ) regarded diverse summarization   datasets as different domains and conducted the   assessment of multi - domain settings . Furthermore ,   various stages of pre - trainings were added to nar-   row the gap between the pre - training in news do-   main and the fine - tuning in dialogue domain ( Yu   et al . , 2021 ; Zou et al . , 2021c ) . However , these   methods focus on the heavy pre - training stage   rather than the lightweight fine - tuning , which is   time - consuming and relies on large - scale external   corpora . Therefore , we try to explore the fine-   tuning methods for domain adaptation of dialogue   summarization task .   Prompts in Summarization With the arrival   of GPT-3 , prompt learning has become a nascent   field , which conducts task - specific adaptation of   large language models ( LMs ) via prepending an   instruction . Schick and Schütze ( 2021 ) explored   the fixed - prompt LM tuning for few - shot text sum-   marization with manually crafted templates . Zhaoet al . ( 2021b ) and Dou et al . ( 2021 ) further adopted   the prompt+LM tuning strategy on text summa-   rization task , where learnable prefix prompts are   different types of guidance signals . Li and Liang   ( 2021 ) investigated fixed - LM prompt tuning , where   learnable prefix tokens are prepended to the input   while parameters in pre - trained models are frozen .   Following Li and Liang ( 2021 ) , we design the do-   main information to initialize the continuous prefix   module , and use discrete prompts and dialogue   texts to optimize prefix parameters , which greatly   reduces the size of parameters and is suitable for   low - resource scenarios .   3 Problem Formulation   Domain adaptation of dialogue summarization   aims to generate the summary yconditioned on   the source dialogue x , where the training and test   are in different domains . We add the domain word   prefix xand discrete prompt xas additional   input which we will describe details later . Note that   we only update the prefix - related parameters and   fix the parameters of BART . We train the model   using the source data and test using the target data .   4 Methodology   As Figure 2 shows , our model is on the basis of the   framework of BART , including a domain - oriented   prefix module , a prompt encoder , and a decoder .   4.1 Domain - oriented Prefix   To alleviate domain entanglement , we present a   domain - oriented prefix module to obtain the shared4850knowledge of the source domain Dand the target   domain D. It is designed as follows :   Initialization We extract some keywords from   dialogue texts in each domain by LDA ( Hoff-   man et al . , 2010 ) and concatenate them all to-   gether as a domain word ( prefix ) sequence x.   Randomly initialized embeddings of the domain   word sequence compose a learnable matrix M∈   R.   Parametrization We use an MLP to encode the   domain - oriented prefix module , which stably elic-   its knowledge from the large pre - trained model in   the prefix - tuning process . Specifically , we first in-   put the domain word sequence to the MLP and the   pre - computed BART respectively , then re - train the   MLP by fitting its outputs with the decoder hid-   den states of the pre - computed BART using MSE   loss . In this fitting process , we only iteratively   update MLP parameters φ∈ Rand keep   the pre - computed BART fixed . Finally , we get the   initialization parameters of MLP and use this pre-   trained MLP to map the initialized embeddings of   prefix representations for each Transformer layer   both in prompt encoder and decoder :   M[i , :] = MLP(M[i , :] ) ( 1 )   where i∈xandM∈R. Note that   this continuous prefix is applied for every layer of   the large - scale pre - trained model independently .   4.2 Prompt Encoder   Discrete Prompts We utilize some discrete   prompts to emphasize key elements in dialogues   and enhance the model generalization to new do-   mains . Here , discrete prompts are dialogue states   of TODSum dataset or queries of QMSum . Con-   sidering that the original form of dialogue states is   book ( people=5 ; day = Monday ) which is not com-   patible with BART encoder , we convert this struc-   tured information into a serialized sequence , i.e. ,   book , people is 5 , day is Monday , to improve the   stability of training . Note that we do not make any   changes to the query of QMSum dataset because it   is already a serialized representation .   For prompt encoder , we firstly concatenate the   discrete prompt sequence xand dialogue text se-   quence xas the input sequence of encoder , i.e. ,   x= [ x;x ] . Then , the xis fed into theprompt encoder based on the BART encoder , con-   taining multiple Transformer layers . Note that we   modify the self - attention mechanism by adding a   domain - oriented prefix sequence of key - value pairs ,   which learns the knowledge from the pre - trained   model through interactions with the dialogue text   to carry out the overall task . For the typical l-   th Transformer layer in encoder , the query ( Q ) ,   key ( K ) , and value ( V ) matrices are computed   through linear transformations on the hidden states   ofx . Here , we further augment the KandV :   K= [ P;K],V= [ P;V ] ( 2 )   where P , Pare computed through lin-   ear transformations on M.K , V∈   Randl∈L. The augmented   self - attention layer is finally calculated as follows :   A= softmax ( QK)V(3 )   4.3 Decoder   We also prepend the prefix module for decoder ,   where the cross - attention and masked - attention   mechanisms are augmented in a similar way . The   cross - attention between the l - th layer of prompt   encoder and the l - th layer of the decoder is de-   signed as :   A = softmax ( QK)V(4 )   where Qis computed through a linear transfor-   mation on the hidden states of the summary text x   andl∈L. Besides , the implementation of masked-   attention layer is the same as the self - attention layer   in the prompt encoder .   4.4 Training Strategy   In the domain - oriented prefix module , the parame-   ter set of all linear transformations is symbolized   asα . For training strategy in DOP , we perform   gradient updates on the following log - likelihood   objective :   maxlogp ( y|x ) = /summationdisplaylogp ( y|y )   ( 5 )   where the BART parameters ϕare fixed . The prefix   parameters α , θ , and φare the only trainable pa-   rameters . During the training , we use the domain   words from source domains as the prefix sequence .   When the training is completed , we save all pa-   rameters of the domain - oriented prefix module and4851   drop the pre - computed BART module . During the   test , the domain words from the target domain are   mapped to the representations of prefixes only via   the reserved MLP ( · ) , where the features of the   source domain Dcan be transferred to the target   domain D.   5 Experimental Setup   5.1 Datasets   We evaluate our model on two multi - domain di-   alogue summarization datasets and the details of   statistics are shown in Table 1 and Table 2 :   TODSum : This dataset is proposed by Zhao et al .   ( 2021b ) , which is a task - oriented dialogue sum-   marization dataset based on the classic dialogue   dataset MultiWOZ ( Budzianowski et al . , 2018 ) .   According to the domain information , the dataset   can be divided into five domains : restaurant , hotel ,   attraction , taxi , and train . Considering that parts of   dialogues in TODSum contain multiple domains ,   in this paper , we firstly select all single - domain di-   alogues from TODSum as the dataset used for this   experiment . Then we integrate all these samples   in any four of five domains as source domains D   and the other one is regarded as target domain D ,   where 200 samples extracted from Dare as the   validation set , the remaining as the training set , and   samples of Dare the test set .   QMSum : This dataset is proposed by Zhong et al .   ( 2021 ) , which contains hundreds of meeting tran-   scriptions and includes three domains , i.e. , aca-   demic , committee , product . In addition , each sam-   ple can be divided into several dialogue fragments   according to some queries and the answer for thecorresponding query is its golden summary . Such   ( query - dialogue - answer ) pairs are usually used for   query - based meeting summarization tasks . In this   paper , we separately integrate the training and vali-   dation sets , and test set in any two of three domains   as the training data and validation data , i.e. , the   data of source domain D. All data of the other   domain is used as the test data , i.e. , the data of   target domain D.   5.2 Baselines and Evaluation Metrics   We compare our methods with saveral baselines .   The extractive baselines are included : ( 1 ) Lead-   3 ; ( 2 ) Oracle ; ( 3 ) BertExt ( Liu and Lapata , 2019 ) .   Some abstractive methods are also added for com-   parison : ( 1 ) PGN ( See et al . , 2017 ) ; ( 2 ) Trans-   former ( Vaswani et al . , 2017 ) ; ( 3 ) BertAbs ( Liu and   Lapata , 2019 ) ; ( 4 ) BART ( Lewis et al . , 2020 ) ; ( 5 )   BART w. DS / QR ( Zhao et al . , 2021b ) ; ( 6 ) Prefix-   tuning ( Li and Liang , 2021 ) . For QMSum , we also   introduce its benchmark ( Zhong et al . , 2021 ) . Since   this method feeds the extracted spans into BART ,   we integrate the results of this method with the re-   sults of BART . We use ROUGEs ( Lin , 2004 ; Lin   and Och , 2004 ) to quantitatively evaluate the per-   formance of our methods . Our codes are publicly   available . We give the baselines and evaluation   metrics in Appendix A.1 and Appendix A.2 .   5.3 Training Details   Our implementation is based on the Hugging Face   Transformer models . BART is used as a   backbone and the source dialogue sequence is trun-   cated to 1024 BPE tokens . For domain - oriented   prefix module , the MLP maps 1024 dimension   into 24576 dimension , which is calculated by   2∗number of decoder layers ∗1024 and the num-   bers of domain words ( prefix length ) are set to   140 and 200 for TODSUM and QMSum datasets .   Following the settings in Li and Liang ( 2021 ) , we   use an AdamW optimizer ( Loshchilov and Hutter ,   2019 ) and a linear learning rate scheduler with ini-   tial rate of 5·10 , and the batch size is set to 5 . Our   model is trained on RTX 2080 Ti machines , taking   only 5 minutes per epoch on TODSum dataset and   3 min per epoch on QSum dataset . However , BART   w. DS takes 13 minutes and 8 minutes per epoch   on TODSum and QMSum datasets . The reason for   the shorter training time of our model is that the4852   trainable number of parameters of the DOP model   is only 15 % of the BART w. DS . For all experi-   ments , we set the number of training epochs to 30 .   At the decoding phase , we use a beam size of 6 and   max generated tokens of 125 . The decoding takes   1.3 seconds per batch with the size of 2 .   5.4 Main Results   Results on TODSum Table 3 presents the re-   sults of the zero - shot setting for TODSum dataset ,   where each of the five domains is regarded as the   target domain respectively . The division of dataset   in the second row intuitively shows that the amount   of data in Dis small and limited . We conduct ex-   periments based on some common extractive mod-   els and some strong abstractive baselines . We also   add a lightweight fine - tuning summarizer for com-   parison . As observed , for most ROUGEs , Prefix-   tuning performs worse than BART and BART w.   DS . It is because the dialogue text is long and com-   plex , and using only 20 % parameters of fine - tuning   can not well understand the domain knowledge and   identify the key contents in dialogues . Compared   to Prefix - tuning of the same magnitude parame-   ters , our model improves by 7 % , 3 % , 7 % for train   domain , 5 % , 5 % , 3 % for taxidomain , 4 % , 8%,4 % for restaurant domain , 5 % , 6 % , 5 % for hotel   domain , and 8 % , 8 % , 9 % for attraction domain .   This shows that the prefix module initialized by do-   main words and the discrete prompts composed of   dialogue states play important roles . Besides , our   model still surpasses BART w. DS , a full - parameter   fine - tuning based model , which further illustrates   that our model efficiently disentangles the knowl-   edge of the source and target domains . Note that   attraction domain gets the highest ROUGEs and   the increased margins are also the largest . This may   be due to the high overlaps between the attraction   and source domains . All the results suggest that   with limited data , the performance of our model   still reaches state - of - the - art .   Results on QMSum Table 4 displays the results   on zero - shot out - domain tests in three domains of   QMSum dataset . As seen from the second row , in   addition to the limited data , the source domain size   may be even less than the target domain size , i.e. ,   product domain . The trend of overall performance   is consistent with that of TODSum dataset , where   the improvement in product domain is the most   obvious and there are 5 % , 4 % and 5 % increased   for R-1 , R-2 , and R - L , respectively . However , all   the ROUGEs are low , which is because there are no   obvious domain words , leading to serious domain   entanglement . Besides , due to the longer meet-   ing text , it is hard to capture the key contents in   dialogues , so as to the poor generalization in the   target domain . Generally , these results show that   the multi - domain setting in meeting summarization   task is apparently necessary and meaningful . Meet-   ing transcripts cover various domains , making the4853   adaptation of models particularly difficult .   6 Qualitative Analysis   6.1 Effect of Domain Words   Number of Domain Words We set different   numbers of domain words , i.e. , prefix length , to   test the performance of our model DOP for train   domain in TODSum dataset . As shown in Figure   3 ( a ) , among these setting candidates , there is a   threshold ( 140 ) that allows the ROUGEs to reach   the peak . When choosing a fewer setting , the model   does not perform well due to insufficient number   of parameters , which is improved as the number   increases . When being more than this threshold ,   a drop in performance occurs . One reason is that   too long a sequence adds a large burden to BART ,   and the other one is that it introduces excessive   noise . However , the change in the number of the   domain words does not have a great impact on the   performance ( only 2 ∼3 % fluctuation ) , which also   reflects the effectiveness of domain - oriented prefix   module and the robustness of our model .   Quality of Domain Words Fortrain domain   in TODSum , we randomly replace a certain per-   centage of the domain words with words that are   not related to the source domain . As Figure 3 ( b )   shows , when more noise is introduced , the model   suffers more interference and its performance de-   creases . However , it performs better than Prefix-   tuning . Only when the proportion of noise reaches   100 % , the performance of our model is even worse   than that of Prefix - tuning . This is because we es-   pecially use completely irrelevant words for initial-   ization and fitting , which introduces more noise   than simple random initialization and affects the   performance of DOP . From this point of view , in-   troducing high - quality domain words is good for   domain disentanglement and the quality of domain   words is critical to summary generation .   6.2 Ablation Study   We perform ablation experiments on train domain   of TODSum dataset and committee domain of QM-   Sum dataset , as shown in Tables 5 and 6 . We can   observe that the removal of domain - oriented initial-   ization in the prefix module will make the ROUGEs   decrease significantly . Especially for TODSum , R-   1 , R-2 , and R - L drop by 4 % , 2 % , and 3 % , which   shows the importance of domain word information   for inducing the relevant knowledge while adapt-   ing to a new domain . In addition , after we remove   the discrete prompts , i.e. dialogue state and query ,   the performance of the models becomes worse , but   still outperforms the results of Prefix - tuning . It   demonstrates that discrete prompts help the model   pay attention to the key elements in the dialogue   and improve the generalization of the model . No-   tably , our model achieves summary generation only   by optimizing the domain - oriented prefix module ,   where domain words are available in all datasets .   Since the DS and QR features happen to exist in the   two datasets , we take advantage of them together   with dialogue texts . When removing both DW and   DS / QR at the same time , the model is equivalent   to Prefix - tuning and the results are consistent .   6.3 Effect of Prefix Module in Encoder and   Decoder   Since both the encoder and the decoder in our DOP   introduce the prefix module , we verify their effects   in the train andcommittee domains respectively.4854   As shown in Table 7 , when the encoder prefix mod-   ule or decoder prefix module is removed , the per-   formance of the model decreases , which shows that   both are necessary and effective . In addition , we   find that it is interesting that removing the prefix   on the encoder side has a smaller impact on the   model than removing the decoder side , especially   in TODSum ( about 5 % on R-1 ) . A reasonable ex-   planation is that the prefix modules in encoder and   decoder are responsible for different tasks . The   prefix module on the encoder side assists the model   to understand the dialogue , while the prefix mod-   ule on the decoder side assists in model generation .   Therefore , for summary generation , the prefix mod-   ule in decoder is more helpful to the model .   6.4 Effect of Training Data   Performance in Few - shot Settings For TOD-   Sum , we fix the size of source domain data , adding   a certain amount of target ( train ) domain data for   training , as shown in Figure 4 . As the size of   target domain data increases , the performance of   both BART w. DS and DOP present a steady im-   provement trend and that of our DOP model is   consistently better than BART w. DS , which is as   expected . Besides , there is a substantial improve-   ment from 50 to 100 . This phenomenon shows   that adding target knowledge can help the model   learn about information of target domain and after   adding a certain amount will help the model more   efficiently .   Effect of Source Domain Data Size We keep   the zero - shot setting unchanged and adjust the   size of source domain data for training to observe   changes in the performance of the two models for   train domain in TODSum . As shown in Figure 5 ,   the smaller of data size , the greater the difference   between the performance of the DOP and BART   w. DS , that is , the performance of BART w. DS is   getting worse , while the DOP maintains excellent   performance steadily . This demonstrates that our   DOP model is insensitive to the data scale and ro-   bustness to a certain extent . This also confirms that   in the main experiment , our model can be outstand-   ing in very limited and uneven data .   6.5 Prefix Length vs. Input Length   Through experiments , we explore an interesting   thing , that is , the prefix length ( number of domain   words ) that makes the model perform best may   be related to the input length . Based on this as-   sumption , we collect the source input length , target   input length , and their corresponding optimal pre-   fix length from two datasets , as shown in Figure 6 .   We conclude a general rule that the longer inputs   may prefer the shorter prefix . This phenomenon   may serve as a research point in the future .   7 Discussion   7.1 Human Evaluation   We further conduct a manual evaluation to assess   the models . We randomly select 50 samples from4855   each target domain dataset of TODSum ( Zhao et al . ,   2021b ) and ask 10 professional linguistic evalua-   tors to score the ground truth and summaries gen-   erated by 4 models according to 5 metrics : fluency ,   informativeness , factual consistency , domain rele-   vance , and redundancy . Each metric is rated by 3   evaluators from 1 ( worst ) to 5 ( best ) and the scores   for each summary are averaged . Note that the intra-   class agreement score is 0.605 .   As shown in Table 8 , the fluency scores of all   models are high , which is because that abstrac-   tive models fine - tuned on contextualized language   backbones can generate fluent sentences ( Lewis   et al . , 2020 ) . For factual consistency , both DOP   and BART w. DS achieve better performance than   Prefix - tuning , which suggests that the dialogue   state information guides the model to focus more   on the key information , such as slots and intents .   Besides , the DOP outperforms all baselines in the   domain relevance metric . This demonstrates that   the domain - oriented prefix module plays a crucial   role in enhancing the ability of the model to identify   domain - related features and disentangle the knowl-   edge of the source and target domains . Surprisingly ,   the scores about the redundancy of Prefix - tuning   and DOP are higher than that of BART and BART   w. DS . This is because the model can efficiently   extract key contents from a limited amount of data   without relying on large - scale pre - trainings .   8 Challenges   Through the analysis of cases in Appendix C , we   summarize two challenges of low - resource domain   adaption for abstractive dialogue summarization   task :   1 . Confusion between domains with high simi-   larity : We found that in domains with high - overlap   vocabularies , i.e. , restaurant andhotel , train and   taxi , the model generates some domain - confusing   sentences . Taken hotel - restaurant pair as an ex-   ample , when restaurant is as the target domain , a sentence like " book a restaurant room that can   accommodate 3 people , ... " is generated , which is   more likely to exist in the hotel domain . Note that   this challenge does not affect the accuracy of key   elements , but the language style is not appropriate .   2 . Information dispersion : Because of the long   input sequence , it is difficult for the models to pay   attention to all aspects of the long dialogue and   there will be problems with attention deviations on   the key elements of dialogues , especially for this   lightweight and small parameter training paradigm .   9 Conclusion   In this paper , we present a domain - oriented prefix-   tuning model to handle the domain adaptation for   dialogue summarization based on an efficient and   generalizable fine - tuning method . The domain   word initialized prefix module disentangles the tar-   get domain knowledge from the source domain and   the discrete prompts enhance the generalization   ability of the model . The experiments in zero - shot   and few - shot settings show that our methods have   made great progress on two datasets .   Acknowledgement   We thank all anonymous reviewers . This work   was partially supported by National Key R&D   Program of China No . 2019YFF0303300 and   Subject II No . 2019YFF0303302 , DOCOMO   Beijing Communications Laboratories Co. , Ltd ,   MoE - CMCC " Artificial Intelligence " Project No .   MCM20190701 , BUPT Excellent Ph.D. Students   Foundation CX2021204 .   References48564857   A Experiment details   A.1 Baselines   We describe baselines in detail as follows.4858Lead-3 : This method is commonly used in news   summarization task , which treats the first three sen-   tences of the document as the summary .   Oracle : The method is used to obtain an oracle   through a greedy way similar to Nallapati et al .   ( 2017 ) , which treats the sentences that maximize   the ROUGE-2 as the summary .   BertExt : Proposed by Liu and Lapata ( 2019 ) ,   this model is extractive and its parameters are ini-   tialized with BERT .   PGN : Proposed by See et al . ( 2017 ) , this model   adopts the pointer mechanism to deal with the issue   of Out - Of - V ocabulary in the summary generation   process .   Transformer : Proposed by Vaswani et al .   ( 2017 ) , this model captures long - distance infor-   mation through the self - attention mechanism .   BertAbs : Proposed by Liu and Lapata ( 2019 ) ,   this model is abstractive , which encoder is initial-   ized with BERT and is trained with a different opti-   mizer than decoder .   BART : Proposed by Lewis et al . ( 2020 ) , the   model is a state - of - the - art abstractive summariza-   tion model pre - trained with a denoising autoencod-   ing objective .   BART w. DS / QR : Proposed by Zhao et al .   ( 2021b ) , the model is a general summarization   framework , with two encoders that share the under-   lying parameters and a decoder , which can fuse the   input text and dialogue state / query .   Prefix - tuning : Proposed by Li and Liang ( 2021 ) ,   this model introduces a prefix matrix on the basis   of fixed pre - training BART parameters and allows   the prefix matrix to learn task information through   training , which optimizes the summarization per-   formance in the small parameters and few - shot sce-   narios .   QMSum : This model is proposed by Zhong   et al . ( 2021 ) , which is a two - stage locate - then-   summarize solution on query - based meeting sum-   marization task .   A.2 Evaluation Metrics   We use the ROUGE ( Lin , 2004 ; Lin and Och ,   2004)metrics to quantitatively evaluate the per-   formance of our model . Rouge ( Recall - Oriented   Understudy for Gisting Evaluation ) is metrics for   evaluating summarization . It calculates by com-   paring the generated summary with references to   obtain the corresponding score to measure the sim-   ilarity between them .   B Parameter Scale of Models   We show the amount of trainable parameters for   our DOP model and other baseline models in Ta-   ble 9 . Among the full - parameter fine - tuning meth-   ods , except for the relatively simple PGN model ,   other models have reached the scale of hundreds   of megabytes , which will take up a lot of time and   space in model training and storage . Prefix - tuning   and DOP - tuning greatly reduce the storage space   of the model , improving the efficiency of the model .   Compared with prefix - tuning , our method achieves   better results with fewer parameters .   C Case Study   Figure 7 shows two examples from the TODSum   and QMSum respectively . For example one of train   domain in TODSum , BART w. DS generates some   incorrect and redundant information related to the   taxidomain and hotel domain . To make matters   worse , for train domain , it loses the intent of the   user about booking tickets and wrongly generates   the key information , i.e. , the departure location .   The Prefix - tuning still confuses the knowledge of   train andtaxidomains , that is , the booking intent   in the train domain is wrongly predicted as the user   wants to know something about " cars " . Moreover ,   the quality of the generated key information is not   high , i.e. , the wrong departure location " Seachage "   and the missing time " Monday " . For example two   ofacademic domain in QMSum , both BART w.   QR and Prefix - tuning predict too many details of   the dialogue , which makes the summary redundant .   Besides , Prefix - tuning generates the wrong speaker   " Professor B " , which leads to the summary being   inconsistent .   Compared to the above two models , our method4859solves some difficult issues in low - resource domain   adaptation for dialogue summarization . By initial-   izing prefix matrix with domain words , our model   achieves domain disentanglement and the predic-   tion of domain - related information is basically ac-   curate . Through discrete prompts , our model has   the ability to generalize to new domains and the   accuracy of prediction about domain - independent   key information is greatly improved .   D Domain Words   We present some domain words for each domain   in Figure 8 . In order to facilitate reading , we only   show some of the domain words , that is , we select   the first 20 words for each domain of TODSum ,   and the first 30 words for each field of QMSum as   display .   For TODSum , we can see that there are rela-   tively many common domain words , which are   more concentrated on location words , or some in-   formation , such as " phone " , " postcode " and so on .   In addition , there are many common domain words   that only appear in restaurant & hotel ortrain &   taxi . For example , price - related descriptions are   usually mentioned when booking a restaurant or   hotel , and " destination " , " depart " , " from " , " to " are   usually mentioned when booking train tickets or   taking a taxi . Special domain words can better   distinguish different domains . There will be more   special domain words in attraction , such as " en-   trance " , " college " , " nightclub " , etc . , which will not   appear in other domains . Besides , users will men-   tion the star rating when booking a hotel , and want   to know the food type when booking a restaurant   seat . When booking train tickets , they usually plan   to travel , and when taking a taxi , they want to know   the color of the car .   Compared with TODSum , QMSum has many   more special field words , because the three fields   contained in QMSum are more different . For prod-   uct , participants will discuss various features of   products such as TVs , LCDs , etc . , such as screens ,   buttons , colors , and functions . For academic , par-   ticipants generally discuss models , experimental   data , or some errors . And for committee , partic-   ipants generally discuss student education or na-   tional government issues . Common domain words   have only some generral words , such as " different " ,   " system " , etc . , and only a few special common do-   main words exist in product & academic , such as   " design " , " bit " and other technology - related words.486048614862