  Zihan Zhao , Lu Chen , Ruisheng Cao , Hongshen Xu , Xingyu Chen and Kai Yu   X - LANCE Lab , Department of Computer Science and Engineering   MoE Key Lab of Artificial Intelligence , AI Institute , Shanghai Jiao Tong University , China   Shanghai Jiao Tong University , Shanghai , China   State Key Laboratory of Media Convergence Production Technology and Systems   zhao_mengxin@sjtu.edu.cn , chenlusz@sjtu.edu.cn   { 211314 , xuhongshen , galaxychen , kai.yu}@sjtu.edu.cn   Abstract   Recently , the structural reading comprehen-   sion ( SRC ) task on web pages has attracted   increasing research interests . Although previ-   ous SRC work has leveraged extra informa-   tion such as HTML tags or XPaths , the in-   formative topology of web pages is not ef-   fectively exploited . In this work , we pro-   pose a Topological Information Enhanced   model ( TIE ) , which transforms the token - level   task into a tag - level task by introducing a   two - stage process ( i.e. node locating and   answer refining ) . Based on that , TIE inte-   grates Graph Attention Network ( GAT ) and   Pre - trained Language Model ( PLM ) to lever-   age the topological information of both logical   structures and spatial structures . Experimen-   tal results demonstrate that our model outper-   forms strong baselines and achieves state - of-   the - art performances on the web - based SRC   benchmark WebSRC at the time of writing .   The code of TIE will be publicly available at   https://github.com/X-LANCE/TIE .   1 Introduction   With the rapid development of the Internet , web   pages have become the most common and rich   source of information ( Dong et al . , 2014 ) . There-   fore , the ability to understand the contents of struc-   tured web pages will guarantee a rich and diverse   knowledge source for deep learning systems . Each   web page is mainly rendered from the correspond-   ing HyperText Markup Language ( HTML ) codes .   In other words , the understanding of a structured   web page can be achieved by the comprehension   of its HTML codes .   One of the commonly used tasks to verify the   model ’s ability of comprehension is Question An-   swering ( QA ) . However , previous QA models only   focus on the comprehension of plain texts ( Ra-   jpurkar et al . , 2016 ; Yang et al . , 2018 ; Reddy et al . ,   2019 ; Zeng et al . , 2020 ) , tables ( Pasupat and Liang , Figure 1 : An example of web pages in WebSRC and   its corresponding Document Object Model ( DOM ) tree   and Node Positional Relation ( NPR ) graph in WebSRC .   The colored HTML tag in ( a ) is corresponding to the   bounding box with the same color in ( a ) and the node   with the same color in ( b ) and ( c ) .   2015 ; Chen et al . , 2020c , 2021b ) , or knowledge   bases ( KBs ) ( Berant et al . , 2013 ; Talmor and Be-   rant , 2018 ) . These sources have either no topo-   logical structure or fixed - form structures . On the   contrary , the topological structures of web pages   are complex and flexible , which are less investi-   gated in previous QA works .   Specifically , HTML codes can be viewed as mul-   tiple semantic unit separated by tag tokens ( e.g.   < div > , < /div > ) . An HTML tag refers to a pair   of matched start and end tags and all the content   in between , which also corresponds to a part of   the web page ( illustrated in Fig . 1 ( a ) ) . Therefore ,   there are two kinds of topological structures in web   pages : logical structures which contain the hierar-   chical relations and clustering of tags ( see Fig . 1   ( b ) ) ; and spatial structures which contain the rel-   ative positions between different tags in the web   pages ( see Fig . 1 ( c ) ) . These topological structures   are as important as the semantics of HTML codes   and screenshots .   Although previous works ( Chen et al . , 2021c ; Li1808et al . , 2021 ) have tried to leverage the topological   structures by adopting HTML tags or XPaths as   tokens or position embeddings , only logical struc-   tures are encoded implicitly . However , it is obvious   for humans to identify key - value pairs if two spans   are located in the same row or column , while this   relation may take various forms in the logical struc-   tures of different web pages . Moreover , tables have   extremely simple spatial structures but will be super   complex in terms of logical structures . Therefore ,   spatial structures are essential and complementary   to logical structures .   The major obstacle that prevents previous mod-   els to leverage spatial relations is that both the two   kinds of topological structures are organized at the   tag level instead of the token level ( Fig . 1 ( b ) and   ( c ) ) . As token - level models , whose computation   and prediction units are the tokens of web pages ,   it is extremely hard and anti - natural for them to   encode the topological structures . Moreover , using   token - level models also means that previous works   have to implicitly imply the logical structures to the   models , which may be less effective than explicitly   telling with the help of prior knowledge .   To tackle these problems , we propose   Topological Information Enhanced model ( TIE ) ,   a tag - level QA model that operates on the   representations of HTML tags to predict which   tag the answer span belongs to . By switching   from token level to tag level , various structures   of web pages can be explicitly encoded into the   model easily . Specifically , TIE encodes both   the logical and spatial structures using Graph   Attention Network ( GAT ) ( Velickovic et al . , 2018 )   with the help of two kinds of graphs . The first   kind of graphs is Document Object Model ( DOM )   trees which is widely used to represent the logical   structures of HTML codes . Secondly , to encode the   spatial structures , we define the Node Positional   Relation ( NPR ) graph based on the bounding box   of HTML tags obtained by the browser . Detail   definition can be found in Section 3.2.2 .   Moreover , to accomplish the token - level predic-   tion tasks by a tag - level QA model , we further   introduce a two - stage process including node locat-   ingstage and answer refining stage . Specifically , in   theanswer refining stage , a traditional token - level   QA model is utilized to extract answer span with   the constraint of the answer node prediction by TIE   in the node locating stage .   Our TIE model is tested on the WebSRC   datasetand achieve state - of - the - art ( SOTA ) per-   formances .   To summarize , our contributions are three folds :   • We propose a tag - level QA model called TIE   with a two - stage inference process : node lo-   cating stage and answer refining stage .   •We utilize GAT to leverage the topological   information of both the logical and spatial   structures with the help of DOM trees and our   newly defined NPR graphs .   •Experimental results on the WebSRC dataset   demonstrate the effectiveness of our model   and its key component .   2 Preliminary   2.1 Task Definition   The Web - based SRC task ( Chen et al . , 2021c ) is   defined as a typical extractive question answering   task based on web pages . Given the user query   q= ( q , q , · · · , q)and the flattened HTML   code sequence c= ( c , c , · · · , c)of relevant   web page as inputs , the goal is to predict the start-   ing and ending position of answer span ( s , e)in   the HTML codes cwhere|q|,|c|denote the length   of the question and the HTML code sequence , re-   spectively , and 1≤s≤e≤ |c| . Notice that each   token cin the flattened HTML codes ccan be a   raw text word or tag symbol such as < div > while   the user query qis a word sequence of plain text .   2.2 DOM Trees of HTML codes   The DOM tree is a special tree structure that is   parsed from raw HTML codes by Document Object   Model . Each node in the tree denotes a tag closure   in the original HTML code . Specifically , each node   contains a start tag token ( e.g. < div > ) , an end1809   tag token ( e.g. < /div > ) , and all the contents in   between . One DOM node nis the descendant   of another node n , iff the contents of node nis   entirely included in the contents of node n.   Furthermore , we define the direct contents of   each DOM node ( and its corresponding HTML   tag ) as all the tokens in its tag closure that are not   contained in any of its children ( see Figure 2 ) .   3 TIE   In this section , we will first introduce the architec-   ture of the whole SRC system in Sec.3.1 , and then   the two kind of graph we used in Sec . 3.2 . Finally ,   the structure of Topological Information Enhance   model ( TIE ) is demonstrated in Sec.3.3 .   3.1 Architecture of the Whole SRC System   With the help of DOM trees and NPR graphs , TIE   can efficiently predict in which node the answer is   located . Therefore , we modify the original archi-   tecture of the SRC system into a two - stage archi-   tecture : node locating andanswer refining . The   two - stage architecture is illustrated in Figure 3 .   In the node locating stage , we first define the   answer node as the deepest node in the DOM tree   which contains the complete answer span . Then ,   TIE is utilized to predict the answer node nfor the   question qgiven the flattened HTML codes cand   the corresponding DOM tree Dand NPR graphs   G(see Sec . 3.2 ) . Formally ,   TIE(q , c,(D , G ) ) = p ,   n = argmax(p ) ,   where pdenotes the probability of node nbeing   the answer node , and Vis the node set of D.Then , in the answer refining stage , we use the   predicted answer node as a constraint during the   prediction of the answer span . In more detail , we   first use a QA model ( e.g. MarkupLM ) to obtain   the start and end probabilities p , pamong all   the tokens of HTML code sequence c. Then , the   predicted answer span is chosen from the spans   which are contained by the predicted answer node   n. To conclude , provided that the starting and   ending position of predicted answer node nin the   HTML code ciss , and e , the second stage can   be formulated as follows :   QA(q , c ) = p , p   ( s , e ) = argmax(p+p )   3.2 Construction of GAT Graphs   Recently , Graph Neural Network ( GNN ) ( Scarselli   et al . , 2008 ) has been widely used in multiple Neu-   ral Language Processing tasks , such as text clas-   sification and generation ( Yao et al . , 2019 ; Zhao   et al . , 2020 ) , information extraction ( Lockard et al . ,   2020 ) , dialogue policy optimization ( Chen et al . ,   2018a , b , 2019 , 2020d ) , dialogue state tracking   ( Chen et al . , 2020a ; Zhu et al . , 2020 ) , Chinese pro-   cessing ( Gui et al . , 2019 ; Chen et al . , 2020b ; Lyu   et al . , 2021 ) , etc . Graph Attention Network ( GAT )   is a special type of GNN that encodes graphs with   attention mechanism . In this work , to leverage both   the logical and spatial structures , we introduce two   kinds of graphs : DOM Trees and NPR graphs .   3.2.1 DOM Trees   The logical relations of HTML codes can be de-   scribed with the assistance of its DOM Tree ( see   Sec . 2.2 ) . However , the original tree is extremely   sparse , which often leads to poor communication   efficiency among nodes . To this end , we mod-   ify the structure to enlarge the receptive fields for   each node . Mathematically , the resulting graph   D= ( V , E)can be constructed from the orig-   inal sparse form D= ( V , E ) ,         V = all nodes in the original DOM tree ,   E={(n , n)|nis the parent of n}∪   { ( n , n)|nis a child of n } ,   into a denser one D= ( V , E ) ,         V = V   E={(n , n)|n∈V}∪   { ( n , n)|nis an ancestor ofn}∪   { ( n , n)|nis adescendant ofn}1810   In this way , each node can directly communicate   with all of its ancestors and descendants , so that   the information can be transferred much faster .   3.2.2 NPR Graphs   To explicitly establish the positional relations be-   tween different texts , we define and construct Node   Positional Relation ( NPR ) graph G= ( V , E )   based on the rendered structured web pages .   Similar to DOM Tree , each NPR node ncorre-   sponds to a tag tin the HTML code of the web   page . The content of NPR nodes is defined as the   direct content of their corresponding HTML tags .   It is worth noticing that under our definition , the   node sets of the NPR graph and the DOM tree of   the same web page are identical ( V = V ) .   Moreover , considering that the nodes with in-   formative relations ( such as " key - value " re-   lations and " header - cell " relations ) are usu-   ally located on the same row or column , we in-   troduce four kinds of directed edges into NPR   graphs : UP , DOWN , LEFT , and RIGHT . Specifi-   cally , ( n , n)∈Ewhen         min(x+w , x+w)−max ( x , x )   ≥γ×min(w , w )   y≥yory+h≥y+h   ( 1 )   both hold , where ( x , y ) , ( x , y ) are the co-   ordinates of the upper - left corner of the bound-   ing boxes corresponding to the nodes nandn ;   w , ware the width of the two bounding boxeswhile h , hare the height of the two bounding   boxes ; and γis a hyper - parameter . Similar func-   tions are used for E , E , and E . Fi-   nally , E = E / uniontextE / uniontextE / uniontextE Fig-   ure 1 ( a ) and ( c ) show an example of the NPR graph   and its corresponding HTML code .   To simplify the NPR graphs , we only consider   the nodes whose direct contents contain text tokens .   That means in NPR graphs , the nodes whose direct   contents only contain tag tokens will be isolated   nodes with no relation .   3.3 Design of TIE   The model we proposed , TIE , mainly consists of   four parts : the Context Encoder Module , the   HTML - Based Mean Pooling , the Structure En-   coder Module , and the Classification Layer . The   overall architecture of TIE is shown in Figure 4 .   Context Encoder Module . We first utilize Pre-   trained Language Model as our context encoder . It   encodes the contextual information of the HTML   codes and gets the contextual word embeddings   used for node representation initialization . Specif-   ically , we use two PLM in our experiments : H-   PLM ( Chen et al . , 2021c ) + RoBERTa ( Liu et al . ,   2019 ) and MarkupLM ( Li et al . , 2021 ) .   HTML - Based Mean Pooling . In this module ,   TIE initializes the node representations based on   the contextual word embedding calculated by Con-   text Encoder . Specifically , for each node , we ini-   tialize its representation as the average embedding1811TypeTraining set Dev set   # QA % # QA %   KV 129990 42.3 21798 41.3   Comparison 52893 12.2 9078 17.2   Table 124432 40.5 21950 41.6   of its corresponding tag ’s direct contents . Formally ,   the representation of node nis calculated as :   n = mean(x ) ( 2 )   where DC(n)means the tokens set of the direct   contents of node n;xis the contextual embed-   ding of token x.   Structure Encoder Module . TIE utilizes GAT   to encode the topological information contained in   DOM trees and NPR graphs . Specifically , for the   i - th attention head of GAT :   Q = WN;K = WN;V = WN   GAT(N ) = softmax ( QK√   d+M)V   m=/braceleftbigg0 ( n , n)∈Edge(G )   −∞ otherwise   G∈ { D , G , G , G , G }   where N= [ n];dis the dimension of the   node representations n;Ws are the learnable pa-   rameters ; M= [ m ] is the mask matrix   for the i - th attention head . Finally , the outputs of   all the attention heads are concatenated to form the   node representations for the next GAT layer .   Classification Layers . Finally , we get the embed-   dings of all the nodes from the Structure Encoder   Module and utilize a single linear layer followed   by a Softmax function to calculate each node ’s   probability of being the answer node .   4 Experiments   4.1 Dataset   We evaluate our proposed methods on WebSRC   ( Chen et al . , 2021c ) . In more detail , the WebSRC   dataset consists of 0.4 M question - answer pairs and   6.4 K web page segments with complex structures .   For each web page segment , apart from its corre-   sponding HTML codes , the dataset also providesthe bounding box information of each HTML tag   obtained from the rendered web page . Therefore ,   we can easily use this information to construct the   NPR graph for each web page segment .   Moreover , WebSRC groups the websites into   three classes : KV , Comparison , and Table . Specif-   ically , KVindicates that the information in the   websites is mainly presented in the form of   " key : value " , where key is an attribute name   andvalue is the corresponding value . Compar-   ison indicates that each web page segment of the   websites contains several entities with the same   set of attributes . Table indicates that the websites   mainly use a table to present information . The   statistics of different types of websites in WebSRC   are shown in Table 1 .   We submit our models to the official of WebSRC   for testing .   4.2 Metrics   To keep consistent with previous studies , we adopt   the following three metrics : ( 1 ) Exact Match ( EM ) ,   which measures whether the predicted answer span   is exactly the same as the golden answer span . ( 2 )   Token level F1 score ( F1 ) , which measures the   token level overlap of the predicted answer span   and the golden answer span . ( 3 ) Path Overlap   Score ( POS ) , which measures the overlap of the   path from the root tag ( < HTML > ) to the deepest tag   that contains the complete predicted answer span   and that contains the complete golden answer span .   Formally , the POS is calculated as follows :   POS = |P / intersectiontextP|   |P / uniontextP|×100 % ( 3 )   where PandPare the set of tags that on the   path from the root ( < HTML > ) tag to the deepest tag   that contains the complete predicted answer span   or the ground truth answer span , respectively .   4.3 Baselines & Setup   We leverage the three models introduced in Chen   et al . ( 2021c ) and MarkupLM ( Li et al . , 2021 ) as   our baselines . Specifically , T - PLM converts the   HTML codes into plain text by simply removing all   the HTML tags , while H - PLM treats HTML tags as   special tokens and uses the origin HTML code se-   quences as input . Then , both of them utilize PLMs   to generate the predicted answer span . To lever-   age visual information , V - PLM concatenates token   embeddings resulting from H - PLM with visual em-   beddings and then feeds the results into multiple1812MethodDev Test   EM↑ F1↑ POS↑ EM↑ F1↑ POS↑T - PLM(BERT ) ( Chen et al . , 2021c ) 52.12 61.57 79.74 39.28 49.49 67.68   H - PLM(BERT ) ( Chen et al . , 2021c ) 61.51 67.04 82.97 52.61 59.88 76.13   V - PLM(BERT ) ( Chen et al . , 2021c ) 62.07 66.66 83.64 52.84 60.80 76.39   MarkupLM ( Li et al . , 2021 ) 68.39 74.47 87.93 - - -   MarkupLM68.99 74.55 88.40 60.43 67.05 80.55   TIE 76.83 82.77 90.90 71.86 75.91 85.74T - PLM(Electra ) ( Chen et al . , 2021c ) 61.67 69.85 84.15 56.32 72.35 79.18   H - PLM(Electra ) ( Chen et al . , 2021c ) 70.12 74.14 86.33 66.29 72.71 83.17   V - PLM(Electra ) ( Chen et al . , 2021c ) 73.22 76.16 87.06 68.07 75.25 84.96   MarkupLM ( Li et al . , 2021 ) 74.43 80.54 90.15 - - -   H - PLM(RoBERTa)70.90 75.15 87.16 67.76 74.61 86.29   TIE 75.57 79.38 88.29 69.65 74.78 85.72   MarkupLM73.38 79.83 89.93 69.09 76.45 87.24   TIE81.66 86.24 92.29 75.87 80.19 89.73   self - attention blocks before generating predictions .   Faster R - CNN is utilized to extract visual embed-   dings from screenshots of the corresponding web   pages . On the other hand , MarkupLM leverages   XPaths to encode the logical position of each token   and use it as an additional position embedding .   In our experiments , we use 3 GAT blocks   as the Structure Encoder Module of TIE . H-   PLM(RoBERTa ) and MarkupLM are leveraged as   context encoders . The implementation of TIE is   based on the official code provided by WebSRC   and MarkupLM . We set the hyperparameter γ   in Eq.1 to be 0.5 . Finally , the models used in the   answer refining stage are of the same architecture   as the context encoder models of TIE while individ-   ually trained on WebSRC . For more setup details ,   please refer to Appendix . A   4.4 Main Results   The experimental results on the development set   and the test set are shown in Table 2 . Specifically ,   the performances of TIE in the following sections   refer to the performances of the proposed two - stage   system , and the subscript of TIE refer to both the   context - encoder for TIE and the QA model used in   answer refining stage.|S||S||S|:|S|   MarkupLM 873 692 1.26:1   TIE 944 314 3.1:1   From the results , we can find out that our   TIE consistently achieves better results compared   with the corresponding baselines . Specifically ,   TIE significantly outperforms the previ-   ous SOTA results , MarkupLM , by 6.78 % EM ,   3.74 % F1 , and 2.49 % POS on the test set . More-   over , it is worth noticing that the performance of   TIE is even higher than the perfor-   mance of the MarkupLM - LARGE model ( 76.83 %   v.s. 73.38 % EM on the development set and   71.86 % v.s. 69.09 % EM on the test set ) . These   results strongly demonstrate that TIE can effec-   tively model the topological information of the   semi - structured web pages with the help of its struc-   ture encoder .   Furthermore , we compare the performances of   TIE and MarkupLM on different types of   websites . The results are shown in Figure 5 . From   the figure , we find that our method achieves signifi-1813   ca nt improvements on the websites of type Table   ( +20.30 % EM , +17.48 % F1 , +7.43 % POS ) while   suffering slight performance drops on the websites   of type KV . We hypothesize the reason is that topo-   logical structures are less important in the websites   of type KV , so that stronger contextual encoding   abilities will lead to better results . More analysis   can be found in Sec . 4.5 .   We also notice that the improvements of F1 are   less considerable compared with those of EM on   the websites of type Compare ( +10.27 % EM v.s.   +0.71 % F1 ) . The reason lies in the cascading error   of our two - stage process . Specifically , in the node   locating stage , the model may generate a wrong   prediction which is not one of the ancestors of the   answer node . In this case , as the answer span is not   contained in the predicted node , the final F1 score   is highly likely to be zero . Detailed calculations ,   see Table 3 , strongly support our analysis .   4.5 Case Study   In Fig . 6 , we compare the answers generated by   TIE and MarkupLM . More examples can   be found in Appendix . B.   Q1is a typical example of Table websites .   It is obvious that multiple " header - cell "   relations need to be recognized when answer-   ing Q1 . Specifically , one should first find   " OlliOlli : Switch Stance ( Switch ) " from col-   umn " Title " ( first " header - cell " relation ) ,   then locate the answer at the crossing cell of   row " OlliOlli : Switch Stance ( Switch ) " ( second   " header - cell " relation ) and column " Game   Score " ( third " header - cell " relation ) . With   the help of topological information , TIE can cor-   rectly answer this question . However , MarkupLM   only successfully locates the row and fails to recog-   nize the long range relation between " Game Score "   and " 84 " . Considering that this row can also be   identified by string matching , this example strongly   demonstrate that TIE is much stronger in terms of   long range topological relation encoding .   Q2is a typical example of KVwebsites . The   topological structures of this web page are far less   complex . To answer Q2 , the most important step   is to discover the semantic similarity among " Ac-   tion " , " Fantasy " , and " Sci - Fi " and then group them   together . In this case , the contextual distances of   these words will be extremely helpful . Therefore ,   MarkupLM is able to generate the correct predic-   tion . However , as TIE focuses on the comprehen-   sion of node structures where sequencing order and   semantics are less valuable , TIE fails to group the   three nodes .   4.6 Ablation Study   To further investigate the contributions of key com-   ponents , we make the following variants of TIE :   ( 1)"w / o DOM " means only using NPR graphs   without the DOM trees . ( 2 ) " w/ ORD " means using   original sparse DOM trees instead of the denser ver-   sion introduced in Sec.3.3 . ( 3 ) " w/o NPR " means   only using the densified DOM trees without the   NPR graphs . ( 4 ) " w/o Hori " removes LEFT and   RIGHT relations in NPR graph . ( 5 ) " w/o Vert "   removes UPandDOWN relations in NPR graph .   The results are shown in Table 4 , from which we1814   have several observations and analysis :   First , we investigate the contribution of DOM   trees . The performance of " w/o DOM " drops   slightly compared with original TIE , which in-   dicates that the contributions of DOM trees are   marginal . That may be because MarkupLM has   leveraged XPaths to encode the logical informa-   tion . Considering that XPaths are defined based on   DOM trees , the information contained in XPaths   and DOM trees may largely overlap . Moreover ,   the results of " w/ ORD " show that densifying   the DOM Tree is vitally important , as the original   DOM tree is extremely sparse and will significantly   lower the performance of TIE .   Finally , the NPR graphs have great contributions   as the performance of " w/o NPR " drops signif-   icantly . It is because NPR graphs can help TIE   efficiently model the informative relations such as   key - value andheader - cell , as they are of-   ten arranged in the same row or column . More-   over , we further investigate the contribution of   different relations in NPR graphs by " w/o Hori "   and"w / o Vert " . Note that , we keep the number   of parameters of TIE unchanged among these ex-   periments , which means no horizontal rela-   tions in NPR graphs will result in more attention   heads assigned to vertical relations . The re-   sults show that , in WebSRC , vertical relations   are much more important than horizontal re-   lations . That is because most of the websites in   WebSRC are constructed row - by - row , which means   that the tags of horizontal relations are often   located near each other in the HTML codes while   those of vertical relations may be located far   apart . Therefore , in most cases , the horizontal   relations are easier to capture in the context en-   coder without the help of NPR graph , while thevertical relations can hardly achieve that .   5 Related Work   Question Answering ( QA ) In recent years , a   large number of QA datasets and tasks have been   proposed , ranging from Plain text QA ( i.e. MRC )   ( Rajpurkar et al . , 2016 ; Joshi et al . , 2017 ; Lai et al . ,   2017 ; Yang et al . , 2018 ; Reddy et al . , 2019 ) to   QA over KB ( Berant et al . , 2013 ; Bao et al . , 2016 ;   Yih et al . , 2016 ; Talmor and Berant , 2018 ; Dubey   et al . , 2019 ) , Table QA ( Pasupat and Liang , 2015 ;   Chen et al . , 2020c , 2021b ) , Visual QA ( VQA )   ( Antol et al . , 2015 ; Wang et al . , 2018 ; Marino   et al . , 2019 ) , and others . However , the topolog-   ical information in the textual inputs is either ab-   sent ( plain text ) or simple and explicitly provided   ( KB / tables ) . The QA task based on semi - structured   HTML codes with implicit and flexible topology is   under - researched .   Among these tasks , Table QA is the most similar   to the Web - based SRC task , as there are many ta-   bles in the WebSRC dataset . To solve the problem ,   Krichene et al . ( 2021 ) first selects candidate answer   cells according to cell embeddings from the whole   table and then finds the accurate answer cell from   the candidates . Their method enables the model to   handle larger tables at little cost . On the other hand ,   Glass et al . ( 2021 ) introduces row and column inter-   actions into their models and determines the final   answers based on the top - ranked relevant rows and   columns . In addition , Text - to - SQL is another group   of methods to tackle Table QA problems and has   been widely studied recently ( Yu et al . , 2018 ; Bo-   gin et al . , 2019 ; Wang et al . , 2020 ; Cao et al . , 2021 ;   Chen et al . , 2021d , e ; Hui et al . , 2022 ) . They use   databases to store the source tables and translate1815natural language queries into Structured Query Lan-   guage ( SQL ) to retrieve answers from the databases .   It is worth noticing that these methods are highly   coupled with the data format and requires simple   and neat structures . Therefore , their methods are   not suitable for Web - based SRC tasks .   Web Question Answering Recent works which   mentioned Web Question Answering mainly fo-   cus on the post - processing of the plain texts ( Su   et al . , 2019 ; Shou et al . , 2020 ) or tables ( Zhang   et al . , 2020 ) resulting from the searching engine .   Moreover , Chen et al . ( 2021a ) has tried to answer   fixed - form questions based on raw HTML codes   with the help of Domain - Specific Language ( DSL ) .   Apart from the above works , Chen et al . ( 2021c )   proposed a QA task called Web - Based SRC which   is targeted at the comprehension of the structured   web pages using raw HTML codes . The method   they proposed is to treat the HTML tags as special   tokens and directly feed the raw flattened HTML   codes into the PLM . They also tried to leverage   screenshots as auxiliary information . Later , Li et al .   ( 2021 ) introduced a novel pre - trained model called   MarkupLM specifically for XML - based documents .   They adopted a new kind of position embedding   generated from the XPath of each token to implic-   itly encode the logical information of XML codes .   In this work , we further explicitly introduce the   topological structures to the models with the help   of DOM trees and NPR graphs . A newly designed   tag - level QA model with a two - stage pipeline is   leveraged to take advantage of these graphs .   6 Conclusion & Future Work   In this paper , we proposed a tag - level QA model   called TIE to better understand the topological in-   formation contained in the structured web pages .   Our model explicitly captures two of the most in-   formative topological structures of the web pages ,   logical and spatial structures , by DOM trees and   NPR graphs , respectively . With the proposed two-   stage pipeline , we conduct extensive experiments   on the WebSRC dataset . Our TIE successfully   achieves SOTA performances and the contributions   of its key components are validated .   Although our TIE can achieve much high per-   formance compared with traditional QA models   on SRC tasks , more improvements are still needed .   Specifically , as our two - stage system needs a sep-   arated token - level QA model to generate final an-   swer spans , the parameter numbers and computa - tion consumption will be at least doubled . We have   tried to tackle this problem by sharing parameters   between the context encoder and the token - level   QA model used in the answer refining stage . But   the results are not promising . Therefore , we leave   this problem for future work .   Acknowledgements   We sincerely thank the anonymous reviewers   for their valuable comments . This work has   been supported by the China NSFC Projects ( No .   62120106006 and No . 62106142 ) , Shanghai Mu-   nicipal Science and Technology Major Project   ( 2021SHZDZX0102 ) , CCF - Tencent Open Fund   and Startup Fund for Youngman Research at SJTU   ( SFYR at SJTU ) .   References181618171818   A Detail Setup   To train the model , we use AdamW ( Loshchilov   and Hutter , 2019 ) with a linear schedule as our op-   timizer . As for the learning rate , we search for thebest learning rate between 1e-6 and 5e-5 . Finally ,   TIE is trained and evaluated on four Nvidia A10   Graphics Cards with batch size 32 for two epochs .   Moreover , for BASE size models ( 12 heads in to-   tal ) , we use DOM Trees to generate the mask ma-   trix for 4 attention heads and each of the 4 NPR   graphs for 2 attention heads . And for LARGE size   models ( 16 heads in total ) , we add one more atten-   tion head using each of the 4 NPR graphs .   B Additional Case Study   Figure 7 , 8 , and 9 shows the typical examples of   the QA pairs in KV , Table , and Compare websites ,   respectively .   Through detailed analysis , we found that TIE can   better capture the long - range relations which have   obvious spacial relations , such as header - cell   andentity - attribute ( see Fig . 7 Q3 , Fig . 8   Q1 , and Fig . 9 Q2 ) . On the other hand , as TIE   focuses more on tag - level structure understanding ,   its ability to understand token - level semantics may   be weaker , which leads to some of the TIE ’s wrong   predictions ( see Fig . 7 Q1 , Fig . 8 Q2 , and Fig . 9   Q3 ) . In addition , TIE has a better awareness of tag   boundaries , which has been proven useful when   answering questions with blurry boundaries ( see   Fig . 7 Q2,Q3 , and Fig . 9 Q1).181918201821