  Danilo Croce   Dept . of Enterprise Engineering   University of Rome , Tor Vergata   Roma , Italy   croce@info.uniroma2.itSimone Filice   Amazon   Tel Aviv , Israel   filicesf@amazon.com   Giuseppe Castellucci   Amazon   Seattle , USA   giusecas@amazon.comRoberto Basili   Dept . of Enterprise Engineering   University of Rome , Tor Vergata   Roma , Italy   basili@info.uniroma2.it   Abstract   Even if recent Transformer - based architec-   tures , such as BERT , achieved impressive re-   sults in semantic processing tasks , their ﬁne-   tuning stage still requires large scale training   resources . Usually , Data Augmentation ( DA )   techniques can help to deal with low resource   settings . In Text Classiﬁcation tasks , the ob-   jective of DA is the generation of well - formed   sentences that ( i ) represent the desired task cat-   egory and ( ii ) are novel with respect to exist-   ing sentences . In this paper , we propose a neu-   ral approach to automatically learn to generate   new examples using a pre - trained sequence - to-   sequence model . We ﬁrst learn a task - oriented   similarity function that we use to pair simi-   lar examples . Then , we use these example   pairs to train a model to generate examples .   Experiments in low resource settings show   that augmenting the training material with the   proposed strategy systematically improves the   results on text classiﬁcation and natural lan-   guage inference tasks by up to 10 % accuracy ,   outperforming existing DA approaches .   1 Introduction   Deep Learning models achieve state - of - the - art re-   sults in many domains , including Computer Vision   and Natural Language Processing ( NLP ) . Training   these large models typically requires many exam-   ples , whose collection and annotation can be costly   and time - consuming . Data augmentation ( DA ) has   proven an efﬁcient way to acquire more training   samples without incurring in the prohibitive anno-   tation cost in a variety of ﬁelds , including com-   puter vision ( Perez and Wang , 2017 ) and speech   recognition ( Rebai et al . , 2017 ) . In NLP , some DA   techniques have been proposed too , as surveyed   in ( Feng et al . , 2021 ): common approaches cre - ate synthetic data by manipulating real examples ,   using Text Editing ( Wei and Zou , 2019 ) or Back-   Translation ( Sennrich et al . , 2015 ) ; the resulting   examples are automatically labeled by inheriting   the class of the original example they were gener-   ated from .   Unfortunately , when using recent pre - trained lan-   guage models , such as BERT ( Devlin et al . , 2018 )   or RoBERTa ( Liu et al . , 2019 ) , the effectiveness   of DA methods is extremely limited , and some-   times they can even hurt the results ( Longpre et al . ,   2020 ) . A possible explanation for this inefﬁcacy   is that these DA techniques introduce lexical and   structural variability that accurate language models   directly induce during pre - training . The usefulness   of synthetic examples is strictly related to their di-   versity from the original training data . At the same   time , diverging too much from the initial data might   increase the risk of introducing noisy annotations ,   i.e. , synthetic data not reﬂecting the class of the   original examples they were generated from .   To directly tackle the trade - off between diver-   sity and label consistency , in this paper we propose   DATS(Data Augmentation based on Task - speciﬁc   Similarity ) , a novel data - augmentation technique   for text classiﬁcation based on Natural Language   Generation ( NLG ) models . Starting from a reduced   set of annotated examples , we ﬁrst learn a task-   oriented similarity function that we use to automat-   ically create pairs of similar examples . Then , we   use these pairs to train a generative model to gen-   erate an example similar to the input one . Finally ,   we employ this model to generate new synthetic   examples and augment the training data . We show   that pairing examples with respect to their task-4587oriented similarity is striking in order to allow the   generative model to automatically understand the   lexical and structural variations that can be applied   to an instance without changing its label . Experi-   mental results on four different text classiﬁcation   datasets demonstrate that DATS achieves better re-   sults than several existing DA solutions and that   it systematically improves NLU models based on   state - of - the - art pre - trained language models . In the   remaining , Section 2 summarizes related works ,   Section 3 describes our method and Section 4 pro-   vides the experimental evaluation .   2 Related Work   Most of the existing approaches for DA perform   some token - level manipulations on individual sen-   tences ( Kolomiyets et al . , 2011 ; Kobayashi , 2018 ) .   For instance , Easy Data Augmentation ( EDA ) ( Wei   and Zou , 2019 ) applies simple operations includ-   ing synonym replacement and random swap . These   operations are also performed in ( Ren et al . , 2021 )   where the framework named Text AutoAugment   ( TAA ) uses Bayesian Optimization algorithm to   automatically search for the best manipulation pol-   icy . On the contrary , Wu et al . ( 2018 ) uses mask   random tokens and use BERT to generate substi-   tuting words . Similarly , Kumar et al . ( 2020 ) use   transformer - based models to apply token - level or   span - level text content manipulation .   Other attempts operate at the entire sentence-   level by paraphrasing the original text using back-   translation ( BT ) ( Edunov et al . , 2018 ; Shleifer ,   2019 ) ; however , BT tends to skew towards high-   frequency words which not only causes redundancy   but also leads to lexical shrinkage in the augmented   data ( Liu et al . , 2020 ) . More recent approaches for   data augmentation use generative models to create   more diverse synthetic data . Anaby - Tavor et al .   ( 2019 ) ﬁne - tuned a GPT-2 model to generate text   given a target class , and use a text classiﬁer to ﬁl-   ter out those synthetic examples whose predicted   class does not match the target class . In our work ,   we show that by conditioning a target model not   only on the class labels , but also on representa-   tive examples , it is possible to better control the   diversity - consistency trade - off .   3 Learning to generate examples   Recent advances in NLG ( Vaswani et al . , 2017 )   demonstrated that sequence - to - sequence ( seq2seq )   models can generate natural sounding and meaning - ful text given a prompt . As shown in ( Keskar et al . ,   2019 ) , the prompt can include style or content-   related information that can help control the gener-   ation process .   In our work , we capitalize these techniques to   augment a datasetDfor the training of a text clas-   siﬁerC. Speciﬁcally , our goal is to ﬁne - tune a   pre - trained seq2seq model M(∗ ) = ethat , given   a prompt∗ , generates novel examples not in D.   The ﬁrst application of this idea was investigated   in Anaby - Tavor et al . ( 2019 ) , where the authors   trained a seq2seq model M(c ) = sthat gener-   ates examples sof a given class c. While this   approach provides interesting results , it must be   said that the variability of the generated examples   can be quite low : by conditioning Monly on c , it   tends to produce the most frequent syntactic and   lexical patterns associated with c , neglecting other   modes this can exhibit . The key challenge here is   to ensure diversity while preserving consistency ,   i.e. , generating diverse examples that are valid rep-   resentatives of the desired class .   We propose to train a seq2seq model Mthat   is conditioned not only on the class c , but also   on an example sof that class , i.e. , such that   M(c , s ) = s. The model is thus expected to syn-   thesize a new example s , which is consistent with   the input class cand is also “ inspired ” by an exist-   ing example s. The problem now is how to build   the dataset to train M. Pairing two random exam-   plessandsbelonging to the same class might be   a viable solution . However , in classiﬁcation tasks ,   examples are not necessarily similar , and coupling   together radically different examples risks destabi-   lizing the training of M. Ideally , we would like   to identify different modes in the same class and   pair only examples belonging to the same mode .   An alternative way to reach the same outcome is   the adoption of a semantic similarity function that   can be used to select linguistically related example   pairs . Several unsupervised metrics exist ( Croce   et al . , 2011 ; Cer et al . , 2018 ; Poerner et al . , 2020 ) ,   however , they are inherently task - independent and   not adequate in capturing task - speciﬁc similarities :   two sentences such as “ The movie is good ” and   “ The movie is not good ” ( or even “ The movie is aw-   ful ” ) should not be paired together , when dealing   with sentiment classiﬁcation , while they are good   candidate pairs in the training of a topic classiﬁer .   To deﬁne a task - oriented similarity measure , we   leverage the text classiﬁer Cwe want to improve4588via data augmentation . Independently on the un-   derlying neural architecture , a neural classiﬁer has   an encoderEthat projects an example s∈D in   ad - dimensional space , i.e. , E(s ) = /vector s∈R. In   these spaces , simple linear classiﬁers ( i.e. , the out-   put layers ) identify the sub - spaces reﬂecting the   target classes ( Goodfellow et al . , 2016 ; Goldberg ,   2016 ) . As a consequence , Eis expected to project   examples in sub - spaces representative for individ-   ual classes . Our task - aware similarity measure is   the cosine similarity operating on these represen-   tations . For instance , after training a BERT model   on the question classiﬁcation task ( Li and Roth ,   2006 ) , the encoding [ CLS ] of the entire question   “ Who developed the vaccination against polio ? ” al-   lows retrieving other questions corresponding to   the most similar [ CLS ] embeddings , such as “ Who   invented the Moog Synthesizer ? ” : they are clearly   not paraphrases , and their purely lexical similarity   is quite low , but they show a similar pattern charac-   terizing the question class [ HUM ] ( i.e. , human ) .   Algorithm 1 : Pseudo - code of DATS .   To generate an augmented dataset for a generic   text classiﬁcation task , we propose DATS ( Data   Augmentation based on Task - speciﬁc Similarity )   that is described in Algorithm 1 . First , we train   a classiﬁerConD. We use the resulting encoder   Eto project each training example einto the task-   speciﬁc vector space , obtaining the embedding /vector e.   Then , we split the training data DintoDandD.   Each example e∈Dis paired with the nexam - plese∈Dof the same class having the highest   cosine similarity computed on their corresponding   embeddings vectors E(e)andE(e ) . The result-   ing pairs are expected to lie in the same subspace   and share some task - oriented linguistic relatedness .   We use these example pairs to ﬁne - tune a seq2seq   model to solve the task M(c , e ) = e , where c   is the category of e. Finally , examples in Dare   provided in input to Mto generate the new syn-   thetic datasetD. By applying multiple splits of   Din a cross - fold scheme , we can use each train-   ing example to condition the model and generate   new synthetic instances . As in almost all existing   formulations , a generator can be used to generate a   set of lvariants for each input ( c , e ) , for simplicity   referred asM(c , e ) . In particular , techniques such   asnucleus - sampling ( Holtzman et al . , 2020 ) enable   the generation of large sets of variants , generally   characterized by a good diversity .   It is worth noting that no assumption is applied   when selectingCorM. Even though in the experi-   mental evaluation we consider only speciﬁc archi-   tectures ( namely BERT and BART ) , this methodol-   ogy can be applied to a wider plethora of models .   Moreover , there is no restriction on the classiﬁ-   cation task type : the experimental section shows   that DATS is applicable to classiﬁcation tasks op-   erating on both individual sentences and text pairs ,   e.g. , in natural language inference ( Bowman et al . ,   2015 ): given text pairs sands , it is sufﬁcient   to extend the above process deﬁning Mdifferently ,   i.e. ,M(c , s , s ) = ( s , s ) .   4 Experimental Evaluation   We test our approach on four tasks : 50 - class Ques-   tion Classiﬁcation ( QC ) over the TREC dataset ( Li   and Roth , 2006 ) ; 5 - class Sentiment Classiﬁcation   ( SA ) over the SST dataset ( Socher et al . , 2013 ) ;   7 - class Intent Classiﬁcation ( IC ) over the SNIPS   dataset ( Coucke et al . , 2018 ) ; sentence - pair classi-   ﬁcation for Natural Language Inference ( NLI ) over   the 3 - category SNLI dataset ( Bowman et al . , 2015 ) .   Details about the datasets are in Appendix A.1 .   Baselines . We compare our approach with mul-   tiple baselines and simpliﬁed versions of DATS   for ablation study : ( i ) Easy Data Augmentation   ( EDA ) ( Wei and Zou , 2019 ) ; ( ii ) Back - Translation   ( BT ) ( Sennrich et al . , 2015 ) in an English - German-   English setting ; ( iii)Random Pairing ( RP ) - DATS4589without task - oriented similarity functions where we   created the training input - output pairs for Mby   selecting two random examples of the same class   from the training set ; ( iv ) Only Class ( OC ) - DATS   where the prompt is only the category name ( i.e. , no   representative example ) , similar to Anaby - Tavor   et al . ( 2019 ) .   Experimental Setting . For QC , SA and IC , we   report the Accuracy when using q=10 , q=50 or   q=100 average examples per class . For the NLI   task , since it is more challenging , we report the Ac-   curacy also for q=500 and q=1000 average exam-   ples per category . We also report the performance   of each model when using the entire ( F ) training   set . We use the bert - base - uncased model   from the Huggingface library ( Wolf et al . , 2019 )   as the classiﬁerC , andbart - base ( Lewis et al . ,   2020 ) as the NLG model M. Both are trained for   10 epochs with early stopping ( patience=3 ) and   learning rate 5e . We adopt nucleous sampling   ( Holtzman et al . , 2020 ) with p=0.90 . We repeat the   experiments 5 times with different seeds and we   report the average classiﬁcation accuracy .   Results . Table 1 shows the results of a BERT-   based model without DA ( NoDA ) , the DA base-   lines and our approach ( DATS ) . We perform hyper-   parameters tuningon the development set of each   task w.r.t . the number of similar examples nand   the number of generated examples l. Task - agnostic   DA approaches , like BT or EDA , seem slightly ben-   eﬁcial when using a transformer based classiﬁer C ,   as also reported in ( Longpre et al . , 2020 ) . In some   cases , they signiﬁcantly hurt accuracy .   For instance , in QC or IC when q=10 , EDA Ac-   curacy is lower than NoDA by about 1 and 6 points ,   respectively . The same is for BT , where the drop is   even higher , i.e. , 8 and 18 points . Using the cate-   gory information in the generation process , i.e. , RP   and OC , provides variable results , with minor im-   provements only in few speciﬁc settings ( e.g. , RP   on SA ) . Instead , DATS improves accuracy for al-   most all qand tasks and such improvements are   often statistically signiﬁcant . When q=10 , the im-   provement ranges between 1 and 10 points . In the   few cases where a minor drop is observed , this is   never statistically signiﬁcant . By comparing DATS   with RP it is clear that the task - speciﬁc similar-   ity measure is striking to learn a good data gen-   erator . DATS works well also on sentence - pair   tasks , like NLI . However , given the complexity   of this type of tasks , it requires more training ex-   amples ( e.g. , q>100 ) to show a positive impact .   Furthermore , DATS outperforms other competitive   methods . With q=10 Kumar et al . ( 2020 ) report   67.30 % accuracy in QC and 87.24 % in IC , which   is 1.74 % and 4.5 % lower than DATS . In the same   setting , the gap with Wu et al . ( 2018 ) is even larger ,   i.e. , 4.71 % and 5.95 % . Moreover , Ren et al . ( 2021 )   report 52.55 % accuracy in SA using the entire train-   ing set , and DATS outperforms it by 1.71 % .   Qualitative Analysis . To better understand the   advantages provided by DATS in the consistency-   diversity trade - off , we report a qualitative evalua-4590   tion of the generated sentences for the SA task .   In particular , Table 2 reports some of the SA ex-   amples generated by different models on classes   5 and 1 when using q= 10 average training ex-   amples per class . Exception for a single case “ ( It   is a masterpiece , brilliantly directed , and incred-   ibly well done . ) ” that is not a negative sentiment   example as it was supposed to be ) , the examples   generated by DATS ( with n= 1 andl= 1 ) are   all label - consistent . However , the diversity intro-   duced by DATS is impressive : all the examples   are very different from the input example used to   condition the NLG model while generating them ,   and therefore can be very useful when augmenting   the training data of NLU models . On the opposite ,   BT introduces very minor modiﬁcations to the in-   put text , resulting in a signiﬁcant lower diversity   A larger and systematic qualitative analysis on the   quality and diversity of the generated material is in   Appendix A.2 and A.3 .   5 Conclusions   This paper proposes DATS , a data augmentation   method based on Natural Language Generation   ( NLG ) models . A generative model is ﬁne - tuned   to produce examples similar to the input ones . The   training input - output pairs are selected according   to a task - oriented similarity function . This pair-   ing allows the NLG model to learn the lexical andstructural variations that can be applied to an in-   stance without changing its label . The experimen-   tal results suggest that the generated sentences are   diverse and label consistent , and can improve state-   of - the - art text classiﬁers , outperforming existing   DA methods . In the future , we plan to apply DATS   to further tasks ( e.g. , Question Answering ) and   neural architectures .   Acknowledgments   We would like to thank the “ Istituto di Analisi dei   Sistemi ed Informatica - Antonio Ruberti " ( IASI )   for supporting the experimentations through access   to dedicated computing resources .   References459145924593A Appendix   A.1 Task Description   In this section we report details and statistics of the dataset we adopted in the experimental section .   TREC . The TREC ( Li and Roth , 2006 ) dataset contains 4,907 , 545 and 500 examples for training ,   development and test , respectively . We adopted the ﬁne - grained version of the dataset that contains 50   categories .   SST5 . The Sentiment Analysis Treebank dataset ( Socher et al . , 2013 ) consists of 8,544 , 1,101 and 2,210   examples for training , development and test , respectively . The dataset is characterized by 5 categories   for sentiment , i.e. , 1 ( Very Negative ) 2 ( Negative ) , 3 ( Neutral ) , 4 ( Positive ) and 5 ( Very Positive ) and it   contains movie review sentences .   SNIPS . The SNIPS dataset ( Coucke et al . , 2018 ) consists of natural language commands for a voice   assistant . The commands are classiﬁed into 7 categories , i.e. , RateBook , BookRestaurant , AddToPlaylist ,   PlayMusic , GetWeather , SearchScreeningEvent , SearchCreativeWork . The dataset consists of 13,084 ,   700 , 700 examples for training , development and test , respectively .   SNLI . The SNLI dataset ( Bowman et al . , 2015 ) consists of examples of pairs for the Natural Language   Inference task . There are 3 categories : entailment , neutral andcontradiction . The dataset consists of   550,152 , 10,000 and 10,000 for training , development and test , respectively .   A.2 Qualitative Analysis   This section reports additional examples generated by DATS . In particular , we report the examples   generated with n= 2andl= 10 . Notice that , the generated examples are not in the original datasets .   It is worth noting that some examples may result odd , such as a question like “ Name the American   president born in 2005 ? ” but we only expected that they are linguistically sound and consistent with the   corresponding class . In general , the number nstrongly affects the example novelty : high values of nmake   the generator observe multiple times the same text as the target sentence , making the generation process   more conservative . As a result , when using n > 20the generator tends to produce the same examples   from the original dataset .   Question classiﬁcation . Here is a list of 20 questions generated by DATS for different classes . Please   refer to the original dataset description for the details about the speciﬁc classes .   •ABBR : exp What does D - DAY stand for ?   •DESC : def What does the term “ Italian Renaissance ” mean ?   •DESC : desc What process takes place after a hydrogen release ?   •DESC : reason Why do people go to the bathroom at night ?   •ENTY : animal What animal was the ﬁrst domesticated creature in the world ?   •ENTY : color What color is the cross on the French ﬂag ?   •ENTY : event What happened on February 27 , 1991 ?   •ENTY : food What cereal is “ sweet , soft and orange ” ?   •ENTY : plant What tree has the longest trunk ?   •ENTY : subst . What are diamond rings made of ?   •ENTY : veh What was the name of the U.S. Navy gunboat used by Dwight D. Eisenhower ?   •HUM : gr Name the two major companies in the energy industry .   •HUM : ind What ’s the name of the author of “ Harry Potter ” ?   •LOC : city What ’s the name of the largest city in Germany ?   •LOC : country What country in 1991 recorded the largest number of cocaine seizures ?   •NUM : count How many hundred ships sank in the Norwegian Sea in 1923 ?   •NUM : date What is the date of the ﬁrst inauguration of President Nixon?4594•NUM : money What is the cost of university admission to Stanford ?   •NUM : period How long does it take to clean up a cache ?   •NUM : weight What is the maximum weight for a healthy adult ?   Although the above examples seem coherent , we can argue if they can be really new , i.e. , useful for the   task according to a Data Augmentation process . We thus focused on the speciﬁc class ENTY : lang that   is underrepresented in the original dataset , with only the six original examples reported below :   •What is the name of a language spoken by the Sioux ?   •What is the only modern language that capitalizes its singular ﬁrst - person pronoun ?   •What is one of the languages spoken by the Sioux called ?   •What ’s the ofﬁcial language of Algeria ?   •What are the two languages of Malta ?   •What is the main language of Sao Paulo , Brazil ?   Below some of the synthetic examples generated by DATS with q=10 . It is worth noting that language   model introduces some expressions like sub - dialect that are novel with respect to the original training   material .   •What is the language of Switzerland ?   •What is the oldest language in the Americas ?   •What ’s the language spoken by the Kootenai people ?   •What is the sub - dialect of English ?   Conversely , if we only use the category name as a prompt ( i.e. , the OC model ) , the generated questions   will have a lower quality . For instance , with q= 10 andl= 10 , the OC model produces questions like :   1.What is the name of the island of Cote D ’ Azubis ?   2.What English language does the language speak ?   3.What languages of English and French are spoken as well as Arabic ?   4.What what languages would French be spoken ?   They have several issues , including label - inconsistency ( question 1 ) , malformed syntax ( question 4 )   and unclear semantics ( question 2 - 3 ) .   Similarly , the questions generated by EDA seems of lower quality . For example , with q= 10 , the EDA   model produces the questions :   1.What is the name of a language spoken by ?   2.What is the name of a language spoken by the Sioux ?   3.What is language mostly spoken in Brazil ?   4.What language is mostly spoken in Brazil ?   5.Name a gaelic language ?   6.What is one of the spoken communication spoken by the Sioux called ?   7.What is one of the languages spoken by the Sioux called ?   8.What ’s the most commonly verbalise language in Belgium ?   9.What ’s the most commonly spoken language in Belgium ?   Similarly , to the OC case , these questions have several problems , including malformed syntax ( question   1 , 3 , 8) , incompleteness ( question 1 ) .   Sentiment Analysis . Below is a list of statements generated by DATS in the movie review domain . To   simplify the qualitative analysis , we report only Negative ( class 1 in the dataset ) , Neutral ( class   3 ) and Positive ( class 5 ) examples . In general , the method generates sound judgments , with a high   syntactic variability even though a quite limited lexical variability , i.e. , a manual inspection suggested4595that most of the adjectives used to express judgments are reused in the generated examples . Sentences in   general are less complex than the original ones . This is reﬂected by generally shorter generated sentences ,   i.e. , with an average length of 16 words per review against the original 20 words per review .   Negative reviews :   •It ’s almost too derivative to stand on its own as a stark portrait of desperation and violence .   •A mediocre work of storytelling , lacking the slightest bit of wit or charm .   •A self - conscious , incoherent , self - interested fable .   Neutral reviews :   •It could be a lot worse if it were , well , more adventurous .   •If Tuck had ever made a movie about a vampire , it probably would look a lot like this one .   •It ’s very much like George Romero ’s ﬁnal work , where he had a hand in making huge cuts to his   movie – only it takes a whole lot more to feel good about .   Positive reviews :   •A gripping and wildly unpredictable comedy .   •It ’s lovely , funny , different , odd .   •A compelling example of why animation is a part of the human spirit .   The sentences generated using only the category as a prompt ( i.e. , OC model ) have a lower quality . For   instance , with the same q=10 the OC model sentences look like :   Negative reviews :   •Why , a good movie , an interesting and absorbing ﬁlm   •The , says a ﬁlmmaker and the ﬁlm ’s creator .   •The its full potential .   Neutral reviews :   •No at its most extreme .   •This and an epic comedy of a documentary .   •So a little razzle dazzle , but not at all riveting , .   Positive reviews :   •No the greatest of the time   •This the ﬁrst movie to touch a dark heart   •No in the form of a poem and a movie itself   Similarly , the sentences generated by EDA contain different issues , which can be explained by the   substitution and word swapping strategies adopted by this method . For example ,   •This tuxedo should have been sent back to the tailor for john major some major alterations   •If you value your time and money ﬁnd an escape clause rehash avoid seeing this trite predictable and   •Spy litigate ﬂick with Antonio Banderas and Lucy Liu never comes together   Neutral reviews :   •It would work a better as much one hour tv documentary   •The fast runner transports the viewer into an unusual space   •Often overwrought and at times positively irritating the ﬁlm turns into an engrossing thriller almost   in spite of4596Positive reviews :   •Documentary enjoyable high adrenaline enormously   •Quite simply a joy to watch follow and especially to listen to   •A solid and reﬁned piece of moviemaking imbued with passion and attitude   Intent Classiﬁcation . Below is a list of examples generated by DATS of each intent in the SNIPS dataset .   Please refer to the original dataset description for the details about the speciﬁc classes . In general , the   examples are different from the original ones introducing variability mostly on the involved named entities ,   e.g. , proper nouns of the music authors or places . In general , the syntactic complexity is the same as the   original material . Sometimes , odd dates are introduced ( here 2037 as a date for a reservation ) .   •AddToPlaylist add this track to my modern psychedelia playlist   •BookRestaurant book a diner for 1 on feb 28th 2037   •GetWeather will it be warm in michigantown at 07:00:00 am   •PlayMusic i want to listen to the last track by michael hayvoronsky   •RateBook i give the current essay a four   •SearchCreativeWork ﬁnd a painting called the night owl   •SearchScreeningEvent show me the movie schedule for national tv   Conversely , using only the category as input to the generation model , i.e. , the OC baseline , is not able   to produce high quality examples . For instance , with q=10 , the OC model generates sentences like :   •AddToPlaylist Add please rewind now my playlist   •BookRestaurant Book the menu for the night in the hotel room   •GetWeather Add a weather forecast from my backyard   •PlayMusic Play tunes and tracks to the chino sound bar   •RateBook Add some jazz music   •SearchCreativeWork Add my novel   •SearchScreeningEvent Add movies this week   Again , EDA is only able to generate some minor variations of the training examples , and sometimes   the swapping / substitution strategies are introducing issues . For example :   •AddToPlaylist I want this record album on my indie alternative playlist   •BookRestaurant I need a table for during midday in Montana   •GetWeather Is it going tea be freezing at to time in Michigantown KS   •PlayMusic Play any song from rebekah hewitt   •RateBook Rate this series one stars   •SearchCreativeWork The me show song spiderman of the rings   •SearchScreeningEvent I want the neediness movie schedule for animated movies in the area   Natural Language Inference . This task is the most challenging as the generated text is not only expected   to be internally consistent , but also pair - wise consistent . In general , the syntactic complexity of the   sentences is preserved , with an average length of 10 words per sentence . Similarly to the original training   set , in many entailment examples generated by DATS the premise and the hypothesis share the same verb   ( e.g. , watching in the ﬁrst example ) . On the contrary , the subject of the action is often changed during the   contradictions ( e.g. , boyvsgirl ) in the ﬁrst contradictory pair . Below we report some example pairs :   Text Pairs in Entailment   • “ Two men watching a sports event in the background . ” entails “ Two men are watching a game . ”   • “ A young woman , wearing sunglasses , is raising her hand . ” entails “ A woman is raising an arm ”   • “ A boy dressed to play soccer is playing ” soccer . entails “ The boy is playing outside . ” 4597Text Pairs in Contradiction   • “ Young Indian boy playing cricket and soccer . contradicts “ Young Indian girl playing cricket . ”   • “ A boy in a yellow shirt , orange and white , playing guitar . ” contradicts “ A boy is sitting quietly . ”   •“A man is attempting a jump on his skateboard . ” contradicts ” A man rides his bike over an obstacle   course . ”   Instead , the examples generated when only prompting the model with the category name are not as   good . For example , the OC model with q=10 produces the following examples :   Text Pairs in Entailment   • “ A man being photographed in a store in Seattle ” entails “ A man is sitting in a car . ”   • “ A woman leaves a beer in the air . ” entails “ A woman is leaving a beer . ”   •“A man walks along a street in St. Pete and walks past a city bus terminal . entails “ Police are   standing near the ground holding a man ”   Text Pairs in Contradiction   •“A white van is driving by and a white man is sitting next to it contradicts “ A man in a blue polo shirt   is sitting on the back of a white car . ”   •“from the middle of a street at a supermarket street , two girls wearing black and a yellow shirt . ”   contradicts “ A woman wearing a pink shirt looks down at a pole , waving a dollar bill . ”   • “ s a little boy playing a ball . ” contradicts ” A dog is jumping out of a ﬁeld . ”   Notice that the OC model is , in general , not able to produce label consistent pairs . For example , the   ﬁrst and third pairs reported above for the entailment category are not correct examples of this class .   In the following , we report also some examples generated with EDA . Again , EDA strategies introduces   some issues in the sentences . For example ,   Text Pairs in Entailment   •“An old man with gray hair wearing a scarf and black jacket ” entails “ An old woman is bundled up ”   • “ Two ladies are maam laughing on the street ” entails “ There are women ”   • “ Lady into the sky on city entails “ Lady looks into the sky ”   Text Pairs in Contradiction   • “ Young man doing a trick jump on a skateboard ” “ A man is writing record ”   • “ A by holds hands while walking couple buildings ” contradicts “ A couple dance in the street ”   •“A a man in a black shirt and shorts sitting at blond table wine with a glass of eating ” contradicts ” a   woman is drinking a beer ”   Besides introducing grammatical errors , EDA is also introducing label consistency errors ( for example ,   the ﬁrst entailment pair ) .   A.3 Are generated examples really different from the original ones ?   To better understand the advantages provided by DATS in the consistency - diversity trade - off , we here   report an exhaustive qualitative evaluation .   Table 3 reports all the SA examples generated by different models on classes 5 and 1 when using   q= 10 average training examples per class . Exception for a single case “ ( It is a masterpiece , brilliantly   directed , and incredibly well done . ) ” that is not a negative sentiment example as it was supposed to be ) , the   examples generated by DATS ( with n= 1andl= 1 ) are all label - consistent . Conversely , many examples   generated with the OC baseline have a wrong sentiment . On the other side , the diversity introduced by   DATS is impressive : all the examples are very different from the input example used to condition the   NLG model while generating them . At the same time , they are also very novel with respect to the full4598training set ( used to train the NLG model ) and therefore can be very useful when augmenting the training   data of NLU models . On the opposite , BT and EDA introduce very minor modiﬁcations to the input text ,   resulting in a signiﬁcant lower diversity .   Table 4 provides examples of generated questions for the QC task . This task is particularly different   from SA as it involves 50 categories and the size of the training dataset causes many of them to be   underrepresented . We generated a set of questions with DATS ( q= 100 , n= 1andl= 1 ) by selecting   one instance for each category as the input to the model . Each row of the table thus reports the class and   example used in input to the generator and the last column reports the generated example .   As it can be noticed , DATS generates examples different from the input ones : only 4 out of 50 ( 8 % )   examples are exactly copied and 3 out of 50 ( 6 % ) are different with respect to only one word . Notice   that the QC task is particularly sensitive to the syntactic structure of the sentence and this is generally   preserved in the generation process . DATS very rarely generates a paraphrase of the original sentence .   The target of the generated question almost always changes ( e.g. , when using ” Who is Nicolo Paganini ? “   as input , DATS generates ” Who is Michael Jackson ? “ ) . Finally , notice that in all cases DATS is able to   generate a question which is coherent with the target category of the input question.4599   A.4 Effects of Hyper - parameters   This section reports an analysis of the role of DATS hyper - parameters . In particular , we show the results   by varying ( i ) the number of most similar elements nused to generate the pairs for training Mand   ( ii ) the number of elements lgenerated with nucleous sampling . Speciﬁcally , we tried n∈[1,2,5]and   l∈[1,2,3,5,7,10 ] . In ﬁgure 1 we report the difference in Accuracy between our approach and the NoDA   baseline for each speciﬁc conﬁguration . Each ﬁgure refers to a speciﬁc q(i.e . , the average number of   examples per class ) value , and reports the average delta accuracy computed on the QC , SA and IC tasks .   In general , when using small datasets it is beneﬁcial to use higher nvalues , i.e. , to create more pairs for   training the NLG model . In fact , when q=10 we can observe that the best improvement is obtained with   n=2 or n=5 .   Regarding the number lof examples generated through M , we can observe that generating a higher   number of examples seems to be beneﬁcial in almost every case , especially when dealing with a small   dataset . For example , when q=10 or q=50 , our DA approach provides the best performance with l= 7   orl=10 . More generally , we can observe a positive trend in generating at least l=5 examples with the   generation model.46004601