  Zheng Gong , Kun Zhou , Wayne Xin Zhao , Jing Sha ,   Shijin Wang , Ji - Rong WenGaoling School of Artificial Intelligence , Renmin University of ChinaSchool of Information , Renmin University of ChinaiFLYTEK Research , Hefei , Anhui , ChinaBeijing Key Laboratory of Big Data Management and Analysis Methods , Beijing , ChinaState Key Laboratory of Cognitive Intelligence , Hefei , Anhui , ChinaAI Research(Central China ) , iFLYTEK , Wuhan , Hubei , China   Abstract   In this paper , we study how to continually pre-   train language models for improving the under-   standing of math problems . Specifically , we   focus on solving a fundamental challenge in   modeling math problems , i.e. ,how to fuse the   semantics of textual description and formulas ,   which are highly different in essence . To ad-   dress this issue , we propose a new approach   called COMUS tocontinually pre - train lan-   guage models for math problem understanding   with syntax - aware memory network . In this   approach , we first construct the math syntax   graph to model the structural semantic informa-   tion , by combining the parsing trees of the text   and formulas , and then design the syntax - aware   memory networks to deeply fuse the features   from the graph and text . With the help of syntax   relations , we can model the interaction between   the token from the text and its semantic - related   nodes within the formulas , which is helpful   to capture fine - grained semantic correlations   between texts and formulas . Besides , we de-   vise three continual pre - training tasks to further   align and fuse the representations of the text   and math syntax graph . Experimental results   on four tasks in the math domain demonstrate   the effectiveness of our approach . Our code and   data are publicly available at the link : https :   //github.com / RUCAIBox / COMUS .   1 Introduction   Understanding math problems via automated meth-   ods is a desired machine capacity for artificial in-   telligence assisted learning . Such a capacity is the   key to the success of a variety of educational appli-   cations , including math problem retrieval ( Reusch   et al . , 2021 ) , problem recommendation ( Liu et al . ,   2018 ) , and problem solving ( Huang et al . , 2020 ) .   To automatically understand math problems , it   is feasible to learn computational representationsFigure 1 : Illustration of a math problem with its textual   description and math syntax graph .   from problem statement texts with pre - trained lan-   guage models ( PLMs ) ( Shen et al . , 2021 ; Peng   et al . , 2021 ) . Pre - trained on the large - scale gen-   eral corpus , PLMs ( Devlin et al . , 2019 ) can be   effectively transferred into new domains or tasks   by continual pre - training on task - specific datasets .   Different from traditional text comprehension tasks ,   as shown in Figure 1 , math problems usually in-   volve a complex mixture of mathematical symbols ,   logic and formulas , which becomes a barrier to the   accurate understanding of math problems .   However , previous works ( Reusch et al . , 2021 ;   Shen et al . , 2021 ) mostly oversimplify the issues   of math problem understanding . They directly con-   catenate the formulas with the textual description   as an entire sentence , and then perform continual   pre - training and encoding without special consid-   erations . Therefore , two major shortcomings are   likely to affect the understanding of math problems .   First , formulas ( the most important elements of the   problem ) contain complex mathematical logic , and   modeling them as plain text may incur the loss of   important information . Second , the textual descrip-   tion contains essential explanations or hints about   the symbols and logic within the formulas . Hence ,   it is necessary to accurately capture fine - grained5923correlations between words from the description   text and symbols from math formulas .   To better model the computational logic of for-   mulas , operator trees are introduced to represent the   math formulas ( Zanibbi and Blostein , 2012 ) , which   are subsequently encoded by graph neural network   ( GNN ) . Although these methods can improve the   comprehension capacity of math problems to some   extent , there still exists a semantic gap between   graph encoding and text encoding due to the hetero-   geneity of formulas and texts . With simple concate-   nation or self - attention mechanisms ( Peng et al . ,   2021 ) , it is still hard to capture the fine - grained   associations among tokens and symbols , e.g. , the   dependency relation between math symbols and   corresponding explanation tokens .   In order to better fuse the information from for-   mulas and texts , our solution is twofold . First , we   construct a syntax - aware memory network based   on a structure called math syntax graph ( Figure 1 ) ,   which integrates operator trees from formulas and   syntax trees from texts . The key point lies in that   we store the node embeddings from the GNN and   dependency relation embeddings as entries of mem-   ory networks , and then design the corresponding   read and write mechanism , using token embed-   dings from the PLM as queries . Such a way can   effectively associate the representation spaces of   the text and formulas . Second , we devise specific   continual pre - training tasks to further enhance and   fuse the text and graph representations , including   the masked language model and dependency triplet   completion tasks to improve the understanding of   math symbols in the text and formulas logic in   the syntax graph , respectively , and the text - graph   contrastive learning task to align and unify the rep-   resentations of the text and graph .   To this end , we propose COMUS , tocontinually   pre - train language models for math problem   understanding with syntax - aware memory network .   In our approach , we first encode the textual de-   scription and math syntax graph via PLM and GAT ,   respectively . Then , we add syntax - aware memory   networks between the last klayers of PLM and   GAT . In each of the last klayers , we first conduct   the multi - view read and write operation to fuse   the token and node representations , respectively ,   and then adopt the next layer of PLM and GAT   to encode the fused representations . All parame-   ters of our model are initialized from PLMs and   will be continually pre - trained by our devised threetasks , namely masked language model , dependency   triplet completion and text - graph contrastive learn-   ing . Experimental results on four tasks in the math   domain have demonstrated the effectiveness of our   approach , especially with limited training data .   Our contributions can be summarized as follows :   ( 1 ) We construct a novel syntax - aware memory   network to capture the fine - grained interactions   between the text and formulas .   ( 2 ) We design three continual pre - training tasks   to further align and fuse the representations of the   text and graph data .   ( 3 ) Experiments on four tasks in the math do-   main demonstrate the effectiveness of our model .   2 Preliminaries   In this section , we formulate the problem statement   and then introduce the math syntax graph .   Problem Statement . Generally , a math problem   consists of a textual description dand several for-   mulas { f , f , · · · , f } . The textual description   provides necessary background information for the   math problem . It is formally denoted as a sequence   of tokens d={t , t , · · · , t } , where tis either   a word token or a mathematical symbol ( e.g. , a   number or an operator ) . The formulas describe the   relationship among mathematical symbols , which   is the key to understand and solve the math problem .   Each formula consists of a sequence of mathemati-   cal symbols , denoted as f={s , · · · , s } .   Based on the above notations , this work focuses   on continually pre - training a PLM on unsupervised   math problem corpus for domain adaptation . After   that , the PLM can be fine - tuned on various tasks in   the math domain ( e.g. , knowledge point classifica-   tion ) , and improve the task performance .   Math Syntax Graph . In order to understand the   mathematical text and formulas , it needs to capture   the complex correlations within words , symbols   and operators . Inspired by previous works ( Man-   souri et al . , 2019 ; Peng et al . , 2021 ) , we construct   asyntax graph , where the textual description is   represented as a syntax dependency tree and the   formulas are represented as operator trees ( OPT ) .   Specifically , given a math problem consisting   of a textual description dand several formulas   { f , f , · · · , f } , we first utilize the open - source   toolkit TangentSto convert each formula into an5924OPT , and Stanzato convert the textual description   into a syntax dependency tree . Then , we com-   bine the syntax dependency tree and the OPTs to   compose an entire graph , where a special token   “ [ MATH ] ” is applied to link them . We call such   a composite graph as the math syntax graph Gof   the math problem . Let NandRdenote the set   of nodes and relations on G , respectively . We can   extract dependency triplets from G , where a depen-   dency triplet ( h , r , t ) denotes that there exists an   edge with the relation r∈ R to link the head node   h∈ N to the tail node t∈ N .   3 Methodology   As shown in Figure 2 , our approach aims to effec-   tively encode the textual description and formulas ,   and fuse these two kinds of information for under-   standing math problems . In what follows , we first   present the base models for encoding math prob-   lems , and then introduce the devised syntax - aware   memory network and continual pre - training tasks .   3.1 Base Models   Encoding Math Text . We use BERT ( Devlin et al . ,   2019 ) as the PLM to encode the math text , i.e. ,the   textual description d. Given d={t , t , · · · , t }   of a math problem , the PLM first projects these to-   kens into corresponding embeddings . Then , a stack   of Transformer layers will gradually encode the em-   beddings to generate the l - th layer representations   { h , h,···,h } . Since the textual description   dmay contain specific math symbols that were not   seen during pre - training , we add them into the vo-   cabulary of the PLM and randomly initialize their   token embeddings . These new embeddings will be   learned during continual pre - training .   Encoding Math Syntax Graph . We incorporate a   graph attention network ( GAT ) ( Veli ˇckovi ´ c et al . ,   2018 ) to encode the math syntax graph , which is   composed of an embedding layer and a stack of   graph attention layers . Given a math syntax graph   Gwith Nnodes , the GAT first maps the nodes   into a set of embeddings { n , n,···,n } . Then   each graph attention layer aggregates the neighbors ’   hidden states using multi - head attentions to update   the node representations as :   n = σ(XαWn ) . ( 1)where n is the representation of the i - th node   in the l+ 1layer , denotes the concatenation op-   eration , σdenotes the sigmoid function , Kis the   number of attention heads , Nis the set of neigh-   bors of node iin the graph , Wis a learnable   matrix , and αis the attention value of the node i   to its neighbor jin attention head k.   3.2 Syntax - Aware Memory Network   To improve the semantic interaction and fusion   of the representations of math text and the syntax   graph , we add ksyntax - aware memory networks   between the last klayers of PLM and GAT . In the   memory network , node embeddings ( from the math   syntax graph ) with dependency relations are con-   sidered as slot entries , and we design multi - view   read / write operations to allow token embeddings   ( e.g. ,explanation tokens or hints ) to attend to highly   related node embeddings ( e.g. , math symbols ) .   Memory Initialization . We construct the mem-   ory network based on the dependency triplets and   node representations of the math syntax graph .   Given the dependency triplets { ( h , r , t ) } , we treat   the head and relation ( h , r)as the key and the   tailtas the value , to construct a syntax - aware   key - value memory . The representations of the   heads and tails are the corresponding node rep-   resentations from GAT , while the relation repre-   sentations are randomly initialized and will be   optimized by continual pre - training . Finally , we   concatenate the representations of heads and rela-   tions to compose the representation matrix of Keys   asK={[n;r],[n;r],···,[n;r ] } ,   and obtain the representation matrix of Values as   V={n , n,···,n } .   Multi - view Read Operation . We read important   semantics within the syntax - aware memory to up-   date the token representations from PLM . Since   a token can be related to several nodes within   the math syntax graph , we design a multi - view   read operation to capture these complex seman-   tic associations . Concretely , via different bilinear   transformation matrices { W , W,···,W } ,   we first generate multiple similarity matrices   { S , S,···,S}between tokens and keys ( head   and relation ) within the memory , and then aggre-   gate the values ( tail ) to update the token represen-   tations . Given the token representations from the   l - th layer of PLM H={h , h,···,h},5925   the similarity matrix Sis computed as   S = HWK(2 )   where Wis a learnable matrix , and an entry   S[j , k]denotes the similarity between the j - th to-   ken and the k - th key in the i - th view . Based on   these similarity matrices , we update the token repre-   sentations by aggregating the value representations   as   ˆH = H+ [ αV;αV;···;αV]W(3 )   α = softmax ( S ) ( 4 )   where Wis a learnable matrix and αis the at-   tention score distribution along the key dimension .   In this way , we can capture the multi - view corre-   lations between tokens and nodes , and the token   representations can be enriched by the represen-   tations of multiple semantic - related nodes . After   that , the updated token representations ˆHare fed   into the next layer of PLM , where the Transformer   layer can capture the interaction among token rep-   resentations to fully utilize the fused knowledge   from the syntax graph .   Multi - View Write Operation . After updating   the token representations , we update the represen-   tations of nodes from GAT via memory writing .   We still utilize the multi - view similarity matrices   { S , S,···,S } . Concretely , we compute the at-   tention score distribution βusing softmax functionalong the token dimension of the similarity matri-   ces , and then aggregate the token representations   as   V= [ βH;βH;···;βH]W(5 )   β = softmax ( S ) ( 6 )   where Wis a learnable matrix . Based on the   aggregated token representations , we incorporate a   gate to update the representations of the values as   z = σ(VW+VW ) ( 7 )   ˆV = z·V+ ( 1−z)·V(8 )   where WandWare learnable matrices . The   updated node representations ˆVare also fed into   the next layer of GAT , where the graph attention   mechanism can further utilize the fused knowledge   from the text to aggregate more effective node rep-   resentations .   3.3 Continual Pre - training   Continual pre - training aims to further enhance and   fuse the math text and math syntax graph . To   achieve it , we utilize the masked language model   and dependency triplet completion tasks to improve   the understanding of math text and math syntax   graph , respectively , and the text - graph contrastive   learning task to align and fuse their representations.5926Masked Language Model ( MLM ) . Since the math   text contains a number of special math symbols ,   we utilize the MLM task to learn it for better under-   standing the math text . Concretely , we randomly se-   lect 15 % tokens of the input sequence to be masked .   Of the selected tokens , 80 % are replaced with a spe-   cial token [ MASK ] , 10 % remain unchanged , and   10 % are replaced by a token randomly selected   from the vocabulary . The objective is to predict the   original tokens of the masked ones as :   L = X−logp(t ) ( 9 )   whereV is the set of masked tokens , and p(t )   denotes the probability of predicting the original   token in the position of t.   Dependency Triplet Completion ( DTC ) . In the   math syntax graph , the correlation within the de-   pendency triplet ( h , r , t ) is essential to understand   the complex math logic of the math problem . Thus ,   inspired by TransE ( Bordes et al . , 2013 ) , we design   the dependency triplet completion task to capture   the semantic correlation within a triplet . Specifi-   cally , for each triplet ( h , r , t ) within the math syn-   tax graph , we minimize the DTC loss by   L = max    γ+d(n+r , n)−d(n+r , n),0   ( 10 )   where γ > 0is a margin hyper - parameter , d(·)is   the euclidean distance , and ris the randomly sam-   pled negative relation embedding . In this way , the   head and relation embeddings can learn to match   the semantics of the tail embeddings , which en-   hances the node and relation representations by   capturing the graph structural information .   Text - Graph Contrastive Learning ( TGCL ) . Af-   ter enhancing the representations of the math text   and math syntax graph via MLM and DTC tasks re-   spectively , we further align and unify the two types   of representations . The basic idea is to adopt con-   trastive learning to pull the representations of the   text and graph of the same math problem together ,   and push apart the negative examples . Concretely ,   given a text - graph pair of a math problem ( d , G ) ,   we utilize the representation of the [ CLS ] token   has the sentence representation of d , and the   mean pooling of the node representations nas   the graph representation of G. Then , we adopt the   cross - entropy contrastive learning objective within - batch negatives to align the two representations   L = −logexp(f(h , n)/τ)Pexp(f(h , n)/τ)(11 )   where f(·)is a dot product function and τdenotes   a temperature parameter . In this way , the represen-   tations of the text and graph can be aligned , and the   data representations from one side will be further   enhanced by another side .   3.4 Overview and Discussion   Overview . Our approach focuses on continually   pre - training PLMs to improve the understanding   of math problems . Given the math text and math   syntax graph of the math problem , we adopt PLM   and GAT to encode them , respectively , and utilize   syntax - aware memory networks in the last klayers   to fuse the representations of the text and graph .   In each of the last klayers , we first initialize the   queries and values of the memory network using   the representations of tokens and nodes , respec-   tively , then perform the read and write operations   to update them using Eq . 3 and Eq . 8 . After that ,   we feed the updated representations into the next   layers of PLM and GAT to consolidate the fused   knowledge from each other . Based on such an ar-   chitecture , we adopt MLM , DTC and TGCL tasks   to continually pre - train the model parameters using   Eq . 9 , Eq . 10 and Eq . 11 . Finally , for downstream   tasks , we fine - tune our model with specific data   and objectives , and concatenate the representations   of text hand graph nfrom the last layer for   prediction .   Discussion . The key of our approach is to deeply   fuse the math text and formula information of the   math problem via syntax - aware memory networks   and continual pre - training tasks . Recently , Math-   BERT ( Peng et al . , 2021 ) is proposed to continually   pre - train BERT in math domain corpus , which ap-   plies the self - attention mechanism for the feature   interaction of formulas and texts , and learns simi-   lar tasks as BERT . As a comparison , we construct   the math syntax graph to enrich the formula in-   formation and design the syntax - aware memory   network to fuse the text and graph information . Via   the syntax - aware memory network , the token from   math text can trace its related nodes along the rela-   tions in the math syntax graph , which can capture   the fine - grained correlations between tokens and   nodes . Besides , we model the math syntax graph5927   via GAT , and devise the DTC task to improve the   associations within triplets from the graph , and the   TGCL task to align the representations of the graph   and text . In this way , we can better capture graph   structural information and fuse it with textual infor-   mation . It is beneficial for understanding logical   semantics from formulas of math problems .   4 Experiment   4.1 Experimental Setup   We conduct experiments on four tasks in the math   domain to verify the effectiveness of our approach .   Pre - training Corpus . Our pre - training corpus   is collected from a Chinese educational website   Zhixue , which consists of 1,030,429 problems of   high school math exams and tests . Each math prob-   lem contains the information of problem statement ,   answer and solution analysis . For data preprocess-   ing , we first transform these collected problems   from the HTML format into plain text format , then   extract and convert the formulas and mathematical   symbols into a unified LaTex mathematical format .   Evaluation Tasks . We construct four tasks based   on the collected math problems for high school   students , which cover math problem classification   and recommendation . The statistics of these tasks   are summarized in Table 1 .   •Knowledge Point Classification ( KPC ) is a   multi - class classification task . Given a math ques-   tion , the goal is to classify what knowledge point   ( KP ) this question is associated with . The knowl-   edge points are defined and annotated by profes-   sionals , and we finally have 387 KPs in this task .   •Question - Answer Matching ( QAM ) is a bi-   nary classification task to predict whether an an-   swer is matched with a question . For each question ,   we randomly sample an answer from other prob-   lems as the negative example .   •Question Relation Classification ( QRC ) is   a 6 - class classification task . Given a pair of math   questions , this task aims to predict their relation(e.g . , equivalent , similar , problem variant , condi-   tional variant , situation variant , irrelevant ) .   •Similar Question Recommendation ( SQR ) is   a ranking task . Given a question , this task aims to   rank retrieved candidate questions by the similarity .   Evaluation Metrics . For classification tasks ( KPC ,   QRC , QAM ) , we adopt Accuracy and F1 - macro   as the evaluation metrics . For the recommen-   dation task ( SQR ) , we employ top- kHit Ratio   ( HR@ k ) and top- kNormalized Discounted Cu-   mulative Gain ( NDCG@ k ) for evaluation . Since   the length of candidate list is usually between 6 and   15 , we report results on HR@3 and NDCG@3 .   Baseline Methods . We compare our proposed ap-   proach with the following nine baseline methods :   •TextCNN ( Kim , 2014 ) is a classic text classifi-   cation model using CNN on top of word vectors .   •TextRCNN ( Lai et al . , 2015 ) combines both   RNN and CNN for text classification tasks .   •GAT ( Veli ˇckovi ´ c et al . , 2018 ) utilizes the at-   tention mechanism to aggregate neighbors ’ repre-   sentations to produce representation for each node .   •R - GCN ( Schlichtkrull et al . , 2018 ) extended   Graph Convolutional Network with multi - edge en-   coding to aggregate neighbors ’ representations .   •BERT - Base ( Devlin et al . , 2019 ) is a popular   pre - trained model . We use the bert - base - chinese ,   and add some new tokens into the original vocab to   represent specific symbols in math problem dataset .   •DAPT - BERT ( Gururangan et al . , 2020 ) con-   tinually pre - trains BERT on the domain - related   corpus . We use our collected math problem dataset   with the masked language model task for imple-   mentation .   •BERT+GAT concatenates the [ CLS ] embed-   ding from BERT and mean node embedding from   GAT as the representation of a math question .   •DAPT - BERT+GAT replaces BERT in   BERT+GAT with the DAPT - BERT .   •MathBert ( Peng et al . , 2021 ) continually pre-   train BERT on the math corpus with similar pre-   training tasks , and revises the self - attention layers   for encoding the OPT of formulas .   Implementation Details . For baseline models , all   hyper - parameters are set following the suggestions   from the original papers . For all PLM - related mod-   els , we implement them based on HuggingFace   Transformers(Wolf et al . , 2020 ) . For the models5928   combining PLM and GAT , we set GAT ’s number   of layer , attention head and hidden states as 6 , 12   and 64 , respectively . And we set the number of   syntax - aware memory network layers kas 2 for   our proposed COMUS .   In the continual pre - training stage , we initialize   the weights of all models with bert - base - chinese   and pre - train them on our pre - training corpus with   the same hyper - parameter setting as follows . We   continually pre - train the parameters with a total   of 128 batch size for 100,000 steps . And the max   length of input sequences is set as 512 . We use   AdamW ( Loshchilov and Hutter , 2019 ) optimiza-   tion with β= 0.9 , β= 0.999 , and apply learning   rate warmup over the first 5 % steps , and linear de-   cay of the learning rate . The learning rate is set   as 1e . We set τas 0.07 for our TGCL tasks .   It costs about 40 hours to perform the continual   pre - training on 4 Tesla - V100 - PCIE-32 G GPUs .   During fine - tuning on downstream tasks , we use   AdamW with the same setting as pre - training . And   batch size for all experiments is set as 32 . The   learning rate is set to 3efor pre - training based   methods , and 1efor other methods.4.2 Main Results   The results of all the comparison methods on four   tasks are shown in Table 2 . Based on these results ,   we can find :   As for non - pre - training methods , text - based   methods ( i.e. ,TextCNN and TextRCNN ) outper-   form GNN - based methods ( i.e. ,GAT and R - GCN ) .   It indicates that text representations are more ca-   pable of understanding math problems than graph   representations in our dataset . Overall , non - pre-   training methods perform worse than pre - training   based methods , since pre - training based models   have learned sufficient general knowledge during   the pre - training on large - scale corpus .   Among the five pre - training methods , we can   have two major findings . First , combining PLMs   with GNN yields performance improvement in   most cases . The reason is that GNN can capture   the structural semantics from formulas as the aux-   iliary information to help PLMs understand the   math problem , but the improvement is unstable ,   since these methods simply concatenate the rep-   resentations of the text and graph without deeply   fusing them . Second , continual pre - training brings   a significant improvement on all the evaluation5929   tasks . General - purpose PLMs ca n’t effectively un-   derstand mathematical semantics , and it is the key   to adapt them to the math domain via continual   pre - training .   Finally , by comparing our approach with all the   baselines , it is clear to see that our model performs   consistently better than them on four tasks . We   utilize the syntax - aware memory network to fuse   and interact the representations of textual descrip-   tions and formulas , and adopt three continual pre-   training tasks to further align and enhance these   representations . Among these results , we can see   that our model achieves a large improvement on   the KPC task . A possible reason is that it requires   a deeper semantic fusion of formulas and text for   identifying the correct knowledge points .   4.3 Few - shot Learning   To validate the reliability of our method under the   data scarcity scenarios , we conduct few - shot exper-   iments on KPC and QRC tasks by using different   proportions of the training data , i.e. ,5 % , 10 % , 20 %   and 40 % . We compare our model with DAPT-   BERT , DAPT - BERT+GAT and MathBERT .   Table 3 shows the evaluation results with dif-   ferent ratios of training data . We can see that the   performance substantially drops when the size of   training set is reduced . However , our model per-   forms consistently better than the others across   different tasks and metrics . It demonstrates that our   model is capable of leveraging the data more effec-   tively with the help of the syntax - aware memory   networks and continual pre - training tasks . With   5 % training data , our model exceeds the best base-   line by a large margin . It further indicates that our   model is more robust to the data scarcity problem .   4.4 Ablation Study   Our proposed approach contains several comple-   mentary modules and pre - training tasks . Thus , we   conduct experiments on KPC and QRC tasks to   verify the contribution of these modules and tasks .   Concretely , we remove the module GAT , BERT ,   Syntax - Aware Memory Network , or the task MLM ,   DTC and TGCL , respectively .   In Table 4 , we can see that the performance   drops by removing any modules or pre - training   tasks . It shows the effectiveness of these mod-   ules or pre - training tasks in our proposed model .   Especially , the model performance significantly   decreases when we removing the textual encoder   BERT , which implies that the text representations   are more important for math problem understand-   ing . Besides , we can see that removing MLM also   results in a large performance drop , since it is the   key pre - training task for our text encoder .   4.5 Hyper - Parameters Analysis   Our proposed model contains a few parameters to   tune . In this part , we tune two parameters and   examine their robustness on model performance ,   i.e. ,the number of GAT Layer and the continual   pre - training steps . We conduct experiments on   KPC and QRC tasks and show the change curves   of Accuracy in Figure 3 .   We can observe that our model achieves the best   performance in 80k steps . It indicates that our   model can be improved by continual pre - training   gradually and may overfit after 80k steps . Besides ,   our model achieves the best performance with 6   GAT layers , which shows that 6 GAT layers are suf-   ficient to capture the information in syntax graph .   5 Related Work   In this section , we review the related work from the   following two aspects , namely math problem un-   derstanding and continual pre - training of language   models .   Math Problem Understanding . Math problem   understanding tasks focus on understanding the   texts , formulas and symbols in math domain . A5930surge of works aim to understand the math for-   mulas for problem solving or mathematical infor-   mation retrieval . In a typical way , the formula is   usually transformed as a tree or graph ( e.g. , Oper-   ator Tree ( Zanibbi and Blostein , 2012 ) ) , then net-   work embedding method ( Mansouri et al . , 2019 )   and graph neural network ( Song and Chen , 2021 )   are utilized to encode it . Besides , a number of   works focus on understanding math problem based   on the textual information . Among them , Math   Word Problem ( MWP ) Solving is a popular task   that generates executable mathematical expression   for the math word problem to produce the final an-   swer . Numerous deep learning based methods have   been proposed to tackle the MWP task , including   Seq2Seq ( Chiang and Chen , 2019 ; Li et al . , 2019 ) ,   Seq2Tree ( Wang et al . , 2019 ; Qin et al . , 2020 ) , and   Pre - trained Language Models ( Kim et al . , 2020 ;   Liang et al . , 2021 ) . More recently , several stud-   ies attempt to model more complex math prob-   lems ( Huang et al . , 2020 ; Hendrycks et al . , 2021 )   that require a deep understanding of both textual   and formula semantics .   Continual Pre - training of Language Models .   Continually pre - training can effectively improve   pre - trained model ’s performance on new domains   or downstream tasks ( Gururangan et al . , 2020 ) . To   achieve it , most of previous works either continu-   ally optimize the model parameters with BERT - like   tasks on domain or task related corpus ( e.g. , scien-   tific ( Beltagy et al . , 2019 ) and bio - media ( Lee et al . ,   2020 ) ) , or design new pre - training objectives for   task adaption ( e.g. , commonsense reasoning ( Zhou   et al . , 2021 ) and dialogue adaption ( Li et al . , 2020 ) ) .   Besides , several works ( Wang et al . , 2020 ; Xiang   et al . , 2020 ) utilize both domain - related corpus   and new pre - training objectives for continual pre-   training , or revise the Transformer structure of   PLMs for better adaption ( Ghosal et al . , 2020 ) . For   math problem understanding , the recently proposed   MathBERT ( Peng et al . , 2021 ) adopts math domain   corpus and formula - related pre - training tasks for   continual pre - training .   6 Conclusion and Future Work   In this paper , we proposed COMUS , a continual   pre - training approach for math problem understand-   ing . By integrating the formulas with the syntax   tree of mathematical text , we constructed the math   syntax graph and designed the syntax - aware mem-   ory network to fuse the semantic information fromthe text and formulas . In the memory network ,   we treated tokens from the text and triplets from   the graph as the queries and slot entries , respec-   tively , and modeled the semantic interaction be-   tween tokens and their semantic - related nodes via   multi - view read and write operations . Besides , we   devised three continual pre - training tasks to fur-   ther enhance and align the representations of the   textual description and math syntax graph of the   math problem . Experimental results have shown   that our approach outperforms several competitive   baselines on four tasks in the math domain .   In future work , we will consider applying our   method to solve more difficult math - related tasks ,   e.g. , automatic math problem solving and analysis   generation . Besides , we will also consider incor-   porating external math domain knowledge into our   model to improve the understanding of mathemati-   cal logic and numerical reasoning .   Ethical Consideration   In this part , we discuss the main ethical considera-   tion of this work : ( 1 ) Privacy . The data adopted in   this work ( i.e. ,pre - training corpus and fine - tuning   data ) is created by human annotation for research   purposes , and should not cause privacy issues . ( 2 )   Potential Problems . PLMs have been shown to cap-   ture certain biases from their pre - trained data ( Ben-   der et al . , 2021 ) . There are increasing efforts to   address this problem in the community ( Ross et al . ,   2021 ) .   Acknowledgement   This work was partially supported by Beijing Natu-   ral Science Foundation under Grant No . 4222027 ,   and National Natural Science Foundation of China   under Grant No . 61872369 , Beijing Outstand-   ing Young Scientist Program under Grant No .   BJJWZYJH012019100020098 , the Outstanding In-   novative Talents Cultivation Funded Programs 2021   and Public Computing Cloud , Renmin University   of China . This work is also supported by Beijing   Academy of Artificial Intelligence ( BAAI ) . Xin   Zhao is the corresponding author .   References593159325933