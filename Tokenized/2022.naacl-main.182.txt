  Nasim Nouri   Raouf Medical Group   Tehran , Iran   nasimnouri@raoufmed.com   Abstract   Text style transfer ( TST ) is a well - known task   whose goal is to convert the style of the text   ( e.g. , from formal to informal ) while preserv-   ing its content . Recently , it has been shown   that both syntactic and semantic similarities   between the source and the converted text are   important for TST . However , the interaction   between these two concepts has not been mod-   eled . In this work , we propose a novel method   based on Optimal Transport for TST to simulta-   neously incorporate syntactic and semantic in-   formation into similarity computation between   the source and the converted text . We eval-   uate the proposed method in both supervised   and unsupervised settings . Our analysis reveal   the superiority of the proposed model in both   settings .   1 Introduction   Text style transfer ( TST ) is an important task in   NLP that aims to change the style of a given text   from source style to target style ( e.g. , formal to   informal ) while preserving its content . For instance ,   the formal sentence “ However , I do believe it to   be punk " is converted to the informal equivalent   sentence “ I ’d say it is punk though " . This task   could be helpful for downstream applications such   as text simplification , information extraction , and   question answering .   Due to the importance of TST , this task has been   approached with different techniques ranging from   feature - based models ( Xu et al . , 2012 ) to recent ad-   vanced deep learning solutions ( Chen et al . , 2018 ;   Lee et al . , 2021a ; Huang et al . , 2021 ) . The recent   work can be categorized as supervised ( i.e. , parallel   corpus with sentences in source and target style )   ( Lai et al . , 2021 ) , unsupervised ( i.e. , sentences in   source and target style are available but they are not   aligned ) ( Krishna et al . , 2020 ) , or semi - supervised   ( combination of parallel and non - aligned corpora )   ( Chawla and Yang , 2020 ) methods . The three crit - ical objectives of any TST system are to ( 1 ) gen-   erate a text in the target style , ( 2 ) keep the con-   tent of the source text , and ( 3 ) generate fluent sen-   tences ( Krishna et al . , 2020 ) . It has been shown that   fine - tuning transformer - based language models on   each of these objectives ( i.e. , using Reinforcement   Learning ) can achieve promising results ( Lai et al . ,   2021 ; Liu et al . , 2021 ) . However , one of the lim-   itations of the existing works is that the content   preservation ( i.e. , the second objective ) is fulfilled   at either the surface - form level ( i.e. , by encouraging   the same words to appear in both texts ) ( Lai et al . ,   2021 ) or at the semantics level ( i.e. , by encourag-   ing high mutual information between the two texts )   ( Chawla and Yang , 2020 ) ; ignoring the role of syn-   tactic information . Syntactic information ( e.g. , de-   pendency tree ) can be used to explicitly encode   the connections between the words of the sentence ,   thereby playing an important role in the equiva-   lency of two sentences . For instance , consider   the source sentence “ a crap touch bar with a nice   screen ! ! ! " and the converted sentence “ The screen   is great but the touch bar is terrible " . The corre-   sponding dependency between “ touch bar →crap "   in the source sentence and “ terrible →touch bar "   in the target sentence and also “ screen →nice " in   the source sentence and “ great→screen " in the   target sentence are helpful to assess the equivalency   of the two sentences . Although the pre - trained lan-   guage models such as BERT have been shown to   be able to encode the syntactic information , it is   not yet verified that these models can take into ac-   count the syntactic dependencies when computing   the similarity between two sentences , especially for   the TST task . To the best of our knowledge , there   is one prior work that shows the importance of the   syntactic information for transformer - based TST   models ( Ma et al . , 2019 ) . Specifically , Ma et al .   ( 2019 ) shows that reconstructing both the words of   the source text and their POS tags could boost the   performance of TST . However , there are two limi-2532tations in this work : ( 1 ) the syntactic structure ( i.e. ,   dependencies between words ) is ignored ; ( 2 ) the   interaction between semantics and the syntax of the   sentences is neglected . More specifically , to obtain   the most value of the syntactic information , it is   crucial to consider the relations between the words   and also their semantics as shown in the example   above . As such , in this work , we propose a novel   method to simultaneously incorporate the interac-   tion between syntax and semantics of the sentences   into the content preservation objective of TST train-   ing . More specifically , for the first time in text style   transfer , we propose to use Optimal Transport ( OT )   as an efficient method to consider both the syn-   tax and the semantics of the two sentences when   computing their content similarity . OT has been   shown to be an effective method for image style   transferring ( Kolkin et al . , 2019 ; Risser , 2020 ) and   our work exhibits its application for the domain   of the text . We evaluate the proposed model on   three benchmark datasets and two settings , i.e. , su-   pervised and unsupervised . Our extensive analysis   reveals the effectiveness of the proposed model by   establishing new state - of - the - art results .   2 Model   Problem Definition : The task of text style trans-   fer is formally defined as follows : Given the in-   put sentence D= [ w , w , . . . , w]with style   s , the goal is to generate a new sentence D=   [ w , w , . . . , w]in the target style twhile pre-   serving the content of DinD. We study both   supervised and unsupervised settings . Specifically ,   in the supervised setting , for every training sam-   ple(D , s)there is an aligned sentence ¯Din target   style t , i.e. , ( ¯D , t ) , whose content is the same as   D. Whereas for the unsupervised setting , there   is no equivalent pair for ( D , s ) . Note that in the   unsupervised setting , there are sentences for both   styles .   In this work , we employ a transformer - based   generative language model , i.e. , GPT-2 ( Radford   et al . , 2019 ) , for TST and we train the model using   REINFORCE algorithm . Specifically , the source   sentence Dis prompted to the GPT-2 model to   generate the target sentence D. Following the   prior work , ( Lai et al . , 2021 ) , the GPT-2 model is   encouraged to generate the sentence Din the target   styletand with the same content as D. Also , in the   supervised setting , we use the gold target sentence   ¯Das an additional supervision signal to train themodel . Since ¯Dis not available in the unsupervised   setting , we follow the prior work ( Lee et al . , 2021a )   to use reconstruction loss as an additional training   signal . The rest of this section provides details for   generating sentences , rewards for generation , and   training procedures .   2.1 Generating Target Sentence   Following the prior work ( Lai et al . , 2021 ) , we   employ the input sentence Das a prompt to GPT-2   model to generate the target sentence D. More   specifically , the prompt to GPT-2 consists of the   sequence P= [ BOS , w , w , . . . , w , SEP ] ,   where BOS andSEP are special token indicating   the beginning and the end of the input sentence   D. In addition to the input document D , during   training of the supervised model , the gold target   sentence ¯D= [ ¯w,¯w , . . . , ¯w]is concatenated   to the prompt to create the training sequence S=   [ BOS , w , w , . . . , w , SEP , ¯w,¯w , . . . , ¯w ]   and the model is trained in an auto - regressive   manner :   L=/summationdisplay−log(Q(S|S , θ ) ) ( 1 )   where θis GPT-2 parameters and Q(·|S , θ)is   the distribution over vocabulary obtained from the   last hidden states of GPT-2 model . During infer-   ence , only the prompt Pis provided to the GPT-2   model and the words of Dare sampled from the   distribution predicted by GPT-2 model until EOS   is sampled .   Unlike the supervised setting in which the GPT-2   model is trained for uni - directional style conver-   sion , i.e. , from the source style to the target style ,   in the unsupervised setting , the model is trained   for both directions , i.e. , from the source to the tar-   get and vice versa . Specifically , given a sentence   and a style , the GPT-2 model is trained to gen-   erate another sentence with the same content in   the given style . Formally , the prompt Pis con-   catenated with the style stwhere st∈ { s , t } , i.e. ,   S= [ BOS , w , w , . . . , w , SEP , st ] . To train   the model , following the prior work ( Lee et al . ,   2021a ) , two types of reconstruction loss are em-   ployed :   Self - Reconstruction : The GPT-2 model is encour-   aged to reconstruct the original input sentence D   when stiss , i.e. , the given style to the model is the2533same as the input sentence style . Concretely , the   loss function Lis defined as follows :   L=/summationdisplay−log(Q(D|D , s , θ ) ) ( 2 )   Cycle Reconstruction : Ifstist , i.e. , the given   style to the model is different from the style of   the input sentence , then the GPT-2 model is first   employed to generate the sentence ¯Din the style   t. Next , the model is encouraged to reconstruct   the original input sentence Dusing the input S=   [ BOS , ¯w,¯w , . . . , ¯w , SEP , s ] , where ¯wis the   i - th word in the generated sentence ¯D. Concretely ,   the loss function Lis defined as follows :   L=/summationdisplay−log(Q(D|¯D , s , θ ) ) ( 3 )   2.2 Rewarding GPT-2 Model   Prior work shows that rewarding generative models   to observe the requirements for TST could improve   the performance ( Lai et al . , 2021 ; Liu et al . , 2021 ) .   Hence , we follow this optimization step to update   the GPT-2 model based on two different rewards ,   i.e. , Style Conversion and Content Preservation .   Style Conversion : One of the critical objectives   of TST is to change the style of the given text . To   encourage the model for this objective , prior works   commonly use a pre - trained discriminator to pre-   dict the style of the generated text . Here , we follow   the same approach by pre - training a BERT model   ( Devlin et al . , 2019 ) on the combination of the train-   ing sentences Din both styles to identify the style   of the given text ( i.e. , a binary text classification   task ) . Next , during the training stage of the GPT-2   model , we send the generated sentence Dto the   pre - trained BERT model . The probability of the   target style is employed as the style conversion re-   ward : R(D ) = Q ( t|D , ϕ ) , where ϕis   the BERT parameters .   Content Preservation Content preservation is an   important requirement of TST and prior works use   either surface form of DandD(Lai et al . , 2021 ;   Huang et al . , 2021 ) , their semantics ( Chawla and   Yang , 2020 ) , or only shallow syntax ( Ma et al . ,   2019 ) to compute the content overlap between the   source and the generated sentence . None of these   works consider the syntactic structure of two sen-   tences and more importantly its interaction with   the semantics of the sentence . As the main novelty   of the proposed work , inspired by the success ofOptimal Transport in image style transfer ( Kolkin   et al . , 2019 ; Risser , 2020 ) and other related NLP   tasks ( Xu et al . , 2021 ) , we show that OT is an effec-   tive tool for addressing the shortcoming of syntax-   semantics interaction for content preservation in   prior TST literature .   To represent the semantics of the source and   target sentence DandD , we employ the hid-   den states of the final GPT-2 layer for each word   wandw , i.e. , H= [ h , h , . . . , h]andH=   [ h , h , . . . , h ] . Moreover , the syntactic structures   of the two sentences are obtained from an off - the-   shelf dependency tree parser , represented by T   andT. The criterion we use to compute the con-   tent preservation between two sentences DandD   is that the semantically related words in both sen-   tences should have the same syntactic importance   too . In particular , we expect that similar words   appear at the same level in the dependency tree   ofTandT. However , since the structure of the   sentence might change during style conversion and   also the number of words might alter , similar words   might appear in other levels too . As such , the op-   timal mapping between similar words in the de-   pendency trees TandTis not trivial . Fortunately ,   optimal transport ( OT ) can be helpful to solve this   issue . OT is a mathematical method to compute the   cheapest plan for converting one data distribution   to another one . We first formally describe OT and   then we elaborate on how it is employed for our   purpose .   OT is an established method to find the opti-   mal plan to convert ( i.e. , transport ) one distribu-   tion to another one . Formally , given the prob-   ability distributions p(x)andq(y)over the do-   mains XandY , and the cost / distance function   C(x , y ) :X × Y → Rfor mapping XtoY ,   OT finds the optimal joint alignment / distribution   π(x , y)(overX × Y ) with marginals p(x)and   q(y ) , i.e. , the cheapest transportation from p(x)to   q(y ) , by solving the following problem :   where Π(x , y)is the set of all joint distributions   with marginals p(x)andq(y ) . Note that if the   distributions p(x)andq(y)are discrete , the inte-   grals in Equation 4 are replaced with a sum and   the joint distribution π(x , y)is represented by a2534   matrix whose entry ( x , y)(x∈ X , y∈ Y ) rep-   resents the probability of transforming the data   point xtoyto convert the distribution p(x)to   q(y ) . Finally , the cost of optimal conversion   ( i.e. , Wasserstein distance Dist ) is computed by :   Dist= ΣΣπ(x , y)C(x , y ) .   In our method , we use the words w∈Das the   domain Xand the words w∈Das the domain   Y. In order to define their distance , we use the   Euclidean distance between their semantic vector   representation C(w , w ) = /vextenddouble / vextenddouble / vextenddoubleh−h / vextenddouble / vextenddouble / vextenddouble . Finally ,   to define the distributions p(x)andq(y ) , we use   the level of words wandwin the dependency   treeTandT , respectively . Concretely , p(w ) =   softmax ( M−L ) , where Mis the maximum   depth of T , Lis the depth of winTandsoftmax   is computed over all words w∈D. Similarly ,   q(w)is defined by q(w ) = softmax ( M−L ) .   By solving the equation 4 , the cheapest conversion   of the two sentence DandDis obtained and its   cost is equal to Wasserstein distance Dist . We   use this distance as the content preservation penalty ,   i.e. ,R(D ) = −Dist .   3 Training   To train the model , we combine the content preser-   vation reward R(D ) , with style conversion   and the language model loss . We use REIN-   FORCE algorithm ( Williams , 1992 ) to train the   model . In particular , the GPT-2 model is trained   on the combination of the language model loss , i.e. ,Land the rewards of style conversion   and content preservation . The REINFORCE al-   gorithm is employed to incorporate rewards into   fine - tuning of GPT-2 . First , the overall reward is   computed by R(D ) = R(D ) + αR(D ) ,   where αis a trade - off hyper - parameter . Next , we   seek to minimize the negative expected reward   R(D)over the possible choices of D : L=   −E[R(ˆD ) ] . The policy gradient is then   estimated by : ∇L=−E[(R(ˆD)−   b)∇logP(ˆD|D ) ] . Using one roll - out sample , we   further estimate ∇Lvia the generated sentence   D:∇L=−(R(D)−b)∇logP(D|D)where   bis the baseline to reduce variance . In this work ,   we obtain the baseline bvia : b=/summationtextR(D ) ,   where|B|is the mini - batch size and Dis the gener-   ated sentence for the i - th sample in the mini - batch .   4 Experiments   Datasets : We evaluate the proposed model ,   i.e. , Optimal Transport - based Text sTyle Transfer   ( OT4 ) , in two different settings , i.e. , supervised   and unsupervised . In the supervised setting we   employ the Grammarly ’s Yahoo Answers Formal-   ity Corpus ( GYAFC ) dataset ( Rao and Tetreault ,   2018 ) . GYAFC is a parallel dataset in two domains   Entertainment & Music ( E&M ) and Family & Re-   lationships ( F&R ) . Table 1 shows the statistics of   this dataset .   For the unsupervised setting , we employ two   commonly used datasets : Yelp ( Li et al . , 2018 )   and IMDB ( Dai et al . , 2019 ) review . Both datasets   contain sentiment - annotated reviews . The text style   transfer on these datasets is defined as sentiment   polarity conversion . In particular , given a sentence   with a specific sentiment polarity ( e.g. , positive ) ,   the goal is to generate a new sentence with the   opposite sentiment polarity ( e.g. , negative ) . Note   that no parallel data is available for the sentences   in these datasets . The statistics of both datasets are   provided in Table 2   Evaluations : We validate the model performance   using both automatic and human evaluation . For   the automatic evaluation in the supervised setting ,   following the prior work ( Lai et al . , 2021 ) , we as-   sess the performance of the models based on : ( 1 )   Style Strength ( ACC ): The binary style classifier   TexCNN ( Kim , 2014 ) ( with 87.0 % and 89.3 % ac-   curacy on E&M and F&R domains , respectively ) is   employed to predict the strength of the style conver-   sion ; ( 2 ) Content Preservation ( BLEU ): The BLEU2535score computed using four reference sentences ; ( 3 )   HM : The harmonic mean of ACC and BLEU ; and   ( 4 ) BLEURT : A new metric for content preserva-   tion proposed by Sellam et al . ( 2020 ) . For the   automatic evaluation in the unsupervised setting ,   following prior work ( Lee et al . , 2021b ) , we use :   ( 1 ) Style Transfer Accuracy ( S - ACC ): Following   ( Lee et al . , 2021b ) , a Bi - GRU layer with atten-   tion mechanism , trained for style classification on   IMDB and Yelp dataset , is employed to assess the   style transfer ; ( 2 ) Content Preservation ( self - BLEU ,   ref - BLEU , BERT - P , BERT - R , and BERT - F1 ): To   validate the content preservation in the generated   sentence , its BLEU score with input sentence , i.e. ,   self - BLEU , and with the human - generated sen-   tence , i.e. , ref - BLEU , are used . Moreover , to in-   corporate contextual semantics , the BERT score   proposed by ( Zhang et al . , 2020 ) is employed to as-   sess the similarity between the generated sentence   and the human reference . Following prior work   ( Lee et al . , 2021b ) , we report precision , recall , and   F1 score for this metric ; ( 3 ) Fluency ( PPL ): The flu-   ency of the generated sentences is evaluated based   on the perplexity of the sentences using the 5 - gram   KenLM ( Heafield , 2011 ) model trained on both   datasets ;   For human evaluation , following prior work ( Lee   et al . , 2021b ) , we randomly select 150 documents   for each test set and we hire 4 annotators to rate   model predictions from 1 ( Very Bad ) to 5 ( Very   Good ) on content preservation , style conversion ,   and fluency . For each annotator , we provide them   with the source text , source style , target style , and   model - generated text .   Baselines : We compare our model with the prior   state - of - the - art models in each setting . Specifically ,   for the supervised setting on GYAFC , we compare   our model with GPT-2 + SC & BLEU ( Lai et al . ,   2021 ): Similar to our model , this baseline employs   GPT-2 to generate the target sentence . The gen-   erative model is trained using rewards for style   conversion ( SC ) and content preservation ( BLEU ) ;   BART + SC & BLEU ( Lai et al . , 2021 ): The same   as the previous baseline with the difference of using   BART instead of GPT-2 ; NMT - Combined ( Rao   and Tetreault , 2018 ): This baseline casts TST as   a machine translation problem and employs atten-   tion based BiLSTM encoder - decoder architecture ;   Bi - directional FT ( Niu et al . , 2018 ): This model   employs BiLSTM encoder to jointly learn text for-   mality style transfer in both direction ( from formalto informal and vice versa ) ; CPLS ( Shang et al . ,   2019 ): This baselines employs an encoder - decoder   architecture to obtain latent space representation   of the styles , then a projection model converts   the styles in the latent space ; GPT - CAT ( Wang   et al . , 2019 ): This baseline combines rule - based   methods with neural - based TST systems . GPT-2   is employed as the neural component ; TS→CP   ( Sancheti et al . , 2020 ): This model exploits re-   inforcement learning to explicitly encourage con-   tent preservation and transfer strength . It exerts   BLEU score between generated and ground - truth   sentence to compute content preservation reward ;   andChawla ’s ( Chawla and Yang , 2020 ): This   baseline uses a language model discriminator to   guide the text formality style transfer . For con-   tent preservation , it employs mutual information   between source and target sentence .   For the unsupervised setting on IMDB and Yelp ,   we compare with Cross - Alignment ( Shen et al . ,   2017 ): This baseline is trained to generate a sen-   tence in the target style that could match the ex-   ample sentences in the source style . To this end , a   cross - aligned auto - encoder is utilized ; Controlled-   Gen ( Hu et al . , 2017 ): This model employs varia-   tional author encoder ( V AE ) with attribute discrim-   inators to impost semantic structure , including text   style ; Style Transformer ( Dai et al . , 2019 ): In   this baseline , a transformer model is employed to   directly takes the input sentence and target style   to generate the target sentence ; Deep Latent ( He   et al . , 2020 ): This baseline models the unsuper-   vised text style transfer as the task of inferring   latent variables , i.e. , target sentences , on the par-   tially observed data of each style . A recurrent lan-   guage model is employed to fulfill the objective .   RACoLN ( Lee et al . , 2021b ): In this baseline , the   reverse attention technique is employed to remove   style information from the representations of the   tokens in the source sentence .   4.1 Results   Supervised : Table 3 shows the results of the evalu-   ations on the test set . Following prior work , we   compare the performance of the proposed OT4   model in the following settings : ( 1 ) Informal ↔   Formal : In this setting the performance of the base-   lines for converting a formal to informal text or vice   versa is evaluated . From this table , we observe that   GPT-2 model has a better capability of style con-   version . However , the baseline model using GPT-22536   employs BLEU score to encourage content preser-   vation . In contrast , we equip our GPT-2 model   with OT - based reward that can incorporate both   semantics and syntax of the sentences and achieve   the best results ; ( 2 ) Informal →Formal : In this   setting , only the conversion from informal to for-   mal text is evaluated . compared to the previous   setting , we see an improvement in the style con-   version and content preservation for the equivalent   models . It shows that this direction of conversion   is relatively easier . However , the proposed OT4   model still significantly outperform the baselines   in this setting too , indicating the importance of con-   tent preservation for this setting ; ( 3 ) Informal ↔Formal & Combined Domains : In this setting ,   the data from both domains are combined for train-   ing and the model is evaluated for conversion in   both direction . The results show that in this setting ,   all baselines benefit from the extra training data   from the other domain , however , the proposed OT4   enjoys the largest improvement . Our hypothesis   for such improvement in OT4 is that the existence   of the other domain data provides more syntac-   tic structure to the model , therefore , compared to   the baselines that miss this information , the pro-   posed OT4 baseline can benefit from more training   signals . ( 5 ) Evaluation with the first reference :   To conduct a comprehensive comparison , we also   compare our model with the baselines that only   report the performance of the models evaluated   on the first reference sentence . In this setting , we   see that the proposed model achieves the highest   BLEU score ; and finally ( 6 ) 10 % Parallel Data :   To show the effectiveness of the proposed model   in the case of low - resource setting , we compare   the performance of the proposed model when only   10 % of the training parallel data is employed . We   see in this setting the improvement achieved by our   proposed model is higher , especially in terms of   BLEURT , reflecting its superiority to benefit more2537   efficiently from training signals .   Unsupervised : The results of the evaluation of   the unsupervised model on the Yelp and IMDB   datasets are presented in Table 4 and 5 , respec-   tively . Note that due to the lack of reference tar-   get sentences in the IMDB dataset , we omit self-   BLEU and BERT scores in Table 5 . There are sev-   eral observations from these tables . First , the pro-   posed OT4 model outperforms all baselines with   respect to style conversion and content preserva-   tion . Specifically , for style conversion , our model   improves the S - ACC on Yelp and IMDB by 2.1 %   and 3.1 % , respectively . Compared the baselines ,   we attribute the style conversion improvement to   the explicit rewards employed in our model to di-   rectly train the model for better style conversion .   More importantly , since our model is equipped with   OT to improve content preservation , we see a sig-   nificant improvement for this metric . In particular ,   on the Yelp dataset , our model improves BERT - F1   score by 6.2 % and self - BLEU by 11.8 % . Consider-   ing the improvement on ref - BLEU on this dataset   also indicates that while our model improves the   content preservation , it is not repeating the input   sentence . Finally , comparison of the fluency of   the generated sentences shows that our model is   competitive with baselines by achieving the second-   lowest PPL on both datasets.4.2 Ablation Study   In order to shed more light on the contribution of   the proposed OT - based content preservation reward ,   in this section we study the performance of alterna-   tive architecture designs : ( 1 ) No Semantics : Here ,   the cost function C(x , y)is replaced by the con-   stant function C(x , y ) = 1 , hence removing all in-   formation regarding the semantics of the sentence ;   ( 2)No Syntax : In this baseline , the probability dis-   tribution p(x)andq(y)are represented by uniform   distribution , thus removing all information about   the syntactic structure ; ( 3 ) NO OT : In this base-   line , the content preservation reward is completely   removed ; ( 4 ) Reconstruct : Following prior work   ( Ma et al . , 2019 ) , instead of using OT - based reward ,   we add another auxiliary task in which the model is   trained to reconstruct the POS tag of the input sen-   tence . Note that , for this auxiliary task , the model   is trained to re - convert the generated sentence D   back to D ; ( 5 ) Graph - based : Instead of directly   encoding the interaction between the syntax and   semantics via OT - based distance , in this baseline   we encode the syntax and the semantics together   using a Graph Convolution Network ( GCN ) ( Kipf   and Welling , 2017 ) . Specifically , before generating   the words of D , the representations Hobtained   from the GPT-2 model are further abstracted using   a two - layer GCN that takes the dependency tree   of the input sentence as the input graph . We eval-   uate the models on the development set of E&M   domain for formal and informal style transfer ( i.e. ,   both direction ) .   Table 9 shows the results . This table shows that   removing both syntax and semantics scores from   OT will hurt the performance . However , syntac-   tic information has more importance as removing   them results in more performance loss . Moreover ,   it is clear from this table that reconstructing syntac-   tic features is not as effective as OT - based reward .   This inferiority is better evident from the loss in   BLEU and BLEURT scores . Our hypothesis for   better performance of the OT - based reward is that   OT can encode the interaction between the syntax   and semantics while reconstruction makes these   two tasks separate . Finally , this table shows that   the graph - based model has poor performance com-   pared to OT4 . Our hypothesis for this observation   is that while the GCN model can encode the syntac-   tic structure of both sentences , it can not encode the2538   alignment between the words of the input sentence   and the generated sentence . Hence , hindering the   content preservation computation .   4.3 Case Study   To provide more insight into the performance of the   proposed model , in this section we conduct a quali-   tative analysis . Specifically , we study how the OT-   based model is able to find the perfect alignment   between the words of the input sentence and the   generated sentence . Note that in case of a success-   ful style conversion , there should be a small Wasser-   stein distance between the two sentences , hence ,   the semantically related words will be aligned with   each other . Table 8 shows two informal sentences   along with their converted formal counterparts gen-   erated by OT4 . To study the role of Optimal Trans-   port , we report the alignments with the highest   probability which are obtained by solving the OT   problem for the given two sentences . This table   shows that there is a high semantic similarity be-   tween the aligned words . More importantly , the   aligned words have the same syntactic connections   with the other words in their sentence . For instance ,   in the first example , the word “ dismal " and its child   in the dependency tree , i.e. , “ skills " , are aligned   with the word “ poor " and its child in the depen-   dency tree , i.e. , “ abilities " . This example shows   that the OT alignment considers both semantic and   syntactic relations between aligned words . How-   ever , OT is not restricted to semantic or syntactic   structures and it can relax the alignments whenever   it is needed . For instance , in the second example ,   we observe that the word “ time " and “ hangs " are   aligned with each other while serving different syn-   tactic roles in the sentence . It shows that when the   semantic relation is more important , OT can break   the syntactic constraints to find a perfect alignment   and the lowest Wasserstein distance . This example   shows that the prior work for reconstructing the   syntactic roles regardless of their semantic impor-   tance will be inferior to our proposed OT - based   approach .   Finally , to qualitatively study the improvement   obtained by the proposed model on the unsuper-   vised setting , we present the generated text for a   sample text from the IMDB dataset . Specifically ,   we compare our model output with the prior SOTA   model , i.e. , RACoLN . Table 10 shows the samples .   It is clear from this table that the proposed model   can retain more content from the input text . In   particular , while RACoNL omits the genre of the   movie , OT4 successfully keeps this information in   the generated text . We hypothesis that the simi-   lar distance of the word “ horror " to the opinion   words in the input and the generated text , i.e. , “ de-   cent " and “ unsatisfactory " , are helping to keep this   information in OT4 output .   5 Conclusion   We propose a new model for encouraging content   preservation in text style transfer . We demonstrate   that both syntax and semantics of the input sen-   tence and the generated sentence should be taken   into account for content preservation . More im-   portantly , we empirically show that the interaction   between syntax and semantics of the input and tar-   get sentences is necessary for TST . We conducted   extensive experiments on benchmark datasets in   supervised and unsupervised settings , achieving   state - of - the - art performance on multiple datasets.2539References25402541