  Zhengliang Shi , Weiwei Sun , Shuo Zhang , Zhen Zhang ,   Pengjie Ren , Zhaochun RenShandong University , Qingdao , ChinaBloomberg , London , United Kingdom   shizhl@mail.sdu.edu.cn { sunnweiwei , zhen.zhang.sdu}@gmail.com   zhaochun.ren@sdu.edu.cn szhang611@bloomberg.net jay.ren@outlook.com   Abstract   Evaluating open - domain dialogue systems is   challenging for reasons such as the one - to-   many problem , i.e. , many appropriate re-   sponses other than just the golden response . As   of now , automatic evaluation methods need bet-   ter consistency with humans , while reliable hu-   man evaluation can be time- and cost - intensive .   To this end , we propose the Reference- Assisted   Dialogue Evaluation ( RADE ) approach un-   der the multi - task learning framework , which   leverages the pre - created utterance as reference   other than the gold response to relief the one - to-   many problem . Speciﬁcally , RADE explicitly   compares reference and the candidate response   to predict their overall scores . Moreover , an   auxiliary response generation task enhances   prediction via a shared encoder . To support   RADE , we extend three datasets with addi-   tional rated responses other than just a golden   response by human annotation . Experiments on   our three datasets and two existing benchmarks   demonstrate the effectiveness of our method ,   where Pearson , Spearman , and Kendall correla-   tions with human evaluation outperform state-   of - the - art baselines .   1 Introduction   Open - domain dialogue system , which focuses on   non - goal - oriented chitchat , may converse on a   broad range of arbitrary topics . Recent years have   witnessed rapid advances in natural language gener-   ation ( Zhang et al . , 2019b ; Roller et al . , 2021 ; Zhao   et al . , 2023 ) , boosting the development of open-   domain dialogue systems . Conversations with such   systems resemble human - human interactions as   various responses might ﬁt the context , given that   users often do not have a speciﬁc goal beyond   enjoying the conversation . Evaluating these con-   versations is thus challenging because of the so-   called one - to - many problem ( Chan et al . , 2021 ; Ji   et al . , 2022 ) ; see Figure 1where three candidateFigure 1 : An example to explain the one - to - many nature   of open - domain dialogues .   responses with different semantics ﬁt the context   while there is only one golden response .   The most common practice of dialogue evalua-   tion is done with reference - based metrics , which   compare the generated response with a pre - created   response , commonly referred to as the golden   standard ( Ji et al . , 2022 ) . The reference - based   metrics calculate the similarity between the gen-   erated and gold responses at either lexical level   ( e.g. , ROUGE ( Lin,2004 ) , BLEU ( Papineni et al . ,   2002 ) ) or semantic level ( e.g. , BERTScore ( Zhang   et al . , 2019a ) , ADEM ( Lowe et al . , 2017 ) ) . How-   ever , these metrics ignore the one - to - many nature   of open - domain dialogues . As illustrated at the bot-   tom of Figure 1 , the generated response “ Amazon   is good but expensive ... ” expresses the opposite   semantics to the golden response “ I shop online ... ”   and is therefore considered a non - good response by   the reference - based metrics . Therefore , these met-   rics may need a higher consistency with humans .   Recently , multi - reference methods andreference-   free methods are proposed to address the drawback   of reference - based metrics . The former explicitly   annotates multiple references for dialogue ( Eric   et al . ,2021 ) , whereas the latter discards the golden   response in the evaluation and achieves high cor-12856relations with human judgments ( Mehri and Eske-   nazi,2020c ; Huang et al . , 2020 ) . However , draw-   back still exists in these two classes of methods .   Multi - reference methods are costly and hard to   generalize to different datasets , while reference-   free methods are often unstable and vulnerable to   data - induced biases .   To overcome the weakness of existing evalua-   tion methods and further resolve the one - to - many   problem , we propose a new technique , namely   Reference- Assisted Dialogue Evaluation ( RADE ) .   RADE considers the pre - created response as a ref-   erence instead of the golden standard .   To support RADE , we design a new human an-   notation task to extend existing datasets , which   includes metric decompose and pairwise annota-   tion , where a pre - scored golden response is paired   with generated responses for rating following a uni-   ﬁed rating score . The ﬁnal scores are arrived at   by aggregating ratings with a weighted sum from   different sub - metrics . The human annotation col-   lects labels for three high - quality datasets with   10,112 dialogues , which correspond to three down-   stream open - domain dialogue system tasks , i.e. ,   chitchat , empathetic dialogue , and personal chat .   These multi - domain datasets make RADE more   robust when generalizing to cross - domain evalua-   tion scenarios while having a better task - speciﬁc   performance .   We propose a RADE model under the multi-   task learning framework for automatic evaluation   based on the newly collected datasets . Speciﬁ-   cally , RADE ﬁrst explicitly encodes the relation   between dialogue context and generated response   with reference assistance . Then RADE discrim-   inates whether the reference or response ﬁts the   context better and predicts the scores for each ut-   terance . To relieve the one - to - many problem , we   augment RADE with a joint response generation   task where RADE learns to generate the reference   responses to better perceive the range of candidate   responses .   Extensive experiments on our three benchmarks   demonstrate that RADE achieves the best corre-   lations with human judgment . We also examine   two existing USR benchmark ( Mehri and Eskenazi ,   2020c ) where RADE outperforms the state - of - the - art methods , e.g. , pushing the Pearson correlation   coefﬁcient to 48 % ( 6.8 % absolute improvement )   and Spearman correlation coefﬁcient to 46.6 %   ( 4.3 % absolute improvement ) . Experiments also   verify the generalizability of our proposed method .   Our contributions can be summarized as follows :   ( 1 ) We propose the reference - assisted evaluation   method , i.e. , RADE , for open - domain dialogue   evaluation ; ( 2 ) We design a new human annota-   tion task and collect three new dialogue evaluation   datasets ; ( 3 ) Experiments on our benchmarks and   two existing benchmarks verify the effectiveness   and robustness of the proposed methods ; ( 4 ) We   release three new benchmarks and the pre - trained   evaluation model to facilitate future research on   dialogue evaluation .   2 Related work   2.1 Reference - based dialogue evaluation   Previous reference - based methods compare the   generated response with the pre - created response at   the lexical or semantic level . Lexical - level metrics ,   e.g. , ROUGE ( Lin,2004 ) , BLEU ( Papineni et al . ,   2002 ) and METEOR ( Banerjee and Lavie , 2005 ) ,   count the n - gram overlap between the candidate   response and the reference response . These meth-   ods usually correlate poorly with human evaluation   results due to the lexical mismatch problem ( Liu   et al . , 2016 ) . Semantic - level metrics evaluate ad-   dress lexical mismatch problem by calculating sim-   ilarity with high - dimension embeddings . For exam-   ple , Sharma et al . ( 2017 ) measures the embedding   distance between golden and generated response .   Ghazarian et al . ( 2019 ) and Zhang et al . ( 2019a )   enhance the text representation using the large pre-   train model , which has shown exemplary perfor-   mance in capturing semantic similarity . However ,   they suffer from the one - to - many problem when   evaluating open - domain dialogues since responses   with various semantics may ﬁt the dialogue context .   Recent works tend to relieve this drawback by   annotating multiple references for dialogue , com-   monly referred to as multi - reference methods ( Li   et al . ,2017 ; Sai et al . , 2020 ) , which are costly and   hard to generalize to agnostic scenarios . The pro-   posed RADE aims to consider the pre - created re-   sponse as a candidate instead of the golden standard   to address the one - to - many problem of dialogue   evaluation.128572.2 Reference - free dialogue evaluation   The reference - free methods are gaining more at-   tention as they correlate more with human judg-   ment only with the dialogue context and re-   sponse . For example , MAUDE predicts the score   of dialogue using pre - trained language models ,   GRADE ( Huang et al . , 2020 ) evaluates the coher-   ence of dialogues with the augmentation of the   commonsense graph , EMS ( Chan et al . , 2021 ) en-   hances the dialogue evaluation by capturing the   representation of the context and response in la-   tent space . Some methods further decompose   the evaluation of responses into multiple perspec-   tives ( Mehri and Eskenazi , 2020a , c;Phy et al . ,   2020 ) , such as relevance , ﬂuency , and engaging-   ness , then aggregate the overall score from differ-   ent sub - metrics with a weighted average . How-   ever , some recent studies ( Khalid and Lee , 2022 ;   Deutsch et al . , 2022 ) reveal that the reference-   free methods are vulnerable to data - induced biases   and inherently biased toward models which are   more similar to their own . In contrast , this paper   proposes a reference - assisted approach , which en-   hances the robustness of the model using reference   responses as a benchmark .   3 Task Formulation   In this work , we propose two tasks : ( 1 ) extending   the existing datasets by human annotation , and ( 2 )   leveraging the rated references collected in ( 1 ) to   enhance automatic evaluation .   Human annotation Human annotation aims to   extend existing datasets with multiple rated re-   sponses to facilitate automatic evaluation . Given   a dialogue context c , which is always paired   with a golden response ( denoted as reference ) r ,   we employ the generation models , e.g. , Blender-   Bot ( Roller et al . , 2021 ) , to generate one more   response r. We then assign a ﬁxed overall score   or derive from existing datasets to the reference as   s. The annotators are instructed to rate rass ,   following the same scale while taking the reference   as a benchmark . The annotators are also asked to   revise the reference score sifsis inappropriate .   Automatic evaluation Given a dialogue context   c , the proposed RADE learns to evaluate the re-   sponse rwith the assistance of reference runder   the multi - task learning framework . The ﬁrst task   explicitly models the relation between reference   and response and discriminates which ﬁts the con-   text better . The scores of reference and response   are predicted simultaneously . And the second task   enhances the score prediction task by implicitly   estimating the distribution of candidate responses .   4 Human Annotation   Our human annotation task aims to rate the candi-   date responses following a pre - scored reference as   a benchmark . Since there are multiple perspectives   to assess the response , we simplify by sorting the   possible aspects into two categories : the general   view and the task - speciﬁc view . As listed in Table 1 ,   the former contains relevance , engagingness , and   ﬂuency , which are suitable for all dialogue agents .   And task - speciﬁc criteria consist of understandabil-   ity , emotional awareness , and personality aware-   ness , which correspond to chitchat dialogue , emo-   tional dialogue , and persona dialogue . We annotate   rates on each metric and calculate the overall rating   score by weighting these sub - metrics . Speciﬁcally ,   the weights are obtained based on the preference   of users ( see section A.1.3 for more details ) .   4.1 Data preparation   We consider three datasets to extend : • DSTC –   ChitChat ( ChitChat ) ( Hori and Hori , 2017 ) , a   chitchat dataset collected from Twitter , each ex-   ample derived from the conversation between a   customer and an agent . • Empathetic Dialogues   ( EmpaDial ) ( Rashkin et al . , 2019 ) , which consists   of 25k dialogues grounded in emotional situations.12858   •PersonaChat ( Zhang et al . , 2018 ) , a real - world   dataset consisting of 10k dialogues where each par-   ticipant plays the part of an assigned persona .   Then , we collect model - generated responses   using the following seven well - performing dia-   logue models on these datasets : BlenderBot ( Roller   et al . , 2021 ) , DialoGPT ( Zhang et al . , 2019b ) ,   KEMP ( Li et al . , 2020b ) , MoEL ( Lin et al . , 2019 ) ,   MIME ( Majumder et al . , 2020 ) , EmpDG ( Li et al . ,   2020a ) , PersonaGPT ( Tang et al . , 2021 ) . The   train - dev - test of collected datasets are split as   Chitchat ( 1490/300/300 , 5/1/1 ) , Empathetic Di-   alogue ( 3022/500/500 , 6/1/1 ) , and Persona Chat   ( 3000/500/500 , 6/1/1 ) . More details of these mod-   els are available in Appendix A.1.1 .   4.2 Human annotation detalis   We hire 40 annotators for data annotation . Follow-   ing a ﬁve - scale standard , they are asked to label   sub - metrics as listed in Table 1 . The ﬁve - scale   allows the annotators to factor in their subjective   interpretation of the extent of success or failure   of a system ’s response to satisfy a user ’s request .   The dialogue context , rated reference response , and   corresponding score are provided in each example .   At least three annotators are required for each ex-   ample . We annotated about 10k dialogues for the   three datasets , and the statistics of the collected   datasets are listed in Table 2 . The ratings achieve   reasonable inter - annotator agreements with Fleiss   Kappa scores of 0.540 , 0.554 , and 0.533 on three   datasets , respectively . More details about the an-   notation guideline and details are provided in Ap-   pendix A.1.2 .   5 Reference - Assisted Automatic   Evaluation   We propose RADE , a Reference- Assisted Au-   tomatic Dialogue Evaluation method under theframework of multi - task learning . Compared with   reference - based methods that evaluate based on   the distance between the golden and generated re-   sponse , the proposed RADE explicitly discrimi-   nates whether the reference or candidate response   ﬁts the dialogue context better . To relieve the   one - to - many problem , we augment RADE with   a joint response generation task , which aims to   perceive the range of feasible candidate responses .   To improve the performance of RADE with the   limited dataset , we propose a two - stage training   strategy , including cross - domain pre - training and   task - speciﬁc ﬁnetune .   5.1 Model architecture   The architecture of RADE is illustrated in Figure 2 ,   which comprises a posterior encoder , a regression   layer , and a candidate response generator .   Posterior encoder . The posterior encoder en-   codes the dialogue context c , reference response   r , and model - generated response rinto hidden   representation . In particular , we ﬁrst concatenate   c , randrtogether into Xwith a speciﬁc token   [ SEP ] :   X={c[SEP ] r[SEP ] r } ( 1 )   Then the concatenated sequence is fed into a   transformer - based encoder to get the representa-   tionH∈R :   H= Encoder ( X ) , ( 2 )   where dis the hidden size of encoder , |X|is the   length of sequence X.   Regression layer . The regression layer aggre-   gates the representation Hand predicts the scores   of both reference and candidate response simulta-   neously . Speciﬁcally , a pooling layer aggregates   the token - level representation into a sequence - level   representation : h∈R :   h= Pooling ( H ) ( 3 )   Then , a feedforward network takes has input to   predict the score of both reference and candidate   response :   ( ˆs,ˆs ) = FeedForward ( h ) , ( 4 )   where ˆsandˆsdenote the predicted score of r   andr , respectively.12859   Candidate response generator . To relieve the   one - to - many problem , we devise a candidate re-   sponse generator to perceive the range of feasible   candidate responses ( Chan et al . , 2021 ) . Speciﬁ-   cally , a Transformer - based generator learns to gen-   erate reference responses autoregressively for a spe-   ciﬁc context . We ﬁrst encode the dialogue context   cusing a encoder :   ˆh= Encoder ( c ) , ( 5 )   where the Encoder shares the same parameters   with the posteriori encoder in Eq . ( 2 ) . Then , we   apply a Transformer - based decoder Decoder to   model the generation probability of reference re-   sponse r :   P(r|c ) = /productdisplayDecoder ( r|r,ˆh),(6 )   where Tdenotes the length of r.   Compared with the previous reference - free meth-   ods , which estimate the relation between context   and response only with the knowledge acquired   from their training data , RADE explicitly takes   the pre - created response as a benchmark to re-   duce the data - induced bias when generalizing to   agnostic scenarios . Moreover , different from ex-   isting reference - based methods , which use the   pre - created response as the golden standard with-   out considering the semantic diversity of the re-   sponse , we relieve the one - to - many problem via   auxiliary response generation tasks . The share en-   coder enhances the capability of context represen - tation which augment the performance of score-   predicting task through multi - task learning .   5.2 Two - stage training   The neural - based model has been proven prone   to data - induced bias , but it is costly to annotate   a large dataset in every speciﬁc task . Therefore ,   we propose a two - stage strategy that includes : ( 1 )   cross - domain pre - training , and ( 2 ) task - speciﬁc   ﬁne - tuning , keeping a tradeoff of performance be-   tween in- and cross - domain . As shown in Fig-   ure2(right ) , we pre - train our model based on   existing human - annotated datasets from different   downstream tasks of open - domain dialogue to im-   prove the generalizability ( Ye et al . , 2021a ) . Since   the cross - domain datasets suffer from domain gaps   and no pair - wised score , we ﬁnetune our model in   the next stage with newly collected task - speciﬁc   datasets .   Cross - domain pre - training . The pre - training   datasets contain 54,438 dialogue - level examples   collected from different downstream tasks , cover-   ing a wide range of domains ( see more details in   Table 7 ) . For learning the coarse - grain judgment   of generated response without human - annotated   reference scores , our model is ﬁrst pre - trained by   minimizing a new cross - domain pre - training loss   L. Concretely , the L is composed of score-   prediction loss and generation loss , which can be   formulated as :   L = L ( ˆs , s ) + L , ( 7)12860where ˆsandsdenote the human - annotated score   and the predicted score of the candidate response   andL ( ˆs , s ) = ( ˆ s−s).Lis the re-   sponse generation loss , which is deﬁned as :   L=−logP(r|c ) , ( 8)   where P(r|c)is the generation probability of r   deﬁned in Eq . ( 6 ) .   Task - speciﬁc ﬁnetuning . We next ﬁnetune our   model with newly annotated datasets to enhance   the performance when evaluating task - speciﬁc di-   alogue agents . The optimize objective Lis com-   posed of score - prediction loss , generation loss , and   pair - wised ranking loss , which can be formulated   as :   L = L ( ˆs , s ) + L ( ˆs , s)+   L+L(9 )   where L ( ˆs , s)andL ( ˆs , s)are MSE   score - prediction loss of reference response and can-   didte response , respectively . Lis the genera-   tion loss as deﬁned in Eq . ( 8).Lis the pair - wise   ranking loss deﬁned as :   L=−g(s , s ) loge   e+ e , ( 10 )   in which g(s , s)is a labeling function deﬁned   as :   g(s , s ) = /braceleftBigg   0 , s≥s   1 , s < s(11 )   TheLis introduced to assure that the rank   order of the predicted scores satisﬁes the pre-   annotated order . Compared to reference - free mod-   els that inherently favor outputs from their under-   lying models or those trained on similar datasets ,   RADE is speciﬁcally optimized to align with hu-   man intentions and effectively alleviate this bias .   6 Experimental Setup   6.1 Dataset and evaluation metrics   We mainly conduct experiments on the three   datasets annotated in Section 4 . We further evalu-   ate the models on two existing benchmarks , USR-   TopicChat and USR - PersonaChat ( Mehri and Es-   kenazi , 2020c ) , to examine the generalizability of   our method . The evaluation metrics include Pear-   son ( r ) , Spearman ( ρ ) , and Kendall ( τ ) correlation ,   which measures the linear relationship , monotonic   relationship , and the ordinal association betweenautomatic evaluation and human evaluation , respec-   tively . We abbreviate the Pearson , Spearman , and   Kendall correlation as r , ρ , and τfor simplicity .   6.2 Implementation details   We initialize the parameters of the encoder and   decoder with BART ( Lewis et al . , 2019 ) , a   Transformer - based pre - trained model . BART is   well - suited to our proposed model because it is ca-   pable of both text representation tasks and text gen-   eration tasks . We optimize the model using Adam   optimizer with parameters β= 0.98,β= 0.97 ,   and the learning rate of 5e−5 . The model is trained   up to 10 epochs , and we tune the hyper - parameters   and pick the checkpoint on the development set .   The training of the model can be done within 5   hours using two 2080Ti GPUs . We denote the   RADE model that pre - trained on cross - domain   datasets as RADE ( PT ) , and the model that fur-   ther ﬁnetuned on task - speciﬁc data as RADE ( TS ) .   6.3 Baselines   We compare our method with two types of base-   lines : reference - based and reference - free methods .   The reference - free baselines include : Di-   aloRPT ( Gao et al . , 2020a ) , which trained on large-   scale social media feedback data to predict ranking-   based scores ; GRADE ( Huang et al . , 2020 ) , which   enhances the contextualized representations via   topic - level commonsense graphs and predicts the   score using a regression module ; FED ( Mehri and   Eskenazi , 2020a ) , an unsupervised dialogue evalu-   ation model based on DialogGPT ; UniEval ( Zhong   et al . , 2022 ) , which evaluates the response from   multiple perspectives ; QuesEval ( Scialom et al . ,   2021 ) , which evaluates the fact - based text using   summarizing asks .   The reference - based baselines include : RU-   BER ( Tao et al . , 2017 ) , an unsupervised eval-   uation metric considering the similarity of the   response with dialog context and reference ;   BERTScore ( Zhang et al . , 2019a ) , which employs   BERT to greedily match the response and the   ground truth at the token level ; BLEURT ( Sel-   lam et al . , 2020 ) , which is a BERT - based model   pre - trained with millions of synthetic examples ;   BARTScore ( De Bruyn et al . , 2020 ) , which weights   the log - likelihood of the generated response as the   score . We also test three reference - based lexical-   level metrics : ROUGE - L , BLEU-2 , and METEOR .12861   Moreover , we implement two reference - based   baselines , BERT and BART , which are   trained with the same human - annotated datasets   as RADE , and provide a reasonable comparison   with our proposed model . Speciﬁcally , we obtain   the text representations of the dialogue using BERT   or BART and then feed the representations into a   multi - layer perception to calculate the scores . For a   more comprehensive analysis , we also ﬁne - tune the   two strongest baselines , QuantiDCE and GRADE ,   on our cross - domain datasets as well as our self-   collected datasets , respectively .   7 Results and Analysis   7.1 Experimental results   Overall performance . Table 3shows the ex-   perimental performance for all methods . Over-   all , RADE achieves the best performance in three   benchmarks in terms of all metrics . Concretely ,   the pre - trained model RADE ( PT ) gets better or   comparable correlation with human judgment than   the best baseline method on three dialogue tasks . The task - speciﬁc model RADE ( TS ) , ﬁne - tuned   with the newly collected reference - assisted data ,   establishes a new state - of - the - art by improving the   performance by about 30 % on average compared   to RADE ( PT ) . For example , RADE ( TS ) gets   r= 0.601,ρ= 0.569 in the ChitChat domain , and   pushes rto0.863(0.314absolute improvements ) ,   τto0.685(0.287absolute improvements ) in Em-   paDial domain . This result suggests that training   with in - domain datasets is critical to enhancing the   task - speciﬁc evaluation capability of RADE . For a   more comprehensive comparison , we also train the   two strongest baselines ( QuantiDCE and GRADE )   with our cross - domain and self - collected datasets ,   respectively . And the result and analysis are pro-   vided in Appendix A.2.3 .   Generalizability . We ﬁnd that the performance   of the reference - free method varies dramatically   across domains . For example , GRADE and Quan-   tiDCE , trained in the chitchat domain , achieve high   correlations with human judgment in ChitChat and   EmpaDial but perform poorly in PersonaChat . The12862result indicates that the contextual representation   capabilities of unsupervised methods are limited by   their training data and , therefore , are prone to data-   induced bias , decreasing their performance when   employing agnostic scenarios . In contrast , the gap   between the proposed RADE ( PT ) methods across   different domains is relatively small . These results   indicate that RADE has better generalizability than   reference - free methods due to the assistance of   reference and the proposed cross - domain training   strategy .   Results on USR benchmarks . We further exam-   ine our methods on two USR datasets ( Mehri and   Eskenazi , 2020c ) to verify the efﬁciency and ro-   bustness of RADE when generalizing to existing   dialogue evaluation benchmarks . The results are   listed in Table 4 . Experiments show that RADE ,   which has not explicitly trained on these datasets ,   achieves better or comparable results to previous   supervised methods . See Appendix A.2.4 for more   results and details .   7.2 Ablation study   We perform an ablation study to investigate the   inﬂuence of different components in our methods .   We examine two ablative variants : ( 1 ) w/o L :   we remove the ranking - based loss Lto verify   its effectiveness ( w/o L ) ; ( 2 ) w/o L : we re-   move the Lto verify training with response   generation task jointly can improve the predicting   correlation with human judgment .   Table 3presents the results . Overall , the vari-   ants of our methods show a decreased performance   compared to the base model . For example , Pearson   drops 0.10 , 0.09 , and 0.07 in three benchmarks ,   respectively , after the Lis removed . This re-   sult indicates that ranking - based loss can enhance   performance by explicitly building the relation be-   tween response and reference . After removing   theL , the correlation in all benchmarks has   a prominent decrease , e.g. , Spearman correlation   drops by 0.15 , 0.10 , and 0.09 , respectively . The re-   sults suggest that the auxiliary response generation   task improves the representation capability of our   method and relieves the one - to - many problem .   7.3 Case study   Our case studies demonstrate that RADE is more   consistent with human judgment than baselines .   Details about our case studies are available in Ap-   pendix A.2.5 .   7.4 Qualitative analysis   To explain more intuitively , we show the scatter   plots against human judgments for different auto-   matic evaluation methods ( i.e. , RADE , GRADE ,   BERTScore , METEOR ) on the EmpaDial dataset   in Figure 3 . As shown in Figure 3(a ) , our method   RADE achieves a stronger correlation with human   judgment than the other methods . Figure 3(d ) illus-   trates that METEOR scores are zero or extremely   low for the most response . It results from the one-   to - many nature of open - domain dialogue , and word   overlapping occasionally occurs . Figure 3(c ) sug-   gests that the BERTScore scores are mainly concen-   trated in the range of 0.3 - 0.6 , indicating no signiﬁ-   ca nt differentiation between the different responses .   Figure 3(b ) shows that GRADE achieves a better12863correlation with human judgments . However , the   distribution of GRADE predicted scores is concen-   trated in the high - scoring band , resulting in a low   distinction of responses ; RADE uses reference as a   benchmark and thus has a more balanced distribu-   tion of predicted scores .   8 Discussions   The impact of the training data scale . To ex-   plore the minimum data scale required for our   method , we train RADE using different amounts   of randomly sampled annotated data . We ob-   serve a minor degradation in RADE ’s performance   as the amount of data decreases . For example ,   when training on 2,400 examples from the Empa-   theticDialogue dataset , RADE(TS ) achieves Pear-   man’r=0.837 and Spearman’rho=0.829 ; whereas   with 1,200 examples , it obtains Pearman’r=0.807   and Spearman’rho=0.806 . All results are averaged   over three runs . Moreover , we ﬁnd that RADE   outperforms all baselines with only 800 training   examples in three datasets , respectively .   The difference between golden and candidate   Responses . Golden response refers to a scenario   where there is only one correct response , and any   different response is given a low score . For ex-   ample , BERTScore calculates the cosine similarity   between the golden and model - generated response .   However , Candidate responses implies that there   can be multiple correct answers , which is more ﬂex-   ible and human - intuitive . And RADE is optimized   to align with this human intention using generative   and pairwise - ranking loss . If more references are   available , the RADE can consider multiple valid   responses to make more reliable evaluations . To   achieve this , we can concatenate model - generated   responses with different references . However , due   to the limitation of our datasets , we concatenate one   reference and model - generated response , which are   then fed to the encoder .   Employing RADE when the reference response   is not available . Considering the reference is not   always available in real - world scenarios , we de-   sign two alternatives to enable RADE , i.e. , con-   structing a pseudo - reference via retrieval or gen-   erative method . We verify the two solutions on   the FED dataset and the details can be found in   Appendix A.3.9 Conclusion   We have presented a new reference - assist dialogue   evaluation ( RADE ) method to address the one - to-   many problem when evaluating open - domain dia-   logue systems . RADE evaluates the response gen-   erated by open - domain dialogue agents with the   assistance of reference response . In addition , we   have curated the reference - assisted dialogue evalu-   ation datasets by expanding three existing datasets   via a pairwise human annotation . The extended   datasets contain over 10 K dialogues . Extensive   experiments on three extended datasets and two   existing benchmarks have veriﬁed the effectiveness   and robustness of the proposed methods and their   generalizability .   Limitations   The main limitation of this paper is the need for   human - labeled reference responses . We will ex-   plore automated or human - machine collaboration   methods to reduce the cost of annotation in the next   stage . Another limitation is that we need to explore   whether other auxiliary tasks can also enhance the   performance of score prediction . In the future , we   also plan to reproduce the proposed method for   other , less resource - rich languages .   Ethics Statement   The paper proposes a dialogue evaluation method ,   which is intended to evaluate open - ended dialogue   on topics such as books and movies . A new dataset   is developed using some existing dialogue systems ,   such as DialoGPT , which are trained on large - scale   web data that is known to contain biased or discrim-   inatory content . The datasets that we trained on   may also include subjective knowledge ( comments   on movies ) that may express the bias of the writers.12864References1286512866A Appendix   A.1 Human Evaluation Details   A.1.1 Details for Data Preparation   We ﬁrst employ the generation models to generate   one more response for our human annotation pro-   posed in Section 3 . The annotators are instructed   to rate the newly generated responses . Speciﬁcally ,   we employ the following generation model :   •Blenderbot ( Roller et al . , 2021 ): Blender is a   conversational agent based on the large - scale   model that mainly focuses on generating per-   sonal , engaging , knowledgeable , and empa-   thetic responses .   •DialogGPT ( Zhang et al . , 2019b ): Dialog-   GPT is a large , tunable neural conversational   response generation model .   •KEMP ( Li et al . , 2020b ): KEMP is an   emotional dialogue agent enhanced with a   knowledge - enriched context graph .   •MoEL ( Lin et al . , 2019 ): MoEL is an   emotional dialogue agent based on encoder-   decoder architecture . MoEL softly combines   the response representation from different de-   coders , each focusing on one type of emotion .   •MIME ( Majumder et al . , 2020 ): MIME is   an empathetic dialogue model considering   polarity - based emotion clusters and emotional   mimicry .   •EmpDG ( Li et al . , 2020a ): EmpDG is a multi-   resolution empathetic chatbot enhanced by ex-   ploiting user feedback .   •PersonaGPT ( Tang et al . , 2021 ): Person-   aGPT is a GPT2 - based open - domain dialogue   agent designed to generate personalized re-   sponses .   As shown in Table 5 , we extend the DSTC   dataset with Blenderbot andDialoGPT , the Em-   pathetic Dialogue dataset with KEMP , MoEL ,   MIME andEmpDG ; the Persona - Chat dataset with   Blenderbot andPersonaGPT .   Since Roller et al . points out the length of the   utterances is crucial to human judgments , i.e. , too   short responses are seen as dull , we only sample   the example with at least two turn interactions with   an average length of utterance no more than 25   vocab . And we randomly split the train - dev - test   of collected datasets as Chitchat ( 1490/300/300 ,   5/1/1 ) , Empathetic Dialogue ( 3022/500/500 , 6/1/1 ) ,   Persona Chat ( 3000/500/500 , 6/1/1 ) .   A.1.2 Annotation Guideline   Table 6provides detailed instructions for the anno-   tators to s help them understand the setting of our   annotation task .   A.1.3 User Study   The dialogue can be evaluated from multiple per-   spectives . Some perspectives are universal to as-   sess all dialogue agents , e.g. , ﬂuency , and rele-   vance , while the other metrics are only used for   task - speciﬁc dialogue agents . For example , the   emotion - aware is a critical property for empathetic   dialogue but is less important for persona dialogue .   Therefore , we ﬁrst simplify by sorting the possible   aspects into two categories , i.e. , the general view   and the task - speciﬁc view . The former contains rel-12867   evance , engagingness , and ﬂuency , while the latter   consists of understandability , emotion - aware , and   personality - aware , which correspond to chitchat di-   alogue , emotional dialogue , and persona dialogue .   To understand the relation between sub - metrics and   overall quality , we conduct a user study to learn   their preference for different sub - metrics . Specif-   ically , we invite 20 experts and 80 users , each of   whom is asked to select the four most important   ones from the sub - metrics . The results are listed in   Figure 4 . The approval rates reﬂect the user prefer-   ence for different sub - metrics , which can be used   as a weight to calculate the overall score . Moreover ,   we apply the softmax function on these weights to   make them more interpretable .   A.2 Experiment Details   A.2.1 Datasets for Pre - train Stage   Our training process includes two stages , e.g. ,   cross - domain pre - train and task - speciﬁc ﬁnetune .   We ﬁrst pre - train the model on diverse open-   domain dialogue datasets as listed in Table 7with   the objective L. The next stage relies on task-   speciﬁc dataset with the objective L(see in sec-   tion5 ) .   These datasets are collected from https://   github.com/e0397123/dstc10_metric_track ,   which contain a variety of open - domain dialogue ,   such as emotional dialogue , personalized dialogue ,   knowledge - grounded dialogue , and chitchat . Every   example in the datasets contains the dialogue   context , response generated by dialogue agent ,   pre - created reference response , and the score of   generated response which has been annotated for   at least three people from several perspectives .   We use cross - domain datasets for pre - training to   improve the robustness and generalisability of the   models across different evaluation scenarios .   A.2.2 Experimental Details on Our   Benchmarks   We show the details of our automatic evaluation   experiments in Table 9 . The BERTScore and   BLEURT are computed based on the large ver-   sion of Roberta . As in Section 6 , we imple-   ment two reference - based baselines , BERT   and BART , using the same human - annotated   datasets as RADE for training , and provide a   reasonable comparison with our proposed model .   Speciﬁcally , the BERT is built on the base   version of BERT ( Devlin et al . , 2018 ) , while   the BART is built on the base version of   BART ( Lewis et al . , 2019 ) .   A.2.3 More Fair Comparison after Training   For a fair analysis , we pre - train the two strongest   baselines ( QuantiDCE and GRADE ) with our   cross - domain dataset . GRADE achieves Pear-   man’r=0.383 , 0.378 , -0.122 , and QuantiDCE   achieves Pearman’r=0.408 , 0.522 , 0.238 in the   ChitChat , EmpatheticDialogue , and Personachat   datasets . However , our proposed RADE(PT ) re-   mains the best results ( Pearman’r=0.472 , 0.650 ,   0.386 ) . We further ﬁne - tune GRADE and Quan-   tiDCE with our self - collected datasets for a more   comprehensive analysis . GRADE achieves Pear-   man’r=0.413 , 0.430 , -0.013 , and QuantiDCE   achieves Pearman’r=0.458 , 0.589 , 0.278 in three   datasets , underperforming the proposed RADE(TS )   ( Pearman’r=0.601 , 0.863 , 0.470 ) .   We skip pre - training/ﬁne - tuning four baselines   for the following reasons : ( 1 ) UniEval and Ques-   tionEval have been pre - trained on multiple datasets   across various domains . ( 2 ) The FED metric is   unsupervised ( cf . Shikib Mehri et al . ) ( 3 ) The   DialoRPT has been trained on a sizeable human-   feedback dataset ( 133 M ) covering various domains .   These analyses validate the superiority of our   method.12868   A.2.4 Results on Existing Benchmarks   We further examine three existing benchmarks , i.e. ,   USR - TopicalChat , USR - PersonaChat and Grade-   DailyDialogue to verify the efﬁciency and robust-   ness of RADE when generalizing to agnostic sce-   narios . USR - TopicalChat and USR - PersonaChat   datasets are collected to assess dialog evaluation   metrics , with examples containing the dialogue con-   text , reference , response and corresponding scores ,   which three people have annotated . The Grade-   DailyDialogue contains high - quality open - domain   conversations about daily life including diverse top-   ics . And the results are summarized in Table 8 .   The experimental results show that RADE out-   performs the state - of - the - art reference - free and   reference - based methods on the USR - TopicalChat   dataset . For example , we push the Pearson cor-   relation to 48.0 % ( 7 % deﬁnite improvement ) and   Spearman correlation to 46.6 % ( 4 % absolute im-   provement ) . Moreover , RADE shows a stronger   correlation with human judgment than existingreference - based methods on the second dataset .   It achieves comparable , even better results with   the reference - free methods except for USL - H. The   results demonstrate that our pre - trained model is   more robust even under agnostic scenarios .   We also compare the two existing methods , and   the results suggest a similar phenomenon as 3 .   Firstly , the reference - free methods achieve bet-   ter consistency than reference - based methods , i.e. ,   the former has the highest result of r= 41 .2 % ,   ρ= 42 .3%while the latter gets r= 34 .2 % ,   ρ= 34 .8%on the USR - TopicalChat dataset . How-   ever , the reference - free methods suffer from more   signiﬁcant variance . For example , the MAUDE   gets r= 0.345 % andρ= 0.298 % on the USR-   PearsonChat dataset but gets r= 0.044 % and   ρ= 0.083 % on the USR - TopicChat dataset . It   indicates that reference - free methods are more vul-   nerable and prone to data - induced bias.12869A.2.5 Case Study   To explain more intuitively , we show examples of   automatic evaluation and them with human judg-   ment in Table 10,11,12 , suggesting that the   scores of our methods are closer to human ratings .   A.3 Presudo reference   Since the original FED does not provide the ref-   erence response , we construct a pseudo - reference   via retrieval or generative method . The former re-   trieves reference from a curated response corpus   based on our cross - domain datasets via BM25 with   the dialogue context as the query . The latter gen-   erates via a large language model GPT-3 based   on the dialogue context . The results show that   RADE(PT ) obtains Pearman’r=0.381 and Spear-   man’rho=0.368 with the retrieved reference while   achieving Pearman’r=0.343 , Spearman’rho=0.347   with generative reference , outperforming the state-   of - the - art baseline ( QuantiDCE , Pearman’r=0.319 ,   Spearman’rho=0.323 ) .   To further validate the generalizability of our   method , we evaluate our proposed RADE(PT )   on another challenging benchmark , GRADE-   Dailydialogue . Our RADE(PT ) achieves Pear-   man’r=0.356 and Spearman’rho=0.370 with 5 %   and 2 % relative improvements compared to state-   of - the - art baseline , indicating that our method can   generalize to more challenging benchmarks.12870128711287212873ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Section Ethics Statement   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Section 6 and 7   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 612874 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 6   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 7   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 6   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 4 and Appendix A   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix A   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Section 4 and Appendix A   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Section 4   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Section 4   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Left blank.12875