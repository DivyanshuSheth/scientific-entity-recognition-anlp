  Kumar ShridharJakub MacinaMennatallah El - Assady   Tanmay SinhaManu KapurMrinmaya SachanDepartment of Computer Science , ETH ZurichETH AI CenterProfessorship for Learning Sciences and Higher Education , ETH Zurich   Abstract   Socratic questioning is an educational method   that allows students to discover answers to   complex problems by asking them a series of   thoughtful questions . Generation of didacti-   cally sound questions is challenging , requir-   ing understanding of the reasoning process in-   volved in the problem . We hypothesize that   such questioning strategy can not only enhance   the human performance , but also assist the   math word problem ( MWP ) solvers . In this   work , we explore the ability of large language   models ( LMs ) in generating sequential ques-   tions for guiding math word problem - solving .   We propose various guided question generation   schemes based on input conditioning and rein-   forcement learning . On both automatic and hu-   man quality evaluations , we find that LMs con-   strained with desirable question properties gen-   erate superior questions and improve the over-   all performance of a math word problem solver .   We conduct a preliminary user study to exam-   ine the potential value of such question genera-   tion models in the education domain . Results   suggest that the difficulty level of problems   plays an important role in determining whether   questioning improves or hinders human perfor-   mance . We discuss the future of using such   questioning strategies in education.https://github.com/eth-nlped/   scaffolding - generation   1 Introduction   Questioning can be a valuable way of supporting   student thinking . It can be conceived as a scaffold   ( Wood et al . , 1976 ; Quintana et al . , 2004 ) , where a   more knowledgeable tutor helps a student in solv-   ing problems otherwise too difficult . One approach   well - suited for mathematics is funneling ( Wood ,   1994 ) , which uses prompting questions to guide   students towards a solution . Figure 1 : Math word problems can be precedurally   solved in multiple reasoning steps . One operationaliza-   tion of Socratic questioning is to map each step in the   procedure to a question . Asking ( machines / humans ) the   right set of questions in a certain sequence ( shown in   green ) can be an effective way to do so . In order to be   effective , the Socratic questioning should be focused   and goal - driven .   Figure 1 shows an example of a math word problem   where this questioning strategy might be beneficial .   We hypothesize that these questions can not only   help humans in understanding the problem better   and improve their performance but can also assist   MWP solvers .   Even though question generation ( QG ) models   have been studied for factual SQuAD - like ques-   tions ( Rajpurkar et al . , 2016 ; Puri et al . , 2020 ) ,   these models fail to generate sequentially - coherent   questions ( Reddy et al . , 2019 ; Choi et al . , 2018 ) .   Furthermore , domain - specific questioning is chal-   lenging as the QG model needs to understand the   reasoning process required to provide fine - grained   responses . Moreover , the role of a teacher us-   ing questioning is to interject questions that focus   on the most critical points in an explanation and   take the understanding forward ( Anghileri , 2006 ) .   As seen in bold in the Figure 1 , we refer later to   these properties of questioning as focused andgoal-   driven .   In this work , we explore the use of large language   models ( Raffel et al . , 2020 ; Radford et al . , 2019 ) to   generate guiding sub - questions for math word prob-   lems . In particular , we use reinforcement learn-4136ing ( RL ) with rewards from various sources includ-   ing Math question answering ( Math QA ) models   and various forms of input conditioning for gener-   ating these questions . We train and evaluate our   models on the recently released GSM8 K MathQA   dataset ( Cobbe et al . , 2021 ) of multi - step reasoning   MWPs . We illustrate the benefit of our RL - based   generation strategy using both automatic and hu-   man evaluation metrics . Our evaluation shows that   our guided approach makes the generation model   ask more logically relevant andstructurally correct   questions , which follow the appropriate sequenc-   ing of questioning at the right granularity level .   We further show that our generated questions , when   provided as additional context , can aid a math ques-   tion answering model , thereby providing further   empirical justification of the value of questioning   for math QA model training . Questioning could   facilitate reasoning of MWP solvers by making in-   termediate reasoning steps explicit . Finally , we   explore the didactic usefulness of our questioning   strategy by conducting a preliminary user study and   use it to show that the generated sequence of ques-   tions may have the potential to improve students ’   problem - solving . However , we cautiously note that   achieving this would require further progress on   many fronts in AI and Education .   In what follows , we begin by discussing related   work and introducing our research questions in sec-   tion 2 and section 3 . We propose ways to induce   these properties in LMs using planning and rein-   forcement learning in section 4 ; section 5 empir-   ically demonstrates the effectiveness of inducing   questioning strategy in LMs and the quality of gen-   erated questions evaluated using automatic metrics   and by humans . Finally , we evaluate the potential   of using such questions as an educational tool for   helping students solve MWPs in section 6 .   2 Related Work   Socratic questioning approaches have evolved   within the learning sciences community into the   theory of scaffolding ( Wood et al . , 1976 ; Reiser ,   2004 ) , which broadly refers to assisting students   in problem - solving beyond their zone of proximal   development ( Quintana et al . , 2004 ) . Computer-   based scaffolds ( e.g. , in the form of hints , prompts ,   feedback ) have moderate effects on student learn-   ing outcomes ( Kim et al . , 2018 ) , and our work can   be used to automatically generate such scaffolds inthe form of questioning prompts . For mathematics ,   Wood ( 1994 ) analyzed interactions in math class-   rooms and proposed two distinct interaction pat-   terns - funneling , which functions by guiding stu-   dents using leading / prompting questions to a prede-   termined solution procedure , and focusing , which   functions by drawing student attention to the crit-   ical aspects of the problem . We draw inspiration   from this strand of work . Our overall question gen-   eration approach can be conceived to be similar to   funneling , with specific sub - questions focusing on   the important domain concepts .   Research on question generation includes visual   question generation ( Fan et al . , 2018 ; Wang et al . ,   2022 ) , generation of questions for student assess-   ment ( Stasaski and Hearst , 2017 ; Wang et al . ,   2018 ) , generation of factual questions based on   Wikipedia articles ( Rajpurkar et al . , 2016 ; Ko et al . ,   2020 ) or generation of sequential information-   seeking questions in dialogue - based scenarios   ( Reddy et al . , 2019 ; Choi et al . , 2018 ) . Other work   has also explored similar ideas of improving an-   swerability by question - asking ( Klein and Nabi ,   2019 ; Shwartz et al . , 2020 ; Perez et al . , 2020 ; Pan   et al . , 2021 ) and ranking them ( Rao and Daumé III ,   2018 ) . However , factual questions do not usually   require much reasoning and mostly boil down to   information retrieval from text . In this work , we fo-   cus on question generation for reasoning problems .   Prior work on guided and controlled question gen-   eration uses either entities as guiding mechanism   ( Huang et al . , 2021 ) or reinforcement learning-   based graph to sequence approach ( Chen et al . ,   2019 ) . Identification of entities and relationships   present in the text often uses rule - based or on - shelf   extraction tools , which are hard to extend ( Dhin-   gra et al . , 2020 ) . Often these single - hop questions   are combined to form a multi - hop question that re-   quires complex reasoning to solve it ( Pan et al . ,   2021 ) . Controllable text generation has been stud-   ied in the past for text generation ( Hu et al . , 2017 ;   Miladinovi ´ c et al . , 2022 ; Carlsson et al . , 2022 ) ,   Wikipedia texts ( Liu et al . , 2018 ; Prabhumoye et al . ,   2018 ) and data - to - text generation ( Puduppully and   Lapata , 2021 ; Su et al . , 2021 ) . Controlled text gen-   eration is particularly useful for ensuring that the   information is correct or the numbers are encapsu-   lated properly ( Gong et al . , 2020 ) . Our task has   similar requirements .   A final strand of related work lies in the ballpark of4137   math problem solvers ( Hosseini et al . , 2014 ; Kush-   man et al . , 2014 ; Roy et al . , 2015 ; Seo et al . , 2015 ;   Sachan and Xing , 2017 ; Sachan et al . , 2017 , 2018 ,   inter alia ) . Recent work in this area uses special-   ized architectures such as graph - based encoders   ( Zhang et al . , 2020 ) and tree - based decoders ( Xie   and Sun , 2019 ) , and more recently , large pretrained   LMs which show state - of - the - art results ( Cobbe   et al . , 2021 ; Shen et al . , 2021 ; Kojima et al . , 2022 ;   Wei et al . , 2022 ; Chowdhery et al . , 2022 ) . Appli-   cation of these approaches to the MWP datasets   like GSM8 K ( our data context ) still holds consid-   erable room for improvement , primarily in the rea-   soning capabilities , and the majority of the latest   approaches are still unable to solve a lot of the   problems and sensitive to even slightest modifica-   tions in the problem ( Patel et al . , 2021 ; Stolfo et al . ,   2022 ; Srivastava et al . , 2022 ) .   3 Research Questions   We now discuss the usefulness of questions in solv-   ing a math word problem and then study the differ-   ent properties of a good questioning strategy .   RQ1 : Does sub - questioning help in understand-   ing a math word problem better ? Question   prompts as a teaching strategy act as instructions   that guide the students throughout a problem-   solving process ( Wood , 1994 ) . Such question-   ing , as a valid scaffolding strategy ( Kim et al . ,   2018 ) , is valuable in supporting student thinking   and is commonplace in high - quality math instruc-   tion ( Boston and Candela , 2018 ) . We explored   the sub - questioning strategy with our trained NLP   model and found that sub - questioning helps answer   the MWPs more effectively ( Table 1 ) . Experiments   with NLP models and humans establish the useful - ness of sub - questioning in solving MWPs .   RQ2 : What are the properties of a good ques-   tioning strategy ? Once we established that sub-   questioning is helpful , we performed the same sub-   questioning experiment as RQ1 with NLP models   but with the permuted ordering of sub - questions ,   change in granularity of sub - questions or changed   content ( Table 2 ) . We observed a decrease in the   answering capabilities of the QA model for all the   cases , establishing that the right sequence of dis-   ciplined questions with relevant content is an es-   sential component of a good questioning strategy .   Based on our results and inspired by prior work   ( Wood , 1994 ; Anghileri , 2006 ) , we hypothesize the   most important components of a Socratic question-   ing strategy as :   ( A)Focused : An essential property of a good   questioning strategy is to ask questions that   are directed towards the most critical domain-   specific content . Irrelevant questions not only   make the process difficult but also force a di-   version in the focus and may increase the cog-   nitive load that a student experiences .   ( B)Goal - driven : Asking the right sequence of   relevant questions that can assist students in   reaching the final goal ( solving the main ques-   tion in case of math word problems ) is a fur-   ther important part of good questioning .   4 Methodology   We discuss our approach to modeling Socratic ques-   tioning using large LMs . We begin by defining our   MWP dataset Das a collection of MWPs . Each   MWP Pin the dataset is accompanied by its so-   lution Sand the numerical answer A. We do not4138always assume the existence of problem solutions   Sand answers Aas they can be automatically de-   rived from various MathQA models . Each MWP   P= ( C , Q)consists of the story context Cand the   question Q. The problem solution Sconsists of n   solution steps S= ( s , ... , s ) . We define Socratic   questioning such that each solution step scan be   mapped to a sub - question q. We refer to qas a   collection of all Socratic questions q , ... , qfor a   given MWP Pin our work . An example MWP is   present in Figure 2 .   Our main module is the Question Generator ( QG )   module , which is a transformer ( Vaswani et al . ,   2017 ) based encoder - decoder model . The QG   model takes the reference Math word problem P   and generates the Socratic questions qas close to   the true sub - questions qas possible . The learning   objective of the QG module is as :   L=−/summationdisplaylogP(q|q;Enc(P))(1 )   where Enc represents the encoder and Dec repre-   sents the decoder for the seq2seq QG model . Note   that the sub - questions qare decoded word by word   in an auto - regressive setting .   Next , we propose to inject the two Socratic ques-   tioning properties in our QG model as follows :   4.1 Focused questions   To learn a sequence of disciplined questions fo-   cused on specific reasoning steps in the MWP , it is   important to ask the right set of questions . We pro-   pose a content planner ψthat serves as a guiding   principle for the QG model to ask the right focused   questions . In principle , the content planner module   can extract any relevant information to assist the   QG model , but for the task of math word problems ,   we restrict it to operators and equations . Our plan-   ning strategies are defined as :   Operators : Given an MWP P , the content plan-   ner learns to identify the operations and operators   ( e.g. , addition , multiplication , .. ) involved in the   problem . Since the operators play a significant role   in a given MWP , the generated operators are used   as the guiding principle to generate sub - questions   by the QG model . Equations : Equations contain important infor-   mation for an MWP as they involve not just the op-   erators but also the quantities involved in the prob-   lem . Similar to operators , equations can play an im-   portant guiding principle for asking more focused   questions leading towards a correct solution .   We use the same seq2seq architecture for the con-   tent planner module as our QG model , with the only   difference being that the output comprises a set of   equations s , .. , sor just the operators within the   equations ( instead of the sub - questions ) . The gen-   erated operators / equations are appended to the in-   put MWP Pin the encoder for the QG module and   the modified focused learning objective Lis :   L=−/summationdisplaylogP(q|q;Enc([P⊕plan ] ) )   ( 2 )   Here , plan depicts the content planner module ’s   output and ⊕depicts the concatenation operation .   4.2 Goal - driven questions   An essential element of a good questioning strategy   is to ask goal - driven questions that are not only fac-   tually associated to the main problem but also even-   tually help in answering the main question . How-   ever , there can be any number of goal - driven ques-   tions that can be asked for a MWP . Thus , our goal   is to optimize the questioning strategy such that it   is goal - driven , efficient , and rewarding at each step ,   making sure that the final goal can be achieved   with these individual questions . We induce these   properties in our QG model using various rewards   that force the model to stay relevant to the problem .   These rewards are defined as :   Fluency : It is important that the generated sub-   questions are easily understandable and fluent in   the meaning they represent . Although the QG train-   ing objective ensures the syntax and semantics of   the questions generated , rewarding the system to   stay fluent is necessary to remove repetitions and   illogical questions .   Granularity : As solving a MWP usually in-   volves multiple reasoning steps , asking relevant   questions at each step can help in solving the MWP .   Moreover , our questioning strategy is based on the   fact that the questions are organised , structured and   follow a sequence . With the granularity reward , the   model can learn to ask the right number of ques-   tions ( compared to the number of reasoning steps4139to solve MWP ) in a specific sequence and refrain   from unstructured questions .   Answerability : For every generated question , it   is important to evaluate if the generated questions   can be answered given context C and can help in   answering the overall MWP . We trained an external   QA model that can answer the MWPs taking help   from the sub - questions and evaluated if the gen-   erated question can assist in answering the main   problem . The answerability reward is provided on   both a step - by - step basis ( if the QA model can an-   swer a sub - part of the main problem ) and overall   ( if using all sub - questions , whether the final answer   was correct or not ) .   During training , the QG model samples a set of sub-   questions q , calculates various rewards based on   q. The parameters of the QG model are updated   using the REINFORCE algorithm ( Williams , 1992 )   as :   L=−E[R(q , q , P ) ]   = −R(q , q , P)/summationdisplaylogP(q|q;Enc(P ) )   The reward function [ R(q , q , P)]measures the in-   dividual rewards for fluency , granularity and an-   swerability and is calculated as :   Fluency : R = BLEU ( q , q )   where , BLEU ( . , . ) represents the BLEU score ( Pap-   ineni et al . , 2002 ) .   Granularity : R = F(q , q )   where , F(q , q ) = 1− , and|q|and|q|   denote the number of questions in qandqrespec-   tively .   Answerability : R = F(A , A )   where , F(A , A ) = 1 if the final answer from the   QA model is correct when it is given sub - questions   qalongside the MWP P , and 0otherwise . Ade-   notes the answer from the QA model and Ade-   notes the true answer .   We also evaluated the step - by - step performance   of the QA model on the generated sub - questions   to check if the QA model can answer the gener-   ated sub - questions correctly . This allows us to pro-   vide partial rewards at each step of the generationVariation GPT-2 GPT-3   P 5.45 ( ↓47 % ) 29 ( ↓38 % )   P⊕ { q } 10.46 47   model . The modified sub - step answerability re-   ward is F(A , A ) = , where # aand|q|de-   note the number of correct answers to the gener-   ated sub - questions and total number of generated   questions respectively .   4.3 Overall Loss Function   Finally , with the induced Socratic properties in the   QG model , the total loss is defined as a combination   of the focused learning loss Land the loss of   the rewards L , as :   L = αL+ ( 1−α)L ( 3 )   where αis a weighting factor .   5 Empirical Analysis   We now demonstrate the effectiveness of inducing   the defined questioning properties in large LMs .   Dataset We study the properties of Socratic ques-   tioning on the GSM8 K dataset(Cobbe et al . , 2021 )   that consists of 8.5 K grade school math word prob-   lems . Each problem requires 2 to 8 reasoning steps   to solve , and solutions primarily involve a sequence   of elementary calculations using basic arithmetic   operations ( + − ∗/ ) . The dataset is segmented   into 7.5 K training problems and 1 K test problems .   Models We used T5 ( Raffel et al . , 2020 ) as the   backbone of both our QG and content planning   modules . For reward generating QA model , we   used GPT-2 ( Radford et al . , 2019 ) for all RQ2 ex-   periments because of resource constraints . How-   ever , a better QA model like GPT-3 ( Brown et al . ,   2020 ) can be used in the future . Both QG and con-   tent planning models are fine - tuned on the GSM8 K   training set using the Huggingface library ( Wolf   et al . , 2020).4140Variation QA Accuracy   Granularity   P⊕ { q}5.45 ( ↓45 % )   P⊕ { q}3.94 ( ↓62 % )   P⊕ { q}3.35 ( ↓67 % )   P⊕ { q}9.70 ( ↓7 % )   P⊕ { q}10.46   Order   P⊕shuffle ( { q } ) 8.94 ( ↓14 % )   Relevance   P⊕<base - ques > 2.57 ( ↓75 % )   Implementation Details For the training of the   models , we used Nvidia Tesla A100 with 40 GB   of GPU memory . We ran each experiment for 50   epochs , with a periodical evaluation of the valida-   tion set . Training time without using rewards is 10   minutes per epoch . With rewards , the training time   per epoch is increased to several hours . We used   the T5 - large model without modifications for the   content planner and question generation module   and GPT-2 small as the QA solver .   Evaluation Metrics We report automatic evalua-   tion using SacreBLEU ( Post , 2018 ) which is based   on exact word overlap , BERT F1 score ( Zhang   et al . , 2019 ) which is based on DeBERTa ( He et al . ,   2020 ) as the similarity model . We also report # Q ,   the number of questions generated compared to the   number of ground truth reasoning steps ( same as   Granularity reward ) , and Math QA Solver accu-   racy ( same as the overall Answerability reward ) to   assess if our generated questions helped the QA   model reach the final numerical solution .   5.1 RQ1 : Does sub - questioning help in   understanding math concepts better ?   We hypothesize that high - quality sub - questioning   helps Math QA solvers to reach the correct solu-   tion , especially when questions are relevant to thePlanning BLEU BERT F1 # Q   None 51.53 0.783 0.428   Operators 54.98 0.788 0.642   + planner 45.05 0.779 0.346   Equations 58.82 0.813 0.807   + planner 52.48 0.787 0.485   concept to be learnt , in the right sequence ( order-   ing ) with high granularity in their structure . We   verify our hypothesis with GPT-2 model as a QA   solver after fine - tuning it on the training set of the   GSM8 K dataset and the GPT-3 model with one-   shot prompting . Table 1 demonstrates that the So-   cratic questioning improves the performance of the   QA solver as high as 45 % . Then , we vary the prop-   erties of the test questions and examine the per-   formance of the QA Solver . Table 2 demonstrates   that Socratic questions significantly improve the   model performance from 5.45 % to 10.46 % . Sub-   questioning even helps when only 75 % Socratic   questions are retained ( denoted as { q}in the ta-   ble ) or when the order is shuffled ( this might be   an artefact of the dataset containing a minority of   examples with strict order ) . An interesting obser-   vation is that when the number of Socratic ques-   tions is reduced by half or lower ( while preserving   their order ) , the model gets confused and performs   worse than when it had no sub - questions . Finally ,   we take the pre - trained T5 model and without fine-   tuning it for our task , we take the outputs and used   it alongside the problem Pas additional informa-   tion to solve the problem . The performance goes   as low as 2.57 % , indicating that non - relevant infor-   mation degrades the performance .   5.2 RQ2 : What are the properties of a good   questioning strategy ?   We now present our analysis on inducing the two   Socratic properties to LMs .   Focused generation : Table 3 compares the two   planning strategies . Results demonstrate that plan-   ning strategies improve the baseline methods by   more than 3%on BLEU score with operators as   planning , and by more than 7%with equations.4141Strategy BLEU BERT F1 # Q   Baseline 13.02 0.566 0.056   Fine - tuned 51.53 0.783 0.428   + fluency 52.21 0.784 0.440   + # of questions 51.86 0.784 0.431   + QA 52.22 0.783 0.417   + all weighted 53.39 0.781 0.431   Eq planning 58.82 0.813 0.807   + fluency 59.52 0.816 0.818   + # of questions 59.75 0.814 0.811   + QA 59.37 0.813 0.799   + all weighted 59.62 0.815 0.815   Similar to the BLEU score , we achieve better per-   formance on BERT F1 scores too . Finally , the num-   ber of correct question count improves with plan-   ning and doubles compared to the no - planning vari-   ant . However , results show that in all the variants   the number of generated sub - questions is less than   the number of reasoning steps . This could be im-   proved further by oversampling during the beam   search ( beam search settings are the same for all   variants in this experiment ) . The results degrade   when the ground truth content ( both equations and   planning ) is replaced by our content planner mod-   ule . This is expected as the errors in the content   planning module are cascaded when generating   sub - questions . However , with more powerful mod-   els , errors in the content planner can be reduced ,   leading to improvement in all the metrics . See the   Appendix for experiments with the iterative split-   ting of MWP into multiple parts for generation .   Goal - driven generation : Table 4 summarizes   the results for the rewards as a strategy to incen-   tivize the model to generate goal - driven and reward-   ing questions . We can observe the gains associated   with each reward for both the baseline model and   the best - performing model from Table 3 ( equation-   based content planning model in our case ) , suggest-   ing the importance of rewards .   QA performance We study the impact of the   QG model considering both Socratic properties as   shown in Table 5 . Sub - questions with operators   and equations as planning improves the QA perfor-   mance by 1−2 % . Rewards , although improves the   QG quality , have a negligible effect on QA perfor - Strategy QA Accuracy   No planning 6.74   + rewards 6.75   Operators 7.50   + rewards 7.52   Equations 8.49   + rewards 8.50   Planning BLEU BERT F1 # Q   None 51.53 0.783 0.428   Diff op , diff # 51.59 0.785 0.415   Diff op , same # 54.26 0.786 0.546   Operators ( op ) 54.98 0.788 0.642   mance . This is mainly because slight improvement   in sub - questions quality does not necessarily help   in reaching the final goal .   5.3 Human quality evaluation   Next , we perform a human evaluation of the ques-   tions generated for 100 randomly selected test   MWPs to assess the quality of our model gener-   ation ( our best model ) compared to the baseline   ( with no planning or reward - based strategies ) . For   this analysis , we divided the questions among 4 an-   notators with an overlap of 40 % of the questions   among themto evaluate the generated question   quality on the following factors . A 5 - point Likert   scale ranging from 1 ( poor ) to 5 ( very good ) was   used for each dimensions of quality assessment :   repetition - whether questions are repeated , factual-   ity- whether all questions can be solved by the in-   formation given in the problem , logical relevance   - if the question is logically related to the MWP ,   right sequence - correct sequence of questions lead-   ing to the final answer , granularity - questions are   granular enough to solve the problem but are still   relevant and no retrieval or basic common sense4142questions are asked , completeness - questions are   complete with all steps covered to reach to the fi-   nal answer , and fluency - grammatical correctness   and fluent in the language .   Figure 3 presents our findings , clearly demonstrat-   ing that our planning and reward strategies lead to   superior quality questions on the MWP task . Al-   though both baselines and our model - generated   text achieve almost full score ( 5 ) on the fluency pa-   rameter , our model - generated questions are more   aligned to the MWP , thus leading to a higher score   on all the other parameters . We also present a ran-   domly selected sample of generated questions in   the Appendix .   5.4 Ablation study : Manipulating question   properties   Both planning strategies help generate better ques-   tions . To gain a deeper understanding of how con-   tent planner ψaffects generated questions , we fur-   ther analyze the influence of operators as a planning   strategy . Here , we randomize operators and their   sequence and measure change in performance . Ta-   ble 6 shows that the correct sequence of operators   with the correct number of operators guides the gen-   eration process better than randomized versions . A   gap between the correct count of operators and ran-   dom count indicates that having a correct number   of operators ( of any type ) is more valuable than the   exact type of operators . We observed that the num-   ber of operators guides the model in terms of the   number of questions that need to be asked , while   type changes the overall quality . Needless to say ,   for the same number of operators , quality matters.6 A preliminary user study with learners   Finally , we designed a preliminary user study to   evaluate whether our generated questions , when   presented as further problem - solving exercises ( as   typically used in educational settings ) can help   learners on the way to solving the overall prob-   lem . Given our research question , we hypothesized   thatguidance with questions can increase the over-   all problem - solving success rate for users in the   questions ( treatment ) group compared to the no-   questions control group . Our study uses Socratic   questions as the main pedagogical intervention . We   focus on participants who can not solve a problem   on the first attempt to clearly distinguish the impact   of automated sub - questioning . The key metric we   measure is the success rate , which is defined as the   percentage of correctly solved problems .   For our study , we built a simple user interface   which allowed participants to solve math word   problems ( see Figure 5 and Figure 6 in the ap-   pendix ) . The interface contained a calculator which   the users could use if needed . The study com-   prises 5 pre - test problems and 8 problem - solving   exercises . These problems were randomly selected   from the GSM8 K test set . Our user study with this   interface was then deployed on Mechanical Turk   and participants were hired using the platform and   were paid 10 - 12 $ per hour . We selected partici-   pants with moderate levels of prior knowledge us-   ing the pre - test scores as the selection criteria , and   only those scoring in the range of 40 - 80 % were se-   lected for the study . This way , we excluded both   low - prior knowledge participants and experts in   our study to ensure there was a learning possibility .   We randomly split the participants into two groups   -no - questions group ( N=19 ) with no question   prompts , and questions group ( N=17 ) with ques-   tions generated from our model . Both groups used   the same interface for solving math word problems   and had the opportunity to resolve their answers af-   ter the first incorrect submission . The only differ-   ence was that after incorrectly solving a problem on   the first submission , participants in the questions   group saw sub - questions , while those in the no-   questions group were only prompted to try again .   The sub - questions were generated using the best-   performing model with planning and rewards .   The results of the user study are shown in Table 7 .   The first attempt success rate is 58.4 % for the4143control group and 66.0 % for the treatment group ,   which might be the result of a slightly skewed prior   knowledge distribution of 0.68 and 0.65 for treat-   ment and control groups respectively . Even though   participants in the treatment group ( M= 124 .9 ,   SD= 92.1 ) spend significantly more time ( p <   0.01 ) solving problems during the second attempt   relative to the control group ( M= 41.5,SD=   31.4 ) , we did not find any statistically significant   difference between the groups in the second sub-   mission success rate ( p= 0.659,BF= 2.755 ,   Cohen ’s d= 0.157 ) , indicating weak odds favour-   ing the null hypothesis and rather a small effect   size .   As our study was unable to establish overall per-   formance improvements , we further analysed the   second submission success rate per problem ( see   Figure 4 ) , and correlated it with the difficulty of   the question . This analysis indicated that sub-   questioning seems to improve the success of sim-   pler problems and degrade the accuracy for rela-   tively more complex problems . Prior work has sug-   gested that the effectiveness of question prompts   varies according to an individual ’s prior knowl-   edge ( Kim et al . , 2018 ) , and with insufficient prior   knowledge , performance for complex problems   may suffer . A posthoc inspection of the generated   sub - questions for more complex problems shows   that they also scored lower in the human quality   evaluation . Thus , we hypothesize that for more   complex questions , the generated sub - questions are   not good enough , and so they may make the task   more challenging for participants .   While we were not able to establish any direct ben-   efits of automatic Socratic questioning in a real   learning scenario , we leave a more complete user   study for future work . Deployment of Socratic   questioning systems in real educational scenarios   would require a better assessment of question gen-   eration quality as well as a better understanding of   learners . We believe this is an interesting avenue   for future research and encourage future work to   attempt to address these issues .   7 Conclusion   We study the importance of sub - questioning for   learning a mathematical concept and explore how   LMs may generate these sub - questions . We demon-   strate the usefulness of Socratic questioning strate-   gies and propose ways to induce these propertiesGroup 1st success 2nd success   M SD M SD   No - questions 58.4 23.0 35.8 32.5   Questions 66.0 21.1 31.0 27.9   in LMs . We further evaluate if these questions can   assist students in learning domain concepts . We   found that the generated questions were generic for   each student and if adapted to their prior knowl-   edge and intermediate solutions , their effectiveness   could have been greater .   A discussion on limitations of our work   Our questioning strategy , although utilizes infor-   mation from the content planner and the reward   strategy , leaves much to be desired in terms of its   controllability . Based on our user study , we need   to be careful in using the questioning strategy in   real educational contexts , as improper content can   sometimes do more harm than good . Based on the   prior work , we focused on two aspects of goodness   in questioning in math education . However , this is   not a complete list and other aspects could also be   important . We note that our user study was focused   on the intermediate success rate rather than on ac-   tual learning . From a learning standpoint , asking   questions that are always easily answerable wo n’t   lead to deeper , wider learning . If learners do not   have to struggle to answer the sub - questions being   asked and are instead repeating something verba-   tim or offering a slightly reconfigured version of4144what they have been asked , they are probably an-   swering sub - questions that do not require concep-   tual understanding . Another limitation of our work   is that our user study was underpowered due to re-   source constraints , which prevents us from draw-   ing strong conclusions at this point . A larger user   study is however forthcoming .   Finally , we choose to focus on Socratic questioning   in a rather narrow sense of trying to call learners ’   attention to relevant facts and then implicitly stimu-   lating them to integrate facts and draw conclusions .   However , when taken together with all its nuances ,   the effectiveness of Socratic questioning can be   posited to depend on other critical question types   that seek clarification ( e.g. , can you rephrase ? ) , ev-   idence ( e.g. , can you provide an example ? ) and im-   plication ( e.g. , why do you think . . . ? ) from learn-   ers too , all of which are truly dialogic and natu-   rally leave room for learner questions . When both   the teacher and learners are jointly responsible for   pushing the dialogue forward , intermediate success   may also not always be desirable as learner errors   and misconceptions may offer an important hook   for the teacher to nudge the dialogue productively .   Acknowledgements   This project was made possible by an ETH AI Cen-   ter Doctoral Fellowship to Jakub Macina with par-   tial support by the Asuera Stiftung and the ETH   Zurich Foundation . Many thanks to the group mem-   bers and our reviewers for their valuable feedback .   References414541464147   A Details of User Study   We perform a user study using Amazon Mechanical   Turk . Participants which did not spend a minimum   time per question were excluded from the analysis .   Generated questions used in the questions group   are listed in the Table 9 .   Planning BLEU BERT F1 # Q   None 49.39 0.763 0.390   Operators 55.25 0.779 0.752   Equations 58.31 0.795 0.819B Experimental details   B.1 Iterative   Except global strategy to generate questions given a   MWP , we experimented with iteratively generation   on the sentence level .   This also explains the # Qfor iterative case to be   not equal to 1as there are some duplicates gener-   ated by the model and sometimes the split is not   perfect .   B.2 GPT-3 prompting   We used one shot prompting for GPT-3 meaning   we provide one example ( Q , A ) to the model and let   it predict the answer ( A ) for the next question ( Q )   provided .   No sub - questions Problem : John has 10   hectares of a pineapple field . There are 100 pineap-   ples per hectare . John can harvest his pineapples   every 3 months .   Q : How many pineapples can John harvest within a   year ? A : John has 100 x 10= « 100 * 10=1000»1000   pineapples on his field . John can harvest his Pineap-   ple 12 / 3 = « 12/3=4»4 times per year . Therefore   John can harvest 1000 x 4 = « 1000 * 4=4000»4000   pineapples per year . # # # # 4000   Socratic sub - questions Problem : John has 10   hectares of a pineapple field . There are 100 pineap-   ples per hectare . John can harvest his pineapples   every 3 months .   Q : How many pineapples does John have ? A : John   has 100 x 10= « 100 * 10=1000»1000 pineapples on   his field .   Q : How many times can John harvest his pineap-   ples ? A : John can harvest his Pineapple 12 / 3 =   « 12/3=4»4 times per year .   Q : How many pineapples can John harvest within   a year ? A : Therefore John can harvest 1000 x 4   = « 1000 * 4=4000»4000 pineapples per year . # # # #   4000   C Error analysis of generated questions   Table 9 studies some of the errors encountered by   us in our question generation strategy.4148Id Text Generated questions   1 James decides to run 3 sprints 3 times a week .   He runs 60 meters each sprint . How many total   meters does he run a week ? ( 540)How many sprints does James run in a week ?   How many meters does James run in a week ?   2 Carla is downloading a 200 GB file . Normally   she can download 2 GB / minute , but 40 % of the   way through the download , Windows forces a   restart to install updates , which takes 20 min-   utes . Then Carla has to restart the download   from the beginning . How long does it take to   download the file ? ( 160)Missing : time to download 40 % of the file   How many minutes does it take to install up-   dates ?   How many minutes does it take to restart the   download ?   How many minutes does it take to download   the file ?   3 John buys twice as many red ties as blue ties .   The red ties cost 50 % more than blue ties . He   spent $ 200 on blue ties that cost $ 40 each . How   much did he spend on ties ? ( 800)How many blue ties did John buy ?   How many red ties did John buy ?   How much did the red ties cost ?   How much did John spend on ties ?   4 Mr. Tan sold his house for $ 400 000 . He paid   the transfer fees that amount to 3 % of the selling   price and also paid a brokerage fee that is 5 %   of the selling price . If he also paid $ 250 000 for   the remaining loan amount of the house , how   much is Mr. Tan ’s net proceeds from selling the   house ? ( 118000)How much did Mr. Tan pay in transfer fees and   brokerage fees ?   How much did Mr. Tan pay in total ?   How much is Mr. Tan ’s net proceeds from sell-   ing the house ?   5 John drives for 3 hours at a speed of 60 mph   and then turns around because he realizes he   forgot something very important at home . He   tries to get home in 4 hours but spends the first   2 hours in standstill traffic . He spends the next   half - hour driving at a speed of 30 mph , before   being able to drive the remaining time of the   4 hours going at 80 mph . How far is he from   home at the end of those 4 hours ? ( 45)How far did John drive in the first 3 hours ?   How far did John drive in the remaining 2   hours ?   How fast did John drive in the next half - hour ?   How fast did John drive in the remaining 2   hours ? Error : remaining 1 and half hour .   How far did John drive in those 4 hours ?   How far is John from home at the end of those   4 hours ?   6 Charlie wants to sell beeswax candles . For ev-   ery pound of beeswax , he can make 10 tapered   candles . One pound of beeswax and the wicks   cost $ 10.00 in supplies . If he sells each can-   dle for $ 2.00 each , what is his net profit if he   makes and sells 20 candles ? ( 20)How many pounds of beeswax does Charlie   need ?   How much will each candle cost ?   How much will Charlie sell the candles for ?   Missing : selling price for 20 candles .   What is Charlie ’s net profit ?   7 Shiela bought five cell phones for $ 150 each   for a 3 - month installment . A 2 % interest will   be charged for each unit . How much in total   will Shiela pay each month for the period of 3   months ? ( 255)How much is the interest ?   How much will Shiela pay in total ?   Missing : question about the price of 5 cell   phones .   How much will Shiela pay each month for 3   months ?   8 Melanie is a door - to - door saleswoman . She   sold a third of her vacuum cleaners at the green   house , 2 more to the red house , and half of what   was left at the orange house . If Melanie has 5   vacuum cleaners left , how many did she start   with ? ( 18)Wrong order : 1 ) before visiting orange   house ? , 2 ) before visiting red house ? , 3)be-   fore visiting green house ?   How many vacuum cleaners did Melanie sell at   the green house ?   How many vacuum cleaners did Melanie sell at   the red house ?   How many vacuum cleaners did Melanie have   left after selling to the red house ?   How many vacuum cleaners did Melanie start   with?4149