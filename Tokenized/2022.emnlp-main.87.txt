  Haoxing Zhang Xiaofeng Zhang Haibo Huang Lei Yu   Sino - French Engineer School , Beihang University , Beijing , China   { mickael97 , xiaofeng_z , huanghaibo , yulei}@buaa.edu.cn   Abstract   Few - shot Text Classification predicts the se-   mantic label of a given text with a handful of   supporting instances . Current meta - learning   methods have achieved satisfying results in   various few - shot situations . Still , they often   require a large amount of data to construct   many few - shot tasks for meta - training , which   is not practical in real - world few - shot scenar-   ios . Prompt - tuning has recently proved to be   another effective few - shot learner by bridg-   ing the gap between pre - train and downstream   tasks . In this work , we closely combine the   two promising few - shot learning methodolo-   gies in structure and propose a Prompt- Based   Meta - Learning ( PBML ) model to overcome   the above meta - learning problem by adding   the prompting mechanism . PBML assigns la-   bel word learning to base - learners and tem-   plate learning to meta - learner , respectively . Ex-   perimental results show state - of - the - art perfor-   mance on four text classification datasets un-   der few - shot settings , with higher accuracy and   good robustness . We demonstrate through low-   resource experiments that our method alleviates   the shortcoming that meta - learning requires too   much data for meta - training . In the end , we use   the visualization to interpret and verify that the   meta - learning framework can help the prompt-   ing method converge better . We release our   code to reproduce our experiments .   1 Introduction   Humans can quickly learn new knowledge from a   few examples , reflecting a high degree of intelli-   gence . The meta - learning method was proposed   to implement such human - like intelligence and   achieved promising results in various few - shot sit-   uations . It lifts the training unit from data point   to task and trains a meta - learner over many tasks   ( called episodes ) to grasp the few - shot learningFigure 1 : An example of prompting . We concatenate   the original text with a template . Then a pre - trained lan-   guage model ( PLM ) will fill in [ MASK ] with words from   a predefined label word set . A verbalizer will further   map the predicted label word into the corresponding   task label .   ( FSL ) pattern . In this paper , we tackle the few-   shot text classification , which aims to predict a   given text ’s label with only a handful of support   instances .   Current methods to solve the few - shot text classi-   fication problem are based on meta - learning ( Finn   et al . , 2017 ; Dong et al . , 2020 ; Bao et al . , 2020 ;   Geng et al . , 2020 ) or prompt - tuning ( Brown et al . ,   2020 ; Gao et al . , 2021 ; Zhao et al . , 2021 ; Liu et al . ,   2021 ; Schick and Schütze , 2021 ) . These two ap-   proaches deal with " few - shot " from different per-   spectives . On the one hand , the meta - learning al-   gorithm aims to train a meta - learner across many   episodes to extract transferable meta - knowledge .   For each episode , the meta - learner further trains   an individual base - learner for task - specific adap-   tation . While existing meta - learning algorithms   allow models to rapidly learn new concepts based   on previous experience , they often require a large   amount of meta - training data to construct meta-   tasks . This bottleneck makes meta - learning meth-   ods less practical in real - world few - shot scenarios1342where diverse and sufficient meta - tasks may not   be available . Prompt - tuning , on the other hand ,   has recently emerged as another effective few - shot   learning methodology . It transforms the original   problem into a cloze - test to bridge the gap between   pre - train and downstream tasks ( Figure 1 ) . By do-   ing so , prompt - tuning is more likely to stimulate the   knowledge from the pre - training stage and can effi-   ciently adapt the masked language model ( MLM )   to downstream tasks with fewer training data .   Given that the above two approaches both   achieved satisfying results in various few - shot sit-   uations , we hope to alleviate the shortcomings   of meta - learning methods by adding a prompting   mechanism . In this work , we use an elegant way   to closely combine these two approaches in struc-   ture and propose Prompt - Based Meta - Learning   ( PBML ) to push the performance on few - shot   text classification . In terms of prompts , we adopt   the " soft " strategy ( i.e. , use continuously differ-   entiable label words and templates ) . Regarding   meta - learning , our meta - learner mainly learns soft   template embeddings and an MLM - based encoder .   The core idea of our combination of the two ap-   proaches is to assign template&encoder learning to   the meta - learner and label word learning to base-   learners , respectively . The intuition is that different   tasks may involve different classes , and label words   need to consider specific classes , so the learning   of label words is handled over to base - learners for   task - specific adaptation . Correspondingly , the out-   put embedding at [ MASK ] position by the prompt-   ing method reflects the model ’s understanding of   the text . Various tasks should share this ability of   natural language understanding ( NLU ) , which is   why the learning of template and encoder is as-   signed to the meta - learner .   Experimental results demonstrate the good   compatibility between meta - learning and prompt-   tuning . Our PBML ( 1 ) achieved a new state - of - the-   art performance on four datasets under few - shot   settings , with higher accuracy and good robust-   ness . ( 2 ) Through low - resource experiments , we   can clearly see that the prompting mechanism helps   PBML maintain good performance when the avail-   able meta - training data is significantly reduced . ( 3 )   Comparison with prompt - tuning baseline demon-   strates that meta - learning enables prompt - tuning to   adapt more efficiently to new tasks . ( 4 ) The visu-   alization results verify that the “ learning - to - learn ”   framework could result in better convergence forprompts .   2 Preliminaries   2.1 Few - Shot Text Classification   Given a text x , we aim to predict its label ywith   a few annotated examples ( x , y ) . We follow the   commonly adopted N - way K - shot setting , where   Nis the number of classes and Kis the number of   examples per class . Each task provides a support   setScontaining N×Ksupport instances and a   query set Q. We train a classifier on Sand evaluate   it onQ. Higher Nand lower Kindicate a more   challenging task . An example of a 5 - way 3 - shot   scenario is given in Appendix A.   2.2 Meta - learning   The purpose of meta - learning is to train a meta-   learner through diverse meta - tasks such that the   meta - learner can quickly obtain a task - specific   base - learner on a small support set .   Formally , we consider two phases : meta - training   and meta - testing . During meta - training , we sample   a batch of tasks { T}from a task distribution   p(T ) . Then the meta - learner trains a base - learner   for each task Tusing the loss on the support set S.   The base - learner is then tested on the query set Q ,   and we optimize the meta - learner by minimizing   the query loss . During meta - testing , new tasks are   sampled , and the accuracy of query instances will   be measured . The two phases share no overlapping   classes , so we can estimate the model ’s ability to   handle new classes .   2.3 Prompt   Prompt aims to bridge the gap between the pre - train   and downstream tasks . It concatenates the origi-   nal text xwith a template containing at least one   [ MASK ] token , converting the problem into a cloze-   test . We note the concatenated text x . A pre-   trained language model Mis applied to predict the   label word at [ MASK ] position . Then , a function   called verbalizer ϕ:w∈ W ∝ ⇕ ⊣√∫⊔≀→ y∈Ymaps the   set of label words Wto the set of task labels Y.   For instance , we can set ϕ("technology " ) = TECH .   We formulate the probability of instance xpredict-   ing class yas follows :   P(y|x ) = P([MASK ] = w|x )   = exp ( w·h)/summationtextexp ( w·h)(1)1343   where ϕ(w ) = y , wis the label word embedding   ofwandhis the output hidden state of [ MASK ] .   3 Methodology   This section presents the details of our proposed   framework , PBML ( Figure 2 ) . PBML consists of   3 parts : Firstly , the meta - learner ( Meta - Encoder )   encodes instances and gets the embedding of the   [ MASK ] token for each instance . Secondly , we ex-   plore an external knowledge graph for continuous   label word initialization . Then a base - learner will   update the label word embeddings using predicted   embeddings of support instances . Inference for   queries is based on the adapted label word embed-   dings , and we use the loss on the query set Qfor   meta - optimization .   3.1 Meta - encoder and Template Design   Given a statement x , we first concatenate a tem-   plate to xand obtain x . If we take the topic   classification task as an example , the prompted text   can be given as :   x = xThe topic is [ MASK ] .   where “ The topic is [ MASK ] ” are template tokens   ( details of template designs are in Appendix C ) .   Then a masked language model , noted M , serving   as the meta - encoder , takes x as input and out - puts h , the hidden state of [ MASK ] as the predicted   answer representation .   Though we manually select template tokens   from vocabulary items , we treat them “ softly ” ( i.e. ,   continuous template ) . They are replaced by learn-   able embeddings , initialized with the exact embed-   dings as manually selected real words . This soft   strategy allows templates to be optimized continu-   ously instead of being limited by discrete tokens .   We note the encoder parameters as θand soft tem-   plate embeddings as θ . The meta - encoder can be   formulated as follows :   h = M(x ; θ , θ ) ( 2 )   3.2 Label word Initialization   Given an N - way K - shot episode , we denote   { C , C , ... , C}theNcategories . Though it is   intuitive to directly apply class names as label   words , the semantic meaning of class names is   sometimes too conceptional without sufficient se-   mantic information . Following the idea of Knowl-   edgeable Prompt ( Hu et al . , 2022 ) , we introduce   Related Words , an external knowledge graph to   expand rich label words for each class Cfrom their   class name . Specifically , we explore the knowl-   edge graph to obtain the top N= 20 relevant1344words with the class name as candidate words . The   obtained candidate word set { w}includes   synonyms and words highly related to the class   name . For example , the candidate words associ-   ated with “ Politics ” are “ policy ” , “ government ” ,   “ law ” , “ diplomatic ” , etc .   Then we merge the candidate words into one   prototype for each class by averaging candidate   word embeddings { w } . In the end , we obtain   Nsynthesized continuous label word embeddings ,   and we note W∈R , the matrix containing   the initial Nlabel word embeddings . Wrepre-   sents roughly the semantic meaning of Nclasses   and will be further tuned in the next module .   3.3 Label word Fast - tuning   In this section , we introduce the fast - tuning mod-   ule for label word adaptation . A base - learner will   continuously tune the initialized label word em-   beddings W , using support instances . Our ob-   jective is to make these label word embeddings   more discriminative by incorporating contextual   information from the support set .   Specifically , we impose two goals we need to   achieve by fast - tuning . ( a ) For each support in-   stance sfrom class C , we note hits hidden state   at the [ MASK ] position . We expect that the similar-   ity between handw(label word embedding of C )   is larger than the similarity between hand other   label words ; ( b ) For each class C , the similarity   between its label word wand the support instances   belonging to Cshould be larger than the similarity   between wand instances from other classes . To   realize these two goals , we define the following   two contrastive losses :   L=−1   NK / summationdisplayαlogexp / parenleftig   h·w / parenrightig   /summationtextexp / parenleftig   h·w / parenrightig(3 )   L=−1   N / summationdisplaylogexp / parenleftig   w·h / parenrightig   /summationtextexp / parenleftig   w·h / parenrightig(4 )   h=/summationdisplayαh   where W∈Ris the label word matrix at   iteration t.wis the irow of W , representing   the label word embedding of class C. To make the   adaptation more robust , we add an instance - levelattention mechanism via coefficient α . Such at-   tention score is to measure the informative degree   of each support instance . According to previous   works ( Gao et al . , 2019a ; Dong et al . , 2020 ) , in-   stances are not equally informative , and we should   make those informative ( resp . noisy instances ) con-   tribute more ( resp . less ) during fast - tuning to im-   prove the robustness . We calculate αas shown   below :   α = exp ( h·w / γ )   /summationtextexp ( h·w / γ)(5 )   where γis a temperature hyper - parameter set to   3 . Equation 5 indicates that if his more similar   to the initial label word embedding , it is consid-   ered more informative and assigned with higher   attention α . In contrast , we assign noisy instances   with little attention in LandL , producing   smaller gradient steps and more robust adaptation   trajectories .   At each iteration of fast - tuning , we apply gradi-   ent descendent as follows :   W = W−β∇L(S , W)(6 )   where βis base - learner ’s task - adaptation learn-   ing rate and L = L+L. The fast - tuning   process will iterate Tsteps and output W.   We clarify here that we only update label words   Wduring fast - tuning , which corresponds to the   task - specific adaptation of the base - learner . The   meta - learner M(θ , θ)is unchanged during fast-   tuning as it contains task - agnostic parameters and   learns across tasks . Since we only update W ,   the fast - tuning process saves lots of computational   costs , with O(N×K×D×T)time complexity .   ( More details in Appendix G )   3.4 Inference for query   We calculate the inner product between the query   embedding hand task - adapted label word embed-   dings Wto predict query instances ’ labels . The   probability score for class Cis   P(C|q ) = exp ( h·w)/summationtextexp ( h·w)(7 )   Then we use the argmax function for the prediction .   /tildewideC = argmaxP(C|q ) ( 8)13453.5 Meta - Optimization   During meta - training , we randomly construct many   few - shot meta - tasks from the meta - training set .   The base - learner of each episode learns task-   specific label word embeddings Was shown   in subsection 3.3 . Then , to meta - learn the template   and encoder from tasks , we update our meta - learner   M(θ , θ)considering the loss on the query set Q.   Hence , the optimization rule of the meta - learner   can be formulated as follows :   θ = θ−β∇L(Q , W , θ ) ( 9 )   where β is the meta - learning rate for meta-   parameters θ= ( θ , θ)andLis the cross - entropy   loss on P(C|q ) . The overall algorithm of PBML   is summarized in Appendix D. In this way , we   combine prompt - tuning with meta - learning such   that both task - specific and task - agnostic knowledge   can be learned . Specifically , 1 ) the meta - learner   learns at a lower speed β to reach appropriate   parameters for soft template embeddings and the   encoder layers 2 ) base - learners learn continuous   label words at a higher speed βfor fast adapta-   tion .   4 Experiments   This section evaluates the performance of our pro-   posed PBML . We conduct extensive experiments   on four widely - used text classification datasets un-   der few - shot settings and make a full - scale com-   parison with existing state - of - the - art baselines . We   report our implementation details in Appendix E.   4.1 Dataset   Following Bao et al . ( 2020 ) , we adopt the follow-   ing four text classification datasets for experiments :   FewRel ( Han et al . , 2018 ) ( relation classification ) ,   HuffPost headlines ( Misra , 2018 ) ( news headlines   classification ) , Reuters ( Lewis , 1997 ) ( articles clas-   sification ) and Amazon product data ( He and   McAuley , 2016 ) ( reviews classification ) . These   datasets provide diverse benchmarks that vary in   domain and text length . We use the same setting as   previous work for splitting training , validation , and   testing sets . More details are in Appendix A.   4.2 Baseline Models   We compare our model with previous strong base-   lines for few - shot text classification . BERT - PAIR(Gao et al . , 2019b ) pairs query and support in-   stance and apply BERT classification head to ob-   tain the similarity score . MTB ( Soares et al . , 2019 )   proposes a pre - training task for Relation Extrac-   tion . The pre - trained relation extractor is further   fine - tuned on FewRel . Frog - GNN ( Xu and Xi-   ang , 2021 ) is a graph neural network learning   query embeddings after the multi - aggregation of   neighbor nodes . We also compare various meta-   learning models . 1 - NN andPrototypical Network   ( Snell et al . , 2017 ) are metric - based meta - learning .   The latter averages support instance embeddings   as prototypes and predict the query label based   on the distance with prototypes . ( PROTO - IDF ,   PROTO - BERT stands for two encoders ) . DS   ( Bao et al . , 2020 ) , MAML ( Finn et al . , 2017 ) and   MIML ( Dong et al . , 2020 ) are optimization - based   meta - learning methods , showing excellent few - shot   learning performance . They apply different meta-   learners and train them across episodes .   We also implement the baseline Prompt - tuning   without meta - learning to assess the impact of meta-   learning in our PBML . Prompt - tuning no longer   constructs SandQto meta - train through episodes   but directly uses label words ( from knowledge   graph ) to pre - train the encoder and soft template .   To test the model ’s ability to solve new N - way K-   shot tasks ( unseen classes ) , we further fine - tune it   on the small support set before query inference .   4.3 Results and Analysis   4.3.1 Main Results   Table 1 shows the results of different models on   four benchmark test sets under 5 - way 1 - shot and   5 - way 5 - shot settings . PBML significantly and   consistently outperforms previous models , espe-   cially under the more challenging one - shot con-   figuration . ( Similar observations under 10 - way   k - shot settings in Appendix H ) . Notably , on the   relation classification task , our method exceeds the   accuracy of human performance reported from the   FewRel leader - board . It also outperforms the state-   of - the - art method MTB , which utilizes larger PLM   and requires external corpus for relation extraction   pre - training .   Meanwhile , we have the following observa-   tions : Our model outperforms previous meta-   learning baselines ( PROTO - BERT , MAML , and   MIML ) by a large margin . We also observed   that PBML converges faster and smoother ( Ap-1346   pendix F ) . We infer that the improvements are   mainly from prompting , which effectively bridges   the gap between pre - train and downstream tasks .   Then , compared to the Prompt - tuning baseline ,   the improvements demonstrate that meta - learning   can help prompting adapt to new tasks more effi-   ciently . These two comparisons show the excellent   compatibility between prompt - tuning and meta-   learning . In subsubsection 4.3.4 , we will further   explain and visualize why such a combination is   good .   4.3.2 Ablation Study   At the bottom of Table 1 , we present the results of   ablations on the external knowledge graph and the   two contrastive losses . We observe that the perfor-   mance drops consistently if we directly use class   names to initialize Wwithout introducing exter-   nal KG . This suggests that our knowledgeable label   word initialization does provide rich and essential   information . The ablation study on loss functions   shows that both LandLcontribute to fast-   tuning , though the former has a more significant   impact .   We also conducted the ablation study on the   instance - level attention αdesigned for robustness   ( i.e. , allowing informative instances to contribute   more to the loss ) . To evaluate the robustness , we   follow the idea of Dong et al . ( 2020 ) by intention-   ally replacing a portion of support instances with   randomly sampled instances from different cate-   gories . Table 2 displays the accuracy and the per-   formance drop scale under various noised settings .   The results show that our attentional loss does help   to improve the robustness . More obvious advan-   tages can be seen in a noisier setting .   4.3.3 Dependence on Training Data   In this section , we investigate our method ’s perfor-   mance when lacking sufficient meta - training data .   As mentioned previously , existing meta - learning   methods often require a sufficiently large dataset   to build diverse episodes for meta - training . Other-   wise , the performance will drop seriously .   To investigate the effect of the amount of meta-   training data , we follow previous works ( Soares   et al . , 2019 ; Ding et al . , 2021b ) and limit the size of   the meta - training set in two ways : 1 ) by decreasing   the number of instances per class ; 2 ) by decreasing   the number of available classes . We evaluate the   5 - way 1 - shot accuracy under various low - resource   configurations on FewRel . Figure 3 and Figure 4   show respectively the results under the two size con-1347   straints . Prompt - tuning and our proposed PBML   suffered little from low resource data under both   constraints . For example , when we limit the meta-   training data per class to 5 , PBML could still main-   tain reasonable accuracy while the performance of   other meta - learning methods ( i.e. , MAML , MIML ,   PROTO - BERT ) drops significantly . We infer that   such improvement is also because prompting can   narrow the gap between pre - train and downstream   tasks , thus making meta - learning less dependent   on meta - training data . The results demonstrate that   our PBML framework can not only learn from a   few supporting instances during meta - testing ( bene-   fits of meta - learning ) but also demand less training   data during meta - training ( benefits of prompting ) ,   showing a comprehensive few - shot learning ability .   4.3.4 Visualization   Previous sections show that prompting makes meta-   learning better few - shot learners . In this section ,   we emphasize the benefits of the meta - learning   side . We will visualize and analyze how " learning   to learn " contributes to prompting .   We made a PCA visualization of different en-   coders ’ output representation at the [ MASK ] posi-   tion to demonstrate that meta - learning can help   the encoder converge . We have selected five cate - gories from FewRel that are difficult to distinguish   ( according to Brody et al . ( 2021 ) ) . As shown in   Figure 5 , the encoder with open - source pre - trained   parameters ( a)presents poor separation . Classical   prompt - tuning ( b , c)directly uses label words to   tune the encoder and can improve the performance .   In comparison , the encoder meta - trained by the   PBML algorithm ( d)produces the best separation .   The results suggest that meta - learning can help the   prompting method learn a better encoder and soft   template , showing a stronger ability to distinguish   different concepts .   Classical prompt - tuning tunes the encoder using   cross - entropy loss on distribution shown in Equa-   tion 1 . Since label word embeddings ( real word   embeddings or learnable embeddings ) contribute   directly to the loss , the convergence of the encoder   and soft template highly depends on the quality of   the label word embeddings . By comparison , our   PBML does not directly use label words to tune   the encoder . Instead , our method has two loops :   In the outer loop , we randomly sample meta - tasks ,   and during the inner loop of each meta - task , the   initialized label words Ware fast - tuned on the   support set within Titerations . At each iteration ,   the contrastive loss enhances the discrimination of   different classes . By considering support instances ,   the base - learner can incorporate contextual infor-   mation into label word embeddings and produces   Wof richer semantic information . When the in-   ner loop is finished , our encoder and soft template   will be optimized using Equation 9 where the more   ideal label word embeddings Wact . Once the   meta - learner is optimized , it can produce better sup-   port instance embeddings and reinforce the qual-   ity of W. In this learning - to - learn framework   with two loops , the meta - learner ( soft template and   encoder ) and the base - learner ( label words ) can   promote each other and enhance their coordina-   tion , resulting in better performance and a smooth   convergence .   5 Related Work   5.1 Meta - Learning   Current promising meta - learning methods are   mainly metric - based or optimization - based .   Metric - based meta - learning learns a metric   space where representations of instances from the   same class can get closer . Han et al . ( 2018 ) applied   popular metric - based few - shot - learning methods1348   ( Koch et al . , 2015 ; Vinyals et al . , 2016 ; Snell et al . ,   2017 ) to few - shot relation classification but the re-   sults suggest that there is still an ample space to   improve . Gao et al . ( 2019a ) introduced a hybrid   attention mechanism into Prototypical Network ,   which can enhance the model ’s robustness . Ye   and Ling ( 2019 ) proposed a Multi - Level Matching   and Aggregation Network to learn representations   through multiple rounds of interaction . Geng et al .   ( 2019 ) proposed Induction Network , applying the   dynamic routing algorithm to build class - aware   representation . Xiao et al . ( 2021 ) integrated label   information into features , providing vital guidance   for the prototypical network . Soares et al . ( 2019 )   and Ding et al . ( 2021b ) proposed novel ways to   pre - train a metric space for relation extraction .   Optimization - based meta - learning aims to im-   prove the optimization procedure so that the base-   learner can learn from a few examples without   dramatic over - fitting . Strong frameworks such   as MAML ( Finn et al . , 2017 ) meta - learns task-   sensitive parameters , which serve as a good initial-   ization . Inspired by their work , Dong et al . ( 2020 )   proposed meta - information guided meta - learning   ( MIML ) , where they introduce information from   class names to realize a class - aware initialization .   Ravi and Larochelle ( 2017 ) studied optimization   rules and proposed an LSTM - based meta - optimizer .   Li et al . ( 2017 ) proposed Meta - SGD , making the   learning rate a meta - parameter to be learned . Bao   et al . ( 2020 ) proposed to meta - learn an attention   generator across all episodes while training an in-   dividual ridge regressor for each episode.5.2 Prompt - tuning   The gap between pre - train and downstream tasks   often causes difficulty in fine - tuning of PLMs like   GPT ( Brown et al . , 2020 ) , BERT ( Devlin et al . ,   2019 ) , RoBERTa ( Liu et al . , 2019 ) , etc . Inspired by   GPT-3 , which presents the strong potential of FSL   by prompting , numerous work has applied prompt-   tuning for Relation Classification ( Han et al . , 2021 ;   Chen et al . , 2021 ) , Named Entity Recognition ( Ma   et al . , 2021 ; Ding et al . , 2021a ) , Topic Classifi-   cation ( Cui et al . , 2022 ; Zhao et al . , 2021 ) , etc .   As for the choice of template , some works ap-   plied continuous template ( Li and Liang , 2021 ;   Zhang et al . , 2021 ; Liu et al . , 2021 ) and some other   works employed generative model like T5 ( Raffel   et al . , 2020 ) to generate templates ( Gao et al . , 2021 ) .   Concerning the choice of label words , Chen et al .   ( 2021 ) used learnable soft label words . Cui et al .   ( 2022 ) applied contrastive loss to learn prototypi-   cal label words . Hu et al . ( 2022 ) explored external   knowledge graphs to enrich semantic information .   6 Conclusion   In conclusion , we propose prompt - based meta-   learning for few - shot text classification . Our   method combines prompt - tuning with meta-   learning closely , where the base learners tune   task - specific label words ( learn ) and the meta-   learner tunes the task - agnostic template and en-   coder ( learning - to - learn ) . Experimental results   demonstrate the excellent compatibility of meta-   learning and prompt - tuning , with state - of - the - art   performance in various few - shot text classification   tasks and good robustness under noisy settings .   Additionally , We show that the prompting mech-1349anism helps PBML maintain good performance   with significantly reduced meta - training data . The   visualization results further demonstrate that meta-   learning enables prompt - tuning to distinguish se-   mantics better .   Limitations   We summarize the limitations of our method as   follows : 1 ) We employ a simple average pooling to   process the candidate label words for label word ini-   tialization . However , candidate words sometimes   contain noise , and average pooling is vulnerable   to extreme outliers , which may reduce essential in-   formation . In our experiment , we consider a fixed   selection of 20 candidate words , but this choice is   not always optimal because , for some categories ,   the obtained candidate words are less relevant to the   class , thus presenting more noise . Even though the   simple average pooling achieves good results , we   think an attention generator is needed to calculate   the contribution of different words dynamically ;   2 ) Although our model outperforms previous base-   lines in various few - shot settings . The improve-   ment of our method from 1 - shot to 5 - shot setting is   not particularly large ( about 3 percent while proto-   typical network can improve more than 10 percent ) .   Therefore , we believe the model still has enough   room for improvement in multi - shot scenarios .   References135013511352A Dataset Description   FewRel is a dataset for few - shot relation classifi-   cation , containing 100 relations ( Han et al . , 2018 ) .   Each statement has an entity pair and is annotated   with the corresponding relation . The position of   the entity pair is given , and the goal is to predict   the correct relation based on the context . The 100   relations are split into 64 , 16 , and 20 for training ,   validation , and test , respectively . We apply the   same pre - process method as Soares et al . ( 2019 ) by   adding special entity markers to highlight the po-   sition of the entity pair . Besides , FewRel provides   each relationship with additional rich information   ( i.e. , alias , descriptions , etc . ) , which we used for   our label word initialization .   HuffPost headlines is a dataset for topic classi-   fication . It contains news headlines published on   HuffPost between 2012 and 2018 ( Misra , 2018 ) .   The 41 topics are split into 20 , 5 , 16 for training ,   validation and test respectively . These headlines   are shorter and more colloquial texts .   Reuters-2157 is a dataset of Reuters articles over   31 classes ( Lewis , 1997 ) , which are split into 15 ,   5 , 11 for training , validation and test respectively .   These articles are longer and more grammatical   texts .   Amazon product data contains customer re-   views from 24 product categories ( He and   McAuley , 2016 ) . Our goal is to predict the product   category based on the content of the review . The 24   classes are split into 10 , 5 , 9 for training , validation   and test respectively .   We summarize the details of the four datasets in   Table 4 . In this work , we focus on few - shot text   classification under N - wayK - shot setting . Table 3   is an example of a 5 - way 3 - shot topic classification .   B Baseline details   1 - NN is a 1 - nearest neighbor classifier under Eu-   clidean distance . The encoder is implemented with   IDF , which represents each statement as a weighted   average of word embeddings from pre - trained fast-   Text ( Joulin et al . , 2016 ) . The weights are calcu-   lated based on inverse document frequency on the   training set .   DS ( Bao et al . , 2020 ) is a meta - learning algo-   rithm aiming to meta - train an attention generator   based on distribution signatures ( DS ) . DSconsiders   two signatures : general word importance and class-   specific word importance . The attention genera-   tor then uses these signatures to generate attention   scores , serving as each word ’s weight for sentence   representation .   MAML ( Finn et al . , 2017 ) is a classic meta-   learning framework aiming to learn initialization   for few - shot tasks . Dong et al . ( 2020 ) implement   MAML for few - shot text classification , which   meta - learns initialization parameters of a general   multilayer perceptron and a BERT - based meta-   encoder .   MIML ( Dong et al . , 2020 ) proposed a novel   meta - learning framework , which uses class - aware   semantic information from GloVe ( Pennington   et al . , 2014 ) to provide strong guidance for meta-   learning . MIML incorporates class names as meta-   information to realize a class - aware initialization .   The meta - initializer module is implemented via a   fully connected layer .   Prototypical network ( Snell et al . , 2017 ) aver-   ages the embedding of support instances as class   prototypes and makes a metric - based prediction   for query instances . ( Bao et al . , 2020 ) use IDF   to implement the instance encoder PROTO - IDF .   We add the performance of PROTO - BERT which   uses BERT as the encoder backbone.1353C Template design   FewRel   x = x[SEP ] his[MASK ] oft   HuffPost   x = x[SEP ] Topic is about [ MASK ]   Reuters   x = Article is about [ MASK ] [ SEP ] x   Amazon   x = Review is about [ MASK ] [ SEP ] x   As mentioned in subsection 3.1 , we adopt a soft   template strategy . Specifically , we use learnable   vectors to replace discrete template tokens and use   their word embeddings for vector initialization . We   show the comparison comparison between the dis-   crete template and the soft template in Figure 6 and   Figure 7.Our soft approach first maps discrete token - ids   from the original text into word embeddings , then   we concatenate the learnable vectors directly to   these word embeddings . We note that we freeze the   word embedding layer of Mand only update soft   template embeddings and parameters from encoder   layers .   During our experiments , we found that if we up-   date the word embedding layer of the meta - learner ,   the latter is very likely to overfit the label words   of training episodes , showing lower meta - train loss   and worse meta - test accuracy . We infer that this   is because the number of categories in the dataset   is far less than the number of samples . Since the   label word embedding appears close to the meta-   loss , the word embedding layer could easily overfit   meta - training labels . The phenomenon suggests   that our meta - learner will tend to memorize the   matching relation between label words and training   samples from the meta - training set and fails to un-   derstand the real meaning of the sentence , ending   up with a poor performance on novel tasks . To   avoid this , we decided to freeze the word embed-   ding layer . The initialization of label word embed-   dings is always copied from the exact BERT - pre-   trained embeddings ( In subsection 3.2 , we call this   a rough estimation of the class ) , then we let base-   learners learn better label representations , allowing   the meta - learner to only focus on the learning of   encoder&soft - template for NLU ability .   D Algorithm of PBML   We present here the overall process of PBML in   Algorithm 1 .   E Implementation Details   We select BERT(110 M ) ( Devlin et al . , 2019 ) as   our meta - encoder and choose AdamW ( Loshchilov   and Hutter , 2019 ) for optimizing . Meanwhile , the   warmup mechanism is used during meta - training .   We implement PBML with PyTorch ( Paszke et al . ,   2019 ) . All the experiments run on one NVIDIA   RTX 3090 .We report the hyper - parameters in Ta-   ble 5 . The model was meta - trained with 500 - 2000   iterations , depending on the dataset and the approx-   imate per - iteration cost is 6 - 8 seconds . During   meta - testing , the per - episode cost for 1 - shot and   5 - shot scenarios is approximately 180 and 540 mil-   liseconds .   The choice of fast - tuning iteration steps Tand   task learning rate βgreatly impacts the perfor-1354Dataset text length ( avg . ) examples / cls train cls val cls test cls   FewRel 24 700 64 16 20   HuffPost 11 900 20 5 16   Reuters 168 20 15 5 11   Amazon 140 1000 10 5 9   Algorithm 1 Prompt - based Meta - learning   Require : p(T ): distribution over mete - training   tasks   T : meta - testing tasks   β , β : learning rates   T : fast - tuning stepsInitialize meta - encoder θ , soft template em-   beddings θwhile not done do Sample batch of tasks T∼p(T ) for all Tdo Initialize label word embeddings W fort= 0 , ... , T−1do Evaluate L(W , S , θ , θ ) Update Wusing Equation 6 end for Evaluate L(W , Q , θ , θ ) end for Update ( θ , θ ) using Equation 9end while   mance . We choose these two hyper - parameters as   follows : First we set ( T , β)to ( 10 , 5e-2 ) . Af-   ter meta - training , we modify the hyper - parameters   pair(T , β)to search for more suitable values   during meta - testing . Once we find better hyper-   parameters ( T , β ) , we perform meta - training   again with ( T , β ) .   We also report in Table 6 , the corresponding   validation performance for our model ’s test results .   The FewRel testing set is not publicly available , so   we visit their benchmark websiteto get our test   performance .   F Convergence speed   We plot the loss on FewRel training set and the   5 - way 1 - shot accuracy on FewRel validation set as   functions of the number of iterations . As we can   see from Figure 8 , PBML training loss descends   very fast while the validation accuracy grows to   93 % within only hundreds of steps . In contrast ,   the convergence of MIML ( state - of - the - art meta-   learning baseline ) is much slower , and the perfor-   mance upper bound is also limited .   G Fast - tuning deployment   During meta - testing , PBML uses support instances   to iteratively fast - tune label word embeddings ,   which corresponds to the task - adaptation process   of the meta - learning algorithm . This means that   our method still requires some " training " during the   testing phase . However , executing any loops during   inference is not recommended in real - world neural   network deployment . Fortunately , our fast - tuning   process only updates Nlabel word embeddings   while leaving the encoder unchanged . Therefore ,   we suggest manually computing the gradients of   the proposed contrastive loss ( Equation 3 and Equa-   tion 4 ) and using Tidentical layers to replace the1355Evaluation setFewRel HuffPost Reuters Amazon   1 shot 5 shot 1 shot 5 shot 1 shot 5 shot 1 shot 5 shot   Validation 94.1 95.2 67.4 76.3 90.1 93.6 75.2 81.7   Test 96.6 97.4 74.9 78.0 96.4 97.6 81.8 88.0   For Loop of Tgradient steps .   ∇L(S , W ) ( 10 )   = 1   |S|/summationdisplayα   exp / parenleftig   h·w / parenrightig   /summationtextexp / parenleftig   h·w / parenrightig−δ(i , i)   h   ∇L(S , W ) ( 11 )   = 1   N / summationdisplay   exp / parenleftig   w·h / parenrightig / summationtextexp / parenleftig   w·h / parenrightig−δ(i , i)   h   h=/summationdisplayαh   The similarity computation of all ( h , w)pairs is of   O(N×K×D)time complexity . Then we use   the similarity table to compute gradients ∇for   each class iand the overall computation cost is also   ofO(N×K×D ) .   We can define the gradient descent layer Las   follows :   W∝ ⇕ ⊣√∫⊔≀→W−β∇L(W , h)(12 )   The initialized label word embeddings Wwill   pass through Tidentical Llayers consecutively   to obtain W   H Discussion on the N - way effect   We also discuss the effects on the number of cate-   gories . Table 7 compares different models under   10 - way 1 - shot and 10 - way 5 - shot settings . We   draw similar conclusions from this table as in Ta-   ble 1 that our method still outperforms previous   state - of - the - art baselines.13561357