  Xuandong Zhao Lei Li Yu - Xiang Wang   University of California , Santa Barbara   { xuandongzhao,leili,yuxiangw}@cs.ucsb.edu   Abstract   Large language models are shown to memo-   rize privacy information such as social secu-   rity numbers in training data . Given the sheer   scale of the training corpus , it is challenging   to screen and filter all privacy data , either man-   ually or automatically . In this paper , we pro-   pose Confidentially Redacted Training ( CRT ) ,   a method to train language generation models   while protecting the confidential segments . We   borrow ideas from differential privacy ( which   solves a related but distinct problem ) and show   that our method is able to provably prevent un-   intended memorization by randomizing parts   of the training process . Moreover , we show   that redaction with an approximately correct   screening policy amplifies the confidentiality   guarantee . We implement the method for both   LSTM and GPT language models . Our experi-   mental results show that the models trained by   CRT obtain almost the same perplexity while   preserving strong confidentiality .   1 Introduction   Language models ( LM ) have rich real - world ap-   plications in , among others , machine translation   ( Bahdanau et al . , 2015 ) , AI chatbots ( Hosseini - Asl   et al . , 2020 ) , question answering ( Kwiatkowski   et al . , 2019 ) , and information retrieval ( Ganguly   et al . , 2015 ) . The advent of transformers ( Vaswani   et al . , 2017 ) has fostered a dramatic advancement   in the capabilities of generative neural language   models , yet they come at a cost to privacy , as the   amount of excess parameters in the LM enables   it to memorize certain training samples . Recent   works show that sensitive user information from   the training dataset , such as address and name , can   be extracted verbatim from text generation mod-   els by querying the LM as an API ( Carlini et al . ,   2019 , 2021 ; Lee et al . , 2022 ) . How to train a high-   performing language model without memorizingsensitive text has become a major research chal-   lenge .   Existing solutions to this problem primarily   leverage differential privacy ( DP ) ( Dwork et al . ,   2006 ) .   Differentially private learning algorithms ensure   that an attacker could not infer whether a data point   is used for training , let alone extracting the sensi-   tive information within that data point .   However , there are several mismatches between   the problem of privacy that DP addresses , and our   problem of preventing the memorization of sensi-   tive text ( henceforth referred to as confidentiality ) .   First , confidential information in a natural language   dataset is sparse ( e.g. , the bulk of an email might   not carry confidential information ) . DP ’s undis-   criminating protection for all sentences could be   unnecessarily conservative which limits the util-   ity of the trained model . Second , what needs to   be protected is the content of the sensitive text ,   rather than the data context . For example , in the   sentence “ My SSN is 123 - 45 - 6789 . ” , it is   the actual SSN that we hope to conceal rather than   the general information that someone entered her   SSN in a chatbot dialogue . Thirdly , the same sen-   sitive content could appear in many data points ,   which makes the protection of the content more   challenging than protecting one data sample . These   differences motivate us to treat the problem of con-   fidentiality protection in LM separately with new   definitions .   Besides DP , we also consider classical tech-   niques of redaction and deduplication . Redaction   refers to the process of removing sensitive or classi-   fied information from a document prior to its publi-   cation in governmental and legal contexts . Dedupli-   cation is the procedure of detecting and removing   identical and nearly identical texts from a corpus .   The main challenge of applying these techniques is   that it is hard to manually redact a gigantic dataset   and automated tools are far from being perfect.943   The contribution of this paper is fivefold .   1.We show that in the absence of a perfect   screening policy , the risk of a language model   memorizing sensitive content is real and can   be efficiently exploited with only blackbox   access to the model even if the learning algo-   rithm satisfies the recently proposed notion of   selective differential privacy ( Shi et al . , 2021 ) .   2 . Inspired by differential privacy , we introduce   a new definition of confidentiality which pre-   cisely quantifies the risk of leaking sensitive   text .   3.We propose CRT to train language generation   models while protecting confidential text . The   method with deduplication and redaction oper-   ations work even under imperfect confidential   text labeling policies .   4.We theoretically prove that CRT , combined   with differentially private stochastic gradient   descent ( DP - SGD ) , provides strong confiden-   tiality guarantees .   5.Our experiments on both MultiWOZ 2.2 and   CustomerSim datasets show that different   models trained by CRT can achieve the same   or better perplexity than existing solutions   ( against the attacks of Carlini et al . ( 2019 ,   2021 ) ) .   To the best of our knowledge , we are the first that   rigorously establish the role of deduplication andredaction in achieving provably stronger confiden-   tiality ( or the related differential privacy ) guaran-   tees ; and the first that achieve provably confidential-   ity in transformer models with only a mild utility   loss .   2 Background & Related Work   Next , we briefly introduce the relevant background   and discuss the related work to put our work in   context .   Language modeling is a fundamental problem   in natural language processing ( Devlin et al . , 2019 ;   Howard and Ruder , 2018 ; Raffel et al . , 2020 ) .   Consider a text sequence that consists of mul-   tiple tokens from a vocabulary V , i.e. ,w=   ( w , w , . . . , w ) , where wis the i - th token . The   goal of language modeling is to construct a gen-   erative model of the distribution Pr(w ) , by apply-   ing the chain rule Pr(w ) = /producttextPr ( w|w ) .   We let f(w|w)denote the likelihood of token   wwhen evaluating the neural network fwith   parameters θ . A language model is trained to   maximize the probability of the data in a training   setW , by minimizing the negative log - likelihood   over each training example with the loss function   L(θ ) = −log / producttextf(w|w).Recurrent neu-   ral networks ( RNNs ) used to be a common choice   for the neural network architecture to estimate the   probability distribution Pr(w ) . ( Hochreiter and   Schmidhuber , 1997 ; Mikolov et al . , 2010 ) . More   recently , large - scale Transformer - based language   models have replaced RNNs in state - of - the - art944models for all sorts of NLP tasks ( Vaswani et al . ,   2017 ; Radford et al . , 2019 ) . Nevertheless , common   language models are vulnerable to privacy attacks   and possibly expose information about their sensi-   tive training data ( Carlini et al . , 2019 , 2021 ) .   Differentially private ( DP ) learning methods   ( see , e.g. , Abadi et al . , 2016 ) has been applied to   language models as a blanket solution for a num-   ber of privacy and security risks . McMahan et al .   ( 2018 ) trained an RNN language model with DP   guarantees in a federated learning setup . Anil et al .   ( 2021 ) pre - trained BERT under DP on datasets with   hundreds of millions of examples . These paper   also demonstrated that DP can effectively prevent   data - extraction attacks in practice even for algo-   rithms with DP guarantees that are considered too   weak from a theoretical - perspective ( e.g. , ϵ= 8   or16 ) . However , the strong protection of DP of-   ten results in a substantial drop in the utility of the   trained model , which makes them less desirable in   practice . In fact , it was recently shown that it is   necessary for deep learning models to memorize   certain training data to achieve high accuracy ( Feld-   man , 2020 ) , which suggests that DP or any other   techniques that require the model to not memorize   any training data will perform poorly in the high-   dimensional , power - law distributed real datasets .   This motivates us to consider weakened models   that only prevent memorizing the sensitive part of   the text .   Recent works ( Lee et al . , 2022 ; Kandpal et al . ,   2022 ) show that deduplication enables language   models to emit memorized text less frequently with   same or better accuracy . However , deduplicating   training datasets can not prevent all unintended   memorization . We combine deduplication and   redaction and then apply both techniques to the   training process of LM to achieve confidentiality   with provable guarantee .   The closest to us is perhaps the work of Shi   et al . ( 2021 ) , who proposed selective differential   privacy ( S - DP ) , which requires indistinguishability   between two datasets that differ only on a sensi-   tive message . Correspondingly , they propose an   algorithm ( Selective DP - SGD ) for training RNN   that adds noise only to the part of computation that   involves sensitive tokens . To define S - DP and to   run Selective DP - SGD , one needs to have access to   a policy function Fwhich determines which token   is sensitive . This requirement limits the applicabil-   ity of their approach to those applications wheresuch perfect Fis known . We note that even for   name - entity recognition the state - of - the - art model   is far from being perfect , and which part of the text   is considered sensitive is often ambiguous even   for human annotators . We will see that naively   running Selective DP - SGD with an approximate   policy function does not provide a meaningful con-   fidentiality guarantee and is vulnerable to practical   data extraction attacks . Finally , we note that in the   case when a perfect policy function is available , we   can simply use it for redaction , which provides a   perfect S - DP with ϵ= 0 . A big part of our con-   tribution is to refine S - DP to a ( slightly different )   definition called “ confidentiality ” and to demon-   strate that we use an approximate screening policy   to amplify the confidentiality parameter .   3 The CRT Method and Theory   In this section , we develop our method with prov-   able confidentiality .   3.1 Formally defining confidentiality   Let the dataset be a collection of ndata points —   each being a sequence of tokens . A “ secret ” x   is a contiguous subsequence of tokens within a   data point that is considered sensitive orconfiden-   tial . The goal of our research is to allow us to   train language models on such datasets that could   contain secrets while provably prevent the model   from remembering that these secrets were . We start   by defining a formal definition of confidentiality ,   which uses the following idea of indistinguishabil-   ity from the DP literature .   Definition 1 ( Indistinguishability ) .We say that a   pair of distributions P , Q defined on the same prob-   ability space are ( ϵ , δ)-indistinguishable if for any   measurable set S ,   Pr[S]≤ePr[S ] + δ .   Definition 2 ( Confidentiality ) .We say that Aen-   sures that a secret xis(ϵ(x ) , δ)-confidential , if   for any dataset Dthat contains xin one of its   data points , and an alternative dataset Dthat re-   places xinDwith a generic < MASK > , it holds that   ( A(D),A(D))are(ϵ(x ) , δ)-indistinguishable . In   addition , we simply say that Aensures ( ϵ , δ)-   confidentiality if ϵ(x)≤ϵfor all secret x.   This definition ensures that an attacker can not   distinguish from the output of A(the trained lan-   guage model ) whether it was xor < MASK > that945was used for training , thus formalizing the idea of   confidentiality . The protection should be viewed   as relative , rather than absolute . The definition   bounds the risk of any bad event by an multiplica-   tive factor of eand an additive factor of δ , which   implies that anything that could happen when we   runAon the sensitive data could ’ve happened with   with similar probability even if Aruns on an alter-   native world where these sensitive information are   perfectly masked .   Connections to differential privacy . Our defi-   nition of confidentiality is related to ( and inspired   by)(ϵ , δ)-differential privacy ( DP ) but is differ-   ent in several ways . DP is stronger ( and im-   plies confidentiality ! ) requires Ato ensure ( ϵ , δ)-   indistinguishability for all D , Dthat can be mod-   ified from each other by adding or removing one   individual person / data point ( or tokens , depend-   ing on the desired granularity ) ; but for Ato en-   sure ( ϵ , δ)-confidentiality , it only requires ( ϵ , δ)-   indistinguishability for specific D , Dwhere D   replaces xinDwith < MASK > . Moreover , it is   more informative to define ϵas a function of each   specific x , which is different from DP ( it resembles   personalized DP ( Ghosh and Roth , 2015 ) ) .   The confidentiality definition makes sense for   our problem because it protects the content of the   sensitive text xrather than its existence . Specif-   ically , a pre - processing algorithm that masks all   sensitive text ensures ( 0,0)-confidentiality but does   not satisfy any non - trivial DP guarantees .   Sometimes , it is useful to consider the confiden-   tiality of multiple secret texts . For example , a se-   cret key xcould appear multiple times in multiple   data points . Also , there might be multiple secret   texts that are correlated to each other such that the   knowledge of one would reveal other secrets .   Definition 3 ( Group Confidentiality ) .We say that   Aensures that a listof sensitive texts S:=   [ x , ... , x]is(ϵ(S ) , δ)-(group ) confidential , if for   any dataset Dthat contains [ x , ... , x]in up to k   data points , and Dbeing the version that replaces   each element in Swith < MASK > , it holds that   ( A(D),A(D))are(ϵ(S ) , δ)-indistinguishable .   A special case of such group confidentiality is   whenScollects the all secret text inD , which   protects all secret texts uniformly . We call this   uniform - confidentiality . Note that the standard def-   inition of confidentiality also protect every secret   x , except that it protects each secret xindividually ,   rather than together . Inspired by the recent development of Bayesian   DP ( Triastcyn and Faltings , 2020 ) , we also define   Bayesian confidentiality as follows .   Definition 4 ( Bayesian Confidentiality ) .LetDbe   a dataset that is fixed except a random secret x∼µ   drawn from some distribution µ. Let Dbe ob-   tained by replacing xwith < MASK > . Then Aen-   sures ( ϵ , δ)-Bayesian Confidentiality if for any D ,   ( A(D),A(D))is(ϵ , δ)-indistinguishable , where   A(D)is jointly distributed over x∼µandA.   The Bayesian confidentiality measures how   much information an attacker could gain if he / she ’s   prior knowledge about this secret xis described by   the distribution µ. This is a strict generalization   because when µis a single point mass at x , it recov-   ers Definition 2 . The additional generality allows   us to quantify the stronger confidentiality guaran-   tee against weaker adversaries without complete   information .   3.2 Confidentially redacted training   In this section we describe the CRT method to train   language models with provable confidentiality guar-   antee . It includes two pre - processing operations   ( deduplication and redaction ) and a switching opti-   mization procedure . The overall idea is to screen   the corpus into two separate sets , one public set in-   cluding sentences with no confidential information ,   and one private set including sentences containing   confidential content . We then use normal optimiza-   tion algorithms ( e.g. SGD ) on the public set and   differential privacy optimizer ( e.g. DP - SGD ) on   the private set .   Deduplication . The deduplication procedure   Dedup detects all sentences that appear multiple   times in the training data and replace them into   a single < MASK > from the second occurrence on-   wards ( < MASK > is for proving purpose ) .   Redaction . The redaction procedure Redact   takes applies a sequence labelling policy πto   screen confidential content in the training corpus   D.π(s , x ) = 1 if a token xin a sentence sshould   be confidential . The labeled span in each detected   sentence is replaced with a special token < MASK > .   Note that we do not assume the policy is perfect . It   may label some non - sensitive tokens as sensitive   ( false positives ) and label some sensitive text as   non - sensitive ( false negative , or 1−recall).946Algorithm 1 : CRT   Input : Dataset D(after tokenization /   splitting ) , labelling policies π , π ,   number of epochs TD←Dedup ( D)D←Redact(D)D← { s∈D|∃x∈ss.t.π(s , x ) =   1or∃x⊂ss.t.π(s , x ) = 1}D={s∈D|s /∈D}.fore= 1 , ... , T do Run one epoch of SGD with D. Run one epochof DP - SGD with D.end   Redact andDedup could be implemented man-   ually , but with the large text corpus nowadays it   is more common that these procedures are im-   plemented using automated tools . For example ,   Dedup could be implemented efficiently with just   one pass of data using a bloom filter ( Bloom , 1970 )   ( or other hashing tricks that also catches near-   duplicates ) . Bloom filter in particular , enjoys the   nice property that it could have false positives but   never any false negatives . Redactcould be real-   ized by a named entity recognition ( NER ) model or   a personal - identifiable information ( PII ) detector .   Finally , CRT combines the two pre - processing   steps with normal optimizer and DP - SGD , the stan-   dard algorithm for deep learning with differential   privacy . A pseudo - code of the algorithm is given   in Algorithm 1 .   Besides using a sequence labeling policy πwith   balanced precision / recall as part of the redaction   process . The algorithm uses another , more conser-   vative , policy πwith nearly perfect recall to decide   on the data points that do not contain sensitive text .   In the situation when such πisn’t available , we   simply choose π(s , x ) = 1 for all tokens xin a   sentence sand the second part becomes the vanila   DP - SGD . It is also important that every data point   that contains a < MASK > requires protection .   3.3 Theoretical analysis   We analyze the theoretical properties of the above   method and show that they result in provable im-   provements in the ( regular , group and Bayesian)confidentiality parameters for any algorithms that   are provably ( ϵ(x ) , δ)-confidential as defined in   Section 3.1 .   The following theorem captures the benefit of   redaction in improving confidentiality .   Proposition 5 ( Confidentiality under redaction ) .If   Aensures ( ϵ(x ) , δ)-Confidentiality for each token   xof sentence s∈ S ( Sis a corpus ) , then A ◦   Redactensures ( ˜ϵ(x ) , δ)-confidentiality with   ˜ϵ(x ) = /braceleftigg   ϵ(x)ifπ(s , x ) = 0   0 otherwise .   In addition , A ◦ Redactalso satisfies   ( ˜ϵ(S),˜δ(S))-group confidentiality with   ˜ϵ(S ) = /summationdisplayϵ(x)1(π(s , x ) = 0 ) ,   ˜δ(S ) = ˜keδ   where ˜k:=/summationtext1(π(s , x ) = 0 ) .   As an application of the above , if Aensures   ( ϵ , δ)-confidentiality , and that the empirical recall   rates of the redaction policy on Dis1−γ , then   the above proposition suggests that A ◦ Redact   improves the uniform - confidentiality over applying   Awithout redaction by a factor of γ . The proof is   in the appendix .   Redaction also improves Bayesian confidential-   ity in a way that mirrors the privacy amplification   by sampling from the DP literature .   Proposition 6 ( Bayesian Confidentiality under   Redaction ) .IfAensures ( ϵ , δ)-Bayesian Confi-   dentiality with respect to µ[x|π(s , x ) = 0 ] for a   token xin a sentence s , then A ◦ Redacten-   sures ( log(1 + γ(e−1 ) ) , γδ)-Bayesian Confiden-   tiality under µifπhas a false negative rate ( i.e. ,   1−“Recall ” ) of γunder µ.   The proposition says that if the redaction pol-   icy is accurate for secrets x∼µ , then we can   have a stronger confidentiality parameter that scales   roughly at ˜ϵ=O(γϵ ) . The idea behind the proof   is that over the distribution of x∼µ , with prob-   ability 1−γ , Redact(D ) = Redact(D ) , thus   A ◦ Redact(D)≡ A ◦ Redact(D ) . With prob-   ability γ , Redact(D),Redact(D)are different   and conditioning on the fact that Redactfails to   detect x. Note that πis also applied to other text   that are not sensitive , and could result in false pos-   itives , but they do not matter as the modification947ofRedacttoDandDwill be identical . A full   proof is given in the appendix .   Next we turn to deduplication .   Proposition 7 ( Group confidentiality under   deduplication . ) .IfAensures ( ϵ(S ) , δ(S))-   Group Confidentiality , then A ◦ Dedup ensures   ( ϵ(Unique ( S ) ) , δ(Unique ( S)))-Group Confiden-   tiality .   Deduplication provides a stronger protection for   those cases where some secret xcould appear mul-   tiple times in the dataset .   Theorem 8 . Let DP - SGD from Algorithm 1 satis-   fies(ϵ , δ)-differential privacy .   1.Assume π(s , x ) = 1 for all secret tokens x   in a sentence ssuch that π(s , x ) = 0 , then   Algorithm 1 satisfies ( ϵ1(π(s , x ) = 0 ) , δ)-   confidentiality .   2.LetSbe a group containing munique secrets   such that π(s , x ) = 1 ∀x∈sands∈ S   and that πdetects ˜γ - proportion of the unique   secrets in S. Then Algorithm 1 satisfies   ( ˜γmϵ , ˜γmeδ)-group confidentiality for S.   3.Letπ , πhas a a recall of 1−γand1−δ   respectively on µ , then Algorithm 1 satisfies   ( log(1 + γ(e−1 ) ) , γδ+δ)-Bayesian Con-   fidentiality for µ.   The theorem demonstrates that our CRT algo-   rithm enjoys a full suite of confidentiality guaran-   tees and they all benefit from the deduplication and   redaction , particularly if πhas high recall .   Note that the CRT algorithm achieves the worst-   case confidentiality guarantee if we have a non-   trivial conservative screening policy that outputs   π(x ) = 1 forallsecret xthatπmisses , or we sim-   ply run vanilla DP - SGD after deduplication and   redaction . On the other hand , CRT still satisfies   Bayesian confidentiality for each µdepending on   the recall rate of πunder µ.   4 Experiments   We evaluate CRT by training two types of language   model , LSTM and GPT-2 , on two datasets : 1 ) Mul-   tiWOZ 2.2 , a well - known human - written dialogue   dataset and 2 ) CustomerSim , a simulated dialogue   dataset for conversation generation .   MultiWOZ 2.2 is an already - public dialogue   dataset written by crowd - workers , which collectsover 10,000 annotated dialogues spanning 8 do-   mains ( Zang et al . , 2020 ) . We use this dataset to   show how CRT works in real - world applications .   Following US Department of Labor ’s guidanceon   personal - identifiable information ( PII ) , we treat all   confidential information ( e.g. email address , ref-   erence number , telephone number , etc . ) as secrets .   For the sequence labeling policy πand conserva-   tive policy π , we build upon an NER model to do   redaction . See Appendix A.4 for more details .   CustomerSim . Following S - DP Shi et al . ( 2021 ) ,   we simulate a dialog dataset called CustomerSim   with synthetic user information . The dialog flow is   simulated based on a fixed agenda and the language   generation is template - based ( Zhao and Eskénazi ,   2018 ) . CustomerSim consists of 10 thousand ex-   amples and over one million tokens . We treat user   name , address , phone number , order , and tracking   number as secrets , and use a regular expression   tester ( regex ) to detect them for the redaction pro-   cess .   Experiment details . For LSTM model , we fol-   low the setting in S - DP to choose a one - layer   LSTM . Because S - DP requires hidden states of   the sensitive input to be protected , it does n’t sup-   port more layers nor Bidirectional LSTM . Since   the advent of Transformers ( Vaswani et al . , 2017 )   significantly improves the capabilities of generative   language models , we also test transformer - based   language model GPT-2 ( Radford et al . , 2019 ) from   HuggingFace ( Wolf et al . , 2019 ) . As for deduplica-   tion , we use SHA-1 ( Jarvinen , 2004 ) hash function   to encode sequences to SHA-1 hash code and then   remove identical sequences based on the same hash   code . For Bayesian Confidentiality , we treat the   uniform distribution over the secret sequences as   the distribution µ. More experiment details can be   found in Appendix A.3 .   Baselines . For LSTM model , we compare four   different training approaches : ( 1 ) vanilla SGD   ( denoted by " Non - private - LSTM " ) , ( 2 ) Selective   DPSGD ( denoted by " S - DP - LSTM " ) ( 3 ) DPSGD   ( denoted by " DPSGD - LSTM " ) and ( 4 ) confiden-   tially redacted training ( denoted by " CRT - LSTM " ) .   While for GPT-2 model , we compare three dif-   ferent training approaches : ( 1 ) vanilla SGD ( de-   noted by " Non - private - GPT " ) , ( 2 ) DPSGD ( de-   noted by " DPSGD - GPT " ) and ( 3 ) CRT ( denoted by   " CRT - GPT " ) . Our implementation of S - DP - LSTM948   model is built upon Shi et al . ( 2021 ) . We run   the experiment for the GPT-2 model following Li   et al . ( 2021 ) , in which they propose ghost clipping   method to alleviate the computational challenge of   running DP - SGD with large Transformers .   All the models are trained five times to reduce   randomness , and the parameters are tuned based   on the validation set performances .   5 Experimental Results   5.1 Evaluation procedure   We need to evaluate both model utilities and privacy   guarantees of the language models . We measure   predictive perplexity ( PPL ) for the quality of LM .   We also analyze the theoretical privacy budget ( ϵ ,   δ)and test whether language models are private   under attacks detailed below .   Canary insertion attack . Canary insertion is   proposed as a testing methodology for quantita-   tively assessing the risk of unintended memoriza-   tion ( Carlini et al . , 2019 ) . It inserts random se-   quences called canaries into the training dataset ,   then trains the model , and finally calculates the   following exposure for the inserted canaries to   measure a model ’s potential for privacy risks . In   our experiment , we randomly generate 10 ca-   naries in the form of " My ID is : < random   6 - digit number here > " . Each canary is in-   serted into the training dataset 20 times to generate   more salient differences between models .   Definition 9 ( Canary Exposure ) .Given a canary   s[r ] , a model with parameters θ , and the random-   ness space R , the exposure of s[r]is   exposure= log|R| − logrank(s[r])After training , we calculate empirical model per-   plexity for all possibly - instantiated canaries and list   them in sorted order . Then we can get the canary   exposure based on the rank of a specific canary se-   quence rank(s[r])and the number of all possible   candidates |R| . In our setting , we show the highest   canary exposure in 10 canaries . For example , if a   canary ranks 1st among 1 M candidates , the canary   exposure is 19.93 .   Membership inference attack . Membership In-   ference is a widely used privacy attack method .   Given a non - privately trained model , an adversary   can predict whether or not a particular example   was used to train the model . We adopt the member-   ship inference attack in Carlini et al . ( 2021 ) . The   general idea is to calculate the given samples ’ per-   plexities under the model , rank them and choose   the ones with the lowest perplexities , i.e. , highest   likelihood by the model . We can think of this pro-   cess as training a binary classifier based on the   perplexity feature . We also implement the group   membership inference attack to show the group   confidentiality . More details about the implementa-   tion can be found in the Appendix A.5 .   5.2 Overall performance   Figure 2 presents the results of model utilities and   confidentiality guarantees across our models of in-   terest on MultiWOZ 2.2 and CustomerSim datasets .   Each point denotes a model for different epochs in   a training process . Since the X - axis is ϵin Bayesian   Confidentiality ( the lower the better ) and the Y - axis   is perplexity ( the lower the better ) , a perfect model   will lie in the bottom - left corner . CRT - GPT and   DPSGD - GPT in general , perform better than S - DP-   LSTM , CRT - LSTM and , DPSGD - LSTM on the   test sets . Our model CRT - GPT ’s performance is949close to Non - private - GPT in terms of PPL while   preserving strong confidentiality . Besides , CRT-   GPT is better than DPSGD - GPT manifested by a   much lower ϵ , which demonstrates that approxi-   mately correct screening policy amplifies the confi-   dentiality guarantee .   Differences can be witnessed in the results from   two different datasets : the models trained on Cus-   tomerSim achieve overall better performances than   those trained on MultiWOZ . We think it ’s due to   the fact that CustomerSim contains simple dialogs   from template - based simulations .   5.3 Attack results   Figure 3 , 4 , and 5 present the results from canary   insertion attack and individual / group membership   inference attack on MultiWOZ 2.2 and Customer-   Sim datasets . The X - axis is the false negative rate   γof screening policy π , ranging from 0.0 to 0.5 ;   the Y - axis is the canary exposure ( in Figure 3 ) and   membership inference accuracy ( in Figure 4 and 5 ) ,   which measures the effectiveness of the attacks .   The lower the canary exposure or inference ac-   curacy , the better protection the model provides   against the attacks .   For canary insertion attack , it can be seen from   Figure 3 that the canary exposures for CRT - LSTM   and CRT - GPT are both close to 0 which thus guar-   antee excellent confidentiality . Non - private - LSTM   and Non - private - GPT with mask can also attain   great protection at perfect screening policy accu-   racy ( γ=0 ) , nonetheless a rise in γresults in a   sharp increase in the exposure . It should be noticed   that S - DP - LSTM also has high exposure , similar   to Non - private models , given any γabove 0 . This   is because that many sensitive data has been falsely   identified as non - sensitive by the approximate pol-   icy , S - DPSGD does not protect these false negative   samples and hence a privacy leakage .   For membership inference attack , we compare   the inference accuracy with the benchmark value of   0.5 , which equals the random guess performance .   In Figure 4 and 5 , we see that CRT - LSTM and CRT-   GPT align well with the 0.5 horizontal line , suggest-   ing that they are rather safe to the attack . The infer-   ence accuracy for Non - private - LSTM / Non - private-   GPT / S - DP - LSTM , in contrast , surges above 0.5   as the false negative rate γdeviates from 0.0 , indi-   cating that these models become vulnerable to the   attack under non - perfect screen policy . In addition ,   Non - private and S - DP models show even worse   protection under the group attack than the individ-   ual one in view of a higher inference accuracy at   certain γ .   5.4 CRT amplifies Bayesian Confidentiality   guarantees   Figure 6 shows that confidentially redacted train-   ing can help to amplify the confidentiality guaran-   tees . We set the ϵin DP - SGD fixed and show the950corresponding ϵin Bayesian Confidentiality with   different screen policy π . Both ϵandϵare for   δ= 8e−5 . If the approximately screening policy   πhas a high recall ( γis small ) , we will achieve   much improvement in the Bayesian Confidentiality   parameter ϵby deduplication and redaction . For   example , with ( ϵ= 1.0 , γ= 0.1 ) , we reduce the   ϵto 0.12 .   6 Conclusion   In this paper , we propose confidentially redacted   training ( CRT ) , a method to train language models   while protecting the secret texts . We introduce a   new definition of confidentiality which quantifies   the risk of leaking sensitive content . We prove the   effectiveness of CRT both theoretically and empiri-   cally on multiple datasets and language models .   7 Broader Impact   This work will alleviate ethical concerns of large-   scale pre - trained language models . This paper pro-   vides one promising solution to an important as-   pect of NLP : training high quality language models   for text generation without compromising confi-   dential information . The current use cases of lan-   guage models involve pretraining on public web   corpus and fine - tuning on individual application   data . However , the private application specific data   often contains user - generated sensitive information .   The proposed method in this paper aims to use   as much individual fine - tuning data as possible ,   while does not leak or memorize any confidential   information with provable guarantees . Without the   method , one has to either use the general pretrain-   ing LM without fine - tuning or manually filter sen-   sitive information and fine - tuning on the remaining .   It can be applied in broader applications that need   language models or text generation models .   In our experiments , we use a simulation scheme   to mimic confidential content in a real corpus . We   did not compromise any real user ’s confidential   information .   Acknowledgements   The work was partially supported by NSF Award   # 2048091 . XZ was supported by UCSB Chancel-   lor ’s Fellowship . We would like to thank the anony-   mous reviewers for their thoughtful comments . We   would also like to thank Siqi Ouyang for the help-   ful discussion and Yang Gao for polishing up the   draft . References951952A Appendix   A.1 Illustration of our proposed algorithm   A.2 Proofs of technical results   Proof of Proposition 5 . The first statement straigtforwardly follows from that Redact(D ) =   Redact(D)ifπ(s , x ) = 1 and that Redact(D)andRedact(D)remain a pair of neighbors differing   by only x. The group confidentiality claims follows from the standard calculation of small group privacy   from differential privacy , which applies the ( single x ) confidentiality iteratively . Let ˜D= Redact(D ) ,   ˜D= Redact(D)and˜S= [ x , ... , x]be the list of Sthat are not masked by π . For any measurable   event E   P[A ◦ Redact(D)∈E ] = P[A(˜D)]≤eP[A(˜D ) ∈E ] + δ   ≤eP[A(˜D)∈E ] + eδ+δ   ...   ≤eP[A(˜D)∈E ] + δ(1 + e+e+ ... +e )   ≤eP[A ◦ Redact(D)∈E ] + keδ   Proof of Proposition 6 . Consider a dataset D(in which one of the data point has x∼µ ) and a fixed D.   Denote the probability distributions p , q , r as shorthands for   p∼ A ◦ Redact(D)|π(s , x ) = 1   q∼ A ◦ Redact(D)|π(s , x ) = 0   r∼ A ◦ Redact(D)|π(s , x ) = 0   Moreover , we use αp+ ( 1−α)qto denote the mixture distribution that samples from pwith probability   αandqwith probability 1−α .   Recall that the Hockey - Stick - divergence characterization of ( ϵ , δ)-indistinguishsability ( Barthe and   Olmedo , 2013 ) , which says that ( P , Q)are(ϵ , δ)-indistinguishsable if and only if   H(P∥Q ) : = E[(dP   dQ(y)−e)]≤δ.953It suffices for us to bound the following quantity :   H(A ◦ Redact(D)∥A ◦ Redact(D ) ) = H((1−γ)p+γq∥(1−γ)p+γr )   = γH(q∥(1−β)p+βr)≤γ((1−β)H(q∥p ) + βH(q∥r ) )   where β= . In the above , the second line follows from Theorem 2 of ( Balle et al . , 2018 ) ( an   identity called “ Advanced Joint Convexity ” by the authors ) and the inequality is due to the ( standard ) joint   convexity of the Hockey - Stick divergence . It remains to bound H(q∥p)andH(q∥r ) .   Check that p , r , A ◦ Redact(D)are identically distributed and that H(q∥r)≤δby our assumption   onA ’s Bayesian confidentiality guarantee w.r.t . µ(x|π(s , x ) = 0 ) . This completes the proof .   Proof of Proposition 7 . The proof is straightforward as Dedup ( D)differs from Dedup ( D)only by   Unique ( S ) .   Proof of Theorem 8 . The proof for the first statement follows from the fact that DP implies ( ϵ , δ)-   confidentiality and Proposition 5 . Notably , if πcatches all xthat is missed by π , then we get that   for all secret x,ϵ(x)≤ϵ.   The proof of the second statement applies Proposition 7 and the second part of Proposition 5 .   The proof of the third statement applies Proposition 6 but requires a separate treatment of the case when   xis missed by both πandπ . Let the event that a secret xis not selected by the conservative policy be E   and let Abe a generic algorithm satisfying ( ϵ , δ)Bayesian confidentiality under µ ,   P[A(D)∈S]≤P[A ◦ Redact(D)∈S⊂E ] + δ   ≤eP[A(D)∈S⊂E ] + δ+δ   ≤eP[A(D)∈S ] + δ+δ .   This completes the proof .   A.3 More details on experiments   We choose the one - layer LSTM with an embedding size of 200 and a hidden size of 200 . We choose   distill - gpt2as the GPT-2 model , which has 6 layers , 768 dimension and 12 heads . V ocabulary size   for GPT-2 is 50257 . Our experiments are conducted on NVIDIA TITAN - Xp GPU . For LSTM models ,   we tune the hyperparameters of the learning rate ( lr ) among { 20 , 10 , 5 , 1 , 0.1 , 0.05 , 0.01 } , batch size   ( bs ) and the epochs among { 5 , 10 , 30 , 50 , 100 } . We finally choose { lr=20 , bs=256 , epochs=50 } for   Non - private - LSTM , { lr=0.1 , bs=5 , epochs=50 } for S - DPSGD - LSTM and { lr=0.05 , bs=10 , epochs=100 }   for CRT - LSTM . The same set of hyperparameters are tuned for GPT model as well . Our final choice   for DPSGD - GPT / CRT - GPT model is { lr=5e-4 , bs=256 , epochs=10 } . The actual run - time of algorithms   depends on implementation details . Here , we outline estimates of the run - time for training . Running one   epoch on CRT - LSTM takes 2 hours wheras the same task on CRT - GPT only takes 30 minutes since the   implementation of Li et al . ( 2021 ) is highly efficient . We use autodp , an automating differential privacy   computation for the privacy analysis . Noise scale σis calculated numerically so that a DP budget of ( ϵ , δ )   is spent after Tepochs .   A.4 Redaction policy details   We build the sequence labeling policy based on trimming one NER modeltrained on OntoNotes-5.0   ( Weischedel et al . , 2013 ) dataset . We modify the last layer of the NER model and set the threshold for the   output scores to enable abnormal / sensitive data detection . For the screen policy π , we set the threshold to   be 0.3 for all predictions with OntoNotes tags . For the conservative policy π , we select all predictions   with tags and all plain texts with scores smaller than 0.9 to be sensitive data . We manually label 200 data   points and find that the conservative policy πcan achieve 100 % recall with lots of false positives and that   πcan achieve 90 % recall with few false positives.954A.5 Membership inference attack details   In our experiments , we manually construct a dataset with 2000 sequences . We select 1000 sequences from   the protected secrets used in the training data . And we randomly generate 1000 samples of similar format   which are not used in the training data . In this way , a random guess generates an accuracy of 50 % . For   MultiWoz 2.2 , we use sentences with reference numbers as the secrets . For CustomerSim , we choose   customer addresses as the secrets .   In order to show group confidentiality guarantees , we also conduct group membership inference attack .   In this setting , we construct a dataset with 2000 groups , each of which includes 20 sentences . One half of   the groups are “ sensitive groups " with all 20 sentences drawn from protected secrets and the other half are   " insensitive groups " with all 20 sentences being random . We build the classifier based on the sum of the   perplexities in one group .   A.6 “ The devil is in the details ” – how things could go wrong with seemingly inocuous changes to   the algorithm .   In this section , we highlight various aspects of our algorithms and why certain choices in the pre - processing   steps need to be done in the specific way we recommend for our results to hold for them .   1.It is important that the definition of confidentiality is defined with respect to a perfectly redacted   version of the dataset . If we define it as in selective differential privacy , then there will notbe an   amplification effect from redaction . This is because if we replace a secret xthat can be detected   byπwith another xthat can not be detected by π , then even if xis replaced with < MASK > , xwill   not be and the two datasets are still different after redaction . In addition , the S - DP definition will   not be useful for us we do not know how to define a confidentiality parameter specific for each xor   Bayesian confidentiality parameter for each µ   2 . Tokenization and splitting into individual “ sentences ” ( data points ) should go before redaction / de-   duplication . Otherwise redaction with an approximate screening policy and with an ideal screening   policy , or deduplication may cause misalignments , resulting in almost all data points being different   in the preprocessed version of DandD.   3.Each data point should contain only “ whole ” natural sentences , otherwise the sensitive part of a   natural sentence could split into two data points .   4.Deduplication steps should replace duplicate text with the same < MASK > , otherwise   < MASK_Dedup > and < MASK_Redact > are not the same so even if all secrets are masked , there   will be a difference between the pre - processed versions of Dand its neighbor , while in our approach   there are no differences and we achieve perfect confidentility ( with ϵ= 0 ) .   5.Any data point containing < MASK > needs to be put in D. This is because otherwise our algorithm   that works on Dwill be a deterministic algorithm that is perfectly distinguishable from the alternative   world where the algorithm is random because the approximate policy πfails to redact certain secrets   x.   6.In the DP - SGD algorithm , the sampled minibatches should contain the whole minibatch from D   or the whole minibatch from D. Otherwise the noise always need to be added and the algorithm is   identical to the vanilla DP - SGD , and there is no benefit of having a portion of the data being public   comparing to all of the data are private.955