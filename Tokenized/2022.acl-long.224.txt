  Bodhisattwa Prasad MajumderHarsh Jhamtani   Taylor Berg - KirkpatrickJulian McAuleyDepartment of Computer Science and Engineering , UC San Diego   { bmajumde , tberg , jmcauley}@eng.ucsd.eduSchool of Computer Science , Carnegie Mellon University   jharsh@cs.cmu.edu   Abstract   A limitation of current neural dialog mod-   els is that they tend to suffer from a lack of   speciﬁcity and informativeness in generated re-   sponses , primarily due to dependence on train-   ing data that covers a limited variety of sce-   narios and conveys limited knowledge . One   way to alleviate this issue is to extract rele-   vant knowledge from external sources at de-   coding time and incorporate it into the dialog   response . In this paper , we propose a post-   hoc knowledge - injection technique where we   ﬁrst retrieve a diverse set of relevant knowl-   edge snippets conditioned on both the dialog   history and an initial response from an exist-   ing dialog model . We construct multiple can-   didate responses , individually injecting each   retrieved snippet into the initial response us-   ing a gradient - based decoding method , and   then select the ﬁnal response with an unsu-   pervised ranking step . Our experiments in   goal - oriented and knowledge - grounded dialog   settings demonstrate that human annotators   judge the outputs from the proposed method   to be more engaging and informative com-   pared to responses from prior dialog systems .   We further show that knowledge - augmentation   promotes success in achieving conversational   goals in both experimental settings .   1 Introduction   Generic responses which lack speciﬁcity have been   a major issue in existing dialog models ( Hosseini-   Asl et al . , 2020 ; Dinan et al . , 2019a ) . The issue   in part stems from bottlenecks in dialog models   due to a limited scope of scenarios and access to   limited knowledge available during training . On   the other hand , encoding all possible world knowl-   edge at training time is not feasible , and even un-   desirable in cases where knowledge sources are   dynamically varying ( Ghazvininejad et al . , 2018 ;   Majumder et al . , 2020b ; Zhao et al . , 2020 ; Bruyn   et al . , 2020 ; Kim et al . , 2020 ; Prabhumoye et al . ,   2021 ) . One possible approach is to incorporateFigure 1 :   relevant knowledge at decoding - time . For exam-   ple , in Figure 1 , the user is seeking options for a   fun activity around Cambridge . While the initial   dialog response suggests watching a movie as an   option , it does not provide any information behind   that choice .   We propose and evaluate an approach for unsu-   pervised knowledge injection into a dialog model ’s   response at decoding time — not addressed in any   previous work . We ﬁrst sample a response from the   model ( trained on dialog data ) conditioned on the   dialog context . Next , we utilize the dialog context   and the sampled response to query external knowl-   edge sources . Finally , the retrieved knowledge is   used to construct a more informative and engaging   response ( Figure 1 ) . A major advantage of such   post - hoc knowledge injection is its ﬂexibility in   adding newer knowledge sources especially where   the success of achieving conversational goals re-   lies upon the availability of relevant knowledge .   Post - hoc injection also promotes efﬁciency in NLP   applications ( Schwartz et al . , 2020 ; Strubell et al . ,   2019 ): it mitigates the need to retrain dialog models   to accommodate dynamically evolving knowledge .   We experiment with two types of knowledge   sources : language models , which we treat as   parametric knowledge bases ( Petroni et al . , 2019;3140   Brown et al . , 2020 ) ; and user review datasets   such as Yelp reviews ( Hajas et al . , 2014 ) as non-   parametric knowledge sources ( § 2 ) . Since it is   possible to gather a large amount of related knowl-   edge given a query , we select a relevant and diverse   ( estimated via information - theoretic measures ) sub-   set of knowledge snippets using an unsupervised   method ( § 3.1 ) . Then , a gradient - based inference   approach is used to construct an updated response   that incorporates the selected knowledge ( § 3.2 ) .   Note that our framework does not require retrain-   ing the existing dialog model — it only relies upon   updating the model ’s output hidden states at decod-   ing time for unsupervised knowledge injection .   We experiment with two scenarios : goal-   oriented and knowledge - grounded dialog where the   training data covers only a fraction of the needed   knowledge . Automatic evaluation reveals that our   method is capable of generating highly diverse   responses in both settings . In some cases , the   generated response shows high overlap with the   original target response showing that our unsu-   pervised method bridges the knowledge gap be-   tween available knowledge and human - written re-   sponses present in the existing dialog corpus . An   extensive human evaluation conﬁrms that gener-   ated responses are indeed engaging , interesting ,   and human - like without any loss in ﬂuency .   To pinpoint the usefulness of knowledge injec-   tion in the above settings , we design a real - time   study ( § 5.3 ) where users interact with our system to   reach a conversational goal ( e.g. planning a holiday   or knowing more about the solar system ) . We ﬁnd   that external knowledge enables users to achieve   their goals more efﬁciently . Additionally , we ob-   serve that the our approach of sub - selecting rele-   vant but diverse knowledge leads to responses that   promote success in achieving conversational goals.2 Post - hoc Knowledge for Dialog   Our goal is to construct a dialog response by inject-   ing knowledge ( from external textual sources ) at   decoding time , without having to retrain the mod-   els . Consider a dialog model Mfrom which we   can sample a dialog response xgiven a dialog   historyH. We shall refer to the response xsam-   pled from such a model without any decoding time   knowledge injection as the initial response .   However , as motivated earlier , samples from   such a dialog model often lack detail . To improve   such responses , we retrieve and incorporate rele-   vant external knowledge kinto the initial response .   To achieve our goal , we construct a query using   both dialog history Hand the initial response x ,   and gather a relevant knowledge candidate kfrom a   knowledge sourceK. The retrieved snippet can pro-   vide useful information to the end - user to achieve   the conversational goal ( see § 5.3 ) . We explore both   parametric ( e.g querying a language model ) and   non - parametric ( e.g. deterministic retrieval using   word - overlap ) ways to obtain post - hoc knowledge .   2.1 Parametric knowledge sources   Pretrained language models ( PTLM ) are typically   trained with a vast amount of text that spans a   diverse range of domains . Petroni et al . ( 2019 ) ;   Brown et al . ( 2020 ) showed that such PTLMs can   be used as a source of knowledge when queried   with suitable textual prompts ( e.g. Seattle is famous   for ) . To use PTLMs in our use - case , we con-   struct useful prompts from dialog history and the   initial response . We assemble simple prompts in-   spired from various knowledge - seeking situations   in dialog ( Shwartz et al . , 2020 ) such as [ KP ] is fa-   mous for , Here is what I know about [ KP ] : , 3141where [ KP ] is a key - phraseextracted from dialog   context . We use gpt2 - large as the PTLM . For   example , a query “ Here is what I know about fun   things around Cambridge : " results in “ There are   plenty of museums to visit around Cambridge . If   you love hiking , you can enjoy the trails alongside   the river ... " as shown in Figure 1 . A complete list   of prompts is provided in Appendix B. We ﬁnally   rank each knowledge snippet kusing the likelihood   obtained from the PTLM for a concatenated input   ofkand dialog history and choose the most likely .   2.2 Non - parametric knowledge sources   External knowledge in the form of a text corpus   can be used as a non - parametric knowledge source   available at decoding time . Compared to paramet-   ric knowledge sources , such sources do not gen-   erate text as knowledge snippets , but offer the ad-   vantage of high quality and reliability of human   written text . We consider the dialog history and   theinitial response as a query to retrieve relevant   knowledge instances from the corpus . Next , we   identify the top relevant instances in the given cor-   pus with respect to the constructed query using   cosine similarity on TF - IDF based representations   ( Robertson et al . , 1995 ) .   3 Unsupervised Knowledge Injection in   Generated Dialog   Effectively utilizing the retrieved knowledge snip-   pets to construct an enriched dialog response en-   compasses two major challenges . Firstly , it is not   practical to use potentially hundreds of knowledge   snippets obtained from the retrieval step for a single   response generation . Thus , we need to ﬁnd a rele-   vant but diverse subset of the snippets . Secondly ,   the dialog modelMis trained to condition only on   the dialog context , and not on the external knowl-   edge . Hence , to leverage the knowledge snippets ,   we need a decoding strategy to rewrite the initial   responsexsuch that the resulting ﬁnal response   xshould closely follow the knowledge snippet to   be injected without a loss in the ﬂuency and con-   sistency . Thus , our method requires no additional   training and only assumes a language model trained   on dialog context ( i.e. M ) . We refer to our pro-   posed framework ( Figure 2 ) as POKI ( Post - hoc   Knowledge Injection in Generated Dialog).3.1 Relevance - Redundancy Tradeoff for   Knowledge Selection   At each turn , we obtain Nknowledge snippets from   both the parametric and non - parametric sources .   We wish to select a subset of B(out ofN ) relevant   but diverse knowledge snippets .   We deﬁne relevance score of a snippet kwith   respect to the dialog history Husing pointwise   mutual information ( PMI ) as follows :   REL= PMI(k;H ) = logp(Hjk )   p(H)   ;   Thus , a high PMI score would imply a larger se-   mantic similarity between the snippet kandH. To   account for redundancy between the snippet pair   k , kwe again use the PMI score as follows :   RED = PMI(k;k ) = logp(kjk )   p(k)   :   The redundancy score is symmetric i.e. RED=   REDasPMI is a symmetric measure .   We estimate probabilities ( both conditional and   marginal)p(:)in the above equations using GPT2   language model , following past work ( Padmaku-   mar and He , 2021 ) . The PMI measure is often con-   sidered better than other n - gram - based overlap met-   rics to measure the degree of association between   two sentences ( Kedzie et al . , 2018 ; Padmakumar   and He , 2021 ) . Semantically similar phrases oc-   cur in both sentences that can easily be ignored by   overlap based metrics .   Selection via Determinantal Point Processes .   To selectBknowledge snippets out of Nwith a   relevance - redundancy trade - off , we use a subset se-   lection process named Determinantal Point Process   ( DPP ) ( Kulesza and Taskar , 2011 ) . DPP employs a   non - uniform selection that assigns low probability   to subsets ( here , of knowledge snippets ) that are   less diverse by modeling the repulsive correlation   between independently occurring datapoints ( see   Figure 2 ) .   We build an NNkernel matrixD , which is   real , symmetric and positive semi - deﬁnite . The   diagonal entriesDare populated by the squared   relevance score of the i - th knowledge RELand   the off - diagonal entries Dare  squared re-   dundancy scores RED . We adjust  in such a   way thatDalways remains positive semi - deﬁnite   ( more details in ( Wilhelm et al . , 2018 ) ) . To select   a subset ofB , a DPP assigns a probability of sam-   pling such a subset proportional to the determinant3142of the submatrixDofD , constructed using the   indices of the subsetted items . The DPP probabil-   ity is geometrically related to the volume of the   parallelepiped spanned by the selected knowledge   snippets . Diverse knowledge snippets tend to be   orthogonal in their space hence span larger volume   ( Kulesza and Taskar , 2012 ) .   ChoosingB - size submatrix from N - sizeDis   a combinatorial problem and can become pro-   hibitively costly when Nis very high . Hence , we   use a greedy method ( Wilhelm et al . , 2018 ) where   we initialize the selection with the most relevant k   and subsequently select the next kthat maximizes   the determinant of the resultant submatrix .   3.2 Gradient - based Constrained Decoding   for Knowledge Injection   Upon selecting Bknowledge snippets , we want   to individually inject each knowledge snippet into   xto construct a candidate ﬁnal response xat   inference time .   Previous works have addressed the problem of   unsupervised modiﬁcation of already - generated   text using gradient - based decoding ( Dathathri et al . ,   2020 ; Qin et al . , 2020 ) that employs an iterative   procedure consisting of a forward and a backward   pass . The forward pass on the generative model   ( here , M ) encourages ﬂuency of the generated   text while the backward pass performs gradient   ascent on certain desired constraints . Note that   due to the discrete nature of x , it is not pos-   sible to directly update it via back - propagation .   Therefore , we maintain the sequence of hidden   representations of each output token as zfrom   the dialog model . Each output token xis re-   alized viap(x)softmax(Wz=  ) , where   is the temperature hyperparameter , Wis the out-   put embedding matrix ( shared with the input ) , and   Wz2R(Vis the size of the vocabulary ) .   Constraints . Following Majumder et al .   ( 2021a ) , we deﬁne a knowledge ﬁdelity objec-   tive that encourages xto be minimally differ-   ent from the knowledge snippet k. We achieve   this by minimizing the cross entropy loss ( CE ) be-   tween knowledge tokens k;:::;kas labels   andWz;:::;Wzas the logits .   We further notice that injected knowledge can   inﬂuence the generation in such a way that it contra-   dicts with responses uttered during previous turns .   Hence , we also want xto be entailed with the di-   alog historyH. We build an entailment classiﬁer(z;H)that predicts the probability of x(ideally ,   the hidden representation zofx ) entailingH. The   classiﬁer(z;H)is a bag - of - words classiﬁcation   layer with hidden states zfromMand ﬁne - tuned   using the DNLI dataset ( Welleck et al . , 2019 ) to   predict whether the current response is entailed   with previous responses or not .   Decoding . In the subsequent forward and back-   ward passes , the hidden representation zis gradu-   ally perturbed via gradient ascent on the respective   objectives . During backward pass , the objective   with constraints is   L(H;k;z ) =  log(z;H) CE(k;Wz )   with hyperparameters  and. We use   back - propagation to update zwith the gradient   rL(H;k;z)while the parameters of Mremain   ﬁxed . The updated latent representations of zafter   the backward pass are denoted as z.   A forward pass with Mis required to regularize   the hidden states ztoward the original dialog model   objective to obtain z. Corresponding to the t   token , the hidden states for the t+ 1time step   are computed via a weighted addition of backward   and forward hidden states , i.e. , z=   z+   ( 1    ) zwhere   2(0;1)is a hyperparameter .   During generation , we start by sampling the ini-   tial response xwith greedy decoding from M.   The hidden states z(ofx ) are iteratively updated   by alternate backward and forward passes . The ﬁ-   nal response is sampled as xsoftmax(Wz=   ) .   The number of iterations ( = 5 ) and the   (= 0:45 )   were chosen by maximizing the Z - normalized sum   of dialog model perplexity and linguistic diversity   ( % of distinct bigrams ) in a greedy hyperparameter   search . More details are in Appendix B.   3.3 Unsupervised Ranking of Candidate   Final Responses   Several previous works often over - generate and   use an additional ranking step in order to select   the ﬁnal candidate in unsupervised text generation   ( Qin et al . , 2020 ; Shwartz et al . , 2020 ; Paranjape   and Manning , 2021 ) . Similarly , here we want to   rank the generated candidate ﬁnal responses ac-   cording to the diversity of the generated text as   well as the conditional likelihood of generation   given the dialog history . For diversity , we mea-   sure the percentage of distinct bigrams present in   the response . For conditional likelihood , we use3143   the pre - trained GPT2 model to obtain the log prob-   ability when the dialog history , followed by the   generated response , passed as a concatenated input .   Since these two scores can have varied scale , we   perform Z - normalization on the individual scores   and add them to obtain a single score for ranking .   The highest ranked candidate response is ﬁnally   rendered to the user .   4 Experimental Setup   4.1 Scenarios and Datasets   We experiment with two dialog scenarios : goal-   oriented and knowledge grounded . Both setups are   knowledge intensive but the training data in such   setups often contains only a fraction of the needed   knowledge . For the goal - oriented setting , we use   the Multi - domain Wizard - of - Oz ( Budzianowski   et al . , 2018 ) dataset . For knowledge grounded dia-   log , we use the Wizard - of - Wikipedia ( Dinan et al . ,   2019b ) dataset . More details are in Appendix A.   Multi - domain Wizard - of - Oz ( MultiWOZ ) is   a multi - domain dialog dataset ( we use v2.0   ( Hosseini - Asl et al . , 2020 ) ) consisting of goal-   oriented human - human conversations . The dataset   spans seven domains ( restaurant , train , attraction ,   hotel , taxi , hospital , police ) and contains 10,438   dialogs with 13.68 average turns . Since , we do not   need any training data , we only use an evaluation   set ( of 7 K utterances ) .   Wizard - of - Wikipedia ( WoW ) is a knowledge   grounded dialog dataset which involves retrieving   relevant knowledge from Wikipedia , reading and   conditioning on it , and ﬁnally generating dialog   responses ( Dinan et al . , 2019b ) . The dataset con-   tains 201 K utterances from 22 K dialogues span-   ning 1300 diverse topics , from which we use only   the test set . The associated Wikipedia knowledge   base has 5.4 M articles and 93 M sentences.4.2 Baselines and Ablations   Baselines for MultiWOZ . For MultiWOZ , we   consider several baselines following ( Sun et al . ,   2021 ) for knowledge injection . First , we use the   current state - of - the - art model , SimpleTOD , for   goal - oriented dialog ( Hosseini - Asl et al . , 2020 ) .   Sun et al . ( 2021 ) extends SimpleTOD by adding   chitchat candidates to dialog histories during train-   ing . They also have other variants that either con-   catenate output from SimpleTOD and candidate   chitchats ( Arranger ) or rewrite by combining both   output and chitchat snippets ( Rewriter ) . We also   have a trivial baseline ( KCopy ) which appends the   retrieved knowledge snippet kfrom POKI with the   initial response x.   Baselines for WoW. For WoW , we use   two current - best knowledge - grounded models ,   KGround ( Wolf et al . , 2019 ) and BART ( Lewis   et al . , 2020a ) that concatenate the associated knowl-   edge snippets ( present in WoW ) and the dialog   history as inputs to generate the response with su-   pervision . KGuide ( Zhao et al . , 2017 ) and RAG   ( Lewis et al . , 2020b ) have an additional knowl-   edge selection step modeled by a latent variable   before response generation similar to knowledge   grounded models . We also use the KCopy baseline ,   as described for MultiWOZ .   Variants of POKI . To investigate the impact of   various decoding constraints in POKI , we consider   the following two variants of POKI — w/o Entail-   ment and w/o Knowledge ( Kw ) Fidelity ( § 3.2 ) .   InPOKI , we use SimpleTOD as the base dialog   model in goal - oriented scenarios and use BART   ( which is a state - of - the - art model for WoW ) as   the base dialog model in the knowledge - grounded   scenario . For all variants of POKI , we use gradient-   based inference for decoding the ﬁnal response.3144   5 Results and Discussion   5.1 Automatic Evaluation   Our primary goal is to generate responses enriched   with relevant external knowledge . Arguably , a   system which can effectively leverage additional   knowledge at decoding time should generate more   diverse responses . We measure percentage of dis-   tinct bigrams as Distinct-(D-2 ) ( Li et al . , 2016 ) and   geometric mean of entropy values of empirical fre-   quency distributions of n - grams ( n= 1;2;3 ) as   Entropy ( ENTR ) ( Jhamtani et al . , 2018 ) for diver-   sity . Additionally , we report overlap between gen-   erated responses and corresponding ground truth   as per BLEU and BERTScore ( BRTSc ) . For Multi-   WOZ , we also report the ﬁnal goal accuracy ( Acc )   following ( Hosseini - Asl et al . , 2020 ) .   MultiWOZ . Table 1 shows POKI outperforms   all the baselines in terms of diversity of generated   responses . More importantly , we see POKI pro-   motes accuracy of reaching the ﬁnal dialog state   i.e. the goal . For ablated versions of POKI , we   ﬁnd the entailment constraint has little effect on   diversity while dropping the knowledge adherence   constraint negatively inﬂuences accuracy and diver-   sity . All variants of SimpleTOD and all versions   ofPOKI show departure from the results obtained   by SimpleTOD on BLEU and BERTScore since   all of these versions add external knowledge that   were not explicitly present in the data . However ,   we observe that the departure is not signiﬁcant and   POKI achieves a much closer BERTScore to Sim-   pleTOD compared to baselines .   WoW. Despite all systems for WoW use knowl-   edge explicitly in the knowledge - grounded dialog   generation task , Table 2 shows POKI generates   the most diverse responses . Similar to MultiWOZ , the knowledge adherence constraint still remains   a signiﬁcant factor for increasing diversity , one of   the main goals of knowledge injection . For WoW ,   we instead see POKI outperform even BART ( pre-   vious SOTA ) in terms of BERTScore when injected   with external knowledge indicating the need of the   external knowledge for modeling WoW dialogs .   5.2 Human Evaluation   We conduct a comparative human evaluation with   300 samples to evaluate the quality of gener-   ated dialog responses following ACUTE - Eval ( Li   et al . , 2019 ) . We show a generated response from   POKI to an annotator with its associated dialog   history to annotate if knowledge injection makes   the ﬁnal response more engaging , interesting and   humanlike compared to a baseline response . As   sanity check , we also investigate if the response   remain coherent after knowledge injection . Each   sample is evaluated by two annotators .   MultiWOZ . Table 3 records the pairwise com-   parison showing POKI consistently outperforms   baselines on all criteria . Responses from POKI are   more engaging and interesting compared to Sim-   pleTOD and Rewriter , demonstrating that gradient-   based decoding is effective for knowledge injection .   InPOKI , entailment constraint mostly inﬂuences   coherence whereas knowledge ﬁdelity constraint is   important for engagingness and interestingness .   WoW. Table 3 shows POKI outperforms base-   lines that use grounding knowledge during training   in all criteria showing that external knowledge can   be useful even in the knowledge - grounded setting   to make the conversation engaging and interesting .   It also indicates the limitation of the training sig-   nal or lack of access to sufﬁcient knowledge and3145   room for improvement in terms of how knowledge   is utilized . A large gap in win percentages in favor   ofPOKI for evaluating how ‘ humanlike ’ is a re-   sponse when compared to state - of - the - art methods   suggests knowledge injection leads to more natural   conversation . Here too , both decoding constraints   show similar trends to MultiWOZ .   Qualitative Analysis . Figure 3 shows a con-   versation by POKI with a user who seeks to ﬁnd   restaurant options around Cambridge . We observe   that in most of the turns the injected knowledge ap-   peared as an additional justiﬁcation over the initial   responses making the dialog engaging and effec-   tive to reach the user ’s goal ( also noted by human   judges in § 5.3 ) . For example , in turn 3 , we observe   that adding the extra information about Indian cui-   sine helped user to reach a conclusion when their   original choice of English cuisine was absent .   Effect of Response Length . Qualitatively , as   seen in Figure 3 , responses generated by POKI are   longer than those from the initial response due to   the post - hoc knowledge injection . In the human   evaluation sample , we found that 37 % of responses   from POKI are similar or smaller in length com-   pared to responses from the best baseline . We in-   vestigate if response length acted as a confounding   factor during human evaluation . Among all the   cases where POKI waslostover a baseline , 45 %   ( 2 % when bootstrapped with 1000 subsets of size   50 ) of responses from POKI were longer than those   from the comparing baseline . Among wincases for   POKI , we observe 49 % ( 3 % when bootstrapped   with 1000 subsets of size 50 ) POKI responses were   longer than those from the comparing method . This   indicates that human users did not only choose   longer responses as better.5.3 User Study for Effectiveness of   Knowledge Injection   Relevant knowledge injection has the beneﬁt of   adding more justiﬁcation to terse dialog outputs   and hence inﬂuencing the task outcome positively .   Mirroring observations from ( Ghandeharioun et al . ,   2019 ) , a real - time full conversation evaluation is   needed to investigate if POKI could achieve the   conversational goal any better than baselines .   We recruited 60 users for this study . One half of   the users interacted with POKI , while the other half   interacted with the best baseline model that does   not augment dialog responses with external knowl-   edge . We construct a speculative goal for each user   to accomplish via the conversation . We allow users   to end the conversation any time they would like   and ask them whether the system helped them to   reach their conversation goal along with additional   comments to justify their annotation . Users who in-   teracted with a knowledge - augmented system also   asked if the system provided any knowledge that   user has not explicitly asked for but indeed the   extra information helped them to reach the conver-   sational goal ( Majumder et al . , 2021b ) . Finally ,   we also ask if they would like to engage with the   system they interacted with in future .   For goal - oriented dialog , we construct specula-   tive goals ( e.g. looking for entertainment options )   manually from the ground truth for 300 dialog   samples . Since we are not using the underlying   databases , we made sure speculative goals do not   require speciﬁc information ( e.g. booking avail-   ability , ﬂight information , etc . ) . For knowledge-   grounded dialog , we provide the intended topic of3146   discussion ( e.g. science ﬁction ) present in the data ;   the speculative goal here is to know more about , or   to have an engaging conversation about the topic .   Results . First of all , we ﬁnd that POKI is unan-   imously preferred by users compared to the base-   line during the user study . More importantly , we   see that when the user successfully accomplished   their goal , 84 % of those times they found the ad-   ditional knowledge helpful in the goal - oriented   setting ( MultiWOZ ) as compared to a baseline   ( Rewriter ) that did not use any external knowl-   edge . Most importantly , POKI takes signiﬁcantly   fewer turns for users to accomplish the goal as   compared to Rewriter implicitly indicating injected   knowledge ( we observe high correlation , 0.67 ) con-   tributes toward more efﬁcient conversations .   For the knowledge - grounded setting ( WoW ) ,   both BART and POKI have access to external   knowledge sources . However , 89 % ( compared   to 70 % ) of success scenarios were directly inﬂu-   enced by the additional post - hoc knowledge . For   knowledge - grounded dialog , a longer conversation   is indicative of engagingness on a particular topic   ( Gopalakrishnan et al . , 2019 ) , hence users preferred   to converse with POKI for more turns as compared   to a BART baseline . We quote a comment from a   user who found a conversation about the Korean   culture with POKI was particularly engaging —   “ Before this conversation , I had less knowledge   about Korean movies and art - forms . This gave   me a new perspective and a handful of popular   opinions to look at it . ” .   5.4 Discussion   Performance of Knowledge Selection . The   knowledge selection step in POKI acts an informa-   tion bottleneck where the quality of the generated   response directly depends on the quality of the   selected knowledge . We perform a human eval-   uation on 200 snippets to measure the relevance   and the factual correctness in two scenarios : when   we randomly select a retrieved snippet or select via   DPP . In Table 5 , we see that the parametric knowl-   edge source ( gpt2 - large ) generates more rel-   evant knowledge snippets than a non - parametric   one . We attribute this to 1 ) a large and diverse   dataset ( webtext ) used during pretraining of gpt2   as compared to yelp reviews ( restricted domains )   we used for retrieval , and 2 ) the limited recall of rel-   evant knowledge when using word - overlap based   retrieval . However , large language models are still   prone to generate non - factual knowledge . We ob-   serve that DPP - based selection in POKI is able   to sub - select more factual knowledge which then   positively inﬂuences the ﬁnal response quality . For   WoW , we also compare the selected snippets with   the gold knowledge available in the dataset that in   turn show high ﬁdelity in terms of BERTScore .   Time Complexity . Madotto et al . ( 2020 ) shows   that iterative gradient - based decoding could be   slower than generating response using single for-   ward pass from an existing model . When we bench-   mark POKI in an Nvidia 2080Ti GPU , in Table 6 ,   we see that knowledge generation ( or retrieval )   could be a computational bottleneck for POKI .   However the greedy selection and the constrained   decoding step do not add signiﬁcant computational   load . Furthermore , POKI ’s performance is compa-   rable with PPCM ( Madotto et al . , 2020)—a more   efﬁcient version of gradient - based decoding . The   efﬁciency of the knowledge retrieval step can be im-   proved with better indexing ( Johnson et al . , 2021 )   which we leave as a future work.31476 Related Work   Knowledge grounded dialog datasets such as   Wizard - of - Wikipedia ( Dinan et al . , 2019a ) and   Topical chat ( Gopalakrishnan et al . , 2019 ) typi-   cally consist of dialog responses paired with rel-   evant knowledge available as collected annota-   tions . Hence , models trained on such datasets   are restricted to the knowledge sources they were   exposed to at training time . Past work ( Sun   et al . , 2021 ; Majumder et al . , 2020a ; Su et al . ,   2020 ; Komeili et al . , 2021 ; Adolphs et al . , 2021 ;   Ghazvininejad et al . , 2018 ; Tuan et al . , 2020 ; Lewis   et al . , 2020c ; Guu et al . , 2020 ) has looked into in-   jecting extra knowledge sources at training time   in a bid to add knowledge not available originally   as paired to dialog responses . However , such ap-   proaches require re - training the model if some   new knowledge source were to be used . More-   over , while previous work focuses on just improv-   ing speciﬁcity of dialog response using external   knowledge , we also study the effect of additional   knowledge in achieving conversational goals .   Improving the diversity of dialog responses by   using diversity - promoting sampling has been ex-   plored in past work ( Fan et al . , 2018 ; Holtzman   et al . , 2020 ) . We use a gradient - based decoding   method , building on past work in this direction   ( Dathathri et al . , 2020 ; Qin et al . , 2020 ; Madotto   et al . , 2020 ; Majumder et al . , 2021a ) . However , we   propose new objectives to inject post - hoc knowl-   edge obtained based on already generated dialog —   an unsupervised knowledge injection method that   has not been explored so far .   7 Conclusion   We propose a framework for unsupervised knowl-   edge injection into dialog responses . We show   that knowledge can be obtained post - hoc from any   knowledge sources that can improve users ’ ability   to reach their conversational goal more effectively .   In future , our idea can be generalized to setups   where external knowledge can justify model ’s pre-   dictions such as conversational recommendation .   Acknowledgements   We thank anonymous reviewers for providing valu-   able feedback . BPM is partly supported by a Qual-   comm Innovation Fellowship , a Friends of the In-   ternational Center Fellowship – UC San Diego , NSF   Award # 1750063 , and MeetElise . References31483149A Datasets   MultiWOZ . To compare with previous works ,   we use MultiWoz 2.0 following ( Hosseini - Asl et al . ,   2020 ) . Note that we do not need any training data   for our models since we perform post - hoc knowl-   edge injection .   WoW For Wizard - of - Wikipedia , all baselines   and the original dialog model for POKI use avail-   able paired knowledge present in the training data   ( not a part of our pipeline ) . However , POKI addi-   tionally uses the external knowledge snippets se-   lected via DPP .   B Implementation Details   We open - source our code at : . We use the publicly avail-   able implementationfor DPP ( Gautier et al . ,   2019 ) .   We obtain the MultiWOZ 2.0 from the ofﬁcial   release . Similarly , we obtain the Wizard - of-   Wikipedia from ParlAI repository . We adapted   codes from original PPLM ( Dathathri et al . , 2020 )   repositoryand modiﬁed them for our own objec-   tive function . We obtained the Yelp review dataset   from the ofﬁcial website . Yelp dataset contains   8,635,403 reviews . For diversity calculation ( in   automatic evaluation ) , we use NLTKto extract   n - grams .   Network architecture For MultiWOZ , we use   the SimpleTODas the base model . Whereas   for WoW , we use BARTas the base model .   For the parametric knowledge source , we use   gpt2 - large .   Hyperparameters POKI does not require any   training since we perform gradient - based decod-   ing at the inference time . For hyperparameters   involved in the decoding stage , we maximize the3150Z - normalized sum of dialog model perplexity and   linguistic diversity ( % of distinct bigrams ) of the   generated response in a greedy fashion to select   the best values . For our best method , in objective   functionL , we use  as 1 andas 1 . We keep   generation length to be 100 to encourage longer   generations . We train the entailment classiﬁer us-   ing code from PPLM repository . The weight   for mixing forward and backward passes was set to   0.45 . We run 5 backward - forward passes to obtain   a candidate ﬁnal response .   Filtering knowledge candidates from PTLMs   Our initial experiments suggests that that knowl-   edge generated from PTLMs can be inappropri-   ate ( contains bias or toxic content ) and mislead-   ing / nonfactual . Sun et al . ( 2021 ) collected annota-   tions of dialog responses with labels positive   ( useful , social ) , negative ( inappropriate and   misleading ) . We learn a binary classiﬁer to classify   a knowledge snippet as positive or negative and use   it as a ﬁltering criteria .   Key - phrase extraction Given a sentence from   the context , we ﬁrst extract n - gram ( n 21,2,3,4 )   key - phrases using YAKE ( Yet - Another - Keyword-   Extractor ) ( Campos et al . , 2020 ) and retain only   those that contain at least a noun .   Prompts We curated prompts inspired by various   knowledge - seeking situations ( such as for : more   information , opinion , review ) ( Shwartz et al . , 2020 )   and are listed in Table 7 .   Statistics on generated and selected knowledge   snippets For both datasets , we retrieve 100 most   relevant knowledge snippets from non - parametric   source ( here , yelp reviews ) , and generate 5 candi-   date knowledge snippets ( using nucleus sampling(Holtzman et al . , 2020 ) , p= 0:95 ) for each key-   phrase extracted from an input instance ( dialog   history + initial response ) . After knowledge selec-   tion by DPP , on an average ( over validation set ) , 5   snippets were selected for MultiWoz and 8 snippets   were selected for WoW.   C Human Evaluation and User Study   Setup   Human Evaluation We hired two Anglophone   ( Lifetime HIT acceptance % > 85 ) annotators for   every test sample . Figure 4 shows a sample ques-   tion for the pairwise comparison between response   generated by POKI and a baseline for informative-   ness . The exact formulations for all criteria are   provided as below :   •Coherent : Which version is more consistent   with the dialog history ?   •Engaging : Which version is more likely to   hold your attention and make you want to   hear more ?   •Interesting : Which version arouses your cu-   riosity or tells you something new or useful ?   •Humanlike : Which version is more natural   and personable ?   All differences in values from human evaluations   are signiﬁcant with p<0:05from bootstrap tests   on 1000 subsets of size 50 . A snapshot of our   human evaluation interface is shown in Figure 4 .   The order of two candidate responses ( R1 and R2 )   is made random for each question .   User Study For user study , we similarly re-   cruited 60 Anglophone users who have at least   high - school level of education and are comfortable   with handling internet - based technologies . Each   session ( depending on the systems they interacted )   lasted on an average 30 minutes ( for MultiWOZ )   and 60 minutes ( for WoW ) including on - boarding ,   performing actual task and answering post - task   questions .   D Qualitative Examples   Figure 5 shows a complete dialog in the knowledge-   grounded scenario where the user discusses about   ‘ science-ﬁction ’ . Figure 6 shows more utter-   ance level examples for both goal - oriented and   knowledge - grounded scenarios.3151   Ethical considerations   We do not foresee any immediate ethical concerns   for our method as we use several constraints ( less   divergence from the extracted knowledge , consis-   tency with the dialog context ) that allow the gen-   eration to be restricted to the context . In general ,   we expect our dialog system to be more engaging   and accessible to the user . Since we use PTLMs   as knowledge source , we inherit the general risk of   generating biased or toxic language , which should   be carefully ﬁltered . In our work , we perform ex-   plicit ﬁltering steps to make sure that the knowl-   edge is appropriate . Furthermore , our selection   step promotes more factually correct knowledge to   be selected . However , the generations may incor-   porate biases that are already present in the dialog   datasets due to crowd - sourced data collection . Fi-   nally , our generations are limited only to the En-   glish language . Hence we suggest that a system like   ours should likely not be used as a ‘ black box , ’ but   would best be used in a setting where its outputs can   be ‘ audited ’ . Carbon footprint : Our system uses   post - hoc knowledge injection which refrains from   retraining newer dialog models to accommodate   dynamically evolving external knowledge . This   promotes green NLP applications ( Schwartz et al . ,   2020 ; Strubell et al . , 2019 ) reducing carbon foot-   prints that stem from training ( or even ﬁnetuning )   large language models.31523153