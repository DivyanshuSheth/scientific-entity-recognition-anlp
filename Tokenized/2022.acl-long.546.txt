  Xiangpeng Wei , Heng Yu , Yue Hu , Rongxiang Weng ,   Weihua Luo , Rong JinMachine Intelligence Technology Lab , Alibaba DAMO Academy , Hangzhou , ChinaInstitute of Information Engineering , Chinese Academy of Sciences , Beijing , ChinaSchool of Cyber Security , University of Chinese Academy of Sciences , Beijing , China   pemywei@gmail.com   https://github.com/pemywei/csanmt   Abstract   The principal task in supervised neural ma-   chine translation ( NMT ) is to learn to gener-   ate target sentences conditioned on the source   inputs from a set of parallel sentence pairs ,   and thus produce a model capable of gener-   alizing to unseen instances . However , it is com-   monly observed that the generalization perfor-   mance of the model is highly influenced by the   amount of parallel data used in training . Al-   though data augmentation is widely used to   enrich the training data , conventional meth-   ods with discrete manipulations fail to gener-   ate diverse and faithful training samples . In   this paper , we present a novel data augmen-   tation paradigm termed Continuous Semantic   Augmentation ( CNMT ) , which augments   each training instance with an adjacency se-   mantic region that could cover adequate vari-   ants of literal expression under the same mean-   ing . We conduct extensive experiments on   both rich - resource and low - resource settings   involving various language pairs , including   WMT14 English →{German , French } , NIST   Chinese →English and multiple low - resource   IWSLT translation tasks . The provided empiri-   cal evidences show that CNMT sets a new   level of performance among existing augmenta-   tion techniques , improving on the state - of - the-   art by a large margin .   1 Introduction   Neural machine translation ( NMT ) is one of   the core topics in natural language processing ,   which aims to generate sequences of words in   the target language conditioned on the source in-   puts ( Sutskever et al . , 2014 ; Cho et al . , 2014 ; Wu   et al . , 2016 ; Vaswani et al . , 2017 ) . In the common   supervised setting , the training objective is to learn   a transformation from the source space to the target   spaceX 7→ Y : f(y|x ; Θ)with the usage of paral-   lel data . In this way , NMT models are expected tobe capable of generalizing to unseen instances with   the help of large scale training data , which poses a   big challenge for scenarios with limited resources .   To address this problem , various methods have   been developed to leverage abundant unlabeled   data for augmenting limited labeled data ( Sen-   nrich et al . , 2016a ; Cheng et al . , 2016 ; He et al . ,   2016 ; Hoang et al . , 2018 ; Edunov et al . , 2018 ; He   et al . , 2020 ; Song et al . , 2019 ) . For example , back-   translation ( BT ) ( Sennrich et al . , 2016a ) makes use   of the monolingual data on the target side to syn-   thesize large scale pseudo parallel data , which is   further combined with the real parallel corpus in   machine translation task . Another line of research   is to introduce adversarial inputs to improve the   generalization of NMT models towards small per-   turbations ( Iyyer et al . , 2015 ; Fadaee et al . , 2017 ;   Wang et al . , 2018 ; Cheng et al . , 2018 ; Gao et al . ,   2019 ) . While these methods lead to significant   boosts in translation quality , we argue that aug-   menting the observed training data in the discrete   space inherently has two major limitations .   First , augmented training instances in discrete   space are lack diversity . We still take BT as an   example , it typically uses beam search ( Sennrich   et al . , 2016a ) or greedy search ( Lample et al . ,   2018a , c ) to generate synthetic source sentences   for each target monolingual sentence . The above   two search strategies are approximate algorithms   to identify the maximum a - posteriori ( MAP ) out-   put ( Edunov et al . , 2018 ) , and thus favor the most   frequent one in case of ambiguity . Edunov et al .   ( 2018 ) proposed a sampling strategy from the out-   put distribution to alleviate this issue , but this   method typically yields synthesized data with low   quality . While some extensions ( Wang et al . , 2018 ;   Imamura et al . , 2018 ; Khayrallah et al . , 2020 ;   Nguyen et al . , 2020 ) augment each training in-   stance with multiple literal forms , they still fail to   cover adequate variants under the same meaning .   Second , it is difficult for augmented texts in dis-7930crete space to preserve their original meanings . In   the context of natural language processing , discrete   manipulations such as adds , drops , reorders , and/or   replaces words in the original sentences often result   in significant changes in semantics . To address this   issue , Gao et al . ( 2019 ) and Cheng et al . ( 2020 )   instead replace words with other words that are   predicted using language model under the same   context , by interpolating their embeddings . Al-   though being effective , these techniques are lim-   ited to word - level manipulation and are unable to   perform the whole sentence transformation , such   as producing another sentence by rephrasing the   original one so that they have the same meaning .   In this paper , we propose Continuous Semantic   Augmentation ( CNMT ) , a novel data aug-   mentation paradigm for NMT , to alleviate both   limitations mentioned above . The principle of   CNMT is to produce diverse training data from   a semantically - preserved continuous space . Specif-   ically , ( 1 ) we first train a semantic encoder via a   tangential contrast , which encourages each training   instance to support an adjacency semantic region   in continuous space and treats the tangent points of   the region as the critical states of semantic equiva-   lence . This is motivated by the intriguing observa-   tion made by recent work showing that the vectors   in continuous space can easily cover adequate vari-   ants under the same meaning ( Wei et al . , 2020a ) .   ( 2 ) We then introduce a Mixed Gaussian Recurrent   Chain ( M ) algorithm to sample a cluster of   vectors from the adjacency semantic region . ( 3 )   Each of the sampled vectors is finally incorporated   into the decoder by developing a broadcasting inte-   gration network , which is agnostic to model archi-   tectures . As a consequence , transforming discrete   sentences into the continuous space can effectively   augment the training data space and thus improve   the generalization capability of NMT models .   We evaluate our framework on a variety of ma-   chine translation tasks , including WMT14 English-   German / French , NIST Chinese - English and multi-   ple IWSLT tasks . Specifically , CNMT sets the   new state of the art among existing augmentation   techniques on the WMT14 English - German task   with 30.94 BLEU score . In addition , our approach   could achieve comparable performance with the   baseline model with the usage of only 25 % of   training data . This reveals that CNMT has great   potential to achieve good results with very few data .   Furthermore , CNMT demonstrates consistentimprovements over strong baselines in low resource   scenarios , such as IWSLT14 English - German and   IWSLT17 English - French .   2 Framework   Problem Definition Supposing XandYare   two data spaces that cover all possible sequences   of words in source and target languages , respec-   tively . We denote ( x , y)∈(X , Y)as a pair   of two sentences with the same meaning , where   x={x , x , ... , x}is the source sentence with   Ttokens , and y={y , y , ... , y}is the tar-   get sentence with Ttokens . A sequence - to-   sequence model is usually applied to neural ma-   chine translation , which aims to learn a transfor-   mation from the source space to the target space   X 7→ Y : f(y|x ; Θ)with the usage of parallel data .   Formally , given a set of observed sentence pairs   C={(x , y ) } , the training objective is to   maximize the log - likelihood :   The log - probability is typically decomposed   as : logP(y|x ; Θ ) = PlogP(y|y , x ; Θ ) ,   where Θis a set of trainable parameters and yis   a partial sequence before time - step t.   However , there is a major problem in the com-   mon supervised setting for neural machine transla-   tion , that is the number of training instances is very   limited because of the cost in acquiring parallel   data . This makes it difficult to learn an NMT model   generalized well to unseen instances . Traditional   data augmentation methods generate more training   samples by applying discrete manipulations to un-   labeled ( or labeled ) data , such as back - translation   or randomly replacing a word with another one ,   which usually suffer from the problems of seman-   tic deviation and the lack of diversity .   2.1 Continuous Semantic Augmentation   We propose a novel data augmentation paradigm   for neural machine translation , termed continuous   semantic augmentation ( CNMT ) , to better gen-   eralize the model ’s capability to unseen instances .   We adopt the Transformer ( Vaswani et al . , 2017 )   model as a backbone , and the framework is shown   in Figure 1 . In this architecture , an extra semantic   encoder translates the source xand the target sen-   tence yto real - value vectors r = ψ(x ; Θ)and   r = ψ(y ; Θ)respectively , where ψ ( · ; Θ)is the   forward function of the semantic encoder parame-   terized by Θ(parameters other than Θ).7931   Definition 1 . There is a universal semantic space   among the source and the target languages for   neural machine translation , which is established   by a semantic encoder . It defines a forward function   ψ ( · ; Θ)to map discrete sentences into continuous   vectors , that satisfies : ∀(x , y)∈(X , Y ) : r = r.   Besides , an adjacency semantic region ν(r , r)in   the semantic space describes adequate variants of   literal expression centered around each observed   sentence pair ( x , y ) .   In our scenario , we first sample a series of vec-   tors ( denoted by R ) from the adjacency semantic   region to augment the current training instance ,   that is R={ˆr,ˆr , ... , ˆr},where ˆ r∼   ν(r , r).Kis the hyperparameter that determines   the number of sampled vectors . Each sample ˆris   then integrated into the generation process through   a broadcasting integration network :   where ois the output of the self - attention module   at position t. Finally , the training objective in Eq .   ( 1 ) can be improved as   By augmenting the training instance ( x , y)with di-   verse samples from the adjacency semantic region ,   the model is expected to generalize to more unseen   instances . To this end , we must consider such two   problems : ( 1 ) How to optimize the semantic en-   coder so that it produces a meaningful adjacency   semantic region for each observed training pair .    ( 2)How to obtain samples from the adjacency   semantic region in an efficient and effective way .   In the rest part of this section , we introduce the   resolutions of these two problems , respectively .   Tangential Contrastive Learning We start from   analyzing the geometric interpretation of adjacency   semantic regions . The schematic diagram is illus-   trated in Figure 2 . Let ( x , y)and(x , y )   are two instances randomly sampled from the train-   ing corpora . For ( x , y ) , the adjacency seman-   tic region ν(r , r)is defined as the union of   two closed balls that are centered by rand   r , respectively . The radius of both balls is   d=∥r−r∥ , which is also considered   as a slack variable for determining semantic equiv-   alence . The underlying interpretation is that vectors   whose distances from r(orr ) do not exceed   d , are semantically - equivalent to both rand   r. To make ν(r , r)conform to the inter-   pretation , we employ a similar method as in ( Zheng   et al . , 2019 ; Wei et al . , 2021 ) to optimize the se-   mantic encoder with the tangential contrast .   Specifically , we construct negative samples by   applying the convex interpolation between the cur-   rent instance and other ones in the same training   batch for instance comparison . And the tangent   points ( i.e. , the points on the boundary ) are consid-   ered as the critical states of semantic equivalence .   The training objective is formulated as :   where Bindicates a batch of sentence pairs ran-   domly selected from the training corpora C , and   s(·)is the score function that computes the cosine   similarity between two vectors . The negative sam-   plesrandrare designed as the following7932   interpolation :   where d=∥r−r∥andd=∥r−   r∥. The two equations in Eq . ( 5 ) set up when   danddare larger than drespectively , or else   r = randr = r. According to   this design , an adjacency semantic region for the   i - th training instance can be fully established by   interpolating various instances in the same training   batch . We follow Wei et al . ( 2021 ) to adaptively   adjust the value of λ(orλ ) during the training   process , and refer to the original paper for details .   M Sampling To obtain augmented data   from the adjacency semantic region for the training   instance ( x , y ) , we introduce a Mixed Gaussian   Recurrent Chain ( denoted by M ) algorithm   to design an efficient and effective sampling strat-   egy . As illustrated in Figure 3 , we first transform   the bias vector ˜r = r−raccording to a pre-   defined scale vector ω , that is ω⊙˜r , where ⊙   is the element - wise product operation . Then , we   construct a novel sample ˆr = r+ω⊙˜rfor aug-   menting the current instance , in which ris either r   orr . As a consequence , the goal of the sampling   strategy turns into find a set of scale vectors , i.e.   { ω , ω , ... , ω } . Intuitively , we can assume   thatωfollows a distribution with universal or Gaus-   sian forms , despite the latter demonstrates better   results in our experience . Formally , we design aAlgorithm 1 M Sampling   mixed Gaussian distribution as follow :   This framework unifies the recurrent chain and the   rejection sampling mechanism . Concretely , we first   normalize the importance of each dimension in ˜ras   W= , the operation |·|takes the   absolute value of each element in the vector , which   means the larger the value of an element is the   more informative it is . Thus N(0,diag(W))lim-   its the range of sampling to a subspace of the adja-   cency semantic region , and rejects to conduct sam-   pling from the uninformative dimensions . More-   over , N(Pω,1)simulates a recurrent   chain that generates a sequence of reasonable vec-   tors where the current one is dependent on the prior   vectors . The reason for this design is that we ex-   pect that pin Eq . ( 6 ) can become a stationary   distribution with the increase of the number of sam-   ples , which describes the fact that the diversity of   each training instance is not infinite . ηis a hyper-   parameter to balance the importance of the above   two Gaussian forms . For a clearer presentation ,   Algorithm 1 summarizes the sampling process .   2.2 Training and Inference   The training objective in our approach is a combi-   nation of J(Θ)in Eq . ( 3 ) and J(Θ)in Eq .   ( 4 ) . In practice , we introduce a two - phase train-   ing procedure with mini - batch losses . Firstly , we   train the semantic encoder from scratch using the   task - specific data , i.e. Θ= argmaxJ(Θ).7933   Secondly , we optimize the encoder - decoder model   by maximizing the log - likelihood , i.e. Θ=   argmaxJ(Θ ) , and fine - tune the semantic en-   coder with a small learning rate at the same time .   During inference , the sequence of target words   is generated auto - regressively , which is almost the   same as the vanilla Transformer ( Vaswani et al . ,   2017 ) . A major difference is that our method in-   volves the semantic vector of the input sequence   for generation : y= argmaxP(·|y , x , r ; Θ ) ,   where r = ψ(x ; Θ ) . This module is plug - in - use   as well as is agnostic to model architectures .   3 Experiments   We first apply CNMT to NIST Chinese - English   ( Zh→En ) , WMT14 English - German ( En →De ) and   English - French ( En →Fr ) tasks , and conduct exten-   sive analyses for better understanding the proposed   method . And then we generalize the capability of   our method to low - resource IWSLT tasks .   3.1 Settings   Datasets . For the Zh →En task , the LDC corpus is   taken into consideration , which consists of 1.25 M   sentence pairs with 27.9 M Chinese words and   34.5 M English words , respectively . The NIST 2006   dataset is used as the validation set for selecting the   best model , and NIST 2002 ( MT02 ) , 2003 ( MT03 ) ,   2004 ( MT04 ) , 2005 ( MT05 ) , 2008 ( MT08 ) are   used as the test sets . For the En →De task , we em-   ploy the popular WMT14 dataset , which consists   of approximately 4.5 M sentence pairs for train-   ing . We select newstest2013 as the valida-   tion set and newstest2014 as the test set . For   the En →Fr task , we use the significantly larger   WMT14 dataset consisting of 36 M sentence pairs .   The combination of { newstest2012 , 2013 }   was used for model selection and the experimental   results were reported on newstest2014 . RefertoAppendix A for more details .   Training Details . We implement our approach   on top of the Transformer ( Vaswani et al . , 2017 ) .   The semantic encoder is a 4 - layer transformer   encoder with the same hidden size as the back-   bone model . Following sentence - bert ( Reimers   and Gurevych , 2019 ) , we average the outputs   of all positions as the sequence - level represen-   tation . The learning rate for finetuning the se-   mantic encoder at the second training stage is set   as1e−5 . All experiments are performed on 8   V100 GPUs . We accumulate the gradient of 8 it-   erations and update the models with a batch of   about 65 K tokens . The hyperparameters Kand   ηinM sampling are tuned on the validation   set with the range of K∈ { 10,20,40,80}and   η∈ { 0.15,0.30,0.45,0.6,0.75,0.90 } . We use the   default setup of K= 40 for all three tasks , η= 0.6   for both Zh →En and En →De while η= 0.45for   En→Fr . For evaluation , the beam size and length   penalty are set to 4 and 0.6 for the En →De as well   as En→Fr , while 5 and 1.0 for the Zh →En task .   3.2 Main Results   Results of Zh →En . Table 1 shows the results on   the Chinese - to - English translation task . From the   results , we can conclude that our approach out-   performs existing augmentation strategies such as   back - translation ( Sennrich et al . , 2016a ; Wei et al . ,   2020a ) and switchout ( Wang et al . , 2018 ) by a   large margin ( up to 3.63 BLEU ) , which verifies   that augmentation in continuous space is more ef-   fective than methods with discrete manipulations .   Compared to the approaches that replace words in   the embedding space ( Cheng et al . , 2020 ) , our ap-   proach also demonstrates superior performance ,   which reveals that sentence - level augmentation   with continuous semantics works better on general-   izing to unseen instances . Moreover , compared to   the vanilla Transformer , our approach consistently7934   achieves promising improvements on five test sets .   Results of En →De and En →Fr . From Table 2 ,   our approach consistently performs better than   existing methods ( Sennrich et al . , 2016a ; Wang   et al . , 2018 ; Wei et al . , 2020a ; Cheng et al . , 2020 ) ,   yielding significant gains ( 0.65 ∼1.76 BLEU ) on   the En →De and En →Fr tasks . An exception is   that Nguyen et al . ( 2020 ) achieved comparable re-   sults with ours via multiple forward and backward   NMT models , thus data diversification intuitively   demonstrates lower training efficiency . Moreover ,   we observe that CNMT gives 30.16 BLEU on   the En →De task with the base setting , signifi-   cantly outperforming the vanilla Transformer by   2.49 BLEU points . Our approach yields a further   improvement of 0.68 BLEU by equipped with the   wider architecture , demonstrating superiority over   the standard Transformer by 2.15 BLEU . Similar   observations can be drawn for the En →Fr task .   3.3 Analysis   Effects of Kandη . Figure 4 illustrates how the   hyper - parameters KandηinM sampling af-   fect the translation quality . From Figures 4(a)-4(c),we can observe that gradually increasing the num-   ber of samples significantly improves BLEU scores ,   which demonstrates large gaps between K= 10   andK= 40 . However , assigning larger values   ( e.g. , 80 ) toKdoes not result in further improve-   ments among all three tasks . We conjecture that the   reasons are two folds : ( 1 ) it is fact that the diversity   of each training instance is not infinite and thus   M gets saturated is inevitable with Kincreas-   ing . ( 2 ) M sampling with a scaled item ( i.e. ,   W ) may degenerate to traverse in the same place .   This prompts us to design more sophisticated al-   gorithms in future work . In our experiments , we   default set K= 40 to achieve a balance between   the training efficiency and translation quality . Fig-   ure 4(d ) shows the effect of ηon validation sets ,   which balances the importance of two Gaussian   forms during the sampling process . The setting   ofη= 0.6achieves the best results on both the   Zh→En and En →De tasks , and η= 0.45consis-   tently outperforms other values on the En →Fr task .   Lexical diversity and semantic faithfulness .   We demonstrate both the lexical diversity ( mea-   sured by TTR =) of various trans-7935   lations and the semantic faithfulness of machine   translated ones ( measured by BLEURT with con-   sidering human translations as the references ) in Ta-   ble 4 . It is clear that CNMT substantially bridge   the gap of the lexical diversity between transla-   tions produced by human and machine . Meanwhile ,   CNMT shows a better capability on preserving   the semantics of the generated translations than   Transformer . We intuitively attribute the signifi-   cantly increases of BLEU scores on all datasets   to these two factors . We also have studied the ro-   bustness of CNMT towards noisy inputs and the   translationese effect , see Appendix D for details .   Effect of the semantic encoder . We introduce   two variants of the semantic encoder to investigate   its performance on En →De validation set . Specif-   ically , ( 1 ) we remove the extra semantic encoder   and construct the sentence - level representations by   averaging the sequence of outputs of the vanilla sen-   tence encoder . ( 2 ) We replace the default 4 - layer   semantic encoder with a large pre - trained model   ( PTM ) ( i.e. , XLM - R ( Conneau et al . , 2020 ) ) . The   results are reported in Table 3 . Comparing line 2   with line 3 , we can conclude that an extra semantic   encoder is necessary for constructing the univer-   sal continuous space among different languages .   Moreover , when the large PTM is incorporated ,   our approach yields further improvements , but it   causes massive computational overhead .   Comparison between discrete and continu-   ous augmentations . To conduct detailed compar-   isons between different augmentation methods , we   asymptotically increase the training data to analyze   the performance of them on the En →De transla-   tion . As in Figure 5 , our approach significantly   outperforms the back - translation method on each   subset , whether or not extra monolingual data ( Sen-   nrich et al . , 2016a ) is introduced . These results   demonstrate the stronger ability of our approach   than discrete augmentation methods on generaliz-   ing to unseen instances with the same set of ob-   served data points . Encouragingly , our approach   achieves comparable performance with the base-   line model with only 25 % of training data , which   indicates that our approach has great potential to   achieve good results with very few data .   Effect of M sampling and tangential con-   trastive learning . To better understand the effec-   tiveness of the M sampling and the tangential   contrastive learning , we conduct detailed ablation   studies in Table 5 . The details of four variants   with different objectives or sampling strategies are   shown in Appendix C . From the results , we can ob-   serve that both removing the recurrent dependence   and replacing the Gaussian forms with uniform dis-   tributions make the translation quality decline , but   the former demonstrates more drops . We also have   tried the training objectives with other forms , such   as variational inference and cosine similarity , to op-   timize the semantic encoder . However , the BLEU   score drops significantly .   Training Cost and Convergence . Figure 67936   shows the evolution of BLEU scores during train-   ing . It is obvious that our method performs consis-   tently better than both the vanilla Transformer and   the back - translation method at each iteration ( ex-   cept for the first 10 K warm - up iterations , where   the former one has access to less unique train-   ing data than the latter two due to the Ktimes   over - sampling ) . For the vanilla Transformer , the   BLEU score reaches its peak at about 52 K iter-   ations . In comparison , both CNMT and the   back - translation method require 75 K updates for   convergence . In other words , CNMT spends   44 % more training costs than the vanilla Trans-   former , due to the longer time to make the NMT   model converge with augmented training instances .   This is the same as the back - translation method .   Word prediction accuracy . Figure 7 illustrates   the prediction accuracy of both frequent and rare   words . As expected , CNMT generalizes to rare   words better than the vanilla Transformer , and the   gap of word prediction accuracy is as large as 16 % .   This indicates that the NMT model alleviates the   probability under - estimation of rare words via con-   tinuous semantic augmentation .   Effects of Additional Parameters and Strong   Baselines . In contrast to the vanilla Transformer ,   CNMT involves with approximate 20 % addi-   tional parameters . In this section , we further com-   pare against the baselines with increased amounts   of parameters , and investigate the performance of   CNMT equipped with much stronger baselines   ( e.g. deep and scale Transformers ( Ott et al . , 2018 ;   Wang et al . , 2019 ; Wei et al . , 2020b ) ) . From the   results on WMT14 testsets in Table 6 , we can ob-   serve that CNMT still outperforms the vanilla   Transformer ( by more than 1.2 BLEU ) under the   same amount of parameters , which shows that the   additional parameters are not the key to the im-   provement . Moreover , CNMT yields at least   0.9 BLEU gains equipped with much stronger base-   lines . For example , the scale Transformer ( Ott   et al . , 2018 ) , which originally gives 29.3 BLEU   in the En →De task , now gives 31.37 BLEU with   our continuous semantic augmentation strategy . It   is important to mention that our method can help   models to achieve further improvement , even if   they are strong enough .   3.4 Low - Resource Machine Translation   We further generalize the capability of the proposed   CNMT to various low - resource machine trans-   lation tasks , including IWSLT14 English - German   and IWSLT17 English - French . The details of the   datasets and model configurations can be found in   Appendix B . Table 7 shows the results of different   models . Compared to the vanilla Transformer , the   proposed CNMT improve the BLEU scores of   the two tasks by 2.7 and 2.9 points , respectively.7937   This result indicates that the claiming of the contin-   uous semantic augmentation enriching the training   corpora with very limited observed instances .   4 Related Work   Data Augmentation ( DA ) ( Edunov et al . , 2018 ;   Kobayashi , 2018 ; Gao et al . , 2019 ; Khayrallah   et al . , 2020 ; Pham et al . , 2021 ) has been widely   used in neural machine translation . The most popu-   lar one is the family of back - translation ( Sennrich   et al . , 2016a ; Nguyen et al . , 2020 ) , which utilizes   a target - to - source model to translate monolingual   target sentences back into the source language . Be-   sides , constructing adversarial training instances   with diverse literal forms via word replacing or em-   bedding interpolating ( Wang et al . , 2018 ; Cheng   et al . , 2020 ) is beneficial to improve the generaliza-   tion performance of NMT models .   Vicinal Risk Minimization ( VRM ) ( Chapelle   et al . , 2000 ) is another principle of data augmen-   tation , in which DA is formalized as extracting   additional pseudo samples from the vicinal distri-   bution of observed instances . Typically the vicin-   ity of a training example is defined using dataset-   dependent heuristics , such as color ( scale , mixup )   augmentation ( Simonyan and Zisserman , 2014 ;   Krizhevsky et al . , 2012 ; Zhang et al . , 2018 ) in   computer vision and adversarial augmentation with   manifold neighborhoods ( Ng et al . , 2020 ; Cheng   et al . , 2021 ) in NLP . Our approach relates to VRM   that involves with an adjacency semantic region as   the vicinity manifold for each training instance .   Sentence Representation Learning is a well in-   vestigated area with dozens of methods ( Kiros et al . ,   2015 ; Cer et al . , 2018 ; Yang et al . , 2018 ) . In recent   years , the methods built on large pre - trained mod-   els ( Devlin et al . , 2019 ; Conneau et al . , 2020 ) have   been widely used for learning sentence level repre-   sentations ( Reimers and Gurevych , 2019 ; Huang   et al . , 2019 ; Yang et al . , 2019 ) . Our work is also   related to the methods that aims at learning the uni - versal representation ( Zhang et al . , 2016 ; Schwenk   and Douze , 2017 ; Yang et al . , 2021 ) for multiple   semantically - equivalent sentences in NMT . In this   context , contrastive learning has become a popular   paradigm in NLP ( Kong et al . , 2020 ; Clark et al . ,   2020 ; Gao et al . , 2021 ) . The most related work are   Wei et al . ( 2021 ) and Chi et al . ( 2021 ) , which sug-   gested transforming cross - lingual sentences into a   shared vector by contrastive objectives .   5 Conclusion   We propose a novel data augmentation paradigm   CNMT , which involves with an adjacency se-   mantic region as the vicinity manifold for each   training instance . This method is expected to   make more unseen instances under generalization   with very limited training data . The main com-   ponents of CNMT consists of the tangential   contrastive learning and the Mixed Gaussian Re-   current Chain ( M ) sampling . Experiments on   both rich- and low - resource machine translation   tasks demonstrate the effectiveness of our method .   In the future work , we would like to further study   the vicinal risk minimization with the combination   of multi - lingual aligned scenarios and large - scale   monolingual data , and development it as a pure data   augmentator merged into the vanilla Transformer .   Acknowledgments   We would like to thank all of the anonymous re-   viewers ( during ARR Oct. and ARR Dec. ) for the   helpful comments . We also thank Baosong Yang   and Dayiheng Liu for their instructive suggestions   and invaluable help .   References7938793979407941   A Details of Rich - Resource Datasets   For the Zh →En task , the LDC corpusis taken into   consideration , which consists of 1.25 M sentence   pairs with 27.9 M Chinese words and 34.5 M En-   glish words , respectively . The NIST 2006 dataset   is used as the validation set for selecting the best   model , and NIST 2002 ( MT02 ) , 2003 ( MT03 ) ,   2004 ( MT04 ) , 2005 ( MT05 ) , 2008 ( MT08 ) are   used as the test sets . We created shared BPE ( byte-   pair - encoding ( Sennrich et al . , 2016b ) ) codes with   60 K merge operations to build two vocabularies   comprising 47 K Chinese sub - words and 30 K En-   glish sub - words . For the En →De task , we employ   the popular WMT14 dataset , which consists of ap-   proximately 4.5 M sentence pairs for training . We   select newstest2013 as the validation set and   newstest2014 as the test set . All sentences had   been jointly byte - pair - encoded with 32 K merge op-   erations , which results in a shared source - target vo-   cabulary of about 37 K tokens . For the En →Fr task ,   we use the significantly larger WMT14 dataset   consisting of 36 M sentence pairs . The combina-   tion of { newstest2012 , 2013 } was used for   model selection and the experimental results were   reported on newstest2014 .We use the Stanford segmenter ( Tseng et al . ,   2005 ) for Chinese word segmentation and apply   the script tokenizer.pl of Moses ( Koehn et al . ,   2007 ) for English , German and French tokeniza-   tion . We measure the performance with the 4-   gram BLEU score ( Papineni et al . , 2002 ) . Both   the case - sensitive tokenized BLEU ( compued by   multi-bleu.pl ) and the detokenized sacre-   bleu(Post , 2018 ) are reported on the En →De and   En→Fr tasks . The case - insensitive BLEU is re-   ported on the Zh →En task .   B Low - Resource Machine Translation   For the low - resource scenario , we choose   the IWSLT14 English - German ( En →De ) and   IWSLT17 English - French ( En →Fr ) tasks .   Datasets . For IWSLT14 En →De , there are 160k   sentence pairs for training and 7584 sentence pairs   for validation . As in previous work ( Ranzato   et al . , 2016 ; Zhu et al . , 2020 ) , the concatenation of   dev2010 , dev2012 , test2010 , test2011 and test2012   is used as the test set . For IWSLT17 En →Fr , there   are236ksentence pairs for training and 10263 for   validation . The concatenation of test2010 , test2011 ,   test2012 , test2013 , test2014 and test2015 is used as   the test set . We use a joint source and target vocabu-   lary with 10kbyte - pair - encoding ( BPE ) types ( Sen-   nrich et al . , 2016b ) for above two tasks .   Model Settings . The model configuration is   transformer_iwslt , representing a 6 - layer   model with embedding size 512 and FFN layer   dimension 1024 . We train all models using the   Adam optimizer with adaptive learning rate sched-   ule ( warm - up step with 4 K ) as in ( Vaswani et al . ,   2017 ) . During inference , we use beam search with   a beam size of 5and length penalty of 1.0 .   C Variants with Different Objectives or   Sampling Strategies   Table 8 describes the details of four variants ( intro-   duced in Table 5 , from row 2 to row 5 ) with differ-   ent objectives or sampling strategies : ( 1 ) default   tangential CTL in Eq . ( 4 ) + M w/o recurrent   dependence , ( 2 ) default tangential CTL in Eq . ( 4 ) +   M w uniform distribution , ( 3 ) variational infer-   ence ( Zhang et al . , 2016 ) + Gaussian sampling , and   ( 4 ) cosine similarity + default M sampling.7942   D Robustness on Noisy Inputs and   Translationese   In this section , we study the robustness of our   CNMT towards both noisy inputs and the trans-   lationese effect ( V olansky et al . , 2013 ) on new-   stest2014 for the WMT14 English - German task .   Noisy Inputs . Inspired by ( Gao et al . , 2019 ) , we   construct noisy test sets via several strategies de-   scribed as follows :   •Original : the original testset without any ma-   nipulations ;   •WS : word swap , randomly swap words   in nearby positions within a window size   3 ( Artetxe et al . , 2018 ; Lample et al . , 2018b ) ;   •WD : word dropout , randomly drop words   with a ratio of 15 % ( Iyyer et al . , 2015 ; Lample   et al . , 2018b ) ;   •WR : word replace , randomly replace word   tokens with a placeholder token ( e.g. ,   [ UNK ] ) ( Xie et al . , 2017 ) or with a relevant   ( measured by the similarity of word embed-   dings ) alternative ( Cheng et al . , 2019 ) . The   replacement ratio also is 15 % .   Translationese Effect . Edunov et al . ( 2020 )   pointed out that back - translation ( BT ) suffers from   the translationese effect . that is BT only shows sig-   nificant improvements for test examples where thesource itself is a translation , or translationese , while   is ineffective to translate natural text . To test the   effect of our method on translationese , we follow   the same settings and testsetsprovided by Edunov   et al . ( 2020 ):   •natural source → translationese target   ( X→Y ) ;   •translationese source → natural target   ( X→Y ) ;   •round - trip translationese source →trans-   lationese target ( X→Y ) , where   X→Y→X.   Results . As shown in Table 9 , our approach   shows better robustness over two baseline meth-   ods across various artificial noises . Moreover ,   CNMT consistently outperforms the baseline in   all three translationese scenarios , the same is true   for back - translation . However , Edunov et al . ( 2020 )   shows that BT improves only in the X→Ysce-   nario . Our explanation for the inconsistency is that   BT without monolingual data in our setting bene-   fits from the natural parallel data to deal with the   translationese sources.7943E Codes of tangential contrastive learning and M sampling   E.1 Tangential Contrastive Learning   E.2 M Sampling7944