  Qingfeng Sun Can Xu Huang Hu Yujing Wang Jian Miao   Xiubo Geng Yining Chen Fei Xu Daxin Jiang   Microsoft , Beijing , China   { qins , caxu , huahu , yujwang , jianm ,   xigeng,yinichen,fexu,djiang}@microsoft.com   Abstract   Current Knowledge - Grounded Dialogue Gen-   eration ( KDG ) models specialize in producing   rational and factual responses . However , to es-   tablish long - term relationships with users , the   KDG model needs the capability to generate   responses in a desired style or sentiment . Thus ,   we study a new problem : Stylized Knowledge-   Grounded Dialogue Generation ( SKDG ) . It   presents two challenges : ( 1 ) How to train a   SKDG model where no < context , knowledge ,   stylized response > triples are available . ( 2 )   How to cohere with context and preserve the   knowledge when generating a stylized response .   In this paper , we propose a novel disentan-   gled template rewriting ( DTR ) method which   generates responses via combing disentangled   style templates ( from monolingual stylized cor-   pus ) and content templates ( from KDG corpus ) .   The entire framework is end - to - end differen-   tiable and learned without supervision . Exten-   sive experiments on two benchmarks indicate   that DTR achieves a significant improvement   on all evaluation metrics compared with pre-   vious state - of - the - art stylized dialogue genera-   tion methods . Besides , DTR achieves compara-   ble performance with the state - of - the - art KDG   methods in standard KDG evaluation setting .   1 Introduction   Every good conversational agent needs the ability   to generate good responses , which are not only   knowledgeable and coherent with contexts but also   have abundant and desirable styles and sentiments   ( Rashkin et al . , 2018 ; Smith et al . , 2020 ; Zhou et al . ,   2020 ) . Such an agent can deliver depth dialogues   on various topics and yield more engaging and   vivacious conversations to attract more users . In   other words , rational and perceptual thought are all   necessary for a perfect dialogue agent . Neverthe-   less , most existing Knowledge - Grounded Dialogue   Generation ( KDG ) methods ( Dinan et al . , 2019;Figure 1 : The KDG models only produce a pedantic   response , which lacks emotion and attraction compared   with the responses with polite style , positive and nega-   tive sentiments .   Kim et al . , 2020 ; Zhao et al . , 2020b ) pay more   attention to the former and ignore the latter . Specif-   ically , let ’s claim our motivation : The previous   KDG works mainly focus on selecting knowledge   and expressing knowledge in response accurately .   However , the excessive emphasis on knowledge   makes the KDG models tend to mechanically copy   large sections from the unstructured knowledge   ( e.g. , Wikipedia ) . As a result , the responses from   the KDG models reflect a “ pedantic " style ( i.e. , use   very technical terms and language ) , making the   conversation less engaging and less natural .   In this paper , we are aiming to have the first at-   tempt to incorporate stylized - text - generation into   KDG to tackle the above challenge . As shown in   Figure 1 , the KDG model takes the context and re-   lated document as input and outputs a knowledge-   able but pedantic response corresponding to the   polite one , which makes people feel respected and   comfortable . In the meanwhile , the polite , positive   responses all show bright and lively styles which   not only are able to condense the core meaning of   the response , but also sound appealing to the users   for more exposure and memorableness .   Specifically , we formulate a new problem : Styl-3304   ized Knowledge - Grounded Dialogue Generation   ( SKDG ) . That is , the responses provided by a   model should be coherent with the dialogue con-   texts and be consistent with the given knowledge   and a designated style or sentiment . The chal-   lenges lie in two aspects : ( 1 ) As lacking styl-   ized knowledge - grounded dialogue triples ( i.e. ,   < context , knowledge , stylized response > ) , we need   to train the SKDG model jointly by both indepen-   dent knowledge - grounded dialogues and monolin-   gual corpus with a target style or sentiment . ( 2 ) In   addition to being coherent with context and consis-   tent with target style / sentiment , a good response   from SKDG needs to ensure objective correctness   in the knowledge section . Especially when the   given knowledge contains style - related content , ex-   isting stylized dialogue generation ( SDG ) models   ( Zheng et al . , 2020 ; Ze et al . , 2020 ) may undermine   the correctness of knowledge section . For example ,   in case of negative - to - positive sentiment transfer   shown in Figure 3 , the first two negative fragments   of KDG response - “ dislike cruel ” and “ horrible ”   should be modified to positive fragments , but the   third “ bad ” should be retained to maintain theoriginal meaning of knowledge section .   Hence , our motivation is : on the one hand , bridg-   ing the separate knowledge - grounded response gen-   eration and stylized rewriting by sharing a disen-   tangled template ( addressing challenge ( 1 ) ) ; on   the other hand , enhancing the fidelity regarding to   given knowledge by using a reinforcement learning   approach ( addressing challenge ( 2 ) ) .   To achieve this goal , we propose a new paradigm :   Generate - Disentangle - Rewrite . Firstly , given a di-   alogue context and the associated external knowl-   edge , a KDG model is adopted to generate a re-   sponse . Then as shown in Figure 2 and 3 , we lever-   age a sequential style disentangler to delete style-   related fragments from the KDG response to form a   style - agnostic template . Then the rewriter rewrites   the entire template token - by - token , injecting style-   related fragments in the process , to generate a vivid   and informative response in the desired style . As   there is no supervision on the style disentangler   and the style rewriter , we propose a reinforcement   learning - based method to train the style disentan-   gling and style rewriting in an end - to - end manner   using a style intensity reward and a semantic sim-   ilarity reward . The huge joint action space of the   two modules fragile the training , thus we propose   a novel weakly supervised stylistic template dis-   entangle method to initialize both the disentangler   and the rewriter . As a result , our method success-   fully produces the knowledgeable response in the   desired style without any paired training data .   We name our model DTR standing for   “ Disentangled Template Rewriting ” . We demon-   strate this approach using knowledge - grounded di-   alogues from Wizard of Wikipedia ( Dinan et al . ,   2019 ) and Topical Chat ( Gopalakrishnan et al . ,33052019 ) with three sets of sentences with distinct   sentiments ( positive , negative ) and styles ( polite ) .   Automatic and human evaluations show that our   method significantly outperforms competitive base-   lines with a large margin in generating coherent and   knowledgeable dialogue responses while rendering   stronger stylistic features .   Our contributions are three - fold : ( 1 ) To the best   of our knowledge , it is the first work on the gen-   eration of stylized knowledge - grounded responses   without any labeled paired data for style - specific   context - knowledge - response . ( 2 ) We proposed a   stylized knowledge - grounded dialogue generation   method via disentangled template rewriting . To   optimize the model , we propose a reinforcement   learning approach with a novel weakly supervised   method to guide the learning of both disentangler   and rewriter . ( 3 ) Extensive experiments on two   benchmarks indicate that DTR significantly outper-   forms previous state - of - the - art SDG methods on   all evaluation metrics . Besides , DTR achieves com-   parable performance with the state - of - the - art KDG   methods in the standard KDG evaluation setting .   Our source code will be released at https://   github.com/victorsungo/SKDG-DTR .   2 Related Work   Knowledge - Grounded Dialogue Generation has   attracted broad interest in recent years , where the   knowledge could be obtained from documents ( Di-   nan et al . , 2019 ; Kim et al . , 2020 ; Rashkin et al . ,   2021 ) and images ( Shuster et al . , 2018 ; Yang et al . ,   2020a ; Liang et al . , 2021 ) . Our study considers   document - grounded dialogue generation . With the   rapid development of pre - training techniques , Zhao   et al . ( 2020a ) proposes pre - trained disentangled de-   coder , and Li et al . ( 2020 ) proves that the KDG   models could achieve comparable performance   with state - of - the - art supervised methods through   an unsupervised learning method . Rather than test-   ing new architectures on the benchmarks , our main   contribution lies in the investigation of transferring   the pedantic and factual knowledge - grounded re-   sponses into a desired style or sentiment , which   roots in the requirement from practice .   Text Style and Sentiment Transfer was inspired   by visual style transfer ( Gatys et al . , 2016 ; Zhu   et al . , 2017 ) , and many methods have made re-   markable work in text style transfer , which aims   to alter the style attributes of text while preserv-   ing the content . A prevalent idea of style transferis to disentangle the content and style of text ( Fu   et al . , 2018 ; Li et al . , 2018 ; Jin et al . , 2020 ; Wen   et al . , 2020 ; Zhu et al . , 2021 ) or leverage the back-   translation ( Lample et al . , 2019 ; Li et al . , 2021 ) .   Stylized dialogue generation has attracted numer-   ous attention in recent years ( Niu and Bansal , 2018 ;   Gao et al . , 2019 ) . Different from style transfer , styl-   ized dialogue generation requires that the response   is also coherent with its context .   Stylized Dialogue Generation refers to generate   a dialogue response in the target style . Akama   et al . ( 2017 ) first train a response generation model   on a dialog corpus then use a style corpus to fine-   tune the model . Yang et al . ( 2020b ) builds a pre-   trained language model and devise both a word-   level loss and a sentence - level loss to fine - tune   the pre - trained model towards the target style . Su   et al . ( 2020a ) proposes an information guided re-   inforcement learning strategy to better balance the   trade - off between the stylistic expression and the   content quality . Sun et al . ( 2021 ) blends textual   and visual responses to make the dialogue style   more attractive and vivid . Zheng et al . ( 2020 ) cap-   tures stylistic features embedded in unpaired texts ,   Su et al . ( 2020b ) uses the pointwise mutual infor-   mation ( PMI ) to determine stylistic word , Ze et al .   ( 2020 ) adopts pre - trained models to tackle the open-   domain stylized response generation .   We propose a novel disentangled template rewrit-   ing approach as the first attempt to study stylized   knowledge - grounded dialogue generation without   any supervised style - specific context - knowledge-   response triples data .   3 Task Definition   For the SKDG task , our model is trained on a di-   alogue dataset D={(K , U , Y)}and a style   corpus D={T } , where∀(K , U , Y)∈D ,   Uis a dialogue context , Ka external document   that contains relevant knowledge regarding to U   andYa response to U , and∀T∈D , Tis a   piece of text in the target style S. We do n’t as-   sume that there exists triples { ( K , U , Y)}withY   expressed in the style or sentiment S , e.g. ,S=   { “ polite ” , “ positive ” , “ negative ” } . Our goal is   to learn a generation method P(Y∣K , U , S)with   DandD , thus given a document Kand a context   U , one can generate a response Yfollowing the   desired style S , where Yalso coheres with context   and preserves the knowledge.33064 Approach   Heading for learning an effective disentangled tem-   plate rewriting model for SKDG task , we need to   deal with several challenges : ( 1 ) how to distinguish   the style - related fragments from a given sentence   without any supervision;(2 ) how to retain the style-   related fragments in knowledge section to defend   the completeness;(3 ) how to rewrite the disentan-   gled template holistically instead of inserting a few   style words , thus to enhance fluency and diversity .   Our DTR model is made up of a knowledge-   grounded response generator G , a sequential style   disentangler Fand a style rewriter G. Given a   dialogue context Uand its associated knowledge   K , we first use Gto generate a response Y. Fig-   ure 2 illustrates the cooperation of FandG. The   former reads Yand disentangles the style - related   content from Yto form a style - agnostic template   sequence ̃Y , which is further provided as input to   Gto generate the transferred response ˆYin a tar-   get style . Since ̃Yis discrete , the major hinder of   learning Flies in the gradient is not differentiable .   To cope with the challenge , we exploit a reinforce-   ment learning approach to optimize Fleveraging   the signals from G.   So why do we need a Disentangler + Rewriter   architecture ? The previous SDG methods fuse the   knowledge and style into mixed representation and   decode a response . Due to the difficulty of mixing   knowledge and style implicitly under the unsuper-   vised setting , it is possible to lose knowledge or   style in the decoding stage . Motivated by this , we   propose to decouple the response generation into   two relatively independent processes : 1 ) knowl-   edge fragments generation 2 ) style fragments gen-   eration . The knowledge fragments and style frag-   ments are explicitly composited into the response   in the final stage . Such a method ensure the knowl-   edge is successfully presented in the final output .   The disentangler plays a central role in decoupling   and composition . In the following , we will elabo-   rate details of each component .   4.1 Model Architecture   4.1.1 Knowledge - Grounded Response   Generator   The generator Gis a sequence - to - sequence model   based on the Transformer architecture ( Vaswani   et al . , 2017 ) , it consists of a 6 - layers encoder   and decoder with a hidden size of 768 . Given a   dialogue context U={u , . . . , u , . . . , u}withuthei - th utterance , and a document K=   { k , . . . , k , . . . , k}withkthei - th sentence . We   concatenate UandKas a long sentence as the   input of the encoder , then the decoder generates a   response Yas output   Y={w , . . . , w , . . . , w}=G(U , K ) ( 1 )   4.1.2 Sequential Style Disentangler   To identify and disentangle the style - related frag-   ments from Y , we employ a sequence labeling   module named Sequential Style Disentangler F   to model the probabilities { x}of being style-   related token at each position in Y. The formula-   tions are as follows :   P(A∣Y , U , K ) =   ∏P(a∣Y , U , K ) ( 2 )   P(a∣Y , U , K ) = x = sigmoid ( We ) ( 3 )   { e , . . . , e}=BERT(Y , U , K ) ( 4 )   where W∈Rande∈R , vis representation   dimension , a∈{replace , retain } , A={a } .   Then when generating ̃Yifx > ε , awill be oper-   ation “ replace " indicating wis a style token and   needs to be replaced with a tag token [ * ] , and vicev-   ersa for x < ε , awill be operation “ retain " indicat-   ingwremains unchanged . Threshold εis equal to   topP%percentile of { x } , where Pis a hyper   parameter . Finally , we perform the the predicted se-   quence of operations on Yto obtain style - agnostic   template ̃Y. As the style disentangler tags each   word in a sentence , it captures the style fragments   ( e.g. , words , phrases , sub - sequences , or even the   whole sentence ) rather than only style tokens . The   learning detail is presented in Appendix A.1 .   4.1.3 Style Rewriter   With̃Yas input , the style rewriter Ggenerates a   newˆYword - by - word in the target style . Ghas the   same architecture as G. The generation process   ofGis formulated as :   P(ˆY∣̃Y)=   ∏P(ˆw∣̃Y ) ( 5 )   where ˆwis the t - th token of ˆYwhose length is h.   4.2 Reinforcement Learning   Neither style disentangler nor style rewriter has   supervision for training . Moreover , We need to   ensure the correctness of ˆYwithout any modifi-   cations of the original content in the knowledge3307section of Y. To cope with the challenges , we ex-   ploit REINFORCE ( Sutton et al . , 2000 ) to train   FandGjointly with a total reward determined   by the semantic similarity with the ground - truth   response and the consistency with the desired style .   Specifically , we maximize the expected reward as :   R = EE[R(̃Y , Y ) ] ( 6 )   where P(ˆY)andP(A)stand for P(ˆY∣̃Y )   andP(A∣Y , U , K ) respectively , R(̃Y , Y)=   Sim(ˆY , Y)+Cls(ˆY),Sim(⋅)is embedding cosine   similarity which supervises the knowledge regular-   ization , Cls(⋅)is the style intensity predicted by a   classifier . We subtract the mean value of rewards R   in a batch to reduce the variance of gradient estima-   tion ( Clark and Manning , 2016 ) . In order to avoid   the destroying issue of RL , we fix the parameters   ofG , then only optimize the F.   4.3 Weakly Supervised Learning   Since the style disentangler and style rewriter need   to be carefully synchronized , ideally we hope they   can benefit each other in learning . However , in the   early stage as the parameters of FandGare far   from optimal . It is possible that , on the one hand ,   the templates that are not decoupled successfully   hinder Glearning from rewriting style fragments   accurately . On the other hand , noise signals from   rewards computed with the low - quality responses   generated by Gflow to the learning of F , result-   ing in inferior F. To alleviate error accumulation   in joint training , we propose a novel weakly su-   pervised stylistic template disentangle method to   assist the learning of FandG.   4.3.1 Weakly Supervised Disentangler   Intuitively , style fragments dominate the distribu-   tion of style corpus Dcompared with content frag-   ments , thus the style fragments are easier to be   reconstructed than content fragments by the de-   noising autoencoder trained on D. As shown in   Figure 4 , a denoising reconstruction model Gre-   constructs the style word “ good ” successfully but   fail to do that for content word “ pizza ” in the same   response from D. Particularly , we randomly di-   videDinto two halves with equal probability :   DandD , thenDis used to train the denois-   ing reconstruction model G. The reconstruction   objective Lis formulated as :   L = E[−logp(T∣̃T ) ] ( 7 )   wherẽTis the corrupted version of Tby randomly   mask 15 % tokens .   Then for each sentence T={t}(with tthe   i - th token in T ) inD , we sequentially mask one   token each time to construct its denoising versions   { ̃T } , then{̃T}are inferenced by Gto re-   construct { ˆT } . We acquire a distance sequence   d={d}={Dis(t,ˆt)}where Dis(⋅,⋅)de-   notes a distance function . Based on above intuition ,   lower dmeans tis more preferable as a style-   related token , thus for tandt , ifd < d , we   define the label y=1 , and viceversa for d > d ,   y=−1 . We aggregate all < t , t , y > triples to   construct Dto optimize the style disentangler   via the pairwise ranking loss :   L(t , t , y)=max(0,−y∗(t−t)+µ)(8 )   where µis a hyper parameter . The action space   of token - level pairwise ranking is large , so for   each sentence in D , we randomly sample Z   non - repetitive < x , x , y > triples to optimize L ,   where Z is a hyper parameter . The style tokens in   various style corpus found by the style disentangler   is presented in Appendix B.6 .   4.3.2 Weakly Supervised Rewriter   The training data for the rewriter are also con-   structed by an unsupervised method : Optimized   style disentangler F(Eq.8 ) infers the style corpus   D={T}and generates a disentangled tem-   plate set ̃D={̃T } . Then the rewriter takes   paired<̃T , T > as input and output respectively .   SincẽTis style - agnostic , the rewriter would focus   on transfering a factual sentence to a desired sen-   tence with target style . The loss function for the   rewriter Gis :   L=−1   M   ∑ (   ∏p(t∣t,⋯ , t;̃T ) ) ( 9)3308where tis the i - th token in l - th sentence . Specif-   ically , the rewriter Ghas a same architecture as   G.   Algorithm 1 Optimization Algorithm . Input : Datasets D , D ; Models G , F , G.Optimize GusingD.Construct D.Optimize FusingD(Eq.8 ) .Construct ̃DusingF.Optimize GusingDand̃D(Eq.9 ) .Further Optimize FusingD ( Eq.6 ) .return G , F , G.   5 Experiments   We conduct experiments on Wizard of Wikipedia   ( Wizard ) and Topical Chat with positive and nega-   tive sentiments , and polite style .   5.1 Datasets   KDG Corpus Wizard consists of 1365 topics , and   each conversation happens between a wizard who   has access to Wikipedia paragraphs and an appren-   tice who talks to the wizard . Topical Chat utilizes   wiki articles , Washington Post , and Reddit fun facts   as the knowledge source . The participants play   symmetric and asymmetric roles according to the   knowledge . Wizard and Topical Chat are split as   training , valid and test set respectively . We com-   pare our method with baselines on Wizard Test   Seen and Topical Chat Test Freq . More details are   described in Appendix B.1 .   Style Corpus We use Amazon dataset published   in Juncen et al . ( 2018 ) and Politeness published   in Madaan et al . ( 2020 ) for style transfer . Ama-   zon consists of product reviews from Amazon for   flipping sentiment , and it contains 27800 positive   sentences and 27700 negative sentences . For Po-   liteness , We use the P9 - bucket as the polite dataset ,   which consists of 27000 polite sentences .   5.2 Evaluation Metrics   Following Zheng et al . ( 2020 ) and Ze et al . ( 2020 ) ,   we use automatic metrics to measure DTR on three   aspects : Style Intensity , Relevance , and Diver-   sity . For style intensity , we use the GPT-2 classifier   prediction mentioned in section 5.3 . Relevance is   measured with F1,BLEU ( Papineni et al . , 2002 )   andRouge ( Lin , 2004 ) . We use Distinct ( Li et al . ,   2016 ) to measure Diversity of different models . To measure the diversity between different styles ,   we propose inner Distinct : given a context and   knowledge , we calculate distinct in three generated   responses with three styles .   For human evaluation , we randomly sample 500   examples from test set , and recruit 3 well - educated   annotators . To each annotator , two responses from   different models are presented , which are randomly   shuffled to hide their sources . The annotators then   judge which response is better from four aspects :   ( 1)Style Consistency : which response exhibits the   desired style more ( 2 ) Knowledge Preservation :   which response is more relevant to the knowledge-   able document ( 3 ) Context Coherence : which re-   sponse is more coherent with the dialogue context   ( 4)Fluency : which response is more fluent and   free from any grammar errors .   5.3 Implementation Details   We use pre - trained MASS ( Song et al . , 2019 ) to ini-   tializeGandG. We adopt Adam optimizer as an   initial learning rate of 5 ×10 , and the batch size   is 4096 tokens for a NVIDIA 1080 Ti GPU . Since   all the baselines do n’t have a knowledge selection   module , we chose the ground - truth knowledge as   input for Wizard and the top-1 knowledge sentence   according to the BLEU-1 with the corresponding   response as input for Topical Chat . We use beam   search(size=5 ) to decode the response . We initial-   izeFwith pre - trained BERT , the replace rate Pis   25,Zin section 4.2 is 10 . We use Glove ( Jeffrey   et al . , 2014 ) 100d embedding and cosine similarity   asDis(⋅,⋅)to calculate distance d.µin Eq.8 is   0.2 . To get the style intensity reward , we follow Ze   et al . ( 2020 ) and train binary GPT-2 ( Radford et al . ,   2019 ) classifiers . Early stopping on validation is   adopted as a regularization strategy . All the above   hyperparameters are determined by grid search .   5.4 Baselines   The following models are selected as baselines :   •StyleFusion ( Gao et al . , 2019 ) bridges conver-   sation modeling and nonparallel style trans-   fer by sharing a latent space . We use the   code https://github.com/golsun/   StyleFusion .   •StylisticDLV ( Zhu et al . , 2021 ) disentangles   the content and style in latent space by di-   luting information in style representations .   We use the code https://github.com/   golsun / StyleFusion .3309   •StylizedDU ( Zheng et al . , 2020 ) lever-   ages back - translation technique to gener-   ate pseudo stylized context - response pairs .   We use the code https://github.com/   silverriver / Stylized_Dialog .   •StyleDGPT ( Ze et al . , 2020 ) exploits   the pre - trained language models on the   stylized response generation task . We   use the code https://github.com/   TobeyYang / StyleDGPT .   All the baselines are jointly learned with datasets   DandD , and take the concatenation of knowl-   edge and context as input.3310   5.5 Evaluation Results   As shown in Table 1 , our DTR model achieves   competitive performance in style transfer and sig-   nificantly outperforms the baselines in all the rel-   evance metrics . This indicates that DTR can pro-   duce high - quality responses which are coherent to   the context , related to the knowledge , and consis-   tent with the target style simultaneously . We also   observe that all SDG methods frequently lost the   knowledge part ( Appendix B.2 ) . DTR significantly   outperforms StyleDGPT on relevance , indicating   that leveraging the style intensity score to optimize   the decoupling of the template is superior to di-   rectly optimizing response generation ( degrading   language modeling ) . We observe the core compo-   nent back - translation in StylizedDU fails to infer   pseudo - knowledge from a response ( generally , the   knowledge carries much more information than the   response ) . Table 2 reports the results of human eval-   uation , DTR significantly outperforms StyleDGPT   on all aspects . DTR is also superior to all the base-   lines as the Case Study section in Appendix B.2 .   5.6 Ablation Study   Firstly , to verify the contributions of the pro-   posed disentangler and weakly supervised learning   method , we consider the following variants : ( 1 )   w / oFWSL : training DTR without the Weakly   Supervised Learning of Fin section 4.3.1 . ( 2 ) w/o   F(Classification ) : replace the pairwise ranking   loss in Fwith a binary classification loss . We de-   fine those tokens with d=0(in section 4.3.1 ) as   style words ( label=1 ) , otherwise non - style words   ( label=0 ) . ( 3 ) w / oF(TFIDF ) : replace Fwith a   TFIDF - based rule ( replace the fragments as [ * ] in a   sentence with the lowest P% TFIDF scores except   stop words ) . Table 7 shows the results of the three   variants . We can conclude that ( 1 ) the weakly su-   pervised learning of Fis crucial to training DTR ,   since the variant with a simple TFIDF significantly   outperforms the one without any initialization ; and(2 ) the ranking loss in Fplays a key role in the   success of style transfer , there is a dramatic drop   on the style intensity of w / oF(Classification ) .   According to our observation , it is overfitting on   the style corpus , leading to a low success rate .   Secondly , to investigate the RL rewards in   Eq.(6 ) , we consider the following variants : ( 1 ) w/o   Rewards : remove the similarity and style intensity   reward . ( 2 ) w/o Sim : remove the similarity reward .   ( 3)w / o Cls : remove the style intensity reward . As   shown in Table 7 , removal any of the two rewards   will cause performance drop , indicating that style   intensity and similarity reward can enhance DTR .   We also add Sim to StylizedDU , the improvement   is +2.1 on F1 , thus it ’s hard for Sim to bridge the   huge gap . Negative and Polite are similar , these   results are presented in Appendix B.3 .   5.7 Discussions   Impact of stylized knowledge - grounded genera-   tion . We annotate the “ Attractiveness ” ( the anno-   tators are given two different responses with the   same context and knowledge from two different   models , and they should determine which response   is more attractive and engaging in a holistic way ) of   DTR and DTR - s ( without style transfer ) following   the same process in 5.2 . Table 2 reports the evalua-   tion results . We can see that introducing a positive   sentiment or a polite style would enhance the en-   gagement of the KDG model while establishing a   negative sentiment harm the attractiveness .   Impact of style transfer on the conversational   ability of SDG models . We are curious about   to what extent the conversational ability of SDG   models will be damaged after style transfer . We   examine DTR and StyleDGPT in two settings : ( 1 )   Gold - K : the given knowledge is the ground - truth   ( 2 ) Predicted - K : the given knowledge is selected   from a knowledge selection model ( Xueliang et al . ,   2020 ) . As shown in Figure 5 , after style transfer on   Wizard , the F1 of DTR drops 2.28 and 2.1 in Gold-3311 K and Predicted - K , while the F1 of StyleDGPT   drops 11.16 and 8.16 respectively . On Topical   Chat , the F1 of DTR drops 1.77 and 1.51 in Gold-   K and Predicted - K , while the F1 of StyleDGPT   drops 7.1 and 6.16 respectively . Compared with   StyleDGPT , DTR dramatically reduces the damage   to the conversational ability while achieving a high   success rate of style transfer . Thanks to the superior   style transferring mechanism , our DTR achieves   comparable performance with the state - of - the - art   KDG models { KnowledGPT(Xueliang et al . , 2020 )   on Wizard , UNILM(Li et al . , 2020 ) on Topical   Chat}in the standard KDG evaluation setting even   after style transfer . The results of Negative and   Polite are similar and presented in Appendix B.7 .   Impact of the replace rate P.As shown in Fig-   ure 6 , P= 25 achieves the best balance between   relevance and diversity . A smaller Pwould re-   main a large number of original style fragments in   the template , leading to tiny differences between   different styles . On the contrary , a larger Pwould   delete those content fragments , which are harder to   restore by rewriter , but the responses from different   styles will be more diverse . Topical Chat follows   the same regularity as shown in Appendix B.5 .   6 Conclustion   We explore stylized knowledge - grounded dialogue   generation by proposing bridging the knowledge-   grounded response generation with the stylized   rewriting via sharing a disentangled template . Eval-   uation results on benchmarks of the task indicate   that our model can achieve state - of - the - art perfor-   mance and exhibits a superior generation ability   over different knowledge domains and styles .   Acknowledgement   We thank anonymous reviewers for their insightful   suggestions to improve this paper .   References331233133314A Method Detail   A.1 Disentangler BERT Learning Detail   We initialize Fwith two pre - trained BERT . In   the unsupervised initialization stage we only train   the BERT , then in the reinforcement learning   stage , we fix the parameters of BERTand loosen   BERT .   P(A∣Y , U , K ) =   ∏P(a∣Y , U , K ) ( 10 )   P(a∣Y , U , K ) = x = x+x(11 )   x = sigmoid ( We ) ( 12 )   x = sigmoid ( We ) ( 13 )   { e , . . . , e}=BERT(Y ) ( 14 )   { e , . . . , e}=BERT(Y , U , K ) ( 15 )   B Experiments   B.1 Datasets   Table 4 reports the statistics of the Wizard of   Wikipedia dataset and the Topical Chat dataset .   B.2 Case Study   Table 5 and Table 6 presents some examples from   Wizard of Wikipedia and Topical Chat respec-   tively . In each case , we show the dialogue con-   text , the knowledge ( ground - truth ) , the human re-   sponse , and responses from different models with   each style . We can see that responses from DTR   and StyleDGPT are well grounded by the pro-   vided knowledge and have obvious style , while   responses from StyleFusion and StylizedDU in gen-   eral lack of both informative content and desired   style . Compared with StyleDGPT , DTR is better at   leveraging target style in the test phase and replies   with more informative and more contextually co-   herent responses , which demonstrates the potential   of the model in practice . For DTR , the knowledge-   grounded response generator Gfirstly generates a   factual response with mixed style - related tokens   ( such as “ yeah ” , “ like ” , “ not ” , “ whether ” , etc . )   and content , then the template generator Freplace   them with a tag token [ * ] to produce a disentan-   gled template , finally the rewriter Gmodifies the   tag [ * ] to generate some new sentences in different   target styles .   B.3 Ablation evaluation   As shown in table 7 , we list all ablation evaluation   results of Positive , Negative and Polite on Wizardof Wikipedia and the Topical Chat .   B.4 Manual evaluation   As shown in Table 8 , we list all manual evaluation   results of Positive , Negative and Polite on Wizard   of Wikipedia and the Topical Chat .   B.5 Replace Rate P   As shown in Figure 7 , we present the F1 and Inner   Distinct with different replace rate in Topical Chat .   B.6 Statistics of frequent style words   As shown in Figure 8 , we present the visualization   of the style tokens in various style corpus found by   the initiated style decoupler .   B.7 F1 Drop P   As shown in Figure 9 and 10 , we present F1 of   DTR , StyleDGPT , and SOTA KDG models in dif-   ferent task mode of negative sentiment and polite   style.3315331633173318