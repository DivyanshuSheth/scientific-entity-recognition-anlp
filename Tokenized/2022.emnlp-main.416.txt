  Yuki Araseand Satoru Uchidaand Tomoyuki KajiwaraGraduate School of Information Science and Technology , Osaka University , JapanFaculty of Languages and Cultures , Kyushu University , JapanGraduate School of Science and Engineering , Ehime University , Japan   arase@ist.osaka-u.ac.jp , uchida@flc.kyushu-u.ac.jp   kajiwara@cs.ehime-u.ac.jp   Abstract   Controllable text simplification is a crucial as-   sistive technique for language learning and   teaching . One of the primary factors hinder-   ing its advancement is the lack of a corpus an-   notated with sentence difficulty levels based   on language ability descriptions . To address   this problem , we created the CEFR - based Sen-   tence Profile ( CEFR - SP ) corpus , containing   17k English sentences annotated with the lev-   els based on the Common European Frame-   work of Reference for Languages assigned by   English - education professionals . In addition ,   we propose a sentence - level assessment model   to handle unbalanced level distribution because   the most basic and highly proficient sentences   are naturally scarce . In the experiments in this   study , our method achieved a macro - F 1score   of84.5%in the level assessment , thus outper-   forming strong baselines employed in readabil-   ity assessment .   1 Introduction   Controllable text simplification , first proposed by   Scarton and Specia ( 2018 ) , is the automatic rewrit-   ing of sentences to make them comprehensible to   a target audience with a specific proficiency level .   Among its primary applications are providing read-   ing assistance to language learners and helping   teachers adjust the difficulty level of their teaching   materials ( Petersen and Ostendorf , 2007 ; Pellow   and Eskenazi , 2014 ; Paetzold , 2016 ) . The fine-   grained control of output levels to match the lin-   guistic ability of the readership is crucial for these   educational applications .   While readability assessments have been actively   studied ( e.g. , in ( Vajjala Balakrishna , 2015 ; Meng   et al . , 2020 ; Deutsch et al . , 2020 ) ) , linking read-   ability to language ability is difficult . Readabil-   ity scores , such as the Flesch – Kincaid grade level   ( Kincaid et al . , 1975 ) , are intended for native speak-   ers , not for language learners to whom very differ-   ent considerations apply . Pilán et al . ( 2014 ) andOzasa et al . ( 2007 ) revealed that readability met-   rics designed for L 1do not apply to L 2learners .   Furthermore , readability definitions use documents   rather than sentences , which are required by text   simplification at the sentence - level , as their unit .   The lack of a corpus annotated by sentence dif-   ficulty level hinders the advancement of control-   lable text simplification . Previous studies ( Scarton   and Specia , 2018 ; Nishihara et al . , 2019 ; Agrawal   et al . , 2021 ) necessarily used corpora annotated   for readability rather than difficulty ; furthermore ,   they assumed that all sentences in a document had   the same readability ( i.e. , the document level in   Newsela ( Xu et al . , 2015 ) ) .   To solve these problems , we created a large - scale   English corpus annotated by sentence difficulty lev-   els based on the Common European Framework of   Reference for Languages ( CEFR),the most widely   used international standard describing learners ’ lan-   guage ability . Our CEFR - based Sentence Profile   ( CEFR - SP ) corpus adapts CEFR to sentence levels .   A sentence is categorised as a certain level if a per-   son with the corresponding CEFR - level can readily   understand it . CEFR - SP provides CEFR levels for   17k sentences annotated by professionals with rich   experience teaching English in higher education .   A major challenge in sentence - level assessment   is the unbalanced distribution of levels : sentences   at the basic ( A 1 ) and highly proficient ( C 2 ) levels   are naturally scarce . To handle this , we propose a   sentence - level assessment model with a macro - F 1   score of 84.5 % . We designed a metric - based clas-   sification method with a simple inductive bias that   avoids overfitting to majority classes ( Vinyals et al . ,   2016 ; Snell et al . , 2017 ) . Our method generates   embeddings representing each CEFR - level and esti-   mates a sentence ’s level based on its cosine similar-   ity to these embeddings . Empirical results confirm   that our method effectively copes with unbalanced6206label distribution and outperforms the strong base-   lines employed in readability assessments .   This study makes two main contributions . First ,   we present the largest corpus to date of sentences   annotated according to established language abil-   ity indicators . Second , we propose a sentence-   level assessment model to handle unbalanced label   distribution . CEFR - SP and sentence - level assess-   ment codes are availablefor future research at .   2 Related Work   Related studies have assessed text levels on differ-   ent granularity ( document and sentence ) and level   definitions ( readability / complexity and CEFR ) .   2.1 Document - based Readability   Previous studies have assessed readability and cre-   ated corpora with document readability annota-   tions . WeeBit ( Vajjala and Meurers , 2012 ) , the   OneStopEnglish corpus ( Vajjala and Lu ˇci´c , 2018 ) ,   and Newsela provide manually written documents   for various readability levels . Working with these   annotated corpora , previous studies have used vari-   ous linguistic and psycholinguistic features to de-   velop models for assessing document - based read-   ability ( Heilman et al . , 2007 ; Kate et al . , 2010 ;   Vajjala and Meurers , 2012 ; Xia et al . , 2016 ; Vaj-   jala and Lu ˇci´c , 2018 ) . Neural network - based ap-   proaches have proven to be better than feature-   based models ( Azpiazu and Pera , 2019 ; Meng et al . ,   2020 ; Imperial , 2021 ; Martinc et al . , 2021 ) . In   particular , Deutsch et al . ( 2020 ) showed that pre-   trained language models outperform feature - based   approaches , and the combination of linguistic fea-   tures plays no role in performance gains .   2.2 Sentence - based Readability   Previous studies annotated sentences ’ complexities   based on crowd workers ’ subjective perceptions .   Stajner et al . ( 2017 ) used a 5 - point scale to rate the   complexity of sentences written by humans or gen-   erated by text simplification models . Brunato et al .   ( 2018 ) used a 7 - point scale for sentences extracted   from the news sections of treebanks ( McDonald   et al . , 2013 ) . However , as Section 3.4 confirms , re-   lating complexity to language ability descriptions is   challenging . Naderi et al . ( 2019 ) annotated German   sentence complexity based on language learners’subjective judgements . In contrast , the CEFR - level   of a sentence should be judged objectively based   on the understanding of language learners ’ skills .   Hence , we presume that a sentence CEFR - level   can be judged only by language education profes-   sionals based on their teaching experience . For   sentence - based readability assessments , previous   studies regarded all sentences in a document to   have the same readability ( Collins - Thompson and   Callan , 2004 ; Dell’Orletta et al . , 2011 ; Vajjala and   Meurers , 2014 ; Ambati et al . , 2016 ; Howcroft and   Demberg , 2017 ) . As we show in Section 3.4 , this   assumption hardly holds .   Thesimplicity of a sentence is one of the primary   aspects in a text simplification evaluation , which   is commonly judged by human . There are a few   corpora annotated by the sentence simplicity for   automatic quality estimation of text simplification   ( Štajner et al . , 2016 ; Alva - Manchego et al . , 2021 ) .   Nakamachi et al . ( 2020 ) applied a pretrained lan-   guage model for estimating the sentence simplic-   ity and used it to reward a reinforcement learn-   ing – based text simplification model . The sentence   simplicity is distinctive from CEFR levels based   on the established language ability descriptions .   2.3 CEFR - based Text Levels   Attempts have been made to establish criteria for   CEFR - level assessments . For example , the English   Profile ( Salamoura and Saville , 2010 ) and CEFR - J   ( Ishii and Tono , 2018 ) projects relate English vo-   cabulary and grammar to CEFR levels based on   learner - written ’ and textbook corpora . Tools such   as Text Inspectorand CVLA ( Uchida and Negishi ,   2018 ) endeavour to measure the level of English   reading passages automatically . Xia et al . ( 2016 )   collected reading passages from Cambridge En-   glish Exams and predicted their CEFR levels using   features proposed to assess readability . Rama and   Vajjala ( 2021 ) demonstrated that Bidirectional En-   coder Representations from Transformers ( BERT )   ( Devlin et al . , 2019 ) consistently achieved high ac-   curacy for multilingual CEFR - level classification .   Although these micro- ( i.e. , vocabulary and   grammar ) and macro - level ( i.e. , passage - level ) ap-   proaches have proven useful , few attempts have   been made to assign CEFR levels at the sentence   level , despite its importance in learning and teach-   ing . Pilán et al . ( 2014 ) conducted a sentence - level   assessment for Swedish based on CEFR ; however,6207they regarded document - based levels as sentence   levels . Furthermore , their level assessment was as   coarse as predicting either above B 1or not .   3 CEFR - SP Corpus   This section describes the design of the annota-   tion procedure and discusses sentence - level pro-   files . CEFR describes language ability on a 6 - point   scale : A 1indicates the proficiency of beginners ;   A2 , B1 , B2 , C1 , and C 2indicates mastery of a   language at the basic ( A ) , independent ( B ) , and   proficient ( C ) levels . Because CEFR is skill - based ,   each level is defined by ‘ can - do ’ descriptors in-   dicating what learners can do , CEFR levels for   sentences can not be defined directly .   Therefore , we used a bottom - up approach , as-   signing CEFR levels to sentences based on the ‘ can-   do ’ descriptors of reading skills under the definition   that a sentence is , for example , at A1 level if it can   be readily understood by A1 - level learners . We hy-   pothesise that with sufficient teaching experience   and CEFR knowledge , it is possible to objectively   determine at which level a learner can understand   each sentence . We therefore carefully selected an-   notators with sufficient expertise through pilot and   trial sessions .   3.1 Annotation Procedure   Pilot Study A pilot study was conducted to verify   the hypothesis that sufficient teaching experience   and CEFR knowledge will allow an objective eval-   uation of sentence levels . We recruited participants   with three levels of expertise to label 228sample   sentences : an English - language education special-   ist with 12years of teaching experience in higher   education , a graduate student majoring in English   education who is familiar with CEFR , and a group   of three graduate students with various majors ( nat-   ural language processing and ornithology ) and no   prior knowledge of CEFR or English - teaching ex-   perience . The results showed that the second expert   had a high agreement rate with the first senior ex-   pert ( Pearson correlation coefficient 0.74 ) , whereas   the members of the third group agreed less often   with the senior expert ( Pearson correlation coeffi-   cients : 0.45,0.50 , and0.59 ) . These results confirm   that annotators with considerable experience and   knowledge agree on the judgement of the CEFR   levels of sentences . Annotation Guidelines The annotators were fa-   miliarised with the annotation guidelines before   beginning their work . The guidelines described   the scales and ‘ can - do ’ descriptions of CEFR read-   ing skills with example sentences of each level   that were assessed by the expert . Importantly , the   guidelines required the annotators to judge each   sentence ’s level based on their English - teaching   experience . Annotators were allowed to look in   a dictionary to establish word levels but were in-   structed not to determine a sentence ’s level solely   based on the levels of the words it contained .   Annotator Selection For formal annotation , we   recruited eight annotators with diversified English-   teaching experience . We then conducted a trial   session in which the annotators were asked to label   100samples extracted from the target corpora of   formal annotation . These samples were labelled by   the senior expert in the pilot study as references .   Pearson correlation coefficients against the expert   ranged from 0.59to0.77 , roughly correlating with   the participants ’ experience in English - teaching   in terms of duration ( years of teaching ) and role   ( as private tutor or teacher in higher education ) .   We finally selected two having high agreement   rates ( Pearson correlation coefficients : 0.75and   0.73 ) and small average level - assignment differ-   ences ( 0.11and0.22 ) compared to the expert .   The annotation guidelines were finalised to provide   example sentences with corresponding CEFR lev-   els on which multiple annotators had agreed in the   pilot and trial sessions .   3.2 Sentence Selection   Sentences were drawn from Newsela - Auto , Wiki-   Auto , and the Sentence Corpus of Remedial En-   glish ( SCoRE ) . Newsela - Auto and Wiki - Auto , cre-   ated by Jiang et al . ( 2020 ) , are specifically used for   text simplification . SCoRE ( Chujo et al . , 2015 )   was created for computer - assisted English learn-   ing , particularly for second language learners with   lower - level proficiency . The sentences in SCoRE   were carefully written by native English speakers ,   understanding the educational goals of each profi-   ciency level ; they include A - level sentences , which   are scarce in text simplification corpora.6208The difficulty level can also be affected by exter-   nal factors , such as discourse and readers ’ knowl-   edge of a topic . For example , consider the sentence   ‘ The white house announced his return . ’ Though it   is simple in terms of wording and grammar , under-   standing it requires the knowledge that ‘ the white   house ’ is an organisation name and the resolution   of the coreference of ‘ he ( his ) ’ from outside the   sentence . We consider comprehension of anaphora   and cultural and factual knowledge to be different   aspects of language proficiency . The dependence   on external factors makes the sentence - level as-   sessment ill - formed . To minimise the effect of   outside factors , we selected stand - alone sentences   for annotation , that is , sentences comprehensible   independent of their surrounding context .   Thus , we selected the first sentences in para-   graphs to avoid requiring coreference resolution .   We excluded sentences with named entities ( al-   though dates , times , country names , and numeral   expressions were allowed ) , quotations , and brack-   ets . Appendix A describes the complete heuris-   tics for sentence selection . We conducted several   rounds of manual checks by observing a few hun-   dred samples to finalise the heuristics of the sen-   tence selection .   After filtering , we randomly sampled 5—30   word sentences to obtain 8.5k sentences each from   Newsela - Auto and Wiki - Auto and 3.0k sentences   from SCoRE ( excluding the 100sentences used in   the trial session ) . Note that we excluded sentences   from the Newsela - Auto test set so that CEFR - SP   can be employed in training text simplification   models in the future .   3.3 Sentence Profile   The two annotators independently supplied 40k la-   bels for the 20k sentences . They assigned the same   level to 37.6%sentences and levels with one grade   difference to 50.8%sentences , which resulted in   88.4%sentences with levels within one grade dif-   ference . Given that many sentences are likely to   have intermediate levels of difficulty , we regarded   both assignments as correct if they differed by only   one ; thus , for example , the same sentence could   be labelled as both B 1and B 2 . This left us with   27,841labels for 17,676unique sentences . Table 1   shows example sentences sampled from CEFR - SP .   Table 2 shows the number of sentences per level ,   average sentence length ( number of words ) , and   distribution ( % ) of lexical levels computed on theA1She had a beautiful necklace around her   neck .   A2Some experts say the classes should be   changed .   B1Historically there have also been negative   consequences .   B2Alligators are generally timid towards hu-   mans and tend to walk or swim away if one   approaches .   C1The metal - carbon bond in organometallic   compounds is generally highly covalent .   C2In the past , non - photosynthetic plants were   mistakenly thought to get food by breaking   down organic matter in a manner similar to   saprotrophic fungi .   content words in the 27,841labelled sentences .   We used the CEFR - J Wordlist , which assigns A 1   to B2levels to pairs of lemmas and part - of - speech   tags . This allowed us to determine word levels   without word sense disambiguation . The content   words in sentences were matched with the CEFR - J   wordlist using their lemmas and part - of - speech tags .   The frequency of each lexical level was computed   by dividing the count of words with that level by   the number of all content words at each sentence-   level . We excluded function words , assuming that   they had less effect on the sentence - level .   As expected , sentences in the A 1and C 2levels6209   LengthLexical level   A1 A2 B1B2   Lv.1 8 .8 22 .3 10 .9 7 .7 6.3   Lv.2 13 .4 17 .7 13 .4 9 .9 6.7   Lv.3 21 .8 17 .2 13 .7 11 .5 7.3   Lv.4 26 .8 16 .5 14 .2 12 .3 8.9   Lv.5 27 .1 16 .4 9 .9 12 .0 7.3   were particularly scarce . Sentence lengths are not   proportional to CEFR levels ; A level - sentences are   short , whereas B - level sentences and above are sim-   ilar in length . In contrast , the distribution of lexical   levels shows a roughly positive correlation to sen-   tence levels ; A 1 - level words appear significantly   more frequently in lower - level sentences , and B 1   and B 2words in higher - level ones . A 2 - level words   form an exception , appearing most frequently in   the intermediate levels of A 2to B2 .   3.4 Comparison with Existing Corpora   Table 3 shows the confusion matrix between CEFR   levels and Newsela - Auto grade levels assembled   using sentences extracted from Newsela - Auto .   Newsela assigns readability levels using Lexile and   converts them into a K – 12grade level . Newsela-   Auto assigns the grade level of the document to   all the sentences contained in it . The Newsela-   Auto levels scatter across CEFR levels , indicating   that document - based readability levels do not agree   with sentence - based CEFR language ability .   Table 4 shows the distribution of sentence   lengths and lexical levels of content words ( % )   in the sentence complexity corpus of Brunato et al .   ( 2018 ) . This corpus rated sentence complexity on a7 - point scale , with 1indicating ‘ very simple ’ and 7   indicating ‘ very complex ’ . Based on this paper , we   extracted sentences having degrees of agreement   greater than or equal to 10and determined their lev-   els as rounded means of assigned levels . We found   that no sentences were assigned levels higher than   5 , which means this corpus lacks sentences at the   most complex levels . In contrast , CEFR - SP pro-   vides C - level sentences , which are considered the   most complex .   The distributions in Table 4 are distinct from   those in our corpus ( Table 2 ) . Although Brunato   et al . ( 2018 ) reported that sentence length shows   a clear correlation with complexity level , this was   not true for our sentences of level B 1or higher .   In Table 4 , the distribution of each lexical level   across complexity levels was relatively uniform . In   contrast , CEFR - SP showed a positive correlation   between sentence and lexical levels . The results   suggest that the standards of our CEFR - level anno-   tations based on formal language ability descrip-   tions were significantly different from the annota-   tors ’ subjective perception of complexity .   4 Sentence - Level Assessment   We propose a sentence - level assessment model ro-   bust to imbalances in label distribution .   4.1 Problem Definition   CEFR levels are ordinal : e.g. , the B 2level is higher   than the B 1level . It might therefore seem natu-   ral to model the level assessment as a regression   problem . However , the gaps between the levels   can be nonuniform , making the interpretation of   regression outputs difficult ; for example , we can-   not decide whether an output of 0.7corresponds to   A1or A2(Heilman et al . , 2008 ; François , 2009 ) .   Therefore , we model CEFR - level assessment as a   multiclass classification problem .   Given a training corpus with Nlabelled samples   { ( x , y),(x , y),···,(x , y ) } , where x   is a sentence and y∈ { 0,1 , · · · , J−1}indicates   the index of the corresponding level , we train a   classifier that classifies an input sentence into J   classes ; J= 6 in CEFR . For brevity , we do not   distinguish between a level and its index hereafter.6210   4.2 Background : Metric - based Method   Table 2 empirically shows that the distribution of   sentence levels is unbalanced ; the most basic and   highly proficient sentences are the least common .   An unbalanced label distribution leads to overfitting   major classes and ignoring minor ones ; for educa-   tional applications , such infrequent levels can not   be dismissed .   Therefore , we propose a sentence - level assess-   ment model that is robust against label imbalance .   We use a metric - based approach ( Vinyals et al . ,   2016 ; Snell et al . , 2017 ; Ye and Ling , 2019 ; Sun   et al . , 2019 ) that classifies samples based on dis-   tances in a vector space , thereby avoiding overfit-   ting by virtue of the simple inductive bias of a clas-   sifier . The metric - based approach has been studied   for few - shot classification , where unlabelled sen-   tences are classified by the embedding distances   between labelled and unlabelled samples . In con-   trast , we explicitly learn embeddings representing   CEFR levels ( hereafter referred to as prototypes )   and predict sentence levels using cosine similarity .   4.3 Metric - based Level Assessment   We assume that representing a CEFR - level by a   single vector may be insufficient ; allowing mul-   tiple prototypes improves the expressiveness of   level representation . We generate Kprototypes   for each CEFR - level , i.e. ,KJprototypes in total ,   constituting a prototype matrix C∈R. The   k - th prototype of the i - th CEFR - level c∈Rhas   the same dimension das the sentence embedding .   We assume that the similarity between the input   sentence embedding and prototype measures the   likelihood that the sentence has the corresponding   label , as shown in Figure 1 .   We employ a pretrained masked language model   ( MLM ) to encode a sentence . Specifically , we   encode an input sentence with mtokens x={w , w , · · · , w}using MLM to obtain the hid-   den outputs of each token   h , h,···,h = MLM ( w , w , · · · , w ) ,   where h∈R. We generate a sentence embed-   dingx∈Rby mean pooling these token embed-   dings ( Reimers and Gurevych , 2019 ):   x = MeanPool ( h , h,···,h).(1 )   Finally , we compute the distribution pover the   levels for xusing softmax considering similarities   to the prototypes :   p(y = j|x ) = exp ( CosSim ( x , c ) )   Σexp ( CosSim ( x , c ) ) ,   where CosSim ( · , · ) calculates cosine similarity .   When a level has multiple prototypes K > 1 , we   compute the mean of the cosine similarities :   CosSim ( x , c ) = ΣCosSim ( x , c )   K.   4.4 Loss Weighting   The entire model , including MLM , is trained to   minimise cross - entropy loss . For further alleviation   of the unbalanced label distribution , loss weighting   is applied according to the multinomial distribu-   tion of the level frequency ( Conneau and Lample ,   2019 ) .   p = q   Σq , ( 2 )   where qis the frequency of level iin the training   set , and α∈[0,1]controls the weight strength . A   small alpha gives large weights to infrequent labels .   4.5 Prototype Initialisation   The experiments established that the initialisation   of prototypes affects the training stability , as the   prototypes are learned from scratch . Therefore ,   the prototypes have consistent values set during   initialisation to stabilise model training . Assuming   that common characteristics of the same level of   sentences are reflected in their embeddings , we use   the mean of sentence embeddings in Equation ( 1 ):   ˆc = MeanPool ( x , x,···,x ) , where xis   thek - th sentence embedding of level iandnis the   number of sentences at level iin the training set.6211   Because a level is allowed to have multiple pro-   totypes , an initialisation vector is generated for   thek - th prototype at level i,ˆc∈ˆC , by adding   Gaussian noise with mean µ= 0 and variance   σset to 5%of that computed on all elements in   ˆc,ˆc , . . . , ˆc :   ˆc=ˆc+N(µ , σ ) .   Finally , expecting these prototypes to capture the   distinctive features of different levels , we orthogo-   nalise the matrix ˆCand set the initial values of the   prototype matrix C.   5 Evaluation   In this section , the proposed level assessment   model is evaluated using the CEFR - SP corpus .   5.1 Corpus Splitting   We split CEFR - SP into three sets : approximately   80 % for training , 10 % for validation , and 10 % for   the test set , as shown in Table 5 . We adjusted   the number of sentences for infrequent levels to   preserve a reasonable number of test and validation   cases . In corpus splitting , we ensured that highly   similar sentences did not appear in both the training   and validation / test sets , as detailed in Appendix B.   A sentence in CEFR - SP may have as many as   two levels , both assignments being regarded as   equally reliable . Therefore , the predictions during   training , validating , and testing were assumed cor-   rect if they matched either of the annotated labels .   5.2 Evaluation Metrics   The ability to predict alllevels correctly is impor-   tant for educational applications . As the distribu-   tion of levels was unbalanced , the models were   evaluated using macro - F 1to penalise models that   ignored minor classes . In addition , because CEFR   levels are ordinal , the models were also evaluatedusing the quadratic weighted kappa ( Bakeman and   Gottman , 1997 ) .   To reduce the dependence of performance fluctu-   ation on initialisation seeds , the experiments were   conducted 12times with randomly selected seeds .   We then discarded the best and worst results and   reported a mean macro - F 1score and kappa value   with a 95 % confidence interval .   5.3 Setting   We used BERT - Base , cased model ( Devlin et al . ,   2019 ) as the pretrained MLM to encode sentences   in the models that were compared . Specifically ,   we used the outputs of the 11 - th layer , which per-   formed strongly . K , the number of prototypes of   the proposed method , was set to 3to maximise the   average macro - F 1of the validation set in the 1–10   range .   Comparison Because of the roughly positive cor-   relation between the word and sentence levels ( Sec-   tion 3.3 ) , we implemented a bag - of - words ( BoW )   classifier using support vector machines ( Cortes   and Vapnik , 1995 ) as the naive baseline . Moreover ,   as a simpler variant of metric - based classification   method , we implemented a k - nearest neighbour   ( kNN ) ( Fix and Hodges , 1989 ) classifier . We used   mean - pooled token embeddings of freezed BERT   as features and the cosine distance for distance com-   putation . The size of kwas set to 6which marked   the highest macro - F 1on the development set .   As the state - of - the - art baseline , we used a   BERT - based classifier that outperforms conven-   tional linguistic - feature - based classifiers in predict-   ing passage - level readability ( Deutsch et al . , 2020 )   and CEFR levels ( Rama and Vajjala , 2021 ) , as well   as on simple and complex binary classification   ( Garbacea et al . , 2021 ) of the WikiLarge corpus   ( Zhang and Lapata , 2017 ) . The proposed model   was compared with these baselines with or without   loss weighting .   Ablation Study We investigated the effect of K   with an ablation study . We also implemented varia-   tions of the proposed method without loss weight-   ing and initialisation based on sentence embed-   dings . The former method achieved its maximum6212   validation macro - F 1score when K= 1 . The lat-   ter method used the same settings as the proposed   method , except for prototype initialisation ; it ini-   tialised the prototype embeddings using a normal   distribution N(0,1 ) .   5.4 Implementation Details   The classifier layer of the BERT baselines com-   prised a linear layer with weights W∈Rand   a10 % dropout to the input sentence embedding .   Other conditions remained the same as those of   the proposed method . We input a sentence embed-   ding computed by Equation ( 1)and calculated the   standard classification loss of cross - entropy . Loss   weights were computed by Equation ( 2 ) .   All models were implemented using the PyTorch ,   Lightning , Transformers ( Wolf et al . , 2020 ) , and   scikit - learn libraries . The neural network mod-   els were trained on an NVIDIA Tesla V 100GPU   using an AdamW ( Loshchilov and Hutter , 2019 )   optimiser with a batch size of 128 . The training   was stopped early , with 10patience epochs and a   minimum delta of 1.0e−5based on the average   macro - F 1score of all levels measured on the vali-   dation set . The loss weighting factor αand other   hyperparameters were tuned using Optuna ( Akiba   et al . , 2019 ) . For the proposed method and BoW   and BERT baselines , αvalues were set to 0.2,0.3 ,   and0.4 , respectively . The complete hyperparame-   ter settings are described in Appendix C.   5.5 Results   Table 6 shows the CEFR - SP test set results by   means of macro - F 1scores ( % ) per level and   quadratic weighted kappa values with 95 % con-   fidence intervals . As in previous studies , the BERT-   based classifiers outperformed the BoW baselines .   This result confirms that words and their levels , de-   spite their importance , are not solely responsible   for determining sentence levels . The kNN classifier   showed higher macro - F 1scores than BoW with-   out loss weighting on A 2and B 1because of the   powerful BERT embeddings . However , it failed to   identify A 1and C levels , which indicates the signif-   icance of addressing unbalanced label distribution .   The proposed method ( last row ) had the highest   F1scores for infrequent levels , i.e. , A1and C 2 , but   a slightly reduced performance for the more com-   mon levels . We consider this acceptable , consider-   ing the method ’s capability to assess infrequent   levels . Overall , the proposed method achieved   the highest average macro - F 1score ( 84.5 % ) and   quadratic weighted kappa value ( 0.628 ) .   Effects of Loss Weighting While loss weight-   ing is highly effective in alleviating the effects of   unbalanced label distribution on all models , it is   more critical for the proposed method . Exclusion   of loss weighting overlooks the A 1and C 2levels ,   as is clear from the sixth row of Table 6 . Confusion   matrices confirmed that A 1and C 2sentences were   misclassified to their adjacent levels.6213   Effects of Initialisation The seventh row of Ta-   ble 6 presents the results for the proposed method   without initialisation using sentence embeddings .   This method tended to have larger confidence in-   tervals than the proposed model . Moreover , we   observed that it fell into an undesired solution that   overlooked A 1and C 2levels depending on initiali-   sation seeds , as reflected in lower macro - F 1scores .   These results confirm that our initialisation was   effective for training stabilisation .   Effects of Number of Prototypes Figure 2   shows the average macro - F 1scores with 95 % con-   fidence intervals measured on the validation set   when the number of prototypes in the proposed   method changed from 1to10 . The average macro-   F1score initially improved as the number of proto-   types increased ; it peaked at three , and then gradu-   ally decreased . This trend empirically confirms the   effectiveness of multiple prototypes and shows that   a relatively small number of prototypes is sufficient   for CEFR - SP .   Visualisation Figure 3 plots the sentence em-   beddings generated by the proposed method , and   Figure 4 those generated by the BERT baseline   with loss weighting . The gold levels are colour-   coded ; for the proposed method , the prototypes are   indicated by diamond markers . We used T - SNE   ( van der Maaten and Hinton , 2008 ) for visualisa-   tion , setting the perplexity to 30and number of   iterations to 5k to ensure convergence .   The class boundaries were not clear in the em-   beddings of the baseline . In contrast , the embed-   dings of the proposed method formed clear clusters   by level owing to the metric - based classification ;   this improved the interpretability . When assessing   the level of a new sentence , the cosine similarity   to each prototype indicates whether the assessment   result is high - confidence , i.e. , prototypes of a sin-   gle level exhibit significantly high cosine similarity   to the sentence , or ambiguous , i.e. , multiple levels   exhibit competitive cosine similarities .   6 Summary and Future Work   In this study , we introduced CEFR - SP , the first En-   glish sentence corpus annotated with CEFR levels .   The carefully designed annotation procedure in-   volved recruiting experts with strong backgrounds   in English education to ensure the reliability of the   assigned labels . CEFR - SP allows the development   of an automatic sentence - level assessment model .   The proposed method can handle unbalanced level   distributions using a metric - based classification .   Our future work will involve collecting parallel   sentences of CEFR - SP to make it directly applica-   ble for training text simplification models . We will   also develop controllable text simplification mod-   els based on reinforcement learning : the proposed   level assessment model will be employed to reward   the generation of lower - level sentences .   Limitations   Because of severe space constraints , we have re-   ported only the lexical profile of CEFR - SP . We will   present its syntactic and psycholinguistic features   and analyse it from an educational perspective in   a future publication . Moreover , CEFR - SP is not   directly applicable to train controllable text sim-   plification models that require parallel sentences   with different levels . Therefore , we are currently   expanding CEFR - SP to make it parallel through the6214manual rewriting of sentences in the corpus . Our   sentence - level assessment model helps this process .   We can complement sentences of scarce levels by   adding additional rewriting tasks .   We suspect that the proposed method is directly   applicable to other label - imbalanced classification   problems . The empirical investigation of this is   out of the scope of the present paper and is left for   future work .   Ethics Statement   Ethics in Annotation Process The sentences   in CEFR - SP were sampled from Newsela - Auto   ( news articles ) , Wiki - Auto ( Wikipedia articles ) ,   and SCoRE ( sentences written for an educational   application of an academic project ) . We believe   them to be free from harmful content that insults   annotators .   We contracted with a commercial company that   provides data annotation services for academia , in-   cluding the management of annotators . We paid   annotators $ 0.50per sentence , i.e. , approximately   $ 44 / h. This was significantly higher than the mini-   mum wage in the place where this study was con-   ducted , reflecting our respect for the expertise re-   quired .   License Compliance We comply with the li-   censes of the original data sources of CEFR - SP .   Specifically , we separate CEFR - SP sentences by   data source and distribute them with the same li-   cense as the original datasets from which they were   sampled .   Wiki - Auto CC BY - SA 3.0   SCoRE CC BY - NC - SA 4.0   Newsela - Auto We ask people first to obtain   Newsela corpus ( )   and then contact us , following the distribution   policy of Newsela - Auto .   For the reproducibility of the study , the training- ,   validation- , and test - set splits are maintained .   Acknowledgements   We appreciate the anonymous reviewers for their   insightful comments and suggestions to improve   the paper . This work was supported by JSPS KAK-   ENHI Grant Number JP21H03564.References621562166217   A Details of Sentence Selection   Dependence on external factors makes the sentence-   level - assessment problem ill - formed . This phe-   nomenon was noticed in ( Jacob and Uitdenbogerd ,   2019 ): linguistic features that are typically well-   correlated with document readability were poorly   correlated with it in tweets , which inevitably de-   pend on external factors . To avoid this problem , we   carefully selected stand - alone sentences for anno-   tation . For Wiki - Auto , we excluded the first paragraphs   of an article to avoid dictionary - definition - like sen-   tences , e.g. , ‘ X is the capital of country Y ’ . While   we excluded sentences containing named entities   recognised by Stanza , we allowed named entities   of types of DATE , TIME , PERCENT , MONEY ,   QUANTITY , ORDINAL , and CARDINAL , as well   as those in a list that we manually prepared contain-   ing names of well - known regions , countries , and   cities ( e.g. , Europe , France , and Paris ) , and com-   mon personal names ( e.g. , William ) . Finally , we   regularised spellings to the American forms using   the localspelling library .   B Details of Corpus Splitting   First , we computed the cosine distances between   all pairs of sentence embeddings obtained using   a pretrained Sentence - BERT model ( Reimers and   Gurevych , 2019).Next , the average cosine dis-   tance for each sentence was calculated . The sen-   tences were then allocated to the test , validation ,   and training sets according to the descending order   of their average cosine distances . Thus , sentences   with the least similarity to other sentences were   allocated to the test and validation sets , and the rest   to the training set .   C Hyperparameter Settings   For all models , the loss weighting factor αwas   searched in the range [ 0.1,1.0]with 0.1interval .   For neural network models , the learning rate was   searched in the range [ 1e−5,7e−5]with1e−5   interval . For the BoW baseline using support vec-   tor machines , the kernel was chosen from linear   or radial basis function networks , and the regu-   larisation parameter γwas searched in the range   [ 0.01,100 ] by loguniform sampling of 40points .   Table 7 presents the hyperparameter settings of the   proposed and BERT baseline models , Table 8 those   of the BoW baseline .   D Hyperlinks to Libraries   Here we list hyperlinks to the libraries used in im-   plementation .   PyTorch   Lightning6218   Kernel γ α   BoWw / o lossW linear 4.6 –   linear 0.7 0.3   Transformers   scikit - learn   Optuna6219