  Masoud Monajatipoor   UCLA   monajati@ucla.eduLiunian Harold Li   UCLA   liunian.harold.li@cs.ucla.eduMozhdeh Rouhsedaghat   USC   rouhseda@usc.edu   Lin F. Yang   UCLA   linyang@ee.ucla.eduKai - Wei Chang   UCLA   kwchang@cs.ucla.edu   Abstract   Large - scale language models have shown the   ability to adapt to a new task via conditioning   on a few demonstrations ( i.e. , in - context learn-   ing ) . Large - scale language models have shown   the ability to adapt to a new task via condition-   ing on a few demonstrations ( i.e. , in - context   learning ) . However , in the vision - language   domain , most large - scale pre - trained vision-   language ( VL ) models do not possess the abil-   ity to conduct in - context learning . How can   we enable in - context learning for VL models ?   In this paper , we study an interesting hypoth-   esis : can we transfer the in - context learning   ability from the language domain to the VL   domain ? Specifically , we first meta - trains a lan-   guage model to perform in - context learning on   NLP tasks ( as in MetaICL ) ; then we transfer   this model to perform VL tasks by attaching a   visual encoder . Our experiments suggest that   indeed in - context learning ability can be trans-   ferred cross modalities : our model considerably   improves the in - context learning capability on   VL tasks and can even compensate for the size   of the model significantly . On VQA , OK - VQA ,   and GQA , our method could outperform the   baseline model while having ∼20 times fewer   parameters .   1 Introduction   Pre - trained language models have shown impres-   sive performance on a range of tasks by learn-   ing from large - scale text corpus ( Radford et al . ,   2018 , 2019 ; Yang et al . , 2019 ) . Recent studies   find that some of these language models can be   used to perform in - context learning out - of - the - box ,   i.e. , adapting to a task by conditioning on a few   demonstrations in context without any gradient up-   date ( Brown et al . , 2020 ; Min et al . , 2022 ) , which   is highly desirable .   In VL modeling , in - context learning is less ex-   plored and only a handful of models are proposedto perform in - context learning mainly by limit-   ing the amount of deviation of a pretrained large-   scale language model from the language space and   translating visual inputs to language embedding   space . They either require a large capacity ( Tsim-   poukelli et al . , 2021 ; Alayrac et al . , 2022 ) or a giant   corpus consisting of in - context learning examples   ( Alayrac et al . , 2022 ; Liu et al . , 2023 ; Koh et al . ,   2023 ) .   In this work , we explore whether we could en-   able in - context learning in VL tasks without resort-   ing to extreme scale - up . We study an interesting   hypothesis : can we transfer the in - context learn-   ing ability from the language domain to the VL   domain ? To elaborate , not every language model   exhibits excellent in - context learning ability ; recent   studies ( Min et al . , 2022 ) show that one could ex-   plicitly train language models to perform in - context   learning , by training the model on multiple tasks   with in - context few - shot examples , a process that   resembles meta - learning . Thus , an intriguing query   arises : when a language model is first meta - trained   to perform in - context learning , can it be transferred   to perform in - context learning for VL tasks better ?   A remarkable observation in our study is the uti-   lization of a meta - trained language model as the   transformer encoder - decoder and the mapping of   visual features to the language embedding space .   This innovative approach led to the development   of our proposed VL model ( we name it MetaVL ) .   Impressively , our experimental results demonstrate   that MetaVL surpasses the baseline model ’s perfor-   mance , even when MetaVL is designed to be 20   times smaller in size .   This study makes three main contributions : 1 )   To the best of our knowledge , this is the first at-   tempt to transfer the meta - learning knowledge for   in - context learning from single - modality to multi-   modality . 2 ) We propose a VL model , MetaVL ,   which outperforms the baseline in in - context learn-495ing while having a much smaller model size . 3 )   Through extensive experiments on VQA , GQA and   OK - VQA , we demonstrate the in - context learning   capability of MetaVL and analyze its components .   2 Related work   In - context learning in VL . Frozen ( Tsim-   poukelli et al . , 2021 ) is the first attempt for in-   context learning in multimodality by leveraging a   frozen GPT - like language model as the language   backbone and mapping visual features to the lan-   guage embedding space . Frozen sheds light on the   feasibility of benefiting from the frozen LMs in VL   modeling to learn a new task from a few examples   in context . MAGMA ( Eichenberg et al . , 2021 ) is   another encoder - decoder architecture for VL pre-   training which showed that adding adaptor blocks   between the frozen language model layers could   further improve the performance for VL tasks in a   few - shot scenario .   Other recent works ( Yang et al . , 2022 ; Alayrac   et al . , 2022 ; Zeng et al . , 2022 ) follow the similar   principle as the previous works to tackle in - context   learning in VL modeling and achieve superior re-   sults by leveraging extremely large - scale models .   In this paper , we study a problem overlooked   in prior work : we delve into the possibility of en-   abling in - context learning for VL tasks without re-   lying on extensive scalability . Our focus lies in ex-   ploring the hypothesis : Is it feasible to transfer the   in - context learning capability from the language   domain to the VL domain ?   Meta - learning in language modeling Large-   scale language models have shown the capability to   be trained on a new task if properly prompted with   in - context examples , i.e. , in - context learning . In   this learning strategy , the language model is asked   to generate the desired output , e.g. , an answer in   the question - answering task , which is prompted   by a few data examples along with their corre-   sponding supervision sampled from the training   split , and the language model learns the task in   context without performing any gradient updates .   Although such training is highly data - efficient , its   performance is far behind supervised fine - tuning .   Therefore , inspired by ( Vilalta and Drissi , 2002 ; Ev-   geniou and Pontil , 2004 ; Finn et al . , 2017 ; Ruder ,   2017 ) , MetaICL ( Min et al . , 2022 ) proposes train-   ing the model for in - context learning as a kind of   meta - learning . MetaICL meta - trained a gpt lan-   guage model on a diverse set of natural language   tasks and datasets and showed that meta - training a   language model in an in - context learning manner   could significantly improve the in - context learning   capability of the language model for a new task .   3 Approach   In this section , we first explain the existing meta-   training procedure for language modeling and then   introduce our proposed method for in - context learn-   ing in VL .   Meta - training in language modeling . MetaICL   has shown that a language model that is meta-   trained on a diverse set of tasks in an in - context   learning setup is a strong few - shot learner . To meta-   train an auto - regressive language model , in each   iteration , a meta - learning task is randomly cho-   sen from a collection of diverse meta - training lan-   guage tasks , and k+ 1 data - label examples are   randomly sampled from its training split . Then ,   the model is supervised by the concatenation of   ( x , y , x , y , ... , x)which will be fed as a   single input to the model for predicting the la-   bel(y)as the training objective , i.e. , the meta-   training step aims to maximize :   P(y|x , y , · · · , x , y , x ) ( 1 )   During inference , the same in - context setup ( k   examples from the training ) are sampled from a496target dataset to be used as the ( x , y)(x , y ) ·   · · , ( x , y)(x)and given to the model to predict the   labely .   The meta - trained language model trained on a   diverse set of natural language datasets has shown   good performance for an unseen task when few   data are given in context ( Min et al . , 2022 ) .   MetaVL - a VL method with meta - learning   knowledge for in - context learning . MetaVL has   three main submodels including a meta - trained   encoder - decoder and is being trained using Pre-   fix Language Modeling ( PrefixLM ) ( Wang et al . ,   2021 ) . In the following , we discuss each submodel   in detail .   Visual encoder and visual prefix . The visual   encoder is defined as a function V(x)that takes an   image of x and outputs visual features . We extract   the feature grid before the pooling layer n×D   where nis the number of feature maps and Dis   the feature size of the visual encoder . Then , the   output features can be viewed as a sequence of n   visual tokens representing the image .   The visual encoder is followed by the visual pre-   fix module that is defined as V(x)∈D×D   which maps the visual features to language embed-   ding space . This module is seeking to properly   project the visual tokens into language tokens .   During the VL training , the parameters of both   of these modules are trainable and are learned with   different learning rates by back - propagation guided   by the frozen language model .   Language encoder - decoder The meta - trained   language encoder - decoder is used as the LM back-   bone and is frozen during the VL training pro-   cess so the meta - trained language model preserves   its few - shot capabilities . The language encoder   encodes the text into text tokens represented by   t , t , ... , t. Then , given the multimodal tokens   ( image and text ) as U = v , v , ... , v , t , t , ... , t   the decoder is trained to reconstruct the correspond-   ing text with a standard language modeling objec-   tive to maximize the following likelihood :   L(U ) = /summationdisplaylogP(t|v , ... , v , t , ... t;θ)(2 )   After the VL training , for learning a new VL task   in - context , given a few examples from a new task   with a new format , we concatenate k sampled data-   label pairs from the training split along with one   data from the val / test split to construct the promptand feed it to the model for predicting the desired   output . The entire process is visualized in Fig . 1 .   4 Experiments   4.1 Datasets and Baseline   We use the dataset proposed in ( Min et al . , 2022 )   as the meta - training dataset for the language model   and the COCO dataset ( Lin et al . , 2014 ) as the VL   training dataset for MetaVL . The evaluation exper-   iments are conducted on three datasets including   VQA ( Antol et al . , 2015 ) , OK - VQA ( Marino et al . ,   2019 ) , and GQA ( Hudson and Manning , 2019 ) .   Frozen leveraged an internal GPT - like language   model with 7 billion parameters as the backbone   of their proposed model . As their model is not   publicly available , we trained Frozen with GPT2-   Medium as the frozen language model and consider   it as our main baseline ( Frozen ) due to its model   size . We also train a frozen with GPT - J 6B ( The   most similar GPT to Frozen ) language model and   obtained a close performance to the original Frozen   model and use it as our second baseline denoted by   Frozen .   4.2 Training and evaluation setting   Initially , We meta - train a GPT2 - Medium LM on a   collection of 142 meta - training language datasets   with a learning rate of 1e-5 and a batch size of 8   using the setting named as “ HR →LR with instruc-   tions ( all ) ” where datasets with equal or greater   than 10,000 training examples are used as meta-   training tasks and the rest of the datasets are used   as target tasks . The training is done on 8 NVIDIA   RTX A6000 for 80,000 steps which took ∼6 hours .   Then , we train MetaVL on the training split of   COCO where we use a learning rate of 5e-5 and   2e-6 for the visual prefix and visual encoder , re-   spectively , while the rest of the model parameters   are frozen . We use a batch size of 32 and trained   MetaVL using 4 NVIDIA RTX A6000 for 8 epochs   which take ∼48 hours . Inference time depends on   the numebr of shots varies from 2 - 5 hours for 0 - 3   shots on 5000 test examples . Our visual encoder is   CLIP - RN50x16 ( Radford et al . , 2021 ) with a fea-   ture grid size of 144×3072 and our visual prefix   is an MLP layer with a dimension of 3072×768 .   For in - context evaluation on VQA datasets , we   randomly pick a specific number -n- of sampled   data - label pairs , known as shots , from the training   set and feed them to the model in - context followed   by a single data from the val / test set . Fig . 2 pro-497vides some illustrative examples for the evaluation   process .   To conduct the evaluation , we utilize a subset   of 5,000 instances from the val / test dataset due to   computational constraints . The generated output   from the model is then compared against the ex-   pected answer , as established in previous studies .   In cases where an exact match is not achieved , we   employ a technique to identify the most closely re-   lated answer from a set of candidate answers ( The   set can be defined as a unique list of all answers in   the training dataset ) . This involves computing the   cosine similarity between the output ’s embedding   and each candidate answer ’s embedding achieved   by Sentence BERT ( Reimers and Gurevych , 2019 ) .   We then compare the selected output with the   corresponding answer to determine the match . The   training datasets for VQA , OK - VQA , and GQA   contain approximately 3,000 , 4,200 , and 3,000 dis-   tinct answers , respectively . Furthermore , we per-   formed an additional round of human evaluation on   model ’s output without matching , and the findings   are summarized in the appendix ( Table 2 ) . The   human evaluation on a separate test set of 2000 ex-   amples aimed to delve deeper into instances where   the model ’s output , while accurate , did n’t precisely   match the provided answer . Three such examples   are presented in Fig 3 , where the initial evaluation   did not consider the prediction as correct , but it   was deemed correct in the subsequent evaluation   setting .   4.3 Results and analysis   Quantitative analysis To evaluate MetaVL , we   consider three common visual question - answering   datasets including VQA , OK - VQA , and GQA . We   compare MetaVL results with the mentioned two   baselines in Table 1 for 3 - shot in - context learning   based on both automatic and human evaluation . Ac-   cording to the results , the performance of Frozen   improves as its model size increases while MetaVL   achieved competitive results in all three tasks . To   further analyze how many image - text pairs are re-   quired to enable In - context learning for the VL   task , we have trained MetaVl with 50 percent of   training data and the results show that the perfor-   mance slightly dropped but the model preserve its   capability to learn from in - context data ( Table 3 ) .   The effect of the number of in - context shots   According to Figure 4 , in almost all settings , the   performance of MetaVL is improving by increasing   the number of shots which shows the model is gain-   ing knowledge from the data in context . This result   further gives us an illustration of the model ’s ca-498   pability to learn from the in - context examples sup-   porting that MetaVL is benefiting from the meta-   learning knowledge for in - context learning . The   numbers on the graph are summarized in Table 2   in the appendix .   The effect of having adaptor layers in LM   MAGMA claims that adding trainable adaptor lay-   ers and letting the LM slightly be trained during   the VL training process is beneficial for in - context   learning . Compared with Frozen , in addition to   being trained on an x8 larger set of VL datasets ,   MAGMA also includes the training splits of the   target datasets to its training set , while Frozen   is adapted to an unseen new task in - context ( in-   context learning ) . We evaluated this method by   adding adaptor layers to both Frozen and MetaVL   and denoted the corresponding models by Frozen   w / adap and MetaVL w / adap , respectively , in Fig .   4 . Our results demonstrate that having a fully   frozen language model in MetaVL could better pre-   serve the in - context learning ability of the language   model . It is also noticeable that adding adaptor lay-   ers improves the zero - shot performance of Frozen .   We hypothesize that this improvement is due to   getting a better vision and language alignment by   letting both vision and language submodels be in-   volved in the alignment process . Qualitative analysis We provide some qualita-   tive examples to better illustrate the performance of   MetaVL for in - context learning in different VQA   tasks . In Fig . 2 , a few examples are provided   which show the output of MetaVL for 3 - shot in-   context learning . More examples are presented in   Appendix .   5 Conclusion   We investigate the feasibility of transferring meta-   learning knowledge for in - context learning from   resource - rich single modality to multimodality . We   have shown that by leveraging a meta - trained lan-   guage model in a VL model , we can transfer the   ability of “ learning to learn ” in context to VL and   it results in a strong VL few - shot leaner . With ex-   tensive experiments on three common VL datasets ,   we have shown that the in - context learning perfor-   mance of MetaVL is superior compared with the   baseline even when the size of our model is 20   times smaller .   6 acknowledgment   This work was supported by DARPA under agree-   ment HR00112190130 and DARPA MSC program   under agreement N660011924032 . We would like   to thank the reviewers for their feedback to improve   this research work.499Limitations   While we have shown the potential of transferring   in - context learning ability from a language model   to VL tasks , the experiments in this paper are lim-   ited in two aspects . ( 1 ) We considered only the   VQA task , which is limited in scope . It is unclear   whether our method generalizes to other VL tasks .   In fact , as most tasks in the VL domain take the   form of visual question answering , it is less well-   defined what would “ cross - task generalization ” en-   tail in VL , compared to in NLP where ( 2 ) Due to   computational limitations , we experiment with only   a moderate - sized LM . It is unclear the performance   of our method after scaling up .   References500   A Appendix501502503504505506ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   6   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   4.2   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   4.2507 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   4.2   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   4.2   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   4.2   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.508