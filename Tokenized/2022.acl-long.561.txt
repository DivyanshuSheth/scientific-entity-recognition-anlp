  Jiangbin Zheng , Yile Wang , Ge Wang , Jun Xia ,   Yufei Huang , Guojiang Zhao , Yue Zhang , Stan Z. Li   School of Engineering , Westlake University   Institute of Advanced Technology , Westlake Institute for Advanced Study   { zhengjiangbin , wangyile , wangge , xiajun , huangyufei ,   zhaoguojiang,zhangyue,Stan.ZQ.Li}@westlake.edu.cn   Abstract   Although contextualized embeddings gener-   ated from large - scale pre - trained models per-   form well in many tasks , traditional static em-   beddings ( e.g. , Skip - gram , Word2Vec ) still   play an important role in low - resource and   lightweight settings due to their low compu-   tational cost , ease of deployment , and stabil-   ity . In this paper , we aim to improve word em-   beddings by 1 ) incorporating more contextual   information from existing pre - trained models   into the Skip - gram framework , which we call   Context - to - Vec ; 2 ) proposing a post - processing   retrofitting method for static embeddings inde-   pendent of training by employing priori syn-   onym knowledge and weighted vector distri-   bution . Through extrinsic and intrinsic tasks ,   our methods are well proven to outperform the   baselines by a large margin .   1 Introduction   Contextualized embeddings such as BERT ( Devlin   et al . , 2018 ) and RoBERTa ( Liu et al . , 2019 ) have   become the default architectures for most down-   stream NLP tasks . However , they are computation-   ally expensive , resource - demanding , hence envi-   ronmentally unfriendly . Compared with contex-   tualized embeddings , static embeddings like Skip-   gram ( Mikolov et al . , 2013 ) and GloVe ( Pennington   et al . , 2014 ) are lighter and less computationally   expensive . Furthermore , they can even perform   without significant performance loss for context-   independent tasks like lexical - semantic tasks ( e.g. ,   word analogy ) , or some tasks with plentiful labeled   data and simple language ( Arora et al . , 2020 ) .   Recent work has attempted to enhance static   word embedding while maintaining the benefits   of both contextualized embedding and static em-   bedding . Among these efforts , one category is the   direct conversion of contextualized embeddingsFigure 1 : The overall training pipeline of our proposed   word embeddings training and post - processing methods .   In the Context - to - Vec phase , static word embeddings   are trained using contextualized embeddings based on   a monolingual corpus . While in the post - processing   phase , external knowledge is introduced to fine - tune the   word vectors based on the graph topology .   to static embeddings ( Bommasani et al . , 2020 ) .   The other category of enhancement is to make   use of contextualized embeddings for static embed-   dings ( Melamud et al . , 2016 ) . The latter category   is a newer paradigm , which we call Context - to - Vec .   This paradigm not only alleviates the word sense   ambiguities from static embedding , but also fuses   more syntactic and semantic information in the   context within a fixed window .   For the Context - to - Vec paradigm , an association   between contextualized word vectors and static   word vectors is essentially required . In this case ,   the contextualized signal serves as a source of   information enhancement for the static embed-   dings ( Vashishth et al . , 2018 ) . However , the ex-   isting efforts only consider the contextualized em-   beddings of center words as the source , which is   actually incomplete since the contextualized fea-   tures for the context words of the center words are   ignored.8154In addition , benefiting from the invariance and   stability of already trained static embeddings , post-   processing for retrofitting word vectors is also an ef-   fective paradigm for improving static embeddings .   For example , one solution is an unsupervised ap-   proach that performs a singular value decompo-   sition to reassign feature weights ( Artetxe et al . ,   2018 ) , but this does not utilize more external knowl-   edge and lacks interpretation . Poor initial spatial   distribution of word embeddings obtained from   training may lead to worse results . Another com-   mon solution is to use a synonym lexicon ( Faruqui   et al . , 2014 ) , which exploits external prior knowl-   edge with more interpretability but does not take   into account the extent of spatial distance in the   context .   In this work , we unify the two paradigms above   within a model to enhance static embeddings .   On the one hand , we follow the Context - to - Vec   paradigm in using contextualized representations   of center words and their context words as refer-   ences for static embeddings . On the other hand ,   we propose a graph - based semi - supervised post-   processing method by using a synonym lexicon   as prior knowledge , which can leverage proximal   word clustering signals and incorporate distribu-   tion probabilities . The overall training pipeline   is shown in Fig.1 . The pipeline is divided into   two separate phases , where the first phase follows   theContext - to - Vec paradigm by distilling contex-   tualized information into static embeddings , while   the second phase fine - tunes the word embeddings   based on graph topology . To validate our proposed   methods , we evaluate several intrinsic and extrin-   sic tasks on public benchmarks . The experimental   results demonstrate that our models significantly   outperform traditional word embeddings and other   distilled word vectors in word similarity , word anal-   ogy , and word concept categorization tasks . Be-   sides , our models moderately outperform baselines   in all downstream clustering tasks .   To our knowledge , we are the first to train   static word vectors by using more contextual   knowledge in both training and post - processing   phases . The code and trained embeddings   are made available at https://github.com/   binbinjiang / Context2Vector .   2 Related Work   Word Embeddings . For traditional static word em-   beddings , Skip - gram and CBOW are two modelsbased on distributed word - context pairs ( Mikolov   et al . , 2013 ) . The former uses center words to   predict contextual words , while the latter uses con-   textual words to predict central words . GloVe is   a log - bilinear regression model which leverages   global co - occurrence statistics of corpus ( Penning-   ton et al . , 2014 ) ; FASTTEXT takes into account   subword information by incorporating character n-   grams into the Skip - gram model ( Bojanowski et al . ,   2017 ) . While contextualized word embeddings ( Pe-   ters et al . , 2018 ; Devlin et al . , 2018 ) have been   widely used in modern NLP . These embeddings   are actually generated using language models such   as LSTM and Transformer ( Vaswani et al . , 2017 )   instead of a lookup table . This paradigm can gen-   erally integrate useful sentential information into   word representations .   Context - to - Vec . The fusion of contextual-   ized and static embeddings is a newly emerged   paradigm in recent years . For instance , Vashishth   et al . ( 2018 ) propose SynGCN using GCN to cal-   culate context word embeddings based on syntax   structures ; Bommasani et al . ( 2020 ) introduce a   static version of BERT embeddings to represent   static embeddings ; Wang et al . ( 2021 ) enhance   the Skip - gram model by distilling contextual in-   formation from BERT . Our work also follows this   paradigm but introduce more context constraints .   Post - processing Embeddings . Post - processing   has been used for improving trained word embed-   dings . Typically , Faruqui et al . ( 2014 ) use synonym   lexicons to constrain the semantic range ; Artetxe   et al . ( 2018 ) propose a method based on eigen-   value singular decomposition . Similar to these   techniques , our post - processing method is easy   for deployment and can be applied to any static   embeddings . The difference is that we not only   take advantage of the additional knowledge , but   also consider the distance weights of the word vec-   tors , overcoming the limitations of existing meth-   ods with better interpretability .   3 Proposed Methods   3.1 Embedding Representations   As shown in Fig.2 , our proposed framework con-   sists of four basic components . Formally , given   a sentence s={w , w , ... , w}(w∈D ) ,   our objective is to model the relationship be-   tween the center word wand its context words   { w , ... , w , w , ... , w } .   Contextualized Embedding Module . To incor-8155   porate contextualized information , an embedding   uof the center word wneeds to be generated from   a pre - trained language model ( Fig.2(a ) ) . Taking the   BERT model as an example , the center word w   is first transformed into a latent vector h , then   his fed to a bidirectional Transformer for self-   attention interaction . Finally , the output represen-   tation o∈Ris linearly mapped to u∈R   through a linear layer as :   u = WLinear(SA ( h ) ) = Wo , ( 1 )   where W∈Rdenotes model parameters ,   Linear ( ∗)denotes a linear mapping layer , and   SA(∗)denotes self - attention . In practice , the size   ofoisd= 768 , and the size of uisd= 300 .   Thehhere is a sum of the Token Embedding E   and the Positional Embedding PEas :   h = E+PE . ( 2 )   Static Embedding Module . The Skip - gram   model ( Fig.2(b ) ) is used as the static embedding   module . Our method does not directly fit the Skip-   gram model by replacing an embedding table , al-   though the original Skip - gram uses an embedding   table of center words as the final embedding . In-   stead , to make the context words predictable andto enable negative sampling from the vocabulary ,   contextualized representations are used for the cen-   ter words , while an embedding table of the context   words is used for the output static embedding .   3.2 Heuristic Semantic Equivalence   As mentioned above , a key issue for the Context-   to - Vec paradigm is to bridge the gap between con-   textualized and static word vectors . To this end , a   main intuition is to find key equivalent semantic   connections between contextualized vectors and   static vectors . We take the following heuristics :   Heuristic 1 : For a given sentence , the contextu-   alized embedding representation of a center word   can be semantically equivalent to the static embed-   ding of the center word in the same context .   According to Heuristic 1 , in order to model the   center word wand its context words w(note   here that the illegal data that indexes less than 0 or   greater than the maximum length are ignored ) , a   primary training target is to maximize the probabil-   ity of the context words w(|j| ∈[1 , w])in the   Skip - gram model :   p(w|w ) = exp(uu)Pexp(uu),(3 )   where uis the contextualized representation of8156the center word , and uis the static embedding   from a center word wthat is generated by a static   embedding table with size d= 300 .   ForHeuristic 1 , the contextualized word em-   bedding of any center word is essentially used as   reference for corresponding static word embedding .   Such a source for information enhancement im-   plicitly contains the context of the contextualized   embedding , but explicitly ignores the contextual   information which is easily accessible . Hence , the   proposed :   Heuristic 2 : Inspired by the idea of Skip - gram-   like modeling , the contextualized embedding rep-   resentation for the context words of a center word   can be also semantically equivalent to represent   the static embedding of the center word .   To model this semantic relationship , we intro-   duce a Tied Contextualized Attention module   ( Fig.2(d ) ) for explicitly attending contextual sig-   nals , which complements Heuristic 1 by incorpo-   rating more linguistic knowledge into the static   embedding . In particular , assume that the center   word win the contextualized embedding module   corresponds to the contextual vocabulary notated   as{w , ... , w , w ... , w } , then the out-   put contextual attention vector can be computed as :   V = λV + λV   = λoW+λτ(Uϕ(oW ) )   = λoW+λϕ(Po )   2wW ,   ( 4 )   where V denotes the embedding representa-   tion of the center word , which is a residual connec-   tion here . And V denotes the embedding   representations of corresponding context words . ϕ   is an optional nonlinear function , U(∗)is a merge   operation , and τis an average pooling operation .   W∈RandW∈Rare trainable   parameters , in which Wdenotes the weight as-   signment of each context vector .   Since each ohas similar linguistic properties ,   the weight Wcan be shared , and we name this   module Tied Contextualized Attention mecha-   nism . Therefore , the weighted average of the linear   transformation of all context vectors can be reduced   to the weighted linear output of the average of all   vectors as shown in Eq.4 . This weight - sharing   mechanism can help speed up calculations .   In practice , to reduce the complexity , the weight   parameter λandλare the same ; the uin Eq.1can be directly used as V ; the value of wis   the same as that of w , e.g. , 5 .   3.3 Training Objectives   The modular design requires our model to satisfy   multiple loss constraints simultaneously , allowing   static embeddings to introduce as much contex-   tual information as possible . Given a training cor-   pus with Nsentences s={w , w , ... , w}(c∈   [ 1 , N ] ) , our loss functions can be described as fol-   lows .   Semantic Loss . As illustrated in Heuristic 1 ,   one of our key objectives is to learn the semantic   similarity between the contextualized embedding   and the static embedding of the center word . To   speed up computation , the inner product of the   normalized vectors can be used as the loss L :   L=−XX(logσ((Xu)u ) ] ,   ( 5 )   where σis the sigmoid function .   Contextualized Loss . As described in Heuristic   2 , the contextualized embeddings for the context   words of the center word are explicitly introduced   to further enhance the static embedding , thus the   Contextualized Loss Lis expressed as :   L=−XX(logσ(V u ) ) . ( 6 )   Contrastive Negative Loss . Negative noisy   samples ( Fig.2(c ) ) can improve the robustness   and effectively avoid the computational bottleneck .   This trick is common in NLP . Our Contrastive Neg-   ative Loss Lis calculated as :   L = XXXE[logσ(uu ) ] ,   ( 7 )   where wdenotes a negative sample , kis the   number of negative samples and P(w)is a noise   distribution set .   Joint Loss . The final training objective is a joint   lossLfor multi - tasks as :   L = ηL+ηL+ηL , ( 8)   where each hyperparameter ηdenotes a weight.8157   3.4 Graph - based Post - retrofitting   In the post - processing stage , we propose a new   semi - supervised retrofitting method for static word   embeddings based on graph topology ( Xia et al . ,   2022 ; Wu et al . , 2021 , 2020 ) . This method over-   comes the limitations of previously existing work   by 1 ) using a synonym lexicon as priori exter-   nal knowledge . Since both contextualized embed-   dings and static embeddings are trained in a self-   supervised manner , the word features originate only   from within the sequence and no external knowl-   edge is considered ; 2 ) converting the Euclidean   distances among words into a probability distribu-   tion ( McInnes et al . , 2018 ) , which is based on the   special attributes that the trained static word vec-   tors are mapped in a latent Euclidean space and   remain fixed .   Word Graph Representation . Suppose that   V={w , ... , w}is a vocabulary ( i.e. , a collec-   tion of word types ) . We represent the semantic   relations among words in Vas an undirected graph   ( V , E ) , with each word type as a vertex and edges   ( w , w)∈Eas the semantic relations of interest .   These relations may vary for different semantic lex-   icons . Matrix Qrepresents the set of trained word   vectors for q∈R , in which qcorresponds to   the word vector of each word winV.   Our objective is to learn a set of refined word   vectors , denoted as matrix Q= ( q , ... , q ) , with   the columns made close to both their counterparts   inQand the adjacent vertices according to the   probability distribution . A word graph with such   edge connectivity is shown in Fig.3 , which can beinterpreted as a Markov random field ( Li , 1994 ) .   Retrofitting Objective . To refine all word vec-   tors close to the observed value qand its neighbors   q((i , j)∈E ) , the objective is to minimize :   Ψ(Q ) = X(α||q−q||   + βXγ||q−q||),(9 )   where α , β , and γcontrol the relative strengths   of associations , respectively . Since Ψis convex   inQ , we can use an efficient iterative update algo-   rithm . The vectors in Qare initialized to be equal   to the vectors in Q. Assuming that whasmad-   jacent edges corresponding to msynonyms , then   we take the first - order derivative of Ψwith respect   to aqvector and equate it to zero , yielding the   following online update :   q = αq+βPγq   m. ( 10 )   By default , αandβtake the same value 0.5 ,   andγcan be expressed as :   γ = g(d|σ , ν ) = C(1 + d   σν )   ∈(0,1 ] ,   ( 11 )   in which σis a scale parameter , νis a positive   real parameter , and Cis the normalization factor   ofνas ( the following Γ(∗)denotes the gamma   function ):   C= 2π(Γ()√νπΓ ( ) ) , ( 12 )   anddcalculates the sum of Euclidean distances   of the feature vectors across all dimensions Dim   as :   d = vuutX(q−q ) . ( 13 )   Through the above process , the distance distribu-   tion is first converted into a probability distribution ,   and then the original word graph is represented as   a weighted graph . This retrofitting method is mod-   ular and can be applied to any static embeddings .   4 Experiments   We use Wikipedia to train static embeddings . The   cleaned corpus has about 57 million sentences and   1.1 billion words . The total number of vocabularies   is 150k . Sentences between 10 and 40 in length   were selected during training.8158   4.1 Evaluation Benchmarks   We conduct both intrinsic and extrinsic evaluations .   Intrinsic Tasks . We conduct word similar-   itytasks on the WordSim-353 ( Finkelstein et al . ,   2001 ) , SimLex-999 ( Kiela et al . , 2015 ) , Rare Word   ( RW ) ( Luong et al . , 2013 ) , MEN-3 K ( Bruni et al . ,   2012 ) , and RG-65 ( Rubenstein and Goodenough ,   1965 ) datasets , computing the Spearman ’s rank   correlation between the word similarity and hu-   man judgments . For word analogy task , we   compare the analogy prediction accuracy on the   Google ( Mikolov et al . , 2013 ) dataset . The Spear-   man ’s rank correlation between relation similarity   and human judgments is compared on the SemEval-   2012 ( Jurgens et al . , 2012 ) dataset . Word concept   categorization tasks involves grouping nominal   concepts into natural categories . We evaluate on   AP ( Almuhareb , 2006 ) , Battig ( Baroni and Lenci ,   2010 ) and ESSLI ( Baroni et al . , 2008 ) datasets .   Cluster purity is used as the evaluation metric .   Extrinsic Tasks . The CONLL-2000 shared   task ( Sang and Buchholz , 2000 ) is used for chunk-   ingtasks and F1 - score is used as the evaluation   metric ; OntoNotes 4.0 ( Weischedel et al . , 2011 ) is   used for NER tasks and F1 - score is used as the   evaluation metric ; And the WSJ portion of Penn   Treebank ( Marcus et al . , 1993 ) is used for POS   tagging tasks , and token - level accuracy is used   as the evaluation metric . These tasks are reimple-   mented with the open tool NCRF++ ( Yang andZhang , 2018 ) .   4.2 Baselines   As shown in Table 1 , baselines are classified into   three categories . For the first category ( Static ) ,   static embeddings come from a lookup table . Note   here that Skip - gram(context ) denotes the results   from the context word embeddings . For the sec-   ond category ( Contextualized ) , static embeddings   come from contextualized word embedding mod-   els ( i.e. , BERT , ELMo , GPT2 , and XLNet ) for   lexical semantics tasks . The models with _ token   use the mean pooled subword token embeddings as   static embeddings ; The models with _ word take   every single word as a sentence and output its   word representation as a static embedding ; The   models with _ avg take the average of output over   training corpus . For the last category ( Context-   to - Vec ) , contextualized information is integrated   into Skip - gram embeddings . Among these mod-   els , ContextLSTM ( Melamud et al . , 2016 ) learns   the context embeddings by using single - layer bi-   LSTM ; SynGCN ( Vashishth et al . , 2018 ) uses GCN   to calculate context word embeddings based on   syntax structures ; BERT+Skip - gram ( Wang et al . ,   2021 ) enhances the Skip - gram model by adding   context syntactic information from BERT , which is   our primary baseline.8159   4.3 Quantitative Comparison   Word Similarity and Analogy . Table 1 shows the   experimental results of intrinsic tasks . Overall , the   models that integrate contextualized information   into static embeddings ( Context - to - Vec ) perform   better than other types ( Contextualized / Satic ) . Our   results outperform baselines across the board . To   be fair , the backbone of our model here is BERT   as that in the main baseline ( BERT+Skip - gram )   ( Wang et al . , 2021 ) .   Within the Context - to - Vec category , our mod-   els perform best on all word similarity datasets .   Our base model without post - processing ob-   tains an average absolute improvement of about   +23.8%(+13.2 ) and related improvement of   +4.4%(+2.9 ) compared with the main baseline .   The performance is further enhanced using post-   processing with a +25.6%(+14.2 ) absolute increase ,   and a +5.8%(+3.8 ) relative increase compared with   the main baseline , and a +1.4%(+1.0 ) relative in-   crease compared with our base model ( w/o post-   processing ) . It is worth mentioning that the main   baseline does not perform better than BERTin   Contextualized group on the RG65 dataset , but our   model does make up for their regrets , which in-   dicates that our model is better at understanding   contextual correlates of synonymy .   For the word analogy task , our performances   are basically equal to the baselines . Overall , we   gain the best score ( +0.5 ) on the Google dataset   but without a significant improvement . Although   we do not gain the best score across all baselines   on the SemEval dataset , our model performs better   than the main baseline .   For different datasets , especially in word simi-   larity tasks , the improvement of our preliminary   model on WS353 , SimiLex , RG65 ( +4.1 , +5.5 ,   and +5.7 , respectively ) is significantly better than   other datasets . For example , the improvement of   the main baseline on the WS353R ( relatedness )   subset and the WS353 set is far greater than that   on the WS353S ( similarity ) subset . While our   model bridges their gaps in the WS353 set and   also ensures that the performance of WS353S and   WS353R is further improved slightly .   Word Concept Categorization . Word concept   categorization is another important intrinsic evalu-   ation metric . We use 4 commonly used datasets as   shown in Table 2 . Overall , our model without post-   processing outperforms the baselines by a large   margin , giving the best performance and obtaining   an average performance gain of +5.2%(+5.1 ) com-   pared to the main baseline . In particular , the largest   increases are observed on the ESSLI(N ) ( +7.5 ) ,   ESSLI(V ) ( +3.8 ) . And with post - processing , our   model can obtain better improvements ( +3.3 vs.   +5.1 ) . The experimental results show the advan-   tage of integrating contextualized and word co-   occurrence information , which can excel in group-   ing nominal concepts into natural categories .   Extrinsic Tasks . Extrinsic tasks reflect the effec-   tiveness of embedded information through down-   stream tasks . We conduct extrinsic evaluation from   chunking , NER , and POS tagging tasks as shown in8160   Table 3 . We select comparison representatives from   theStatic group , the Contextualized group , and the   Context - to - Vec group , respectively . Although the   improvement is not significant compared with the   intrinsic evaluations , it can be seen that our per-   formances are better than the baselines , which can   prove the superiority of our model . The primary   baseline BERT+Skip - gram obtains the second - best   average score , but does not excel in the chunking   task . In contrast , our model not only outperforms   all baselines moderately on average , but also per-   forms best in every individual task .   4.4 Ablation and Analysis   Post - processing Schemes . From Table 1 , we can   initially find that the post - processing method has   a positive impact . To further quantitatively ana-   lyze , we compare more related methods as shown   in Table 4 . In this ablation experiment , the compar-   ison baseline is our trained original word vectors   ( w/o retrofitting ) , and the other comparison meth-   ods include the singularity decomposition - based   method ( Artetxe et al . , 2018 ) , and the synonym-   based constraint method ( Faruqui et al . , 2014 ) .   From the results , we can see that other post-   processing schemes can improve the word vectors   to some extent , but do not perform better in all   datasets . However , our proposed post - processing   scheme performs the best across the board here ,   which shows that converting the distance distribu-   tion into a probability distribution is more effective .   Nearest Neighbors . To further understand the   results , we show the nearest neighbors of the words   " light " and " while " based on the cosine similarity ,   as shown in Table 5 . For the noun " light " , other   methods generate more noisy and irrelevant words ,   especially static embeddings . In contrast , theContext - to - Vec approaches ( Ours & BERT+Skip-   gram ) can capture the key meaning and generate   cleaner results , which are semantically directly re-   lated to " light " literally . For the word " while " , the   static approaches tend to co - occur with the word   " while " , while Context - to - Vec approaches return   conjunctions with more similar meaning to " while " ,   such as " whilst " , " whereas " and " although " , which   demonstrates the advantage of using contextualiza-   tion to resolve lexical ambiguity .   Word Pairs Visualization . Fig.4 shows the   3D visualization of the gender - related word pairs   based on t - SNE ( Van der Maaten and Hinton ,   2008 ) . These word pairs differ only by gender ,   e.g. , " nephew vs.niece " and " policeman vs.po-   licewoman " . From the topology of the visualized   vectors , the spatial connectivity of the word pairs in   Skip - gram and GloVe is rather inconsistent , which   means that static word vectors are less capable of   capturing gender analogies . In contrast , for vec-   tors based on contextualized embeddings , such as   BERT , SynGCN , BERT+Skip - gram , and our   model , the outputs are more consistent . In par-   ticular , our outputs are highly consistent in these   instances , which illustrates the ability of our model   to capture relational analogies better than baselines   and the importance of contextualized information   based on semantic knowledge .   5 Conclusion   We considered improving word embeddings by in-   tegrating more contextual information from exist-   ing pre - trained models into the Skip - gram frame-   work . In addition , based on inherent properties of   static embeddings , we proposed a graph - based post-   retrofitting method by employing priori synonym   knowledge and a weighted distribution probability.8161The experimental results show the superiority of   our proposed methods , which gives the best results   on a range of intrinsic and extrinsic tasks compared   to baselines . In future work , we will consider prior   knowledge directly during training to avoid a multi-   stage process .   Acknowledgements   This work is supported in part by the Science and   Technology Innovation 2030 - Major Project ( No .   2021ZD0150100 ) and National Natural Science   Foundation of China ( No . U21A20427 ) . We thank   all the anonymous reviewers for their helpful com-   ments and suggestions .   References81628163