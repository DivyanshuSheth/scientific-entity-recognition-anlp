  Zhenrui Yue   UIUC   zhenrui3@illinois.eduHuimin Zeng   UIUC   huiminz3@illinois.eduBernhard Kratzwald   EthonAI   bernhard.kratzwald@ethon.ai   Stefan Feuerriegel   LMU Munich   feuerriegel@lmu.deDong Wang   UIUC   dwang24@illinois.edu   Abstract   Question answering ( QA ) has recently shown   impressive results for answering questions from   customized domains . Yet , a common challenge   is to adapt QA models to an unseen target do-   main . In this paper , we propose a novel self-   supervised framework called QADA for QA   domain adaptation . QADA introduces a novel   data augmentation pipeline used to augment   training QA samples . Different from exist-   ing methods , we enrich the samples via hid-   den space augmentation . For questions , we   introduce multi - hop synonyms and sample aug-   mented token embeddings with Dirichlet distri-   butions . For contexts , we develop an augmenta-   tion method which learns to drop context spans   via a custom attentive sampling strategy . Addi-   tionally , contrastive learning is integrated in the   proposed self - supervised adaptation framework   QADA . Unlike existing approaches , we gener-   ate pseudo labels and propose to train the model   via a novel attention - based contrastive adapta-   tion method . The attention weights are used   to build informative features for discrepancy   estimation that helps the QA model separate   answers and generalize across source and tar-   get domains . To the best of our knowledge , our   work is the first to leverage hidden space aug-   mentation and attention - based contrastive adap-   tation for self - supervised domain adaptation in   QA . Our evaluation shows that QADA achieves   considerable improvements on multiple target   datasets over state - of - the - art baselines in QA   domain adaptation .   1 Introduction   Question answering ( QA ) is the task of finding   answers for a given context and a given question .   QA models are typically trained using data triplets   consisting of context , question and answer . In the   case of extractive QA , answers are represented as   subspans in the context defined by a start position   and an end position , while question and context aregiven as running text ( e.g. , Seo et al . , 2016 ; Chen   et al . , 2017 ; Devlin et al . , 2019 ; Kratzwald et al . ,   2019 ) .   A common challenge in extractive QA is that QA   models often suffer from performance deterioration   upon deployment and thus make mistakes for user-   generated inputs . The underlying reason for such   deterioration can be traced back to the domain shift   between training data ( from the source domain )   and test data ( from the target domain ) ( Fisch et al . ,   2019 ; Miller et al . , 2020 ; Zeng et al . , 2022b ) .   Existing approaches to address domain shifts   in extractive QA can be grouped as follows . One   approach is to include labeled target examples or   user feedback during training ( Daum√© III , 2007 ;   Kratzwald and Feuerriegel , 2019a ; Kratzwald et al . ,   2020 ; Kamath et al . , 2020 ) . Another approach is to   generate labeled QA samples in the target domain   for training ( Lee et al . , 2020 ; Yue et al . , 2021a ,   2022a ) . However , these approaches typically re-   quire large amounts of annotated data or extensive   computational resources . As such , they tend to be   ineffective in adapting existing QA models to an   unseen target domain ( Fisch et al . , 2019 ) . Only re-   cently , a contrastive loss has been proposed to han-   dle domain adaptation in QA ( Yue et al . , 2021b ) .   Several approaches have been used to address   issues related to insufficient data and generaliza-   tion in NLP tasks , yet outside of QA . For exam-   ple , augmentation in the hidden space encourages   more generalizable features for training ( Verma   et al . , 2019 ; Chen et al . , 2020 , 2021 ) . For domain   adaptation , there are approaches that encourage   the model to learn domain - invariant features via a   domain critic ( Lee et al . , 2019 ; Cao et al . , 2020 ) ,   or adopt discrepancy regularization between the   source and target domains ( Kang et al . , 2019 ; Yue   et al . , 2022b ) . However , to the best of our knowl-   edge , no work has attempted to build a smooth and   generalized feature space via hidden space augmen-   tation and self - supervised domain adaption.2308In this paper , we propose a novel self - supervised   QA domain adaptation framework for extractive   QA called QADA . Our QADA framework is de-   signed to handle domain shifts and should thus   answer out - of - domain questions . QADA has three   stages , namely pseudo labeling , hidden space aug-   mentation and self - supervised domain adaptation .   First , we use pseudo labeling to generate and fil-   ter labeled target QA data . Next , the augmenta-   tion component integrates a novel pipeline for data   augmentation to enrich training samples in the hid-   den space . For questions , we build upon multi-   hop synonyms and introduce Dirichlet neighbor-   hood sampling in the embedding space to generate   augmented tokens . For contexts , we develop an   attentive context cutoff method which learns to   drop context spans via a sampling strategy using   attention scores . Third , for training , we propose   to train the QA model via a novel attention - based   contrastive adaptation . Specifically , we use the at-   tention weights to sample informative features that   help the QA model separate answers and generalize   across the source and target domains .   Main contributions of our work are :   1.We propose a novel , self - supervised framework   called QADA for domain adaptation in QA .   QADA aims at answering out - of - domain ques-   tion and should thus handle the domain shift   upon deployment in an unseen domain .   2.To the best of our knowledge , QADA is the first   work in QA domain adaptation that ( i ) lever-   ages hidden space augmentation to enrich train-   ing data ; and ( ii ) integrates attention - based con-   trastive learning for self - supervised adaptation .   3.We demonstrate the effectiveness of QADA in   an unsupervised setting where target answers   are not accessible . Here , QADA can consid-   erably outperform state - of - the - art baselines on   multiple datasets for QA domain adaptation .   2 Related Work   Extractive QA has achieved significantly progress   recently ( Devlin et al . , 2019 ; Kratzwald et al . , 2019 ;   Lan et al . , 2020 ; Zhang et al . , 2020 ) . Yet , the ac-   curacy of QA models can drop drastically under   domain shifts ; that is , when deployed in an un-   seen domain that differs from the training distribu-   tion ( Fisch et al . , 2019 ; Talmor and Berant , 2019).To overcome the above challenge , various ap-   proaches for QA domain adaptation have been   proposed , which can be categorized as follows .   ( 1)(Semi- ) supervised adaptation uses partially la-   beled data from the target distribution for train-   ing ( Yang et al . , 2017 ; Kratzwald and Feuerriegel ,   2019b ; Yue et al . , 2022a ) . ( 2 ) Unsupervised adap-   tation with question generation refers to settings   where only context paragraphs in the target domain   are available , QA samples are generated separately   to train the QA model ( Shakeri et al . , 2020 ; Yue   et al . , 2021b ) . ( 3 ) Unsupervised adaptation has   access to context and question information from   the target domain , whereas answers are unavail-   able ( Chung et al . , 2018 ; Cao et al . , 2020 ; Yue   et al . , 2022d ) . In this paper , we focus on the third   category and study the problem of unsupervised   QA domain adaptation .   Domain adaptation for QA : Several ap-   proaches have been developed to generate synthetic   QA samples via question generation ( QG ) in an   end - to - end fashion ( i.e. , seq2seq ) ( Du et al . , 2017 ;   Sun et al . , 2018 ) . Leveraging such samples from   QG can also improve the QA performance in out - of-   domain distributions ( Golub et al . , 2017 ; Tang et al . ,   2017 , 2018 ; Lee et al . , 2020 ; Shakeri et al . , 2020 ;   Yue et al . , 2022a ; Zeng et al . , 2022a ) . Given unla-   beled questions , there are two main approaches : do-   main adversarial training can be applied to reduce   feature discrepancy between domains ( Lee et al . ,   2019 ; Cao et al . , 2020 ) , while contrastive adapta-   tion minimizes the domain discrepancy using maxi-   mum mean discrepancy ( MMD ) ( Yue et al . , 2021b ,   2022d ) . We later use the idea from contrastive   learning but tailor it carefully for our adaptation   framework .   Data augmentation for NLP : Data augmenta-   tion for NLP aims at improving the language under-   standing with diverse data samples . One approach   is to apply token - level augmentation and enrich the   training data with simple techniques ( e.g. , synonym   replacement , token swapping , etc . ) ( Wei and Zou ,   2019 ) or custom heuristics ( McCoy et al . , 2019 ) .   Alternatively , augmentation can be done in the hid-   den space of the underlying model ( Chen et al . ,   2020 ) . For example , one can drop partial spans   in the hidden space , which aids generalization per-   formance under distributional shifts ( Chen et al . ,   2021 ) but in NLP tasks outside of QA . To the best   of our knowledge , we are the first to propose a hid-   den space augmentation pipeline tailored for QA2309data in which different strategies are combined for   question and context augmentation .   Contrastive learning for domain adaptation :   Contrastive learning is used to minimize distances   of same - class samples and maximize discrepancy   among classes ( Hadsell et al . , 2006 ) . For this , dif-   ferent metrics are adopted to measure pair - wise   distances ( e.g. , triplet loss ) or domain distances   with MMD ( Cheng et al . , 2016 ; Schroff et al . ,   2015 ) . Contrastive learning can also be used for   domain adaptation by reducing the domain discrep-   ancy : this ‚Äú pulls together ‚Äù intra - class features and   ‚Äú pushes apart ‚Äù inter - class representations . Here ,   several applications are in computer vision ( Kang   et al . , 2019 ) . In QA domain adaptation , contrastive   learning was applied with averaged token features   to separate answer tokens and minimize the dis-   crepancy between source and target domain ( Yue   et al . , 2021b , 2022d ) . However , our work is dif-   ferent in that we introduce a novel attention - based   strategy to construct more informative features for   discrepancy estimation and contrastive adaptation .   3 Setup   We consider the following problem setup for QA   domain adaptation , where labeled source data and   unlabeled target data are available for training . Our   goal is to train a QA model fthat maximizes the   performance in the target domain using both source   data and unlabeled target data ( Cao et al . , 2020 ;   Shakeri et al . , 2020 ; Yue et al . , 2021b , 2022d ) .   Data : Our research focuses on question answer-   ing under domain shift . Let Ddenote the source   domain , and let Ddenote the ( different ) target do-   main . Then , labeled data from the source domain   can be used for training , while , upon deployment ,   it should perform well on the data from the target   domain . Specifically , training is two - fold : we first   pretrain a QA model on the source domain Dand ,   following this , the pretrained QA model is adapted   to the target domain D. The input data to each   domain is as follows :   ‚Ä¢Labeled source data : Training data is provided   by labeled QA data Xfrom the source domain   D. Here , each sample ( x , x , x)‚ààX   is a triplet comprising a context x , a question   x , and an answer x. As we consider extrac-   tive QA , the answer is represented by the start   and end position in the context .   ‚Ä¢Unlabeled target data : We assume partial ac - cess to data from the target domain D , that is ,   only contexts and unlabeled questions . The con-   texts and questions are first used for pseudo la-   beling , followed by self - supervised adaptation .   Formally , we refer to the contexts and questions   viaxandx , with ( x , x)‚ààXwhere   Xis the unlabeled data from the target domain .   Model : The QA model can be represented with   function f.ftakes both a question and con-   text as input and predicts an answer , i.e. , x=   f(x , x ) . Upon deployment , our goal is to   maximize the model performance on Xin the tar-   get domain D. Mathematically , this corresponds   to the optimization of fover target data X :   minL(f , X ) , ( 1 )   where Lis the cross - entropy loss .   4 The QADA Framework   4.1 Overview   Our proposed QADA framework has three stages to   be performed in each epoch ( see Fig . 1 ): ( 1 ) pseudo   labeling , where pseudo labels are generated for the   unlabeled targeted data ; ( 2 ) hidden space aug-   mentation , in which the proposed augmentation   strategy is leveraged to generated virtual examples   in the feature space ; and ( 3 ) contrastive adapta-   tion that minimizes domain discrepancy to transfer   source knowledge to the target domain .   To address the domain shift upon deployment ,   we use the aforementioned stages as follows . In   the first stage , we generate pseudo labels for the   unlabeled target data X. Next , we enrich the set of   training data via hidden space augmentation . In the   adaptation stage , we train the QA model using both   the source and the target data with our attention-   based contrastive adaptation . We summarize the   three stages in the following :   1.Pseudo labeling : First , we build labeled target   data ÀÜXvia pseudo labeling . Formally , a source-   pretrained QA model fgenerates a ( pseudo )   answer xfor context xand question x ,   i= 1 , . . .Each sample x‚ààÀÜXnow contains   the original context , the original question , and   a predicted answer . We additionally apply con-   fidence thresholding to filter the pseudo labels .   2.Hidden space augmentation : The QA model   ftakes a question and context pair as input.2310   For questions , we perform Dirichlet neighbor-   hood sampling in the word embedding layer   to generate diverse , yet consistent query infor-   mation . We also apply a context cutoff in the   hidden space after transformer layers to reduce   the learning of redundant domain information .   3.Contrastive adaptation : We train the QA model   fwith the source data Xfrom the source do-   mainDandthe target data ÀÜXwith pseudo   labels from the previous stage . We impose reg-   ularization on the answer extraction and further   minimize the discrepancy between the source   and target domain , so that the learned features   generalize well to the target domain .   4.2 Pseudo Labeling   Provided with the access to labeled source data , we   first pretrain the QA model to answer questions in   the source domain . Then , we can use the pretrained   model to predict target answers for self - supervised   adaptation ( Cao et al . , 2020 ) . The generated pseudo   labels are filtered via confidence thresholding , in   which the target samples above confidence thresh-   oldœÑ(= 0.6 in our experiments ) are preserved   for the later stages . We repeat the pseudo - labeling   step in each epoch to dynamically adjust the target   distribution used for self - supervised adaptation .   The QA model fis pretrained on the source   dataset Xvia a cross - entropy loss L , i.e. ,   minL(f , X ) . When selecting QA pairs from   the target domain , we further use confidence thresh-   olding for filtering and , thereby , build a subset of   target data with pseudo labels ÀÜX , i.e. ,   ÀÜX=/braceleftbig   ( x , x , f(x , x)/vextendsingle / vextendsingle   œÉ(f(x , x))‚â•œÑ,(x , x)‚ààX / bracerightbig   , ( 2)where œÉcomputes the output answer confidence   ( i.e , softmax function ) .   4.3 Hidden Space Augmentation   We design a data augmentation pipeline to enrich   the training data based on the generated QA pairs .   The augmentation pipeline is divided into two parts :   ( i)question augmentation via Dirichlet neighbor-   hood sampling in the embedding layer and ( ii ) con-   text augmentation with attentive context cutoff in   transformer layers . Both are described below .   Question augmentation : To perform augmen-   tation of questions , we propose Dirichlet neigh-   borhood sampling ( see Fig . 2 ) to sample synonym   replacements on certain proportion of tokens , such   that the trained QA model captures different pat-   terns of input questions . Dirichlet distributions   have been previously applied to find adversarial   examples ( Zhou et al . , 2021 ; Yue et al . , 2022c ) ;   however , different from such methods , we propose   to perform question augmentation in the embed-   ding layer . We first construct the multi - hop neigh-   borhood for each token in the input space . Here ,   1 - hop synonyms can be derived from a synonym   dictionary , while 2 - hop synonyms can be extended   from 1 - hop synonyms ( i.e. , the synonyms of 1 - hop   synonyms ) .   For each token , we compute a convex hull   spanned by the token and its multi - hop synonyms   ( i.e. , vertices ) , as shown in Fig . 2 . The convex   hull is used as the sampling area of the augmented   token embedding , where the probability distribu-   tion in the sampling area can be computed using a   Dirichlet distribution . That is , the sampled token   embedding is represented as the linear combina-   tions of vertices in the convex hull . Formally , for a   token xand the set of its multi - hop synonyms C,2311   we denote the coefficients of the linear combina-   tion by Œ∑= [ Œ∑ , Œ∑ , . . . , Œ∑ ] . We sample   coefficients Œ∑from a Dirichlet distribution :   Œ∑‚àºDirichlet ( Œ± , Œ± , . . . , Œ± ) , ( 3 )   where Œ±values are selected differently for the orig-   inal token and its multi - hop synonyms . Using the   sampled Œ∑ , we can compute the augmented token   embedding with the embedding function fvia   f(Œ∑ ) = /summationdisplayŒ∑f(j ) . ( 4 )   Dirichlet distributions are multivariate proba-   bility distributions with / summationtextŒ∑= 1 andŒ∑‚â•   0,‚àÄj‚àà C. The augmented embedding is there-   fore a linear combination of vertices in the convex   hull . By adjusting Œ±values , we can change the   probability distribution in the sampling area , that   is , how far the sampled embedding can travel from   the original token . For example , with increasing Œ±   values , we can expect the sampled points approach-   ing the center point of the convex hull .   The above augmentation is introduced in order   to provide semantically diverse yet consistent ques-   tions . At the same time , by adding noise locally ,   it encourages the QA model to capture robust in-   formation in questions ( see Zhou et al . , 2021 ) . We   control the question augmentation by a token aug-   mentation ratio , Œ∂ , to determine the percentage of   tokens within questions that are augmented .   Context augmentation : For contexts , we adopt   augmentation in the hidden space of the trans-   former layers instead of the embedding layer . Here ,   we propose to use an attentive context cutoff in the   hidden space . Specifically , we zero out sampled   context spans in the hidden space after each trans-   former layer in the QA model . This is shown in   Fig . 3 , where all hidden states in the selected span   along the input length are dropped ( i.e. , setting val-   ues to zero as shown by the white color ) . Thereby ,   our cutoff forces the QA model to attend to context   information that is particularly relevant across all   input positions and thus hinders it from learning   redundant domain information .   Formally , our attentive sampling strategy learns   to select cutoff spans : we compute a probability dis-   tribution and sample a midpoint using the attention   weights A‚ààRin the context span from   the previous transformer layer . The probability of   thei - th token pis computed via   p = œÉ / parenleftbigg1   H / summationdisplay / parenleftbigg / summationdisplayA / parenrightbigg / parenrightbigg , ( 5 )   where His the number of attention heads , Lis the   context length , and œÉdenotes the softmax function .   Once the cutoff midpoint is sampled , we introduce   a context cutoff ratio , œÜ , as a hyperparameter . It   determines the cutoff length ( as compared to length2312of the original context ) . We avoid context cutoff   in the final transformer layer to prevent important   answer features from being zeroed out .   Eventually , the above procedure of question aug-   mentation should improve the model capacity in   question understanding . Combined with context   cutoff , the QA model is further forced to attend   context information globally in the hidden space .   This thus encourages the QA model to reduce re-   dundancy and capture relevant information , i.e. ,   from all context positions using self - attention .   4.4 Contrastive Adaptation   To adapt the QA model to the target domain , we   develop a tailored attention - based contrastive adap-   tation . Here , our idea is to regularize the intra - class   discrepancy for knowledge transfer and increase   the inter - class discrepancy for answer extraction .   We consider answer tokens and non - answer tokens   as different classes ( Yue et al . , 2021b ) .   Loss : We perform contrastive adaptation to re-   duce the intra - class discrepancy between source   and target domains . We also maximize the inter-   class distances between answer tokens and non-   answer tokens to separate answer spans . For a   mixed batch XwithXandXrepresenting the   subset of source and target samples , our contrastive   adaptation loss is   L = D(X , X ) + D(X , X )   ‚àí D(X , X)with   D=1   |X||X|/summationdisplay / summationdisplayk(œï(x ) , œï(x ) )   +1   |X||X|/summationdisplay / summationdisplayk(œï(x ) , œï(x ) )   ‚àí2   |X||X|/summationdisplay / summationdisplayk(œï(x ) , œï(x ) ) ,   ( 6 )   where Xrepresents answer tokens , Xrepresents   non - answer tokens in X.xis the i - th sample   fromX , andxis the j - th sample from X.D   computes the MMD distance with empirical kernel   mean embeddings and Gaussian kernel kusing our   scheme below . In L , the first two terms re-   duce the intra - class discrepancy ( discrepancy term ) ,   while the last term maximizes the distance of an-   swer tokens to other tokens , thereby improving   answer extraction ( extraction term ) .   MMD : The maximum mean discrepancy   ( MMD ) computes the proximity between prob-   abilistic distributions in the reproducing kernel   Hilbert space Husing drawn samples ( Gretton   et al . , 2012 ) . In previous research ( Yue et al . ,   2021b ) , the MMD distance Dwas computed using   the BERT encoder . However , simply using œïas in   previous work would return the averaged feature   of relevant tokens in the sample rather than more   informative tokens ( i.e. , tokens near the decision   boundary which are ‚Äú harder ‚Äù to classify ) .   Unlike previous methods , we design an attention-   based sampling strategy . First , we leverage the at-   tention weights A‚ààRof input xusing   the encoder of the QA model . Based on this , we   compute a probability distribution for tokens of the   relevant class ( e.g. , non - answer tokens ) using the   softmax function œÉand sample an index . The cor-   responding token feature from the QA encoder is   used as the class feature , i.e. ,   œï(x ) = f(x)withi‚àºœÉ / parenleftbigg1   H / summationdisplay / summationdisplayA / parenrightbigg   ,   ( 7 )   where fis the encoder of the QA model . As a   result , features are sampled proportionally to the   attention weights . This should reflect more rep-   resentative information of the token class for dis-   crepancy estimation . We apply the attention - based   sampling to both answer and non - answer features .   Illustration : We visualize an illustrative QA   sample in Fig . 4 to explain the advantage of our2313ModelHotpotQA NaturalQ. NewsQA SearchQA TriviaQA   EM / F1 EM / F1 EM / F1 EM / F1 EM / F1   ( I ) Zero - shot target performance   BERT 43.34/60.42 39.06/53.7 39.17/56.14 16.19/25.03 49.70/59.09   ( II ) Target performance w/ domain adaptation   DAT ( Lee et al . , 2019 ) 44.25/61.10 44.94/58.91 38.73/54.24 22.31/31.64 49.94/59.82   CASe ( Cao et al . , 2020 ) 47.16/63.88 46.53/60.19 43.43/59.67 26.07/35.16 54.74/63.61   CAQA ( Yue et al . , 2021b ) 46.37/61.57 48.55/62.60 40.55/55.90 36.05/42.94 55.17/63.23   CAQA(Yue et al . , 2021b ) 48.52/64.76 47.37/60.52 44.26/60.83 32.05/41.07 54.30/62.98   QADA ( ours ) 50.80/65.75 52.13/65.00 45.64/61.84 40.47/48.76 56.92/65.86   ( III ) Target performance w/ supervised training   BERT w/ 10k Annotations 49.52/66.56 54.88/68.10 45.92/61.85 60.20/66.96 54.63/60.73   BERT w/ All Annotations 57.96/74.76 67.08/79.02 52.14/67.46 71.54/77.77 64.51/70.27   attention - based sampling for domain discrepancy   estimation . We visualize all token features and then   examine the extraction term from Eq . 6 . We fur-   ther show the feature mapping œïfrom Yue et al .   ( 2021b ) , which , different from ours , returns the   average feature . In contrast , our œïfocuses on the   estimation of more informative distances . As a   result , our proposed attention - based sampling strat-   egy is more likely to sample ‚Äú harder ‚Äù context to-   kens . These are closer to the decision boundary ,   as such token positions have higher weights in A.   Owing to our choice of œï , QADA improves the   measure of answer - context discrepancy and , there-   fore , is more effective in separating answer tokens .   4.5 Learning Algorithm   We incorporate the contrastive adaptation loss from   Eq . 6 into the original training objective . This gives   our overall loss   L = L+ŒªL , ( 8)   where Œªis a weighting factor for the contrastive   loss .   5 Experiments   Datasets : We use the following datasets ( see Ap-   pendix A for details ):   ‚Ä¢For the source domain D , we use SQuAD   v1.1 ( Rajpurkar et al . , 2016 ) .   ‚Ä¢For target domain D , we select MRQA   Split I ( Fisch et al . , 2019 ): HotpotQA ( Yanget al . , 2018 ) , Natural Questions ( Kwiatkowski   et al . , 2019 ) , NewsQA ( Trischler et al . , 2016 ) ,   SearchQA ( Dunn et al . , 2017 ) , and TriviaQA   ( Joshi et al . , 2017 ) . This selection makes our   results comparable with other works in QA do-   main adaptation ( e.g. , Lee et al . , 2020 ; Shakeri   et al . , 2020 ; Cao et al . , 2020 ; Yue et al . , 2021b ,   2022d ) .   Baselines : As a na√Øve baseline , we pretrain a   BERT on the source dataset as our base model and   evaluate on each target dataset with zero knowl-   edge of the target domain . In addition , we adopt   three state - of - the - art baselines : domain - adversarial   training ( DAT ) ( Lee et al . , 2019 ) , conditional ad-   versarial self - training ( CASe ) ( Cao et al . , 2020 ) ,   and contrastive adaptation for QA ( CAQA ) ( Yue   et al . , 2021b ) . For a fair comparison , we adopt   both the original CAQA and CAQA with our self-   supervised adaptation framework ( = CAQA ) .   Baseline details are reported in Appendix B.   Training and Evaluation : We use the proposed   method to adapt the pretrained QA model , augmen-   tation hyperparameters are tuned empirically by   searching for the best combinations . To evaluate   the predictions , we follow ( Lee et al . , 2020 ; Shak-   eri et al . , 2020 ; Yue et al . , 2021b ) and assess the   exact matches ( EM ) and the F1 score on the dev   sets . Implementation details are in Appendix C.2314ModelHotpotQA NaturalQ. NewsQA SearchQA TriviaQA   EM / F1 EM / F1 EM / F1 EM / F1 EM / F1   QADA ( ours ) 50.80/65.75 52.13/65.00 45.64/61.84 40.47/48.76 56.92/65.86   w/o Dirichlet sampling 49.57/64.71 51.15/64.24 45.27/61.44 35.90/44.28 56.83/65.51   w/o context cutoff 50.36/65.71 50.30/62.98 45.39/61.47 33.94/42.43 56.04/64.87   w/o contrastive adaptation 48.21/64.54 48.35/61.76 44.35/60.66 30.85/39.42 55.42/64.38   6 Experimental Results   6.1 Adaptation Performance   Our main results for domain adaptation are in Ta-   ble 1 . We distinguish three major groups : ( 1 ) Zero-   shot target performance . Here , we report a na√Øve   baseline ( BERT ) for which the QA model is solely   trained on SQuAD . ( 2 ) Target performance w/ do-   main adaptation . This refers to the methods where   domain adaptation techniques are applied . This   group also includes our proposed QADA . ( 3 ) Tar-   get performance w/ supervised training . Here ,   training is done with the original target data . Hence ,   this reflects an ‚Äú upper bound ‚Äù .   Overall , the domain adaptation baselines are   outperformed by QADA across all target datasets .   Hence , this confirms the effectiveness of the pro-   posed framework using both data augmentation   and attention - based contrastive adaptation . In addi-   tion , we observe the following : ( 1 ) All adaptation   methods achieve considerable improvements in an-   swering target domain questions compared to the   na√Øve baseline . ( 2 ) QADA performs the best over-   all . Compared to the best baseline , QADA achieves   performance improvements by 6.1%and4.9%in   EM and F1 , respectively . ( 3 ) The improvements   with QADA are comparatively larger on HotpotQA ,   Natural Questions , and SearchQA ( ‚àº8.1%in EM )   in contrast to NewsQA and TriviaQA ( ‚àº3.1%in   EM ) . A potential reason for the gap is the limited   performance of BERT in cross - sentence reasoning ,   where the QA model often fails to answer composi-   tional questions in long input contexts . ( 4 ) QADA   can perform similarly or outperform the supervised   training results using 10k target data . For exam-   ple , QADA achieve 56.92 ( EM ) and 65.86 ( F1 ) on   TriviaQA in contrast to 54.63 and 60.73 of the 10k   supervised results , suggesting the effectiveness of   QADA.6.2 Ablation Study for QADA   We evaluate the effectiveness of the proposed   QADA by performing an ablation study . By com-   paring the performance of QADA and CAQAin   Table 1 , we yield an ablation quantifying the gains   that should be attributed to the combination of all   proposed components in QADA . We find distinc-   tive performance improvements due to our hidden   space augmentation and contrastive adaptation . For   example , we observe that EM performance can   drop up to 20.8%without QADA , suggesting clear   superiority of the proposed QADA .   We further evaluate the effectiveness of the in-   dividual components in QADA . We remove the   proposed Dirichlet neighborhood sampling , atten-   tive context cutoff and attention - based contrastive   adaptation in QADA separately and observe the per-   formance changes . The results on target datasets   are reported in Table 2 . For all components , we ob-   serve consistent performance drops when removed   from QADA . For example , the performance of   QADA reduces , on average , by 3.3%,4.5 % , and   8.3%in EM when we remove Dirichlet sampling ,   context cutoff , and contrastive adaptation , respec-   tively . The results suggest that the combination   of question and context augmentation in the hid-   den space is highly effective for improving QA   domain adaptation . Moreover , the performance im-   proves clearly when including our attention - based   contrastive adaptation .   6.3 Sensitivity Analysis for Hidden Space   Augmentation   Our QADA uses question augmentation ( i.e. ,   Dirichlet neighborhood sampling ) and context aug-   mentation ( i.e. , context cutoff ) , where the augmen-   tation ratios determine the percentage of tokens   that are augmented . Figure 5 compares different   augmentation ratios from 0to0.4on HotpotQA ,   Natural Questions , and NewsQA . Overall , we ob-   serve some variations but , importantly , the perfor-   mance of QADA improves adaptation performance2315   and remains fairly robust for different nonzero ra-   tios . Moreover , we find comparatively large im-   provements for HotpotQA by introducing Dirichlet   neighborhood sampling ( 2.5%in EM ) , while Nat-   ural Questions benefits more from context cutoff   ( 3.6%in EM ) . A potential reason for such improve-   ments is that HotpotQA has more complex ques-   tions that need potential matching and reasoning ,   while Natural Questions provides longer unstruc-   tured text as contexts , thereby requiring improved   understanding of long paragraphs .   7 Conclusion   In this paper , we propose a novel self - supervised   framework called QADA for QA domain adapta-   tion . QADA introduces : ( 1 ) hidden space augmen-   tation tailored for QA data to enrich target train-   ing corpora ; and ( 2 ) an attention - based contrastive   adaptation to learn domain - invariant features that   generalize across source and target domain . Our ex-   periments demonstrate the effectiveness of QADA :   it achieves a superior performance over state - of-   the - art baselines in QA domain adaptation .   8 Limitations   Despite having introduced hidden space augmen-   tation in QADA , we have not discussed different   choices of Œ±values for multi - hop synonyms to ex-   ploit the potential benefits of the Dirichlet distribu-   tion . For context cutoff , dropping multiple context   spans in each QA example may bring additional   benefits to improve context understanding and the   answer extraction process of the QA model . Com-   bined with additional question value estimation in   pseudo labeling , we plan to explore such directions   in adaptive QA systems as our future work . Acknowledgments   This research is supported in part by the National   Science Foundation under Grant No . IIS-2202481 ,   CHE-2105005 , IIS-2008228 , CNS-1845639 , CNS-   1831669 . The views and conclusions contained in   this document are those of the authors and should   not be interpreted as representing the official poli-   cies , either expressed or implied , of the U.S. Gov-   ernment . The U.S. Government is authorized to   reproduce and distribute reprints for Government   purposes notwithstanding any copyright notation   here on .   References2316231723182319Appendix   A Dataset Details   For the source domain , we adopt SQuAD v1.1 ( Ra-   jpurkar et al . , 2016 ) following ( Cao et al . , 2020 ;   Lee et al . , 2020 ; Shakeri et al . , 2020 ; Yue et al . ,   2021b ) . SQuAD v1.1 is a question - answering   dataset where context paragraphs originate from   Wikipedia articles . The QA pairs were then anno-   tated by crowdworkers .   In our experiments , we adopt all datasets from   MRQA Split I ( Fisch et al . , 2019 ) for the target   domains :   1.HotpotQA is a question - answering dataset with   multi - hop questions and supporting facts to pro-   mote reasoning in QA ( Yang et al . , 2018 ) .   2.NaturalQuestions ( Kwiatkowski et al . , 2019 )   builds upon real - world user questions . These   were then combined with Wikipedia articles as   context . The Wikipedia articles may or may not   contain the answer to each question .   3.NewsQA ( Trischler et al . , 2016 ) provides news   as contexts and challenging questions beyond   simple matching and entailment .   4.SearchQA ( Dunn et al . , 2017 ) was built based   on an existing dataset of QA pairs . The QA   pairs were then extended by contexts , which   were crawled through Google search .   5.TriviaQA ( Joshi et al . , 2017 ) is a question-   answering dataset containing evidence infor-   mation for reasoning in QA .   B Baseline Details   As a na√Øve baseline , we adopt BERT ( uncased base   version with additional batch normalization layer )   and train on the source dataset ( Devlin et al . , 2019 ;   Cao et al . , 2020 ) . Additionally , we implemented   the following three baselines for unsupervised QA   domain adaptation :   1.Domain adversarial training ( DAT ) ( Tzeng   et al . , 2017 ; Lee et al . , 2019 ) consists of a   QA system and a discriminator using [ CLS ]   output in BERT . The QA system is first trained   on labeled source data . Then , input data from   both domains is used for domain adversarial   training to learn generalized features .   2.Conditional adversarial self - training   ( CASe ) ( Cao et al . , 2020 ) leverages self-   training with conditional adversarial learning   across domains . CASe iteratively performpseudo labeling and domain adversarial   training to reduce domain discrepancy . We   adopt the entropy weighted CASe+E in our   work as baseline .   3.CAQA ( Yue et al . , 2021b ) leverages QAGen-   T5 for question generation but extends the   learning algorithm with a contrastive loss on   token - level features for generalized QA fea-   tures . Specifically , CAQA uses contrastive   adaptation to reduce domain discrepancy and   promote answer extraction .   4.Self - supervised contrastive adaptation for   QA ( CAQA)(Yue et al . , 2021b ) is a modi-   fied self - supervised baseline based on CAQA .   We exclude question generation and adopt   the same process of pseudo labeling and   self - supervised adaptation as in QADA . Un-   like QADA , hidden space augmentation and   attention - based contrastive loss are removed .   C Implementation Details   QA model : We adopt BERT with an additional   batch norm layer after the encoder for QA domain   adaptation , as in ( Cao et al . , 2020 ) . We first pre-   train BERT with a learning rate of 3¬∑10for two   epochs and a batch size of 12on the source dataset .   We use the AdamW optimizer with 10 % linear   warmup . We additionally use Apex for mixed pre-   cision training .   Adaptation : For the baselines , we use the origi-   nal BERT architecture and follow the default set-   tings provided in the original papers . For QADA ,   adaptation is performed 4 epochs with the AdamW   optimizer , learning rate of 2¬∑10 , and 10 % pro-   portion as warmup in each epoch ( as training data   changes after pseudo labeling ) . In the pseudo label-   ing stage , we perform inference on unlabeled target   data and preserve the target samples with confi-   dence above the threshold œÑ= 0.6 . For batching   in self - supervised adaptation , we perform hidden   space augmentation and sample 12 target examples   and another 12 source examples .   QADA : For our experiments , the scaling fac-   torŒªfor the adaptation loss is chosen from   [ 0.0001,0.0005 ] depending on the target dataset .   For Dirichlet neighborhood sampling , we use Œ±=   1for the original token and a decay factor of 0.1for   multi - hop synonyms ( i.e. , 0.1for 1 - hop synonyms   and0.01for 2 - hop synonyms ) . For hyperparame-   ters in hidden space augmentation , we search for2320a combination of question augmentation ratio Œ∂   and context cutoff ratio œÜ . Specifically , we empiri-   cally search for the best combination in the range   of[0.1,0.2,0.3,0.4]for both Œ∂andœÜ . Eventually ,   the best hyperparameter combination is selected.2321