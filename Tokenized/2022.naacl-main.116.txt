  Simiao Zuo , Qingru Zhang , Chen Liang , Pengcheng He ,   Tuo Zhaoand Weizhu ChenGeorgia Institute of TechnologyMicrosoft   { simiaozuo,qzhang441,cliang73,tourzhao}@gatech.edu   { Pengcheng.H,wzchen}@microsoft.com   Abstract   Pre - trained language models have demon-   strated superior performance in various natu-   ral language processing tasks . However , these   models usually contain hundreds of millions   of parameters , which limits their practicality   because of latency requirements in real - world   applications . Existing methods train small   compressed models via knowledge distillation .   However , performance of these small mod-   els drops significantly compared with the pre-   trained models due to their reduced model ca-   pacity . We propose MoEBERT , which uses a   Mixture - of - Experts structure to increase model   capacity and inference speed . We initialize   MoEBERT by adapting the feed - forward neu-   ral networks in a pre - trained model into multi-   ple experts . As such , representation power of   the pre - trained model is largely retained . Dur-   ing inference , only one of the experts is acti-   vated , such that speed can be improved . We   also propose a layer - wise distillation method   to train MoEBERT . We validate the efficiency   and effectiveness of MoEBERT on natural lan-   guage understanding and question answering   tasks . Results show that the proposed method   outperforms existing task - specific distillation   algorithms . For example , our method outper-   forms previous approaches by over 2%on the   MNLI ( mismatched ) dataset . Our code is pub-   licly available at https://github.com/   SimiaoZuo / MoEBERT .   1 Introduction   Pre - trained language models have demonstrated   superior performance in various natural language   processing tasks , such as natural language under-   standing ( Devlin et al . , 2019 ; Liu et al . , 2019 ; He   et al . , 2021b ) and natural language generation ( Rad-   ford et al . , 2019 ; Brown et al . , 2020 ) . These models   can contain billions of parameters , e.g. , T5 ( Raffel   et al . , 2019 ) contains up to 11billion parameters ,   and GPT-3 ( Brown et al . , 2020 ) consists of up to175billion parameters . Their extreme sizes bring   challenges in serving the models to real - world ap-   plications due to latency requirements .   Model compression through knowledge distil-   lation ( Romero et al . , 2015 ; Hinton et al . , 2015 )   is a promising approach that reduces the compu-   tational overhead of pre - trained language models   while maintaining their superior performance . In   knowledge distillation , a large pre - trained language   model serves as a teacher , and a smaller student   model is trained to mimic the teacher ’s behavior .   Distillation approaches can be categorized into   two groups : task - agnostic ( Sanh et al . , 2019 ; Jiao   et al . , 2020 ; Wang et al . , 2020 , 2021 ; Sun et al . ,   2020a ) and task - specific ( Turc et al . , 2019 ; Sun   et al . , 2019 ; Li et al . , 2020 ; Hou et al . , 2020 ; Sun   et al . , 2020b ; Xu et al . , 2020 ) . Task - agnostic dis-   tillation pre - trains the student and then fine - tunes   it on downstream tasks ; while task - specific dis-   tillation directly fine - tunes the student after ini-   tializing it from a pre - trained model . Note that   task - agnostic approaches are often combined with   task - specific distillation during fine - tuning for bet-   ter performance ( Jiao et al . , 2020 ) . We focus on   task - specific distillation in this work .   One major drawback of existing knowledge dis-   tillation approaches is the drop in model perfor-   mance caused by the reduced representation power .   That is , because the student model has fewer pa-   rameters than the teacher , its model capacity is   smaller . For example , the student model in Distil-   BERT ( Sanh et al . , 2019 ) has 66million parame-   ters , about half the size of the teacher ( BERT - base ,   Devlin et al . 2019 ) . Consequently , performance   of DistilBERT drops significantly compared with   BERT - base , e.g. , over 2%on MNLI ( 82.2v.s.84.5 )   and over 3%on CoLA ( 54.7v.s.51.3 ) .   We resort to the Mixture - of - Experts ( MoE ,   Shazeer et al . 2017 ) structure to remedy the repre-   sentation power issue . MoE models can increase   model capacity while keeping the inference com-1610putational cost constant . A layer of a MoE model   ( Shazeer et al . , 2017 ; Lepikhin et al . , 2021 ; Fedus   et al . , 2021 ; Yang et al . , 2021 ; Zuo et al . , 2021 )   consists of an attention mechanism and multiple   feed - forward neural networks ( FFNs ) in parallel .   Each of the FFNs is called an expert . During train-   ing and inference , an input adaptively activates a   fixed number of experts ( usually one or two ) . In   this way , the computational cost of a MoE model   remains constant during inference , regardless of the   total number of experts . Such a property facilitates   compression without reducing model capacity .   However , MoE models are difficult to train - from-   scratch and usually require a significant amount of   parameters , e.g. , 7.4billion parameters for Switch-   base ( Fedus et al . , 2021 ) . We propose MoEBERT ,   which incorporates the MoE structure into pre-   trained language models for fine - tuning . Our model   can speedup inference while retaining the represen-   tation power of the pre - trained language model .   Specifically , we incorporate the expert structure by   adapting the FFNs in a pre - trained model into mul-   tiple experts . For example , the hidden dimension   of the FFN is 3072 in BERT - base ( Devlin et al . ,   2019 ) , and we adapt it into 4experts , each has a   hidden dimension 768 . In this way , the amount   ofeffective parameters ( i.e. , parameters involved   in computing the representation of an input ) is cut   by half , and we obtain a ×2speedup . We remark   thatMoEBERT utilizes more parameters of the pre-   trained model than existing approaches , such that   it has greater representation power .   To adapt the FFNs into experts , we propose an   importance - based method . Empirically , there are   some neurons in the FFNs that contribute more to   the model performance than the other ones . That   is , removing the important neurons causes signif-   icant performance drop . Such a property can be   quantified by the importance score ( Molchanov   et al . , 2019 ; Xiao et al . , 2019 ; Liang et al . , 2021 ) .   When initializing MoEBERT , we share the most   important neurons ( i.e. , the ones with the highest   scores ) among the experts , and the other neurons   are distributed evenly . This strategy has two ad-   vantages : first , the shared neurons preserve perfor-   mance of the pre - trained model ; second , the non-   shared neurons promote diversity among experts ,   which further boost model performance . After ini-   tialization , MoEBERT is trained using a layer - wise   task - specific distillation algorithm .   We demonstrate efficiency and effectiveness ofMoEBERT on natural language understanding and   question answering tasks . On the GLUE ( Wang   et al . , 2019 ) benchmark , our method significantly   outperforms existing distillation algorithms . For   example , MoEBERT exceeds performance of state-   of - the - art task - specific distillation approaches by   over2%on the MNLI ( mismatched ) dataset . For   question answering , MoEBERT increases F1 by   2.6on SQuAD v1.1 ( Rajpurkar et al . , 2016 ) and 7.0   on SQuAD v2.0 ( Rajpurkar et al . , 2018 ) compared   with existing algorithms .   The rest of the paper is organized as follows :   we introduce background and related works in Sec-   tion 2 ; we describe MoEBERT in Section 3 ; ex-   perimental results are provided in Section 4 ; and   Section 5 concludes the paper .   2 Background   2.1 Backbone : Transformer   The Transformer ( Vaswani et al . , 2017 ) backbone   has been widely adopted in pre - trained language   models . The model contains several identically-   constructed Transformer layers . Each layer has   a multi - head self - attention mechanism and a two-   layer feed - forward neural network ( FFN ) .   Suppose the output of the attention mechanism   isA. Then , the FFN is defined as :   H = σ(AW+b),X = WH+b,(1 )   where W∈R , W∈R , b∈R   andb∈Rare weights of the FFN , and σis the   activation function . Here ddenotes the embedding   dimension , and ddenotes the hidden dimension   of the FFN .   2.2 Mixture - of - Experts Models   Mixture - of - Experts models consist of multiple ex-   pert layers , which are similar to the Transformer   layers . Each of these layers contain a self - attention   mechanism and multiple FFNs ( Eq . 1 ) in parallel ,   where each FFN is called an expert .   Let{E}denote the experts , and Ndenotes   the total number of experts . Similar to Eq . 1 , the   experts in layer ℓtake the attention output Aas   the input . For each a(thet - th row of A ) that   corresponds to an input token , the corresponding   output xof layer ℓis   x=/summationdisplayp(a)E(a ) . ( 2 )   Here , T ⊂ { 1···N}is the activated set of experts   with|T |=K , and pis the weight of expert E.1611Different approaches have been proposed to con-   structTand compute p. For example , Shazeer   et al . ( 2017 ) take   p(a ) = [ softmax ( aW ) ] , ( 3 )   where Wis a weight matrix . Consequently , Tis   constructed as the experts that yield top- Klargest   p. However , such an approach suffers from load   imbalance , i.e. , Wcollapses such that nearly all   the inputs are routed to the same expert . Existing   works adopt various ad - hoc heuristics to mitigate   this issue , e.g. , adding Gaussian noise to Eq . 3   ( Shazeer et al . , 2017 ) , limiting the maximum num-   ber of inputs that can be routed to an expert ( Lep-   ikhin et al . , 2021 ) , imposing a load balancing loss   ( Lepikhin et al . , 2021 ; Fedus et al . , 2021 ) , and   using linear assignment ( Lewis et al . , 2021 ) . In   contrast , Roller et al . 2021 completely remove the   gate and pre - assign tokens to experts using hash   functions , in which case we can take p= 1 / K.   In Eq . 2 , a token only activates Kinstead of N   experts , and usually K≪N , e.g. , K= 2 and   N= 2048 in GShard ( Lepikhin et al . , 2021 ) . As   such , the number of FLOPs for one forward pass   does not scale with the number of experts . Such   a property paves the way for increasing inference   speed of a pre - trained model without decreasing the   model capacity , i.e. , we can adapt the FFNs in a pre-   trained model into several smaller components , and   only activate one of the components for a specific   input token .   2.3 Pre - trained Language Models   Pre - trained language models ( Peters et al . , 2018 ;   Devlin et al . , 2019 ; Raffel et al . , 2019 ; Liu et al . ,   2019 ; Brown et al . , 2020 ; He et al . , 2021b , a ) have   demonstrated superior performance in various nat-   ural language processing tasks . These models are   trained on an enormous amount of unlabeled data ,   such that they contain rich semantic information   that benefits downstream tasks . Fine - tuning pre-   trained language models achieves state - of - the - art   performance in tasks such that natural language un-   derstanding ( He et al . , 2021a ) and natural language   generation ( Brown et al . , 2020 ) .   2.4 Knowledge Distillation   Knowledge distillation ( Romero et al . , 2015 ; Hin-   ton et al . , 2015 ) compensates for the performance   drop caused by model compression . In knowledge   distillation , a small student model mimics the be-   havior of a large teacher model . For example , Dis-   tilBERT ( Sanh et al . , 2019 ) uses the teacher ’s softprediction probability to train the student model ;   TinyBERT ( Jiao et al . , 2020 ) aligns the student ’s   layer outputs ( including attention outputs and hid-   den states ) with the teacher ’s ; MiniLM ( Wang et al . ,   2020 , 2021 ) utilizes self - attention distillation ; and   CoDIR ( Sun et al . , 2020a ) proposes to use a con-   trastive objective such that the student can distin-   guish positive samples from negative ones accord-   ing to the teacher ’s outputs .   There are also heated discussions on the num-   ber of layers to distill . For example , Wang et al .   ( 2020 , 2021 ) distill the attention outputs of the last   layer ; Sun et al . ( 2019 ) choose specific layers to   distill ; and Jiao et al . ( 2020 ) use different weights   for different transformer layers .   There are two variants of knowledge distillation :   task - agnostic ( Sanh et al . , 2019 ; Jiao et al . , 2020 ;   Wang et al . , 2020 , 2021 ; Sun et al . , 2020a ) and   task - specific ( Turc et al . , 2019 ; Sun et al . , 2019 ;   Li et al . , 2020 ; Hou et al . , 2020 ; Sun et al . , 2020b ;   Xu et al . , 2020 ) . The former requires pre - training a   small model using knowledge distillation and then   fine - tuning on downstream tasks , while the latter   directly fine - tunes the small model . Note that task-   agnostic approaches are often combined with task-   specific distillation for better performance , e.g. ,   TinyBERT ( Jiao et al . , 2020 ) . In this work , we   focus on task - specific distillation .   3 Method   In this section , we first present an algorithm that   adapts a pre - trained language model into a MoE   model . Such a structure enables inference speedup   by reducing the number of parameters involved in   computing an input token ’s representation . Then ,   we introduce a layer - wise task - specific distillation   method that compensates for the performance drop   caused by model compression .   3.1 Importance - Guided Adaptation of   Pre - trained Language Models   Adapting the FFNs in a pre - trained language model   into multiple experts facilitates inference speedup   while retaining model capacity . This is because   in a MoE model , only a subset of parameters are   used to compute the representation of a given token   ( Eq . 2 ) . These activated parameters are referred to   aseffective parameters . For example , by adapting   the FFNs in a pre - trained BERT - base ( Devlin et al . ,   2019 ) ( with hidden dimension 3072 ) model into   4experts ( each has hidden dimension 768 ) , the1612   number of effective parameters reduces by half ,   such that we obtain a ×2speedup .   Empirically , we find that randomly converting   a FFN into experts works poorly ( see Figure 3a in   the experiments ) . This is because there are some   columns in W∈R(correspondingly some   rows in Win Eq . 1 ) contribute more than the   others to model performance .   The importance score ( Molchanov et al . , 2019 ;   Xiao et al . , 2019 ; Liang et al . , 2021 ) , originally   introduced in model pruning literature , measures   such parameter importance . For a dataset Dwith   sample pairs { ( x , y ) } , the score is defined as   I=/summationdisplay / vextendsingle / vextendsingle / vextendsingle(w)∇L(x , y )   + ( w)∇L(x , y)/vextendsingle / vextendsingle / vextendsingle . ( 4 )   Herew∈Ris the j - th column of W , wis the   j - th row of W , andL(x , y)is the loss .   The importance score in Eq . 4 indicates variation   of the loss if we remove the neuron . That is ,   |L− L| ≈/vextendsingle / vextendsingle / vextendsingle(w−0)∇L / vextendsingle / vextendsingle / vextendsingle   = |w∇L| ,   where Lis the loss with neuronwandLis   the loss without neuron w. Here the approximation   is based on the first order Taylor expansion of L   around w=0 .   After computing Ifor all the columns , we adapt   Winto experts . The columns are sorted in as - cending order according to their importance scores   asw···w , where whas the largest I   andwthe smallest . Empirically , we find that   sharing the most important columns benefits model   performance . Based on this finding , suppose we   share the top- scolumns and we adapt the FFN   intoNexperts , then expert econtains columns   { w,···,w , w , w , · · · } . Note   that we discard the least important columns to keep   the size of each expert as ⌊d / N⌋. Figure 1 is an   illustration of adapting a FFN with 4neurons in a   pre - trained model into two experts .   3.2 Layer - wise Distillation   To remedy the performance drop caused by adapt-   ing a pre - trained model to a MoE model , we adopt a   layer - wise task - specific distillation algorithm . We   use BERT - base ( Devlin et al . , 2019 ) as both the   student ( i.e. , the MoE model ) and the teacher . We   distill both the Transformer layer output X(Eq . 2 )   and the final prediction probability .   For the Transformer layers , the distillation loss   is the mean squared error between the teacher ’s   layer output Xand the student ’s layer output X   obtained from Eq . 2.Concretely , for an input x ,   the Transformer layer distillation loss is   L(x ) = /summationdisplayMSE ( X , X ) , ( 5 )   where Lis the total number of layers . Notice that   we include the MSE loss of the embedding layer   outputs XandX.   Letfdenotes the MoE model and fthe   teacher model . We obtain the prediction probabil-   ity for an input xasp = f(x)andp = f(x ) ,   where pis the prediction of the MoE model and   pis the prediction of the teacher model . Then   the distillation loss for the prediction layer is   L(x ) = 1   2(KL(p||p ) + KL ( p||p)),(6 )   where KLis the Kullback – Leibler divergence .   The layer - wise distillation loss is the sum of   Eq . 5 and Eq . 6 , defined as   L(x ) = L(x ) + L(x ) . ( 7 )   We will discuss variants of Eq . 7 in the experiments.16133.3 Model Training   We employ the random hashing strategy ( Roller   et al . , 2021 ) to train the experts . That is , each   token is pre - assigned to a random expert , and this   assignment remains the same during training and   inference . We will discuss more about other routing   strategies of the MoE model in the experiments .   Given the training dataset Dand samples   { ( x , y ) } , the training objective is   L=/summationdisplayCE(f(x ) , y ) + λL(x ) ,   where CEis the cross - entropy loss and λ is a   hyper - parameter .   4 Experiments   In this section , we evaluate the effectiveness and   efficiency of the proposed algorithm on natural lan-   guage understanding and question answering tasks .   We implement our algorithm using the Hugging-   face Transformers(Wolf et al . , 2019 ) code - base .   All the experiments are conducted on NVIDIA   V100 GPUs .   4.1 Datasets   GLUE . We evaluate performance of the proposed   method on the General Language Understanding   Evaluation ( GLUE ) benchmark ( Wang et al . , 2019 ) ,   which is a collection of nine natural language un-   derstanding tasks . The benchmark includes two   single - sentence classification tasks : SST-2 ( Socher   et al . , 2013 ) is a binary classification task that clas-   sifies movie reviews to positive or negative , and   CoLA ( Warstadt et al . , 2019 ) is a linguistic ac-   ceptability task . GLUE also contains three sim-   ilarity and paraphrase tasks : MRPC ( Dolan and   Brockett , 2005 ) is a paraphrase detection task ; STS-   B ( Cer et al . , 2017 ) is a text similarity task ; and   QQP is a duplication detection task . There are also   four natural language inference tasks in GLUE :   MNLI ( Williams et al . , 2018 ) ; QNLI ( Rajpurkar   et al . , 2016 ) ; RTE ( Dagan et al . , 2006 ; Bar - Haim   et al . , 2006 ; Giampiccolo et al . , 2007 ; Bentivogli   et al . , 2009 ) ; and WNLI ( Levesque et al . , 2012 ) .   Following previous works on model distillation ,   we exclude STS - B and WNLI in the experiments .   Dataset details are summarized in Appendix A.   Question Answering . We evaluate the proposed   algorithm on two question answering datasets : SQuAD v1.1 ( Rajpurkar et al . , 2016 ) and SQuAD   v2.0 ( Rajpurkar et al . , 2018 ) . These tasks are   treated as a sequence labeling problem , where we   predict the probability of each token being the start   and end of the answer span . Dataset details can be   found in Appendix A.   4.2 Baselines   We compare our method with both task - agnostic   and task - specific distillation methods .   In task - agnostic distillation , we pre - train a small   language model through knowledge distillation ,   and then fine - tune on downstream tasks . The fine-   tuning procedure also incorporates task - specific   distillation for better performance .   DistilBERT ( Sanh et al . , 2019 ) pre - trains a small   language model by distilling the temperature-   controlled soft prediction probability .   TinyBERT ( Jiao et al . , 2020 ) is a task - agnostic dis-   tillation method that adopts layer - wise distillation .   MiniLMv1 ( Wang et al . , 2020 ) and MiniLMv2   ( Wang et al . , 2021 ) pre - train a small language   model by aligning the attention distribution be-   tween the teacher model and the student model .   CoDIR ( Contrastive Distillation , Sun et al . 2020a )   proposes a framework that distills knowledge   through intermediate Transformer layers of the   teacher via a contrastive objective .   In task - specific distillation , a pre - trained lan-   guage model is directly compressed and fine - tuned .   PKD ( Patient Knowledge Distillation , Sun et al .   2019 ) proposes a method where the student pa-   tiently learns from multiple intermediate Trans-   former layers of the teacher .   BERT - of - Theseus ( Xu et al . , 2020 ) proposes a pro-   gressive module replacing method for knowledge   distillation .   4.3 Implementation Details   In the experiments , we use BERT - base ( Devlin   et al . , 2019 ) as both the student model and the   teacher model . That is , we first transform the pre-   trained model into a MoE model , and then apply   layer - wise task - specific knowledge distillation . We   set the number of experts in the MoE model to 4 ,   and the hidden dimension of each expert is set to   768 , a quarter of the hidden dimension of BERT-   base . The other configurations remain unchanged .   We share the top- 512important neurons among the   experts ( see Section 3.1 ) . The number of effective1614RTE CoLA MRPC SST-2 QNLI QQP MNLI   Acc Mcc F1 / Acc Acc Acc F1 / Acc m / mm   BERT - base 63.5 54.7 89.0/84.1 92.9 91.1 88.3/90.9 84.5/84.4   Task - agnostic   DistilBERT 59.9 51.3 87.5/- 92.7 89.2 -/88.5 82.2/-   TinyBERT ( w/o aug ) 72.2 42.8 88.4/- 91.6 90.5 -/90.6 83.5/-   MiniLMv1 71.5 49.2 88.4/- 92.0 91.0 -/91.0 84.0/-   MiniLMv2 72.1 52.5 88.9/- 92.4 90.8 -/91.1 84.2/-   CoDIR ( pre+fine ) 67.1 53.7 89.6/- 93.6 90.1 -/89.1 83.5/82.7   Task - specific   PKD 65.5 24.8 86.4/- 92.0 89.0 -/88.9 81.5/81.0   BERT - of - Theseus 68.2 51.1 89.0/- 91.5 89.5 -/89.6 82.3/-   CoDIR ( fine ) 65.6 53.6 89.4/- 93.6 90.4 -/89.1 83.6/82.8   Ours ( task - specific )   MoEBERT 74.0 55.4 92.6/89.5 93.0 91.3 88.4/91.4 84.5/84.8   parameters of the MoE model is 66M(v.s.110 M   for BERT - base ) , which is the same as the base-   line models . We use the random hashing strategy   ( Roller et al . , 2021 ) to train the MoE model , we   will discuss more later . Detailed training and hyper-   parameter settings can be found in Appendix B.   4.4 Main Results   Table 1 summarizes experimental results on the   GLUE benchmark . Notice that our method out-   performs all of the baseline methods in 6/7tasks .   In general task - agnostic distillation behaves better   than task - specific algorithms because of the pre-   training stage . For example , the best - performing   task - specific method ( BERT - of - Theseus ) has a 68.2   accuracy on the RTE dataset , whereas accuracy   of MiniLMv2 and TinyBERT are greater than 72 .   Using the proposed method , MoEBERT obtains a   74.0accuracy on RTE without any pre - training , in-   dicating the effectiveness of the MoE architecture .   We remark that MoEBERT behaves on par or bet-   ter than the vanilla BERT - base model in all of the   tasks . This shows that there exists redundancy in   pre - trained language models , which paves the way   for model compression .   Table 2 summarizes experimental results on two   question answering datasets : SQuAD v1.1 and   SQuAD v2.0 . Notice that MoEBERT significantlyoutperforms all of the baseline methods in terms   of both evaluation metrics : exact match ( EM )   and F1 . Similar to the findings in Table 1 , task-   agnostic distillation methods generally behave bet-   ter than task - specific ones . For example , PKD   has a 69.8F1 score on SQuAD 2.0 , while per-   formance of MiniLMv1 and MiniLMv2 is over   76 . Using the proposed MoE architecture , perfor-   mance of our method exceeds both task - specific   and task - agnostic distillation , e.g. , the F1 score   ofMoEBERT on SQuAD 2.0 is 76.8 , which is 7.0   higher than PKD ( task - specific ) and 0.4higher than   MiniLMv2 ( task - agnostic ) .   4.5 Ablation Study   Expert dimension . We examine the affect of ex-   pert dimension , and experimental results are illus-   trated in Figure 2a . As we increase the dimension   of the experts , model performance improves . This   is because of the increased model capacity due to a   larger number of effective parameters .   Number of experts . Figure 2b summarizes ex-   perimental results when we modify the number of   experts . As we increase the number of experts ,   model performance improves because we effec-   tively enlarge model capacity . We remark that hav-   ing only one expert is equivalent to compressing1615SQuAD v1.1 SQuAD v2.0   EM F1 EM F1   BERT - base ( Devlin et al . , 2019 ) 80.7 88.4 74.5 77.7   Task - agnostic   DistilBERT ( Sanh et al . , 2019 ) 78.1 86.2 66.0 69.5   TinyBERT ( w/o aug ) ( Jiao et al . , 2020 ) - - - 73.1   MiniLMv1 ( Wang et al . , 2020 ) - - - 76.4   MiniLMv2 ( Wang et al . , 2021 ) - - - 76.3   Task - specific   PKD ( Sun et al . , 2019 ) 77.1 85.3 66.3 69.8   Ours ( task - specific )   MoEBERT 80.4 87.9 73.6 76.8   RTE MNLI SQuAD v2.0   Acc m / mm EM / F1   MoEBERT 74.0 84.5/84.8 73.6/76.8   -distill 73.3 83.2/84.0 72.5/76.0   the model without incorporating MoE. In this case   performance is unsatisfactory because of the lim-   ited representation power of the model .   Shared dimension . Recall that we share impor-   tant neurons among the experts when adapting the   FFNs . In Figure 2c we examine the effect of vary-   ing the number of shared neurons . Notice that   sharing no neurons yields the worst performance ,   indicating the effectiveness of the sharing strategy .   Also notice that performance of sharing all the neu-   rons is also unsatisfactory . We attribute this to the   lack of diversity among the experts.4.6 Analysis   Effectiveness of distillation . After adapting the   FFNs in the pre - trained BERT - base model into ex-   perts , we train MoEBERT using layer - wise knowl-   edge distillation . In Table 3 , we examine the ef-   fectiveness of the proposed distillation method .   We show experimental results on RTE , MNLI and   SQuAD v2.0 , where we remove the distillation and   directly fine - tune the adapted model . Results show   that by removing the distillation module , model   performance significantly drops , e.g. , accuracy de-   creases by 0.7on RTE and the exact match score   decreases by 1.1on SQuAD v2.0 .   Effectiveness of importance - based adaptation .   Recall that we adapt the FFNs in BERT - base   into experts according to the neurons ’ importance   scores ( Eq . 4 ) . We examine the method ’s effective-   ness by experimenting on two different strategies :   randomly split the FFNs into experts ( denoted Ran-   dom ) , and adapt ( and share ) the FFNs according1616   to the inverse importance , i.e. , we share the neu-   rons with the smallest scores ( denoted Inverse ) .   Figure 3a illustrated the results . Notice that perfor-   mance significantly drops when we apply random   splitting compared with Import ( the method we   use ) . Moreover , performance of Inverse is even   worse than random splitting , which further demon-   strates the effectiveness of the importance metric .   Different distillation methods . MoEBERT is   trained using a layer - wise distillation method   ( Eq . 7 ) , where we add a distillation loss to every   intermediate layer ( denoted All ) . We examine two   variants : ( 1 ) we only distill the hidden states of the   last layer ( denoted Last ) ; ( 2 ) we distill the hidden   states of every other layer ( denoted Skip ) . Fig-   ure 3b shows experimental results . We see that   only distilling the last layer yields unsatisfactory   performance ; while the Skip method obtains similar   results compared with All(the method we use ) .   Different routing methods . By default , we use a   random hashing strategy ( denoted Hash - r ) to route   input tokens to experts ( Roller et al . , 2021 ) . That   is , each token in the vocabulary is pre - assigned to   a random expert , and this assignment remains the   same during training and inference . We examine   other routing strategies :   1.We employ sentence - based routing with a   trainable gate as in Eq . 3 ( denoted Gate ) . Note   that in this case , token representations in a sen-   tence are averaged to compute the sentence   representation , which is then fed to the gating   mechanism for routing . Such a sentence - level   routing strategy can significantly reduce com-   munication overhead in MoE models . There-   fore , it is advantageous for inference com-   pared with other routing methods .   2.We use a balanced hash list ( Roller et al . ,   2021 ) , i.e. , tokens are pre - assigned to expertsRTE MNLI - m MNLI - mm   Acc Acc Acc   BERT 71.1 86.3 86.2   MoEBERT 72.2 86.3 86.5   according to frequency , such that each expert   receives approximately the same amount of   inputs ( denoted Hash - b ) .   From Figure 3c , we see that all the methods yield   similar performance . Therefore , MoEBERT is ro-   bust to routing strategies .   Inference speed . We examine inference speed of   BERT , DistilBERT and MoEBERT on the SST-2   dataset , and Figure 4 illustrates the results . Note   that for MoEBERT , we use the sentence - based gat-   ing mechanism as in Figure 3c . All the methods   are evaluated on the same CPU , and we set the   maximum sequence length to 128and the batch   size to 1 . We see that the speed of MoEBERT is   slightly slower than DistilBERT , but significantly   faster than BERT . Such a speed difference is be-   cause of two reasons . First , the gating mecha-   nism in MoEBERT causes additional inference la-   tency . Second , DistilBERT develops a shallower   model , i.e. , it only has 6layers instead of 12layers ;   whereas MoEBERT is a narrower model , i.e. , the   hidden dimension is 768instead of 3072 .   Compressing larger models . Task - specific distil-   lation methods do not require pre - training . There-   fore , these methods can be easily applied to other   model architectures and sizes beyond BERT - base .   We compress the BERT - large model . Specifically ,   we adapt the FFNs in BERT - large ( with hidden   dimension 4096 ) into four experts , such that each   expert has hidden dimension 1024 . We share the   top-512neurons among experts according to the1617importance score . After compression , the num-   ber of effective parameters is reduces by half . Ta-   ble 4 demonstrates experimental results on RTE   and MNLI . We see that similar to the findings in   Table 1 , MoEBERT behaves on par or better than   BERT - large in all of the experiments .   5 Conclusion   We present MoEBERT , which uses a Mixture - of-   Experts structure to distill pre - trained language   models . Our proposed method can speedup in-   ference by adapting the feed - forward neural net-   works ( FFNs ) in a pre - trained language model into   multiple experts . Moreover , the proposed method   largely retains model capacity of the pre - trained   model . This is in contrast to existing approaches ,   where the representation power of the compressed   model is limited , resulting in unsatisfactory perfor-   mance . To adapt the FFNs into experts , we adopt   an importance - based method , which identifies and   shares the most important neurons in a FFN among   the experts . We further propose a layer - wise task-   specific distillation algorithm to train MoEBERT .   We conduct systematic experiments on natural lan-   guage understanding and question answering tasks .   Results show that the proposed method outper-   forms existing distillation approaches .   Ethical Statement   This paper proposes MoEBERT , which uses a   Mixture - of - Experts structure to increase model ca-   pacity and inference speed . We demonstrate that   MoEBERT can be used for model compression . Ex-   periments are conducted by fine - tuning pre - trained   language models on natural language understand-   ing and question answering tasks . In all the exper-   iments , we use publicly available data and mod-   els , and we build our algorithms using public code   bases . We do not find any ethical concerns .   References1618161916201621A Dataset details   Statistics of the GLUE benchmark is summarized   in Table 6 . Statistics of the question answering   datasets ( SQuAD v1.1 and SQuAD v2.0 ) are sum-   marized in Table 5 .   # Train # Validation   SQuAD v1.1 87,599 10,570   SQuAD v2.0 130,319 11,873   B Training Details   We use Adam ( Kingma and Ba , 2015 ) as the op-   timizer with parameters ( β , β ) = ( 0 .9,0.999 ) .   We employ gradient clipping with a maximum gra-   dient norm 1.0 , and we choose weight decay from   { 0,0.01,0.1 } . The learning rate is chosen from   { 1×10,2×10,3×10,4×10 } , and   we do not use learning rate warm - up . We train   the model for { 3,4,5,10}epochs with a batch size   chosen from { 8,16,32,64 } . The weight of the   distillation loss λis chosen from { 1,2,3,4,5 } .   Hyper - parameters for distilling BERT - base is   summarized in Table 7 . We use Adam ( Kingma   and Ba , 2015 ) as the optimizer with parameters   ( β , β ) = ( 0 .9,0.999 ) . We employ gradient clip-   ping with a maximum gradient norm 1.0 . We do   not use learning rate warm - up . For the GLUE   benchmark , we use a maximum sequence length   of512except MNLI and QQP , where we set the   maximum sequence length to 128 . For the SQuAD   datasets , the maximum sequence length is set to   384.1622Corpus Task # Train # Dev # Test # Label Metrics   Single - Sentence Classification ( GLUE )   CoLA Acceptability 8.5k 1k 1k 2 Matthews corr   SST Sentiment 67k 872 1.8k 2 Accuracy   Pairwise Text Classification ( GLUE )   MNLI NLI 393k 20k 20k 3 Accuracy   RTE NLI 2.5k 276 3k 2 Accuracy   QQP Paraphrase 364k 40k 391k 2 Accuracy / F1   MRPC Paraphrase 3.7k 408 1.7k 2 Accuracy / F1   QNLI QA / NLI 108k 5.7k 5.7k 2 Accuracy   Text Similarity ( GLUE )   STS - B Similarity 7k 1.5k 1.4k 1 Pearson / Spearman corr   lr batch epoch decay λ   RTE 1×101×8 10 0 .01 1 .0   CoLA 2×101×8 10 0 .0 3 .0   MRPC 3×101×8 5 0 .0 2 .0   SST-2 2×102×8 5 0 .0 1 .0   QNLI 2×104×8 5 0 .0 2 .0   QQP 3×108×8 5 0 .0 1 .0   MNLI 5×108×8 5 0 .0 5 .0   SQuAD v1.1 3×104×8 5 0 .01 2 .0   SQuAD v2.0 3×102×8 4 0 .1 1 .01623