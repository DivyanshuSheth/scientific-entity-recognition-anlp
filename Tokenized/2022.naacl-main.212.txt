  Wang Xu , Kehai Chen , Lili Mou , Tiejun ZhaoHarbin Institute of Technology , ChinaDept . Computing Science , Alberta Machine Intelligence Institute ( Amii )   University of Alberta , Canada   xuwang@hit-mtlab.net , { chenkehai,tjzhao}@hit.edu.cn , doublepower.mou@gmail.com   Abstract   Document - level relation extraction ( DocRE )   aims to determine the relation between two   entities from a document of multiple sentences .   Recent studies typically represent the entire   document by sequence- or graph - based models   to predict the relations of all entity pairs .   However , we find that such a model is not   robust and exhibits bizarre behaviors : it   predicts correctly when an entire test document   is fed as input , but errs when non - evidence   sentences are removed . To this end , we   propose a Sentence Importance Estimation   and Focusing ( SIEF ) framework for DocRE ,   where we design a sentence importance score   and a sentence focusing loss , encouraging   DocRE models to focus on evidence sentences .   Experimental results on two domains show   that our SIEF not only improves overall   performance , but also makes DocRE models   more robust . Moreover , SIEF is a general   framework , shown to be effective when   combined with a variety of base DocRE   models .   1 Introduction   Document - level relation extraction ( DocRE ) aims   to predict entity relations across multiple sentences .   It plays a crucial role in a variety of knowledge-   based applications , such as question answer-   ing ( Sorokin and Gurevych , 2017 ) and large - scale   knowledge graph construction ( Baldini Soares   et al . , 2019 ) . Different from sentence - level relation   extraction ( Zeng et al . , 2014 ; Xiao and Liu , 2016 ;   Song et al . , 2019 ) , the supporting evidence in the   DocRE setting may involve multiple sentences   scattering in the document . Thus , DocRE is more   a realistic setting , attracting increasing attention in   the field of information extraction .   Most recent DocRE studies use the entire   document as a clue to predict the relations ofFigure 1 : A DocRE model predicts correctly for an   entire document , but errs when a non - evidence sentence   is removed .   all entity pairs without concerning where the   evidence is located ( Nan et al . , 2020 ; Zeng et al . ,   2020 ; Xu et al . , 2021a , b ) . However , one can   identify the relation of a specific entity pair from   a few sentences . Huang et al . ( 2021 ) show that   irrelevant sentences in the document would hinder   the performance of the model .   Moreover , we observe that a DocRE model ,   trained on the entire document , may err when   non - evidence sentences are removed . In Figure 1 ,   for example , we need to identify the relation   “ MemberOf ” between the entities Brad Wilk and   Rage Against the Machine . The evidence sentences   are { 1,2 } , and humans can easily identify such   a relation when reading sentences { 1,2 } only .   However , the recent DocRE model GAIN ( Zeng   et al . , 2020 ) identifies the relation “ MemberOf ”   correctly from the entire document { 1,2,3 } , but   predicts “ not MemberOf ” from sentences { 1,2 } .   Intuitively , removing sentence { 3 } should not   change the result , as this sentence does not provide   information regarding whether “ MemberOf ” holds   or not for the two entities . Such model behaviors   are undesired , because it shows that the model is   not robust and lacks interpretability .   To this end , we propose a novel Sentence   Importance Estimation and Focusing ( SIEF )   framework to encourage the model to focus on   evidence sentences for predicting the relation of2920an entity pair . Specifically , we first evaluate the   importance of each sentence by the difference   between the output probabilities of the document   with and without this sentence . If the predicted   probability of a relation does not change , or even   increases , when a sentence is removed , it typically   indicates that the sentence is non - evidence . Then ,   we propose an auxiliary loss to encourage the   model to produce the same output distribution ,   when the entire document is fed as input and when   a non - evidence sentence is removed . In this way ,   the model pays more attention to the evidence   sentences for the classification . Our SIEF method   is a general framework that can be combined with   different underlying DocRE models .   We evaluated the generality and effectiveness   of our approach on the large - scale DocRED   dataset ( Yao et al . , 2019 ) . Experimental results   show that the proposed approach combines   well with various recent DocRE models and   significantly improves the performance . We further   evaluated our approach on a dialogue relation   extraction dataset , DialogRE ( Yu et al . , 2020 ) ; our   SIEF yields consistent improvement , showing the   generality of our approach in different domains .   2 Related Work   Relation extraction ( RE ) can be categorized by its   granularity , such as sentence - level ( Doddington   et al . , 2004 ; Xu et al . , 2016 ; Wei et al . , 2020 )   and document - level ( Gupta et al . , 2019 ; Zhu   et al . , 2019 ) . Early work mainly focuses   on sentence - level relation extraction . Pantel   and Pennacchiotti ( 2006 ) propose a rule - based   approach , and Mintz et al . ( 2009 ) manually design   features for classifying relations . In the past several   years , neural networks have become a prevailing   approach for relation extraction ( Xu et al . , 2015 ;   Song et al . , 2019 ) .   Document - level relation extraction ( DocRE ) is   attracting increasing attention in the community ,   as it considers the interactions of entity mentions   expressed in different sentences ( Li et al . , 2016 ;   Yao et al . , 2019 ) . Compared with the sentence   level , DocRE requires the model collecting and   integrating inter - sentence information effectively .   Recent efforts design sequence - based and graph-   based models to address such a problem .   Sequence - based DocRE models encode a   document by the sequence of words and/or   sentences , for example , using the Transformerarchitecture ( Devlin et al . , 2019 ) . Zhou et al . ( 2021 )   argue that the Transformer attentions are able to   extract useful contextual features across sentences   for DocRE , and they adopt an adaptive threshold   for each entity pair . Zhang et al . ( 2021 ) model   DocRE as a semantic segmentation task and predict   an entity - level relation matrix to capture local and   global information .   Graph - based DocRE models abstract a docu-   ment by graphical structures . For example , a   node can be a sentence , a mention , and/or an   entity ; their co - occurrence is modeled by an   edge . Then graph neural networks are applied to   aggregate inter - sentence information ( Quirk and   Poon , 2017 ; Christopoulou et al . , 2019 ; Zeng   et al . , 2020 ) . Zeng et al . ( 2020 ) construct   double graphs , applying graph neural networks to   mention – document graphs and performing path   reasoning over entity graphs . Xu et al . ( 2021a )   explicitly incorporate logical reasoning , common-   sense reasoning , and coreference reasoning into   DocRE , based on both sequence and graph features .   Different from previous work , our paper   proposes SIEF as a general framework that can   be combined with various sequence - based and   graph - based DocRE models . In our approach , we   propose a sentence importance score and a sentence   focusing loss to encourage the model to focus on   evidence sentences , improving the robustness and   the overall performance of DocRE models .   3 Problem Definition   In this section , we present the formulation of   document relation extraction ( DocRE ) . Consider   an unstructured document comprising Nsentences ,   D={s , s,···,s } , where each sentence sis a   sequence words . In a DocRE dataset , the document   Dis typically annotated with entity mentions ,   each mention ( e.g. , U.S. and USA ) labeled by   its conceptual entity eand its entity type ( e.g. ,   location ) .   A DocRE model Fis usually formulated as   multi - label classification ( Yao et al . , 2019 ) . F   predicts whether the jth relation holds for the ith   marked entity pair in a document , given by   P = F(D , e , e ) = Pr [ r= 1|D , e , e ]   ( 1 )   where eis the head entity and eis the tail entity ;   r∈ { 0,1}is the groundtruth label regarding   entity pair iand relation j.2921   To train the model , the binary cross - entropy loss   is used as the objective for parameter estimation :   L=−/summationdisplay / summationdisplay / summationdisplay{rlogP   + ( 1−r ) log(1 −P)}(2 )   where Cdenotes the entire corpus and Rdenotes   the set of relation types .   During inference , we obtain the relation(s ) of   a given entity pair by thresholding the predicted   probabilities , following most previous work ( Yao   et al . , 2019 ; Zhou et al . , 2021 ) .   4 Methodology   In this section , we will describe our approach in   detail . The overview of our framework is shown   in Figure 2 . First , we describe the estimation of   sentence importance in Section 4.1 . Sentences with   low importance scores are treated as non - evidence .   Then , Sections 4.2 and 4.3 present our approach   that encourages the model to produce the same   output distribution , when the entire document is   fed as input and when non - evidence sentences   are removed . Section 4.4 further presents the   architectures of DocRE models .   4.1 Sentence Importance Estimation   We estimate the importance of each sentence for a   specific entity pair . Low - scored sentences will be   treated as non - evidence , and in principle , can be   removed without changing DocRE predictions .   We propose a sentence importance score based   on the DocRE predictions with and without the   sentence in question . Our observation is that the   relation extraction task is usually monotonic to   evidence , i.e. , ( non - strictly ) more relations willbe predicted with more sentences . If we remove a   sentence and the predicted probability of a relation   decreases , then the sentence is likely to be the   evidence . If the predicted probability does not   change , then the sentence is likely to be non-   evidence . Moreover , the predicted probability may   sometimes increase when a sentence is removed ,   in which case the DocRE model is not robust , as   this violates monotonicity .   Formally , we consider removing one sentence   at a time , and the document with the nth   sentence removed is denoted by ˆD=   { s,···,s , s,···,s } . For a DocRE   model F , we obtain the classification probabilities   P = F(D , e , e)based on the original   document , and ˆP = F(ˆD , e , e)with   sentence nremoved .   We propose the importance score as   g = Plog(3 )   The formula appears similar to Kullback – Leibler   ( KL ) divergence . However , we only take one term   in the KL summation , because the KL divergence ,   albeit asymmetric in its two arguments , can not   model the increase or decrease of ˆP , whereas   ourgis monotonically decreasing with ˆP.   Compared with a naive difference or ratio between   PandˆP , we find that our KL - like score is   more robust in the scale of Pwhen determining   non - evidence sentences .   We treat a sentence nasnon - evidence ifg <   βfor a thresholding hyperparameter β . The   resulting set of non - evidence sentences is denoted   byKfor the an entity pair ( e , e)and relation j.29224.2 Sentence Focusing Loss   We propose a sentence focusing loss to encourage   the model to produce the same output distribution   when the entire document is fed as input and when   non - evidence sentences are removed .   Ideally , the predicted probability should remain   the same if we remove any combination of the   sentences in K. Therefore , we penalize the extent   to which the predicted probability is changed .   We propose the sentence focusing loss as :   L=−/summationdisplay / summationdisplay / summationdisplay / summationdisplay{Plog(ˆP )   + ( 1−P ) log(1 −ˆP ) }   ( 4 )   where Jis a subset of Kand ˆP =   F(D\J , e , e)is the predicted probability with   Jremoved from D , and the total loss is L=   ( L+L)/2 .   Essentially , our sentence focusing loss ensures   Pis close to ˆP , which intuitively makes   sense because non - evidence sentences should not   affect the prediction . Our approach can also be   thought of as a way of data augmentation . However ,   compared with one - hot groundtruth labels , our   sentence focusing loss works with soft labels P   andˆP , which are believed to contain more   information ( Hinton et al . , 2015 ) , and our gradient   propagates to both PandˆP for training .   The calculation of Eqn . ( 4)is time- and resource-   consuming , because the number of the subsets   Jgrows combinatorially with the number of   non - evidence sentences . Moreover it should be   calculated repeatedly once the parameter of the   model is updated . To this end , we propose a   simplified training strategy to approximate Eqn . ( 4 )   in the next subsection .   4.3 Training Strategy   We propose a strategy to simplify the calculation   and the training procedure . Concretely , we only   remove one non - evidence sentence in Kat a time   instead of a subset of J⊆K , and we aggregate   the effect of different non - evidence sentences by :   L=−/summationdisplay / summationdisplay / summationdisplay / summationdisplayI(g < β )   { Plog(ˆP ) + ( 1 −P ) log(1 −ˆP ) }   ( 5)where Iis the indicator function . Essentially ,   we linearly approximate the combination of   multiple non - evidence sentences in ( 4)by an outer   summation . In this way , the number of terms does   not grow combinatorially , but linearly w.r.t . N.   In implementation , we further simply the   summation over nby Monte Carlo sampling of   a randomly selected sentence nin each gradient   update . The loss is reformulated as follows :   L=−/summationdisplay / summationdisplay / summationdisplayI(g < β )   { Plog(ˆP ) + ( 1 −P ) log(1 −ˆP ) }   ( 6 )   As seen , we need to forward the base models   twice in each update , with and without the sentence   n. Huang et al . ( 2021 ) propose a similar idea   but train different entity pairs in a document   based on different sets of sentences ; all sentence   are processed repeatedly among entity pairs in a   document . Their approach is much slower than   ours .   To sum up , the proposed SIEF framework   identifies non - evidence sentences and penalizes the   difference of predicted probabilities when a non-   evidence sentence is removed . Our approach is a   generic framework and can be adapted to various   DocRE model easily , without introducing extra   parameters into the model .   4.4 DocRE Model Architectures   Our SIEF can be applied to various base DocRE   models . To evaluate its generality , we consider the   following recent models .   BiLSTM ( Yao et al . , 2019 ) . A bi - directional   long short term memory ( BiLSTM ) encodes the   document , and an entity is representated by   BiLSTM ’s hidden states , averaged over entity   mentions . The head and tail entity representations   are fed to a multi - layer perceptron ( MLP ) for   relation extraction .   BERT(Devlin et al . , 2019 ) . A pre - trained   language model is used for document encoding .   HeterGSAN ( Xu et al . , 2021b ) . HeterGSAN   is a recent graph - based DocRED model , which   constructs a heterogeneous graph of sentence ,   mention , and entity nodes ; it uses graph neural   networks for relation extraction.2923   GAIN ( Zeng et al . , 2020 ) . GAIN constructs   two graphs : mention – document graphs and entity   graphs , and performs graph and path reasoning over   the two graphs separately . When combining our   SIEF with GAIN , we achieve the best performance   among all the base models with SIEF on DocRED .   Thus , we will explain this model in more detail .   Essentially , a node in the mention – document   graph is either a mention or a document . The   mentions are connected to its document , and two   mentions are connected if they co - occur in one   sentence . In the entity graph , two entities are   connected if they are mentioned in one sentence . To   classify the relation , GNN is applied to the mention –   document graph , enhanced with path information   in the entity graph , shown in Figure 3 .   When combining SIEF with GAIN , we randomly   remove one sentence from the document . The   corresponding nodes and edges are removed in   the GAIN ’s graphs . Then we obtain the output   probabilities with and without the sentence , P   andˆP , separately . If the sentence important   score g in Eqn . ( 3)is below a threshold β , the   sentence is treated as non - evidence for the entity   pair(e , e)and relation j. We apply the sentence   focusing loss Eqn . ( 4 ) to improve the robustness .   For prediction , we apply the trained DocRE   model to the entire document , because with our   approach the model is already robust when non-   evidence sentences are presented . Empirical results   will show that our SIEF consistently improves the   performance of base DocRE models.5 Experiments   5.1 Setup   Datasets . DocRED is a large - scale human-   annotated dataset for document - level relation   extraction ( Yao et al . , 2019 ) . The dataset   is constructed from Wikipedia and Wikidata ,   containing 3053 documents for training , 1000 for   development , and 1000 for test . In total , it has   132,375 entities and 56,354 relational facts in 96   relation types . More than 40 % of the relational   facts require reasoning over multiple sentences .   The standard evaluation metrics are F1 and Ign F1   ( Yao et al . , 2019 ; Zeng et al . , 2020 ) , where Ign F1   refers to the F1 score excluding the relational facts   in the training set .   We also evaluated our approach on DialogRE   ( V2 , Yu et al . , 2020 ) , which contains 36 relation   types , 17 of which are interpersonal . We followed   the standard split with 1073 training dialogues , 358   validation , and 357 test . Following Yu et al . ( 2020 ) ,   we report macro F1 scores in both the standard and   conversational settings ; the latter is denoted by F1 .   Competing Methods . We experimented our   SIEF on a number of base models , namely ,   BiLSTM , BERT , HeterGSAN , and GAIN   ( Section 4.4 ) . These base models are all considered   for comparison .   For DocRED , we consider additional competing   methods : Two Phase ( Wang et al . , 2019 ) , which   first predicts whether the entity pair has a relation   and then predicts the relation type ; LSR ( Nan et al . ,   2020 ) , which constructs the graph by inducing a   latent document - level graph ; Reconstructor ( Xu   et al . , 2021b ) , which encourages the model   to reconstruct a reasoning path during training ;   DRN ( Xu et al . , 2021a ) , which considers   different reasoning skills explicitly and uses   graph representation and context representation to   model the reasoning skills ; ATLOP ( Zhou et al . ,   2021 ) , which aggregates contextual information   by the Transformer attentions and adopts an   adaptive threshold for different entity pairs ; and   DocuNet ( Zhang et al . , 2021 ) , which models   DocRE as a semantic segmentation task .   For DialogRE , we followed Yu et al . ( 2020 ) and   considered BERT andBERTfor comparison ,   where BERTprevents a model from overfitting   by replacing of the interpersonal augment with a   special token.2924   Implementation Details . We use the   repositoriesof base models to implement   our approach . We mostly followed the standard   hyperparameters used in the base models . Our   SIEF has one hyperparameter βin Eqn . ( 5 ) . It was   set to 0.8 , and Section 5.2 presents the effect of   tuning β .   5.2 Results and Analyses   Main results . Table 1 presents the detailed results   on the development and test sets of the DocRED   dataset . We first compare DocRE systems with   GloVe embeddings ( Yao et al . , 2019 ) . We see that   the proposed SIEF method significantly improves   the performance of all base models , including   the sequence model ( i.e. , BiLSTM ) and graph   models ( i.e. , HeterGSAN and GAIN ) ; the average   improvement is 2.05 points in terms of test F1 . This   shows that SIEF is compatible with both sequence   and graph models , indicating the generality and   effectiveness of the proposed method .   For the DocRE system with BERT , SIEF also   consistently improves the base models , showing   that SIEF is complementary to the modern BERT   architecture . Especially , combining SIEF and   GAIN ( Zeng et al . , 2020 ) with BERTencoding   yields state - of - the - art performance in terms of F1 .   We further conducted experiments on the   DialogRE dataset , and compare our approach with   the BERT baselines in Yu et al . ( 2020 ) . As seen ,   the results are consistent with the improvement on   DocRED , as our SIEF largely improves F1 and F1   for both base models . This further confirms the   generality of our approach in different domains .   In the rest of this section , we present in - depth   analyses to better understand our model with   DocRED as the testbed . All base models use GloVe   embeddings as opposed to BERT due to efficiency   concerns .   Intra- and Inter - Sentence Performance . We   breakdown the relation classification performance   into intra - sentence reasoning and inter - sentence   reasoning . Ideally , if only one sentence is needed   to determine the relation of an entity pair , then   it belongs to the intra - sentence category ; if two   or more sentences are needed , then it belongs to   the inter - sentence category . We follow Nan et al .   ( 2020 ) and approximate it by checking whether two   entities are mentioned in one sentence .   The results are shown in Table 3 . SIEF again   consistently improves base models in terms of both   Intra - F1 and Inter - F1 . However , the improvement   on Intra - F1 is larger than that on Inter - F1 . This is   because our SIEF encourages the model to focus   on evidence by removing one sentence at a time ,   but does not explicitly model sentence relations .   Based on this analysis , we plan to extend the SIEF   framework with multi - sentence DocRE reasoning   in our future work .   Performance of predicting evidence sentences .   In our paper , we propose a sentence importance   score to measure how much a sentence contributes   to the classification without using additional2925   annotation . We evaluate such performance in   Table 4 by Precision , Recall , and F1 scores against   manually annotated evidence sentences that are   provided in the dataset . In this analysis , we   do not perform relation prediction , but concern   about entity pairs knowingly having certain   relations . Specifically , for entity pair ( e , e )   with relation j , we calculate the importance score   g for each sentence and cut off evidence / non-   evidence sentences with a threshold based on the   development F1 score .   As seen , all base models achieve above 60 % F1 ,   suggesting that the proposed importance score is   indeed indicative for predicting evidence and non-   evidence sentences .   With the proposed SIEF framework , the   performance improves for all metrics , with an   average improvement of 2.95 F1 points across   three base models . This further verifies that   our SIEF framework not only improves relation   extraction performance , but also is able to better   detect evidence and non - evidence sentences , which   is important for the interpretability of machine   learning models .   Robustness of DocRE models . We further   investigate the robustness of DocRE models by   showing the difference between the predicted   distributions with and without non - evidence   sentences . We show in Figure 5 the scatter plots of   the probability Pbased on the entire document and   the probability ˆPwith a random non - evidence   sentence removed .   As shown in the figure , the points of the base   models ( left magenta plots ) scatters over a wider   range , whereas our SIEF training ( right cyan plots )   makes them more concentrated on the diagonal ,   indicating that the prediction Pon the entire   document is mostly the same as ˆP with a non-   evidence removed . This shows the robustness of   SIEF - trained models , as they are less sensitive to   non - evidences sentences for DocRE .   Analysis on hyperparameter β . Our SIEF   framework has one hyperaparameter βthat controls   how strict we treat a sentence as evidence or non-   evidence ( Section 4.3 ) . We analyze the effect of β   in Figure 4.2926   As seen , our SIEF approach consistently benefits   the base models with a large range of βvalues .   Intuitively , if βis too small , very few sentences   will be treated as non - evidence and our sentence   focusing loss is less effective ; if βis too large , it has   a high false positive rate of non - evidence sentences .   Empirically , a moderate βaround ( 0.6–0.8 ) yields   the highest performance . From the plots , we also   see that our hyperparameter βis insensitive to the   base models , justifying our design of Eqn . ( 3 ) .   Sentence importance score VS other heuris-   tics . To investigate the effectiveness of our   sentence importance score in Eqn . ( 3 ) , we compare   it with several alternative heuristics : 1 ) We   randomly select half of the sentences as the non-   evidence set , denoted by Rand ; and 2 ) We consider   the non - evidence set as the sentences without entity   mentions , denoted by NoMention .   The results of the performance in terms of F1 and   Ign F1 on the development set are shown in Table 5 .   As seen , the simple heuristic Rand outperforms   the base model , as Rand can be thought of as   noisy data augmentation . The NoMention heuristic   outperforms Rand , as sentences without entity   mentions are more likely to be non - evidence .   Moroever , SIEF is superior to both Rand and   NoMention , showing that our sentence importance   scores is a more effective indicator of evidence and   non - evidence sentences .   Our sentence focusing loss VS learning from   groundtruth . We encourage the DocRE models   to generate consistent output probabilities with   and without non - evidence ( Section 4.2 ) by a cross-   entropy loss between two soft distributions P   and ˆP. To investigate the effect of such a   sentence focusing loss , we compare it with an   { 1,2,3 } { 1,2 }       { 1,3 } { 2,3 } GAIN   0.713      0.283 0.106 0.319Entity Pair : { Brad Wilk , Rage Against the Machine }      Reference : MemberOf Evidence : { 1,2 }   Input   SentencesThreshold0.574   { 1,2,3 }      { 1,2 }       { 1,3 }        { 2,3 } GAIN+SIEF   0.796      0.744 0.280 0.381   Input   SentencesThreshold0.506Predicted   Probability   Predicted   Probability [ 1]Rage Against theMachine isanAmerican rapmetal band from   LosAngeles , California .[2]Formed in1991 , thegroup consists of   vocalist Zack delaRocha , guitarist Tom Morello , bassist Tim   Commerford anddrummer Brad Wilk.[3]After aself - issued demo ,   theband signed with Epic Records and released itsdebut album   Rage Against theMachine in1992 . …   alternative choice : we learn ˆP directly from   the groundtruth label r.   Table 6 shows the results on the development   set in terms of F1 and Ign F1 . As seen , both   methods can improve the performance of the base   models . This confirms that removing non - evidence   sentences can serve as a way of data augmentation ,   boosting the performance of DocRE models .   Moreover , we observe that our sentence focusing   loss is better than learning from the groundtruth   labels , showing that the soft predictions provide   more information than one - hot labels , consistent   with knowledge distillation literature ( Hinton et al . ,   2015 ) .   Case Study . Figure 6 shows a case study of   GAIN and GAIN+SIEF models . For the entity   pair ( Brad Wilk , Rage Against the Machine ) ,   both GAIN and GAIN+SIEF predicts the relation   “ MemberOf ” , which is consistent with the reference .   We see that Sentence 3 is non - evidence , and in   principle , it should not affect DocRE prediction   in this case . However , the base GAIN model   makes a wrong prediction “ not MemberOf ” , as   the predicted probability is below the threshold ,   which is determined by validation based on   predicted binary probabilities of all relations . By   contrast , our SIEF model is able to make correct   predictions when different non - evidence sentences   are removed , demonstrating its robustness .   6 Conclusion   In this paper , we propose a novel Sentence   Information Estimation and Focusing ( SIEF )   approach to document relation extraction ( DocRE).2927We design a sentence importance score and a   sentence focusing loss to encourage the model   to focus on evidence sentences . The proposed   SIEF is a general framework , and can be combined   with various base DocRE models . Experimental   results show that SIEF consistently improves the   performance of base models in different domains ,   and that it improves the robustness of DocRE   models .   Acknowledgments   We are grateful to the anonymous reviewers and   meta reviewers for their insightful comments   and suggestions . This work is funded by   the National Key Research and Development   Program of China ( No . 2020AAA0108000 ) . The   corresponding authors are Kehai Chen and Tiejun   Zhao . Lili Mou is supported in part by the Natural   Sciences and Engineering Research Council of   Canada ( NSERC ) under grant No . RGPIN2020-   04465 , the Amii Fellow Program , the Canada   CIFAR AI Chair Program , a UAHJIC project , a   donation from DeepMind , and Compute Canada   ( www.computecanada.ca ) .   References29282929