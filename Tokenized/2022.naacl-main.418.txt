  Mingqi Gao , Xiaojun Wan   Wangxuan Institute of Computer Technology , Peking University   The MOE Key Laboratory of Computational Linguistics , Peking University   { gaomingqi,wanxiaojun}@pku.edu.cn   Abstract   Dialogue summarization is receiving increas-   ing attention from researchers due to its ex-   traordinary difﬁculty and unique application   value . We observe that current dialogue sum-   marization models have ﬂaws that may not be   well exposed by frequently used metrics such   as ROUGE . In our paper , we re - evaluate 18   categories of metrics in terms of four dimen-   sions : coherence , consistency , ﬂuency and rel-   evance , as well as a uniﬁed human evaluation   of various models in dialogue summarization   for the ﬁrst time . Some noteworthy trends   which are different from the conventional sum-   marization tasks are identiﬁed . We will release   DialSummEval , a multi - faceted dataset of hu-   man judgments containing the outputs of 14   models on SAMSum .   1 Introduction   Neural network based approaches and sizable   datasets have led to signiﬁcant progress in   researches towards conventional summarization   tasks such as news and scientiﬁc papers ( Lin and   Ng,2019 ) . Compared with conventional sum-   marization tasks , dialogue summarization has re-   ceived increasing attention from researchers due   to its great difﬁculty and unique application value   ( Feng et al . , 2021a ) . With the proposal of dialogue   summary datasets such as SAMSum ( Gliwa et al . ,   2019 ) , DialogSum ( Chen et al . , 2021 ) and Medi-   aSum ( Zhu et al . , 2021 ) , a number of models for   automatic generation of dialogue summaries have   emerged ( Feng et al . , 2021b ; Liu and Chen , 2021 ;   Zou et al . , 2021 ; Qi et al . , 2021 ; Chen and Yang ,   2020 ; Chen and Yang , 2021 ; Zhao et al . , 2020 ; Liu   et al . , 2021 ) .   There is no denying that these studies have   made promising progress , but it remains a chal-   lenge to evaluate these advances comprehen-   sively . Current studies generally use the SAMSumdataset and adopt ROUGE ( Lin,2004 ) , an n - gram-   based automatic evaluation metric using reference   summaries , as the overall evaluation criterion for   summary quality , complemented by manual eval-   uation . Schluter ( 2017 ) and Graham ( 2015 ) illus-   trate the limitations of ROUGE in evaluating sum-   marization tasks . Also the manual evaluation pro-   tocols vary from one research to another based on   our observations .   We argue that the inadequate evaluation mech-   anism may have become a major obstacle to the   progress of dialogue summarization researches .   Many studies , such as Chen and Yang ( 2020 )   andTang et al . ( 2021 ) , have pointed out that the   current dialogue summarization models still have   many shortcomings , such as wrong references , in-   correct reasoning and improper gender pronouns ,   and ROUGE may not reﬂect these problems ef-   fectively . For example , Gabriel et al . ( 2021 )   note that ROUGE-1 and ROUGE - L fail to ac-   curately measure factual inconsistency across do-   mains . Our case study in Table 1also illustrates   this point . However , it is impractical to perform   frequent time - consuming and costly manual eval-   uation . The alternative is to introduce or propose   more reliable automatic evaluation metrics to eval-   uate the models in a more comprehensive and ﬁne-   grained manner .   Although there are automatic evaluation met-   rics for measuring the quality of all aspects of   summaries on conventional summarization tasks ,   especially for factual consistency ( Huang et al . ,   2021 ) , it is difﬁcult to guarantee that they will   still perform well on dialogue summarizarion . Re-   cently proposed automatic metrics for evaluating   generic natural language generation tasks such as   BERTScore ( Zhang * et al . , 2020 ) , BARTScore   ( Yuan et al . , 2021 ) have also not been experi-   mented on dialogue summarization . The high ab-   straction level , low extraction rate , and the re-   quirement for complex reasoning power of the5693Dialogue Reference Summary Generated Summary R-1 R-2 R - L   Kirsten : Youth group   this Friday , do n’t be late .   Alex : What time ?   Kirsten : 7 pm . We ’re going   bowling , so we ’ll meet up   and then all go together .   Alex : Cool . See you .   Kirsten : ByeKirsten reminds Alex   that the youth group   meets this friday at   7 pm and go bowling . Kirsten is going   bowling with her   youth group this   Friday at 7 pm.0.69 0.44 0.61   Ola : Hey running late   Ola : I should be free by 8   Kurt : Sure no prob , call meOla should be free   by 8 . Kurt wants her   to call him . Ola will be late .   She should be   free by 8 . Kurt   will call her .0.69 0.42 0.67   dialogue summarization task present new chal-   lenges to automatic evaluation metrics . There   have been a number of manual evaluation datasets   and analytical studies for conventional summariza-   tion tasks ( ( Dang and Owczarzak , 2008 ) ; Fabbri   et al . , 2021b ; Bhandari et al . , 2020 ) , but very little   work has been done on systematic analysis of di-   alogue summarization models and evaluation met-   rics . Our work will ﬁll the gap in this area and   includes the following contributions : 1 ) We iden-   tify evaluation problems in the ﬁeld of dialogue   summarization and point out the urgent need of au-   tomatic evaluation metrics that better adapt to dia-   logue summarization . 2 ) We collect and provide   a sizable , multi - faceted dataset of manual evalua-   tions for dialogue summarization , which contains   the output of 14 models , and the dataset will be   released . 3 ) We re - evaluate the performance of 18   types of automatic evaluation metrics on dialogue   summarization . 4 ) We evaluate a variety of dia-   logue summarization models ( extractive , abstrac-   tive , and recently based on pre - trained language   models ) in a uniﬁed manner .   2 Related Work   Meta - Evaluation with Human Judgments Au-   tomatic evaluation Metrics such as ROUGE ( Lin ,   2004 ) and BERTScore ( Zhang * et al . , 2020 ) )   were compared with other metrics when proposed .   However , they are basically not using the dialogue   summarization dataset as an experimental corpus ,   and rarely provide new human judgments data .   Bhandari et al . ( 2020 ) used pyramid ( Nenkovaand Passonneau , 2004 ) , a widely used human eval-   uation method on several conventional summa-   rization datasets to obtain relevance scores for   some of the system outputs and re - evaluated the   metrics in 6 categories . Similarly , Fabbri et al .   ( 2021b ) used CNN / DailyMail dataset ( Hermann   et al . ,2015 ) and the output of some models for hu-   man evaluation covering four facets of relevance ,   consistency , ﬂuency , and coherence , and then re-   evaluated the metrics in 14 categories . None of   these involved dialogue summarization datasets .   Pagnoni et al . ( 2021 ) made a careful categoriza-   tion of factual errors and benchmarked factuality   metrics using human annotations they collected on   CNN / DailyMail and XSum dataset ( Narayan et al . ,   2018 ) . Notably , Gabriel et al . ( 2021 ) is one of   the few current studies using the dialogue summa-   rization dataset SAMSum ( Gliwa et al . , 2019 ) for   meta - evaluation , but it focuses on factual consis-   tency and selects a small number of metrics .   Analysis and Evaluation for Dialogue Sum-   marization Models Tang et al . ( 2021 ) and Chen   and Yang ( 2020 ) sampled the output of models   on SAMSum and analyzes the error types when   proposing a new model . Due to the different man-   ual evaluation protocols and the small number of   models included , it is difﬁcult to comprehensively   compare the strengths and weaknesses of differ-   ent models . Khalifa et al . ( 2021 ) designed sev-   eral tricks to address the special challenges in dia-   logue summarization and analysized their effects ,   such as using name substitution to cope with the   presence of multiple speakers in dialogues . Zhang5694et al . ( 2021 ) focused on the problem of lengthy   input and relevant information location in long di-   alogue summarization , and compared the perfor-   mance of some models and strategies . No manual   evaluation was involved in these studies .   3 Preliminaries   In this section , we introduce the involved dataset ,   metrics and models .   3.1 Dataset   SAMSum ( Gliwa et al . , 2019 ) is the ﬁrst man-   ually annotated , high - quality chat summarization   dataset , containing over 16k dialogues . We use   it in this study as it is most widely used and has   greatly promoted the research in the ﬁeld of dia-   logue summarization , and we are able to collect   the outputs of various models on this dataset .   3.2 Evaluation Metrics   We selected a number of evaluation metrics that   are frequently used on summarization or other nat-   ural language generation tasks . Some are for over-   all quality ; others are speciﬁc to a particular aspect .   Some require reference summaries or source doc-   uments ; some only need the summary itself . Here   is a brief categorization and description .   Metrics based on n - gram overlap include :   ROUGE ( Lin,2004 ) is the most widely used   automatic evaluation metric in summarization . Re-   searchers mainly adopt ROUGE-1 , ROUGE-2 and   ROUGE - L , which measure the unigram - overlap ,   bigram - overlap and longest common sequence be-   tween two texts respectively .   BLEU ( Papineni et al . , 2002 ) is the primary   evaluation metric for machine translation . It calcu-   lates n - gram overlap between texts using precision   scores and includes a brevity penalty .   METEOR ( Banerjee and Lavie , 2005 ) com-   putes an alignment by mapping unigrams in two   texts , based on surface forms , stemmed forms , and   meanings .   CHRF ( Popovi ´ c,2015 ) computes character   based n - gram overlap between two texts . Metrics based on pre - trained language models   include :   BERTScore ( Zhang * et al . , 2020 ) measures the   soft - overlap between two texts at token level using   contextual embeddings from BERT .   MoverScore ( Zhao et al . , 2019 ) applies the se-   mantic distance between two texts at n - gram level   using n - gram embeddings pooled from BERT .   BARTScore ( Yuan et al . , 2021 ) treats evalua-   tion as a nature language generation task and as-   sumes that when the quality of generated text is   better , the conditional language model has a higher   probability of generating it from the source text   or the reference , or is more likely to generate the   reference from it . It can be ﬂexibly applied to   evaluation of text from different perspectives us-   ing BART .   BLANC ( Vasilyev et al . , 2020 ) is a reference-   less metric . It hypothesizes that a good summary   is beneﬁcial for a pre - trained language model   to conduct language understanding tasks on the   source document . Speciﬁcally , it measures the per-   formance boost of the masked language modeling   for BERT utilizing the summary in two different   ways .   PPL , namely perplexity , is often used to evalu-   ate the quality of a language model or the ﬂuency   of an utterance . We adopt GPT-2 ( Radford et al . ,   2019 ) as the language model for computing the   perplexity for the whole summary .   Metrics based on word embeddings include :   SMS ( Clark et al . , 2019 ) , namely Sentence   Mover Similarity , extends Word Movers Distance   ( Kusner et al . , 2015 ) to measure the distance be-   tween two texts which are represented as a bag of   sentence embeddings .   Embedding average ( Landauer and Dumais ,   1997 ) is an embedding based metric computing   the cosine similarity between the embeddings of   two texts . A sentence - level embedding is repre-   sented by averaging the embeddings of the words   composing the sentence .   Vector extrema ( Forgues et al . , 2014 ) is also an   embedding based metric similar to Embedding av-5695erage . The metric computes a sentence - level em-   bedding by taking the most extreme value of the   embeddings of the words composing the sentence   for each dimension of the embedding .   Greedy matching ( Rus and Lintean , 2012 )   is another embedding based metric . The metric   does not compute a sentence - level embedding . It   directly compares the embeddings of words in the   two sentences using a greedy matching algorithm   to calculate similarity .   Metrics based on question - answering include :   FEQA ( Durmus et al . , 2020 ) employs a BERT-   based question - answering model to answer ques-   tions using source document . Questions are gener-   ated by a ﬁne - tuned BART model using generated   summaries with masked named entities as inputs .   The metric reports F1 scores against the gold an-   swer , which are often regarded as a measure of   factual consistency .   SummaQA ( Scialom et al . , 2019 ) is also a QA-   based metric . Unlike FEQA , it generates questions   from source documents instead of summaries to   be evaluated and then uses summaries to answer   them . The F1 overlap score and QA - model conﬁ-   dence are reported .   QuestEval ( Scialom et al . , 2021 ) is another a   QA - based metric . This metric can be considered   as a combination of FEQA and SummaQA . It   takes into account the scores obtained from both   styles . For comparison purposes , We use the   reference - less mode .   Metrics based on entailment classiﬁcation in-   clude :   FactCC ( Kryscinski et al . , 2020 ) is a metric   based on entailment classiﬁcation . We follow the   wayPagnoni et al . ( 2021 ) used it . Each sentence of   the summary is fed into the classiﬁer together with   the document to determine whether the facts are   consistent , and the proportion of consistent sen-   tences is used to indicate how consistent the sum-   mary is .   DAE ( Goyal and Durrett , 2020 ; Goyal and Dur-   rett,2021 ) is an entailment classiﬁcation metric   based on dependencies . We use it in a similar wayto FactCC . When a sentence can not be parsed by   the metric , we default it factually inconsistent .   3.3 Summarization Models   We select some representative models and get the   outputs of them on the test set of SAMSum . We   choose LEAD-3 and LONGEST-3 as representa-   tives of the simple extractive approaches . PGN   ( See et al . , 2017 ) and Transformer ( Vaswani et al . ,   2017 ) are selected as representatives of the earlier   neural summarization models . For generic pre-   trained generative models , we use BART ( Lewis   et al . , 2020 ) , PEGASUS ( Zhang et al . , 2020 ) and   UniLM ( Dong et al . , 2019 ) . We retrain these mod-   els above to obtain the outputs and the automatic   evaluation results are close to Gliwa et al . ( 2019 )   andWu et al . ( 2021 ) in default settings . For mod-   els speciﬁcally designed for dialogue summariza-   tion , we choose CODS ( Wu et al . , 2021 ) , Con-   voSumm ( Fabbri et al . , 2021a ) , MV - BART ( Chen   and Yang , 2020 ) , PLM - BART ( Feng et al . , 2021c ) ,   Ctrl - DiaSumm ( Liu and Chen , 2021 ) , S - BART   ( Chen and Yang , 2021 ) and the outputs are all pro-   vided by their authors . We also regard the refer-   ence summary as a kind of model output .   4 Data Annotation   4.1 Annotation Setup   Since human evaluation is expensive and time-   consuming , we decide to randomly sample 100 di-   alogues from the test set of SAMSum and evaluate   the summaries generated by all models on these di-   alogues . To comprehensively evaluate each metric   and model , we perform human evaluation in four   aspects , as in Kryscinski et al . ( 2019 ):   Coherence measures the quality of all sen-   tences in the summary as a whole . It focuses on   whether the summary is coherent and natural .   Consistency measures how well the summary   aligns with the dialogue in facts . It focuses on   whether the summary contains factual errors .   Fluency measures the quality of individual sen-   tences in the summary compared to Coherence . It   focuses on whether the sentences are well - written   and grammatically correct .   Relevance measures how well the summary   captures the key points of the dialogue . It focuses   on whether all and only the important aspects are   contained in the summary.5696To ensure the quality of the annotation , we tried   to annotate some of the data ourselves at the be-   ginning to judge the difﬁculty of the task and the   approximate time spent .   4.2 Annotation Process   We initially tried to annotate the data using crowd-   sourcing platforms . We published the annotation   task on Amazon Mechanical Turk . The in-   terface contained instructions and deﬁnitions of   the four aspects . A dialogue and a correspond-   ing summary were included in the interface , and   the summaries of different models on the same di-   alogue were presented to the annotators in a se-   quence to facilitate comparison . For each dimen-   sion / aspect , annotators were asked to rate the sum-   mary on a Likert scale from 1 to 5 . Each sum-   mary was evaluated by 5 different annotators , and   For each dimension we would receive a total of   100×14×5 = 7000 human annotations . The an-   notation was done quickly in one day , but the qual-   ity was not satisfactory . We calculated the average   score of each model in each aspect based on these   annotation data and found that the scores of the   models are close in each dimension , which is not   in the accordance with the reality . For example , in   terms of consistency , the reference summary and   the extractive approaches should have had a deﬁ-   nite advantage , but this failed to be reﬂected from   the data . The result is shown in Table 5 . For relia-   bility reasons , we do not use these annotations for   our analysis .   Then , we decided to recruit annotators from the   school forum who are required to be capable of   reading daily conversations and articles in English   ﬂuently . We recruited three annotators , using a   similar annotation interface and approach as in   the crowd - sourcing platforms . These annotators   were college students and they are ﬂuent in En-   glish . The differences with the crowd - sourcing   platform annotation are as follows : 1 ) For a stu-   dent who wanted to participate in the annotation ,   we would ask him to annotate all models on the   ﬁrst 10 conversations ( 10×14 = 140 annotations ) ,   and let her / him continue the annotation only when   these annotation results were checked by us to con-   ﬁrm that the annotator had understood the task   correctly and could ﬁnish the annotation respon-   sibly . Otherwise , we paid the annotator directly   for this part and terminated his annotation task . 2)We required each annotator to annotate all data   ( 100×14 = 1400 annotations ) to ensure the con-   sistency within the annotator . 3 ) During the anno-   tation process , we kept in touch with the annota-   tors via email or instant messaging app to answer   their questions at any time .   It took around 10 days to ﬁnish the annotation .   We received 100×14×3 = 4200 annotations   for each perspective . For each aspect of each sum-   mary , if two scores were the same and the other   was different from them , we considered the differ-   ent one as noise . For each dimension , we removed   the noise separately and calculated the the Krip-   pendorff ’s alpha coefﬁcient ( Krippendorff , 2011 ) .   We found the inter - annotator interval metric to   be within an acceptable range - from 0.5621 to   0.7564 , as detailed in Table 2 . The raw anno-   tated data will be released and we use the cleaned   data for analysis . At last , we use the average of   the cleaned data to represent the human evaluation   score of an summary on a dimension .   5 Metric Evaluation   In this section , we will introduce several deﬁni-   tions in meta - evaluation and re - evaluate the met-   rics mentioned in Section 3.2 .   5.1 Task Formulation   As mentioned by Bhandari et al . ( 2020 ) , there are   two common ways to measure the correlation of   automatic evaluation metrics to manual evaluation :   system - level and summary - level .   Assuming there are Ndialogues , the i - th dia-   logue is represented as d. For a dialogue d , there   areJsummaries generated by Jmodels , and we   denote each of them as s , j= 1 · · · J. There   areKevaluation metrics ( or human evaluation )   in total , and mrefers to an automatic evaluation   metric or human evaluation of a certain dimension .   m(s)means the score of k - th metric towards   a pair of dialogue and summary ( d , s ) . We use   R(m , m)to denote the correlation coefﬁcient be-   tween two metrics mandm .   System - level correlation is deﬁned as follows .   The corresponding p - value which indicates statis-   tical signiﬁcance can be obtained:5697Coherence Consistency Fluency Relevance   cleaned 3161 3360 3050 3439   total 4200 4200 4200 4200   Krippendorff ’s alpha 0.7564 0.6709 0.6782 0.5621   R(m , m ) = R (   [ 1   N∑m(s ) , · · · , 1   N∑m(s ) ] ,   [ 1   N∑m(s ) , · · · , 1   N∑m(s ) ] )   Summary - level correlation is deﬁned as follows ,   and the p - value can not be derived here because the   Summary - level correlation is an average value :   R(m , m ) = 1   N∑R (   [ m(s ) , · · · , m(s ) ] ,   [ m(s ) , · · · , m(s ) ] )   5.2 Discussion   Comparing the performance of various metrics re-   veals some trends in Table 3 . In each dimension ,   metrics which are strongly correlated with human   judgments exist , but few metrics show signiﬁcant   strengths in all four dimensions . Of all the met-   rics , QuestEval has the most comprehensive ca-   pabilities at the system level . Generally metrics   that perform better on coherence and ﬂuency per-   form worse on consistency and relevance , and vice   versa . This can be attributed to the deﬁnition of the   dimensions , i.e. there is some correlation between   the four dimensions themselves , which is shown   in Figure 4 . In all dimensions , automatic evalu-   ation metrics based on pre - trained language mod-   els generally outperform metrics based on n - gram   overlap and context - independent word embedding .   Among them , the recently proposed BARTScore   and the increasingly popular QA - based metrics   perform the best . This suggests that both direc-   tions have the potential to be explored in terms ofevaluation for dialogue summarization . Across di-   mensions , almost all metrics correlate better with   human judgments at the system level than at the   summary level , and both showed good agreement   with each other . This indicates that the summary-   level correlations are also worth referring to when   enough data are not available for system - level   analysis . In addition , metrics such as BLEU and   CHRF , which are frequently used in other natural   language generation tasks ( e.g. , machine transla-   tion , dialogue , etc . ) , do not show advantages on   dialogue summarization .   The characteristics presented by the automatic   evaluation metrics on the dialogue summarization   differ from those of the conventional summariza-   tion tasks . For ROUGE , we ﬁnd that increasing   the size of n in ROUGE - n is not better in almost all   dimensions , which is different from the ﬁndings   ofRankel et al . ( 2013 ) and Fabbri et al . ( 2021b ) .   The ability of ROUGE to reﬂect content selec-   tion , i.e. , relevance , as we usually believe , is also   questionable . Compared to the results of Fabbri   et al . ( 2021b ) , metrics based on n - gram overlap   such as ROUGE and CHRF perform worse on dia-   logue summarization , while some metrics that use   source documents such as BLANC perform better .   We need to focus on the limitations of ROUGE   and the role of the source dialogues in evaluating   dialogue summaries .   We have also observed some interesting phe-   nomena . Entailment classiﬁcation metrics such   as FactCC and DAE outperform many metrics   in terms of consistency , but not as well as   BARTScore and QA - based metrics . This may be   due to the large gap between the corpus used in   training and dialogues , and the need to slice the   summaries by sentence when using them . FEQA ,   which is designed for factual consistency , however ,   performs best in coherence and ﬂuency , and rather   poorly in consistency and relevance . Comparing   its performance with QuestEval and SummaQA ,   generating questions from the original dialogue   may be more reliable in measuring consistency ,   which corroborates with the points of Gabriel et al.5698Coherence Consistency Fluency Relevance   Metrics sys sum sys sum sys sum sys sum   ROUGE-1 0.590.30 0.42 0.33 0.580.27 0.40 0.30   ROUGE-2 0.47 0.26 0.41 0.32 0.43 0.22 0.41 0.30   ROUGE-3 0.39 0.22 0.39 0.30 0.33 0.17 0.40 0.30   ROUGE-4 0.33 0.20 0.37 0.27 0.27 0.14 0.38 0.28   ROUGE - L 0.570.32 0.39 0.30 0.540.27 0.37 0.27   BERTScore - p 0.570.37 0.11 0.10 0.50 0.31 0.08 0.06   BERTScore - r 0.43 0.21 0.45 0.38 0.42 0.20 0.46 0.39   BERTScore - f1 0.53 0.31 0.28 0.24 0.48 0.27 0.27 0.22   MoverScore 0.50 0.28 0.39 0.32 0.46 0.25 0.38 0.31   SMS 0.33 0.18 0.38 0.28 0.27 0.14 0.40 0.29   BARTScore - s - h + 0.09 0.08 0.620.44 0.24 0.15 0.600.42   BARTScore - h - 0.08 0.05 -0.09 -0.09 0.16 0.13 -0.18 -0.12   BARTScore - h - r 0.50 0.21 0.550.46 0.51 0.21 0.560.46   BARTScore - r - h 0.670.42 0.31 0.23 0.670.40 0.26 0.17   BLANC - help + -0.32 -0.21 0.54 0.45 -0.13 -0.08 0.600.50   BLANC - tune + -0.37 -0.23 0.50 0.38 -0.18 -0.10 0.560.43   FEQA + 0.820.27 0.32 0.16 0.840.26 0.25 0.10   QuestEval + 0.50 0.15 0.850.39 0.750.20 0.830.37   SummaQA - conf + -0.08 -0.03 0.640.39 0.03 -0.01 0.670.39   SummaQA - fscore + -0.26 -0.11 0.580.26 -0.06 -0.06 0.620.29   PPL - -0.13 -0.01 -0.49 -0.30 -0.34 -0.15 -0.43 -0.30   CHRF 0.42 0.20 0.46 0.38 0.41 0.20 0.47 0.39   BLEU-1 0.35 0.15 0.34 0.29 0.30 0.13 0.36 0.30   BLEU-2 0.31 0.16 0.35 0.29 0.25 0.12 0.37 0.30   BLEU-3 0.28 0.15 0.33 0.27 0.21 0.11 0.36 0.28   BLEU-4 0.25 0.14 0.33 0.25 0.17 0.09 0.36 0.28   METEOR 0.37 0.19 0.42 0.35 0.33 0.17 0.43 0.35   Embedding average 0.43 0.17 0.17 0.20 0.52 0.22 0.15 0.19   Vector extrema 0.47 0.22 0.35 0.28 0.43 0.21 0.35 0.26   Greedy matching 0.43 0.21 0.35 0.31 0.43 0.21 0.36 0.30   FactCC + -0.29 -0.09 0.46 0.19 -0.23 -0.09 0.49 0.19   DAE + -0.24 -0.07 0.50 0.29 -0.15 -0.02 0.540.295699Models Coherence Consistency Fluency Relevance R-1 R-2 R - L   reference summary 4.500 4.370 4.560 4.210 1.000 1.000 1.000   LONGEST-3 3.230 4.393 4.100 4.363 0.304 0.099 0.267   LEAD-3 4.370 4.093 4.200 3.843 0.309 0.092 0.296   PGN 3.568 2.103 3.657 2.293 0.356 0.126 0.357   Tranformer 3.403 1.573 3.673 1.650 0.329 0.098 0.319   BART 4.480 3.667 4.667 3.500 0.533 0.299 0.520   PEGASUS 4.590 3.730 4.640 3.417 0.508 0.254 0.476   UniLM 4.303 3.320 4.523 3.290 0.489 0.232 0.470   CODS 4.268 3.637 4.567 3.397 0.523 0.278 0.509   ConvoSumm 4.507 3.743 4.643 3.437 0.532 0.268 0.498   MV - BART 4.320 3.937 4.660 3.747 0.539 0.290 0.513   PLM - BART 4.360 3.717 4.680 3.500 0.533 0.284 0.507   Ctrl - DiaSumm 4.320 3.893 4.650 3.670 0.564 0.312 0.549   S - BART 4.227 3.307 4.520 3.337 0.497 0.244 0.472   ( 2021 ) . It is surprising that metrics based on the   language model such as PPL , BARTScore - h per-   forms poorly in measuring both coherence and ﬂu-   ency . The exact reasons for this need further inves-   tigation .   6 Model Evaluation   In each dimension , we evaluate each model men-   tioned in Section 3.3 using the average of the hu-   man evaluation scores of all summaries . Analyz-   ing Table 4 , we conclude the following .   The reference summaries in SAMSum are not   perfect , and the annotators felt that they also con-   tained some factual inconsistencies compared to   the source dialogues , as well as important ele-   ments of the dialogues that were not all captured   by them . However , comparing the human evalua-   tion scores of the reference summaries in CNNDM   ( Fabbri et al . , 2021b ) , the quality is already supe-   rior .   Extractive models produce summaries that dif-   fer in style from abstractive models , and many   conversations contain ungrammatical utterances ,   which can affect the reading experience and im-   pair their ﬂuency and coherence . In particular ,   LONGEST-3 , which extracts some potentially dis-   continuous sentences from dialogues , has low co-   herence . However , since they do not modify the   content , they still perform well in terms of con - sistency . Since the average length of dialogues   in SAMSum is small , extracting a few sentences   from it can generally include important contents ,   so the relevance is also high . The evaluation of the   extractive models raises a qustion : what kind of   summaries do readers actually want ?   The early neural summrization models repre-   sented by PGN and Transformer perform rela-   tively poorly in all dimensions compared to the   reference summaries , especially consistency and   relevance . This is to be expected because of the   high difﬁculty of dialogue summarization and the   small size of SAMSum dataset .   An important ﬁnding is that the generic pre-   trained language models represented by BART ,   PEGASUS and UniLM , and various recently pro-   posed models speciﬁcally designed on the dia-   logue summarization task do not have signiﬁcant   differences in each dimension . They are already   comparable , and in some cases better , in terms   of coherence and ﬂuency compared to the refer-   ence summaries . They have improved dramati-   cally compared to earlier neural summarization   models with respect to consistency and relevance ,   but there is still some room for enhancement . On   the one hand , this ﬁnding afﬁrms the capability of   these models ; On the other hand , it urges us to re-   ﬂect on how much these recently proposed com-   plex models or fancy techniques are an improve-   ment over the generic pre - trained language mod-5700els .   7 Conclusion   We point out the problems with the evaluation   in the dialogue summarization and introduce Di-   alSummEval , a multi - faceted dataset containing   the output of various models and the correspond-   ing human judgments . Based on this dataset , we   provide a comprehensive re - evaluation and analy-   sis of the performance of widely used automatic   evaluation metrics and each model . There are   three important ﬁndings : 1 ) Few metrics are excel-   lent in all dimensions , and the recently proposed   BARTScore and QA - based metrics are compara-   tively outstanding and worth exploring . 2 ) The   automatic evaluation metrics and their variants   present some trends that differ from conventional   summarization . 3 ) A variety of models specif-   ically designed for dialogue summarization per-   form comparably to reference summaries in terms   of coherence and ﬂuency , but still have shortcom-   ings in consistency and relevance . We hope that re-   searchers in the ﬁeld recognize the importance of   evaluation in current research , choose some other   metrics in addition to ROUGE when evaluating   models , propose automatic evaluation metrics that   can be better adapted to the ﬁeld of dialogue sum-   marization based on our work .   8 Ethical Considerations   Whether recruiting annotators through Amazon   Mechanical Turk or campus , we paid them 15 dol-   lars per hour , more than the local average mini-   mum wage . We removed all content in the dataset   that might contain personal information about the   annotators .   Acknowledgements   We thank all authors for providing the sum-   maries generated by their models . We thank   Baizhou Huang for his help in the process of   retraining some models . This work was sup-   ported by National Science Foundation of China   ( No . 62161160339 ) , National Key RD Program   of China ( No.2018YFB1005100 ) , State Key Lab-   oratory of Media Convergence Production Tech-   nology and Systems and Key Laboratory of Sci-   ence , Technology and Standard in Press Industry   ( Key Laboratory of Intelligent Press Media Tech-   nology ) . We appreciate the anonymous reviewersfor their helpful comments . Xiaojun Wan is the   corresponding author .   References570157025703   A Annotation Interface   Figure 1and Figure 2show the instructions for an-   notation and deﬁnition of each aspect . They were   read by all annotators . Figure 3shows a source   dialogue and a summary to be evaluated .   B Correlation between different   dimensions   Figure 4shows the system - level correlation be-   tween coherence , consistency , ﬂuency and rele-   vance . Consistency is strongly correlated with rel-   evance .   C Correlation between different metrics   Figure 5shows the system - level correlation be-   tween different metrics.5704D Reasons for discarding data from   Amazon Mechanical Turk   Table 5shows the result of model evaluation us-   ing annotations from Amazon Mechanical Turk .   The performance of the models is indistinguish-   able , which is not consistent with our observation .   E The evaluation results for the models   we reproduced   Table 6shows the value of ROUGE-1 , ROUGE-   2 and ROUGE - L on the test set of SAMSum for   the models we reproduced . The results is close to   those in Gliwa et al . ( 2019 ) and Wu et al . ( 2021 ) .57055706570757085709