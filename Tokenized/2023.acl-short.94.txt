  Ricardo Rei , Nuno M. Guerreiro , Marcos Treviso ,   Alon Lavie , Luisa Coheur , André F. T. MartinsUnbabel , Lisbon , Portugal , INESC - ID , Lisbon , PortugalInstituto de Telecomunicações , Lisbon , PortugalInstituto Superior Técnico , University of Lisbon , Portugal   Abstract   Neural metrics for machine translation eval-   uation , such as C , exhibit significant   improvements in their correlation with hu-   man judgments compared to traditional met-   rics based on lexical overlap , such as B.   Yet neural metrics are , to a great extent ,   “ black boxes ” that return a single sentence - level   score without transparency about the decision-   making process . In this work , we develop and   compare several neural explainability meth-   ods and demonstrate their effectiveness for   interpreting state - of - the - art fine - tuned neural   metrics . Our study reveals that these met-   rics leverage token - level information that can   be directly attributed to translation errors , as   assessed through comparison of token - level   neural saliency maps with Multidimensional   Quality Metrics ( MQM ) annotations and with   synthetically - generated critical translation er-   rors . To ease future research , we release our   code at .   1 Introduction   Reference - based neural metrics for machine trans-   lation evaluation are achieving evergrowing suc-   cess , demonstrating superior results over traditional   lexical overlap - based metrics , such as B ( Pa-   pineni et al . , 2002 ) andF(Popovi ´ c , 2015 ) ,   in terms of both their correlation with human   ratings and their robustness across diverse do-   mains ( Callison - Burch et al . , 2006 ; Smith et al . ,   2016 ; Mathur et al . , 2020 ; Kocmi et al . , 2021 ;   Freitag et al . , 2022 ) . However , lexical overlap-   based metrics remain popular for evaluating the   performance and progress of translation systems   and algorithms . Concerns regarding trust and in-   terpretability may help explain this ( Leiter et al . ,   2022 ): contrary to traditional metrics , neural met-   rics are considered “ black boxes ” as they often useFigure 1 : Illustration of our approach . In this example ,   the metric assigns the translation a low score . We aim   to better understand this sentence - level assessment by   examining the correspondence between our token - level   explanations and human annotated error spans .   increasingly large models ( e.g. , the winning metric   of the WMT 22 Metrics shared task was a 10B   parameter model ( Freitag et al . , 2022 ) ) .   While some recent work has focus on explaining   the predictions made by reference - free quality es-   timation ( QE ) systems ( Fomicheva et al . , 2021 ;   Zerva et al . , 2022 ) , explaining reference - based   metrics has remained a largely overlooked prob-   lem ( Leiter et al . , 2022 ) . It is an open question   whether the observations from studies of explain-   able QE carry over to this scenario . Thus , in this   work , we fill that gap by turning to state - of - the-   art reference - based metrics — we aim to interpret   their decision - making process by exploiting the   fact that these metrics show consistently good cor-   relations with Multidimentional Quality Metrics   ( MQM ) ( Freitag et al . , 2021 , 2022 ; Sai et al . , 2022 ) ,   which are fine - grained quality assessments that re-   sult from experts identifying error spans in transla-   tion outputs ( Lommel et al . , 2014 ) . We hypothesize   that reference - based metrics leverage this token-   level information to produce sentence - level scores .   To test this hypothesis , we assess whether our expla-   nations – measures of token - level importance ob-   tained via attribution and input attribution methods   such as attention weights and gradient scores ( Tre-   viso et al . , 2021 ; Rei et al . , 2022b ) – align with1089human - annotated spans ( Fomicheva et al . , 2021 ,   2022 ; Zerva et al . , 2022 ) , as illustrated in Figure 1 .   Our analysis focuses on two main vectors : ( i ) un-   derstanding the impact of the reference information   on the quality of the explanations ; and ( ii ) finding   whether the explanations can help to identify po-   tential weaknesses in the metrics . Our main contri-   butions are :   •We provide a comparison between multiple ex-   plainability methods for different metrics on all   types of evaluation : src - only , ref - only , and   src+ref joint evaluation .   •We find that explanations are related to the un-   derlying metric architecture , and that leveraging   reference information improves the explanations .   •We show that explanations for critical translation   errors can reveal weaknesses in the metrics .   2 Explaining Neural Metrics   We aim to explain sentence - level quality assess-   ments of reference - based metrics by producing   token - level explanations that align to translation   errors . In what follows , we describe the metrics   and how we produce the explanations that we study .   2.1 Metrics   We focus our analysis on two state - of - the - art   neural metrics : C ( Rei et al . , 2020 ) and   UTE(Wan et al . , 2022).While both metrics   use a multilingual encoder model based on XLM-   R ( Conneau et al . , 2020 ) , they employ distinct   strategies to obtain sentence - level quality scores .   On the one hand , C separately encodes the   source , translation and reference to obtain their re-   spective sentence embeddings ; these embeddings   are then combined to compute a quality score . On   the other , UTEjointly encodes the sentences   to compute a contextualized representation that is   subsequently used to compute the quality score .   Interestingly , UTEis trained to obtain qual-   ity scores for different input combinations : [ mt ;   src ] ( S),[mt ; ref ] ( R ) , and [ mt ; src ;   ref ] ( S+R ) . In fact , when the input is S ,   UTEworks like TransQuest ( Ranasinghe et al . ,   2020 ) ; R , like B ( Sellam et al . , 2020 ) ;   andS+R , like RB ( Wan et al . , 2021).2.2 Explanations via Attribution Methods   In this work , we produce explanations using attri-   bution methods that assign a scalar value to each   translation token ( i.e. a token - level attribution ) to   represent its importance . While many input attribu-   tion methods exist and have been extensively stud-   ied in the literature ( Ribeiro et al . , 2016 ; Shrikumar   et al . , 2017 ; Sundararajan et al . , 2017 ; Jain and   Wallace , 2019 ; Atanasova et al . , 2020 ; Zaman and   Belinkov , 2022 ) , we focus specifically on those   that have been demonstrated to be effective for   explaining the predictions of QE models ( Treviso   et al . , 2021 ; Fomicheva et al . , 2022 ; Fernandes   et al . , 2022 ; Zerva et al . , 2022 ) and extend them to   our reference - based scenario . Concretely , we use   the following techniques to extract explanations :   •embed – align : the maximum cosine similar-   ity between each translation token embedding   and the reference and/or source token embed-   dings ( Tao et al . , 2022 ) ;   •grad ℓ : theℓ-norm of gradients with respect   to the word embeddings of the translation to-   kens ( Arras et al . , 2019 ) ;   •attention : the attention weights of the transla-   tion tokens for each attention head of the en-   coder ( Treviso et al . , 2021 ) ;   •attn×grad : the attention weights of each head   scaled by the ℓ-norm of the gradients of the   value vectors of that head ( Rei et al . , 2022b ) .   3 Experimental Setting   MQM annotations . We use MQM annotations   from the WMT 2021 Metrics shared task ( Fre-   itag et al . , 2021),covering three language pairs   — English - German ( en →de ) , English - Russian   ( en→ru ) , and Chinese - English ( zh →en ) — in two   different domains : News and TED Talks . For each   incorrect translation , human experts marked the   corresponding error spans . In our framework , these   error spans should align with the words that the   attribution methods assign higher importance to.1090   Models . ForC , we use the latest publicly   available model : wmt22 - comet - da ( Rei et al . ,   2022a).ForUTE , we train our own model   using the same data used to train C in or-   der to have a comparable setup . We provide full   details ( training data , correlations with human an-   notations , and hyperparameters ) in Appendix A.   Overall , the resulting reference - based UTEmod-   els ( RandS+R ) are on par with C .   Evaluation . We want our explanations to be di-   rectly attributed to the annotated error spans , in the   style of an error detection task . Thus , we report   Area Under Curve ( AUC ) and Recall@K.These   metrics have been used as the main metrics in pre-   vious works on explainable QE ( Fomicheva et al . ,   2021 , 2022 ; Zerva et al . , 2022 ) .   4 Results   4.1 High - level analysis   Explanations are tightly related to the under-   lying metric architecture . The results in Ta - ble 1 show that the predictive power of the at-   tribution methods differ between UTEand   C : attn×grad is the best method for UTE-   based models , while embed – align works best for   C .This is expected as UTEconstructs   a joint representation for the input sentences , thus   allowing attention to flow across them ; C ,   in contrast , encodes the sentences separately , so it   relies heavily on the separate contextualized embed-   dings that are subsequently combined via element-   wise operations such as multiplication and abso-   lute difference . Interestingly , embed – align and   attn×grad were the winning explainability ap-   proaches of the WMT 2022 Shared - Task on Quality   Estimation ( Zerva et al . , 2022 ) . This suggests that   explainability methods developed for QE systems   can translate well to reference - based metrics . We   provide examples of explanations in Appendix C.   Reference information boosts explainability   power . Table 1 also shows that , across all met-   rics , using reference information brings substantial   improvements over using only the source informa-   tion . Moreover , while reference - based attributions   significantly outperform source - based attributions ,   combining the source and reference information to1091   obtain token - level attributions does not consistently   yield superior results over using the reference alone .   Notably , the best attribution method for C   does not require any source information . This is   interesting : in some cases , reference - based met-   rics may largely ignore source information , relying   heavily on the reference instead .   4.2 How do the explanations fare for critical   translation errors ?   The MQM data analyzed until now consists primar-   ily of high quality translations , with the majority of   annotated errors being non - critical . However , it is   important to assess whether our explanations can   be accurately attributed to critical errors , as this   may reveal potential metric shortcomings . To this   end , we employ SMAUG ( Alves et al . , 2022 ) , a   tool designed to generate synthetic data for stress-   testing metrics , to create corrupted translations that   contain critical errors . Concretely , we generate   translations with the following pathologies : nega-   tion errors , hallucinations via insertions , named   entity errors , and errors in numbers .   Explanations identify critical errors more eas-   ily than non - critical errors . Figure 2 shows   that explanations are more effective in identify-   ing critical errors compared to other non - critical   errors ( see Table 1 ) . Specifically , we find sig-   nificant performance improvements up to nearly   30 % in Recall@K for certain critical errors . Over-   all , hallucinations are the easiest errors to identify   across all neural metrics . This suggests that neuralmetrics appropriately identify and penalize halluci-   nated translations , which aligns with the findings   of Guerreiro et al . ( 2022 ) . Moreover , explanations   for both UTEmodels behave similarly for all er-   rors except numbers , where the source information   plays a key role in improving the explanations . No-   tably , contrary to what we observed for data with   non - critical errors , C explanations are less   effective than those of U RandUTE   S+Rfor identifying critical errors .   Explanations can reveal potential metric weak-   nesses . Figure 2 suggests that C explana-   tions struggle to identify localized errors ( nega-   tion errors , named entity errors and discrepancies   in numbers ) . We hypothesize that this behavior   is related to the underlying architecture . Unlike   UTE - based metrics , C does not rely on   soft alignments via attention between the sentences   in the encoding process . This process may be key   to identify local misalignments during the encod-   ing process . In fact , the attention - based attributions   forUTEmetrics can more easily identify these   errors . C , however , encodes the sentences   separately , which may result in grammatical fea-   tures ( e.g. numbers ) being encoded similarly across   sentences ( Chi et al . , 2020 ; Chang et al . , 2022 ) . As   such , explanations obtained via embedding align-   ments will not properly identify these misalign-   ments on similar features . Importantly , these find-   ings align with observations made in ( Amrhein and   Sennrich , 2022 ; Raunak et al . , 2022 ) . This show-   cases how explanations can be used to diagnose   and reveal shortcomings of neural - based metrics .   5 Conclusions and Future Work   In this paper , we investigated the use of explain-   ability methods to better understand widely used   neural metrics for machine translation evaluation ,   such as C andUTE . Concretely , we an-   alyzed how explanations are impacted by the ref-   erence information , and how they can be used to   reveal weaknesses of these metrics . Our analysis   shows that the quality of the explanations is tightly   related to the underlying metric architecture . In-   terestingly , we also provide evidence that neural   metrics like C may heavily rely on reference   information over source information . Additionally ,   we show that explanations can be used to reveal   reference - based metrics weaknesses such as fail-   ing to severely penalize localized critical errors .   This opens up promising opportunities for future1092research on leveraging explanations to diagnose   reference - based metrics errors . To support these   studies , we call for future datasets illustrating crit-   ical errors ( e.g. , challenge sets ( Karpinska et al . ,   2022 ) ) to be accompanied by annotated error spans .   Limitations   We highlight three main limitations of our work .   First , although we have explored gradient - based   explanations that take the whole network into con-   sideration and have been shown to be faithful in   previous work ( Bastings et al . , 2021 ) , we do not ex-   plicitly explore how C combines the sentence   representations in the feed - forward that precedes   the encoder model to produce the sentence - level   score .   Second , we have shown that combining atten-   tion with gradient information results in the best   explanations for UTE - based metrics . However ,   from a practical standpoint , running inference and   extracting the explainability scores simultaneously   may be more computationally expensive than other   techniques : gradient - based metrics benefit from   GPU infrastructure and require storing all gradient   information .   Third , we have not explored extracting expla-   nations in low - resource settings . That is because   high - quality MQM annotations for such language   pairs are not yet available . Nevertheless , further   research in those settings is needed to access the   broader validity of our claims .   Acknowledgements   This work was partially supported by the P2020   programs ( MAIA , contract 045909 ) , the Por-   tuguese Recovery and Resilience Plan ( PRR )   through project C645008882 - 00000055 , Center for   Responsible AI , by the European Research Council   ( ERC StG DeepSPIN , 758969 ) , by EU ’s Horizon   Europe Research and Innovation Actions ( UTTER ,   contract 101070631 ) , and by the Fundação para a   Ciência e Tecnologia ( contracts UIDB/50021/2020   and UIDB/50008/2020 ) .   References109310941095A Model Details   In Section 2.1 , we employed the latest publicly   available model ( wmt22 - comet - da ) for C ,   which emerged as a top - performing metric in the   WMT 2022 Metrics task ( Freitag et al . , 2022 ) . To   ensure a comparable setting for UTE(Wan et al . ,   2022 ) , we trained our own model . In doing so , we   utilized the same data employed in the develop-   ment of the C model by ( Rei et al . , 2022a ) ,   without pretraining any synthetic data , as origi-   nally suggested . Additionally , our implementation   did not incorporate monotonic regional attention ,   as our preliminary experiments revealed no dis-   cernible benefits from its usage . The hyperparame-   ters used are summarized in Table 3 , while Table 4   presents the number of Direct Assessments utilized   during training . Furthermore , Table 5 displays the   segment - level correlations with WMT 2021 MQM   data for the News and TED domains .   Regarding computational infrastructure , a single   NVIDIA A10 G GPU with 23 GB memory was used .   The resulting UTEmodel has 565 M parameters   while C has 581 M parameters .   A.1 Output Distribution   To better understand the output of the models and   what scores are deemed low , we plotted the output   distributions for the two models we used in our   study . The average score for English →German   data is 0.856for the C model and 0.870for   theUTEmodel we trained . From Figure 3 we   can observe the distribution of scores . This means   that the 0.6692 score from the example in Figure 1   corresponds to a low quality output ( 5th percentile ) .   A.2 SMAUG Corpus   As we have seen in Section 4.2 , we have cre-   ated synthetic translation errors for the following   pathologies : negation errors , hallucinations via in-   sertions , named entity errors , and errors in numbers .   Table 7 presents a summary of the examples created   using SMAUG and in Table 8 we show examples   of each error category .   B Comparison between C and   X - Alignments   From Table 1 , it is evident that the alignments be-   tween the reference and/or source and the transla-   tion yield effective explanations for C . This   raises the question of how these alignments com-   pare to the underlying encoder of C beforethe fine - tuning process with human annotations . To   investigate this , we examine the results for XLM - R   without any fine - tuning , as presented in Table 2 .   Overall , the explanations derived from the align-   ments of C prove to be more predictive of   error spans than those obtained from XLM - R align-   ments . This suggests that during the fine - tuning   phase , COMET models modify the underlying   XLM - R representations to achieve better alignment   with translation errors .   C Examples   In Tables 9 and 10 , we show examples of   C explanations for Chinese →English and   English →German language pairs , respectively . We   highlight in gray the corresponding MQM annota-   tion performed by an expert linguist and we sort the   examples from highest to lowest C scores .   From these examples we can observe the following :   •Highlights provided by C explanations   have a high recall with human annotations . In   all examples , subword tokens corresponding to   translation errors are highlighted in red but we   often see that not everything is incorrect .   •Explanations are consistent with scores . For ex-   ample , in the third example from Table 10 , the   red highlights do not correspond to errors and in   fact the translation only has a major error griffen .   Nonetheless , the score assigned by C is a   low score of 0.68which is faithful to the explana-   tions that was given even if the assessment does   not agree with human experts.10961097109810991100110111021103ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Yes . Section 6   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Assistance purely with the language of the paper along every section . Grammarly and DeepL write   B / squareDid you use or create scientiﬁc artifacts ?   Section 3 explains the methods we used . We will release the adaptations required to use the explainability   methods over COMET framework , the UniTE model we trained , and all data synthetically - generated data .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 2   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   footnote on the ﬁrst page . The License will be Apache 2.0   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   in the Appendix we have several statistics for training and testing data .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Left blank .   C / squareDid you run computational experiments ?   Appendix provides detailed information about the trained model .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix provides detailed information about the trained model including GPU infrastructure and   total number of parameters.1104 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Not applicable . Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Appendix has all information needed about test data and performance of the models .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 2 and Appendix   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.1105