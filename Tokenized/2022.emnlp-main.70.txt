  Jitkapat Sawatphol , Nonthakit Chaiwong ,   Can Udomcharoenchaikit , and Sarana Nutanong .   School of Information Science and Technology ,   Vidyasirimedhi Institute of Science and Technology , Thailand   { jitkapat.s_s20,nonthakitc_pro,canu_pro,snutanon}@vistec.ac.th   Abstract   Authorship attribution is a task that aims to   identify the author of a given piece of writing .   We aim to develop a generalized solution that   can handle a large number of texts from au-   thors and topics unavailable in training data .   Previous studies have proposed strategies to   address only either unseen authors or unseen   topics . Authorship representation learning has   been shown to work in open - set environments   with a large number of unseen authors but has   not been explicitly designed for cross - topic en-   vironments at the same time . To handle a large   number of unseen authors and topics , we pro-   pose Authorship Representation Regularization   ( ARR ) , a distillation framework that creates au-   thorship representation with reduced reliance   on topic - specific information . To assess the per-   formance of our framework , we also propose   a cross - topic - open - set evaluation method . Our   proposed method has improved performances   in the cross - topic - open set setup over baselines   in 4 out of 6 cases .   1 Introduction   Authorship attribution is a task that aims to identify   the authors of anonymous texts . Applications of   this task include academic and forensic ones , such   as finding the authors of literary works , historical   writings ( Koppel and Seidman , 2013 ; Juola , 2013 ;   Stover et al . , 2016 ) or threatening online messages   ( Abbasi and Chen , 2005 ; Lambers and Veenman ,   2009 ; Coulthard , 2012 ) .   Solution Design Factors . Three factors affect   our solution design . First , our technique should   be able to handle a large number of authors due   to the endless number of candidate authors in the   real world . Second , we want our technique to allow   style comparison of texts written by unseen authors   so that we do nothave to adjust the model every   time a new author is introduced . Third , our tech-   nique should be effective with out - of - distribution   topics ( Mikros and Argiri , 2007 ) since it is im - practical to assume that the training data covers all   possible topics during runtime .   Existing Techniques . Prior research efforts on   authorship attribution have focused on solving ei-   ther out - of - distribution in topics or authors . For   out - of - topic , methods such as text distortion ( Sta-   matatos , 2017 ) , multi - task learning ( Song et al . ,   2019 ) , and data augmentation ( Rivera - Soto et al . ,   2021 ) have been used in conjunction with classifi-   cation algorithms to reduce topic bias and improve   performance on unseen topic texts . For out - of-   author , Hay et al . ( 2020 ) and Rivera - Soto et al .   ( 2021 ) have used representation learning to handle   thousands of unseen authors . Such methods aim to   convert texts into fixed - length embeddings . This   paradigm allows the comparison of unseen author   texts without pre - defining a fixed number of au-   thor classes at training time . Yet , to the best of our   knowledge , no study has proposed a representation   learning method that is explicitly designed to deal   with out - of - topic and out - of - author simultaneously .   Proposed Research . In this paper , we propose   Authorship Representation Regularization ( ARR ) .   Our objective is to enhance the cross - topic capabil-   ity of authorship representation models that can   handle a large number of unseen authors . The   principle of our method lies in the self - distillation   framework that reduces the authorship representa-   tion ’s reliance on topic - specific information . Our   experimental results reveal improvements in large-   scale cross - topic - open - set authorship attribution   over existing representation learning baselines in   4 out of 6 cases , as well as demonstrated minimal   performance tradeoff in in - distribution - topic setup .   Contributions . Our work has the following con-   tributions :   ( i)We propose Authorship Representation Reg-   ularization ( ARR ) , a framework that can be   applied to enhance cross - topic performances   of any existing authorship representation en-   coders with any model architecture.1076(ii)We introduce an evaluation method to assess   the performance of cross - topic - open - set au-   thorship attribution methods .   ( iii ) Our proposed framework achieves improved   performances in cross - topic - open - set setup   over baselines in 4 out of 6 cases .   2 Proposed Method   Authorship representation learning has shown to be   effective for large - scale open - set authorship attri-   bution ( Hay et al . , 2020 ; Rivera - Soto et al . , 2021 ) .   However , these approaches have not been explicitly   designed to help with generalization toward unseen   topics . We hypothesize that we can improve gen-   eralization by reducing the topic information of   an authorship representation . Therefore , we pro-   pose a solution based on the concept of supervised   contrastive learning ( Khosla et al . , 2020 ) and con-   fidence regularization ( Utama et al . , 2020 ) . Our   framework can be applied to remove bias from any   text encoder model regardless of the architecture .   We propose Authorship Representation Regu-   larization ( ARR ) , a framework to obtain topic-   regularized authorship representation , i.e. , an em-   bedding that can be used to compare writing style   similarity with minimum topic influence .   Our pipeline consists of three steps . Step A :   Base Model Construction . Construct a base   model Gfor authorship representation . Step B :   Topic Regularization . Create a bias model H   and re - scale the base model ’s output to reduce   topic dependency . Step C : Distillation . Transfer   knowledge into target model Fto create a topic-   regularized embedding space . As shown in Fig-   ure 1 , these training steps are described as follows .   Step A : Base Model Construction . First , wetrain a base encoder Gon an authorship represen-   tation learning objective . We then freeze all the   parameters of the base model .   At target encoder training time , we sample a   minibatch represented by a set of texts N. We cal-   culate the probability score pfrom the cosine   similarity score of each pair ( i , j)∈N×Nto use   in the next step .   We define N×N={(i , j ) : i∈N∧j∈N } .   For each ( i , j ) , we compute cosine similarity of   encoded representation of text iand another text   jfrom encoder G. We denote the L2 normalized   representation of text icomputed from Gasg   and denote variable τas the temperature scaling   hyperparameter .   p = exp(g·g)/τ   /summationtextexp(g·g)/τ(1 )   We also use Eq . 1 to derive probability score   pfrom text pairs encoded by topic bias model   Handpfrom target model F. We only calcu-   late scores for text pairs where i̸=j .   Step B : Topic Regularization . We perform   topic regularization using a bias model Hthat is   designed to encode topic similarity . We use TF - IDF   as a proxy for a topic bias model .   We denote ( a , b)∈M⊂N×N. where M   only includes text pairs ( a , b)with the same author .   Afterward , we compute the probability score p   derived from the similarity score of the vector rep-   resentations of text pairs ( a , b ) , encoded by bias   model H. Then , we aggregate pinto a single   value B. For each minibatch , Brepresents the de-   gree of topic bias for every same - author pair in the   minibatch .   B=1   |M|/summationdisplayp(2 )   After obtaining B , we apply a scaling function S   topandBto obtain a topic - regularized proba-   bility score S(p , B ) .   S(p , B ) = p / summationtextp(3 )   Step C : Distillation . Finally , we train the tar-   get model with the same model architecture and   pre - trained weights as the base model . We mini-   mize the loss function calculated from each text   pair(i , j ) . The loss function is the cross - entropy1077between the base model ’s topic - regularized proba-   bility score and the target model ’s probability score .   The final loss value is computed from a mean of   Lfor some i and j where i̸=j .   L = S(p , B)·log(p ) ( 4 )   At inference time , the target model will be a   single encoder that can produce a representation   similar to the base model representation with topic   regularization applied .   3 Evaluation Method   As stated in Section 1 , we want our solution to   handle a large number of classes as well as deal   with texts from both unseen authors and topics .   This section describes the strategies to assess our   method as follows .   Dataset . To assess the capability to handle a   large number of authors , we choose three datasets   that contain thousands of authors from three het-   erogeneous genres : Amazon reviews ( Ni et al . ,   2019 ) , Reddit ( Baumgartner et al . , 2020 ) , and Fan-   fiction ( Bevendorff et al . , 2020 , 2021 ) .   Train - validation - test split . To measure the ca-   pability to handle unseen authors and topics , we   propose a train - test split scheme to create a cross-   topic open - set environment for authorship attribu-   tion , as illustrated in Figure 2 . This scheme can be   used with any data labeled with author and topics .   The number of samples , authors , and topics of the   datasets we used in our experiments are described   in Table 1 .   Training data . First , we split the training portion   from the original dataset by randomly selecting   samples from the authors and topics that have the   most samples . For each dataset , we use manually   selected thresholds to determine the candidates for   training data , which is elaborated in Appendix A.2 .   Cross - topic test data . This test set aims to de-   scribe the effectiveness of feature representations   in a setup where observed topical and authorship   information might have minimal benefits . There-   fore , we sample the cross - topic - open - set test data   so that the test author and topic set do not overlap   with the training set .   In - distribution - topic test data . Additionally , we   want to assess our method ’s performance in an in-   distribution environment . Therefore , we sample   another in - distribution - topic test data to measure   performance . In this scenario , the author set in test   data does not overlap with the training data .   Validation data . We also randomly sample the   training data into a smaller subset to use as val-   idation data to tune hyperparameters during the   training process . The size of the validation set is   randomly selected to be the same as the cross - topic   test set .   Comparative Studies . We compare our method   against existing authorship representation tech-   niques using the described train - validation - test   split . For each model and hyperparameter setting ,   we train on three different random seeds . For each   seed , we validate the model to pick the best hy-   perparameter , then evaluate with each of the two   described test data . For each model , we report the   mean score of the three seeds in Section 4.1078Competitive Methods . Transformer - based   ( Vaswani et al . , 2017 ) models has shown high   performance in large - scale authorship attribution   with unseen authors ( Rivera - Soto et al . , 2021 ) .   Therefore , we compare our method with two mod-   els based on transformers : Multiclass log loss   ( MLL ) ( Hay et al . , 2020 ) and Contrastive loss   ( CL ) ( Rivera - Soto et al . , 2021 ; Khosla et al . , 2020 ) .   We use pre - trained sBERT ( Reimers and Gurevych ,   2019)as the base encoder . Then , we apply mean   pooling to the hidden vectors from the last encoder   layer . Finally , we fine - tune the encoder with one of   the two loss functions . We also include zero - shot   results from the sBERT model without fine - tuning .   Additionally , we also include two simple statis-   tical representation : Bag of words ( BOW ) and   Term frequency - inverse document frequency   ( TF - IDF ) .   Evaluation Measures . We use evaluation process   and metrics with respect to that of Rivera - Soto et al .   ( 2021 ) . At testing time , we further divide the test   data into two subsets . Firstly , we pick 50 % of   each author ’s texts and use them as a query set .   We use the rest of the test samples as a target set .   Additionally , we also add texts from authors with   only a single sample into the target set to serve as   distractors . For each query in the query set , we   perform a nearest neighbor search using cosine   similarity on the encoded representation of each   query and text in the target set . We use recall@8   ( R@8 ) and mean reciprocal rank ( MRR ) as the   performance metrics in our experiments .   4 Experimental Results   We conducted experimental studies according to   the evaluation method described in Section 3 . Ta-   bles 2 and 3 show results from the cross - topic and   in - distribution - topic studies , respectively .   Cross - topic . Table 2 shows that ARR provides   improvements in 4 out of 6 cases , i.e. , 3 out of 3   for MLL and 1 out of 3 for CL . The performances   of both MLL and CL baselines for the Amazon   dataset are improved by 1.9 % for R@8 and 2.25 %   for MRR on average . Also , in Reddit and Fanfic-   tion dataset , there are improvements in the MLL   baseline at an average of 6.95 % for R@8 and 8.7 %   for MRR . However , we have also observed perfor-   mance penalties for CL baseline at 1.4 % for R@8   and 1 % for MRR with our method applied .   In - distribution topic . Table 3 shows that ARR   reveals performance penalties in in - distribution   topic setup . That is , for the MLL model , our   method reveals an average of 1.2 % penalty in both   R@8 and MRR compared to base models in Ama-   zon and Reddit datasets . Additionally , our method   applied to CL models reveals 0.25 % penalty in   R@8 and 1.2 % in MRR for both datasets .   Discussion . Results from the cross - topic study   reveal that ARR is effective in 4 out of 6 cases .   Such results show the effectiveness of our method   in reducing the influence of topical information .   However , the method also reveals performance   penalties in scenarios where topic shortcut seems   beneficial , as shown in in - distribution topic experi-   ments . This result is expected since our method re-   duces the usage of topical information in a text rep-   resentation . Furthermore , we have also observed   performance penalties in some cases from cross-   topic experiments ( Reddit and Amazon ) . We hy-   pothesize that the resemblance between these ex-   periments is caused by topic information leakage .   Topic information leakage . It is important to   note that the Reddit and Fanfiction datasets have   more topics than the Amazon dataset , i.e. , 4,8491079   and 1,200 topics in comparison to 4 topics , respec-   tively . Since we randomly split these topics into   training , validation , and test sets , Reddit and Fan-   fiction are more prone to topic information leak-   age than Amazon . To illustrate , in the Fanfiction   dataset , the topic of “ Captain America ” can be in   the training set while “ Doctor Stange ” can be in   the test data . For the Reddit dataset , the topic of   “ literature ” and “ poetryreading ” are similar but our   split method does notprevent them from being as-   signed to training and test data separately . Table 4   shows examples of texts from the overlapping top-   ics in Reddit and Fanfiction datasets . Since these   topics have overlapping information , learning a   topic shortcut from the former can still benefit the   latter . These leaked topics share the same named   entities and concepts that diminishes the “ unseen   topics ” aspect of the cross - topic test sets . Together ,   these observations suggest that it might be bene-   ficial for future cross - topic experiments to use a   train - validation - test split that considers the similar-   ity between topics to prevent information leakage.5 Conclusion   In conclusion , we propose authorship attribution   solutions that can handle large amount of unseen   authors and topics .   Firstly , we present Authorship Representation   Regularization , a self - distillation framework that   helps authorship representation to generalize to-   ward unseen topics and authors at scale .   Secondly , we propose studies in authorship at-   tribution with a cross - topic - open - set environment   to assess our method . Our experimental results   show that our framework can improve recall@8   and MRR over baselines in 4 out of 6 cases in   cross - topic environments . However , our method ’s   effectiveness is diminished in the in - distribution   topic ( or topic leaked ) scenarios where models can   still use topic - related features to help discriminate   the text ’s writing styles .   In future works , it is interesting to investigate the   cross - topic data split that can prevent the topic in-   formation leakage issue . Such investigation should   help create a more challenging evaluation method   for cross - topic authorship attribution , as well as   help create an authorship attribution method that is   robust toward various real - world applications .   Limitations   In this section , we describe the limitation of our   studies in the terms of topic information leakage   and dataset properties .   First , our data split uses the topic label acquired   from each text ’s labeled category . However , such   " topics " are not guaranteed to be distinct from each   other . Therefore , there seems to be a topic informa-   tion leakage in Reddit and Fanfiction datasets , as   described in the discussion in Section 4 .   Moreover , the datasets used in our experiments   are obtained only from online texts written in En-   glish language . To the best of our knowledge , these   datasets are the only sufficiently large data sources .   A large size ensures that after applying our data   split , the dataset still has a sufficiently large num-   ber of samples with diverse authors and topics . As   a result of our limited selection of datasets , our   findings might not apply to texts in other domains ,   such as historical or forensic writings . Furthermore ,   our proposed method has experimented only on En-   glish language texts , and its finding might not apply   to languages with different morphosyntactic prop-   erties . For example , it is possible that our proxy for   topic bias model ( TF - IDF ) might not be as effec-1080tive on text in languages with grammatical genders ,   which have more morphological variations .   Acknowledgement   This study is partially supported by the Digital   Economy Promotion Agency Thailand .   References1081   A Appendix   A.1 Reproducibility .   The source code and configurations used to   reproduce our experiments are available at   https://www.github.com/jitkapat/TopicReg   A.2 Additional dataset information   Topic label acquisition . To allow cross - topic data   split , the topics of each text must be labeled . We   use the metadata that are available for each text as   topics . For the Amazon dataset , we use product   review categories as topics . For the Reddit dataset ,   we consider texts from different subreddit ( sub-   forums with specific interests ) as different topics .   For Fanfiction , we use the fandom label ( fandom   describes the original story that each fan - written   fiction is based on , e.g. , Harry Potter ) as topics .   Train - validation - test - split parameters . For   Amazon and Reddit , we use the train - test split de-   scribed in Section 3 . We use hand - picked percent-   age threshold values of authors and topics with the   most samples as training candidates . We use 10 %   author threshold and 20 % topic threshold for both   Amazon and Reddit . We pick 80 % of the training   candidates in both datasets as the training data . The   rest of each dataset is then sampled into validation   data and test data as described in 3 . Finally , we   downsample the Reddit dataset into 10 % to get   a similar dataset size and training time to other   datasets .   However , for Fanfiction , we use the data split   introduced in PAN2021 authorship verification   ( Bevendorff et al . , 2021 ) , which does not include   an in - distribution - topic but unseen author test data .   We also use the test data from PAN2020 ( Beven-   dorff et al . , 2020 ) as the validation data for our   Fanfiction experiments .   Text anonymity . Since all texts in every dataset   we use have been collected from publicly accessi-   ble websites , we did not additionally anonymize   any mention of people or organizations . A.3 Computation details   Computing Infrastructures . We use a single   Tesla A100 GPU on a single machine to train each   model in all of our experiments .   Model Parameters . All deep learning baselines   ( CL and MLL ) use the same pre - trained sBERT   encoder , which has 82.1 million parameters . Al-   though ARR training includes both base model and   target model , only 82.1 million parameters of the   target model are updated .   Run time . The average training time for each   model in our experiments is approximately 6 hours .   In total , we have trained 117 models ( including   model variations in learning objectives , hyperpa-   rameters , and random seed . ) with a total training   time of approximately 700 hours . At testing time ,   it took an average of 20 minutes to perform infer-   ence and evaluation on the whole test set of at most   about 207,000 samples .   A.4 Hyperparameters   In our experiments , we search for the best hyperpa-   rameters for each base model and ARR - enhanced   model using manual search . We choose the best   model based on recall@8 evaluated on a validation   set . We vary the following values as hyperparame-   ters and random seeds .   1 . learning rate = [ 1e−3,1e−4,1e−5 ]   2 . temperature = [ 0.5 , 0.1 , 0.05 , 0.01 ]   3 . random seed = [ 0 , 43 , 314 ]   Additionally , we set the batch size to 64 for all   models . Also , we set the number of epochs to 3   for both MLL and CL baselines and the number of   epochs to 1 for additional ARR training , to avoid   over - fitting.1082