  Xiaochuang HanDániel SimigTodor Mihaylov   Yulia TsvetkovAsli CelikyilmazTianlu WangMeta AIUniversity of Washington   Abstract   In - context learning ( ICL ) improves language   models ’ performance on a variety of NLP tasks   by simply demonstrating a handful of examples   at inference time . It is not well understood why   ICL ability emerges , as the model has never   been specifically trained on such demonstra-   tions . Unlike prior work that explores implicit   mechanisms behind ICL , we study ICL via in-   vestigating the pretraining data . Specifically ,   we first adapt an iterative , gradient - based ap-   proach to find a small subset of pretraining data   thatsupports ICL . We observe that a continued   pretraining on this small subset significantly im-   proves the model ’s ICL ability , by up to 18 % .   We then compare the supportive subset con-   strastively with random subsets of pretraining   data and discover : ( 1 ) The supportive pretrain-   ing data to ICL do nothave a higher domain rel-   evance to downstream tasks . ( 2 ) The supportive   pretraining data have a higher mass of rarely   occurring , long - tail tokens . ( 3 ) The support-   i ve pretraining data are challenging examples   where the information gain from long - range   context is below average , indicating learning to   incorporate difficult long - range context encour-   ages ICL . Our work takes a first step towards   understanding ICL via analyzing instance - level   pretraining data . Our insights have a potential   to enhance the ICL ability of language models   by actively guiding the construction of pretrain-   ing data in the future .   1 Introduction   In - context learning in NLP has drawn tremendous   attention recently ( Dong et al . , 2022 ) . Unlike tra-   ditional learning paradigms that rely on training   or finetuning models , in - context learning only pro-   vides a handful of demonstration examples to lan-   guage models as a prefix to the test input , without   any parameter updates . In - context learning has   shown superior performance on a range of NLP   tasks ( Brown et al . , 2020 ; Zhang et al . , 2022b;Figure 1 : An example from the pretraining data of OPT   ( Zhang et al . , 2022b ) and an illustrative in - context learn-   ing example of topic classification . The in - context learn-   ing task data can be drastically different from pretraining   instances , both in content and format .   Chowdhery et al . , 2022 ; Hoffmann et al . , 2022 ) ,   but the origin and reason of this emergent ability   remain under - investigated . In - context learning is   surprising since language models have not been   explicitly trained to learn from demonstration ex-   amples ( Xie et al . , 2022 ) . As shown in an illus-   trative scenario in Figure 1 , a typical pretraining   data instance is highly different from an in - context   learning example for downstream tasks , in both   content and format .   Prior work have attempted to answer what in-   context learning is , through empirically investigat-   ing useful and irrelevant attributes of the demon-   stration examples ( Min et al . , 2022 ; Zhang et al . ,   2022a ) , or theoretically proving certain synthetic   language models implicitly do Bayesian inference   with demonstrations ( Xie et al . , 2022 ) . Further-   more , recent work have drawn connections between   the mechanism of in - context learning and standard   learning algorithms , such as regression , nearest   neighbor , and gradient descent ( Olsson et al . , 2022 ;   Akyürek et al . , 2022 ; Dai et al . , 2022 ; von Oswald   et al . , 2022 ) .   Differently , in this work we are interested in   understanding from where the in - context learning   ability is acquired , through a perspective of pre-12660training data . Although not many , some recent   work have investigated this direction . For instance ,   Shin et al . ( 2022 ) pretrain a variety of language   models on different corpora . They study corre-   lations between attributes of pretraining datasets   and in - context learning performance , at a relatively   coarse dataset - level . Chan et al . ( 2022 ) construct   pretraining data with different attributes and dis-   cover that some distributional properties of the data   drive the emergence of in - context learning . How-   ever , their experiment is limited to synthetic data   of image - label pairs .   In this work , we investigate a large language   model OPT ( Zhang et al . , 2022b ) and its pretrain-   ing data . We first hypothesize that there exists some   specific pretraining data instances that are partic-   ularly helpful to the model ’s in - context learning   ability . As an attempt to find such instances , we   adapt an iterative , gradient - based method ORCA   ( Han and Tsvetkov , 2022 ) to search within OPT ’s   pretraining corpus . The process is guided by the   gradients of the in - context learning data from down-   stream tasks , and we refer to the identified subset   as supportive pretraining data to in - context learn-   ing following Han and Tsvetkov ( 2022 ) . Further-   more , we quantitatively verify through a pertur-   bative continued pretraining , that the supportive   subset does improve the model ’s in - context learn-   ing performance on downstream tasks , while not   affecting a spurious zero - shot performance ( § 2 ) .   We then analyze the identified supportive data   in contrast to the general pretraining data , to ob-   tain data features particularly relevant to in - context   learning . We specifically approach from three as-   pects : the domain relevance to downstream tasks ,   the token frequency distribution , and the informa-   tion gain of incorporating long - range pretraining   context . Our major findings include : ( 1)Compared   to general pretraining data , the supportive data do   nothave a higher domain relevance to the down-   stream tasks . ( 2)The supportive pretraining data   contain a relatively higher amount of rarely occur-   ring , long - tail tokens . ( 3)The supportive pretrain-   ing data are challenging examples in incorporating   long - range context for language modeling ( § 3 ) .   Our work offers a first step towards interpret-   ing in - context learning in NLP tasks via analyzing   instance - level pretraining data . We believe it can   help improve the transparency and interpretability   of language models ’ in - context learning behavior .   Our analysis can also pave the way to improvedin - context learning in the future by informing pre-   training data construction .   2Finding supportive pretraining data for   in - context learning   Han and Tsvetkov ( 2022 ) propose an iterative ,   gradient - based method ORCA to find supportive   pretraining data of BERT ( Devlin et al . , 2019 ) un-   der a vanilla zero - shot prompting setup . In this   section , we provide some background and adapt   ORCA for large language models in a setting of in-   context learning ( ICL ) , finding supportive pretrain-   ing data for downstream tasks with demonstration   examples .   2.1 Methodology   Assume we have a pretrained language model ( LM )   θand data pairs ( x , y)representing the inputs and   ground truth outputs of task D. Both xandy   are in natural language . For classification tasks , the   target labels can be converted to natural language   via verbalizers ( Schick and Schütze , 2021 ) .   Zero - shot prompting A pretrained language   model can be applied to perform downstream   tasks via zero - shot prompting ( e.g. , Petroni   et al . , 2019 ) . For classification tasks , the lan-   guage model θoutputs a candidate answer with   top probability , argmaxp(y|x ) =   argmax / producttextp(y|x , y ) , where Y   contains all candidate answers y. For generation   tasks , outputs can be obtained by sampling autore-   gressively from θconditioned on x(e.g . , Holtzman   et al . , 2019 ) . This is a zero - shot scenario with no   demonstration examples .   In - context learning Instead of modeling p(y|   x ) , ICL estimates p(y| { ( x , y)},x ) ,   prepending the original model input with sev-   eral demonstration examples ( x , y)sam-   pled from the target task D. The language   model θis never trained on the task data with   demonstrations . However , we can form a loss   on the in - context data as a surrogate for θ’s12661ICL performance , which will be used for a   later guidance step , L(x , y ) = −logp(y|   { ( x , y)},x ) = −log / producttextp(y|   { ( x , y)},x , y ) .   Pretraining The pretraining data of θoften con-   sists of texts wfrom large , general - domain cor-   pora . During pretraining , the LM θis updated   via stochastic gradient descent with a loss to re-   construct wgiven a prefixing context , L(w ) =   −log / producttextp(w|w ) .   Supportive pretraining data Our goal is to lo-   cate what pretraining data wif upweighted would   be most helpful to the LM θ ’s ICL ability . Fol-   lowing ORCA ( Han and Tsvetkov , 2022 ) , we use   the similarity between gradients ∇L(w)and   ∇L(x , y)iteratively to find such supportive   pretraining data . We show details of our adapted al-   gorithm ORCA - ICL in Figure 2 . The algorithm   finds pretraining data that exert a gradient to θ   similarly as a group of guidance ICL task data   would . ∇L(x , y)provides a guidance for the   direction the model parameters should be updated   towards to be better at ICL , while ∇L(w)ap-   proximates how the direction the model parameters   would be updated based on individual pretraining   instances . We conduct a multi - iteration process   ( a total of Miterations each selecting ksupport-   i ve instances ) to mitigate noise . SGD denotes an   one - pass stochastic gradient descent to mimick an   incremental upweight to the selected data , with a   minimum number of steps to prevent overfitting .   The resulting supportive set Shas a very small size   ( under 2000 in this work ) .   Verifying supportiveness To quantitatively eval-   uate the supportiveness of the selected set of pre-   training data , we perform an one - pass gradient de-   scent on the original LM with the selected set S ,   which mimics a perturbative continued pretrain-   ingwith a minimum number of updates : θ←   SGD(θ ) . We then benchmark this perturbed   model ( θ ) with the original model ( θ ) and a   model perturbed with a random set of pretraining   data . We expect the perturbed model using our   selected supportive pretraining data to achieve a   better ICL performance . Algorithm 1 ORCA - ICL   2.2 Setup   Language model Throughout the work , we use a   pretrained , autoregressive OPT-6.7B ( Zhang et al . ,   2022b ) as our LM θ .   Tasks In this work , we focus on classification   problems and first retrieve 48 classification - based   tasks from Natural Instructions v2 ( NI - v2 , Wang   et al . , 2022 ) . We apply the LM on the tasks with   both a zero - shot and in - context learning setup . We   extract tasks that achieve at least 10 % better perfor-   mance with in - context demonstrations . We group   17 tasks that satisfies the constraint and further se-   lect 6 typical tasks among them :   SST-2 : Movie review sentiment classification   ( Socher et al . , 2013 ) . AG News : News topic clas-   sification ( Zhang et al . , 2015 ) . Story Cloze Test :   Story coherence classification ( Mostafazadeh et al . ,   2017 ) . SMS Spam Collection : Spam classification   ( Almeida et al . , 2011 ) . Sentiment 140 : Tweet sen-   timent classification ( Go et al . , 2009 ) . TweetQA :   Answer verification ( Xiong et al . , 2019 ) .   For each task , we randomly sample 500 exam-   ples with a balanced class distribution as D ,   guiding the ORCA - ICL algorithm . The quanti-   tative evaluation is performed on the full dataset .   For ICL , for each instance in the task data , we   randomly sample 4 demonstration examples under   each candidate class defined in the task . The or-   der of demonstration examples in the context is   randomly shuffled . The template and verbalizer   of each task follows the original NI - v2 dataset ,   though we did not include the task instructions , as12662the focus of this work is in - context learning with   demonstration examples .   Pretraining Considering the size of pretraining   dataD , we include an as large portion of OPT ’s   pretraining data as possible under a reasonable bud-   get . Specifically , in this work we use a total of   2.5 M pretraining instances each consists of 2048   tokens . For computing efficiency , we use intra-   layer model parallelism ( Shoeybi et al . , 2019 ) and   fully sharded data parallel ( Ott et al . , 2021 ) .   Implementation Details We run ORCA - ICL   with a maximum of M= 5 iterations . In each   iteration we extract k= 400 pretraining instances   with top gradient similarity with the ICL task data .   We use a batch size of 16 and learning rate of 2e-5   for the one - pass gradient descent with an Adam   optimizer ( Kingma and Ba , 2014 ) . This results in   a total of 125 updatesto the original LM after all   iterations as the perturbative continued pretraining .   2.3 Results   Perturbative continued pretraining As the   main evaluation of the supportive pretraining data   obtained by ORCA - ICL , we perform perturbative   continued pretraining on both the selected support-   i ve data and random pretraining data as a con-   trol . Table 1 shows the main results of task ac-   curacy . The leftmost column shows a source task   Dguiding the selection of supportive pretrain-   ing data . At each row , we evaluate the perturbed   model ( SGD(θ ) ) on all 6 tasks . The ICL perfor-   mance of the original LM is reported in the headers   of the table .   In each cell of the table , the top number shows   the continued pretraining result with the support-   i ve data we identified . We consider M∈[1,5 ]   iterations as a hyperparameter and report result   with a best M. We want to know at a same size   of selection , how our identified subset performs   compared to random pretraining data . We there-   fore run random selection with 5 seeds , and the   bottom number of the cell shows the continued pre-   training result with random data at a same size of   our selection , accompanied by a standard deviation .   The performance of our selection is bolded whenthe performance difference with random selection   exceeds one standard deviation .   The diagonal cells show the performance of per-   turbed models on the same task used for selecting   supportive data . We observe on 4 of the 6 source   tasks , our selection of supportive pretraining data   is effective . For the cross - task performance , we   observe on 5 of the 6 source tasks , our selection   is effective for at least three tasks . We conclude   thatour identified supportive pretraining data   is overall effective for ICL , though the cross - task   results show a portion of the ICL behavior can be   task - specific and not universal across tasks .   Control evaluation on zero - shot data Being ef-   fective on the ICL data does not necessarily mean   a direct support for a model ’s ICL ability , which   is to learn from the demonstration examples . The   test input can be a confounding factor : if our se-   lection is effective as well on zero - shot test input   without demonstrations , then the selection is not   specific to the ICL ability . Therefore , we further   confirm the supportiveness of our selected support-   i ve pretraining data to ICL , contrastively in a zero-   shot setup . We evaluate our models after perturba-   tive continued pretraining in Table 1 on the same   tasks but without the in - context demonstrations .   We present the results in Table 2 . The two columns   show the zero - shot prompting performance of the   original LM and the model after continued pre-   training with our ICL - supportive selection , respec-   tively . We do not observe performance gain for   most tasks , indicating our selection is specific to   the ICL ability without benefiting the zero - shot ,   no - demonstration task performance .   3 Analyzing supportive pretraining data   for in - context learning   In the previous section , we identify a small sub-   set of pretraining data that supports the ICL abil-   ity of language models . In this section , we an-   alyze the selected supportive pretraining data to   understand what makes them useful to ICL . Specif-   ically , we compare the supportive pretraining data   contrastively with randomly sampled pretraining   instances , investigating three aspects of the pre-   training data : the domain relevance to downstream12663SourceEval SST-2 AG News Story   ClozeSMS   SpamSentiment   140TweetQA   75.47 74.12 66.09 45.07 67.23 62.36   SST-2 83.15   75.8774.91   73.2467.76   66.2452.48   49.8269.03   66.2362.20   61.75   AG News 79.04   74.9975.40   73.7768.34   66.3859.24   46.5568.96   66.2361.86   62.02   Story Cloze 75.33   72.5074.12   73.7767.47   65.2551.36   47.1569.92   66.2362.33   62.02   SMS Spam 73.88   75.8772.78   73.7767.25   65.2564.69   46.5563.70   66.3362.13   61.75   Sentiment 140 77.56   73.4972.78   73.7766.78   66.3851.64   44.5266.66   66.0062.93   61.64   TweetQA 75.22   72.5071.52   73.0166.27   64.9143.09   44.5266.76   66.3361.31   61.33   Zero - shot Eval Original +   SST-2 46.82 46.83   AG News 46.14 44.05   Story Cloze 50.43 51.39   SMS Spam 44.41 43.84   Sentiment 140 55.84 54.90   TweetQA 50.44 50.32   tasks , the token frequency distribution , and the in-   formation gain of incorporating long - range context .   3.1 Domain relevance   Xie et al . ( 2022 ) and Min et al . ( 2022 ) imply that   in - context demonstration is useful since it helps   locate a particular domain or concept of the test in-   put the LM already learned through the pretrainingdata . On the other hand , Olsson et al . ( 2022 ) imply   that in - context demonstration is useful because the   decision over the test input may be done through a   soft - copy mechanism from the demonstration ex-   amples . These lead to two different expectations of   the role of supportive pretraining data : ( 1 ) Inferred   from Xie et al . ( 2022 ) and Min et al . ( 2022 ) , the   supportive pretraining data should be from a same   domain as the demonstration and test examples ,   providing direct supporting knowledge to solve the   downstream task . ( 2 ) Inferred from Olsson et al .   ( 2022 ) , the supportive pretraining data should be   beneficial to the soft - copy mechanism , providing   meta support for the abstract ability , unconstrained   with the concrete data domain . We aim to measure   the domain relevance between supportive pretrain-   ing data and downstream tasks .   Method To quantify domain relevance , we use   MAUVE score ( Pillutla et al . , 2021 ) to measure an   information divergence between two text distribu-   tions . We compute two MAUVE scores , between   the target task data and our selected supportive   pretraining data , and between the task data and ran-12664   dom pretraining data . We then compute and report   their difference . A positive MAUVE difference in-   dicates a higher domain relevance of our supportive   pretraining data . We use RoBERTa ( Liu et al . ,   2019 ) as MAUVE ’s embedding model following   He et al . ( 2022 ) .   Results We show the difference of MAUVE   scores in Figure 3 . The error bar shows the 95 %   confidence interval using 32 random seeds . We   find that for 5 of the 6 tasks , there is no signif-   icant difference between the MAUVE scores of   supportive pretraining data and random data . For   SST-2 , the supportive pretraining data even shows   a lower MAUVE score . Therefore , the supportive   pretraining data to ICL do nothave a higher do-   main relevance to the task , compared to general   pretraining data . This result aligns with the do-   main relevance finding in Shin et al . ( 2022 ) where   dataset - level analyses were performed . This im-   plies the improved ICL behavior of our models   may be a meta ability , aided by pretraining data   unrelated to the specific domain knowledge for   solving the task , but related to a domain - invariant   mechanism to learn from a data ’s context . § 3.3   continues this discussion .   3.2 Token frequency distribution   Providing demonstrations to a task input under an   ICL setup creates repetitions ( e.g. , of label tokens ) ,   which changes the token frequency distribution of   the ICL task data . Therefore , we are interested in   whether the supportive pretraining data possess a   different token frequency distribution from general   pretraining data . Experimented with sequences of   image - label pairs , Chan et al . ( 2022 ) find that a   skewed class distribution ( high burstiness ) and a   large number of rarely occurring classes in training   data promote the ICL ability of Transformer mod-   els ( Vaswani et al . , 2017 ) . However , it is unknown   whether the findings on the synthetic image - label   data can transfer to the natural language pretraining   data , a gap we address in this subsection .   Method We fit a Zipfian distribution over each   supportive and random pretraining instance that   consists of 2048 tokens . The Zipf ’s coefficient   is the negative slope of a linear regression over   the tokens ’ log - rank v.s. log - frequency . A higher   Zipf ’s coeffcient indicates a higher mass on the   frequent tokens ( i.e. , more skewed distribution ) . A   lower Zipf ’s coefficient indicates a higher mass on   the rare , long - tail tokens ( i.e. , flatter distribution ) .   Results In Figure 4 , we show the difference in   average Zipf ’s coefficients between supportive and   random pretraining data , each with a group size   of 2000 . The error bar shows the 95 % confidence   interval with 32 random seeds . We find that for   all tasks , the Zipf ’s coefficient of the supportive   pretraining data is significantly lower than that of   the random pretraining data . This indicates a flatter   Zipfian distribution with a relatively higher mass   over the long - tail tokens . In other words , though   the overall burstiness of data is lower , there is a rel-   atively higher amount of rarely occurring , long-12665tail tokens in the supportive pretraining data   for ICL . Flatter frequency distribution also indi-   cates higher entropy over the tokens , presumably   making the supportive pretraining data challenging   examples to fit by the model , a concept we explore   further in the next subsection .   3.3 Information gain from long - range context   In § 3.1 , we find that the domain relevance of the   supportive pretraining data to downstream tasks   is not higher than that of random pretraining data .   This is comprehendible if we follow the aforemen-   tioned perspective of Olsson et al . ( 2022 ) , hypoth-   esizing that there exists a soft - copy mechanism   between the in - context demonstrations and test in-   put . The supportive pretraining data may provide   meta support for the abstract soft - copy mechanism   rather than task - specific knowledge . We further hy-   pothesize that to facilitate such meta support , the in-   corporation of long - range context during language   modeling in supportive pretraining data should be   different from random pretraining data , since the   demonstration examples in the ICL setup is a form   of long - range context . We propose a novel infor-   mation gain measure to quantify this feature of   incorporating long - range context .   Method Recall that the canonical definition of in-   formation gain ( IG ) is IG(T , a ) = H(T)−H(T|   a ) , where Tis a target variable , ais an attribute   conditioned on by T , and H(·)computes entropy .   It measures the decrease of entropy ( thus the gain   of information ) in Tif conditioned on a. We adapt   the canonical IG to measure the decrease of cross   entropy for each token ( w ) in a pretraining dataset   when conditioned on a long ( l ) context over a short   ( s ) context :   IG(l , s ) = CE ( w|ctx)−CE(w|ctx )   Ideally the length of long or short context should   remain constant across different tokens w , but it   would be a very expensive computation due to a   lack of parallelism . We approximate the compu-   tation by splitting a full sequence of pretraining   tokens ( e.g. , 2048 tokens ) to smaller blocks and cal-   culate cross entropy with the boundary of blocks :   IG(l , s ) = −logp(w|w )   + log p(w|w )   With the above definition , the average length of   context for all wissandl , respectively . In theexperiments below , we keep s= 128 for the length   of short context and increase the length of long   context at l={256,512,1024 } .   We report the difference in the average informa-   tion gain ( across w ) of incorporating long - range   context for a language modeling objective , in sup-   portive pretraining data over random pretraining   data . Additionally , we want to use the defined in-   formation gain measure as a standalone feature of   data , so we use a different LM to compute the cross   entropy than the LM on which we perform ICL .   Below we report results using OPT-1.3B , while ex-   periments using OPT-350 M shows a similar trend .   Results In Figure 5 , we see for all of the exper-   imented tasks , there is a significant trend that in-   creasing the length lfor the long - range context   for supportive pretraining data has a lower rela-   tive information gain compared to random pretrain-   ing data . Though seeming counterintuitive at first   glance , this suggests that the supportive pretrain-   ing data are more challenging examples in incor-   porating the long - range context information .   A possible explanation for this is that such chal-   lenging examples contain confounding spans that   harms the information gain measure . The language   model has to learn to decide which part of the long-   range context is truly relevant to the prediction of   next tokens . This would resemble more and thus   helpful to the ICL task scenario where there are   multiple demonstrations from different classes .   3.4 Future work   Despite our aforementioned findings , we mainly   conduct correlational analyses throughout the work .   Despite the potential confounding factors , future   work can try converting the correlational findings to   causal ones . For example , to actively refine or con-   struct pretraining data to improve existing models ’   ICL performance , with a metric of token frequency   distribution ( i.e. , find data with a higher mass of   long - tail tokens ) or context information gain ( i.e. ,   find difficult examples in incorporating long - range   context ) . Additionally , we only investigate classifi-   cation tasks in this work . However , the ORCA - ICL   method can be applicable to generation tasks as   well in the future , if the ICL loss is defined over a   sequence probability of the generation.12666   4 Related Work   Demonstration examples Min et al . ( 2022 ) un-   derstand ICL through analyzing which aspects of   the demonstration examples contribute or are ir-   relevant to task performance . They find replacing   ground truth demonstration labels with random la-   bels would not hurt task performance , while ICL   still benefits from knowing the label space , distri-   bution of inputs , and sequence format specified in   demonstration examples . Zhang et al . ( 2022a )   further show on sequence labeling tasks , the length   of demonstrations and the relevance of their tokens   are important for ICL .   Learning mechanism Xie et al . ( 2022 ) explain   ICL as implicit Bayesian inference , occurring when   language models infer a shared latent concept from   demonstration examples at inference time . They   show language models exhibit such ICL behav-   ior by constructing synthetic pretraining data with   a controlled distribution of concepts . Garg et al .   ( 2022 ) empirically show that Transformer models   can be trained to learn unseen linear functions from   in - context demonstration examples . Olsson et al .   ( 2022 ) present evidence that multi - layer attention - based models form an induction head and perform   ICL by a pattern copying behavior from the pre-   fixing context . More recent work like Akyürek   et al . ( 2022 ) , Dai et al . ( 2022 ) , and von Oswald   et al . ( 2022 ) explain ICL in Transformer models   as a kind of standard learning algorithms over the   demonstration examples , such as gradient descent   and regression .   Pretraining data Razeghi et al . ( 2022 ) find on   numerical reasoning tasks , a language model ’s ICL   performance is highly correlated with the term fre-   quency of the input data in the pretraining corpus .   Shin et al . ( 2022 ) investigate how ICL can be af-   fected when the pretraining dataset varies . They   discover that ICL heavily depends on the corpus do-   main source , but pretraining with a corpus related   to a downstream task does not always translate to   a competitive ICL performance on the task . Chan   et al . ( 2022 ) experiment on a synthetic image - label   pairs dataset . They show certain distributional prop-   erties of the synthetic pretraining data , such as the   burstiness of classes and large numbers of rarely   occurring classes , promote the emergence of ICL .   Our work belongs to this line of work , but offers   a first step towards understanding ICL in realistic   NLP tasks through analyzing instance - level pre-   training data . Additionally , concurrent to our work ,   Gu et al . ( 2023 ) propose a method that groups pre-12667training data by their instrinsic tasks , enhancing   instead of interpreting existing language models ’   ICL ability .   5 Conclusion   In - context learning has shown superior perfor-   mance on a range of NLP tasks , yet it remained   unclear from where language models acquired this   ability . We approach the problem by identifying   a small subset of pretraining data that particularly   supports language models to do in - context learning   on downstream tasks . We analyze common features   of the supportive instances in contrast to general   pretraining data and find that : ( 1 ) The supportive   pretraining data do nothave a higher domain rele-   vance to the downstream tasks . ( 2 ) The supportive   data contain a relatively larger amount of rare , long-   tail tokens . ( 3 ) The supportive pretraining data are   more challenging instances in incorporating long-   range context in language modeling . Our findings   may be beneficial to future work that refine or con-   struct pretraining data , in order to actively improve   existing models ’ in - context learning performance .   Limitations   It is worth noting that the supportive pretraining   data we investigated throughout the work is w.r.t .   thecurrent LM , such that a perturbative continued   pretraining with the supportive data would improve   the final LM checkpoint deployed to downstream   tasks . It is possible that for some data which we did   not determine as supportive , they had been support-   i ve w.r.t . early checkpoints of the LM . With more   computing resources , future work may investigate   the trend of supportive patterns across multiple   checkpoints of a LM throughout the pretraining   process .   Additionally , another significant limitation of   our work is the amount of involved computing re-   source . The ORCA - ICL method is gradient - based   that requires back - propagation . Since we iterate   through a large size of pretraining data , the cost   of computation is similar to training a language   model with a batch size of 1 on the considered   pretraining data . On our 4 nodes each consists of   8 Nvidia V100 GPUs , finding the supportive pre-   training data for each source task in our experiment   would take about a week . One mitigating aspect of   such computation is that the gradient calculation   can be done asynchronously , therefore enabling the   use of idle , leftover GPUs scattered across a clusterof nodes . We plan to explore efficient computation   of gradient similarity or move from a paradigm of   extracting supportive data to generating supportive   data in future work .   Acknowledgements   We thank Naman Goyal , Anjali Sridhar , Zeyu Liu ,   Victoria Lin , Mengzhou Xia , Weijia Shi , Jiacheng   Liu , and Tianxing He for helpful discussions . We   also thank the anonymous ACL reviewers and all   members of TsvetShop for the valuable feedback .   This research is supported in part by the Office   of the Director of National Intelligence ( ODNI ) ,   Intelligence Advanced Research Projects Activity   ( IARPA ) , via the HIATUS Program contract 2022-   22072200004 . The views and conclusions con-   tained herein are those of the authors and should   not be interpreted as necessarily representing the   official policies , either expressed or implied , of   ODNI , IARPA , or the U.S. Government . The U.S.   Government is authorized to reproduce and dis-   tribute reprints for governmental purposes notwith-   standing any copyright annotation therein .   References1266812669   A Qualitative examples   In Table 3 , we show some qualitative examples of   the supportive pretraining data to ICL and random   pretraining data . Note that these are illustrative   examples extracted from long pretraining instances   ( each instance consists of 2048 tokens ) , for a bet-   ter understandability of our findings . A manual   examination of such data is difficult , and we thus   propose the quantitative analyses described in the   main paper.12670Supportive pretraining data to ICL   Random pretraining data12671ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Page 9   /squareA2 . Did you discuss any potential risks of your work ?   Page 9   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Page 9   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Page 912672 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 2.2   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 2 , Section 3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.12673