  Yafu Li , Leyang Cui , Jianhao Yan , Yongjng Yin   Wei Bi , Shuming Shi , Yue ZhangZhejiang UniversityTencent AI labSchool of Engineering , Westlake UniversityInstitute of Advanced Technology , Westlake Institute for Advanced Study   yafuly@gmail.com   { leyangcui,victoriabi,shumingshi}@tencent.com   { yanjianhao,yinyongjing,zhangyue}@westlake.edu.cn   Abstract   Most existing text generation models follow   the sequence - to - sequence paradigm . Gener-   ative Grammar suggests that humans gener-   ate natural language texts by learning language   grammar . We propose a syntax - guided gener-   ation schema , which generates the sequence   guided by a constituency parse tree in a top-   down direction . The decoding process can be   decomposed into two parts : ( 1 ) predicting the   infilling texts for each constituent in the lexical-   ized syntax context given the source sentence ;   ( 2 ) mapping and expanding each constituent   to construct the next - level syntax context . Ac-   cordingly , we propose a structural beam search   method to find possible syntax structures hierar-   chically . Experiments on paraphrase generation   and machine translation show that the proposed   method outperforms autoregressive baselines ,   while also demonstrating effectiveness in terms   of interpretability , controllability , and diversity .   1 Introduction   Natural language generation ( NLG ) , such as para-   phrase generation ( Sun et al . , 2021 ) , text sum-   marization ( Lin et al . , 2018 ) , machine translation   ( Vaswani et al . , 2017 ; Edunov et al . , 2018 ) , and lan-   guage models ( Brown et al . , 2020 ; OpenAI , 2023 ) ,   have shown remarkable progress in the past few   years . Most of the highest - performing NLG models   train the model based on source - target correspon-   dence and conduct autoregressive inference , which   achieves competitive empirical performances yet   deviates from a range of desirable attributes of hu-   man language generation , e.g. , lack of interpretabil-   ity ( Alvarez - Melis and Jaakkola , 2017 ; He et al . ,   2019 ; Li and Yao , 2021 ) .   It has been shown that humans generate lan-   guage by learning and manipulating language gram-   mar ( Zholkovskii and Mel’chuk , 1965 ; Montague ,   1974 ) , which generative grammar ( Chomsky , 1965)Figure 1 : Syntax - guided generation : searching the hy-   potheses hierarchically throughout the syntax tree in a   top - down direction , starting from the root node “ < T > ” .   The green blocks denote the possible syntax structures   at different tree depths , the blue one denotes the external   modification , whereas the gray ones denote the finalized   hypotheses , marking the end of search paths .   considers as a finite rule set that combines words to   form grammatical sentences , thereby avoiding enu-   meration of surface sequences , which can signifi-   cantly increase data sparsity and reducing learning   efficiency ( Li et al . , 2021 ; Dankers et al . , 2022 ) . In   this process , syntax plays a crucial role , imposing   constraints on how to construct sentences . Syntax   knowledge has been found implicitly contained by   deep neural models ( Kovaleva et al . , 2019 ; Clark   et al . , 2019 ) and also useful for NLG tasks ( Yang   et al . , 2020a ; Sun et al . , 2021 ; Xie et al . , 2021 ) .   However , relatively little recent work has consid-   ered explict syntax in NLG ( Wang et al . , 2018 ) .   Inspired by the above psycholinguistic obser-   vation , we propose a syntax - guided generation   scheme , which generates text by following a well-   defined grammar . As shown in Figure 1 , instead of   sequential generation , the model generates the sen-   tence in a hierarchically top - down manner guided   by the constituency parse tree , starting with the   root node < T > . Syntactic categories such as noun   phrases < NP > and verb phrases < VP > are inte-   grated with tokens in the generation process , and14095the model simultaneously considers multiple syn-   tax structures at each tree depth , hierarchically ex-   ploring the syntax tree for reasonable hypotheses .   Intuitively , such a generation paradigm has the   following advantages compared with autoregres-   sive generation . First , akin to the language learn-   ing process of human beings , grammar learning   breaks down non - enumerable surface sequences   into finite pieces , acting as a training curriculum .   Second , it provides an effective and interpretable   pathway to probe into the generation process . Con-   sequently , generation errors can be traced back to   specific constituent expansion at the respective tree   depth . Third , one can manipulate the generation   process by exerting versatile control at arbitrary   depths , e.g. , modifying the translation of a verb   phrase and constraining the paraphrase style with   syntax templates . Forth , diverse sequences can be   generated by exploring various syntax structures   hierarchically throughout the syntax tree .   We implement the above process on Transformer   ( Vaswani et al . , 2017 ) . As shown in Figure 1 , the   generation process proceeds under the guidance   of syntactic grammar . Starting from the root node   “ < T > ” , the model recursively generates the infilling   texts ( e.g. , “ he ” and ” seems < S > ” ) for each con-   stituent in the current lexicalized syntax context   ( e.g , “ < NP > < VP > . ” . ) , and infills each one accord-   ingly to construct the next - level lexicalized syntax   context ( e.g. , “ he seems < S > . ” ) . The generation   proceeds until there is no remaining constituent .   The infilling texts are predicted by a Transformer-   based model , which is trained by maximizing the   likelihood of infilling texts for each constituent in   the syntax context based on the source input . To   explore more syntactically diverse and reasonable   hypotheses during inference , we propose structural   beam search , which searches promising syntax   structures over the entire syntax tree in a top - down   manner , as shown in Figure 1 .   To isolate the effect of syntax and avoid the in-   fluence of other transformation factors , we con-   duct experiments on two sequence - to - sequence   ( seq2seq ) tasks with semantic equivalence between   the source and target sequences : paraphrase gen-   eration and machine translation . Empirical re-   sults demonstrate that our method can generate   sequences with higher quality than the seq2seq   baselines . Quantitative analysis demonstrates that   the generation process can be interpreted effec-   tively . In addition , our method demonstrates the ca - pability of executing control from both syntax tem-   plates and fine - grained manual modifications . Fi-   nally , we show the diversity advantage through both   automatic evaluation and human evaluation . We   release the code on https://github.com/   yafuly / SyntacticGen .   2 Related Work   Syntax as Extra Input . A line of work incorpo-   rates syntax knowledge as extra input to boost task   performance . In paraphrase generation , Iyyer et al .   ( 2018 ) , Chen et al . ( 2019 ) , Kumar et al . ( 2020 )   and ( Sun et al . , 2021 ) additionally encode a con-   stituency tree to produce controllable paraphrases .   For machine translation , researchers utilize syntac-   tic information to boost the neural machine trans-   lation system using syntactic encoders ( Li et al . ,   2017 ; Ma et al . , 2018 ; Eriguchi et al . , 2019 ; Ma   et al . , 2020 ; Yang et al . , 2020a ) , position encod-   ing ( Ma et al . , 2019 ; Xie et al . , 2021 ) , attention   mechanism ( Chen et al . , 2018 ; Peng et al . , 2019 ) ,   and auxiliary training objectives ( Ma et al . , 2019 ) .   Syntax for Generation Guidance . Different   from the above work , we focus on guiding genera-   tion explicitly following syntactic grammar . Typ-   ically , Aharoni and Goldberg ( 2017 ) and Le et al .   ( 2017 ) learn the mapping from sequences to lin-   earized constituency trees to improve machine   translation . Eriguchi et al . ( 2017 ) proposes a   hybrid decoder with RNNG ( Dyer et al . , 2016 )   to jointly learn parse actions and word predic-   tions . Wu et al . ( 2017 ) and Wang et al . ( 2018 )   design a syntactic tree decoder based on LSTM   ( Hochreiter and Schmidhuber , 1997 ) , with an ex-   tra rule decoder . Yang et al . ( 2020b ) introduce a   syntax - guided soft target template as extra prompts   in Transformer . Different from their work , our   method leverages Transformer strengths and breaks   down the sequence - to - sequence generation process   into a hierarchically top - down generation guided   by the syntax tree .   3 Method   3.1 Baseline Transformer   Transformer models the correspondence between   the source sequence x={x , . . . , x}and the tar-   get sequence y={y , . . . , y}in an end - to - end   fashion . The Transformer encoder transforms the   discrete source sequence xinto a continuous repre-   sentation , which the Transformer decoder utilizes14096   to generate the target sequence . The conditional   probability p(y|x)can be factorized in an autore-   gressive way :   p(y|x ) = /productdisplayp(y|x , y ) , ( 1 )   where θdenotes the model parameters .   Given a source - target training set D=   { x , y}| , the model is optimized by minimiz-   ing the cross - entropy ( CE ) loss :   L=−/summationdisplay / summationdisplaylogp(y|x , y).(2 )   3.2 Syntax - guided Generation   In this section , we introduce syntax - guided gener-   ation , which generates texts by hierarchically ex-   panding constituents in syntax contexts throughout   the syntax tree , while also leveraging the strengths   of Transformer . In general , the generation process   can be decomposed into two stages : ( 1 ) neural gen-   eration : the neural decoder ( Section 3.2.2 ) gener-   ates the infilling sequences based on the source se-   quence and the syntax context ; ( 2 ) constituent ex-   pansion : predicted infilling sequences are mapped   and filled into each constituent in the syntax context   accordingly ( Section 3.2.3 ) , forming the next - level   syntax context . To facilitate parallelism during   training , we decompose the sequence - to - sequence   dataset to a triplet set , where the neural decoder   is optimized to maximize the probability of the   infilled sequence ( e.g. , " < c > I < c > ate < NP > . " )   given the lexicalized syntax context ( e.g. , " < NP >   < VP > . " ) , as shown in Figure 2 .   3.2.1 Triplet Construction   Given a target sequence y , the corresponding con-   stituency parse tree of depth |T|can be composedby a set of labeled spans T :   T={T}|={{(a , b , d , l)}|}| ,   ( 3 )   where aandbrepresent the k - th constituent   span ’s fencepost positions at depth d , and lrep-   resents the constituent label . Our model is op-   timized to predict the next - level span sets T   given the previous one and the source input , i.e. ,   p(T|T , x ) .   Given the set of labeled spans at depth d ,   i.e. ,T , we transform the target sequence into   a lexicalized syntax sequence of length |s| :   s={s , s , . . . , s } , by keeping the lex-   ical tokens and replacing the constituent spans   with corresponding labels . For instance , the se-   quence “ I ate an apple . ” is transformed to   s={<NP>,<VP > , . } at depth 2 , and is transformed   tos={I , ate , < NP > , . } at depth 3 , as shown in Fig-   ure 2 . The alignment between sandscan be   modeled as a text - infilling task . For example , the   { < NP > } , { < VP > } and at depth 2 are replaced by   { I } and { ate < NP > } at depth 3 , respectively . To   generate the whole sbased on sin one pass , we   concatenate all the infilling texts with a special to-   ken “ < c > ” , yielding an infilling sequence f=   { < c>,I,<c>,ate , < NP > } .   Similarly for each syntax context s , we collect   the respective infilling texts for each constituent in   the lexicalized sequence at depth d+1 , and concate-   nate them to construct the target infilling sequence   of length |f|:f={f , f , . . . , f } . In   this way , a triplet is constructed for a source - target   sequence pair at depth d:{(x , s , f ) } . We tra-   verse the target syntax tree in level - order to obtain   the full set Φof training triplets for a training in-   stance :   Φ = { Φ}|={(x , s , f)}|.(4 )   Given a sequence - to - sequence training set D=14097{x , y}| , we go through the full training set to   construct the complete triplet set Ψ :   Ψ = { Φ}|={(x , s , f)}| .(5 )   3.2.2 Neural Decoder   Given a triplet instance Ψ , we construct the neu-   ral decoder based on Transformer to model the   generative probability p(f|x , s ) . The neural   decoder takes the source sequence and the lexi-   calized syntax context as input and generates the   corresponding infilling texts , as shown in Figure 2 .   Besides the encoder that encodes source context ,   we introduce an extra Transformer encoder , i.e. ,   syntax context encoder , to encode the lexicalized   syntax context into a representation . On top of self-   attention and source context attention , we insert an   extra attention layer ( syntax context attention ) into   each decoder layer to incorporate syntax contexts ,   as shown in the right part of Figure 2 .   Similarly , the probability of the infilling se-   quence can be factorized as :   p(f|x , s ) = /productdisplayp(f|x , s , f).(6 )   We define the scoring function for an infilling   sequence as the sum of the log probabilities :   score ( x , s , f ) = /summationdisplaylogp(f|x , s , f).(7 )   We adopt the standard cross - entropy loss ( CE   loss ) to optimize our model , where the loss for the   j - th triplet in the training set Ψcan be written as :   L=−/summationdisplaylogp(f|x , s , f),(8 )   and the CE loss across the whole triple set Ψbe-   comes :   L=/summationdisplayL. ( 9 )   3.2.3 Generation Process   Given a source sequence , our model generates the   target sequence in a top - down manner which is   grounded on syntactic grammar rules . As shown   in Figure 2 , the neural decoder first encodes the   source sequence xinto the source context represen-   tation h , which remains fixed and can be reusedthroughout the generation process . Initially , the   neural decoder generates the infilling sequences   tgiven xands={<T > } , based on Equation   6 . Then the model proceeds with the generation   process via iteratively generating infilling texts and   expanding constituents .   At each iteration step ( i.e. , tree depth ) , the neural   decoder generates the infilling sequence ffor the   syntax context s :   f= arg maxp(f|x , s ) ( 10 )   Then the constituent expansion function yields the   next - level syntax context given the syntax context   and the infilling sequences predicted by the neural   decoder :   s= expand ( s , f ) . ( 11 )   Specifically , we first separate the infill-   ing sequences by the special separator “ < c > ”   into a group of infilling texts , e.g. , split-   ingf={{<c>,I,<c>,ate , < NP > } } to { { I},{ate   < NP > } } . Then we fill in each of the infilling texts   into the corresponding constituent in the syntax   context sto obtain the syntax context at the fol-   lowing level , e.g. , s={I , ate , < NP > , . } . The syntax   context encoder encodes the updated syntax context   sand starts the next iteration . The remaining   decoding process loops between these two stages ,   until there is no constituent label in the syntax con-   text , or a maximum tree depth is reached , as shown   in Figure 2 .   As the model behavior on expanding con-   stituents over the entire syntax tree is completely   accessible , the generation process can be effec-   tively interpreted , as shown in Section 6.2 . More-   over , manual modifications can be directly incor-   porated into the expansion process for each con-   stituent throughout the syntax tree ( Section 6.3 ) .   Finally , more than one syntax structure can be con-   sidered simultaneously at each tree depth , enabling   searching for hypotheses of better syntactical diver-   sity(Section 6.4 ) .   3.2.4 Structural Beam Search   By default , our model selects the best infilling texts   greedily in each iteration . We introduce structural   beam search to explore the hypothesis space for   a more accurate and diverse generation . Similar   to standard beam search ( Sutskever et al . , 2014 ) ,   structural beam search maintains a beam width of14098   candidates at each iteration . Thanks to explicitly   traversing the constituency parse tree during infer-   ence , our method is able to search promising syntax   structures throughout the syntax tree in a top - down   manner . We show a real example of our model   generating a paraphrase in Figure 3 .   At each level , we apply standard beam search   for neural generation and keep top kinfilling texts   along with their scores , computed by Equation 7 .   Taking previous predictions into consideration , we   introduce a moving average mechanism to trade   off confidence between the predictions from lower   levels and the current - level prediction . Specifically ,   suppose sis the i - th syntax context in the k - width   beam at the current depth , with an accumulated   score of δ ; andfis the j - th infilling sequence   candidate from the neural generation beam given   the syntax context s , with a score of δ . A   beam of next - level syntax contexts is constructed ,   by filling in the current syntax context with the   corresponding infilling sequences :   s= expand ( s , f ) . ( 12 )   The updated score for each of the next - level syn-   tax contexts in the beam is given by :   δ = αδ+ ( 1−α)δ , ( 13 )   where αis a hyper - parameter ( accumulation   weight ) that determines how much weight is put on   predictions at lower levels . Then the beam is fur-   ther pruned by their updated scores to maintain the   beam width . For example , the first two candidate   syntax contexts are selected at depth 2 in Figure 3 .   Algorithm implementation details can be referred   to in Appendix A.   4 Experiment Setup   Datasets For paraphrase generation , we experi-   ment on ParaNMT - small ( Chen et al . , 2019 ) , whichcontains 500 K sentence - paraphrase pairs for train-   ing , 500 for validation , and 800 for testing . Both   validation and test sets are provided with human-   annotated sentence exemplars from which syn-   tax information can be extracted for controlling   paraphrase generation . For machine translation ,   we use NIST Chinese - English ( Zh - En ) , WMT’16   Romanian - English ( Ro - En ) , WMT’14 English-   German ( De - En ) , and WMT’14 English - German   ( En - De ) . For WMT datasets , we follow the offi-   cial split for validation and testing . For NIST Zh-   En , we use MT06 as the validation set and choose   MT02 , MT03 , MT04 , MT05 , and MT08 as the test   sets . For all datasets , we use Berkeley Parser ( Ki-   taev and Klein , 2018 ; Kitaev et al . , 2019 ) to obtain   constituency parse trees and use the most frequent   constituents ( e.g. , < NP > , < VP > , < PP > and < S > )   for syntactic guidance .   Model Settings For Transformer baselines , we   adopt the Transformer_Base configuration which   consists of a 6 - layer encoder and decoder . For our   model , we keep the 6 - layer source context encoder ,   and set the number of layers for both the syntax   context encoder and the decoder as 3 , resulting   in a similar model size with Transformer_Base .   The accumulation weight αis as 0.8 for structural   beam search based on validation experiments . For   machine translation , we adopt sequence - level dis-   tillation ( Kim and Rush , 2016 ) for both our model   and the corresponding baseline Transformer . More   details are shown in Appendix B.   Evaluation We use the BLEU score ( Papineni   et al . , 2002 ) to evaluate machine translation perfor-   mance . For paraphrase generation , we also adopt   ROUGE ( Lin , 2004 ) and METEOR ( Banerjee and   Lavie , 2005 ) as reference - based metrics . Besides,14099   we report iBLEU ( Sun and Zhou , 2012 ):   iBLEU = r·BLEU(hypothesis , reference )   −(1−r)·BLEU(hypothesis , source ) ,   which evaluates the generation fidelity with nov-   elty to the source sentence considered . Following   Bandel et al . ( 2022 ) , we consider two reference-   free metrics : ( 1 ) lexical diversity score , i.e. , D ,   which is the normalized character - level minima   edit distance between the bag - of - words ; and ( 2 )   syntax diversity score , i.e. , D , which is the nor-   malized tree edit distance . Both scores measure   generated paraphrases with the source sequences   unless specified .   5 Results   Paraphrase We compare our method with the   baselines and previous work on syntax - control para-   phrase generation . Another two baselines are alsolisted , i.e. , copy the source input and use the refer-   ence as the output . The results are shown in Table 1 .   For paraphrase generation without syntax control   ( the center section in Table 1 ) , our method achieves   higher performance than the seq2seq Transformer ,   in both greedy and beam search settings . Typically ,   our method under greedy decoding obtains com-   parable results with the Transformer under beam   search , and even outperforms under some metrics .   The advantage of our method becomes larger for   metrics such as iBLEU , D , andD , which   consider generation novelty compared with the   source input . For example , compared with Trans-   former ( beam 5 ) , our method ( beam 5 ) gives a   much lower self - BLEU score ( 16.4 v.s.33.8 ) and   higher diversity scores ( 21.5 v.s. 16.2 for lexi-   cal diversity and 25.1 v.s.18.1 for syntax diver-   sity ) , indicating better generation diversity and con-   tributing to a significant improvement on iBLEU   ( 8.6v.s . 2.2).With annotated exemplars ( the   lower section in Table 1 ) , our model obtains further   improvement over the non - exemplar setting and   achieves better performance compared to previous   work which utilizes full syntactic parse .   We extend our method to the pre - trained lan-   guage model ( PLM ) setting and present the result   in Table 3 ( Details in Appendix A ) . It can be seen   from the table that the utilization of BART ( Lewis14100et al . , 2019 ) improves the generation diversity for   the sequence - to - sequence model significantly . De-   spite the narrowed gap , our model outperforms the   seq2seq counterpart in terms of iBLEU and lexical   diversity by a considerable margin .   Machine Translation As shown in Table 2 , our   method achieves consistent performance ( BLEU   score ) improvement over the Transformer base-   line . The improvement is larger for the greedy   setting ( +1.5 BLEU scores on average ) , compared   with the beam search setting ( +1.2 ) . This indi-   cates that using syntax to guide and constrain gen-   eration yields more reasonable and high - quality   hypotheses than the greedy autoregressive genera-   tion , and thus relies less on search algorithms ( e.g. ,   beam search ) . Note that compared with the English-   oriented datasets , our model obtains a smaller per-   formance improvement on WMT’14 En - De . This   can be because the German parser is less accurate   than the English one ( 92.1 v.s. 96.3 for F1 score ) ,   resulting in a training set with lower quality .   6 Analysis   We first discuss the influence of grammar quality ,   then we understand the potential advantages of our   method from three perspectives , i.e. , interpretabil-   ity , controllability , and diversity .   6.1 The Influence of Grammar Quality   Intuitively , learning syntactic grammar of higher   quality results in better generation performance ,   e.g. , the advantage of our method on English-   oriented datasets is larger than the German - oriented   one . To further explore the influence of grammar   quality , we randomly replace a certain ratio of the   constituent labels with a random one to simulate a   less accurate parser . We conduct experiments on   the WMT’16 Ro - En dataset . By injecting noise of   ratios of 0.2and0.4 , the model performance de-   teriorates from 34.9 to 34.6 and32.3 accordingly ,   indicating the quality of syntactic grammar exerts a   large influence on model ’s generation performance .   6.2 Interpretability   We evaluate the model ’s interpretability based on   its capability of providing explanations in under-   standable terms to a human ( Doshi - Velez and Kim ,   2017 ) , i.e. , whether it generates texts following   language grammar . We trace each constituent ex-   pansion during generation and compare the model-   induced tree with the tree parsed by a benchmark   parser , e.g. , Berkeley Parser . Specifically , we use   the Berkeley parser to parse the same generated   hypotheses by our model and treat the correspond-   ing parsing results as golden parses . Quantitative   results ( Figure 4 ) show that our model achieves   an average F1 score of 94.6 , which demonstrates   the generation process highly corresponds to the   syntactic grammar and thus can be effectively inter-   preted . Note that the score for WMT’14 En - De is   lower ( 89.0 ) , possibly due to the less accurate Ger-   man parser for constructing the syntactic grammar ,   as discussed in Section 6.1 .   6.3 Controllability   Control with Complete Syntax Template To   leverage control signals from delexicalized syntax   templates ( e.g. , “ ( S(NP ) ( VP(NP ) ) ) ” for the se-   quence “ I ate an apple . ” ) , we introduce a reward γ   into Equation 13 :   δ = αδ+ ( 1−α)δ+γ . ( 14 )   If the updated syntax context smatches the   corresponding template pattern at depth d+ 1 , the   γis a positive value otherwise 0 . For example , the   syntax context “ < NP > < VP > ” in Figure 3 matches   the pattern “ ( ( NP)(VP ) ) ” at depth 2 . Intuitively , the   reward encourages the model to favor beam candi-   dates that match the syntax template . We set the   reward value as 0.32 based on validation results14101(Appendix F ) . The testset of ParaNMT - small is pro-   vided with human - annotated exemplars and we use   it to control generation , with results shown in Ta-   ble 1 . More generally , golden templates can be   derived by parsing the reference sentences for each   dataset with a parser ( e.g. , the Berkeley Parser ) . We   present the results in Table 5 . Guided by the refer-   ence syntax template , our model obtains consistent   improvement in terms of hypothesis similarity with   references , which is reflected by the decreased syn-   tax edit distance to the references , i.e. , D. For   the multi - reference dataset NIST Zh - En , our model   can generate translations of different styles which   are prompted by alternative syntax templates from   multiple references .   Control with Partial Syntax Template We fur-   ther explore whether the model can handle fine-   grained arbitrary controls . Specifically , we ask   three annotators to modify the intermediate syn-   tax contexts output by the model , based on the   source input . 100 instances are randomly selected   from the NIST Zh - En test set and each annota-   tor gives different modifications for each instance .   The modified contexts are fed to the model to pre-   dict the infilling texts . We then ask the annotators   to evaluate whether their controls ( i.e. , modifica-   tions ) are safely responded to by the model . We   show some of the control examples in Appendix   G. The average control success rate is 81 % , which   demonstrates the capability of our model to handle   arbitrary fine - grained controls .   6.4 Diversity   Beam Diversity We expect the model to gener-   ate diverse hypotheses under beam search , while   also maintaining generation quality . To this end ,   we measure the model ’s beam diversity by com-   puting two average scores : ( 1 ) the average of the   mutual diversity scores of every two of the beam   candidates , i.e. , DandD ; ( 2 ) the average   generation quality of the beam candidates , mea-   sured by BLEU scores . The results for paraphrase   generation are shown in Table 6 . In terms of gen-   eration quality , our model generates consistently   better beam candidates on average than the baseline   model . Besides , we can see that structural beam   search can yield more diverse beam candidates , in-   dicated by the higher mutual diversity ( i.e. , D   andD ) among beam candidates .   Effects of Accumulation Weight A larger accu-   mulation weight ( αin Eq . 13 ) indicates a larger   weight on previous decisions when re - ranking the   newly updated beam candidates . As a result , early   determined syntax structures are less likely to be   surpassed throughout the whole structural beam   search . On the contrary , a smaller αencourages the   model to explore promising candidates at higher   levels , and can therefore find more diverse hypothe-   ses . We explore the effects of αwith results shown   in Figure 4 . As the weight grows smaller , the model   generates sequences of better syntactic diversity ,   i.e. ,D. However , an overly small weight de-   teriorates generation quality ( iBLEU ) , which can   be caused by the model ’s overconfidence in local   predictions without considering the predictions of   syntax contexts at lower levels . Such deterioration   is also seen for overly large weights ( > 0.95 ) , due   to limited exploration at higher levels .   Human Evaluation We further conduct a hu-   man evaluation to evaluate generation quality and   diversity on paraphrase generation . We ask three   annotators to vote for one of the two candidates : hy-   potheses from the seq2seq baseline and our method .   The annotators are required to decide , which one is   better by considering Fidelity , Novelty , and Diver-   sity(See Appendix H for details ) . The results are   shown in Table 7 . As can be seen from the table ,   our method achieves much better generation nov-   elty and beam diversity compared with the baseline ,   while maintaining semantic fidelity , which further14102   validates the results of the automatic evaluation .   7 Conclusion   We proposed a syntax - guided generation paradigm ,   which leverages the strengths of Transformer and   generates sequences by hierarchically expanding   constituents in the lexicalized syntax contexts   throughout the syntax tree . The neural decoder was   trained by maximizing the likelihood of the infill-   ing texts for each constituent in the syntax contexts   given the source sequence . Moreover , we proposed   the structural beam search to better explore the hy-   pothesis space . Empirical results demonstrated the   advantage of generation quality over the seq2seq   baseline , and also the effectiveness in terms of in-   terpretability , controllability , and diversity .   Our method can be seen as a step towards ex-   plicit modelling of psycholinguistic structures dur-   ing neural text generation , helping the model to   have a degree of control over what it intends to gen-   erate , which can potentially address salient issues   of current neural NLG , such as hallucination ( Guer-   reiro et al . , 2023 ; Dziri et al . , 2022 ) and ethical   issues ( Sheng et al . , 2019 , 2021 ; Weidinger et al . ,   2021 ) , if semantics , pragmatics , and other factors   are also integrated .   Limitations   Despite the competitive performance , there are sev-   eral limitations of this work : ( 1 ) As discussed in   Section 6.1 , the generation performance relies on   the parser performance , which is strong enough   for English but still less satisfactory for other lan-   guages . Dedicated methods need to be considered   to compensate for the weak parser performance if   we want to extend our method to more languages .   ( 2 ) In this work , we consider two NLG tasks with   semantic equivalence to testify if the proposed   method can convey the source semantics accurately   by following the target syntactic grammar . Other   tasks such as summarization and dialogue genera-   tion can also be tested , where the semantics are not   equivalent between the source and target . ( 3 ) To   train the neural decoder parallelly , we break down   the source - target dataset into a triple set . However , the global dependency of the syntax parse tree is   not considered , which can deteriorate generation   performance . ( 4 ) Due to the recursive encoding of   the syntax contexts , our model ’s inference speed is   approximately half that of the seq2seq counterpart   ( Appendix E ) . ( 5 ) Future work should include ex-   periments on large language models ( Brown et al . ,   2020 ; OpenAI , 2023 ; Zeng et al . , 2022 ; Touvron   et al . , 2023 ; Taori et al . , 2023 ) . to further demon-   strate the effectiveness of our method beyond pre-   trained language models .   Ethics Statement   We honor the ACL Code of Ethics . No private data   or non - public information is used in this work . For   human annotation ( Section 6.3 and Section 6.4 ) ,   we recruited our annotators from the linguistics   departments of local universities through public   advertisement with a specified pay rate . All of   our annotators are senior undergraduate students or   graduate students in linguistic majors who took this   annotation as a part - time job . We pay them 60 CNY   an hour . The local minimum salary in the year 2022   is 25.3 CNY per hour for part - time jobs . The an-   notation does not involve any personally sensitive   information . The annotated is required to rank the   system output and label factual information ( i.e. ,   syntactic annotation ) .   Acknowledgement   We would like to thank all reviewers for their in-   sightful comments and suggestions to help im-   prove the paper . We thank Deng Cai and Xinting   Huang for their insightful suggestions . This work is   funded by the Ministry of Science and Technology   of China ( grant No . 2022YFE020038 ) .   References1410314104141051410614107Algorithm 1 Structural beam search   A Algorithms   The scoring algorithm 7 can be rewritten with the   source context xencoded into h :   score ( h , s , f ) = /summationdisplaylogp(f|h , s , f )   ( 15 )   The algorithm of structural beam search is   demonstrated in Algorithm 1 , which employs the   standard beam search for autoregressive generation ,   depicted in Algorithm 2 . The termination function   in Algorithm 1 ( i.e. , terminated ( · ) ) returns true if   the there is no remaining constituent in the input   sequence .   B Experiment Details   For NIST Zh - En , we use parts of the bitext pro-   vided within NIST’12 OpenMTand the final train   set consists of about 1.8 M sentence pairs . We ap-   ply BPE ( Sennrich et al . , 2016 ) on all datasets : the   number of BPE operations is 6 K for ParaNMT-   small , and 40 K for the other datasets . We imple-   ment our model using Fairseq ( Ott et al . , 2019).Algorithm 2 Beam search   We train the model using Adam ( Kingma and Ba ,   2015 ) optimizer . The learning rate increases to   7·10 in the first 10 K steps and then anneals   exponentially . We set the weight decay as 0.01   and label smoothing as 0.1 . The dropout is 0.3   for ParaNMT - small , and 0.1 for the other datasets .   The batch size is 64 K tokens for ParaNMT - small ,   256 K for WMT’16 Ro - En and NIST Zh - En , and   512 K for WMT’14 De ↔En . All models are trained   for a maximum update of 300 K steps unless early   stopped . We train the model using 4 V100s and in-   crease gradient accumulation steps for large batch   sizes . We choose the 5 best checkpoints based   on validation sets and average them for inference .   We set the beam width as 5 for beam search . For   machine translation , the teacher models for knowl-   edge distillation are Transformer_Base for NIST   Zh - En and WMT’16 Ro - en , and Transformer_Big   for WMT’14 De ↔En .   C Model Architecture   We conduct experiments to compare different   model architectures to incorporate syntax context   on the WMT’16 Ro - En validation set . We consider   the following settings :   •Concat : concatenate the syntax context with   the source sequence , with the vanilla Trans-   former unmodified .   •Extra - attention : reuse the source encoder for   encoding syntax context and insert an extra at-14108   tention layer , i.e. , the syntax context attention ,   into each decoder layer .   •Extra - encoder : introduce an additional en-   coder for encoding syntax context and also   uses the syntax context attention .   Empirical results are shown in Table 9 . Based   on validation results , we adopt the Extra - encoder   model in all experiments except for training on   BART ( Table 3 ) , where we adopt the Concat   model .   D Experiments on PLM   In this section , we introduce our experiment set-   tings of PLM . Following previous work ( Sun et al . ,   2021 ) , we use BART - base ( Lewis et al . , 2019 ) as   our base model . All models are finetuned for 10   epochs with a batch size of 64k tokens . The learn-   ing rate is 3e-5 and the linear decay schedule , as   recommended in BART ’s official repository .   We use the Concat ( Appendix C ) model archi-   tecture for extending our method to BART . The   source text and the syntax context are concatenated   with a special token “ < sep > ” , e.g. , “ I ate an apple   . < sep > < NP > < VP > . ” . To effectively employ our   method with BART , whose inputs are tokenized   sequences byte - level , as same as Radford et al . , we   make several modifications . In the pre - processing ,   we make sure our special tokens ( e.g. , < sep > , < c > ,   < NP > , < VP > ) are not split and add extra byte - level   spaces before and after the special token . Thanks to   the unused tokens in BART embeddings , we do not   need to modify the embedding matrix . Instead , we   assign our special tokens to unused token indexes .   Finally , in the inference stage , we find the con-   stituency expansion causes a discrepancy between   inputs of train and test . Thus , we first detokenize   each layer ’s outputs and then tokenize them back   with the same procedure in the preprocessing to   avoid such a gap .   E Generating Linearized Trees Directly   A baseline method to induce grammar simultane-   ously during generation is generating linearized   parse trees directly , i.e. , training a seq2seq model   which takes in source sequences and outputs lin-   earized parse trees . We compare it with our method   on WMT’16 Ro - En . Specifically , the BLEU score   for WMT’16 Ro - En is only 27.6 compared to the   seq2seq baseline ( 34.1 ) and our method ( 34.9 ) .   This can be because the additional parentheses and   constituency tags in linearized trees may deterio-   rate sequence coherence , making learning more   difficult . Our method , on the other hand , breaks   down syntax trees into level pieces to create a better   learning curriculum . Furthermore , Generating lin-   earized parse trees is much slower than the seq2seq   counterpart , since the average sequence length of   linearized tree sequences is longer ( 152.3 vs 28.4 ) .   As a result , the average speed for generating lin-   earized parse trees is only 0.8 sentences / s com-   pared to 3.6 sentences / s for the seq2seq baseline .   Our method achieves an inference speed of 1.7   sentences / s under the same computing condition   ( V100 GPU ) . Additionally , generating a linearized   parse tree is not easily interpretable or controllable ,   due to the black - box nature of the sequence - to-14109   sequence paradigm .   F Effects of Control Reward   The magnitude of the reward γdetermines how   much priority is given to beam candidates that   match the syntax exemplar . We experiment with   different reward values to give a quantitative   demonstration , shown in Figure 5 . It can be seen   that the control effectiveness grows with the in-   crease of the reward value until 0.64 , which sug-   gests that all possible matched beam candidates are   re - ranked to the top in the search space .   G Control with Partial Syntax Template   We present 3 sample cases to demonstrate fine-   grained controls over the generation process ,   shown in Figure 6 . Each Chinese source sentence   is paired with 3 manual controls from three anno-   tators . The model takes in the annotated syntax   context and proceeds to obtain the respective trans-   lations .   H Human Evaluation for Paraphrase   Generation   We ask three annotators to conduct side - by - side   human evaluations and report averaged results of   their annotations . For each instance , the annotators   vote for one of the two outputs by the baselineand our model . The outputs contain top-5 beam   candidates under beam search . The annotators are   asked to evaluate both the best candidate and the   beam results as a whole , based on the following   three aspects :   •Fidelity : Whether the best candidate is   semantics - equivalent with the input .   •Novelty : Whether the best candidate modifies   the input sentence structure .   •Diversity : Whether the generated five candi-   dates are different from each other given the   input.14110ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section Limitations .   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section Abstract and Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Not applicable . Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4 .   C / squareDid you run computational experiments ?   Section 4 & 5 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix B & C.14111 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4 ; Appendix B & C.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Not applicable . Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 5 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 6.4 & 6.4 .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix G & H.   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Section Ethics Consideration .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Appendix G & H.   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Section Ethics Consideration.14112