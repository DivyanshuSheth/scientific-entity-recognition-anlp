  Li Lucy , Divya Tadimeti , and David Bamman   University of California , Berkeley   { lucy3_li , dtadimeti , dbamman}@berkeley.edu   Abstract   A common paradigm for identifying seman-   tic differences across social and temporal con-   texts is the use of static word embeddings and   their distances . In particular , past work has   compared embeddings against “ semantic axes ”   that represent two opposing concepts . We ex-   tend this paradigm to BERT embeddings , and   construct contextualized axes that mitigate the   pitfall where antonyms have neighboring repre-   sentations . We validate and demonstrate these   axes on two people - centric datasets : occupa-   tions from Wikipedia , and multi - platform dis-   cussions in extremist , men ’s communities over   fourteen years . In both studies , contextual-   ized semantic axes can characterize differences   among instances of the same word type . In the   latter study , we show that references to women   and the contexts around them have become   more detestable over time .   1 Introduction   Warning : This paper contains content that may be   offensive or upsetting .   Quantifying and describing the nature of lan-   guage differences is key to measuring the impact   of social and cultural factors on text . Past work has   compared English embeddings for people to adjec-   tives or concepts ( Garg et al . , 2018 ; Mendelsohn   et al . , 2020 ; Charlesworth et al . , 2022 ) , or projected   embeddings against axes representing contrasting   attributes ( Turney and Littman , 2003 ; An et al . ,   2018 ; Kozlowski et al . , 2019 ; Field and Tsvetkov ,   2019 ; Mathew et al . , 2020 ; Kwak et al . , 2021 ; Lucy   and Bamman , 2021b ; Fraser et al . , 2021 ; Grand   et al . , 2022 ) . Static representations for the same   word can also be juxtaposed across corpora that   reflect different time periods ( Gonen et al . , 2020 ;   Hamilton et al . , 2016 ) . This paradigm of using em-   bedding distances to uncover socially meaningful   patterns has also transferred over to studies that   measure biases in contextualized embeddings , such   as Wolfe and Caliskan ( 2021 ) ’s finding that BERTFigure 1 : An axis is constructed using embeddings of   adjectives in selected contexts . These contexts are pre-   dictive of synonyms , but not antonyms , of the target   adjective during masked language modeling . Token-   level embeddings for people are then projected onto this   axis .   embeddings of less frequent minority names are   closer to words related to unpleasantness .   The use of “ semantic axes ” is enticing in that it   offers an interpretable measurement of word differ-   ences beyond a single similarity value ( Turney and   Littman , 2003 ; An et al . , 2018 ; Kozlowski et al . ,   2019 ; Kwak et al . , 2021 ) . Words are projected onto   axes where the poles represent antonymous con-   cepts ( such as beautiful – ugly ) , and the projected   embedding ’s location along the axis indicates how   similar it is to either concept . Semantic axes con-   structed using static , type - based embeddings have   been used to analyze socially meaningful differ-   ences , such as words ’ associations with class ( Ko-   zlowski et al . , 2019 ) , or gender stereotypes in nar-   ratives ( Huang et al . , 2021 ; Lucy and Bamman ,   2021b ) .   Our work investigates the extension and appli-   cation of semantic axes to contextualized embed-   dings . We present a novel approach for construct-   ing semantic axes with English BERT embeddings   ( Figure 1 ) . These axes are built to encourage self-   consistency , where antonymous poles are less con-   flated with each other . They are able to capture   semantic differences across word types as well as   variation in a single word across contexts . Their   ability to differentiate contexts makes them suitable3477for studying how a word changes across domains   or across individual sentences . These axes are also   more self - consistent and coherent than ones created   using GloVe and other baseline approaches .   We demonstrate the use of contextualized axes   on two datasets : occupations from Wikipedia , and   people discussed in misogynistic online commu-   nities . We use the former as a case where terms   appear in definitional contexts , and characteristics   of people are well - known . In the latter longitudi-   nal , cross - platform case study , we examine lexical   choices made by communities whose attitudes to-   wards women tend to be salient and extreme . We   chose this set of online communities as a substan-   tive use case of our method , in light of recent at-   tention in web science on analyzing online extrem-   ism and hate at scale ( e.g. Ribeiro et al . , 2021b , a ;   Aliapoulios et al . , 2021 ) . There , we analyze lan-   guage change and variation along axes through a   sociolinguistic lens , emphasizing that speakers use   language that reflects their social identities and be-   liefs ( CH - Wang and Jurgens , 2021 ; Huffaker and   Calvert , 2017 ; Card et al . , 2016 ; Lakoff and Fergu-   son , 2006 ) .   Our code , vocabularies , and other resources can   be found in our Github repo : https://github.c   om / lucy3 / context_semantic_axes .   2 Constructing semantic axes   Static embeddings . Several formulae for calculat-   ing the similarity of a target word to two sets of   pole words have been proposed in prior work on   static semantic axes . These differ in whether they   take the difference between a target word ’s simi-   larities to each pole ( Turney and Littman , 2003 ) ,   calculate a target word ’s similarity to the differ-   ence between pole averages ( An et al . , 2018 ; Kwak   et al . , 2021 ) , or calculate a target word ’s similar-   ity to the average of several word pair differences   that represent the same antonymous relationship   ( Kozlowski et al . , 2019 ) . We build on the approach   of An et al . ( 2018 ) and Kwak et al . ( 2021 ) , be-   cause it does not require us to curate multiple   paired antonyms for each axis , and it draws out   the difference between two concepts before a tar-   get word is compared to them , rather than after .   We define an axis Vcontaining antonymous sets   of adjective vectors , S={l , l , l , ... , l}and   S={r , r , r , ... , r } , as the following :   V=1   nl−1   mr . Relying on single - word poles for axes can be un-   stable to the choice of each word ( An et al . , 2018 ;   Antoniak and Mimno , 2021 ) . An et al . ( 2018 ) cre-   ates a pole ’s set of words using the nearest neigh-   bors of a seed word , which may risk conflating   unintended meanings or antonymous neighbors   ( Mrkši ´ c et al . , 2016 ; Sedoc et al . , 2017 ) . For exam-   ple , one axis uses the opposite seed words green   andexperienced , but green ’s nearest neighbors in-   clude redrather than inexperienced . Instead of us-   ing this nearest neighbors approach , we construct   poles using WordNet antonym relations . Each end   of an axis aggregates synonymous and similar lem-   mas in WordNet synsets , which are expanded using   thesimilar to relation ( Miller , 1992 ) .   Our type - based embedding baseline , ,   uses 300 - dimensional GloVe vectors pretrained on   Wikipedia and Gigaword ( Pennington et al . , 2014 ) .   We only keep poles where both sides have at least   three adjectives that appear in the GloVe vocab-   ulary , and we also exclude acronyms , which are   often more ambiguous in meaning . We start with   723 axes , where poles have on average 9.63 adjec-   tives each .   Contextualized embeddings . Static embed-   dings , however , present a number of limitations .   Such embeddings can not easily handle polysemy   or homonymy ( Wiedemann et al . , 2019 ) , and even   when they are trained on different social or tem-   poral contexts , they require additional steps to be   aligned ( Gonen et al . , 2020 ) . Context - specific em-   beddings also need enough training examples of   target words to create usable representations . These   limitations prevent the analysis of token - based se-   mantic variation , such as measuring how one men-   tion of a word is more or less beautiful than another .   Our main contribution of contextualized axes uses   the same WordNet - based formulation as our GloVe   baseline . Rather than each word in SorSbeing   represented by a single GloVe embedding , we ob-   tain BERT embeddings over multiple occurrences   of each adjective . We use BERT - base , as this model   is small enough for efficient application on large   datasets and is popular in previous work on seman-   tic change and differences ( e.g. Hu et al . , 2019 ;   Lucy and Bamman , 2021a ; Giulianelli et al . , 2020 ;   Zhou et al . , 2022 ; Coll Ardanuy et al . , 2020 ; Mar-   tinc et al . , 2020 ) . It is also used in tutorials for   researchers outside of NLP , which means it has   high potential use in computational social science   and cultural analytics ( Mimno et al . , 2022).3478For contextualized axes , we obtain a potential   pool of contexts for adjectives sampled over all of   Wikipedia from December 21 , 2021 , preprocessed   using Attardi ( 2015 ) ’s text extractor . This sample   contains up to 1000 sentences , or contexts , that   contain each adjective , and we avoid contexts that   are too short ( over 10 tokens ) or too long ( over 150   tokens ) .   We experiment with two methods of obtaining   contextualized BERT embeddings for each adjec-   tive : a random “ default " ( - ) and one   where contexts are picked based on word probabili-   ties ( - ) . For - , we take a   random sample of 100 contextualized embeddings   across the adjectives in each pole . Since words   can be nearest neighbors with their antonyms in   semantic space ( Mrkši ´ c et al . , 2016 ; Sedoc et al . ,   2017 ) , our main approach , - , aggregates   word embeddings over contexts that highlight con-   trasting meanings of axes ’ poles .   To select contexts , we mask out the target ad-   jective in each of its 1000 sentences , and have   BERT - base predict the probabilities of synonyms   and antonyms for that masked token . We remove   contexts where the average probability of antonyms   is greater than that of synonyms , sort by average   synonym probability , and take the top 100 contexts .   One limitation of our approach is that predictions   are restricted to adjectives that can be represented   by one wordpiece token . If none of the words on a   pole of an axis appear in BERT ’s vocabulary , we   backoff to - to represent that axis .   For each axis type , we also have versions where   words ’ embeddings are z - scored , which has been   shown to improve BERT ’s alignment with humans ’   word similarity judgements ( Timkey and van Schi-   jndel , 2021 ) . For z - scoring , we calculate mean and   standard deviation BERT embeddings from a sam-   ple of around 370k whole words from Wikipedia .   As recommended by Bommasani et al . ( 2020 ) , we   use mean pooling over wordpieces to produce word   representations when necessary , and we extend this   approach to create bigram representations as well .   These embeddings are a concatenation of the last   four layers of BERT , as these tend to capture more   context - specific information ( Ethayarajh , 2019 ) .   3 Internal validation   We internally validate our axes for self - consistency .   For each axis , we remove one adjective ’s embed-   dings from either side , and compute its cosine sim-   ilarity to the axis constructed from the remaining   adjectives . For BERT approaches , we average the   adjective ’s multiple embeddings to produce only   one before computing its similarity to the axis . In   a “ consistent ” axis , a left - out adjective should be   closer to the pole it belongs to . That is , if it be-   longs to S , its similarity to the axis should be pos-   itive . We average these leave - one - out similarities   for each pole , negating the score when the adjective   belongs to S , to produce a consistency metric , C.   Table 1 shows Cfor different axis - building meth-   ods . An axis is “ consistent ” if both of its poles   haveC ≥0 . ’s most inconsistent axis poles often in-   volve directions , such as east↔west , left - handed   ↔right - handed , and right↔left . These concepts   may be difficult to learn from text without ground-   ing . We find that the various BERT approaches ’   most inconsistent axes include direction - related   ones as well , but they also struggle to separate   concepts such as lower - class ↔upper - class .   The best method for producing consistent axes   isz - scored - , with a significant dif-   ference in Cfrom z - scored - and ( Mann - Whitney U - test , p < 0.001 ) . It also   produces the highest number of consistent axes .   G presents itself as a formidable baseline ,   and - struggles in comparison to it .   4 External validation   Previous work on static semantic axes validates   them using sentiment lexicons , exploratory anal-3479   yses , and human - reported associations ( An et al . ,   2018 ; Kwak et al . , 2021 ; Kozlowski et al . , 2019 ) .   We perform external validation of self - consistent   axes on a dataset where people appear in a variety   of well - defined and known contexts : occupations   from Wikipedia . We conduct two main experi-   ments . In the first , we test whether contextualized   axes can detect differences across occupation terms ,   and in the second , we investigate whether they can   detect differences across contexts .   4.1 Data   We collect eleven categories of unigram and bigram   occupations from Wikipedia lists : Writing , Enter-   tainment , Art , Health , Agriculture , Government ,   Sports , Engineering , Science , Math & Statistics ,   and Social sciences ( Appendix A ) . The number of   occupations per category ranges from 3 in Math &   Statistics to 48 in Entertainment , with an average of   27.2 . We use the MediaWiki API to find Wikipedia   pages for occupations in each list if they exist and   follow redirects when necessary ( e.g. Blogger redi-   rects to Blog ) . For each occupation ’s singular form ,   we extract sentences in its page that contains it . In   total , we have 3,015 sentences for 300 occupations .   4.2 Term - level experiment ( occupations )   Each occupation is represented by a pre - trained   GloVe embedding or a BERT embedding averaged   over all occurrences on its page . If an axis uses   z - scored adjective embeddings , we also z - score   the occupation embeddings compared to it . We   assign poles to occupations based on which side   of the axis they are closer to via cosine similarity .   Top poles are highly related to their target occupa-   tion category , as seen by the examples for z - scored - in Table 2 .   One limitation for interpretability is that word   embeddings ’ proximity can reflect any type of se-   mantic association , not just that a person actually   hasthe attributes of an adjective . For example ,   adjectives related to unhealthy are highly associ-   ated with Health occupations , which can be ex-   plained by doctors working in environments where   unhealthiness is prominent . Therefore , embedding   distances only provide a foggy window into the na-   ture of words , and this ambiguity should be consid-   ered when interpreting word similarities and their   implications . This limitation applies to both static   embeddings and their contextualized counterparts .   We conduct human evaluation on this task of   using semantic axes to differentiate and charac-   terize occupations . Three student annotators ex-   amined the top three poles retrieved by each axis-   building approach and ranked these outputs based   on semantic relatedness to occupation categories   ( Appendix B ) . These annotators had fair agree-   ment , with an average Kendall ’s Wof 0.629 across   categories and experiments . Though is a   competitive baseline , z - scored - is the   highest - ranked approach overall ( Table 3 ) . This   suggests that more self - consistent axes also pro-   duce measurements that better reflect human judge-   ments of occupations ’ general meaning.34804.3 Context - level experiment ( person )   The identity of a word , and prior associations   learned from BERT ’s training data , have the po-   tential to overpower its in - context use ( Field and   Tsvetkov , 2019 ) . Thus , we may want to discount   word associations originally learned by BERT   when we examine the use of a target word in a   narrower context . Prior work has shown that words   with higher frequency in BERT ’s training data tend   to encode more context - specific information in   their embeddings ( Ethayarajh , 2019 ; Zhou et al . ,   2021 ; Wolfe and Caliskan , 2021 ) . To investigate   whether contextualized axes can measure context   changes for people , we replace all occupation bi-   grams and unigrams with person , a very common   word . This also makes contexts across different   words comparable to each other , a property which   we will leverage later in Section 5.4 .   Each person embedding is averaged over one   occupation ’s contexts . The identity of person tends   to overpower its similarity to axes across contexts ,   in that the top - ranked poles are similar across oc-   cupation categories . So , in contrast to the previous   occupation experiment , additional steps are needed   to draw out meaningful differences in how person   is used in one group of contexts from its typical use .   To do this , we estimate the average cosine similar-   ity to axes of nperson embeddings in occupational   contexts using 1000 bootstrapped samples , where   nis the number of terms in an occupation category .   We take the axes with the highest statistically sig-   nificant ( p < 0.001 , one - sample t - test ) difference   in cosine similarity .   We assume that occupations ’ Wikipedia pages   mention them within definitional contexts , so top-   ranked poles should reflect the original occupation   replaced by person . These top poles are less intu-   itive than those outputted by the earlier term - level   experiment ( Table 2 ) . Still , in some cases , such as   for Government and Math & Statistics occupations ,   we uncover relative differences that distinguish one   category from others . We only show three adjec-   tives in the top two poles in Table 2 due to space   considerations , but moving further down the list for   z - scored - uncovers additional meaning-   ful poles . For example , the pole spry , gymnastic ,   sporty is the third most prominent shift and highest   similarity increase ( + ) in the person experiment   for Sports occupations . In addition , human evalu-   ators preferred - over other approaches   ( Table 3 , Appendix B).5 Measuring change and variation   Now that we have contextualized semantic axes   that can measure differences across words and con-   texts , we apply them onto a domain that can show-   case salient and socially meaningful variation . NLP   research on harmful language often employs meth-   ods that focus on the target group , such as measur-   ing their association with other words ( Zannettou   et al . , 2020 ; Garg et al . , 2018 ; Tahmasbi et al . , 2021 ;   Field and Tsvetkov , 2019 ) , or with biases in models   ( Wolfe and Caliskan , 2021 ; Ghosh et al . , 2021 ) . We   illustrate the application of self - consistent z - scored - axes onto the manosphere , which is a   collection of communities with mostly male users   who hold alternative beliefs around relationships   and gender . We use the same axes we presented   earlier , which were created using Wikipedia data ,   because Wikipedia provides more normative cov-   erage of a variety of adjectives than topic - specific   communities . This way , we examine how entities   in the manosphere orient themselves against typical   adjectival uses and meanings .   The manosphere has been linked to acts of vio-   lence in the physical world ( Hoffman et al . , 2020 ) ,   and most members believe that men are systemi-   cally disadvantaged in society ( Van Valkenburgh ,   2021 ; Marwick and Caplan , 2018 ; Lin , 2017 ; Ging ,   2019 ) . These communities focus on heterosexual   relationships and masculinity , and feature a dy-   namic linguistic landscape . Much prior work on   the manosphere has been qualitative , such as ethno-   graphies ( Lin , 2017 ; Lumsden , 2019 ; Van Valken-   burgh , 2021 ) . There have been a few quantitative   analyses of their language , usually focusing on   phrase and word frequencies in a few communities   ( Farrell et al . , 2019 ; Gothard et al . , 2021 ; LaVio-   lette and Hogan , 2019 ; Jaki et al . , 2019 ) . As an ex-   ample involving word vectors , Farrell et al . ( 2020 )   uses static embeddings identify the meanings of   incels ’ neologisms by inspecting words ’ nearest   neighbors .   Our case study extends beyond prior work with   its methodology and scale . We use contextualized   semantic axes to tackle one question : how have   references to women and contexts around them   changed over fourteen years ?   5.1 Data   We use a taxonomy of subreddits and external fo-   rums described by Ribeiro et al . ( 2021a ) , who show   that the manosphere began with ideologies such as3481pick - up artists ( PUA ) and Men ’s Rights Activists   ( MRA ) , and evolved into more extreme ones such   as The Red Pill ( TRP ) , incels ( short for involun-   tary celibate ) and Men Who Go Their Own Way   ( MGTOW ) , with users moving from older to newer   ideologies . We call this dataset _ ,   because it contains extreme views of relationships .   We use Reddit posts and comments from March   2008 to December 2019 from subreddits listed in   Ribeiro et al . ( 2021a ) ’s study , downloaded from   Pushshift ( Baumgartner et al . , 2020 ) . We slightly   modify their taxonomy by separating out incel sub-   reddits where the intended userbase are women   ( femcels ) , and also include a newer set of subred-   dits focused on “ Female Dating Strategy " ( FDS ) , a   women - led community analogous to TRP ( Holden ,   2020 ; Clark - Flory , 2021 ) . Therefore , we have   60 subreddits in seven ideological categories : In-   cels , MGTOW , PUA , MRA , TRP , FDS , and Fem-   cels(Appendix C ) . This Reddit subset of- _ contains over 1.3 billion tokens .   We also include seven external forums provided   by Ribeiro et al . ( 2021a ) . These public forums   include A V oice for Men ( A VFM ) , Master Pick - up   Artist ( MPUA ) Forum , The Attraction , incels.co ,   MGTOW Forum , RooshV , and Red Pill Talk . This   forum subset of _ contains over 800   million tokens spanning November 2005 to June   2019 , and we remove duplicates and quoted text   from posts .   Some experiments use a subset of Reddit that   shares a similar topical focus as _ ,   but may have more mainstream views of women   and relationships . We use a listof com-   mon “ Relationship ” subreddits : r / relationships ,   r / dating , r / relationship_advice , r / dating_advice ,   and r / breakups . We call this dataset- _ , and it contains 1.2 billion tokens from   September 2009 to December 2019 . For Reddit   data , we do not use posts and comments written   by usernames who have bot - like behavior , which   we define as repeating any 10 - gram more than 100   times.5.2 Vocabulary   We use a mix of NER , online glossaries , and man-   ual inspection to curate a unique vocabulary of   people ( details in Appendix D ) . This vocabulary   has 2,434 unigrams and 4,179 bigrams , tokenized   using BERT ’s tokenizer without splitting words   into wordpieces ( Devlin et al . , 2019 ; Wolf et al . ,   2020 ) . These terms appear at least 500 times in _ .   Since gender is central to the manosphere , we   infer these labels based on terms ’ social gender in   a dataset . For example , accuser is not semantically   gendered like girlandwoman , but its social gen-   der , estimated using pronouns , is more feminine in _ than _ . We use two   stages of gender inference to account for pronoun   sparsity and noise . First , we use a list of semanti-   cally gendered nouns , and second , we use feminine   and masculine pronouns linked to terms via coref-   erence resolution ( details in Appendix E ) . We label   each vocabulary term based on its fraction of co-   occurring feminine pronouns in _   and _ , separately . We are able to   label 72.5 % of the vocabulary in _   and 67.0 % of it in _ .   5.3 Term - level change   Contextualized semantic axes can reveal how   word and phrase types change over time . Here ,   our analyses focus on 1,482 feminine ( gender-   leaning > 0.75 ) terms in _ . To cap-   ture broad snapshots of words ’ use , we randomly   sample up to 500 sentence - level occurrences of   each term in each platform and ideology ( e.g. a   specific forum or Reddit category ) in each year .   Overall z - scored BERT embeddings for each vo-   cab word are averages over this stratified sample of   its contexts .   The history of the manosphere is characterized   by waves of different ideological communities   ( Ribeiro et al . , 2021a ) . To reflect this character-   ization through language , we segment our vocabu-   lary based on when terms peak in popularity . We   cluster normalized frequency time seriesfor each   term using K - Spectral Centroid clustering ( KSC )   ( Yang and Leskovec , 2011 ) . We use their default   parameters , including K= 6 . In contrast to their   original approach , our symmetric distance measure3482   ˆdis invariant to scaling by αbut not to the transla-   tion of the time series , so that peaks earlier in time   are not clustered with those later in time :   ˆd(x , y ) = ||x−αy||   ||x|| ,   where α = xy/||y|| .   “ Waves " of term types for people correspond to   ideological change . Figure 2 shows examples of   feminine terms , but the top masculine terms are   often labels of ideological groups , such as mgtow   andincels , which we use to estimate which clusters   align with ideological up and downturns . Clus-   ter A and cluster D tend to have terms that have   widespread use .   We examine the shifts of high variance , sub-   stantive axes across temporal clusters . High vari-   ance axes include those related to gender , appear-   ance , and desirability ( Table 4 ) . For example , the   lovable versus detestable pole contrasts beautiful   girls with degenerate whores . As another exam-   ple , the axis for clean versus dirty contrasts loyal   wifewith harlots . Prior studies using toxicity detec-   tion and lexicon - based approaches found that hate   and misogyny rose with the arrival of later MG-   TOW and incel communities ( Farrell et al . , 2019 ;   Ribeiro et al . , 2021a ) . Similarly , we find that lex-   ical choices for women are more detestable and   dirty in later waves associated with MGTOW and   incels ( Figure 3 ) . Often , low and high frequency   words share similar patterns in each wave .   5.4 Context - level change   Contextualized semantic axes can reveal how   the contexts around people have changed over   time . Women in online communities can be ref-   erenced in a variety of ways ( Figure 2 ) . To com-   pare overall changes around women between main-   stream and extremist communities , we examine the   contexts around feminine ( gender - leaning > 0.75 )   words . We use instances of 287 unigram types ,   since bigrams can include modifiers that would   be considered “ context ” . As discussed earlier ,   word identities impact measurements of contex-   tual changes across them ( Section 4.3 ) . We replace   each target word with person orpeople depend-   ing on whether it is singular or plural , estimated   through the Python package . We choose3483   replacements to respect singular / plural forms to   ensure ecological validity and not perturb BERT ’s   sensitivity to grammaticality ( Yin et al . , 2020 ) . We   use reservoir sampling to obtain up to 1000 oc-   currences of person - orpeople -replaced feminine   words in each month on _ and- _ .   In comparison to _ , - _ has more detestable , sickening ,   anddirty contexts for women ( Figure 4 ) . Both _ and _ discuss   relationship issues , but contextualized axes   reveal how contrasting and changing attitudes   toward women can influence context . Negative   associations especially peak during the height   of the incels ’ movement around late 2017 to   mid 2019 . These persist despite Reddit ’s ban of   r / incels in November 2017 and the quarantine of   r / braincels and r / theredpill in September 2018 .   Thus , the widespread efficacy of community - level   moderation is worthy of closer study ( e.g. Copland ,   2020 ; Ribeiro et al . , 2021b ) . An advantage of   computing scores at the token - level rather than   at the type - level is interpretability . That is , one   can see which contexts land at the extreme ends of   axes ( as illustrated in Table 5 ) .   Contextualized semantic axes can also illu-   minate differences among lexical variables , or   different linguistic forms that share the same refer-   ential meaning ( Nguyen et al . , 2021 ; Labov , 1972 ) .   As prominent examples , men - led communities use   the lexical innovations femoids andfoids , which   are shortenings of female humanoids , as dehuman-   izing words for all women ( Chang , 2020 ; Pra ˙zmo ,   2020 ) . Two women - led communities , Femcels and   FDS , use moids as an analogous way to refer to   men . Prior work studying three manosphere sub-   reddits showed that the lemmas woman andgirl   are constructed negatively as immoral , deceptive ,   incapable and insignificant ( Krendel , 2020 ) . We hy-   pothesize that the contexts of community - specific   variants should have even more dehumanizing con-   notations along similar dimensions . In this exper-   iment , we replace all terms ( men , moids , foids ,   femoids , and women ) with people .   We sample up to 100 occurrences of each variant   in each platform and ideology per year , limiting   time ranges to when domain - specific variants are   widely used by their home community . We exam-   ine the use of variants for men by Femcels and FDS   in 2018 - 2019 , and the use of variants for women by   all other communities in _ in 2017-   2019 . Unlike in the person experiment for occupa-   tions , we have substantial pools of occurrences to   compare . Thus , to find axes that distinguish one3484   variant from another , we use axis scores as features   in random forest classifiers ( Pedregosa et al . , 2011 ) ,   and perform binary classification of word identity :   women versus foids orfemoids , and men versus   moids ( Appendix G ) . We rank axes based on their   feature importance , and select three highly ranked   and relevant axes to show in Figure 5 . Shifts along   these axes confirm our hypothesis that community-   specific variants are more dehumanized than their   widely - used counterparts .   6 Conclusion   In this work , we examine the capability of contex-   tualized embeddings for discovering differences   among words and contexts . Our method uses pre-   dicted word probabilities to pinpoint which con-   texts to include when aggregating BERT embed-   dings to construct axes . This approach creates more   self - consistent axes that better fit different occupa-   tion categories , in comparison to baselines . We   further demonstrate the use of these axes in a lon-   gitudinal , cross - platform case study . Overall , con-   textualized embeddings offer more flexibility and   granularity compared to static ones for the analysis   of content across time and communities . That is ,   rather than train static word embeddings for var-   ious subsets of data , we can characterize change   and variation at the token - level .   Though we focus on analyzing associations be-   tween adjectives and people , our approach can gen-   eralize to other types of entities as well . Measuring   and comparing the contexts of other entity types   should include many of the same considerations we   did , such as reducing the conflation of antonyms ,   controlling for word identity by replacing target   words with a shared hypernym , and experimenting   withz - scoring . Future work includes understand-   ing why some opposing concepts are conflated in   large language models , and how a word embed - ding ’s identity influences its encoding of contexts .   7 Limitations   Aside from computing power requirements ( Ap-   pendix H ) , we outline a few additional limitations   of our methodology and its application not dis-   cussed in the main text .   Domain shift . The use of pretrained BERT on   a niche set of communities makes our approaches   susceptible to domain shift , such as rare words   having less robust embeddings ( Zhou et al . , 2022 ,   2021 ) , or target words carrying over learned asso-   ciations from a broader corpus that are less appli-   cable in a narrower one . Domain shift is difficult   to avoid without retraining or further pretraining   BERT , which is resource - intensive , may risk catas-   trophic forgetting , and inaccessible to some disci-   plines in computational social science ( Gururangan   et al . , 2020 ; Ramponi and Plank , 2020 ; Goodfellow   et al . , 2014 ) . Also , training a large language model   on text with toxic and misogynistic origins intro-   duces additional risk of dual use ( Kurenkov , 2022 ) .   We suggest some potential workarounds that lessen   the severity of domain shift , such as replacing tar-   get words with common ones for context - focused   analyses .   WordNet . WordNet is a popular lexical resource   for NLP , but its senses for words can be overly fine-   grained ( Pilehvar and Camacho - Collados , 2019 )   and not suitable for all domains . We use WordNet   version 3.0 , which is included in NLTK , and this   version was last updated in 2006 . Since English is   constantly changing , some synonym and antonym   relations may be outdated .   Errors . Our method for drawing out differences   in words is better than common baselines yet still   imperfect , and some of the opposing concepts in   embedding space that BERT struggles to separate   may be important for an application domain . There-   fore , domain expertise is needed to recognize spu-   rious patterns from real ones and fill these gaps .   In the main text we mention that embeddings   offer a “ foggy window ” into how two concepts   may be associated or related , and the exact type   of relation is not always clear . For example , if   contexts for women are closer to unpleasant , does   it mean that the text discusses unpleasant events   that affect women , or that the writers believe that   women are unpleasant , or both ? Some of this uncer-   tainty could be resolved qualitatively by inspecting3485sentences at poles ’ extremes . We compare embed-   dings for people to axes , but it is also possible   to include relation - based approaches such as de-   pendency parsing and compare words that share   specific relations with people to axes ( e.g. Lucy   and Bamman , 2021b ) . One trade - off of doing this   is that informative verbs and adjectives connected   to mentions of target groups can be sparse . Our   method is able to find that mathematician replaced   with person is highly similar to calculable in a vari-   ety of sentence structures , such as this one modified   off Wikipedia : Aperson is someone who uses an   extensive knowledge of mathematics in their work ,   typically to solve mathematical problems .   8 Ethical considerations   User privacy . Online data opens many doors for   research , but its use raises concerns around user   privacy . For our use case , we believe that the ben-   efits of our work outweigh privacy - related harms .   Consent is infeasible to obtain for large datasets   ( Buchanan , 2017 ) , and in the manosphere , it is un-   likely that users would give consent , especially if   the researchers using their data believe that their   ideologies are harmful and wrong . Obtaining con-   sent would pose risks to the safety of the researcher   ( Conway , 2021 ; Doerfler et al . , 2021 ) .   All online discussions included in our work were   public when downloaded by their original curators ,   mainly Baumgartner et al . ( 2020 ) and Ribeiro et al .   ( 2021a ) . Some forums and online glossaries were   relocated , shutdown , banned , or made private later   on . A user ’s “ right to be forgotten ” confronts re-   searchers who have interests in documenting and   studying the histories of communities . We truncate   the examples shown in our paper rather than use   them in full verbatim ( Bruckman , 2002 ) .   Communities may expect their posts to stay   within their in - group , but the content in our work   was posted on public platforms . This publicness   and increased visibility plays a key role in how this   content impacts others , such as those who view this   information and propagate it elsewhere , or those   who are direct targets of hate . Common targets   such as women and people of color carry a bigger   burden when participating in online spaces ( Hoff-   mann and Jonas , 2017 ) , and our broader research   agenda aims to mitigate this issue .   Social biases in models and resources . We use   WordNet to group similar adjectives into semantic   axes , but we observe some socially harmful asso - ciations in this resource . For example , gross and   fatare listed as similar lemmas . As another exam-   ple , WordNet conflates gender and sexuality when   androgynous andbisexual are also listed as similar   lemmas . The BERT language model , like all large ,   pretrained models , is also susceptible to social bi-   ases in its training data ( Bender et al . , 2021 ) .   Gender inference . In this paper ’s main case   study , we perform gender inference for word and   phrase types . This step was necessary to study   how women are portrayed over time , which is a   key question due to the centrality of misogyny in   these communities . However , perfect prediction of   each word ’s perceived gender in our dataset using   pronouns is impossible ( Cao and Daumé III , 2021 ) .   Not all mentions of people co - occur with pronouns ,   pronouns do not equate gender , and coreference   resolution systems can produce errors . So , we ap-   proximate the social gender of terms by aggregat-   ing coreference patterns over all instances of that   term . Since it is difficult to separate noisy errors   from meaningful word - level pronoun variation at   scale , we had to use a score threshold to pinpoint   what words were feminine - leaning enough to be   included in our analyses .   Restricting pronouns to the traditional binary of   feminine and masculine is limiting , since individu-   als use other pronouns as well . They /them pronouns   are predominantly used to reference plural terms in   this dataset , and the coreference model we use does   not handle neopronouns . The manosphere and the   typical framing under which it is studied is heavily   cisheteronormative . We use a frequency cutoff to   determine our vocabulary ( Appendix D ) , so refer-   ences to transgender and nonbinary people may be   filtered out . V ocab terms retained for transgender   people are outdated or typically offensive terms   such as transsexuals andtransgenders , and no vo-   cab term includes non - binary , nb , ornonbinary .   9 Acknowledgements   We thank Manoel Horta Ribeiro for sharing his   dataset and materials for our case study , and Sam   Robertson , Alexus Lopez , and Harold Cha for eval-   uating model outputs . In addition , we are grateful   for feedback provided by Nicholas Tomlin , Kaitlyn   Zhou , and our anonymous reviewers . This research   was supported by funding from the National Sci-   ence Foundation ( DGE-1752814 , IIS-1813470 , and   IIS-1942591).3486References3487348834893490   A Wikipedia page titles   Table 6 lists the categories of occupations , the titles   of Wikipedia pages that list them , and the number   of terms in each category . These lists were retrieved   in February 2022 .   B Human evaluation for occupations   We recruited three student volunteers with familiar-   ity with NLP coursework and tasks to rank the top   poles provided by each axis - building method for   our occupation and person experiments . We used   Qualtrics to design and launch the survey . Since   we were not asking about personal opinions but   rather evaluating models , we were determined ex-   empt from IRB review by the appropriate office at   our institution . Each question pertains to a specific   occupation category , and within each experiment ,   question order and answer option order are ran-   domly shuffled . Each model option is presented   with its top three poles , in order of most to less   relevant . Figure 6 shows screenshots of instruc-   tions . In the toy example , the options are labeled   with “ Model A " , “ Model B " , “ Model C " , to al-   low explanation clarity , but in the actual task ques-   tions , options are not labeled with model letters   to avoid biasing the evaluators towards a specific   model . Some annotators expressed that the task   was difficult , and for some occupations , different   approaches output similar axes , just in different   order .   C Reddit communities   We used a list of subredditsfor the manosphere   provided by ( Ribeiro et al . , 2021a ) in their detailed,3491data - driven sketch of the manosphere .   Five of the subreddits included in Ribeiro et al .   ( 2021a ) ’s taxonomy of the Reddit manosphere   ( r / malecels , r / lonelynonviolentmen , r/1ncels ,   r / incelbrotherhood , r / incelspurgatory ) were not   on Pushshift ’s dump of Reddit . We curated   the list of communities for our new ideological   category , Female Dating Strategy ( FDS ) , using a   now removed list of FDS ’s “ sister communities "   on the subreddit r / FemaleDatingStrategy ’s   sidebar : r / PinkpillFeminism , r / AskFDS ,   r / FDSSuperFans , r / PornFreeRelationships ,   and r / FemaleLevelUpStrategy . The Femcels set of   subreddits include : r / Trufemcels , r / TheGlowUp ,   and r / AskTruFemcels . Though the main user base   of the manosphere are men , there are also small   populations of women in other ideologies as well ,   such as r / redpillwomen . We mainly portion out   FDS and Femcels due to their role in Section 5.4 ’s   lexical variant experiment as communities who use   moids .   In total we have 12 subreddits in TRP , 11 in   MRA , 7 in PUA , 22 in Incels , 3 in MGTOW , 4   in Femcels , and 6 in FDS . The complete list of   subreddits and their categories is also in our Github   repo .   D Vocabulary creation   First , we extract nominal and proper persons using   NER , keeping ones that are popular ( occur at least   500 times in _ ) , and unambiguous ,   where at least 20 % of its instances in these datasets   are tagged as a person . Gathering a substantial   number of labels from our domain to train an in-   domain NER system from scratch is outside the   scope of our work , so we experimented with three   models trained on other labeled datasets : ACE ,   contemporary literature , and a combination of both .   We evaluated these models on a small set of posts   and comments labeled by one author , after retriev-   ing 25 examples per forum or Reddit ideological   category using reservoir sampling . The annotator   only labeled spans for nominal and named P   entities . Table 7 shows the performance of each   model on _ . Based on these evalua-   tion results , we chose to use the model trained on   contemporary literature .   We extract bigrams and unigrams from de-   tected spans , excluding determiners and posses-   sives whose heads are the root of the span . Named   entities that refer to types of people rather than   specific individuals were estimated through their   co - occurrence with the determiner a , e.g. a Chad .   Then , one author consulted community glos-   saries and examined in - context use of words to   manually correct the list of automatically extracted   terms . We include additional popular and unam-   biguous words not tagged sufficiently often enough   by NER , but defined as people in prior work and   online resources .   Table 8 lists the sources and glossaries for vocab-   ulary words and the ideologies they include . Some   of these sources , such as the Shedding of the Ego ,   are created by insiders in the community , while   some , such as academic papers and news articles ,   are by outsiders . For each of these glossaries and   lists of terms , we manually separated them into two   categories : 269 people ( singular and plural forms )   and 1776 non - people . Two of these sites , Shedding   of the Ego and Pualingo , no longer exists , but were   publicly available until at least late 2020 . We in-   clude 93 terms for people that were initially filtered   out in our NER pipeline in our final vocabulary ,   excluding ambiguous ones that also occur often   as non - human entities , such as tool(a fool who is   taken advantage of ) and jaw(short for just another   wannabe ) .   The resulting vocabulary contains niche lan-   guage , where 20.7 % of unigrams are not found   in WordNet , and 85.1 % of those missing are also   not in the Internet resource Urban Dictionary .   The full list is also available in our Github repo .   E Gender inference   This section includes additional details around our   gender inference process .   Our list of semantically gendered terms , or   words gendered by definition , expands upon the   one used by Hoyle et al . ( 2019 ): man , men , boy ,   boys , father , fathers , son , sons , brother , both-   ers , husband , husbands , uncle , uncles , nephew,3492   nephews , emperor , emperors , king , kings , prince ,   princes , duke , dukes , lord , lords , knight , knights ,   waiter , waiters , actor , actors , god , gods , policeman ,   policemen , postman , postmen , hero , heros , wizard ,   wizards , steward , stewards , woman , women , girl ,   girls , mother , mothers , daughter , daughters , sis-   ter , sisters , wife , wives , aunt , aunts , niece , nieces ,   empress , empresses , queen , queens , princess ,   princesses , duchess , duchesses , lady , ladies , dame ,   dames , waitress , waitresses , actress , actresses ,   goddess , goddesses , policewoman , policewomen ,   postwoman , postwomen , heroine , heroines , witch ,   witches , stewardess , stewardesses .   We include the following additional semantically   gendered terms : male , males , dude , dudes , guy ,   guys , boyfriend , boyfriends , bf , female , females ,   chick , chicks , girlfriend , girlfriends , gf , gal , gals ,   bro , transmen , transwomen , she , he .   We check if any of the above words appear in   a unigram or bigram vocabularly term . Around   29.9 % of our vocabulary in _ is gen-   dered through this word list approach .   To infer gender for the remaining words using   pronouns , we ran coreference resolution on- _ , and extracted all pronouns that are   clustered in coreference chains with terms in our   vocabulary ( Clark and Manning , 2016 ) . We label   the masculine to feminine leaning of vocab terms   by calculating the proportion of feminine pronouns   ( she , her , hers , herself ) over the sum of feminine   and masculine pronouns ( he , him , his , himself ) . We   only consider a word to have a usable gender sig-   nal if it appears in at least 10 coreference clusters   with feminine or masculine pronouns . Since pluralwords do not usually appear with he / shepronouns ,   we have plural words take on the gender leaning of   their singular forms . We pair plural and singular   forms using the Python package . We   also transfer unigrams ’ gender to bigrams , after   examining the modifiers ( the first token ) in bigram   terms to check that they are not differently and   semantically gendered . Around 20.9 % of our vo-   cabulary in _ is gendered through   pronouns alone , an additional 12.6 % is gendered   through plural to singular mapping , and an addi-   tional 9.1 % is gendered through bigram to unigram   mapping .   F High variance axes   Table 9 shows the top vocabulary terms that corre-   spond to the poles of high variance axes .   G Classification of lexical variants   Our main goal here is to tease out which axes dif-   ferentiate the contexts of lexical variants , rather   than find the best model that performs well on a   classification task . Therefore , we choose to use a   random forest classifier for its interpretability : it   outputs weights that indicate what features were   most important across its decisions . We use scikit-   learn ’s implementation , and perform randomized   search with 5 - fold cross validation and weighted   F1 scoring to select model parameters ( Table 10 ) .   Table 11 shows the most important axis features   of these models . In general , the set of most impor-   tant features did not change much with parameter   choices and roughly aligns with axes that showcase   the largest mean differences between each pair of   variants . That is , the three axes we show in the   main text in Figure 5 are also among the top ten   ordered by mean difference for men vs.moids and   women vs.femoids .   H Runtime and infrastructure   We only use BERT - base for inference , but the over-   all runtime cost is high due to the size of our cor-   pora : English Wikipedia and social media discus-   sions . We use one Titan XP GPU with 8 CPU   cores for most of the paper , and occasionally ex-   panded to multiple machines with 1080ti and K80   GPUs in parallel when handling social media data .   We use BERT for two main purposes : predicting   word probabilities to select contexts for construct-   ing axes , and obtaining word embeddings . On one3493   Titan XP GPU , the former takes ∼1 hour for one   million sentences containing one masked target   word each , and the latter takes ∼2.5 hours for one   million sentences , including wordpiece aggrega-   tion.3494