  Benfeng Xu , Quan Wang , Yajuan Lyu , Yabing Shi   Yong Zhu , Jie GaoandZhendong MaoUniversity of Science and Technology of China , Hefei , ChinaBeijing University of Posts and Telecommunications , Beijing , ChinaBaidu Inc. , Beijing , China   benfeng@mail.ustc.edu.cn , zdmao@ustc.edu.cn   Abstract   Multi - triple extraction is a challenging task due   to the existence of informative inter - triple cor-   relations , and consequently rich interactions   across the constituent entities and relations .   While existing works only explore entity rep-   resentations , we propose to explicitly intro-   duce relation representation , jointly represent it   with entities , and novelly align them to identify   valid triples . We perform comprehensive exper-   imentson document - level relation extraction   and joint entity and relation extraction along   with ablations to demonstrate the advantage of   the proposed method .   1 Introduction   Relation extraction aims at discovering struc-   tured knowledge in the form of < subject - relation-   object > triples from plain text . It is an essential   task towards constructing knowledge bases . Al-   though a lot of efforts have been made in build-   ing advanced relation extraction systems , it is   still a challenging problem under certain practi-   cal scenarios where multiple entities and relations   are involved , e.g. , document - level relation extrac-   tion ( Yao et al . , 2019 ) and joint entity and rela-   tion extraction ( Riedel et al . , 2010 ; Gardent et al . ,   2017 ) .   Existing works mostly take the entity perspec-   tivethat focuses on exploring cross - entity interac-   tions ( Xu et al . , 2021 ; Zeng et al . , 2020 ) . They   either treat relations as atomic labels speciﬁed in a   ﬁnal classiﬁer ( Xu et al . , 2021 ; Zeng et al . , 2020 ;   Wang et al . , 2020 ) , or simply search subjects and   objects for each individual relation(Wei et al . , 2020 ;   Zheng et al . , 2021 ) . However , as an essential com-   ponent , relations also interact with entities and con-   text , which jointly exhibit informative inter - tripleFigure 1 : Different formulations for multi - triple ex-   traction . 1 ) entity perspective constructs only entity   representation and feed them into a relation - speciﬁc   classiﬁer . 2 ) joint triple perspective constructs both en-   tity representation and relation representation to model   comprehensive correlations across all components .   correlations . e.g. , the two relations capital of and   located at often co - occur between the same pair of   entities but with different probabilities conditioned   on speciﬁc contextual clues . As a consequence ,   the capability to model and make full use of rich   interactions across relations , entities , and context   is crucial for the task .   In this paper , we advocate a novel joint triple   perspective for relation extraction ( see Figure 1 for   illustration ) . Different from previous works that   only seek to represent entities , we propose EmRel   that creates , reﬁnes and leverages the Embedded   representations of Relations . Speciﬁcally , we ﬁrst   explicitly create relation representations as embed-   ded vectors ; then reﬁne these relation ( as well as   entity ) representations by modeling rich relation-   entity - context interactions via an attention - based   fusion module ; and ﬁnally identify valid triples   by aligning the representation of entities and re-   lations in a joint space , with a novel alignment   function based on Tucker Decomposition . This   joint triple perspective actually considers entities   along with relations as components of a small , in-   context knowledge graph , and completes this graph   by aligning and reasoning to extract multiple valid   triples.659To demonstrate the advantage of the proposed   EmRel framework , we conduct experiments on   two speciﬁc scenarios of multi - triple extraction :   document - level relation extraction(RE ) and joint   entity and relation extraction , with three popu-   lar datasets including DocRED ( Yao et al . , 2019 ) ,   NYT ( Riedel et al . , 2010 ) and WebNLG ( Gardent   et al . , 2017 ) . The results verify the superiority of   the joint triple perspective over the traditional en-   tity perspective in multi - triple extraction . We also   provide further ablation study to show the effective-   ness of our fusion module and alignment function .   2 Related Works   Document - level Relation Extraction Extracting   multi - triples from document - level text has recently   aroused increasing interests ( Yao et al . , 2019 ) . Ex-   isting methods take the entity perspective that pro-   poses various techniques to model entity interac-   tions . Nan et al . ( 2020 ) and Zeng et al . ( 2020 )   construct an entity graph , and perform graph - level   reasoning to reﬁne the entity node representations .   Xu et al . ( 2021 ) introduces entity structure as useful   prior , and models such information within the trans-   former attention layer . Zhang et al . ( 2021 ) utilizes   a segmentation network to model the interdepen-   dency among entity pairs . Therefore , inter - triple   correlations are only captured at the entity level   while relation - based ones are neglected .   Joint Entity and Relation Extraction Joint en-   tity and relation extraction is a popular task that   extracts multi - triples along with their entities . Ex-   isting works can be concluded into two frameworks :   one that searches subjects and objects for each in-   dividual relation ( Liu et al . , 2020 ; Wang et al . ,   2020 ; Wei et al . , 2020 ) , and the other that directly   see each word as a candidate entity and assign them   with relation labels ( Gupta et al . , 2016 ; Zheng et al . ,   2021 ) . Both formulations do not explicitly include   inter - triple correlations . Very recently , Wu and Shi   ( 2021 ) propose to model the interdependencies be-   tween entity labels and relation labels . However ,   such correlation is constrained within a speciﬁc   word position , while EmRel exploits the global   correlations among all triples and across entities ,   context , and relations . Li et al . ( 2021 ) introduces   a translation - based function that predicts object   from subject and relation , while EmRel proposes a   more expressive alignment function that models the   ternary interaction of subject , relation and object . Relation Embedding There is one speciﬁc pre-   vious work ( Chen and Badlani , 2020 ) that also con-   siders modeling relation representations . CRE uses   the sentence representation as relation embeddings ,   and scores them with the entity embeddings trained   along with the knowledge base . This raises signif-   icant differences with EmRel in both 1 ) technical   design , EmRel constructs and models independent   relation representations that are not inherited from   speciﬁc context , and 2 ) task settings , CRE requires   all entities be aligned to an existing knowledge base   to train their embedding .   3 Methodology   3.1 Task Formulation   We ﬁrst formulate the multi - triple extraction task to   suitably contain both document - level RE and joint   entity and relation extraction . Given a sequence of   text{w } , a set of candidate entities E={e}and   the pre - deﬁned relations R={r } , the candidate   triples can be derived as :   T={<s , r , o>|s , o∈{e},r∈{r } } ( 1 )   the target is to assign each tinTa binary label that   discriminates its validity . The candidate entities   can either be pre - annotated , as in document - level   relation extraction , or be jointly recognized , as in   joint entity and relation extraction . In the latter sce-   nario , one prevailing solution is to directly see each   word as a candidate entity , such as tagging - based   methods ( Wang et al . , 2020 ) or table ﬁlling meth-   ods ( Gupta et al . , 2016 ) . Here we follow Wang   et al . ( 2020 ) as our baseline , and thus formulate   both tasks under a uniﬁed framework that extracts   multi - triples from a given candidate entity set .   3.2 EmRel   EmRel consists of three modules : Representation   Construction for both entities and relations , Rep-   resentation Fusion that captures multi - triple cor-   relations by modeling the informative interactions   across entities , context and relations , and Repre-   sentation Alignment that leverages these represen-   tations to extract triples by aligning their ternary   structures ( see Figure 2 for illustration ) .   Representation Construction The entity repre-   sentation is constructed similar to existing practices .   We employ a text encoder , e.g. , pretrained language   models like BERT ( Devlin et al . , 2019 ) , and obtain660   the output from its last layer on corresponding po-   sition as the contextualized representation :   which we denote as H∈R. Then we   construct each entity representation e∈Rby   applying a pooling operation on its corresponding   mention positions , and further map it into respec-   tive subject and object representation e , e. We   thus denote all extracted entity representations as   E , E∈R.   We embed the target relations Rinto an embed-   ding matrix R∈R , where each row R   represents a vectorized relation r. This matrix   is maintained as part of the model parameter and   trained accordingly .   Representation Fusion In order to jointly repre-   sent entities and relations in a shared knowledge   representation space , we fuse them to be aware of   each other . We adopt the attention network ( Bah-   danau et al . , 2015 ) to model inter - component in-   teractions , which has proven to be very successful   in modeling rich interactions across contexts ( Yu   et al . , 2018 ) or modalities ( Lu et al . , 2016 ) . Speciﬁ-   cally , we employ the canonical multi - head attention   ( MHA ) network ( Vaswani et al . , 2017 ) . Given the   target representation Xand the source represen-   tation X , each head of MHA operates them as :   where / hatwideXis the updated representation of X   w.r.t . X , all heads operate in parallel and will be   concatenated together .   InEmRel , to exploit the comprehensive inter-   actions across all components , we ﬁrst construct   entity / context - aware relation representation :   which are then aggregated together using layer nor-   malization :   we symmetrically construct relation - aware entity   representation :   s , o , care abbreviations for subject , object and   context . Each attention module is wrapped with   residual connection , feedforward layer , layer nor-   malization , and is instantiated with different pa-   rameters ofW , W , Wto model distinguished   attending patterns . The outputs of fusion module   are reﬁned representations /hatwideR,/hatwideE,/hatwideEfor relations ,   subjects and objects.661   Representation Alignment EmRel extracts   triples by aligning their ternary components /hatwideR,/hatwideE ,   and / hatwideE. In order to fully leverage their expressive-   ness , we propose factorization - based alignment   using Tucker decomposition ( Tucker et al . , 1964 ) .   We introduce a core tensor Z∈R , and the   validity for each < s , r , o > is scored as :   where ˆe=/hatwideE,ˆr=/hatwideR,ˆe=/hatwideE , and×   indicates tensor product along the n - th mode , σ   denotes sigmoid function . We compute φfor all   triples in parallel using batched tensor product , and   train them using cross - entropy loss :   where 1indicates the ground truth validity .   4 Experiments   4.1 Main Results   We conduct comprehensive experiments on   document - level RE dataset DocRED ( Yao et al . ,   2019 ) and joint entity and relation extrac-   tion dataset NYT ( Riedel et al . , 2010 ) and   WebNLG ( Gardent et al . , 2017 ) . We use BERT-   Base - Cased ( Devlin et al . , 2019 ) as the con-   text encoder and we also provide results with   RoBERTa ( Liu et al . , 2019 ) on DocRED . The di-   mension of embedded relation representation is set   as 768 for Base models , 1024 for Large models   on DocRED , and 128 on NYT / WebNLG . The   number of attention heads in the fusion module is   simply set as 4 . We provide our reproduced results   of TPLinker ( Wang et al . , 2020 ) and the baseline   system of Xu et al . ( 2021 ) . Both are competitive   baselines based on the entity perspective , and are   directly comparable with EmRel . Further speciﬁcs   about these datasets and implementation details can   be referred to Appendix .   The results ( see Table 1 and Table 2 ) show that   EmRel universally outperforms its baselines on all   datasets . Respectively , +0.3 F1 for NYT,+0.8   F1for WebNLG,+1.0 F1 for NYT and +1.1 F1   for WebNLG . On DocRED , EmRel improves the   baseline by +0.95 Dev F1 , +1.47 Test F1 , and   also outperforms several previous studies including   BERT - TS ( Wang et al . , 2019 ) , CorefBERT ( Ye   et al . , 2020 ) , LSR ( Nan et al . , 2020 ) , and SSAN ( Xu   et al . , 2021 ) . On stronger backbone encoders like   RoBERTa , similar improvements over baselines   can also be observed.662   4.2 Ablation Studies   On EmRel Modules We ﬁrst varify the design   ofEmRel modules . Table 3 shows that both fusion   and alignment module contribute to the improve-   ments . We also observe that EmRel has more robust   performance across multiple runs . This can be at-   tributed to our alignment function , which , once   removed , would result in an increased standard   deviation from±0.20to±0.47 .   On the Dimensionality of Relation Representa-   tions We investigate the effects of choices for   din Fig 3 . First of all , the advantage of EmRel   is general across variant choices comparing to the   baseline . As we gradually set a higher dfrom 64 to   1024 , we get improved performance for its stronger   expressive capability . While we further increase   dto 2048 , the performance starting to degrades ,   which might attribute to overﬁtting . Overall , the   optimal dimension lies within [ 512 , 2048 ] , which is   quite robust and also computationally acceptable .   5 Conclusion   In this paper , we propose EmRel for multi - triple   extraction . Distinguished from existing works , Em-   Relexplicitly creates , reﬁnes , and leverages the   embedded representation of relations . Notably , we   design a novel alignment function that discrimi-   nates triple validity by aligning its components in   a joint representation space . We conduct experi-   ments on both document - level relation extraction   and joint entity and relation extraction , to demon-   strate the advantage of EmRel over its baselines .   EmRel also provides a new joint triple perspec-   tive , where multi - triple extraction is formulated as   completion of a small , context - dependent knowl-   edge graph , with candidate entities and relations as   its components . In the future , we think more intri-   cate techniques e.g. , graph - based reasoning , can be   explored following such formulation . Acknowledgements   We thank all anonymous reviewers for their valu-   able comments . This work is supported by the   National Key Research and Development Program   of China No.2020AAA0109400 , the National Nat-   ural Science Foundation of China under grants   No.61876223 , No . U19A2057 .   References663664   A Benchmarks   We introduce the benchmarks used in this work .   Table 4 gives their detailed statistics . DocRED is   constructed from Wikipedia document . It provides   comprehensive human annotations for entity men-   tions , entity types , relational triples , along with   their supporting evidences . Each document is a   semantically integrate unit that centers in one con-   cept ( the title of the wiki page ) , resulting multiple   triples with rich correlations . NYT is constructed   from New York Times news articles and annotated   through distant supervision . WebNLG is originally   created for natural language generation task , and   the sentences are written by humans to cover given   triples . Both datasets have the other version de-   noted as NYTand WebNLG . The texts in NYT   and WebNLG are much shorter than DocRED doc-   uments . These two datasets also feature in multiple   triples . In this paper , we solve all three datasets   under a uniﬁed multi - triple extraction formulation   with EmRel .   B Implementation Details   To provide comparable results , we set hyper-   parameters following previous works ( Wang et al . ,   2020 ; Xu et al . , 2021 ) . On NYT / WebNLG , we   set learning rate as 5e-5 , batch size as 24 / 6 , and   epoch as 100 , as each word is seen as a candidate   entity , we directly take the word representation as   entity representation . On DocRED , we set learning   rate as 3e-5 , batch size as 4 , and search epochs in   { 40 , 60 , 80 , 100 } . Each document is truncated by   512 sequence length . Entity representation is con-   structed by pooling from all its mention positions .   To produce more robust results , we further perform   multiple searches using 5 different seeds , resulting   a grid search on both epochs and random seeds .   The mean and standard deviation results across dif-   ferent seed are reported on development set . Allexperiments are conducted on a single NIVDIA   V100 or A100 GPU machine .   C Grouped Alignment   The WebNLG dataset has up to 216 relations ,   which requires increased computational cost . In-   spired by ( Zheng et al . , 2019 ) , we split the align-   ment tensors into Ngroups across its dimensions   to reduce the computational overhead , and re - write   Eq . 7 as :   φ(s , p , o ) = /summationdisplayZ×ˆe×ˆr×ˆe+b   ( 9 )   ˆe=/hatwideE   ˆr=/hatwideR   ˆe=/hatwideE(10 )   We set group Nto 4 for WebNLG , and 1 for other   datasets ( that is , without further spliting).665